1
00:00:00,090 --> 00:00:03,000
The following is a conversation with 
Juergen Schmidhuber.

2
00:00:03,590 --> 00:00:04,423
He's the CO director of at [inaudible] 
Swiss ai lab and a cocreator of long 

3
00:00:08,061 --> 00:00:08,894
short term memory networks.
Lsts are used in billions of devices 

4
00:00:12,961 --> 00:00:17,961
today for speech recognition,
translation and much more over 30 years.

5
00:00:18,690 --> 00:00:22,170
He has proposed a lot of interesting out
of the box ideas,

6
00:00:22,410 --> 00:00:25,860
a metal learning adversarial networks,
computer vision,

7
00:00:26,070 --> 00:00:29,730
and even a formal theory of quote,
creativity,

8
00:00:29,790 --> 00:00:30,623
curiosity and fun.
This conversation is part of the mit 

9
00:00:34,651 --> 00:00:35,484
course and artificial general 
intelligence and the artificial 

10
00:00:37,411 --> 00:00:41,070
intelligence podcast.
If you enjoy subscribe on Youtube,

11
00:00:41,100 --> 00:00:45,090
itunes or simply connect with me on 
twitter at [inaudible] Friedman,

12
00:00:45,240 --> 00:00:46,530
spelled f,
r I.

13
00:00:46,531 --> 00:00:47,364
D.
And now here's my conversation with 

14
00:00:49,981 --> 00:00:50,814
Juergen Schmidhuber early on you dreamed
of AI systems that self improve 

15
00:00:57,410 --> 00:01:00,230
cursively when was that dream born

16
00:01:01,690 --> 00:01:05,310
man and was a baby now has not true when
I was a teenager.

17
00:01:06,470 --> 00:01:09,680
And what was the catalyst for that 
birth?

18
00:01:09,681 --> 00:01:11,480
What was the thing that first inspired 
you

19
00:01:13,090 --> 00:01:14,660
when it wasn't?
Why I'm,

20
00:01:17,680 --> 00:01:18,513
I was thinking about what to do in my 
life and then I thought the most 

21
00:01:23,080 --> 00:01:26,380
exciting thing as to soul,
the riddles,

22
00:01:26,520 --> 00:01:27,790
the universe.
And,

23
00:01:28,150 --> 00:01:30,130
and that means you have to become a 
physicist.

24
00:01:30,880 --> 00:01:35,080
However,
then I realized that that is something,

25
00:01:35,081 --> 00:01:35,914
even grandma,
you can't try to build a machine that 

26
00:01:40,061 --> 00:01:40,894
isn't really a machine any longer.
That learns to become a much better 

27
00:01:43,781 --> 00:01:48,460
physicist then I could ever hope to be.
And that's how I thought.

28
00:01:48,461 --> 00:01:53,290
Maybe I can multiply my tiny little bit 
of creativity and to engineer

29
00:01:53,380 --> 00:01:54,213
team,
but ultimately that creativity will be 

30
00:01:56,561 --> 00:01:59,170
multiplied to understand the universe 
around us.

31
00:01:59,200 --> 00:02:01,030
That's,
that's the,

32
00:02:02,200 --> 00:02:05,380
the curiosity for that mystery that that
drove you.

33
00:02:05,740 --> 00:02:06,220
Yes.
Uh,

34
00:02:06,220 --> 00:02:11,220
so if you can build a machine that lance
to solve more and more complex problems,

35
00:02:13,841 --> 00:02:16,330
I'm more and more general problem 
solver,

36
00:02:16,840 --> 00:02:21,840
then you basically have solved all the 
problems,

37
00:02:22,630 --> 00:02:25,300
at least all the solvable problems.

38
00:02:26,030 --> 00:02:26,863
So how do you think,
what is the mechanism for that kind of 

39
00:02:28,971 --> 00:02:33,971
general solver look like?
Obviously we don't quite yet have one or

40
00:02:35,121 --> 00:02:35,954
know how to build one boy of ideas.
And you have had throughout your career 

41
00:02:39,171 --> 00:02:40,004
several ideas about it.
So how do you think about that 

42
00:02:41,961 --> 00:02:42,794
mechanism?

43
00:02:43,650 --> 00:02:44,483
So in the 80s,
I thought about how to build this 

44
00:02:48,181 --> 00:02:53,181
machine that lance was so of all these 
columns and I cannot solve myself.

45
00:02:54,180 --> 00:02:55,013
And I thought it is clear that it has to
be a machine that not only learns too 

46
00:02:59,440 --> 00:03:02,320
solve this problem here and this problem
here,

47
00:03:02,680 --> 00:03:07,680
but it also has to learn to improve the 
learning algorithm itself,

48
00:03:08,640 --> 00:03:12,520
right?
So it has to have the learning algorithm

49
00:03:12,521 --> 00:03:15,550
and um,
representation that allows it to inspect

50
00:03:15,551 --> 00:03:20,551
it and modify it so that it can come up 
with a better learning algorithm.

51
00:03:22,090 --> 00:03:23,620
So when I called that and metal 
learning,

52
00:03:24,010 --> 00:03:27,670
learning to loan and recursive self 
improvement,

53
00:03:28,060 --> 00:03:32,440
that is really the pinnacle of that 
where you then not only you learn,

54
00:03:32,870 --> 00:03:33,703
um,
how to improve on that problem and on 

55
00:03:37,211 --> 00:03:40,210
that,
but you also improve the way the machine

56
00:03:40,220 --> 00:03:44,440
improves and you also improve the way it
improves the way and improves itself.

57
00:03:45,760 --> 00:03:46,593
And that was my 1987 diploma thesis,
which was all about that hierarchy of 

58
00:03:52,120 --> 00:03:52,953
metal rnrs that have no computational 
limits except for the well known limits 

59
00:03:59,920 --> 00:04:04,210
that Google identified in 1931 and a 
four.

60
00:04:04,211 --> 00:04:05,500
The limits of physics

61
00:04:06,500 --> 00:04:10,320
in the recent years matter learning has 
gained popularity in a,

62
00:04:10,630 --> 00:04:11,463
in a specific kind of form.
You've talked about how that's not 

63
00:04:14,151 --> 00:04:17,230
really metal learning with,
with neural networks,

64
00:04:17,260 --> 00:04:18,093
that's more basic transfer learning.
Can you talk about the difference 

65
00:04:22,721 --> 00:04:23,554
between the big general metal learning 
and a more narrow sense of metal 

66
00:04:27,671 --> 00:04:30,730
learning the way it's used today,
the waste talked about today.

67
00:04:30,880 --> 00:04:31,450
Let's take,

68
00:04:31,450 --> 00:04:32,283
what's the example of a deep neural 
network that has a learn to classify 

69
00:04:35,710 --> 00:04:36,543
images and maybe you have trained that 
network on 100 different databases of 

70
00:04:43,061 --> 00:04:43,894
images and now a new database comes 
along and you want to quickly learn the 

71
00:04:50,471 --> 00:04:51,304
new thing as well.
So when a simpler way of doing that as 

72
00:04:55,151 --> 00:04:55,984
you take the network,
which already knows 100 types of 

73
00:05:00,850 --> 00:05:01,683
databases and then you would just take 
the top layer of that and you retrain 

74
00:05:07,451 --> 00:05:09,790
that,
uh,

75
00:05:09,820 --> 00:05:13,930
using the new label data that you have 
in the new image database.

76
00:05:14,950 --> 00:05:19,390
And then it turns out that it really,
really quickly kind of learn that to one

77
00:05:19,391 --> 00:05:24,130
shot basically,
because from the first 100 datasets,

78
00:05:24,370 --> 00:05:25,203
it already has learned so much about,
about computer vision that it can reuse 

79
00:05:28,691 --> 00:05:29,524
that.
And that is then almost good enough to 

80
00:05:32,201 --> 00:05:36,940
solve the new task except you need a 
little bit of adjustment on the top.

81
00:05:38,460 --> 00:05:42,370
So that is transfer learning and it has 
been done.

82
00:05:42,371 --> 00:05:43,204
And principal for many decades,
people have done similar things with 

83
00:05:45,961 --> 00:05:50,961
decades met aligning.
True mental learning is about having the

84
00:05:52,590 --> 00:05:53,423
learning algorithm itself open to 
introspection by the system that is 

85
00:05:59,661 --> 00:06:00,494
using it and also open to modification 
such that the lighting system has an 

86
00:06:06,980 --> 00:06:11,980
opportunity to modify any part of the 
learning algorithm and then evaluate the

87
00:06:14,271 --> 00:06:15,104
consequences of that modification and 
then learn from that to create a better 

88
00:06:21,471 --> 00:06:22,304
learning algorithm and so on.
Recursively so that's a very different 

89
00:06:27,610 --> 00:06:28,443
animal where you are opening the space 
off possible learning algorithms to the 

90
00:06:34,061 --> 00:06:35,380
learning system itself.

91
00:06:35,530 --> 00:06:37,060
Right.
So you've,

92
00:06:37,061 --> 00:06:37,894
uh,
like in the 2004 paper he described a 

93
00:06:40,181 --> 00:06:43,540
gate on machines and programs that were 
right themselves.

94
00:06:43,600 --> 00:06:44,433
Yeah.
Right.

95
00:06:44,500 --> 00:06:47,500
Philosophically,
and even in your paper mathematically,

96
00:06:47,501 --> 00:06:48,334
these are really compelling ideas.
But practically do you see the self 

97
00:06:54,060 --> 00:06:54,893
referential programs being successful in
the near term to having an impact where 

98
00:07:00,030 --> 00:07:05,030
sort of a demonstrates to the world that
this direction is a

99
00:07:06,240 --> 00:07:08,160
is a good one to pursue in the near 
term?

100
00:07:08,670 --> 00:07:09,503
Yes.
We had these two different types of 

101
00:07:12,050 --> 00:07:15,470
fundamental research,
how to build a universal problem solver.

102
00:07:15,800 --> 00:07:16,633
One basically exploiting poof such and 
things like that that you need to come 

103
00:07:24,791 --> 00:07:29,791
up with a some topically optimal 
theoretically optimal self improvers and

104
00:07:31,960 --> 00:07:36,960
problem solvers.
However one has to admit that.

105
00:07:37,790 --> 00:07:42,790
So it was this proof.
So ads comes in an additive constant,

106
00:07:43,630 --> 00:07:48,630
an overhead and additive overhead that 
vantages in comparison to uh,

107
00:07:52,420 --> 00:07:54,700
what you have to do to solve large 
problems.

108
00:07:55,180 --> 00:07:56,013
However,
for many of the small problems that we 

109
00:07:58,391 --> 00:08:03,220
want to solve in our everyday life,
we cannot ignore this constant overhead.

110
00:08:03,460 --> 00:08:07,840
And that's why we also have been doing 
other things,

111
00:08:08,110 --> 00:08:12,160
non universal things such as recurrent 
neural networks,

112
00:08:12,161 --> 00:08:17,161
which are trained by gradient descent 
and local search techniques which aren't

113
00:08:17,711 --> 00:08:21,130
universally adored,
which aren't provably optimal at all.

114
00:08:21,280 --> 00:08:25,960
Like the other stuff that we did but 
which are much more practical as long as

115
00:08:25,961 --> 00:08:30,961
we only want to solve the small problems
that we are typically trying to solve in

116
00:08:33,520 --> 00:08:35,420
this environment here.
Yeah,

117
00:08:35,620 --> 00:08:36,453
so it's a universal problem solvers and 
like the Google machine but also marcus 

118
00:08:40,061 --> 00:08:40,894
hooters,
fastest way of solving all possible 

119
00:08:43,301 --> 00:08:44,134
problems,
which he developed around 2002 in my 

120
00:08:47,471 --> 00:08:48,304
lab.
They are associated with these constant 

121
00:08:51,761 --> 00:08:52,594
overheads for proof search,
which guarantees that the that you're 

122
00:08:55,441 --> 00:08:58,320
doing is optimum,
for example,

123
00:08:59,090 --> 00:09:04,090
that is this fastest way off solving all
problems with a computable solution,

124
00:09:05,310 --> 00:09:09,020
which is due to a macros macro Ceuta and
uh,

125
00:09:09,810 --> 00:09:12,250
um,
to explain what's going on there.

126
00:09:12,251 --> 00:09:17,251
Let's take traveling salesmen problems 
with traveling salesman problems.

127
00:09:17,400 --> 00:09:18,233
You have a number of cities in cities 
and you try to find the shortest path 

128
00:09:23,670 --> 00:09:27,670
through all these cities without 
visiting any city twice.

129
00:09:29,470 --> 00:09:34,470
And nobody knows the fastest way of 
solving traveling salesman problems.

130
00:09:35,380 --> 00:09:36,213
Tsp is,
but let's assume there is a method of 

131
00:09:40,661 --> 00:09:41,494
solving them within end to the five 
operations where n is the number of 

132
00:09:47,861 --> 00:09:48,694
cities.
Then the universal method of Macros is 

133
00:09:54,761 --> 00:09:58,570
going to solve the same traveling 
salesman problem.

134
00:09:58,571 --> 00:09:59,404
Also within enter the five steps plus a 
couple of one plus a constant number of 

135
00:10:05,561 --> 00:10:09,000
steps that you need for the proofs 
archer,

136
00:10:09,250 --> 00:10:10,083
which you need to show that this 
particular class of problems that 

137
00:10:14,831 --> 00:10:18,250
traveling salesman,
salesman problems can be solved within a

138
00:10:18,251 --> 00:10:20,500
certain time bound,
um,

139
00:10:20,530 --> 00:10:23,680
within Oda into the five steps 
basically.

140
00:10:24,400 --> 00:10:25,233
And there's a additive constant doesn't 
care for and which means as end is 

141
00:10:29,681 --> 00:10:34,240
getting larger and larger as you have 
more and more cities,

142
00:10:34,900 --> 00:10:35,733
the constant overhead pales in 
comparison and that means that almost 

143
00:10:40,241 --> 00:10:45,241
are large problems are solved in the 
best possible way of a today.

144
00:10:46,630 --> 00:10:49,720
We already have a universal problem 
solver like that.

145
00:10:50,590 --> 00:10:54,370
However,
it's not practical because the overhead,

146
00:10:54,610 --> 00:10:55,443
the constant overhead is so large that 
for the smaller kinds of problems that 

147
00:11:00,490 --> 00:11:03,670
you want to solve in this level 
biosphere,

148
00:11:04,680 --> 00:11:06,270
by the way,
when you say small,

149
00:11:06,480 --> 00:11:07,313
you're talking about things that fall 
within the constraints of our 

150
00:11:09,721 --> 00:11:13,110
computational systems that they can,
they can seem quite large joints,

151
00:11:13,111 --> 00:11:13,944
mere humans,

152
00:11:14,210 --> 00:11:14,961
right?
That's right.

153
00:11:14,961 --> 00:11:15,794
Yeah.
So they seem large and even unsolvable 

154
00:11:19,100 --> 00:11:19,933
in a practical sense today,
but they are still small compared to 

155
00:11:23,570 --> 00:11:24,403
almost all problems because almost all 
problems are large problems which are 

156
00:11:29,030 --> 00:11:30,830
much larger than any constant.

157
00:11:32,040 --> 00:11:32,873
Do you find it useful as a person who is
dreamed of creating a general learning 

158
00:11:37,621 --> 00:11:40,680
system,
has worked on creating one is done a lot

159
00:11:40,681 --> 00:11:43,470
of interesting ideas there to think 
about

160
00:11:44,190 --> 00:11:45,023
p

161
00:11:45,110 --> 00:11:45,943
versus np,
this a formalization of how hard 

162
00:11:49,701 --> 00:11:50,534
problems are,
how they scale this kind of case 

163
00:11:53,621 --> 00:11:54,454
analysis type of thinking.
Do you find that useful or is it only 

164
00:11:57,701 --> 00:12:02,701
just a mathematical,
it's a set of mathematical techniques to

165
00:12:02,801 --> 00:12:05,080
give you intuition about what's good and
bad.

166
00:12:05,210 --> 00:12:07,760
Hmm.
So p versus NP,

167
00:12:07,790 --> 00:12:12,740
that's super interesting from a 
theoretical point of view and in fact as

168
00:12:12,741 --> 00:12:13,574
you thinking about that problem,
you can also get inspiration for better 

169
00:12:18,380 --> 00:12:20,630
practical problems.
All of us,

170
00:12:21,340 --> 00:12:22,173
on the other hand,
we have to admit that at the moment as 

171
00:12:24,701 --> 00:12:29,701
the best practical problems,
all of us for all kinds of problems that

172
00:12:30,321 --> 00:12:33,380
we are now solving through what is 
called ai at the moment.

173
00:12:35,100 --> 00:12:35,933
No,
not the kind that is inspired by these 

174
00:12:37,441 --> 00:12:38,220
questions.
Yeah,

175
00:12:38,220 --> 00:12:39,200
no,
they have,

176
00:12:39,201 --> 00:12:40,870
you are using,
um,

177
00:12:41,350 --> 00:12:44,620
general purpose computer such as 
recurrent neural networks,

178
00:12:44,830 --> 00:12:49,050
but we have a such technique which is 
just local search,

179
00:12:49,051 --> 00:12:49,884
great in descend to try to find a 
program that is running on the 

180
00:12:52,950 --> 00:12:53,783
[inaudible] such that it can solve some 
interesting problems such as speech 

181
00:12:59,431 --> 00:13:02,850
recognition or machine translation and 
something like that.

182
00:13:03,360 --> 00:13:04,193
And there is very little theory behind 
the best solutions that we have at the 

183
00:13:09,181 --> 00:13:09,750
moment

184
00:13:09,750 --> 00:13:12,670
that can do that.
Do you think that needs to change?

185
00:13:12,671 --> 00:13:14,970
Do you think that will change or can we 
go,

186
00:13:15,150 --> 00:13:19,440
can we create a general intelligence 
systems without ever really proving that

187
00:13:19,441 --> 00:13:23,670
that system was intelligent and some 
kind of mathematical way solving machine

188
00:13:23,671 --> 00:13:24,504
translation perfectly or something like 
that within some kind of syntactic 

189
00:13:27,700 --> 00:13:31,770
definition of a language?
Or can we just be super impressed by the

190
00:13:31,771 --> 00:13:34,560
thing working extremely well and that's 
sufficient?

191
00:13:35,090 --> 00:13:39,050
There's an old saying and I don't know 
who brought it up first,

192
00:13:39,350 --> 00:13:44,350
which says there's nothing more 
practical than a good theory and um,

193
00:13:46,320 --> 00:13:50,330
yeah,
and a good theory off problem solving.

194
00:13:52,380 --> 00:13:52,800
Okay.

195
00:13:52,800 --> 00:13:57,800
Under limited resources like he in this 
universe or on this little planet has to

196
00:13:58,891 --> 00:14:00,990
take into account these limited 
resources.

197
00:14:01,830 --> 00:14:02,663
And so probably that is locking a theory
which is related to what we already 

198
00:14:10,361 --> 00:14:11,194
have.
Fees isn't tightly optimal problems 

199
00:14:13,570 --> 00:14:14,950
almost,
which,

200
00:14:15,340 --> 00:14:16,173
which tells us what we need.
In addition to that to come up with a 

201
00:14:19,331 --> 00:14:20,164
practically optimal problems over psalm,
I believe we will have something like 

202
00:14:26,081 --> 00:14:31,030
that and maybe just a few little tiny 
twists unnecessary to,

203
00:14:31,960 --> 00:14:35,710
to change what we already have to come 
up with that as well.

204
00:14:36,340 --> 00:14:37,173
As long as we don't have that,
we admit that we are taking sub optimal 

205
00:14:41,981 --> 00:14:46,981
ways and recurrent neural network isn't 
long short term memory for equipped with

206
00:14:48,831 --> 00:14:53,570
local search techniques.
And we are happy that it works better as

207
00:14:53,571 --> 00:14:56,370
in any competing methods.
But um,

208
00:14:56,600 --> 00:15:00,140
that doesn't mean that we would be,
think we had done.

209
00:15:00,870 --> 00:15:04,320
You've said that an Agi system will 
ultimately be a simple one,

210
00:15:04,570 --> 00:15:07,620
a general intelligence system,
and ultimately be as simple one,

211
00:15:08,010 --> 00:15:11,430
maybe a pseudocode have a few lines,
we'll be able to describe it.

212
00:15:11,820 --> 00:15:16,410
Can you talk through your intuition 
behind this idea,

213
00:15:16,800 --> 00:15:21,800
why you feel that us as core 
intelligence is a simple

214
00:15:24,810 --> 00:15:29,810
algorithm experience tells us that the 
stuff that works best as really simple.

215
00:15:33,150 --> 00:15:38,150
So the [inaudible] ugly optimal ways of 
solving problems if you look at them and

216
00:15:38,971 --> 00:15:41,010
just a few lines of code,
it's really true,

217
00:15:41,850 --> 00:15:45,210
although they ask these amazing property
is just a few lines of code.

218
00:15:45,810 --> 00:15:46,643
Then the most promising and most useful 
practical things maybe don't have this 

219
00:15:54,901 --> 00:15:57,600
proof of optimality associated with 
them.

220
00:15:57,810 --> 00:16:00,000
However they are.
So just a few lines of code.

221
00:16:00,870 --> 00:16:01,703
The most successful and recurrent neural
networks you can write them down and 

222
00:16:06,421 --> 00:16:07,950
five lions up pseudo code.

223
00:16:08,290 --> 00:16:10,870
That's a beautiful,
almost poetic idea.

224
00:16:10,930 --> 00:16:13,510
But what you're

225
00:16:14,010 --> 00:16:14,380
okay

226
00:16:14,380 --> 00:16:15,213
describing there is this,
the lines of pseudocode are sitting on 

227
00:16:17,921 --> 00:16:21,280
top of layers and layers of abstractions
in a sense.

228
00:16:22,270 --> 00:16:27,270
So you're saying at the very top,
it'll be a beautifully written sort of a

229
00:16:30,070 --> 00:16:30,903
algorithm,
but do you think that there's many 

230
00:16:33,101 --> 00:16:36,880
layers of abstraction we have to first 
learn to construct?

231
00:16:37,470 --> 00:16:40,930
Of course we are building on all these 
um,

232
00:16:41,340 --> 00:16:45,330
great obstructions that people have 
invented over the millennia.

233
00:16:46,020 --> 00:16:46,853
Such as Matrix multiplications and roll 
numbers and basic arithmetic x and 

234
00:16:56,370 --> 00:16:57,203
Calculus and durations of um,
error functions and derivatives off 

235
00:17:02,551 --> 00:17:07,551
error functions and stuff like that.
So without that language,

236
00:17:08,100 --> 00:17:13,100
that greatly simplifies our way of 
thinking about these problems.

237
00:17:13,860 --> 00:17:15,780
We couldn't do anything.
So in that sentence,

238
00:17:15,781 --> 00:17:16,614
as always,
we are standing on the shoulders of the 

239
00:17:18,901 --> 00:17:21,920
giants who in the past,
um,

240
00:17:22,210 --> 00:17:25,470
simply fired the problem off problem 
solving.

241
00:17:25,500 --> 00:17:29,130
So much that now we have a chance to do 
the final step

242
00:17:29,900 --> 00:17:34,380
to the final step will be as simple one.
If we,

243
00:17:34,420 --> 00:17:35,253
if we take a step back through all of 
human civilization and just the 

244
00:17:37,081 --> 00:17:39,000
university chair,
uh,

245
00:17:40,000 --> 00:17:40,833
how do you think about evolution and 
what if creating a universe is required 

246
00:17:45,401 --> 00:17:50,401
to achieve this final step?
What if going through very painful,

247
00:17:50,970 --> 00:17:51,803
an inefficient process of evolution is 
needed to come up with this set of 

248
00:17:55,291 --> 00:17:57,360
abstractions that ultimately to 
intelligence?

249
00:17:57,780 --> 00:17:58,613
Do you think

250
00:17:59,030 --> 00:18:02,330
there's a shortcut or do you think we 
have to create

251
00:18:03,360 --> 00:18:04,193
something like our universe in order to 
create something like human level 

252
00:18:06,991 --> 00:18:07,824
intelligence?

253
00:18:07,880 --> 00:18:09,940
Mm.
So far,

254
00:18:09,941 --> 00:18:14,941
the only example we have is this one is 
this universe in which we live.

255
00:18:15,161 --> 00:18:20,161
You better,
maybe not,

256
00:18:22,100 --> 00:18:23,320
but um,
yeah,

257
00:18:23,390 --> 00:18:24,960
part of this whole process,
right?

258
00:18:27,430 --> 00:18:28,263
Apparently.
So it might be the case that the code 

259
00:18:30,851 --> 00:18:31,684
that runs the universe Israeli really 
simple everything points to that 

260
00:18:35,891 --> 00:18:40,891
possibility because gravity and other 
basic forces are really simple laws that

261
00:18:42,701 --> 00:18:46,300
can be easily described also in just a 
few lines of code basically.

262
00:18:47,080 --> 00:18:48,850
And uh,
and then,

263
00:18:48,900 --> 00:18:49,733
uh,
the use of the events that the 

264
00:18:53,291 --> 00:18:56,380
apparently random events in the history 
of the universe,

265
00:18:56,530 --> 00:19:00,370
which as far as we know at the moment 
don't have a compact code,

266
00:19:00,700 --> 00:19:01,533
but who knows,
maybe somebody and the near future is 

267
00:19:03,341 --> 00:19:06,790
going to figure it out.
Pseudo random generator,

268
00:19:06,791 --> 00:19:08,200
which is,
um,

269
00:19:08,710 --> 00:19:13,030
which is computing,
whether it's the measurement of that,

270
00:19:13,060 --> 00:19:14,620
um,
spin up and down.

271
00:19:14,800 --> 00:19:18,310
The thing here is I'm going to be 
positive or negative

272
00:19:18,380 --> 00:19:19,213
underlying quantum mechanics.
Do you ultimately think quantum 

273
00:19:22,161 --> 00:19:26,630
mechanics is a pseudo random number 
generator all deterministic?

274
00:19:26,930 --> 00:19:28,550
There's no randomness scenario.
Our universe

275
00:19:30,430 --> 00:19:32,600
does God play dice?
Okay.

276
00:19:33,390 --> 00:19:35,950
A couple of years ago on a famous 
physicist,

277
00:19:37,060 --> 00:19:39,460
quantum physicists,
Anton Zeilinger,

278
00:19:39,490 --> 00:19:40,323
he wrote an essay in nature and it 
started more or less like that one 

279
00:19:47,630 --> 00:19:49,730
aussie fundamental insights,
ah,

280
00:19:49,760 --> 00:19:50,593
see offset 20th century was that the 
universe is fundamentally random on the 

281
00:20:00,441 --> 00:20:01,274
quantum level.

282
00:20:02,600 --> 00:20:02,930
Yeah.

283
00:20:02,930 --> 00:20:07,590
And that whenever you measure spin up or
down or something like that,

284
00:20:08,290 --> 00:20:11,100
a new bit of information enters the 
history.

285
00:20:11,101 --> 00:20:16,020
I'll see you on it.
And while I was reading that,

286
00:20:16,021 --> 00:20:16,854
I was honored the typing,
the responds and they had to publish it 

287
00:20:20,401 --> 00:20:24,270
because it was right that there is no 
evidence,

288
00:20:24,271 --> 00:20:29,271
no physical evidence for that.
So there is an alternative explanation,

289
00:20:30,450 --> 00:20:31,283
but everything that'd be considered 
around them is actually the random such 

290
00:20:36,121 --> 00:20:40,980
as the decimal expansion of Pi 3.141
and so on,

291
00:20:41,730 --> 00:20:42,870
which looks around,
Huh,

292
00:20:43,080 --> 00:20:43,913
but isn't.

293
00:20:44,400 --> 00:20:49,120
So Pi is interesting because every three
digit sequence,

294
00:20:49,900 --> 00:20:50,733
every sequence off three digits,
a p is roughly one in a thousand times 

295
00:20:55,990 --> 00:21:00,990
and every five digit sequence a p has 
roughly one in 10,000

296
00:21:02,621 --> 00:21:03,700
times.
What do you,

297
00:21:03,710 --> 00:21:06,850
what you would expect if it was run 
random.

298
00:21:07,000 --> 00:21:10,720
But that's a very short algorithm,
a short program that computers,

299
00:21:10,721 --> 00:21:13,420
all of that.
So it's extremely compressible.

300
00:21:13,870 --> 00:21:14,703
And who knows,
maybe tomorrow somebody and some Grad 

301
00:21:16,421 --> 00:21:20,800
student at som goes back over all these 
data points,

302
00:21:20,920 --> 00:21:23,440
better decay and whatever and figures 
out,

303
00:21:23,470 --> 00:21:24,303
oh,
it's the second billion digits of Pi or 

304
00:21:27,341 --> 00:21:28,174
something like that.
We don't have any fundamental reason at 

305
00:21:31,001 --> 00:21:31,834
the moment to believe that this is truly
random and not just a deterministic 

306
00:21:37,871 --> 00:21:41,110
video game.
If it was a deterministic video game,

307
00:21:41,111 --> 00:21:45,010
it would be much more beautiful because 
beauty,

308
00:21:45,110 --> 00:21:45,943
simplicity and many of the basic laws of
the universe like gravity and the other 

309
00:21:52,481 --> 00:21:54,130
basic forces are very simple.

310
00:21:54,130 --> 00:21:59,130
So very short programs can explain what 
these are doing and um,

311
00:22:00,550 --> 00:22:04,540
and it would be awful and ugly.
The universe would be ugly.

312
00:22:04,541 --> 00:22:08,140
The history of the universe would be 
ugly if for the extra things,

313
00:22:08,141 --> 00:22:12,700
the random seemingly random data points 
that we get all the time,

314
00:22:13,840 --> 00:22:18,840
that we really need a huge number of 
extra bits to strive all these,

315
00:22:19,960 --> 00:22:23,170
um,
these extra bits of information.

316
00:22:24,790 --> 00:22:29,790
So as long as we don't have evidence 
that that is no short programs,

317
00:22:29,801 --> 00:22:33,790
that computes the entire history of the 
entire universe,

318
00:22:35,320 --> 00:22:40,320
we are as scientists compelled to look 
further for that shortest program.

319
00:22:43,730 --> 00:22:46,910
Your intuition says there exists a 
shortage,

320
00:22:46,970 --> 00:22:51,320
a program that can backtrack to the,
to the creation of the universe,

321
00:22:52,550 --> 00:22:54,590
the shortest path to the creation.
Yes.

322
00:22:54,870 --> 00:22:55,703
Including all the um,
entanglement things and all the spin up 

323
00:23:00,541 --> 00:23:05,541
and down measurements that have been 
taken place,

324
00:23:05,700 --> 00:23:09,840
um,
since 13.8

325
00:23:09,841 --> 00:23:10,850
billion years ago.
And so,

326
00:23:11,370 --> 00:23:12,203
yeah,
so we don't have a proof that it is a 

327
00:23:15,300 --> 00:23:16,133
random,
we don't have a proof of that it is 

328
00:23:18,840 --> 00:23:22,350
compressible to a short program.
But as long as we don't have that fruit,

329
00:23:22,380 --> 00:23:27,360
we are obliged as scientists to keep 
looking for that simple explanation.

330
00:23:27,680 --> 00:23:28,513
Absolutely.
So you said simplicity is beautiful or 

331
00:23:30,321 --> 00:23:32,660
beauties simple.
Either one works,

332
00:23:33,260 --> 00:23:36,920
but you also work on curiosity,
discovery,

333
00:23:37,600 --> 00:23:41,300
you know,
the romantic notion of randomness,

334
00:23:41,690 --> 00:23:43,310
of serendipity,
of,

335
00:23:43,820 --> 00:23:44,840
of,
um,

336
00:23:45,050 --> 00:23:46,280
being surprised

337
00:23:46,340 --> 00:23:51,340
by things that are about you,
kind of in our poetic notion of reality.

338
00:23:53,480 --> 00:23:58,480
We think as humans require randomness.
So you don't find randomness beautiful.

339
00:23:58,930 --> 00:24:03,260
You,
you s you find simple determinism.

340
00:24:03,530 --> 00:24:05,360
Beautiful.
Yeah.

341
00:24:06,890 --> 00:24:08,240
Okay.
So why,

342
00:24:08,630 --> 00:24:09,980
why?
Because see,

343
00:24:09,981 --> 00:24:10,814
explanation becomes shorter.
A universe that is compressible to a 

344
00:24:17,761 --> 00:24:18,594
short program as much more elegant and 
much more beautiful than another one 

345
00:24:24,080 --> 00:24:29,080
which needs an almost infinite number of
bits to be described as far as we know.

346
00:24:31,100 --> 00:24:31,730
Okay.

347
00:24:31,730 --> 00:24:32,563
Many things that are happening in this 
universe are really simple in terms of 

348
00:24:35,600 --> 00:24:38,930
um,
short programs does that compute gravity

349
00:24:39,110 --> 00:24:39,943
and uh,
see interaction between elementary 

350
00:24:42,201 --> 00:24:45,050
particles and so on.
So all of that seems to be very,

351
00:24:45,051 --> 00:24:45,884
very simple.
Every electron seems to reuse the same 

352
00:24:49,370 --> 00:24:50,203
sub program all the time as it is 
interacting with other elementary 

353
00:24:53,811 --> 00:24:58,811
particles.
If b now

354
00:25:01,150 --> 00:25:01,650
okay,

355
00:25:01,650 --> 00:25:02,483
required an extra article,
injecting new bits of information all 

356
00:25:07,151 --> 00:25:11,620
the time for these extra things which 
are commonly not understood,

357
00:25:11,920 --> 00:25:16,920
such as better tk,
then,

358
00:25:19,120 --> 00:25:20,470
um,
the whole

359
00:25:22,120 --> 00:25:22,953
description length,
awesome data that we can observe the 

360
00:25:26,170 --> 00:25:27,003
history,
I'll say you'll never else would become 

361
00:25:29,790 --> 00:25:34,360
a much longer and therefore uglier and 
uglier.

362
00:25:34,920 --> 00:25:37,830
Again,
simplicities elegant and beautiful.

363
00:25:38,580 --> 00:25:42,270
All the history of science has a history
of compression progress.

364
00:25:42,690 --> 00:25:43,640
Yeah.
So you,

365
00:25:43,790 --> 00:25:44,623
you've described sort of as we build up 
of distractions and you've talked about 

366
00:25:49,951 --> 00:25:54,570
the idea of compression.
How do you see this,

367
00:25:54,571 --> 00:25:57,630
the history of science,
the history of humanity,

368
00:25:57,631 --> 00:25:58,464
our civilization and life on earth as 
some kind of a path towards greater and 

369
00:26:02,851 --> 00:26:05,010
greater compression?
What do you mean by that?

370
00:26:05,011 --> 00:26:07,460
How do you think about that?
And deeds?

371
00:26:07,461 --> 00:26:08,294
He,
history of science is a history of 

372
00:26:10,261 --> 00:26:13,860
compression progress.
What does that mean?

373
00:26:14,580 --> 00:26:15,413
Hundreds of years ago there was an 
astronomer who his name was Kepler and 

374
00:26:20,341 --> 00:26:25,341
he looked at the data points that he got
by watching planets move.

375
00:26:25,740 --> 00:26:30,330
And then you had all these data points 
and suddenly you out that he can greatly

376
00:26:30,331 --> 00:26:35,331
compress the data by predicting it 
through an ellipse lawn.

377
00:26:38,050 --> 00:26:38,883
So it turns out that all these data 
points are more or less on ellipsis 

378
00:26:42,640 --> 00:26:43,473
around sun.
And another guy came along whose name 

379
00:26:48,241 --> 00:26:49,074
was Newton and before him hook and they 
set the same thing that is making these 

380
00:26:55,831 --> 00:26:56,664
planets move.
Like that is what makes the apples fall 

381
00:27:00,511 --> 00:27:02,460
down.
And uh,

382
00:27:03,210 --> 00:27:08,210
also holds form stones and form all 
kinds of other objects.

383
00:27:11,040 --> 00:27:11,873
And some of the many,
many of these compressions off these 

384
00:27:14,401 --> 00:27:15,234
observations became much more 
compressible because as long as you can 

385
00:27:18,181 --> 00:27:21,420
predict the next thing,
given what you have seen so far,

386
00:27:21,720 --> 00:27:24,990
you can compress it.
You don't have to store that data extra.

387
00:27:25,370 --> 00:27:30,370
This is called predictive coding.
And then there was still something wrong

388
00:27:31,411 --> 00:27:32,244
with that theory of the universe.
And you had deviations from these 

389
00:27:35,821 --> 00:27:39,090
predictions of the theory.
And 300 years later,

390
00:27:39,091 --> 00:27:42,510
another guy came along whose name was 
Einstein and he,

391
00:27:42,560 --> 00:27:45,270
um,
he was able to explain a way,

392
00:27:45,271 --> 00:27:50,271
all these deviations from the 
predictions are the old theory through a

393
00:27:51,541 --> 00:27:52,374
new theory,
which was called the general theory of 

394
00:27:55,531 --> 00:27:56,364
relativity,
which at first glance it looks a little 

395
00:27:59,581 --> 00:28:02,640
bit more complicated and you have to 
warp space and time.

396
00:28:02,760 --> 00:28:05,370
But you can phrase it within one single 
sentence,

397
00:28:05,640 --> 00:28:10,640
which is no matter how fast you 
accelerate and how fast are hard,

398
00:28:10,890 --> 00:28:13,380
you decelerate.
And,

399
00:28:13,430 --> 00:28:14,263
um,
no matter what is the gravity in your 

400
00:28:17,431 --> 00:28:21,180
local framework,
lightspeed always looks the same.

401
00:28:21,420 --> 00:28:22,260
And from,
from that,

402
00:28:22,261 --> 00:28:23,094
you can calculate all the consequences.
So it's a very simple thing and that 

403
00:28:26,131 --> 00:28:26,964
allows you to further compress all the 
observations because suddenly there are 

404
00:28:33,390 --> 00:28:34,223
hardly any deviations any longer that 
you can measure from the predictions of 

405
00:28:38,011 --> 00:28:38,844
this new theory.
And so on of signs is a history of 

406
00:28:43,140 --> 00:28:43,973
compression progress.
You would never arrive immediately at 

407
00:28:47,251 --> 00:28:52,251
the shortest explanation of the data,
but you're making progress.

408
00:28:52,590 --> 00:28:55,700
Whenever you are making progress,
you have an insight,

409
00:28:56,150 --> 00:28:57,300
you will see,
oh,

410
00:28:57,360 --> 00:29:00,870
first I needed so many bits of 
information to describe the data,

411
00:29:01,170 --> 00:29:03,360
to describe my falling apples,
my video,

412
00:29:03,361 --> 00:29:05,850
or falling apples.
I need so many data now,

413
00:29:05,880 --> 00:29:08,130
so many pixels we'll have to be stored.

414
00:29:08,160 --> 00:29:09,870
But then suddenly I realize,
no,

415
00:29:10,080 --> 00:29:10,913
that is a very simple way of predicting 
the third frame in the video from the 

416
00:29:14,911 --> 00:29:16,960
first tool.
And um,

417
00:29:17,550 --> 00:29:20,100
and maybe not every little detail can be
predicted,

418
00:29:20,101 --> 00:29:22,050
but more or less,
most of these orange blogs,

419
00:29:22,230 --> 00:29:25,620
blobs that are coming down,
they accelerate in the same way,

420
00:29:25,830 --> 00:29:26,663
which means that I can greatly compress 
the video and the amount of compression 

421
00:29:32,100 --> 00:29:32,933
progress.
That is the depths of the insight that 

422
00:29:35,791 --> 00:29:38,880
you have at that moment.
That's the fun that you have,

423
00:29:38,881 --> 00:29:39,714
the scientific fun,
that fun and that discovery and we can 

424
00:29:43,541 --> 00:29:45,850
build artificial systems that do the 
same thing.

425
00:29:46,060 --> 00:29:51,060
So measure as a depth off their insights
as they are looking at the data which is

426
00:29:51,071 --> 00:29:55,570
coming in through their own experiments 
and we give them a reward.

427
00:29:55,720 --> 00:29:56,553
And in terms of getting ward and 
proportion to this depth of insight w 

428
00:30:01,180 --> 00:30:06,180
and since they are trying to maximize 
the rewards they get,

429
00:30:08,140 --> 00:30:08,973
they are suddenly motivated to come up 
with new actions sequences with new 

430
00:30:13,301 --> 00:30:14,134
experiments that half the property that 
the data that is coming in as a 

431
00:30:18,101 --> 00:30:18,934
consequence is experiments has the 
property that they can learn something 

432
00:30:23,711 --> 00:30:27,070
about,
see a pattern in there which they hadn't

433
00:30:27,071 --> 00:30:28,120
seen yet before.

434
00:30:28,730 --> 00:30:29,563
So there's an idea of power play.
You described a training in general 

435
00:30:33,600 --> 00:30:37,280
problem solver in this kind of way of 
looking for the unsolved problems.

436
00:30:38,090 --> 00:30:40,310
Can you describe that idea a little 
further?

437
00:30:40,430 --> 00:30:41,263
It's another very simple idea.
So normally what you do and computer 

438
00:30:44,331 --> 00:30:45,164
science you have,
you have some guy who gives you a 

439
00:30:49,341 --> 00:30:50,174
problem and then there is a,
a huge search space of potential 

440
00:30:55,491 --> 00:30:59,480
solution candidates and you somehow try 
them out.

441
00:30:59,510 --> 00:31:02,690
And um,
you have more or less sophisticated ways

442
00:31:02,691 --> 00:31:07,691
of moving around in that search space 
until you finally found a solution which

443
00:31:10,341 --> 00:31:11,174
you consider a satisfactory.
That's what most of computer science is 

444
00:31:14,721 --> 00:31:15,554
about.
Power play just goes one little step 

445
00:31:18,291 --> 00:31:23,291
further and says,
let's not only search for solutions to a

446
00:31:23,631 --> 00:31:24,464
given problem,
but let such to pass of problems and 

447
00:31:29,241 --> 00:31:30,074
their solutions where the system itself 
has the opportunity to phrase its own 

448
00:31:35,451 --> 00:31:36,284
problem.
So we are looking suddenly at pairs of 

449
00:31:40,551 --> 00:31:41,384
problems and their solutions or 
modifications off the problem solver 

450
00:31:46,250 --> 00:31:49,790
that is opposed to generate a solution 
to that new problem.

451
00:31:51,080 --> 00:31:51,913
And,
and this additional degree of freedom 

452
00:31:56,930 --> 00:31:57,763
allows us to build who you are.
Systems that are like scientists in the 

453
00:32:01,791 --> 00:32:02,624
sense that they not only trying to solve
and try to find answers to existing 

454
00:32:07,611 --> 00:32:08,510
questions.
No,

455
00:32:08,511 --> 00:32:13,040
there are also free to pose their own 
questions.

456
00:32:13,250 --> 00:32:15,260
So if you want to build an artificial 
scientists,

457
00:32:15,410 --> 00:32:18,860
we have to give it that freedom and 
power play is exactly doing that.

458
00:32:19,580 --> 00:32:21,040
So that's,
that's a dimension of

459
00:32:21,090 --> 00:32:23,570
freedom that's important to have.
But how do you,

460
00:32:23,930 --> 00:32:24,763
how hard do you think that,
how multidimensional and difficult the 

461
00:32:31,081 --> 00:32:35,220
space of then coming up with your own 
questions is as,

462
00:32:35,221 --> 00:32:38,400
it's one of the things that as human 
beings we are considered to be,

463
00:32:38,590 --> 00:32:39,423
the thing makes us special.
The intelligence that makes us special 

464
00:32:42,200 --> 00:32:44,940
is that brilliant insight.
Yeah.

465
00:32:45,050 --> 00:32:47,300
That can create something totally new.

466
00:32:47,600 --> 00:32:51,320
Yes.
So now let's look at the extreme case.

467
00:32:51,350 --> 00:32:52,183
Let's look at the set of all possible 
problems that you can formally this 

468
00:32:56,841 --> 00:32:59,240
crime,
which is infinite,

469
00:33:00,170 --> 00:33:01,003
which should be the next problem that a 
scientist or a power play is going to 

470
00:33:07,041 --> 00:33:08,540
solve.
Well,

471
00:33:09,860 --> 00:33:10,693
it should be

472
00:33:12,460 --> 00:33:15,940
the easiest problem that goes beyond 
what you already know.

473
00:33:17,570 --> 00:33:22,230
So it should be the simplest problem 
that the current problems,

474
00:33:22,231 --> 00:33:23,064
all of that you have,
which can already solve 100 problems 

475
00:33:26,610 --> 00:33:30,060
that he cannot sold yet by just 
generalizing.

476
00:33:31,080 --> 00:33:34,540
So it has to be new.
So it has to require a modification.

477
00:33:34,541 --> 00:33:38,610
On the problem solver such that the new 
problem solver canceled this new thing,

478
00:33:38,880 --> 00:33:41,100
but the only problem solve,
I cannot do it.

479
00:33:41,550 --> 00:33:42,383
And in addition to that,
we have to make sure that the problem 

480
00:33:46,681 --> 00:33:47,514
solver,
it doesn't forget any of the previous 

481
00:33:49,501 --> 00:33:50,640
solutions.
Right.

482
00:33:51,240 --> 00:33:52,073
And so by definition,
power play is now trying to search and 

483
00:33:55,681 --> 00:33:58,070
this pair and,
and,

484
00:33:58,160 --> 00:34:01,440
and the set of pairs of problems and 
problem solve,

485
00:34:01,441 --> 00:34:04,440
um,
modifications for our combination that,

486
00:34:04,760 --> 00:34:05,593
uh,
minimize the time to achieve these 

487
00:34:07,291 --> 00:34:08,820
criteria.
So as honest,

488
00:34:08,840 --> 00:34:09,673
trying to find the problem,
which is easiest to add to the 

489
00:34:13,171 --> 00:34:15,650
repertoire.
So just like grads

490
00:34:15,710 --> 00:34:16,543
Zudans and academics and researchers can
spend their whole career in a local 

491
00:34:20,571 --> 00:34:23,950
minima hmm.
Stuck trying to,

492
00:34:23,951 --> 00:34:25,890
uh,
come up with interesting questions,

493
00:34:25,891 --> 00:34:27,530
but ultimately doing very little.
Yeah.

494
00:34:27,620 --> 00:34:28,940
Do you think it's easy?

495
00:34:29,760 --> 00:34:30,593
Well,
in this approach of looking for the 

496
00:34:32,111 --> 00:34:32,944
simplest,
unsolvable problem to get stuck in a 

497
00:34:34,541 --> 00:34:37,390
local minima is not never really 
discovering

498
00:34:38,820 --> 00:34:39,653
mew,

499
00:34:39,940 --> 00:34:40,541
uh,
you know,

500
00:34:40,541 --> 00:34:44,620
really jumping outside of the hunter 
problems that you've already solved in a

501
00:34:44,621 --> 00:34:46,150
genuine creative way.

502
00:34:47,060 --> 00:34:47,630
Okay.

503
00:34:47,630 --> 00:34:48,463
No,
because that's the nature of power play 

504
00:34:49,960 --> 00:34:50,793
that it's always trying to break.
It's crown generalization abilities by 

505
00:34:56,181 --> 00:35:00,350
coming up with a new problem,
which is beyond the current horizon.

506
00:35:00,920 --> 00:35:05,000
Just shifting the horizon,
I'm knowledge a little bit out there.

507
00:35:05,750 --> 00:35:10,750
Breaking the existing rules such as a 
new thing becomes solvable,

508
00:35:10,970 --> 00:35:15,320
but it wasn't solvable by the old thing.
So like adding a new axiom,

509
00:35:15,610 --> 00:35:16,443
um,
like what Google did when he came up 

510
00:35:18,981 --> 00:35:19,814
with these new sentences,
new he runs that didn't have a proof in 

511
00:35:23,081 --> 00:35:23,914
the former system,
which means you can add them to the 

512
00:35:25,940 --> 00:35:27,060
repertoire.
Yeah.

513
00:35:27,670 --> 00:35:30,340
Hoping that,
that they,

514
00:35:30,341 --> 00:35:31,174
um,
are not going to damage the consistency 

515
00:35:33,480 --> 00:35:34,313
also whole thing.

516
00:35:35,850 --> 00:35:37,140
So in the,
uh,

517
00:35:37,200 --> 00:35:41,280
paper with the amazing title,
formal theory of creativity,

518
00:35:41,700 --> 00:35:42,533
fun and intrinsic motivation,
you talk about discovery as intrinsic 

519
00:35:46,411 --> 00:35:47,244
reward.
So if you view human as intelligent 

520
00:35:50,611 --> 00:35:51,444
agents,
what do you think is the purpose and 

521
00:35:53,521 --> 00:35:58,521
meaning of life for us humans is,
you've talked about this discovery.

522
00:35:58,641 --> 00:36:02,580
I do you see humans as an instance of 
power play agents?

523
00:36:04,220 --> 00:36:08,760
Yeah.
So humans are curious and um,

524
00:36:09,030 --> 00:36:13,190
that means they behave like scientists.
Not only the official scientists,

525
00:36:13,191 --> 00:36:15,860
but even the baby is behave like 
scientists.

526
00:36:15,861 --> 00:36:16,694
And they play around with that toy is to
figure out how the world works and how 

527
00:36:20,601 --> 00:36:21,434
it is responding to that actions.
And that's how they learned about 

528
00:36:25,131 --> 00:36:28,190
gravity and everything.
And Yeah,

529
00:36:28,250 --> 00:36:30,980
in 1995 we had the first systems like 
that.

530
00:36:31,010 --> 00:36:31,843
We just try to,
to play around with the environment and 

531
00:36:34,970 --> 00:36:39,970
come up with situations that go beyond 
what they knew at that time and then get

532
00:36:40,761 --> 00:36:43,670
a reward for creating these situations 
and then becoming,

533
00:36:44,000 --> 00:36:45,440
um,
more general problems,

534
00:36:45,441 --> 00:36:48,320
all of us and being able to understand 
more of the wild.

535
00:36:48,920 --> 00:36:51,440
So yeah,
I think in principle that,

536
00:36:51,500 --> 00:36:54,620
um,
that,

537
00:36:54,621 --> 00:36:56,810
that curiosity,
um,

538
00:36:57,830 --> 00:36:58,663
strategy

539
00:36:59,920 --> 00:37:03,980
or sophist more sophisticated versions 
of what a justice crime they are,

540
00:37:04,660 --> 00:37:05,493
what we have built in as well because 
evolution discovered that that's a good 

541
00:37:08,621 --> 00:37:09,454
way of exploring the unknown wild.
And the guy who explores the unknown 

542
00:37:13,151 --> 00:37:13,984
wild has a higher chance of solving 
problems that he needs to survive in 

543
00:37:18,431 --> 00:37:20,230
this world.
On the other hand,

544
00:37:21,080 --> 00:37:21,460
yeah,

545
00:37:21,460 --> 00:37:25,240
those guys who were too curious,
they were weeded out as well.

546
00:37:25,290 --> 00:37:28,870
So you have to find this tradeoff 
evolution and found a certain trade off.

547
00:37:29,260 --> 00:37:30,093
Apparently in our society there is a 
certain percentage of extremely 

548
00:37:34,211 --> 00:37:35,044
expansive guys and it doesn't matter if 
they die because many of the others are 

549
00:37:40,120 --> 00:37:43,390
more conservative.
And um,

550
00:37:43,990 --> 00:37:47,290
and so yeah,
it would be surprising to me if I'm

551
00:37:49,120 --> 00:37:49,390
okay

552
00:37:49,390 --> 00:37:50,223
if that principal of artificial 
curiosity wouldn't be present in almost 

553
00:37:56,091 --> 00:37:57,980
exactly the same form here

554
00:37:58,390 --> 00:37:59,223
in our brains.
So you're a bit of a musician and an 

555
00:38:01,631 --> 00:38:02,464
artist.
So continuing on this topic of 

556
00:38:05,681 --> 00:38:06,514
creativity,
what do you think is the role of 

557
00:38:08,531 --> 00:38:09,364
creativity and intelligence?
So you've kind of implied that it's 

558
00:38:12,401 --> 00:38:13,234
essential for intelligence if you think 
of intelligence as a problem solving 

559
00:38:20,230 --> 00:38:24,550
system is ability to solve problems.
But do you think it's essential,

560
00:38:25,300 --> 00:38:26,740
this idea of creativity?

561
00:38:28,990 --> 00:38:29,823
We never have a program,
a sub program that is called creativity 

562
00:38:33,551 --> 00:38:34,384
or something.
It's just a effect of what our problems 

563
00:38:37,001 --> 00:38:40,240
all of us do.
They are searching a space of problems.

564
00:38:40,270 --> 00:38:41,103
Oh,
I space off a cannon dates off solution 

565
00:38:43,391 --> 00:38:47,620
candidates until they hopefully find a 
solution to a given problem.

566
00:38:48,190 --> 00:38:51,110
But then there are these two types of 
creativity and uh,

567
00:38:51,190 --> 00:38:53,590
both of them are now present in our 
machines.

568
00:38:53,920 --> 00:38:56,050
Um,
the first one has been around for a long

569
00:38:56,051 --> 00:38:59,620
time,
which is human gives problem to machine,

570
00:38:59,650 --> 00:39:03,100
machine,
tries to find a solution to that.

571
00:39:03,460 --> 00:39:04,293
And this has been happening for many 
decades and for many decades machines 

572
00:39:07,211 --> 00:39:08,044
have found creative solutions to 
interesting problems where humans were 

573
00:39:12,940 --> 00:39:14,710
not aware of these,
um,

574
00:39:15,520 --> 00:39:16,353
particularly in creative solutions but 
then appreciate it that the machine 

575
00:39:19,691 --> 00:39:24,160
found that.
The second is the pure creativity that I

576
00:39:24,161 --> 00:39:25,600
would call it what I just mentioned.

577
00:39:25,600 --> 00:39:30,600
I would call the applied creativity like
applied art where somebody tells you now

578
00:39:31,880 --> 00:39:36,490
make a nice picture of off this pope and
you will get money for that.

579
00:39:36,760 --> 00:39:37,593
Okay,
so here's the artist and he makes a 

580
00:39:40,151 --> 00:39:43,900
convincing picture as the pope and the 
pope lakes it and gives them the money.

581
00:39:45,880 --> 00:39:48,760
And then there is the pure creative 
creativity,

582
00:39:48,761 --> 00:39:52,960
which is more like the power play and 
the artificial curiosity thing where you

583
00:39:52,961 --> 00:39:56,440
have the freedom to select your own 
problem.

584
00:39:57,070 --> 00:40:02,070
Like a scientist who defines his on 
question two study.

585
00:40:03,400 --> 00:40:06,790
And so that is the pure creativity,
if you will,

586
00:40:07,840 --> 00:40:12,840
as opposed to the applied creativity,
which serves another

587
00:40:14,380 --> 00:40:15,213
in that distinction,
there's almost echoes of narrow ai 

588
00:40:17,651 --> 00:40:18,484
versus generally I say this kind of 
constrained painting or pope seems like 

589
00:40:25,580 --> 00:40:26,413
the,
the approaches of what people are 

590
00:40:28,101 --> 00:40:32,590
calling narrow ai and pure creativity 
seems to be okay.

591
00:40:33,360 --> 00:40:34,193
Maybe I'm just biased as a human,
but it seems to be an essential element 

592
00:40:38,400 --> 00:40:42,150
of human level intelligence.
Is that what you're implying?

593
00:40:44,330 --> 00:40:48,320
To a degree,
if you zoom back a little bit and you're

594
00:40:48,321 --> 00:40:51,230
just look at 'em gentlemen problem 
solving machine,

595
00:40:51,260 --> 00:40:53,420
which is trying to solve arbitrary 
problems,

596
00:40:53,600 --> 00:40:54,433
then this machine will figure out in the
course are solving problems that it's 

597
00:40:58,641 --> 00:40:59,474
good to be curious.
So all of what I said just now about 

598
00:41:02,991 --> 00:41:03,824
this pre one curiosity and there's 
Seville to invent new problems that the 

599
00:41:08,721 --> 00:41:09,554
system doesn't know how to solve yet 
should be just a byproduct of the 

600
00:41:13,651 --> 00:41:15,690
general signage.
However,

601
00:41:17,590 --> 00:41:21,040
apparently evolution has built it into 
us.

602
00:41:21,860 --> 00:41:24,020
The cards are turned out to be so 
successful.

603
00:41:24,050 --> 00:41:25,080
Uh,
uh,

604
00:41:25,130 --> 00:41:26,290
pre wiring,
uh,

605
00:41:26,291 --> 00:41:30,830
buyers are very successful exploratory 
buyers that,

606
00:41:30,831 --> 00:41:32,750
um,
that'd be a born with.

607
00:41:33,890 --> 00:41:34,723
And you've also said that consciousness 
in the same kind of way may be a 

608
00:41:37,461 --> 00:41:40,700
byproduct of,
of problem solving.

609
00:41:41,210 --> 00:41:42,043
Do you think,
do you find this an interesting 

610
00:41:44,061 --> 00:41:46,420
byproduct?
Do you think is a useful byproduct?

611
00:41:47,320 --> 00:41:49,340
What are your thoughts on consciousness 
in general?

612
00:41:49,341 --> 00:41:50,174
Or is it simply a byproduct of greater 
and greater capabilities of problem 

613
00:41:54,171 --> 00:41:57,110
solving that's,
uh,

614
00:41:57,560 --> 00:41:59,870
that's similar to creativity in that 
sense?

615
00:42:00,940 --> 00:42:01,773
Yeah.
We never have a procedure called 

616
00:42:03,521 --> 00:42:04,354
consciousness and now let machines,
however we get as side effects of what 

617
00:42:08,740 --> 00:42:09,573
these machines are doing,
things that seem to be closely related 

618
00:42:13,521 --> 00:42:17,780
to what people can and consciousness.
So for example,

619
00:42:18,890 --> 00:42:19,723
in 1990,
we had simple systems which were 

620
00:42:21,860 --> 00:42:23,100
basically,
um,

621
00:42:23,150 --> 00:42:23,983
recurrent networks and therefore 
university of computers trying to map 

622
00:42:28,040 --> 00:42:32,720
incoming data into actions that lead to 
success.

623
00:42:33,210 --> 00:42:34,043
Uh,
maximizing reward and a given 

624
00:42:35,691 --> 00:42:36,524
environment always findings a charging 
station in time whenever the battery is 

625
00:42:41,331 --> 00:42:43,850
low and negative signals are coming from
the battery.

626
00:42:44,190 --> 00:42:45,023
Always find the charging station in time
without bumping against painful 

627
00:42:49,281 --> 00:42:50,114
obstacles on the way.
So complicated things but very easily 

628
00:42:53,091 --> 00:42:55,310
motivated.
And then,

629
00:42:55,330 --> 00:43:00,050
uh,
we give these little guys a separate,

630
00:43:00,760 --> 00:43:01,593
we can't run a truck,
which is just predicting what's 

631
00:43:03,081 --> 00:43:04,200
happening.
If I do,

632
00:43:04,201 --> 00:43:05,034
that happened that what will happen as a
consequence are these actions that I'm 

633
00:43:08,631 --> 00:43:09,464
executing and it's just trained on the 
long and long history of interactions 

634
00:43:12,771 --> 00:43:13,760
with the wild.

635
00:43:14,030 --> 00:43:17,540
So it becomes a predictive model.
Lots of wild basically.

636
00:43:18,110 --> 00:43:18,943
And therefore also a compressor arsene 
observations after awhile because 

637
00:43:23,451 --> 00:43:26,550
whatever you can predict you don't have 
to store extra.

638
00:43:26,551 --> 00:43:29,840
And so compression as a side effect our 
prediction.

639
00:43:30,560 --> 00:43:32,990
And how does this require network 
compress?

640
00:43:33,230 --> 00:43:34,063
Well it's inventing little subprograms 
little sub natural networks that stand 

641
00:43:38,271 --> 00:43:39,104
for everything that frequently appears 
and the environment like bottles and 

642
00:43:43,191 --> 00:43:47,040
microphones and faces maybe lots of 
faces in my um,

643
00:43:47,410 --> 00:43:48,243
uh,
environments or I'm learning to create 

644
00:43:50,031 --> 00:43:50,864
something like a prototype phase and the
new phase comes along and all I have to 

645
00:43:53,660 --> 00:43:55,520
encode the deviations from the 
prototype.

646
00:43:56,390 --> 00:43:59,960
So it's compressing all the time.
The stuff that frequently appears.

647
00:44:00,920 --> 00:44:01,753
There's one thing that appears all the 
time that is present all the time when 

648
00:44:06,621 --> 00:44:09,410
the agent is interacting with its 
environment,

649
00:44:09,530 --> 00:44:14,240
which is the agent itself.
So just for data compression reasons,

650
00:44:14,510 --> 00:44:19,510
it is extremely natural for this 
recurrent network to come up with little

651
00:44:19,731 --> 00:44:23,600
sub networks that stand for the 
properties off the agents,

652
00:44:23,601 --> 00:44:25,790
the Hams,
you know the,

653
00:44:25,791 --> 00:44:26,624
the,
the other actuators and all the stuff 

654
00:44:29,090 --> 00:44:29,923
said you need to better in code the data
which is influenced by the actions of 

655
00:44:33,391 --> 00:44:34,224
the agent.

656
00:44:34,350 --> 00:44:39,350
So they're just as a side effect of they
are compression during problem solving,

657
00:44:40,420 --> 00:44:41,253
you have internal self models now you 
can use this model of the wand to plan 

658
00:44:50,170 --> 00:44:51,003
your future and that's what you also 
have done since 1990 so the recurrent 

659
00:44:55,121 --> 00:44:57,400
network,
which is c controller,

660
00:44:57,670 --> 00:44:58,503
which is trying to maximize reward can 
use this model as a network of the 

661
00:45:02,141 --> 00:45:02,974
wilds.
It says model network Arthur wild 

662
00:45:04,390 --> 00:45:05,223
predictive model after wilde to plan 
ahead and say let's not do this action 

663
00:45:08,501 --> 00:45:09,334
sequence.
Let's do this action sequence instead 

664
00:45:11,411 --> 00:45:13,780
because it leads to more predicted three
rewards.

665
00:45:14,590 --> 00:45:15,423
And whenever it's waking up,
these learners up networks that stand 

666
00:45:19,191 --> 00:45:22,420
for itself,
it's like it's thinking about itself and

667
00:45:22,421 --> 00:45:23,254
it's thinking about itself and it's okay
exploring mentally the consequences of 

668
00:45:29,651 --> 00:45:30,484
its own actions.
And now you tell me what is still 

669
00:45:35,061 --> 00:45:35,894
missing

670
00:45:37,190 --> 00:45:39,680
missing the next,
the the gap to consciousness.

671
00:45:39,730 --> 00:45:40,371
Yeah,
I,

672
00:45:40,371 --> 00:45:41,061
there,
there isn't,

673
00:45:41,061 --> 00:45:44,150
that's a really beautiful idea that um,
you know,

674
00:45:44,390 --> 00:45:45,223
if life is a collection of data and in 
life is a process of compressing that 

675
00:45:49,431 --> 00:45:54,431
data to act efficiently in that data,
you yourself appear very often.

676
00:45:57,590 --> 00:45:58,423
So it's useful to a form compressions of
yourself and it's a really beautiful 

677
00:46:02,091 --> 00:46:05,330
formulation or cautiousness is as 
unnecessary side effect.

678
00:46:05,690 --> 00:46:10,690
It's actually quite compelling to me.
You've described Rnn ins and developed a

679
00:46:15,741 --> 00:46:16,574
Lstm as long short term memory networks 
that are there a type of recurrent 

680
00:46:20,960 --> 00:46:23,960
neural networks and they have gotten a 
lot of success recently.

681
00:46:23,961 --> 00:46:28,961
So these are networks that model the 
temporal aspects in the data to temporal

682
00:46:29,871 --> 00:46:30,704
patterns in the data.
And you've called them the deepest of 

683
00:46:34,101 --> 00:46:35,450
the new on that works.
Right.

684
00:46:36,290 --> 00:46:37,123
So what do you think is the value of 
depth in the models that we use to 

685
00:46:40,911 --> 00:46:41,744
learn?

686
00:46:43,940 --> 00:46:44,773
Yeah,
since you mentioned the long short term 

687
00:46:46,131 --> 00:46:48,240
memory and the Lstm,
um,

688
00:46:48,350 --> 00:46:52,190
I have to mention the names off the 
brilliant students who has of course,

689
00:46:52,191 --> 00:46:53,350
of course.
Um,

690
00:46:53,420 --> 00:46:54,253
first of all,
and my first student ever set for Haida 

691
00:46:56,720 --> 00:46:57,553
who had fundamental insights already and
this diploma thesis then Felix 

692
00:47:01,080 --> 00:47:04,430
[inaudible] had additional important 
contributions.

693
00:47:04,610 --> 00:47:07,520
Alex Grey's a guy from Scotland who,
um,

694
00:47:07,860 --> 00:47:11,300
is mostly responsible for this CTC 
algorithm,

695
00:47:11,301 --> 00:47:12,134
which is now how often use to,
to train the Lstm to do the speech 

696
00:47:15,681 --> 00:47:16,514
recognition on all the Google,
android phones and whatever and CV and 

697
00:47:20,571 --> 00:47:21,860
so on.
So,

698
00:47:22,040 --> 00:47:23,170
um,
uh,

699
00:47:23,210 --> 00:47:24,800
these guys,
without these guys,

700
00:47:24,801 --> 00:47:26,780
um,
I would be nothing.

701
00:47:26,810 --> 00:47:29,580
It's a lot of incredible work.
What does,

702
00:47:29,581 --> 00:47:30,414
now the depth,

703
00:47:30,570 --> 00:47:32,820
what is the importance of depth?
Well,

704
00:47:32,850 --> 00:47:36,180
um,
most problems in the real world,

705
00:47:36,210 --> 00:47:37,710
uh,
deep in the sense that,

706
00:47:37,711 --> 00:47:38,544
um,
the kind input doesn't tell you all you 

707
00:47:41,371 --> 00:47:46,140
need to know about the environment.
So instead,

708
00:47:46,430 --> 00:47:47,263
um,
you have to have a memory of what 

709
00:47:48,871 --> 00:47:53,871
happened in the past and often important
pads of that memory I dated.

710
00:47:54,780 --> 00:47:56,970
They are pretty old.
And so,

711
00:47:56,990 --> 00:47:59,340
um,
when you're doing speech recognition for

712
00:47:59,341 --> 00:48:02,520
example,
and somebody says 11,

713
00:48:04,260 --> 00:48:08,700
then that's about half a second or 
something like that,

714
00:48:09,090 --> 00:48:11,250
which means it's already a 50 time 
steps.

715
00:48:11,970 --> 00:48:15,120
And another guy or the same guys that 
says seven.

716
00:48:16,200 --> 00:48:17,033
So the ending is the same Evan,
but now the system has to see the 

717
00:48:20,131 --> 00:48:20,964
distinction between seven and 11.
And the only way I can see the 

718
00:48:24,571 --> 00:48:28,290
differences it has to store that,
uh,

719
00:48:28,410 --> 00:48:33,410
50 steps ago there was an s or an uber 
11 or seven.

720
00:48:34,920 --> 00:48:37,560
So there you have already a problem of 
depth 50,

721
00:48:38,070 --> 00:48:42,210
because for each time step you have 
something like a virtual,

722
00:48:42,211 --> 00:48:45,900
a layer in the expanded and evolved 
version of [inaudible] network,

723
00:48:45,901 --> 00:48:50,160
which is doing the speech recognition.
So these long time lax,

724
00:48:50,161 --> 00:48:55,161
they translate into problem depth and 
most rotten ones.

725
00:48:56,250 --> 00:48:57,083
And this wild I that you're rarely have 
to look far back in time to understand 

726
00:49:03,360 --> 00:49:07,610
what is the problem and to solve it.
But just like with Lstm,

727
00:49:07,910 --> 00:49:11,090
you don't necessarily need to,
when you look back in time,

728
00:49:11,091 --> 00:49:11,924
remember every aspect,
you just need to remember the important 

729
00:49:13,881 --> 00:49:14,714
aspects.

730
00:49:14,840 --> 00:49:15,673
That's fine.
It's a network has to learn to put the 

731
00:49:17,841 --> 00:49:22,841
important stuff and into memory and to 
ignore the unimportant noise.

732
00:49:23,840 --> 00:49:25,580
So,
but in that sense,

733
00:49:26,420 --> 00:49:29,810
deeper and deeper is better.
Or is there a limitation?

734
00:49:29,811 --> 00:49:34,340
Is there,
I mean Lstm is one of the great examples

735
00:49:34,670 --> 00:49:39,230
of architectures that,
uh,

736
00:49:39,260 --> 00:49:41,810
do something beyond just deeper and 
deeper networks,

737
00:49:42,370 --> 00:49:47,120
this clever mechanisms for filtering 
data for remembering and forgetting.

738
00:49:47,780 --> 00:49:51,110
So do you think that kind of thinking is
necessary?

739
00:49:51,290 --> 00:49:53,360
If you think about it,
Lstm is a leap,

740
00:49:53,390 --> 00:49:56,630
a big leap forward over traditional 
Vanilla rnn.

741
00:49:57,650 --> 00:50:02,650
What do you think is the next leap it 
within this context?

742
00:50:02,841 --> 00:50:03,674
So as Sam was a very clever improvement,
but lcm still don't have the same kind 

743
00:50:09,951 --> 00:50:12,270
of ability to see far back in the future
in the,

744
00:50:12,340 --> 00:50:17,340
in the past as us humans do the credit 
assignment problem across way back,

745
00:50:18,830 --> 00:50:21,920
not just 50 times stuff.
So a hundred or a thousand,

746
00:50:21,921 --> 00:50:23,990
but millions and billions,

747
00:50:24,840 --> 00:50:25,673
not clear.
What are the practical limits as the 

748
00:50:28,611 --> 00:50:33,140
LSTM when it comes to looking back 
already in 2006,

749
00:50:33,141 --> 00:50:33,974
I think we had examples where it not 
only look back tens or thousands of 

750
00:50:37,581 --> 00:50:41,480
steps,
but really millions of steps and um,

751
00:50:41,500 --> 00:50:42,070
who,
um,

752
00:50:42,070 --> 00:50:43,200
Paris,
um,

753
00:50:43,370 --> 00:50:46,730
artists in my lap,
I think once the first author of a paper

754
00:50:47,060 --> 00:50:47,893
where we really was a 2006 or something,
had examples where I'd learn to look 

755
00:50:52,671 --> 00:50:56,940
back for more than 10 million steps.
Right?

756
00:50:57,500 --> 00:51:02,030
So for most problems I've speech 
recognition,

757
00:51:02,060 --> 00:51:04,640
it's not necessary to look that far 
back.

758
00:51:04,670 --> 00:51:09,670
But the examples where does now,
so looking back thing that's rather easy

759
00:51:11,600 --> 00:51:12,433
because that has only one past,
but there are many possible futures and 

760
00:51:17,961 --> 00:51:18,794
so our reinforcement learning system,
which is trying to maximize its future 

761
00:51:22,371 --> 00:51:23,204
expected reward and doesn't know yet 
which of these many possible future 

762
00:51:27,621 --> 00:51:32,621
should I select give him this one single
pass is facing problems that the Lstm by

763
00:51:34,191 --> 00:51:35,150
itself cannot solve.

764
00:51:36,530 --> 00:51:37,363
So the other Sam is good for coming up 
with a compact representation of the 

765
00:51:41,331 --> 00:51:42,410
history.
So far.

766
00:51:42,411 --> 00:51:46,010
I'll say history and observations and 
action so far.

767
00:51:46,310 --> 00:51:51,310
But now how do you plan in an efficient 
and good way among all these,

768
00:51:54,320 --> 00:51:55,153
how do you select one of these many 
possible action sequences that are 

769
00:51:58,431 --> 00:51:59,264
reinforcement learning system has to 
consider to maximize reward in this 

770
00:52:04,340 --> 00:52:06,350
unknown future?
So again,

771
00:52:06,620 --> 00:52:11,150
we have this basic setup where you have 
one that we cannot work,

772
00:52:11,390 --> 00:52:15,740
which gets in the video and the speech 
and whatever,

773
00:52:15,920 --> 00:52:19,490
and it's executing the actions and is 
trying to maximize reward.

774
00:52:19,610 --> 00:52:23,660
So that is no teacher who tells it what 
to do at which point in time.

775
00:52:24,440 --> 00:52:29,440
And then there's the other network,
which is just predicting what's going to

776
00:52:30,621 --> 00:52:32,180
happen if I do that.
And then,

777
00:52:32,960 --> 00:52:33,793
and that could be an lstm network and it
allows us to look back all the way to 

778
00:52:38,721 --> 00:52:40,950
make better predictions off the next 
time step.

779
00:52:41,600 --> 00:52:42,433
So essentially,
although it's predicting only the next 

780
00:52:44,750 --> 00:52:45,583
time step and is motivated to learn to 
put into memory something that happened 

781
00:52:51,051 --> 00:52:53,470
maybe a million steps ago,
because it's important,

782
00:52:53,670 --> 00:52:54,740
uh,
to memorize that.

783
00:52:54,741 --> 00:52:57,620
If you want to predict that at the next 
time step,

784
00:52:57,621 --> 00:52:58,670
the next event,
you know,

785
00:52:59,600 --> 00:53:00,510
now,
um,

786
00:53:01,010 --> 00:53:01,843
how can a model of the one,
I like that a predictive model of the 

787
00:53:04,870 --> 00:53:05,703
wild be used by the first guy,
it's called it the controller and the 

788
00:53:09,771 --> 00:53:11,390
model,
the controller and the model.

789
00:53:11,510 --> 00:53:12,343
How can the model be used by the 
controller to efficiently select a ma 

790
00:53:17,500 --> 00:53:21,560
among these many possible futures?
So now eve way we had,

791
00:53:21,561 --> 00:53:22,394
um,
about 30 years ago was let's just is 

792
00:53:25,500 --> 00:53:28,560
some model last awhile as a stand in,
as a simulation,

793
00:53:28,570 --> 00:53:31,290
as a wild and millisecond by 
millisecond.

794
00:53:31,291 --> 00:53:35,070
We plan the future and that means we 
have to roll it out.

795
00:53:35,070 --> 00:53:38,370
It's really in detail and it will work 
only if the model is really good.

796
00:53:38,610 --> 00:53:39,443
And it will still be inefficient because
we have to look at all these possible 

797
00:53:42,481 --> 00:53:46,560
futures and there are so many awesome.
So instead,

798
00:53:46,680 --> 00:53:47,513
what do we do now since 2015 and 
[inaudible] systems control model 

799
00:53:51,510 --> 00:53:52,343
systems.
If you give as the controller the 

800
00:53:54,181 --> 00:53:58,980
opportunity to learn by itself,
how to use theory,

801
00:53:59,010 --> 00:53:59,843
potentially relevant parts of the m of 
the model and trying to solve new 

802
00:54:04,591 --> 00:54:07,350
problems more quickly and if it wants 
to,

803
00:54:08,010 --> 00:54:08,843
it can learn to ignore the m and 
sometimes there's a good idea to ignore 

804
00:54:11,881 --> 00:54:12,714
the m because it's really bad,
it's a bad predictor and this 

805
00:54:16,171 --> 00:54:17,670
particular,
um,

806
00:54:17,730 --> 00:54:19,180
situation of life,
uh,

807
00:54:19,230 --> 00:54:22,470
where the control is currently trying to
maximize rewind,

808
00:54:23,040 --> 00:54:23,873
however it can also allow them to 
address and exploit some of the sub 

809
00:54:27,721 --> 00:54:32,721
programs that came about in the model 
network through compressing as a data by

810
00:54:35,161 --> 00:54:35,994
predicting it.
So it now has an opportunity to reuse 

811
00:54:40,201 --> 00:54:41,034
that code,
the algorithmic inflammation in the 

812
00:54:43,321 --> 00:54:44,154
modern or trying to reduce its own 
search space search that it can solve a 

813
00:54:49,981 --> 00:54:52,770
new problem more quickly then without 
the model

814
00:54:53,810 --> 00:54:54,643
compression.
So you're ultimately optimistic and 

815
00:54:58,761 --> 00:55:03,761
excited about the power of our,
of reinforcement learning in the context

816
00:55:04,011 --> 00:55:05,960
of real systems.
Absolutely.

817
00:55:05,961 --> 00:55:09,650
Yeah.
So you see rl as a potential,

818
00:55:10,110 --> 00:55:10,943
having a huge impact beyond just sort of
the m part is often develop on 

819
00:55:17,870 --> 00:55:22,870
supervised learning methods.
You see rl as a,

820
00:55:22,941 --> 00:55:26,060
uh,
for problems of self driving cars or any

821
00:55:26,061 --> 00:55:31,040
kind of applied side of robotics.
That's the correct,

822
00:55:31,130 --> 00:55:33,680
interesting direction for research in 
your view.

823
00:55:34,690 --> 00:55:35,523
I do think so.
We have a company called NASCENCE 

824
00:55:37,391 --> 00:55:39,280
nascence which,
um,

825
00:55:39,340 --> 00:55:43,780
has applied during placement learning 
too little audis.

826
00:55:44,090 --> 00:55:44,960
There are these

827
00:55:45,180 --> 00:55:50,180
where's land to park without a teacher.
The same principles where you were stock

828
00:55:50,390 --> 00:55:52,740
cars.
So these little Audi is,

829
00:55:52,741 --> 00:55:54,990
they are small,
maybe like that,

830
00:55:55,020 --> 00:55:59,370
so much smaller than the real audis,
but they have all the sends Aras,

831
00:55:59,371 --> 00:56:00,204
uh,
that you find in the real audi is you 

832
00:56:01,261 --> 00:56:05,580
find the cameras that Leanne sends us,
they'd go up to 120,

833
00:56:05,610 --> 00:56:07,430
20 kilometers an hour if you,
if,

834
00:56:07,590 --> 00:56:09,420
if they want to.
And,

835
00:56:09,421 --> 00:56:10,254
um,
and they are pain sensor ass basically 

836
00:56:12,451 --> 00:56:13,284
and they don't want to bump against 
obstacles and the Audis and so on as 

837
00:56:17,781 --> 00:56:18,760
they,
um,

838
00:56:19,000 --> 00:56:24,000
my salon like little babies to park 
takes the raw vision input and translate

839
00:56:25,151 --> 00:56:28,840
that into actions that lead to 
successful packing behavior,

840
00:56:29,380 --> 00:56:31,270
which is a rewarding thing.
And yes,

841
00:56:31,271 --> 00:56:36,130
they learned that.
We have examples like that and it's only

842
00:56:36,131 --> 00:56:37,330
in the beginning,
um,

843
00:56:37,390 --> 00:56:38,223
know this is just the tip of the iceberg
and I believe the next wave of ai is 

844
00:56:43,091 --> 00:56:44,710
going to be all about that.

845
00:56:45,250 --> 00:56:46,083
So at the moment,
the current wave of AI is about passive 

846
00:56:49,240 --> 00:56:53,260
pattern observation and the prediction 
and um,

847
00:56:53,680 --> 00:56:54,513
and that's what you have on your 
smartphone and what the major companies 

848
00:56:57,221 --> 00:57:02,080
on the Pacific of Mri using to sell you 
ads to do marketing.

849
00:57:02,320 --> 00:57:05,050
That's the crown,
a source of profit in ai.

850
00:57:05,590 --> 00:57:08,030
And that's only one or 2% of the wild 
economy.

851
00:57:08,040 --> 00:57:08,873
Him.
Yeah.

852
00:57:08,960 --> 00:57:09,793
Um,
which is big enough to make these 

853
00:57:12,051 --> 00:57:15,080
companies is pretty much the most 
valuable companies in the hand.

854
00:57:15,500 --> 00:57:19,250
But there's a much,
much bigger practice,

855
00:57:19,251 --> 00:57:21,780
enough the economy going to be affected 
by the next wave,

856
00:57:21,960 --> 00:57:22,793
which is really about machines that 
shape the data through it was our own 

857
00:57:27,040 --> 00:57:27,873
axons.
Think simulation is ultimately the 

858
00:57:30,721 --> 00:57:31,554
biggest way that that though those 
methods will be successful in the next 

859
00:57:35,911 --> 00:57:36,840
10,
20 years.

860
00:57:36,841 --> 00:57:38,460
We're not talking about a hundred years 
from now.

861
00:57:38,850 --> 00:57:42,650
We're talking about sort of the near 
term impact of rl.

862
00:57:42,660 --> 00:57:43,493
Do you think really good simulation is 
required or is there other techniques 

863
00:57:47,251 --> 00:57:48,420
like imitation,
learning,

864
00:57:48,950 --> 00:57:49,783
you know,
observing other humans operating in the 

865
00:57:53,251 --> 00:57:53,821
real world,
where,

866
00:57:53,821 --> 00:57:56,550
where do you think the success will come
from?

867
00:57:57,660 --> 00:57:58,493
So at the moment we have a tendency of 
using physics simulations to learn 

868
00:58:05,150 --> 00:58:08,350
behavior for machines that,
um,

869
00:58:08,690 --> 00:58:12,620
learn to solve problems that humans also
do not know how to solve.

870
00:58:13,970 --> 00:58:14,803
However,
this is not the future because the 

871
00:58:16,401 --> 00:58:17,234
future is and what little babies do or 
they don't use a physics engine to 

872
00:58:21,501 --> 00:58:22,640
simulate the wild.
No,

873
00:58:22,970 --> 00:58:25,460
they learn a predictive model of the 
wild,

874
00:58:26,180 --> 00:58:27,140
which,
um,

875
00:58:27,650 --> 00:58:28,483
maybe sometimes it's wrong in many ways,
but captures all kinds of important 

876
00:58:33,051 --> 00:58:33,884
abstract,
high level predictions which are really 

877
00:58:35,721 --> 00:58:38,820
important to be successful.
And,

878
00:58:38,850 --> 00:58:40,550
um,
and that's what is,

879
00:58:41,230 --> 00:58:44,300
what's the future 30 years ago when he 
started that type of free sites.

880
00:58:44,330 --> 00:58:47,660
But it's still the future and now we 
know much better how to go there,

881
00:58:47,690 --> 00:58:48,530
um,
uh,

882
00:58:49,060 --> 00:58:50,000
to,
to move that,

883
00:58:50,180 --> 00:58:51,013
to move forward and to really make work 
in systems based on that where you have 

884
00:58:55,820 --> 00:58:56,653
a learning model,
lots of odd and model off the wild that 

885
00:58:58,461 --> 00:59:01,550
learns to predict what's going to happen
if I do that and that.

886
00:59:01,880 --> 00:59:02,940
And then,
uh,

887
00:59:04,310 --> 00:59:05,143
the controller uses and model to more 
quickly learn successful action 

888
00:59:10,131 --> 00:59:12,560
sequences.
And then of course,

889
00:59:12,580 --> 00:59:15,260
always this curiosity thing and the 
beginning of the model and stupid.

890
00:59:15,261 --> 00:59:16,094
So the controller should be motivated to
come up with experiments with action 

891
00:59:19,971 --> 00:59:23,780
sequences that lead to data.
Does that improve as a model,

892
00:59:24,110 --> 00:59:25,110
do you think,
uh,

893
00:59:26,150 --> 00:59:26,983
improving the model,
constructing and understanding of the 

894
00:59:28,281 --> 00:59:32,150
world in this connection?
Is that now the,

895
00:59:32,350 --> 00:59:33,183
the popular approaches that have been 
successful or you're grounded in ideas 

896
00:59:37,011 --> 00:59:40,700
of neural networks,
but in the 80s with expert systems,

897
00:59:40,701 --> 00:59:41,534
there's symbolic ai approaches which uh,
to us humans are more intuitive in the 

898
00:59:47,901 --> 00:59:48,734
sense that it makes sense that you build
up knowledge and just knowledge 

899
00:59:51,471 --> 00:59:52,304
representation.
What kind of lessons can we draw into 

900
00:59:53,960 --> 00:59:58,960
our current approaches in four from 
expert systems from symbolic Ai.

901
01:00:00,570 --> 01:00:01,403
So I um,
became aware of all of that in the ats 

902
01:00:04,621 --> 01:00:08,850
and back then a lottery program logic 
programming was a huge thing.

903
01:00:08,920 --> 01:00:10,590
It was inspiring to you yourself.

904
01:00:10,850 --> 01:00:14,450
Did you find it compelling because most,
a lot of your work was,

905
01:00:14,451 --> 01:00:16,610
uh,
not so much in that realm,

906
01:00:16,611 --> 01:00:18,380
right?
Is more in the learning systems,

907
01:00:18,530 --> 01:00:20,850
yes or no,
but we did all of that is,

908
01:00:20,910 --> 01:00:23,030
are we,
my first,

909
01:00:23,031 --> 01:00:25,010
um,
publication ever actually was,

910
01:00:25,011 --> 01:00:28,080
um,
1987 was a,

911
01:00:28,130 --> 01:00:29,800
the implementation of,
um,

912
01:00:29,840 --> 01:00:33,980
genetic algorithm of a genetic 
programming system in prolog,

913
01:00:35,300 --> 01:00:37,790
prolog.
That's what you learned back then,

914
01:00:37,791 --> 01:00:41,300
which is a logic programming language 
and the Japanese,

915
01:00:41,301 --> 01:00:45,440
the anthis huge fifth generation ai 
project,

916
01:00:45,500 --> 01:00:48,950
which was mostly about logic programming
back then.

917
01:00:49,310 --> 01:00:51,560
Although in your network exists,
existed and,

918
01:00:51,810 --> 01:00:56,810
and well known back then and deep 
learning has existed since 1965.

919
01:00:57,561 --> 01:00:59,750
Um,
since this guy in the Ukraine,

920
01:00:59,800 --> 01:01:01,700
um,
eva can echo started it,

921
01:01:02,180 --> 01:01:03,240
but,
um,

922
01:01:03,710 --> 01:01:04,543
the Japanese and many other people,
they focus really on this logic 

923
01:01:07,071 --> 01:01:07,904
programming.
And I was influenced to the extent that 

924
01:01:09,771 --> 01:01:10,521
I said,
okay,

925
01:01:10,521 --> 01:01:15,290
let's take these biologically inspired 
Ireland's like evolution,

926
01:01:15,760 --> 01:01:16,610
uh,
programs,

927
01:01:16,910 --> 01:01:18,050
uh,
and um,

928
01:01:18,110 --> 01:01:19,020
and,
and,

929
01:01:20,090 --> 01:01:22,310
and implement that in the language,
which I know,

930
01:01:22,311 --> 01:01:25,490
which was prologue for example back then
and then,

931
01:01:25,520 --> 01:01:26,820
um,
in,

932
01:01:26,821 --> 01:01:31,220
in many ways as came back later because 
the Google machine for example,

933
01:01:31,880 --> 01:01:35,540
has a proof search on board and without 
that it would not be optimal.

934
01:01:35,770 --> 01:01:36,603
While mark was hooked as a universal 
algorithm for solving all well defined 

935
01:01:39,701 --> 01:01:44,701
problems as approved search on board.
So that's very much logic programming.

936
01:01:46,430 --> 01:01:48,530
Without that,
it would not be a,

937
01:01:48,531 --> 01:01:51,170
some talking optimum.
But then on the other hand,

938
01:01:51,171 --> 01:01:53,540
because we have a very pragmatic guys 
also,

939
01:01:53,950 --> 01:01:54,783
um,
we focused on recon you on and that's 

940
01:01:58,351 --> 01:01:59,780
where I was and,
and,

941
01:02:00,090 --> 01:02:02,090
and sub optimal,
uh,

942
01:02:02,360 --> 01:02:05,720
stuff such as gradient base search and 
program space.

943
01:02:05,870 --> 01:02:09,060
Rather than prove up the optimal thing,
things

944
01:02:09,140 --> 01:02:13,290
that logic programming does it certain 
certainly has a usefulness in,

945
01:02:13,700 --> 01:02:14,533
uh,
when you're trying to construct 

946
01:02:15,361 --> 01:02:18,780
something provably optimal approvable 
good or something like that.

947
01:02:18,960 --> 01:02:21,450
But is it useful for,
for practical problems?

948
01:02:21,950 --> 01:02:22,783
It's really useful for our theory.
Improving the best deer improvers today 

949
01:02:25,711 --> 01:02:27,950
are not neural networks,
right?

950
01:02:28,000 --> 01:02:31,340
No.
Say a logic programming systems and they

951
01:02:31,341 --> 01:02:32,174
are much better theater improvers then 
most math students in the first or 

952
01:02:36,531 --> 01:02:37,460
second semester.

953
01:02:37,830 --> 01:02:40,400
Hmm.
But for reasoning to,

954
01:02:40,401 --> 01:02:44,750
for playing games of go or chess or for 
robots,

955
01:02:44,810 --> 01:02:49,010
autonomous vehicles that operate in the 
real world or a object manipulation,

956
01:02:49,490 --> 01:02:50,810
you think learning.

957
01:02:51,220 --> 01:02:53,230
Yeah.
As long as the problem is have little to

958
01:02:53,231 --> 01:02:58,231
do with um,
with theo improving themselves,

959
01:02:58,660 --> 01:03:01,480
then um,
as long as that is not the case,

960
01:03:01,481 --> 01:03:02,314
um,
you,

961
01:03:02,890 --> 01:03:05,260
you were just wanting to have better 
pattern recognition.

962
01:03:05,290 --> 01:03:06,123
So to build a self driving car you want 
to have better pattern recognition and 

963
01:03:09,401 --> 01:03:10,234
um,
and a pedestrian recognition and all 

964
01:03:12,761 --> 01:03:16,120
these things and you want to your 
minimum,

965
01:03:16,150 --> 01:03:19,030
you want to minimize the number of false
positives,

966
01:03:19,031 --> 01:03:21,310
which is currently is slowing down self 
driving cars.

967
01:03:21,311 --> 01:03:23,290
In many ways.
And um,

968
01:03:23,380 --> 01:03:26,170
and all of that has very little to do 
with logic programming.

969
01:03:26,260 --> 01:03:27,093
Yeah.

970
01:03:27,510 --> 01:03:32,510
What are you most excited about in terms
of directions of artificial intelligence

971
01:03:34,051 --> 01:03:38,460
at this moment and then the next few 
years in your own research,

972
01:03:38,490 --> 01:03:39,840
in the broader community?

973
01:03:41,560 --> 01:03:42,393
So I think in the not so distant future,
we will have for the first time little 

974
01:03:49,571 --> 01:03:52,930
robots that learn like kids.
Um,

975
01:03:53,020 --> 01:03:56,280
I will be able to say to the robot,
um,

976
01:03:57,390 --> 01:03:58,223
lucky a robot,
we are going to assemble as much fun 

977
01:04:00,910 --> 01:04:04,250
that's takes a slab of plastic,
um,

978
01:04:04,570 --> 01:04:08,080
and the school driver and let's screw 
and the screwed like that,

979
01:04:08,110 --> 01:04:09,130
you know,
not,

980
01:04:09,160 --> 01:04:10,670
not like that,
like that,

981
01:04:11,620 --> 01:04:12,730
not like that,
like that.

982
01:04:13,600 --> 01:04:17,020
And I don't have a data lover or 
something.

983
01:04:17,050 --> 01:04:17,883
He will see me and he will hear me and 
he tried try to do something with his 

984
01:04:23,231 --> 01:04:24,064
own actuators,
which will be really different from 

985
01:04:25,691 --> 01:04:26,524
mine,
but he will understand the difference 

986
01:04:28,090 --> 01:04:33,090
and we'll learn to imitate me,
but not in the supervised way.

987
01:04:34,440 --> 01:04:35,273
Um,
where a teacher has giving targets 

988
01:04:37,440 --> 01:04:40,030
signals for all his muscles all the 
time.

989
01:04:40,180 --> 01:04:43,300
No.
By doing this high level imitation where

990
01:04:43,301 --> 01:04:44,134
he first has to learn to imitate me and 
then to interpret these additional 

991
01:04:48,041 --> 01:04:50,270
noises coming from my mouth,
um,

992
01:04:50,410 --> 01:04:51,243
as helping helpful signals to,
to do that banner and then it will by 

993
01:04:57,671 --> 01:04:58,504
itself come up with a faster ways and 
more efficient ways of doing the same 

994
01:05:02,591 --> 01:05:03,424
thing.

995
01:05:03,730 --> 01:05:04,563
And finally I stopped his learning 
algorithm and make a million copies and 

996
01:05:09,671 --> 01:05:10,504
sell it.
And so at the moment this is not 

997
01:05:12,881 --> 01:05:17,881
possible but we already see how we are 
going to get them and you can imagine to

998
01:05:18,851 --> 01:05:22,150
the extent that this works economically 
and cheaply,

999
01:05:22,151 --> 01:05:22,984
it's going to change everything.
Almost our production is going to be 

1000
01:05:28,841 --> 01:05:33,841
effected by that and a much bigger wave,
much bigger ai wave is coming.

1001
01:05:36,431 --> 01:05:38,380
Then the one that we are currently 
witnessing,

1002
01:05:38,381 --> 01:05:41,740
which is mostly about passive pattern 
recognition on your smartphone.

1003
01:05:42,140 --> 01:05:42,973
This is about active machines that 
shapes a data through the actions they 

1004
01:05:46,991 --> 01:05:50,200
are executing and they learned to do 
that in a good way.

1005
01:05:51,780 --> 01:05:52,090
Okay.

1006
01:05:52,090 --> 01:05:56,680
So many of the traditional industries 
are going to be affected by that.

1007
01:05:56,681 --> 01:06:00,130
All the companies that are building 
machines,

1008
01:06:01,730 --> 01:06:05,600
well equipped for you as machines with 
cameras and other sense auras.

1009
01:06:05,900 --> 01:06:06,733
And they are going to learn to solve all
kinds of problems through interaction 

1010
01:06:12,321 --> 01:06:13,154
with humans but also a lot on their own 
to improve what they already can do 

1011
01:06:16,671 --> 01:06:17,504
them.
And lots of old economy is going to be 

1012
01:06:23,161 --> 01:06:26,930
effected by that.
And in recent years I have seen that old

1013
01:06:26,950 --> 01:06:30,320
economy is actually waking up and 
realizing that those are the canes.

1014
01:06:31,280 --> 01:06:32,113
And um,

1015
01:06:32,190 --> 01:06:35,160
are you optimistic about the future?
Are you concerned?

1016
01:06:35,660 --> 01:06:37,480
Uh,
there's a lot of people concerned on,

1017
01:06:37,540 --> 01:06:42,240
in the near term about the 
transformation of the nature of work.

1018
01:06:43,050 --> 01:06:43,883
The kind of ideas he just suggested 
would have a significant impact of what 

1019
01:06:47,611 --> 01:06:52,020
kinds of things could be automated.
Are you optimistic about that future?

1020
01:06:52,030 --> 01:06:52,863
Are you nervous about that future?
And looking a little bit farther into 

1021
01:06:57,841 --> 01:07:00,150
the future,
there's people like Elon Musk,

1022
01:07:00,750 --> 01:07:01,583
uh,
Stuart Russell concerned about the 

1023
01:07:03,511 --> 01:07:08,511
existential threats of that future.
So in the near term job loss in the long

1024
01:07:09,051 --> 01:07:09,884
term existential threat or these 
concerns to you or you ultimately 

1025
01:07:12,961 --> 01:07:13,794
optimistic.

1026
01:07:15,600 --> 01:07:19,380
So let's first address the near future.

1027
01:07:22,920 --> 01:07:27,920
We have had predictions off job losses 
for many decades.

1028
01:07:28,080 --> 01:07:31,470
For example,
when industrial robots came along,

1029
01:07:32,970 --> 01:07:35,980
many pete,
many people predict and lots of jobs are

1030
01:07:35,981 --> 01:07:36,814
going to get lost.
And in a sense they were right because 

1031
01:07:42,961 --> 01:07:43,794
back then there were car factories on 
hundreds of people and these factories 

1032
01:07:49,831 --> 01:07:50,664
assembled cars and today the same car 
factories have hundreds of robots and 

1033
01:07:54,061 --> 01:07:59,061
maybe three guys watching the robots.
On the other hand,

1034
01:08:00,570 --> 01:08:03,210
those countries that have lots of 
robots,

1035
01:08:03,211 --> 01:08:04,110
para camp,
Aton,

1036
01:08:04,860 --> 01:08:05,850
Japan,
Korea,

1037
01:08:05,890 --> 01:08:06,960
Germany,
Switzerland,

1038
01:08:07,260 --> 01:08:12,260
a couple of other countries,
they have really low unemployment rates.

1039
01:08:14,190 --> 01:08:19,190
Somehow all of new jobs were created 
accent and nobody anticipated.

1040
01:08:20,470 --> 01:08:23,950
There's always jobs.
And um,

1041
01:08:24,830 --> 01:08:25,663
decades ago I always said it's really 
easy to say which jobs are going to get 

1042
01:08:31,491 --> 01:08:32,324
lost,
but it's really hard to predict the new 

1043
01:08:33,351 --> 01:08:36,920
ones.
30 years ago,

1044
01:08:37,010 --> 01:08:37,843
who would have predicted all these 
people and making money as a youtube 

1045
01:08:42,020 --> 01:08:42,950
bloggers.

1046
01:08:43,070 --> 01:08:46,830
For example,
200 years ago,

1047
01:08:47,580 --> 01:08:52,580
60% of all people used to work in 
agriculture today,

1048
01:08:54,181 --> 01:08:59,181
maybe 1%,
but still only,

1049
01:08:59,401 --> 01:09:01,230
I dunno,
5% unemployment,

1050
01:09:02,100 --> 01:09:05,370
lots of new jobs were created and Homo 
Ludens,

1051
01:09:05,590 --> 01:09:10,470
the playing man is inventing new jobs 
all the time.

1052
01:09:10,500 --> 01:09:11,333
Most of these jobs are not existentially
necessary for the survival of our 

1053
01:09:16,951 --> 01:09:17,784
species.
There are only very few existentially 

1054
01:09:21,871 --> 01:09:26,040
necessary jobs such as farming and 
building houses and,

1055
01:09:26,360 --> 01:09:27,193
and warming up the houses.
But less than 10% of the population is 

1056
01:09:30,211 --> 01:09:31,044
doing that.
And most of these newly invented jobs 

1057
01:09:33,571 --> 01:09:34,780
are about,
um,

1058
01:09:36,480 --> 01:09:40,740
interacting with other people in new 
ways through new media and so on,

1059
01:09:41,760 --> 01:09:45,960
getting new kite types of Kudos and 
forms of likes and whatever,

1060
01:09:46,200 --> 01:09:50,460
and even making money through that Homo 
Ludens.

1061
01:09:50,461 --> 01:09:51,294
The playing man doesn't want to be an 
implied and that's why he's inventing 

1062
01:09:54,781 --> 01:09:55,614
new jobs all the time.
And he keeps considering these jobs as 

1063
01:10:00,901 --> 01:10:05,901
really important and there's investing a
lot of energy and hours of work and to,

1064
01:10:06,310 --> 01:10:07,830
and to those new jobs,

1065
01:10:08,130 --> 01:10:08,963
there's a quite beautifully put,
were really nervous about the future 

1066
01:10:11,961 --> 01:10:14,690
because we can't predict what kind of 
new jazz would be created.

1067
01:10:14,950 --> 01:10:19,210
But your ultimately optimistic that we,
uh,

1068
01:10:19,300 --> 01:10:24,300
humans are so restless that we create 
and give meaning to newer in your jobs.

1069
01:10:24,981 --> 01:10:25,814
Totally new likes on faith,
things that get likes on facebook or 

1070
01:10:30,231 --> 01:10:31,064
whatever the social platform is.
So what about long term existential 

1071
01:10:35,301 --> 01:10:36,134
threat of ai or our whole civilization 
may be swallowed up by this ultra super 

1072
01:10:42,771 --> 01:10:44,300
intelligent systems.

1073
01:10:45,290 --> 01:10:48,420
Maybe it's not going to be smaller dub,
but um,

1074
01:10:50,120 --> 01:10:50,953
I'd be surprised if a b where we humans 
were the last step and the evolution of 

1075
01:10:57,021 --> 01:11:00,250
the universe.
And um,

1076
01:11:00,420 --> 01:11:01,253
you,
you've actually had this beautiful 

1077
01:11:03,021 --> 01:11:03,854
comments somewhere that I've seen saying
that artificial quite insightful is 

1078
01:11:09,951 --> 01:11:14,100
artificial general intelligence systems.
Jessica as humans will likely

1079
01:11:14,100 --> 01:11:14,933
not want to interact with humans.
They'll just interact amongst 

1080
01:11:17,491 --> 01:11:18,324
themselves,
just like ants interact amongst 

1081
01:11:20,311 --> 01:11:24,630
themselves and only tangentially 
interact with humans.

1082
01:11:25,410 --> 01:11:27,870
And it's,
it's quite an interesting idea that once

1083
01:11:27,871 --> 01:11:31,660
we create a gi,
they will lose interest in humans and,

1084
01:11:31,710 --> 01:11:36,180
and have compete for their own facebook 
likes on their own social platforms.

1085
01:11:36,750 --> 01:11:38,530
So within that,
uh,

1086
01:11:38,700 --> 01:11:40,110
quite elegant idea,

1087
01:11:40,900 --> 01:11:42,950
the,
how do we know

1088
01:11:43,350 --> 01:11:48,350
in a hypothetical sense that there's not
already intelligent systems out there?

1089
01:11:48,841 --> 01:11:53,841
How do you think broadly of general 
intelligence greater than us,

1090
01:11:54,330 --> 01:11:58,530
how would we know it's out there?
How do we know it's around us?

1091
01:11:59,160 --> 01:12:00,300
And could it already be,

1092
01:12:01,940 --> 01:12:06,200
I'd be surprised if within the next few 
decades or something like that,

1093
01:12:07,340 --> 01:12:08,173
the um,
the won't have ais are truly smart in 

1094
01:12:12,381 --> 01:12:13,214
every single way and better problem 
solvers and almost every single 

1095
01:12:15,591 --> 01:12:16,424
important way.
And I'd be surprised that they wouldn't 

1096
01:12:22,551 --> 01:12:24,860
realize what we have realized a long 
time ago,

1097
01:12:24,861 --> 01:12:25,694
which has that almost all physical 
resources are not here and this 

1098
01:12:30,420 --> 01:12:31,810
biosphere but without

1099
01:12:34,480 --> 01:12:39,480
the rest of the solar system gets 2 
billion times more solar energy than our

1100
01:12:41,921 --> 01:12:42,754
little plot.
There's lots of material out there that 

1101
01:12:45,731 --> 01:12:50,480
you can use to build robots and self 
replicating robot factories and all that

1102
01:12:50,481 --> 01:12:53,110
stuff.
And they're going to do with that.

1103
01:12:53,111 --> 01:12:55,250
And they will be scientists,
um,

1104
01:12:55,540 --> 01:12:58,690
and curious and they will explore what 
they can do them.

1105
01:12:59,800 --> 01:13:00,633
And in the beginning they will be 
fascinated by life and by their own 

1106
01:13:05,501 --> 01:13:07,270
origins.
And I was urbanization.

1107
01:13:07,300 --> 01:13:09,520
They will want to understand that 
completely.

1108
01:13:09,790 --> 01:13:14,010
Just like people today would like to 
understand how life works and um,

1109
01:13:16,210 --> 01:13:17,610
and also,
um,

1110
01:13:18,760 --> 01:13:22,680
the history of our own existence and 
subluxation,

1111
01:13:22,690 --> 01:13:25,480
but then also the physical laws that 
created all of that.

1112
01:13:27,130 --> 01:13:28,040
So they,
um,

1113
01:13:28,300 --> 01:13:29,133
in the beginning they will be fascinated
by life once they understand that there 

1114
01:13:31,920 --> 01:13:32,753
was interest.
I'm like anybody who loses interest and 

1115
01:13:36,940 --> 01:13:41,550
things he understands.
And then as you said,

1116
01:13:41,580 --> 01:13:46,580
um,
the most interesting sources,

1117
01:13:49,520 --> 01:13:52,940
information for them will be others have
their own kinds.

1118
01:13:58,210 --> 01:14:03,210
So at least in the long run,
that seems to be some sort of protection

1119
01:14:06,540 --> 01:14:08,640
through lack of interest on the other 
side.

1120
01:14:11,200 --> 01:14:15,280
And um,
and now it seems also clear as far as we

1121
01:14:15,281 --> 01:14:16,114
understand physics,
you need maton energy to compute and to 

1122
01:14:20,771 --> 01:14:25,300
build more robots and infrastructure and
more Ai.

1123
01:14:25,301 --> 01:14:26,134
Civilization and I ecology is consisting
of trillions of different types of ai 

1124
01:14:31,600 --> 01:14:32,433
and so it seems inconceivable to me that
this thing is not going to expand some 

1125
01:14:37,961 --> 01:14:38,794
ai ecology not controlled by one ai by 
trillions of different types of ai as 

1126
01:14:43,841 --> 01:14:44,674
competing and all kinds of quickly 
evolving and disappearing ecological 

1127
01:14:49,301 --> 01:14:52,090
niches in ways that we cannot fathom at 
the moment,

1128
01:14:52,510 --> 01:14:56,950
but it's going to expound limited 
lightspeed and physics,

1129
01:14:57,010 --> 01:15:02,010
but it's going to expand and we realize 
that the universe is still young.

1130
01:15:03,010 --> 01:15:07,420
It's only 13.8
billion years old and it's going to be a

1131
01:15:07,420 --> 01:15:08,410
thousand times older than that.

1132
01:15:11,800 --> 01:15:12,633
There's plenty of time to conquer the 
entire universe and to fill it with 

1133
01:15:18,511 --> 01:15:19,344
intelligence and senders and receivers 
that ais can travel the way they are 

1134
01:15:24,870 --> 01:15:25,703
traveling an hour labs today,
which is by radio or from sender to 

1135
01:15:29,341 --> 01:15:34,341
receiver and let's call the current age 
of the universe one Ian,

1136
01:15:35,990 --> 01:15:36,823
when Ian now it will take just a few 
eons from now and the entire visible 

1137
01:15:43,201 --> 01:15:44,034
universe.
There's going to be full of that stuff 

1138
01:15:47,370 --> 01:15:51,330
and let's look ahead to a time when the 
universe is going to be 1000 times older

1139
01:15:51,331 --> 01:15:54,690
than it is now.
They will look back and they will say,

1140
01:15:54,691 --> 01:15:57,150
look,
almost immediately after the big bang,

1141
01:15:57,180 --> 01:15:58,013
only a few eons later,
the entire universe started to become 

1142
01:16:01,651 --> 01:16:04,350
intelligent.
Now to your question,

1143
01:16:05,670 --> 01:16:10,470
how do we see whether anything like that
has already happened or has already in a

1144
01:16:10,471 --> 01:16:14,760
more advanced stage in some other parts 
of the universe,

1145
01:16:14,890 --> 01:16:18,810
the visible unit routes we are trying to
look out there and nothing like that has

1146
01:16:18,811 --> 01:16:19,644
happened so far?

1147
01:16:20,670 --> 01:16:22,900
Or is that charter?
What do you think?

1148
01:16:22,901 --> 01:16:24,370
We'll recognize it.
Well,

1149
01:16:24,400 --> 01:16:25,233
how do we know it's not among us?
How do we know planets aren't in 

1150
01:16:28,271 --> 01:16:33,271
themselves intelligent beings?
How do we know ants seen as a collective

1151
01:16:36,610 --> 01:16:40,000
or not much greater intelligence than 
our own?

1152
01:16:40,270 --> 01:16:43,060
These kinds of ideas.
And it was a boy.

1153
01:16:43,090 --> 01:16:44,620
I was thinking about these things

1154
01:16:45,190 --> 01:16:49,840
and I thought maybe it has already 
happened because back then I know,

1155
01:16:50,170 --> 01:16:51,003
I knew,
I learned from poplar physics box that 

1156
01:16:54,730 --> 01:16:55,563
the structure of the large scale 
structure of the universe is not 

1157
01:16:58,600 --> 01:16:59,433
homogeneous and you have these clusters 
of galaxies and then in between the 

1158
01:17:04,540 --> 01:17:09,380
these huge empty spaces.
And I thought,

1159
01:17:09,410 --> 01:17:12,410
hmm,
maybe say island's really empty.

1160
01:17:12,411 --> 01:17:14,810
It's just that in the middle of that 
some ai,

1161
01:17:14,811 --> 01:17:15,644
so lization already has expanded and 
then has covered a bottle of 1 billion 

1162
01:17:20,930 --> 01:17:25,930
light years time visa and it's using all
the energy of all the styles within that

1163
01:17:26,271 --> 01:17:29,090
bubble for its own unfathomable 
practices.

1164
01:17:29,600 --> 01:17:34,600
And so it always happened and we just 
fail to interpretate the signs.

1165
01:17:34,880 --> 01:17:35,713
But then alarm of that gravity by itself
explains the large scale structure of 

1166
01:17:41,631 --> 01:17:44,660
the universe and that this is not a 
convincing explanation.

1167
01:17:45,470 --> 01:17:46,303
And then I thought maybe vb,
it's the dark matter because as long as 

1168
01:17:53,091 --> 01:17:53,924
we know today 80% of the measurable 
matter is invisible and we know that 

1169
01:18:01,851 --> 01:18:05,890
because otherwise our galaxy or other 
galaxies worked fall apart.

1170
01:18:05,950 --> 01:18:06,783
They would,
they are rotating too quickly and then 

1171
01:18:11,621 --> 01:18:12,454
the idea was maybe all us,
he is ai civilizations that are already 

1172
01:18:16,571 --> 01:18:17,404
out there.
They they just invisible because they 

1173
01:18:22,751 --> 01:18:26,800
are really efficient and using the 
energies are their own local systems and

1174
01:18:26,801 --> 01:18:29,820
that's why they appear doctors.
What's,

1175
01:18:29,960 --> 01:18:30,793
this is awesome at a convincing 
explanation because then the question 

1176
01:18:34,211 --> 01:18:37,070
becomes why is there,

1177
01:18:38,250 --> 01:18:41,820
are there still any visible stars were 
left in our own galaxy,

1178
01:18:42,060 --> 01:18:44,160
which also must have a lot of dark 
matter,

1179
01:18:44,610 --> 01:18:45,443
so that is awesome.
Not a convincing thing and today I like 

1180
01:18:49,111 --> 01:18:54,111
to thing it's quite plausible that maybe
the first,

1181
01:18:54,580 --> 01:18:59,580
at least in our local light cone within 
it's a few hundreds of millions of light

1182
01:19:04,181 --> 01:19:09,010
years and we can reliably observe,
observe.

1183
01:19:09,220 --> 01:19:10,720
Is that exciting to you?
They will.

1184
01:19:10,721 --> 01:19:11,554
Might be the first,
and it would make us much more 

1185
01:19:15,491 --> 01:19:19,780
importance because if we mess it up 
through a nuclear war,

1186
01:19:20,740 --> 01:19:21,573
then,
then maybe this will have an effect on 

1187
01:19:25,671 --> 01:19:26,900
the,
on the,

1188
01:19:26,960 --> 01:19:30,270
on the development on the entire 
universe.

1189
01:19:31,130 --> 01:19:33,360
So let's not mess it up.
It's not miss it.

1190
01:19:33,630 --> 01:19:35,690
You again,
thank you so much for talking today.

1191
01:19:35,750 --> 01:19:38,060
I really appreciate it.
It's my pleasure.

