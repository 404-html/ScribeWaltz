1
00:00:00,510 --> 00:00:05,510
Today we have sterling Anderson is the 
cofounder of Aurora and exciting new 

2
00:00:05,641 --> 00:00:10,641
self driving car company.
Previously he was the head of the Tesla 

3
00:00:10,641 --> 00:00:13,950
autopilot team that brought both the 
first and second generation autopilot to

4
00:00:13,951 --> 00:00:17,880
life.
Before that he did his phd at Mit,

5
00:00:17,881 --> 00:00:21,660
working on shared humor,
machine control of ground vehicles,

6
00:00:21,990 --> 00:00:25,170
the very thing I've been harping on over
and over in this class,

7
00:00:25,680 --> 00:00:29,010
and now he's back at mit to talk with 
us.

8
00:00:29,190 --> 00:00:30,540
Please give him a warm welcome.

9
00:00:36,130 --> 00:00:37,810
Thank you.
It's good to be here.

10
00:00:37,811 --> 00:00:42,811
I was telling Lex just before,
I think it's been a little while since 

11
00:00:42,811 --> 00:00:44,050
I've been back to the institute and so 
great to be here.

12
00:00:44,620 --> 00:00:49,620
I want to apologize in advance.
I've just landed this afternoon from 

13
00:00:49,620 --> 00:00:53,581
Korea via Germany where I've been 
spending the last week and so I may 

14
00:00:53,921 --> 00:00:58,921
speak a little slower than normal.
Please bear with me if I become 

15
00:00:58,921 --> 00:00:59,770
incoherent or a slur,
my speech,

16
00:01:00,010 --> 00:01:03,220
somebody flag it too and we'll uh,
we'll try to make corrections.

17
00:01:03,520 --> 00:01:08,520
So tonight I thought I'd chat with you a
little bit about my journey over the 

18
00:01:08,520 --> 00:01:10,930
last decade span,
just over 10 years since I was at mit.

19
00:01:11,410 --> 00:01:16,410
A lot has changed,
a lot has changed for the better in the 

20
00:01:16,410 --> 00:01:18,991
self driving community and I've been 
privileged to be a part of many of those

21
00:01:19,191 --> 00:01:24,191
changes and so on at the time with you,
a little bit about some of the things 

22
00:01:24,191 --> 00:01:24,191
that I've learned,
some of the things that I've experienced

23
00:01:24,230 --> 00:01:29,230
and then maybe end by talking about sort
of where we go from here and what the 

24
00:01:29,781 --> 00:01:32,600
next steps are both for the industry at 
large,

25
00:01:32,601 --> 00:01:37,601
but also for the company that we're 
building that Alex mentioned is called 

26
00:01:37,601 --> 00:01:39,290
Aurora to start out with.

27
00:01:40,230 --> 00:01:45,230
There are a few sort of key phases or 
transitions in my journey over the last 

28
00:01:45,591 --> 00:01:49,550
10 years.
As I mentioned when I started at Mit,

29
00:01:49,551 --> 00:01:52,560
I worked with Carl Yani,
Mr Amelio for Zalia,

30
00:01:52,561 --> 00:01:57,561
John Leonard,
a few others on some of these sort of 

31
00:01:57,561 --> 00:02:01,170
shared adaptive automation approaches.
Um,

32
00:02:01,171 --> 00:02:04,310
I'll talk a little bit about those from 
there.

33
00:02:04,340 --> 00:02:09,340
I spent some time at Tesla where I first
sled the model x program as we both 

34
00:02:10,851 --> 00:02:12,660
finished the development and ultimately 
launched it,

35
00:02:13,640 --> 00:02:18,640
took over the autopilot program where we
introduce a number of new,

36
00:02:19,790 --> 00:02:24,790
both active safety,
but also sort of enhanced convenience 

37
00:02:25,371 --> 00:02:30,371
features from autosteer to debit cruise 
control that we're able to refine a few 

38
00:02:30,381 --> 00:02:35,381
unique ways and we'll talk a little bit 
about that and then from there in 

39
00:02:35,381 --> 00:02:39,551
December of last year of 2016,
I guess now we started a new company 

40
00:02:39,551 --> 00:02:41,480
called Aurora and I'll tell you a little
bit about that.

41
00:02:42,350 --> 00:02:45,530
So to start out with,
when I came,

42
00:02:45,531 --> 00:02:46,910
no,
it was 2007.

43
00:02:47,030 --> 00:02:52,030
The Darpa urban challenges were well 
underway at that stage and one of the 

44
00:02:52,030 --> 00:02:56,201
things that we wanted to do is find a 
way to address some of these safety 

45
00:02:56,201 --> 00:03:00,281
issues in human driving earlier.
Then potentially full cell driving could

46
00:03:00,971 --> 00:03:05,380
do and so we developed what became known
as the intelligent copilot.

47
00:03:06,010 --> 00:03:09,280
What you see here is a simulation of 
that operating.

48
00:03:09,281 --> 00:03:11,260
I'll tell you a little bit more about 
that in just a second,

49
00:03:12,520 --> 00:03:15,250
but to explain a little bit about the 
methodology.

50
00:03:15,251 --> 00:03:20,251
The innovation,
the key approach that we took there was 

51
00:03:20,251 --> 00:03:22,620
slightly different from what a 
traditional planning control theory,

52
00:03:22,630 --> 00:03:26,920
which we were doing was instead of 
designing in path space,

53
00:03:26,960 --> 00:03:27,910
uh,
for the robot,

54
00:03:27,911 --> 00:03:32,140
we instead a found a way to identify a 
plan,

55
00:03:32,150 --> 00:03:37,150
optimize and design a controller subject
to a set of constraints rather than 

56
00:03:37,150 --> 00:03:37,420
paths.

57
00:03:38,080 --> 00:03:40,940
And so what we're doing is looking for 
Hamas certain environment.

58
00:03:40,941 --> 00:03:45,941
So imagine for a moment in an 
environment that's a pockmarked by 

59
00:03:45,941 --> 00:03:46,630
objects,
by their vehicles,

60
00:03:46,631 --> 00:03:48,040
by pedestrians,
etc.

61
00:03:49,210 --> 00:03:53,020
If you were to create the voronoi 
diagrams through that environment,

62
00:03:53,170 --> 00:03:56,580
you would have a set of each unique,
uh,

63
00:03:56,710 --> 00:04:01,000
set of pads or hama is continuously 
deformable paths that will take you from

64
00:04:01,001 --> 00:04:03,970
one,
one location to another through it.

65
00:04:04,960 --> 00:04:07,360
If you then turn that into its dual,
which is the dullest,

66
00:04:07,361 --> 00:04:12,361
a triangulation of set environment,
presuming that you've got conduct's 

67
00:04:12,361 --> 00:04:15,991
obstacles,
you can then tie all those together 

68
00:04:15,991 --> 00:04:15,991
rather trivially to create a set of,
uh,

69
00:04:15,991 --> 00:04:17,650
[inaudible] and,
uh,

70
00:04:17,680 --> 00:04:19,780
transitions across,
switch those paths.

71
00:04:20,110 --> 00:04:25,110
A canon can stake out sort of a giVen 
set of options for the human.

72
00:04:25,181 --> 00:04:30,181
It turns out humans tend to.
This tends to be a more intuitive way of

73
00:04:30,310 --> 00:04:35,310
imposing certain constraints on human 
operation rather than enforcing that the

74
00:04:36,160 --> 00:04:41,160
ego vehicle stick to some arbitrary 
position within some distance of a safe 

75
00:04:41,831 --> 00:04:42,190
path.

76
00:04:42,590 --> 00:04:46,210
A,
you instead look to enforce only that,

77
00:04:46,211 --> 00:04:51,211
the,
that the state of the vehicle remain 

78
00:04:51,211 --> 00:04:52,090
within a constraint bounded end 
dimensional tube in state space.

79
00:04:52,570 --> 00:04:57,570
Those constraints being spatial.
Imagine for a moment edges of the 

80
00:04:57,570 --> 00:04:59,950
roadway or circumventing various objects
in the roadway.

81
00:05:00,360 --> 00:05:03,580
I'm a imagine them also being dynamic,
right?

82
00:05:03,581 --> 00:05:08,581
So limits of tire,
a tire friction imposed limits on side 

83
00:05:09,201 --> 00:05:11,650
slip angles.
And so using that,

84
00:05:11,920 --> 00:05:15,400
what we did is found a way to create 
those hammad topis forward simulate,

85
00:05:15,480 --> 00:05:17,380
uh,
the trajectory of the vehicle,

86
00:05:17,660 --> 00:05:21,110
a given its current state and some 
optimal set of controls.

87
00:05:21,130 --> 00:05:26,130
Seek inputs that would optimize its 
stability through that we use model 

88
00:05:26,130 --> 00:05:26,800
predictive control,
uh,

89
00:05:26,830 --> 00:05:30,340
in that work and then taking that 
forward,

90
00:05:30,341 --> 00:05:34,810
simulated trajectory computing some 
metric of threat.

91
00:05:34,840 --> 00:05:37,930
For instance,
if the objective function for that,

92
00:05:38,290 --> 00:05:43,290
uh,
minimize the or maximize stability or 

93
00:05:43,290 --> 00:05:43,840
minimize some,
some of these parameters like we'll side

94
00:05:43,841 --> 00:05:48,841
slip,
then we'll side slip is a fairly good 

95
00:05:48,841 --> 00:05:52,090
indication of how threatening that 
optimal maneuver is becoming.

96
00:05:52,780 --> 00:05:57,130
And so what we did is then use that in a
modulation of control between the human,

97
00:05:57,290 --> 00:06:02,290
the car,
such that should the car ever find 

98
00:06:02,290 --> 00:06:05,421
yourself in a state where that forward 
simulated optimal trajectory is very 

99
00:06:05,421 --> 00:06:07,310
near the limits of what the vehicle and 
it can actually handle.

100
00:06:07,940 --> 00:06:10,700
We will have transitioned control fully 
to the,

101
00:06:10,760 --> 00:06:15,760
to the vehicle,
to the automated system so that it can 

102
00:06:15,760 --> 00:06:18,041
avoid an accident.
And then it transitions back in some 

103
00:06:18,041 --> 00:06:18,041
manner and we played with a number of 
different,

104
00:06:18,260 --> 00:06:22,100
uh,
methods of transitioning this control to

105
00:06:22,101 --> 00:06:23,220
ensure that,
um,

106
00:06:23,790 --> 00:06:28,790
uh,
that we didn't throw off the human 

107
00:06:28,790 --> 00:06:28,880
mental model which was,
which was one of the key concerns.

108
00:06:29,270 --> 00:06:34,270
We also wanted to make sure that we were
able to arrest a accidents before they 

109
00:06:34,270 --> 00:06:39,071
happen.
What you see here is a simulation that 

110
00:06:39,071 --> 00:06:42,671
was fairly faithful to the behavior we 
saw in test drivers up at dearborn in 

111
00:06:43,911 --> 00:06:48,911
dearborn,
Michigan ford provided was provided us 

112
00:06:48,911 --> 00:06:50,900
with a jaguar site to test this on and 
what we did.

113
00:06:50,900 --> 00:06:53,240
So what you see here is there's a blue 
vehicle and a gray vehicle,

114
00:06:53,270 --> 00:06:57,800
both in both cases.
We have a poorly tuned driver model.

115
00:06:57,801 --> 00:07:01,640
In this case it pure pursuit controller 
with a fairly short look ahead,

116
00:07:01,690 --> 00:07:06,500
a shorter than would be appropriate 
given this scenario in these dynamics.

117
00:07:07,160 --> 00:07:12,160
Uh,
the gray vehicle is without the 

118
00:07:12,160 --> 00:07:12,160
intelligent copilot in the loop.
Um,

119
00:07:12,230 --> 00:07:15,470
you'll notice that obviously the driver 
becomes unstable,

120
00:07:15,471 --> 00:07:17,510
loses control,
and leaves the safe roadway.

121
00:07:18,380 --> 00:07:19,610
The copilot,
remember,

122
00:07:19,820 --> 00:07:23,390
is in,
is interested not in following any given

123
00:07:23,391 --> 00:07:28,391
path.
It doesn't care where the vehicle lands 

124
00:07:28,391 --> 00:07:30,080
on this roadway provided it remains on 
inside the road.

125
00:07:30,510 --> 00:07:32,750
Uh,
in the blue vehicles case.

126
00:07:32,820 --> 00:07:35,000
Uh,
it's the exact same human driver model.

127
00:07:35,630 --> 00:07:39,250
Now at the copilot and the loop,
you'll notice that as a,

128
00:07:39,260 --> 00:07:44,260
as this scenario a continues.
What you see here on the left is the 

129
00:07:44,260 --> 00:07:48,821
green is in this green bar,
is the portion of available control 

130
00:07:48,821 --> 00:07:49,970
authority is being taken by the 
automated system.

131
00:07:49,970 --> 00:07:52,180
You don't understand,
I've never exceeds half of the available

132
00:07:52,190 --> 00:07:54,800
control,
which is to say that the steering inputs

133
00:07:54,801 --> 00:07:56,560
received by the vehicle,
uh,

134
00:07:56,570 --> 00:08:01,570
and end up being a blend of what the 
human and what the automation are 

135
00:08:01,570 --> 00:08:02,420
providing,
um,

136
00:08:03,260 --> 00:08:08,260
and what,
what results is a path for the blue 

137
00:08:08,260 --> 00:08:11,801
vehicle that actually better tracks the 
humans intended trajectory.

138
00:08:12,290 --> 00:08:15,170
Then even the copilot understood,
right?

139
00:08:15,230 --> 00:08:20,230
Again,
the copilot is keeping the vehicle 

140
00:08:20,230 --> 00:08:21,701
stable as keeping it on the road.
The human is hewing to the center line 

141
00:08:21,830 --> 00:08:26,830
of that roadway.
So there was some very interesting 

142
00:08:26,830 --> 00:08:26,830
things that came out of this.
There were a lot of,

143
00:08:26,830 --> 00:08:30,660
uh,
we did a lot of work in understanding 

144
00:08:30,660 --> 00:08:31,850
what kind of feedback was most natural 
to provide a human.

145
00:08:32,030 --> 00:08:37,030
Our biggest concern was if you throw off
a human's mental model by causing the 

146
00:08:37,030 --> 00:08:41,651
vehicles behaviors to deviate from what 
they expect it to do in response to 

147
00:08:41,751 --> 00:08:43,900
various control inputs,
that can be a problem.

148
00:08:43,920 --> 00:08:46,100
So we tried various things from,
you know,

149
00:08:46,101 --> 00:08:47,150
adjusting,
for instance,

150
00:08:47,151 --> 00:08:52,151
one of the,
one of the key questions that we had 

151
00:08:52,151 --> 00:08:53,741
early on was a,
if we couple the computer control and 

152
00:08:54,591 --> 00:08:59,591
the human control via planetary gear and
allow the human to feel a actually a 

153
00:09:01,321 --> 00:09:03,180
backwards torque to what the vehicle is 
doing.

154
00:09:03,181 --> 00:09:07,230
So the car starts to turn right,
human will feel the wheel turn left,

155
00:09:07,231 --> 00:09:10,950
they'll see it started to turn left.
Is that more confusing or less confusing

156
00:09:10,951 --> 00:09:15,951
to human.
And it turns out it depends on how 

157
00:09:15,951 --> 00:09:15,951
experienced the human is.
Some,

158
00:09:15,951 --> 00:09:19,820
some drivers will modulate their input 
space on tHe torque feedback that they 

159
00:09:19,820 --> 00:09:20,670
feel through the wheel.
For instance,

160
00:09:20,671 --> 00:09:25,671
a very experienced driver expects to 
fuel the we'll pull left when they're 

161
00:09:25,671 --> 00:09:27,120
turning right.
However,

162
00:09:27,150 --> 00:09:32,150
less experienced drivers in response to 
seeing the wheel turning opposite to 

163
00:09:32,150 --> 00:09:36,441
what the,
what the car is supposed to be doing 

164
00:09:36,441 --> 00:09:36,441
this for a rather confusing experience.

165
00:09:36,441 --> 00:09:39,540
So there were a lot of really 
interesting a human interface challenges

166
00:09:39,960 --> 00:09:43,030
that we were dealing with here.
Um,

167
00:09:43,890 --> 00:09:45,930
we ended up working through a lot of 
that,

168
00:09:46,290 --> 00:09:48,360
developing a number of,
uh,

169
00:09:50,220 --> 00:09:52,530
uh,
sort of micro applications for it.

170
00:09:53,040 --> 00:09:53,910
One of those,
uh,

171
00:09:53,911 --> 00:09:58,911
at the time gill pratt was leading a 
darpa program focused on what they call 

172
00:09:58,911 --> 00:10:00,510
the time maximum ability and 
manipulation.

173
00:10:01,440 --> 00:10:06,440
Uh,
we decided to see what the system could 

174
00:10:06,440 --> 00:10:08,820
do in application to unmanned ground 
vehicles.

175
00:10:08,850 --> 00:10:11,110
So in this case,
what you see is a,

176
00:10:11,111 --> 00:10:14,370
a human driver sitting at a remote 
console as,

177
00:10:14,371 --> 00:10:18,360
as one would want operating and unmanned
vehicle,

178
00:10:18,361 --> 00:10:19,590
for instance,
in the military.

179
00:10:20,090 --> 00:10:24,510
Um,
what you see on the left top left is the

180
00:10:24,780 --> 00:10:29,780
top down view of what the vehicle sees.
I should've played this in repeat mode 

181
00:10:30,810 --> 00:10:32,880
with bounding boxes,
bounding various cones.

182
00:10:32,881 --> 00:10:35,070
And what we did is he set up about 20 
drivers,

183
00:10:35,071 --> 00:10:37,440
20,
20 a test subjects,

184
00:10:37,880 --> 00:10:39,360
uh,
looking at this,

185
00:10:39,450 --> 00:10:40,340
this,
uh,

186
00:10:40,380 --> 00:10:44,040
control screen and operating the vehicle
through this track.

187
00:10:44,070 --> 00:10:49,070
And we set this up as a race with a 
prizes for the winners as,

188
00:10:49,251 --> 00:10:54,251
as one would expect.
And I'm penalize them for every barely 

189
00:10:54,251 --> 00:10:56,130
hits.
If they knocked over the barrel,

190
00:10:56,131 --> 00:11:01,131
I think they got a five second penalty.
If they brushed a barely got one second 

191
00:11:01,131 --> 00:11:04,371
penalty and they were to cross,
they worked across the field as fast as 

192
00:11:04,371 --> 00:11:05,010
possible and they couldn't.
They had no line of sight connection the

193
00:11:05,011 --> 00:11:06,320
vehicle.
And we played with some things on,

194
00:11:06,370 --> 00:11:07,470
on their interface.
We did,

195
00:11:07,930 --> 00:11:08,390
uh,
you know,

196
00:11:08,410 --> 00:11:13,410
we,
we caused it to drop out occasionally 

197
00:11:13,410 --> 00:11:13,410
we,
we delayed it as,

198
00:11:13,410 --> 00:11:14,220
as one realistically expect in the 
field.

199
00:11:15,030 --> 00:11:20,030
And then we either engaged or didn't 
engage the copilot to try to understand 

200
00:11:20,030 --> 00:11:22,500
what effect that had on their 
performance and their experience.

201
00:11:23,070 --> 00:11:27,210
And what we found was not surprisingly,
the incidence of collisions declined,

202
00:11:27,290 --> 00:11:32,290
declined by about 72 percent when the 
copilot was engaged versus when it was 

203
00:11:32,290 --> 00:11:32,290
not a.

204
00:11:32,290 --> 00:11:34,320
We also found that,
you know,

205
00:11:34,950 --> 00:11:37,380
even with that 72 percent decline in 
collisions,

206
00:11:37,590 --> 00:11:39,690
uh,
the speed increased by,

207
00:11:39,750 --> 00:11:43,280
I'm blanking on the amount,
but it was 20 or 30 percent ish.

208
00:11:44,700 --> 00:11:47,610
Finally,
and perhaps most interesting to me after

209
00:11:47,611 --> 00:11:49,050
every run.
Uh,

210
00:11:49,051 --> 00:11:51,120
I would ask the driver in the,
again,

211
00:11:51,121 --> 00:11:56,121
these were blind tests.
They didn't know if the copilot was 

212
00:11:56,121 --> 00:11:57,960
accurate or not,
and I would ask them how much control 

213
00:11:57,960 --> 00:11:57,960
did you feel like you've had over the 
vehicle?

214
00:11:59,080 --> 00:12:04,080
And I found that there was a 
statistically significant increase of 

215
00:12:04,080 --> 00:12:04,370
about 12 percent when the copilot was 
engaged in,

216
00:12:04,490 --> 00:12:08,800
in that is to say drivers reported 
feeling more control the vehicle,

217
00:12:09,160 --> 00:12:13,150
12 percent more of the time when the 
copilot was engaged and when it wasn't.

218
00:12:13,780 --> 00:12:15,340
And then I,
the statistics,

219
00:12:15,341 --> 00:12:18,280
it turns out they actually at the 
average level of control that the,

220
00:12:18,281 --> 00:12:20,560
that the copilot was taking was 43 
percent.

221
00:12:20,560 --> 00:12:25,560
So they were reporting that they felt 
more in control when in fact there were 

222
00:12:25,560 --> 00:12:26,970
43 percent less in control.
um,

223
00:12:27,190 --> 00:12:28,810
which was,
which was interesting.

224
00:12:28,811 --> 00:12:29,500
And,
uh,

225
00:12:29,501 --> 00:12:34,501
I think a bears a little bit on sort of 
the human psyche in terms of,

226
00:12:35,050 --> 00:12:35,500
uh,
you know,

227
00:12:35,501 --> 00:12:37,840
they were reporting the vehicle was 
doing what I wanted to do,

228
00:12:37,841 --> 00:12:39,570
maybe not what I told it to do,
uh,

229
00:12:39,750 --> 00:12:41,350
which was,
which was kind of fun.

230
00:12:41,640 --> 00:12:43,450
Uh,
observation and then fund a,

231
00:12:43,850 --> 00:12:44,340
uh,
I think,

232
00:12:44,390 --> 00:12:46,390
I think the most enjoyable part of this 
was,

233
00:12:46,391 --> 00:12:48,280
uh,
getting together with the,

234
00:12:48,310 --> 00:12:50,130
with the whole group at the end of the 
study and,

235
00:12:50,131 --> 00:12:52,840
and presenting some of this and seeing 
some of the reactions.

236
00:12:54,700 --> 00:12:56,710
So from there,
um,

237
00:12:56,800 --> 00:12:57,160
you know,
we,

238
00:12:57,190 --> 00:12:59,020
we,
we looked at a few other areas,

239
00:12:59,080 --> 00:13:00,610
uh,
my,

240
00:13:00,670 --> 00:13:03,910
uh,
carl unm and I looked at a few different

241
00:13:03,940 --> 00:13:05,940
opportunities to commercialize this.
Again,

242
00:13:05,950 --> 00:13:10,950
this was years ago and the industry was 
in a very different place than it is 

243
00:13:10,950 --> 00:13:10,950
today.
Uh,

244
00:13:10,950 --> 00:13:13,540
we started a company called gimlet than 
another called ride.

245
00:13:13,930 --> 00:13:18,930
Um,
this is the logo may look familiar to 

246
00:13:18,930 --> 00:13:19,660
you.
We turn that into a,

247
00:13:19,680 --> 00:13:23,380
we had at the time it intended to roll 
this out across,

248
00:13:23,470 --> 00:13:25,690
um,
um,

249
00:13:26,290 --> 00:13:28,870
you know,
various automakers in their operations,

250
00:13:28,970 --> 00:13:29,950
uh,
at the time,

251
00:13:30,220 --> 00:13:34,820
very few saw self driving as a 
technology.

252
00:13:34,821 --> 00:13:37,330
It was really going to impact their 
business going forward.

253
00:13:38,270 --> 00:13:43,270
They were,
in fact even even ride sharing at the 

254
00:13:43,270 --> 00:13:44,570
time was a fairly new concept.
Um,

255
00:13:44,830 --> 00:13:49,830
that was,
I think to a large degree viewed as 

256
00:13:49,830 --> 00:13:49,830
unproven.
Um,

257
00:13:49,830 --> 00:13:52,250
so as I mentioned december of last year,

258
00:13:56,200 --> 00:14:01,200
I cofounded aurora with a couple of 
folks who have been making significant 

259
00:14:02,171 --> 00:14:04,510
progress in this space for many years.
Chris urmson,

260
00:14:04,990 --> 00:14:08,890
who formerly led google's self driving 
car group at drew back now,

261
00:14:08,891 --> 00:14:10,720
is a professor at carnegie mellon 
university,

262
00:14:11,030 --> 00:14:12,580
uh,
exceptional machine learning,

263
00:14:12,680 --> 00:14:14,200
uh,
and an applied machine learning,

264
00:14:14,520 --> 00:14:19,520
uh,
was one of the founding members of uber 

265
00:14:19,520 --> 00:14:19,520
self driving car team and lead autonomy 
and perception there.

266
00:14:19,530 --> 00:14:22,120
Um,
we felt like we had a unique opportunity

267
00:14:22,121 --> 00:14:24,670
at the convergence of a few things.
One,

268
00:14:25,060 --> 00:14:30,060
the automotive world has really come 
into the full on realization that self 

269
00:14:31,271 --> 00:14:36,271
driving and particularly self driving 
and ride sharing and vehicle 

270
00:14:36,271 --> 00:14:39,331
electrification,
our three vectors that will change the 

271
00:14:39,331 --> 00:14:39,331
industry.
Um,

272
00:14:39,331 --> 00:14:43,710
that was something that didn't exist 10 
years ago to a significant advances have

273
00:14:44,261 --> 00:14:46,470
been made in,
um,

274
00:14:46,510 --> 00:14:51,510
you know,
some of these machine learning 

275
00:14:51,510 --> 00:14:51,510
techniques in particular deep learning,
um,

276
00:14:51,510 --> 00:14:52,020
and other neural network

277
00:14:52,040 --> 00:14:55,940
network approaches in the computers that
run them.

278
00:14:56,530 --> 00:15:01,530
And the availability of low power gpu 
and tpu options to really do that well 

279
00:15:04,400 --> 00:15:07,640
in sensing technologies in high 
resolution radar.

280
00:15:07,730 --> 00:15:09,140
And a lot of the light,
our development,

281
00:15:09,620 --> 00:15:12,140
so it's really a unique time in the self
driving world.

282
00:15:12,141 --> 00:15:14,780
A lot of these things are really coming 
together now.

283
00:15:15,140 --> 00:15:20,140
Uh,
and we felt like by bringing together 

284
00:15:20,140 --> 00:15:22,451
and experienced team,
we had an interesting opportunity to 

285
00:15:22,451 --> 00:15:23,240
build from a clean sheet,
a new,

286
00:15:23,420 --> 00:15:24,740
uh,
platform,

287
00:15:25,010 --> 00:15:30,010
a new self driving a architecture that 
leverages the latest events as applied 

288
00:15:30,040 --> 00:15:33,250
machine learning together with our,
um,

289
00:15:33,970 --> 00:15:35,790
uh,
together with our experience of,

290
00:15:35,840 --> 00:15:40,840
of where some of the pitfalls tend to be
down the road as you develop these 

291
00:15:40,840 --> 00:15:41,180
systems because you don't tend to see 
them early on.

292
00:15:41,181 --> 00:15:46,181
they tend to express themselves as you 
get into the long tail of corner cases 

293
00:15:46,181 --> 00:15:46,700
that she ended up needing to resolve.

294
00:15:47,780 --> 00:15:49,490
So we've built that team.
Uh,

295
00:15:49,520 --> 00:15:52,550
we have offices in palo alto,
California and pittsburgh,

296
00:15:52,551 --> 00:15:54,140
Pennsylvania.
Uh,

297
00:15:54,141 --> 00:15:57,170
we've got fleets of vehicles operating 
in both politics in Pennsylvania.

298
00:15:57,770 --> 00:15:59,330
A couple of weeks ago,
uh,

299
00:15:59,390 --> 00:16:01,520
we announced that a volkswagen group,
uh,

300
00:16:01,580 --> 00:16:02,990
one of the largest auto makers in the 
world,

301
00:16:03,050 --> 00:16:08,050
hyundai motor company,
also on the largest automakers in the 

302
00:16:08,050 --> 00:16:08,330
world,
have both partnered with aurora.

303
00:16:08,380 --> 00:16:10,640
Um,
we will be developing.

304
00:16:10,670 --> 00:16:14,640
And are developing with them a set of 
platforms and ultimately we'll,

305
00:16:14,650 --> 00:16:16,030
we'll scale that,
uh,

306
00:16:16,031 --> 00:16:19,040
our technology.
I'm on their vehicles across the world.

307
00:16:19,041 --> 00:16:20,450
and,
and one of the important,

308
00:16:21,480 --> 00:16:25,550
the important elements of building.
I asked lex before coming out here,

309
00:16:25,551 --> 00:16:28,460
what's this group would be most 
interested in hearing?

310
00:16:28,461 --> 00:16:33,461
One of the things that he mentioned was 
what does it take to build a self 

311
00:16:33,461 --> 00:16:33,461
driving,
you know,

312
00:16:33,461 --> 00:16:33,461
build a new company in a space like 
this.

313
00:16:34,100 --> 00:16:39,100
One of the things that we found very 
important was a business model that was 

314
00:16:39,100 --> 00:16:41,720
nonthreatening to others.
Uh,

315
00:16:41,780 --> 00:16:46,780
we recognize that our strengths and our 
experience over the last say in my case 

316
00:16:46,780 --> 00:16:51,431
at decade in chris's case,
almost hear a really lies in the 

317
00:16:51,471 --> 00:16:54,170
development of the self driving systems.
Uh,

318
00:16:54,200 --> 00:16:57,170
not in building vehicles that I have had
some experience there.

319
00:16:57,171 --> 00:16:59,060
But,
but in developing the self driving.

320
00:16:59,061 --> 00:17:04,061
And so our feeling was if our mission is
to get a signal to market as quickly as 

321
00:17:04,131 --> 00:17:09,131
broadly and as safely as possible.
And that mission is best served by 

322
00:17:09,131 --> 00:17:12,050
playing our position and working well 
with others who can play.

323
00:17:12,070 --> 00:17:17,070
There's.
Which is why you see the model that 

324
00:17:17,070 --> 00:17:20,171
we've adopted and is now a,
you'll start to see some of the fruits 

325
00:17:20,171 --> 00:17:21,100
of that through these partnerships with 
some of these automakers.

326
00:17:21,580 --> 00:17:26,580
So at the end of the day are our 
aspiration and our hope is that this 

327
00:17:26,580 --> 00:17:28,170
technology that,
that is so important,

328
00:17:28,210 --> 00:17:33,210
the world in increasing safety and 
improving access to transportation and 

329
00:17:33,210 --> 00:17:35,870
improving efficiency in the utilization 
of our roadways and our cities.

330
00:17:36,260 --> 00:17:36,830
I mean,
I,

331
00:17:36,840 --> 00:17:41,840
I,
this is maybe the first doc I've ever 

332
00:17:41,840 --> 00:17:44,441
given where I didn't start by rattling 
off statistics about safety and all the 

333
00:17:44,441 --> 00:17:45,950
other things.
If you haven't heard them yet,

334
00:17:45,980 --> 00:17:48,050
you should look them up.
They're there stark,

335
00:17:48,200 --> 00:17:53,200
right?
The fact that most vehicles in the 

336
00:17:53,200 --> 00:17:56,361
United States today have an average on 
average three parking spaces as spaces 

337
00:17:56,641 --> 00:18:01,641
allocated to them.
The amount of land that's taken up 

338
00:18:01,641 --> 00:18:05,290
across the world in housing vehicles 
that are used less than five percent of 

339
00:18:05,881 --> 00:18:08,580
the time.
Uh,

340
00:18:08,790 --> 00:18:10,710
the number of people,
I think the United States,

341
00:18:10,720 --> 00:18:15,720
the estimate to spend somewhere between 
six and 15 million people don't have 

342
00:18:15,720 --> 00:18:19,671
access to the transportation they need 
either the because they're elderly or 

343
00:18:19,671 --> 00:18:22,911
disabled or one of many other factors,
and so this technology is potentially 

344
00:18:24,991 --> 00:18:27,990
one of the most impactful for our 
society in the coming years.

345
00:18:28,770 --> 00:18:33,770
It's a tremendously exciting 
technological challenge and the 

346
00:18:33,770 --> 00:18:38,180
confluence of those two things I think 
is a really unique opportunity for 

347
00:18:38,180 --> 00:18:41,871
engineers and others who are not 
engineers who really want to get 

348
00:18:41,871 --> 00:18:44,880
involved to play a role in changing our,
changing our world going forward.

349
00:18:46,200 --> 00:18:47,310
So with that,
maybe I'll.

350
00:18:47,311 --> 00:18:50,130
Maybe I'll stop with this and we can go 
to go to questions.

351
00:18:52,380 --> 00:18:52,890
It's good.

352
00:18:58,310 --> 00:18:59,530
Hello.
Thanks for coming on.

353
00:18:59,630 --> 00:19:04,630
The question.
A lot of a self driving car companies 

354
00:19:04,630 --> 00:19:07,650
are making extensive use of lidar but 
you don't see a lot of that with tesla.

355
00:19:07,710 --> 00:19:08,400
One of the.
No.

356
00:19:08,401 --> 00:19:10,420
If you had any thoughts about that.
The.

357
00:19:10,500 --> 00:19:13,740
I don't want to talk about tesla too 
much in terms of our specific.

358
00:19:13,900 --> 00:19:16,050
Any anything that wasn't public 
information,

359
00:19:16,051 --> 00:19:19,140
I'm not going to get into it.
I will say that for aurora,

360
00:19:19,890 --> 00:19:24,030
we believe that the right approach is 
getting to market quickly and you get to

361
00:19:24,031 --> 00:19:27,870
market and doing so safely and you get 
to market quickly and safely.

362
00:19:27,940 --> 00:19:30,630
You leveraged multiple that modalities 
including lighter.

363
00:19:31,890 --> 00:19:33,770
These are,
these are all just just to clarify what,

364
00:19:33,790 --> 00:19:37,550
what's running in the background.
These are all just aurora videos of um,

365
00:19:38,400 --> 00:19:40,370
our car cars driving on various test 
routes.

366
00:19:40,990 --> 00:19:43,370
Luke from the sloan school,
a lot of,

367
00:19:43,380 --> 00:19:46,210
so a lot of customers have visceral type
connections to their automobile.

368
00:19:46,530 --> 00:19:51,530
Um,
I was wondering how you see that market 

369
00:19:51,530 --> 00:19:52,300
to the car enthusiast market being 
affected by avs and then vice versa,

370
00:19:52,301 --> 00:19:53,250
how the,
uh,

371
00:19:53,350 --> 00:19:56,230
how the ads will be designed around 
those types of customers.

372
00:19:56,950 --> 00:19:59,640
And thanks for asking me.
If I am one of those enthusiasts,

373
00:20:00,540 --> 00:20:05,540
I very much appreciate being able to 
drive a car in certain settings.

374
00:20:08,640 --> 00:20:10,710
I very much don't appreciate driving it 
and others.

375
00:20:10,800 --> 00:20:11,790
Right?
Uh,

376
00:20:11,850 --> 00:20:16,080
I remember distinctly several evenings 
I,

377
00:20:16,250 --> 00:20:21,250
I almost literally pounding my steering 
wheel sitting in quogue and in boston 

378
00:20:21,250 --> 00:20:22,560
traffic,
you know,

379
00:20:23,320 --> 00:20:25,080
on my way to somewhere,
uh,

380
00:20:25,230 --> 00:20:30,230
I do the same in san francisco.
I think the opportunity really is to 

381
00:20:30,230 --> 00:20:34,821
turn that in turn sort of personal 
vehicle ownership and driving into more 

382
00:20:35,581 --> 00:20:37,470
of a sport and something you do for 
leisure.

383
00:20:38,890 --> 00:20:43,890
I see it a gentleman sometime I go,
uh,

384
00:20:44,190 --> 00:20:46,020
asked me to talk.
Hey,

385
00:20:46,230 --> 00:20:50,830
don't you think this is a problem for 
the country?

386
00:20:50,890 --> 00:20:55,030
I think you meant the world.
If people don't learn how to drive,

387
00:20:55,031 --> 00:20:56,830
that's just something a human should 
know how to do.

388
00:20:57,370 --> 00:21:02,370
A,
my perspective is it's as much of a 

389
00:21:02,370 --> 00:21:04,750
problem as people not intrinsically 
knowing how to ride a horse today.

390
00:21:05,110 --> 00:21:07,240
If you want to know how to ride a horse,
go ride a horse.

391
00:21:07,810 --> 00:21:09,160
If you want to,
you want to race a car,

392
00:21:09,170 --> 00:21:14,170
go to a race track or go out to a 
mountain road that's been allocated for 

393
00:21:14,170 --> 00:21:14,330
it.
Um,

394
00:21:14,410 --> 00:21:19,410
ultimately I think,
I think there is an important place for 

395
00:21:19,410 --> 00:21:21,871
that because I certainly agree with you.
I'm very much a vehicle enthusiast 

396
00:21:21,871 --> 00:21:22,120
myself.
Um,

397
00:21:22,600 --> 00:21:27,600
but I think there is so much opportunity
here in alleviating some of these other 

398
00:21:28,331 --> 00:21:33,331
problems,
particularly in places where it's not 

399
00:21:33,331 --> 00:21:33,331
fun to drive that.
I think there's a place for both.

400
00:21:33,890 --> 00:21:37,760
Yeah,
right.

401
00:21:38,700 --> 00:21:39,940
You need to get

402
00:21:41,540 --> 00:21:44,540
congratulations on the partnership was 
announced recently,

403
00:21:44,541 --> 00:21:45,430
I think.
Um,

404
00:21:45,860 --> 00:21:48,260
so I have a two part question.
The first one is,

405
00:21:48,261 --> 00:21:51,000
um,
so we heard last week from a,

406
00:21:51,280 --> 00:21:56,280
I think there was a gentleman from bimo 
talking about how long they've been 

407
00:21:56,280 --> 00:21:59,351
working on this autonomous car 
technology and you seem to have ramped 

408
00:21:59,351 --> 00:22:01,130
up extremely fast.
So,

409
00:22:01,640 --> 00:22:04,400
uh,
is there a licensing model that you have

410
00:22:04,401 --> 00:22:05,090
taken out?
I mean,

411
00:22:05,091 --> 00:22:09,290
how were you able to commercialize the 
technology in one year?

412
00:22:11,160 --> 00:22:16,160
Just to be clear,
we're not actually commercializing 

413
00:22:16,160 --> 00:22:20,790
distinguish,
we are partnering and developing 

414
00:22:20,790 --> 00:22:23,490
vehicles and will ultimately be running 
pilots as we announced a week or two ago

415
00:22:24,120 --> 00:22:27,150
with the shuttles we are.
However,

416
00:22:27,151 --> 00:22:32,151
I will distinguish that from ron 
commercialization of the technology and 

417
00:22:32,151 --> 00:22:35,200
I don't want to get too much into,
you know,

418
00:22:35,220 --> 00:22:39,660
the nuances of that business model.
I will say that it is,

419
00:22:39,730 --> 00:22:43,800
is one that's done in very close 
partnership with our automotive partners

420
00:22:45,620 --> 00:22:48,170
because at the end of the day they 
understand their cars,

421
00:22:48,171 --> 00:22:51,290
they understand their customers,
they have distribution networks.

422
00:22:51,340 --> 00:22:53,170
Um,
they are,

423
00:22:53,310 --> 00:22:56,000
our automotive partners are fairly well 
positioned,

424
00:22:56,560 --> 00:23:01,560
uh,
it provided they have the right support 

425
00:23:01,560 --> 00:23:01,560
in developing the self driving 
technology,

426
00:23:01,560 --> 00:23:02,210
the fairly,
fairly well positioned to,

427
00:23:02,560 --> 00:23:03,420
uh,
you know,

428
00:23:03,470 --> 00:23:04,580
roll it out at the scale.

429
00:23:05,820 --> 00:23:08,060
So the second month,
my question is again,

430
00:23:08,070 --> 00:23:11,400
looking at the you pace of adoption and 
the maturity of technology,

431
00:23:12,450 --> 00:23:17,450
do you see like an opensource model for 
autonomous cars as they become more and 

432
00:23:17,731 --> 00:23:17,940
more

433
00:23:19,550 --> 00:23:22,170
unclear?
I,

434
00:23:22,410 --> 00:23:27,410
I'm not convinced that an open source 
model is what gets to market most 

435
00:23:27,410 --> 00:23:28,760
quickly.
Um,

436
00:23:29,480 --> 00:23:31,280
in the long run,
I,

437
00:23:31,281 --> 00:23:32,160
I,
it's,

438
00:23:32,161 --> 00:23:34,250
it's not clear to me,
uh,

439
00:23:34,280 --> 00:23:39,280
what will happen.
I think there will be a handful of 

440
00:23:39,280 --> 00:23:41,590
successful self driving stacks that will
make it a nowhere near the number of 

441
00:23:43,330 --> 00:23:45,240
self driving companies today.
Um,

442
00:23:45,380 --> 00:23:46,970
but a handful,
I think

443
00:23:50,960 --> 00:23:55,960
two questions.
One is in invariably new product 

444
00:23:55,960 --> 00:23:59,201
development,
there's typically two types of 

445
00:23:59,201 --> 00:24:01,330
bottlenecks.
There's a technological bottleneck and 

446
00:24:01,330 --> 00:24:01,330
an economic bottleneck,
right?

447
00:24:01,330 --> 00:24:04,070
So technological bottleneck might be,
hey,

448
00:24:04,071 --> 00:24:05,840
you know,
the sensors aren't good enough,

449
00:24:05,870 --> 00:24:08,510
or the machine learning algorithms 
aren't good enough and so on.

450
00:24:08,511 --> 00:24:11,600
I'd be interested to hear and it'll 
shift obviously over time,

451
00:24:11,930 --> 00:24:16,930
so I'd be interested to know what you 
would say is the current thing that if 

452
00:24:16,930 --> 00:24:16,930
hey,
if,

453
00:24:16,930 --> 00:24:19,190
if this part of the,
of the architecture was 10 times better,

454
00:24:19,191 --> 00:24:21,260
we would.
And then on the economic side,

455
00:24:21,261 --> 00:24:22,500
I'd be interested to know,
you know,

456
00:24:22,590 --> 00:24:26,210
jeff,
if sensors were 100 times cheaper than,

457
00:24:26,240 --> 00:24:27,740
so it'd be interesting to hear your 
perspective.

458
00:24:28,270 --> 00:24:33,270
That's a great question.
Let me start with the economic side of 

459
00:24:33,270 --> 00:24:37,591
it.
And just to get that at the wake is a 

460
00:24:37,591 --> 00:24:37,591
little bit quicker.
Answer.

461
00:24:37,591 --> 00:24:42,250
The economics of operating a self 
driving vehicle in a shared network 

462
00:24:42,250 --> 00:24:47,011
today would close that,
that business case closes even with high

463
00:24:47,231 --> 00:24:48,160
costs.
So sensors,

464
00:24:48,470 --> 00:24:49,720
um,
that is not,

465
00:24:49,750 --> 00:24:54,750
that is not what's stopping us.
And that's part of why the gentleman 

466
00:24:54,750 --> 00:24:55,900
earlier who asked,
you know,

467
00:24:56,290 --> 00:25:01,290
should use lidar or not.
If your target is to initially deploy 

468
00:25:01,290 --> 00:25:06,271
these in fleets,
you would be wise to start at the top 

469
00:25:06,271 --> 00:25:09,991
end of the market,
develop and deploy a system that's as 

470
00:25:09,991 --> 00:25:10,450
capable as possible,
as quickly as possible,

471
00:25:10,780 --> 00:25:15,780
and then costs it down over time.
And you can do that as computer vision 

472
00:25:15,780 --> 00:25:17,650
with precision recall increase today.
They're not good enough,

473
00:25:17,740 --> 00:25:20,590
right?
And so,

474
00:25:20,700 --> 00:25:25,700
so economically,
depending on your model of going to 

475
00:25:25,700 --> 00:25:28,631
market,
and we believe that the right model is 

476
00:25:28,631 --> 00:25:30,290
through a,
you know,

477
00:25:30,291 --> 00:25:31,690
on the mobility services,

478
00:25:33,960 --> 00:25:36,320
you can cost out your costs down the 
center.

479
00:25:36,380 --> 00:25:37,530
Inevitably,
you know,

480
00:25:37,531 --> 00:25:39,990
there's no unobtainium in lidar units 
today.

481
00:25:40,020 --> 00:25:45,020
There's no reason fundamentally that he 
should cost of lidar unit will lead you 

482
00:25:45,020 --> 00:25:45,540
to a $70,000
price point,

483
00:25:45,890 --> 00:25:46,890
right?
Um,

484
00:25:46,950 --> 00:25:51,950
however,
if you build anything in low enough 

485
00:25:51,950 --> 00:25:52,500
volumes is going to be expensive.
Many of these things will work their way

486
00:25:52,501 --> 00:25:57,501
into the standard automotive process.
They'll work their way into tier one 

487
00:25:57,501 --> 00:26:01,431
suppliers and when they do,
the automotive community has shown 

488
00:26:01,431 --> 00:26:02,850
themselves to be exceptional driving 
those costs down.

489
00:26:02,851 --> 00:26:06,510
And so I expect them to come way down to
your other question,

490
00:26:06,590 --> 00:26:08,640
a technological bottlenecks and 
challenges.

491
00:26:09,800 --> 00:26:14,800
One of the key challenges of self 
driving is and remains that of 

492
00:26:14,800 --> 00:26:19,390
forecasting the intent and behave and 
future behaviors of other actors both in

493
00:26:21,041 --> 00:26:26,041
response to one another,
but also in response to your own 

494
00:26:26,041 --> 00:26:28,000
decisions in motion.
That's a perception problem,

495
00:26:28,390 --> 00:26:30,310
but it's something more than a 
perception problem.

496
00:26:30,311 --> 00:26:34,790
It's also a prediction and you know,
there,

497
00:26:34,791 --> 00:26:39,791
there are a number of different things 
that come together to have that have to 

498
00:26:39,791 --> 00:26:43,981
come together to solve this.
We're excited about some of the that 

499
00:26:43,981 --> 00:26:48,441
we're using and interleaving various,
a modern machine learning techniques 

500
00:26:48,441 --> 00:26:53,300
throughout the system to do things like 
project our own behaviors that were 

501
00:26:53,911 --> 00:26:58,911
learned for the ego vehicle on others 
and assume that they'll behave as we 

502
00:26:58,911 --> 00:27:00,930
would had we been in that situation.
Like an expert system kind of approach,

503
00:27:00,931 --> 00:27:01,580
right?
Yeah.

504
00:27:01,980 --> 00:27:02,640
Yeah.
Uh,

505
00:27:02,641 --> 00:27:07,641
you,
you assume nominal behavior and you 

506
00:27:07,641 --> 00:27:07,641
guard against all phenomenal,
right?

507
00:27:07,641 --> 00:27:07,940
Um,
but it's,

508
00:27:07,941 --> 00:27:09,990
it's very much a,
it's not a solved problem.

509
00:27:09,991 --> 00:27:14,991
I wouldn't say it's very much as you get
into that really long tail development 

510
00:27:16,790 --> 00:27:20,780
when you're no longer putting out 
demonstration videos,

511
00:27:20,781 --> 00:27:25,781
but you're,
instead of just putting your head down 

512
00:27:25,781 --> 00:27:25,781
and seeking out those fine online's 
yeah.

513
00:27:25,781 --> 00:27:27,590
That's the kind of problem you tend to 
deal with.

514
00:27:28,610 --> 00:27:29,490
Yeah.

515
00:27:31,530 --> 00:27:34,550
Um,
so this question isn't necessarily about

516
00:27:34,551 --> 00:27:38,660
the development of a self driving cars,
but more of like an ethics question.

517
00:27:39,040 --> 00:27:44,040
Um,
when you're putting a human lives into 

518
00:27:44,040 --> 00:27:46,480
the hands of software,
isn't there always the possibility for 

519
00:27:46,480 --> 00:27:50,110
like outside agents with malicious 
intent to use it for their own gain and 

520
00:27:50,110 --> 00:27:52,460
how do you guys,
if you do have a plan,

521
00:27:52,461 --> 00:27:56,190
how do you intend to protect against 
that?

522
00:27:57,330 --> 00:28:02,330
So security is a very real aspect of 
this that has to be solved.

523
00:28:07,070 --> 00:28:12,070
It's a constant game of cat and mouse 
and so I think it just requires a very 

524
00:28:12,621 --> 00:28:15,470
good team and a concerted effort over 
time.

525
00:28:16,220 --> 00:28:16,700
Um,
I,

526
00:28:16,701 --> 00:28:21,701
I don't think,
I don't think you solve it once and I 

527
00:28:21,701 --> 00:28:24,370
certainly wouldn't pretend to have a 
plan that solves it and has done with 

528
00:28:24,370 --> 00:28:24,370
it.
Um,

529
00:28:24,490 --> 00:28:25,280
we're,
we're,

530
00:28:25,310 --> 00:28:30,310
we,
we try to leverage best practices where 

531
00:28:30,310 --> 00:28:32,291
we can in the fundamental architecture 
of the system to make it less exposed 

532
00:28:32,330 --> 00:28:37,330
and in particular,
key parts of the system was exposed to 

533
00:28:37,330 --> 00:28:38,960
nefarious actions of others.
But at the end of the day,

534
00:28:38,961 --> 00:28:41,510
it's just a constant,
um,

535
00:28:41,610 --> 00:28:43,010
it's a constant development effort.

536
00:28:46,060 --> 00:28:47,270
Thanks for being here.
Um,

537
00:28:47,590 --> 00:28:52,590
so I had a question about what 
opportunities self driving cars open up 

538
00:28:52,590 --> 00:28:56,940
since driving has been designed around a
human being at the center since the 

539
00:28:56,940 --> 00:28:58,780
beginning.
If you put a computer at the center,

540
00:28:59,470 --> 00:29:00,610
what,
you know,

541
00:29:00,670 --> 00:29:05,670
a society wide differences and maybe 
even within individual car differences 

542
00:29:05,670 --> 00:29:09,661
that open up like cars go 150 miles an 
hour on the highway and get places much 

543
00:29:09,661 --> 00:29:10,860
faster with cars,
be like,

544
00:29:11,080 --> 00:29:16,080
look differently when a human doesn't 
need to be paying attention and stuff 

545
00:29:16,080 --> 00:29:16,080
like that.

546
00:29:16,080 --> 00:29:17,700
I think the answer is yes.
The,

547
00:29:17,780 --> 00:29:18,580
and,
and that's,

548
00:29:18,610 --> 00:29:19,960
that's something that's very exciting,
right?

549
00:29:19,961 --> 00:29:24,961
So one of the,
I think one of the unique opportunities 

550
00:29:24,961 --> 00:29:29,821
that automakers in particular have when 
self driving technology gets 

551
00:29:29,821 --> 00:29:31,820
incorporated into the vehicles is they 
can do things like play with,

552
00:29:31,900 --> 00:29:35,170
like differentiate the user experience.
They can provide services,

553
00:29:35,560 --> 00:29:36,490
um,
you know,

554
00:29:37,010 --> 00:29:40,070
uh,
augmented reality services or you know,

555
00:29:40,240 --> 00:29:42,640
a location resilient and many other sort
of.

556
00:29:42,880 --> 00:29:47,880
It opens a new window into an entirely 
new market that automakers haven't 

557
00:29:47,880 --> 00:29:49,870
historically played in.
Um,

558
00:29:49,900 --> 00:29:53,190
and it allows them to change the,
the,

559
00:29:53,290 --> 00:29:56,140
the vehicles themselves,
as you've mentioned,

560
00:29:56,230 --> 00:30:01,230
the interior.
Can change as we validate some of these 

561
00:30:02,051 --> 00:30:06,220
self driving systems and confirm that 
they do in fact reduce the collision,

562
00:30:06,230 --> 00:30:08,380
the rate of collisions as we hope they 
will.

563
00:30:08,850 --> 00:30:11,740
Um,
you can start to pull out a lot of the,

564
00:30:12,000 --> 00:30:13,930
um,
extra,

565
00:30:14,390 --> 00:30:19,390
you know,
mass and other things that we've added 

566
00:30:19,390 --> 00:30:19,390
to vehicles to make them more passively 
safe.

567
00:30:19,390 --> 00:30:20,200
Right?
Roll cages,

568
00:30:20,201 --> 00:30:21,750
crumple zones,
airbags,

569
00:30:22,060 --> 00:30:23,500
you know,
a lot of these things.

570
00:30:23,720 --> 00:30:25,150
Um,
you know,

571
00:30:25,270 --> 00:30:28,060
presumably in a world where we don't 
crash,

572
00:30:28,600 --> 00:30:33,600
there is,
there is much less need for passive 

573
00:30:33,600 --> 00:30:33,910
safety system.
So yes.

574
00:30:36,260 --> 00:30:41,260
I have a question about the gullah nogo.
A desk that you conduct for certain 

575
00:30:41,260 --> 00:30:42,080
features.
Like you mentioned,

576
00:30:42,081 --> 00:30:44,180
the throttle control,
where your slow down the throttle.

577
00:30:45,380 --> 00:30:47,930
Assuming that the driver has pressed the
wrong wrong pedal.

578
00:30:48,530 --> 00:30:51,020
When you test,
when you decided to launch that feature,

579
00:30:51,021 --> 00:30:56,021
how do you know it's definitely going to
work in all scenarios because your data 

580
00:30:56,021 --> 00:30:56,021
set might not be censored.

581
00:30:56,021 --> 00:31:00,750
It's a,
it's a statistical evaluation every 

582
00:31:00,750 --> 00:31:00,750
case,
right?

583
00:31:00,750 --> 00:31:01,250
You're right there,
you will.

584
00:31:01,730 --> 00:31:06,730
This is,
this is part of the art of self driving 

585
00:31:06,730 --> 00:31:09,221
vehicle development is you will never 
have comprehensively captured every 

586
00:31:09,741 --> 00:31:13,100
case,
every scenario that is as uh,

587
00:31:14,800 --> 00:31:15,440
my,
my,

588
00:31:15,470 --> 00:31:16,250
my,
uh,

589
00:31:16,340 --> 00:31:18,200
some of you may want to correct me on 
this.

590
00:31:18,201 --> 00:31:21,710
I think that's an unbounded set in fact,
have factory banner at some point,

591
00:31:21,711 --> 00:31:23,240
but I think it's on.
Um,

592
00:31:23,590 --> 00:31:26,090
and so you'll,
you'll never actually have characterized

593
00:31:26,091 --> 00:31:27,890
everything.
What you will have done,

594
00:31:28,100 --> 00:31:33,100
hopefully if you do it right,
is you will have established with a 

595
00:31:33,100 --> 00:31:37,241
reasonable degree of confidence that you
can perform at a level of safety that's 

596
00:31:37,241 --> 00:31:41,020
better than the average human driver.
And once you've reached that threshold 

597
00:31:41,020 --> 00:31:41,020
and you're confident that you have 
reached that threshold,

598
00:31:41,570 --> 00:31:45,020
I think the,
the opportunity to launch is,

599
00:31:45,170 --> 00:31:47,720
is real,
and you should seriously consider it.

600
00:31:49,840 --> 00:31:52,420
So thank you for your talk today.
First a,

601
00:31:52,421 --> 00:31:57,421
and my question is self driving seems to
be able to ultimately take over their 

602
00:31:57,431 --> 00:32:02,431
world to some extent,
but just like other technologists today 

603
00:32:02,830 --> 00:32:06,670
that open up new opportunities but also 
bring in adverse effects.

604
00:32:07,340 --> 00:32:12,340
Um,
so how do you respond to fear and 

605
00:32:12,340 --> 00:32:15,541
negative effects that may come in one 
day and specifically what do you see as 

606
00:32:15,541 --> 00:32:18,640
the positive and negative implications 
of future day self driving

607
00:32:19,710 --> 00:32:24,630
positive and negative implications?
So the positive ones,

608
00:32:24,631 --> 00:32:29,631
I kind of listen and go find your 
favorite press article on note list them

609
00:32:29,701 --> 00:32:34,620
as well.
The negative ones in the near term.

610
00:32:34,920 --> 00:32:36,090
Uh,
I do.

611
00:32:36,390 --> 00:32:40,910
I do worry a little bit about the 
displacement of jobs,

612
00:32:41,290 --> 00:32:46,290
not a little bit this will happen.
It happens with every technology like 

613
00:32:46,290 --> 00:32:50,021
this.
I think it's incumbent on us to find a 

614
00:32:50,021 --> 00:32:55,010
good way of transitioning those who are 
employed in some of the transportation 

615
00:32:55,010 --> 00:32:57,990
sectors that will be affected into a 
better work.

616
00:32:58,340 --> 00:32:59,150
Right.
Um,

617
00:32:59,880 --> 00:33:03,990
there are a few opportunities that are 
interesting in that regard,

618
00:33:04,230 --> 00:33:09,230
but I think it's an important thing to 
start discussing now because it's going 

619
00:33:09,230 --> 00:33:09,230
to take,
you know,

620
00:33:09,230 --> 00:33:14,211
a few years and you know,
by the time we got these self driving 

621
00:33:14,211 --> 00:33:16,350
systems on the roads really starting to 
place that labor,

622
00:33:16,351 --> 00:33:18,270
I'd really like to have a new home for 
it.

623
00:33:19,280 --> 00:33:20,140
Hi.
Uh,

624
00:33:20,690 --> 00:33:22,780
I'm kasha from the sloan school.
Uh,

625
00:33:22,940 --> 00:33:25,130
my question was more about your business
model,

626
00:33:25,190 --> 00:33:30,190
again,
with partnering with both vw and he and 

627
00:33:30,190 --> 00:33:32,930
day and you just perspective on how you 
were able to effectively do that.

628
00:33:33,600 --> 00:33:36,560
Did not one of them want to go sort of 
exclusive with you.

629
00:33:37,490 --> 00:33:39,920
And what was your sort of thought 
process about that?

630
00:33:40,880 --> 00:33:41,600
So,
our,

631
00:33:41,660 --> 00:33:43,160
our mission,
as I mentioned,

632
00:33:43,720 --> 00:33:47,660
is to get the technology to market 
broadly and quickly and safely.

633
00:33:48,030 --> 00:33:50,450
Uh,
we are,

634
00:33:50,660 --> 00:33:55,660
you know,
haven't been and remain convinced that 

635
00:33:55,660 --> 00:33:55,820
the right way to do that is by providing
it to,

636
00:33:55,821 --> 00:34:00,821
as much of the industry as possible to 
every automaker who shares our vision 

637
00:34:01,041 --> 00:34:02,300
and our approach.
Um,

638
00:34:02,330 --> 00:34:05,630
we were pleased to see that both 
volkswagen group,

639
00:34:06,120 --> 00:34:11,120
uh,
and I'm assuming you all know the scope 

640
00:34:11,120 --> 00:34:11,120
of volkswagen,
right?

641
00:34:11,120 --> 00:34:14,700
This is a massive automaker hyundai 
motor,

642
00:34:14,701 --> 00:34:18,090
also very large cross honda,
ikea and genesis.

643
00:34:19,080 --> 00:34:22,410
They both shared our vision of how we 
should do this,

644
00:34:22,740 --> 00:34:24,660
which was important to us.
Um,

645
00:34:24,690 --> 00:34:26,420
they both shared,
you know,

646
00:34:26,610 --> 00:34:31,610
a,
a keen interest in making a difference 

647
00:34:32,070 --> 00:34:37,070
at scale through their platforms.
A volkswagen has a very admirable set of

648
00:34:37,441 --> 00:34:39,460
initiatives around electric vehicle 
electrification.

649
00:34:39,461 --> 00:34:41,940
And few other things,
honda is doing similar things.

650
00:34:42,350 --> 00:34:47,350
Um,
and so for us it was important that we 

651
00:34:47,350 --> 00:34:49,170
enable everyone and that was kind of 
what aurora was started to do.

652
00:34:49,830 --> 00:34:52,980
Hi,
I had a question now that I see a lot of

653
00:34:53,280 --> 00:34:55,020
companies are coming up with self 
driving cars,

654
00:34:55,080 --> 00:34:58,110
right?
So most of the costs are pretty much all

655
00:34:58,111 --> 00:35:03,111
the technology is bound only to the car.
So would we see something like an open 

656
00:35:03,111 --> 00:35:07,281
network where car communicate with each 
other regardless of which company they 

657
00:35:07,281 --> 00:35:08,640
come from and what this in any way,
uh,

658
00:35:08,641 --> 00:35:13,641
you know,
increase the safety or the performance 

659
00:35:13,641 --> 00:35:13,641
of vehicles and stuff like that.

660
00:35:13,641 --> 00:35:17,200
I think.
I think you're getting a vehicle to 

661
00:35:17,200 --> 00:35:19,030
vehicle vehicle to infrastructure 
communication efforts ongoing and that,

662
00:35:19,080 --> 00:35:20,210
uh,
and it's certainly,

663
00:35:20,211 --> 00:35:22,010
it's,
it's only positive,

664
00:35:22,011 --> 00:35:22,850
right?
The,

665
00:35:22,930 --> 00:35:27,560
it having that information available to 
you can only make things better.

666
00:35:28,070 --> 00:35:33,070
The challenge who's historically 
bandwidth vehicle to vehicle and break 

667
00:35:33,070 --> 00:35:33,680
your particular vehicle to 
infrastructure or vice versa,

668
00:35:34,050 --> 00:35:36,440
um,
that it doesn't scale well,

669
00:35:36,680 --> 00:35:38,910
one and two,
it's been slow.

670
00:35:38,970 --> 00:35:40,770
It's been much slower and coming in our 
development.

671
00:35:41,400 --> 00:35:46,400
And so when we develop these systems,
we develop them without the expectation 

672
00:35:46,681 --> 00:35:51,681
that those,
that those communication protocol are 

673
00:35:51,681 --> 00:35:54,621
available to us,
we'll certainly protect for them and it 

674
00:35:54,621 --> 00:35:54,621
will certainly be,
you know,

675
00:35:54,621 --> 00:35:56,550
a benefit once they're,
once they're here,

676
00:35:56,910 --> 00:36:01,910
but until then,
many of the hard problems that I would 

677
00:36:01,910 --> 00:36:04,731
have welcomed 10 years ago to have a 
beacon on every traffic light that just 

678
00:36:04,731 --> 00:36:06,120
told me it's state rather than having to
perceive it.

679
00:36:06,510 --> 00:36:11,510
Uh,
I would have certainly use those 10 

680
00:36:11,510 --> 00:36:13,401
years ago.
Now they're less significant because 

681
00:36:13,401 --> 00:36:17,061
we've kind of worked our way through a 
lot of the problems they would have 

682
00:36:17,061 --> 00:36:17,061
solved.

683
00:36:17,061 --> 00:36:17,870
Sinky food talk.
My question is,

684
00:36:18,290 --> 00:36:23,290
what's your opinion about cooperation of
self driving vr curse?

685
00:36:23,570 --> 00:36:28,570
So maybe I think if you can control a 
group of self driving because at the 

686
00:36:28,570 --> 00:36:31,280
same time you can achieve a lot of 
benefits to the traffic.

687
00:36:31,430 --> 00:36:33,140
Yes.
That is where one of the,

688
00:36:33,370 --> 00:36:36,230
that is where a lot of the benefits come
from and infrastructure utilization,

689
00:36:36,231 --> 00:36:41,231
right,
is in ride sharing with autonomous 

690
00:36:41,231 --> 00:36:41,440
vehicles.
And specifically,

691
00:36:42,090 --> 00:36:47,090
you know,
the better we understand demand 

692
00:36:47,090 --> 00:36:47,900
patterns,
people movement goods movement,

693
00:36:48,380 --> 00:36:53,380
the better we can sort of optimally 
allocate these vehicles at locations 

694
00:36:53,871 --> 00:36:55,400
where they're needed.
So yes,

695
00:36:55,401 --> 00:36:57,560
that's certainly that,
that coordination,

696
00:36:57,890 --> 00:36:59,000
this is where,
as I mentioned,

697
00:36:59,001 --> 00:37:01,550
these three vectors of vehicle 
electrification,

698
00:37:01,850 --> 00:37:04,470
ride sharing and autonomy or,
or transparent,

699
00:37:04,480 --> 00:37:09,480
you know,
mobility as a service and autonomy 

700
00:37:09,480 --> 00:37:09,620
really come together with a unique value
proposition.

701
00:37:10,300 --> 00:37:11,120
Yeah.
Okay.

702
00:37:11,121 --> 00:37:12,150
Thank you.
Yeah.

703
00:37:13,200 --> 00:37:17,540
Thank you so much for a great talk.
Thank you.

