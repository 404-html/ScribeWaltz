1
00:00:00,390 --> 00:00:05,390
Welcome to mit course six zero nine,
nine artificial general intelligence.

2
00:00:06,900 --> 00:00:11,900
Today we have ray Kurzweil.
He is one of the world's leading 

3
00:00:11,900 --> 00:00:16,401
inventors,
thinkers and futurists with a 30 year 

4
00:00:16,401 --> 00:00:19,341
track record of accurate predictions 
called the restless genius by the Wall 

5
00:00:19,341 --> 00:00:22,080
Street Journal and the ultimate thinking
machine by Forbes magazine.

6
00:00:22,890 --> 00:00:26,910
He was selected as one of the top 
entrepreneurs by ink magazine,

7
00:00:27,060 --> 00:00:30,630
which described him as the rightful heir
to Thomas Edison.

8
00:00:31,470 --> 00:00:35,220
PBS selected him as one of the 16 
revolutionaries who made America.

9
00:00:36,810 --> 00:00:40,980
Ray was the principal investigator of 
the first CCD flatbed scanner,

10
00:00:41,010 --> 00:00:44,250
the first Omni font optical character 
recognition,

11
00:00:44,251 --> 00:00:46,950
the first point to speech reading 
machine for the blind,

12
00:00:46,951 --> 00:00:51,951
the first text to speech synthesizer,
the first music synthesizer capable of 

13
00:00:51,951 --> 00:00:54,930
recreating the grand piano and other 
orchestra instruments,

14
00:00:55,020 --> 00:00:58,650
and the first commercially marketed 
large vocabulary speech recognition.

15
00:00:59,340 --> 00:01:04,340
Among his many honors,
he received a Grammy Award for 

16
00:01:04,340 --> 00:01:04,770
outstanding achievements in music 
technology.

17
00:01:04,980 --> 00:01:07,560
He's the recipient of the National Medal
of Technology,

18
00:01:07,650 --> 00:01:10,410
was inducted into the national inventors
hall of fame,

19
00:01:10,860 --> 00:01:15,860
holds 21 honorary doctorates,
and honors from three US presidents.

20
00:01:16,890 --> 00:01:21,890
Ray has written five national 
bestselling books including the New York

21
00:01:22,440 --> 00:01:27,440
Times bestsellers.
The singularity is near from 2005 and 

22
00:01:27,440 --> 00:01:30,981
how to create a mind from 2012.
He's Co founder and Chancellor of 

23
00:01:31,381 --> 00:01:35,400
Singularity University and a director of
engineering at Google.

24
00:01:36,210 --> 00:01:41,210
Heading up a team developing machine 
intelligence and natural language 

25
00:01:41,210 --> 00:01:42,180
understanding.
Please give a warm welcome.

26
00:01:49,840 --> 00:01:54,840
It's good to be back.
I've been in this lecture hall many 

27
00:01:54,840 --> 00:01:59,431
times and walked the infinite Carter.
I came here as an undergraduate in 1965.

28
00:02:03,400 --> 00:02:08,050
Within a year of my being here,
they started a new major called computer

29
00:02:08,051 --> 00:02:12,310
science.
It did not get its own course number.

30
00:02:12,530 --> 00:02:17,530
It's six one.
Even biotechnology recently got its own 

31
00:02:17,530 --> 00:02:21,100
course number,
but how many of you are cs majors?

32
00:02:22,570 --> 00:02:27,570
Okay.
How many of you do work in deep 

33
00:02:27,570 --> 00:02:29,911
learning?
How many of you have heard of deep 

34
00:02:29,911 --> 00:02:33,781
learning?
I came here first in 1962 when I was 14.

35
00:02:38,310 --> 00:02:41,520
I became excited about artificial 
intelligence.

36
00:02:42,000 --> 00:02:45,450
It it had only gotten its name six years
earlier,

37
00:02:45,451 --> 00:02:50,451
the 1956 dartmouth conference by Marvin 
Minsky and John Mccarthy,

38
00:02:52,560 --> 00:02:55,860
so I wrote a Minsky,
a letter.

39
00:02:56,390 --> 00:03:00,490
There was no email back then and he 
invited me up.

40
00:03:00,491 --> 00:03:03,040
He spent all day with me,
is if he had nothing else to do,

41
00:03:03,070 --> 00:03:05,530
he was the consummate educator.

42
00:03:07,240 --> 00:03:08,920
I then,
and uh,

43
00:03:09,630 --> 00:03:13,000
the AI field had already bifurcated into
two warring camps,

44
00:03:14,250 --> 00:03:19,090
the symbolic school which Minsky was 
associated with a.

45
00:03:19,091 --> 00:03:21,180
and the connectionist school,
uh,

46
00:03:21,540 --> 00:03:23,020
was not widely known.
In fact,

47
00:03:23,021 --> 00:03:28,021
I think it's still not widely known that
minsky actually invented the neural net 

48
00:03:28,021 --> 00:03:31,240
in 1953,
but he had become negative about.

49
00:03:31,250 --> 00:03:36,250
It's largely because there's a lot of 
hype that these giant brains could solve

50
00:03:36,581 --> 00:03:41,581
any problem.
So the first popular neural nets,

51
00:03:42,431 --> 00:03:47,431
the perceptron was being promulgated by 
Frank Rosenblatt at Cornell.

52
00:03:49,270 --> 00:03:50,860
So minsky says,
where are you going now?

53
00:03:50,861 --> 00:03:55,861
And saying,
I said to c Rosenblatt at Cornell is 

54
00:03:55,861 --> 00:03:56,200
that,
don't bother doing that.

55
00:03:56,920 --> 00:04:01,920
And I went there and Rosenblatt was 
touting the perceptron that had 

56
00:04:02,231 --> 00:04:04,720
ultimately would be able to solve any 
problem.

57
00:04:05,860 --> 00:04:10,860
So I brought some printed letters that 
had the camera and it did a perfect job 

58
00:04:10,860 --> 00:04:14,191
of recognizing them as long as they were
carrier 10 different type style.

59
00:04:15,160 --> 00:04:17,230
Didn't work at all.
And he said,

60
00:04:17,231 --> 00:04:20,530
but don't worry,
we can take the output of the perceptron

61
00:04:20,531 --> 00:04:25,531
undefeated as the input to another 
perceptron and take the output of that 

62
00:04:25,531 --> 00:04:26,680
and feed it to a third layer,
and as we add more layers,

63
00:04:26,681 --> 00:04:28,840
it'll get smarter and smarter and 
generalize.

64
00:04:29,500 --> 00:04:31,900
And so that's interesting.
Have you tried that?

65
00:04:31,901 --> 00:04:32,560
Well,
no,

66
00:04:32,561 --> 00:04:37,561
but it's high in our research agenda.
Things did not move quite as quickly 

67
00:04:38,170 --> 00:04:41,020
back then as they do now.
He died nine years later.

68
00:04:41,110 --> 00:04:45,970
Never having tried that idea turns out 
to be remarkably prescient.

69
00:04:45,971 --> 00:04:50,971
I mean he never tried multilayer neural 
nets and all the excitement that we've 

70
00:04:50,971 --> 00:04:55,741
seen now about deep learning comes from 
a combination of two things,

71
00:05:00,190 --> 00:05:04,450
both many layer neural nets and the law 
of accelerating returns,

72
00:05:04,451 --> 00:05:09,451
which I'll get to a little bit later,
which is basically the exponential 

73
00:05:09,451 --> 00:05:13,801
growth of computing so that we can run 
these massive nets and handle massive 

74
00:05:14,021 --> 00:05:14,980
amounts of data.

75
00:05:17,530 --> 00:05:22,530
It would be decades before that idea was
tried several decades later,

76
00:05:23,091 --> 00:05:25,930
the three level neural nets were tried.
There were a little bit better.

77
00:05:25,931 --> 00:05:30,250
They could deal with multiple types.
Styles still weren't very flexible.

78
00:05:31,250 --> 00:05:35,530
That's not hard to add other layers.
It's a very straightforward concept.

79
00:05:35,970 --> 00:05:37,600
Uh,
there was a math problem,

80
00:05:38,170 --> 00:05:43,170
a,
the disappearing gradient or the 

81
00:05:43,170 --> 00:05:45,331
exploding gradient,
which I'm sure many of you are familiar 

82
00:05:45,331 --> 00:05:45,760
with.
Basically,

83
00:05:45,761 --> 00:05:50,761
you need to take maximum advantage of 
the range of values,

84
00:05:53,350 --> 00:05:55,390
uh,
in the gradients,

85
00:05:55,810 --> 00:05:58,070
uh,
not let them disappear.

86
00:05:58,071 --> 00:06:01,120
And a loser resolution,
a,

87
00:06:01,130 --> 00:06:04,310
that's a fairly straightforward 
mathematical transformation.

88
00:06:04,760 --> 00:06:08,420
With that insight,
we could now go to 100 layer neural nets

89
00:06:09,450 --> 00:06:14,450
and s that's behind sort of all the 
fantastic gains that we've seen 

90
00:06:15,201 --> 00:06:18,830
recently.
A alpha go,

91
00:06:18,831 --> 00:06:23,831
a trained on every online game and then 
became a fair go player and then trained

92
00:06:28,701 --> 00:06:33,701
itself by playing itself and soared past
the best human alphago.

93
00:06:33,860 --> 00:06:38,860
Zero started with no human input at all,
within hours of iteration,

94
00:06:40,610 --> 00:06:43,440
soared past Alphago.
Uh,

95
00:06:43,460 --> 00:06:48,460
also sort past the best chest programs 
that had another innovation.

96
00:06:49,350 --> 00:06:54,350
Uh,
basically you need to evaluate the 

97
00:06:54,350 --> 00:06:54,900
quality of the board at each point.
And they used a,

98
00:06:55,370 --> 00:06:59,720
another 100 layer neural nets to do that
evaluation.

99
00:07:00,320 --> 00:07:05,320
Um,
so there's still a problem in the field,

100
00:07:06,590 --> 00:07:11,590
uh,
which is there's a motto that life 

101
00:07:11,590 --> 00:07:14,531
begins at a billion examples.
One of the reasons I'm at Google is we 

102
00:07:14,531 --> 00:07:16,730
have a billion examples,
for example,

103
00:07:16,731 --> 00:07:20,120
does have pictures of dogs and cats that
are labeled.

104
00:07:20,350 --> 00:07:25,350
So you've got a picture of a cat and it 
says cat and then you can learn from it 

105
00:07:25,350 --> 00:07:28,361
and you need a lot of them,
a Alphago trained on a million online 

106
00:07:28,760 --> 00:07:33,760
moves.
That's how many we had of master games 

107
00:07:33,760 --> 00:07:37,690
and that only created a,
a sort of fair go player.

108
00:07:37,700 --> 00:07:42,700
A good amateur could defeat it.
So they worked around that in the case 

109
00:07:43,251 --> 00:07:48,251
of goal by basically generating an 
infinite amount of data by having the 

110
00:07:49,581 --> 00:07:52,900
system play itself,
uh,

111
00:07:52,940 --> 00:07:57,940
had a chat with Dennis Hassabis.
What kind of situations can you do that 

112
00:07:57,940 --> 00:08:02,501
with?
You have to have some way of simulating 

113
00:08:02,501 --> 00:08:05,591
the world.
So go or chess or even though go is 

114
00:08:05,591 --> 00:08:07,310
considered a difficult game,
it's a,

115
00:08:07,580 --> 00:08:12,580
you know,
the definition of it is exists on one 

116
00:08:12,580 --> 00:08:12,580
page,
uh,

117
00:08:12,580 --> 00:08:13,630
so you can simulate it.
A,

118
00:08:13,640 --> 00:08:15,480
that applies to math.
I mean,

119
00:08:15,660 --> 00:08:19,400
mass axioms are going to be contained on
a page or two.

120
00:08:19,820 --> 00:08:22,250
It's not very complicated.
Uh,

121
00:08:22,251 --> 00:08:27,140
it gets more difficult when you have 
real life situations like biology,

122
00:08:27,620 --> 00:08:30,920
so we have biological simulators,
but the simulators on perfect,

123
00:08:30,921 --> 00:08:35,390
so learning from the simulators will 
only be as good as the simulators.

124
00:08:36,410 --> 00:08:40,250
That's actually the key to being able to
do deep learning on biology.

125
00:08:42,560 --> 00:08:45,770
Autonomous Vehicles.
You need real life data.

126
00:08:46,240 --> 00:08:50,240
Uh,
so the waymo systems have gone three and

127
00:08:50,241 --> 00:08:52,280
a half million miles.
Uh,

128
00:08:52,281 --> 00:08:54,860
that's good.
That's enough data to then create a very

129
00:08:54,861 --> 00:08:59,861
good simulator.
The simulator is really quite realistic 

130
00:08:59,861 --> 00:09:02,550
because they had a lot of real world 
experience and the,

131
00:09:02,580 --> 00:09:05,100
they've got a billion miles in the 
simulator,

132
00:09:06,060 --> 00:09:11,060
but we don't always have that 
opportunity to either create the data or

133
00:09:11,101 --> 00:09:16,101
have the data around humans can learn 
from a small number of examples.

134
00:09:18,150 --> 00:09:20,160
Uh,
your significant other,

135
00:09:20,161 --> 00:09:21,540
your professor,
your boss,

136
00:09:21,541 --> 00:09:26,541
your investor a can tell you something 
once or twice and you might actually 

137
00:09:26,541 --> 00:09:31,221
learn from that.
Some humans have been reported to do 

138
00:09:31,221 --> 00:09:31,221
that.
And,

139
00:09:31,510 --> 00:09:31,890
uh,
that's,

140
00:09:32,020 --> 00:09:37,020
that's,
that's kind of the remaining advantage 

141
00:09:37,020 --> 00:09:37,020
of humans.
Now.

142
00:09:37,020 --> 00:09:39,540
There's actually no backpropagation and 
the human brain,

143
00:09:39,570 --> 00:09:44,570
it doesn't use deep learning.
It uses a different architecture that 

144
00:09:45,330 --> 00:09:48,600
same year in 1962 or older paper,
how I thought the human brain worked.

145
00:09:48,920 --> 00:09:53,920
Uh,
there was actually very little 

146
00:09:53,920 --> 00:09:54,250
neuroscience to go on.
There was one neuro science test,

147
00:09:54,270 --> 00:09:56,880
Vernon mountcastle.
It had something relevant to say,

148
00:09:57,860 --> 00:09:59,430
which he did.
I mean,

149
00:09:59,431 --> 00:10:01,770
there was a,
the common wisdom at the time,

150
00:10:01,771 --> 00:10:04,020
and there's still a lot of neuroscience 
instead say this.

151
00:10:04,021 --> 00:10:06,300
So we have all these different regions 
of the brain.

152
00:10:06,301 --> 00:10:11,301
They do different things.
They must be different than v one in the

153
00:10:11,461 --> 00:10:16,461
back of the head where the optic nerve 
skills into that can tell that that's a 

154
00:10:16,680 --> 00:10:19,650
curved line.
That's a straight line.

155
00:10:20,100 --> 00:10:23,640
Does a simple feature extractions on 
visual images.

156
00:10:23,760 --> 00:10:28,760
It's actually a large part of the 
neocortex does a cruciform gyrus up 

157
00:10:28,760 --> 00:10:30,270
here,
which can recognize faces.

158
00:10:30,660 --> 00:10:35,660
Uh,
we know that because if it gets knocked 

159
00:10:35,660 --> 00:10:36,210
out through injury or stroke,
people can't recognize faces.

160
00:10:36,450 --> 00:10:40,080
They will learn it again with a 
different region of the neocortex is the

161
00:10:40,081 --> 00:10:45,081
famous frontal Cortex,
which does language and poetry and 

162
00:10:45,081 --> 00:10:45,630
music.

163
00:10:46,030 --> 00:10:51,030
Uh,
so these must work on different 

164
00:10:51,030 --> 00:10:53,061
principles.
He did autopsies on the neocortex and 

165
00:10:53,061 --> 00:10:53,730
all these different regions and found 
they all look the same.

166
00:10:53,731 --> 00:10:57,090
They had the same repeating pattern,
same interconnections.

167
00:10:57,390 --> 00:10:59,760
Uh,
he said Neocortex is neocortex.

168
00:10:59,761 --> 00:11:04,761
So I had that hint.
Otherwise I could actually observe human

169
00:11:04,801 --> 00:11:07,440
brains and action,
which I did from time to time.

170
00:11:07,920 --> 00:11:10,860
And there's a lot of hints that you can 
get that way.

171
00:11:11,070 --> 00:11:13,680
For example,
if I ask you to recite the alphabet,

172
00:11:14,070 --> 00:11:18,660
you actually don't do it from a to z,
you do it as a sequence of sequences a,

173
00:11:18,661 --> 00:11:19,090
b,
c,

174
00:11:19,110 --> 00:11:19,920
d,
e,

175
00:11:19,921 --> 00:11:20,700
f,
g,

176
00:11:20,940 --> 00:11:21,360
H,
I,

177
00:11:21,361 --> 00:11:22,110
j,
k,

178
00:11:22,340 --> 00:11:27,340
two.
We learn things as forward sequences of 

179
00:11:27,340 --> 00:11:29,980
sequences forward because if I ask you 
to recite the alphabet backwards,

180
00:11:30,330 --> 00:11:33,270
you can't do it unless you learn that as
a new sequence.

181
00:11:33,540 --> 00:11:37,650
So these are all interesting.
Hence I wrote a paper that I,

182
00:11:37,800 --> 00:11:42,630
that the Neocortex is organized as a 
hierarchy of modules and each module can

183
00:11:42,631 --> 00:11:47,631
learn to simple pattern and that's how I
got to meet President Johnson.

184
00:11:48,570 --> 00:11:53,570
And that initiated a half century of 
thinking about issue.

185
00:11:54,370 --> 00:11:57,640
I came to mit to study with Marvin 
Minsky.

186
00:11:58,030 --> 00:11:59,350
Actually,
I came for two reasons.

187
00:11:59,360 --> 00:12:04,030
Once Minsky became my mentor,
which was the mentorship that lasted for

188
00:12:04,031 --> 00:12:09,031
over 50 years,
the fact that mit was so advanced and 

189
00:12:09,031 --> 00:12:12,841
actually had a computer,
which the other colleges I considered 

190
00:12:12,841 --> 00:12:15,450
didn't have a.
It was an IBM 70,

191
00:12:15,470 --> 00:12:19,600
94,
32 K of 36 fit words.

192
00:12:19,601 --> 00:12:24,601
So it's 150 k of core storage to 
microsecond cycle time to cycles for 

193
00:12:24,821 --> 00:12:29,350
instruction.
So a quarter of a map and that thousands

194
00:12:29,351 --> 00:12:34,060
of students and professors share that 
one machine in 2012.

195
00:12:34,061 --> 00:12:39,061
I wrote a book about this thesis is now 
actually an explosion of neuroscience 

196
00:12:39,221 --> 00:12:44,221
evidence to support it.
The European brain reverse engineering 

197
00:12:44,221 --> 00:12:47,380
project has identified or repeating 
module of about 100 neurons,

198
00:12:47,530 --> 00:12:51,880
repeated it $300 million times.
So it's about $30 billion neurons in the

199
00:12:51,881 --> 00:12:56,881
NEOCORTEX.
The neocortex is the outer layer of the 

200
00:12:56,881 --> 00:12:56,881
brain.

201
00:12:56,881 --> 00:12:58,740
That's part where we do our thinking,
uh,

202
00:12:58,741 --> 00:13:02,590
and they can see in each module,
axons coming in from another,

203
00:13:02,930 --> 00:13:05,970
a module,
and then the output acts,

204
00:13:05,980 --> 00:13:10,980
the single output accident of that 
module goes as the input to another 

205
00:13:10,980 --> 00:13:14,791
module.
So we can see it organized as a 

206
00:13:14,791 --> 00:13:15,190
hierarchy.
It's not a physical hierarchy.

207
00:13:16,240 --> 00:13:18,250
The hierarchy comes from these 
connections.

208
00:13:18,580 --> 00:13:22,420
The neocortex is a very thin structure.
It's actually one module thick,

209
00:13:22,870 --> 00:13:27,820
the six layers of neurons,
but it constitutes one module.

210
00:13:27,970 --> 00:13:32,970
And we can see that it learns and simple
pattern and various reasons I cite in 

211
00:13:33,791 --> 00:13:38,791
the book the pattern recognition model 
that's using is basically a hidden 

212
00:13:38,791 --> 00:13:43,741
Markov model.
How many of you have worked with mark 

213
00:13:43,741 --> 00:13:46,981
off models?
And that's usually no hands go up when I

214
00:13:48,790 --> 00:13:50,450
asked that question.
Um,

215
00:13:51,160 --> 00:13:54,610
but a Markov model is not.
It is learned a,

216
00:13:54,611 --> 00:13:56,470
but it's not backpropagation.

217
00:13:56,680 --> 00:14:00,430
It can learn local features.
So it's very good for speech recognition

218
00:14:00,431 --> 00:14:01,810
and speech recognition.
I work,

219
00:14:01,811 --> 00:14:06,811
I did in the eighties,
used these mark models that became the 

220
00:14:06,811 --> 00:14:11,230
standard approach because it can deal 
with local variations.

221
00:14:11,470 --> 00:14:14,500
So the fact that a vow will is 
stretched.

222
00:14:15,020 --> 00:14:20,020
It can learn that in a Markov model,
it doesn't learn long distance 

223
00:14:20,591 --> 00:14:25,591
relationships that's handled by the 
hierarchy and something we don't fully 

224
00:14:25,591 --> 00:14:29,650
understand yet is exactly how the 
neocortex creates that hierarchy.

225
00:14:30,370 --> 00:14:35,170
But we have figured out how I can 
connect this module two,

226
00:14:35,171 --> 00:14:37,000
this module,
does it then grow?

227
00:14:37,270 --> 00:14:42,270
I mean,
there's no virtual communication or 

228
00:14:42,270 --> 00:14:42,880
wireless communication.
It's actually a connection.

229
00:14:42,881 --> 00:14:47,881
So does it grow and Axon in are from one
place to another which could be inches 

230
00:14:48,580 --> 00:14:49,930
apart.
Uh,

231
00:14:49,931 --> 00:14:54,931
actually they all,
all these connections are there from 

232
00:14:54,931 --> 00:14:54,931
birth,
uh,

233
00:14:54,931 --> 00:14:57,230
like the streets and avenues of 
Manhattan,

234
00:14:57,620 --> 00:14:59,540
this vertical and horizontal connection.

235
00:14:59,540 --> 00:15:04,540
So if the decides and how it makes that 
decision is still not fully understood 

236
00:15:05,660 --> 00:15:07,220
that it wants to connect this module 
two,

237
00:15:07,221 --> 00:15:09,260
this module,
there's already a vertical,

238
00:15:09,530 --> 00:15:12,470
horizontal and a vertical connection.
It just activates them.

239
00:15:13,620 --> 00:15:18,620
We can actually see that now and could 
see that happening in real time on 

240
00:15:18,620 --> 00:15:21,180
noninvasive brain scans.
Uh,

241
00:15:21,320 --> 00:15:26,320
so this utterance amount of evidence 
that in fact the neocortex is a 

242
00:15:26,320 --> 00:15:30,761
hierarchy of modules that can learn.
Each module learns so simple sequential 

243
00:15:33,680 --> 00:15:38,000
pattern.
And even though the patterns we perceive

244
00:15:38,030 --> 00:15:43,030
don't seem like sequences,
they may seem three dimensional or even 

245
00:15:43,030 --> 00:15:44,420
more complicated.
They are in fact represented,

246
00:15:44,750 --> 00:15:46,160
uh,
as sequences.

247
00:15:46,161 --> 00:15:48,380
But the complexity comes in with the 
hierarchy.

248
00:15:49,430 --> 00:15:53,450
So the NEOCORTEX emerged 200 million 
years ago,

249
00:15:53,950 --> 00:15:55,160
uh,
with mammals.

250
00:15:55,161 --> 00:16:00,161
All mammals have a neocortex.
It's one of the distinguishing features 

251
00:16:00,161 --> 00:16:02,660
of mammals.
These first mammals were small.

252
00:16:02,690 --> 00:16:07,190
They were rodents where they were 
capable of a new type of thinking.

253
00:16:07,670 --> 00:16:12,670
Uh,
other nonmelanoma animals had fixed 

254
00:16:12,670 --> 00:16:13,070
behaviors,
but those fixed behaviors were very well

255
00:16:13,460 --> 00:16:18,460
adapted for their ecological niche.
But these new mammals could invent a new

256
00:16:19,281 --> 00:16:24,281
behavior.
So creativity and innovation was one 

257
00:16:24,281 --> 00:16:25,320
feature of the NEOCORTEX.
So analysis,

258
00:16:25,450 --> 00:16:27,470
escaping a predator,
it's usual.

259
00:16:27,471 --> 00:16:32,471
Escape Path is blocked.
It will invent a new behavior to deal 

260
00:16:32,471 --> 00:16:33,500
with.
It probably wouldn't work,

261
00:16:33,501 --> 00:16:35,120
but if it did work,
it would remember it.

262
00:16:35,121 --> 00:16:40,121
And we'd have a new behavior and that 
behavior could spread virally through 

263
00:16:40,121 --> 00:16:43,841
the community.
Another mouse watching this say to 

264
00:16:43,841 --> 00:16:44,660
itself,
that was really clever going around that

265
00:16:44,661 --> 00:16:46,970
rock,
I'm going to remember to do that.

266
00:16:47,390 --> 00:16:50,300
Uh,
and it would have a new behavior.

267
00:16:51,200 --> 00:16:54,950
Didn't help these early mammals that 
much because as I say,

268
00:16:55,010 --> 00:17:00,010
the Mammalian and animals who were very 
well adapted to their niches and nothing

269
00:17:02,211 --> 00:17:07,211
much happened for $135 million years,
but then 65 million years ago something 

270
00:17:08,541 --> 00:17:11,750
did happened,
there was a sudden violent change to the

271
00:17:11,751 --> 00:17:12,470
environment.

272
00:17:12,890 --> 00:17:15,380
We now call it the cretaceous extinction
event.

273
00:17:15,950 --> 00:17:19,790
There's been debate as to whether it was
a immediate or an asteroid.

274
00:17:20,120 --> 00:17:25,120
I mean a media or a volcanic eruption.
A the asteroid or media or hypothesis is

275
00:17:27,651 --> 00:17:32,651
in the ascendancy.
But if you dig down to an area of rock 

276
00:17:32,651 --> 00:17:37,010
reflecting 65 million years ago,
the geologist will explain that it shows

277
00:17:37,011 --> 00:17:39,800
a very violent sudden change to the 
environment.

278
00:17:39,801 --> 00:17:44,180
And we see it all around the globe.
So it was a worldwide phenomenon.

279
00:17:44,750 --> 00:17:49,750
The reason we call it an extinction 
event is that's when the dinosaurs went 

280
00:17:49,831 --> 00:17:51,210
extinct.
Uh,

281
00:17:51,240 --> 00:17:56,220
that's when 75 percent of all the animal
and plant species went extinct.

282
00:17:56,580 --> 00:17:59,640
And that's when mammals overtook their 
ecological niche.

283
00:18:00,270 --> 00:18:04,200
So to anthropomorphize,
biological evolution said to itself,

284
00:18:04,590 --> 00:18:06,390
hmm,
this neocortex is pretty good stuff.

285
00:18:06,391 --> 00:18:09,450
And it began to grow it.
So now mammals got bigger,

286
00:18:09,660 --> 00:18:11,970
their brains got bigger at an even 
faster pace,

287
00:18:11,971 --> 00:18:13,980
taking up a larger fraction of their 
body.

288
00:18:14,460 --> 00:18:17,280
The neocortex got bigger even faster 
than that.

289
00:18:17,640 --> 00:18:20,310
And developed these curvatures that are 
distinctive,

290
00:18:20,340 --> 00:18:25,340
have a primate brain basically to 
increase its surface area.

291
00:18:25,501 --> 00:18:27,610
But if you stretched it out,
uh,

292
00:18:27,611 --> 00:18:29,850
the human neocortex is still a flat 
structure.

293
00:18:29,851 --> 00:18:33,360
It's about the size of a table,
Napkin justice thin,

294
00:18:33,900 --> 00:18:36,750
uh,
and uh,

295
00:18:37,030 --> 00:18:39,510
it's basically,
uh,

296
00:18:40,050 --> 00:18:45,050
created a primates which became dominant
in their ecological niche.

297
00:18:46,400 --> 00:18:49,550
A then something else happened 2 million
years ago.

298
00:18:49,551 --> 00:18:54,551
And biological evolution decided to 
increase the neocortex further and 

299
00:18:55,201 --> 00:18:59,850
increase the size of the enclosure and 
basically filled up the frontal cortex,

300
00:19:00,110 --> 00:19:01,590
uh,
with our big skulls,

301
00:19:01,740 --> 00:19:05,070
with more neocortex.
And up until recently,

302
00:19:05,071 --> 00:19:06,900
it was felt that,
as I said,

303
00:19:06,901 --> 00:19:09,990
that this was,
the frontal cortex was different because

304
00:19:09,991 --> 00:19:12,300
it does these qualitatively different 
things.

305
00:19:13,920 --> 00:19:18,920
But we now realized that it's really 
just additional neocortex.

306
00:19:24,390 --> 00:19:29,370
So remember what we did with it.
We were already doing a very good job of

307
00:19:29,371 --> 00:19:34,371
being primates.
So we put it at the top of the 

308
00:19:34,371 --> 00:19:36,831
neocortical hierarchy.
And we increased the size of the 

309
00:19:36,831 --> 00:19:39,000
hierarchy,
it was maybe 20 percent more neocortex,

310
00:19:39,001 --> 00:19:44,001
but it doubled and tripled the number of
levels because as you go up the 

311
00:19:44,001 --> 00:19:44,280
hierarchy,
it's kind of like a pyramid.

312
00:19:44,281 --> 00:19:49,281
There's fewer and fewer modules and that
was the enabling factor for us to invent

313
00:19:51,630 --> 00:19:53,790
language and art.
Music.

314
00:19:53,791 --> 00:19:57,180
Every human culture we've ever 
discovered has music.

315
00:19:57,240 --> 00:20:00,750
No primate culture has music.
There's debate about that,

316
00:20:00,751 --> 00:20:02,820
but it's really true.

317
00:20:04,890 --> 00:20:09,890
Invention technology,
a technology required another 

318
00:20:09,890 --> 00:20:13,110
evolutionary adaptation,
which is this humble appendage here.

319
00:20:13,620 --> 00:20:15,210
Uh,
no other animal has add.

320
00:20:15,211 --> 00:20:17,400
If you look at a chimpanzee,
it looks like they have a similar hand,

321
00:20:17,401 --> 00:20:22,401
but the thumb is actually down here.
It doesn't work very well if you watch 

322
00:20:22,401 --> 00:20:25,461
them trying to grab a stick a so we 
could imagine creative solutions.

323
00:20:26,061 --> 00:20:31,061
Yeah.
I could take that branch and strip off 

324
00:20:31,061 --> 00:20:33,861
the leaves and put a point on it and we 
could actually carry out these ideas and

325
00:20:34,771 --> 00:20:39,771
create tools and then use tools to 
create new tools and started a whole 

326
00:20:39,771 --> 00:20:42,210
nother evolutionary process of tool 
making.

327
00:20:42,690 --> 00:20:45,420
And that all came with the,
with the NEOCORTEX.

328
00:20:47,110 --> 00:20:52,110
So Larry Page read my book a 2012 and 
liked it,

329
00:20:52,511 --> 00:20:57,511
so I met with him in essence for an 
investment in a company I'd started 

330
00:20:57,511 --> 00:21:01,780
actually a couple of weeks earlier to 
develop those ideas commercially because

331
00:21:01,781 --> 00:21:06,781
that's how I went about things as a 
serial entrepreneur and he said,

332
00:21:07,541 --> 00:21:08,380
well,
we'll invest,

333
00:21:08,381 --> 00:21:09,550
but let me give you a better idea.

334
00:21:09,550 --> 00:21:14,550
Why don't you do it here at Google,
we have a billion pictures of dogs and 

335
00:21:14,550 --> 00:21:17,320
cats and we got a lot of other data and 
lots of computers and lots of talents,

336
00:21:17,710 --> 00:21:19,810
all of which is true.
And says,

337
00:21:19,811 --> 00:21:20,260
well,
I don't know.

338
00:21:20,261 --> 00:21:22,920
I just started this company,
uh,

339
00:21:23,020 --> 00:21:25,780
to develop this as well by your company 
and said,

340
00:21:25,800 --> 00:21:30,800
Hey,
are you going to value a company that 

341
00:21:30,800 --> 00:21:32,161
hasn't done anything?
And just started a couple of weeks ago 

342
00:21:32,161 --> 00:21:33,250
and he said we can value anything.
Uh,

343
00:21:33,910 --> 00:21:38,910
so I took my first job five years ago 
and had been basically applying this 

344
00:21:40,511 --> 00:21:43,120
model,
this hierarchical model,

345
00:21:43,510 --> 00:21:46,570
um,
to understanding language,

346
00:21:46,600 --> 00:21:49,930
which I think really is the holy grail 
of Ai.

347
00:21:50,440 --> 00:21:55,440
I think touring was correct in 
designating basically text communication

348
00:21:58,060 --> 00:22:03,060
as well.
We now call a turing complete problem 

349
00:22:03,060 --> 00:22:06,211
that it requires.
There's no simple NLP tricks that you 

350
00:22:06,211 --> 00:22:09,691
can apply to pass a valid turing test 
with an emphasis on the word valid Mitch

351
00:22:11,350 --> 00:22:11,350
Kapor.

352
00:22:11,350 --> 00:22:16,350
And I had a six month debate on what the
rules should be because if you read 

353
00:22:16,350 --> 00:22:18,620
turing's 1950 paper,
uh,

354
00:22:18,730 --> 00:22:23,730
he describes this in a few paragraphs 
and doesn't really describe how to go 

355
00:22:23,730 --> 00:22:25,120
about it,
but if it's a valid turing test,

356
00:22:25,121 --> 00:22:30,121
meaning it's really convincing you 
through into interrogation and dialogue,

357
00:22:31,420 --> 00:22:36,420
uh,
that it's a human that requires a full 

358
00:22:36,420 --> 00:22:39,901
range of human intelligence.
And I think that a test says to the test

359
00:22:40,601 --> 00:22:44,230
of time,
we're making very good progress on that.

360
00:22:44,231 --> 00:22:49,231
I mean,
just last week you may have read that 

361
00:22:49,231 --> 00:22:51,090
two systems,
uh,

362
00:22:51,310 --> 00:22:55,900
past the paragraph comprehension test.
It's really very impressive.

363
00:22:55,901 --> 00:23:00,901
When I came to Google,
we were trying to pass these paragraph 

364
00:23:00,901 --> 00:23:03,490
comprehension tests.
We aced the first,

365
00:23:03,580 --> 00:23:08,580
the first grade test,
second grade tests were Kinda got 

366
00:23:08,580 --> 00:23:12,211
average performance.
And the third grade test had too much 

367
00:23:12,211 --> 00:23:12,470
inference.
I already,

368
00:23:12,471 --> 00:23:17,471
you had to know some common sense 
knowledge is it's called a and make 

369
00:23:18,101 --> 00:23:23,101
implications of things that were in 
different parts of the paragraph and 

370
00:23:23,101 --> 00:23:23,770
there's too much in France and really 
didn't,

371
00:23:23,830 --> 00:23:24,340
didn't work.

372
00:23:24,340 --> 00:23:29,340
So this is now an adult level,
just slightly surpassed average human 

373
00:23:29,621 --> 00:23:31,450
performance.
Uh,

374
00:23:31,480 --> 00:23:36,480
but we've seen that one something,
an ai does something at average human 

375
00:23:36,791 --> 00:23:39,460
levels.
It doesn't take long for it to soar past

376
00:23:40,000 --> 00:23:45,000
a average human levels.
I think it'll take longer in language 

377
00:23:45,000 --> 00:23:45,010
and it didn't sort of simple games like 
go,

378
00:23:45,680 --> 00:23:50,680
uh,
it's actually very impressive that it 

379
00:23:50,680 --> 00:23:53,080
surpasses now average human performance 
to use an Lstm long short temporal 

380
00:23:54,470 --> 00:23:59,470
memory.
But if you look at the adult test in 

381
00:23:59,470 --> 00:24:03,671
order to answer these questions,
it has to put together inferences and 

382
00:24:03,671 --> 00:24:06,800
implications of several different things
in the paragraph with some common sense.

383
00:24:06,801 --> 00:24:10,430
Knowledge is not explicitly stated.
So that's,

384
00:24:10,431 --> 00:24:15,431
I think a pretty impressive milestone.
So I've been developing,

385
00:24:16,610 --> 00:24:20,140
I've got a team of about 45 people,
um,

386
00:24:20,600 --> 00:24:23,570
and we've been developing this 
hierarchical model.

387
00:24:24,110 --> 00:24:29,110
We don't use Markov models because we 
can use deep learning for each module.

388
00:24:30,260 --> 00:24:35,260
And so we create an embedding for each 
word and recreate an embedding for each 

389
00:24:35,260 --> 00:24:36,110
sentence,
uh,

390
00:24:36,170 --> 00:24:38,720
this week.
Have a can talk about it because we have

391
00:24:38,721 --> 00:24:43,640
a published paper on it.
It can take into consideration context.

392
00:24:45,620 --> 00:24:49,660
If you use smart reply on Ge Fuse,
g mail on your phone,

393
00:24:49,680 --> 00:24:53,120
you'll see it gives you three 
suggestions for responses.

394
00:24:53,121 --> 00:24:58,121
That's called smart reply there that are
simple suggestions.

395
00:24:59,031 --> 00:25:02,750
But it has to actually understand 
perhaps a complicated email,

396
00:25:03,110 --> 00:25:03,820
uh,
and the,

397
00:25:03,860 --> 00:25:06,290
the quality of the suggestions and 
squarely quite good.

398
00:25:06,530 --> 00:25:08,120
Quite on point.
Uh,

399
00:25:08,121 --> 00:25:11,510
that's where my team using this kind of 
hierarchical model,

400
00:25:11,850 --> 00:25:13,770
uh,
so instead of mark off models,

401
00:25:13,780 --> 00:25:16,130
it uses embeddings,
uh,

402
00:25:16,640 --> 00:25:20,630
cause we can use backpropagation,
we might as well use it.

403
00:25:21,120 --> 00:25:26,120
Uh,
but I think what's missing from deep 

404
00:25:26,120 --> 00:25:29,500
learning is this hierarchical aspect of 
understanding because the world is 

405
00:25:29,841 --> 00:25:34,841
hierarchical.
That's why a evolution developed a 

406
00:25:34,841 --> 00:25:39,071
hierarchical brain structure to 
understand the natural hierarchy in the 

407
00:25:39,071 --> 00:25:39,071
world.

408
00:25:41,120 --> 00:25:44,810
And there are several problems with big,
deep neural nets.

409
00:25:44,811 --> 00:25:48,560
One is the fact that you really do need 
a billion examples and we don't.

410
00:25:48,830 --> 00:25:51,560
Sometimes we can generate them as in the
case of go,

411
00:25:52,050 --> 00:25:54,920
uh,
or if we have a really good simulator as

412
00:25:54,921 --> 00:25:58,610
in the case of autonomous vehicles,
not quite the case yet in biology,

413
00:25:59,120 --> 00:26:02,740
a very often you don't have ability to 
example,

414
00:26:02,741 --> 00:26:07,741
if you suddenly have billions of 
examples of language but they're not 

415
00:26:07,741 --> 00:26:10,961
annotated.
And how would you annotate it anyway 

416
00:26:10,961 --> 00:26:10,961
with more language that we can 
understand in the first place.

417
00:26:10,961 --> 00:26:13,670
So it's kind of a chicken and an egg 
problem.

418
00:26:14,210 --> 00:26:19,210
Uh,
so I believe this hierarchical 

419
00:26:19,210 --> 00:26:19,510
structures needed another criticism of 
deep neural nets.

420
00:26:19,550 --> 00:26:22,010
They don't explain themselves very well.
It's a big,

421
00:26:22,370 --> 00:26:27,280
a black box that gives you pretty 
remarkable answers.

422
00:26:27,320 --> 00:26:32,320
I mean,
in the case of these Games Dennis 

423
00:26:32,320 --> 00:26:34,971
described it's playing in both go and 
chess is almost an alien intelligence 

424
00:26:34,971 --> 00:26:39,521
because it will do things that were 
shocking to you and experts like 

425
00:26:39,521 --> 00:26:41,090
sacrificing a queen and a bishop at the 
same time,

426
00:26:41,600 --> 00:26:46,600
uh,
or in close succession which shocked 

427
00:26:46,600 --> 00:26:50,511
everybody,
but then went on win or early in a go 

428
00:26:50,511 --> 00:26:53,541
game,
putting a piece at the corner of the 

429
00:26:53,541 --> 00:26:56,691
board,
which is kinda crazy to most experts 

430
00:26:56,691 --> 00:26:57,360
because you really want to start 
controlling territory.

431
00:26:57,690 --> 00:27:01,410
And yet on reflection,
that was the brilliant move that enabled

432
00:27:01,411 --> 00:27:02,700
us to win that game.

433
00:27:05,250 --> 00:27:07,560
But it doesn't really explain how it 
does these things.

434
00:27:07,561 --> 00:27:08,060
So if,
yeah,

435
00:27:08,090 --> 00:27:13,090
if you have a hierarchy,
it's much better at explaining it 

436
00:27:13,090 --> 00:27:13,830
because you could look at the content of
the,

437
00:27:14,070 --> 00:27:19,070
of the modules in the hierarchy and 
they'll explain what they're doing.

438
00:27:20,310 --> 00:27:22,070
And,
uh,

439
00:27:22,320 --> 00:27:27,320
just to end on the first application of 
applying this to health and medicine,

440
00:27:28,290 --> 00:27:33,290
this will get into high gear and we're 
going to really see a breakout of the 

441
00:27:33,290 --> 00:27:36,621
linear extension to longevity that we've
experienced.

442
00:27:37,200 --> 00:27:39,210
A,
I believe we're only about a decade away

443
00:27:39,211 --> 00:27:41,310
from longevity,
escape velocity.

444
00:27:41,311 --> 00:27:46,290
We're adding more time than is going by,
not just to infant life expectancy,

445
00:27:46,291 --> 00:27:50,730
but to your remaining life expectancy.
I think if someone is diligent,

446
00:27:50,731 --> 00:27:54,570
they can be there already.
I think I've at longevity,

447
00:27:54,571 --> 00:27:59,571
escape velocity now a word on what life 
expectancy means.

448
00:28:01,050 --> 00:28:03,790
It used to be assumed that not much 
would happen.

449
00:28:03,791 --> 00:28:08,791
So whatever your life expectancy is a 
with or without scientific progress,

450
00:28:09,781 --> 00:28:12,450
it really didn't matter.
Now it matters a lot.

451
00:28:12,451 --> 00:28:14,700
So life expectancy really means,
you know,

452
00:28:14,730 --> 00:28:19,730
how long would you live,
what's the statistical likelihood if 

453
00:28:21,151 --> 00:28:23,910
there were not continued tying antic 
progress,

454
00:28:24,270 --> 00:28:29,270
but that's a very inaccurate assumption 
that scientific progress is extremely 

455
00:28:29,270 --> 00:28:29,370
rapid.
I mean,

456
00:28:29,371 --> 00:28:32,730
just as an ai and biotech,
there are advances now.

457
00:28:33,030 --> 00:28:35,790
Every week is quite stunning.

458
00:28:38,040 --> 00:28:41,190
Now you could have a computed life 
expectancy,

459
00:28:41,191 --> 00:28:43,400
let's say 30 years,
50 years,

460
00:28:43,401 --> 00:28:47,880
70 years from now,
you could still be hit by the proverbial

461
00:28:47,881 --> 00:28:52,881
bus tomorrow.
We're working on that with self driving 

462
00:28:52,881 --> 00:28:53,300
vehicles.
Um,

463
00:28:54,780 --> 00:28:56,490
but we'll get,
we'll get to a point.

464
00:28:56,540 --> 00:29:01,540
I think if you're diligent,
you can be there now in terms of 

465
00:29:01,540 --> 00:29:04,410
basically advancing your own statistical
life expectancy,

466
00:29:06,070 --> 00:29:08,800
at least to keep pace with the passage 
of time.

467
00:29:08,801 --> 00:29:13,801
I think it will be there for most of the
population at least if they're diligent 

468
00:29:14,820 --> 00:29:18,300
within about a decade.
So if he can hang in there,

469
00:29:19,200 --> 00:29:21,570
we may get to see the remarkable century
ahead.

470
00:29:21,660 --> 00:29:22,530
Thank you very much.

471
00:29:26,010 --> 00:29:27,710
You,
I have a

472
00:29:27,720 --> 00:29:29,520
question.
Please raise your hand and we'll get you

473
00:29:29,521 --> 00:29:32,780
on mic.
Uh,

474
00:29:32,970 --> 00:29:34,110
high.
Uh,

475
00:29:34,111 --> 00:29:35,490
so you mentioned,
uh,

476
00:29:35,520 --> 00:29:39,030
both your neural network models and 
symbolic models.

477
00:29:39,280 --> 00:29:41,470
Uh,
and I was wondering how far have

478
00:29:42,040 --> 00:29:44,680
you had been thinking about combining 
these two approaches?

479
00:29:44,980 --> 00:29:47,820
Creating a symbiosis between neural 
models.

480
00:29:47,830 --> 00:29:48,940
Send symbolic ones.

481
00:29:51,140 --> 00:29:56,140
I don't think we want to use symbolic 
models as they've been used.

482
00:29:58,600 --> 00:30:00,760
How many are familiar with the psych 
project?

483
00:30:03,920 --> 00:30:08,920
That was a very diligent effort in Texas
to define all of common sense reasoning 

484
00:30:10,590 --> 00:30:15,590
and it kind of collapsed on itself and 
became impossible to debug because you 

485
00:30:17,041 --> 00:30:19,170
fix one thing and it would break three 
other things.

486
00:30:19,860 --> 00:30:24,760
That complexity ceiling has become 
typical of,

487
00:30:25,660 --> 00:30:29,770
of trying to define things through 
logical rules.

488
00:30:30,480 --> 00:30:35,480
Uh,
now it does seem that humans can 

489
00:30:35,480 --> 00:30:37,711
understand logical rules.
We have logical rules written down for 

490
00:30:37,711 --> 00:30:40,840
things like law and game playing and so 
on.

491
00:30:41,360 --> 00:30:45,040
Uh,
but you can actually define a connection

492
00:30:45,041 --> 00:30:50,041
as system to have such a high 
reliability on a certain type of action 

493
00:30:53,021 --> 00:30:58,021
that it looks like it's a symbolic role,
even though it's represented in a 

494
00:30:58,960 --> 00:31:03,960
connectionist way.
And connection systems can both capture 

495
00:31:03,960 --> 00:31:08,881
the soft edges because many things in 
life are not a sharply defined.

496
00:31:09,490 --> 00:31:12,430
They can also generate exceptions.
So you,

497
00:31:12,520 --> 00:31:17,080
you don't want to sacrifice your queen 
and chest except certain situations that

498
00:31:17,081 --> 00:31:22,081
might be a good idea.
So you can capture that kind of 

499
00:31:22,081 --> 00:31:22,081
complexity.
Uh,

500
00:31:22,540 --> 00:31:27,540
so we do want to be able to learn from 
accumulated human wisdom that looks like

501
00:31:28,661 --> 00:31:30,070
it's symbolic.
But,

502
00:31:30,270 --> 00:31:32,780
uh,
I think we'll do it with a connection of

503
00:31:32,790 --> 00:31:33,760
system.
But again,

504
00:31:33,761 --> 00:31:38,761
I'm,
I think the connection systems should 

505
00:31:38,761 --> 00:31:42,890
develop a sense of hierarchy and not 
just be one big massive neural net.

506
00:31:44,800 --> 00:31:47,260
So I understand how we won,
you know,

507
00:31:47,340 --> 00:31:51,420
using your cortex to extract useful 
stuff and commercialize that,

508
00:31:51,421 --> 00:31:53,650
but I'm wondering how,
you know,

509
00:31:53,790 --> 00:31:58,150
our middle brain and the organs that are
below the neocortex will be useful for,

510
00:31:58,460 --> 00:31:59,330
um,
you know,

511
00:31:59,680 --> 00:32:01,840
turning that into what you want to do.
So.

512
00:32:03,360 --> 00:32:05,490
Well,
the cerebellum is an interesting case in

513
00:32:05,491 --> 00:32:10,491
point.
It actually has more neurons in the 

514
00:32:10,491 --> 00:32:13,530
neocortex and it's used to govern most 
of our behavior.

515
00:32:15,650 --> 00:32:16,760
Uh,
some things,

516
00:32:17,000 --> 00:32:19,890
if you write a signature that's actually
controlled by the cerebellum,

517
00:32:19,950 --> 00:32:24,870
so a simple sequence is stored in the 
cerebellum,

518
00:32:25,950 --> 00:32:29,760
but there's not any reasoning to it.
It's basically a script.

519
00:32:30,240 --> 00:32:35,240
Uh,
and most of our movement now has 

520
00:32:35,240 --> 00:32:37,500
actually been migrated from the 
cerebellum to the NEOCORTEX.

521
00:32:37,860 --> 00:32:40,530
Cerebellum is still there.
Uh,

522
00:32:40,580 --> 00:32:45,580
some people,
a tire cerebellum is destroyed through 

523
00:32:45,580 --> 00:32:45,830
disease.
Uh,

524
00:32:45,860 --> 00:32:48,950
they still function fairly.
Normally they're moving,

525
00:32:48,951 --> 00:32:53,951
might be a little erratic as our 
movement is largely controlled by the 

526
00:32:53,951 --> 00:32:58,631
Neocortex,
but some of the subtlety is a kind of 

527
00:32:58,631 --> 00:33:01,310
preprogrammed script and so they'll look
a little clumsy,

528
00:33:01,311 --> 00:33:04,570
but they're actually functioning okay.
Um,

529
00:33:05,060 --> 00:33:07,350
a lot of other areas of the brain 
control,

530
00:33:07,351 --> 00:33:11,200
autonomic functions like breathing and 
uh,

531
00:33:11,310 --> 00:33:16,310
but our thinking really is,
is controlled by the neocortex in terms 

532
00:33:16,310 --> 00:33:19,871
of a mastering intelligence.
I think the NEOCORTEX is the brain 

533
00:33:22,551 --> 00:33:23,780
region we want to study.

534
00:33:25,860 --> 00:33:30,860
I'm curious what you think might happen 
after the singularity is reached in 

535
00:33:31,411 --> 00:33:34,260
terms of this exponential growth of 
information.

536
00:33:35,390 --> 00:33:40,390
Yeah.
Do you think it will continue or will 

537
00:33:40,390 --> 00:33:41,010
there be a whole paradigm shift?
What do you predict?

538
00:33:42,270 --> 00:33:47,270
Well,
in the singularity is near talk about 

539
00:33:47,270 --> 00:33:49,701
the atomic limits based on molecular 
computing as we understand it,

540
00:33:50,340 --> 00:33:55,340
uh,
and it can actually go well past 20 

541
00:33:55,340 --> 00:33:57,781
slash 45 and actually go to trillions of
trillions of times a greater 

542
00:33:57,781 --> 00:34:00,060
computational capacity than we have 
today.

543
00:34:01,890 --> 00:34:06,890
So I don't see that stopping anytime 
soon and we'll go way beyond what we can

544
00:34:08,580 --> 00:34:10,210
imagine.
Um,

545
00:34:11,760 --> 00:34:13,740
and it becomes an interesting 
discussion.

546
00:34:13,741 --> 00:34:18,210
What the impact on a human civilization 
will be.

547
00:34:18,900 --> 00:34:23,900
So take it maybe slightly more mundane 
issue that comes up as us going to 

548
00:34:24,150 --> 00:34:29,150
eliminate most jobs are all jobs.
A point I make is it's not the first 

549
00:34:29,791 --> 00:34:31,650
time in human history.
We've done that.

550
00:34:32,100 --> 00:34:36,350
How many jobs circa 1900 exists today.
Uh,

551
00:34:36,360 --> 00:34:39,690
and that was the feeling of the 
Luddites,

552
00:34:39,691 --> 00:34:44,691
which was some actual society and that 
formed in 1800 after the automation of 

553
00:34:44,691 --> 00:34:46,590
the textile industry in England.

554
00:34:47,180 --> 00:34:52,180
They looked at all these jobs going away
and felt old employment's going to be 

555
00:34:52,180 --> 00:34:53,790
just limited to an elite.
Uh,

556
00:34:53,791 --> 00:34:55,560
indeed,
those jobs did go away,

557
00:34:55,561 --> 00:35:00,561
but new jobs were created.
So if I were oppression futurist in 

558
00:35:00,561 --> 00:35:02,370
1900,
I would say,

559
00:35:02,371 --> 00:35:07,371
well,
38 percent of your work on farms and 25 

560
00:35:07,371 --> 00:35:08,880
percent work in factories.
That's two thirds of the working for us,

561
00:35:11,490 --> 00:35:16,200
but I predict by 20 1,515
years from now,

562
00:35:16,201 --> 00:35:19,470
it's going to be two percent on farms 
and nine percent in factories.

563
00:35:19,471 --> 00:35:21,210
And everybody would go,
oh my God,

564
00:35:21,211 --> 00:35:22,950
we're going to be out of work.
And I said,

565
00:35:22,951 --> 00:35:27,951
well,
don't worry for all these jobs we 

566
00:35:27,951 --> 00:35:27,951
eliminate through automation.
We're going to invent new jobs.

567
00:35:27,951 --> 00:35:28,410
People say,
Oh,

568
00:35:28,411 --> 00:35:29,470
really?
What new jobs?

569
00:35:29,471 --> 00:35:30,510
And I'd say,
well,

570
00:35:30,511 --> 00:35:32,100
I don't know.
We haven't invented them yet.

571
00:35:33,570 --> 00:35:38,570
That's the political problem.
We can see jobs very clearly going away 

572
00:35:38,570 --> 00:35:38,570
fairly soon.

573
00:35:38,570 --> 00:35:40,550
Like driving a car or truck,
uh,

574
00:35:41,190 --> 00:35:42,960
and the new jobs haven't been invented.
I mean,

575
00:35:42,961 --> 00:35:47,961
it's just look at the last five or six 
years as many of the increase in 

576
00:35:47,961 --> 00:35:52,461
employment has been through mobile app 
related types of ways of making money 

577
00:35:53,311 --> 00:35:56,220
that just weren't contemplated even six 
years ago.

578
00:35:57,990 --> 00:35:59,430
If I really pressured,
I would say,

579
00:35:59,431 --> 00:36:01,200
well,
you're going to get jobs creating mobile

580
00:36:01,201 --> 00:36:06,201
apps and websites and doing data 
analytics and a self driving cars,

581
00:36:07,770 --> 00:36:08,820
cars.
What's a car?

582
00:36:08,940 --> 00:36:13,940
And nobody would have any idea what I'm 
talking about a now the new job.

583
00:36:16,050 --> 00:36:16,920
Some people say,
yeah,

584
00:36:16,921 --> 00:36:19,740
we created new jobs for.
It's not as many actually.

585
00:36:20,210 --> 00:36:25,210
We've gone from 24 million jobs in 1900,
242 million jobs today for 30 percent of

586
00:36:26,191 --> 00:36:28,680
the population to 45 percent of the 
population.

587
00:36:29,330 --> 00:36:34,330
The new jobs pay 11 times as much in 
constant dollars and they're more 

588
00:36:34,330 --> 00:36:34,330
interesting.

589
00:36:34,330 --> 00:36:39,260
I mean,
as I talk to people starting out their 

590
00:36:39,260 --> 00:36:39,380
career now,
they really want a career that gives him

591
00:36:39,381 --> 00:36:43,560
some life definition and purpose and 
gratification.

592
00:36:43,561 --> 00:36:47,700
We're moving up maslow's hierarchy a 100
years ago.

593
00:36:47,701 --> 00:36:51,060
You are happy if you had a backbreaking 
job to put food on your family's table.

594
00:36:51,061 --> 00:36:56,061
So,
and we couldn't do these new jobs 

595
00:36:56,061 --> 00:37:00,141
without enhancing our intelligence.
So we've been doing that a well for most

596
00:37:00,271 --> 00:37:05,040
of the last 100 years through education.
We've expanded k through 12 and constant

597
00:37:05,041 --> 00:37:09,660
dollars tenfold.
We've gone from 38,000

598
00:37:09,661 --> 00:37:13,530
college students in 1870 to $50 million 
today.

599
00:37:14,070 --> 00:37:19,070
Uh,
more recently we have brain extenders 

600
00:37:19,070 --> 00:37:19,920
and not yet connected directly in our 
brain,

601
00:37:20,250 --> 00:37:25,250
but they're great close at hand when I 
was here at mit to take my bicycle 

602
00:37:25,250 --> 00:37:28,440
across campus to get to the computer and
show an ID to get in the building.

603
00:37:28,441 --> 00:37:33,441
Now we carry them in our,
in our pockets and under our belts.

604
00:37:33,990 --> 00:37:38,990
A,
they're going to go inside our bodies 

605
00:37:38,990 --> 00:37:41,391
and brains.
I think that's a really important 

606
00:37:41,391 --> 00:37:44,660
distinction,
but so we're basically going to be 

607
00:37:44,660 --> 00:37:46,620
continuing to enhance our capability 
through merging with Ai.

608
00:37:47,790 --> 00:37:52,790
And that's the,
I think ultimate answer to the kind of 

609
00:37:52,790 --> 00:37:56,360
Dystopian view we see in futures movies 
where it's the ai versus a brave band of

610
00:37:56,731 --> 00:38:01,731
humans for control of humanity.
We don't have one or two ais in the 

611
00:38:01,731 --> 00:38:03,090
world today.
We have several billion,

612
00:38:03,120 --> 00:38:08,120
3 billion smartphones at last count and 
it'll be 6 billion in just a couple of 

613
00:38:08,250 --> 00:38:10,310
years according to the projections.
Uh,

614
00:38:10,430 --> 00:38:15,430
so we're,
we're already deeply integrated with 

615
00:38:15,430 --> 00:38:15,430
this,
uh,

616
00:38:15,430 --> 00:38:18,170
and I think that's going to continue and
it's going to continue to do things that

617
00:38:18,171 --> 00:38:21,330
we can't even imagine today just as we 
are doing today.

618
00:38:21,331 --> 00:38:24,420
Things we couldn't imagine know even 20 
years ago

619
00:38:26,490 --> 00:38:29,670
you showed many grasp that go through 
exponential growth,

620
00:38:29,671 --> 00:38:33,360
but I haven't seen one that isn't.
So I would be very interested in hearing

621
00:38:33,660 --> 00:38:36,280
you haven't seen what that,
what that is not exponential.

622
00:38:36,790 --> 00:38:41,790
So tell me about regions that you've 
investigated that have not seen 

623
00:38:41,980 --> 00:38:44,800
exponential growth.
And why do you think that's the case?

624
00:38:45,980 --> 00:38:47,720
Well,
price,

625
00:38:47,721 --> 00:38:52,721
performance and capacity of information 
technology invariably follows a 

626
00:38:52,910 --> 00:38:57,350
exponential where it impacts human 
society.

627
00:38:57,351 --> 00:38:59,240
It can be linear.
So for example,

628
00:38:59,241 --> 00:39:02,010
the growth of democracy,
uh,

629
00:39:02,300 --> 00:39:07,300
has been linear but still pretty steady.
I could count the number of democracies 

630
00:39:07,641 --> 00:39:11,330
on the fingers of one hand to century,
go a two centuries ago.

631
00:39:11,331 --> 00:39:16,331
You could count the number of 
democracies in the world on the fingers 

632
00:39:16,331 --> 00:39:18,881
of one finger.
Now there are dozens of them that this 

633
00:39:18,881 --> 00:39:22,470
has become kind of a consensus that 
that's how we should be governed.

634
00:39:25,030 --> 00:39:30,030
So the,
and I attribute all this to the growth 

635
00:39:30,030 --> 00:39:34,231
in information technology,
communication in particular for a 

636
00:39:34,780 --> 00:39:39,430
progression of social cultural 
institutions,

637
00:39:40,140 --> 00:39:45,140
um,
but information technology because it 

638
00:39:45,140 --> 00:39:48,211
ultimately depends on a vanishingly 
small energy and material requirement 

639
00:39:51,730 --> 00:39:54,580
grows exponentially and will for a long 
time.

640
00:39:55,390 --> 00:39:57,340
Uh,
there was recently a criticism that,

641
00:39:57,370 --> 00:39:58,480
well,
chest scars have.

642
00:39:59,080 --> 00:40:04,080
It's actually a remarkably a straight 
linear progression,

643
00:40:05,020 --> 00:40:10,020
so humans think it's like 2,800
and it just soared past that in 1997 

644
00:40:10,871 --> 00:40:13,780
with deep blue and it's kept going,
uh,

645
00:40:14,160 --> 00:40:16,000
and a remarkably straight and St.
well,

646
00:40:16,001 --> 00:40:17,500
this is linear,
not exponential,

647
00:40:17,501 --> 00:40:20,380
but the chest score is a logarithmic,
uh,

648
00:40:20,410 --> 00:40:22,140
measurement,
uh,

649
00:40:22,540 --> 00:40:24,080
so,
uh,

650
00:40:24,110 --> 00:40:26,650
it,
it really is exponential progression.

651
00:40:28,590 --> 00:40:32,250
So philosophers like to think about the 
meaning of things,

652
00:40:32,251 --> 00:40:34,680
especially in the 20th century.
So for instance,

653
00:40:34,681 --> 00:40:39,681
Martin Heidegger gave a couple of 
speeches and lectures on the 

654
00:40:39,681 --> 00:40:41,460
relationship of human society to 
technology,

655
00:40:41,760 --> 00:40:45,960
and he particularly distinguished 
between the mode of thinking,

656
00:40:45,961 --> 00:40:48,600
which is calculating,
thinking and a mode of thinking,

657
00:40:48,601 --> 00:40:51,390
which is reflective thinking or 
meditative thinking.

658
00:40:51,440 --> 00:40:52,560
Um,
and,

659
00:40:52,590 --> 00:40:53,970
uh,
he posed this question,

660
00:40:53,971 --> 00:40:58,080
what is the meaning and purpose of 
technological development?

661
00:40:58,110 --> 00:41:00,120
And he couldn't find an answer.
He,

662
00:41:00,240 --> 00:41:03,530
he recommended it to remain open to what
you called.

663
00:41:04,530 --> 00:41:06,690
He called this an openness to the 
mystery.

664
00:41:07,040 --> 00:41:12,040
Uh,
I wonder whether you have any thoughts 

665
00:41:12,040 --> 00:41:12,040
on this.
Is there a meaning,

666
00:41:12,040 --> 00:41:13,290
a purpose of technological element?
And,

667
00:41:13,291 --> 00:41:17,040
and is there a way for us human success,
excess that meaning.

668
00:41:21,430 --> 00:41:26,430
Well,
we started using technology to shore up 

669
00:41:26,430 --> 00:41:30,850
weaknesses and our own capabilities.
So physically,

670
00:41:31,180 --> 00:41:33,190
I mean,
who here could build this building?

671
00:41:33,200 --> 00:41:36,370
So we've leveraged the power of our 
muscles with machines,

672
00:41:36,750 --> 00:41:41,750
uh,
and were in fact very bad at doing 

673
00:41:41,750 --> 00:41:42,000
things that,
you know,

674
00:41:42,080 --> 00:41:47,080
the simplest computers can do a pike 
factor numbers or even just multiply two

675
00:41:49,730 --> 00:41:53,750
digit numbers.
Computers can do that trivially.

676
00:41:53,751 --> 00:41:57,920
We can't do it.
So we originally started using computers

677
00:41:57,921 --> 00:42:02,160
to make up for that weakness.
Uh,

678
00:42:02,210 --> 00:42:07,210
I think the essence of what I've been 
writing about is to master the unique 

679
00:42:07,210 --> 00:42:09,110
strengths of,
of humanity,

680
00:42:10,370 --> 00:42:14,420
creating loving expressions and poetry 
and music and,

681
00:42:15,050 --> 00:42:20,050
uh,
the kinds of things we associate with 

682
00:42:20,050 --> 00:42:20,720
the better qualities of humanity with 
machines.

683
00:42:20,721 --> 00:42:23,910
That's the true promise of Ai,
uh,

684
00:42:24,250 --> 00:42:29,250
that we're not there yet,
but we're making pretty stunning 

685
00:42:29,250 --> 00:42:31,930
progress just in the last year,
this so many milestones that are really 

686
00:42:31,930 --> 00:42:33,830
significant,
including in language.

687
00:42:34,400 --> 00:42:36,890
Um,
and,

688
00:42:37,130 --> 00:42:41,090
but I think of technology as an 
expression of humanity.

689
00:42:41,510 --> 00:42:45,860
It's part of who we are and the human 
species is already a,

690
00:42:46,390 --> 00:42:51,390
a biological technological civilization 
and it's part of who we are and ai is as

691
00:42:52,910 --> 00:42:55,110
part of humans.
Uh,

692
00:42:55,550 --> 00:43:00,550
so ai is human and it's just,
it's part of the technological 

693
00:43:00,951 --> 00:43:05,951
expression of humanity.
And we use technology to extend our 

694
00:43:05,951 --> 00:43:06,820
reach.
You know,

695
00:43:06,880 --> 00:43:09,500
I couldn't reach that fruit at that 
higher branch a thousand years ago.

696
00:43:09,501 --> 00:43:11,990
So we invented a tool to extend our 
physical reach.

697
00:43:12,550 --> 00:43:17,550
Now extend our mental reach.
We can access all of human knowledge 

698
00:43:17,600 --> 00:43:20,400
with a few keystrokes.
Um,

699
00:43:21,200 --> 00:43:26,200
and we're going to make ourselves 
literally smarter by merging with Ai.

700
00:43:31,410 --> 00:43:33,950
Hi,
first of all,

701
00:43:34,160 --> 00:43:36,180
honor to hear you speak here.
Uh,

702
00:43:36,410 --> 00:43:41,410
so I first read the singularity is near 
nine years ago or so,

703
00:43:42,260 --> 00:43:45,260
and it changed the way I thought 
entirely,

704
00:43:45,350 --> 00:43:50,350
but something I think it caused me to 
oversee deeply discount was tail risk in

705
00:43:54,500 --> 00:43:59,500
geopolitics in systems that span the 
entire globe.

706
00:44:00,920 --> 00:44:05,920
Um,
and my concern is that there are,

707
00:44:06,980 --> 00:44:10,820
there's obviously the possibility of 
tail risk,

708
00:44:12,490 --> 00:44:17,490
existential level events,
swamping all of these trends that are 

709
00:44:17,650 --> 00:44:22,240
otherwise waterproof climate proof,
you name it.

710
00:44:22,450 --> 00:44:27,450
So my question for you is,
what steps do you think we can take in 

711
00:44:29,051 --> 00:44:34,051
designing engineered systems in 
designing social and economic 

712
00:44:34,051 --> 00:44:38,961
institutions to kind of minimize our 
exposure to these tail risks and,

713
00:44:40,050 --> 00:44:42,480
and survive to make it,
to,

714
00:44:42,481 --> 00:44:43,740
um,
you know,

715
00:44:43,741 --> 00:44:47,520
a beautiful mind filled future?

716
00:44:48,350 --> 00:44:49,280
Yeah.
Well,

717
00:44:51,920 --> 00:44:56,920
the world was first introduced to a 
human made existential risk.

718
00:44:57,770 --> 00:45:00,020
Uh,
when I was in elementary school,

719
00:45:00,021 --> 00:45:05,021
we would have these civil defense drills
get under our desks and put our hands 

720
00:45:05,021 --> 00:45:07,490
behind our head to protect this former 
thermonuclear war.

721
00:45:07,970 --> 00:45:09,470
Uh,
and it worked.

722
00:45:09,471 --> 00:45:14,471
We made it through,
but that was really the first 

723
00:45:14,751 --> 00:45:17,740
introduction to an existential risk,
uh,

724
00:45:17,860 --> 00:45:21,050
and those weapons are still there by the
way,

725
00:45:21,051 --> 00:45:25,970
and they're still on a hair trigger and 
they don't get that much attention.

726
00:45:26,430 --> 00:45:28,460
Uh,
there's been a lot of discussion,

727
00:45:29,030 --> 00:45:34,030
a much of which I've been in the 
forefront of initiating the existential 

728
00:45:34,030 --> 00:45:37,940
risks of what's sometimes referred to as
gnr g for genetics,

729
00:45:37,941 --> 00:45:42,410
which is biotechnology and financial 
technology and grey goo robotics,

730
00:45:42,411 --> 00:45:44,480
which is ai.
Uh,

731
00:45:45,110 --> 00:45:47,990
and I've been accused of being an 
optimist.

732
00:45:48,520 --> 00:45:51,130
I think you have to be an optimist to be
an entrepreneur.

733
00:45:51,140 --> 00:45:53,480
If you knew all the problems you were 
going to encounter,

734
00:45:53,481 --> 00:45:55,790
you'd never start any project.
But,

735
00:45:56,270 --> 00:46:00,230
uh,
I've written a lot about the downsides.

736
00:46:00,410 --> 00:46:03,060
I remain optimistic.
Uh,

737
00:46:03,080 --> 00:46:08,080
there are specific paradigms and not 
foolproof that we can follow to keep 

738
00:46:08,080 --> 00:46:09,920
these technologies safe.
So,

739
00:46:09,921 --> 00:46:11,960
for example,
uh,

740
00:46:12,020 --> 00:46:13,740
over 40 years ago,
uh,

741
00:46:13,880 --> 00:46:17,720
some visionaries recognize the 
revolutionary potential.

742
00:46:18,080 --> 00:46:21,530
Both were promise and peril of 
biotechnology,

743
00:46:22,430 --> 00:46:25,700
neither of the promise and the peril of 
what's feasible 40 years ago.

744
00:46:26,690 --> 00:46:30,800
But they had a conference at the Sylmar 
Conference Center in California,

745
00:46:31,240 --> 00:46:36,240
uh,
to develop both a professional ethics 

746
00:46:36,240 --> 00:46:40,601
and strategies to keep biotechnology 
safe and they've been known to see in 

747
00:46:40,601 --> 00:46:45,000
Sylmar guidelines.
They've been refined through successive 

748
00:46:45,000 --> 00:46:48,810
sylmar conferences.
Much of that is baked into law and an,

749
00:46:49,250 --> 00:46:51,110
in my opinion,
it's worked quite well.

750
00:46:51,111 --> 00:46:52,220
We're now,
as I mentioned,

751
00:46:52,221 --> 00:46:55,100
getting profound benefit.
It's a trickle today.

752
00:46:55,101 --> 00:46:57,740
It'll be a flood over the next decade.

753
00:46:58,250 --> 00:47:03,250
And the number of people who have been 
harmed either through intentional or 

754
00:47:03,250 --> 00:47:04,760
accidental abuse of biotechnology so 
far,

755
00:47:04,790 --> 00:47:06,410
zero.
Actually,

756
00:47:06,411 --> 00:47:11,411
I take that back.
There was one boy who died in gene 

757
00:47:11,411 --> 00:47:14,561
therapy trials,
but 12 years ago and there's 

758
00:47:14,561 --> 00:47:15,060
congressional hearings and the,
uh,

759
00:47:15,110 --> 00:47:20,110
canceled all research for a gene therapy
for a number of years.

760
00:47:20,661 --> 00:47:23,900
You could do an interesting message 
thesis and demonstrate that,

761
00:47:23,930 --> 00:47:25,010
you know,
300,000

762
00:47:25,011 --> 00:47:28,370
people died as a result of that delay,
but you can't name them.

763
00:47:28,371 --> 00:47:30,670
They can't go on.
So we don't know who they are.

764
00:47:30,671 --> 00:47:32,040
But,
um,

765
00:47:32,200 --> 00:47:34,300
so it has to do with the balancing of 
risk.

766
00:47:34,301 --> 00:47:39,301
But in large measure,
virtually no one has been hurt by 

767
00:47:39,301 --> 00:47:39,560
biotechnology.
Now,

768
00:47:39,570 --> 00:47:41,380
that doesn't mean you can cross it off 
our list.

769
00:47:41,660 --> 00:47:46,660
Okay?
We took care of that one because the 

770
00:47:46,660 --> 00:47:46,660
technology keeps getting more 
sophisticated.

771
00:47:46,730 --> 00:47:51,730
Crispers great opportunity.
This hundreds of trials of crispr 

772
00:47:52,240 --> 00:47:56,440
technologies overcome disease,
but it could be abused.

773
00:47:56,440 --> 00:48:00,370
You can easily describe scenarios,
so we have to keep reinventing it.

774
00:48:00,730 --> 00:48:02,110
Uh,
January,

775
00:48:02,111 --> 00:48:05,440
we had our first Sylmar conference on ai
ethics.

776
00:48:05,950 --> 00:48:09,310
And so I think this is a good paradigm.
It's not foolproof.

777
00:48:09,870 --> 00:48:14,870
Uh,
I think the best way we can assure a 

778
00:48:16,060 --> 00:48:21,060
democratic,
a future that includes ideas of liberty 

779
00:48:21,060 --> 00:48:25,201
is to practice that in the world today 
because the future world of the 

780
00:48:25,201 --> 00:48:29,011
singularity,
which is a merger of biological 

781
00:48:29,011 --> 00:48:29,500
biological intelligence,
uh,

782
00:48:29,520 --> 00:48:31,300
is not going to come from Mars.
I mean,

783
00:48:31,301 --> 00:48:33,520
it's going to emerge from our society 
today.

784
00:48:34,050 --> 00:48:36,310
Uh,
so if we practice these ideals today,

785
00:48:36,311 --> 00:48:41,311
it's gonna have a higher chance of us 
practicing them as we get more enhanced 

786
00:48:41,311 --> 00:48:42,730
with technology.
Uh,

787
00:48:42,731 --> 00:48:45,550
that doesn't sound like a foolproof 
solution or it isn't.

788
00:48:45,551 --> 00:48:50,350
But I think that's the best approach in 
terms of technological solutions.

789
00:48:50,710 --> 00:48:52,450
I mean,
ai is the most daunting.

790
00:48:52,451 --> 00:48:53,470
You can imagine.

791
00:48:53,800 --> 00:48:58,800
A,
there are technical solutions to 

792
00:48:58,800 --> 00:49:00,190
biotechnology and nanotechnology.
Uh,

793
00:49:00,191 --> 00:49:05,191
there's really no set routine.
You can put it in your ai software that 

794
00:49:05,191 --> 00:49:08,080
will assure that have remained safe 
intelligence.

795
00:49:08,250 --> 00:49:13,250
It's inherently not controllable.
There's some ai that's smarter than you 

796
00:49:13,250 --> 00:49:16,840
that's out for your destruction.
The best way to deal with that is not to

797
00:49:16,841 --> 00:49:19,480
get in that situation in the first 
place.

798
00:49:20,590 --> 00:49:24,460
If you are in that situation,
find some ai that will be on your side.

799
00:49:25,030 --> 00:49:27,490
Um,
but basically,

800
00:49:27,700 --> 00:49:28,910
uh,
it's going to it,

801
00:49:28,990 --> 00:49:32,500
I believe we have been headed through 
technology,

802
00:49:32,730 --> 00:49:33,390
uh,
to a,

803
00:49:33,670 --> 00:49:38,670
to a better reality,
a look around the world and people 

804
00:49:38,670 --> 00:49:40,720
really think things are getting worse.
Uh,

805
00:49:40,721 --> 00:49:45,721
and I think that's because our 
information about what's wrong with the 

806
00:49:45,721 --> 00:49:46,300
world is getting exponentially better.
I say,

807
00:49:46,301 --> 00:49:51,301
Oh,
this is the most peaceful time in human 

808
00:49:51,301 --> 00:49:51,301
history in previously.
What are you crazy?

809
00:49:51,301 --> 00:49:54,430
Didn't you hear about the event 
yesterday and last week and a while,

810
00:49:54,510 --> 00:49:57,190
a hundred years ago,
that could be a battle and wiped out the

811
00:49:57,191 --> 00:50:02,191
next village.
And you wouldn't even hear about it for 

812
00:50:02,191 --> 00:50:02,191
months.
Uh,

813
00:50:02,191 --> 00:50:06,080
have all these graphs on education and 
literacy has gone from 10 percent to 90 

814
00:50:07,841 --> 00:50:12,841
percent over a century and a health 
wealth,

815
00:50:15,140 --> 00:50:20,140
uh,
poverty's declined 95 percent in Asia 

816
00:50:20,140 --> 00:50:23,110
over the last 25 years was documentary 
about the World Bank.

817
00:50:23,620 --> 00:50:28,620
All these trends are very smoothly 
getting better and everybody thinks 

818
00:50:28,620 --> 00:50:28,620
things are getting,
but,

819
00:50:28,620 --> 00:50:29,540
but,
but you're right,

820
00:50:30,170 --> 00:50:34,910
like on violence,
that curve could be quite disruptive.

821
00:50:35,000 --> 00:50:38,270
There's an existential event,
uh,

822
00:50:38,600 --> 00:50:40,040
as I say,
I'm optimistic,

823
00:50:40,070 --> 00:50:45,070
but I think that is something we need to
deal with and a lot of it is not 

824
00:50:45,141 --> 00:50:50,141
technological,
it's dealing with our social cultural 

825
00:50:50,141 --> 00:50:50,141
institutions.

826
00:50:51,760 --> 00:50:56,170
So you mentioned also exponential growth
of software and Ivs,

827
00:50:56,171 --> 00:51:00,310
I guess related to software.
So one of the reasons for which you said

828
00:51:00,311 --> 00:51:05,311
that all that information technology is 
exponential is because of fundamental 

829
00:51:05,311 --> 00:51:08,350
properties of matter and energy.
But in the case of ideas,

830
00:51:08,380 --> 00:51:10,000
why would it have to be exponential?

831
00:51:10,950 --> 00:51:15,950
Well,
a lot of ideas produce exponential 

832
00:51:15,950 --> 00:51:18,800
gains.
They don't increase performance 

833
00:51:18,800 --> 00:51:23,241
linearly.
There's a case study during the Obama 

834
00:51:23,241 --> 00:51:26,660
Administration by the scientific 
advisory board on assessing this 

835
00:51:26,660 --> 00:51:31,071
question,
how much gains on 23 classical 

836
00:51:31,071 --> 00:51:35,271
engineering problems were gained through
hardware improvements over the last 

837
00:51:37,830 --> 00:51:42,830
decade and software improvement.
So there's about a thousand to one 

838
00:51:42,830 --> 00:51:45,450
improvement.
It's about doubling every year from 

839
00:51:45,450 --> 00:51:46,510
hardware that was an average of 
something like 26,000

840
00:51:46,560 --> 00:51:50,340
to one through software improvements,
algorithmic improvements.

841
00:51:52,380 --> 00:51:57,380
So we do see both ends.
Apparently if you come up with an 

842
00:51:57,380 --> 00:52:01,881
advance,
it doubles the performance so it 

843
00:52:01,881 --> 00:52:03,870
multiplies it by 10.
We see basically exponential growth from

844
00:52:03,871 --> 00:52:05,950
each innovation.
Um,

845
00:52:07,350 --> 00:52:12,350
so,
and we certainly see that in deep 

846
00:52:12,350 --> 00:52:12,350
learning.
Uh,

847
00:52:12,350 --> 00:52:15,980
the architectures are getting better 
while we also have more data and more 

848
00:52:15,980 --> 00:52:19,260
computation and more memory to throw at 
these at these algorithms.

849
00:52:19,610 --> 00:52:20,480
Thank you very much.

850
00:52:23,540 --> 00:52:26,210
Thank you.

