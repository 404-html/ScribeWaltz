1
00:00:00,170 --> 00:00:03,350
The following is a conversation
with Leslie Kale bling.

2
00:00:03,770 --> 00:00:06,080
She's a roboticist and professor at Mit.

3
00:00:06,590 --> 00:00:09,290
She's recognized for her work
and reinforcement learning,

4
00:00:09,530 --> 00:00:13,370
planning robot navigation and
several other topics in AI.

5
00:00:13,970 --> 00:00:18,860
She won the Edge Chi computers and thought
award and was the editor and chief of

6
00:00:18,861 --> 00:00:21,470
the prestigious journal
machine learning research.

7
00:00:22,520 --> 00:00:27,260
This conversation is part of
the artificial intelligence
podcasts at MIT and

8
00:00:27,261 --> 00:00:30,440
beyond.
If you enjoy subscribe on Youtube,

9
00:00:30,470 --> 00:00:34,520
iTunes or simply connect with
me on Twitter at Lex Friedman,

10
00:00:34,820 --> 00:00:36,530
spelled f, r I. D.

11
00:00:37,100 --> 00:00:41,180
And now here's my conversation
with Leslie Kale. Blaine.

12
00:00:42,880 --> 00:00:46,930
What made me get excited about Ai,
I can say that is I read girdle Escher,

13
00:00:46,931 --> 00:00:48,520
Bach when I was in high school.

14
00:00:48,820 --> 00:00:53,820
That was pretty formative
for me because it exposed a,

15
00:00:54,390 --> 00:00:59,390
the interesting ness of primitives and
combination and how you can make complex

16
00:01:00,201 --> 00:01:05,201
things out of simple parts and ideas
of AI and what kinds of programs might

17
00:01:06,080 --> 00:01:07,790
generate an intelligent behavior.
So,

18
00:01:07,940 --> 00:01:12,450
so you first fell in love with
AI reasoning, logic versus robot.

19
00:01:12,880 --> 00:01:16,240
Yeah, the robots came
because, um, my first job,

20
00:01:16,241 --> 00:01:20,590
so I finished an undergraduate degree in
philosophy at Stanford and was about to

21
00:01:20,591 --> 00:01:25,300
finish a master's in computer
science. And I got hired at Sri, uh,

22
00:01:25,480 --> 00:01:28,630
in their AI lab and they
were building a robot.

23
00:01:28,690 --> 00:01:31,120
It was a kind of a follow on to shaky,

24
00:01:31,121 --> 00:01:33,190
but all the shaky people
were not there anymore.

25
00:01:33,580 --> 00:01:37,270
And so my job was to try to get this robot
to do stuff and that's really kind of

26
00:01:37,271 --> 00:01:39,490
what got me interested in robots.

27
00:01:40,090 --> 00:01:43,240
So maybe taking a small step back,
your bachelors,

28
00:01:43,390 --> 00:01:46,300
Stanford Philosophy did Master's
and phd in computer science,

29
00:01:46,301 --> 00:01:50,170
but the bachelor's in philosophy as a,
what was that journey like?

30
00:01:50,171 --> 00:01:54,580
What elements of philosophy do you
think you bring to your work in computer

31
00:01:54,581 --> 00:01:55,220
science?

32
00:01:55,220 --> 00:01:56,790
So it's surprisingly relevant.

33
00:01:56,791 --> 00:02:00,720
So the part of the reason that I didn't
do a computer science undergraduate

34
00:02:00,721 --> 00:02:03,120
degree was that there wasn't
one at Stanford at the time,

35
00:02:03,540 --> 00:02:07,350
but that there's part of philosophy and
in fact Stanford has a special sub major

36
00:02:07,351 --> 00:02:11,970
in something called now symbolic systems,
which is logic model theory,

37
00:02:12,150 --> 00:02:14,340
formal semantics of national language.

38
00:02:14,460 --> 00:02:19,410
And so that's actually a perfect
preparation for working in AI and computer

39
00:02:19,411 --> 00:02:20,220
science

40
00:02:20,220 --> 00:02:21,410
that this is kind of interesting.

41
00:02:21,411 --> 00:02:24,200
So if you were interested
in artificial intelligence,

42
00:02:25,400 --> 00:02:25,770
yeah.

43
00:02:25,770 --> 00:02:29,730
The what, what kind of majors, where
people even thinking about taking,

44
00:02:29,731 --> 00:02:33,870
what does it in neuroscience was?
So besides philosophies, what,

45
00:02:33,871 --> 00:02:37,260
what were you supposed to do if you're
fascinated by the idea of creating

46
00:02:37,261 --> 00:02:37,860
intelligence,

47
00:02:37,860 --> 00:02:41,670
there weren't enough people who did that
for that even to be at conversation.

48
00:02:42,060 --> 00:02:46,290
I mean,
I think probably probably philosophy.

49
00:02:46,291 --> 00:02:48,030
I mean it's interesting in my class,

50
00:02:48,420 --> 00:02:51,810
my graduating class of
undergraduate philosophers,

51
00:02:51,811 --> 00:02:56,811
probably maybe slightly less than half
way done and computer science slightly

52
00:02:58,381 --> 00:03:02,500
less than half went in law and like
one or two we're not in philosophy.

53
00:03:03,020 --> 00:03:05,800
Uh,
so it was a common kind of connection.

54
00:03:05,900 --> 00:03:08,010
Do you think AI researchers have a role,

55
00:03:08,040 --> 00:03:11,540
be part time philosophers or should
they stick to the solid science and

56
00:03:11,541 --> 00:03:16,370
engineering without sort of taking
the philosophizing tangents? I mean,

57
00:03:16,371 --> 00:03:20,270
you work with robots, you think about what
it takes to create intelligent beings.

58
00:03:20,900 --> 00:03:24,980
Uh, aren't you the perfect person to
think about the big picture philosophy,

59
00:03:24,981 --> 00:03:25,520
if at all?

60
00:03:25,520 --> 00:03:27,980
The parts of philosophy that
are closest to AI I think,

61
00:03:27,981 --> 00:03:30,680
or at least the closest to AI
that I think about our stuff like

62
00:03:32,240 --> 00:03:37,190
belief and knowledge and deportation and
that kind of stuff and that's, you know,

63
00:03:37,191 --> 00:03:42,191
it's quite formal and it's like just
one step away from the kinds of computer

64
00:03:42,231 --> 00:03:44,180
science work that we do kind of routinely.

65
00:03:45,890 --> 00:03:50,890
I think that there are important questions
still about what you can do with a

66
00:03:52,551 --> 00:03:54,050
machine and what you can't and so on.

67
00:03:54,051 --> 00:03:58,040
Although at least my personal view is
that I'm completely a materialist and I

68
00:03:58,041 --> 00:04:03,041
don't think that there's any reason why
we can't make a robot be behaviourally

69
00:04:03,651 --> 00:04:04,970
indistinguishable from human.

70
00:04:05,750 --> 00:04:09,920
And the question of whether
it's distinguishable internally,

71
00:04:09,921 --> 00:04:13,820
whether it's a Zombie or not. In
philosophy terms, I actually don't,

72
00:04:14,930 --> 00:04:17,090
I don't know and I don't know
if I care too much about that.

73
00:04:17,230 --> 00:04:19,820
I would.
There is a philosophical notions,

74
00:04:19,970 --> 00:04:24,710
they're mathematical and philosophical
because we don't know so much of how

75
00:04:24,711 --> 00:04:27,440
difficult it is.
How difficult is it perception problem?

76
00:04:27,680 --> 00:04:29,360
How difficult is the planning problem?

77
00:04:29,660 --> 00:04:32,570
How difficult is it to operate
in this wall successfully?

78
00:04:32,810 --> 00:04:37,730
Because our robots are not currently as
successful as human beings in many tasks.

79
00:04:38,190 --> 00:04:43,190
The question about the gap between
current robots and human beings borders a

80
00:04:43,851 --> 00:04:46,910
little bit on philosophy.
Uh, you know, the,

81
00:04:47,390 --> 00:04:52,070
the expanse of knowledge that's required
to operate in this world and the

82
00:04:52,071 --> 00:04:54,410
ability to a form common sense knowledge,

83
00:04:54,440 --> 00:04:58,910
the ability to reason about uncertainty.
Much of the work you've been doing.

84
00:04:59,180 --> 00:05:03,430
There's, there's open questions
there that uh, I dunno,

85
00:05:03,490 --> 00:05:07,640
it required to activate a
certain big picture view.

86
00:05:07,950 --> 00:05:11,320
To me that doesn't seem like a
philosophical gap at all. To me it's a,

87
00:05:11,790 --> 00:05:14,430
there is a big technical
gap. Yes, huge technical gap,

88
00:05:15,150 --> 00:05:19,890
but I don't see any reason why it's
more than a technical gap. Perfect.

89
00:05:20,650 --> 00:05:23,350
So when you mentioned Ai,

90
00:05:23,351 --> 00:05:25,740
you mentioned Sri and uh,

91
00:05:26,350 --> 00:05:30,670
maybe can you describe to me when
you first fell in love with robotics,

92
00:05:30,870 --> 00:05:33,940
the robots or inspired a which,

93
00:05:34,120 --> 00:05:37,900
so you should maybe mention a
flaky or shaky, shaky, flaky.

94
00:05:38,470 --> 00:05:42,000
And what, what was the robot that
first captured your imagination?

95
00:05:42,020 --> 00:05:42,853
What's possible?

96
00:05:42,880 --> 00:05:43,531
Right.
Well those are,

97
00:05:43,531 --> 00:05:47,530
the first robot I worked with was like
shaky was there about that the Sri people

98
00:05:47,531 --> 00:05:51,010
had built. But by the time,
I think when I arrived,

99
00:05:51,011 --> 00:05:53,440
it was sitting in a corner
of somebodies office,

100
00:05:53,441 --> 00:05:56,740
dripping hydraulic fluid into a pan.
Uh,

101
00:05:57,050 --> 00:06:01,430
but it's iconic and really
everybody should read the
shaky Tucker report because

102
00:06:01,431 --> 00:06:03,080
it has so many good ideas in it.

103
00:06:03,110 --> 00:06:08,110
I mean they invented a star search and
symbolic planning and learning macro

104
00:06:09,741 --> 00:06:10,820
operators.

105
00:06:11,240 --> 00:06:16,220
They had a low level kind of configuration
space planning for the robot.

106
00:06:16,221 --> 00:06:20,360
They had vision, they had all this,
the basic ideas of a ton of things.

107
00:06:21,410 --> 00:06:24,500
Shaky have arms that,
what was the job? Cheeky.

108
00:06:25,580 --> 00:06:26,660
Cheeky was a mobile robot,

109
00:06:26,661 --> 00:06:31,661
but it could push objects and so it would
move things around which actually with

110
00:06:32,120 --> 00:06:35,590
its self, with spot, with his base. Um,

111
00:06:36,140 --> 00:06:40,340
so it could, but it, and they had
painted the base boards black.

112
00:06:41,010 --> 00:06:45,560
Uh, so it used, it used vision
to localize itself in a map.

113
00:06:45,561 --> 00:06:49,730
It detected objects. It could detect
objects that were surprising to it.

114
00:06:50,290 --> 00:06:50,600
Uh,

115
00:06:50,600 --> 00:06:55,340
it would plan and replan based on what
it saw it reasons about whether to look

116
00:06:55,341 --> 00:06:59,720
and take pictures.
I mean it really had the basics of,

117
00:07:00,330 --> 00:07:03,250
of so many of the things
that we think about now. Um,

118
00:07:03,470 --> 00:07:05,840
how did it represent the space around it?

119
00:07:06,260 --> 00:07:09,830
So it had representations that a bunch
of different levels of abstraction.

120
00:07:09,831 --> 00:07:14,150
So it had I think a kind of an occupancy
grid of some sort at the lowest level.

121
00:07:14,590 --> 00:07:17,450
Uh,
at the high level it was a abstract,

122
00:07:17,451 --> 00:07:22,410
symbolic kind of rooms and
connectivity does flaky coming? Yeah.

123
00:07:22,480 --> 00:07:25,460
Okay. So I should have, but Sri and the,

124
00:07:25,970 --> 00:07:28,310
we were building a brand new robot.
As I said,

125
00:07:28,311 --> 00:07:31,010
none of the people from
the previous project,

126
00:07:31,011 --> 00:07:32,660
we're kind of there are involved anymore.

127
00:07:32,660 --> 00:07:37,660
So we were kind of starting from scratch
and my advisor was standards and shine.

128
00:07:39,240 --> 00:07:44,210
He ended up being my thesis advisor
and he was motivated by this idea of

129
00:07:44,211 --> 00:07:46,910
situated computation
are situated automata.

130
00:07:46,940 --> 00:07:51,940
And the idea was that the tools of
logical reasoning were important,

131
00:07:53,840 --> 00:07:58,840
but possibly only for the engineers or
designers to use in the analysis of a

132
00:08:00,921 --> 00:08:05,780
system, but not necessarily
to be manipulated in the
head of the system itself.

133
00:08:05,810 --> 00:08:10,130
Right? So I might use logic to prove a
theorem about the behavior of my robot,

134
00:08:10,580 --> 00:08:13,580
even if the robot's not using logic
and it's had to prove their upstream.

135
00:08:13,610 --> 00:08:15,110
So that was kind of the distinction.

136
00:08:16,460 --> 00:08:21,460
And so the idea was to kind of use those
principles to make a robot do stuff.

137
00:08:22,880 --> 00:08:27,880
But a lot of the basic things we had to
kind of learn for ourselves because I

138
00:08:28,461 --> 00:08:31,550
had zero background in robotics,
I didn't know anything about control.

139
00:08:31,550 --> 00:08:32,750
I didn't know anything about sensors.

140
00:08:33,110 --> 00:08:36,830
So we reinvented a lot of wheels on the
way to getting that robot to do stuff.

141
00:08:36,831 --> 00:08:40,910
Do you think that was an advantage or
a hindrance? Oh No, it's, I mean, I,

142
00:08:40,970 --> 00:08:44,920
I'm big in favor of wheel
reinvention actually everyone,

143
00:08:44,950 --> 00:08:46,490
I think you learn a lot by doing it.

144
00:08:47,420 --> 00:08:51,710
It's important though to eventually
have the pointers to so that you can see

145
00:08:51,711 --> 00:08:56,490
what's really going on. But I think
you can appreciate much better the,

146
00:08:56,610 --> 00:08:59,880
the good solutions once you've messed
around a little bit on your own and found

147
00:08:59,881 --> 00:09:00,620
a bad one.

148
00:09:00,620 --> 00:09:00,771
Yeah.

149
00:09:00,771 --> 00:09:04,970
I think you've mentioned reinventing
reinforcement learning and referring to a

150
00:09:05,000 --> 00:09:06,590
rewards as pleasures.

151
00:09:07,020 --> 00:09:11,100
I have pleasure or I think,
which I think is a nice name for it.

152
00:09:12,930 --> 00:09:14,370
It's more,
it's more fun to almost,

153
00:09:14,930 --> 00:09:18,420
do you think you could
tell the history of AI,

154
00:09:18,490 --> 00:09:20,260
machine learning,
reinforcement learning,

155
00:09:20,500 --> 00:09:23,560
how you think about it
from the fifties to now?

156
00:09:23,700 --> 00:09:26,420
One thing is that it's oscillates,
right?

157
00:09:26,490 --> 00:09:30,390
So things become fashionable and then
they go out and then something else

158
00:09:30,391 --> 00:09:32,880
becomes cool on that goes out and so on.
And I think there's,

159
00:09:33,210 --> 00:09:37,560
so there's some interesting sociological
process that actually drives a lot of

160
00:09:37,561 --> 00:09:38,394
what's going on.

161
00:09:39,060 --> 00:09:42,990
Early days was kind of
cybernetics and control, right?

162
00:09:42,991 --> 00:09:46,380
And the idea that of homeostasis.

163
00:09:46,740 --> 00:09:49,710
People who've made these robots
that could, I don't know,

164
00:09:49,711 --> 00:09:54,480
try to plug into the wall when they
needed power and then come loose and roll

165
00:09:54,481 --> 00:09:55,410
around and do stuff.

166
00:09:55,440 --> 00:09:59,940
And then I think over time the thought,
well,

167
00:09:59,941 --> 00:10:01,620
that was inspiring, but
people said, no, no, no.

168
00:10:01,621 --> 00:10:04,920
We want to get maybe closer to what
feels like real intelligence or human

169
00:10:04,921 --> 00:10:05,754
intelligence.

170
00:10:07,830 --> 00:10:11,940
And then maybe the expert
systems people tried to do that,

171
00:10:13,530 --> 00:10:18,300
but maybe a little too
superficially. Right? So, oh,

172
00:10:18,301 --> 00:10:22,470
we get this surface understanding of
what intelligence is like because I

173
00:10:22,471 --> 00:10:25,800
understand how a steel mill works and I
can try to explain it to you and you can

174
00:10:25,801 --> 00:10:28,620
write it down in logic and then we
can make a computer in for that.

175
00:10:29,310 --> 00:10:33,990
And then that didn't work out.
But what's interesting,

176
00:10:33,991 --> 00:10:37,770
I think is when the thing starts
to not be working very well.

177
00:10:38,790 --> 00:10:42,480
It's not only do we change
methods, we change problems, right?

178
00:10:42,540 --> 00:10:45,960
So it's not like we have better ways
of doing the problem with the expert

179
00:10:45,961 --> 00:10:49,530
systems people were trying to do. We have
no ways of trying to do that problem.

180
00:10:50,310 --> 00:10:54,060
Oh yeah. I know. I think maybe a few,

181
00:10:54,330 --> 00:10:58,350
but we kind of give up on that problem
and we switched to a different problem

182
00:10:58,560 --> 00:11:03,350
and we, we worked that for a while
and we make progress as a community.

183
00:11:03,420 --> 00:11:03,571
Yeah.

184
00:11:03,571 --> 00:11:07,080
And there's a lot of people who would
argue you don't give up on the problem is

185
00:11:07,081 --> 00:11:09,930
just the you a decrease the
number of people working on it.

186
00:11:09,931 --> 00:11:11,640
You almost kind of like
put it on the shelf.

187
00:11:11,790 --> 00:11:13,890
So we'll come back to this 20 years later.

188
00:11:13,930 --> 00:11:18,310
Yeah, that's right. Or you might
decide that it's malformed.

189
00:11:18,340 --> 00:11:20,430
Like you might say,
okay,

190
00:11:21,700 --> 00:11:25,210
it's trying to just try to make
something that does superficial,

191
00:11:25,211 --> 00:11:27,310
symbolic reasoning,
behave like a doctor.

192
00:11:27,311 --> 00:11:32,311
You can't do that until you've had the
sensory motor experience of being a

193
00:11:33,431 --> 00:11:34,570
doctor or something.
Right.

194
00:11:34,571 --> 00:11:37,390
So there's arguments that say that
that's [inaudible] problem was not well

195
00:11:37,391 --> 00:11:39,980
formed where it could be
that it is well formed, but,

196
00:11:40,240 --> 00:11:42,460
but we just weren't approaching it.
Well,

197
00:11:43,120 --> 00:11:46,830
you mentioned mentioned that your favorite
part of logic and symbolic systems is

198
00:11:46,831 --> 00:11:51,810
that they give short names for large sets.
So there is some use to this.

199
00:11:52,020 --> 00:11:54,040
Uh,
they use some symbolic

200
00:11:54,040 --> 00:11:58,330
reasoning. So looking at expert
systems and symbolic computing,

201
00:11:58,610 --> 00:12:01,960
where do you think are the roadblocks that
were hit in the eighties and nineties?

202
00:12:02,350 --> 00:12:03,890
Ah, okay. So right.

203
00:12:03,891 --> 00:12:08,480
So the fact that I'm not a fan of expert
systems doesn't mean that I'm not a fan

204
00:12:08,481 --> 00:12:11,160
of some kinds of symbolic
reasoning. Right? So yeah,

205
00:12:13,190 --> 00:12:14,480
let's see.
Roadblocks and well,

206
00:12:14,481 --> 00:12:19,481
the main road block I think was that the
idea that humans could articulate their

207
00:12:20,421 --> 00:12:24,530
knowledge effectively
into, into, you know,

208
00:12:24,560 --> 00:12:26,180
some kind of logical statements.

209
00:12:26,410 --> 00:12:31,230
So it's not just the cost, the effort,
but really just the capability of doing it

210
00:12:31,330 --> 00:12:34,840
right. Because we're all
experts in vision, right? Yeah,

211
00:12:34,960 --> 00:12:38,530
totally don't have introspective
access into how we do that. Right.

212
00:12:39,040 --> 00:12:41,830
And it's true that,

213
00:12:43,840 --> 00:12:46,690
I mean, I think the idea was, well of
course even if people then would know,

214
00:12:46,691 --> 00:12:47,081
of course,

215
00:12:47,081 --> 00:12:50,140
I wouldn't ask you to please write down
the rules that you use for recognizing

216
00:12:50,160 --> 00:12:53,470
water bottle. That's crazy.
And everyone understood that.

217
00:12:53,471 --> 00:12:58,471
But we might ask you to please write
down the rules you use for deciding,

218
00:12:58,751 --> 00:13:03,751
I dunno what tie to put on or how to set
up a microphone or something like that.

219
00:13:04,780 --> 00:13:08,530
But even those things,
I think people maybe,

220
00:13:09,040 --> 00:13:11,260
I think what they found,
I'm not sure about this,

221
00:13:11,261 --> 00:13:14,860
but I think what they found was that
the so called experts could give

222
00:13:14,861 --> 00:13:19,780
explanations that sort of
post hoc explanations for
how and why they did things,

223
00:13:19,781 --> 00:13:23,830
but they weren't necessarily
very good and then they differ.

224
00:13:24,190 --> 00:13:29,070
They depended on maybe some
kinds of perceptual things,

225
00:13:29,071 --> 00:13:33,360
which again, they couldn't really
define very well. So I think,

226
00:13:33,450 --> 00:13:38,220
I think fundamentally I think that the
underlying problem with that was the

227
00:13:38,221 --> 00:13:42,060
assumption that people could articulate
how and why they make their decisions.

228
00:13:43,000 --> 00:13:46,670
Right. So it's almost I
encoding the knowledge, uh,

229
00:13:47,590 --> 00:13:51,100
converting from experts is something
that a machine can understand and reason

230
00:13:51,101 --> 00:13:51,530
with.

231
00:13:51,530 --> 00:13:56,470
No, no, no, no. Not even just in coding,
but getting it out of you. Right.

232
00:13:57,010 --> 00:13:59,650
Not, not, not writing
it down. I mean, yes,

233
00:13:59,740 --> 00:14:01,750
hard also to write it
down for the computer,

234
00:14:02,410 --> 00:14:04,930
but I don't think that
people can produce it.

235
00:14:05,740 --> 00:14:09,490
You can tell me a story about why
you do stuff, but I'm not so sure.

236
00:14:09,491 --> 00:14:12,010
That's the Y.
Great.

237
00:14:12,150 --> 00:14:17,150
So there are still on the
hierarchical planning side

238
00:14:18,690 --> 00:14:23,500
places where symbolic reasoning
is very useful. So, um, as,

239
00:14:23,530 --> 00:14:25,500
as you've talked about,
so yeah,

240
00:14:27,840 --> 00:14:30,660
where, where's the gap? Okay, good.

241
00:14:30,840 --> 00:14:35,840
So saying that humans can't provide a
description of their reasoning processing

242
00:14:36,401 --> 00:14:39,180
these, that's okay, fine.

243
00:14:39,181 --> 00:14:42,540
But that doesn't mean that it's not
good to do reasoning of various styles

244
00:14:42,541 --> 00:14:45,060
inside a computer.
Those are just two orthogonal points.

245
00:14:45,810 --> 00:14:50,700
So then the question is what kind of
reasoning should you do inside a computer?

246
00:14:50,701 --> 00:14:51,780
Right?
Uh,

247
00:14:52,340 --> 00:14:55,820
and the answer is I think you need to do
all different kinds of reasoning inside

248
00:14:55,821 --> 00:14:59,570
a computer depending on what
kinds of problems you face.

249
00:14:59,870 --> 00:15:04,870
I guess the question is what kinds
of things can you encode Incode,

250
00:15:06,300 --> 00:15:08,060
symbolic,
the seat can reason about,

251
00:15:08,240 --> 00:15:12,920
I think the idea about an end,

252
00:15:12,930 --> 00:15:14,240
even symbolic.

253
00:15:14,510 --> 00:15:17,570
I don't even like that terminology
because they don't know what it means

254
00:15:17,600 --> 00:15:21,470
technically. Informally. I
do believe in abstractions.

255
00:15:21,750 --> 00:15:23,450
Abstractions are critical,
right?

256
00:15:23,510 --> 00:15:28,490
You cannot reason at completely fine
grain about everything in your life,

257
00:15:28,580 --> 00:15:28,880
right?

258
00:15:28,880 --> 00:15:33,170
You can't make a plan at the level of
images and torques for getting a phd.

259
00:15:33,230 --> 00:15:34,063
Right?

260
00:15:34,100 --> 00:15:38,750
So you have to reduce the size of the
state space and you have to reduce the

261
00:15:38,770 --> 00:15:43,250
horizon if you're going to reason
about getting a phd or even buying the

262
00:15:43,251 --> 00:15:45,710
ingredients to make dinner.
And so,

263
00:15:45,950 --> 00:15:50,950
so how can you reduce the spaces and the
horizon of the reasoning you have to do

264
00:15:51,021 --> 00:15:54,020
and the answer's abstractions, spacial
extraction and temporal abstraction.

265
00:15:54,380 --> 00:15:57,380
I think abstraction along the
lines of goals is also interesting.

266
00:15:57,381 --> 00:15:59,930
Like you might or,
well,

267
00:15:59,931 --> 00:16:03,920
abstraction in de composition goals is
maybe more of a decomposition thing.

268
00:16:04,520 --> 00:16:06,440
So I think that's where these kinds of,

269
00:16:06,680 --> 00:16:10,730
if you want to call it symbolic
or discrete models come in you,

270
00:16:10,790 --> 00:16:15,710
you talk about a room of your house
instead of your pose. You talk about,

271
00:16:16,040 --> 00:16:20,270
uh, you know, doing something during
the afternoon instead of at two 54.

272
00:16:20,900 --> 00:16:25,490
And you do that because it makes you
reasoning problem easier. And also because

273
00:16:27,140 --> 00:16:27,973
you have,

274
00:16:28,670 --> 00:16:33,670
you don't have enough information to
reason in high fidelity about your pose of

275
00:16:34,671 --> 00:16:39,590
your elbow at two 35 this afternoon
anyway. Right? When you try to get a phd,

276
00:16:40,600 --> 00:16:44,510
you're doing anything except for
at that moment. At that moment,

277
00:16:44,540 --> 00:16:47,330
you do have to reason about the
posterior elbow maybe, but then you,

278
00:16:47,331 --> 00:16:51,980
maybe you do that and some continuous
joint space kind of model it. So again, I,

279
00:16:53,590 --> 00:16:56,530
my biggest point about all of
this is that there should be,

280
00:16:56,590 --> 00:16:59,980
the dogma is not the
thing, right? We shouldn't,

281
00:17:00,010 --> 00:17:03,370
it shouldn't be that I am in favor or
against symbolic reasoning and you're in

282
00:17:03,371 --> 00:17:06,760
favor against neural networks.
It should be that just,

283
00:17:07,440 --> 00:17:11,530
just computer science tells us what the
right answer to all these questions is.

284
00:17:11,531 --> 00:17:12,940
If we were smart enough to figure it out.

285
00:17:13,270 --> 00:17:16,100
Yeah. When you tried to actually
solve the problem with computers,

286
00:17:16,450 --> 00:17:20,150
the right answer comes out. Uh,
you mentioned abstractions. I mean,

287
00:17:20,151 --> 00:17:24,860
you all know works form abstractions
or, uh, rather. Uh, there's,

288
00:17:25,280 --> 00:17:29,120
there's automated ways to farm strategies
and there's expert driven waste to

289
00:17:29,121 --> 00:17:32,540
forums tractions and a
expert human driven ways.

290
00:17:33,080 --> 00:17:37,190
Humans just seems to be way better at
forming abstractions currently and certain

291
00:17:37,191 --> 00:17:38,024
problems.

292
00:17:38,210 --> 00:17:43,210
So when you're referring to
2:45 PM versus afternoon,

293
00:17:45,020 --> 00:17:47,210
how do we construct that taxonomy?

294
00:17:47,330 --> 00:17:52,020
Is there any room for automated
construction of such abstractions?

295
00:17:52,360 --> 00:17:53,800
Oh, I think eventually, yeah. I mean,

296
00:17:53,801 --> 00:17:58,030
I think when we get to be
better and machine learning,

297
00:17:58,031 --> 00:18:01,870
engineers will build algorithms
that bill awesome extractions

298
00:18:02,330 --> 00:18:04,590
that are useful in this kind of
way that you're describing. Yeah.

299
00:18:05,150 --> 00:18:08,630
So let's then step from the,

300
00:18:09,250 --> 00:18:14,250
the abstraction discussion and let's talk
about a bomb MDPs partially observable

301
00:18:16,861 --> 00:18:19,050
mark decision prophecies.
So uncertainty.

302
00:18:19,350 --> 00:18:24,350
So first watermark off decision procesees
what are America decision process and

303
00:18:24,660 --> 00:18:29,380
maybe how much of our world can be
models and MDPs how much when you,

304
00:18:29,390 --> 00:18:31,800
when you wake up in the morning,
he making breakfast, how do you,

305
00:18:31,801 --> 00:18:33,690
do you think of yourself as an MDP?

306
00:18:34,090 --> 00:18:37,920
So how do you think about a MDPs
and how they relate to our world?

307
00:18:38,210 --> 00:18:41,420
Well, so there's a stance question, right?

308
00:18:41,421 --> 00:18:44,150
So a stance is a position that I
take with respect to their problem.

309
00:18:44,660 --> 00:18:49,660
So I as a researcher or a
person who designed systems
can decide to make a model

310
00:18:52,731 --> 00:18:55,790
of the world around me in
some terms and the right.

311
00:18:55,791 --> 00:19:00,791
So I take this messy world and I say
I'm going to treat it as if it were a

312
00:19:00,951 --> 00:19:02,450
problem of this formal kind.

313
00:19:02,750 --> 00:19:06,350
And then I can apply solution concepts
or algorithms or whatever to solve that

314
00:19:06,351 --> 00:19:09,650
formal thing, right? So of
course the world is not anything,

315
00:19:09,651 --> 00:19:12,350
it's not an MDP or a palm DP.
I don't know what it is,

316
00:19:12,351 --> 00:19:15,560
but I can model aspects of it
in some way or some other way.

317
00:19:15,890 --> 00:19:18,740
And when I model some aspect
of it in a certain way,

318
00:19:18,741 --> 00:19:20,990
that gives me some set
of algorithms I can use.

319
00:19:21,280 --> 00:19:24,790
You can model the world and all
kinds of ways. Yeah. Uh, some have,

320
00:19:25,300 --> 00:19:29,680
some are more accepting of uncertainty,

321
00:19:29,980 --> 00:19:32,080
more easily modeling
uncertainty of the world.

322
00:19:32,081 --> 00:19:35,080
Some really forced the
world to be deterministic.

323
00:19:35,980 --> 00:19:40,660
And so there's certainly MDPs, uh,
model the uncertainty of the world.

324
00:19:40,830 --> 00:19:45,600
Yes. Model some uncertainty. They
model not present state uncertainty,

325
00:19:45,630 --> 00:19:49,380
but they model uncertainty in the
way of the future will unfold, right?

326
00:19:49,520 --> 00:19:53,750
Yep. So what are mark off
decision process? Okay,

327
00:19:53,950 --> 00:19:55,670
so Margaret decision process as a model.

328
00:19:55,770 --> 00:19:57,780
It's a kind of a model that
you could make that says,

329
00:19:57,810 --> 00:20:02,810
I know completely the current state
of my system and what it means to be a

330
00:20:03,361 --> 00:20:05,520
state. Is that I, that all the info,

331
00:20:05,550 --> 00:20:08,940
I have all the information right now that
will let me make predictions about the

332
00:20:08,941 --> 00:20:13,500
future as well as I can so
that remembering anything
about my history wouldn't

333
00:20:13,501 --> 00:20:18,150
make my predictions any better. Um, and,

334
00:20:18,420 --> 00:20:18,850
but,

335
00:20:18,850 --> 00:20:22,890
but then it also says that then I can
take some actions that might change the

336
00:20:22,891 --> 00:20:26,790
state of the world and that I don't have
a deterministic model of those changes.

337
00:20:26,791 --> 00:20:31,020
I have a probabilistic model of
how the world might change. Uh,

338
00:20:31,470 --> 00:20:35,600
it's a, it's a useful model for some
kinds of systems. I think it's a, I mean,

339
00:20:35,610 --> 00:20:40,610
it's certainly not a good model
for most problems I think.

340
00:20:42,001 --> 00:20:45,900
Because for most problems that you
don't actually know the state, uh,

341
00:20:45,960 --> 00:20:48,250
for most problems you,
it's partially observed.

342
00:20:48,610 --> 00:20:51,190
So that's now a different problem class.

343
00:20:51,690 --> 00:20:52,540
So the,
okay,

344
00:20:52,560 --> 00:20:57,180
that's where the poverty
piece of partially absorbed
Mako decision process step.

345
00:20:57,181 --> 00:21:02,181
And so how do they address the
fact that you can't observe most,

346
00:21:02,730 --> 00:21:05,940
uh, you have incomplete information
about most of the world around you,

347
00:21:06,040 --> 00:21:10,840
right? So now the idea is we still kind
of postulate that there exists a state.

348
00:21:10,870 --> 00:21:15,400
We think that there is some information
about the world out there such that if

349
00:21:15,401 --> 00:21:19,000
we knew that we could make good
predictions but we don't know the state.

350
00:21:19,540 --> 00:21:23,080
And so then we have to think about how,
but we do get observations.

351
00:21:23,081 --> 00:21:24,580
Maybe I get images right here.

352
00:21:24,581 --> 00:21:29,581
Things are I feel things and those
might be local or noisy and so therefore

353
00:21:29,801 --> 00:21:31,480
they're going to tell me
everything about what's going on.

354
00:21:31,481 --> 00:21:36,160
And then I have to reason about,
given the history of actions,

355
00:21:36,161 --> 00:21:40,390
I've taken an observations, I've gotten,
what do I think is going on in the world?

356
00:21:40,391 --> 00:21:43,450
And then given my own kind of uncertainty
about what's going on in the world,

357
00:21:43,451 --> 00:21:44,860
I can decide what actions to take.

358
00:21:45,240 --> 00:21:50,240
And so difficult is this
problem of planning under
uncertainty in your view and

359
00:21:50,731 --> 00:21:54,240
you long experience of modeling the world,

360
00:21:54,750 --> 00:21:59,520
trying to deal with this uncertainty
in suspicion, real wall systems,

361
00:21:59,980 --> 00:22:04,980
dumb all planning for even discrete palm
dps can be undecidable depending on how

362
00:22:05,921 --> 00:22:10,760
you set it up and for,
so lots of people say,

363
00:22:10,790 --> 00:22:13,910
I don't use palm [inaudible]
because they are intractable.

364
00:22:14,780 --> 00:22:19,190
And I think that that's our kind of
a very funny thing to say because the

365
00:22:19,191 --> 00:22:21,920
problem you have to solve is
the problem you have to solve.

366
00:22:22,130 --> 00:22:24,260
So if the problem you have
to solve as intractable,

367
00:22:24,470 --> 00:22:27,490
that's what makes us AI
people, right? So, uh, we saw,

368
00:22:27,500 --> 00:22:29,560
we understand that the
problem we're solving is,

369
00:22:29,610 --> 00:22:32,510
is complete wildly
intractable that we can't,

370
00:22:32,511 --> 00:22:36,580
we will never be able to solve it
optimally. At least I don't. Yeah. Right.

371
00:22:36,650 --> 00:22:41,650
So later we can come back to an idea
about bounded optimality something.

372
00:22:42,081 --> 00:22:45,110
But anyway, we can't come up with
optimal solutions to these problems.

373
00:22:45,590 --> 00:22:47,720
So we have to make approximations,

374
00:22:47,780 --> 00:22:51,890
approximations in modeling approximations
in solution algorithms and so on.

375
00:22:51,891 --> 00:22:56,180
And so I don't have a problem with saying,
yeah,

376
00:22:56,181 --> 00:22:59,930
my problem actually it is a palm DP
in continuous space with continuous

377
00:22:59,931 --> 00:23:04,580
observations in, it's so computationally
complex. I can't even think about it's,

378
00:23:04,640 --> 00:23:06,290
you know,
big or whatever.

379
00:23:07,880 --> 00:23:10,460
But that doesn't prevent me from,
it helps me,

380
00:23:10,490 --> 00:23:15,490
gives me some clarity to think about it
that way and to then take steps to make

381
00:23:16,880 --> 00:23:20,210
approximation after approximation
to get down to something that's like

382
00:23:20,211 --> 00:23:22,130
computable in some reasonable time.

383
00:23:22,270 --> 00:23:23,660
When you think about optimality,

384
00:23:24,340 --> 00:23:29,340
the community broadly is shifted on that
I think a little bit in how much they

385
00:23:29,711 --> 00:23:34,711
value the idea of a optimality
of chasing an optimal solution.

386
00:23:35,810 --> 00:23:40,810
How is of use of chasing
an optimal solution changed
over the years and when you

387
00:23:41,231 --> 00:23:42,250
work with robots?

388
00:23:42,440 --> 00:23:43,273
That's interesting.

389
00:23:43,390 --> 00:23:48,390
I think we have a little bit of a meth
for the logic go crisis actually from the

390
00:23:50,421 --> 00:23:53,660
theoretical side. I mean, I do think that
there is important in that right now.

391
00:23:53,661 --> 00:23:55,820
We're not doing much of it.

392
00:23:56,990 --> 00:24:01,010
So there's lots of empirical
hacking around and training
this and doing that and

393
00:24:01,070 --> 00:24:03,530
reporting numbers. But
is it good? Is it bad?

394
00:24:03,531 --> 00:24:05,660
We don't know if there's
very hard to say things.

395
00:24:08,360 --> 00:24:10,460
And if you look,
uh,

396
00:24:10,520 --> 00:24:14,150
like computer science theory,

397
00:24:14,151 --> 00:24:18,050
so people talk for a while, everyone
was about solving problems optimally.

398
00:24:18,051 --> 00:24:22,370
You're completely and, and then there
were interesting relaxations. Wait,

399
00:24:22,371 --> 00:24:23,780
so people look at,
Oh,

400
00:24:23,781 --> 00:24:27,770
can I are their regret bounds
or can I do some kind of,

401
00:24:28,250 --> 00:24:29,420
um,
you know,

402
00:24:29,421 --> 00:24:33,130
approximation can improve something that
I can approximately solve this problem

403
00:24:33,131 --> 00:24:36,110
or that I get closer to the solution
as I spend more time and so on.

404
00:24:36,620 --> 00:24:37,580
What's interesting,

405
00:24:37,640 --> 00:24:42,640
I think is that we don't have good
approximate solution concepts for very

406
00:24:46,101 --> 00:24:50,360
difficult problems. Right? I like
to, you know, I like to say that I,

407
00:24:50,510 --> 00:24:54,410
I'm interested in doing a very
bad job of very big problems. Uh,

408
00:24:55,040 --> 00:24:59,570
uh, right. So very big, very big problems.

409
00:24:59,571 --> 00:25:03,800
I like to do that, but I would,
I wish I could say something.

410
00:25:04,310 --> 00:25:06,410
I wish I had a,
I don't know,

411
00:25:06,411 --> 00:25:11,411
some kind of a of a formal solution
concept that I could use to say,

412
00:25:12,501 --> 00:25:16,550
oh, this, this algorithm actually
it gives me something like,

413
00:25:16,551 --> 00:25:17,570
I know what I'm going to get.

414
00:25:17,660 --> 00:25:20,720
I can do something other than
just run it and get out six seven.

415
00:25:20,800 --> 00:25:24,790
The notion is still somewhere
deeply compelling to you. Hmm.

416
00:25:24,930 --> 00:25:29,930
The notion that you can say you can
drop thing on the table says this,

417
00:25:30,440 --> 00:25:33,460
that you can expect this, this
algorithm and gave me some good results.

418
00:25:34,380 --> 00:25:39,090
I hope science will, I mean there's
engineering and there's science.

419
00:25:39,091 --> 00:25:41,100
I think that they're not exactly the same.

420
00:25:42,450 --> 00:25:46,770
And I think right now we're making
huge engineering like leaps and bounds.

421
00:25:46,771 --> 00:25:50,160
So the engineering is running way
ahead of the science, which is cool.

422
00:25:50,670 --> 00:25:52,290
And often how it goes,
right?

423
00:25:52,291 --> 00:25:55,320
So we're making things and nobody
knows how and why they work roughly,

424
00:25:56,370 --> 00:26:00,660
but we need to turn that into science

425
00:26:00,820 --> 00:26:05,640
was some form. There's some room for
formalizing. We need to know what the

426
00:26:05,880 --> 00:26:08,250
Bulls are. Why does this, why
or why does that not work?

427
00:26:08,251 --> 00:26:11,190
I mean for awhile people
build bridges by trying,

428
00:26:11,191 --> 00:26:14,580
but now we can often predict whether it's
going to work it out without building

429
00:26:14,581 --> 00:26:16,350
it.
Can we do that

430
00:26:16,500 --> 00:26:18,630
for learning?
Systems are for robots.

431
00:26:18,780 --> 00:26:23,730
So your hope is from a materialistic
perspective, that intelligence,

432
00:26:23,731 --> 00:26:28,290
artificial intelligence systems, robots,
a Chi. I would just fancier bridges,

433
00:26:29,310 --> 00:26:33,000
belief space. What's the difference
between belief space in state space?

434
00:26:33,001 --> 00:26:36,730
Who mentioned MDPS MDPs you reasoning,
uh,

435
00:26:37,530 --> 00:26:42,090
about you sense the world. There's
a state, uh, w w what's this belief?

436
00:26:42,091 --> 00:26:43,700
Space idea. Yeah. Okay.

437
00:26:44,520 --> 00:26:47,940
That sounds good.
So I believe space that is,

438
00:26:48,720 --> 00:26:52,680
instead of thinking about what's the
state of the world and trying to control

439
00:26:52,681 --> 00:26:53,940
that as a robot,

440
00:26:54,360 --> 00:26:59,360
I think about what is the space of
beliefs that I could have about the world.

441
00:26:59,881 --> 00:27:00,160
What's,

442
00:27:00,160 --> 00:27:04,110
if I think of a belief as a probability
distribution of her ways the world could

443
00:27:04,111 --> 00:27:06,870
be a belief state is a distribution.

444
00:27:07,560 --> 00:27:09,450
And then my control problem,

445
00:27:09,451 --> 00:27:14,451
if I'm reasoning about how to move
through a world I'm uncertain about my

446
00:27:14,521 --> 00:27:17,460
control problem is actually the
problem with controlling my beliefs.

447
00:27:17,670 --> 00:27:19,590
So I think about taking actions,

448
00:27:20,160 --> 00:27:22,140
not just what effect they'll
have on the world outside,

449
00:27:22,141 --> 00:27:25,290
but what effect I'll have on my own
understanding of the world outside.

450
00:27:25,291 --> 00:27:30,291
And so that might compel me to ask a
question or look somewhere together,

451
00:27:31,050 --> 00:27:34,320
information, which may not
really change the world state,

452
00:27:34,770 --> 00:27:36,690
but it changes my own
belief about the world.

453
00:27:37,490 --> 00:27:38,780
That's a powerful way to,

454
00:27:40,640 --> 00:27:43,850
to empower the agent to
reason about the world,

455
00:27:43,851 --> 00:27:48,140
explore the world to what kind of
problems does that allow you to solve, to,

456
00:27:48,500 --> 00:27:51,830
to uh, consider belief space
versus just stay space?

457
00:27:52,530 --> 00:27:56,400
Well, any problem that requires
deliberate information gathering, right?

458
00:27:56,430 --> 00:27:59,310
So if in some problems,

459
00:28:00,250 --> 00:28:00,740
okay,

460
00:28:00,740 --> 00:28:01,431
like chess,

461
00:28:01,431 --> 00:28:04,490
there's no uncertainty or maybe
there's uncertainty about the opponent.

462
00:28:05,240 --> 00:28:09,740
There's no uncertainty about the
state. Uh, and some problems,

463
00:28:09,741 --> 00:28:14,510
there's uncertainty, but you
gather information as you
go, right? You might say, Oh,

464
00:28:14,511 --> 00:28:17,990
I'm driving my autonomous car down the
road and it doesn't know perfectly where

465
00:28:17,991 --> 00:28:20,240
it is, but the lidars are
all going on at the time.

466
00:28:20,630 --> 00:28:23,390
So I don't have to think about
whether to gather information.

467
00:28:24,350 --> 00:28:26,030
But if you're a human
driving down the road,

468
00:28:26,210 --> 00:28:30,290
you sometimes look over your shoulder
to see what's going on behind you in the

469
00:28:30,291 --> 00:28:35,291
lane and you have to decide whether you
should do that now and you have to trade

470
00:28:37,191 --> 00:28:40,280
off the fact that you're not seeing in
front of you when you're looking behind

471
00:28:40,281 --> 00:28:42,800
you. And how valuable is
that information and so on.

472
00:28:43,220 --> 00:28:47,780
And so to make choices about information
gathering, you have to reasonably space

473
00:28:50,310 --> 00:28:51,300
also also,

474
00:28:51,360 --> 00:28:56,360
I mean also too just take into account
your uncertainty before trying to do

475
00:28:57,091 --> 00:28:59,610
things.
So you might say,

476
00:28:59,700 --> 00:29:04,110
if I understand where I'm
standing relative to the door jam,

477
00:29:04,580 --> 00:29:07,830
uh, pretty accurately, then it's
okay for me to go through the door.

478
00:29:07,860 --> 00:29:09,990
But if I'm really not
sure where the door is,

479
00:29:09,991 --> 00:29:12,960
then it might be better to not do that.
Right now.

480
00:29:13,110 --> 00:29:17,220
The degree of your uncertainty ball
about the world is actually part of the

481
00:29:17,221 --> 00:29:19,870
thing you're trying to
optimize, informing, forming
the plan. Right. Gotcha.

482
00:29:21,000 --> 00:29:26,000
So this idea of a long horizon of a
planning for a phd or just even how to get

483
00:29:26,581 --> 00:29:28,770
out of the house or how
to make breakfast here,

484
00:29:28,771 --> 00:29:30,980
that you show this presentation of the,

485
00:29:31,010 --> 00:29:35,340
the WTF or is the fork of
robot looking at the sink?

486
00:29:35,600 --> 00:29:37,580
Uh,
and uh,

487
00:29:38,340 --> 00:29:42,610
can you describe how we plan in this
world is idea of hierarchical planning?

488
00:29:42,940 --> 00:29:46,120
We've mentioned this, this is, yeah.

489
00:29:46,121 --> 00:29:50,230
How can a robot hope to plan
about something? Uh, oh,

490
00:29:50,320 --> 00:29:54,070
this was such a long horizon
where the goal is quite far away.

491
00:29:54,600 --> 00:29:59,600
People since probably reasoning began
how thought about hierarchical reasoning,

492
00:29:59,960 --> 00:30:02,700
the temporal hierarchy and predict,
well, their spacial hierarchy.

493
00:30:02,701 --> 00:30:07,670
But let's talk about temporal
hierarchy. So you might say,
oh, I have this long, uh,

494
00:30:07,710 --> 00:30:09,150
execution I have to do,

495
00:30:09,151 --> 00:30:13,290
but I can divide it into some
segments abstractly, right?

496
00:30:13,291 --> 00:30:16,650
So maybe you have to get out of the house.
I have to get in the car,

497
00:30:16,651 --> 00:30:18,360
I have to drive and so on.

498
00:30:18,930 --> 00:30:23,850
And so you can plan if you can
build abstractions. So this,

499
00:30:23,851 --> 00:30:27,060
we started out by talking about
abstractions and we're back to that now.

500
00:30:27,061 --> 00:30:32,061
If you can build abstractions in
your state space and abstractions,

501
00:30:33,330 --> 00:30:34,800
sort of temporal abstractions,

502
00:30:35,280 --> 00:30:38,340
then you can make plans at a
high level and you can say,

503
00:30:38,341 --> 00:30:42,180
I'm going to go to town and then I'll
have to get gas and then I can go here and

504
00:30:42,181 --> 00:30:45,690
I can do this other thing and you
can reason about the dependencies and

505
00:30:45,691 --> 00:30:48,420
constraints among these actions.
Again,

506
00:30:48,421 --> 00:30:51,450
without thinking about
the complete details,

507
00:30:52,620 --> 00:30:56,730
what we do in our hierarchical
planning work is then say, all right,

508
00:30:56,731 --> 00:30:58,650
I make a plan at a high
level of abstraction.

509
00:30:59,580 --> 00:31:04,580
I have to have some reason to think that
it's feasible without working it out in

510
00:31:04,861 --> 00:31:08,190
complete detail and that's
actually the interesting step.

511
00:31:08,191 --> 00:31:10,410
I always liked to talk about
walking through an airport,

512
00:31:10,710 --> 00:31:15,710
like you can plan to go to New York
and arrive at the airport and then find

513
00:31:16,051 --> 00:31:17,550
yourself in a office building later.

514
00:31:18,360 --> 00:31:21,240
You can't even tell me in advance what
your plan is for walking through the

515
00:31:21,241 --> 00:31:25,140
airport. Partly because you're
too lazy to think about it maybe,

516
00:31:25,141 --> 00:31:27,200
but partly also because you
just don't have the information.

517
00:31:27,220 --> 00:31:31,350
You don't know what get your landing in
or what people are going to be in front

518
00:31:31,351 --> 00:31:35,820
of you or anything.
So there's no point in planning in detail,

519
00:31:36,180 --> 00:31:37,350
but you have to have,

520
00:31:38,070 --> 00:31:42,360
you have to make a leap of faith that
you can figure it out once you get there.

521
00:31:43,170 --> 00:31:47,880
And it's really interesting
to me how you arrive at that.

522
00:31:48,810 --> 00:31:53,160
How do you say you have learned over your
lifetime to be able to make some kinds

523
00:31:53,161 --> 00:31:57,030
of predictions about how hard it is
to achieve some kinds of sub goals.

524
00:31:57,630 --> 00:32:01,680
And that's critical. Like you would never
plan to fly somewhere if you couldn't,

525
00:32:02,100 --> 00:32:04,980
didn't have a model of how hard it was
to do some of the intermediate steps.

526
00:32:05,310 --> 00:32:08,460
So one of the things we're thinking about
now is how do you do this kind of very

527
00:32:08,461 --> 00:32:12,320
aggressive generalization,
uh,

528
00:32:12,640 --> 00:32:16,470
we to situations that you haven't been
in it and so on to predict how long will

529
00:32:16,471 --> 00:32:19,740
it take to walk through the Kuala Lumpur
airport like you can you give me an

530
00:32:19,741 --> 00:32:21,150
estimate and it wouldn't be crazy.

531
00:32:21,660 --> 00:32:26,660
And you have to have an estimate of
that in order to make plans that involve

532
00:32:26,880 --> 00:32:28,560
walking through the qual on port airport,

533
00:32:28,590 --> 00:32:30,240
even if you don't need
to know it in detail.

534
00:32:31,050 --> 00:32:35,010
So I really interested in these kinds
of abstract models and how do we acquire

535
00:32:35,011 --> 00:32:39,410
them, but once we have them, we can use
them to do hierarchical reasoning is,

536
00:32:39,411 --> 00:32:40,250
I think it's very important.

537
00:32:40,360 --> 00:32:40,631
Yeah.

538
00:32:40,631 --> 00:32:45,631
There's this notion of go a
goal regression and pretty
image back chaining this

539
00:32:46,751 --> 00:32:49,350
idea of starting at the
goal and it's just for me,

540
00:32:49,390 --> 00:32:53,610
big clouds of states you get,

541
00:32:53,620 --> 00:32:58,180
I mean it's almost like saying to
the airport, you know, you know,

542
00:32:58,181 --> 00:33:02,770
once you show up to the,
uh, the airport did,

543
00:33:02,800 --> 00:33:05,500
that's you, you're like a
few steps away from the goal.

544
00:33:05,501 --> 00:33:08,710
So like thinking of it this way,
uh, it's kind of interesting.

545
00:33:08,711 --> 00:33:12,990
I don't know if you have sort of
a further comments on that, uh,

546
00:33:13,010 --> 00:33:14,690
of starting at the goal,
right?

547
00:33:14,880 --> 00:33:19,260
Yeah. I mean, it's interesting
that Simon Herb Simon,

548
00:33:19,261 --> 00:33:20,620
back in the early days,

549
00:33:20,621 --> 00:33:24,630
the way I talked a lot about means ends
reasoning and reasoning back from the

550
00:33:24,631 --> 00:33:28,110
goal. There's a kind of an
intuition that people have that

551
00:33:30,030 --> 00:33:33,150
the number of state space is big.

552
00:33:33,151 --> 00:33:36,630
The number of actions you could
take is really big. So if you say,

553
00:33:36,631 --> 00:33:38,910
here I sit and I want to
search forward from where I am,

554
00:33:38,911 --> 00:33:42,420
what are all the things I could do
that's just overwhelming. If you say,

555
00:33:42,421 --> 00:33:46,620
if you can reason at this other level and
say, here's what I'm hoping to achieve,

556
00:33:46,621 --> 00:33:50,700
what can I do to make that true?
That somehow the branching is smaller.

557
00:33:51,300 --> 00:33:55,740
Now what's interesting is that like in
the AI planning community that hasn't

558
00:33:55,770 --> 00:33:57,480
worked out in the class of problems,

559
00:33:57,481 --> 00:33:59,690
that they look at it and the
methods that they tend to use it,

560
00:33:59,700 --> 00:34:02,830
how's it turned out that it's
better to go backward? Um,

561
00:34:03,270 --> 00:34:05,100
it's still kind of my
intuition that it is,

562
00:34:05,101 --> 00:34:08,430
but I can't prove that to you right now.
Right?

563
00:34:09,030 --> 00:34:12,250
Tuition, at least for
us mere humans. Uh Huh.

564
00:34:13,830 --> 00:34:18,590
Speaking of which, uh, when you, uh,
maybe I would take it and take a look,

565
00:34:18,720 --> 00:34:23,460
take a little step into that philosophy
circle. Uh, how hard would it,

566
00:34:24,210 --> 00:34:27,780
when you think about human life,
you should give those examples often.

567
00:34:27,810 --> 00:34:31,440
How hard do you think it is to formulate
human life supplanting problem or

568
00:34:31,441 --> 00:34:34,650
aspects of human life?
So when you look at robots,

569
00:34:34,651 --> 00:34:39,651
you're often trying to think about
object manipulation tasks about moving a

570
00:34:40,470 --> 00:34:41,000
thing.
When you,

571
00:34:41,000 --> 00:34:45,660
when you take a slight
step outside the room,

572
00:34:45,690 --> 00:34:49,970
let the robot leave and go get
lunch, uh, or maybe tried to uh,

573
00:34:50,490 --> 00:34:55,170
pursue more fuzzy goals,
how hard do you think is that problem?

574
00:34:55,290 --> 00:35:00,120
If you were to try to maybe put another
way, try to formulate a human life as,

575
00:35:00,121 --> 00:35:01,440
as a planning problem?

576
00:35:02,140 --> 00:35:05,260
Well that would be a mistake. I mean
it's not all the planning problem, right?

577
00:35:05,280 --> 00:35:06,370
Every,
I think it's really,

578
00:35:06,371 --> 00:35:11,371
really important that we understand
that you have to put together pieces and

579
00:35:11,681 --> 00:35:15,190
parts that have different styles of
reasoning and representation and learning.

580
00:35:15,640 --> 00:35:20,300
I think. I think it's, it's, it
seems probably clear to anybody that,

581
00:35:20,470 --> 00:35:21,390
that it,

582
00:35:21,470 --> 00:35:26,440
it can't all be this or all be that
brains aren't all like this are all like

583
00:35:26,441 --> 00:35:29,830
that, right? They have different pieces
and parts and substructure. And so on.

584
00:35:30,310 --> 00:35:33,280
So I don't think that there's any good
reason to think that there's going to be

585
00:35:33,281 --> 00:35:37,890
like when true algorithmic thing
that's going to do the whole job,

586
00:35:38,090 --> 00:35:40,360
it's just a bunch of pieces together,
uh,

587
00:35:40,380 --> 00:35:43,140
designed to solve a bunch
of specific problem.

588
00:35:43,170 --> 00:35:47,970
One specific styles of problems.

589
00:35:47,971 --> 00:35:51,540
I mean there's probably some reasoning
that needs to go on in image space.

590
00:35:51,750 --> 00:35:53,400
I think again,

591
00:35:56,020 --> 00:35:58,560
there's this model based
versus model free idea, right?

592
00:35:58,561 --> 00:36:01,450
So in reinforcement learning, people
talk about, oh, should I learn,

593
00:36:02,620 --> 00:36:06,280
I could learn a policy just
straight up a way of behaving.

594
00:36:06,970 --> 00:36:08,560
I could learn it's popular,

595
00:36:08,561 --> 00:36:12,700
only a value function and that's some
kind of weird intermediate ground, uh,

596
00:36:13,600 --> 00:36:16,600
or I could learn a transition model
or it's tells me something about the

597
00:36:16,601 --> 00:36:19,320
dynamics of the world.
If I take a train,

598
00:36:19,330 --> 00:36:22,510
if I imagine that I learned
in a transition model and
I couple it with a planner

599
00:36:22,511 --> 00:36:26,020
and I draw a box around
that, I have a policy. Again,

600
00:36:26,350 --> 00:36:29,800
it's just stored in a
different way. Right? Right.

601
00:36:30,070 --> 00:36:33,340
It's an but it's just as much
of a policy is the other policy.

602
00:36:33,341 --> 00:36:34,174
It's just I've made,

603
00:36:34,180 --> 00:36:39,180
I think the way I see it is it's a
timespace tradeoff in computation,

604
00:36:40,300 --> 00:36:45,040
right? At more overt policy
representation. Maybe it takes more space,

605
00:36:45,520 --> 00:36:49,330
but maybe I can compute quickly what
action I should take. On the other hand,

606
00:36:49,331 --> 00:36:53,110
maybe a very compact model of the
world dynamics plus a planner.

607
00:36:53,830 --> 00:36:57,790
Let's make compute what action to take
to just more slowly. There's no, I don't,

608
00:36:57,820 --> 00:36:59,740
I mean I don't think there's
no argument to be had.

609
00:36:59,741 --> 00:37:04,741
It's just like a question of what form
of computation is best for us for the

610
00:37:05,771 --> 00:37:08,200
various sub problems.
Right.

611
00:37:08,201 --> 00:37:13,201
So and and so like learning to do Algebra
and manipulations for some reason is,

612
00:37:14,170 --> 00:37:17,470
I mean that's probably going to
want naturally a sort of a different

613
00:37:17,471 --> 00:37:20,950
representation then riding
a unicycle at the time.

614
00:37:20,951 --> 00:37:24,610
Constraints on the unicycle or serious
this thing. Spaces maybe smaller,

615
00:37:24,611 --> 00:37:26,480
I don't know.
But so I

616
00:37:27,290 --> 00:37:30,310
be the more human sides
of falling in love.

617
00:37:30,340 --> 00:37:32,320
Having a relationship
that might be another

618
00:37:32,990 --> 00:37:35,350
Ah, yeah. Good. Another style up. No

619
00:37:35,820 --> 00:37:36,990
how to model that.
Yeah.

620
00:37:37,890 --> 00:37:41,560
That was first to solve the Algebra
and object manipulation. Yeah.

621
00:37:42,630 --> 00:37:46,890
What do you think is harder?
Perception or planning?

622
00:37:47,100 --> 00:37:51,540
Perception. That's why
understanding, that's fine. Uh,

623
00:37:51,940 --> 00:37:55,050
so what do you think is so hard about
perception by understanding the world

624
00:37:55,051 --> 00:37:55,560
around you?

625
00:37:55,560 --> 00:37:58,080
Well, I mean, I think the big question

626
00:38:00,370 --> 00:38:04,360
is representational. A hugely,
the question is representation.

627
00:38:04,870 --> 00:38:09,770
So perception that has
made great strides lately,

628
00:38:09,771 --> 00:38:12,980
right in the weekend.
Classify images and we can

629
00:38:13,870 --> 00:38:14,480
okay.

630
00:38:14,480 --> 00:38:17,480
Play certain kinds of games and predict
how to steer the car and all that sort

631
00:38:17,481 --> 00:38:19,250
of stuff.
Um,

632
00:38:20,660 --> 00:38:25,660
I don't think we have a very good idea
of what perception should deliver.

633
00:38:27,290 --> 00:38:30,020
Right? So if you, if you believe
in modularity, okay, there's,

634
00:38:30,050 --> 00:38:32,300
there's a very strong view which says

635
00:38:34,760 --> 00:38:39,030
we build in any modularity.
We should make a giant Jag,

636
00:38:39,360 --> 00:38:42,280
can't take neural network training
end to end to do the thing.

637
00:38:42,670 --> 00:38:44,140
And that's the best way forward.

638
00:38:44,950 --> 00:38:49,950
And it's hard to argue with that
except on a sample complexity basis,

639
00:38:50,921 --> 00:38:52,360
right? So you might say, oh,

640
00:38:52,361 --> 00:38:55,090
well if I wanted to do end to end
reinforcement learning on this giant,

641
00:38:55,091 --> 00:38:55,924
giant neural network,

642
00:38:55,960 --> 00:38:59,500
it's going to take a lot of data and
a lot of like broken robots and stuff.

643
00:39:01,180 --> 00:39:02,013
So,

644
00:39:02,980 --> 00:39:03,400
yeah.

645
00:39:03,400 --> 00:39:07,720
Then the only answer is to say,
okay,

646
00:39:08,230 --> 00:39:11,590
we have to build something in
building some structure or some bias.

647
00:39:11,591 --> 00:39:13,660
We know from theory of machine learning,

648
00:39:13,661 --> 00:39:17,470
the only way to cut down in the sample
complexity is to kind of cut down somehow

649
00:39:17,471 --> 00:39:22,090
cut down the hypothesis space.
You can do that by building in bias.

650
00:39:22,660 --> 00:39:26,980
There's all kinds of reason to think
that nature built bias into humans. Um,

651
00:39:28,330 --> 00:39:30,270
convolution is a bias,
right?

652
00:39:31,150 --> 00:39:33,610
It's a very strong bias and
is a very critical bias.

653
00:39:34,570 --> 00:39:39,570
So my own view is that we should look for
more things that are like convolution,

654
00:39:40,181 --> 00:39:42,790
but the address, other
aspects of reasoning, right?

655
00:39:42,791 --> 00:39:46,150
So convolution helps us a lot with a
certain kind of spatial reasoning that's

656
00:39:46,151 --> 00:39:47,710
quite close to the imaging.

657
00:39:48,970 --> 00:39:53,290
I think there's other ideas like that,

658
00:39:54,420 --> 00:39:58,440
maybe some amount of forward search,
maybe some notions of abstraction,

659
00:39:58,980 --> 00:40:01,410
maybe the notion that objects exist.
Actually,

660
00:40:01,411 --> 00:40:04,620
I think that's pretty important in a lot
of people won't give you that to start

661
00:40:04,621 --> 00:40:09,280
with almost like a
convolution in the, uh, uh,

662
00:40:09,530 --> 00:40:13,260
uh, in the object semantic objects,
spaces, some kind, some kind of,

663
00:40:13,280 --> 00:40:15,160
some kind of idea.
That's right.

664
00:40:15,180 --> 00:40:18,600
And people just started like the graph
graph convolutions or an idea that are

665
00:40:18,601 --> 00:40:20,850
related to racial
relational representations.

666
00:40:20,851 --> 00:40:25,740
And so I think there are,
so you,

667
00:40:25,920 --> 00:40:29,340
I've come far afield from
perception, but I think, um,

668
00:40:29,580 --> 00:40:33,270
I think the thing that's going to make
perception that kind of the next step is

669
00:40:33,271 --> 00:40:37,170
actually understanding better
what it should produce, right?

670
00:40:37,171 --> 00:40:39,810
So what are we going to do
with the output of it? Right?

671
00:40:39,811 --> 00:40:42,050
It's fine when what we're going
to do with the output is steer.

672
00:40:42,060 --> 00:40:47,060
It's less clear when we're just trying
to make one integrated intelligent agent.

673
00:40:49,020 --> 00:40:51,720
What should the output of perception be?
We have no idea.

674
00:40:52,200 --> 00:40:54,870
And how should that hook up to the
other stuff we don't know? Right?

675
00:40:55,080 --> 00:41:00,080
So I think the pressing question is what
kinds of structure can we build in that

676
00:41:00,931 --> 00:41:05,580
are like the moral equivalent
of convolution that will
make a really awesome

677
00:41:05,600 --> 00:41:10,200
superstructure that then learning
can kind of progress on efficiently?

678
00:41:10,370 --> 00:41:10,920
I agree.

679
00:41:10,920 --> 00:41:14,010
Very compelling description of actually
where we stand with the perception of

680
00:41:14,030 --> 00:41:17,790
them. You're teaching a course
and embodied intelligence.

681
00:41:18,330 --> 00:41:21,600
What do you think it takes to build a
robot with human level intelligence?

682
00:41:23,440 --> 00:41:25,000
I don't know if we knew we would do it

683
00:41:27,670 --> 00:41:32,490
if you were to, I mean, okay, so do
you think a robot and used to have a,

684
00:41:32,740 --> 00:41:34,670
a self awareness,

685
00:41:35,340 --> 00:41:40,340
a consciousness and fear
of mortality or is it,

686
00:41:41,270 --> 00:41:45,180
is it simpler than that or
is consciousness a simple
thing they [inaudible] do

687
00:41:45,190 --> 00:41:46,640
you think about these notions?

688
00:41:46,990 --> 00:41:49,690
I don't think much about it.
Consciousness,

689
00:41:50,020 --> 00:41:54,130
even most philosophers who care
about it will give you that.

690
00:41:54,131 --> 00:41:56,170
You could have robots that are zombies,
right?

691
00:41:56,171 --> 00:42:00,160
That behave like humans but they're not
conscious and I at this moment we'd be

692
00:42:00,161 --> 00:42:02,580
happy enough for that. So I'm not
really worried one way or the other.

693
00:42:02,700 --> 00:42:07,120
So the technical side, you're not
thinking of the use of self awareness.

694
00:42:08,340 --> 00:42:11,100
Okay.
But then what is self awareness mean?

695
00:42:11,130 --> 00:42:16,130
I mean that you need to have some part
of the system that can observe other

696
00:42:17,101 --> 00:42:19,800
parts of the system and tell
whether they're working well or not.

697
00:42:19,801 --> 00:42:23,130
That seems critical.
So does that count this,

698
00:42:23,280 --> 00:42:26,190
I mean does that kind of
self awareness or not? Well,

699
00:42:26,191 --> 00:42:31,191
it depends on whether you
think that there's somebody
at home who can articulate

700
00:42:31,261 --> 00:42:34,980
whether they're self aware, but
clearly if I have like, you know,

701
00:42:34,981 --> 00:42:38,430
some piece of code that's counting how
many times this procedure gets executed.

702
00:42:39,490 --> 00:42:42,870
That's a kind of self awareness.
Right? So there's a big spectrum.

703
00:42:42,960 --> 00:42:44,960
It's clear you have to have some of it.
Right.

704
00:42:45,300 --> 00:42:47,370
You know, we're quite far
away, I many dimensions,

705
00:42:47,371 --> 00:42:52,080
but is a direction of research
that's most compelling to you for,

706
00:42:52,410 --> 00:42:54,810
you know,
tried to achieve human level intelligence

707
00:42:55,100 --> 00:42:57,200
and our robots? Well, to me,

708
00:42:57,201 --> 00:43:00,920
I guess the thing that seems most
compelling to me at the moment is this

709
00:43:00,921 --> 00:43:05,840
question of what to build in
and want to learn. Um, I think,

710
00:43:07,090 --> 00:43:07,923
yeah,

711
00:43:07,960 --> 00:43:12,100
we're, we don't, we're
missing a bunch of ideas and,

712
00:43:12,640 --> 00:43:15,100
and we, you know, people, you know,

713
00:43:15,101 --> 00:43:18,190
don't you dare ask me how many years
it's going to be till that happens.

714
00:43:18,191 --> 00:43:22,870
Cause I won't even participate in the
conversation because I think we're missing

715
00:43:22,871 --> 00:43:25,150
ideas and I don't know how long
it's going to take to find them.

716
00:43:25,710 --> 00:43:29,650
So I won't ask you how many years,
but, uh, maybe I'll ask you.

717
00:43:30,150 --> 00:43:30,590
Yeah.

718
00:43:30,590 --> 00:43:31,423
Would it

719
00:43:31,720 --> 00:43:35,590
when you will be sufficiently impressed
that we've achieved it. So what's,

720
00:43:35,591 --> 00:43:38,260
what's a a good test of intelligence?

721
00:43:38,261 --> 00:43:42,610
Do you like the Turing test and
natural language and the robotic space?

722
00:43:42,611 --> 00:43:46,690
Is there something, wait, you
would sit back and think, oh,

723
00:43:46,691 --> 00:43:50,800
us as pretty impressive, uh,
as a test, as a benchmark.

724
00:43:50,830 --> 00:43:52,840
Do you think about
these kinds of problems?

725
00:43:52,910 --> 00:43:54,710
No, I resist. I mean,

726
00:43:54,711 --> 00:43:58,790
I think all the time that we spend arguing
about those kinds of things could be

727
00:43:58,791 --> 00:44:00,950
better spent just making
the robots work better,

728
00:44:03,080 --> 00:44:07,530
you know, value competition. So I mean
there's the nature of benchmark, uh,

729
00:44:07,800 --> 00:44:12,190
benchmarks and datasets or touring test
challenges where everybody kind of gets

730
00:44:12,191 --> 00:44:15,760
together and tries to build a better robot
because they want to out compete each

731
00:44:15,761 --> 00:44:18,850
other. I the Darpa challenge
with the autonomous vehicles,

732
00:44:19,540 --> 00:44:21,700
do you see the value of that

733
00:44:23,870 --> 00:44:26,160
or it can get in the way? I think
it can get in the way. I mean,

734
00:44:26,161 --> 00:44:29,090
there's some people, many people find
it motivating and so that's good.

735
00:44:29,150 --> 00:44:33,990
I find it anti motivating personally.
Yeah. Uh, but I think what,

736
00:44:34,050 --> 00:44:34,321
I mean,

737
00:44:34,321 --> 00:44:39,270
I think you'd get an interesting cycle
where for a contest or a bunch of smart

738
00:44:39,271 --> 00:44:43,230
people get super motivated and they hacked
the brains out and much of what gets

739
00:44:43,231 --> 00:44:44,250
done is just hacks.

740
00:44:44,251 --> 00:44:48,900
But sometimes really cool ideas emerge
and then that gives us something to chew

741
00:44:48,901 --> 00:44:53,580
on after that. So I'm, I, it's
not a thing for me, but I don't,

742
00:44:54,330 --> 00:44:56,700
I don't regret that other people do it.
Yeah.

743
00:44:56,820 --> 00:44:59,310
And like you said with everything
else to the mix is good.

744
00:44:59,700 --> 00:45:01,410
So jumping topics a little bit,

745
00:45:01,500 --> 00:45:06,500
he started the journal and
machine learning research
and served as its editor in

746
00:45:06,811 --> 00:45:10,290
chief. Uh, how did the
publication come about?

747
00:45:10,790 --> 00:45:11,623
Hmm.

748
00:45:11,790 --> 00:45:12,330
And uh,

749
00:45:12,330 --> 00:45:17,200
what do you think about the
current publishing model
space in a machine learning

750
00:45:17,201 --> 00:45:18,080
or artificial intelligence?

751
00:45:18,580 --> 00:45:23,440
Okay, good. So it came about because there
was a journal called machine learning.

752
00:45:23,441 --> 00:45:25,840
We're still exists,
which was own by Kluwer.

753
00:45:26,500 --> 00:45:28,930
And there was,

754
00:45:28,931 --> 00:45:31,960
I was on the editorial board and we used
to have these meetings and really where

755
00:45:31,961 --> 00:45:35,530
we were to complain to Kluwer that it
was too expensive for the libraries on

756
00:45:35,531 --> 00:45:38,980
that people couldn't publish and we
would really like to have some kind of

757
00:45:38,981 --> 00:45:43,180
relief on those friends and they would
always sympathize but not doing anything.

758
00:45:43,720 --> 00:45:48,020
So, uh, I just decided to
make a new journal. And, uh,

759
00:45:48,080 --> 00:45:51,790
there was the journal of Ai Research,
which has, was on the same model,

760
00:45:51,791 --> 00:45:55,060
which had been in existence
for maybe five years or so.

761
00:45:55,061 --> 00:45:59,650
And it was going along
pretty well. So, uh,

762
00:45:59,710 --> 00:46:04,180
we just made a new journal. It
wasn't, I mean, it, um, I don't know,

763
00:46:04,181 --> 00:46:07,390
I guess it was work, but it wasn't that
hard. So basically the editorial board,

764
00:46:07,720 --> 00:46:12,720
probably 75% of the editorial board
of a machine learning resigned and we

765
00:46:15,641 --> 00:46:19,740
founded the new, this new
journal. But it was sort of,

766
00:46:19,910 --> 00:46:23,410
there's more open. Yeah.
Right. So it's completely open.

767
00:46:23,411 --> 00:46:27,960
It's open access. Actually.
Uh, uh, I had a postdoc,

768
00:46:27,970 --> 00:46:28,701
George kind of Doris,

769
00:46:28,701 --> 00:46:33,701
we wanted to call these journals
free for all because there were,

770
00:46:34,330 --> 00:46:38,200
I mean,
it both has no page charges and has no,

771
00:46:38,830 --> 00:46:40,120
uh,
uh,

772
00:46:40,210 --> 00:46:44,960
access restrictions and the reason
and so lots of people, I mean,

773
00:46:45,130 --> 00:46:45,670
there were,

774
00:46:45,670 --> 00:46:49,330
there were people who were mad about the
existence of this journal who thought

775
00:46:49,331 --> 00:46:52,240
it was a fraud or something.
It would be impossible.

776
00:46:52,241 --> 00:46:55,630
They said to run a journal like
this with basically, I mean,

777
00:46:55,631 --> 00:46:58,320
for a long time I didn't
even have a bank account. Uh,

778
00:46:58,410 --> 00:47:03,410
I paid for the lawyer to incorporate and
the Ip address and just due to cost a

779
00:47:06,550 --> 00:47:09,130
couple hundred dollars a year to run.
It's a little bit more now,

780
00:47:09,131 --> 00:47:10,150
but not that much more.

781
00:47:10,780 --> 00:47:15,780
But that's because I think
computer scientists are
competent and autonomous in a

782
00:47:17,261 --> 00:47:21,550
way that many scientists and other fields
aren't doing these kinds of things.

783
00:47:21,790 --> 00:47:23,740
We already type set around papers.

784
00:47:24,040 --> 00:47:27,790
We all have students and people who can
hack a website together in the afternoon.

785
00:47:28,090 --> 00:47:31,060
So the infrastructure us
was like not a problem,

786
00:47:31,090 --> 00:47:34,180
but for other people in other fields,
it's a harder thing to do.

787
00:47:34,770 --> 00:47:39,630
Yeah. And this kind of open access
journal and nevertheless one of the most

788
00:47:39,631 --> 00:47:40,830
prestigious journals.

789
00:47:40,890 --> 00:47:45,890
So it's not like a prestige and it can
be achieved without any of the companies

790
00:47:47,240 --> 00:47:50,140
are required. Yeah. For
Prestige. Yeah. Turns out. Yeah.

791
00:47:50,720 --> 00:47:53,730
So on the review process side
of actually a long time ago,

792
00:47:53,731 --> 00:47:58,731
I don't remember when I reviewed a paper
where you were also a reviewer and I

793
00:47:58,801 --> 00:48:02,910
remember reading your view
of being influenced by it.
It was really well written.

794
00:48:02,970 --> 00:48:07,080
It did influence how I write
feature reviews. Uh, you
disagreed with me actually,

795
00:48:07,360 --> 00:48:11,700
uh, you made it, uh, my review,

796
00:48:11,701 --> 00:48:15,780
but much better. So, but
nevertheless, the review process,

797
00:48:16,530 --> 00:48:20,880
you know, has its flaws.
And how do you think,

798
00:48:21,120 --> 00:48:23,370
what do you think works well?
How can it be improved?

799
00:48:23,660 --> 00:48:27,770
So actually when I started
jam are I wanted to do
something completely different

800
00:48:28,860 --> 00:48:33,860
and I didn't because it felt like we
needed a traditional journal of record.

801
00:48:34,171 --> 00:48:39,171
And so we just made jam our be almost
like a normal journal except for the open

802
00:48:39,391 --> 00:48:43,980
access parts of it
basically. Um, increasingly,

803
00:48:43,981 --> 00:48:46,530
of course publication is
not even a sensible word.

804
00:48:46,560 --> 00:48:48,830
You can publish something about
putting it in an archive for.

805
00:48:48,840 --> 00:48:50,370
So I can publish everything tomorrow.

806
00:48:50,880 --> 00:48:54,900
So making stuff public is,

807
00:48:55,080 --> 00:48:56,010
there's no barrier.

808
00:48:57,240 --> 00:49:02,240
We still need curation and evaluation.

809
00:49:03,180 --> 00:49:05,550
I don't have time to read all of archive.

810
00:49:07,020 --> 00:49:10,440
And you could argue that

811
00:49:12,720 --> 00:49:17,720
kind of social thumbs
upping of articles suffices,

812
00:49:20,130 --> 00:49:24,060
right? You might say, Oh heck with
this we don't need journalists and all,

813
00:49:24,480 --> 00:49:27,750
we'll put everything on archive and people
will upload and download the articles

814
00:49:27,751 --> 00:49:31,040
and then your CV, we'll say, oh
man, they, he got a lot of boats.

815
00:49:31,220 --> 00:49:33,930
So that's good.
Um,

816
00:49:35,100 --> 00:49:37,380
but I think there's still

817
00:49:37,420 --> 00:49:38,253
cool

818
00:49:38,850 --> 00:49:43,850
value in careful reading
and commentary of things.

819
00:49:44,651 --> 00:49:48,400
And it's hard to tell when people aren't
voting and down voting or arguing about

820
00:49:48,401 --> 00:49:51,640
your paper on Twitter and Reddit,

821
00:49:51,670 --> 00:49:54,910
whether they know what
they're talking about. Right.

822
00:49:54,911 --> 00:49:58,600
So then I have the second order problem
of trying to decide whose opinions I

823
00:49:58,601 --> 00:50:01,640
should value and such. So, I dunno,

824
00:50:01,650 --> 00:50:04,660
I W W if I had infinite time,
which I don't,

825
00:50:04,661 --> 00:50:07,450
and I'm not going to do this because
I really want to make the robots work.

826
00:50:07,990 --> 00:50:12,250
But if I felt inclined to do something
more than a publication direction,

827
00:50:13,090 --> 00:50:15,580
I would do this other thing which I
thought about doing the first time,

828
00:50:15,581 --> 00:50:20,230
which is to get together some set of
people whose opinions I value and who were

829
00:50:20,231 --> 00:50:24,370
pretty articulate. And I guess we would
be public, although it could be private,

830
00:50:24,371 --> 00:50:26,950
I'm not sure.
And we would review papers,

831
00:50:26,990 --> 00:50:29,060
we wouldn't publish them and
you wouldn't submit that.

832
00:50:29,060 --> 00:50:33,740
We were just fine papers and we would
write reviews and we would make those

833
00:50:33,741 --> 00:50:37,820
reviews public. And
maybe if you, you know,

834
00:50:37,821 --> 00:50:42,530
so we're Leslie's friends who review
papers and maybe eventually if we,

835
00:50:42,531 --> 00:50:46,550
our opinion was sufficiently valued,
like the opinion of jam ours valued,

836
00:50:46,850 --> 00:50:50,570
then you'd say on your CV that Leslie's
friends gave my paper of five star

837
00:50:50,571 --> 00:50:54,300
rating and that would be just as
good as saying, I got it, you know,

838
00:50:54,370 --> 00:50:57,680
accepted into this
journal. Um, so I think,

839
00:50:58,490 --> 00:51:01,890
I think we should have
good public commentary, uh,

840
00:51:01,970 --> 00:51:05,210
and organize it in some way,
but I don't really know how to do it.

841
00:51:05,211 --> 00:51:06,290
It's interesting times

842
00:51:06,350 --> 00:51:09,080
the way the, the way you describe
it actually is really interesting.

843
00:51:09,090 --> 00:51:12,950
Then he would do it for movies.
I am db.com there's a experts,

844
00:51:12,951 --> 00:51:17,840
critics come in, they write reviews,
but there's also regular non critics.

845
00:51:17,841 --> 00:51:21,350
Humans write reviews and
they're separated. Open Review.

846
00:51:21,770 --> 00:51:23,310
Well the,
the,

847
00:51:23,340 --> 00:51:28,340
the I I clear process
I think is interesting.

848
00:51:29,490 --> 00:51:34,190
It's a step in the right direction, but
it's still not as compelling as a, uh,

849
00:51:34,200 --> 00:51:38,460
reviewing movies or video games.
I mean it sometimes,

850
00:51:38,461 --> 00:51:41,700
I'll almost as, it might be silly
as least from my perspective to say,

851
00:51:41,701 --> 00:51:43,860
but it boils down to the user interface,

852
00:51:43,861 --> 00:51:47,790
how fun and easy it is to actually
perform the reviews. How efficient,

853
00:51:48,150 --> 00:51:53,150
how much you as a reviewer get a
street cred for being a good reviewer.

854
00:51:54,510 --> 00:51:56,910
Those elements,
those human elements come into play.

855
00:51:57,370 --> 00:52:00,890
No, it's a big investment to
do a good review or a paper.

856
00:52:01,310 --> 00:52:05,460
And the flood of papers is that
control. Right. So, you know,

857
00:52:05,480 --> 00:52:08,480
there aren't 3000 new, I don't know how
many new movies are there in a year.

858
00:52:08,630 --> 00:52:09,111
I don't know.

859
00:52:09,111 --> 00:52:12,290
But that's probably going to be less
than how many machine learning papers are

860
00:52:12,291 --> 00:52:16,890
in a year now. And I'm
worried. I, you know, I,

861
00:52:17,060 --> 00:52:21,320
I uh, right, so I'm like an old person,

862
00:52:21,321 --> 00:52:25,340
so of course I'm going to say Rar, Rar,
rar and things are moving too fast.

863
00:52:25,341 --> 00:52:28,940
I'm a stick in the mud.
Uh, so I can say that,

864
00:52:28,941 --> 00:52:33,941
but my particular flavor of that is I
think the horizon for researchers has

865
00:52:34,881 --> 00:52:39,881
gotten very short that students want to
publish a lot of papers and there's a

866
00:52:41,060 --> 00:52:42,200
huge,
there's value,

867
00:52:42,201 --> 00:52:46,670
it's exciting and there's value in that
and you get patted on the head for it

868
00:52:46,671 --> 00:52:50,780
and so on. But, and some of that is fine,

869
00:52:51,680 --> 00:52:56,680
but I'm worried that we're driving out
people who would spend two years thinking

870
00:53:00,111 --> 00:53:00,944
about something.

871
00:53:02,000 --> 00:53:06,860
Back in my day when we worked on our
theses, we did not publish papers.

872
00:53:06,890 --> 00:53:11,120
You did your thesis for years, you pick
the hard problem and then you worked in,

873
00:53:11,121 --> 00:53:14,990
chewed on it and did stuff and
waste of time for a long time.

874
00:53:15,500 --> 00:53:18,230
And when it was roughly when it was done,
you would write papers.

875
00:53:18,950 --> 00:53:23,900
And so I don't know how to answer and I
don't think that everybody has to work

876
00:53:23,901 --> 00:53:24,734
in that mode,

877
00:53:24,860 --> 00:53:28,830
but I think there's some problems that
are hard enough that it's important to

878
00:53:28,831 --> 00:53:30,600
have a longer her research horizon.

879
00:53:30,601 --> 00:53:34,650
And I'm worried though we don't
incentivize that at all. At this point

880
00:53:34,860 --> 00:53:37,050
in this current structure,

881
00:53:38,860 --> 00:53:41,440
what do you see as a,

882
00:53:41,520 --> 00:53:46,520
what are your hopes and fears about the
future of AI and continue on this theme?

883
00:53:46,591 --> 00:53:51,240
So Ai has gone through a few winters,
ups and downs.

884
00:53:51,520 --> 00:53:54,830
Do you see another winter of AI coming,
uh,

885
00:53:55,000 --> 00:53:59,610
or do you more hopeful
about making robots work?

886
00:53:59,611 --> 00:54:00,444
As he said,

887
00:54:00,610 --> 00:54:02,680
I think the cycles are inevitable,

888
00:54:03,160 --> 00:54:07,570
but I think each time we get
higher, right. I mean, so, you know,

889
00:54:08,320 --> 00:54:12,290
it's like climbing some kind
of landscape within noisy, uh,

890
00:54:12,520 --> 00:54:17,230
optimizer. Yeah. So it's
clear that the, the,

891
00:54:17,231 --> 00:54:17,651
you know,

892
00:54:17,651 --> 00:54:22,651
the deep learning stuff has made
deep and important improvements.

893
00:54:23,531 --> 00:54:26,710
And so the high water mark is now
higher. I, there's no question,

894
00:54:27,130 --> 00:54:32,130
but of course I think people
are overselling and eventually,

895
00:54:32,610 --> 00:54:36,790
uh, investors I guess, and other
people look around and say,

896
00:54:37,570 --> 00:54:38,141
well,

897
00:54:38,141 --> 00:54:43,141
you're not quite delivering on this
grand claim and that wild hypothesis it,

898
00:54:43,500 --> 00:54:48,190
so probably it's going to crash some
of them out. And then it's okay.

899
00:54:48,220 --> 00:54:49,600
I mean it,
but I don't,

900
00:54:49,660 --> 00:54:54,610
I can't imagine that there's like some
awesome monotonic improvement from here

901
00:54:54,611 --> 00:54:59,500
to human level Ai. So in, uh, you know,

902
00:54:59,501 --> 00:55:04,200
I have to ask this question, I'd probably
anticipate answers the answers, but, uh,

903
00:55:04,750 --> 00:55:09,750
do you have a worry short term or long
term about the existential threats of Ai

904
00:55:11,110 --> 00:55:11,950
and

905
00:55:12,980 --> 00:55:14,570
maybe short term,

906
00:55:14,720 --> 00:55:19,220
less existential but more
a robot's taking away jobs?

907
00:55:19,430 --> 00:55:21,350
Hmm. Well, actually,

908
00:55:21,750 --> 00:55:26,370
let me talk a little bit about utility.
Actually,

909
00:55:26,371 --> 00:55:30,990
I had an interesting conversation with
some military ethicists who wanted to

910
00:55:30,991 --> 00:55:32,760
talk to me about autonomous weapons.

911
00:55:34,050 --> 00:55:37,200
And there they were interesting,
smart,

912
00:55:37,530 --> 00:55:41,340
well educated guys who didn't know
too much about AI or machine learning.

913
00:55:41,341 --> 00:55:43,110
And the first question they asked me was,

914
00:55:43,111 --> 00:55:45,960
has your robot ever done
something you didn't expect?

915
00:55:46,830 --> 00:55:50,120
And I like burst out laughing because
anybody who's ever done something other

916
00:55:50,130 --> 00:55:52,090
robot right knows that they don't do it.

917
00:55:52,620 --> 00:55:56,970
And what I realized was that their
model of how we program a robot was

918
00:55:56,971 --> 00:55:57,900
completely wrong.

919
00:55:58,050 --> 00:56:02,670
Their model of how we can put program
robot was like Lego Mindstorms.

920
00:56:02,671 --> 00:56:06,600
Like oh go for it, meet her, turn
left, take a picture, do this, do that.

921
00:56:06,601 --> 00:56:10,320
And so if you have that model
of programming, then it's true.

922
00:56:10,321 --> 00:56:13,140
It's kind of weird that you run what
would do something that you didn't

923
00:56:13,141 --> 00:56:16,350
anticipate. But the fact
is, and and actually,

924
00:56:16,351 --> 00:56:19,680
so now this is my new educational mission.
If I have to talk to non experts,

925
00:56:19,980 --> 00:56:24,700
I tried to teach them the,
that we don't operate,

926
00:56:24,730 --> 00:56:28,630
we operate at least one or maybe many
levels of abstraction about that.

927
00:56:28,631 --> 00:56:31,060
And we say, Oh, here's a hypothesis class.

928
00:56:31,061 --> 00:56:35,680
Maybe it's a space of plans or maybe
it's a space of classifiers or whenever.

929
00:56:35,681 --> 00:56:38,380
But there's some set of answers
and the objective function.

930
00:56:38,381 --> 00:56:43,330
And then we work on some optimization
method that tries to optimize a solution,

931
00:56:43,750 --> 00:56:47,950
a solution in that class. And we don't
know what solution is going to come out.

932
00:56:48,370 --> 00:56:51,430
Right. So I think it's
important to communicate that.

933
00:56:51,490 --> 00:56:54,390
So I'm going of course probably people
who are listening to this, they,

934
00:56:54,630 --> 00:56:55,660
they know that lesson.

935
00:56:55,661 --> 00:56:58,510
But I think it's really critical
to communicate that lesson.

936
00:56:58,511 --> 00:57:00,820
And then lots of people are
now talking about, you know,

937
00:57:00,821 --> 00:57:02,320
the value alignment problem.

938
00:57:02,350 --> 00:57:07,350
So you want to be sure as robots or
software systems get more competent that

939
00:57:09,131 --> 00:57:12,760
their objectives are aligned
with your objectives or that, uh,

940
00:57:12,761 --> 00:57:16,840
our objectives are compatible on some
way or we have a good way of mediating

941
00:57:16,841 --> 00:57:18,700
when they have different objectives.

942
00:57:18,730 --> 00:57:22,450
And so I think it is important
to start thinking in terms like,

943
00:57:23,200 --> 00:57:28,090
you don't have to be freaked out by the
robot apocalypse to accept that it's

944
00:57:28,091 --> 00:57:31,120
important to think about objective
functions of value alignment. Yes.

945
00:57:31,330 --> 00:57:33,040
And that you have to really,

946
00:57:33,041 --> 00:57:36,520
everyone who's done optimization knows
that you have to be careful what you wish

947
00:57:36,521 --> 00:57:40,150
for that are, you know, sometimes you'd
get the optimal solution and you realize,

948
00:57:40,151 --> 00:57:42,100
man, that was, that objective was wrong.

949
00:57:43,000 --> 00:57:46,480
So pragmatically in the shortest term,

950
00:57:46,481 --> 00:57:51,460
it seems to me that that those are really
interesting and critical questions.

951
00:57:51,461 --> 00:57:55,330
And the idea that we're going to go from
being people who engineer algorithms to

952
00:57:55,331 --> 00:57:58,930
being people who engineer objective
functions. I think that's,

953
00:57:59,110 --> 00:58:00,400
that's definitely going to happen.

954
00:58:00,401 --> 00:58:03,570
And that's going to change our
thinking and methodology and stuff.

955
00:58:03,940 --> 00:58:08,770
You started at Stanford a philosophy
that's where she could go back to

956
00:58:08,771 --> 00:58:13,000
philosophy maybe. Well, signing up there,
I mean they're mixed together because,

957
00:58:13,010 --> 00:58:17,700
because as we also know as
machine learning people,
right? When you're designing,

958
00:58:17,710 --> 00:58:19,780
in fact,
this is the lecture I gave in class today,

959
00:58:19,781 --> 00:58:23,500
when you design an objective function,
you have to wear both hats.

960
00:58:23,501 --> 00:58:27,720
There's the hat that says, what do I
want? And there's the hat that says, ah,

961
00:58:27,790 --> 00:58:31,600
but I know what my optimizer can do to
some degree and I have to take that into

962
00:58:31,601 --> 00:58:33,800
account. All right. So it's,

963
00:58:33,830 --> 00:58:36,280
it's always a trade off and we
have to kind of be mindful of that.

964
00:58:37,180 --> 00:58:40,690
The part about taking people's jobs,

965
00:58:40,691 --> 00:58:42,760
I understand that that's important.

966
00:58:43,060 --> 00:58:47,980
I don't understand sociology or
economics or people very well.

967
00:58:47,981 --> 00:58:49,690
So I don't know how to think about that.

968
00:58:49,960 --> 00:58:53,500
So that's, yeah, so there might
be a sociological aspect there.

969
00:58:53,501 --> 00:58:56,560
The economic aspect that's very
difficult to think about it. Okay.

970
00:58:56,840 --> 00:58:59,000
I mean I think other people should
be thinking about it, but I'm just,

971
00:58:59,030 --> 00:59:00,330
that's not my strength.
So

972
00:59:00,610 --> 00:59:04,660
what do you think is the most exciting
area of research in the short term for

973
00:59:04,661 --> 00:59:06,670
the community and for you?
For Yourself?

974
00:59:06,960 --> 00:59:07,201
Well,

975
00:59:07,201 --> 00:59:12,201
so I mean there's the story I've been
telling you about how to engineer

976
00:59:12,241 --> 00:59:17,130
intelligent robots. So that's what we want
to do. We all kind of want to do well,

977
00:59:17,190 --> 00:59:17,430
I mean,

978
00:59:17,430 --> 00:59:21,060
some set of us want to do this and the
question is what's the most effective

979
00:59:21,080 --> 00:59:21,913
strategy?

980
00:59:21,950 --> 00:59:25,370
And we've tried and there's a bunch of
different things you could do at the

981
00:59:25,371 --> 00:59:26,240
extremes,
right?

982
00:59:26,390 --> 00:59:31,100
One super extreme is we do introspection
and then we write a program. Okay.

983
00:59:31,101 --> 00:59:32,750
That has not worked out really well.

984
00:59:32,840 --> 00:59:37,280
Another extreme is we'd take a giant bunch
of neural guru and we try to train it

985
00:59:37,281 --> 00:59:39,530
up to do something.
I don't think that's going to work either.

986
00:59:40,310 --> 00:59:44,900
So the question is what's
the middle ground? And again,

987
00:59:44,901 --> 00:59:49,490
this isn't a a theological
question or anything like that.

988
00:59:49,491 --> 00:59:54,200
It's just like how do just, how do we,
what's the best way to make this work out?

989
00:59:55,070 --> 00:59:59,660
And I think it's, it's clear, it's a
combination of learning. To me it's clear,

990
00:59:59,661 --> 01:00:04,190
it's a combination of learning and not
learning and what should that combination

991
01:00:04,191 --> 01:00:07,400
B and what's the stuff we build in. So to
me that's the most compelling question.

992
01:00:07,850 --> 01:00:09,860
And when you say engineer robots,

993
01:00:09,861 --> 01:00:14,380
you mean engineering systems that
work in the real world? Is that,

994
01:00:14,480 --> 01:00:18,530
that's the emphasis.
The last question,

995
01:00:19,310 --> 01:00:23,360
which robots? A robot is your
favorite from science fiction.

996
01:00:24,630 --> 01:00:29,630
So you can go with star wars are our Rtg
d two or you can go with a more modern,

997
01:00:31,780 --> 01:00:33,430
uh,
maybe how firm,

998
01:00:34,200 --> 01:00:38,600
I don't think I have a favorite
robot from science fiction. This is,

999
01:00:38,630 --> 01:00:40,550
this is back to you.

1000
01:00:40,590 --> 01:00:45,590
You like to make robots work in
the real world here? Not a, not an,

1001
01:00:45,650 --> 01:00:50,180
I mean, I love the process. I
care more about the process.

1002
01:00:51,380 --> 01:00:53,990
Yeah. I mean, I do
research because it's fun.

1003
01:00:53,991 --> 01:00:58,430
Not because I care about what
we produce. Well, that's a,

1004
01:00:58,431 --> 01:01:00,800
that's a beautiful note actually to,
and unless they,

1005
01:01:00,801 --> 01:01:02,900
thank you so much for talking
today. Sure. It's been fun.

