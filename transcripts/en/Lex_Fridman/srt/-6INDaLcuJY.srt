1
00:00:01,070 --> 00:00:06,070
Thank you everyone for braving the cold 
and the snow to be here.

2
00:00:08,030 --> 00:00:13,030
This is six zero,
nine for deep learning for self driving 

3
00:00:13,030 --> 00:00:16,751
cars and it's a course where we cover 
the topic of deep learning,

4
00:00:19,700 --> 00:00:24,700
which is a set of techniques that have 
taken a leap in the last decade for our 

5
00:00:26,240 --> 00:00:31,240
understanding of what artificial 
intelligence systems are capable of 

6
00:00:31,240 --> 00:00:34,481
doing and self driving cars,
which is systems that can take these 

7
00:00:35,991 --> 00:00:39,680
techniques and integrate them in a 
meaningful,

8
00:00:39,681 --> 00:00:44,681
profound way into our daily lives in a 
way that transforms society.

9
00:00:45,890 --> 00:00:50,890
So that's why both of these topics are 
extremely important and extremely 

10
00:00:50,890 --> 00:00:55,511
exciting.
My name is Lex Friedman and I'm joined 

11
00:00:55,511 --> 00:00:59,120
by an amazing team of engineers and Jack
Terwilliger,

12
00:00:59,760 --> 00:01:02,630
Julia Kendall's Burger,
Dan Brown,

13
00:01:02,840 --> 00:01:04,910
Michael Glaser,
lead Ding,

14
00:01:05,180 --> 00:01:06,190
Spencer,
dod,

15
00:01:06,470 --> 00:01:11,470
and Ben antigenic among many others.
We build autonomous vehicles here at 

16
00:01:13,221 --> 00:01:18,221
Mit,
not just ones that perceive and move 

17
00:01:18,221 --> 00:01:21,650
about the environment,
but ones that interact,

18
00:01:21,651 --> 00:01:26,651
communicate and earn the trust and 
understanding of human beings inside the

19
00:01:27,501 --> 00:01:32,501
car,
the drivers and the passengers and the 

20
00:01:32,501 --> 00:01:34,991
human beings outside the car,
the pedestrians and other drivers and 

21
00:01:35,361 --> 00:01:36,080
cyclists.

22
00:01:39,310 --> 00:01:43,330
The website for this course,
self driving cars that mit did to you,

23
00:01:43,750 --> 00:01:47,320
if you have questions,
email deep cars at Mit Dot Edu,

24
00:01:48,100 --> 00:01:53,100
slack,
deep dash mit for registered mit 

25
00:01:53,100 --> 00:01:57,810
students.
You have to register on the website and 

26
00:01:58,390 --> 00:02:00,100
by midnight,
Friday,

27
00:02:00,160 --> 00:02:05,160
January 19th,
building your own network and submitted 

28
00:02:05,160 --> 00:02:09,330
to the competition that achieves the 
speed of 65 miles per hour on the new 

29
00:02:09,431 --> 00:02:10,540
deep traffic.
Two point.

30
00:02:10,541 --> 00:02:15,541
Oh,
it's much harder and much more 

31
00:02:15,541 --> 00:02:16,570
interesting than last years.
For those of you who participated,

32
00:02:18,010 --> 00:02:19,930
there's three competitions in this 
class.

33
00:02:20,200 --> 00:02:22,630
Deep traffic segued,
fuse,

34
00:02:22,950 --> 00:02:27,950
deep crash.
There's guest speakers that come from 

35
00:02:27,950 --> 00:02:28,540
Waymo,
Google,

36
00:02:29,680 --> 00:02:34,680
Tesla,
and those are starting new autonomous 

37
00:02:34,691 --> 00:02:39,691
vehicle startups in voyage.
You autonomy and Aurora and the news a 

38
00:02:44,301 --> 00:02:49,301
lot today from cs and we have shirts for
those of you who brave the snow and 

39
00:02:52,731 --> 00:02:57,500
continue to do so towards the end of the
class there'll be free shirts.

40
00:02:57,830 --> 00:03:00,700
Yes.
I said free and in the same sentence you

41
00:03:00,701 --> 00:03:03,820
should be here.
Okay.

42
00:03:03,940 --> 00:03:06,280
First,
the deep traffic competition.

43
00:03:07,910 --> 00:03:10,790
There's a lot of updates and we'll cover
those on Wednesday.

44
00:03:11,270 --> 00:03:13,340
It's a deeper enforcement learning 
competition.

45
00:03:13,820 --> 00:03:17,270
Last year we received over 18,000
submissions.

46
00:03:18,350 --> 00:03:20,840
This year we're going to go bigger.

47
00:03:23,150 --> 00:03:26,000
Not only can you control one car,
well then you will now work.

48
00:03:26,030 --> 00:03:28,990
You can control up to 10.
This is multiagent,

49
00:03:29,030 --> 00:03:31,910
deeper enforcement learning.
This is super cool.

50
00:03:34,190 --> 00:03:35,990
Second,
psych fuse,

51
00:03:36,080 --> 00:03:41,080
dynamic driving scene segmentation 
competition where you're given the raw 

52
00:03:42,651 --> 00:03:43,400
video,

53
00:03:44,900 --> 00:03:49,160
the the kinematics of the vehicles and 
the movement of the vehicle,

54
00:03:49,790 --> 00:03:54,790
the state of the art segmentation for 
the training set you're given ground 

55
00:03:54,981 --> 00:03:56,870
truth labels,
pixel level labels,

56
00:03:56,871 --> 00:04:01,871
scene segmentation and optical flow and 
would those pieces of data your task to 

57
00:04:03,411 --> 00:04:08,411
try to perform better than the state of 
the art in image based segmentation.

58
00:04:10,460 --> 00:04:15,460
Why is this critical and fascinating and
open research problem?

59
00:04:17,030 --> 00:04:22,030
Because robots that act in this world 
and the physical space not only must 

60
00:04:23,901 --> 00:04:28,901
interpret,
use these deep learning methods to 

61
00:04:28,901 --> 00:04:28,901
interpret the spacial visual 
characteristics of a scene,

62
00:04:29,210 --> 00:04:34,210
they must also interpret,
understand and track the temporal 

63
00:04:34,210 --> 00:04:37,811
dynamics of the scene.
This competition is about temporal 

64
00:04:37,811 --> 00:04:39,680
propagation of information,
not just seen as segmentation.

65
00:04:40,460 --> 00:04:45,460
You must understand the space and time 
and finally,

66
00:04:47,470 --> 00:04:50,500
deep crash where we use deeper 
enforcement learning.

67
00:04:50,800 --> 00:04:54,490
The Slam cars thousands of times at uh,
at,

68
00:04:54,491 --> 00:04:55,540
uh,
here at Mit,

69
00:04:55,541 --> 00:05:00,541
at the gym,
you're given data on a thousand runs or 

70
00:05:01,570 --> 00:05:06,100
car or a car knowing nothing is using a 
monocular camera as a single input,

71
00:05:06,310 --> 00:05:11,310
driving over 30 miles an hour through a 
scene that has very little control 

72
00:05:11,310 --> 00:05:13,330
through very little capability to 
localize itself.

73
00:05:13,390 --> 00:05:18,390
It must act very quickly in that scene.
You're given a thousand runs to learn 

74
00:05:18,390 --> 00:05:18,640
anything.

75
00:05:21,470 --> 00:05:26,470
We'll discuss this in the coming weeks.
This competition will result in for 

76
00:05:27,811 --> 00:05:32,290
submissions that we evaluate everyone's 
in simulation,

77
00:05:32,710 --> 00:05:37,710
but the taskforce submissions we put 
head to head at the gym and until there 

78
00:05:37,871 --> 00:05:42,871
is a winner declared what keep slamming 
cars at 30 miles an hour deep crash and 

79
00:05:45,551 --> 00:05:49,180
also on the website is from last year.
And on get hub,

80
00:05:49,181 --> 00:05:54,181
there's deep tesla,
which is using the large scale 

81
00:05:54,181 --> 00:05:57,601
naturalistic driving data set.
We have to train a neural network to do 

82
00:05:57,601 --> 00:06:01,471
end to end steering.
It takes in monocular video from the 

83
00:06:01,471 --> 00:06:04,811
forward roadway and produce the steering
commands that steering commands for the 

84
00:06:04,821 --> 00:06:05,120
car

85
00:06:07,140 --> 00:06:12,140
lectures.
Today we'll talk about deep learning 

86
00:06:12,140 --> 00:06:15,170
tomorrow.
We'll talk about autonomous vehicles 

87
00:06:15,170 --> 00:06:16,260
drls on Wednesday,
driving,

88
00:06:16,261 --> 00:06:17,670
seeing,
understanding,

89
00:06:17,671 --> 00:06:20,970
so segmentation,
that's Thursday.

90
00:06:21,930 --> 00:06:26,930
On Friday we have Sasha or knew the 
director of engineering at Waymo.

91
00:06:27,570 --> 00:06:32,570
Waymo is one of the companies that's 
truly taking huge strides and fully 

92
00:06:32,570 --> 00:06:37,341
autonomous vehicles.
They're taking the fully l four l five 

93
00:06:37,341 --> 00:06:38,850
autonomous vehicle approach and it's 
fascinating to learn.

94
00:06:39,240 --> 00:06:44,240
He's also the head of perception for 
them to learn from him what kind of 

95
00:06:44,611 --> 00:06:47,310
problems they're facing,
what kind of approach they're taking on.

96
00:06:47,770 --> 00:06:49,320
We have a meal.
If Rizzoli,

97
00:06:49,590 --> 00:06:52,020
who won to last year,
speakers start to ask.

98
00:06:52,021 --> 00:06:55,110
Carmen said,
Amelia is the smartest person who knows,

99
00:06:55,770 --> 00:06:59,490
so Amelia for is the cto of autonomy and
autonomous vehicle,

100
00:07:00,190 --> 00:07:05,190
a company that was just acquired by 
Delphi for a large sum of money and 

101
00:07:05,221 --> 00:07:08,490
they're doing a lot of incredible work 
in Singapore and here in Boston.

102
00:07:10,320 --> 00:07:15,320
Next Wednesday we are going to talk 
about the topic of our research or my 

103
00:07:16,201 --> 00:07:19,820
personal fascination is deep learning 
for drivers,

104
00:07:19,821 --> 00:07:22,290
states sensing,
understanding the human perceiving,

105
00:07:22,291 --> 00:07:24,380
everything about the human being inside 
the car,

106
00:07:24,460 --> 00:07:29,460
outside the car.
One Talk I'm really excited about is 

107
00:07:29,950 --> 00:07:31,500
Oliver Cameron.
On Thursday,

108
00:07:32,640 --> 00:07:36,990
he is now the CEO of of autonomous 
vehicle startup voyage.

109
00:07:37,160 --> 00:07:41,400
He was previously the director of the 
self driving car program free udacity.

110
00:07:41,790 --> 00:07:45,990
He will talk about how to start a self 
driving car company.

111
00:07:46,440 --> 00:07:50,550
For those it said that Mit folks and 
entrepreneurs,

112
00:07:50,690 --> 00:07:55,690
if you want to start one yourself,
they'll tell you exactly how it's super 

113
00:07:55,690 --> 00:07:59,631
cool and then sterling Anderson who was 
the director previously,

114
00:08:01,360 --> 00:08:05,880
a tesla autopilot team and now is the 
cofounder of Aurora,

115
00:08:06,650 --> 00:08:11,650
the car,
the self driving car startup that I 

116
00:08:11,650 --> 00:08:13,680
mentioned that has now partnered with 
Nvidia and many others,

117
00:08:13,890 --> 00:08:18,890
so why self driving cars?
Those classes about applying data driven

118
00:08:19,710 --> 00:08:22,620
learning methods to the problem of 
autonomous vehicles.

119
00:08:23,280 --> 00:08:28,280
Why self driving cars are fascinating 
and interesting problem space quite 

120
00:08:30,571 --> 00:08:35,571
possibly in my opinion,
this is the first wide reaching and 

121
00:08:35,851 --> 00:08:40,851
profound integration of personal robots 
in society wide reaching because there's

122
00:08:41,551 --> 00:08:45,810
1 billion cars on the road.
Even a fraction of that will change

123
00:08:47,110 --> 00:08:52,110
the face of transportation and how we 
move about the world profound,

124
00:08:53,050 --> 00:08:56,160
and this is an important point that's 
always understood,

125
00:08:57,570 --> 00:09:02,570
is there's an intimate connection 
between a human and a vehicle.

126
00:09:04,290 --> 00:09:07,110
When there's a direct transfer of 
control,

127
00:09:07,620 --> 00:09:12,620
it's a direct transfer of control that 
takes that his or her life into the 

128
00:09:13,381 --> 00:09:15,180
hands of an artificial intelligence 
system.

129
00:09:16,110 --> 00:09:17,260
I showed a few quick,
quick,

130
00:09:17,960 --> 00:09:22,960
quick,
quick clips here you can google first 

131
00:09:22,960 --> 00:09:24,090
time with Tesla,
autopilot on Youtube,

132
00:09:24,270 --> 00:09:27,240
and watch people perform that transfer 
of control.

133
00:09:27,720 --> 00:09:32,720
There's something magical about a human 
and a robot working together that will 

134
00:09:34,230 --> 00:09:39,230
transform what artificial intelligence 
is in the 21st century and this 

135
00:09:39,601 --> 00:09:42,720
particular autonomous system,
ai system,

136
00:09:42,750 --> 00:09:47,750
self driving cars,
is on the scale and the profound,

137
00:09:47,880 --> 00:09:52,880
the life critical nature of it is 
profound in a way that will truly test 

138
00:09:52,880 --> 00:09:57,560
the capabilities of Ai.
There is a personal connection that will

139
00:09:58,350 --> 00:10:01,680
argue throughout these lectures that we 
cannot escape.

140
00:10:01,681 --> 00:10:06,681
Considering the human being,
that autonomous vehicle must not only 

141
00:10:06,681 --> 00:10:08,490
perceive and control its movement 
through the environment.

142
00:10:08,720 --> 00:10:13,720
It must also perceive everything about 
the human driver and the passenger and 

143
00:10:13,720 --> 00:10:13,770
interact,
communicate,

144
00:10:13,771 --> 00:10:15,270
and build trust with that driver.

145
00:10:20,270 --> 00:10:25,270
Because in my view,
as I will argue throughout this course,

146
00:10:27,170 --> 00:10:32,170
an autonomous vehicle is more of a 
personal robot than it is a perfect 

147
00:10:32,601 --> 00:10:37,601
perception control system.
Because perfect perception and control,

148
00:10:38,090 --> 00:10:43,090
so this world full of humans is 
extremely difficult and could be two,

149
00:10:44,601 --> 00:10:46,160
three,
four decades away,

150
00:10:46,940 --> 00:10:51,940
full autonomy.
Autonomous Vehicles are going to be 

151
00:10:51,940 --> 00:10:56,920
flawed,
they're going to have flaws and we'll 

152
00:10:56,920 --> 00:11:00,261
have to design systems that are 
effectively caught effectively transfer 

153
00:11:00,351 --> 00:11:03,950
control to human beings when they can't 
handle the situation,

154
00:11:04,130 --> 00:11:09,130
and that transfer of control isn't as a 
fascinating opportunity for ai because 

155
00:11:14,780 --> 00:11:19,640
the obstacle avoidance perception of 
obstacles,

156
00:11:19,670 --> 00:11:22,850
an obstacle avoidance is the easy 
problem.

157
00:11:23,900 --> 00:11:28,900
It's the safe problem.
Going 30 miles an hour and navigating 

158
00:11:28,900 --> 00:11:32,140
through streets of Boston is easy.
It's when you have to get to work in 

159
00:11:33,651 --> 00:11:38,360
your late or you're sick of the person 
in front of you that you want to go into

160
00:11:38,361 --> 00:11:41,000
the er in the opposing lane and speed 
up.

161
00:11:41,540 --> 00:11:46,540
That's human nature and we can't escape 
it are artificial assist intelligence 

162
00:11:46,581 --> 00:11:50,210
systems can't escape human nature.
They must work with it.

163
00:11:50,990 --> 00:11:54,940
What's shown here is one of the 
algorithms will talk about next week for

164
00:11:54,941 --> 00:11:59,941
cognitive load or we take the raw 3d 
accomplished in your networks,

165
00:12:00,680 --> 00:12:03,280
taken the eye region,
the blinking,

166
00:12:03,310 --> 00:12:06,490
and the pupil movement to determine the 
cognitive load of the driver.

167
00:12:06,910 --> 00:12:09,790
We'll see how we can detect everything 
about the driver,

168
00:12:09,880 --> 00:12:12,100
where they're looking.
Emotion,

169
00:12:12,550 --> 00:12:14,530
cognitive load,
body pose,

170
00:12:14,531 --> 00:12:17,200
estimation,
drowsiness,

171
00:12:18,420 --> 00:12:22,900
the the.
The movement towards full autonomy is so

172
00:12:22,901 --> 00:12:27,901
difficult.
I would argue that it almost requires 

173
00:12:27,901 --> 00:12:32,170
human level intelligence that the,
as I said,

174
00:12:32,200 --> 00:12:32,710
two,
three,

175
00:12:32,720 --> 00:12:37,720
four decade out journey for artificial 
intelligence researchers to achieve full

176
00:12:38,291 --> 00:12:42,010
autonomy will require achieving,
solving some of the problems,

177
00:12:42,011 --> 00:12:45,010
fundamental problems of creating 
intelligence,

178
00:12:46,720 --> 00:12:51,720
and that's something we'll discuss in 
much more depth in a broader view in two

179
00:12:52,301 --> 00:12:57,301
weeks.
For the artificial general intelligence 

180
00:12:57,301 --> 00:12:58,180
course where we have Andrea neuropathy 
from Tesla,

181
00:12:58,181 --> 00:13:01,480
Ray Kurzweil,
Mark Robert from Boston,

182
00:13:01,481 --> 00:13:06,481
dynamics who asked for the dimensions of
the room because he's bringing robots.

183
00:13:08,500 --> 00:13:12,610
Nothing else was told to me,
it'll be a surprise.

184
00:13:16,060 --> 00:13:21,060
So that is why I argued the human 
centered artificial intelligence 

185
00:13:21,060 --> 00:13:23,200
approach in every algorithm design 
considers the human

186
00:13:26,230 --> 00:13:31,060
for autonomous vehicle on the left,
the perception seen understanding,

187
00:13:31,450 --> 00:13:36,450
and the control problem as we'll explore
through the competitions and the 

188
00:13:36,450 --> 00:13:39,841
assignments of this course can handle 90
and increasing percent of the cases,

189
00:13:42,430 --> 00:13:47,430
but it's the ten one point one percent 
of the cases as we get better and better

190
00:13:48,370 --> 00:13:53,370
that we have to,
we're not able to handle to these 

191
00:13:53,370 --> 00:13:56,731
methods.
And that's where the human perceiving 

192
00:13:56,731 --> 00:13:59,190
the human is really important.
This is the video from last year of Arc 

193
00:13:59,190 --> 00:14:00,100
de Triomphe.
Thank you.

194
00:14:00,190 --> 00:14:01,720
I didn't know what last year.
I know now.

195
00:14:03,180 --> 00:14:08,180
That's one of millions of cases where 
human to human interaction is the is the

196
00:14:10,570 --> 00:14:15,570
dominant driver,
not the basic perception control 

197
00:14:15,570 --> 00:14:20,191
problem.
So why deep learning in this space,

198
00:14:23,760 --> 00:14:28,760
because deep learning is a set of 
methods that do well from a lot of data 

199
00:14:31,500 --> 00:14:35,520
and to solve these problems or human 
life is a stake.

200
00:14:35,760 --> 00:14:39,750
We have to be able to have techniques 
that learn from data,

201
00:14:39,930 --> 00:14:44,930
learn from real world data.
This is the fundamental reality of 

202
00:14:44,930 --> 00:14:46,890
artificial intelligence systems that 
operate in the real world.

203
00:14:47,040 --> 00:14:51,090
They must learn from real world data,
whether that's on the left,

204
00:14:51,230 --> 00:14:53,000
the perception,
the control side,

205
00:14:54,830 --> 00:14:59,030
on the right for the human,
the perception and the communication,

206
00:14:59,031 --> 00:15:04,031
interaction and collaboration with the 
human and the human robot interaction.

207
00:15:06,760 --> 00:15:09,580
Okay,
so what is deep learning?

208
00:15:13,200 --> 00:15:18,200
It's a set of techniques.
If you allow me the definition of 

209
00:15:18,200 --> 00:15:20,520
intelligence being the ability to 
accomplish complex goals,

210
00:15:21,480 --> 00:15:24,690
then I would argue definition of 
understanding,

211
00:15:25,080 --> 00:15:30,080
maybe reasoning is the ability to turn 
complex information into simple,

212
00:15:31,440 --> 00:15:36,440
useful,
actionable information and that is what 

213
00:15:36,440 --> 00:15:39,300
deep learning does.
Deep learning is representation,

214
00:15:39,301 --> 00:15:42,300
learning or feature learning,
if you will.

215
00:15:43,200 --> 00:15:48,200
It's able to take raw information,
raw complicated information that's hard 

216
00:15:48,541 --> 00:15:53,541
to do.
Anything with and construct here are 

217
00:15:53,541 --> 00:15:56,030
hierarchical representation of that 
information to be able to do something 

218
00:15:56,030 --> 00:15:59,991
interesting with it.
It is the branch of artificial 

219
00:15:59,991 --> 00:16:02,790
intelligence which is most capable and 
focused on this task.

220
00:16:04,030 --> 00:16:06,030
For me,
representations from data,

221
00:16:06,360 --> 00:16:08,340
whether it's supervised and 
unsupervised,

222
00:16:08,370 --> 00:16:10,410
whether it's with the help of humans or 
not,

223
00:16:10,770 --> 00:16:15,770
it's able to construct structure,
find structure in the data such that you

224
00:16:16,591 --> 00:16:19,740
can extract simple,
useful,

225
00:16:19,741 --> 00:16:24,741
actionable information.
On the left for me in Goodfellas book is

226
00:16:27,720 --> 00:16:30,060
the basic example of a 
misclassification,

227
00:16:30,840 --> 00:16:35,840
the input of the image on the bottom 
with the raw pixels,

228
00:16:36,560 --> 00:16:41,560
and as we go up the stack as a go up,
the layers hiring higher order 

229
00:16:41,560 --> 00:16:45,881
representations of formed from edges to 
contours the corners to object parts,

230
00:16:46,400 --> 00:16:50,030
and then finally the full object 
semantic classification of what's in the

231
00:16:50,031 --> 00:16:53,210
image.
This is representation learning.

232
00:16:54,300 --> 00:16:55,760
A favorite example for me

233
00:16:57,580 --> 00:17:02,580
is one from for centuries ago,
our place in the universe and 

234
00:17:05,281 --> 00:17:09,510
representing that place in the universe,
whether it's relative to Earth,

235
00:17:09,810 --> 00:17:14,810
are relative to the sun.
On the left is our current belief.

236
00:17:16,930 --> 00:17:21,760
On the right is the one that is held 
widely for centuries ago.

237
00:17:23,290 --> 00:17:28,290
Representation matters because what's on
the right is much more complicated than 

238
00:17:28,331 --> 00:17:29,140
what's on the left.

239
00:17:34,400 --> 00:17:39,400
You can think of in a simple case here,
when the task is to draw a line that 

240
00:17:39,400 --> 00:17:43,711
separates green triangles and blue 
circles in the cartesian coordinate 

241
00:17:43,711 --> 00:17:45,560
space on the left,
the task is much more difficult.

242
00:17:45,590 --> 00:17:49,320
Impossible to do well on the right is 
trivial.

243
00:17:49,470 --> 00:17:54,470
In polar coordinates,
this transformation is exactly what we 

244
00:17:54,661 --> 00:17:59,661
need to learn.
This is representation learning so you 

245
00:17:59,661 --> 00:18:01,740
can take the same task of having to draw
a line that separates the blue curve and

246
00:18:01,741 --> 00:18:05,520
the red curve on the left.
If we draw a straight line,

247
00:18:05,730 --> 00:18:10,730
it's going to be a high.
There is no way to do it was zero error 

248
00:18:11,380 --> 00:18:16,080
with 100 percent accuracy.
Shown on the right is our best attempt,

249
00:18:18,510 --> 00:18:23,510
but what we can do with deep learning 
with a single hidden layer network done 

250
00:18:23,510 --> 00:18:27,441
here is formed the the topology,
the mapping of the space in such a way 

251
00:18:28,741 --> 00:18:31,980
in the middle that allows for a straight
line to be drawn.

252
00:18:31,981 --> 00:18:34,080
This separates the blue curve and the 
red curve.

253
00:18:35,130 --> 00:18:40,130
The learning of the function in the 
middle is what we're able to achieve 

254
00:18:40,130 --> 00:18:43,770
with deep learning.
It's taking raw,

255
00:18:43,800 --> 00:18:47,820
complicated information and making it 
simple,

256
00:18:48,600 --> 00:18:50,460
actionable,
useful,

257
00:18:51,840 --> 00:18:56,840
and the point is that this kind of 
ability to learn from raw sensory 

258
00:18:57,290 --> 00:19:01,620
information means that we can do a lot 
more with a lot more data,

259
00:19:03,030 --> 00:19:06,150
so deep learning gets better with more 
data

260
00:19:09,640 --> 00:19:12,220
and that's important for real world 
applications

261
00:19:14,470 --> 00:19:19,470
were edge cases are everything.
This is us driving to perception control

262
00:19:20,801 --> 00:19:25,801
systems.
One is an tesla vehicle with the 

263
00:19:25,801 --> 00:19:28,360
autopilot version one system that's 
using a monocular camera to perceive the

264
00:19:28,361 --> 00:19:33,361
external environment and produce control
decisions and our own neural network 

265
00:19:33,701 --> 00:19:38,701
running an adjustment [inaudible] that's
taking in the same with a monocular 

266
00:19:38,701 --> 00:19:42,601
camera and producing controlled 
decisions and the two systems argue and 

267
00:19:43,181 --> 00:19:46,750
when they disagree,
they raise up a flag to say that this is

268
00:19:46,751 --> 00:19:49,210
an edge case.
These that needs human intervention.

269
00:19:50,350 --> 00:19:55,350
There is covering such edge cases using 
machine learning is the main problem,

270
00:19:57,010 --> 00:20:00,850
our artificial intelligence and in when 
applied to the real world,

271
00:20:01,150 --> 00:20:05,620
it is the main problem to solve.
Okay,

272
00:20:06,190 --> 00:20:11,020
so what are neural networks inspired?
Very loosely,

273
00:20:11,080 --> 00:20:16,080
and I'll discuss about the key 
difference here in our own brains and 

274
00:20:16,080 --> 00:20:19,630
artificial brains because there's a lot 
of insights in that difference,

275
00:20:20,350 --> 00:20:23,890
but inspired loosely by biological 
neural networks.

276
00:20:23,891 --> 00:20:28,450
Here is a simulation of a 
thalamocortical brain network,

277
00:20:28,480 --> 00:20:33,400
which is only 3 million neurons,
476 million synapses.

278
00:20:33,430 --> 00:20:37,330
The full human brain is a lot more than 
that 100 billion neurons.

279
00:20:37,840 --> 00:20:42,840
One thousand trillion synapses.
There's inspirational music with this 

280
00:20:47,951 --> 00:20:52,951
one that I didn't realize was here 
should make you think artificial neural 

281
00:20:54,520 --> 00:20:56,010
networks.
Yeah,

282
00:20:56,020 --> 00:20:59,870
let's just let it play.
The.

283
00:21:00,500 --> 00:21:03,610
The human neural network is 100 billion 
neurons,

284
00:21:03,611 --> 00:21:06,130
right?
One thousand trillion synapses,

285
00:21:06,910 --> 00:21:11,830
one of the state of the state of the art
neural networks as resident at 1:52,

286
00:21:12,010 --> 00:21:14,800
which has $60,
million synapses.

287
00:21:17,160 --> 00:21:22,160
That's a difference of about a seven 
order of magnitude difference.

288
00:21:22,410 --> 00:21:27,410
The human brains have 10 million times 
more synapses than artificial neural 

289
00:21:27,421 --> 00:21:32,160
networks plus or minus one order of 
magnitude depending on the network.

290
00:21:34,090 --> 00:21:38,820
So what's the difference between a 
biological neuron and artificial neuron?

291
00:21:40,210 --> 00:21:42,760
The topology of the human brain have no 
layers.

292
00:21:42,820 --> 00:21:47,740
Neural networks are stacked and layers,
they're fixed for the most part.

293
00:21:49,000 --> 00:21:53,410
There is chaos.
Very little structure in our human brain

294
00:21:53,411 --> 00:21:58,411
in terms of Han neurons are connected.
They're connected often to 10,000

295
00:21:58,540 --> 00:22:03,540
plus other neurons.
The number of synopsis from individual 

296
00:22:03,540 --> 00:22:03,540
neurons that are,
uh,

297
00:22:03,540 --> 00:22:08,500
that are input into the neuron is huge.
There are asynchronous.

298
00:22:08,530 --> 00:22:11,020
The human brain brain works 
asynchronously.

299
00:22:11,230 --> 00:22:13,270
Artificial neural networks work 
synchronously.

300
00:22:15,450 --> 00:22:18,990
The learning algorithm for artificial 
neuron networks,

301
00:22:19,500 --> 00:22:24,120
the only one,
the best one is backpropagation.

302
00:22:25,470 --> 00:22:30,470
And we don't know how human brains learn
processing speed.

303
00:22:35,680 --> 00:22:40,680
This is one of the,
the only benefits we have with 

304
00:22:40,680 --> 00:22:44,320
artificial neural networks is artificial
neurons are faster,

305
00:22:45,430 --> 00:22:48,250
but they're also extremely power and 
efficient.

306
00:22:50,730 --> 00:22:55,730
And there is a division into stages of 
training and testing when you're on that

307
00:22:55,791 --> 00:22:56,850
works,
uh,

308
00:22:56,851 --> 00:22:57,720
with,
uh,

309
00:22:57,750 --> 00:23:00,870
biological neural networks is,
you're sitting here today,

310
00:23:00,990 --> 00:23:05,070
they're always learning.
The only profound similarity,

311
00:23:05,730 --> 00:23:09,060
the inspiring one,
the captivating one,

312
00:23:09,420 --> 00:23:13,380
is that both are distributed computation
at scale.

313
00:23:14,850 --> 00:23:19,850
There is an emergent aspect to neural 
networks where the basic element of 

314
00:23:21,241 --> 00:23:25,230
computation,
a neuron is simple,

315
00:23:25,510 --> 00:23:28,800
is extremely simple,
but when connected together,

316
00:23:29,220 --> 00:23:31,530
beautiful,
amazing,

317
00:23:32,130 --> 00:23:34,170
powerful,
approximators can be formed.

318
00:23:34,970 --> 00:23:39,300
A neural network is built up with these 
computational units where the inputs,

319
00:23:39,540 --> 00:23:42,300
there's a set of edges with weights on 
them.

320
00:23:43,170 --> 00:23:47,000
The edges are the weights are by this 
input signal.

321
00:23:47,720 --> 00:23:52,720
A bias is added with a nonlinear 
function that determines whether the 

322
00:23:53,361 --> 00:23:58,361
network gets activated or not.
The neuron gets activated or not 

323
00:23:58,361 --> 00:24:02,440
visualized here,
and these neurons can be combined in a 

324
00:24:02,440 --> 00:24:06,401
mall in number of ways.
It can form a feed forward and will not 

325
00:24:06,401 --> 00:24:09,820
work or they can feed back into itself 
to form to have state memory in 

326
00:24:13,761 --> 00:24:18,761
recurrent neural networks.
The ones on the left are the ones that 

327
00:24:18,761 --> 00:24:23,141
are most successful for most 
applications in computer vision.

328
00:24:24,050 --> 00:24:29,050
The ones in the right are very popular 
and specific when temporal dynamics or 

329
00:24:29,091 --> 00:24:31,550
dynamics time series of any kind are 
used.

330
00:24:32,030 --> 00:24:37,030
In fact,
the ones in the right a much closer to 

331
00:24:37,030 --> 00:24:39,230
the way our human brains are and the 
ones on the left,

332
00:24:40,160 --> 00:24:42,020
but that's why they're really hard to 
train.

333
00:24:45,090 --> 00:24:50,090
One beautiful aspect of this emerging 
power from multiple neurons being 

334
00:24:50,461 --> 00:24:55,461
connected together is the universal 
property that with a single hidden 

335
00:24:55,461 --> 00:24:58,560
layer,
these networks can learn any function,

336
00:24:58,620 --> 00:25:03,620
learn to approximate at any function 
which is an important property to be 

337
00:25:03,620 --> 00:25:07,671
aware of because the limits here are not
in the power of the networks.

338
00:25:11,040 --> 00:25:16,040
The limits in the is in the methods by 
which we construct them and train them.

339
00:25:21,870 --> 00:25:25,740
What kinds of machine learning deep 
learning are there.

340
00:25:26,430 --> 00:25:31,430
We can separate into two categories,
memorizers now,

341
00:25:34,831 --> 00:25:37,410
uh,
the approaches that essentially memorize

342
00:25:37,411 --> 00:25:42,411
patterns in the data and approaches that
we can loosely say are beginning to 

343
00:25:43,800 --> 00:25:47,640
reason to generalize over the data with 
minimal human input.

344
00:25:47,970 --> 00:25:52,970
On top,
on the left are the quote unquote 

345
00:25:52,970 --> 00:25:55,490
teachers is how much human input and 
blue is needed to make the method 

346
00:25:55,490 --> 00:25:59,841
successful for supervised learning,
which is what most of the deep learning 

347
00:25:59,841 --> 00:26:02,310
success has come from.
Or most of the data's annotated by human

348
00:26:02,311 --> 00:26:07,311
beings.
The human is at the core of the success.

349
00:26:07,560 --> 00:26:12,560
Most of the data that's part of the 
training needs to be annotated by human 

350
00:26:12,560 --> 00:26:15,831
beings was some additional successes 
coming from augmentation methods that 

351
00:26:16,560 --> 00:26:21,560
extend that extend the data based on 
which is networks have trained and the 

352
00:26:27,040 --> 00:26:31,140
semi-supervised reinforcement learning 
and unsupervised methods that we'll talk

353
00:26:31,141 --> 00:26:36,141
about later in the course.
That's where the near term successes we 

354
00:26:36,151 --> 00:26:39,480
hope are and where the unsupervised 
learning approaches,

355
00:26:39,540 --> 00:26:44,540
that's where the true excitement about 
the possibilities of artificial lie 

356
00:26:44,700 --> 00:26:49,700
being able to make sense of our world 
with minimal input from humans.

357
00:26:53,300 --> 00:26:58,300
So we can think of two kinds of deep 
learning impacts spaces.

358
00:27:00,080 --> 00:27:04,610
One is a special purpose intelligence is
taking a problem,

359
00:27:04,670 --> 00:27:07,640
formalizing it,
collecting enough data on it,

360
00:27:07,700 --> 00:27:12,700
and being able to solve a particular 
case that's that provides value of 

361
00:27:16,371 --> 00:27:21,371
particular interest.
Here is a a network that estimates 

362
00:27:21,371 --> 00:27:24,401
apartment costs in the Boston area,
so you could take the number of 

363
00:27:24,401 --> 00:27:25,490
bedrooms,
the square feet and the neighborhood and

364
00:27:25,491 --> 00:27:29,120
provide is.
I'll put the estimated costs on the.

365
00:27:29,220 --> 00:27:33,680
On the right is the actual data of 
apartment cost.

366
00:27:34,130 --> 00:27:39,130
We're actually standing at a in a area 
that has over $3,000

367
00:27:40,131 --> 00:27:41,180
for a studio apartment.

368
00:27:44,200 --> 00:27:49,200
Some of you may be feeling that pain and
then there's general purpose 

369
00:27:49,241 --> 00:27:54,241
intelligence or something that feels 
like approaching general purpose 

370
00:27:55,271 --> 00:28:00,271
intelligence,
which is reinforcement and unsupervised 

371
00:28:00,271 --> 00:28:00,640
learning.
Here with Andrea,

372
00:28:00,641 --> 00:28:03,000
come from Monica potties Pong,
the pixels,

373
00:28:03,200 --> 00:28:08,200
a system that takes in 80 by 80 pixel 
image and with no other information is 

374
00:28:08,200 --> 00:28:10,420
able to beat,
is able to win at this game,

375
00:28:10,780 --> 00:28:13,330
no information except a sequence of 
images,

376
00:28:13,450 --> 00:28:16,000
raw sensory information,
the same way,

377
00:28:16,090 --> 00:28:19,750
the same kind of information that human 
beings taken from the visual,

378
00:28:19,751 --> 00:28:23,470
audio touch,
sensory data,

379
00:28:23,530 --> 00:28:28,530
the very low level data and be able to 
learn to win and this very simplistic 

380
00:28:28,530 --> 00:28:31,240
and it's very artificially constructed 
world,

381
00:28:31,360 --> 00:28:35,170
but nevertheless a world where no 
feature learning is performed.

382
00:28:35,440 --> 00:28:40,440
Only raw sensory information is used to 
win with very sparse minimal human 

383
00:28:40,721 --> 00:28:45,430
input.
We'll talk about that on Wednesday.

384
00:28:46,900 --> 00:28:51,900
We're deep reinforcement learning so but
for now we'll focus on supervised 

385
00:28:52,721 --> 00:28:57,430
learning where there is input data,
there is a network.

386
00:28:57,431 --> 00:29:02,431
We're trying to train a learning system 
and there's a correct output that's 

387
00:29:02,431 --> 00:29:07,230
labeled by human beings,
that's the general training process for 

388
00:29:07,230 --> 00:29:11,460
neural network input,
data labels and the training of that 

389
00:29:11,530 --> 00:29:13,000
network,
that model,

390
00:29:13,360 --> 00:29:18,360
so that in the testing stage,
our new input data that has never seen 

391
00:29:18,360 --> 00:29:22,501
before as task with producing guesses 
and is evaluated based on that for 

392
00:29:23,171 --> 00:29:28,171
autonomous vehicles,
that means being released either in 

393
00:29:28,171 --> 00:29:28,660
simulation or in the real world to 
operate

394
00:29:32,200 --> 00:29:37,200
and how they learn,
how neural networks learn is given the 

395
00:29:37,200 --> 00:29:40,930
forward pass of taking the input data,
whether it's from the training stage,

396
00:29:42,190 --> 00:29:44,920
in the training stage,
the taking the input data,

397
00:29:44,921 --> 00:29:48,040
producing a prediction,
and then given that there's ground truth

398
00:29:48,041 --> 00:29:49,540
in the training stage,
we can,

399
00:29:49,710 --> 00:29:54,710
we can have a measure of error based on 
a loss function that then punishes the,

400
00:29:55,920 --> 00:29:57,100
uh,
the synapses,

401
00:29:57,101 --> 00:30:02,101
the connections,
the parameters that were involved with 

402
00:30:02,101 --> 00:30:04,840
making that a,
that wrong prediction.

403
00:30:07,300 --> 00:30:10,790
A back propagates the error through 
those weights.

404
00:30:11,270 --> 00:30:13,730
We'll discuss that a little bit more 
detail in a bit here.

405
00:30:14,810 --> 00:30:18,110
So what can we do with deep learning?
You can do one to one mapping.

406
00:30:18,800 --> 00:30:23,800
Really,
you can think of input as being 

407
00:30:23,800 --> 00:30:23,800
anything.
It can be a number of vector numbers,

408
00:30:23,800 --> 00:30:25,220
a sequence of numbers,
a sequence of vector,

409
00:30:25,221 --> 00:30:27,980
of numbers,
anything you can think of from images to

410
00:30:27,981 --> 00:30:30,770
video to audio to text and represented 
in this way,

411
00:30:31,010 --> 00:30:35,660
and the output can the same be a single 
number or it can be images,

412
00:30:35,661 --> 00:30:36,860
video,
text,

413
00:30:37,250 --> 00:30:39,290
audio,
one to one,

414
00:30:39,291 --> 00:30:41,300
mapping on the bottom,
one to many,

415
00:30:41,301 --> 00:30:46,301
many to many to many and many to many 
with different starting points for the 

416
00:30:47,751 --> 00:30:50,120
data,
a synchronous,

417
00:30:53,340 --> 00:30:58,340
some quick terms that will come up.
Deep learning is the same as neural 

418
00:30:58,340 --> 00:31:01,500
networks.
It's really deep neural networks,

419
00:31:01,830 --> 00:31:06,830
large neural networks.
It's a subset of machine learning that 

420
00:31:06,830 --> 00:31:09,690
has been extremely successful in the 
past decade.

421
00:31:10,530 --> 00:31:13,110
Multilayer Perceptron,
deep neural network,

422
00:31:13,170 --> 00:31:16,530
recurrent neural network,
long short term memory network,

423
00:31:16,531 --> 00:31:18,990
Lstm,
convolutional neural network,

424
00:31:19,290 --> 00:31:24,290
and deep belief networks.
All of these will come up through the 

425
00:31:24,290 --> 00:31:26,751
slides and there is specific operations 
layers within these networks of 

426
00:31:28,651 --> 00:31:31,650
convolution pooling activation and 
backpropagation.

427
00:31:31,920 --> 00:31:36,060
This concepts that we'll discuss in this
class,

428
00:31:36,840 --> 00:31:41,840
activation functions.
There's a lot of variance than the left 

429
00:31:41,881 --> 00:31:43,620
is the activation function and left 
column,

430
00:31:44,170 --> 00:31:47,910
and the x axis is the input.
On the y axis is the output,

431
00:31:49,440 --> 00:31:51,270
the sigmoid function,
the output.

432
00:31:52,080 --> 00:31:56,640
If the font is too small,
the output is not centered at zero.

433
00:31:58,770 --> 00:32:02,130
For the Tan age function,
it's centered at zero,

434
00:32:02,131 --> 00:32:04,170
but it's still suffers from vantage 
ingredients.

435
00:32:04,890 --> 00:32:09,630
Vanishing gradients is one of the value.
The input is low or high.

436
00:32:11,770 --> 00:32:12,810
The,
uh,

437
00:32:12,820 --> 00:32:17,820
the output of the network,
because you see in the right column 

438
00:32:17,820 --> 00:32:20,581
there,
the derivative of the function is very 

439
00:32:20,581 --> 00:32:23,221
low,
so the learning rate is very low for 

440
00:32:23,221 --> 00:32:27,180
revenue,
not it's also not zero centered,

441
00:32:28,630 --> 00:32:31,030
but it does not suffer from vanishing 
gradients.

442
00:32:32,470 --> 00:32:34,510
Backpropagation is the process of 
learning.

443
00:32:34,990 --> 00:32:37,720
It's the way we take go from error 
computers,

444
00:32:37,721 --> 00:32:40,510
the last function and bottom right of 
the slide,

445
00:32:40,580 --> 00:32:44,450
taking the actual output of the network 
with the Ford Pass,

446
00:32:44,600 --> 00:32:49,600
subtracting it from the ground truth 
squaring dividing were to and using that

447
00:32:51,471 --> 00:32:56,471
lost function than back propagate 
through to construct a great aunt to 

448
00:32:56,471 --> 00:32:59,560
back propagate the error to the weights 
that we're responsible for making either

449
00:32:59,570 --> 00:33:04,570
correct or incorrect decision.
So the subtests that there's a forward 

450
00:33:04,570 --> 00:33:08,810
pass as a backward pass and a fraction 
of the weights,

451
00:33:08,811 --> 00:33:11,180
gradients of tractor from the weight,
that's it.

452
00:33:11,720 --> 00:33:16,720
That process is modular,
so it's local to each individual neuron,

453
00:33:17,000 --> 00:33:22,000
which is why it's extremely dis.
We're able to distribute it across 

454
00:33:22,250 --> 00:33:27,250
multiple across the GPU parallelize 
across the GPU,

455
00:33:31,010 --> 00:33:36,010
so learning for a neural network.
These competition units are extremely 

456
00:33:36,441 --> 00:33:41,441
simple,
extremely simple to then correct when 

457
00:33:41,441 --> 00:33:45,071
they make an error,
when they're part of a larger network 

458
00:33:45,071 --> 00:33:47,231
that makes an error and all that boils 
down to is essentially an optimization 

459
00:33:47,231 --> 00:33:51,551
problem where the objective utility 
function is the loss function and the 

460
00:33:52,041 --> 00:33:54,980
goal is to minimize it and we have to 
update the parameters,

461
00:33:54,981 --> 00:33:59,090
the weights and the synapses and the 
biases to decrease that loss function.

462
00:34:01,900 --> 00:34:04,000
And that last function is highly 
nonlinear.

463
00:34:06,520 --> 00:34:08,950
Depending on the activation functions,
different properties,

464
00:34:08,951 --> 00:34:13,951
different issues arise.
There's vanishing gradients for sigmoid 

465
00:34:15,300 --> 00:34:18,900
where the learning can be slow.
There's dying.

466
00:34:18,901 --> 00:34:23,800
Rarely use where the derivatives exactly
zero,

467
00:34:24,610 --> 00:34:29,610
four inputs less than zero.
There are solutions to this like leaky 

468
00:34:30,391 --> 00:34:35,391
rallies and a bunch of details that you 
may discover when you try to win the 

469
00:34:35,391 --> 00:34:38,460
deep traffic competition,
but for the most part these are the main

470
00:34:38,461 --> 00:34:43,461
activation functions and it's the choice
of the you'll network designer,

471
00:34:45,360 --> 00:34:49,070
which one works best.
They're saddle points,

472
00:34:49,100 --> 00:34:54,100
all the problems from numerical,
nonlinear optimization that arise come 

473
00:34:54,100 --> 00:34:58,811
up here.
It's hard to break symmetry and 

474
00:34:59,840 --> 00:35:04,840
stochastic gradient descent without any 
kind of tricks to it can take a very 

475
00:35:05,511 --> 00:35:10,511
long time to arrive at the minimum.
One of the biggest problems in all of 

476
00:35:11,801 --> 00:35:14,860
machine learning and certainly in deep 
learning is overfitting.

477
00:35:15,700 --> 00:35:20,700
You can think of the blue dots and 
applied here as the data to which we 

478
00:35:20,700 --> 00:35:20,980
want to fit a curve.

479
00:35:22,750 --> 00:35:27,310
We want to design a learning system that
approximates the aggression of that,

480
00:35:27,450 --> 00:35:28,540
uh,
of this data.

481
00:35:29,350 --> 00:35:34,350
So in green is a sine curve,
simple fits well,

482
00:35:35,470 --> 00:35:40,470
and then there's a ninth degree 
polynomial which fits even better in 

483
00:35:40,470 --> 00:35:43,170
terms of the error,
but it clearly over fits this data.

484
00:35:43,500 --> 00:35:48,500
If there's other data that has not seen 
yet that it has to fit,

485
00:35:49,470 --> 00:35:52,250
it's likely to produce a high error,
so it's overfitting.

486
00:35:52,260 --> 00:35:57,260
The training set.
This is a big problem for small data 

487
00:35:57,260 --> 00:36:00,210
sets and so we have to fix that with 
regularization.

488
00:36:00,780 --> 00:36:05,780
Regularization is a set of methodologies
that prevent overfitting learning the 

489
00:36:06,301 --> 00:36:11,301
training too well in order and then to 
not be able to generalize to the testing

490
00:36:11,521 --> 00:36:16,521
stage and overfitting.
The main symptom is the air decreases in

491
00:36:18,241 --> 00:36:20,460
training set but increases in the test 
set,

492
00:36:22,360 --> 00:36:27,360
so there's a lot of techniques and 
traditional machine learning that deal 

493
00:36:27,360 --> 00:36:27,360
with this and cross validation,
so on,

494
00:36:27,360 --> 00:36:30,520
but because of the cost of training for 
neural networks,

495
00:36:31,120 --> 00:36:35,050
it's traditional to use of what's called
the validation set,

496
00:36:35,560 --> 00:36:39,040
so you create a subset of the training 
they use,

497
00:36:39,220 --> 00:36:44,220
keep away for which you have the ground 
truth and use that as a representative 

498
00:36:44,470 --> 00:36:49,470
of the testing set so you perform early 
stoppage or more realistically.

499
00:36:49,841 --> 00:36:54,841
Just save a checkpoint often to see how 
as the training evolves,

500
00:36:57,310 --> 00:37:02,310
the performance changes on the 
validation set and so you can stop when 

501
00:37:03,041 --> 00:37:05,590
the performance and the validation set 
is getting a lot worse.

502
00:37:05,770 --> 00:37:08,080
It means you're overtraining on the 
training set.

503
00:37:11,840 --> 00:37:13,250
In practice,
of course,

504
00:37:13,370 --> 00:37:16,600
we run training which longer and see 
when a,

505
00:37:16,790 --> 00:37:19,230
what is the best performing,
uh,

506
00:37:19,250 --> 00:37:24,250
what,
what is the best performing snapshot 

507
00:37:24,250 --> 00:37:26,921
checkpoint of the network dropout is 
another very powerful regularization 

508
00:37:27,591 --> 00:37:29,780
technique.
What were you randomly remove?

509
00:37:29,781 --> 00:37:33,410
Part of the network.
Randomly remove some of the nodes in the

510
00:37:33,411 --> 00:37:37,790
network along with its incoming and 
outgoing edges.

511
00:37:38,420 --> 00:37:43,420
So what that really looks like is a 
probability of keeping a node and in 

512
00:37:43,420 --> 00:37:47,510
many deep learning frameworks today,
it comes with a dropout layer.

513
00:37:47,540 --> 00:37:51,960
So it's essentially a probability that's
usually greater than point five than a,

514
00:37:51,961 --> 00:37:56,210
that a node will be kept for the input 
layer,

515
00:37:56,300 --> 00:37:59,960
the probability should be much higher or
more effectively.

516
00:38:00,140 --> 00:38:03,410
What works well is just adding noise.
What's the point here?

517
00:38:03,830 --> 00:38:08,830
You want to create enough diversity in 
the training data such that it is 

518
00:38:09,291 --> 00:38:14,291
generalizable to the testing and as 
you'll see with deep traffic,

519
00:38:15,041 --> 00:38:20,041
competition is l two and l one penalty 
weight decay way penalty where there's a

520
00:38:22,121 --> 00:38:24,850
penalization on the weights.
They get too large.

521
00:38:25,060 --> 00:38:28,660
The [inaudible] penalty keeps the 
weights small unless the aero derivative

522
00:38:28,661 --> 00:38:33,661
is huge and produces a smoother model 
and prefers to distribute when there is 

523
00:38:36,250 --> 00:38:41,250
too similar inputs.
That prefers to put half the weights on 

524
00:38:41,250 --> 00:38:44,041
each distribute the weights as opposed 
to putting the weight on one of the 

525
00:38:44,041 --> 00:38:44,041
edges,

526
00:38:45,260 --> 00:38:49,940
makes the network more robust.
Our one penalty has the one benefit that

527
00:38:49,941 --> 00:38:53,810
for really large weights,
they're allowed to be to stay,

528
00:38:54,350 --> 00:38:57,020
so it's allows her a few ways to remain 
very large.

529
00:38:57,680 --> 00:39:02,680
These are the regularization techniques 
and I wanted to mention them because 

530
00:39:02,680 --> 00:39:06,251
they're useful to some of the 
competitions here in the course and I 

531
00:39:06,251 --> 00:39:09,611
recommend to go to a playground and 
tenser and tenser flow playground to 

532
00:39:09,611 --> 00:39:13,040
play around with some of these 
parameters where you get to online,

533
00:39:13,041 --> 00:39:15,560
in the browser,
play around with different inputs,

534
00:39:15,561 --> 00:39:20,561
different features,
different number of layers and 

535
00:39:20,561 --> 00:39:20,561
regularization techniques,
uh,

536
00:39:20,561 --> 00:39:22,910
and to build your intuition about 
classification,

537
00:39:22,911 --> 00:39:26,000
regression problems given different 
input data sets.

538
00:39:28,920 --> 00:39:33,450
So what changed why over the past many 
decades,

539
00:39:34,140 --> 00:39:39,140
neural networks that have gone through 
two winters are now again dominating the

540
00:39:39,631 --> 00:39:43,720
artificial intelligence community CPU 
GPU,

541
00:39:44,610 --> 00:39:49,610
a six.
So computational power has skyrocketed 

542
00:39:49,610 --> 00:39:53,810
from Moore's law to gps.
There is huge data set including image 

543
00:39:55,401 --> 00:39:56,840
net and others.

544
00:39:58,740 --> 00:40:03,740
There is research backpropagation in the
eighties,

545
00:40:04,760 --> 00:40:06,540
uh,
uh,

546
00:40:06,541 --> 00:40:09,250
the convolutional neural networks,
Lstm,

547
00:40:09,540 --> 00:40:14,540
there's been a lot of interesting 
breakthroughs about how to design these 

548
00:40:14,540 --> 00:40:18,531
architectures,
how to build them such that they're 

549
00:40:18,531 --> 00:40:18,531
trainable,
efficiently using gps.

550
00:40:20,400 --> 00:40:25,400
There is the software infrastructure 
from being able to share the data will 

551
00:40:25,400 --> 00:40:28,910
get to being able to train networks and 
share code and effectively view neural 

552
00:40:30,390 --> 00:40:35,390
networks as a stack of layers as opposed
to having to start from scratch with 

553
00:40:35,700 --> 00:40:40,700
tensorflow,
Pi Torch and other than that and other 

554
00:40:40,700 --> 00:40:41,580
deep learning frameworks and there's 
huge financial backing from Google,

555
00:40:41,581 --> 00:40:46,581
facebook and so on.
Deep learning is it in order to 

556
00:40:52,121 --> 00:40:57,121
understand why it works so well and 
where it's limitations are,

557
00:40:58,030 --> 00:41:03,030
we need to understand where our own 
intuition comes from about what is hard 

558
00:41:03,030 --> 00:41:06,451
and what is easy.
The important thing about computer 

559
00:41:06,451 --> 00:41:09,541
vision,
which is a lot of what this course is 

560
00:41:09,541 --> 00:41:09,541
about even as in deeper enforcement 
learning formulation,

561
00:41:10,240 --> 00:41:15,240
is that visual perception for us human 
beings was formed 540 million years ago.

562
00:41:17,470 --> 00:41:22,470
That's $540.
Millions million years worth of data and

563
00:41:23,351 --> 00:41:26,710
abstract thought is only formed a a 
100,000

564
00:41:26,711 --> 00:41:31,711
years ago.
That's several orders of magnitude less 

565
00:41:31,711 --> 00:41:35,621
data so we can with neural networks 
predictions that seemed trivial.

566
00:41:40,450 --> 00:41:41,260
The,
uh,

567
00:41:41,440 --> 00:41:46,440
the trivial to us human beings,
but completely challenging and wrong to 

568
00:41:47,611 --> 00:41:51,000
neural networks.
Here on the left showing a prediction of

569
00:41:51,001 --> 00:41:54,750
a dog with a little bit of a distortion 
and noise added to the image,

570
00:41:54,840 --> 00:41:56,730
producing the image on the right and you
know,

571
00:41:56,740 --> 00:42:01,740
network is confidently 99 percent plus 
accuracy predicting that it's an ostrich

572
00:42:05,630 --> 00:42:10,630
and there's all these problems as to 
deal with whether it's an computer 

573
00:42:10,630 --> 00:42:10,880
vision data,
whether it's in text,

574
00:42:10,881 --> 00:42:11,900
data,
audio,

575
00:42:12,230 --> 00:42:17,230
all of this variation arises in vision.
It's elimination variability.

576
00:42:18,590 --> 00:42:23,590
The set of pixels in the numbers look 
completely different depending on the 

577
00:42:23,590 --> 00:42:24,140
lighting conditions.
It's the biggest problem.

578
00:42:24,141 --> 00:42:29,141
And driving is lighting conditions.
Letting variability pose variation.

579
00:42:29,900 --> 00:42:33,140
Objects need to be learned from every 
different perspective.

580
00:42:33,141 --> 00:42:36,020
I'll discuss that for when sensing the 
driver.

581
00:42:36,080 --> 00:42:41,080
Most of most of,
most of the deep learning work that's 

582
00:42:41,080 --> 00:42:44,111
done in the face on the human is done on
the frontal face or semi frontal face.

583
00:42:44,980 --> 00:42:49,980
That has very little work done on the 
full three 60 a pose variability that a 

584
00:42:51,051 --> 00:42:56,051
human being can take on.
Interclass variability for the 

585
00:42:57,161 --> 00:42:59,530
classification problem,
for the detection problem,

586
00:42:59,860 --> 00:43:02,680
there is a lot of different kinds of 
objects for cats,

587
00:43:02,681 --> 00:43:03,730
dogs,
cars,

588
00:43:03,731 --> 00:43:05,140
bicyclists,
pedestrians,

589
00:43:07,190 --> 00:43:12,190
so that brings us to object 
classification and I'd like to take you 

590
00:43:12,190 --> 00:43:15,761
through where deep learning has taken 
big strides for the past several years 

591
00:43:16,791 --> 00:43:21,791
leading up to this year to 2018.
So let's start at object classification 

592
00:43:24,440 --> 00:43:29,440
is when you take a single image and you 
have to say one class that's most likely

593
00:43:31,041 --> 00:43:36,041
to belong in that image.
The most famous variant of that as the 

594
00:43:36,041 --> 00:43:40,301
image net competition.
Image net challenge image not data set 

595
00:43:40,301 --> 00:43:41,660
is a data set of 14 million images with 
21,000

596
00:43:41,661 --> 00:43:45,980
categories and for say the category of 
fruit,

597
00:43:46,280 --> 00:43:51,280
there's a total of 188,000
images of fruit and there is 1200 images

598
00:43:53,121 --> 00:43:58,121
of granny smith apples.
It gives you a sense of what we're 

599
00:43:58,121 --> 00:44:00,851
talking about here.
So this is been the source of a lot of 

600
00:44:02,001 --> 00:44:06,080
interesting breakthroughs in deep 
learning and a lot of the excitement and

601
00:44:06,081 --> 00:44:09,920
deep learning is first.
The big successful network,

602
00:44:10,310 --> 00:44:15,310
at least one that became famous and deep
learning is Alex Net in 2012.

603
00:44:17,120 --> 00:44:22,120
That took a leap of a significant leap 
in performance on the image net 

604
00:44:22,120 --> 00:44:27,071
challenge,
so it was one of the first neural 

605
00:44:27,071 --> 00:44:30,191
networks that have successfully trained 
on the GPU and achieved an incredible 

606
00:44:30,191 --> 00:44:31,860
performance boost over the previous 
year.

607
00:44:32,070 --> 00:44:34,980
On the image net challenge.
The challenges,

608
00:44:35,410 --> 00:44:37,080
and I'll talk about some of these 
networks,

609
00:44:37,290 --> 00:44:40,620
is to given a single image,
give five guesses,

610
00:44:40,830 --> 00:44:45,830
and you have five guests.
This to guess for one of them to be 

611
00:44:45,830 --> 00:44:49,431
correct.
The human annotation is the question 

612
00:44:49,431 --> 00:44:50,340
often comes up.
So how do you know the ground truth?

613
00:44:51,030 --> 00:44:55,530
Human level of performance is five point
one percent accuracy on this task,

614
00:44:57,060 --> 00:45:02,060
but the way the annotation for image net
is performed is there's a google search 

615
00:45:03,180 --> 00:45:06,480
where you pull the images are already 
labeled for you,

616
00:45:06,630 --> 00:45:09,600
and then the annotation that on 
mechanical Turk,

617
00:45:09,601 --> 00:45:11,610
other humans perform.
It's just binary.

618
00:45:11,611 --> 00:45:13,900
Is this a cat or not a cat?
So they're.

619
00:45:13,930 --> 00:45:18,930
They're not tasked with performing the 
very high resolution semantic labeling 

620
00:45:19,021 --> 00:45:22,260
of the image.
Okay.

621
00:45:22,261 --> 00:45:27,261
So through from 2012 with Alex Net to 
today and the big transition in 2018 of 

622
00:45:29,971 --> 00:45:33,180
the image net challenge leaving Stanford
and go into Cagle,

623
00:45:35,170 --> 00:45:40,170
it's sort of a monumental step because 
in 2015 with the resident network was 

624
00:45:40,511 --> 00:45:45,511
the first time that the human level 
performance was exceeded and I think 

625
00:45:45,511 --> 00:45:49,210
this is a very important

626
00:45:51,480 --> 00:45:56,480
map of where deep learning is for 
particular what I would argue as a toy 

627
00:45:56,480 --> 00:46:00,801
example,
despite the fact that it's 14 million 

628
00:46:00,801 --> 00:46:03,621
images.
So we're developing state of the art 

629
00:46:03,621 --> 00:46:06,651
techniques here and the next stage as we
are now exceeding human level 

630
00:46:06,651 --> 00:46:10,941
performance on this task is how to take 
these methods into the real world to 

631
00:46:10,941 --> 00:46:15,150
perform scene perception,
to perform driver's state perception

632
00:46:18,630 --> 00:46:23,630
in 2016 and 2017.
See you image and see net has a very 

633
00:46:24,451 --> 00:46:29,451
unique new addition to the previous 
formulations that has achieved an 

634
00:46:29,451 --> 00:46:33,330
accuracy of two point two percent error,
two point two,

635
00:46:33,480 --> 00:46:36,440
five percent error on the image net 
declassification challenge.

636
00:46:36,560 --> 00:46:38,790
This is an incredible result.
Okay,

637
00:46:38,791 --> 00:46:43,791
so you have this image classification 
architecture that takes in a single 

638
00:46:43,791 --> 00:46:45,830
image and produces convolution,
uh,

639
00:46:45,840 --> 00:46:48,060
and uh,
it takes it through pool and convolution

640
00:46:48,420 --> 00:46:51,150
and at the end fully connected layers 
and performance,

641
00:46:51,151 --> 00:46:53,250
the classification task or regression 
task,

642
00:46:53,550 --> 00:46:56,580
and you can swap out that layer to 
perform any kind of,

643
00:46:57,000 --> 00:47:02,000
um,
other task including with the recurrent 

644
00:47:02,000 --> 00:47:03,030
neural networks of image captioning and 
so on,

645
00:47:03,450 --> 00:47:08,450
or localization of bonding boxes.
Or you can do fully convolutional 

646
00:47:09,061 --> 00:47:12,300
networks,
which we'll talk about on Thursday,

647
00:47:13,110 --> 00:47:18,110
which is when you take a,
a images and input and producing images 

648
00:47:18,110 --> 00:47:18,110
that output.

649
00:47:18,530 --> 00:47:23,530
But where the output image in this case 
is the segmentation is a wear a color 

650
00:47:24,001 --> 00:47:25,020
indicates what,
ah,

651
00:47:25,070 --> 00:47:27,340
what the object is of the category of 
the,

652
00:47:27,640 --> 00:47:30,190
of the object.
So it's level of segmentation.

653
00:47:30,191 --> 00:47:35,191
Every single pixel in the image is 
assigned a class of category of where 

654
00:47:35,191 --> 00:47:39,901
that pixel belongs to.
This is the kind of task that's overlaid

655
00:47:41,471 --> 00:47:45,760
on top of other sensory information 
coming from the car.

656
00:47:45,761 --> 00:47:49,510
In order to perceive the external 
environment,

657
00:47:50,290 --> 00:47:54,910
you can continue to extract information 
from images in this way to produce image

658
00:47:54,911 --> 00:47:56,530
to image mapping.
For example,

659
00:47:56,531 --> 00:48:01,531
to colorize images and take from gray 
scale images to color images or you can 

660
00:48:04,621 --> 00:48:08,550
use that kind of heat map information to
localize objects in the image,

661
00:48:08,980 --> 00:48:13,980
so as opposed to just classifying that 
this is an image of a of a cow are CNN 

662
00:48:14,070 --> 00:48:19,070
fast and faster.
Our CNN and a lot of other localization 

663
00:48:19,070 --> 00:48:22,911
networks allow you to propose different 
candidates for where exactly the car was

664
00:48:23,791 --> 00:48:27,600
located in this image and thereby being 
able to perform object detection,

665
00:48:27,720 --> 00:48:32,720
not just object classification in 2017 
has been a lot of cool applications of 

666
00:48:36,111 --> 00:48:39,290
these architectures.
One of which is back on removal.

667
00:48:40,340 --> 00:48:42,230
Again,
mapping from image to image,

668
00:48:42,350 --> 00:48:47,350
ability to remove a background from 
selfies of humans or human like

669
00:48:50,450 --> 00:48:55,450
pictures of faces.
The references with some incredible 

670
00:48:57,281 --> 00:49:02,281
animations are in the bottom of the 
slide and the slides are now available 

671
00:49:02,281 --> 00:49:06,091
online.
Big Stupid Hd.

672
00:49:08,010 --> 00:49:12,310
There's been a lot of work in gans 
generative artifice.

673
00:49:12,311 --> 00:49:17,311
Ariel networks in particular in driving 
gans have been used to generate examples

674
00:49:20,280 --> 00:49:25,280
that generate examples from source data.
Whether that's from raw data or in this 

675
00:49:26,491 --> 00:49:30,540
case with pics to picks.
Hd is taking course,

676
00:49:30,720 --> 00:49:35,720
semantic labeling of the images,
pixel level and producing photo 

677
00:49:36,081 --> 00:49:41,081
realistic,
high definition images of the forward 

678
00:49:41,081 --> 00:49:45,111
roadway.
This is an exciting possibility for 

679
00:49:45,111 --> 00:49:48,360
being able to generate a variety of 
cases for self driving cars,

680
00:49:48,510 --> 00:49:51,300
for autonomous vehicles to be able to 
learn to generate,

681
00:49:51,301 --> 00:49:55,170
to augment the data and be able to 
change the way different rows look,

682
00:49:55,200 --> 00:49:57,570
road conditions to change the way 
vehicles look,

683
00:49:57,630 --> 00:49:58,860
cyclists,
pedestrians.

684
00:50:00,440 --> 00:50:02,540
Then we can move on to recur in your own
networks.

685
00:50:02,541 --> 00:50:05,180
Everything I've talked about was one to 
one,

686
00:50:05,181 --> 00:50:08,030
mapping from image to image or image to 
number,

687
00:50:08,390 --> 00:50:10,820
but currently all networks or work with 
sequences.

688
00:50:11,410 --> 00:50:16,410
We can use sequences to generate 
handwriting to generate text captions 

689
00:50:21,650 --> 00:50:26,210
from an image based on the localization 
is the various detections in that image.

690
00:50:28,350 --> 00:50:32,080
What can provide video description 
generation,

691
00:50:32,230 --> 00:50:37,230
so taking a video and combining 
convolution neural networks with 

692
00:50:37,230 --> 00:50:41,560
recurrent neural networks,
using convolutional neural networks to 

693
00:50:41,560 --> 00:50:44,551
extract features frame to frame and 
using those extracted features to input 

694
00:50:44,551 --> 00:50:49,531
into our the rns to then generate a a,
a labeling,

695
00:50:50,260 --> 00:50:52,510
a description,
what's going on in the video.

696
00:50:54,910 --> 00:50:57,610
A lot of exciting approaches for 
autonomous systems,

697
00:50:57,970 --> 00:51:02,970
especially in drones were the time to 
make a decision a is short,

698
00:51:04,720 --> 00:51:09,720
same with the RC car,
traveling 30 miles an hour at tensional 

699
00:51:09,720 --> 00:51:13,321
mechanisms for steering the attention of
the network had been very popular for 

700
00:51:13,420 --> 00:51:18,420
the localization task and for just 
saving how much interpretation of the 

701
00:51:18,420 --> 00:51:20,020
image,
how many pixels need to be considered in

702
00:51:20,021 --> 00:51:25,021
the classification task so we can steer,
we can model the way a human being looks

703
00:51:27,371 --> 00:51:32,371
around and image to interpret it and use
the network to do the same and we can 

704
00:51:32,371 --> 00:51:35,800
use that kind of steering to a draw 
images as well.

705
00:51:41,550 --> 00:51:46,550
Finally,
the big breakthroughs in 2017 came from 

706
00:51:46,550 --> 00:51:48,390
this,
the pong to pixels,

707
00:51:48,391 --> 00:51:51,450
the reinforcement learning,
using sensory data,

708
00:51:51,451 --> 00:51:56,451
raw sensory data,
and use reinforcement learning methods 

709
00:51:56,451 --> 00:51:56,610
deeper.
All methods of which we'll talk about on

710
00:51:56,611 --> 00:52:01,611
Wednesday.
I'm really excited about the underlying 

711
00:52:01,611 --> 00:52:05,331
methodology of deep traffic and deep 
crash is using neural networks as the 

712
00:52:07,861 --> 00:52:11,610
approximators inside reinforcement 
learning approaches.

713
00:52:11,940 --> 00:52:16,940
So Alphago in 2016 have achieved a 
monumental task that when I first 

714
00:52:17,671 --> 00:52:22,671
started in artificial intelligence was 
told to me it was impossible for any 

715
00:52:22,671 --> 00:52:26,181
system to accomplish,
which is to win at the game of go 

716
00:52:26,181 --> 00:52:27,120
against the top human player in the 
world.

717
00:52:29,250 --> 00:52:34,250
However,
that method was trained on human expert 

718
00:52:34,250 --> 00:52:37,491
positions.
The alphago system was trained on 

719
00:52:37,491 --> 00:52:41,691
previous games played by human experts 
and an incredible accomplishment.

720
00:52:43,020 --> 00:52:48,020
Alphago zero in 2017 was able to beat 
Alphago and many of its variance by 

721
00:52:52,781 --> 00:52:57,781
playing itself from zero information.
So no knowledge of human experts,

722
00:53:01,840 --> 00:53:04,510
no games,
no training data,

723
00:53:04,690 --> 00:53:09,690
very little human input.
And what more it was able to generate 

724
00:53:10,870 --> 00:53:13,720
moves that were surprising to human 
experts.

725
00:53:15,290 --> 00:53:19,340
I think it's Einstein that said that 
intelligence,

726
00:53:19,940 --> 00:53:22,610
that the key mark of intelligence is 
imagination.

727
00:53:23,810 --> 00:53:28,810
I think it's beautiful.
See an artificial intelligence system 

728
00:53:28,810 --> 00:53:30,360
come up with something that surprises 
human experts,

729
00:53:31,670 --> 00:53:32,900
truly surprises

730
00:53:36,050 --> 00:53:41,050
for the gambling junkies.
Deep Stack and a few other variants have

731
00:53:41,121 --> 00:53:44,630
been used in 2017 to when a heads up 
poker.

732
00:53:45,260 --> 00:53:47,090
Again,
another incredible results.

733
00:53:47,360 --> 00:53:50,760
I was always told and artificial 
intelligence will be impossible for dave

734
00:53:51,020 --> 00:53:56,020
for any machine learning method to 
achieve and it was able to beat a 

735
00:53:56,020 --> 00:54:00,401
professional player and several 
competitors have come along since we're 

736
00:54:00,920 --> 00:54:04,040
yet to be able to beat to win a 
tournament setting.

737
00:54:04,041 --> 00:54:06,230
So multiple players,
for those of you familiar heads up,

738
00:54:06,231 --> 00:54:08,450
poker is one on one.
It's a much,

739
00:54:08,480 --> 00:54:10,730
much smaller,
easier space to,

740
00:54:10,731 --> 00:54:11,930
uh,
solve.

741
00:54:12,620 --> 00:54:17,620
There's a lot more humidity,
human dynamics going on from when 

742
00:54:17,620 --> 00:54:19,010
there's multiple players,
but that's the task for 2018

743
00:54:22,000 --> 00:54:24,880
and the drawbacks.
So one of my favorite videos,

744
00:54:24,881 --> 00:54:29,881
a show it often have coast runners for 
these deep reinforcement learning 

745
00:54:30,941 --> 00:54:35,560
approaches.
The learning of the reward function,

746
00:54:35,860 --> 00:54:40,860
the definition of the word function 
controls how the actual system behaves.

747
00:54:42,880 --> 00:54:47,880
And this will come.
This will be extremely important for us 

748
00:54:47,880 --> 00:54:51,301
with autonomous vehicles.
Here the boat is tasked with gaining the

749
00:54:51,970 --> 00:54:56,470
highest number of points and it figures 
out that it does not need to race,

750
00:54:56,471 --> 00:54:59,410
which is the whole point of the game in 
order to gain points.

751
00:54:59,440 --> 00:55:04,440
But instead pickup green circles that 
regenerate themselves over and over.

752
00:55:05,890 --> 00:55:10,890
This is the the counter intuitive 
behavior of a system that would not be 

753
00:55:14,471 --> 00:55:16,750
expected when you first designed the 
reward function,

754
00:55:17,080 --> 00:55:22,080
and this is a very formal simple system,
nevertheless is extremely difficult to 

755
00:55:22,601 --> 00:55:26,670
come up with a reward function that 
makes it operate in the way you expected

756
00:55:26,680 --> 00:55:31,680
to operate.
Very applicable for autonomous vehicles 

757
00:55:31,680 --> 00:55:36,270
and of course on the perception side,
as I mentioned with the hostage and the 

758
00:55:36,270 --> 00:55:37,510
dog,
a little bit of noise.

759
00:55:38,020 --> 00:55:43,020
The 99 point six percent confidence,
we can predict that the noise up top is 

760
00:55:43,020 --> 00:55:44,020
a robin,
a Cheetah,

761
00:55:44,170 --> 00:55:45,760
Armadillo,
lesser panda.

762
00:55:45,880 --> 00:55:49,450
These are outputs from actual state of 
the art and neural networks

763
00:55:51,650 --> 00:55:54,560
taking into noise and producing a 
confident prediction.

764
00:55:55,880 --> 00:55:58,820
It should build our intuition to 
understand that we don't,

765
00:55:59,090 --> 00:56:02,040
that the visual characteristics,
the vision,

766
00:56:02,070 --> 00:56:06,800
the special characteristics of an image 
that not necessarily convey the level of

767
00:56:06,801 --> 00:56:11,801
hierarchy necessary to function in this 
world in a similar way with a dog and 

768
00:56:14,181 --> 00:56:19,181
the ostrich and everything in an ostrich
and network confidently with a little 

769
00:56:19,521 --> 00:56:21,770
bit of noise can make the wrong 
prediction.

770
00:56:22,570 --> 00:56:26,830
Thinking the bus is an ostrich and as an
ostrich,

771
00:56:29,090 --> 00:56:34,090
they're easily fooled,
but not really because they performed 

772
00:56:34,501 --> 00:56:37,120
the task that they were trained to do 
well,

773
00:56:38,200 --> 00:56:41,860
so we have to make sure we keep our 
intuition

774
00:56:44,130 --> 00:56:49,130
optimized to the way machines learn,
not the way humans have learned over the

775
00:56:49,711 --> 00:56:54,590
540 million years of data that we've 
gained through developing the eye,

776
00:56:54,591 --> 00:56:58,290
the revolution,
the current challenge is we're taking on

777
00:56:58,650 --> 00:57:03,650
first transfer learning.
There's a lot of success in transfer 

778
00:57:03,650 --> 00:57:05,940
learning between domains that are very 
close to each other,

779
00:57:06,300 --> 00:57:09,090
so image classification from one domain 
to the next.

780
00:57:10,530 --> 00:57:15,530
There's a lot of value in forming 
representations of the way scenes look 

781
00:57:15,530 --> 00:57:18,630
in order to see natural scenes look in 
order to do scene segmentation,

782
00:57:18,631 --> 00:57:20,040
the driving case,
for example,

783
00:57:20,370 --> 00:57:25,370
but we're not able to do any any bigger 
leaps in the way we perform transfer 

784
00:57:26,701 --> 00:57:31,701
learning.
The biggest challenge for deep learning 

785
00:57:31,701 --> 00:57:32,670
is to generalize,
generalize across domains.

786
00:57:33,480 --> 00:57:38,480
It lacks the ability to reason and the 
way that we've defined understanding 

787
00:57:38,480 --> 00:57:42,681
previously,
which is the ability to turn complex 

788
00:57:42,681 --> 00:57:42,930
information into simple,
useful information,

789
00:57:44,870 --> 00:57:49,870
convert domain specific complicated 
sensory information that doesn't relate 

790
00:57:51,830 --> 00:57:56,830
to the initial training set.
That's the open challenge for deep 

791
00:57:56,830 --> 00:57:57,020
learning,
training,

792
00:57:57,050 --> 00:58:02,050
very little data,
and then go and reason and operate in 

793
00:58:02,050 --> 00:58:03,830
the real world right now.
You know now it's a very inefficient.

794
00:58:04,020 --> 00:58:08,330
They're acquiring a big data.
They require supervised data,

795
00:58:08,360 --> 00:58:10,820
which means they need human costs,
the human input.

796
00:58:12,440 --> 00:58:17,440
They're not fully automated despite the 
fact that the feature learning 

797
00:58:17,440 --> 00:58:18,620
incredibly,
the big breakthrough feature learning is

798
00:58:18,621 --> 00:58:23,621
performed automatically.
You're still have to do a lot of design 

799
00:58:23,621 --> 00:58:27,591
or the actual architecture of the 
network and all the different hyper 

800
00:58:27,591 --> 00:58:30,671
parameter tuning and he used to perform 
human input perhaps a little bit more 

801
00:58:30,801 --> 00:58:33,770
educated human input and former phd 
students.

802
00:58:33,860 --> 00:58:38,860
Postdocs faculty is required to high 
during these hyper parameters,

803
00:58:39,230 --> 00:58:41,270
but nevertheless human input is still 
necessary.

804
00:58:41,840 --> 00:58:45,560
They cannot be left alone.
For the most part,

805
00:58:47,130 --> 00:58:49,410
their award defining the award is with 
south coast.

806
00:58:49,411 --> 00:58:54,090
Ron is extremely difficult for systems 
that operate in the real world.

807
00:58:54,150 --> 00:58:58,650
Transparency quite possibly is not an 
important one,

808
00:58:58,890 --> 00:59:02,010
but neural networks currently our black 
box for the most part,

809
00:59:02,220 --> 00:59:07,220
they're not able to accept through a few
successful visualization methods that 

810
00:59:07,220 --> 00:59:09,060
visualize different aspects of the 
activations.

811
00:59:09,330 --> 00:59:14,330
They're not able to reveal to us humans 
why they work or where they fail

812
00:59:17,290 --> 00:59:22,290
and this.
This is a philosophical question for 

813
00:59:22,290 --> 00:59:23,990
autonomous vehicles that we may not care
as human beings if the system works well

814
00:59:23,991 --> 00:59:28,910
enough,
but I would argue that it be a long time

815
00:59:29,120 --> 00:59:34,120
before systems work well enough or we 
don't care well care and will have to 

816
00:59:35,121 --> 00:59:37,850
work together with these systems and 
that's where transparency,

817
00:59:37,851 --> 00:59:42,851
communication,
collaboration is critical and edge 

818
00:59:42,851 --> 00:59:46,121
cases.
It's all about edge cases in robotics 

819
00:59:46,121 --> 00:59:50,140
and autonomous vehicles.
The 99 point nine percent of driving is 

820
00:59:50,140 --> 00:59:53,420
really boring.
It's the same especially highway driving

821
00:59:53,480 --> 00:59:56,030
traffic driving.
It's the same.

822
00:59:56,450 --> 00:59:59,630
The obstacle avoidance,
the car following the lane centering.

823
00:59:59,780 --> 01:00:02,870
All of these problems with trivial is 
the edge cases,

824
01:00:03,260 --> 01:00:08,260
the trillions of edge cases that need to
be generalized over on a very small 

825
01:00:08,781 --> 01:00:13,781
amount of training data.
So again,

826
01:00:16,091 --> 01:00:21,091
I returned to why deep learning.
I mentioned a bunch of challenges and 

827
01:00:24,921 --> 01:00:29,921
this is an opportunity.
It's an opportunity to come up with 

828
01:00:31,130 --> 01:00:35,840
techniques that opera is successful in 
this world,

829
01:00:36,230 --> 01:00:41,230
so I hope the competitions were 
presented in this class and the 

830
01:00:41,230 --> 01:00:44,381
autonomous vehicle domain.
We'll give you some insight and 

831
01:00:44,381 --> 01:00:47,321
opportunity to apply in.
Some of these cases are open research 

832
01:00:47,321 --> 01:00:51,431
problems with semantic segmentation of 
external perception with control of the 

833
01:00:51,861 --> 01:00:56,861
vehicle and deep traffic and with deep 
crash of control of the vehicle and 

834
01:00:59,810 --> 01:01:04,810
under actuator.
Good high speed conditions and the 

835
01:01:04,941 --> 01:01:06,110
driver's state perception.

836
01:01:07,350 --> 01:01:08,940
Okay,

837
01:01:10,940 --> 01:01:15,940
so would that.
I wanted to introduce deep learning to 

838
01:01:15,940 --> 01:01:16,640
you today before we get to the fun 
tomorrow of autonomous vehicles.

839
01:01:17,270 --> 01:01:21,360
So we'd like to thank and video,
Google auto live,

840
01:01:22,260 --> 01:01:27,110
Toyota and the risk of setting off 
people's phones.

841
01:01:27,440 --> 01:01:28,850
Amazon,
Alexa Auto,

842
01:01:31,280 --> 01:01:36,280
but truly I would like to say that I've 
been humbled over the past year by the 

843
01:01:41,090 --> 01:01:46,090
thousands of messages were received by 
the attention by the 18,000

844
01:01:46,220 --> 01:01:50,240
competition entries by the many people 
across the world.

845
01:01:50,241 --> 01:01:55,241
Not just here at Mit that are brilliant 
that I got a chance to interact with and

846
01:01:55,671 --> 01:01:59,630
I hope we go bigger and do some 
impressive stuff in 2018.

847
01:02:00,410 --> 01:02:02,660
Thank you very much and tomorrow is self
driving.

