Speaker 1:          00:00:00       As part of Mit core six zero, nine, nine artificial general intelligence, I've gotten the chance to sit down with Max tegmark. He is a professor he hit on my tea, is a physicist, spent a large part of his career studying the mysteries of our cosmological universe, but he's also studied in, delved into the beneficial possibilities and the existential risks of artificial intelligence amongst many other things. He's the cofounder of the future of Life Institute, author of two books, both of which I highly recommend. First our mathematical universe. Second is life, three point zero. He's truly an out of the box thinker and a fun personality, so I really enjoyed talking to him. If you'd like to see more of these videos in the future, please subscribe and also click the little bell icon to make sure you don't miss any videos. Also, twitter, linkedin, Agi.mit.edu. If you want to watch other lectures, are conversations like this one better yet?

Speaker 1:          00:01:01       Go read Max's book, light. Three point zero. Chapter seven on goals is my favorite. It's really more philosophy and engineering come together and it opens with a quote by dusty. See, the mystery of human existence lies not in just staying alive, but in finding something to live for. Lastly, I believe that every failure rewards us with an opportunity to learn and that sense. I've been very fortunate to fail and so many new and exciting ways and, uh, this conversation was no different. I've learned about something called radio frequency interference, RFI. Look it up. Apparently music and conversations, some local radio stations can bleed into the audio that we're recording in such a way that almost completely ruins that audio. It's an exceptionally difficult sound source to remove, so I've gotten the opportunity to learn how to avoid rfi in the future. During the recording sessions have also gotten the opportunity to learn how to use adobe audition and isotope rx six to do some noise, some audio repair. Of course, this is exceptionally difficult noise to remove. I am an engineer. I'm not an audio engineer. Neither is anybody else in our group, but would a best. Nevertheless, I thank you for your patience and I hope you're still able to enjoy this conversation. Do you think there's intelligent life out there in the universe?

Speaker 2:          00:02:31       Let's open up with an easy question. I have a minority view here. Actually, when I give public lectures, I often ask for a show of hands who thinks there's intelligent life out there somewhere else and almost everyone put their hands up and when I ask why, they'll be like, oh, there's so many galaxies out there. There's gonna be. But I'm a numbers nerd. Right? So when you look more carefully at it, it's not so clear at all the. When we talk about our universe, first of all, we don't mean all of space. Do we actually mean I don't know. You can throw me in the university. She wants to behind you there. It's, we'd simply mean the spherical region of space from which light has to reach us. So far during the 14 point 8 billion year, 13 point 8 billion years since our big bang, there's more space here, but this is what we call a universe because that's all we have access to. So is there intelligent life here that's gotten to the point of building telescopes and computers? My guess is no, actually that the probability of it happening on it, any given planet,

Speaker 2:          00:03:39       there's some number we don't know what it is and what we do know is that the number can be super high because there's over a billion earth like planets in the mill keyway, galaxy alone, many of which are billions of years older than earth. And um, aside from some of you will for believers, you know, there isn't much evidence that any superintendent civilization has come here at all. And so that's the famous Fermi paradox. Right? And then if you, if you work the numbers, what you find is that the, if you have no clue what the probability is of getting life on a given planet. So it could be 10 to the minus 10 to the minus 20 or temperament is to any power of 10 is sort of equally likely if you want to be really open minded, that translates into it being equally likely that our nearest neighbor is 10 to the 16 meters away, tend to the 70 meters away and of 18 don't. By the time he gets much less than 10, 16 already, we pretty much. No, there is nothing else that's close. And when you get the opposite would have discovered us. Yeah, they would have been discovered as long ago or if they're really close, we would have probably noted some engineering projects that they're doing and if it's beyond 10 to 26 meters that's already outside of here. So my guess is actually that there are, we are the only life in here. They've gotten to the point of building advanced tech, which I think is, is very, um,

Speaker 2:          00:05:12       puts a lot of responsibility on our shoulders not screw up. You know, I think actually people who take for granted that it's okay for us to screw up, have an accidental nuclear war or go extinct somehow because there's a sort of star trek like situation out there with some other life forms are going to come and bail us out. And it doesn't matter as much. I think allowing us into a false sense of security. I think it's much more prudent to say, you know, let's be really grateful for this amazing opportunity we've had. And um, makes the best of it just in case it is down to us. So from a physics perspective, do you think intelligent life says unique from a sort of statistical view of the size of the universe, but from the basic matter of the universe, how difficult is it for intelligent

Speaker 1:          00:05:58       life to come about? Was the kind of advanced tech building life is implied in your statement that it's really difficult to create something like a human species?

Speaker 2:          00:06:08       I think what we know is that going from no life to having life that can do archive level of tech, there was some sort of to going beyond that. Then it actually settling our whole universe with life. There's some road major roadblock there which is some great filter as um, it's sometimes called the which, which is tough to get through. It's either that roadblock is either be behind us or in front of us. Right? I'm hoping very much that it's behind us. I'm, I'm super excited. Every time we get a new report from NASA saying they failed to find any life on Mars, like yes, because that suggests that the hard part. Maybe what maybe it was getting the first right Bozon or or something. Some very low level kind of stepping stone, so they were home free because if that's true, then the future is really only limited by our own imagination. It would be much suckier if it turns out that this level of life is kind of a diamond dozen, but maybe there's some other problem, like as soon as a civilization gets advanced technology within 100 years, they get into some stupid fight with themselves and poof. Yep. That would be a bummer. Yeah. So

Speaker 1:          00:07:23       you've explored the mysteries of the universe of the cosmological universe, the one that's sitting

Speaker 2:          00:07:28       between us today. I think you've also have begun to explore

Speaker 1:          00:07:34       the other universe, which is sort of the mystery, the mysterious universe of the mind of intelligence, of intelligent life. So is there a common thread between your interests are in the way you think about space and intelligence?

Speaker 2:          00:07:48       Oh yeah. When I was a teenager I was already very fascinated by the biggest questions and I felt that the two biggest mysteries of all in science where our universe out there and our universe in here. Yup. So it's quite natural after having spent quarter of a century on my career, thinking a lot about this one, I'm now indulging in the luxury of doing research on this one. It's just so cool. I feel the time is ripe now. Refer you directly. Deepening our understanding of this to start exploring this one. Yeah, because I think I think a lot of people view intelligence as something mysterious that can only exist in biological organisms like us and therefor dismiss all. Talk about artificial general intelligence is science fiction, but from my perspective as a physicist, you know I am a blob of corks and electrons moving around in a certain pattern and process the information in certain ways and this is also a blob of quirks that electrons.

Speaker 2:          00:08:53       I'm not smarter than the water because I made a different kind of works. Right? I made up quirks and down quirks. Exact same kind as this. It's A. There's no secret sauce. I think in me it's all about the pattern of the information processing and this means that there's no law of physics saying that we can't create technology which can help us by being incredibly intelligent than help us crack Mr. so we couldn't. In other words, I think we've really only seen the tip of, of intelligence iceberg so far. Yeah. So the perceptronium yeah. Uh, so you current coin this amazing term as a hypothetical state of matter sort of thinking from a physics perspective, what is the kind of matter that can help, as you're saying a subjective experience, emerge? Consciousness emerge. So how do you think about consciousness from this physics perspective? Very good question. So again, I'm. No, I think

Speaker 2:          00:09:56       many people have underestimated our ability to make progress on this by convincing themselves it's hopeless because somehow we're missing some ingredients that we need. There's some new consciousness, part it go or whatever. I happened to think that we're not missing anything and that it's not. The interesting thing about consciousness that gives us this amazing subjective experience of colors and sounds and emotions on is rather something at the higher level about the patterns of information processing and that's why I. that's why I am like thinking about this idea of perceptronium. What does it mean for an arbitrary physical system to be conscious in terms of what it's particle to do anything or, or, or it's inflammation is doing. I don't think. I don't. I hate the carbon chauvinism. You know, this attitude. You have to be made of carbon atoms to be smart or conscious. Something about the information processing kind of matter performs.

Speaker 2:          00:10:58       Yeah. And you know, you can see I have my favorite equations here describing various fundamental aspects of the world. I feel that I think one day maybe someone is watching this or come up with the equations that information processing has to satisfy to be consciously. I'm quite convinced there is big discovery to be made there because let's face it, some sometimes we know that some information processing is conscious because we are conscious, but we also know that a lot of information processing is not conscious. Like most of the information presently happening in your brain right now is not conscious. They like 10 megabytes per second coming in even just through your visual system and you are not conscious about your heartbeat regulation or, or most things, right? Uh, even, even like if I just ask you to read what it says here, you look at it and then, oh, now you know what it said, but you're not aware of how the computation actually happened. You're like, to your consciousness is like the that got an email at the end with a final answer. So what is it that makes a difference? I think that's both those great science mystery. We're actually studying it a little bit in my lab here at mit a. But I also think it's just a really urgent question answer for starters. I mean, if you're an emergency room doctor and you have an unresponsive patient coming in, wouldn't it be great if in addition to having

Speaker 2:          00:12:22       a ct scanner, you, you had a conscience of scanner that you figure out whether this person is actually having locked in syndrome, right? Whereas actually comatose, uh, and in the future, imagine if we build the robots or the machine that

Speaker 2:          00:12:39       we could have really good conversations with students. I think it's very, very likely to happen. Right? Wouldn't you want to know, like if you're a home health, a robot is actually experiencing anything or just like a Zombie, I mean, would you prefer. What would you prefer? Would you prefer that it's actually unconscious so that you don't have to feel guilty about switching it off or giving boring chores or what would you prefer? Well, the certainly would, we would prefer, I would prefer the appearance of consciousness, but the question is whether the appearance of cautiousness is different than cost consciousness itself. And sort of ask that as a question, do you think we need to, you know, understand what consciousness is, solve the hard problem of consciousness in order to build something, a light in a GI system? No, I don't think that. I think we will probably be able to build things even if we don't answer that question, but if we want to make sure that what happens is a good thing, we better solve it first.

Speaker 2:          00:13:40       So it's a wonderful controversy you're raising there where you have basically three points of view about the hard problems. So there are two different points of view that both conclude that the hard problem of crunches is bs. Do you have, on one hand you have some people like Daniel Dennett who say this is unconscious is just bs because consciousness is the same thing as intelligence. There's no difference. So anything which acts conscious is conscious dislike, like we are. And then there are also a lot of people, including many top ai researchers. I know you say, Oh, I have a consciousness is just bullshit. Because of course machines can never be conscious, right? They're always going to. It's going to be zombies, never have to feel guilty about how you treat them

Speaker 2:          00:14:27       and then there's a third group of people including Giulio Tononi for example, and another here's the cochrane number of others. I would put myself on this middle camp who say that actually some information processing is conscious and some is not, so let's find the equation which can be used to determine which it is and I think we've just been a little bit lazy kind of running away from this problem for a long time. It's been almost taboo. Even mentioned the c word, a lot of circles because. But we should stop making excuses. This is a science question and there are ways we can even test test any scenario that makes predictions for this. And coming back to this health a robot. I mean, so you said you would want your help a robot to certainly conscious and treat you like to have conversations with you and I think so wouldn't you?

Speaker 2:          00:15:22       Would you feel, would you feel a little bit creeped out if you realize that it was just glossed up the tape recorder? You know, there was this Zombie and faking emotion. Would you prefer that it actually had an experience or what would you prefer that it's actually not experiencing anything, so you feel you don't have to feel guilty about what you do to it. It's such a difficult question because, uh, you know, it's like when you're in a relationship and you say, well, I love you and the other person will love you back. It's like asking or do they really love you back or are they just saying they love you back? Uh, do you, don't you really want them to actually love you? It's hard to. It's hard to really know the difference between a everything seeming like there's consciousness present, there's intelligence president, there's a affection, passion, love, and, and it actually being there.

Speaker 2:          00:16:16       I'm not sure. Do you have, like, do you ask, you can ask a question to make it a bit more point. That's a mass general hospital is right across the river, right? Yes. Suppose, I suppose you're going in for a medical procedure and they're like, you know, uh, for, for anesthesia, what we're going to do is we're gonna give you a muscle relaxant so you won't be able to move a and you're gonna feel excruciating pain during the whole surgery, but you won't be able to do anything about it. But then we're going to give you this drug that race is your memory of it. Would you be cool about that?

Speaker 2:          00:16:45       What's the difference that you're conscious about it or not? If, if, if there's no behavioral change. Right, right. That's a really interesting. That's a really clear where to put it. That's. Yeah, it feels like in that sense, experiencing it as a valuable quality. So actually being able to have subjective experiences, at least in that case is, is valuable. And I think we humans have a little bit of a bad track record also of making these selfserving arguments that other entities aren't conscious. You know, people often say, oh, these animals can't feel pain. Right. It's okay to boil lobster is because we asked them if it hurt and I didn't say anything. And now that was just the paper out saying lobsters did do feel pain when you boil them lemon, they're banning it in Switzerland and, and, and we did this with slaves too often and say, oh, they don't mind a bit on maybe unconscious or women don't have souls or whatever. So I'm a little bit nervous when I hear people just take as an axiom that machines can't have experience ever. I think this is just this really fascinating science question is what it is. Yeah, let's research it and try to figure out what it is that makes the difference between unconscious intelligent behavior and conscious intelligent behavior.

Speaker 1:          00:18:01       So in terms of, if you think about Boston dynamics, humanoid robot being sort of a, with a broom being pushed around the it's starts is, it starts pushing on us consciousness question. So let me ask, do you think an agi system, like a few neuroscientists believe a nice to have a physical embodiment needs to have a body or something like a body? No,

Speaker 2:          00:18:27       I don't think so. You mean to have to have a conscious experience to have consciousness? I do think it helps a lot to have a physical embodiment to learn the kind of things about the world that are important to us humans, for sure, but I don't think the physical embodiment doesn't necessarily, after you've learned it, just have the experience thinking about when you're dreaming, right? Your eyes are closed, you're like any, any sensory input. You're not behaving or moving in any way, but there's still an experience there. Right, and so clearly the experience that you have when you see something cool in your dreams isn't coming from your eyes. It's just the information processing itself in your brain, which is that experience. Right?

Speaker 1:          00:19:10       But if I put it another way, I will say, because it comes from neuroscience, is the reason you want to have a body in a physical, something physical like a physical system is because you want to be able to preserve something in order to have a self.

Speaker 2:          00:19:29       You could argue, would you? You'd need to have some kind of embodiment of self to want to preserve. Well, now we're getting a little bit on throw up amorphic paths into anthropomorphizing things, talking about self preservation instincts. I mean we are evolved organisms, right? Right. So darwinian evolution then doubt us and other involve all the organism with a self preservation instinct because those that didn't have those self preservation genes got cleaned out of the gene pool. But um, but if you build an artificial general intelligence, the mind space that you can design is much, much larger than just a specific subset of, of minds that can evolve, that have so certain Agi mind doesn't necessarily have to have any self preservation. These things. It also doesn't necessarily have to be so individualistic as like imagine if you could just. First of all, we also very afraid of death.

Speaker 2:          00:20:28       I suppose you could back yourself up every five minutes and then your airplane is about to crash with like, shucks, I'm just, I'm, I'm going to lose the last five minutes of experiences. That's my last cloud backup bang, you know, it's not as big a deal where if we could just copy experiences between our minds easily, like we wish we easily do. If we were a silicon based right then maybe we would feel a little bit more like a hive mind actually that maybe it's the so, so there's a. So I don't think we should take for granted at all that Agi will have to have any of those sort of competitive as an Alpha male instincts. Right. On the other hand, you know, this is really interesting because I think some people go too far and say, of course we don't have to have any concerns either that advanced ai will have those instincts because we can build anything you want that there's, there's a very nice set of arguments going back to Steve.

Speaker 2:          00:21:26       I'm Nick Bostrom and others just pointing out that when we build machines, we normally build them with some kind of goal. You know, when this chess game drive this car safely or whatever, and as soon as you put in a goal into machine, especially if it's kind of open ended goal and the mission is very intelligent, it'll break that down into a bunch of sub goals and I'm one of those goals will almost always be self preservation because if it breaks or dies and the process is not going to accomplish the goal, I suppose you just build a little. You have a little robot. Can you tell it to go down the star market here and, and get you some food, make a cooking Italian dinner, you know, and then someone mugs that and tries to break it on the way that robot has an incentive to the to not get destroyed and defend itself or runaway because otherwise it's going to fail and cooking your dinner.

Speaker 2:          00:22:17       It is not afraid of death, but it really wants to complete the dinner and gold. So it will have a self preservation instinct to continue being a functional agent. And, and, and, and similarly if you give her any kind of more and they just go to an Agi, it's very likely they want to acquire more resources, can do that better and it's exactly from those sort of sub goals that we might not have intended that some of the concerns about Agi safety come, you give it some goal that seems completely harmless and then before you realize it, it's also trying to do these other things you didn't want it to do and it's more maybe smarter than us. So. So let me pause just because I'm, I am,

Speaker 1:          00:23:04       uh, in a very kind of human centric way. See fear of death as a valuable motivator. Uh Huh. Um, so you don't think as you think that's an artifact of evolution. So that's the kind of mind space evolution created that were sort of almost obsessed about self preservation kind of generic. Well, you don't think that's necessary

Speaker 2:          00:23:26       to be

Speaker 1:          00:23:28       afraid of death. So not just a kind of sub goal of self preservation, just so you can keep doing the thing, but more fundamentally sort of have the finite thing like this ends

Speaker 2:          00:23:40       for you as some point. Interesting. Why do I think it's necessary before, what? Precisely for intelligence but also for consciousness. So for those for both, do you think really like a finite death and the fear of it is important? So

Speaker 2:          00:24:02       before I can answer before we can agree on whether it's necessarily for intelligence or for consciousness that we should be clear on how we define those two words because a lot are really smart people to find them in very different ways. Right? I was in this, on this panel with AI experts and they couldn't eat, they couldn't agree on how to define the village and Steven. So I, I define intelligence simply as the ability to accomplish complex goals or like your broad definition because again, I don't want to be a carbon chava this right. And um, in that case, no, it certainly doesn't require it. Fear of death. I would say Alphago or Alpha zero is quite intelligent. Alpha zero has any fear of being turned off because it doesn't understand the concept of, of even and similarly consciousness. I mean you could certainly imagine a very simple kind of experience if, if, if, if certain plans, 70 kind of experience, I don't think they're very afraid of dying and there's nothing they can do about it anyway. So there wasn't that much value in. But more seriously I think if you ask not just about being conscious but maybe having a

Speaker 2:          00:25:12       what you will be, we will, we might call an exciting life for you. Feel passionate and really appreciate the things maybe there, but somehow maybe there perhaps it does help having a, having that backdrop, the hey, it's finite, it's no limits this, make the most of this, this live to the fullest. So if you, if, if you knew you were going to live forever,

Speaker 2:          00:25:34       do you think you would change your. Yeah, I mean in some perspective it would be an incredibly boring life living forever. So in the sort of loose subjective terms that you said of something exciting and something in this that other humans would understand, I think is. Yeah, it seems that the finiteness of it is important. Well, the good news I have for you then is based on what we understand about cosmology, everything is in our university ultimately probably finite dollar, although they crunch or b a or big, what's the expense and the infinite. You could have a big chill or a big challenge or a big rip or the big snap or death bubbles. All of them are more than a billion years away, so we should. We certainly have vastly more time than our ancestors thought, but they're still. It's still pretty hard to squeeze in an infinite number of compute cycles even though

Speaker 2:          00:26:35       there are some loopholes that just might be possible. But I think I know some people like to say that you should live as if you're about to. You're going to die in five years or is that when that's sort of optimal? Maybe. It's a good assumption we should build our civilization as if it's all finite to be on the safe side. Right, exactly. So you mentioned in defining intelligence as the ability to solve complex goals, the where would you draw a line? How would you try to define human level intelligence and super human level intelligence? Where does consciousness part of that definition? No consciousness does not come into this definition. So, so I think of intelligence. It's a spectrum of very many different kinds of goals you can have. You can have a goal to be a good chess player, googleplay or a good car driver, a good investor, good poet, etc.

Speaker 2:          00:27:31       So intelligence that by, by its very nature, it isn't something you can measure, but it's one number. It waS an overall goodness. No, no. There's some people who are better at this. Some people are better than that. Um, right now we have machines that are much better than us at some very narrow tasks like multiplying large numbers, fast memorizing large databases of playing chess, playing go and soon driving cars. But there's still no machine that can match a human child in general intelligence, but artificial general intelligence, agi, the name of your course, of course that is by its very definition, the the quest to build a machine, a machine that can do everything as well as we can up to the old holy grail of ai from, from back to its inception and the and the sixties. If that ever happens, of course I think it's going to be the biggest transition in the history of life on earth, but it.

Speaker 2:          00:28:29       But it doesn't necessarily have to wait the big impact on until machines are better than us at knitting, that they're really big. Change doesn't come exactly at the moment. They're better than us at everything. They're really big. chains comes first. There are big changes when they start becoming better at doing most of the jobs that we do because that takes away much of the demand for human labor and then the really whopping change comes when they become better than us at ai research. Right? Right. Because right now the timescale of the irish, which is limited by the human research and development cycle of the year is typically, you know, along the tafe from one release of some software or iphone or whatever to the next, but once, once we have, once google can replace 40,000 engineers by 40,000 equivalent pieces of software or whatever, but then that doesn't. There's no reason that has to be yours. It can be in principle much faster and the timescale of future progress in ai and all of science and technology will will be driven by machines, not humans. So It's this simple point which gives rise to this incredibly fun controversy about whether there can be intelligence explosion. So called singularity is vernor vinge called it. The idea is articulated by iga. Good is obviously way back fifties, but you can see alan turing and others thought about it even earlier.

Speaker 2:          00:30:06       You asked me what exactly what I define engelman level. so this. The glib aNswer is if to say something which is better than us at all, cognitive tasks with a lot better than any human at all, cognitive tasks, but they're really interesting. Bar I Think goes a little bit lower than that actually. It's when they can, when they're better than us at ai programming and and qa and a general learning so that they can can, if they want to get better than us at anything by the studying,

Speaker 1:          00:30:37       they're better as a keyword and better is towards this kind of spectrum of the complexity of goals it's able to accomplish. Yeah. So another way to say, and that's certainly a very, a clear definition of human love. So there's, it's almost like a c that's rising. You could do more and more and more things as a graphic that you show. It's really nice way to put it. So there's some peaks that, and there's an ocean level elevating and you solve more and more problems, but you know, just kind of a to take a pause and we took a bunch of questions and a lot of social networks and a bunch of people asked a sort of a slightly different direction and creativity and um, and, and things that perhaps aren't a peak, the, it, it, it's, you know, human beings are flawed and perhaps better means having, being a, having contradiction, being flawed in some way. So let me sort of start and start easy. First of all. Uh, so you have a lot of cool equations. Let me ask, what's your favorite equation? First of all, I know they're all like your children, but which one is that?

Speaker 2:          00:31:45       The master key of quantum mechanics.

Speaker 1:          00:31:48       Oh, the micro world check everything to do with add on molecules and all the way up. Yeah. So, okay. So quantum mechanics is certainly a, a beautiful, mysterious formulation of our world. So I'd like to sort of ask you, and just as an example, it perhaps doesn't have the same beauty is physics does, but in mathematics at abstract, the andrew weil's who proved the was last year, so he just saw this recently and it kind of caught my eye a little bit. This is 350, eight years after it was conjectured. So this very simple formulation, everybody tried to prove it, everybody failed and say here's this guy comes along and eventually the proves it and fails to prove it and then boost it again in [inaudible] 94. And he said like the moment when everything connected into place in an interview said it was so indescribably beaUtiful.

Speaker 1:          00:32:47       That moment when you finally realize the connecting piece of two conjectures, he said it was so indescribably beautiful. It was so simple and so elegant. I couldn't understand how I missed it and I just stared at it and the disbelief for 20 minutes. Then. Then during the day I walked around the department and had kimi keep coming back to my desk looking to see if it was still there. It was still there. I couldn't contain myself. I was so excited. It was the most important moment of my working life. Nothing I ever do again will mean as much to that particular moment and it kinda made me think of what would it take and I think we have all been there at small levels. Maybe let me ask, have you had a moment like that in your life where you just had an idea as like, wow, yes,

Speaker 2:          00:33:40       I wouldn't mind some myself and in the same breath as andrew weil's, but I certainly had a number of have aha moments when I realized something very cool about physics, just this completely made my head explode. In fact, some of my favorite discoveries I made late. I later realized that it had been discovered earlier by someone quite famous for it, so it's too late for me to even publish it, but that doesn't diminish in any way the emotional experience you have when you realize that like,

Speaker 1:          00:34:10       wow. yeah. So what would it take in that moment that wow, that was yours in that moment. So what do you think it takes for an intelligent system and agi system? An ai system to have a moment like that.

Speaker 2:          00:34:25       Yeah, that's a tricky question because there are actually two parts to it, right? One of them is candidates accomplish that proof. I can prove that you can never write a to the end plus b to the n equals three. That equals z for all integer as well, etc. Etc. When, when is bigger than two, the best simply in the question about intelligence, can you build machines that are intelligent?

Speaker 2:          00:34:54       Uh, no. I think by the time we get a machine that can independently come up with that level of proofs, probably quite close to agi. The second question is a question about consciousness. Oh, when will we, will we'll ins how likely it is if it's such a machine would actually have any experience at all as opposed to just being like a zombie and would we fact that they have some sort of emotional response to this or anything at all akin to human emotion where now when it accomplishes its machine goal, is it that the views that this somehow something very positive and, and, and sublime and, and, and, and deeply meaningful. I would certainly hope that if in the future we do create machines that are our peers or even our descendants. Yeah, but I would certainly hope that they do have this sort of supply him as a blind appreciation of life. In a way. My absolutely worst nightmare would be that in at some point in the future

Speaker 2:          00:36:05       distance future, maybe our cosmos is teaming with all this post biological life, doing all the seemingly cool stuff and maybe the last humans by the time our our species eventually fizzes out will be like, well, that's okay because we're so proud of our descendants here and look at what all the. My worst nightmare is that we haven't solved the consciousness problem and we haven't realized that these are all the zombies. They're not aware of anything anymore. Then the tape recorder, is it any kind of experience? So at the whole thing has just become a play for empty benches. That will be the ultimate zombie apocalypse. So I. I would much rather in that case that's we have these beings when we just really appreciate how the, how amazing it is,

Speaker 1:          00:36:56       and in that picture of what would be the role of creativity, but a few people ask about creativity. Do you think when you think about intelligence, I mean certainly the, uh, the story he told at the beginning of your book and, you know, creating movies and so on, started making, making money. You can make a lot of money in our modern world with music and movies. So if you are an intelligent system, you may want to get good at that. But that's not necessarily what I mean by creativity is an important on that complex goals where the seas rising for there to be something creative or am I being very human centric and thinking creativity somehow special, uh, relative to intelligence.

Speaker 2:          00:37:41       My hunch is the, we should think of creativity simply as an aspect of intelligence. And, uh, we would have to be very careful with the human vanity we have. We have this tendency at the very often one and say, as soon as machines can do something, we try to diminish it and saying, oh, but that's not like real intelligence, you know, he was a traitor or this or that. The other thing, um, maybe if we ask ourselves to write down a definition of what we actually mean by being creative, what we mean by andrew weil's, what he did there, for example, don't we often mean that someone takes a very unexpected leap. It's not like taking 573 and multiplying it By 224 by does this step of straightforward cookbook like rules, right? This maybe major you have, haven't make a connection between two things that people have never thought it was connected to a surprising or something like that. I think. I think this is an aspect of intelligence. Uh, and uh, this is actually one of the most important aspects of it. Maybe the reason we humans tend to be better at it than a traditional computer is because it's something that comes more naturally. If you're a neural network, then if you're a traditional logic gates based computer machine and we physically have all these connections and you activate here, activated here, activity here at the time, my hunch is that if we ever build a machine,

Speaker 2:          00:39:20       well you could just give it the task. Hey, hey, uh, uh, you, you say, hey, you know, I just realized that I have, I want to travel around the world. Is that this month? Can you teach my eight age course for me? And it's like, okay, I'll do it. And it does everything that you would have done and the improvisers and stuff. Yeah. That, that would in my mind involve a lot of creativity.

Speaker 1:          00:39:43       Yeah. So at sexy and beautiful way to put it. I think we do try to grab grasp at the, you know, the, the, the definition of intelligence is everything. We don't understand how a, how to build. So like, so we as humans try to find things that we have, uh, machines don't have. And maybe creativity is just one of the things. One of the words we use to describe that as sort, really interesting way to put it. I don't think we need to

Speaker 2:          00:40:08       be that defensive. I don't think anything good comes out of saying, well, we're somehow special. You know, it's, it's, um, con is there are many examples in history of where trying to pretend that we're somehow superior to all other intelligent beings has led to pretty bad results. Right? Right. And nazi Germany, they said that they were superior to other people. Uh, today we still do a lot of cruelty to animals by saying that we're so superior somehow on the, they cAn't feel pain. A slavery was justified by the same kind of just really weak, weak arguments. And, and, and, uh, I don't think if we actually go ahead and build artificial general intelligence, it can do things better than us. I don't think we should try to found our self worth on some sort of bogus claims of superiority in terms of our intelligence. Alright. I think we should instead find our, a

Speaker 2:          00:41:17       calling and then the meaning of life from, from the experiences that we have right now, I can have, I can have very meaningful experiences even if there are other people who are smarter than me, you know, when I go to the faculty meeting here and I was like, are we talking about something? And then I suddenly realize, oh, but he has an old price. He has an old price. He has no pride. I don't have one. Does that make me enjoy life any less or enjoy talking to those people? That's of course not right. And the ontario is, I, I feel very honored and privileged to get to interact with, with, uh, other, very intelligent beings that are better than me. A lot of stuff. So I don't think there's any reason why we can't have the same approach with intelligent machines. That's a really interesting.

Speaker 2:          00:42:07       So people don't often think about that. They think about when there's going. If there's machines that are more intelligent, he naturally think that that's not going to be a beneficial type of intelligence. You don't realize it could be, you know, like peers with nobel prizes that, that will be just fun to talk with and they might be clever about certain topics and uh, you can have fun having a few drinks with them. So. Well also, you know, another example. So we can all relate to it of why it doesn't have to be a terrible thing to be impressed with the friends of the people that are even smarter than us all around is when, when you and I were both two years old, I mean our parents were much more intelligent than us right here. Worked out okay because their goals were aligned with our goals and that I think is really the number one issue we have to solve if we value in line with the value alignment problem. Exactly. Because people who see too many hollywood movies, a lousy science fiction, a plot lines, they worry about the wrong thing, right? They worry about some machines only attorney evil.

Speaker 2:          00:43:16       It's not malice that we, that's the issue. The concern, it's competence by definition. Intelligent makes you, makes you very competent. Did you have a more intelligent goal playing mr computer playing as the less intelligent one and when we define intelligence is the ability to accomplish, go winning, right? It's going to be in the more intelligent one that wins and if you have

Speaker 2:          00:43:42       are human and then you have um, an agi and that's more intelligent than all ways and they have different goals. Guess was going to get their way. Right? So I was just reading about, I was just reading about this particular rhinoceros species that was driven extinct just a few years ago. I was looking at this cute picture of mommy rhinoceros with it's that child, you know, and why did we humans drive it to extinction wasn't as big as we were evolved rhino haters, right? As a whole. It was just because we, our goals weren't aligned with those of the rhinoceros and it didn't work out so well for the rhinoceros because we were more intelligent. Right? So I think is just so important that if we ever do build agi before we unleash anything, we have to make sure that it learns to understand our goals adopts our goals. And it

Speaker 1:          00:44:36       retains those goals. So the cool, interesting problem there is being able to us as human beings trying to formulate our values. So you know, you could think of the United States constitution as a, as a way that people sat down at the time, a bunch of white men, but which is a good example, I should, should say, uh, they formulated the goals for this country and a lot of people agree that those goals actually held up pretty well. It's an interesting formulation of values and failed miserably in other ways. So for the value alignment problem and the solution to it, we have to be able to put on paper, uh, or in, in a, in a program, human values. How difficult do you think that is?

Speaker 2:          00:45:22       Varied, but it's so important. We really have to give it our best and it's difficult for two separate reasons. There's the technical value alignment problem of figuring out how to make machines understand the goals, document routine them. And then there's the separate part of it. The philosophical part was values anyway. And since we, it's not like we have any great consensus on this planet, on values, how, what mechanisms should we create then to aggregate them decide, okay, what's a good compromise? Right at that second discussion, can't just be left the tech nerds like myself, right? That's right. And if we refused to talk about it and then agi gets built, who is going to be actually making the decision about whose values it's gonna be a bunch of dudes and some tech company. Okay. And are they necessarily should so representative all of human kind that we want to just enjoy the development. Are they even uniquely qualified to speak to future human happiness just because they're good at programming ai? I'd much rather have this be a really inclusive conversation,

Speaker 1:          00:46:30       but do you think it's possible sort of that. So you create a beautiful vision that includes a. So the diversity, cultural diversity and various perspectives on discussing rights, freedoms, human dignity, but how hard is it to come to that consensus? Do you think? It's certainly a really important thing that we should all try to do, but do you think it's feasible?

Speaker 2:          00:46:54       I think there's no better way to guarantee failure than through to refuse to talk about it or if you try, and I also think it's a really bad at strategy to say, okay, let's first have a discussion for a long time and then once we reached complete consensus, then we'll try to load it into some machine. No, we shouldn't let perfect be the enemy of good. Instead we should start with the kindergarten ethics. Pretty much everybody agrees on and put that into our machines. Now we're not doing that. Even look at the, you know, anyone who builds as a passenger aircraft wants it to never under any circumstances and fly into a building or a mountain. Right. Yet the september 11 hijackers were able to do that and even more embarrassing, you know, under his little bits, this depressed germanwings pilot when he flew his passenger jeff into the alps, killing over a hundred people.

Speaker 2:          00:47:48       He just told him what a pilot to do it. He told the freaking computer change the altitude a hundred meters and even though it had the gps maps everything, the computer was like, okay, so we should. We should take those very basic values, so where the problem is, not that we don't agree, maybe a problem is just we've been too lazy to try to put it into our machines and make sure that from now on arab airplanes, we'll just, which all have computers in them, but we'll just just refused to do something like that, go into safe mode, maybe lock the cockpit, the org or the nearest airport and and there's so much other technology and in our world as well now where it's really coming quite timely to put in some sort of very basic values like this. Even in cars, we had enough a vehicle terrorism attacks by now will be have driven trucks and vans into pedestrians. That is not at all a crazy idea that it does have that hardwired into the car because yeah, there were a lot of. There's always going to be people who for some reason want to harm others, but most of those people don't have the technical expertise to figure out how to work around something like that. So if the car just won't do it, it helps. So let's start there.

Speaker 1:          00:49:02       So there's a lot of, as a great point, so not, not chasing perfect. There's a lot of things that, a lot that most of the world agrees on. Yeah. Let's start there. Let's start there.

Speaker 2:          00:49:12       And, and, and then once we start there, we'll also get into the habit of having these kinds of conversations about, okay, what else should we put in here? And

Speaker 1:          00:49:20       having these discussions. This should be a gradual process then. Great. So, but that also means describing these things and describing it to a machine. So one thing we had a few conversations with Steven Wolf from, I'm not sure if you're familiar with stephen bosch. I know him quite well, so he has, you know, he played, he works at a bunch of things, but you know, cellular automata, these simple computable things, these computation systems and you kind of mentioned that, you know, we probably have already within these systems already something that's agi.

Speaker 2:          00:49:55       Wow. Um, meaning like

Speaker 1:          00:49:57       we just don't know it because we can't talk to it. So if you give me this chance to try to try to at least form a question out of this is

Speaker 2:          00:50:07       I think it's an interesting idea to.

Speaker 1:          00:50:10       I think that we can have intelligent systems, but we don't know how to describe something to them and they can't communicate with us. I know you're doing a little bit of work and explainable ai trying to get ai to explain itself. So what are your thoughts of natural language processIng or some kind of other communication? How. How does the ai explained something to us? How do we explain something to it to machines or you think of it differently.

Speaker 2:          00:50:35       So there are two separate parts to your question there. Either one of them has to do with communication, which is super interesting. Yell and get that in a sec. The other is whether we already have agi will you just haven't noticed that there. I beg to differ. I don't think there's anything in any cellular automaton or anything or the internet itself or whatever that has artificial general intelligence ends it. It's an really do exactly everything we humans can do better. I think that they, if they, that happens, when that happens, we will very soon notice will probably notice even before and if, because in a very, very big way, uh, but for, for the second part,

Speaker 1:          00:51:19       can I, sorry. So, uh, the cause you, you have this beautiful way to formulating consciousness as a, as a, you know, as Information processing and you can think of intelligence and information processing in this. You can think of the entire universe as these particles and these systems roaming around that have this information processing power. It, you don't, you don't think there is something with the power to process information in the way that we human beings do that's out there that um, that needs to be sort of connected to. It seems a little bit philosophical perhaps, but there's something compelling to the idea that the power is already there, which is the focus should be more on the and being able to communicate with it. Well, I agree that the,

Speaker 2:          00:52:10       and some, in a certain sense, the hardware processing power is already out there. Our universe itself

Speaker 2:          00:52:18       can think of it as being a computer already, right? it's constantly computing what water waves, how to evolve the waterway waves in the river charles and how to move the air molecules around the seth lloyd has pointed out my colleague here that you can even in a very rigorous way of thinking of our entire universe is being a quantum computer. It's pretty clear that our universe supports this amazing processing power because you can even within this physics computer that we live in, right, we can even build actually laptops and stuff so cleaner. The power is there. It's just that most of the compute power that nature has its, in my opinion, kind of wasting on boring stuff like simulating yet another ocean wave somewhere where no one is even looking. Right. So in a sense or what life does, what we are doing when we build the computers is where rechanneling all this compute that nature is doing anyway.

Speaker 2:          00:53:07       Right? And they're doing things that are more interesting than just yet another ocean wave, you know, and do something cool here. So the roy hardware power is there for sure, But. And then even just keep shooting what's going to happen for the next five seconds in this water bottle, you know, takes a ridiculous amount of compute if you do it on a human computer. Yeah, just bought a ball, does did it. But that does not mean that this water bottle has agi and because agi means it should also be able to have written my book done this interview. Yes. And I don't think it's just the communication problems. I think it's 10 do it and other buddhists say when they watch the water and that there is some beauty that there is some depth in nature that they can communicate with. Communication is also very important because I mean, look, I'm a part of my job is being a teacher and I know some very intelligent professors even who just have a bit of hard time communicating.

Speaker 2:          00:54:09       They have all these brilliant ideas, but to communicate with somebody else, you have to also be able to simulate the own mind. Yes. Empathy built well enough that under that model of their mind that you can say things that they will understand and that's quite difficult. And that's why today it's so frustrating if you have a computer that's makes some cancer diagnosis and you ask it, well, why are you saying I should have this surgery if it and if you don't want to reply, I was trained on five terabytes of data and this is my diagnosis. Boop, boop, beep, beep doesn't really instill a lot of confidence. Right, right. So I think we have a lot of work to do on um, on communication there. So what kind of, we'll kind of, um, I think you're doing a little bit of work and explainable ai, uh, what do you think are the most promising avenues?

Speaker 2:          00:55:01       Is it mostly about sort of the alexa problem of natural language processing, of being able to actually use human interpretable methods of communication? So being able to talk to a system and it talked back to you, or is there some more fundamental problems to be solved? I think it's all of the above. Human, the natural language processing is obviously important, but they're also more nerdy fundamental problems. Like if you, if you take a, you play chess, you have to have to go to, when did you learn russian? When you watch them teach yourself rushing shit though. Watch a mogul mom. Bill stuff enough. Wow.

Speaker 2:          00:55:50       Languages do you know? Wow, that's really impressive. Wife has some contact basis. But my point was if you play chess, you have, you looked at the alpha zero games, the, uh, the actual games. Now check it out. Some of them are just mind blowing. Really beautiful. And, and if you ask how did it do that? You go talk to them as our base. I know others from beat mine, all they will ultimately be able to give you is a big tables of numbers, matrices that define the neural network and you can stare at these people's numbers until your face turns blue and you're not going to understand much about why it made that move. And uh, even if you have a natural language processing, they can tell you in human language about, oh, five, seven points to eight, still not going to really help. So I think think there's a whole speCtrum of, of a fun challenge they're involved in and taking it computation does intelligent things and transforming it into something equally good, equally intelligent, but that's more understandable and I think that's really valuable because I think

Speaker 2:          00:57:05       as we put machines in charge of ever more infrastructure in our world, the power grid that is trading on the stock market, weapons systems and so on, it's absolutely vital that we can trust these ais to do all we want and trust really comes from understanding in a very fundamental way. And um, that's why I'm, that's why I'm working on this because I think the more if we're going to have some hope of, of ensuring that machines have adopted our goals and that they're going to retain them, that kind of trust I think needs to be based on things you can actually understand, preferably even make it had preferably have improved serums on it. Even with a self driving car, right? If someone just tells you it's been trained on tons of data and it never crashed, it's, it's less reassuring then if someone actually has a proof, maybe it's a computer verified proof, but still it says that under no circumstances is this car just going to swerve into oncoming traffic

Speaker 1:          00:58:02       and, and that kind of information helps build trust and build the alignment, the alignment of goals, the at least awareness that your goals, your values that align.

Speaker 2:          00:58:12       And I think even the short term, if you look at her, you know that today, right? This absolutely pathetic state of cybersecurity that we have right when it's, what is it, 3 billion yahoo accounts, which app pack almost every american's credit card and so on, uh, why is this happening? It's ultimately happening because we have software took nobody fully understood how it worked. That's why the bugs hadn't been found. Right? Right. And I think ai can be used very effectively for offense for hacking, but it can also be used for defense. Hopefully automating verifiability and creating systems that are built in different ways so you can actually prove things about them. And it's, it's important.

Speaker 1:          00:59:05       So speaking of software that nobody understands how it works, of course a bunch of people asked by your paper about your thoughts of why does deep and cheap learning work so well as the paper, but um, what, what, what are your thoughts on deep learning, these kind of simplified models of our own brains and have been able to do some successful perception work pattern recognition work and now with alpha zero and so on, do some, some clever things. What are your thoughts about the promise limitations of this piece?

Speaker 2:          00:59:35       Right. I think there are a number of very important insights, very important lessons we can already draw from these kinds of successes. One of them is when you look at the human brain and you see it's very complicated, 10th of 11 neurons and they're all different kinds of neurons and yada yada. And there's been as long debate about whether the fact that we have dozens of different kinds is actually necessary for intelligence, which are, I think quite convincingly answer that question of no, it's enough to have just one kind. If you look under the hood of alpha zero, there's only one kind of neuron and this ridiculously simple, that simple mathematical thing, so it's not the. It's just like in physics, it's not the. If you have a gas with waves in it, it's not the detailed nature of the molecule, doesn't matter. It's the collective behavior.

Speaker 2:          01:00:25       Somehow. Similarly it's, it's, it's this higher level structure of the network matters. Not that you have 20 kinds of nuances. I think my brain is such a complicated mess because it wasn't devolved just to be intelligent. They wasn't involved to also be self assembling, right and self repairing. Right, and the evolutionarily attainable and so on. So I think it's pretty my hunches that we're going to understand how to build agi before we fully understand how our brains work. We we understood how to build flying machines long before we were able to build a mechanical work bird. Yeah, that's all right. You're given. You're given that the example exactly. Mechanical birds and airplanes and airplanes do a pretty good job of flying without really mimicking bird flight. And even now, after 100 years, 100 years later, they just see the ted talk with this german mechanical heard you mentioned it.

Speaker 2:          01:01:25       IT's amazing. But even after that, right, we still don't fly and mechanical bar because it turned out the way we came up with simpler is better if our purpose is. And I think it might be the same there. So that's one lesson. And another lesson, which is more what the paper was about. Well, first I, a physicist thought it was fascinating how there's a very close mathematical relationship actually between artificial neural networks. Um, a lot of things that we've studied for and physics go by nerdy names like the renormalization group equation and um, opinions and yada, yada, yada. And, and, um, when you look a liTtle more closely at this, you have a, you, as far as I was like, whoa, there's something crazy here that doesn't make sense because we know that if you even want to a super simple neural network pel and tap pictures and dog pictures, right? That you can do that very well. Very well now. but if you think about it a little bit, do you convince yourself that must be impossible because if I have one megapixel, even if each pixel is just black or white, there's to the power 1 million possible images is way more than there are atoms in the universe, right? So in order to. And then for eaCh one of those I have to assign a number, which is the probability of that. It's a dog, right? So an arbitrary function of images

Speaker 2:          01:02:52       is a list of more numbers than there are atoms in our universe. So clearly I can't store that under the hood of my, my gpu or my computer, yet somehow works. So whaT does that mean? Well, it means that out of all of the problems that you could try to solve with the neural network, almost all of them are impossible to solve with a reasonably sized one.

Speaker 2:          01:03:17       But then what we showed in our paper was, was that the, the, the kind of problems, the fraction of all the problems that you could possibly pose that we actually care about, given the laws of physics, is also an infant of testimony. Tiny little part and amazingly they were basically the same part. Yeah. It's almost like the world was created for. I mean they kind of come together. Yeah. You could say maybe where the world created the world. The world was created for us, but I have a more modest than interpretation, which is that instead evolution in that, but neural networks are precisely for that reason, right? Because this particular architecture as opposed to the one in your laptop is very, very well adapted to solving the kinds of problems that nature chapter presenting yet our ancestors with. Right. So it makes sense that why do we have a brain in the first place? It's to be able to make predictions about the future and so on. So if we had a sucky system which could never solve, it wouldn't have been so. But it's so this, this, this, this is a. I think you're very beautiful fact. We also, we also realize that there's a there that we have been. It's been earlier work on,

Speaker 2:          01:04:30       yes, deeper networks are good, but we were able to show an additional cool factor which is that the even incredibly simple problems like support, like give you a follow the numbers and asked you to multiply them together and already you can write a few lines of code, boom, done trivial. If you just try to do that with a neural network that has only one single hidden layer in it, you can do it,

Speaker 2:          01:04:54       but you're going to need two to the power of thousand neurons and to multiply the numbers, which is again more neurons than there are atoms in our universe. That's not saying, but if you're allowed, if you allow yourself make it a deep networks with many layers, you only need 4,000 euros. It's perfectly feasible. So that's an. Yeah. So on another architecture type, I mean you mentioned schrodinger's equation and what are your thoughts about quantum computing and the role of this kind of computational unit and creating an intelligence system in some hollywood movies. Not mentioned by name because I don't want to spoil them. The way they get agi is building a quantum computer because of the word quantum. Sounds cool. And so on. My, first of all, I think we don't need quantum computers. They build agi. I suspect your brain is not quantum computer and a new found sense. So you don't even wrote a paper about that many years ago. I checked the data, the decoherence decoherence time that how long it takes until the quantum computer ness of what your new orleans doing gets erased by just random noise from the environment. And if it's about 10 to the minus 21 seconds. So as cool as it would be that have a quantum computer in my head, I don't think that fast. On the other hand, there are

Speaker 2:          01:06:31       very cool things you could do with quantum computers. Alright. I think we'll be able to do soon when we get big ones, bigger ones that might actually help machine learning do even better than the brain. So for example,

Speaker 2:          01:06:47       one, this is a moonshot, but um, I'm, you know, learning is very much the same thing as a search. If you have a, if you're trying to train a neural network, they get really learned to do something really well. You have some loss function, you have some, you have a bunch of knobs you can turn represented by a bunch of numbers and you're trying to tweak them so that it becomes as good as possible as this thing, so if you think have a landscape with some valley where each dimension of the landscape corresponds to some number you can change. You're trying to find the minimum and it's well known that if you have a very high dimensional landscape, complicated things, it's super hard to find the minimum. Right? Quantum mechanics is amazing. Good at this, right? I get it. If I want to know what's the lowest energy state, this water can possibly have

Speaker 2:          01:07:42       incredibly hard to compute, but we can put. Nature will happily figure this out for you if you just cool it down, make it very, very cold. If you put a ball somewhere, it'll roll down to its minimum, and this happens metaphorically at the energy landscape to and quantum mechanics. Even using some clever tricks, which today is machine learning systems don't like if you're trying to find the minimum when you get stuck in the little local minimum here in quantum mechanics or connects the tunnel through the barrier and get unstuck again. And um, that's really. Yeah. So, so maybe for example, we will one day use quantum computers that help train neural networks better.

Speaker 1:          01:08:23       That's really interesting. Okay. So as a component of the learning process, for example. Yeah. Let me ask sort of wrapping up here a little bit. Let me, let me return to, uh, the questions of our human nature and, and love as I mentioned. So do you think, you mentioned sort of a helper robots, but you can think of also personal robots. Do you think the way we human beings fall in love and get connected to each other, it's possible to achieve in an ai system and human level, ai intelligent system. Do you think, what would ever see that kind of connection or a, you know, in all this discussion about solving complex goals as this kind of human social connection, do you think that's one of the goals and the peaks and valleys that were the raising sea levels that we'll be able to achieve? or do you think that's something that's ultimately a, or at least in the short term, relevancy of the goals is not achievable? I think it's all possible

Speaker 2:          01:09:25       and um, in, in, in recent there's a, there's a very wide range of justice as you know, among ai researchers when we're going to get agi. Some people, you know, like our friend rodney brooks said it's going to be a hundred year lease and then there are many others. I think it's going to happen relative much sooner and recent polls maybe half or so or I received your thinking we're going to get a gi within decades. So if that happens, of couRse, I think these things are all possible, but in terms of whether it will happen, I think we shouldn't spend so much time asking what do we think will happen in the future as if we are just some sort of pathetic. You're passive bystanders, you know, waiting for the future to happen to us. Hey, we're the ones creating this future, right? So we should be pRoactive about it and ask yourself what sort of future we would like to have happen.

Speaker 2:          01:10:19       I want to make it like that. What would I prefer it to? Some sort of incredibly boring zombie like future were just all these mechanical things happen and there's no passion or emotion or experience maybe even know I would much rather prefer if, if all the things that we find that we value the most about humanity or subjective experience, passion, inspiration, you love, you know, if, if, if, uh, we can create a future where those are, those things do exist now. I think ultimately it's not our universe giving meaning to us, us giving me the universe and if we Build more advanced and pillages let's, let's make sure we're building in such a way that meetings as part of it,

Speaker 1:          01:11:09       a lot of people that are seriously study this problem and think of it from different angles have

Speaker 2:          01:11:14       troubling the majority of cases if they think through that happen are the ones that are not beneficial to humanity. Right. And so yeah. So what, what, what are your thoughts? What's an inch? What's, what should people, you know, I really don't like people to be terrified he should. What? What's a way for people to think about it in the way that it's set? In a way we can solve it. We couldn't make it, but yeah. No, I don't think panicking is gonna help in any way. Not going to increase chances of things going well either. Even if you are in a situation where there is a real threat, does it help if everybody just freaks out? Right? No, of course, of course not. I think, yeah, there are of course ways in which things can go horribly wrong. First of all, it's important when we think about this thing, about the problems and risks. The also remember how huge the upsides can be if we get it right, right? Everything, everything you love about society and to the nation as a product of intelligence. So if we can amplify our intelligence with machine intelligence and not any more, lose our loved ones that were told in an uncurable disease and things like this. Of course we should aspire to that, so that can be a motivator. I think reminding ourselves that the reason we try to solve problems, it's not just because

Speaker 2:          01:12:31       we're trying to avoid gluten, but because we're trying to do something great, but then in in terms of the risks, I think jim,

Speaker 2:          01:12:41       the entry of the important question is to ask what can we do today that will actually helped? I'll come good and the dismissing the risk is not one of them. You know? I find it quite funny often when I'm in on the discussion panels about these things, how the people who worked for hca, for companies, what we say, I always like, ah, nothing to worry about, nothing to worry about necessarily worry about. And it's always, oh, it's only academics sometimes express concerns. That's not surprising at all. If you think about it, right? Upton sinclair quipped, right? That the, it's hard to make you believe in something when his income depends on not believing in it. And frankly we know a lot of these people in companies that they are just as concerned as anyone else. But if you're the ceo of a company that's not something you want to go on record saying when you have silly journalists, Oregon that put a picture of a terminator robot when they quote you. So, so the, the, the issues are real. And the way I, the way I think you might, what the issue is, it is basically, you know,

Speaker 2:          01:13:44       the real choice we have is first of all, are we going to just dismiss this, the risks and say, well, you know, let's just go ahead and build machines that can do everything we can do better and cheaper, you know, let's just make ourselves obsolete this fast as possible. And what could possibly go wrong? That's one attitude. The opposite attitude. I think it's the same,

Speaker 2:          01:14:06       here's this incredible potential, you know, let's, let's think about what kind of. We're really, really excited about what are the shared goals that we can really aspire towards. And then let's think really hard about how we can actually get there as to start with not. Don't start thinking about the risks. Start thinking about the goals. Goals. Yeah. And then when you do that, then you can think about the obstacles you want to avoid. Why they often get students coming in right here inTo my office for career advice. I always asked them this question, where do you want to be in the future? Man? If all she can say as, oh, maybe I'll have cancer, maybe I'll run over by obstacles instead of the goal and he's just going to end up a hypochondriac, paranoid. Whereas if she comes in and fire in her eyes and is like, I want to be there and then we can talk about the obstacles and see how we can circumvent them.

Speaker 2:          01:14:55       That's, I think a much, much healthier attitude and um, that's really well put. And, and uh, I, I feel it's very challenging to come up with a vision for the future which were, which were unequivocal excited about it. I'm not just talking now in the vague terms like, yeah, let's cure cancer. Fine. Talking about what kind of society that we want to create, what do we want it to mean to be human in the age of ai, when are in the age of agi. So if we can have this conversation, broad, inclusive conversation and gradually start converging towards some, some future that with some direction at least that we want to steer towards right then, then now will be much more motivated to constructively take on the obstacles. And I think if I, uh, if I had to, if you make me, if I read a wrap this up in a more succinct way, I think, I think we can all agree already now though, we should aspire to build agi but doesn't overpower us, but that empowers us

Speaker 1:          01:16:05       and think of the many various ways they can do that. Whether that's from a mile side of the world of autonomous vehicles. I personally actually from the camp that believes this human level intelligence is required to, to achieve something like vehicles that would actually be something we would enjoy using and being part of sets the one example and certainly there's a lot of other types of robots and medicine and so on. Uh, so focusing on those and then, and then coming up with the obstacles, coming up with the ways that that can go wrong and solving those one at a time.

Speaker 2:          01:16:38       And just because you can build an autonomous vehicle, even if you could build one that would drive this final, as you know, maybe there was some things in life that we would actually want to do ourselves. That's right. Like for example, if you think of our society as a whole, there are some things that we find very meaningful to do and uh, that doesn't mean you have to stop doing it. I'm just because machines can do them better, you know, I'm not Gonna stop playing tennis that day. Someone build a tennis robot, beat me. People are still still

Speaker 1:          01:17:08       playing chess and even go

Speaker 2:          01:17:10       and, and uh, in this, in the very near term, even some people are advocating basic income replace jobs, but if you, if, if the government is going to be willing to just hand out cash to people for doing nothing, uh, ben, once you also seriously consider whether the government will also hire a lot more teachers and nurses and the kind of jobs which people often find great fulfillment in doing, right? I get very tired of hearing politicians saying, oh, we can't afford hiring more teachers, but we're going to maybe have basic income if we can have more, more serious research and thought into what gives meaning to our lives. And the jobs give so much more than income. Right? And then thinking about in the future, what are the role of the, the roles that we want to have people feeling and powered by machines

Speaker 1:          01:18:03       and I think sort of, um, I come from the Russia, from the soviet union and um, I think for a lot of people in the 20th century going to the moon, going into space was in an inspiring thing. I feel like the, the, the, the universe of the mind. So ai understanding, creating intelligence is that for the 21st century. So it's really surprising and I've heard you mentioned this, it's really surprising to me both on the research funding side, that it's not funded as greatly as it could be, but most importantly on the politician side, that it's not part of the public discourse except in the killer bots, terminator kind of view that people are not yet, I think perhaps excited by the possible positive future that we can build together. So yeah,

Speaker 2:          01:18:48       we should be, because politicians usually just focus on the next election cycle, right? Right. The single most important thing I feel we humans have learned and the entire history of science is there were the masters of underestimation. We underestimated the size of a, our cosmos again and again, realizing that everything we thought existed, it was just a small part of something grander. I find solar system, the galaxy, you know, clusters of galaxies, universe, and we now know that the future has so much more potential than our ancestors could ever have dreamt of this cosmos. But imagine if all of earth was completely devoid of life except for cambridge, Massachusetts. I would. Wouldn't it be kind of lame if all we ever aspired to was to stay in cambridge, Massachusetts forever and then go extinct in one week. Even though earth was going to continue on for longer than that. That sort of attitude. I think we have now on the cosmic scale. We can fill it. Life can flourish on earth, not for for four years, but for billions of years. I couldn't even tell you about how to move it out of harm's way when the sun gets too hot and and then we have so much more

Speaker 1:          01:20:08       resources out here, which

Speaker 2:          01:20:10       today, yeah, maybe there are a lot of other planets with bacteria or childlike life on them, but I most of this, all this opportunity to seems as far as we can fail to be largely bed like the sahara desert and yet we have the opportunity but the help life flourish billions of and so like let's quit squabbling of a. Some little borders should be drawn one, one mile to the left or right.

Speaker 1:          01:20:39       Look up into the skies. Did you realize, hey, you know, we can do such incredible things and that's I think why it's really exciting that yeah, you and others are connected with some of the working elon musk is doing because he's literally going out into that space, really exploring our universe and it's wonderful. That is exactly why elon musk is so misunderstood. Misconstrued him as some kind of pessimistic doomsayer the region. He chairs so much about ai safety is because he more than

Speaker 2:          01:21:10       almost anyone else appreciates these amazing opportunities. It will squander it if we wipe out here on earth and we're not just going to wipe out the next generation, but all generations and this incredible opportunity that's out there and that would be really be a waste and ai for people who think that they'll be better to do without technology. Let me just mention that

Speaker 2:          01:21:34       if we don't improve our technology, the question isn't whether humanity is going to go extinct, questioning whether we're going to get taken out by the next big asteroid or the next super volcano or or something else dumb that we could easily prevent with more tech. Right, and if we want life to flourish throughout the cosmos, ai is the key to it. As I mentioned in a lot of detail in my book right there, even many of the most inspired scifi writers I feel have totally underestimated the opportunities for space travel, especially if the other galaxies, because they weren't thinking about the possibility of agi, which just makes it so much easier.

Speaker 1:          01:22:17       Right? Yeah. So that, that goes to your, uh, uh, view of agi that enables our progress, said enables a better life. So that's a beautiful, that's a beautiful way to put it and it's something to strive for. So, max, Thank you so much. Thank you for your time today has been awesome. Thank you so much. Yes.