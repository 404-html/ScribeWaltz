Speaker 1:          00:00:00       Today we have Josh Tenenbaum, he's a professor here at mit leading the computational cognitive science group among many other topics and cognition and intelligence. He is fascinated with the question of how human beings learn so much from so little and how these insights can lead to build ai systems that are much more efficient learning from data. So please give Josh a warm welcome.

Speaker 2:          00:00:28       Thank you.

Speaker 1:          00:00:31       All right. Thank you very much. Thanks for having me. Excited to be part of what looks like really quite a very impressive lineup, especially starting after today and it's I think quite a great opportunity to get to see perspectives on artificial intelligence from many of the leaders in industry and other entities working on this great quest. So I'm going to talk to you about some of the work that we do in our group, but also I'm going to try to give a broader perspective, reflective of a number of mit faculty, especially those who are affiliated with the center for brains, minds and machines. So you can see up there on my affiliation academically, I'm part of brain and cognitive science or course nine. I'm also part of resale, but I'm also part of the center for brains, minds and machines, which is an NSF funded center science and Technology Center, which really stands for the bridge between the science and the engineering of intelligence.

Speaker 1:          00:01:22       It literally straddles Vassar Street and that we have see sale and bcs members. We also have partners at Harvard and other academic institutions and again, what we stand for. I want to try to convey some of the specific things we're doing in the center and where we want to go with a vision that really is about jointly pursuing the science, the basic science of how intelligence arises in the human mind and brain and also the engineering enterprise of how to build something increasingly like human intelligence in machines and we deeply believe that these two projects have something to do with each other and our best pursued jointly. Now it's a really exciting time to be doing anything related to intelligence or certainly to ai for all the reasons that brought you all here. I don't have to tell you this. We have all these ways in which ai is kind of.

Speaker 1:          00:02:06       Finally here we finally live in the era of something like real practical ai or for those who've been around for awhile and have seen some of the rises and falls, you know, ai is back in a big way, but from my perspective, and I think maybe this reflects why we distinguish what we might call Agi from Ai, we don't really have any real ai. Basically we have what I like to call ai technologies, which are systems that do things we used to think that only humans could do and now we have machines that do them often quite well, maybe even better than any human who's ever lived. Right? Like a machine that plays go, but none of these systems I would say are truly intelligent. None of them have anything like common sense. None of them have anything like the flexible general purpose intelligence that each of you might use to learn every one of these skills or tasks.

Speaker 1:          00:02:53       Right? Each of these systems had to be built by large teams of engineers working together often for a number of years. I'd often at great cost to somebody who's willing to pay for it and each of them just does one thing. So alphago might beat the world's best, but it can't drive to the match or even tell you that. Go what go is, I can't even tell you the go is a game because it doesn't even know what a game is. Right? So what's missing? Why? What is it that makes every one of your brains, maybe you can't beat, you know, the world's best in go, but any one of you can get behind the wheel of a car. I think of this because my daughter is going to turn 16 tomorrow if she lived in California, she'd have a driver's license. It's a little bit down the line for us here in Massachusetts.

Speaker 1:          00:03:36       But um, you know, she didn't have to be specially engineered by a billion dollar startups and you know, she got really into chess recently and now she's taught herself chest by playing just a handful of games basically. Um, and she can do any one of these activities at any one of us can. So what is it? What's the, what makes up the difference? Well, there's many things, right? Um, I'll talk about the focus for us and our research and a lot of us, again, in CBMM is summarized here. Um, what, what drives the success is right now in ai, especially in industry. Okay. And all of these ai technologies is many, many things, many things, but what, what, where the progress has been made most recently and what's getting most of the attention is of course deep learning, but other kinds of machine learning technologies which essentially represent the maturation of a decades long effort to solve the problem of pattern recognition.

Speaker 1:          00:04:28       That means taking data and finding patterns in the data that tells you something you care about, like how to label a class or how to predict some other signal. Okay. Um, and pattern recognition is great. It's an important part of intelligence and it's reasonable to say that deep learning as a technology has really made great strides on pattern recognition and maybe even, you know, has coming close to solving the problems with pattern recognition. But intelligence is about many other things. Intelligence about a lot more in particular. It's about modeling the world and think about all the activities that a human does from all over the world that that go beyond just say, recognizing patterns in data, but actually trying to explain and understand what we see, for instance, okay. Or to be able to imagine things that we've never seen that never seen, maybe even very different from anything we've ever seen, but we want to see and then two, to set those as goals to make plans and solve problems needed to make those things real or thinking about learning.

Speaker 1:          00:05:26       Again, some kinds of learning can be thought of as pattern recognition if you're learning sufficient statistics or weights and a neural net that are used for those purposes, but many activities of learning are about building out new models, right? Either refining, reusing and proving old bottles or actually building fundamentally new models as you've experienced more of the world. And then think about sharing our models, communicating our models to others, modeling their models, learning from them. All of these activities of modeling. These are at the heart of human intelligence and it requires a much broader set of tools. So I want to talk about the ways that we're studying these activities of modeling the world and something in a pretty nontechnical way about what are the kinds of tools that allow us to capture these abilities. Now I think it's. I want to be very honest upfront and to say this is just the beginning of a story, right?

Speaker 1:          00:06:13       When you look at deep learning successes, that itself is a story that goes back decades. I'll say a little bit about that history in a minute, but where we are now is just looking forward to a future when we might be able to capture these abilities at a really mature engineering scale and I would say we are far from being able to capture the all the ways in which humans richly flexibly, quickly build models of the world at the kind of scale that say silicon valley wants either big tech companies like Google or Microsoft or IBM or facebook or small startups. Right? We can get there and I think what I want to talk to you about here is one route for trying to get there and this is the route that CBMM stands for, the idea that by reverse engineering how intelligence works in the human mind and brain that will give us a route to engineering these abilities in machines.

Speaker 1:          00:06:59       When we say reverse engineering, we're talking about science, but doing science like engineers. This is our fundamental principle that if we approach cognitive science and neuroscience like an engineer, where so the output of our science isn't just a description of the brain or the mind in words, but in the same terms that an engineer would use to build an intelligent system, then that will be both the basis for a much more rigorous and deep, the insightful science, but also direct translation of those insights into engineering applications. Now I said before I talk a little about history, um, what I mean by that is this, again, if, if part of what brought you here as deep learning, and I know even if you've never heard of deep learning before, which I'm sure is unlikely you saw some, you know, a good spectrum of that in the, in the overview session, uh, last night.

Speaker 1:          00:07:42       Okay. Um, it's really interesting and important to look back on the history of where did techniques for deep learning come from or reinforcement learning. Those are the two tools in the, in the current machine learning arsenal that are getting the most attention, things like backpropagation or end to end, stochastic gradient descent or temporal difference learning or cue learning. Here's a few papers from the literature. May, you know, maybe some of you have read these original papers. Here's, here's the original paper by Romel heart, Hinton and colleagues in which they introduced the backpropagation algorithm for training multilayer, perceptrons, right? Multilayer neural networks. Here's the original perceptron paper by Rosenblatt, which introduced the one layer version of that architecture and the basic perceptron learning algorithm. Here's the first paper on sort of the temporal difference learning method for reinforcement learning from Sutton and bartow. Here's the original bolt machine paper also by Hinton and colleagues, which you know again is a, those who don't know that architecture can give a kind of probabilistic undirected multilayer perceptron, um, or for example, before the Lstm is if you know about current recurrent neural network architecture earlier, as much simpler versions of the same idea were proposed by Jeff Ellman and his simple recurrent networks.

Speaker 1:          00:08:49       The reason I want to put up the original papers here is for you to look at both when they were published, where they were published. So if you look at the dates, you'll see papers going back to the eighties, but even the sixties or even the 19 fifties, and look at where they were published. Most of them were published in psychology journals. So the journal psychological review, if you don't know it, is like the leading journal of theoretical psychology and mathematical psychology or cognitive science. The Journal of the cognitive science society or the back prop paper was published in nature, which is a general interest science journal. But by people who are mostly affiliated with the Institute for Cognitive Science in San Diego, so what you see here is already a long history of scientists thinking like engineers, these are people who are in psychology or cognitive science departments and publishing in those places, but by formalizing even very basic insights about how humans might learn or how, you know, brains might learn in the right kind of math that led to, of course, progress on the science side, but it led to all the engineering that we see now.

Speaker 1:          00:09:49       It wasn't sufficient, right? We needed, we needed, of course, lots of innovations and advances in computing hardware and software systems, right? But this is where the basic, the basic math came from and it came from doing science like an engineer. So what I want to talk about and our vision is what is the future of this look like? If we were to look 50 years into the future, what would we be looking back on now? Or you know, over this timescale. Well, here's a, here's a longterm research roadmap that reflects some of my ambitions and some of our centers goals and many others to right. We'd like to be able to address basic questions, fundamental questions of what it is to be and to think like a human questions for example, of consciousness or meaning and language or real learning, right? Questions like, um, you know, even beyond the individual at questions of culture or creativity.

Speaker 1:          00:10:35       Those are our big ideas up there. And for each of these there are basic scientific questions, right? How do we become aware of the world and ourselves in. It starts with perception, but it really turns into awareness, awareness of yourself and of the world and what we might call consciousness, right? Or how does a word starts to have a meaning? What really is a meaning and how does a child grasp it or how to children actually learn what to babies brains actually start with are they blank slates or do they start with some kind of cognitive structure? And then what is real learning look like? These are just some of the questions that we're interested in working on or when we talk about culture remain, how do you learn all the things you didn't directly experience, right? But that somehow you got from the accumulation of knowledge in society over many generations or how do you ever think of new ideas or answers to new questions?

Speaker 1:          00:11:19       How do you think of the new questions themselves? How do you decide what to think about? These are all key activities of human intelligence. When we talk about how we model the world, where our models come from, what we do with our models. This is what we're talking about and if we could get machines that could do these things well again on the bottom row, think of all the actual real engineering payoffs now in our center and in both my own activities and a lot of what my group does these days and what a number of other colleagues in the center for brains minds and machines do as well as an abrupt, very broadly. People in pcs and see sale. One place where we work on the beginnings of these problems in the near term, this is the longterm, like think 50 years, maybe, maybe shorter, maybe longer, I don't know, but think well beyond, well beyond 10 years, but in the short term, five to 10 years, a lot of our focus is around visual intelligence and there's many reasons for that.

Speaker 1:          00:12:04       Again, we can build on the successes of deep networks and a lot of pattern recognition and machine vision. It's a good way to put these ideas into practice. When we, when we look at the actual brain, the visual system in the brain, in the human and other mammalian brains, for example, is really very clearly the best understood part of the brain and at a circuit level, it's the part of the brain that's most inspired current, deep learning and neural network systems. But even there, there's things which we still don't really understand like engineers. So here's an example of a basic problem in visual intelligence that we and others in the center are trying to solve. Look around you and you feel like there's a whole world around you and there is a whole world around you, you know, feel like your brain captures it, but what the actual sensor data that's coming in through your eyes, it looks more like this photograph here where you can see there's a crowd scene, but it's mostly blurry except for a small region of high resolution in the center, so that corresponds biologically to what part of the images in your phobia.

Speaker 1:          00:13:01       That's the central region of cells in the retina where you have really high resolution visual data. The size of your phobia is roughly like if you hold out your thumb at arm's length, it's a little bit bigger than that, but not much bigger. Right? Most of the image it in terms of the actual information coming in and a bottom up sense to your brain is really quite blurry, but somehow by looking at just one part and then by being around or making a few eye movements, you get a few glimpses each. Not much bigger than the size of your thumb at arm's length. Somehow you stitch that information together into what feels like and really is a rich representation of the whole world around you. And when I say around you, I mean literally around you. So here's another kind of demonstration. Without turning around, nobody's allowed to turn around.

Speaker 1:          00:13:44       Ask yourself what's behind you. Now the answer is going to be different for different people. Depending on where you're sitting, right? For most of you, you might think, well, there's, I think there's a person pretty close behind me, right? You know you're in a crowded auditorium, although you haven't seen that person, you know that they're there, right? For people in the very back row, you know there isn't a person behind you and you're conscious of being in the back row, right? You might be conscious if there's a wall right behind you, but now for the people who are in the room, not in the very back, think about how far behind you is the back. Like, where's the nearest wall behind you so we can look. Maybe we can call out a try a little demonstration. So I didn't know. I'm pointing to someone there. Can you see phrase, say something if you think I'm pointing at you? Uh, well I could have been pointing at you but I'm pointing someone behind you. Okay. I'll point to you. Yeah, I'm pointing to you. Alright. So how far is the nearest wall? But no, you can't turn around. You've blown your chance without turning around. Okay. So you, you were loud. Okay. Do you see, I'm pointing to you there with the tie. Okay. So without turning around, how far is the nearest wall behind you?

Speaker 1:          00:14:46       That's how far. Five meters. Okay. Well let me, that might be about right. No other people can turn around a how now, how about you? How far is the nearest wall behind you? Ten meters. Okay. Um, that might be right? Yeah. How about here? How, how, what do you think? 20. Okay. Um, so yeah, since I didn't grow up in the metric system, I barely know, but yeah, I mean, I mean the point is that like your, your, your, each of you is surely not exactly right, but you're certainly within an order of magnitude. And I guess if we actually tried to measure, you know, you're probably, my guess is you're probably right within, you know, 50 percent or less often, maybe just 20 percent error. Okay. So how do you know this? I mean, even if it's not, what did you say? Twenty meters. Even if it's not 20 meters, it's probably closer to 20 meters than it is to five or 10 meters. And then it is to 50 meters. So how did you know this? You haven't turned around in a while, right? But some part of your brain is tracking the whole world around you, right? Um, and how many people are behind you?

Speaker 1:          00:15:49       Yeah, like a few hundred, right? I mean, I don't know if it's 200 or $300, but it's not a thousand. I don't think so. Um, and it's certainly not 10 or 20 or 50. Right? So you track these things and you use them to plan your actions. Okay. So again, think about how instantly, effortlessly and very reliably, okay, your brain computes all these things. So the people and objects around you. And it's not just, you know, approximations certainly when we're talking about what's, what's behind you in space, there's a lot of imprecision, but when it comes to reaching for things right in front of you, very precise shape and physical property estimates needed to pick up and manipulate objects. And then when it comes to people, it's not just the existence of the people but something about what's in their head, right? You track whether someone's paying attention to you and you're talking to them, what they might want from you, what they might be thinking about you, what they might be thinking about other people.

Speaker 1:          00:16:37       Okay? So when we talk about visual intelligence, this is the whole stuff we're talking about and you can start to see how it turns into basic questions I think of, of, of what we might call the beginnings of consciousness or at least our awareness of ourself in the world and of ourselves as a self in the world, but also other aspects of higher level intelligence and cognition that are not just about perception like symbols, right? To describe even to ourselves, what's around us and where we are. And what we can do with it, you have to go beyond just what we would normally call the stuff that perception to to say the thoughts in somebody's head in your own thoughts about that. Okay. So what we've been doing in CBMM is trying to develop an architecture for visual intelligence and I'm not going to go into any of the details of how this works and this is just notional.

Speaker 1:          00:17:23       This is just a picture. It's like a just a, a sketch from a grant proposal of what we say we want to do, but it's based on a lot of scientific understanding of how the brain works. There are different parts of the brain that correspond to these different modules in our architecture as well as some kind of emerging engineering way to try to capture at the software and maybe even hardware levels, how these modules might work. So we talk about a sort of an early module of a visual or perceptual stream, which is like bottom up visual or other perceptual input. That's the kind of thing that is pretty close to what we currently have and say deep convolutional neural networks. But then we talk about some kind of the output of that isn't just pattern class labels, but what we call the cognitive core core cognition.

Speaker 1:          00:18:03       So we get an understanding of space and objects, their physics, other people, their minds. That's the real stuff of cognition. That has to be the output of perception. But somehow we have to, we have, we have to have, this is what we call the brain ostp in this picture. We have to get there by stitching together the bottom up inputs from a glimpse here, a glimpse here, a little bit here and there, and accessing prior knowledge that comes from our memory systems to tell us how to stitch these things together into the really core cognitive representations of what's out there in the world. And then if we're going to start to talk about it in language or to build plans on top of what we have seen and understood, that's where we talk about symbols coming into the picture. Okay. The building blocks of language and plans and so on.

Speaker 1:          00:18:48       Okay. Um, so now we might say, well, okay, this is an architecture that is brain inspired and cognitively inspired and, and we're planning to turn into real engineering and you can say, well, do we need that? Maybe, you know, again, I know this is a question you consider it in the first lecture, maybe the engineering tool kit that's currently been making a lot of progress in let's say industry. Maybe that's good enough. Maybe you know, let's take deep learning, but to stand for a broader set of modern pattern recognition based on reinforcement learning based tools and say, okay, well maybe, uh, that can scale up to this and you might, you know, maybe that's possible. I'm happy in the question period of people want to debate this. My sense is no. Um, I think that, um, it's not a. When I say no, I don't mean like it, it can't happen or it won't happen.

Speaker 1:          00:19:32       What I mean is the highest value, the highest expected route right now is to take this more science based reverse engineering approach. And that if at least if you follow the current trajectory that industry incentives especially optimize for, it's not even really trying to take us to these things. So think about, for example, a case study of visual intelligence that is in some ways as pattern recognition, very much of a success. It's again been mostly driven by industry. It's something that if you read in the news or even play around with in certain publicly available data sets, feels like we've made great progress. And this is an aspect of visual intelligence, which is sometimes called image captioning. It's bay or mapping images to text. Um, you know, basically there's been a bunch of systems. Here's a couple of press releases. I guess this one's about Google.

Speaker 1:          00:20:18       Google's Ai can now capture images almost as well as humans. Um, here's ones about Microsoft. Um, a couple of years ago I think there were something like eight papers all released on to archive around the same time from basically all the major industry computer vision groups as well as a couple of academic partners. Okay. Which all driven by basically the same dataset produced by some Microsoft researchers. And other collaborators, I'm trained a combination of deep convolutional neural networks, you know, state of the art, visual pattern recognition with recurrent neural networks, which had recently been developed for, you know, basically kinds of neural statistical language modeling, glued them together and produce the system which, which, which made very impressive results in a big training set and a held out test set where the goal was to take an image and write a sentence like short sentence caption that that would seem like the kind of way a human would describe that image.

Speaker 1:          00:21:09       And these systems have surpassed human level accuracy on the held out test set from a big training center. But what you can see when you really dig into these things is there's often a lot of what I would call Dataset overfitting. It's not overfitting to the trainings, but it's overfitting to whatever are the particular characteristics of this data set. You know, wherever or wherever it came from. Certain set of photographs in certain ways of captioning them. Okay. Which even a big Dataset, um, it's not about quantity. It's more about the quality of the nature of what people are doing. All right. Um, so one way to test this system is to apply it to what seems like basically the same problem but not within the, a certain curated or built dataset. And there's a convenient a twitter bot that lets you do this. So there's something called the pick desk bought, which takes one of the state of the art industry.

Speaker 1:          00:21:56       Ai captioning systems. A very good one. Again, this is not meant to. I'm not trying to critique these systems for what they're trying to do. I'm just trying to point out what they don't really even try to do. So this takes the Microsoft caption bought and just every couple of hours takes a random image from the web, captions it and uploads the results to twitter. And a couple of months ago when I prepared a first version of this talk, I just took a few days in the life of this twitter Bot. I didn't take every single image, but I took, you know, most of the images in a way that was meant to be representative of the successes. And the kinds of failures that such a system will make so we can go through this and it's a little bit entertaining and I think quite informative. So here's just a somewhat random sample of a few days in the life of one of these caption bots.

Speaker 1:          00:22:39       So here we have a picture of a person holding. Fortunately my screen is very small here and I can't read up there, so maybe you'll have to tell me what sets, but a person holding a cell phone. I guess I'll just read along with you. So you have a person holding a cell phone. Well, it's not a person holding a cell phone, but it's kind of close. It's a person holding some kind of machine to. I don't even know what that is, but it's some kind of musical instrument, right? Um, so that's a mixed success or failure. Here's a pretty good one. A group of people on a, on a field playing football. That's, I would call that a, you know, a result, maybe even a plus. Um, here's a group of people standing on top of a mountain. So less good there was a mountain, but as far as I can tell there's no people, but these systems like to see people because of both the combination because in the data set they were trained on, there's a lot of people and people often talk about people.

Speaker 1:          00:23:22       Okay. And the fact that you can appreciate both what I said and why it's funny. That's there. You did some of my cognitive activities that the system is not even trying to do. Okay. Here we've got a building with a cake. I'll go through these fast building with a cake, a large stone building with the clock tower. I think that's pretty good. I'd give that like a b plus. There's no clock, but it's plausibly right. There might be a clock in there. There's definitely something like that. Here's a truck parked on the side of a building. I don't know, maybe a b minus. There is a car on the side of a building, but it's not a truck and it's. And it's not. Doesn't seem like the main thing and the image. Okay. Here's a necklace made of bananas. Here's a large ship in the water.

Speaker 1:          00:24:01       This is pretty good. I give this like an a minus or b plus because there is a ship in the water, but it's not very large. It's really more of like a tug boat or something. Here's a sign sitting on the grass, you know, in some sense that's great. No, but, but in another sense, it's really missing what's actually interesting and important and meaningful to humans. Um, here's a, uh, here's a garden is in the dirt, a pizza sitting on top of a building, a small house with a red brick building. That's pretty good. Although it kind of weird way of saying it. A vintage photo of a pond. That's good. They liked vintage photos. A group of people that are standing in the grass near bridge. Again, there's two people and there's some grass and there was a bridge, but it's really not what's going on.

Speaker 1:          00:24:39       A person in the yard. Okay. Kind of, um, a group of people standing on top of the boat. There's a boat, there's a group of people there standing. But again, it's the sentence that you see is, is more based on a bias of what people have said in the past about images that are only vaguely like this. A clock tower lit up at night. That's really, I think, pretty impressive. A large clock mounted to the side of a building a little bit less. So a snow covered feel very good. A building with snow on the ground. A little bit less good. There's no snow white. Some people who I don't know them, but I bet that's probably right because face identifying faces and recognizing people who are famous because they won medals in the Olympics. Probably I would trust current pattern recognition systems to get that.

Speaker 1:          00:25:20       A painting of a vase in front of a mirror. I'm less good. Also a famous person there, but we didn't get him a person walking in the rain. Again, there is sort of a person and there's some puddles but not, you know, a group of stuffed animals. A car parked in a parking lot that's good. A car parked in front of a building. Less good. A plate with a fork and knife. A clear blue sky. Okay, so you get the idea again, like if you actually go and play with the system, partly because I think Mike, but my friends at Microsoft told me they've improved it. Some, you know, uh, uh, this is partly for entertainment value is, you know, I chose what also would be the fun here example. So I'm clear, I want to be quite honest about this and these are. I'm not trying to take away what are impressive ai technologies, but I think it's clear that there's a sense of understanding any one of these images that it's important to see that even when it seems to be correct, right?

Speaker 1:          00:26:13       If it can make the kind of errors that it makes, that even when it seems to be correct, it's probably not doing what you're doing and it's probably not even trying to seel towards the dimensions of intelligence that we think about when we're talking about human intelligence. Okay. Another way to put this, I'm going to show you a really insightful blog post from one of your other speakers. So, uh, in a couple of days, I'm not sure you're going to have Andre carpathy who's one of the leading people in deep learning. This is a really great blog post. He wrote a couple of years ago when he was, I think still at Stanford. He got his phd from Stanford. He did. He worked at Google a little bit on some early big neural net ai projects there. Uh, he was at open Ai. He was one of the founders of open ai and recently he joined Tesla as their director of Ai Research.

Speaker 1:          00:26:58       But about five years ago he was looking at the state of computer vision from a human intelligence point of view and, and lamenting how far away we were. Okay. So this is the title of his blog posts, the state of computer vision and ai and ai. We are really, really far away. And he took this image which was a sort of a famous image in its own right. It was a popular image of Obama back when he was president, kind of playing around us. He liked to do when he was on tour. So if you take a look at this, you can see you probably all can recognize the previous president of the United States, but you can also get the sense of where he is and what's going on and you might see people smiling and you might get the sense that he's playing a joke on someone.

Speaker 1:          00:27:34       Can you see that? Right? So how do you know that he's playing a joke and what that joke is? Well, as Andre goes on to talk about and his blog posts to, if you think about all the things that, that you have to really deploy in your mind to understand that it's a huge list. Of course it starts with seeing people and objects and maybe doing some face recognition, but you have to do things like, for example, notice his foot on the scale and understand enough about how scales work that when a foot presses down, it exerts forest at the scale of sensitive, doesn't just magically measure people's weight. But it does that somehow through force. You have to see who can see that he's doing that and who can't, who cannot see that he's doing that right. And particularly the person on the scale and why some people can see that he's doing that and can see that some other people can't see it, why that makes it funny to them.

Speaker 1:          00:28:18       Okay. And someday we shouldn't have machines that can understand this, but hopefully you can see why, what I, what I, what, what the kind of architecture that I'm talking about would be the building blocks of the ingredients to be able to get them to do that. Now I, when I, again, I, I prepared a version of this talk a few months ago and I wrote to Andre and I said I was going to use this and I was curious if he, what if he had any reflections on this and where he thought we were relative to five years ago because certainly a lot of progress has been made, but he said, here's this email. Um, I hope he doesn't mind me sharing it, but I mean, again, he's a very honest person and that's one of the many reasons why he's such an important person right now in ai.

Speaker 1:          00:28:58       Okay. He's both very technically strong and honest about what we can do, but we can't do. And as he says, but what does he say? It's nice to hear from you. Uh, it's funny you should bring this up. I was also thinking about writing a, a return to this and in short, basically I don't believe we've made very much progress, right? He points out that in his long list of things that you need to understand the image we have made progress on some the ability to again, the tech people and do face recognition for well known individuals. Okay. Um, but that's kind of about it all right. Um, and he wasn't particularly optimistic that the current route that's being pursued and industry is, is anywhere close to solving or even really trying to solve these larger questions. Um, if we give this image to that a captioned bought, you know, what we see is again, represents the same point.

Speaker 1:          00:29:41       So here's the caption, but it says, I think it's a group of people standing next to a man in a suit and tie, right? So that's right, right. As far as it goes, it's just doesn't go far enough. And the current, the current ideas of build a Dataset, train a deep learning algorithm on it and then repeat, um, aren't really even, I would venture trying to get to what we're talking about or here's another. I'll just give you one other example of a couple of photographs from my recent vacation. I'm in a nice warm, tropical local, um, which I think illustrate ways in which, again, the gap where we have machines that can say beat the world's best at go but can't even beat a child. A tic TAC toe. Now what do I mean by that? Well, you know, of course we can build, we don't even need reinforcement learning or deep learning to build a machine that can win or tie do as do optimally in tic TAC toe.

Speaker 1:          00:30:31       But think about this. This is a real tic TAC toe game, which I saw on the grass outside my hotel, right? Um, what do you have to do to look at this and recognize that it's a tic Tac toe game. You have to see the objects that you have to see what's, you know, in some sense there's a three by three grid, but it's, but it's only abstract, right? It's only limited by this. These ropes are strings. Okay? It's not actually a grid in any simple geometric sense. Alright? But yet a child can look at that and indeed here's an actual child who was looking at it and recognize, oh, it's a game of tic Tac toe and even know what they need to do to win, namely put the x and completed and now they've got three in a row, right? That's literally childsplay.

Speaker 1:          00:31:06       Okay. Um, you show this sort of thing though to one of these, you know, image understanding captioned bots, and I think it's a closeup of a sign. Okay? Um, again, it's not saying that this is a closeup of a sign, is, is not the same thing. I would venture as a, as a cognitive or computational activity that's going to give us what we need to say. Recognize the objects to recognize it as a game to understand the goal and how to plan to achieve those goals. Whereas this kind of architecture is designed to try to do all of these things ultimately. Right? And I bring in these examples of games or jokes to really show where perception goes to cognition, you know, that, uh, at all the way up to symbols, right? So to get objects and forces and mental states, that's the cognitive core, but to be able to get goals and plans and what do I do or how do I talk about it?

Speaker 1:          00:31:57       That's symbols. Okay, here's another way into this. And it's one that also motivates, I think a lot of really good work on the engineering side and a lot of our interest in the science side is think about robotics and think about what do you have to do to, you know, what, what does the brain have to be light to control the body? So again, you're gonna hear from certainly, I think maybe it's next week from Mark Robert, who's a one of the founders of Boston dynamics, which is one of my favorite companies anywhere there, uh, without doubt the leading maker of humanoid robots, legged locomoting robots and industry. They have also all sorts of other really cool robots, robots like dogs, robots that have, you know, you'll, you'll, you'll, I think you'll even get to see live demonstration of one of his robots. This really awesome, impressive stuff.

Speaker 1:          00:32:44       But what about the minds and brains of these robots will, again, if you ask mark, ask them how much of of human light cognition do they have in their robots? And I think he would say very little. In fact, we have asked him that and he would say very little, very little. He's, he's actually one of the advisors of our center. And I think in many ways we're very much on the same page. We both want to know how do you build the kind of intelligence that can control these bodies. I'm like the way a human does. All right. Um, here's another example of an industry robotics effort. This is Google's arm farm where you know, they've, they've got lots of robot arms and they're trying to train them to pick up objects using various kinds of deep learning and reinforcement learning techniques. And I think it's one approach.

Speaker 1:          00:33:23       I just think it's very, very different from the way humans learn to say, control their body and manipulate objects and you can see that in terms of things that go back to what you were saying when you were introducing me, right? Think about how quickly we learn the things right here. You have these arm farm is trying to generate, you know, effectively maybe if not infinite, but hundreds of thousands, millions of examples of reaches and pickups of objects even with just a single gripper and yet a child who in some ways can't control their body nearly as well as robots can be controlled at the low level, is able to do so much more. So I'll show you two of my favorite videos from youtube here, which motivates some of the research that we're doing. The one on the left is a one and a half year old and the other one's a one year old. So just watch this one and a half year old here doing a popular activity for many kids as a playing a video up there.

Speaker 1:          00:34:19       Okay, there we go. Okay. So He's, he's on doing this stacking cup activity. Alright. He's stacking up cups to make a tall tower. He's got a stack of three. And what you can see for the first part of this video is it looks like he's trying to make a second stack at that he's trying to pick up at once. Basically he's trying to make a stack of two that'll go on the stack of three and you know, he's, he's trying to debug his plan because it's, it got a little bit stuck here. Um, but, and, and think about, I mean, again, if you know anything about robots, manipulating objects, even just what he did, no robot can, can decide to do that and actually do it right at some point he's almost got it. It's a little bit tricky, but some point he's going to get that stack of two.

Speaker 1:          00:35:00       He realizes he has to move that opted out of the way, look at what he did, move out of the way, use two hands to pick it up. And now he's got a stack of two on a stack of three and suddenly, you know, sub goal completed. He's now got a stack of five and he, he gives himself a hand because he know, he knows he accomplished a key way point along the way to his final goal. That's a kind of early symbolic cognition, right? To understand that I'm trying to build a tall tower, but the tower is made up of little towers. It's, you know, and, and you can take a tower and put it on top of another tower or a stack, a stack on a stack and you have a bigger stack, right? So think about how he goes from bottom up perception to the objects of the physical needed to manipulate the objects.

Speaker 1:          00:35:35       So the ability to make those early kinds of symbolic plans. At some point he keeps doing this. He puts another stack on there. I'll just jump to the end. Oops, sorry. You missed it. Sorry. Keep. He gets really excited and he gives himself another big hand, but falls over again. Um, Boston dynamics now has robots that could pick themselves up after that. That's really impressive again. Um, but all the other stuff to get to that point, we don't really know how to do in a robotic setting or thinking about this baby here. This is a younger baby. This is one of the Internet's most popular videos because it features a baby and a cap and, but the baby's doing something interesting. He's got the same cups, but he's decided he's again, decided to try a new thing. So this is thinking about creativity. He's decided that his goal is to stack up cups on the back of a cat.

Speaker 1:          00:36:25       I guess he's asking how many cups going to fit on the back of a cat? Well, three. Let's see. Can I fit more? Let's try another one. Okay. Um, while he can't fit more than three, it turns out. And then he then does. It's not working, so he changes his goal. Now his goal appears to be to get the cuffs on the other side of the cat. Now watch that part when he reaches back behind him there. That's. I'll just pause it there for a moment. Um, so many just reached back there. That's a particularly striking moment in the video. It shows a very strong form of what we call in cognitive science. Object permanence. Okay. That's the idea that you represent objects as these permanent enduring entities in the world. Even when you can't see them in this case. He hadn't seen or touched that optic behind him for like at least a minute, right?

Speaker 1:          00:37:07       Maybe much longer. I don't know. And yet he still knew it was there and he was able to incorporate it in this plan. Right. There's a moment before that when he's about to reach for it, but then he sees this other one. Right. And it's only when he's now exhausted all the other objects here that he can see. He's like, okay, now time to get this audit and bring it into play. Right? So think about what has to be going on in his brain for him to be able to do that. Right? That's like the analog of you understanding what's behind you. Okay. It's not that these things are impossible to capture machines far from it. It's just that like training a deep neural network or any kind of pattern recognition system we don't think is going to do it, but we think by reverse engineering how it works in the brain, we might be able to do it because we can do it. Okay. It's not just humans that do this kind of activity. Here's a couple of, again, fit rather famous videos. You can watch all of these on youtube. Crows are famous objects, manipulators and tool users, but also orangutans, other primates, rodents. We can watch if we just had. Let me pause this one for a second. If we watched this orangutan here, he's got a bunch of big legos and over the course of this video he's building up a stack. Legos. It's really quite impressive. Just jumping to the end.

Speaker 1:          00:38:17       There's actually some controversy out there of what this video is a fake. Um, but the controversy isn't about, it's not like whether it was, I dunno, done with computer animation, some people think the video was actually filmed backwards, that a human built up the stack and the orangutan just slowly disassembled it piece by piece and it turns out it's remarkably hard to tell whether it's played forward or backwards in time and people have argued over a little details because you know, it will be quite impressive if an orangutan actually was able to build up this really impressive stack of Legos. But I would submit that it would be almost as impressive if he disassembled it. Think about the activity. I mean, if I want it to disassemble that, the easiest thing to do would just be to knock it over. That's really all most robots could do.

Speaker 1:          00:38:55       But to piece by piece, disassemble it even if it's played backwards like this. That's still a really impressive act of symbolic planning on physical objects. Or here you've got this, this famous mouse, this you can find on the internet under the mouse versus cracker video. And what you'll see here over the course of this video is a mouse valiantly and mostly hopelessly struggling with a cracker that they're hoping to bring back to their nest. I guess it's a very appealing big meal. Um, and at some point after just trying to get it over the, over the wall, at some point the mouse just gives up because it's just never going to happen and he just goes away except that because even mouses can dream or my dream some point he decides, okay, I'm just going to come back for one more. Try and he tries one more time and this time validly gets it over.

Speaker 1:          00:39:42       Yeah. Isn't that very impressive? Congratulations about. Okay. You don't have to clap. You can clap for me at the end or clap for whoever it later. Okay. But I will, I want to applaud the mouse there every time I see that. Okay. But again, think what had to be going on in his brain to be able to do that. All right. Um, it's a crazy thing and yet he formulated the goal and was able to achieve it. I'll just show one more video that is really more about science. These other ones or you know, some of them actually were from scientific experiments, but this is one that motivates a lot of the science that I do and it's to me it sets up kind of a grand cognitive science challenge for ai and robotics. It's from an experiment with humans, again, 18 month old or one and a half year old, so the kids in this experiment, we're the same age as the first baby I showed you.

Speaker 1:          00:40:24       The one who did the stacking and 18 months is really a very, very good age to study. If you're interested in intelligence for reasons we can talk about later if you're interested. This is from a very famous experiment done by two psychologists, Felix Worn Akin and Michael Thomas Cielo, and it was studying the spontaneous helping behavior of young children. It also contrasted humans and chimps and the punchline is that chimps sometimes do things that are kind of like with this human did, but not nearly as reliably or as flexibly. Okay, so not nearly as an. I'll show you a particular kind of unusual situation where human kids had relatively little trouble figuring out what to do or even whether they should do it, whereas basically no chimp did what you're going to see humans sometimes doing here. So the experimenter in this movie. I'll turn on the sound here if you can hear it.

Speaker 1:          00:41:11       The experimenter is the tall guy and the participant is the little kid in the corner. The sound but no words, right? And at some point he stops and then the kid just does whatever they want to do. So watch what he does. He goes over, he opens the cabinet, looks inside, then he steps back and he looks up at Felix and then looks down, okay, and then the action is completed. Now what I want you to watch it one more time and think about what's going to be going inside the kid's head to understand this, to understand like. So it seems like what it looks like to us is the kid figured out that this guy needed help and helped him. And the paper is full of many other situations like this. This is just one. Okay. But the key idea is that the situation is somewhat novel.

Speaker 1:          00:41:55       People have seen people holding books and opening cabinets, but probably it's been, it's very rare to see this kind of situation. Exactly right. It's, it's different in some important details from what you might have seen before. And there's other ones in there that are really truly novel because they just made up a machine right there. Okay. Um, but somehow he has to understand causally from the way the guys banging the books against the thing that it's, it's, it's sort of both a symbol, but it's also somehow he's got to understand what he can do and what he can't do and then what the kid can do to help. And I'll, I'll show this again, but really just watch. The main part I want you to see is, um, I'll just sort of skip ahead. So watch this part here. Let's say I'll just jump right when he watched.

Speaker 1:          00:42:38       Right now he's about to look up. He looks up and makes eye contact and then his eyes look down. So again, he looks up, he looks up, and then a Sakata sudden rapid eye movement down down to his hands up, down. Okay? So that's again, that's this brain, ostp and action, right? He's making one glance, small glance at the big guys, eyes to to make eye contact, to see, to get a signal. Did I understand what you wanted and did you. Did you register that joint attention and then he makes a prediction about what the guy's going to do. So he looks right down. He doesn't just like look around randomly. He looks right down to the guys hands to track the action that he expects to see happening. If I did the right thing to help you, then I expect you're going to put the books there.

Speaker 1:          00:43:22       Okay? So you can see these things happening and we want to know what's going on inside the mind that guides all of that. All right? So that's the sort of big scientific agenda that we're working on over the next few years where we think some kind of human understanding of human intelligence and scientific terms could lead to all sorts of ai payoffs in particular. I suppose we could build a robot that could do what this kid and many other kids in these experiments do just say, help you out around the house without having to be programmed or even really instructed just to kind of get a sense, oh yeah, yeah, you need to have with that shirt. Let me help you out. Okay. Even 18 month olds will do that. Sometimes not very reliably or effectively. Sometimes they'll try to help and really do the opposite. Right? But imagine if you could take the flexible understanding of humans, actions, goals and so on, and make those reliable engineering technology that would be very useful.

Speaker 1:          00:44:12       And it would also be related to say machines that you could actually start to talk to and trust in some ways. Right? That shared understanding. So how are we going to do this? Well, let me spend the rest of the time talking about how we tried to do this, right, some of the, some of the technology that we're building both in our group and more broadly to try to make these kinds of architectures real. And I'll talk about two or three technical ideas, again, not in any detail. Alright. Um, one is the idea of a probabilistic program. So this is a kind of, um, think of it as a computational abstraction that we can use to capture the common sense knowledge of this core cognition. So when I say we have an intuitive understanding of physical objects and people's goals, how do I build a model of that model you have in the head probabilistic programs a little bit more technically are one way to understand them is as a generalization of Bayesian networks or other kinds of directed graphical models.

Speaker 1:          00:45:05       If you know those. Okay. But we're, instead of defining a probability model on a graph, you defined it on a program and thereby have access to a much more expressive toolkit of knowledge representation. So data structures, other kinds of algorithmic tools for representing knowledge. Okay. But you still have access to the ability to do probabilistic inference, like in a graphical model, but also causal inference in a directed graphical model. So for those of you who know about graphical models, that might make some sense to you, but just more broadly what this is, think of this as, as a toolkit that allows us to combine several of the best ideas, not just of the recent deep learning era, but over if you look back over the whole scope of ai and as well as cognitive science. I think there's three or four ideas and more but definitely like three ideas we could really put up there that have proven their worth and have had that have risen and fallen in terms of each of these had ideas when the mainstream of the field thought this was totally the way to go and every other idea was was obviously a waste of time and also had its time when many people thought it was a waste of time.

Speaker 1:          00:46:08       Okay. And these three big ideas, I would say our. First of all, the idea of symbolic representation or symbolic languages for knowledge representation, probabilistic inference in generative models to capture uncertainty, ambiguity, learning from sparse data and in their hierarchical setting, learning to learn, right? And then of course the recent developments with neuro inspired architectures for pattern recognition. Okay? Each of these things, each of these ideas, symbolic languages, probabilistic inference and neural networks have some distinctive strengths that are real weak points of the other approaches, right? So to take one example that I haven't really talked about here, um, people in the butt but you, but you mentioned as an outstanding challenge for neural networks, transfer learning or learning to take knowledge across a number of previous tasks to transfer to others. This is a real challenge and has always been a challenge in a neural net.

Speaker 1:          00:46:57       Okay. But if something that's addressed very naturally and very scalably in, for example, a hierarchical basie and model, and if you look at some of the recent attempts, really interesting attempts within the deep learning world to try to get kinds of transfer learning and learning to learn, they're really cool. Okay. But many of them are in some ways kind of reinventing within a neural network paradigm ideas that people, you know, maybe just 10 or 15 years ago, developed in very sophisticated ways in let's say hierarchical Bayesian models. Okay. And a lot of attempts to get sort of symbolic an algorithm like behavior in neural networks are really, they're very small steps towards something which is a very mature technology in computer systems and programming languages, probabilistic programs, I'll just sort of advertise mostly are a way to combine the strengths of all of these approaches to have knowledge representations which are as expressive as anything that anybody ever did in the symbolic paradigm that are as flexible at dealing with uncertainty and sparse data as anything in the probabilistic paradigm.

Speaker 1:          00:47:55       But that also can support pattern recognition tools to be able to, for example, do very fast, efficient inference in very complex scenarios. And there's a number of, probably that's the, that's the kind of conceptual framework. There's a number of actually implemented tools. Um, I point to here on the slide a number of public programming languages which you can go explore. Um, for example, there's one that was developed in our group a few years ago, almost 10 years ago now called church, which was the antecedent of some of these other languages built on a functional programming course. A church is a problematic programming language built on the lambda calculous or really enlist basically, um, but there are many other more modern tools, especially if you are interested in neural networks. There are tools like, for example, pyro or prob torch or base flow that try to combine all these ideas in a or, or for example, jen here, which is a project of the Kauffman singles problems, the computing group.

Speaker 1:          00:48:46       Um, these are all things which are just in the very beginning stages, very, very alpha. You can find out more about them online or by writing to their creators. And I think this is a, this is a very exciting place where the convergence of a number of different ai tools are happening and when, and this will be absolutely necessary for making the kind of architecture that I'm talking about work. Another key idea which we've been building on in our lab. Um, and I think again, many people are using some version of this idea, but maybe a little bit different from the way we're doing it is what will the version of this idea that I like to talk about is what I call the game engine in the head. So this is the idea that it's really what the programs are about. When I talk about problems with programs, I haven't said anything about what kind of programs we're using.

Speaker 1:          00:49:32       We're just basically these programming languages at their best and church. The language that was developed by Noah Goodman and mccosh and others and Dan Roy in our group some 10 years ago was intended to be a turing complete probabilistic programming language. So any probability model that was computable or for who's inferences conditional inferences are computable. You could represent in these languages, but that leaves completely open. What, what I'm actually going to, what kind of protocol I'm going to write to model the world. And I've been very inspired in the last few years by thinking about the kinds of programs that are in modern video game engines. So again, I'm probably, most of you are familiar with these, but if you're an increasingly they are playing a role in all sorts of ways in ai, but these are tools that were developed by the video game industry to allow a game designer to make a new game with, without having to do most of, in some sense many it must have the hard technical work by from scratch, but rather to focus on the characters, the world, the story, okay.

Speaker 1:          00:50:28       The things that are more interesting for, for designing a novel game in particular, we, if we want the player to explore some new three dimensional world, but to have them be able to interact with the world in real time and to render nice looking graphics in, in real time, in an interactive way as the player moves around and explores the world or if you want to populate the world with non player characters that will behave in a even vaguely intelligent way. Okay. Game engines give you tools for doing all of this without having to write all of graphics from scratch or all of physics. The rules of physics from scratch. Um, so what are called game physics engines and in some sense are a set of principles but also hacks from newtonian mechanics and other areas of physics that allow you to simulate plausible looking physical interactions in very complex world, very approximately, but very fast.

Speaker 1:          00:51:16       There's also what's called [inaudible], which are basically very simple planning models. So let's say I want to have an ai in the game that is like a guard that gardens have base and a player is going to attack the space. So back in the old Atari days, like when I was a kid, you know, the guards would just be like random things that would fire missiles kind of randomly in random directions at random times, right? But let's say you want a garden to be a little intelligent, so to actually look around and Oh, and I see the player and then to actually start shooting at you and to even maybe pursue you. So that requires putting a little ai in the game. And you do that by having basically simple agent models in the game. So what we think, and some of you might think this is crazy and some of you might think this is very natural idea, I get both kinds of reactions.

Speaker 1:          00:51:56       What we think is that these tools have, you know, fast, approximate renderers physics engines and sort of very simple kinds of ai planning are an interesting first approximation to the kinds of common sense knowledge representations that evolution has built into our brains. So when we talk about the cognitive core or how do babies start, what's, you know, ways in which a baby's brain isn't a blank slate. One interesting idea is that it starts with something like these tools and then wrapped inside a framework for probabilistic inference. That's what we mean by promising programs that can support many activities of common sense perception and thinking. So I'll just give you one example, what we call this intuitive physics engine. Okay. So this is work that we did in our groups that Pete, Natalia, and Jess Hamrick did started this work about five years ago now where we showed people in some sense this is also an illustration of a kind of experiment that you might do.

Speaker 1:          00:52:52       I keep talking about science, like I'll show you now a couple of experiments, right? So we would show people simple physical scenes like these blocks worlds scenes, and asked them to make a number of judgments. And the model we built does it basically a little bit of probabilistic inference in a game style physics engine. It perceives the physical state and imagines a few different possible ways the world could go over the next one or two seconds to answer questions like will the stack of blocks fall? Or if they fall, how far will they fall? Or which way will they fall? Or what would happen if say one of the colored one color of blocks or one material like the green stuff is 10 times heavier than the gray stuff or vice versa. How will that change the direction of fall or look at those red and yellow stack blocks, some of which look like they should be falling but aren't.

Speaker 1:          00:53:35       So why can you infer from the fact that they are not fall in that one color block is much heavier than the other. Or let me show you a sort of a slightly weird task. It's an like other behavioral experiments. Sometimes we do weird things so that we can test ways in which you use your knowledge that you didn't just, you know, learn from pattern recognition, but use it to do new kinds of tasks that you'd never seen before. So here's a task which many of you have maybe seen me talk about these things. So you might have seen this task, but probably only if you saw me give a talk around here before we call this the red, yellow task. And again we'll make this one interactive. So imagine that the blocks on the table or knocked hard enough to bump the tables, bumped hard enough to knock some of the blocks onto the floor. So you tell me, is it more likely to be red blocks or yellow blocks? What do you say? Red. Okay, good. How about here?

Speaker 3:          00:54:25       Yeah,

Speaker 1:          00:54:27       yellow. Good. How about here? Uh Huh.

Speaker 3:          00:54:30       Here, here. Okay. Here. Here.

Speaker 1:          00:54:42       Okay. So, so you just experienced for yourself what it's like to be a subject and one of these expenses. We just did the experiment here. The data's is all captured on video, sort of. Okay. You could see that sometimes people were very quick. Other times people were slower, sometimes there was a lot of consensus, sometimes there was a little bit less consensus. Right? That reflects uncertainty. So again, there's a long history of studying this scientifically, um, that, you know, you could, but you can see some, you can see the probabilistic inference at work. Probabilistic inference over what? Well, I would say one way to describe it is over a one or a few short, low precision simulations of the physics of these scenes. So here is what I mean by this. I'm going to show you a video of a game engine reconstruction of one of these scenes that simulates a small.

Speaker 1:          00:55:26       So here's a small bump, here's that same scene with a big bump. Okay, now notice that at the micro level, different things happen, but at the cognitive or macro level that matters for common sense reasoning, the same thing happen, namely all the yellow blocks went over onto one side of the table and few or none of the red blocks did so it didn't matter which of those simulations you run in your head, you'd get the same answer in this case, right? This is one that's very easy and high confidence. And also you, you didn't have to run the simulation for very long. You only have to run it for a few times. Steps like that to see what's going to happen. Or similarly here, you only have to run it for a few times. Steps, okay? And it doesn't have to be even very accurate.

Speaker 1:          00:56:01       Even a fair amount of imprecision will give you basically the same answer at the level that matters for common sense. So that's the kind of thing our model does. It runs a few low precision simulations for a few times steps. But if you take the average of what happens there and you compare that with people's judgments, you get results like what I show you here, the scatter plot shows on the y axis, the average judgments of people on the x axis, the average of this model, and it does a pretty good job. It's not perfect, but the model basically captures peoples graded sense of what's going on in this scene and many of these others, okay. And it doesn't do it with any learning, but I'll come back to that in a second. It just does it by probabilistic reasoning over a game physics simulation. Now we can use and we have used the same kind of technology to capture in very simple form.

Speaker 1:          00:56:45       It's really just proofs of concept at this point, but kind of common sense physical scene understanding in child and a child playing with blocks or other objects or in what might go on in a young child, understanding of other people's actions, what we call the intuitive psychology engine. We're now, the programs are defined over these kind of very simple planning and perception programs and I won't go into any details. I'll just point to a couple of papers that my group played a very small role in, but we provided some models which together with some infant researchers, people working on both of these are experiments that were done with 10 or 12 month infant, so younger than even some of the babies I showed you before, but basically like that youngest baby, the one with the cat. Here's an example of showing simple physical scenes. These are moving objects to 12 month olds where they saw a few objects bouncing around inside a gumball machine and after some point in time the scene gets occluded.

Speaker 1:          00:57:36       You'll see the scene as occluded and then after another period of time, but one of the objects will appear at the bottom and the question is, is that the audit you expected to see or not is it's expected or surprising? The standard way you study what infants know is by is by what's called looking time methods. Just like an adult. If I show you something that's surprising, you might look longer. Okay. If you're bored, you'll look away. All right? Um, so you can do that same kind of thing with infants and by measuring how long they look at a scene, you can measure whether you've shown them something surprising or not. All right? People. There are literally hundreds of studies, if not more, using looking time measures to study what infants know, but only with this paper that we published a few years ago did we have a quantitative model, but we're able to show a relation between inverse probability in this case and surprise, so things which were objectively lower probability under one of these probabilistic physics simulations across a number of different manipulations of how fast the objects were, where they were when the scene was occluded, how long the delay was, various physically relevant variables, how many objects that were of one type or another infant's expectations connected with this model or another paper that we published that one was, was done.

Speaker 1:          00:58:43       The experiments that were done by Aaronow tagless and Luca Menotti's lab. Here is a study that was done just recently by Sheri Lou enlists, spelled [inaudible] lab at there at Harvard, but they're part, they're partners with us in CBMM, which was about infants understanding of goals, so this is more like again, understanding of agents and intuitive psychology. We're in, again, in very simple cartoon scenes, you show an infant and agent that seems to be doing something like an animated cartoon character, but it jumps over a wall or it rolls up a hill or it jumps over a gap and the question is basically how much does the agent want the goal that it seems to be trying to achieve and what this study showed. Okay, and the models here we're done by Tomer omen was that infants appeared to be sensitive to the physical work done by the agent.

Speaker 1:          00:59:26       The more work the agent did in the sense of the integral of force applied over a path, the more the the infant's thought the agent wanted the goal. We think of this as representing what we've sometimes called the naive utility calculus. So the idea that there's a basic calculus of, of costs and benefits, as you know, we take actions which are a little bit costly to achieve goal states, which gives us some reward that's the most basic way, the oldest way to think about rational, intentional action and it seems that even 10 month olds understand some version of that where the cost can be measured in physical terms. Okay. Um, I see I'm running a little bit behind on time and, and, uh, I wanted to leave some time for discussion so I'll, I'll just go very quickly through a couple of other things and I'm happy to stay around at the end for discussion.

Speaker 1:          01:00:11       Okay. Um, the, what I showed you here was the science. Where does the engineering go? So one way, one thing you can do with this is say build a machine system that can look not a little animated cartoon like these baby experiments, but a real person doing something and again, combined physical cough and constraints of actions with some understanding of, of the agents, utilities, that's the math of planning. Trying to figure out what they wanted. So look in the scene here and see if you can judge which object the woman is reaching for. So you can see there's, there's, um, a grid of four by four objects. There's 16 objects here and she's going to be reaching for one of them raised. It's going to play in slow motion, but raise your hand when you know which ones he's reaching for. Okay? So just watch and raise your hand when you know which one she wants.

Speaker 1:          01:01:00       Okay, so most of the hands are up by now. All right. And notice I was looking at your hands. Not here, but when. But what happened is most of the hands were up at about the time when that great or the one that dash line shot up. Okay. That's not human data. You provided the data. This is our model. So our model is predicting more or less when you're able to say what her goal was. Okay. It's well before she actually touched the object. How does the model work? Again, I'll skip the details, but it does the same kind of thing that, that are models of those infants did namely get it. But in this case, it does it with a full body model from robotics, so we use what's called the physics engine, which is a standard tool in robotics for planning, physically efficient reaches of say, a humanoid robot, and we say we can give this planner program a goal object as input.

Speaker 1:          01:01:45       We can give each of the possible goal. Optics is input and say, plan the most physically efficient action, so the one that uses like the least energy to get to that object, and then we can do a Bayesian inference. This is the probabilistic inference part. The program is the planner. Okay, but then we can say, I want to do basie and inference to work backwards from what I observed, which was the action to the input to that program. What goal was provided as input to the planner, and here you can see the full array of four by four possible inputs and those bars that are moving up and down. That's the basie and posterior probability of how likely each of those was to be the goal and what you can see as it converges on the right answer at least. Well, it turns out to be the ground truth right answer, but it's also the right answer according to what people think with about the same kind of data that people took.

Speaker 1:          01:02:30       Now you might say, well, okay, I'm sure if I just wanted to build a system that could detect what somebody was reaching for, I could generate a training data set of this sort of scene and train something up to analyze patterns of motion, but again, because the engine in your head actually does something, we think more like this, it does what we call inverse planning over a physics model. It can apply to much more interesting scenes that you haven't really seen much of before. So take the seat on the left, right where again, you see somebody reaching for one of a four by four array of objects, but what you see as a strange kind of reach, can you see why he's doing this? Strange reach up there? It's a little small, but what is he? You can see that he's reaching over something, right?

Speaker 1:          01:03:07       It's actually a pane of glass, right? You see that and then there's this other guy who's helping him, who sees what he wants and hands him the thing he wants. So how does the, for the guy in the foreground see the other guy's goal, how does he in for his goal and know how to help him and then how do we look at the two of them and figure out who's trying to help who or that in a scene like this one here that it's not somebody trying to help somebody but rather the opposite. Okay, so here's a model on the left of how that might work. Right? And we think this is the kind of model needed to tackle this sort of challenge here, right? Basically it's a model. We take this model of, of planning, sort of maximal expected utility planning, which you can run backwards, but then we recursively nest these models inside each other.

Speaker 1:          01:03:48       So we say an agent is helping another agent if this agent is acting apparently to us, seems to be maximizing unexpected utility. That's a positive function of that agent's expectation about another agent's expecting to tilly and that's what it means to be a helper. Hindering is sort of the opposite if one seems to be trying to lower somebody's else's utility. Okay, and we've used these same kind of models to also describe infants understanding of helping and hindering and a range of scenes. I'll just say one last word about learning because everybody wants to know about learning and and the the key thing here, and it's. It's definitely part of any picture of Agi, but the thought I want to leave you on is really about what learning is about. Okay. It will be just a few more slides and then I'll stop. I promise none of the models I showed you so far really did any learning.

Speaker 1:          01:04:34       They certainly didn't do any task specific learning. Okay. We set up a provost at program and then we let it do inference. Now that's not to say that we don't think people learn to do these things. We do, but the real learning goes on when you're much younger, right? Everything I showed you in basic form, even a one year old baby can do. Okay. The basic learning goes on to support these kinds of abilities. Not that there isn't learning beyond one year, but the basic way you learn to say solve these physics problems is what goes on in your bay, in the brain of a child between zero and 12 months. So this is just an example of some phenomena that come from the literature on infant cognitive development. These are very rough timelines. You can take pictures of this if you like. This is always a popular slide because it's.

Speaker 1:          01:05:13       It's. It really is quite inspiring I think, and I can give you lots of literature, but I'm summarizing in very broad strokes with big error bars, what we've learned in the field of infant cognitive development about when and how kids seemed to at least come to a certain understanding of basic aspects of physics. So if you really want to study how people learn to be intelligent, a lot of what you have to study our kids at this age, you have to study what's already in their brain at zero months and what they learn and how they learn between four, six, eight, 10, 12, and so on and on and on up beyond that. Okay. Now, well, effectively what that amounts to we think is if what you're learning is something like, let's say an intuitive game, physics engine to capture these basic abilities than what we need.

Speaker 1:          01:05:57       If we're going to try to reverse engineer, that is what we might think of as a program learning program. If your knowledge is in the form of a program, then you have to have programs that build other programs. Right? This is what I was talking about the beginning about learning as building models of the world are ultimately. If you think what we start off with is something like a game engine that can play any game than what you have to learn is the program of the game that you're actually playing or the many different games that you might be playing over your life. So think of learning as like programming the game engine in your head to fit with your experience and to fit with the possible actions that you seem like you can take. Now, this is what you could call the hard problem of learning if you come to learning from, say, neural networks or other tools in machine learning.

Speaker 1:          01:06:36       Right? So what makes machine makes most of machine learning go right now and certainly what makes neural networks so appealing is that you can set up a basically a big function approximator that can approximate many of the functions you might want to do in a certain application or task, but in a way that's end to end differentiable and with a meaningful cost function. So you can have one of these nice optimization landscapes. You can compute the gradients on basically just rolled down hill until you get to an optimal solution, but if you're talking about learning as something like search in the space of programs, we don't know how to do anything like that yet. We don't know how to set this up as any kind of a nice optimization problem with any notion of smoothness or gradients. Okay. Rather, what we need is a, instead of learning as like rolling down hill, effectively write a process which just, if you're willing to wait long enough, you know, some, uh, you know, simple algorithm will take care of, think of what we call the idea of learning as programming.

Speaker 1:          01:07:28       There's a popular metaphor in cognitive development called the child as scientists, which emphasizes children as active theory builders and children's play as a kind of casual experimentation. But this is the algorithmic compliment to that. What we could call the child is coder or around Mit will say the child is hacker, but the rest of the world, if you say child is hacker, they think of something that someone who breaks into your email and steals your credit card numbers. We all know that hacking is, you know, making your code more awesome, right? If, if, if your knowledge is some kind of code or Lena Library of programs, then learning is all the ways that a child acts on their code to make it more awesome, more awesome can mean more accurate, but it can also mean faster, more elegant, more transportable to other applications or their tasks more explainable to others.

Speaker 1:          01:08:13       Maybe just more entertaining. Okay. Children do. All of them have all of those goals and learning and the activities by which they make their code more awesome. Also correspond to many of the activities of coding, right? So think about all the ways on a day to day basis. You might make your code more awesome. Alright? Um, you might tune, you might have a big library of existing functions with some parameters that you can tune on a data set that's basically what you do with backdrop or stochastic gradient descent and training a deep learning system, but think about all the ways in which you might actually modify the underlying function. So write new code or take old code from some other thing. And Map it over here or make a whole new library of code or refactor your code to some other, some other basis for that that will work more robustly and be more extensible or transpiling or compiling.

Speaker 1:          01:08:58       Right. Or even just commenting your code or asking someone else for their code. Okay. Again, these are all ways that we make our code more awesome and children's learning has analogs to all of these that we would want to understand as an engineer from an algorithmic point of view. So in our group we've been working on on various early steps towards this. And again, we don't have anything like, um, program writing programs at the level of children's learning algorithms. But one example of something that we did in our group, which you might not have thought of being about this, but it's definitely the ai work we did that got the most attention. And the last couple of years from our group, we had this paper that was in science. It was actually on the cover of science, sort of just hit the market at the right time if you like.

Speaker 1:          01:09:38       And it got about 100 times more publicity than anything else I've ever done, which is partly a testament to the really great work that Brendan Lake, who was the first author did for his phd here, but much more so it just about the hunger for ai systems at the time when we published this in 2015 and we built a machine system that the way we described it was doing human level concept learning for simple concept, very simple visual concepts. These handwritten characters in many of the world's alphabets. For those of you who know the famous m, this Dataset, the Dataset of handwritten digits zero through 10 or 33 nine, sorry. Uh, that drove so much good research in deep learning and pattern recognition. It did that not because Yann Macun who put that together or Jeff Hinton, who did a lot of work on deep learning with Ms. They weren't interested fundamentally in character recognition that they saw that as a very simple test bed for developing more general ideas.

Speaker 1:          01:10:26       And similarly, we did this work on getting machines to do what we are kind of one shot learning of generative models also to to develop more general ideas. We saw this as learning very simple little mini probabilistic programs in this case. What are those programs that the programs you used to draw a character. So ask yourself how can you look at any one of these characters and see in a sense how somebody might draw it. The way we tested this in our system was this little visual turing test where we showed people one character in a novel alphabet and we said draw another one and then we compared nine people like say on the left and nine samples from our machine say on the right. And we said, we asked other people, could you tell which was the human drawing another example or imagining another example in which was the machine and people couldn't tell when I said ones on the left, ones on the right.

Speaker 1:          01:11:10       I don't actually remember. And and different ones you could see if you can tell. It's very hard to tell. Can you tell which is for each one of these characters, which new set of examples were drawn by a human versus the machine? Here's the right answer and probably you couldn't tell. The way we did this was by assembling a simple kind of program learning program, right? So we basically said when you draw a character, you're assembling strokes and sub strokes with goals and sub goals that produce ink on the page, and when you see a character, you're working backwards to figure out what was the program, the most efficient program that did that. So you're basically inverting a probabilistic program, doing basie and inference to the program most likely to have generated what you saw. This is one small step we think towards being able to learn programs to being able to learn something ultimately like a whole game engine program.

Speaker 1:          01:11:56       The last thing I'll leave you with is just a pointer to sort of work in action, right? So this is some work being done by a current phd student who works partly with me, but also with Armando solar, Lezama and Cecil sale. This is Kevin Ellis is an example of what's now. I think again, an emerging exciting area in Ai, well beyond anything that we're doing is combining techniques from where Amando comes from, which is the world of programming languages, not machine learning or ai, but tools from programming languages which can be used to automatically synthesize code. Okay. With the machine learning tool kit, in this case, a kind of basie and minimum minimum description link idea to be able to make again, what is really one small step towards machines that can learn programs by basically trying to efficiently find the shortest, simplest program which can capture some data set.

Speaker 1:          01:12:42       So we think by combining these kinds of tools, in this case, let's say from basie and infants over programs with a number of tools that have been developed in other areas of computer science that don't look anything or haven't been considered to be machine learning or ai like programming languages. It's one of the many ways that going forward we're going to be able to build smarter, more human like machines. So just to end then, what I've tried to tell you here is taught first of all, identify the ways in which human intelligence goes beyond pattern recognition to really all these activities of modeling the world. Okay. To give you a sense of some of the domains where you can start to study this in common sense, seen understanding for example, um, or you know, something like a one shot learning for example, like what we were just doing.

Speaker 1:          01:13:24       Their learning is programming the engine in your head. Okay. And to give you a sense of some of the technical tools, probabilistic programs, program synthesis, game engines for example, as well as a little bit of deep learning that bringing together, we're starting to be able to make these things real. Okay. Now that's the science agenda and the reverse engineering agenda. But think about for those of you who are interested in technology, what are the many big ai frontiers that this opens up? So the one I'm most excited about is this idea which is, which I've highlighted here in our big research agenda. This is the one I'm most excited about to work on for the, you know, it could be the rest of my career honestly, but it's really what is, what is the oldest and maybe the best dream of AI researchers of how to build a human like intelligence system, a real agi system.

Speaker 1:          01:14:10       It's the idea that terrain proposed when he proposed the turing test or Marvin Minsky proposed this at different times in his life or many people have proposed this, right? Which is to build a system that grows into intelligence the way a human does that starts like a baby and learns like a child, and I've tried to show you how we're starting to be able to understand those things. What a baby's mind starts with how children actually learn and looking forward, we might. We might imagine that someday we'll be able to build machines that can do this. I think we can actually start working on this right now and we're. And that's something that we're doing in our group. So if that kind of thing excites you, then I encourage you to work on it and maybe even with us or if any one of these other activities of human intelligence excite you, I think taking the kind of science based reverse engineering approach that we're doing and then trying to put that into engineering practice, it's, this is, this is, uh, this is not just a possible route, but I think it's, it's quite possibly the most valuable route that you could work on right now to try to actually achieve at least some kind of artificial general intelligence, especially the kind of intelligence ai system that's going to live in a human world and interact with humans.

Speaker 1:          01:15:13       There's many kinds of ai systems that could live in worlds of data that none of us can understand or whatever live in ourselves. But if you want to build machines that can live in our world and interact with us the way we are used to interacting with other people, then I think this is a route that you should consider. Okay. Thank you.

Speaker 2:          01:15:30       Thank you.

Speaker 4:          01:15:39       So earlier in the talk you expressed some skepticism about whether or not industry would get us to understanding human level intelligence. It seems that there's a couple of trends that favorite industry. One is the industry is better than academia accumulating resources and plowing back into the topic and it seems at the moment we've got a bit of brain drain going on from academia into industry and that seems like an ongoing trend. If you look at something like learning to fly, learning to fly into space, then it looks like the story is one of industry kind of taking over the field and going off on its own a little bit. Academic academics still have a role. The industry kind of dominate industry and I would take the field. Do you think?

Speaker 1:          01:16:20       Well that's a really good question and it's got several good questions packed into one there. Right? I didn't mean to say this wasn't meant to say go academia, bad industry. What I was taught, what I tried to say was the approaches that are currently getting the most attention in industry and they're really because they are really the most valuable ones right now for the short term, you know, any industry is really focused on what it can do. What are the value propositions on basically a two year timescale at most. I mean if you ask, say Google researchers to take the most prominent example, that's pretty much what they'll all tell you. Okay. Maybe maybe things that might pay off initially in two years, but maybe take five years or more to really develop, but if, if you can't show that it's going to do something practical for us in two years in a way that matters for our bottom line, then it's not really worth doing.

Speaker 1:          01:17:05       Okay, so what, when we see what I'm talking about is technologies, which right now industry sees as meeting that specification. And what I'm saying is right now, I think those. That's not where the route is to something like human, like the not the most valuable promising route to humanlike kinds of AI systems. All right, but I hope that like in the case, as you said, you know, the basic research that we're doing now will be successful enough that it will get the attention of industry when the time is right, but I think so, so I hope at some point it won't. It will at least the engineering side will have to be done in industry, not just in academia, but you're also pointing to issues of like brain drain and other things like that. That I think these are real issues confronting our community. I think everybody knows this and I'm sure this will come up multiple times here, which is, you know, I think we have to find ways to even now to combine the best of the ideas, the energy and the resources of academia and industry.

Speaker 1:          01:18:02       If we want to keep doing basically something interesting, right? If we, if, if we just want to redefine ai to be, well, whatever people currently call ai but scaled up well then then then fine, forget about it and or if we just want to say, let me and people like me do what we're doing. At what industry would consider a snail's pace on toy problems. Okay fine, but if but if we want to, if you know, if I want to take what I'm doing to the level that that will really be paying off that level the industry can appreciate or just that really has technological impact on a broad scale. Right? Or I think if industry wants to take what it's doing and really build machines that are actually intelligent, right? Or machine learning that actually learns like a person, then I think we need each other now and not just in some point in the future.

Speaker 1:          01:18:46       So this is a general challenge for mit and for, for everywhere. And for Google. I mean we just spent a few days talking to Google about exactly this issue of, in fact this was a talk I prepared partly for that purpose. So we wanted to raise those issues and and it's just. I mean really there, I don't know what I mean. What rather I can think of some solutions to that problem of what you could call brain drain from the academic point of view or what you could call just narrowing in into certain local minima and the industry point of view, but they will require the leadership of both academic institutions like mit and companies like Google being creative about how they might work together in ways that are a little bit outside of their comfort zone. I hope that will start to happen, um, including at mit and at many other universities and companies like Google and many others. And I think we need it to happen for the health of all parties concerned. Okay. Thank you very much. Thanks.

Speaker 5:          01:19:36       Uh, I'm curious about sort of the premise that you gave that a. One of the big gaps missing at determining intelligence is the fact that we need to teach machines how to recognize models. And I'm curious as to what you think sort of non goal oriented cognitive activity comes into play there. Things like feelings and emotions and, and why you don't think that might not necessarily be like the most

Speaker 1:          01:20:08       important question. The only reason emotions didn't appear on my slide is because there's a few reasons, but the slide is only so big. I wanted the font to be readable for such an important slide. I had versions of my slide in which I do talk about that. Okay. Um, I, it's not that I think feelings or emotions aren't important. I think they are important and I, and I used to not have many insights on it about what to do about them, but actually partly based on some of my colleagues here at Mit, bcs, Laura Schultz and Rebecca Saxe to, of my cognitive colleagues who I work closely with, um, they've been starting to, to, to do research on how people understand emotions, both their own and others and we've been starting to work with them on computational models. So that's actually something I'm actively interested in and even working on.

Speaker 1:          01:20:52       But I would say, and again, for those of you who study emotion or know about this, actually you're going to have lisa coming in, right? Oh. So she's going to basically say a version of the same thing. I think the deepest way to understand she's one of the world's experts on this. The deepest way to understand emotion is very much based on our mental models of ourselves, of the situation we're in and of other people, right? Think about, for example, all of the different. I mean if, if, if, if, if you think about them, I mean, again, Lisa will talk all about this, but if you think about emotion, it's just a very small set of what are sometimes called basic emotions, like being happy or angry or sad or you know, those are small number of them, right? There's usually only a few, right? You might not say, you might see that as somehow like very basic things that are opposed to some kind of cognitive activity.

Speaker 1:          01:21:39       But think about all the different words we have for emotion, right? Um, for example, think about an, a, a famous cognitive emotion like regret. What does it mean to feel regret or frustration? Right? To just to, to know both for yourself when you're not just feeling kind of down or negative, but you're feeling regret that that means something like, I have to feel like there's a situation that came out differently from how I hoped and I realized I could have done something differently. Right? So that means you have to be able to understand, you have to have a model, you have to be able to do a kind of counterfactual reasoning and to think, oh, if only I had acted a different way than I can predict that the world would have come out differently and that's the situation I wanted, but instead it came up this other way, right?

Speaker 1:          01:22:20       Um, or think about frustration again, that requires something like understanding, okay, I've tried a bunch of times, I thought this would work, but it doesn't seem to be working. Maybe I'm ready to give up though. Those are all. Those are. Those are very important human emotions. We have to understand to understand ourselves. We need that to understand other people to understand communication, but those are all filtered through the kinds of models of action that I was. Just the ones I was talking about here with these say cost benefit analysis of action. So what I'm. So I'm just trying to say I think this is very basic stuff that will be the basis for building, I think better engineering style models of the full spectrum of human emotion beyond just like, well, I'm feeling good or bad or scared. Okay. And if I think when you see Lisa, she will in her own way, say something very similar.

Speaker 1:          01:23:05       Interesting. Thanks. Yeah, thanks Josh for your stock. So all these are both human cognition and try to build a model to mimic those cognition. What you don't. How much could help you to understand how the circuit implement those things. I mean like the circuits in the brain. What's the. Is that what you work on by Newcastle? Sorry, what? Is that? What you work on by any chance? Yeah, yeah. Yeah. So, so in the center for brains, minds and machines as well as in brain and cognitive science. Yet we have a number of colleagues who study the actual hardware basis of this stuff in the brain and that includes like the large scale architecture of the brain, say like what Nancy Campbell shirt, Rebecca Saxe study with functional brain imaging or the more detailed circuitry, which usually requires recording from say non brands right at the level of individual neurons and connections between neurons.

Speaker 1:          01:23:51       Alright. So I'm very interested in those things, although it's not mostly what I work on. Right. But I would say, you know, again like in many other areas of science, certainly in neuroscience, the kind of work I'm talking about here in a sort of classic reductionist program sets the target for what we might look for. Like if I, if I just want to go, I would, I would, I would assert right. Or my working conjecture is that if, if you do the kind of work that I'm talking about here, it gives you the right targets or gives you a candidates that have targets to look for what are the neural circuits computing. Right? Whereas if you just go in and just say, start poking around in the brain or have some idea that what you're going to try to do is find the neural circuits which underlie behavior without a sense of the computations needed to produce those behaviors.

Speaker 1:          01:24:39       I don't, I think it's going to be very difficult to, to know what to look for and to know when you found even Bible answers. So I think that's, you know, that's the standard kind of reductionist program, but it's not, it's, it's not. Um, I also think it's, it's not one that is divorced from the study of neurocircuits. It's also one, if you look at the broad picture of reverse engineering, it's one where you were neurocircuits and understanding the circuits in the brain play an absolutely critical role. Okay. I would say the main as an. When you look at the brain at the hardware level, as an engineer, I'm mostly looking at the software level, right? But when you look at the hardware level, there are some remarkable properties. One remarkable property, again, is how much parallelism there is and in many ways how fast the computations are.

Speaker 1:          01:25:24       Okay, neurons are slow, but the computation of the tubs are very fast. So how do we get elements that are in some sense quite slow in their time constant to produce such intelligent behavior so quickly? That's a great mystery. And I think if we understood that it would have payoff for building all sorts of, uh, you know, apple basically application embedded circuits. Okay. But also maybe most important is the power consumption. And again, many people have, have, have noted this, right? If you look at the power consumption, the power that the brain consumes, like what did I eat today? Okay? Almost nothing. My daughter, who's again, she's doing an internship here, she literally yesterday, all she ate was a burrito and yet she wrote 300 lines of code for her internship project on really cool computational linguistics project. So somehow she turned a Burrito into a model of child language acquisition.

Speaker 1:          01:26:12       Okay, but how did she do that? Or how do any of us do this? Right? Um, where if you look at the power that we consume when we simulate even a very, very small chunk of cortex on our conventional hardware, or we do any kind of machine learning thing, we have systems which are very, very, very, very far from the power of the human brain computationally, but in terms of physical energy consumed way, way past what any individual brain is doing. So how do we get circuitry of any sort, biological, or just any physical circuits to be as smart as we are with, with as little energy as we are. This is, this is a huge problem for basically every area of engineering, right? If you want to, if you want to have any kind of robot, the power consumption is a key bottleneck. Same for self driving cars.

Speaker 1:          01:26:56       If we want to build ai without contributing to global warming and climate change, let alone use ai to solve climate change, we really need to address these issues and the brain is a, is a huge a guide there, right? I think there are some people who are really starting to think about this. How can we say, for example, build somehow brain inspired computers, which are very, very low power but maybe only approximate. So I'm thinking here of Joe Bates, I don't know if I don't have, you know, Joe, he's, he's, uh, been around mit and other places for quite awhile. Can I tell them about your company? So, so joe has a low, a startup in Kendall Square called singular computing and they have some very interesting ideas, including some actual implemented technology for low power, approximate computing in a sort of a brain like way that might lead to possibly even like the ability to build something.

Speaker 1:          01:27:42       This is Joe's dream to build on this, about the size of this table, but that has a billion course, a billion cores and runs on a reasonable kind of power consumption. I would love to have such a machine if anybody wants to help joe build that. I think he'd love to talk to you, but it's one of a number of, of, of ideas. I mean google x, people are working on similar things. Probably most of the major chip companies are also inspired by this idea. And I think even if you didn't think you were interested in the brain, if you want to build the kind of ai we're talking about and run it on physical hardware of any sort and understanding how the brain circuits compute what they do, what I'm talking about with as little power as they do, I don't know any better place to look.

Speaker 6:          01:28:25       It seems like a lot of the improvements in ai have been driven by increasing computational power. How far are you, would you say me like Gpu or CPU? How far would you say we are from hardware that could run a general artificial intelligence

Speaker 1:          01:28:40       of the kind that I'm talking about? Yeah, I dunno. I'll start with a billion cores and then we'll see. I mean, I think we're, I think we're, I mean, I think, I think there's no way to answer that question in a way that software independent. I don't know how to do that right. But I think that um, it's and, and you know, I don't know, like when you say how far we you mean? Uh, how far am I with the resources I have right now? How far my, if, if Google decides to put all of its resources at my disposal, like they might if I were working at deepmind. I don't know the answer to that question, but I think the, I think what we can say is this individual neurons. I mean, again, this goes back to another reason to study neurocircuits. Um, if you look at what we currently call neural networks in the Ai side, the model of a neuron is, is very, very simple thing.

Speaker 1:          01:29:28       Individual neurons are not only much more complex, but I have a lot more computational power. It's not clear how they use it or whether they use it, but I think it's just as likely that a neuron is something like a revenue, right? Is that a neuron is something like a computer, like under one neuron in your brain is more like a CPU note. Okay. Maybe, um, and thus the 10 billion or trillion, you know, the large number of neurons in your brain, um, I think it's like 10 billion cortical pyramidal neurons or something might be like 10 billion corps. Okay. For example, that's at least as plausible I think to me as any other estimate. So. And I think so. I think we're definitely on the underside with very big error bars, so I completely agree that um, or if this is what you might be suggesting and May, you know, going back to my answer to your question, I don't think we're going to get to what I'm talking about.

Speaker 1:          01:30:16       Anything like a real brain scale without major innovations on the hardware side. And you know, it's, it's interesting that what drove those innovations in the support current ai was mostly not ai. It was the video game industry. I'm a way when I point to the video game engine in your head, that's a similar thing that was driven by the video game industry on the software side. I think we should all play as many video games as we can and contribute to the growth of the video game industry because. No, because I mean, I mean you can see this [inaudible] like there are companies out there. For example, there's a company called improbable which is a London company. I'm London based startup, a pretty sizable startup at this point, which is building something that they call spatial Oscp, which is a. it's a, it's not a, it's not a hardware idea, but it's a kind of software idea for very, very big distributed computing environments to run much, much more complex, realistic simulations of the world for a much more interesting immersive permanent video games. I think that's one thing that might hopefully that will lead to more fun new kinds of games, but that's one example of where we might look to that industry to drive some of the computer systems, really hardware and software systems that will take. We'll take our game to the next level.

Speaker 7:          01:31:29       Just understanding algorithmic level or cognitive level is just to understanding the learning. The meaning of learning will be hard to predict, but on the circuit level these different, but at the what level under could live with.

Speaker 1:          01:31:43       Well of course it's different. Right, but already I think you made a mistake there. Honestly, like you said, the cognitive goal is learning how to predict, but I'm not sure what you mean by that. There's many things you could mean and are what our cognitive science is about is learning which of those versions, like I don't think it's learning how to predict. I think it's learning what need to know to plan actions and to uh, you know, all those things. Like it's not just about predicting, it's because there are things we can imagine that you would never predict because they would never happen unless we somehow make the world different generalizations. Sorry, not when you were methodical journalize, but especially in this transfer learning that you are interested in a few hundred neurons in prefrontal cortex. The generalize a lot. Yes, but not kind of a Bayesian model could do that.

Speaker 1:          01:32:25       You said, but lazy and model won't do that or they don't do it the way a Bayesian model does for sure. Because that's in the abstract level. Well, I mean, how do you really know like, and what does it mean to say that some neurons do it like. So maybe another way to put this is to say, look, we have a certain math that we use to capture these. You could call it abstract or I call it software level abstractions, right? I mean all engineering is based in some kind of abstraction, but you might have a circuit level abstraction, a certain kind of hardware level that you're interested in describing the brain at and I'm mostly working out or starting from a more software level of abstraction. Right? They're all abstractions. We're not talking about molecules here. Right. We're talking about some abstract notion of maybe a circuit or have a program.

Speaker 1:          01:33:05       Okay. Right now it's a really interesting question. If I look at some circuits, how do I know what program they're implementing? Right. If I look at the circuits and this machine, could I tell what program they implementing? Well, maybe, but certainly it will be a lot easier if I knew something about what programs they might be implementing before I start to look at the circuitry. If I just looked at the circuitry without knowing what the program was or what programs the thing might be doing or what kind of programming components would be mappable two circuits in different ways. Right. I don't even know how to begin to answer that question, so I think we've made some progress at understanding what neurons are doing in certain low level parts of sensory system and certain parts of the motor system, like primary motor cortex, like basically the parts of the neurons that are closest to the inputs and outputs of the brain, right where we don't eat when you can say we don't need the kind of software abstractions that I'm talking about or where we sort of agree on what those things already are so we can make enough progress on knowing what to look for and how to.

Speaker 1:          01:34:02       How to know when we found it. But if you want to talk about flexible planning, things that are more like cognition that go on and prefrontal Cortex, right? I, at this point, I don't. I don't think that just by recording from those neurons, we're going to be able to answer those questions in a meaningful engineering way. A way that that any engineer, software, hardware, whatever, could really say, yeah, okay, I get it. I get those insights in a way that I can engineer with and that's what my goal is, right? So my goal, that's my goal to do at the software level, the hardware level or the entire systems level, connecting them and I think that, you know, we can do that by taking what we're doing and bringing into contact with people studying neural circuits, but I don't think you can, you can leave this level out and just go straight to the neural circuits and I think the more you have, the more progress we make, the more we can help people who are studying at the neuro circuit level and they can help us address these other engineering questions that we don't really have access to like the power issue or the speed issue.

Speaker 1:          01:34:53       Okay. Thanks. That was great. Maybe give Janssen again

Speaker 2:          01:34:57       man. Thanks.