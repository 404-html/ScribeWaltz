Speaker 1:          00:00:00       The following is a conversation with Leslie Kale bling. She's a roboticist and professor at Mit. She's recognized for her work and reinforcement learning, planning robot navigation and several other topics in AI. She won the Edge Chi computers and thought award and was the editor and chief of the prestigious journal machine learning research. This conversation is part of the artificial intelligence podcasts at MIT and beyond. If you enjoy subscribe on Youtube, iTunes or simply connect with me on Twitter at Lex Friedman, spelled f, r I. D. And now here's my conversation with Leslie Kale. Blaine.

Speaker 2:          00:00:42       What made me get excited about Ai, I can say that is I read girdle Escher, Bach when I was in high school. That was pretty formative for me because it exposed a, the interesting ness of primitives and combination and how you can make complex things out of simple parts and ideas of AI and what kinds of programs might generate an intelligent behavior. So,

Speaker 3:          00:01:07       so you first fell in love with AI reasoning, logic versus robot.

Speaker 2:          00:01:12       Yeah, the robots came because, um, my first job, so I finished an undergraduate degree in philosophy at Stanford and was about to finish a master's in computer science. And I got hired at Sri, uh, in their AI lab and they were building a robot. It was a kind of a follow on to shaky, but all the shaky people were not there anymore. And so my job was to try to get this robot to do stuff and that's really kind of what got me interested in robots.

Speaker 3:          00:01:40       So maybe taking a small step back, your bachelors, Stanford Philosophy did Master's and phd in computer science, but the bachelor's in philosophy as a, what was that journey like? What elements of philosophy do you think you bring to your work in computer science?

Speaker 2:          00:01:55       So it's surprisingly relevant. So the part of the reason that I didn't do a computer science undergraduate degree was that there wasn't one at Stanford at the time, but that there's part of philosophy and in fact Stanford has a special sub major in something called now symbolic systems, which is logic model theory, formal semantics of national language. And so that's actually a perfect preparation for working in AI and computer science

Speaker 3:          00:02:20       that this is kind of interesting. So if you were interested in artificial intelligence,

Speaker 2:          00:02:25       yeah.

Speaker 3:          00:02:25       The what, what kind of majors, where people even thinking about taking, what does it in neuroscience was? So besides philosophies, what, what were you supposed to do if you're fascinated by the idea of creating intelligence,

Speaker 2:          00:02:37       there weren't enough people who did that for that even to be at conversation. I mean, I think probably probably philosophy. I mean it's interesting in my class, my graduating class of undergraduate philosophers, probably maybe slightly less than half way done and computer science slightly less than half went in law and like one or two we're not in philosophy. Uh, so it was a common kind of connection.

Speaker 1:          00:03:05       Do you think AI researchers have a role, be part time philosophers or should they stick to the solid science and engineering without sort of taking the philosophizing tangents? I mean, you work with robots, you think about what it takes to create intelligent beings. Uh, aren't you the perfect person to think about the big picture philosophy, if at all?

Speaker 2:          00:03:25       The parts of philosophy that are closest to AI I think, or at least the closest to AI that I think about our stuff like belief and knowledge and deportation and that kind of stuff and that's, you know, it's quite formal and it's like just one step away from the kinds of computer science work that we do kind of routinely. I think that there are important questions still about what you can do with a machine and what you can't and so on. Although at least my personal view is that I'm completely a materialist and I don't think that there's any reason why we can't make a robot be behaviourally indistinguishable from human. And the question of whether it's distinguishable internally, whether it's a Zombie or not. In philosophy terms, I actually don't, I don't know and I don't know if I care too much about that.

Speaker 1:          00:04:17       I would. There is a philosophical notions, they're mathematical and philosophical because we don't know so much of how difficult it is. How difficult is it perception problem? How difficult is the planning problem? How difficult is it to operate in this wall successfully? Because our robots are not currently as successful as human beings in many tasks. The question about the gap between current robots and human beings borders a little bit on philosophy. Uh, you know, the, the expanse of knowledge that's required to operate in this world and the ability to a form common sense knowledge, the ability to reason about uncertainty. Much of the work you've been doing. There's, there's open questions there that uh, I dunno, it required to activate a certain big picture view.

Speaker 2:          00:05:07       To me that doesn't seem like a philosophical gap at all. To me it's a, there is a big technical gap. Yes, huge technical gap, but I don't see any reason why it's more than a technical gap. Perfect.

Speaker 1:          00:05:20       So when you mentioned Ai, you mentioned Sri and uh, maybe can you describe to me when you first fell in love with robotics, the robots or inspired a which, so you should maybe mention a flaky or shaky, shaky, flaky. And what, what was the robot that first captured your imagination? What's possible?

Speaker 2:          00:05:42       Right. Well those are, the first robot I worked with was like shaky was there about that the Sri people had built. But by the time, I think when I arrived, it was sitting in a corner of somebodies office, dripping hydraulic fluid into a pan. Uh, but it's iconic and really everybody should read the shaky Tucker report because it has so many good ideas in it. I mean they invented a star search and symbolic planning and learning macro operators. They had a low level kind of configuration space planning for the robot. They had vision, they had all this, the basic ideas of a ton of things. Shaky have arms that, what was the job? Cheeky. Cheeky was a mobile robot, but it could push objects and so it would move things around which actually with its self, with spot, with his base. Um, so it could, but it, and they had painted the base boards black.

Speaker 2:          00:06:41       Uh, so it used, it used vision to localize itself in a map. It detected objects. It could detect objects that were surprising to it. Uh, it would plan and replan based on what it saw it reasons about whether to look and take pictures. I mean it really had the basics of, of so many of the things that we think about now. Um, how did it represent the space around it? So it had representations that a bunch of different levels of abstraction. So it had I think a kind of an occupancy grid of some sort at the lowest level. Uh, at the high level it was a abstract, symbolic kind of rooms and connectivity does flaky coming? Yeah. Okay. So I should have, but Sri and the, we were building a brand new robot. As I said, none of the people from the previous project, we're kind of there are involved anymore.

Speaker 2:          00:07:32       So we were kind of starting from scratch and my advisor was standards and shine. He ended up being my thesis advisor and he was motivated by this idea of situated computation are situated automata. And the idea was that the tools of logical reasoning were important, but possibly only for the engineers or designers to use in the analysis of a system, but not necessarily to be manipulated in the head of the system itself. Right? So I might use logic to prove a theorem about the behavior of my robot, even if the robot's not using logic and it's had to prove their upstream. So that was kind of the distinction. And so the idea was to kind of use those principles to make a robot do stuff. But a lot of the basic things we had to kind of learn for ourselves because I had zero background in robotics, I didn't know anything about control.

Speaker 2:          00:08:31       I didn't know anything about sensors. So we reinvented a lot of wheels on the way to getting that robot to do stuff. Do you think that was an advantage or a hindrance? Oh No, it's, I mean, I, I'm big in favor of wheel reinvention actually everyone, I think you learn a lot by doing it. It's important though to eventually have the pointers to so that you can see what's really going on. But I think you can appreciate much better the, the good solutions once you've messed around a little bit on your own and found a bad one.

Speaker 1:          00:09:00       Yeah. I think you've mentioned reinventing reinforcement learning and referring to a rewards as pleasures.

Speaker 2:          00:09:07       I have pleasure or I think, which I think is a nice name for it. It's more, it's more fun to almost, do you think you could tell the history of AI,

Speaker 1:          00:09:18       machine learning, reinforcement learning, how you think about it from the fifties to now?

Speaker 2:          00:09:23       One thing is that it's oscillates, right? So things become fashionable and then they go out and then something else becomes cool on that goes out and so on. And I think there's, so there's some interesting sociological process that actually drives a lot of what's going on. Early days was kind of cybernetics and control, right? And the idea that of homeostasis. People who've made these robots that could, I don't know, try to plug into the wall when they needed power and then come loose and roll around and do stuff. And then I think over time the thought, well, that was inspiring, but people said, no, no, no. We want to get maybe closer to what feels like real intelligence or human intelligence.

Speaker 2:          00:10:07       And then maybe the expert systems people tried to do that, but maybe a little too superficially. Right? So, oh, we get this surface understanding of what intelligence is like because I understand how a steel mill works and I can try to explain it to you and you can write it down in logic and then we can make a computer in for that. And then that didn't work out. But what's interesting, I think is when the thing starts to not be working very well. It's not only do we change methods, we change problems, right? So it's not like we have better ways of doing the problem with the expert systems people were trying to do. We have no ways of trying to do that problem. Oh yeah. I know. I think maybe a few, but we kind of give up on that problem and we switched to a different problem and we, we worked that for a while and we make progress as a community.

Speaker 1:          00:11:03       Yeah. And there's a lot of people who would argue you don't give up on the problem is just the you a decrease the number of people working on it. You almost kind of like put it on the shelf. So we'll come back to this 20 years later.

Speaker 2:          00:11:13       Yeah, that's right. Or you might decide that it's malformed. Like you might say, okay, it's trying to just try to make something that does superficial, symbolic reasoning, behave like a doctor. You can't do that until you've had the sensory motor experience of being a doctor or something. Right. So there's arguments that say that that's [inaudible] problem was not well formed where it could be that it is well formed, but, but we just weren't approaching it. Well,

Speaker 1:          00:11:43       you mentioned mentioned that your favorite part of logic and symbolic systems is that they give short names for large sets. So there is some use to this. Uh, they use some symbolic

Speaker 3:          00:11:54       reasoning. So looking at expert systems and symbolic computing, where do you think are the roadblocks that were hit in the eighties and nineties?

Speaker 2:          00:12:02       Ah, okay. So right. So the fact that I'm not a fan of expert systems doesn't mean that I'm not a fan of some kinds of symbolic reasoning. Right? So yeah, let's see. Roadblocks and well, the main road block I think was that the idea that humans could articulate their knowledge effectively into, into, you know, some kind of logical statements.

Speaker 3:          00:12:26       So it's not just the cost, the effort, but really just the capability of doing it

Speaker 2:          00:12:31       right. Because we're all experts in vision, right? Yeah, totally don't have introspective access into how we do that. Right. And it's true that, I mean, I think the idea was, well of course even if people then would know, of course, I wouldn't ask you to please write down the rules that you use for recognizing water bottle. That's crazy. And everyone understood that. But we might ask you to please write down the rules you use for deciding, I dunno what tie to put on or how to set up a microphone or something like that. But even those things, I think people maybe, I think what they found, I'm not sure about this, but I think what they found was that the so called experts could give explanations that sort of post hoc explanations for how and why they did things, but they weren't necessarily very good and then they differ. They depended on maybe some kinds of perceptual things, which again, they couldn't really define very well. So I think, I think fundamentally I think that the underlying problem with that was the assumption that people could articulate how and why they make their decisions.

Speaker 3:          00:13:43       Right. So it's almost I encoding the knowledge, uh, converting from experts is something that a machine can understand and reason with.

Speaker 2:          00:13:51       No, no, no, no. Not even just in coding, but getting it out of you. Right. Not, not, not writing it down. I mean, yes, hard also to write it down for the computer, but I don't think that people can produce it. You can tell me a story about why you do stuff, but I'm not so sure. That's the Y. Great.

Speaker 3:          00:14:12       So there are still on the hierarchical planning side places where symbolic reasoning is very useful. So, um, as, as you've talked about, so yeah,

Speaker 2:          00:14:27       where, where's the gap? Okay, good. So saying that humans can't provide a description of their reasoning processing these, that's okay, fine. But that doesn't mean that it's not good to do reasoning of various styles inside a computer. Those are just two orthogonal points. So then the question is what kind of reasoning should you do inside a computer? Right? Uh, and the answer is I think you need to do all different kinds of reasoning inside a computer depending on what kinds of problems you face.

Speaker 1:          00:14:59       I guess the question is what kinds of things can you encode Incode, symbolic, the seat can reason about,

Speaker 2:          00:15:08       I think the idea about an end, even symbolic. I don't even like that terminology because they don't know what it means technically. Informally. I do believe in abstractions. Abstractions are critical, right? You cannot reason at completely fine grain about everything in your life, right? You can't make a plan at the level of images and torques for getting a phd. Right? So you have to reduce the size of the state space and you have to reduce the horizon if you're going to reason about getting a phd or even buying the ingredients to make dinner. And so, so how can you reduce the spaces and the horizon of the reasoning you have to do and the answer's abstractions, spacial extraction and temporal abstraction. I think abstraction along the lines of goals is also interesting. Like you might or, well, abstraction in de composition goals is maybe more of a decomposition thing.

Speaker 2:          00:16:04       So I think that's where these kinds of, if you want to call it symbolic or discrete models come in you, you talk about a room of your house instead of your pose. You talk about, uh, you know, doing something during the afternoon instead of at two 54. And you do that because it makes you reasoning problem easier. And also because you have, you don't have enough information to reason in high fidelity about your pose of your elbow at two 35 this afternoon anyway. Right? When you try to get a phd, you're doing anything except for at that moment. At that moment, you do have to reason about the posterior elbow maybe, but then you, maybe you do that and some continuous joint space kind of model it. So again, I, my biggest point about all of this is that there should be, the dogma is not the thing, right? We shouldn't, it shouldn't be that I am in favor or against symbolic reasoning and you're in favor against neural networks. It should be that just, just computer science tells us what the right answer to all these questions is. If we were smart enough to figure it out.

Speaker 1:          00:17:13       Yeah. When you tried to actually solve the problem with computers, the right answer comes out. Uh, you mentioned abstractions. I mean, you all know works form abstractions or, uh, rather. Uh, there's, there's automated ways to farm strategies and there's expert driven waste to forums tractions and a expert human driven ways. Humans just seems to be way better at forming abstractions currently and certain problems. So when you're referring to 2:45 PM versus afternoon, how do we construct that taxonomy? Is there any room for automated construction of such abstractions?

Speaker 2:          00:17:52       Oh, I think eventually, yeah. I mean, I think when we get to be better and machine learning, engineers will build algorithms that bill awesome extractions

Speaker 1:          00:18:02       that are useful in this kind of way that you're describing. Yeah. So let's then step from the, the abstraction discussion and let's talk about a bomb MDPs partially observable mark decision prophecies. So uncertainty. So first watermark off decision procesees what are America decision process and maybe how much of our world can be models and MDPs how much when you, when you wake up in the morning, he making breakfast, how do you, do you think of yourself as an MDP? So how do you think about a MDPs and how they relate to our world?

Speaker 2:          00:18:38       Well, so there's a stance question, right? So a stance is a position that I take with respect to their problem. So I as a researcher or a person who designed systems can decide to make a model of the world around me in some terms and the right. So I take this messy world and I say I'm going to treat it as if it were a problem of this formal kind. And then I can apply solution concepts or algorithms or whatever to solve that formal thing, right? So of course the world is not anything, it's not an MDP or a palm DP. I don't know what it is, but I can model aspects of it in some way or some other way. And when I model some aspect of it in a certain way, that gives me some set of algorithms I can use.

Speaker 1:          00:19:21       You can model the world and all kinds of ways. Yeah. Uh, some have, some are more accepting of uncertainty, more easily modeling uncertainty of the world. Some really forced the world to be deterministic. And so there's certainly MDPs, uh, model the uncertainty of the world.

Speaker 2:          00:19:40       Yes. Model some uncertainty. They model not present state uncertainty, but they model uncertainty in the way of the future will unfold, right?

Speaker 1:          00:19:49       Yep. So what are mark off decision process? Okay, so Margaret decision process as a model.

Speaker 2:          00:19:55       It's a kind of a model that you could make that says, I know completely the current state of my system and what it means to be a state. Is that I, that all the info, I have all the information right now that will let me make predictions about the future as well as I can so that remembering anything about my history wouldn't make my predictions any better. Um, and, but, but then it also says that then I can take some actions that might change the state of the world and that I don't have a deterministic model of those changes. I have a probabilistic model of how the world might change. Uh, it's a, it's a useful model for some kinds of systems. I think it's a, I mean, it's certainly not a good model for most problems I think. Because for most problems that you don't actually know the state, uh, for most problems you, it's partially observed. So that's now a different problem class.

Speaker 1:          00:20:51       So the, okay, that's where the poverty piece of partially absorbed Mako decision process step. And so how do they address the fact that you can't observe most, uh, you have incomplete information about most of the world around you,

Speaker 2:          00:21:06       right? So now the idea is we still kind of postulate that there exists a state. We think that there is some information about the world out there such that if we knew that we could make good predictions but we don't know the state. And so then we have to think about how, but we do get observations. Maybe I get images right here. Things are I feel things and those might be local or noisy and so therefore they're going to tell me everything about what's going on. And then I have to reason about, given the history of actions, I've taken an observations, I've gotten, what do I think is going on in the world? And then given my own kind of uncertainty about what's going on in the world, I can decide what actions to take.

Speaker 1:          00:21:45       And so difficult is this problem of planning under uncertainty in your view and you long experience of modeling the world, trying to deal with this uncertainty in suspicion, real wall systems,

Speaker 2:          00:21:59       dumb all planning for even discrete palm dps can be undecidable depending on how you set it up and for, so lots of people say, I don't use palm [inaudible] because they are intractable. And I think that that's our kind of a very funny thing to say because the problem you have to solve is the problem you have to solve. So if the problem you have to solve as intractable, that's what makes us AI people, right? So, uh, we saw, we understand that the problem we're solving is, is complete wildly intractable that we can't, we will never be able to solve it optimally. At least I don't. Yeah. Right. So later we can come back to an idea about bounded optimality something. But anyway, we can't come up with optimal solutions to these problems. So we have to make approximations, approximations in modeling approximations in solution algorithms and so on. And so I don't have a problem with saying, yeah, my problem actually it is a palm DP in continuous space with continuous observations in, it's so computationally complex. I can't even think about it's, you know, big or whatever. But that doesn't prevent me from, it helps me, gives me some clarity to think about it that way and to then take steps to make approximation after approximation to get down to something that's like computable in some reasonable time.

Speaker 1:          00:23:22       When you think about optimality, the community broadly is shifted on that I think a little bit in how much they value the idea of a optimality of chasing an optimal solution. How is of use of chasing an optimal solution changed over the years and when you work with robots?

Speaker 2:          00:23:42       That's interesting. I think we have a little bit of a meth for the logic go crisis actually from the theoretical side. I mean, I do think that there is important in that right now. We're not doing much of it. So there's lots of empirical hacking around and training this and doing that and reporting numbers. But is it good? Is it bad? We don't know if there's very hard to say things. And if you look, uh, like computer science theory, so people talk for a while, everyone was about solving problems optimally. You're completely and, and then there were interesting relaxations. Wait, so people look at, Oh, can I are their regret bounds or can I do some kind of, um, you know, approximation can improve something that I can approximately solve this problem or that I get closer to the solution as I spend more time and so on.

Speaker 2:          00:24:36       What's interesting, I think is that we don't have good approximate solution concepts for very difficult problems. Right? I like to, you know, I like to say that I, I'm interested in doing a very bad job of very big problems. Uh, uh, right. So very big, very big problems. I like to do that, but I would, I wish I could say something. I wish I had a, I don't know, some kind of a of a formal solution concept that I could use to say, oh, this, this algorithm actually it gives me something like, I know what I'm going to get. I can do something other than just run it and get out six seven.

Speaker 3:          00:25:20       The notion is still somewhere deeply compelling to you. Hmm. The notion that you can say you can drop thing on the table says this, that you can expect this, this algorithm and gave me some good results.

Speaker 2:          00:25:34       I hope science will, I mean there's engineering and there's science. I think that they're not exactly the same. And I think right now we're making huge engineering like leaps and bounds. So the engineering is running way ahead of the science, which is cool. And often how it goes, right? So we're making things and nobody knows how and why they work roughly, but we need to turn that into science

Speaker 3:          00:26:00       was some form. There's some room for formalizing. We need to know what the

Speaker 2:          00:26:05       Bulls are. Why does this, why or why does that not work? I mean for awhile people build bridges by trying, but now we can often predict whether it's going to work it out without building it. Can we do that

Speaker 3:          00:26:16       for learning? Systems are for robots. So your hope is from a materialistic perspective, that intelligence, artificial intelligence systems, robots, a Chi. I would just fancier bridges, belief space. What's the difference between belief space in state space? Who mentioned MDPS MDPs you reasoning, uh, about you sense the world. There's a state, uh, w w what's this belief? Space idea. Yeah. Okay.

Speaker 2:          00:26:44       That sounds good. So I believe space that is, instead of thinking about what's the state of the world and trying to control that as a robot, I think about what is the space of beliefs that I could have about the world. What's, if I think of a belief as a probability distribution of her ways the world could be a belief state is a distribution. And then my control problem, if I'm reasoning about how to move through a world I'm uncertain about my control problem is actually the problem with controlling my beliefs. So I think about taking actions, not just what effect they'll have on the world outside, but what effect I'll have on my own understanding of the world outside. And so that might compel me to ask a question or look somewhere together, information, which may not really change the world state, but it changes my own belief about the world.

Speaker 3:          00:27:37       That's a powerful way to, to empower the agent to reason about the world, explore the world to what kind of problems does that allow you to solve, to, to uh, consider belief space versus just stay space?

Speaker 2:          00:27:52       Well, any problem that requires deliberate information gathering, right? So if in some problems,

Speaker 4:          00:28:00       okay,

Speaker 2:          00:28:00       like chess, there's no uncertainty or maybe there's uncertainty about the opponent. There's no uncertainty about the state. Uh, and some problems, there's uncertainty, but you gather information as you go, right? You might say, Oh, I'm driving my autonomous car down the road and it doesn't know perfectly where it is, but the lidars are all going on at the time. So I don't have to think about whether to gather information. But if you're a human driving down the road, you sometimes look over your shoulder to see what's going on behind you in the lane and you have to decide whether you should do that now and you have to trade off the fact that you're not seeing in front of you when you're looking behind you. And how valuable is that information and so on. And so to make choices about information gathering, you have to reasonably space also also, I mean also too just take into account your uncertainty before trying to do things. So you might say, if I understand where I'm standing relative to the door jam, uh, pretty accurately, then it's okay for me to go through the door. But if I'm really not sure where the door is, then it might be better to not do that. Right now.

Speaker 3:          00:29:13       The degree of your uncertainty ball about the world is actually part of the thing you're trying to optimize, informing, forming the plan. Right. Gotcha. So this idea of a long horizon of a planning for a phd or just even how to get out of the house or how to make breakfast here, that you show this presentation of the, the WTF or is the fork of robot looking at the sink? Uh, and uh, can you describe how we plan in this world is idea of hierarchical planning? We've mentioned this, this is, yeah. How can a robot hope to plan about something? Uh, oh, this was such a long horizon where the goal is quite far away.

Speaker 2:          00:29:54       People since probably reasoning began how thought about hierarchical reasoning, the temporal hierarchy and predict, well, their spacial hierarchy. But let's talk about temporal hierarchy. So you might say, oh, I have this long, uh, execution I have to do, but I can divide it into some segments abstractly, right? So maybe you have to get out of the house. I have to get in the car, I have to drive and so on. And so you can plan if you can build abstractions. So this, we started out by talking about abstractions and we're back to that now. If you can build abstractions in your state space and abstractions, sort of temporal abstractions, then you can make plans at a high level and you can say, I'm going to go to town and then I'll have to get gas and then I can go here and I can do this other thing and you can reason about the dependencies and constraints among these actions.

Speaker 2:          00:30:48       Again, without thinking about the complete details, what we do in our hierarchical planning work is then say, all right, I make a plan at a high level of abstraction. I have to have some reason to think that it's feasible without working it out in complete detail and that's actually the interesting step. I always liked to talk about walking through an airport, like you can plan to go to New York and arrive at the airport and then find yourself in a office building later. You can't even tell me in advance what your plan is for walking through the airport. Partly because you're too lazy to think about it maybe, but partly also because you just don't have the information. You don't know what get your landing in or what people are going to be in front of you or anything. So there's no point in planning in detail, but you have to have, you have to make a leap of faith that you can figure it out once you get there.

Speaker 2:          00:31:43       And it's really interesting to me how you arrive at that. How do you say you have learned over your lifetime to be able to make some kinds of predictions about how hard it is to achieve some kinds of sub goals. And that's critical. Like you would never plan to fly somewhere if you couldn't, didn't have a model of how hard it was to do some of the intermediate steps. So one of the things we're thinking about now is how do you do this kind of very aggressive generalization, uh, we to situations that you haven't been in it and so on to predict how long will it take to walk through the Kuala Lumpur airport like you can you give me an estimate and it wouldn't be crazy. And you have to have an estimate of that in order to make plans that involve walking through the qual on port airport, even if you don't need to know it in detail. So I really interested in these kinds of abstract models and how do we acquire them, but once we have them, we can use them to do hierarchical reasoning is, I think it's very important.

Speaker 3:          00:32:40       Yeah. There's this notion of go a goal regression and pretty image back chaining this idea of starting at the goal and it's just for me, big clouds of states you get, I mean it's almost like saying to the airport, you know, you know, once you show up to the, uh, the airport did, that's you, you're like a few steps away from the goal. So like thinking of it this way, uh, it's kind of interesting. I don't know if you have sort of a further comments on that, uh, of starting at the goal, right?

Speaker 2:          00:33:14       Yeah. I mean, it's interesting that Simon Herb Simon, back in the early days, the way I talked a lot about means ends reasoning and reasoning back from the goal. There's a kind of an intuition that people have that the number of state space is big. The number of actions you could take is really big. So if you say, here I sit and I want to search forward from where I am, what are all the things I could do that's just overwhelming. If you say, if you can reason at this other level and say, here's what I'm hoping to achieve, what can I do to make that true? That somehow the branching is smaller. Now what's interesting is that like in the AI planning community that hasn't worked out in the class of problems, that they look at it and the methods that they tend to use it, how's it turned out that it's better to go backward? Um, it's still kind of my intuition that it is, but I can't prove that to you right now. Right?

Speaker 3:          00:34:09       Tuition, at least for us mere humans. Uh Huh. Speaking of which, uh, when you, uh, maybe I would take it and take a look, take a little step into that philosophy circle. Uh, how hard would it, when you think about human life, you should give those examples often. How hard do you think it is to formulate human life supplanting problem or aspects of human life? So when you look at robots, you're often trying to think about object manipulation tasks about moving a thing. When you, when you take a slight step outside the room, let the robot leave and go get lunch, uh, or maybe tried to uh, pursue more fuzzy goals, how hard do you think is that problem? If you were to try to maybe put another way, try to formulate a human life as, as a planning problem?

Speaker 2:          00:35:02       Well that would be a mistake. I mean it's not all the planning problem, right? Every, I think it's really, really important that we understand that you have to put together pieces and parts that have different styles of reasoning and representation and learning. I think. I think it's, it's, it seems probably clear to anybody that, that it, it can't all be this or all be that brains aren't all like this are all like that, right? They have different pieces and parts and substructure. And so on. So I don't think that there's any good reason to think that there's going to be like when true algorithmic thing that's going to do the whole job, it's just a bunch of pieces together, uh, designed to solve a bunch of specific problem. One specific styles of problems. I mean there's probably some reasoning that needs to go on in image space.

Speaker 2:          00:35:51       I think again, there's this model based versus model free idea, right? So in reinforcement learning, people talk about, oh, should I learn, I could learn a policy just straight up a way of behaving. I could learn it's popular, only a value function and that's some kind of weird intermediate ground, uh, or I could learn a transition model or it's tells me something about the dynamics of the world. If I take a train, if I imagine that I learned in a transition model and I couple it with a planner and I draw a box around that, I have a policy. Again, it's just stored in a different way. Right? Right. It's an but it's just as much of a policy is the other policy. It's just I've made, I think the way I see it is it's a timespace tradeoff in computation, right? At more overt policy representation.

Speaker 2:          00:36:43       Maybe it takes more space, but maybe I can compute quickly what action I should take. On the other hand, maybe a very compact model of the world dynamics plus a planner. Let's make compute what action to take to just more slowly. There's no, I don't, I mean I don't think there's no argument to be had. It's just like a question of what form of computation is best for us for the various sub problems. Right. So and and so like learning to do Algebra and manipulations for some reason is, I mean that's probably going to want naturally a sort of a different representation then riding a unicycle at the time. Constraints on the unicycle or serious this thing. Spaces maybe smaller, I don't know. But so I

Speaker 1:          00:37:27       be the more human sides of falling in love. Having a relationship that might be another

Speaker 2:          00:37:32       Ah, yeah. Good. Another style up. No

Speaker 1:          00:37:35       how to model that. Yeah. That was first to solve the Algebra and object manipulation. Yeah. What do you think is harder? Perception or planning? Perception. That's why understanding, that's fine. Uh, so what do you think is so hard about perception by understanding the world around you?

Speaker 2:          00:37:55       Well, I mean, I think the big question is representational. A hugely, the question is representation. So perception that has made great strides lately, right in the weekend. Classify images and we can

Speaker 5:          00:38:13       okay.

Speaker 2:          00:38:14       Play certain kinds of games and predict how to steer the car and all that sort of stuff. Um, I don't think we have a very good idea of what perception should deliver. Right? So if you, if you believe in modularity, okay, there's, there's a very strong view which says we build in any modularity. We should make a giant Jag, can't take neural network training end to end to do the thing. And that's the best way forward. And it's hard to argue with that except on a sample complexity basis, right? So you might say, oh, well if I wanted to do end to end reinforcement learning on this giant, giant neural network, it's going to take a lot of data and a lot of like broken robots and stuff. So,

Speaker 5:          00:39:02       yeah.

Speaker 2:          00:39:03       Then the only answer is to say, okay, we have to build something in building some structure or some bias. We know from theory of machine learning, the only way to cut down in the sample complexity is to kind of cut down somehow cut down the hypothesis space. You can do that by building in bias. There's all kinds of reason to think that nature built bias into humans. Um, convolution is a bias, right? It's a very strong bias and is a very critical bias. So my own view is that we should look for more things that are like convolution, but the address, other aspects of reasoning, right? So convolution helps us a lot with a certain kind of spatial reasoning that's quite close to the imaging. I think there's other ideas like that,

Speaker 2:          00:39:54       maybe some amount of forward search, maybe some notions of abstraction, maybe the notion that objects exist. Actually, I think that's pretty important in a lot of people won't give you that to start with almost like a convolution in the, uh, uh, uh, in the object semantic objects, spaces, some kind, some kind of, some kind of idea. That's right. And people just started like the graph graph convolutions or an idea that are related to racial relational representations. And so I think there are, so you, I've come far afield from perception, but I think, um, I think the thing that's going to make perception that kind of the next step is actually understanding better what it should produce, right? So what are we going to do with the output of it? Right? It's fine when what we're going to do with the output is steer. It's less clear when we're just trying to make one integrated intelligent agent.

Speaker 2:          00:40:49       What should the output of perception be? We have no idea. And how should that hook up to the other stuff we don't know? Right? So I think the pressing question is what kinds of structure can we build in that are like the moral equivalent of convolution that will make a really awesome superstructure that then learning can kind of progress on efficiently? I agree. Very compelling description of actually where we stand with the perception of them. You're teaching a course and embodied intelligence. What do you think it takes to build a robot with human level intelligence? I don't know if we knew we would do it if you were to, I mean, okay, so do you think a robot and used to have a, a self awareness,

Speaker 1:          00:41:35       a consciousness and fear of mortality or is it, is it simpler than that or is consciousness a simple thing they [inaudible] do you think about these notions?

Speaker 2:          00:41:46       I don't think much about it. Consciousness, even most philosophers who care about it will give you that. You could have robots that are zombies, right? That behave like humans but they're not conscious and I at this moment we'd be happy enough for that. So I'm not really worried one way or the other.

Speaker 1:          00:42:02       So the technical side, you're not thinking of the use of self awareness.

Speaker 2:          00:42:08       Okay. But then what is self awareness mean? I mean that you need to have some part of the system that can observe other parts of the system and tell whether they're working well or not. That seems critical. So does that count this, I mean does that kind of self awareness or not? Well, it depends on whether you think that there's somebody at home who can articulate whether they're self aware, but clearly if I have like, you know, some piece of code that's counting how many times this procedure gets executed. That's a kind of self awareness. Right? So there's a big spectrum. It's clear you have to have some of it. Right.

Speaker 1:          00:42:45       You know, we're quite far away, I many dimensions, but is a direction of research that's most compelling to you for, you know, tried to achieve human level intelligence

Speaker 2:          00:42:55       and our robots? Well, to me, I guess the thing that seems most compelling to me at the moment is this question of what to build in and want to learn. Um, I think,

Speaker 4:          00:43:07       yeah,

Speaker 2:          00:43:07       we're, we don't, we're missing a bunch of ideas and, and we, you know, people, you know, don't you dare ask me how many years it's going to be till that happens. Cause I won't even participate in the conversation because I think we're missing ideas and I don't know how long it's going to take to find them.

Speaker 1:          00:43:25       So I won't ask you how many years, but, uh, maybe I'll ask you.

Speaker 4:          00:43:30       Yeah.

Speaker 2:          00:43:30       Would it

Speaker 1:          00:43:31       when you will be sufficiently impressed that we've achieved it. So what's, what's a a good test of intelligence? Do you like the Turing test and natural language and the robotic space? Is there something, wait, you would sit back and think, oh, us as pretty impressive, uh, as a test, as a benchmark. Do you think about these kinds of problems?

Speaker 2:          00:43:52       No, I resist. I mean, I think all the time that we spend arguing about those kinds of things could be better spent just making the robots work better,

Speaker 1:          00:44:03       you know, value competition. So I mean there's the nature of benchmark, uh, benchmarks and datasets or touring test challenges where everybody kind of gets together and tries to build a better robot because they want to out compete each other. I the Darpa challenge with the autonomous vehicles, do you see the value of that

Speaker 2:          00:44:23       or it can get in the way? I think it can get in the way. I mean, there's some people, many people find it motivating and so that's good. I find it anti motivating personally. Yeah. Uh, but I think what, I mean, I think you'd get an interesting cycle where for a contest or a bunch of smart people get super motivated and they hacked the brains out and much of what gets done is just hacks. But sometimes really cool ideas emerge and then that gives us something to chew on after that. So I'm, I, it's not a thing for me, but I don't, I don't regret that other people do it. Yeah.

Speaker 3:          00:44:56       And like you said with everything else to the mix is good. So jumping topics a little bit, he started the journal and machine learning research and served as its editor in chief. Uh, how did the publication come about?

Speaker 2:          00:45:10       Hmm.

Speaker 3:          00:45:11       And uh, what do you think about the current publishing model space in a machine learning or artificial intelligence?

Speaker 2:          00:45:18       Okay, good. So it came about because there was a journal called machine learning. We're still exists, which was own by Kluwer. And there was, I was on the editorial board and we used to have these meetings and really where we were to complain to Kluwer that it was too expensive for the libraries on that people couldn't publish and we would really like to have some kind of relief on those friends and they would always sympathize but not doing anything. So, uh, I just decided to make a new journal. And, uh, there was the journal of Ai Research, which has, was on the same model, which had been in existence for maybe five years or so. And it was going along pretty well. So, uh, we just made a new journal. It wasn't, I mean, it, um, I don't know, I guess it was work, but it wasn't that hard.

Speaker 2:          00:46:05       So basically the editorial board, probably 75% of the editorial board of a machine learning resigned and we founded the new, this new journal. But it was sort of, there's more open. Yeah. Right. So it's completely open. It's open access. Actually. Uh, uh, I had a postdoc, George kind of Doris, we wanted to call these journals free for all because there were, I mean, it both has no page charges and has no, uh, uh, access restrictions and the reason and so lots of people, I mean, there were, there were people who were mad about the existence of this journal who thought it was a fraud or something. It would be impossible. They said to run a journal like this with basically, I mean, for a long time I didn't even have a bank account. Uh, I paid for the lawyer to incorporate and the Ip address and just due to cost a couple hundred dollars a year to run. It's a little bit more now, but not that much more. But that's because I think computer scientists are competent and autonomous in a way that many scientists and other fields aren't doing these kinds of things. We already type set around papers. We all have students and people who can hack a website together in the afternoon. So the infrastructure us was like not a problem, but for other people in other fields, it's a harder thing to do.

Speaker 1:          00:47:34       Yeah. And this kind of open access journal and nevertheless one of the most prestigious journals. So it's not like a prestige and it can be achieved without any of the companies are required. Yeah. For Prestige. Yeah. Turns out. Yeah. So on the review process side of actually a long time ago, I don't remember when I reviewed a paper where you were also a reviewer and I remember reading your view of being influenced by it. It was really well written. It did influence how I write feature reviews. Uh, you disagreed with me actually, uh, you made it, uh, my review, but much better. So, but nevertheless, the review process, you know, has its flaws. And how do you think, what do you think works well? How can it be improved?

Speaker 2:          00:48:23       So actually when I started jam are I wanted to do something completely different and I didn't because it felt like we needed a traditional journal of record. And so we just made jam our be almost like a normal journal except for the open access parts of it basically. Um, increasingly, of course publication is not even a sensible word. You can publish something about putting it in an archive for. So I can publish everything tomorrow. So making stuff public is, there's no barrier. We still need curation and evaluation. I don't have time to read all of archive. And you could argue that kind of social thumbs upping of articles suffices, right? You might say, Oh heck with this we don't need journalists and all, we'll put everything on archive and people will upload and download the articles and then your CV, we'll say, oh man, they, he got a lot of boats. So that's good. Um, but I think there's still

Speaker 6:          00:49:37       cool

Speaker 2:          00:49:38       value in careful reading and commentary of things. And it's hard to tell when people aren't voting and down voting or arguing about your paper on Twitter and Reddit, whether they know what they're talking about. Right. So then I have the second order problem of trying to decide whose opinions I should value and such. So, I dunno, I W W if I had infinite time, which I don't, and I'm not going to do this because I really want to make the robots work. But if I felt inclined to do something more than a publication direction, I would do this other thing which I thought about doing the first time, which is to get together some set of people whose opinions I value and who were pretty articulate. And I guess we would be public, although it could be private, I'm not sure. And we would review papers, we wouldn't publish them and you wouldn't submit that.

Speaker 2:          00:50:29       We were just fine papers and we would write reviews and we would make those reviews public. And maybe if you, you know, so we're Leslie's friends who review papers and maybe eventually if we, our opinion was sufficiently valued, like the opinion of jam ours valued, then you'd say on your CV that Leslie's friends gave my paper of five star rating and that would be just as good as saying, I got it, you know, accepted into this journal. Um, so I think, I think we should have good public commentary, uh, and organize it in some way, but I don't really know how to do it. It's interesting times

Speaker 1:          00:51:06       the way the, the way you describe it actually is really interesting. Then he would do it for movies. I am db.com there's a experts, critics come in, they write reviews, but there's also regular non critics. Humans write reviews and they're separated. Open Review.

Speaker 2:          00:51:21       Well the, the, the I I clear process I think is interesting.

Speaker 1:          00:51:29       It's a step in the right direction, but it's still not as compelling as a, uh, reviewing movies or video games. I mean it sometimes, I'll almost as, it might be silly as least from my perspective to say, but it boils down to the user interface, how fun and easy it is to actually perform the reviews. How efficient, how much you as a reviewer get a street cred for being a good reviewer. Those elements, those human elements come into play.

Speaker 2:          00:51:57       No, it's a big investment to do a good review or a paper. And the flood of papers is that control. Right. So, you know, there aren't 3000 new, I don't know how many new movies are there in a year. I don't know. But that's probably going to be less than how many machine learning papers are in a year now. And I'm worried. I, you know, I, I uh, right, so I'm like an old person, so of course I'm going to say Rar, Rar, rar and things are moving too fast. I'm a stick in the mud. Uh, so I can say that, but my particular flavor of that is I think the horizon for researchers has gotten very short that students want to publish a lot of papers and there's a huge, there's value, it's exciting and there's value in that and you get patted on the head for it and so on.

Speaker 2:          00:52:48       But, and some of that is fine, but I'm worried that we're driving out people who would spend two years thinking about something. Back in my day when we worked on our theses, we did not publish papers. You did your thesis for years, you pick the hard problem and then you worked in, chewed on it and did stuff and waste of time for a long time. And when it was roughly when it was done, you would write papers. And so I don't know how to answer and I don't think that everybody has to work in that mode, but I think there's some problems that are hard enough that it's important to have a longer her research horizon. And I'm worried though we don't incentivize that at all. At this point

Speaker 3:          00:53:34       in this current structure, what do you see as a, what are your hopes and fears about the future of AI and continue on this theme? So Ai has gone through a few winters, ups and downs. Do you see another winter of AI coming, uh, or do you more hopeful about making robots work? As he said,

Speaker 2:          00:54:00       I think the cycles are inevitable, but I think each time we get higher, right. I mean, so, you know, it's like climbing some kind of landscape within noisy, uh, optimizer. Yeah. So it's clear that the, the, you know, the deep learning stuff has made deep and important improvements. And so the high water mark is now higher. I, there's no question, but of course I think people are overselling and eventually, uh, investors I guess, and other people look around and say, well, you're not quite delivering on this grand claim and that wild hypothesis it, so probably it's going to crash some of them out. And then it's okay. I mean it, but I don't, I can't imagine that there's like some awesome monotonic improvement from here to human level Ai. So in, uh, you know, I have to ask this question, I'd probably anticipate answers the answers, but, uh, do you have a worry short term or long term about the existential threats of Ai and

Speaker 3:          00:55:12       maybe short term, less existential but more a robot's taking away jobs?

Speaker 2:          00:55:19       Hmm. Well, actually, let me talk a little bit about utility. Actually, I had an interesting conversation with some military ethicists who wanted to talk to me about autonomous weapons. And there they were interesting, smart, well educated guys who didn't know too much about AI or machine learning. And the first question they asked me was, has your robot ever done something you didn't expect? And I like burst out laughing because anybody who's ever done something other robot right knows that they don't do it. And what I realized was that their model of how we program a robot was completely wrong. Their model of how we can put program robot was like Lego Mindstorms. Like oh go for it, meet her, turn left, take a picture, do this, do that. And so if you have that model of programming, then it's true. It's kind of weird that you run what would do something that you didn't anticipate.

Speaker 2:          00:56:14       But the fact is, and and actually, so now this is my new educational mission. If I have to talk to non experts, I tried to teach them the, that we don't operate, we operate at least one or maybe many levels of abstraction about that. And we say, Oh, here's a hypothesis class. Maybe it's a space of plans or maybe it's a space of classifiers or whenever. But there's some set of answers and the objective function. And then we work on some optimization method that tries to optimize a solution, a solution in that class. And we don't know what solution is going to come out. Right. So I think it's important to communicate that. So I'm going of course probably people who are listening to this, they, they know that lesson. But I think it's really critical to communicate that lesson. And then lots of people are now talking about, you know, the value alignment problem.

Speaker 2:          00:57:02       So you want to be sure as robots or software systems get more competent that their objectives are aligned with your objectives or that, uh, our objectives are compatible on some way or we have a good way of mediating when they have different objectives. And so I think it is important to start thinking in terms like, you don't have to be freaked out by the robot apocalypse to accept that it's important to think about objective functions of value alignment. Yes. And that you have to really, everyone who's done optimization knows that you have to be careful what you wish for that are, you know, sometimes you'd get the optimal solution and you realize, man, that was, that objective was wrong. So pragmatically in the shortest term, it seems to me that that those are really interesting and critical questions. And the idea that we're going to go from being people who engineer algorithms to being people who engineer objective functions.

Speaker 2:          00:57:58       I think that's, that's definitely going to happen. And that's going to change our thinking and methodology and stuff. You started at Stanford a philosophy that's where she could go back to philosophy maybe. Well, signing up there, I mean they're mixed together because, because as we also know as machine learning people, right? When you're designing, in fact, this is the lecture I gave in class today, when you design an objective function, you have to wear both hats. There's the hat that says, what do I want? And there's the hat that says, ah, but I know what my optimizer can do to some degree and I have to take that into account. All right. So it's, it's always a trade off and we have to kind of be mindful of that. The part about taking people's jobs, I understand that that's important. I don't understand sociology or economics or people very well. So I don't know how to think about that.

Speaker 7:          00:58:49       So that's, yeah, so there might be a sociological aspect there. The economic aspect that's very difficult to think about it. Okay.

Speaker 2:          00:58:56       I mean I think other people should be thinking about it, but I'm just, that's not my strength. So

Speaker 7:          00:59:00       what do you think is the most exciting area of research in the short term for the community and for you? For Yourself?

Speaker 2:          00:59:06       Well, so I mean there's the story I've been telling you about how to engineer intelligent robots. So that's what we want to do. We all kind of want to do well, I mean, some set of us want to do this and the question is what's the most effective strategy? And we've tried and there's a bunch of different things you could do at the extremes, right? One super extreme is we do introspection and then we write a program. Okay. That has not worked out really well. Another extreme is we'd take a giant bunch of neural guru and we try to train it up to do something. I don't think that's going to work either. So the question is what's the middle ground? And again, this isn't a a theological question or anything like that. It's just like how do just, how do we, what's the best way to make this work out?

Speaker 2:          00:59:55       And I think it's, it's clear, it's a combination of learning. To me it's clear, it's a combination of learning and not learning and what should that combination B and what's the stuff we build in. So to me that's the most compelling question. And when you say engineer robots, you mean engineering systems that work in the real world? Is that, that's the emphasis. The last question, which robots? A robot is your favorite from science fiction. So you can go with star wars are our Rtg d two or you can go with a more modern, uh, maybe how firm, I don't think I have a favorite robot from science fiction. This is, this is back to you. You like to make robots work in the real world here? Not a, not an, I mean, I love the process. I care more about the process. Yeah. I mean, I do research because it's fun. Not because I care about what we produce. Well, that's a, that's a beautiful note actually to, and unless they, thank you so much for talking today. Sure. It's been fun.