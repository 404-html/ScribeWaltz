Speaker 1:          00:00:00       The following is a conversation with Stuart Russell. He's a professor of computer science at UC Berkeley and a coauthor of a book that introduced me and millions of other people to the amazing world of AI called artificial intelligence and modern approach. So it was an honor for me to have this conversation as part of MIT course and artificial general intelligence and the artificial intelligence podcasts. If you enjoy it, please subscribe on Youtube, Itunes, or your podcast provider of choice, or simply connect with me on Twitter at Lex Friedman, spelled f, r I. D. And now here's my conversation with Stuart Russell.

Speaker 2:          00:00:41       So you've mentioned in 1975 in high school, you've created one year first AI programs that play chess, where you ever able to build a program that beat you at chess or another board game? Uh, so my program never beat me at chess. I actually wrote the program at Imperial College. I used to take the bus every Wednesday with a box of cards this big, uh, and shoved them into the card reader and they gave us eight seconds of CPU time. It took about five seconds to read the cards in and compile the code. So we had three seconds, a CPU time, which was enough to make one move, you know, with a not very deep search. And then we would print that move out and then we have to go to the back of the queue and wait to feed the cards. And again, how deep was the search?

Speaker 2:          00:01:37       Well, I were talking about the move to move. So now I think we got a, we got an eight move, eight depth fleet with Alpha Beta and we had some tricks of our own about a move ordering and some pruning of the tree. And, but you were still able to beat that program? Yeah, yeah. I was a reasonable chess player in my youth. I did an a fellow program, uh, and a backgammon program. So when I got to Berkeley, I worked a lot on what we call Meta reasoning, which really means reasoning about reasoning. And in the case of a game playing program, you need to reason about what parts of the search tree you're actually going to explore. Because the search tree is enormous, you know, bigger than the number of atoms in the universe. And, and uh, the way programs succeed and the way human succeed is by only looking at a small fraction of the search tree.

Speaker 2:          00:02:34       And if you look at the right fraction, you played really well. If you look at the wrong fraction, if you waste your time thinking about things that are never going to happen, the moves and no one's ever going to make, then you're going to lose cause you, you won't be able to figure out the right decision. So that question of how machines can manage their own computation, how they, how they decide what to think about is the reasoning question. And we developed some methods for doing that and very simply the machine should think about whatever thoughts are going to improve its decision quality. We were able to show that both for a fellow, which is a standard to play game and uh, for backgammon, which includes a dice roll. So it's a to play a game with uncertainty. For both of those cases, we could come up with algorithms that were actually much more efficient than the standard Alpha Bita search, uh, which chest programs at the time were using and that those programs could beat me.

Speaker 2:          00:03:38       And I think you can see the same basic ideas in Alphago and Alpha zero today. The way they explore the tree is using a form of matter reasoning to select what to think about based on how useful it is to think about it. Is there any insights you can describe without Greek symbols of how do we select which paths to go down? There's really two kinds of learning going on. So as you say, Alphago learns to evaluate board position. So it can, it can look at a goal board ended actually has powerfully a superhuman ability to instantly tell how promising that situation is. To me, the amazing thing about Alphago is not that it can be the world champion, whether its hands tied behind his back, but, uh, the fact that if you stop it from searching altogether, Susie, okay, you're not allowed to do any thinking ahead, right?

Speaker 2:          00:04:42       You can just consider each of your legal moves in and look at the resulting situation and evaluate it. So what we call a, a depth of one search. So just the immediate outcome of your moods and decide if that's good or bad. That version of Alphago can still play at a professional level, right? And human professionals is sitting there for five, 10 minutes deciding what to do and Alphago in less than a second can't instantly intuit what is the right move to make based on its ability to evaluate positions. Um, and that is remarkable. Um, because you know, we don't have that level of intuition about go, we actually have to think about the situation. So anyway, that capability that, um, Alphago has is one big part of why it beats humans. The other big part is that it's able to look ahead. 40 50 60 moves into the future.

Speaker 2:          00:05:44       And you know, if it was considering all possibilities, 40 or 50 or 60 moves into the future, that would be, you know, 10 to the 200 possibilities. So way, way more than atoms in the universe and and so on. So eats very, very selective about what it looks at. So let me try to give you an intuition about how you decide what to think about is a combination of two things. One is how promising it is, right? So if you're already convinced that a movie is terrible, there's no point spending a lot more time convincing yourself that it's terrible because it's powerfully not going to change your mind. So the, the real reason you think it's because there's some possibility of changing your mind about what to do, right? And is that changing of mind that would result then in, in a better final action in the real world.

Speaker 2:          00:06:44       So that's the purpose of thinking is to improve the final action in the real world. And so if you think about a move that is guaranteed to be terrible, you can convince yourself as terrible. And you're still not going to change your mind, right? But on the other hand, you were suppose you had a choice between two moves. One of them you've already figured out is guaranteed to be a draw, let's say. And then the other one looks a little bit worse. Like it looks fairly likely that if you make that move you're going to lose. But there's still some uncertainty about the value of that move. There is still some possibility that it will turn out to be a win. Right? Then it's worth thinking about that. So even though it's less promising on average than the other move, which is guaranteed to be a draw, there's still some purpose in thinking about it because there's a chance that you will change your mind and and discover that in fact it's a better move. So it's a combination of how good the move appears to be and how much uncertainty there is about its value. The more uncertainty, the more it's worth thinking about because as a higher upside, if you want to think of it that way. And of course in the beginning

Speaker 1:          00:07:54       machine, the Alphago zero formulation, it's everything is shrouded in uncertainty. So you really swimming in a sea of, uh, uncertainty. So it had benefits. You too, I mean actually following the same process as you described, but because you're so uncertain about everything, you basically have to try a lot of different directions.

Speaker 2:          00:08:15       Yeah. So, so the, the early parts of the search tree, a fairly bushy, um, that it will, it will look in a lot of different possibilities, but fairly quickly the degree of certainty about some of the moves. I mean, if a movie's really terrible, you'll pretty quickly find out, right? You lose half your pieces or half your territory and uh, and then you'll say, okay, this, this is not worth thinking about anymore. And then, so a further down the tree becomes very long and narrow and, uh, you're, you're following various lines of play. 10, 20, 30, 40, 50 moves into the future. And, um, that's again, it's something that the human beings have a very hard time doing. Uh, mainly because they just lack the short term memory. You just can't remember a sequence of moves that's 50 movies long. Uh, and you can't do, you can't imagine the board correctly, uh, for that money moves into the future. Of course the top

Speaker 1:          00:09:15       players, I'm much more familiar with chess, but the top players probably have, they have echoes of the same kind of intuition instinct that in a moment's time alpha go applies when they see a board. I mean, they've seen those patterns. Human beings have seen those patterns before. At the top of the grand master level, it seems that there is some similarities or maybe it's, it's our imagination creates a vision of those similarities. But it feels like this kind of pattern recognition that the Alphago approaches are using is similar to what human beings at the top level or using.

Speaker 2:          00:09:56       I think there's, uh, there's some truth to that, but not entirely. I mean, I think the, the extent to which human grandmaster can reliably rec instantly recognize the right move and instantly recognize the value of a position. Uh, I think that's a little bit overrated.

Speaker 1:          00:10:17       But if you sacrifice a queen for example, I mean there's these, there's these beautiful games of chess with Bobby Fischer, somebody where it's seeming to make a bad move. And I'm not sure there's a perfect degree of calculation involved where they've calculated all the possible thing that happened. But there's an instinct there, right? That somehow adds up to,

Speaker 2:          00:10:42       yeah, I think what happens is you, you, you get a sense that there's some possibility in the position, uh, even if you make a weird looking move, um, that it opens up some, some lines of, of calculation that otherwise would be, uh, definitely bad. And, um, and is that intuition that there's something here in this position that might, uh, might yield at when don the site and then you follow that, right? And, and in some sense when a, when a chess player is following a line in his or her mind there they mentally simulating what the other person is going to do while the opponent is going to do. And they can do that as long as the moves a kind of forced, right. As long as there's, you know, there's a, a fort we call a forcing variation where the opponent doesn't really have much choice how to respond and then you see if you can force them into a situation where you win.

Speaker 2:          00:11:43       You know, we see plenty of mistakes, uh, even even in grandmaster games where they just miss some simple three, four, five move, a combination that, you know, it wasn't particularly apparent in, in the position but was still there. That's the thing that makes us human. Yup. So when you mentioned that in Othello, those games or after some matter reasoning improvements and research was able to beat you, how did that make you feel? Part of the metal reasoning capability that it had? Um, [inaudible] was based on learning and, um, and you could sit down the next day and you could just feel that it had got a lot smarter and all of a sudden you really felt like, you know, sort of pressed against the wall because a, it was, it was much more aggressive and, and was totally unforgiving of, of any minor mistake that you might make.

Speaker 2:          00:12:42       And, uh, and Ashley's, it seemed understood the game better than I did. And Garry Kasparov has this quote where it, um, during his match against deep blue, he said he suddenly felt that there was a new kind of intelligence across the board. Do you think that's a scary or an exciting possibility? Kasparov and for yourself in the context of chess? Purely sort of in this, like that feeling, whatever that is. I think it's definitely an exciting feeling. You know, this is what made me work on AI in the first place, was as soon as I really understood what a computer was, they wanted to make it smart. You know, I started out with, the first program I wrote was for the sink layer programmable calculator. Uh, and I think you could right at 21 step, uh, algorithm that was the biggest program. You could write something like that.

Speaker 2:          00:13:42       Um, and do a little arithmetic calculation. So I sent think are implemented Newton's method for square roots, a few other things like that. Um, but then, you know, I thought, okay, if I just had more space, I could make this thing intelligent. And so I started thinking about Ai and, and I think the, the, the thing that's scary is not, is not the chess program because, you know, chess programs, they're not in the taking over the world business. Uh, but if you extrapolate, you know, there are things about chess that don't resemble the real world, right? We know, we know the rules of chess.

Speaker 2:          00:14:35       The chess board is completely visible to the program where, of course, the real world is not most, most of the real world is he's not visible from wherever you're sitting, so to speak. And, uh, to overcome those kinds of problems, you need qualitatively different algorithms. Another thing about the real world is that, you know, we, we regularly plan ahead on the timescales involving billions or trillions of steps. Um, now we don't plan those in detail, but you know, when you choose to do a phd at Berkeley, that's a five year commitment and that amounts to about a trillion motor control steps that you will eventually be committed to including going up the stairs, opening doors, drinking water type. Yeah, I mean every finger movement while you're typing every character of every paper and the thesis and everything. So you're not committing and advanced to the specific motor control steps, but you're still reasoning on a timescale that will eventually reduce to trillions of motor control actions and a, so for all of these reasons, you know, Alphago and deep blue and so on don't represent any kind of threat to humanity, but they are a step towards it, right?

Speaker 2:          00:16:01       And progress in AI occurs by essentially removing one by one. These assumptions that make problems easy, like the assumption of complete observability of the situation, right? We remove that assumption. You need a much more complicated kind of computing design. It needs, it needs something that actually keeps track of all the things you can't see and tries to estimate what's going on. Uh, and there's inevitable uncertainty in that, so it becomes a much more complicated problem. But you know, we are removing those assumptions. We are starting to have algorithms that can cope with much longer timescales. They can cope with uncertainty that can cope with partial observability. And so each of those steps sort of magnifies by a thousand the range of things that we can do with AI systems. So the way I started in a, I wanted to be a psychiatrist for a long time and energy and the mine in high school and of course program and so on.

Speaker 2:          00:17:03       And I showed up, uh, university of Illinois to an AI lab and they said, okay, I don't have time for you, but here's a book, Ai, a modern approach. I think it was the first edition at the time. Here it go, go, go learn this. And I remember the lay of the land was all, it's incredible that we solved chess, but we'll never solve go. I mean it was pretty certain that go in the way we thought about systems that reason was impossible to solve and now solve this as a very, I think I would have said that it's unlikely we could take the kind of algorithm that was used for chess and just get it to scale up a and work well for go and at the time what we thought was that in order to solve go, we would have to do something similar to the way humans manage the complexity you go, which is to break it down into kind of sub games.

Speaker 2:          00:18:04       So when a human thinks about a go board, they think about different parts of the board as sort of weakly connected to each other and they think about, okay, within this part of the board, here's how things could go in that part of what his, how things could go. And then you try to sort of couple those two analyses together and deal with the interactions and maybe revise your views of how things are going to go in each park in. Then you've got maybe five, six, seven, 10 parts of the board. And, um, that actually resembles the real world much more than Chester's because in the real world, you know, we have work, we have home life, we have sport, you know, whatever different kinds of activities, you know, shopping. These all are connected to each other, but they're weakly connected. So when I'm typing a paper, you know, I don't simultaneously have to decide which order I'm going to get the, you know, the milk and the butter, you know, that doesn't affect the typing.

Speaker 2:          00:19:07       But I do need to realize, okay, but if an this before the shops close is cause I don't have any ticket. You don't have any food at home. Right, right. So there's some weak connection but not in the way that chess works where everything is tied into a single stream of thought. So the thought was that go to solve go, we'd have to make progress on stuff that will be useful for the real world. And in a way Alpha goes a little bit disappointing, right? Because the, the program designed for Alpha was actually not that different from, from deep blue or, or even from Arthur Samuel's checker playing program for the 1950s.

Speaker 2:          00:19:48       And in fact, the, so the two things that make Alphago work is one, one is his amazing ability to ability to evaluate the positions. And the other is the Metta reasoning capability, which, which allows it to, to explore some paths in the tree very deeply into abandoned other paths very quickly. So this word matter reasoning, uh, while technically correct inspires perhaps the, the wrong degree of power that Alphago has. For example, the word reasonings as a powerful word. So let me ask you sort of the, you were part of the symbolic AI world for awhile. Like learn AI was, uh, there's a lot of excellent, interesting ideas there that unfortunately met I winter. And so it, do you think it re-emerges oh, so I would say, yeah, it's not quite as simple as that. So the, the Ai Winter, the first winter that was actually named as such was the one in the late eighties.

Speaker 2:          00:20:56       And that came about because, uh, in the mid eighties, there was a really a concerted attempt to push AI out into the real world, uh, using what was called expert system technology. And for the most part, that technology was just not ready for prime time. They were trying, uh, in many cases to do a form of uncertain reasoning, a judge judgment, combinations of evidence, diagnosis, those kinds of things, which was simply invalid. Uh, and when you try to apply in valid reasoning methods to real problems, you can fudge it for small versions of the problem. But when it starts to get larger, the thing just falls apart. So many companies found that, uh, the stuff just didn't work and they were spending tons of money on consultants to try and make it work. And there were other practical reasons, like, you know, they, they were asking the companies to buy incredibly expensive lisp machine workstations, which were literally between 50 and $100,000 in, uh, you know, in 1980 is money, which was, would be like between 150 and $300,000 per workstation in current prices. So the bottom line and they weren't seeing a profit from it. Yeah. Um, they, they, in many cases, I think there were some successes. There's no doubt about that. But people, I would say overinvested every major company was starting an AI department just like now.

Speaker 2:          00:22:40       And I worry a bit that we might see similar disappointments, not because of the tech. The current technology is invalid, but it's limited in its scope. And, uh, it's almost the, the duel of the, you know, the scope problems that expert systems had. So what have you learned from that hype cycle and what can we do to prevent another winter, for example? Uh, yeah, so when I'm giving talks these days, that's one of the warnings that I give. Uh, so those, those two, two pot warning slide one is that, uh, you know, rather than data being the new oil data is the new snake oil, uh, it's a good line. And then, um, and then the other, uh, is that we might see a kind of very visible failure in some of the major application areas. And I think self driving cars would be the flagship.

Speaker 2:          00:23:40       And, uh, I think when you look at the history, so the first self driving car was on the freeway, uh, driving itself, changing lanes overtaking in 1987 and uh, so it's more than 30 years and that kind of looks like where we are today. Right. You know, prototypes on the freeway, changing lanes and overtaking. Um, now I think significant progress has been made, particularly on the perception side. So we worked a lot on autonomous vehicles in the early, mid nineties at Berkeley, you know, and we had our own big demonstrations, you know, we, we put congressmen into self driving cars and, and had them zooming along the freeway. And, uh, the problem was clearly perception at the time. The problem. Yeah. W in simulation with perfect perception, you could actually show that you can drive safely for a long time and even if the other cars and he's behaving and so on.

Speaker 2:          00:24:48       But simultaneously we worked on machine vision for detecting cars and tracking pedestrians and so on. And we couldn't get the reliability of detection and tracking up to a high enough particular level, particularly in bad weather conditions. A nighttime rain fall. Good enough for demos, but perhaps not good enough to cover the general, the general operation. The thing about driving is, you know, so it, suppose you're a taxi driver and you drive every day, eight hours a day for 10 years, right? That's a hundred million seconds of driving, you know, and any one of those seconds you could make a fatal mistake. Yeah. So you're talking about eight nines of reliability right? Now, if your vision system only detects 98.3% of the vehicles, right? That's sort of, you know, one on a bit nines of reliability. So you have another seven orders of magnitude to go. And um, and this is what people don't understand.

Speaker 2:          00:25:54       They think, oh, because I had a successful demo, I'm pretty much done, but you're, you're not even within seven orders of magnitude of being done. And that's the difficulty. And it's, it's not the, Oh, can I follow up white line? That's not the problem. Right? We follow a white line all the way across the country, but it's the, it's the weird stuff that happens. It's sort of the edge cases. Yeah. The edge case, other drivers doing weird things. Um, you know, so if you talk to Google, right? So they had, um, actually very classical architecture where, you know, you had machine vision, which would detect all the other cars and pedestrians and the white lines in the road signs. And then basically that was fed into a logical database. And then you had a classical 19 seventy's rule based expert system. I'm telling you, okay, if you're in the middle lane and there's a bicyclist in the right lane who is signaling this, then then, then, then he'd do that.

Speaker 2:          00:26:58       Yeah. Right. And what they found was that every day they go out and there'd be another situation that the rules didn't cover. You know, so they, they come to a traffic circle and there's a little girl riding a bicycle the wrong way around the traffic circle. Okay, what do you do? We don't have a rule. Oh my God. Okay, stop. And then you, you know, they'd come back and add more rules and they just found that this was not really converging. And, um, and if you think about it, right? How, how do you deal with an unexpected situation, meaning one that you've never previously encountered and the sort of the reasoning required to figure out the solution. But that situation has never been done. It's, it doesn't match any, uh, previous situation in terms of the kind of reasoning you have to do. Well, you know, in chess programs, this happens all the time.

Speaker 2:          00:27:48       You're constantly coming up with situations that you haven't seen before and you have to reason about them. And you have to think about, okay, here are the possible things I could do. Here are the outcomes, here's how desirable the outcomes are. And then pick the right one. You know, in the 90s we were saying, Hey, this is how you're going to have to do automated vehicles. They're going to have to have a look ahead capability. But the look ahead for driving is more difficult than it is for chess because humans, the other, right, there's humans in there, less predictable than chess pieces, then well then you have an opponent in chess who's also somewhat unpredictable, but for example, in chess, you always know the opponent's intention. They're trying to beat you, right? Whereas in driving, you don't know, is this guy trying to turn left or has he just forgotten to turn off his turn signal?

Speaker 2:          00:28:38       Or is he drunk or is he, you know, changing the channel on his radio or whatever it might be. You've got to try and figure out the mental state, the intent of the other drivers to forecast the possible evolutions of their trajectories. And then you gotta figure out, okay, which is the directory for me, that's going to be safest. And those all interact with each other because the other drivers are going to react to your trajectory. Um, and so on. So, you know, they've got the classic merging onto the freeway, a problem where you're kind of racing a vehicle that's already on the freeway and you're gonna pull ahead of them. Or are you going to let them go first and pull in behind and, and you get this sort of uncertainty about who's going first. So all those kinds of things mean that you need a decision making architecture that's very different from either a rule based system or it seems to me it kind of an end to end neural network system, you know, so just as Alphago is pretty good when it doesn't do any look ahead, but it's way, way, way, way better when it does.

Speaker 2:          00:29:48       I think the same is going to be true for driving. You can have a driving system that's pretty good when it doesn't do any look ahead, but that's not good enough. Um, you know, and we've already seen multiple deaths caused by poorly designed machine learning algorithms that don't really understand what they're doing. Yeah. And on several levels, I think. Uh, so on the perception side, there's mistakes being made by those algorithms where the perception is very shallow on the planning side of the look ahead, like you said. And the thing that we come up against that's really interesting when you try to deploy systems in the real world is you can't think of an artificial intelligence system as a thing that responds to the world always. You have to realize that it's an agent that others will respond to as well. So in order to drive successfully, you can't just try to do obstacle avoidance, right?

Speaker 2:          00:30:48       You got and pretend that you're invisible, right? Are the invisible car, right? So that way, I mean, but you have to assert yet others have to be scared of you just we're all, there's this tension, there's this game. So we study a lot of work with pedestrians. If you approached pedestrians as purely an obstacle avoidance of you, you are doing look ahead, is in modeling the intent that you're, you're, they're not going to, they're going to take advantage of you. They're not going to respect you at all. There has to be a tension, a fear. Some amount of uncertainty. That's how we have Cree, or at least just have kind of a a resoluteness right? Yes. You have to display a certain amount of resolution is you can't, you can't be too tentative. Yeah. And uh, yeah, so the, the, the solutions then become pretty complicated, right?

Speaker 2:          00:31:40       You get into game theoretic analyses and so we, you know, at Berkeley now we're working a lot on this kind of interaction between machines and humans and that's exciting. And, uh, so my colleague, uh, and could drag an actually, you know, if you, if you formulate the problem game theoretically and you just let the system figure out the solution, you know, it does interesting. Unexpected things like sometimes at a stop sign, if no one is going first, right? The car will actually back up a little, right. And just to indicate to the other cars that they should go, uh, and that's something it invented entirely by itself. That's interesting. Right? There was, you know, we didn't say this is the language of communication at stop signs. It figured it out. That's really interesting. So let me one, just step back for a second. Just this beautiful philosophical notion. So Pamela McCormick in 1979 wrote, Ai began with the ancho wish to forge the gods. So when you think about the history of our civilization, do you think that there is an inherent desire to create,

Speaker 1:          00:32:59       let's not say gods, but to create superintelligence? Is it inherent to us is in our genes, that the natural arc of human civilization is to create things that are of greater and greater power and perhaps, uh, echoes of ourselves. So to create the gods as, as, uh, Pamela said,

Speaker 2:          00:33:25       if the, maybe, I mean, you know, we're all, we're all individuals, but certainly we see over and over again in history,

Speaker 2:          00:33:35       uh, individuals who thought about this possibility, hopefully when I'm not being too philosophical here. But if you look at the arc of this, you know where this is going and we'll talk about AI safety. We'll talk about greater and greater intelligence. Do you see that they're in, when you created the Othello program and you felt this excitement, what was that excitement? Was it excitement of a tinkerer who created something cool like a clock or was there a magic or was it more like a child being born that, yeah, yeah. So I mean, I certainly understand that viewpoint. And if you look at, um, the light hill report, um, which was coming, so in the 70s, there was a lot of controversy in the UK about Ai and you know, whether it was for real and how much the money, money the government should invest. And there was a long, long story, but the government commissioned a report by, by Light Hill who was a physicist and uh, he wrote a very damning report about Ai, which I think was the point. Uh, and uh, he said that that these are, uh, you know, frustrated man who unable to have children, would like to, to create a and, you know, create a life, um, you know, as a kind of replacement, which I, which I think is really pretty unfair.

Speaker 2:          00:35:13       But there is, I mean there, there is a kind of magic, I would say you when you, you build something and, and what you're building in is really just, you're building in some understanding of the principles of learning and decision making. And to see those principles actually then turn into intelligent behavior in, in specific situations. It's an incredible thing. And, uh, you know, that is a naturally going to make you think, okay, where does this end?

Speaker 1:          00:36:00       And so there's a, there's magical optimistic views of where it ends. Whatever your view of optimism is, whatever your view of utopia is probably different for everybody, but you've often talk about concerns you have of how things might go wrong. So, uh, I've talked to a Max Tegmark. Uh, there's a lot of interesting ways to think about AI safety. You're one of the Seminole people thinking about this problem amongst sort of being in the weeds of actually solving specific AI problems. You also think about the big picture of where we're going. So can you talk about several elements of it? Let's just talk about maybe the control problems. So this idea of losing the ability to control the behavior I AI system. So how do you see that? How do you see that coming about?

Speaker 2:          00:37:00       Do you think we can do, uh, to manage it well? So it doesn't take a genius to realize that if you make something that's smarter than you, you might have a problem. You know, and Turing, Alan Turing, you wrote about this and gave lectures about this ninth, 19, 51, he did a lecture on the radio and uh, he basically says no, once the machine thinking method starts, uh, you know, very quickly they'll outstrip humanity. And uh, you know, if we're lucky we might be able to, I think he says it, we may be able to turn off the power at strategic moments, but even so, our species would be humbled. Yeah. Actually I think was wrong about that. Right, because you, you know, if it's a sufficiently intelligent machine is not going to let you switch it off. It's actually in competition with you. So what do you think is meant?

Speaker 2:          00:38:01       Just for quick tangent, if we shut off this super intelligent machine that our species will be humbled, I think he means that we would realize that we are inferior, right? That we, we only survived by the skin of our teeth because we happen to get to the off switch, you know, Justin, Justin time, uh, you know, and if we hadn't, then, uh, we would have lost control over the earth. So do you, are you more worried when you think about this, the stuff about superintelligent AI or are you more worried about super powerful AI that's not aligned with their values? So the paperclip, uh, scenarios kind of, uh, I think, um, so the main problem I'm working on is he's the

Speaker 2:          00:38:52       control problem. The problem of machines pursuing objectives that are not aligned with human objectives. And, and this has been, it has been the way we've thought about AI since the beginning. You, you build a machine for optimizing and then you put in some objective and it optimizes right? And, and, um, you know, we, we can think of this as the, the king midas problem, right? Because if you know, the King Midas put in this objective, right, everything I touched turned to gold and the gods, you know, that's like the machine. They said, okay, done. You know, you now have this power and of course his food and his drink and his family all turned to gold and then he's dies, misery and starvation. And, um, this is, you know, it's, it's a warning. It's a failure mode that pretty much every culture in history has had some story along the same lines. You know, there's the, the genie that gives you three wishes and you know, third wish is always, you know, please undo the first two wishes because I messed up.

Speaker 2:          00:40:05       And, uh, you know, and when author Samuel wrote his chest, his checker playing program, which learned to play checkers considerably better than Arthur Samuel could play and actually reached a pretty decent standard. Uh, no, but we know who was, uh, one of the major mathematicians of the 20th century. He sort of the father of modern automation control systems. You know, he saw this and he basically extrapolated, uh, you know, as touring did and said, okay, this is how we could lose control and, uh, specifically that we have to be certain that the purpose we put into the machine as the purpose, which we really desire. And the problem is we can't do that.

Speaker 2:          00:40:57       You mean we're not, it's a very difficult to encode to, to put our values on paper is really difficult or you're just saying it's impossible. Uh, line is, hey, did you mean the test? So theoretically it's possible, but, uh, in practice it's extremely unlikely that we could specify correctly in advance the full range of concerns of humanity. The, you talked about cultural and transmission of values I think is how humans to human transmission of values happens. Right? Uh, well we learned, yeah. I mean, as we grow up, we learn about the values that matter, how things, how things should go, what is reasonable to pursue and what isn't reasonable to pursue. Big machines can learn it in the same kind of way. Yeah. So I think that, um, what we need to do is to get away from this idea that you build an optimizing the sheet and then you put the objective into it.

Speaker 2:          00:41:56       Because if it's possible that you might put in a wrong objective and we already know there's as possible cause it's happened lots of times, all right? That means that the machine should never take an objective that's given as Gospel truth because once it takes the, the, the objective is Gospel truth, right? Then it's the leaves that whatever actions it's taking in procedure, that objective are the correct things to do. So you could be jumping up and down and saying, no, you know, no, no, no, you're going to destroy the world. But the machine knows what the true objective is and is pursuing it and tough luck to you. You know, and this is not restricted to AI, right? This is, you know, I think many of the 20th century technologies, right? So in statistics you, you minimize a loss function. The loss function is exogenously specified in control theory.

Speaker 2:          00:42:52       You minimize the cost function in operations research, you maximize the reward function. And so on. So in all these disciplines, this is how we conceive of the problem. And it's the wrong problem because we cannot specify with certainty the correct objective, right? We need uncertainty. We the machine to be uncertain about as objective what it is that it's posted. It's maximizing favorite idea of years. Uh, I've heard you say somewhere, uh, well I shouldn't pick favorites, but it just sounds beautiful. We need to teach machines humility. Uh, yeah. I mean, that's a beautiful way to put it. I love it. Um, but they are humble in it. They know, they know that they don't know what it is they're supposed to be doing and the, and that, those, those objectives, I mean, they exist there within us, but we may not be able to explicate them.

Speaker 2:          00:43:49       We may not even know, uh, you know how we want our future to go. So you exactly. And the machine, you know, machine that's uncertain is going to be deferential to us. So if we say don't do that well now the machines learn something a bit more about our true objectives because something that it thought was reasonable in pursuit of our objective, it turns out not to be. So now it's learn something. So it's going to differ because it wants to be doing what we really want. And um, you know, that that point I think is absolutely central to solving the control problem. Yeah. Uh, and it's a different kind of AI when you, when you take away this idea that the objective is known then in fact a lot of the theoretical frameworks that we're so familiar with, you know, mark off decision processes, a goal based planning, uh, you know, standard game tree search, all of these, uh, techniques actually become in applicable. Uh, and you get a more complicated problem because, because now the interaction with the human becomes part of the problem cause the human by making choices is giving you more information about the true objective and that information helps you achieve the objective better. And so that really means that you're mostly dealing with game theoretic problems where you've got the machine and the human and they're coupled together, uh, rather than a machine going off by itself with a fixed objective,

Speaker 1:          00:45:39       which is fascinating on the machine and the human level that we, when you don't have an objective means you're together coming up with an objective. I mean, there's a lot of philosophy that, you know, you could argue the life doesn't really have meaning we we together agree on what gives it meaning. And we've kind of culturally create a things that give, why the heck we are in this earth anyway. Uh, we together as a society create that meeting and you have to learn that objective. And one of the biggest, I thought that's what you were going to go for a second. Uh, one of the biggest troubles we run into outside of statistics and machine learning and AI and just human civilization is when, uh, you look at, I came from this, I was born in the Soviet Union and the history of the 20th century, we run into the most trouble us humans when there was a, uh, uh, a certainty about the objective and you do whatever it takes to achieve that objective with he talking bout in Germany or communist Russia. All right.

Speaker 2:          00:46:45       Yeah, I get into trouble with humans say with, uh, you know, corporations. In fact, some people argue that, you know, we don't have to look forward to a time when AI systems take over the world they already have and they call a corporations, right? That corporations happened to be, uh, using people as components right now. Um, but they are effectively algorithmic machines and they're optimizing an objective, which is quarterly profit that isn't aligned with overall wellbeing of the human race and they are destroying the world. They are primarily responsible for our inability to tackle climate change. Right? So I think that's one way of thinking about what's going on with, uh, with corporations. But I think the point you're making is valid. That there, there are many systems in the real world where we've sort of prematurely fixed on the objective and then decoupled the, uh, the machine from those and it's supposed to be serving. Um, and I think you see this with government, right? Government is supposed to be a machine that serves people, but instead it tends to be taken over by people who have their own objective, uh, and use government

Speaker 1:          00:48:05       to optimize that objective regardless of what people want. Do, do you have, do you find appealing the idea of almost, uh, arguing machines where you have multiple AI systems with a clear fixed objective? We have in government, the red team and the blue team, they're are very fixed on their objectives and they argue and it kind of, uh, made would disagree, but it kind of seems to make it work somewhat that, uh, the, the, the duality of it I though, okay, let's go a hundred years back when there was still was going on or at the founding of this country, there was disagreements and that disagreement is where, uh, so it was a balance between certainty and forced humility because the power was distributed.

Speaker 2:          00:48:53       Yeah. I think that the, um, the, the nature of debate and disagreement argument takes, uh, as a premise, the idea that you could be wrong, right? Which means that you're not necessarily absolutely convinced that your objective is, is the correct one. All right? Um, if you were absolutely, there'd be no point in having any discussion or argument because you would never change your mind and there wouldn't be any, any sort of synthesis or, or anything like that. So, so I think you can think of argumentation as a, as an implementation of a form of uncertain reasoning. Uh, and, um, you know, I, I, I've been reading recently about utilitarianism and the history of efforts to define in a sort of clear mathematical way, a, if you like, a formula for moral or political decision making. And it's really interesting that the parallels between the philosophical discussions going back 200 years and what you see now in discussions about existential risk because yeah, it's almost exactly the same.

Speaker 2:          00:50:13       So someone would say, okay, well here's a formula for how we should make decisions, right? So utilitarian design roughly, you know, each person has a utility function and then we make decisions to maximize the sum of everybody's utility, right? And then people point out, well, you know, in that case, the best policy is one that leads to the enormously vast population, all of whom are living a life that's barely worth living, right? And, and this is called the repugnant conclusion. And, uh, you know, another version is, you know, that we, we should maximize pleasure and that's what we mean by utility. And then you'll get people effectively saying, well, in that case, you know, we might as well just have everyone hooked up to a heroin drip, you know, and they didn't use those words. But that debate, you know, was happening in the 19th century as it is now, uh, about AI that if we get the formula wrong, you know, we're going to have AI systems working towards an outcome that in retrospect would be exactly wrong.

Speaker 2:          00:51:22       Do you think theirs has beautifully put, so the echoes are there, but do you think, uh, I mean if you look at Sam Harris, is our imagination worries about the Ai version of that because of the, uh, the, the speed at which the things going wrong in the utilitarian context could happen? Yeah. Is that, is that a worry for you? Yeah, I think that, you know, in most cases, not in all, but if we, if we have a wrong political idea, you know, we see it starting to go wrong and we're, you know, we're, we're not completely stupid. And so we said, okay, that was, maybe that was a mistake. Uh, let's try something different. And, and also we're very slow and inefficient about implementing these things and so on. So you have to worry when you have corporations or political systems that are extremely efficient.

Speaker 2:          00:52:19       But when we look at AI systems or even just computers in general, right? They have this different characteristic from ordinary human activity in the past. So let's say you were a surgeon, you had some idea about how to do some operation, right? Well, and let's say you were wrong, right? That that way of doing the operation would mostly kill the patient. Well, you'd find out pretty quickly that like off to three, maybe three or four tries. Right? But that isn't true for pharmaceutical companies because they don't do three or four operations. They, they manufacturer three or 4 billion pills and they sell them and then they find out maybe six months or a year later that, oh, people are dying of heart attacks or getting cancer from this drug. And so that's why we have the FDA, right? Because of the scalability of pharmaceutical production and you know, and there have been some unbelievably bad episodes in the history of pharmaceuticals and adulteration of, of products and so on that debt have killed tens of thousands or paralyzed hundreds of thousands of people.

Speaker 2:          00:53:39       Now with computers, we have that same scalability problem that you can sit there and type for I equals one to 5 billion do, right? And all of a sudden you are having an impact on a global scale. And yet we have no FDA, right? There's absolutely no controls at all over what a bunch of undergraduates with too much caffeine can do to the world. And uh, you know, we look at what happened with Facebook, well, social media in general and click through optimization. So you have a simple feedback algorithm that's trying to just optimize, click through, right? That sounds reasonable. Right? Cause you don't want to be feeding people ads that they don't care about. I'm not interested in, and you might even think of that process as simply adjusting the, the feeding of ads or news articles or whatever it might be to match people's preferences.

Speaker 2:          00:54:44       Right. Which sounds like a good idea, but in fact that isn't how the algorithm works, right? You make more money, the algorithm makes more money. If it can better predict what people are going to click on it cause then it can feed them exactly that. Right? So the way to maximize click true is actually to modify the people to make them more predictable. And one way to do that is to feed them information which will change their behavior and preferences towards extremes that make them predictable and whatever is the nearest extreme or the nearest predictable point, uh, that's where you're going to end up. And the machines will force you there. Now, and I think there's a reasonable argument to say that this, among other things is contributing to the destruction of democracy in the world.

Speaker 2:          00:55:47       And where was the oversight of this process? Where would the people saying, okay, you would like to apply this algorithm to 5 billion people on the, on the face of the earth? Can you show me that it's safe? Can you show me that it won't have various kinds of negative effects? No. There was no one asking that question. There was no one placed between, you know, the undergrads with too much caffeine and the human race. Well it's just they just did it as, but some, uh, we're outside of the scope of my knowledge. So economists would argue that the, what is it? The invisible

Speaker 1:          00:56:24       hand. So the, the capitalist system, it was the oversight. So if you're going to corrupt society with whatever decision you make as a company, then that's going to be reflected in people not using your product. Sort of one. That's one model of oversight. So we shall see

Speaker 2:          00:56:41       time, you know, but you, you might even have broken the, uh, the political system that enables capitalism to function while you've changed it.

Speaker 1:          00:56:53       So we should see change changes often painful. So my question is, uh, absolutely, it's fascinating. You're absolutely right that there were zero oversight on algorithms that can have a profound civilization changing a effect. So do you think it's possible? I mean I haven't, have you seen government, so do you think is possible to create regulatory bodies, uh, oversight over Ai Algorithms, which are inherently such cutting edge set of ideas and technologies?

Speaker 2:          00:57:31       Yeah, but I think it takes time to figure out what kind of oversight, what kinds of controls and it took time to design the FDA regime. Uh, you know, and some people still don't like it and they want to fix it. Um, and I think there are clear ways that it could be improved, but the whole notion that you have stage one, stage two, stage three, and here are the criteria for what you have to do to pass the stage one trial. Right? We haven't even thought about what those would be for algorithms. So I mean, I think there are, there are things we could do right now with regard to bias. For example, we have a pretty good technical handle on, uh, how to detect algorithms that are propagating bias that exists in datasets, um, how to de bias, those algorithms, um, and, and even what it's going to cost you to do that.

Speaker 2:          00:58:27       So I think we could start having some standards on that. I think there are, there are things to do with impersonation of falsification that we could, we could work on. So I thanks. Yeah. Or in a very simple point. So impersonation ism is a machine, uh, acting as if it was a person. I can't see a real justification for why we shouldn't insist that machine self identify as machines. Uh, you know, where, where is the social benefit in, in fooling people into thinking that this is really a person when it isn't, you know, I don't mind if it uses a human light voice that's easy to understand, that's fine, but it should just say, oh, I'm a machine in some shape, some form,

Speaker 1:          00:59:20       and not many are speaking to that.

Speaker 2:          00:59:22       I would think relatively obvious facts. So I think most people, it's actually a law in California that bands impersonation, but only in certain restricted circumstances. So, uh, for the purpose of engaging in a forging and transaction and for the purpose of modifying someone's voting behavior. Uh, so those are, those are the circumstances where, uh, machines have to self identify. Um, but I think there's, you know, arguably it should be in all circumstances. Um, and then when you, when you talk about deep fakes, you know, we're just beginning, but already it's possible to make a movie of anybody saying anything, uh, in ways that are pretty hard to detect,

Speaker 1:          01:00:11       including yourself because you're on camera now and your voice is coming through with a high riser.

Speaker 2:          01:00:15       Yeah. As, as a, you could take what I'm saying and replace it with a pretty much anything else you wanted me to be saying. And even it would change my lips and expressions, expressions to fit. And, uh, there's actually not much in the way of a real legal protection against that. I think in the commercial area you could say, yeah, that's a, you're using my brand. And so on that, there, there are rules about that, but in the political sphere, I think it's, uh, at, at the moment it's, you know, anything goes. So, and that, that could be really, really damaging.

Speaker 1:          01:00:53       And let me just, uh, tried to make, not an argument, but try to look back at history and a say something a dark in essence is while regulation seems to be oversight seems to be exactly the right thing to do here. It seems that human beings, what they naturally do is they wait for something to go wrong. If you're talking about nuclear weapons, you can't talk about nuclear weapons being dangerous until somebody actually like the United States drops the bomb or Chernobyl melting. Do you think we will have to wait for things going wrong in a way that's obviously damaging to society? Not An existential risk, but obviously damaging. Okay.

Speaker 2:          01:01:42       Or do you have faith that, I hope not, but I mean, I think we do have to look at history and uh, you know, so the two examples you gave UCLE weapons and nuclear power are very, very interesting because, you know, when nuclear weapons, we knew in the early years of the 20th century that atoms contained a huge amount of energy, right? We had equals MC squared. We knew the, the mass differences between the different atoms and their components. And we knew that you might be able make an incredibly powerful explosive. So Hg wells wrote science fiction book, I think in 1912. Um, Frederick Saudi, who was the guy who discovered isotopes, a Nobel Prize winner, he gave a speech in 1915 saying that, you know, one pound of this new explosive would be the equivalent of 150 tons of dynamite, which turns out to be about right.

Speaker 2:          01:02:45       And, uh, you know, this was in World War One, right? So he was imagining how much worse the World War would be, uh, if we were using that kind of explosive. But the physics establishment simply refused to believe that these things could be made, including the people who are making it well. So they were doing the nuclear physics, I mean eventually wore the ones who made it and Jaguar for me or whoever. Well, so up to, um, the, the development, uh, was, was mostly theoretical. So it was people using sort of primitive kinds of particle acceleration and doing experiments, uh, at the, at the level of single particles are collections of particles that they, they weren't yet thinking about how to actually make a bomb or anything like that, but they knew the energy it was there and they figured if they understood it better, uh, it might be possible, but the physics establishment, their view, and I think because they did not want it to be true, the view was that it could not be true, uh, that this could not provide a way to make a super weapon.

Speaker 2:          01:03:57       And, um, you know, there was this famous speech given by Rutherford who was the sort of leader of nuclear physics. And, um, it was on September 11th, 1933. And he, he said, you know, anyone who talks about the possibility of obtaining energy from transformation of atoms is talking complete moonshine. And, uh, the next, uh, the next morning, Leo's Zelar read about that speech and then invented the nuclear chain reaction. And so as soon as he invented the, as soon as he had that idea that you could make a chain reaction with neutrons because neutrons were not repelled by the nuclear so they could enter the nuclear center and think, continue the reaction. As soon as he has that idea, he instantly realized that the world was in deep doodoo. Uh, because this is 1933, right in a Hitler had a recently come to power in Germany. Zilara was in London, uh, and eventually became a refugee and a, and came to the u s and the, I'm in the process of, of having the idea about the chain reaction.

Speaker 2:          01:05:12       He figured out basically how to make a bomb and also how to make a reactor. And he patented the reactor in 1934, but because of the situation, the great power conflict situation that he could see happening, um, he kept that a secret. And so, um, between then and the beginning of world war two, people were working, including the Germans on how to actually create neutron sources. Right. What specific fission reactions would produce neutrons have the right energy to continue the reaction. And, and that was demonstrated in Germany, I think in 1938 if I remember correctly. The first, uh, nuclear weapon patent was 1939 by the French. Um, so this was actually a, you know, this was actually going on, you know, well before World War Two really got going. And then, you know, the British parable, he had the most advanced capability in this area, but for safety reasons among others and plus, which is sort of just resources.

Speaker 2:          01:06:28       They moved the program from Britain to the u s and then that became Manhattan project. Uh, so the, the, the reason why we couldn't have any kind of oversight of nuclear weapons and nuclear technology was because we were basically already in a, an arms race in a war. And, um, but you, you mentioned that in the twenties and thirties. So what are the echoes, the way you've described the story, I mean, there's clearly echoes. Why do you think most AI researchers, folks who are really close to the metal, a really are not concerned about AI? They don't think about it, uh, whether it's, they don't want to think about it. It's, well, what are the, yeah, why do you think that is? What are the echoes of the nuclear situation to the current situation and what can we do about it? I think there is a, you know, a kind of motive motivated cognition, which is a, a term in psychology means that you believe what you would like to be true, uh, rather than what is true.

Speaker 2:          01:07:42       And, uh, you know, it's, it's unsettling to think that what you're working on might be the end of the human race, obviously. So you would rather instantly deny it and come up with some reason why it couldn't be true. And the, you know, I have, I collected a long list of regions that extremely intelligent, competent AI scientists have come up with for why we shouldn't worry about this. You know, for example, calculators are superhuman at arithmetic and they haven't taken over the world. So there's nothing to worry about. Well, okay, my five year old, you know, could've figured out why that was, uh, uh, an unreasonable and, and really quite weak argument. Um, you know, another one was, uh, you know, while it's theoretically possible that you could have a super human AI destroy the world, you know, it's also theoretically possible that a black hole could materialize right next to the earth and destroy humanity. I mean, yes, it's theoretically possible quantum, theoretically extremely unlikely that it would just materialize right there. Um, but that's a completely bogus and allergy because if the whole physics community on earth was working to materialize a black hole in near Earth orbit, right, wouldn't you ask them, is that a good idea? Is that going to be safe? You know, what if you succeed? Right? And that's the thing, right? The Ai community is sort of refused to ask itself what if you succeed?

Speaker 2:          01:09:24       And initially I think that was because it was too hard, but you know, Alan Turing asked himself that and he said we'd be toast. Right? If we were lucky, we might be able to switch off the power, but preferably we'd be toast. But there's also an aspect that, because we're not exactly sure what the future holds, it's not clear exactly. So technically what to worry about, sort of how things go wrong. And so, uh, there is something, it feels like, maybe you can correct me if I'm wrong, but there's something paralyzing about worrying about something that logically is inevitable, but you don't really know what that will look like. Yeah, I think that's, that's it's a reasonable point. And you know, the, you know, it's certainly in terms of existential risks, it's different from asteroid collides with the earth. Right, right. Which again, is quite possible.

Speaker 2:          01:10:26       Uh, you know, it's happened in the past. It'll probably happen again. We don't, right. We don't know right now. But if we did detect an asteroid that was going to hit the earth in 75 years time, we'd certainly be doing something about it. Well, it's clear that he's got a big rock and as we'll probably have a meeting and see what do we do about the big rock with Ai, right. Where they, I mean there are very few people who think it's not going to happen within the next 75 years. I know Rod Brooks doesn't think it's going to happen. Uh, maybe Andrew Ng doesn't think it's happened, but you know, a lot of the people who work day to day, uh, you know, as you say, he had the rock face, they think it's going to happen. I think the median estimate from AI researchers is somewhere in 40 to 50 years from, from now when maybe you little, you know, I think in Asia they think it's going to be even faster than that.

Speaker 2:          01:11:18       I am, I'm a little bit more conservative. I think probably take longer than that. But I think it's, you know, as happened with nuclear weapons and overnight it can happen overnight that you have these breakthroughs and we need more than one breakthrough. But you know, it's on the order of half a dozen. This is a very rough scale, but sort of half a dozen breakthroughs of that nature would have to happen for us to reach superhuman Ai. But the, you know, the AI research community is vast now. The massive investments from governments, from corporations, uh, tons of really, really smart people. You know, you just have to look at the rate of progress in different areas of AI to see that things are moving pretty fast. So, so to say, oh, it's just going to be thousands of years. I mean, I don't see any basis for that. You know, I see, you know, for example, the, the, the Stanford hundred year Ai Project, right?

Speaker 2:          01:12:22       Which, um, is supposed to be sort of, you know, the serious establishment view, uh, their most recent report actually said it's probably not even possible. Oh, wow. Right. Which if you want a perfect example of people in denial, that's it. Because you know, for the whole history of Ai, we've been saying to philosophers who said it wasn't possible. Well, you have no idea what you're talking about. Of course it's possible. Right. Give me an, give me an argument for why it couldn't happen and there isn't one. Right. And now because people are worried that maybe a, I might get a bad name or, or I just don't want to think about this, they are saying, okay, well of course it's not really possible. You know, if we imagine, right? Imagine if, you know, the, the leaders of the cancer biology community, uh, got up and said, well, you know, of course curing cancer, it's not really possible.

Speaker 2:          01:13:19       It'd be at complete outrage, dismay, and you know, I find this really strange phenomenon, so, okay, so if you accept that as possible and if you accept that it's probably going to happen. The point that you're making that, you know, how does it go wrong? A valid question without that, without an answer to that question, then you're stuck with what I call the griller problem, which is, you know, the problem that the gorillas face, right? They made something more intelligent than them, namely us a few million years ago, and now, now they're in deep doodoo. Uh, so there's really nothing they can do. They've lost the control. They failed to solve the control problem of controlling humans. And, uh, so they, they lost. Um, so we don't want to be in that situation. And if the gorilla problem is, is the only formulation you have it, there's not a lot you can do, right, other than to say, okay, we should try to stop, you know, we should just not make the humans or in this case, not make the AI.

Speaker 2:          01:14:31       And I think that's really hard to do to, uh, and I'm not actually proposing that that's a feasible of course of action. Um, and I also think that if properly controlled AI could be incredibly beneficial. So the, but it seems to me that there's a, there's a consensus that one of the major failure modes is this loss of control. That we create AI systems that are pursuing incorrect objectives. And because the AI system believes it knows what the objective is, it has no incentive to listen to us anymore, so to speak. Right. It, it's just carrying out the, the strategy that it, it has computed as being the optimal solution.

Speaker 2:          01:15:24       And, uh, you know, it may be that in the process it needs to acquire more resources to increase the possibility of success or you know, prevent various failure modes by defending itself against interference. And so that collection of problems I think is something we can address. The other problems are roughly speaking, you know, misuse, right? So even if we solve the control problem, we make perfectly safe, controllable AI systems. Well, why, you know, why does Dr Evil, we're going to use those, right? He wants to just take over the world and he'll make unsafe AI systems but then get out of control. So that's one problem, which is sort of a, you know, uh, partly a policing problem. Parlier uh, sort of a cultural problem for the profession of how we teach people. Uh, what kinds of AI systems are safe. You talk about autonomous weapon system and how pretty much everybody agrees, there's too many ways that that can go horribly wrong.

Speaker 2:          01:16:30       Had this great a slot of arts movie that kind of illustrates that beautifully. I want to talk about that. That's another, there's another topic I'm happy we talk about the, just want to mention that what I see is the third major failure mode, which is overused, not so much misuse but overuse of AI that we become overly dependent. So I call this the Wally problems. If you seen Morley, the movie, all right, all the humans are on the spaceship and the machines look after everything for them and they just watch TV and drink big gulps and uh, they're all sort of obese and stupid and, and they sort of totally any notion of human autonomy and um, you know, so it in effect, right? This would happen like those slow boiling frog, right? We would gradually turn over more and more of the management of our civilization to machines as we are already doing.

Speaker 2:          01:17:28       And this, you know, this, if this process continues, you know, we, we sort of gradually switch from sort of being the masters of technology to just being the guests, right? So, so we become guests on a cruise ship, you know, which is fine for a week, but not, not for the rest of eternity, you know. And it's almost irreversible, right? Once you, once you lose the incentive to, for example, learn to be an engineer or a doctor or a sanitation, uh, operative or, or any other of the, the infinitely many ways that we maintain and propagate our civilization. You know, if you, if you don't have the incentive to do any of that, you won't. And then it's really hard to recover. And of course they are just one of the technologies that could, that third failure mode results in that there's probably other technology in general detaches us from, uh, it does a bit, but the, the, the differences that in terms of the knowledge to, to run our civilization, you know, up to now we've had no alternative but to put it into people's heads.

Speaker 2:          01:18:40       Right, right. And if you, the software with Google, I mean, so software in general, so cute. The computers in general, but, but the, you know, the knowledge of how, you know, how a sanitation system works, you know, that's an AI has to understand that it's no good putting it into Google. So, I mean, we, we've always put knowledge in on paper, but paper doesn't run our civilization and it only runs when it goes from the paper into people's heads again, right? So we've always propagated civilization through human minds. And we've spent about a trillion person years doing that. I literally write, and you can, you can work it out. It's about right is about just over 100 billion people who've ever lived. And uh, each of them has spent about 10 years learning stuff to keep their civilization going. And, uh, so that's a trillion person years we put into this effort, beautiful way to this hand, all of civilization.

Speaker 2:          01:19:34       And now we're, you know, we're in danger of throwing that away. So this is a problem that AI can't solve. It's not a technical problem. It's a, you know, if we do our job right, the AI systems will say, you know, the human race doesn't, in the long run want to be passengers and a cruise ship. The human race wants autonomy. This is part of human preferences. So we, the AI systems are not going to do this stuff for you. You've got to do it for yourself, right? I'm not going to carry you to the top of Everest in an autonomous helicopter. You have to climb it if you want to get the benefit. Um, and so on. So, but I'm afraid that because we are short sighted and lazy, we're going to override the AI systems. And, and there's an amazing short story that I recommend to everyone that I talk to about this called the machine stops written in 1909 by Ian Forster, who, you know, wrote novels about the British Empire and sort of things that became costume dramas on the BBC.

Speaker 2:          01:20:40       But he wrote this one science fiction story, which is an amazing vision of the future. It has, it has basically iPads. Uh, it has video conferencing, it has moocs. Uh, it has computer, computer induced obesity. I mean, literally it's what people spend their time doing is giving online courses or listening to online courses and, and, and talking about ideas. But they never get out there in the real world that they don't really have a lot of face to face contact. Uh, everything is done online, you know, so all the things we're worrying about now, uh, were described in the story and, and then the human race becomes more and more dependent on the machine, loses knowledge of how things really run a, and then becomes vulnerable to collapse. And, uh, so it's, uh, it's a pretty unbelievably amazing story for someone writing in 1909 to two.

Speaker 2:          01:21:38       Imagine all this less. Yeah. So there's very few people that represent artificial intelligence more than you suit Russell, if you say, okay, so it was all my fault. It's all right. Um, you're often brought up as the person while Stuart Russell, like the AI person is worried about this. That's why you should be worried about, do you feel the burden of that? I don't know if you feel that at all, but when I talk to people like from it, you talk about people outside of computer science, when they think about this still Russell, uh, is worried about AI safety. You should be worried too. Do you feel the burden of that? I mean in a practical sense, uh, yeah, because, uh, I get, uh, you know, a dozen, sometimes 25 invitations a day to talk about it, to give interviews, to write press articles and so on.

Speaker 2:          01:22:39       So, um, in that very practical sense, I'm seeing that people are concerned and really interested about this. Um, by you worried that you could be wrong as all good scientists are. Of course, I worry about that all the time. I mean, that's, that's always been the way that I, I worked, you know, is like I have an argument in my head with myself, right? So I have, I have some idea and I think, okay, how could that be wrong? Or did someone else already have that idea? So I'll go and search and as much literature as I can and to see whether someone else already thought of that or, or even refuted it. So, you know, I, right now I'm reading a lot of philosophy because you know, in, in the form of the debate, so over utilitarianism and other kinds of moral, uh, moral formula formulas shouldn't, shall we say, people have already thought through some of these issues.

Speaker 2:          01:23:43       But you know what, one of the things I'm, I'm not seeing in a lot of these debates is, is this specific idea about, uh, the importance of uncertainty in the objective, um, that this is the way we should think about machines that are beneficial to humans. So this idea of, of provably beneficial machines based on, uh, explicit uncertainty in the objective. Um, you know, it seems to be, you know, my, my gut feeling is this is the core of it. It's going to have to be elaborated in a lot of different directions and there are a lot of really beneficial, yeah. But they're there. I mean, it has to be right. We can't afford, you know, hand wavy beneficial because they're, you know, whenever we do hand wavy stuff, there are loopholes. And the thing about super intelligent machines is they find the loopholes, you know, just like, you know, tax evaders.

Speaker 2:          01:24:41       Uh, if you don't write your tax little properly that people will find the loopholes and end up paying no tax and, and, uh, so you should think of it this way and getting those definitions right. You know, it is really a long process, you know, so you can, you can define mathematical frameworks and within that framework you can prove mathematical theorems that yes, this, this, this theoretical entity will be provably beneficial to that theoretical entity. But that framework may not match the real world in some crucial way, a long process of thinking through it to iterating and so on. Last question. Yep. Uh, you have 10 seconds to answer it. What is your favorite Scifi movie about Ai? I would say interstellar has my favorite robots or beets? Topsy. Yeah, yeah. Yeah. So, so Taz, the robots, one of the robots in interstellar is the way a robot should behave. And, uh, I would say x Makena is in some ways the one that's the one that makes you think a, in a nervous kind of way about, about where we're going. Osser sir, thank you so much for talking today. Pleasure.