Speaker 1:          00:00          Today we'll talk about how to make machines see computer vision and we will present thank you. Whoever said yes, and today we will present a competition that unlike deep traffic which is designed to explore ideas, teach you about concepts so deeper, deeper enforcement learning SegFuse, the deep dynamic driving scene segmentation, competition that are present today is at the very cutting edge. Whoever does well in this competition is likely to produce a publication or ideas that would lead the world in the area of perception, perhaps together with the people running this class, perhaps in your own. I encourage you to do so even more cast today. Computer Vision today as it stands is deep learning majority of the successes in how we interpret form representations, understand images and videos utilize to a significant degree and you're on that works. The very ideas we've been talking about that applies for supervised, unsupervised and reinforcement learning and for the supervised case is the focus of today. The process is the same. The data is essential. There's annotated data where the human provides the labels that serves as the ground truth and the training process. Then the neural network goes through that data, learning to map from the raw sensory input to the ground truth labels and then generalize or the testing data set

Speaker 1:          01:57          and the kind of raw senses we're dealing with are numbers. I'll say this again and again that for human vision for us here, we take for granted this particular aspect of our ability is to take in raw sensor information through our eyes and interpret it, but it's just numbers. That's something whether you're an expert in computer vision person or new to the field, you have to always go back to meditate on is what kind of things the machine is given, what, what? What is the data that is tasked to work with in order to perform the task you're asking it to do? Perhaps the data is given is highly insufficient to do what you wanted to do. That's the question I'll come up again and again our images and enough to understand the world around you and given these numbers, these set of numbers, sometimes with one channel, sometimes with three rgb where every single have three different colors. The task is to classify or regress producing continuous variable or one of a set of class labels as before, we must be careful about our intuition of what is hard, what is easy and computer vision.

Speaker 1:          03:28          Let's take a step back to the inspiration for an year old networks, our own biological neural networks because the human vision system and the computer vision system is a little bit more similar in these regards. The structure of the human visual Cortex is in layers and his information passes from the eyes of the to the parts of the brain that makes sense of the influence, the raw sensor information hiring higher order representations of formed. This is the inspiration, the idea behind using deep neural networks for images higher and higher order representation is a form for the layers,

Speaker 1:          04:19          the early layers taking in the very raw in sensory information that extracting edges, connecting those edges, forming those edges to form more complex features and finally into the higher order semantic meaning that we hope to get from these images and computer vision. Deep learning is hard. I'll say this again. The illumination variability is the biggest challenge, or at least one of the, one of the biggest challenges in driving for visible light cameras pose variability. The objects, as I'll also discuss about some of the advances from Geoff Hinton and the capsule networks. The idea with neural networks as they are currently used for computer vision are not good with representing variable pose. These objects in images and it's too deep. Plane of color and texture look very different numerically when the object is rotated and the object is mangled and shaped in different ways. The deformable will truncate a cat interclass variability. The classification task, which would be an example today throughout to introduce some of the networks over the past decade that have received success and some of the intuition and insight that made those networks work. Classification, there is a lot of variability inside the classes and very little variability between the classes.

Speaker 1:          05:57          All of these cats

Speaker 2:          05:58          at top, all those are dogs a bottom. They look very different and the other, I would say the second biggest problem in driving perception, visible light camera perceptions, occlusion when part of the object is occluded due to the three dimensional

Speaker 1:          06:15          nature of our world, some objects in front of others and they occlude the background object. And yet we're still tasked with identifying the object when only part of it is visible. And sometimes that part I told you there's cats is very hardly visible here. We're tasked with classifying a cat with just an ears visible, just the leg and on a philosophical level as we'll talk about the motivation for our competition here. Here's a, a, a, a cat dressed as a monk, eating a banana on a philosophical level. Most of us, uh, understand what's going on in the scene. In fact, a neural network to today successfully classify this, uh, image this video as a cat, but the context, the humor of the situation, and in fact you could argue it's a monkey is missing. And what else is missing is the dynamic information, the temporal dynamics of the scene.

Speaker 1:          07:34          That's what's missing in a lot of the perception work that has been done to date in the autonomous vehicle space, uh, in terms of visible light cameras and we're looking to expand on that. That's what segue. Fuse is all about. Image classification pipeline. There's a been with different categories inside each class. Cat, dog Mug, hat, those bins. There's a lot of examples of each and your task with when a new example comes along you've never seen before to put that image in a bin. It's the same as the machine learning task before and everything relies on the data that has been ground truth, that have been labeled by human beings. Amnesty as a toy data set of handwritten digits, often using as examples and Coco psyfari image net places, and a lot of other incredible datasets. Rich data sets of 100 thousands, millions of images out there represent scenes, people's faces and different objects.

Speaker 1:          08:39          Those are all ground truth data for testing algorithms and for competing architectures to be evaluated against each other. Cfr Ten, one of the simplest, almost toy datasets of tiny with 10 categories of airplane, automobile, Bird, cat, deere, dog, frog, course, ship and truck is commonly used to explore. Some of the basic convolutional neural networks will discuss, so let's come up with a very trivial, classify it to explain the concept of how we could go about it. In fact, this is maybe if you start to think about how to classify an image. If you don't know any of these techniques, this is perhaps the approach you would take is you would subtract images. So in order to know that an image of a cat is different than image of a dog, you have to compare them when given those two images, what? What's the what's the way you compare them? One way you could do it is you just subtract it and then some all the pixel wise differences in the image. Just subtract the intensity of the image pixel by Pixel. Sum It up if that intent, if that difference is really high, that means the image is a very different.

Speaker 1:          09:51          Using that metric, we can look at cfr 10 and use it as a classifier saying, based on this difference function, I'm going to find one of the 10 bins for a new image that that is cool, that has the lowest difference. Find an image in this dataset that is most like the image I have and put it in the same bin. Is that images in? So there's 10 classes. If we just flip a coin, the accuracy of our classifier will be 10 percent. Using our image difference classify, we can actually do pretty good. Much better than random was better than 10 percent. We can do 35, 38 percent accuracy. That's a classifier wherever first classifier, k nearest neighbors. Let's take our classifier to a whole new level instead of comparing it to just fight. Trying to find one image that's the closest in our dataset. We tried to find k closest and say what is what class do the majority of them belong to? And we take that K and increase it for one to two, to three, to four to five, and see how that changes the problem with seven years neighbors, which is the optimal under this approach for cfr 10,

Speaker 1:          11:20          we achieved 30 percent accuracy. Human level is 95 percent accuracy and convolutional neural networks will get very close to a 100 percent. That's were you on. That works shine this very task of binning images. It all starts with this basic computational unit signal in each of the signals are wade summed, bias added

Speaker 1:          11:55          and put an input into a nonlinear activation function that produces an output. The nonlinear activation function is key. All of these put together and more and more hidden layers form a deep neural network, and that deep neural network is trained as we've discussed by taking a forward pass on examples, have ground truth labels. Seeing how close those labels are too, the real ground truth, and then punishing the weights that resulted in the incorrect decisions and rewarding the weights that results in incorrect decisions. For the case of 10 examples, the output of the network is 10 different values. The input being handwritten digits from zero to nine, 10 of those and one of our network to classify what is in this image of a handwritten digit is at one zero, one, two, three through nine. The way it's often done is there's 10 outputs of the network and each of the neurons and the output is responsible for getting really excited when it's number is called and everybody else is supposed to be not excited. Therefore the number of classes is the number of outputs. That's how it's commonly done and you assign a class to the input image based on the highest, the neuron which produces the highest output,

Speaker 1:          13:36          but that's for a fully connected network that we've discussed on Monday.

Speaker 1:          13:42          There is in deep learning a lot of tricks that make things work that make training much more efficient on large class problems where there's a lot of classes on large data sets. When the representation that the neural network is tasked with learning is extremely complex and that's where convolutional neural network step in that trick. They use a spatial invariance. They use the idea that a cat in the top left corner of an image is the same as a cat in the bottom right corner of an image, so we can learn the same features across the image. That's where the convolution operation steps in. Instead of the fully connected networks here, there's a third dimension of depth, so the blocks in this neural network that as input take three d volumes in this output produced three d volumes.

Speaker 1:          14:46          They take a slice of the image, a window and it across applying this same exact weights and we'll go through an example, the same exact weights as in the fully connected network on the edges that are used to map the input to the output. Here are used to map the slice of an image, this window of an image to the output, and you can make several, many of such convolutional filters, many layers, many different options of what kind of features do you look for in an image or kind of window you slide across in order to extract all kinds of things. All kinds of edges, all kinds of higher order patterns and the images. The very important thing is the parameters on each of these filters, these subset of the image, these windows are shared. If the feature, the that defines a cat is useful in the top left corner, it's useful on the top right corner. It's useful in every aspect of the image. This is the trick that makes convolutional neural network save a lot of a lot of parameters. Reduced parameter significantly is the reuse, the spatial sharing of features across the space of the image.

Speaker 1:          16:12          The depth of these three d volumes is the number of filters. The stride is the skip of the filter, the step size, how many pixels you skip when you apply the filter to the input, and the padding is the padding, the zero padding on the outside of the input to a convolutional layer. Let's go through an example, so on the left here and the slides are available online. You can follow them along and I'll step through this example. On the left here is input volume of three channels. The left column is the input. The three blocks, the three squares. There are the three channels and there's numbers inside those channels, and then we have a filter in red,

Speaker 1:          17:12          two of them, two channels of filters with a bias, and we those filters are three by three. Each one of them is size three by three, and what we do is we take those three by three filters that are to be learned. These are variables are weights that will have to learn, and then we slide it across an image to produce the output on the right, the green, so by applying the filters in the red, there's two of them, and within each one there's one for every input channel, we go from the left to the right, from the input volume in the left to the output volume green on the right, and you can look at it. You can pull up the slides yourself. Now if you can't see the numbers on the screen, but the operations are performed on the input to produce the single value that's highlighted there in the green and the output, and we slide this convolution. No filter along the image with the stride in this case, have to skipping, skipping along. They some to the to the right, the two channel I'll put in green,

Speaker 1:          18:39          that's it. That's the convolutional operation. That's what's called a convolutional layer neural networks and the parameters here, besides the bias of the red values in the middle, that's what we're trying to learn and there's a lot of interesting tricks we'll discuss today on top of those, but this is at the core. This is the specialty. Invariant sharing of parameters that may convolution you're on that works. I'm able to efficiently learn and find patterns and images to build your intuition a little bit more about convolution. Here's an input image on the left and on the right, the identity filter produces the output you see on the right, and then there is different ways you can, different kinds of edges. You can extract with the activate where the resulting activation maps scene on the right, so when applying the filters, those edge detection filters to the image on the left you produce in white are the parts that activate the convolution. The results of these filters,

Speaker 1:          19:55          and so you can do any kind of filter. That's what we're trying to learn. Any kind of edge, any kind of any kind of pattern you can move along in this window and this way that's shown here. You slide around the image and you produce the output you see on the right and depending on how many filters you have in every level, you have many of the slices, the on the right, the input on the left, I'll put it on the right. If you have dozens of filters, you would have dozens of images on the right, each with different results that show where each of the individual filter patterns were found and we learn what patterns are useful to look for in order to perform the classification task. That's the task for the neural network to learn. These filters and the filters have higher and higher order of representation. Going from the very basic edges to the high semantic meaning. That spans entire images and the ability to span images can be done in several ways, but traditionally has been successfully done through Max pooling, through pooling of taking the output of a convolutional operation and reducing the resolution of that by by condensing that information, but for example, taking the maximum values, the maximum activations,

Speaker 1:          21:31          therefore reducing the spatial resolution which has detrimental effects as we'll talk about in scene segmentation, but it's beneficial for finding higher order representations and the images that bring images together that bring features together to form an entity that we're trying to identify and classify. Okay, so that forms a convolution. You'll networks such convolutional layers stacked on top of each other is the only addition to a neural network that makes for a convolutional neural network and then at the end the fully connected layers or any kind of other architectures allow us to apply particular domains. Let's take image net as a case study in image net dataset and image net. The challenge, the task is classification. As I mentioned, the first lecture, imaging as a data set, one of the largest in the world of images. The $14 million images, 21,000 categories and a lot of depth to many of the categories, as I mentioned, 1200 granny Smith apples. These allow to these allow them to learn the rich representations in both posed lighting variability and intercluster class variation for the particular things. Particular classes like Granny Smith apples, so let's look through the various networks. Let's discuss them. Let's see the insights. It started with Alex Net, the first really big successful GPU train neural network on image net that's achieved a significant boost over the previous year and moved onto vgg net, Google net, a Goo lanette resonate yet cua image and as ynet in 2017.

Speaker 1:          23:40          Again, the numbers will show for the accuracy or based on the top five error rate, we get five guesses and it's a one or zero. If you get guests, if one of the five is correct, you get a one for that particular guest. Otherwise it's a zero

Speaker 1:          24:01          and human error is five point one. When a human tries to achieve the same tries to perform the same task. As the machinist asks, we're doing the air is five point one, the human adaptation that's performed on the images based on binary classification, Granny Smith, apple are not cat or not the actual task that the machine has to perform and that the human competing has to perform, is given an image, is provide one of the many classes under that human error is five point one percent, which was surpassed in 2015 by residents yet, uh, to achieve four percent error. So let's start with Alex Net. I'll zoom in on the later networks. They have some interesting insights, but Alex Net and Vgg net both followed a very similar architecture, very uniform throughout its depth.

Speaker 1:          25:02          Vgg Net in 2014 is convolution convolution pooling, convolution pooling, convolution pooling, and fully connected layers at the end does a certain kind of beautiful simplicity uniformity to these architectures because you can just make a deeper and deeper and makes it very amenable to a implementation in the layer stack kind of way. And in any of the deep learning frameworks, it's clean and beautiful to understand. In the case of egg net 16 or 19 layers with 138 million parameters, not many optimizations on these parameters. Therefore the number of parameters is much higher than the networks that followed it. Despite the layers not being that large. Google net introduced the inception module, starting to do some interesting things with the small modules within these networks which allow for the training to be more efficient and effective. The idea behind the inception module shown here with the previous layer on bottom and the convolutional layer here with the inception module

Speaker 1:          26:16          on top producer on top is it used the idea that different size convolutions provide different value for the network. Smaller convolutions are able to capture or propagate forward features that are very local in high resolution and in, in, in texture. Larger convolutions are better able to to represent and capture and catch highly abstracted features, higher order features. So the idea behind the inception module is to say, well, as opposed to choosing an in a hyper parameter tuning process or architecture design process, choosing which convolution size we want to go with, why not do all of them? To get well several together in the case of the Google net model, there's the one by one, three by three and five by five convolutions with the old trusty friend of Max pooling still left in there as well, which has lost favor more and more over time for the image classification task, and the result is there's fewer parameters are required. If you pick the placing of these inception modules correctly, the number of parameters required to achieve a higher performance is much lower

Speaker 1:          27:46          radnet, one of the most popular still to date architectures that we'll discuss in and scene segmentation as well came up and use the idea of a residual block. The initial inspiring observation, which doesn't necessarily hold true as it turns out, but that network depth increases representation power, so these residual blocks allow you to have much deeper networks, and I'll explain why in a second here, but the thought was they worked so well because the networks how much deeper. The key thing that makes these blocks so effective is the same idea. It's very reminiscent of recurrent neural networks that I hope we get a chance to talk about the training of them as much easier. They will take a simple block repeated over and over, and they passed the input along with our transformation along with the ability to transform and to learn to learn the filters, learn the weights so you're allowed to. You're allowed every layer to not only take on the processing of previous layers, but to take in the wrong transform data and learn something new. The ability to learn something new allows you to have much deeper networks and the simplicity of this block allows for more effective training.

Speaker 1:          29:33          These state of the art in 2017, the winner is squeezing excitation networks. That unlike the previous year was cu image with shimply took ensemble methods and combined a lot of successful approaches to take a marginal improvement as seen. Net got a significant improvement, at least in percentages, I think as a 25 percent reduction in error

Speaker 1:          30:00          from four percent to three percent. Something like that. By using a very simple idea that I think is important to mention as simple insight. It added a parameter to each channel and the convolutional layer and the convolutional block, so the network can now adjust the weighting and each channel based for for each feature map, based on the content, based on the input to the network. This is kind of a takeaway to think about about any of the networks who talk about any of the architectures is a lot of times you were recurrent neural networks and convolution neural networks have tricks that significantly reduced the number of parameters, the bulk, the sort of low hanging fruit. They use spatial invariants of temporal and to reduce the number of parameters to represent the input data, but they also lead certain things not parametrized. They don't allow the network to learn it. Allowing this case to network to learn the waiting on each of the individual channels. So each of the individual filters is something that you learn as a long with the filters takes. It makes a huge boost. The cool thing about this is it's applicable to any architecture. This kind of block, this kind of what the squeeze and excitation block is applicable to any architecture.

Speaker 1:          31:30          And, uh, because obviously it's a, it's just simply a parametrized is the ability to choose which filter you go with based on the content. It's a subtle but crucial thing I think is pretty cool. And for future research it inspires to think about, uh, what else can be parametrized and neural networks, what else can be controlled as part of the learning process, including hiring higher order hyper parameters, which, which aspects of the training and the architecture of the network can be part of the learning. This is what this network inspires.

Speaker 1:          32:11          Another network has been in development since the nineties ideas but Jeff Hinton, but really received, has been published on received significant attention, 2017 that I won't go into detail here. Uh, we are going to release an online only video about capsule networks. It's a little bit too technical, but they inspire very important point that we should always think about deep learning. Uh, whenever it's successful is to think about what, as I mentioned with the cat eating a banana on a philosophical and the mathematical level, we have to consider what assumptions these networks make and what through those assumptions they throw away. So neural networks due to the spacial with convolutional neural networks due to their spatial invariance throwaway information about the relationship between the hierarchies between the simple and the complex objects. So the face on the left and the face on the right looks the same to accomplish in neural network.

Speaker 1:          33:23          The presence of eyes and nose and mouth is the essential aspect of what makes the classification task work for accomplished or network. It will what worry will fire and say this is definitely a face, but the spatial relationship is lost, is ignored. Which means there's a lot of implications to this. But for things like pose variation, that information is lost. We're throwing away that away completely and hoping that the pooling operation that's performing these networks is able to sort of Mesh everything together to come up with the features that are firing of the different parts of the face. That then come up with a total classification that is a face without representing really the relationship between these features at the low level and the high level at the low level of the hierarchy, the simple and the complex level. This is super exciting field now that's hopefully will spark developments of how we design your networks that are able to learn this, the rotational, the orientation and variance a as well.

Speaker 1:          34:40          Okay, so as I mentioned, you take these convolutional neural networks, chop off the final layer in order to apply to a particular domain, and that is what we'll do with fully convolutional neural networks. The ones that we tasked to segment the image at a pixel level. As a reminder, these networks through the convolutional process are really producing a heat map, different parts of the network and getting excited based on the different aspects of the image and so it can be used to do the localization of detecting, not just classifying the image but localized in the object. And it could do so at a pixel level. So the convolutional layers are doing the encoding process. They're taking the rich raw sensory information in the image and encoding them into

Speaker 1:          35:38          an interpretable set of features, a representation that can then be used for classification, but we can also then use a decoder up sample that information and produce a map like this fully convolution neural network segmentation, semantic segmentation, image segmentation. The goal is to, as opposed to classify the entire image, it can classify every single pixel, this pixel level segmentation you call her every single pixel with what that Pixel, what object that pixel belongs to in this two d space of the image, the two d projection, the, uh, in the image of a three dimensional world. So the thing is, there's been a lot of advancement in the last three years, but it's still an incredibly difficult problem if you think, if you think about, uh, the amount of data that's used for training and the task of Pixel level, uh, of megapixels here of millions of pixels that are tasked with having a, a single label, it's an extremely difficult problem. Why is this interesting, important problems to try to solve as opposed to Bonnie boxes around cats? Well, it's whenever precise boundaries of objects are important. Certainly medical applications when looking at imaging and detecting particular, for example, detecting tumors and, uh, in, in medical imaging of, of different, uh, uh, different organs

Speaker 1:          37:17          and in driving and robotics, when objects are involved as a dense scene of all the vehicles, pedestrians, cyclists, we need to be able to not just have a loose estimate of where objects are. We need to be able to have the exact boundaries and then potentially through data fusion, fusing sensors together, fusing this rich textual information about pedestrians, cyclists, and vehicles to light our data that's providing us the three dimensional map of the world or have both these semantic meaning of the different objects and their exact three mentioned location. Um, a lot of this work successfully, a lot of the work and the semantic segmentation started with fully convolutional networks for semantic segmentation, paper, fcn. That's where the name fcn came from in November 2014. Now go through a few papers here to give you some intuition where the field has gone and how that takes us to segue views, the segmentation competition. So fcn, repurpose the image, net pretreating nets the nets that were trained to classify what's in an image, entire image, and chopped off the fully connected layers. And then have added decoder parts that up, sample the image to produce a heat map here, showing a, uh, with a tabby cat, a heat map of where the cat is in the image. It's a much slower, much coarser resolution than the input image

Speaker 1:          39:00          one eighth at best. Skip connections to improve courses of upsampling. There's a few tricks. If you do the most naive approach, the upsampling is going to be extremely course because that's the whole point of the neural network. The encoding part is you throw away all the useless data, uh, the, you to the most essential aspects that represent that image. So you're throwing away a lot of information that's necessary to then form a high resolution image. So there's a few tricks where you skip a few of the final pooling operations to go in a similar way. And this is the residual block to a go to go to the output produced higher and higher resolution heat map at the end segment in 2015. I applied this to the driving context and really taking it to Kitty Dataset and have have have shown a lot of interesting results and really explored the encoder decoder or formulation of the problem. Really solidifying this the place of the encoder decoder framework for the segmentation task dilated convolution. I'm taking you through a few components which are critical here to the state of the art dilated convolutions, so the convolution operation as the pooling operation reduces resolution significantly and dilated convolution has a certain kind of gritting as visualized there that maintains the the local high resolution textures while still capturing the spatial window necessary. It's called dilated convolutional layer and that's in a 2015 paper, proved to be much better at upsampling. A high resolution image.

Speaker 1:          41:11          The lab with a B v One v Two v three added conditional random fields, which is the final piece of the of the state of the art puzzle here. A lot of the successful networks today that do segmentation, not all do post process using a crs conditional random fields, and what they do is they smoothed the segmentation, the ups sampled segmentation that results from the fcn by looking at the underlying image intensities.

Speaker 1:          41:50          So that's the key aspects of the successful approaches. Today. You have the encoder decoder framework of a fully accomplished in your network. It replaces the fully connected layers with a convolutional layers, dee dee convolutional layers, and as the years progressed from 2014 to today as usual, the underlying networks from Alex Net to Vgg net and to now Raza net have been one of the big reasons for the improvements of these networks to be able to perform the segmentation. So naturally they mirrored the image, net challenge performance and adapting these networks. So the state of the art uses resonate or similar networks conditional random fields for smoothing based on the input image intensities and the dilated convolution that maintains the computational cost, but increases the resolution of the upsampling throughout the intermediate feature maps. And that takes us to the state of the art that we used to produce the images to produce the images for the competition. Brezhnev do you see for dance upsampling convolution instead of by linear upsampling, you make the upsampling learnable.

Speaker 1:          43:18          You learned the upscaling filters that's on the bottom. That's really the key part that made it work. There should be a theme here. Sometimes the biggest addition, they can be done. This parametrizing one of the aspects of the network that you've taken for granted. Letting the network learn that aspect and the other. I'm not sure how important it is to the success, but it's a. it's a cool little addition, is a hybrid dilated convolution. As I showed that visualization where the convolution is spread apart a little bit at in the input from the input to the output, the steps of that dilated convolution filter, when they are changed, it produces a smoother results because when it's kept the same, are there certain input? Pixels get a lot more attention than others, so losing that favoritism as well. It's achieved by using a variable difference dilation rate. Those are the two tricks, but really the biggest one is the parametric, one of the upscaling filters.

Speaker 1:          44:27          Okay, so that's what we're. That's what we use to generate that data and that's we provide you the code with if you're interested in competing in psych fuse. The other aspect here that everything we've talked about, ball from the classification to the segmentation to making sense of images is there the information about a time the temporal dynamics of the scene is thrown away and for the driving context of the robotics contests and what we'd like to do is take fuse for the segmentation dynamic scene segmentation context of when you try to interpret what's going on in and seen over time and use that information. Time is essential. The the movement of pixels is essential through time. That that understanding how those objects move in a three d space through the two d projection of an image is fascinating and there's a lot of set of open problems there. So flow is what's very helpful too as a starting point to help us understand how these pixels move flow, optical flow, dance optical computation that are best at our best approximation of where each pixel in image one

Speaker 1:          45:55          and moved in temporarily following image. After that, there's two images in 30 frames. A second is one image at time zero, the other is 33 point three milliseconds later, and the dense optical flow is our best estimate of how each pixel in the input image moved to end. The output image and the optical flow for every pixel produces a direction of where we think that pixel moved and the magnitude of how far moved that allows us to take information that we detected about the first frame and try to propagate it forward. This is the competition is to try to segment and image and propagate that information forward for manual annotation of a of an image. So this kind of coloring book annotation, will you call every single pixel in the state of the art data set for driving city scapes that it takes one point five, one point five hours, 90 minutes to do that coloring, that's 90 minutes per image. That's extremely long time. That's why it doesn't exist today. Dataset. And in this class we're going to create one

Speaker 1:          47:15          of segmentation of these images is through time, through video, so long videos where every single frame is fully segmented. That's still an open problem that we need to solve flows, a piece of that. And we also provide you the this computer state of the art flow using flow net two point. Oh, so flow net one point, oh, in May 2015, use neural networks to learn the optical flow, the dense optical flow. And it did so with two kinds of architectures. Flow net s flown out, simple and flown on. Core flow in that. See the simple one is simply taking the two images. So what's, what's the task here? There's two images and you want to produce in those two images, they follow each other in time, 33 point three milliseconds apart, and uh, your task is the Zl output to produce the dense optical flow. So for the simple architecture, you just stack them together, each our rgb, so it produces a six channel input to the network. There's a lot of convolution and finally it's the same kind of process as the fully convolutional neural networks to produce the optical flow. Then there is flown at correlation architecture where you performed some convolutions separately before using a correlation later to combine the feature maps.

Speaker 1:          48:48          Both are effective and different datasets and different applications. So flow net two point zero in December 2016 is one of the state of the art frameworks code bases that we use to generate the data. I'll show combines the flown at sm, flown as C and improves over the initial flow net. Producing a smoother flow field preserves the fine motion detail along the edges of the objects and it runs extremely efficiently depending on the architecture as a few variants, either eight to 140 frames a second, and the process there is essentially one that's common across various applications. Deep learning is stacking these networks together. The very interesting aspect here that we're still exploring, and again applicable in all of deep learning in this case, it seemed that there was a strong effect in taking sparse small, multiple Dataset and doing the training, the order of which those datasets were used for the training process mattered a lot. That's very interesting. So using flow net two point zero, here's the Dataset we're making available for psych fuse. The competition cars that mit did, you use less sex views first. The original video, US driving in high definition 10 ADP and a eightK , three 60 video

Speaker 1:          50:31          original video driving around a cambridge. Then we're providing the ground truth for a training set for that training set for every single frame. 30 frames a second, we're providing the segmentation frame to frame to frame segmented on mechanical Turk. We're also providing the output of the network that I mentioned, the dad, there are segmentation network that's pretty damn close to the ground truth, but still not. And our task is. This is the interesting thing is our task is to take the output of this network. Well there's two options. One is to take the output of this network and use CA use other networks to help you propagate the information better. So what this segmentation, the output of this network does is it only takes a frame by frame by frame. It's not using the temporal information at all. So the question is can we figure out a way, can we figure out tricks to use temporal information to improve this segmentation? So it looks more like this segmentation and we're also providing the optical flow from frame to frame to frame. So the optical flow based on flown at two point zero of how each of the pixels moved.

Speaker 1:          52:07          Okay. And now forms the psych fuse competition. 10,000 images. And the task is to submit code. We have starter code in python and on good hub to take in the original video, take in for the training, set the ground truth, the segmentation from the state of the art segmentation network, the optical flow from the state of the art optical flow network, and taking that together to improve the. The stuff on the bottom left, the segmentation to try to achieve the ground truth on the on the top right. Okay. With that, I'd like to thank you. Tomorrow at one PM is Waymo in data 32, one, two, three. The next lecture, next week we'll be on deep learning for a sensing the human understanding the human, and we will release online only lecture on capsule networks and Gans generative adversarial networks. Thank you very much.