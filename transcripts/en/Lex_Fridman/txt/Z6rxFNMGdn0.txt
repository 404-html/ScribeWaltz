Speaker 1:          00:00:00       The following is a conversation with Ian Goodfellow. He is the author of the popular textbook on deep learning, simply titled Deep Learning. He coined the term of generative adversarial networks, otherwise known as Ganz and with his 2014 paper is responsible for launching the incredible growth of research and innovation. In this subfield of deep learning. He got his bs and ms at Stanford, his Phd at University of Montreal with Yoshua Bengio and Aaron Kerrville. He held several research positions including an open AI, Google brain and now at apple as the director of machine learning. This recording happened while Ian was still a Google brain, but we don't talk about it and you think specific to Google or any other organization. This conversation is part of the artificial intelligence podcast. If you enjoy, subscribe on Youtube, iTunes or simply connect with me on Twitter at Lex Friedman, spelled f, r I. D. And now here's my conversation with Ian Goodfellow. You Open your popular deep learning book with a Russian doll type diagram that shows deep learning is a subset of representation learning, which in turn is a subset of machine learning. And finally a subset of Ai. So this kind of implies that there may be limits to deep learning in the context of Ai. So what do you think is the current limits of deep learning and a, are those limits something that we can overcome with time?

Speaker 2:          00:01:35       Yeah, I think one of the biggest limitations of deep learning is the right now it requires really a lot of data, especially labeled data. There are some unsupervised and semi supervised learning algorithms that can reduce the amount of label data you need, but they still require a lot of unlabeled data. Hmm. Reinforcement learning algorithms, they don't need labels, but they need really a lot of experiences. Um, as human beings we don't learn to play Pong by failing at park 2 million times. So just getting the generalization ability better is one of the most important bottlenecks in the capability of the technology today. And then I guess I'd also say deep learning is like a component of a bigger system. Um, so far nobody has really proposing to have, uh, only what you'd call deep learning as the entire ingredient of intelligence. You use deep learning as sub modules of other systems, like Alphago has a deep learning model that estimates the value function. Um, most reinforcement learning algorithms have a deep learning module that estimates which action to take next, but he might have other components if you're basically building a function estimator. Do you think it's

Speaker 1:          00:02:47       possible? He said nobody's kind of been thinking about this so far, but do you think you're on that works could be made to reason in the way symbolic systems did in the eighties and nineties to do more, create more like programs

Speaker 2:          00:03:00       as opposed to functions? Yeah, I think we already see that a little bit. I already kind of think of neural nets as a kind of of program. I think of deep learning as basically learning programs that have more than one step. So if you'd draw a flow chart or, or if you draw a tensor flow graph describing your machine learning model, I think of the depth of that graph is describing the number of steps that run in sequence and then the width of that graph has a number of steps that run in parallel now. It's been long enough that we've had deep learning working that it's a little bit silly to even discuss shallow learning anymore. But back when I first got involved in Ai, when we used machine learning, we were usually learning things like support vector machines. You could have a lot of input features to the model and you could multiply each feature by a different weight.

Speaker 2:          00:03:48       All those multiplication locations were done in parallel to each other and there wasn't a lot done in series. I think. Well we got with deep learning was really the ability to have, uh, steps of a program that run in sequence. And I think that we've actually started to see that what's important with deep learning is more the fact that we have a multistep program rather than the fact that we've learned to representation. If you look at things like a resonance, for example, they take one particular kind of representation and they update it several times back when deep learning first really took off in the academic world in 2006 when Geoff Hinton showed that you could train deep belief networks, everybody who was interested in the idea thought of it as each layer learns a different level of abstraction, that the first layer trained on images, learn something like edges and the second layer learns corners.

Speaker 2:          00:04:40       And eventually you get these kind of grandmothers sell units that recognize specific objects. Today I think most people think of it more as a computer program where as you add more layers, you can do more updates before you output your final number. But I don't think anybody believes that layer 150 of their resume yet. Uh, is a grandfather, grandmother cell and you know, layer 100 is contours or something like that. Okay. So you think you're not thinking of it as a singular representation that keeps building, you think of it as a program, sort of almost like a state representation has a state of understanding. Yeah, I think of it as a program that makes several updates and arrives at better and better understandings, but it's not replacing the representation at each step. It's refining it and in some sense that's a little bit like reasoning. It's not reasoning in the form of deduction, but it's reasoning in the form of taking a thought and refining it and refining it carefully until it's good enough to use. Do you think, and I hope you don't mind, we'll jump philosophical every once in a while. Do you think a of cognition, human cognition or even consciousness as simply a result of this kind of a sequential sequential representation learning,

Speaker 1:          00:05:58       do you think that can emerge? Okay.

Speaker 2:          00:06:00       Cognition? Yes, I think so. Consciousness, that's really hard to even define what we mean by that. I guess there's consciousness is often defined as things like having self awareness and that's relatively easy to turn into something actionable for a computer scientist to reason about people also defined find consciousness in terms of having qualitative states of experience like qual, Leah, right? There's all these philosophical problems, like, could you imagine a Zombie who does all the same information processing as a human but doesn't really have the qualitative experiences that we have, that sort of thing. I have no idea how to formalize or turn it into a scientific question. I don't know how you could run an experiment to tell whether a person is a Zombie or not. And similarly, I don't know how you could run an experiment to tell whether an advanced AI system had become conscious in the sense of quality or not.

Speaker 1:          00:06:52       But in the more practical sense, like almost like self attention, you think consciousness and cognition can in an impressive way emerge from current types of architectures?

Speaker 2:          00:07:03       Sure. Yeah. Or, or if, if you think of consciousness in terms of self awareness and just, um, making plans based on the fact that the agent itself exists in the world reinforcement learning algorithms that are already more or less forced to muddle the agent's effect on the environment so that that more limited version of consciousness is already something that we get limited versions of with reinforcement learning algorithms if they're trained well. But, uh,

Speaker 1:          00:07:36       you say limited. So the, the big question really is how you jumped from limited to human level. Yeah. Right. And, uh, whether it's possible the, you know, the, even just building common sense reasoning seems to be exceptionally difficult. So Ken, if we scale things up, if we get much better on supervised learning, if we get better at labeling, if forget, get bigger datasets, uh, more compute, do you think we'll start to see really impressive things that go from limited to, you know, uh, something echoes of human level cognition?

Speaker 2:          00:08:10       I think so. Yeah. I'm, I'm optimistic about what can happen just with more computation and more data. Uh, I do think it'll be important to get the right kind of data. Today, most of the machine learning systems we train are mostly trained on one type of data for each model. But the human brain, we get all of our different senses and we have many different experiences like, you know, riding a bike, driving a car, or talking to people, reading. Um, I think when you get that kind of integrated dataset working with a machine learning model that can actually close the loop and interact, we may find that algorithm's not so different from what we have today. Learn really interesting things when you scale them up a lot and train them on a large amount of multimodal data. And so multimodal is really interesting. But within like your work in a adversarial examples.

Speaker 2:          00:09:03       So selecting within modal, within a one mode of data, uh, selecting better at what are the difficult cases when we share most of these slow to learn from. Oh yeah. Like could we could each get a whole lot of mileage out of uh, designing a model that's resistant to adversarial examples or something like that. But my thinking on that has evolved a lot over the last few years. When I asked him, when I first started to really invest in studying adversarial examples, I was thinking of it mostly as adversarial examples reveal a big problem with machine learning. And we would like to close the gap between how machine learning models respond to adversarial examples and how humans respond after studying the problem more. I still think that adversarial examples are important. I think of them now more of as a security liability then as an issue that necessarily shows there's something uniquely wrong with machine learning as opposed to humans.

Speaker 2:          00:10:02       Also. Do you see them as a tool to improve the performance of the system? Not, not on the security side, but literally just accuracy. I do see them as a kind of tool on that side, but maybe not quite as much as I used to think. Ah, we've started to find that there's a trade off between accuracy on adversarial examples and accuracy on cleaning samples back in 2014 when I did the first adversarily trained classifier that showed resistance to some kinds of adversarial examples. It also got better at the clean data on omnist and that's something we've replicated several times and feminist, uh, that when we train against weak adversarial examples, Agnes classifiers get more accurate so far that hasn't really held up on other datasets and hasn't held up. When we train, I get against stronger adversaries. It seems like when you confront a really strong adversary, uh, you tend to have to give something up.

Speaker 2:          00:10:58       Here's your thing. But it's such a compelling idea cause it feels, uh, it feels like that's how us humans learn the difficult cases. We try to think of what would it be, screw up and then we make sure we fixed that. Yeah. It's also in a lot of branches of engineering, you do a worst case analysis and make sure that your system will work in the worst case and then that guarantees that it'll work. And all of the messy average cases that happen when you go out into a really randomized world. Yeah. With driving with autonomous vehicles, there's seems to be a desire to just look for think. Adversarially tried to figure out how to mess up the system. And if you can be robust to all those difficult cases, then you can, it's a hand wavy, empirical way to show your system is a, yeah.

Speaker 2:          00:11:46       Yeah. Today, most adversarial example research isn't really focused on a particular use case, but there are a lot different use cases where you'd like to make sure that the adversary can interfere with the operation of your system. Like in finance, if you have an algorithm making trades for you, people go to a lot of an effort to obfuscate their algorithm. That's both to protect their IP because you don't want to research and develop a profitable trading algorithm than have somebody else capture the gains, but it's at least partly because you don't want people to make adversarial examples that full your algorithm and to making bad trades or I guess one area that's been popular in the academic literature is speech recognition. If you use speech recognition to hear an audio wave form and then in turn that into a command that a phone executes for you, you don't want a malicious adversary to be able to produce audio. That gets interpreted as malicious commands, especially if a human in the room doesn't realize that something like that is happening in speech recognition. Has there been much success

Speaker 1:          00:12:53       in being able to uh, create adversarial examples that fool the system?

Speaker 2:          00:12:59       Yeah, actually I guess the first work that I'm aware of is a paper called hidden voice commands that came out in 2016 I believe. And they were able to show that they could make sounds that are not understandable by a human, but are recognized as the target phrase that the attacker wants the phone to recognize it as. Since then things have gotten a little bit better on the attacker side. When worse on the defender side, it's become possible to make sounds that sound like normal speech but are actually interpreted as a different sentence than the human. Here's the level of perceptibility of the adversarial perturbation is still kind of high, uh, that when you listened to the recording, it sounds like there's some noise in the background just like rustling sounds. But those wrestling sounds are actually the adversarial perturbation that makes the phone here are completely different sentence. Yeah. That's so fascinating.

Speaker 1:          00:14:00       Peter Norvig mentioned that you're writing the deep learning chapter for the fourth edition of the artificial intelligence and modern approach book, so how do you even begin a summarizing? They feel the deep learning in a chapter?

Speaker 2:          00:14:13       Well, I am. In my case, I waited like a year before I actually read anything. I think it has it even having written a full length textbook before, it's still a pretty intimidating to try to start writing just one chapter that covers everything. One thing that helped me make that plan was actually the experience of, and having written the full book before and then watching how the field changed after the book came out, I realized there's a lot of topics that were maybe extraneous and the first book and just seeing what stood the test of a few years of being published and what seems a little bit less important to have included now help me pare down the topics I wanted to cover for the pork. It's also really nice now that the field has kind of stabilized to the point where some core ideas from the 1980s are still used today.

Speaker 2:          00:15:04       When I first started studying machine learning, almost everything from the 1980s had been rejected and now some of it has come back. So that stuff that's really stood the test of time is what I focused on putting into the book. There is also I guess two different philosophies about how you might write a book, a one philosophy philosophies. He tried to write a reference that covers everything and the other philosophy is you try to provide a high level summary that gives people the language to understand a field and tells them what the most important concepts are. The first deep learning book that I wrote with the, and Erin was somewhere between the, the two philosophies that it's trying to be both a reference and an introductory guide. Uh, writing this chapter for Russell and Norvig's book. I was able to focus more on just a concise introduction of the key concepts and the language you need to read about them more.

Speaker 2:          00:15:55       And a lot of cases actually just wrote paragraphs that said, here's a rapidly evolving area that you should pay attention to. It's, it's pointless to try to tell you what the latest and best version of a, you know, learn to learn model is, um, you know, I can, I can point you to a paper that's recent right now, but there isn't a whole lot of a reason to delve into exactly what's going on with the latest learning to learn approach or the latest module produced by learning to learn algorithm. You should know that learning to learn as a thing and that it may very well be the source of the latest and greatest convolutional net or recurrent net module that you would want to use in your latest project. But there isn't a lot of point in trying to summarize exactly which architecture in which learning approach got to which level of performance.

Speaker 1:          00:16:44       So you may be focused more on the basics of the methodology. So from backpropagation to feed forward to recurrent neural networks, convolutional, that kind of thing. Yeah. Yeah. So if I were to ask you, I remember I took a algorithms and data structures algorithms. So of course I remember the, the professor asked what is an algorithm and a yelled at everybody in a good way that nobody was answering it correctly. Everybody knew what the Alpha, it was a graduate course. Everybody knew what an algorithm was, but they weren't able to answer it. Well. So let me ask you, uh, in that same spirit, what is deep learning?

Speaker 2:          00:17:24       I would say deep learning is any kind of machine learning that involves learning parameters of more than one consecutive step. So that would mean shallow learning is things where you learn a lot of operations that happen in parallel. You might have a system that makes multiple steps, um, and like you might have had designed extractors, uh, but really only one step is learning. Deep learning is anything where you have multiple operations in sequence and that includes the things that are really popular today, like convolutional networks and recurrent networks. Uh, but it also includes some of the things that have died out, uh, like bolts and machines where we weren't using backpropagation today. I hear a lot of people define deep learning as gradient descent applied to these differentiable functions. And I think that's a legitimate usage of the term. It's just different from the way that I used the term myself. So what's an example

Speaker 1:          00:18:29       of a deep learning that is not Grady and descendant differentiable functions in your, I mean, not specifically perhaps, but more even looking into the future. What, what, what's your thought about that space of approaches?

Speaker 2:          00:18:44       Yeah, so I tend to think of machine learning algorithms is decomposed into really three different pieces. There's the model which can be something like a neural nut or a bolt and machine or a recurrent model. And that basically just describes how do you take data and how do you take parameters and you know, what function do you use to make a prediction given the data on the parameters. Um, another piece of the learning algorithm is the optimization algorithm. Or not every algorithm can be really described in terms of optimization. But what's the algorithm for updating the parameters or updating whatever the state of the network is? Uh, and then the, the last part is the, the Dataset. Like how do you actually represent the world as it comes into your machine learning system? Um, so I think of deep learning as telling us something about what does the model look like and basically to qualify as deep, I say that it just has to have multiple layers that can be multiple steps in a feed forward differentiable computation that can be multiple layers in a graphical model.

Speaker 2:          00:19:52       There's a lot of ways that you could satisfy me that something has, uh, multiple steps that are each parameterized separately. I think of gradient descent as being all about that other piece. The how do you actually update the parameters piece? So you could imagine having a deep model like a convolutional net and training it with something like evolution or a genetic algorithm. And I would say that still qualifies as deep learning. Uh, and then in terms of models that aren't necessarily differentiable, uh, I guess bolts and machines are probably the main example of something where you can't really take a derivative and use that for the learning process. Uh, but you, you can still argue that the model has many steps of processing that it applies when you run in France in the model.

Speaker 1:          00:20:35       So the steps of processing, that's key. So Geoff Hinton suggest that we need to throw away back pop backpropagation and start all over. What do you think about that? What could an alternative direction of training neural networks look like?

Speaker 2:          00:20:50       I don't know that backpropagation is going to go away entirely. Most of the time when we decided that a machine learning algorithm isn't on the critical path to research for improving AI, the algorithm doesn't die. It just becomes used for some specialized set of things. A lot of algorithms like logistic regression don't seem that exciting to AI researchers who are working on things like speech recognition or autonomous cars today. But there's still a lot of use for logistic regression and things like analyzing really noisy data in medicine and finance or um, making really rapid predictions in really time limited contexts. So I think, I think uh, backpropagation and gradient descent are around to stay but they may not end up being, um, everything that we need to get to real human level or superhuman AI. Are you optimistic?

Speaker 1:          00:21:43       Bought us discovering s you know, backpropagation has been around for a few decades so uh, I optimistic about us as a community being able to discover something better.

Speaker 2:          00:21:56       Yeah, I am I think, I think we likely will find something that works better. You could imagine things like having stacks of models where some of the lower level models predict perimeters of the higher level models. And so at the top level you're not learning in terms of literally calculate ingredients, but just predicting how different values will perform. You can kind of see that already in some areas like Basie and optimization where you have a Gaussian process that predicts how well different parameter values will perform. We already use those kinds of algorithms for things like hyper parameter optimization and in general we know a lot of things other than back prop that worked really well for specific problems. The main thing we haven't found is a way of taking one of these other non back prop based algorithms and having it really advanced the state of the art on an AI level problem.

Speaker 2:          00:22:46       Right. But I, I, I wouldn't be surprised if eventually we find that some of these algorithms that even the ones that already exist, not even necessarily a new one, we might find some way of customizing one of these algorithms to do something really interesting at the level of cognition or, or the, the level of, um, I think one system that we really don't have working quite right yet is uh, like short term memory. We have things like Lstm, they're called long short term memory. Uh, they still don't do quite what a human does with short term memory. Um, like gradient descent to learn a specific fact has to do multiple steps on that fact. Like, if I, I tell you, the meeting today is at 3:00 PM, um, I don't need to say over and over again. It's at 3:00 PM 3:00 PM. It's at 3:00 PM. It's at 3:00 PM for you to do a great step on each one.

Speaker 2:          00:23:40       You just hear it once and you remember it. Um, there's been some work on things like, uh, self attention and attention, like mechanisms like the neural Turing machine that can write to memory cells and update themselves with facts like that right away. But I don't think we've really nailed it yet. And that's one area where I'd imagine that new optimization algorithms or different ways of applying existing optimization algorithms could give us a way of just lightening fast updating the state of a machine learning system to contain a specific fact like that without needing to have it presented over and over and over again.

Speaker 1:          00:24:16       So some of the success of symbolic systems in the 80s is they were able to assemble these kinds of facts, uh, better, but there's a lot of expert input required and it's very limited in that sense. Do you ever look back to that as a something that will have to return to eventually sort of dust off the book from the shelf and uh, think about how we build knowledge, representation, knowledge, you have to use graph searches and searches, right? And like first order logic and entailment and things like that. A thing. Yeah, exactly.

Speaker 2:          00:24:49       In my particular line of work, which has mostly been machine learning, security and, and also generative modeling, I haven't usually found myself moving in that direction for generative models. I could see a little bit of, it could be useful if you had something like a, a differentiable, uh, knowledge base or some other kind of knowledge base where it's possible for some of our fuzzy or machine learning algorithms to interact with the knowledge base. I mean, you're on that one.

Speaker 1:          00:25:17       It's kind of like that. It's a differentiable knowledge base of sorts.

Speaker 2:          00:25:21       Yeah. But if we had a really easy way of giving feedback to machine learning models, that would clearly help a lot with, with generative models. And so you could imagine one way of getting there would be get a lot better at natural language processing. But another way of getting there would be a tick, some kind of knowledge base and figure out a way for it to actually interact with a neural network, being able to have a chat, we'll then y'all network. Yeah. So like one thing and generative models we see a lot today is you'll get things like faces that are not symmetrical, like people that have two eyes that are different colors. And I mean there are people with eyes that are different colors in real life, but not nearly as many of them as you tend to see in the machine learning generated data.

Speaker 2:          00:26:06       So if you had either a knowledge base that could contain the fact, uh, people's faces are generally approximately symmetric and eye color is especially likely to be the same on both sides. Uh, being able to just inject that hint into the machine learning model without having to discover that itself after studying a lot of data would be a really useful feature. I could see a lot of ways they're getting there without bringing back some of the 1980s technology. But I also see some ways that you could imagine extending the 1980s technology to play nice with neural nets and have it helped get there.

Speaker 1:          00:26:40       Awesome. So, uh, he talked about the story of you coming up with idea of gans at a bar with some friends. You were arguing that this, you know, uh, gans would work generative adversarial networks and the others didn't think so. Then you went home at midnight, code it up and it worked. So if, if I was a friend of yours, a at the bar, I would also have doubts. It's a really nice idea, but I'm very skeptical that it would work. Uh, what was the basis of their skepticism? What was the basis of your intuition? Why should work?

Speaker 2:          00:27:14       I don't want to be somebody who goes around to promoting alcohol for the message of science, but in this case, I do actually think that drinking helped a little bit. When your inhibitions are lowered, you're more willing to try out things that you wouldn't try out otherwise. So I have noticed in general that I'm less prone to shooting down. So am I an idea is when I'm, when I have had a little bit to drink. I think if I had had that idea at lunch time, yeah it probably would have thought it. It's hard enough to train wonder. I'll net you can't train a second neural net in the inner loop of the outer neural net. That was basically my friends objection. Was that trying to train to neural nets at the same time, we'd be too hard. So it was more about the training

Speaker 1:          00:27:55       process and less so my skepticism would be, you know, I'm sure you could train it, but uh, the thing would converge to, would not be able to generate anything reasonable and, and, and you kind of reasonable realism.

Speaker 2:          00:28:08       Yeah. So, so part of what all of us were thinking about when we had this conversation was deep bolts and machines, which a lot of us in the lab, including me, we're a big fan of deep Bolton machines at the time. They involved two separate processes running at the same time. One of them is called the positive phase where you load data into the model and tell the model to make the data more likely. The other one is called the negative phase where you draw samples from the model until the model to make those samples less likely. Um, in a deep bolts and machine, it's not trivial to generate a sample. You have to actually run an iterative process that gets better and better samples coming closer and closer to the distribution the model represents. So during the training process, you're always running these two systems at the same time, uh, one that's updating the parameters of the model and another one that's trying to generate samples from the model.

Speaker 2:          00:29:01       And they worked really well and things like [inaudible], but a lot of it's in the lab, including me, had tried to get people to machines to scale past feminist to things like generating color photos and we just couldn't get the two processes to stay synchronized. Um, so when I had the idea for gains, a lot of people thought that the discriminator would have more or less the same problem as the negative phase. And the bolts and machine that trying to train the discriminator in the inner loop, you just couldn't get it to keep up with the generator and the outer loop and that would prevent it from converging to anything useful. Yeah, I share that intuition. Yeah. Um, well it turns out to not be the, a lot of the time with machine learning algorithms, it's really hard to predict ahead of time how well they'll actually perform.

Speaker 2:          00:29:46       Right? You have to just run the experiment and see what happens. And I would say I still today don't have like one factor I can put my finger on and say, this is why Ganz worked for photo generation and deep bolts and machines don't. There are a lot of theory papers showing that under some theoretical settings, the again algorithm does actually converge, but those settings are restricted enough that they don't necessarily explain the whole picture in terms of all the results that we see in practice. So taking a step back, can you, uh, in the same way as we've talked about deep learning, can you tell me what generative adversarial networks are? A, yeah. So generative adversarial networks are a particular kind of generative model. A generative model is a machine learning model that can train on some set of data. Like say you have a collection of photos of cats and you want to generate more photos of cats or you want to estimate a probability distribution over cats.

Speaker 2:          00:30:47       So you can ask how likely it is that some new images, a photo of a cat, um, gans are one way of doing this. Uh, some generative models are good at creating new data. Uh, other generative models are good at estimating that density function and telling you how likely particular pieces of data are to come from the same distribution as the training data. Gans are more focused on generating samples rather than estimating the density function. There are some kinds of gowns like slogan that can do both, but mostly gowns are about generating samples, uh, generating new photos of cats that look realistic. Uh, and they do that completely from scratch. It's analogous to human imagination. When again creates a new image of a cat. It's using a neural network to produce a cat that has not existed before. It isn't doing something like compositing photos together.

Speaker 2:          00:31:44       You're not, you're not literally taking the eye off of one cat and the Erath off of another cat. It's, it's more of this digestive process where the, the neural net trains in a lot of data and comes up with some representation of the probability distribution and generates entirely new cats. There are a lot of different ways of building a generative model. What's specific to Ganz is that we have a two player game and the game theoretic sense and as the players in this game compete, one of them becomes able to generate realistic data. The first player is called the generator. It produces output data such as just images for example. And at the start of the learning process, it'll just produce completely random images. The other player is called the discriminator. The discriminator takes images as input and guesses whether they're real or fake. Uh, you train it both on real data.

Speaker 2:          00:32:34       So photos that come from your training set, actual photos of cats, and you try not to say that those are real. You also train it on images that come from the generator network and you train it to say that those are fake. As the two players compete in this game, the discriminator tries to become better at recognizing where their images are real or fake. And the genitor becomes better at fooling the discriminator into thinking that it's outputs are our real. Uh, and you can analyze this through the language of Game Theory and find that there's a nash equilibrium where the generator has captured the correct probability distribution. So in the cat example, it makes perfectly realistic cat photos and the discriminator is unable to do better than random guessing because all the, all the samples coming from both the data and the generator look equally likely to have come from either source.

Speaker 2:          00:33:25       So do you ever, do you ever sit back and just blow your mind that this thing works so from very, so it's able to estimate that Desi function enough to generate, generate realistic images? I mean, uh, yeah. Do you ever sit back? How does this even, why this is quite incredible, especially where against have gone in terms of realism. Yeah. And, and not just a flutter in my own work, but generative models all over them have this property that if they really did what we asked them to do, they would do nothing but memorize the training data, right? Um, models that are based on maximizing the likelihood, the way that you obtain the maximum likelihood for a specific training set is you assign all of your probability mass to the training examples and nowhere else, uh, for gans that game is played using a training set. So the way that you become unbeatable in the game is you literally memorized training examples.

Speaker 2:          00:34:25       One of my former interns read a paper, uh, his name is if I should not have an Auger Rajan. And he showed that it's actually hard for the generator to memorize the try and get a heart and uh, a statistical learning theory sense that you can actually create reasons for why it would require quite a lot of learning steps and a lot of observations of, of different latent variables before you could memorize the training data. That still doesn't really explain why when you produce samples that are new, why do you get compelling images rather than just garbage? That's different from the training set. And I don't think we really have a good answer for that, especially if you think about how many possible images are out there and how few images the generative model sees during training. It seems just unreasonable that generative models create new images as well as they do, especially considering that we're basically training them to memorize rather than generalized.

Speaker 2:          00:35:25       Uh, I think part of the answer is there's a paper called deep image prior where they show that you can take a convolutional net and you don't even need to learn the parameters of it at all. You just use the model architecture and it's already useful for things like in painting images. I think that shows us that the convolutional network architecture captures something really important about the structure of images. And we don't need to actually use the learning to capture all the information coming out of the convolutional net. Uh, that would, that would imply that it would be much harder to make generative models and other domains. So far we're able to make reasonable speech models and things like that. But to be honest, we haven't actually explored a whole lot of different data sets all that much. We don't, for example, see a lot of deep learning models of, um, like biology datasets where you have lots of microarrays measuring the amount of different enzymes and things like that. So we may find that some of the progress that we've seen for images and speech turns out to really rely heavily on the model architecture. And we were able to do what we did for vision by trying to reverse engineer the human visual system. Right. Um, and, and maybe it'll turn out that we can't just, uh, use that same trick for arbitrary kinds of data.

Speaker 1:          00:36:43       Right? So there's aspects of the human vision system, the hardware of it that makes it without learning, with all cognition just makes it really effective at detecting the patterns we've seen. The visual world. Yeah. That's, yeah, that's really interesting. Uh, what, um,

Speaker 2:          00:36:59       in a big

Speaker 1:          00:37:01       quick overview, in your view and your view, what types of gangs are there and what other generative models besides Gans are there?

Speaker 2:          00:37:10       Yeah. Um, so it's maybe a little bit easier to start with. What kinds of generative models are there other than cans? Um, so most generative models are likelihood based where to train them. You have a model that tells you how, how much probability to signs to a particular example, and you just maximize the probability assigned to all the training examples. It turns out that it's hard to design a model that can create really complicated images, are really complicated audio wave forms and still have it be possible to estimate the likelihood function from a computational point of view. Uh, most interesting models that you would just write down. Intuitively it turns out that it's almost impossible to calculate the amount of probability they assigned to a particular point. Um, so there's a few different schools of generative models in the likelihood family. Uh, one approach is to very carefully designed the model so that it is computationally tractable to measure the density, to assign to a particular point.

Speaker 2:          00:38:15       So there are things like, uh, auto regressive models like Pixel, CNN, uh, those basically break down the probability distribution into a product over every single feature. So for an image, you estimate the probability of each pixel given all of the pixels that came before it. Uh, there's tricks where if you want to measure the density function, you can actually calculate the density for all these pixels, more or less in parallel, uh, generating the image still tends to require you to go one pixel at a time, and that can be very slow. Uh, but there again, tricks for doing this in a hierarchical pattern where you can keep the runtime under control or the quality of the images the generates putting runtime aside pretty good. Uh, they're, they're reasonable. Yeah. I would say a lot of the best results are from gans these days, but it can be hard to tell how much of that is based on who's studying which type of algorithm, if that makes sense.

Speaker 2:          00:39:17       The amount of effort and invest in empty. Yeah. Or, or like the kind of expertise. So a lot of people who have traditionally been excited about graphics or art and things like that have gotten interested in gans and to some extent it's hard to tell are gans doing better, uh, because they have a lot of graphics and art experts behind them or are against doing better because they're more computationally efficient or are against doing better because they prioritize the realism of samples. All right. Over the accuracy of that density function. I think, I think all of those are potentially valid explanations and it's, it's hard to tell. So can you give a brief history of gans from 2014 we paid for 13. Yeah. Um, so a few highlights in the first paper we just showed that gans basically work, if you look back at the samples we had now they look terrible on the CFR 10 data set.

Speaker 2:          00:40:10       You can't even recognize objects in them. Your paper, sorry. It'll use see far 10. We Use Eminence, which is a little handwritten digits. Uh, we use the Toronto face database, which is small grayscale photos of faces. We did have recognizable faces. My colleague Bing Shoe put together the first Gan face model for that paper. Um, we also had the CFR 10 data set, which is things like very small, 32 by 32 pixels of, of uh, cars and cats and dogs have for that. We didn't get recognizable objects, but all the deep learning people back then were really used to looking at these failed samples and kind of reading them like tea leaves. And people who are used to reading the tea leaves recognize that our tea leaves at least look different, right? Maybe not necessarily better, but there was something unusual about them and that got a lot of us excited.

Speaker 2:          00:41:03       Uh, one of the next really big steps was lept gown by Emily Denton and Sumif tele at Facebook AI research where they actually got really good high resolution photos working with gangs for the first time. They had a complicated system where they generated the image starting at low Rez and then scaling up to high res. Uh, but they were able to get it to work. And then, um, in 2015, I believe later that same year, uh, Alec Radford and [inaudible] Chin Tele and Luke mets, uh, published the DC Gann paper, which it stands for deep convolutional, again, it's kind of a non unique name because these days basically all gans and even some before that were deepened convolutional, but, uh, they just kind of picked a name for a really great recipe where they were able to actually using only one model instead of a multistep process, actually generate realistic images of faces and things like that.

Speaker 2:          00:42:01       Um, that was sort of like the beginning of the Cambrian explosion of Gans. Like, you know, once, once you had animals that had a backbone, you suddenly got lots of different versions of, of, you know, like fish and four legged animals and things like that. So, so DC Gann became kind of the backbone for many different models that came out used as a baseline even still. Yeah. Yeah. And so from there I would say some interesting things we've seen are um, there's a lot you can say about how just the quality of standard image generation Ganz has increased. But what's also maybe more interesting on an intellectual level is how the things you can use gowns for has also changed. Um, one thing is that you can use them to learn classifiers without having to have class labels for every example in your, your training set.

Speaker 2:          00:42:48       So that's called semi supervised learning. Um, my colleague at open Ai, Tim Solomons, who's at brain now a, wrote a paper called improved techniques for training. Gans um, I'm a coauthor on this paper, but I can't claim any credit for this particular part. Uh, one thing he showed in the paper is that you can take the Gan discriminator and use it as a classifier that actually tells you, you know, this image is a cat, this image is a dark, this image is a car, this image is a truck. And so on. Not just to say whether the image is real or fake, but if it is real to say specifically what kind of object it is. And he found that you can train these classifiers with far fewer labeled examples then traditional classifiers. So a few supervise based on also not just your discrimination ability, but your ability to classify, you going to do much, you're going to conversion much faster to being effective at being at this command.

Speaker 2:          00:43:43       Yeah. So for example, for the MDS data set, you want to look at an image of a handwritten digit and say whether it's a zero, a one or a two and so on. Um, to get down to less than 1% accuracy required around 60,000 examples until maybe about 2014 or so in 2016 with this a semi supervised Gannon project, Tim was able to get below 1% error, uh, using only a hundred labeled examples. So that was about a 600 x decrease in the amount of labels that he needed. He's still using more images in that, but he doesn't need to have each of them labeled. As you know, this one's a one, this one's a two, this one's a zero and so on. Then to be able to, for gas to be able to generate recognizable objects, so objects from

Speaker 1:          00:44:31       a particular class, you still need labeled data because you need to know what it means to be a particular class cat dog. How do you think we can move away from that?

Speaker 2:          00:44:44       Yeah. Some researchers at Brain Zurich actually just released a really great paper on a semi supervised gans where their, their goal isn't to classify, it's to make recognizable objects despite not having a lot of labeled data. They were working off of deep mines began project and they showed that they can match the performance of big began using only 10%, I believe of the, of the labels. A big gun was trained on the image net Dataset, which is about 1.2 million images and had all of them labeled, um, this latest project from brains direct shows that they're able to get away with only having about 10% of the, uh, of the images labeled. And they do that essentially using a, a clustering algorithm where the discriminator learns to assign the objects to groups. And then this understanding that objects can be grouped into similar types helps it to for more realistic ideas of what should be appearing in the image because it knows that every image it creates has to come from one of these architectural groups rather than just being some arbitrary image. If you train again with no class labels, you tend to get things that look sort of like grass or water or brick or dirt. But, um, but without necessarily a lot going on in them. And I think that's partly because if you look at a large image, that image, the object doesn't necessarily occupy the whole image. And so you learn to create realistic sets of Pixels, but you don't necessarily learn that the object is the star of the show. And he wanted to be in every image you make.

Speaker 1:          00:46:22       Yeah. You've, I've heard you talk about the, uh, the horse to zebra cycle again mapping and how it turns out, uh, again thought provoking that horses are usually on grass and zebras are usually on dry terrain. So when you're doing that kind of generation, you're going to end up generating greener horses or whatever. Uh, and so those are connected together. It's not just, yeah, you can be able to, you're not able to segment, uh, be able to generate in a segment way. So are there other types of games you've come across in your mind, uh, that, uh, neural networks can play with each other to, uh, uh, to, uh, to be able to solve problems?

Speaker 2:          00:47:05       Yeah. The, the one that I spent most of my time on is in security. You can muddle most interactions as a game where there's attacker is trying to break your system and you are the defender trying to build a resilient system. Um, there's also domain adversarial learning, which is uh, an approach to a domain adaptation that looks really a lot like gans. Uh, the, the authors, the idea before the Gan paper came out with their paper came out a little bit later. Um, and you know, they, they were very nice and cited again paper, but I know that they actually had the idea before I came out. Um, domain adaptation is when you want to train a machine learning model in one, one setting called a domain and then deploy it in another domain later and he would like it to perform well in the new domain even though the new domain is different from how it was trained. Um, so for example, you might want to train on a really clean image dataset like image net, but then deploy on users' phones where the user is taking pictures in the dark pictures. We'll move in quickly and just pictures that aren't really centered or composed all that well.

Speaker 2:          00:48:13       When you take a normal machine learning model, it often degrades really badly when you moved to the new domain because it looks so different from what the model is trained on. A domain adaptation algorithms try to smooth out that gap. And the domain adversarial approach is based on training a feature extractor where the features have the same statistics regardless of which domain you extracted them on. So in the domain adversarial game, you have one player that's a feature extractor and another player that's a domain recognizer. Uh, the domain recognizer wants to look at the output of the feature extractor and guests, which of the two domains, oh, the features came from. So it's a lot like the real versus fake discriminator in Gans. Uh, and then the feature extractor you can think of as loosely analogous to the generator and gans, except what's trying to do here is, uh, both full the domain recognizer and to not knowing which domain the data came from and also extract features that are good for classification.

Speaker 2:          00:49:09       So at the end of the day you can, in in the cases where it works out, you can actually get um, features that work about the same in both domains. Sometimes this has a drawback where in order to make things work the same in both domains, it just gets worse at the first one. But there are a lot of cases where it actually works out well on both. Do you think of gas being useful in the context of data augmentation? Yeah. One thing you could hope for with Ganz is you could imagine I've got a limited training set and I'd like to make more training data to train something else, like a classifier. You could train the Gan on the training set and then create more data and then maybe the classifier would perform better on the test set after training on this bigger generate a data set. Uh, so that's the simplest version of, of something you might hope would work.

Speaker 2:          00:50:03       I've never heard of that particular approach working. But I think there's some, there's some closely related things that that I think could work in the future and some that actually already have worked. Um, so if you think a little bit about what we'd be hoping for for use, again to make more training data, we're hoping that the Gan will generalize to new examples better than the classifier would have generalized if it was trained on the same data. And I don't know of any reason to believe that again would generalize better than the class of firewood. Um, but what we might hope for is that the gun could generalize differently from a specific classifier. So one thing I think is worth trying to, I haven't personally tried, but someone could try is what have you trained a whole lot of different generative models on the same training set, create samples from all of them and then train a classifier on that.

Speaker 2:          00:50:50       Because each of the generative models might generalize in a slightly different way. They might capture many different axes of variation that one individual model wouldn't. And then the classifier can capture all of those ideas by training on all of their data. So it'd be a little bit like making an ensemble of classifiers and I think that Ganz, yeah, in a way I think that could generalize better. The other thing that gans are really good for is um, not necessarily generating new data that's exactly like what you already have, but by a generating new data that has different properties from the data you already had. One thing that you can do is you can create differentially private data. So suppose that you have something like medical records and you don't want to train a classifier on the medical records and then publish the classifier because someone might be able to reverse engineer some of the medical records you trained on. Uh, there's a paper from Casey Greens lab that shows how you can train again using differential privacy. And then the samples from, again still have the same differential privacy guarantees as the parameters of, again, so you can make a fake patient data for other researchers to use and they can do almost anything they want with that data. Because it doesn't come from real people. And the differential privacy mechanism gives you clear guarantees on how much the original peoples data has been protected.

Speaker 1:          00:52:09       That's really interesting. Actually. I haven't heard you talk about that before. Um, in terms of fairness, a scene from a triple AI, your talk, how can adversarial machine learning help models beef more fair? Well, the respect of sensitive variables.

Speaker 2:          00:52:25       Yeah. So there's a paper from Amos Starkey's lab about how to learn machine learning models that are incapable of using specific variables. So say for example, you want it to make predictions that are not affected by gender. Um, it isn't enough to just leave gender out of the input to the model. You can often infer gender from a lot of other characteristics, like say that you have the person's name, but you're not told their gender. Well if, if their name is Ian, they're kind of obviously a man. Um, so what you'd like to do is make a machine learning model that can still take in a lot of different attributes and make it really accurate informed prediction, but be confident that it isn't reverse engineering gender or another sensitive variable internally. You can do that using something very similar to the domain adversarial approach where you have one player that's a feature extractor and another player that's a feature analyzer and you want to make sure that the feature analyzer is not able to guess the value of the sensitive variable that you're trying keep private.

Speaker 1:          00:53:26       Right. That's, yeah. I love this approach. So what do, yeah. With the, with the feature, uh, you're not able to infer, right? This sensitive variables. Yeah. Brilliant. That's quite, quite brilliant and simple. Actually.

Speaker 2:          00:53:39       Another way I think that gans in particular could be used for fairness would be to make something like a cycle again where you can take data from one domain and convert it into another. We've seen cycle again turning horses into zebras. We've seen, uh, other unsupervised gans made by menu. Lou doing things like turning day photos into night photos. Um, I think for fairness, you could imagine taking records for people in one group and transforming them into analogous people in another group and testing to see if there there are treated equitably across those two groups. There's a lot of things it'd be hard to get right to make sure that the conversion process itself is fair. Uh, and, and I don't think it's anywhere near something that we could actually use yet. But if you could design that conversion process very carefully and might give you a way of doing audits where you say, what if we took people from this group, converted them into equivalent people in another group? Does this system actually treat them how it ought to?

Speaker 1:          00:54:39       That's also really interesting, you know, in, in a popular, in popular press and in general in our imagination, you think a well gans are able to generate data and you start to think about deep fakes or being able to sort of maliciously generate data that fakes the identity of other people. Is this something of a concern to you? Is this something, if you look 10 20 years into the future, is that something that pops up in your work and the work of the community that's working on January models?

Speaker 2:          00:55:13       I'm a lot less concerned about 20 years from now then the next few years. I think there'll be a kind of bumpy cultural transition as people encounter this idea that there can be very realistic videos and audio that aren't real. I think 20 years from now people will mostly understand that you shouldn't believe something is real just because you saw a video of it, people will expect to see that it's been cryptographically signed, uh, or, or have some other mechanism to make them believe that the content is real. Um, there's already people working on this, like there's a startup called trupick that provides a lot of mechanisms for authenticating that an image is real there. There may be not quite up to having a state actor tried to, to evade their, their verification techniques, but uh, it's not that people are already working on and I think we'll get right eventually.

Speaker 1:          00:56:04       So you think authentication will, will eventually win out. So being able to authenticate it, this is real and this is not, yeah, as opposed to gans just getting better and better or generative models being able to get better and better to where the nature of what is real. I just don't think

Speaker 2:          00:56:22       you'll ever be able look at the pixels of a photo and tell you for sure that it's real or not real. And I think it would actually be a somewhat dangerous to rely on that approach too much. Um, if you make a really good fake detector and then someone's able to full your thick detector and your fake detector says this image is not fake, then it's even more credible than if you've never made a thick detector in the first place. What I do think we'll get to is, um, systems that we can kind of use behind the scenes for, to make estimates of what's going on and maybe not like use them in court for a definitive analysis. I also think we will likely get better authentication systems where, uh, you know, if imagine that every phone cryptographically signs everything that comes out of it, uh, you wouldn't able to conclusively tell that an image was real, but you would be able to tell somebody who knew the appropriate private key for this phone, uh, was actually able to sign this image and upload it to this server at this timestamp.

Speaker 2:          00:57:27       Right. Um, so you could imagine maybe you make phones that have the private keys hardware embedded in them. Um, if like a state security agency really wants to infiltrate the company, they could probably, you know, planet private key of their choice or break open the chip and learn the private key or something like that. But it would make it a lot harder for an adversary with fewer resources to fake things for most of us who, yeah. Okay. Okay. So you mentioned the beer and the bar and the new ideas. You are able to implement this or come up with this new idea pretty quickly and implemented pretty quickly. Do you think there are still many such groundbreaking ideas and deep learning that could be developed so quickly? Yeah, I do think that there are a lot of ideas that can be developed really quickly. Gans we're probably a little bit of an outlier on the whole, like one hour timescale, but um, just in terms of like low resource I ideas where you do something really different on the algorithm scale and get an a big payback.

Speaker 2:          00:58:28       Um, I think it's not as likely that you'll see that in terms of things like core machine learning technologies, like a better classifier or a better reinforcement learning algorithm or a better generative model. Um, if I had the gun idea today, it'd be a lot harder to prove that it was useful than it was back in 2014 because I would need to get it running on something like image net or sell a bay at high resolution. You know, those take a while to train. You couldn't, you couldn't train it in an hour and know that it was something really new and exciting. Uh, back in 2014 treading on amnesty was enough. But there are other areas of machine learning where I think a new idea could actually be developed really quickly with low resources. What's your intuition about what areas of machine learning are ripe for this?

Speaker 2:          00:59:17       Yeah, so I think, um, fairness and uh, interpretability are areas where we just really don't have any idea how anything should be done yet. Like for interpretability, I don't think we even have the right definitions and even just defining a really useful concept, you don't even need to run any experiments could have a huge impact on the field. We've seen that, for example, in differential privacy that uh, Cynthia Dwork and her collaborators made this technical definition of privacy where before a lot of things are really mushy. And then with that definition you could actually design randomized algorithms for accessing databases and guarantee that they preserved individual people's privacy in a, in like a mathematical quantitative sense. Right now we all talk a lot about how interpretable different machine learning algorithms are, but it's really just people's opinion and everybody probably has a different idea of what interpretability means in their head. If we could define some concept related to interpretability, that's actually measurable, that would be a huge leap forward. Even without a new algorithm that increases that quantity. And also once, once we had the definition of differential privacy, it was fast to get the algorithms that guaranteed it. So you could imagine once we have definitions of good concepts and interpretability, we might be able to provide the algorithms that have the interpretability guarantees quickly too.

Speaker 2:          01:00:42       What do you think it takes to build a system with human level intelligence as we quickly venture into the philosophical, so artificial general intelligence when you get to take, um, I, I think that it definitely takes a better environments than we currently have for training agents. That we want them to have a really wide diversity of experiences. Uh, I also think it's going to take really a lot of computation. It's hard to imagine exactly how much, so you're optimistic about simulation simulating a variety of environments. Is the path forward as opposed to it's a necessary ingredient? Yeah, I, I, I don't think that we're going to get to artificial general intelligence by training on fixed datasets or, or by thinking really hard about the problem. I think that the agent really needs to interact and have a variety of experiences within the same a lifespan.

Speaker 2:          01:01:41       And today we have many different models that can each do one thing and we tend to train them on one dataset or one RL environment. Um, sometimes they're actually papers about getting one set of perimeters to perform well in many different RL environments, but we don't really have anything like an agent that goes seamlessly from one type of experience to another and, and really integrates all the different things that it does over the course of its life. Uh, when we do see multi-agent environments, they tend to be, um, there's so many, uh, multi environment agents. They tend to be similar environments. Like all, all of them are playing like an action based game. Right? Um, we don't really have an agent that goes from, you know, playing a video game to like reading the Wall Street Journal, uh, to predicting how effective a molecule will be as a drug or something like that.

Speaker 1:          01:02:33       What do you think is a good test for intelligence in you view? Spend a lot of benchmarks started with the, uh, with Alan Turing and natural conversation and being good, being a good benchmark for intelligence.

Speaker 2:          01:02:46       What are, what would a

Speaker 1:          01:02:48       uh, Ian Goodfellow sit back and be really damn impressed if a system was able to accomplish,

Speaker 2:          01:02:55       um, something that doesn't take a lot of glue from human engineers. So imagine that instead of having to go to the, uh, CFR website and download CFR 10 and then write a python script to parse it and all that, you could just point an agent at the CFR 10 problem and it downloads and extracts the data and trains a model and starts giving you predictions. Um, I feel like something that doesn't need to have every step of the pipeline assembled for it. Definitely understand. So what it's doing is auto Amelle moving, right?

Speaker 1:          01:03:31       So that direction are you thinking wave

Speaker 2:          01:03:33       bigger? Mel has mostly been moving toward, um, once we've built all the glue, can the machine learning system, uh, to design the architecture really well? And so I'm more of saying like, if something knows how to preprocess the data so that it successfully accomplishes the task, then it would be very hard to argue that it doesn't truly understand the task in some fundamental sense. And I don't necessarily know that that's like the philosophical definition of intelligence, but that's something that'd be really cool to build. That'd be really useful and would impress me. And when he convinced me that we've made a step forward in real AI, so you give it

Speaker 1:          01:04:11       the URL for Wikipedia and then, uh, at next day expect it to be able to solve CFR 10

Speaker 2:          01:04:18       or like you type in a paragraph explaining what you want it to do and it figures out what web searches it should run and downloads all the, all the necessary ingredients.

Speaker 1:          01:04:28       So, uh, you have a very clear, calm way of speaking. No ums, easy to edit. I've seen comments for both you and I, uh, have been identified as both potentially being robots. If you have to prove to the world that you are indeed human, how would you do it?

Speaker 2:          01:04:50       Uh, well I can understand thinking that I'm a robot. It's the flip side. Turing test I think. Yeah. Yeah. The proof prove you're human test. Um,

Speaker 1:          01:05:02       she see you have to, uh, is there something that's truly unique in your mind? I suppose as it, does it go back to just natural language again, just being able to, uh,

Speaker 2:          01:05:13       proving that I'm not a robot with today's technology know that's pretty straight forward. Like my conversation today hasn't veered off into, you know, talking about the stock market or something because it was in my training data. But I guess more generally trying to prove that something is real from the content alone, it was incredibly hard. That's one of the main things I've gotten out of Mike and Research that you can simulate almost anything. And so you have to really step back to a separate channel to prove that song is real. So like, I guess I should have had myself stepped on a blockchain when I was born or something, but I didn't do that. So I coined to my own research methodology. There's just no way to know at this point. So what, uh, last question problem stands off for you that you're really excited about challenging in the near future?

Speaker 2:          01:05:59       So I think a resistance to adversarial examples, figuring out how to make machine learning secure against an adversary who wants to interfere it and control with it is, uh, one of the most important things researchers today could solve in all domains, in image language driving and every, I guess I'm most concerned about domains we haven't really encountered yet. Like, like imagine 20 years from now when we're using advanced AI is to do things we haven't even thought of yet. Um, like if you ask people what are the important problems in security of phones in like 2002, I don't think we would have anticipated that we're using them for, you know, nearly as many things as we're using them for today. I think it's going to be like that with AI that you can kind of try to speculate about where it's going. But really the business opportunities that end up taking off would be hard to predict ahead of time.

Speaker 2:          01:06:54       Well you can predict ahead of time is that almost anything you can do with machine learning, you would like to make sure that people can't get it to do what they want rather than what you want. Just by showing it a funny QR code or a funny input pattern. And you think that this set of methodologies to do that, it can be bigger than any one domain. And that's the thing. So yeah. Yeah. Like, um, one methodology that I think is not, not a specific methodology be like a category of solutions that I'm excited about today was making dynamic models that change every time they make a prediction. So right now we tend to train models and then after they're trained, we freeze them and we just use the same rule to classify everything that comes in from then on. Uh, that's really a sitting duck from a security point of view.

Speaker 2:          01:07:41       Uh, if you always output the same answer for the same input, um, then people can just run and puts through until they find a mistake that benefits them and then they use the same mistake over and over and over again. Um, I think having a model that updates its predictions so that it's harder to predict what you're going to get. We'll make it harder for the, for an adversary to really take control of the system and make it do what they want it to do. Yeah. Models that maintain a, a bit of a sense of mystery about them cause they always keep changing. Yeah. And thanks so much for talking today. That was awesome. Thank you for coming in. That's

Speaker 3:          01:08:16       great to see you.