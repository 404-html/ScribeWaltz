Speaker 1:          00:00          Welcome back to six zero, nine, nine artificial general intelligence. Today we have Richard Moise. He's the founder and managing director of article 36, a UK based not for profit organization, working to prevent the unintended, unnecessary and unacceptable harm caused by certain weapons, including autonomous weapons and nuclear weapons. He will talk with us today about autonomous weapon systems and the context of Ai Safety. This is an extremely important topic for engineers, humanitarians, legal minds, policymakers, and everybody involved in paving the path for a safe, positive future for ai in our society, which I hope is what this course is about. Richard flew all the way from the UK to visit us today in snow in Massachusetts, so please give him a warm welcome.

Speaker 2:          01:02          Thank you.

Speaker 3:          01:07          Thanks very much lex and thank you all for coming out. I was, like I said, I work for a not for profit organization based in the UK. We specialize in thinking about policy and legal frameworks around weapon technologies particularly and generally about how to establish more constraining policy and legal frameworks around around weapons and I guess I'm mainly going to talk today about these issues of to what extent we should enable machines to kill people to make decisions to kill people. It's, I think, a conceptually very interesting topic. Quite challenging in lots of ways. There's lots of unstable terminology, lots of sort of blurry boundaries. My own background, the sidewalk on weapons policy issues that I've worked on, the development of have two legal, international legal treaties prohibiting certain types of weapons. I worked on the development of 2008 convention on cluster munitions, which prohibits cluster bombs and worked on our organization.

Speaker 3:          02:11          Pyre pioneered the idea of a treaty prohibition on nuclear weapons, which was agreed last year in the UN and we're part of the steering group of Icann, the international campaign to abolish nuclear weapons, which won the Nobel peace prize last year, so that was a good year for us. The issue of autonomous weapons, killer robots which are going to talk about the day. We're also part of an NGO, nongovernmental organization coalition on this issue called the campaign to stop killer robots. It's a good name. However, I think when we get into some of the details of the issue, we'll find that perhaps the snappiness of the name in a way masks some of the complexity that lies underneath this, but this is a live issue in international policy, in legal discussions at the United Nations for the last several years, three or four years now that are being groups of governments coming together to discuss autonomous weapons and whether or not there should be some new legal instrument that, uh, the tackles this, this issue.

Speaker 3:          03:14          So it's a, it's a live political issue that is being debated in, in policy, legal circles. And really my, my comments a day are going to be speaking to that context. I guess I'm going to try and give us a bit of a briefing about what the issues are in this international debate, how different actors, a orientating to these issues. Some of the conceptual models that we, we use in that. So I'm not really going to give you a particular sales pitches to what you should think about this issue though. My own biases are probably going to be fairly evident during the process, but really to try and lay out a bit of a sense of, of how these, uh, these questions debated in the international political scene and maybe in a way that's useful for reflecting on sort of wider questions of how a AI technologies might be orientate it to an approach by, uh, by policy makers and, and, and the legal framework.

Speaker 3:          04:11          So in terms of the structure of my comments, I'm gonna talk a bit about some of the pros and cons that are put forward a around autonomous weapons or movements towards greater autonomy in weapons systems. Uh, I'm going to talk a bit more about the political legal framework within, within which these discussions are taking place. And then I'm going to try to sort of lay out some of the models, the conceptual models that we're as an organization have developed and are sort of using a in relation to these issues. And perhaps to reflect a bit on where I see the, the political conversation, the legal conversation on this going at an international level and maybe just finally to try to reflect on or just draw out some more general thoughts that I think occurred to me a about what some of those says about thinking about ai functions in a, in different social social roles.

Speaker 3:          05:02          Um, but before getting into that sort of pros and cons type type stuff, I just wanted to start by suggesting a bit of a sort of conceptual timeline because one of the things this could be the president, one of the things you find when we start, when you say to somebody who will work on this issue of autonomous weapons, they tend to orientate to it. And in two fairly distinct ways. Some people will say, oh, you mean om drones and you know, we know drones are they being used, uh, in the world today. And that's kind of an issue here in the present. Right?

Speaker 3:          05:51          But other people, most of the media, and certainly pretty much every media photo editor thinks she talking about terminator of hair. Maybe even a skynet thrown in, so this is a sort of advanced, futuristic, Saifai orientation to the, to the issues. My thinking about this, I come from a background of, of working on the impact of weapons in the present. I'm less, I'm less concerned about this area of my thinking and my anxieties or concerns around this issue. Don't come from, from this area. I'll do this light a bit wiggly here because I also don't want to suggest there's any kind of, you know, telia logical certainty going on here. This is just an imaginary timeline, but I think it's important just in terms of situating where I'm coming from in the debate that I'm definitely not starting at that end. And yet in the political discussion amongst governments and states, well you have people come in and it all sorts of different positions along here.

Speaker 3:          06:59          Imagining a little autonomous weapons may existed, you know, somewhere along this, this sort of spectrum. So I'm going to think more about stuff that's going on around here and how some of our conceptual model is really built around some of this thinking. Not so much actually armed drones, but some other, some other systems. But my background before I started working on policy and law around weapons was, um, was setting up and managing landmine clearance operations overseas and well, they'd been around for. I'd been around for quite a long time, like months. Um, and I think it's interesting just to start with, just to reflect on the basic anti-personnel landmine. A simple, but it gives us, I think some sort of useful entry points into thinking about what an autonomous weapon system might be in its most simple form. Uh, if we think about a landmine, well, essentially we have a person and as an input into the landmine and there's a function that goes on here. Pressure is greater than x person. They tried on the landmine. There's a basic mechanical algorithm goes on and you get an output.

Speaker 3:          08:22          That explosion that goes back against the person who taught on the Lamont. So it's a fairly simple system of a signal, a sensor taking a signal from the outside world. The landmine is viewing the outside world through its sensor to basic pressure plate and according to a certain calculus here, you got output and it's directed back at this person and it's a loop. And that's one of the things that I think is fundamental essentially to understanding the idea of autonomous weapons. And in a way this is where the autonomy comes in, that there's no other person intervening in this process at any point. There's just a sort of straightforward relationship from the person or object that has initiated the, uh, the system back into the, uh, back into the effects that are being, that are being applied. So in some ways, we'll come back to this later and think about how some of the basic building blocks of this, uh, maybe they're not thinking about other weapons systems and weapons technologies as they're developing and maybe thinking about landmines and thinking about these processes of technological change.

Speaker 3:          09:30          We see a number of different, uh, dynamics at play in this sort of imaginary timeline. Anti personnel, land mines of course a static. They just sit in the ground where you left them. But we get more and more mobility perhaps as we go through this system. Certainly drones and other systems that I'll talk about, you start to see more mobility in the, in the weapon system. Perhaps greater sophistication of sensors, I mean a a basic pressure plate. Just gauging, wait, that's a very simple a census structure for interrogating the world. We have much more sophisticated a sensor systems and weapons now. So we have weapons systems now that are looking at radar signatures. They're looking at the heat shapes of objects and we'll come back and talk about that. But more sophistication of sensors and more sophistication of the, um, of the computer algorithms that are basically interrogating those sensor, the sensor inputs.

Speaker 3:          10:31          I'm perhaps a little bit as well of a movement in this sort of trajectory from physically very unified objects always sort of wrestle slightly whether this is the word I want, but it's a sort of self contained entity. The landmine. Whereas as we move in this direction, maybe we see more dispersal of functions through different three different systems. And I think that's another dynamic that when we think about the development of autonomy and weapon systems, it might not all live in one place physically moving around in one place. It can be, uh, an array of different systems functioning in different, in different places, uh, perhaps for people with a sort of ai type of mindset. Maybe there's some sort of movement from more specific types of ai functioning here, use of different specific ai functions here to something more general going in this direction. I'm wary of necessarily buying straight forward into that, but maybe you could see some movement in that sort of direction.

Speaker 3:          11:29          So I just want to sort of put this to one side for now, but we'll come back to it and think about some systems that are existing here that I think sort of raise issues for us and around which we could expand some, some models. But I just wanted to have this in mind when thinking about this. So we're not necessarily, we're definitely not for me thinking about a humanoid robots walking around fighting like a soldier, a rather. We're thinking about developments and trajectories. We can see coming out of established military systems now, so I was going to talk now a bit about the political and the legal context. Obviously there's a lot of complexity in the worlds of politics and legal structures and I don't want to get too bogged down in it, but I think in terms of understanding the basics of this, a debate on the international landscape have to have a bit of background in that area.

Speaker 3:          12:23          Essentially there's, I think three main types of international law that we're concerned with here and again, concerned with international law rather than domestic legislation, which any individual state can put in place whatever domestic legislation they want. We're looking at the international legal landscape. Basically you have international human rights law which applies in pretty much all circumstances and it involves the right to life and the right to dignity and, uh, various other, uh, legal protections people. And then particularly prominent in this debate if you have what's called international humanitarian law, which is the rules that govern behavior during armed conflict and provide obligations on military is engaged in armed conflict for how they have to conduct themselves. This isn't the legal framework that decides whether it's okay to have a war or not. This is a legal framework that once you have in the war, this is the obligations that you've got to, you've got to follow.

Speaker 3:          13:19          And it, it basically includes rules that say, you know, you're not allowed to directly kill civilians. You've got to aim your military efforts at the forces of the, of the enemy that enemy competence. You're not allowed to kill civilians directly or deliberately, but you are allowed to kill some civilians as long as you don't kill too many of them for the military advantage that you're trying to achieve. So there's a sort of balancing acts like this. This is called proportionality. Nobody ever really knows where the balance lies, but it's a, it's a sort of principle of the law that, uh, while she can kill civilians, you mustn't kill an excessive number of civilians. These are general rules. These apply pretty much to all states, uh, in, in, um, conflict situations. And then you have treaties on specific weapon, a specific weapon types. And this is really where you have weapons that are considered to be particularly problematic in some way, and it's decided a group of states decides to develop and put in place agree, a treaty that, uh, that applies specifically to those, to those weapons.

Speaker 3:          14:24          I think it's important to recognize that the illegal treaties are all developed and agreed by states that are agreed by international governments talking together, negotiating what they think the law should say. And they generally only bind on states if they choose to adopt that legal instrument. So, uh, I guess what I'm emphasizing there is a sense that these are sort of social products in a way that political products, it isn't the sort of magical law that's come down from on high, perfectly written to match the needs of humanity. It's a negotiated outcome developed by a complicated set of actors who may or may not agree with each other and all sorts of things. And what I means is there's quite a lot of wiggle room in these legal frameworks. And quite a lot of uncertainty within the lawyers of international humanitarian law will tell you that's not true.

Speaker 3:          15:18          But that's because there are particularly keen on that legal framework, but in reality there's a lot of, um, a lot of fuzziness to, to what some of the legal provisions or some of the legal provisions say. And it also means that the extent to which this law binds on people and bears on people is also require some social enactment. There's not a sort of world police who, who could follow up on all of these, uh, these legal frameworks. It requires a sort of social function from states and from other actors to keep articulating a sense of the importance of these legal rules and keep trying to put pressure on other factors to, to a chord with them. So the issue of autonomous weapons is in discussion at the United Nations under a framework called the UN Convention on conventional weapons. And this is a body that has the capacity to agree new protocols, new treaties essentially on specific weapon systems.

Speaker 3:          16:10          And that means that diplomats from lots of countries, diplomats from the US, from the UK, from Russia, and Brazil and China and other countries of the world will be sitting around in a conference room, put it forward that perspectives on this issue and trying to find common ground or trying not to find common ground, just depending on what sort of outcome that they're working towards. So, you know, the UN isn't a completely separate entity of its own. It's just the community of states in the world sitting together talking about talking about things. So main focus of concern and those discussions when it comes to autonomy is not some sort of generalized, uh, autonomy, autonomy of all of its forms that may be pertinent in the military space. It's, it's rather much more these, these questions of how the targets of an attack, a selected, uh, identified, decided upon. And how is the decision to apply force to those targets made. And it's really a, these sort of the critical functions of weapons systems where the movement towards greater autonomy is considered a source of anxiety, essentially that we may see machines making decisions on what is a target for an attack and choosing when and where forces apply to that specific. So that specific target.

Speaker 3:          17:34          So obviously in this context, not everybody is like minded on this. There are potential advantages to increasing autonomy in weapons systems and there's potential disadvantages and, and, uh, problems associated with it. Then within the, within this, uh, international discussion, we say different perspectives laid out on some states. Of course we'll be able to see some, some advantages and some disadvantages. It's not a black and white sort of a discussion in terms of the possible advantages for autonomy. I mean one of the key ones ultimately is framed in terms of military advantage that we want to have more autonomy and weapons systems because it will maintain or give us military advantage over possible adversaries. Because in the end military stuff is about winning wars, right? So you want to maintain military advantage and military advantage number of factors really within that speed is one of them. Speed of decision making can computerize autonomous systems make decisions about where to apply force faster than a human would be capable of doing.

Speaker 3:          18:42          And therefore this is a, this is advantageous for us. Uh, also, speed allows for coordination of numbers. So if you want to have swarms of a systems, you know, swarms of small drones or some such a unique, quite a lot of probably autonomy in decision making and communication between those systems because again, the level of complexity and the speed involved is, is greater than a human would be able to sort of manually a engineer. So speed, both in terms of responding to external effects, but also coordinating your own forces, reach, um, potential for autonomous systems to be able to operate in a communication denied environments where if you're relying on a, on an electronic communications link to say a current arm drone, maybe in a future battle space where the enemy is denying communications in some way, you could use an autonomous system to still fulfill a mission without a, without needing to rely on that communications infrastructure, general force multiplication.

Speaker 3:          19:47          There's a bit of a sense that there's going to be more and more teaming of machines with, with humans, so machines operating alongside humans in the battle space. And then this importantly as it presents it at least a sense that these are systems which could allow you to reduce the risk to your own forces. That maybe if we can put some sort of autonomous robotic system at work in a specific environment, then we don't need to put one of our own soldiers in that position. And as a result, we're less likely to have casualties coming home. Which of course politically is a problematic for maintaining any sort of conflict, posture set against all that stuff. Um, there's a sense that I think most fundamentally this perhaps a moral hazard that we come across at some point that the, some sort of a boundary where seeing or conceptualizing a situation where machines are deciding who to kill in a certain context is just somehow wrong and well, that's not a very easy argument to just start, you know, articulate in a sort of rationalized sense, but there's some sort of moral revulsion that perhaps it comes about at this sense that machines are now deciding who should be killed in a particular, in a particular environment.

Speaker 3:          21:04          Um, there's a set of legal concerns. Can these systems be used in accordance with the existing legal obligations? And I'm going to come on a little bit later to our orientation in the legal side, which is, is also about how they may stretch the fabric of the law and the structure of the law. As we say it. The some concerns in this sort of legal arguments for me that, um, we sometimes slip into a language of talking about machines making legal decisions. Will a machine be able to apply the rule of proportionality properly? There's dangers in that. I know what it means, but at the same time the law is addressed to humans. The law isn't addressed to machines. So it's humans who have the obligation to, uh, an act, the legal obligation and machine may do a function that is sort of analogous to that legal decision.

Speaker 3:          21:58          But ultimately to my mind is still a human who has to be making the legal determination based on some predict prediction of what that machine will do. And I think this is a very dangerous slippage because even, you know, senior legal academics can slip into this mindset which is, which is a little bit like handing over the legal framework to machines before you've even got onto arguing about what we should or shouldn't have. So we need to be careful in that area. And it's a little bit to do with, for me continuing to treat these technologies as machines rather than into treating them as agents in some way of sort of equal or equivalent or similar moral standing to, to humans. And then we have a whole set of wider concerns that are raised. So we've got moral anxieties, a legal concerns, and then a set of other concerns around risks, uh, that could be unpredictable risks, does the sort of normal accidents theory.

Speaker 3:          22:51          Maybe you've come across that stuff. There's a bit of that sort of language in the debate about complicated systems and not been able to, uh, you know, not be able to avoid accidents and in some respects some anxieties about maybe this will reduce the barriers to engaging in military action, may be being able to use autonomous weapons, will make it easier to go to war. And some anxieties about sort of international security and balance of power and arms races and the like. These are all significant concerns. I don't tend to think much in this area, partly because they involve quite a lot of speculation about what may or may not be in the future and they're quite difficult to populate with, um, with sort of more grounded arguments I find. But uh, that doesn't mean that they are significant in themselves, but I find them less straightforward as an entry point.

Speaker 3:          23:43          So been all of these different issues. There's multiple unstable terminology, lots of arguments coming in, different directions. And our job as an NGO in a way is we're trying to find ways of building a constructive conversation in this environment which can move toward states adopting a more constraining orientation to this movement towards autonomy. And the main tool we've used to work towards that so far has been to perhaps stop focusing on the technology per se and the idea of what is autonomy and how much autonomy is a problem. And to bring the focus back a bit onto what is the human element that we want to preserve in all of this. Because it seems like most of the anxieties that come from a sense of a problem with autonomous weapons are about some sort of absence of a human element that we want to preserve, but unless we can in some way define what this human element is that we want to preserve, I'm not sure we can expect to define its absence.

Speaker 3:          24:44          Very straightforward. So I kind of feel like we want to pull the discussion onto a focus on the, on the human, uh, on the human element. Um, and the tool we've used for this so far has been basically a terminology about the need for meaningful human control. And this is just a form of words that we've sort of introduced into the debate and we've promoted it in discussions with diplomats and with different actors. And we've built up the idea of this terminology as, as being a sort of tool. It's a bit like a meme, right? You, you create the, the terms and then you use that to sort of structure the discussion in a, in a productive, in a productive way. One of the reasons I like it is it works partly because the word meaningful, it doesn't mean anything particular or at least it means whatever you might want it to mean.

Speaker 3:          25:32          And I, I find that, uh, uh, an enjoyable sort of tension and in that, but the term meaningful human control has been quite well picked up in and the literature on this issue and in the diplomatic discourse and it's helping to structure us towards a, uh, what we think are the key questions, basic arguments for the idea of meaningful human control from my perspective, a quite simple and intended to use basically a sort of absurdist sort of logic if there is such a thing. First of all, really to recognize that no governments are in favor of an autonomous weapon system that has no human control whatsoever, right? Nobody is arguing that it would be a good idea for us to have some sort of autonomous weapon that just flies around the world deciding to kill people. We don't know who it's gonna count a why it doesn't have to report back to us, but you know, we're in favor. Nobody's in favor of this, right? This is obviously ridiculous, so there needs to be some form of human control because we can rule out that sort of ridiculous extension of the argument and on the other hand, if you just have a person in a dark room with a red light that comes on every now and again and they don't know anything else about what's going on, but that the human who's controlling this autonomous weapon and when the red light comes on, they pushed the fire button to launch a rocket or something.

Speaker 3:          26:59          We know that that isn't sufficient tumor control either, right? There's a person doing something, there's a person engaged in the process, but clearly it's just some sort of mechanistic, pro forma human engagement. So between these two kinds of ridiculous extremes, I think we get the idea that there's. There's some sort of fuzzy, fuzzy line that that must exist in there, in there somewhere and that everybody can in some way agree to the idea that's such a line should exist. So the question then for us is how to move the conversation in the international community towards a productive sort of discussion of where the parameters of this line might be conceptualized.

Speaker 3:          27:43          So that's brought us on to thinking about a more substantive side of questions about what are the key elements of meaningful human control and we've laid out some basic elements. So I'm like, get rid of that fuzzy line because it's a bit useless anyway, isn't it? And then I can put my key elements on here. Well, one of them is I'm predictable, reliable, transparent technology. This is kind of before you get into exactly what the system is going to do, we want the technology itself to be sort of well made and it's, you know, it's going to basically do what it says it's going to do whatever that is and we want to be able to understand it to some extent. Obviously this becomes a bit of a challenge and some of the Ai type functions where you start to have machine learning, uh, issues and these issues of transparency. Perhaps they start to come up a little bit, a little bit, but these kinds of issues in the design and the development of of of systems. Another thing we want to have is, and I think this is a key one, is accurate information

Speaker 3:          29:09          and it's accurate information on the intent of the commander. Well the outcome, what's the outcome we're trying to achieve? How does the technology work and what's the context so it is already full so it won't take long. Third one is

Speaker 4:          29:43          timely intervention, human intervention. It should be.

Speaker 3:          29:53          It'd be good if we could turn it off some points if it's going to be a very long acting system would be good if we turn it off. Maybe that's the fourth one is just the sort of framework of accountability.

Speaker 3:          30:09          So we're thinking the basic elements of humor control can be broken down into these areas. Some of them have about the technology itself, how it's designed and made. How do you verify and validate that it's gonna do what the manufacturers have said it's going to do. Can you understand it? This one I think is the key one in terms of thinking about the issue and this is what I'm going to talk about a bit more now. The accurate information on what's the commander's intent, what do you want to achieve in the use of this system? What effects is it going to have? I mean this makes a big difference how it works. This factors here involve what are the target profiles that it's going to use. Whereas mine on the landmine, of course it was just pressure. Pressure is being taken as a pressure on the ground is being taken as a proxy for a military target for a human who we're going to assume as a military target, but, and these systems, we're going to have different target profiles, different heat shapes, different, uh, different patterns of data that the system is going to operate on.

Speaker 3:          31:12          The basis of what sort of actual weapons are going to use to apply force. It makes a difference if it's going to just fire a bullet from a gun or if it's gonna drop a 2000 pound bomb. I mean, that has a different effect. And the way in which you envision sort of control for those effects is going to be different in those different cases. And finally, very importantly, these issues of context, information on the context in which the system will will operate. Okay.

Speaker 3:          31:41          Contexts of course, includes all the going to be civilians present in the area. Can you assess, are they going to be other objects in the area that may present a similar pattern to the proxy data? You know, if you're using a heat shape of a vehicle engine, oh, it might be aimed at a tank, but if there's an ambulance in the same location is an ambulance, is vehicle engine heat shape sufficiently similar to the tank to cause some confusion between the two sets of information like that? And context of course varies in different. You know, obviously varies in different environments. But I think we can see different domains in this area as well, which is significant operating in the water or in the ocean. You've probably got a less cluttered environment or less complex environment than if you're operating in an urban in an urban area. So that's another factor that needs to be taken into, into accountant in this. So I just wanted to talk a little bit about some existing perhaps and think about them in the context of this, these sort of set of issues here.

Speaker 3:          32:50          One system that you may maybe aware of is it's on a boat. Um, okay. So, so then like the Phalanx, anti missile system, it's on a boats, but there's various anti missile systems. I mean it doesn't, the details don't matter in this context. These are systems that human terms that aren't so human is choosing when to turn it on and a human turns it off again. But when it's operating it's the radar is basically scanning and area of an area of sky up here. And it's looking for fast moving incoming objects because basically it's designed to automatically shoot down incoming missiles. Rockets, right? So thinking about these characteristics, you know, what the outcome you want is you want your boat not to get blown up by an incoming missile and you want to shoot down any incoming missiles. You know, how the technology works because you know that it's basically using radar to see incoming fast moving, uh, signatures and you have a pretty good idea of the context because the sky is a fairly uncluttered comparatively and you'd like to think that any fast moving incoming objects towards your hair are probably going to be a incoming missiles, not guaranteed to be the case.

Speaker 3:          34:26          One of these systems shut down, uh, an Iranian passenger airliner, but, uh, by accident, which is obviously a significant accident, um, but basically you have a sentence of, you know, the fact that the data that you're using tracks pretty well to the target objects, if not absolutely precisely. You Bet are relatively controllable environments in terms of the sky and you've got a human being. The system isn't really mobile. I mean, it's kind of mobile insofar as the boat can move around, but the person who's operating it as you know, the mobile in the same place. So, uh, so it's relatively static. So I think looking at that, you could suggest that there's still a reasonable amount of human control over this system because when we look at it in terms of a number of the functions here, we can understand how that, um, how that system is being managed in a human controls by and although there's still a degree of autonomy or at least it's sort of highly automated and the way that it actually identifies the targets and moves the gun and shoots down the incoming object. The basic framework is volume, which I fail like it. I mean, it's not for me to say, but I feel like I'm still a reasonable amount of human controlled is bang, Bang, applaud. Okay. Another sort of system. I'm gonna draw some tanks or something now say, okay, well I'm just going to draw them out a lot because otherwise it's too long.

Speaker 3:          35:57          Days of tanks all but fighting vehicles. Ignore the graphic design skills. Um, there a sense of use weapons systems, Blah commander, a significant distance, right? Can't necessarily see the location of the, of the tanks, but they know that the cement tanks in this area over here, right, and maybe they have some sense of what this area is that aren't in the middle of a town. They're out in the open so they have an understanding of the context, but maybe not a detailed understanding of the context. So the weapons system is going to file multiple warheads into this target area. The commander is decided upon the target of the attack, this group of tanks here, but as the war heads approached the target area, the warheads are going to communicate amongst themselves and that kind of allocate it themselves to the specific to the specific objects. And I got to detect the heat shape of the vehicles engines.

Speaker 3:          36:58          They're going to match that with some profile that says this is a enemy almost fighting vehicle as far as we're concerned. And then they're going to apply force downwards from the, uh, using a better of explosive engineering shape charge which focus a plus a plus to have explosive basically a jet of explosives downwards onto the specific targets. And so in this situation, well have the, has the weapon system chosen the target? It's a bit ambiguous because as long as we conceptualize the group of tanks as the as the target, then a human has chosen the target and the weapon system is essentially just being efficient in its distribution of force to the target objects. But if we see the individual vehicles as individual targets, maybe the weapon system has chosen a, the targets potentially some advantages of automated of autonomy in this situation. From my perspective, this kind of ability to focus a jet of explosive force directly on the object that you're looking to strike so long as you've got the right object.

Speaker 3:          38:10          This is much better than setting off lots of artillery shells in this area, which would have a much greater explosive force effect on the surrounding area. Probably put a wider population at risk. So just sort of set of considerations here that I think a significant. So we have these systems, these systems exist, exist today. You could ask questions about whether those heat shaped profiles of those objects sufficiently tightly tied to energy fighting vehicles or whatever. But I think it can be conceptualized reasonably straightforwardly in those times, but the area where I started having a problem with this stuff is in the potential for this circle or this pattern just to get bigger and bigger essentially because it's all reasonably straightforward. When you put the tanks reasonably close together and you can envisage having one sort of one set of information about this area which allows you to make a legal determination is that you need to make.

Speaker 3:          39:10          But once these tanks get spread out over a much larger area and you have a weapon system that using basically the same sorts of technological approach is able to cover a substantially wider area of enemy terrain over a longer period of time. Then it suddenly gets much more difficult for the. For the military commander to have any really detailed information about the context in which force will actually be applied. And for me, this is. I think the main point of anxiety or point of concern that I have in the way in which autonomy and weapons systems is, is likely to develop over the immediate future. Because under the legal framework, a military commander has an obligation to apply certain rules in an attack and an attack is. It's not precisely defined, but it needs to have some, I think, some spatial and conceptual boundaries to it that allow a sufficient granularity of legal application.

Speaker 3:          40:07          Because if you. If you treat this as an attack, I think that's fine as you expand it out, so you've got vehicles across a whole wide area of a country, say across the country as a whole, using the same sort of extension logic has as in some previous arguments. Once you've got vehicles across the whole country and you're saying, in this attack, I'm going to just talk at the vehicles of the enemy and you send out your warheads across the whole location. Now, I don't think that's gonna happen in the immediate term, but I'm just using that as a sort of conceptual challenge. You start to have applications of actual physical force in all sorts of locations where a commander really can't assess in any realistic way what the actual effects of that are going to be. And I think at that point you can't.

Speaker 3:          40:50          You can no longer say that there is sufficient a human control being being applied, so this capacity of ai enabled systems or ai driven systems to expand the attacks across a much wider geographical area and potentially over a longer period of time I think is a significant challenge to how the legal framework is, is understood at present, not one that relies upon determinations about whether this weapon system will apply the rules properly or not, but rather one which involves the frequency and the proximity of human decision making to be sort of diluted a progressively over progressively over time. So that's a significant area of concern for me. Final sort of set of concerns in these areas is around these issues about encoding of targets. I think we could say pretty clearly that weight is a very mega, a basis for evaluating whether something is a valid military target.

Speaker 3:          41:52          All right, um, the significant problems, but it's suggesting that we could just take the weight of something as being sufficient for us to decide is this a target or not. In any of these processes, we have to decide that certain patterns of data represent a military objects of some type and of course in a way I think what we sort of see in sort of proponents of greater and greater autonomy and weapons systems is a sense that, well, as we expand the scope of this attack, we just need to have a more sophisticated system that's undertaking the attack that can take on more of the evaluation and, and more of this, um, process of basically mapping the coding of the world into a set of decisions about the application of force. But overall, yeah, I'm skeptical about the way in which our social systems are likely to go about mapping people's indicators of identities into some sort of fixed sense of military objects or military targets as a society over the last hundred years.

Speaker 3:          43:00          There's been plenty of times where we've applied certain labels to certain types of people, certain groups of people, um, based on various indicators, which apparently seemed reasonable to some significant section of society at the time. But then ultimately I think we've subsequently thought, well, highly problematic. And so I think we need to be very wary of any sort of ideas of thinking that we can encode in terms of humans, particularly I'm very concrete indicators that certain groups of people should be considered by the targets or not just going to say a couple of final things about future discussions in the CCW. The chair of the Group of governmental experts, that's the body that's going discuss autonomous weapons, uh, has us states for the next meeting we should take place in April to come prepared with ideas about the touchpoints of human machine interaction. This is a sort of code for one of the ways in which we can control technology.

Speaker 3:          44:04          So I suppose from our context as an organization, we'll be looking to get states to start to try and lay out this kind of framework as being the basis for their perception of the ways in which the entry points to control of technology could be thought about. Again, it's really a question of structuring the debate. We won't get into detail across all of this, but I think it's plausible that this year and next we'll start to see the debate falling into some adoption of this kind of framework, which I think will give us some tools to work with. I think at least if we start to get some agreement from a significant body of states that these are the sort of entry points we should be thinking about in terms of control of technology that will give us a bit of leverage for a start towards suggesting an overarching obligation that there should be some sort of meaningful or sufficient human control, um, but also in a way of thinking about that and interrogating that as new technologies develop in the future, uh, that we can leverage in some.

Speaker 3:          45:01          In some ways I felt reasonably confident about that. But it's a difficult political environment and you know, it's quite possible that I don't see any rush amongst states to move towards any legal controls in this, in this area. Just as a few very final thoughts, which may be a bit more abstract in my thinking on this. I feel like, and this sort of reflecting on maybe some dynamics of ai functioning, my anxiety here about the expansion of the concept of attacks and in the same, in conjunction with that, a sort of breaking down of the granularity of the legal framework. I think this is another, a sort of generalizing function again, and it's a movement away from more specific legal application by humans to perhaps a pushing humans people towards a more general, a legal orientation. And I feel like in the context of conflict, we should be pushing for a more specific and more focused and more regular, um, application of human judgments and moral.

Speaker 3:          46:07          A moral agency that isn't to say that I think humans are perfect in any way. Uh, there's lots of problems with humans, but at the same time I think the, we should be very wary of thinking that violence is something that can be somehow perfected and that we can encode how to conduct violence in some machinery that will then provide an adequate social product for society as a, as a whole. And I guess it was a very final thought, a bit linked to that is some questions in my mind about how this all relates to bureaucracy in a way and in a sense that some of the functions that we're saying here and some of the Ai functions that we see here in many ways related to bureaucracy, to the encoding and categorization of data in certain ways and just a very fast management of that bureaucracy, which is really an extension of the bureaucracies that we already, that we already have. And I think extending that to far into the world of violence and the application of force to people will well precipitate painful effects for us as a, as a society. And as it brings to the fore. I think some of the underpinning underpinnings of the rationales of that, uh, of that bureaucratic framework so that we go. It's a bit of a broad brush sketch.

Speaker 2:          47:23          Thank you.

Speaker 4:          47:31          So, um, this question's kind of a little bit of multifaceted, but, um, as humans evolve and adapt to increasingly autonomous weapons, the complexity and sophistication could increase with expansion of targets and types and target area is. Do you think there's a limit to which we can prepare against such an evolution and a, do you think that'd be ocracy can keep up with how fast these, the autonomy of you couldn't develop over time?

Speaker 1:          48:00          Yeah, I'm not sure I caught all the first bit of the question, but there's definitely, it's definitely a challenge that the types of legal discussions at the UN Convention on Conventional Weapons, they are not famous for going too quickly. In fact, they're incredibly slow. And in that framework, every state essentially has a veto over everything. So even over the agenda of the next meeting, if you know, if the US wants to block the agenda, they can lock the agenda, let alone block the outcome that might come if you could agree on agenda. So, so every state has an ability to keep things moving very slowly there. And that's definitely a challenge in the context where the pace of technological development moves pretty quickly. The only thing I would say, which I forgot to mention before in terms of thinking about the dynamics in this debate, is that it's not straightforwardly a situation where militaries really want loads more autonomous weapons and other people don't have in military commanders also like control and they they like troops on the ground like control and they like trust and confidence in the systems that they're operating around.

Speaker 1:          49:06          They don't want to get blown up by their own equipment and military commanders like controlling and like to know what's happening. So there are some constraints within the military structures as well. So the overall sort of development here, I guess from our side in terms of this sort of how to constrain against the expansion of attacks and the expansion of sort of objects that may be attacked by autonomous systems in a way. That's where I felt like developing the idea that there's a principle of human control that needs to be applied. Even if it's a bit fuzzy in its boundaries, we can use that and interrogated as a social process to try and keep constraint going back towards the specific because in the end, like I said earlier, these legal structures or sort of social processes as well and it's not very easy. It's not something where you can just straightforward to draw a line and then no new technologies will come along that challenge your expectations, right? We need to find the sort of camp on the international legal political landscape. We need to sketch out the parameters of that come in legal terms. Then we need people to turn up at those meetings and continuously complain about things and put pressure on things because that's the only way over time where you maintain that sort of interrogation of future technologies as they come out of the pipeline or or whatever. So it's a sort of social I think.

Speaker 4:          50:28          Yeah, that answered my question is like the balance between like how fast science would be like advancing in this field versus like how fast we obviously can move to keep up

Speaker 1:          50:37          be resolved. I think it's an ongoing. It's got to be an ongoing social political process in a way. Right.

Speaker 4:          50:41          Awesome. Thank you. So

Speaker 1:          50:46          given that this course is on Agi and we'll likely see a wide variety of different kinds of autonomous systems in the future, can you give us perhaps some sort of extrapolation from this domain to a broader set of potentially risky behaviors that more autonomous and more intelligent systems would do and ways that the creators of such systems such as potentially the folks sitting in this room can change what they're doing to make those safer? Yeah, I mean I think useful to think about in some ways these ideas of from the president, from where we are now, how can people involved in developing different technologies, new technological capacities, just be thinking of the potential outcomes in this sort of weaponization area and building in some orientation to their work that thinks about that and thinks about what the potential consequences of work can be. I mean, I think in some ways the risk outcomes type thinking.

Speaker 1:          51:47          I mean again, it gets you into hypothetical arguments, but they. The idea of two sides, both with substantial autonomous weapons system capabilities is probably the sort of area where these ideas of accidental escalations come to the fore, that if you've got to adversarily orientated states with substantial autonomous systems, then there's a potential for interactions to occur between those systems that rapidly escalating violence situation in a way that greater capacity for human engagement would allow you to, to curtail it and stole it. And I think, I mean I know in other areas of, you know, of Algo written functioning in society, we've seen aspects of that, right? And sort of probably in the financial sector and other such location. So. So I think yeah, those are those ideas of sort of rapidly escalating, cascading risks is a, is a, is a concern in that area. But again, based on hypothetical thinking about

Speaker 4:          52:55          stuff. Last question, right?

Speaker 5:          52:58          What do you think of this criteria? So we have this tank example on the right, our simulations, our ability to simulate things is getting better and better. What if we showed a simulation of what would happen to a person that has the ability to hit the go button on it and if the simulation does not have enough fidelity, we consider that a Nogo we cannot do that. Or if the simulation shows it does have enough fidelity and it shows a, a bad outcome, then maybe that would be a criteria in which to, uh, to judge this circumstance on the right. And that could also let us a, as that circle gets bigger and bigger, it can let us kind of put a, uh, it could let us cap that by saying, hey, if we don't, if we do not have enough information to make this a simulation to even show the person, then it's a no go.

Speaker 1:          54:02          Yup. Yup. I think in a way this is an issue of modeling, right? Based on contextual information that you, that you have. So maybe with technological developments you have a better capacity for modeling specific situations. I suppose the challenge is how do you, in a sort of timely manner, especially in a conflict environment where tempo is significant. Can you, can you put the data that you have into some sort of modeling system adequately, but I don't see any problem with the idea of using ai to model the outcomes of specific attacks and you know, give you read outs on what the likely effects are going to be. I guess the challenges that, what counts are adequate effect where the boundary lines have sufficient information and insufficient information fall that kind of open questions as well. Right. And you know, military is tend to like to leave some openness on those, those points as well. But, but I think that can be definitely a goal for modeling and in better understanding what the facts. It cannot be great. Give Richard a big hand.

Speaker 2:          55:15          Thank you very much.