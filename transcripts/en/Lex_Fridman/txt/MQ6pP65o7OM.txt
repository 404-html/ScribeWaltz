Speaker 1:          00:00          Today we will talk about deep reinforcement learning.

Speaker 1:          00:07          The question we would like to explore is to which degree we can teach systems to act, to perceive and act in this world from data. So let's take a step back and think of what is the full range of tasks than artificial intelligence system needs to accomplish. Here's the stack from top to bottom, top the input bottom output, the environment at the top, the world that the agent is operating in sensed by sensors taking in the world outside and converting it to raw data and interpretable by machines, sensor data, and from that raw sensor data, you extract features. You extract structure from that data such that you can input it, makes sense of it, discriminate, separate, understand the data, and as we discussed, you form higher and higher order representations, a hierarchy of representations based on which the machine learning techniques can then be applied. Once the machine learning techniques, the understanding, as I mentioned, converts the data into features, into higher order representations and into simple actionable, useful information. We aggregate that information into knowledge, would take the pieces of knowledge extracted from the data through the machine learning techniques and to build a taxonomy,

Speaker 1:          01:47          a library of knowledge, and would that knowledge we reason an agent is tasked to reason, to aggregate, to connect pieces of data seen in the recent past or the distant past to make sense of the world that's operating in and finally to make a plan of how to act in that world based on its objectives based on what he wants to accomplish. As I mentioned, a simple but commonly accepted definition of intelligence is a system that's able to accomplish complex goals, so system that's operating in an environment and this world must have a goal, must have an objective function or reward function and based on that it forms of plan and takes action and because they're operates in many cases in the physical world, it must have tools, effectors with which it applies to actions to change. Something about the world. That's the full stack of an artificial intelligence system that acts in the world. And the question is,

Speaker 1:          02:56          what kind of task can such a take on what kind of tasks can an artificial intelligence system learn? As we understand ai today, we will talk about the advancement of deeper enforcement learning approaches and some of the fascinating ways it's able to take much of the stack and treat it as an end to end learning problem. But we look at games, we'll look at simple formalized worlds. While it's still impressive, beautiful and unprecedented accomplishments. It's nevertheless formal tasks. Can we then move beyond games into expert tasks of medical diagnosis, of design and into natural language, and finally the human level tasks of emotion, imagination, consciousness.

Speaker 1:          03:56          That's once again, review the stack and practicality in the tools. We have the input for robots operating in the world from cars to humanoid to drones as Lidar, camera radar, gps, stereo cameras, audio microphone, networking for communication, and the various ways to measure kinematics with Imu. The raw sensory data is then processed features. The forum to representations are formed and multiple higher and higher order representations. That's what deep learning gets us before neural networks. Before the advent of, before the recent successes of neural networks to go deeper and therefore be able to form high order representations of the data that was done by experts by human experts today, networks are able to do that. That's the representation piece and on top of the representation piece, the final layers. These networks are able to accomplish the supervised learning task, the generative tasks, and the unsupervised clustering tasks through machine learning.

Speaker 1:          05:09          That's what we talked about a little in lecture one and we'll continue tomorrow and Wednesday. That's supervised learning and you could think about the output of those networks as simple, clean, useful, valuable information. That's the knowledge and that knowledge can be in the form of single numbers. It can be regression, continuous variables. It can be a sequence of numbers, it can be images, audio sentences, text, speech. Once that knowledge is extracted and aggregated, how do we connect it in multi resolution? Always form hierarchies of ideas, connect ideas. The trivial silly example is connecting images, activity recognition and audio. For example, if it looks like a duck, quacks like a duck and swims like a duck, we do not currently have approaches that effectively integrate this information to produce a higher confidence estimate that is in fact the duck and the planning piece. The task of taking the sensory information, fusing the sensory information, and making action control and longer term plans based on that information, as we will discuss today are more and more amenable to the learning approach to the deep learning approach, but to date have been the most successful as non learning optimization based approaches like with the several of the guest speakers would have, including the creator of this robot atlas in Boston Dynamics.

Speaker 1:          06:54          So the question how much of the stack can be learned and to end from the input to the output. We know we can learn the representation and the knowledge from the representation and to knowledge even with the kernel methods of Svm and certainly with with neural networks. Mapping from representation to information has been where the primary success and machine learning over the past three decades has been mapping from raw sensory data to knowledge. That's where the success, the automated representation, learning of deep learning has been a success, going straight from raw data to knowledge. The open question for us today and beyond is if we can expand the red box there of what can be learned end to end from sensory data to reasoning, so aggregating for me higher representations of the extract of knowledge and forming plans and acting in this world from the raw sensory data, we will show the incredible fact that we're able to do learn exactly what's shown here and to end with deeper enforcement learning on trivial tasks in a generalizable way. The question is whether that can then move on to real world tasks of autonomous vehicles, of humanoid, robotics and so on.

Speaker 1:          08:23          That's the open question. So today let's talk about reinforcement learning. There's three types of machine learning.

Speaker 1:          08:32          Supervised unsupervised are the categories that the extremes in relative to the amount of human and human input that's required for supervised learning. Every piece of data that's used for teaching these systems is first labeled by human beings and unsupervised learning on the right is no data is labeled by beings in between is some sparse input from humans. Semi supervised learning is when only part of the data is provided by humans. Ground Truth and the rest was being first gen analyzed by the system, and that's what reinforcement learning falls reinforcement learning as shown there with the cats. As I said, every successful presentation must include cats. They're supposed to be Pavlov's cats and a ringing a bell, and every time they ring a bell, they're given food and they learn this process. The goal of reinforcement learning is to learn from sparse reward data from learn from spar, supervised data, and take advantage of the fact that in simulation or in the real world, there is a temporal consistency to the world.

Speaker 1:          09:52          There is a temporal dynamics that follows from state to state to state through time and so you can propagate information even if the information that you received about the supervision. The ground truth is sparse. You can follow that information back through time to infer something about the reality of what happened before then even if your reward signals were weak, so it's using the fact that the physical world evolves through time in some, some sort of predictable way to take sparse information and generalize it over the entirety of the experience as being learned. So we apply this to two problems today. We'll talk about deep traffic as a methodology, as a, as a way to introduce deeper enforcement. Learning. The deep traffic is a competition that we ran last year and expanded significantly this year and I'll talk about some of the details and how the folks in this room can on your smartphone today or if you have a laptop training agent while I'm talking training a neural network in the browser. Some of the things you've added our. We've added the capability. We've now turned it into a multiagent deep reinforcement learning problem where he can control up to 10 cars within your own network. Perhaps less significant but pretty cool is the ability to customize the way the agent looks so you can upload and people have to an absurd degree have already begun doing so, uploading different images instead of the car that's shown there as long as it maintains the dimensions. Shown here is a space x rocket.

Speaker 1:          11:38          The competition is hosted on the website self driving cars that mit.edu/deep traffic. We'll return to this later. The code is on get hub with some more information, a starter code and the paper describing some of the fundamental insights that will help you win at this competition is an archive, so from supervised learning and lecture one to today. Supervised learning we can think of is memorization of ground true data in order to form representations that generalizes from that Ground Truth. Reinforcement learning is we can think of as a way to brute force propagate that information, the sparse information through time to to assign quality reward to state that does not directly have a reward to make sense of this world. When the rewards a sparse but are connected through time. You can think of that as reasoning, so the connection through time is modeled in most reinforcement learning approach is very simply that there's an agent taken an action in the state and receiving a reward, and the agent operating in an environment executes an action, receives an observed state and use state and receives the award. This process continues over and over and some examples we can think of any of the video games, some of which we'll talk about today, like Atari breakout as the environment. The agent is the paddle.

Speaker 1:          13:36          Each action that the agent takes has an influence on the evolution of the environment and the success is measured by some reward mechanism. In this case, points are given by the game and every game has a different point scheme that must be converted normalized until a way that's interpretable by the system and the goal is to maximize those points, maximize the reward. The continuous problem of card poll by balancing the goal is to balance the pole on top of a moving cart. The state is the angle that angular speed, the position, the horizontal velocity, the actions are the horizontal force applied to the cart, and the award is one at each time step. If the pole is still upright, all the first person shooters, the video games, and now star craft the, uh, strategy games in case of first person shooter and doom. What is the goal? The environment is the game that goes to eliminate all opponents. The state is the raw game. Pixels coming in. The actions is moving up, down, left, right, and so on. And the reward is positive when eliminating and opponent and negative. When the agent has eliminated industrial robotics, been packing with a robotic arm, the goal is to pick up a device from a box and put it into a container. The state is the raw pixels of the real world that the robot observes the actions or the possible actions with the robot that different degrees of freedom and moving through those degrees, moving the different actuators to realize the position of the arm, and then award is positive on placing a device successfully and negative. Otherwise,

Speaker 1:          15:35          everything can be modeled in this way. Mark off decision process as a state, as zero action, a zero and reward received and you state is achieved. Again, Action Reward State Action Award, state until a terminal state is, is reached, and the major components of reinforcement learning is a policy, some kind of plan of what to do in every single state. What kind of action to perform a value function, uh, some kind of sense of what is a good state to be in, of what is a good action to take in a state and sometimes a model

Speaker 1:          16:19          that the agent represents the environment with some kind of sense of the environment. It's operating in the dynamics of that environment that's useful for making decisions about actions. Let's take a trivial example, a grid world, a three by four, 12 squares where you start at the bottom left and their task with walking about this world to maximize reward. The award that the top right is a plus one and a one square below that is a negative one. And every step you take is a punishment or as a negative reward of zero point zero four. So what is the optimal policy in this world? Now, when everything is deterministic, perhaps this is the policy. When you start at the bottom left, well, because every step hurts, every step has a negative reward. Then you want to take the shortest path to the maximum square with a maximum reward. When the state space is non deterministic, as presented before, with the probability of point eight, when you choose to go up, you go up. But with probability point one, you go left and point one, you go right, unfair. Again, much like life, that would be the optimal policy. What is the key observation here that every single in the space must have a plan

Speaker 1:          17:53          because you can't because then a non deterministic aspect of the control, you can't control where are going to end up. So you must have a plan for every place that's the policy. Having an action, an optimal action to take in every single state. Now, suppose we change their reward structure and for every step we take as a negative or award is a negative two, so it really hurts. There's a high punishment for every single step we take. So no matter what, we always take the shortest path, the optimal policies to take the shortest path to the to the only spot on the board that doesn't result in punishment. If we decrease the reward of each step two negative point one, the policy changes were there a some extra degree of wandering encouraged, and as we go further and further in, lowering the punishment does before to negative zero point zero, four more. Wandering and wandering is allowed. And when we finally turned the reward into positive, so every step it, every step is increases the award. Then there's a significant incentive to to stay on the board without ever reaching the destination. Kind of like college for a lot of people.

Speaker 1:          19:28          So the value function, the way we think about the value of the estate or the value of anything in the environment is the reward we're likely to receive in the future and the way we see the reward we're likely to receive as we discount the future award because we can't always count on it, hair or gamma further and further out into the future. More and more discounts decreases the era, the the importance of that reward received and a good strategy is taking the summer of these rewards and maximizing it, maximizing discounted future award. That's what reinforcement learning hopes to achieve. And with cue learning, we use any policy to estimate the value of taking an action in the state. So off policy, forget policy, we move about the world and use the bellman equation here on the bottom to continuously update our estimate of how good a certain action is in a certain state. So we don't need this. This allows us to operate in a much larger space in a much larger action space. We move about this world or simulation or in the real world, taking actions and updating our estimate of how good certain actions are over time.

Speaker 1:          20:59          The new state that the left is the is the update of value. The old state is the starting value for the equation and we update that old state estimation with, uh, some of the reward received by taking action as a tax action, a in status, and the maximum reward that's possible to be received in the following states. Discounted that update is decreased with the learning rate. The higher the learning rate, the more value we, the, the faster we learn, the more value we assigned to new information. That's simple. That's it. That's cute. Learning as simple update rule allows us to, to explore the world and as we explore, get more and more information about what's good to do in this world, and there's always a balance in the various problem spaces we'll discuss. There's always a balance between exploration and exploitation.

Speaker 1:          22:04          As you form a better and better estimate of the function of what actions are good to take, you start to get a sense of what is the best action to take, but it's not a perfect sense. It's still an approximation and so there's value of exploration, but the better and better your estimate becomes less and less. Exploration has a benefit, so usually want to explore a lot in the beginning and less and less so towards the end and when we finally released the system out into the world and wish it to operate as best, then we have it operate as a greedy system, always taking the optimal action according to the q, a q value function and everything I'm talking about now is parametrized and our parameters that are very important for winning the deep traffic competition, which is using this very algorithm within your own network as a score.

Speaker 1:          23:03          So for simple table representation of a function where the y axis to state four states s one, two, three, four, and the x axis is actions a, one, two, three, four. We can think of this table as randomly initiated or initiated initialized in any kind of way that's not representative of actual reality. And as we move about the world and we take actions, we update this table with a bellman equation shown up top and here slides now are online. You can see a simple pseudo code algorithm of how to update it. I to run this bellman equation and over the approximation becomes the optimal q table. The problem is when that cute table, it becomes exponential in size. When we take in raw sensor information as we do with cameras with deep crash or deep traffic sticking the full grid space and taking that information, the Ra, the Ra grid pixels of deep traffic. When you take the arcade games here, they're taking the raw pixels of the game or when we take go the game of go when it's taking the units, um, the, the board, the raw state of the board as the input, the potential state space, the number of possible combinatorial variations of what states as possible is extremely large, larger than we can certainly hold the memory and larger than we can ever be able to accurately approximate through the bellman equation over time to simulation

Speaker 1:          24:50          through the simple update of the equation. So this is where deep reinforcement learning comes in. Neural networks are really good approximators. They're really good at exactly this task of learning this kind of queue table. So as we started with supervised learning, when all networks help us memorize patterns, using supervised ground truth data, and we'll move to reinforcement learning that hopes to propagate outcomes to knowledge. Deep learning allows us to do so. A much larger state spaces are much larger action spaces, which means it's generalizable, it's much more capable to deal with the raw stuff of sensory data, which means it's much more capable to deal with a broad variation of real world applications, and it does so because it's able to learn the representations as we've discussed on Monday. They're understanding comes from converting the raw sensor information into into simple, useful information based on which the action in this particular state can be taken in the same exact way, so instead of the queue table, instead of this queue function with plugging in your network where they input is the state space. No matter how complex and the output is a value for each of the actions that you could take.

Speaker 1:          26:34          Input is the state. Opera is the value of the function. It's simple. This is deep q network dqn at the core of the success at deep mind. A lot of the cool stuff you see about video games, dqn or variants of dqs or play. This is what a first with a nature paper, deep mind, the success came of playing the different games including Atari Games. So how are these things trained? Very similar to supervise learning. The bellman equation, laptop takes the reward and the discounted expected reward from future states. The loss function here for neural network. Then you'll network learns with the loss function. It takes the reward received at the current state. Does the forward pass through in your network to estimate the value of the future state of the best action to take in the future state and then subtracts that from the forward pass through the network for the current state of action. So you take the difference between what your Acu estimator, than your own network believes the value of the current state is, and what it more likely as to be based on the value of the future states that are reachable based on the actions you can take.

Speaker 1:          28:26          Here's the algorithm. Input is the state output as the q value for each action or in this diagram and as a state of inaction, and the output is the q value. It's very similar architectures, so given the transition of s a r s prime as current state taken an action, we're seeing your reward and achieving as prime state. The update is do a feed forward past to the network for the current state, do a feed forward, pass for each of the possible actions taken in the next state, and that's how we compute the two parts of the loss function and the update the weights using backpropagation. Again, last function backpropagation is how the network is trained. This has actually been around for much longer than deep mind. A few tricks made made it really work. Experience replays the biggest one,

Speaker 1:          29:38          so as the games are played through simulation or if it's a physical system as it acts in the world, it's actually collecting the observations into a library of experiences and that training is performed by randomly sampling the library in the past by randomly sampling that previous experiences in batches. So you're not always training on the natural continuous evolution to the system you're training on randomly picked batches of those experiences. That's a huge. It's a. it's a, seems like a subtle trick, but it's a really important one. So the system doesn't overfit a particular evolution of, of, uh, of the game of the simulation. Uh, another important, again, subtle trick as in a lot of deep learning approaches, the subtle tricks make all the difference is fixing the target network for the last function. If you notice, you have to use the neural network, the neural network, the Gq, I network to estimate the value of the current state and action pair and the next.

Speaker 1:          30:58          So using it multiple times. And as you perform that operation, you're updating the network, which means that target function inside that lost function is always changing. So you're, the very nature of your loss function is changing all the time is you're learning. And that's a big problem for stability that can create big problems for the learning process. So this little trick is to fix the network and only update it every say thousand steps. So as you train the network, the network that's used to compute the target function inside the last function is fixed. It produces a more stable computation on the loss function. So the ground doesn't shift under you as you're trying to find a minimal for the loss function. The last function doesn't change and unpredictable, difficult to understand ways and reward clipping, which is always true with general. Those systems that are operating a seeking to operate in a generalized way is for very for these various games, the points are different, some, some points are low, some points are high, some go positive, negative, and they're all normalized to a point where the good points are, the positive points are a one and negative points are a negative one.

Speaker 1:          32:30          That's reward clipping, simplify the reward structure and because a lot of the games have 30 fps or 60 fps and the actions are not, it's not valuable to take actions as such a high rate inside of these particularly Atari Games. Then you only take an action every four steps while still taking into the frames as part of the temporal window to make decisions, tricks. But hopefully it gives you a sense of the kinds of things necessary for both seminal papers like this one and for the more important accomplishment of winning deep traffic is the. Is the tricks make all the difference here? On the bottom is the circle is when the technique is used and the excellent. It's not looking at replay and target, fixed target, network and experience replay when both are used for the game of breakout river, raid, sciquest and space invaders. The higher the number, the better it is, the more points achieved so when it gives you a sense that one replay and target both give significant improvements in the performance of the system, order of magnitude improvements to orders of magnitude for breakup. And here is pseudocode of implementing dqn. The learning.

Speaker 1:          34:02          The key thing to notice and you can look through the slides, is the, the, the loop, the wild loop of playing through the games and selecting the actions to play is not part of the training. It's part of the saving, uh, the observations, the state action reward next state observations is saving them and to replay memory into that library. And then you sample randomly from that replay memory to then train the network based on the last function

Speaker 1:          34:38          and with probability up up top of the probability epsilon select a random action that epsilon is the probability of exploration that decreases at something you'll see in traffic as well, is the rate of which that exploration decreases over time through the training process. You want to explore a lot first and less and less over time. So this algorithm has been able to accomplish in 2015 and since a lot of incredible things, things that made the ai world think that we were onto something that general ai is within reach for the first time that raw sensor information was used to create a system that acts it, make sense of the world, make sense of the physics of the world enough to be able to succeed in it for very little information. But these games are trivial even though there is a lot of them.

Speaker 1:          35:48          This dq and approach has been able to outperform a lot of the Atari Games. That's what been reported on, outperform the human level performance. But again, these games are trivial. What I think, and perhaps biased, I'm biased, but one of the greatest accomplishments of artificial intelligence in the last decade, at least from the philosophical or the research perspective, is Alphago Zero First Alphago, Alphago zero is deepmind system that beat the best in the world in the game of go. So what's the game of go simple. I won't get into the rules, but basically it's a 19 by 19 board showing on the bottom of the slide for the bottom row of the table for board of 19 by 19. The number of legal game positions is two times 10 to the power of one 70. It's a very large number of possible positions to consider at any one time, especially the game evolves. The number of possible moves is huge, much larger than in chess. So that's why Ai Community thought that this game is not solvable until 2016 when Alphago used to use human expert position play to seed in a supervised way. Reinforcement learning approach, and I'll describe it in a little bit of detail in this couple of slides here to beat the best in the world

Speaker 1:          37:42          and then Alphago zero. That is the accomplishment of the decade for me in Ai is being able to play with no training data on human expert Games and beat the best in the world and an extremely complex game. This is not a tare. This is an this is a a much higher order difficulty game and the and the quality of player that is competing in as much higher and it's able to extremely quickly here to achieve a rating that's better than Alphago and better than the different variants of Alphago and certainly better than the best of the human players in 20 days of self play. So how does it work? All of these approaches, much, much like the previous ones. The traditional ones that are not based on deep learning are using Monte Carlo tree search MCTs, which is when you have such a large state space, you start at a board and you play and you choose moves with some exploitation exploration.

Speaker 1:          39:08          Balancing choosing to explore a totally new positions or to go deep on the positions you know are good until the bottom of the game is reached until the final state is reached and then you back propagate the quality of the choices you've made leading to that position and in that way you learn the value of of board positions and play that's been used by the most successful. Go playing engines before and Alphago sense, but you might be able to guess what's the difference with Alphago versus the previous approaches. They use the neural network as the intuition quote on quote to what are the good states, what are the good next board positions to explore, and the key things, again, the tricks make all the difference that made Alphago zero work and work much better than Alphago is first because there was no expert play instead of human games.

Speaker 1:          40:23          Alphago used that very same Monte Carlo tree search algorithm, MCTs to do an intelligent look ahead based on in neural network prediction of what are the good states to take. It checked that instead of human expert play it checked. How good indeed are those states? It's a simple look ahead action that does the ground truth that does the target, a correction that produces the last function. The second part is the multitask learning or what's now called multitask learning is the network is is a quote unquote two headed in the sense that first it outputs the probability of which moved to take the obvious thing and it's also producing a probability of winning and there's a few ways to combine that information and continuously train both parts of the network depending on the choice taken. So you want to take the best choice in the short term and achieved the positions that are highly likelihood of winning for the player. That's whose turn it is, and another big step is that they updated from 2015 the update of the state of the art architecture, which are now the architecture. That one image that is residual networks resume yet for image net. Those that's it. And those little changes made all the difference. So that takes us to do deep traffic and 8 billion hours stuck in traffic

Speaker 1:          42:02          America's pastime. So we tried to simulate driving that behavior layer of driving, so not the immediate control, not the motion planning, but beyond that on top, on top of those control decisions, the human interpretable decisions of changing lane speeding up, slowing down, modeling that in a microtrauma traffic simulation framework that's popular and traffic engineering, the kindness shown here, we applied deeper enforcement learning to that, we call it deep traffic. The goal is to achieve the highest average speed over a long period of time weaving in and out of traffic for students here, their requirement is to follow the tutorial and that Shiva speed of 65 miles an hour, and if you really want to achieve a speed over 70 miles an hour, which is what's required to win and perhaps upload your own image to make sure you look good doing it. What you should do, clear instructions to compete. Read the tutorial. You can change parameters in the cold box on that website. Cars that mit.edu site traffic. Click the button that says apply code, which applies the code that you write. These are the parameters that you specify for. Then you'll network it, applies those parameters, creates the architecture that you specify, and now you have a network written in javascript living in the browser ready to be trained. Then you click the blue button that says run training and that trains the network

Speaker 1:          43:49          much faster than what's actually being visualized in the browser a thousand times faster by evolving the game, making decisions, taking in the grid space, as I'll talk about here in a second, the speed limit is 80 miles an hour based on the various adjustments were made to the game. Reaching 80 miles an hour, certainly impossible on average and reaching some of the speeds that we achieved last year is much, much, much more difficult. Finally, when you are happy and the training is done, submit the model to competition for those super eager, dedicated students. You can do so every five minutes and to visualize your submission, you can click the request visualization, specifying the custom image and the color. Okay, so here's the simulation. Speed limit, 80 miles an hour, cars 20 on the screen, one of them is a red one in this case, that's that one is controlled by neural network. It's speed. It's allowed the actions of speed up, slow down, change lanes, left, right or stay exactly the same. The other cars are pretty dumb. They speed up, slow down, turn left, right, but they don't have a purpose in their existence. They do so randomly or at least purpose has not been discovered. The road, the car, the speed, the road is a grid space and occupancy grid that specifies when it's empty. It's set to

Speaker 1:          45:35          Ab, meaning that the grid value is whatever speed is achievable. If you were inside that grid and when there's other cars that are going slow, the value in that grid is the speed of that car. That's the state space. That's the state representation and you can choose how much, what slice that state space you've taken. That's the input to the neural network. For visualization purposes, you can choose normal speed or fast speed for watching the network operate

Speaker 1:          46:15          and there's display options to help you build intuition about the network takes in and what space the cars operating in. The default is there's no extra information is added. Then there's the learning input which visualizes exactly which part of the road the serves as the input to the network. Then there is the safety system, which I'll describe in a little bit, which is all the parts of the road the car is not allowed to go into because it would result in a collision and that with javascript will be very difficult to animate and the full map. Here's the safety system. You can think of this system as ACC basic radar ultrasonic sensors, helping you avoid the obvious collisions to obviously detectable objects around you. And the task for this red car, for this neural network is to move about this space, uh, is to move about the space under the constraints of the safety system. The red shows all the parts of the greatest, not able to move into. So the goal for the car is to not get stuck in traffic, is make big sweeping motions to avoid crowds of cars. The input like dqn is the state space. The output is the value of the different actions and based on the absalon parameter to training and through inference evaluation process, you choose how much exploration you want to do. These are all parameters. The learning is done in the browser on your own computer,

Speaker 1:          47:59          utilizing only the CPU,

Speaker 1:          48:02          the action space. There's five, giving you some of the variables here. Perhaps you'd go back to the slides to look at it. The brain, quote unquote, is the thing that takes in the state and the reward takes a forward pass to the state and producer. The next action, the brain is where the neural can. It's contained both of the training and the evaluation. The learning input can be controlled in width, forward length and backward length. Lane side number of lanes to the side that you see patches ahead as the patches ahead that you see patches behind his patches behind these. See new this year can control the number of agents that are controlled by the neural network anywhere from one to 10,

Speaker 1:          48:58          and the evaluation is performed exactly the same way. You have to achieve the highest average speed for the agents. The very critical thing here is the agents are not aware of each other so they're not jointly jointly planning. The network is trained under the joint objective of achieving the average speed for all of them, but the actions of taking in a greedy way for each. It's very interesting what can be learned in this way because this kinds of approaches are scalable to an arbitrary number of cars and you could imagine us plopping down the best cars from this class together and having them compete in this way, the best neural networks because there are full in their greedy operation. The number of networks that can concurrently operate is fully scalable. There's a lot of parameters. The temporal window, the layers, the many layers types that can be added here is a fully connected layer with 10 year olds. The activation functions. All of these things can be customized as specified in the tutorial, the final layer, a fully connected layer with I'll put a five regression given the value of each of the five actions and there's a lot of more specific parameters, some of which I've discussed from gamma to epsilon

Speaker 1:          50:38          to experience replay size to learning rate and temporal window, the optimizer, the learning rate momentum, batch size l two l, one, two, k for regularization and so on. There's a big white button that says apply code that you press that kills all the work you've done up to this point, so be careful doing it. It should be doing it only at the very beginning.

Speaker 1:          51:05          If you happen to leave your computer running and training for several days is as folks have done the blue training button, you press and it trains based on the parameters you specify and the network state gets shipped to the main simulation from time to time. So the thing you see in the browser, as you open up the website is running the same network that's being trained and regularly updates that network so it's getting better and better. Even if the training takes weeks for you, it's constantly updated the network you see on the left, so if the car for the network that your training is just standing in place and not moving, it's probably time to restart and change the parameters. Maybe add a few layers to your network, number of iterations, a certainly an important parameter to control and the evaluation is something we've done a lot of work done since last year to remove the degree of randomness, to remove the the incentive to submit the same code over and over again to hope to produce a higher award, a higher evaluation score.

Speaker 1:          52:14          The method for evaluation is we collect the average speed over 10 runs, about 45 seconds of game each, not minutes, 45 seconds, and there is five hundreds of those and we take the median speed of the 500 runs has done server side. It's extremely difficult to cheat. I urge you to try. You can try it locally. There's a start evaluation run, but that one doesn't count. That's just for you to feel better about your network. That's. That should produce a result that's very similar to the one we were produced on. The server is to build your own intuition and as I said, was significantly reduce the influence of randomness, so the the score of the speed you get for the network you designed should be very similar with every evaluation. Loading is saving. If the network is huge and you want to switch computers, you can save the network. It saves both the architecture of the network and the weights and the on the network and you can load it back in. Obviously when you load it in, it's not a saving any of the data you've already done. You can't do transfer learning with javascript in the browser yet submitting your network, submit model to competition and make sure you run training first. Otherwise it will be initiated or initiated randomly and will not do so well. You can resubmit as often as you like, and the highest score is what counts. The coolest part is you can load your custom image, specify colors, and request the visualization.

Speaker 1:          53:57          We have not yet shown the visualization, but I promise you it's going to be awesome. Again, read the tutorial, change the parameters and the code box. Click apply code run training. Everybody in this room on the way home on the train, hopefully not in your car, should be able to do this in the browser and they can visualize request visualization because it's an expensive process. You have to want it for us to do it well because we have to run in server side. Competition link is their get hub started. Code is there and the details for those that truly want to win is in the archive paper. So the question that will come up throughout is whether these reinforcement learning approaches are at all or rather if action planning control is amenable to learning. Certainly in the case of driving, we can't do it. Alpha go zero did. We can't learn from scratch from self play because that will result in millions of crashes in order to learn to avoid the crashes. Unless we're working like we are deep crashed on the RC car or we're working on a simulation so we can look at expert data, we can look at driver data, which we have a lot of and learn from. It's an open question. This is applicable to date that I'll bring up two companies because they're both guest speakers

Speaker 1:          55:28          deep irl is not involved in the most successful robots operating in the real world. In the case of Boston dynamics, most of the perception control and planning I can. This robot does not involve learning approaches except with minimal addition on the perception side. Best of our knowledge and certainly the same is true with Waymo. As the speaker on Friday, we'll talk about deep learning is used a little bit in perception on top, but most of the work is done from the sensors and the optimization based, the model based approaches, trajectory generation and optimizing which trajectory trajectory is best to avoid collisions. Deep Arrows not involved and coming

Speaker 2:          56:24          back and back again. The unexpected local is a higher award which arise in all of these situations and applied in the real world. So for the cat video, that's pretty short where the cats are ringing the bell and they're learning that the ringing of the bell is, is mapping to food. I urge you to think about how that can evolve over time in unexpected ways that may not have a desirable effect where the final reward is in the form of food and the intended effect is to ring the bell.

Speaker 1:          57:03          That's where ai safety comes in for the artificial general intelligence course in two weeks. That's something we'll explore extensively. It's how these reinforcement learning planning algorithms will evolve in ways that not expected and how we can constrain them, how we can design reward functions that result in safe operation. So I encourage you to come to the talk on Friday at 1:00 PM as a reminder. So 1:00 PM that 7:00 PM instead of 32, one, two, three, and two, the awesome talks in two weeks from Boston dynamics to Ray Kurzweil and so on for Agi. Now tomorrow we'll talk about computer vision and psych fuse. Thank you everybody.