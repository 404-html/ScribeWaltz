Speaker 1:          00:00          Welcome back to six, says zero, nine, nine artificial general intelligence. Today we have Eylea says Giver, Cofounder and Research Director of Open Ai. He started in the aml group in Toronto. Geoffrey Hinton, then at Stanford with Andrew Wang. Co founded DNN research for three years as a research scientist at Google brain and finally cofounded open ai citations aren't everything, but they do indicate impact. And his work, recent work in the past five years has been cited over 46,000 times. He has been the key creative intellect and driver behind some of the biggest breakthrough ideas and deep learning and artificial intelligence ever. So please welcome Eylea.

Speaker 2:          00:56          Thanks.

Speaker 1:          00:59          Alright, thanks for the introduction next. Alright, thanks for coming to my talk. I will tell you about some work we've done over the past year on on Melania and self at open Ai and before I dive into some of the more technical details of the work, I wanted to spend a little bit of time talking about deep learning and why it works at all in the first place, which I think it's actually not as self evident, seeing that they should work. One fact it's actually a fact, it's a mathematical theory that you can prove is that if you could find the this program does very, very well in your data, then you will achieve the best generalization possible with a little bit of modification. You can turn it into a precise theory and on a very intuitive level, it's easy to see why it shouldn't be the case.

Speaker 1:          02:00          If you have some data and you're able to find a short term program which generates this data, then you've essentially extracted all the old conceivable regularity from this data into your program and then you can use this objects to make the best predictions possible. If if you have data which is so complex and there is no way to express it as a shorter program and it means that your data is totally random, there is no way to extract any irregularity from it whatsoever. Now, there is little known mathematical theory behind this and the proofs of these statements. Actually, not even that hard, but the one minor, slight disappointment is that it's actually not possible, at least given today's tools and understanding to find the best short program that explains or generates or solves your problem. Given your data. This problem is computationally intractable. The space of old programs is a very nasty space.

Speaker 1:          03:03          Small changes to a program result in massive changes in the behavior of the program as it should be. It makes sense. We have a loop. We changed the inside of the loop. Of course you get something totally different, so the space of programs is so hard, at least given what we know today, search, there seems to be completely off the table. Well, if we give up on the short side on short programs, what about small circuits? Well, it turns out that we are lucky. It turns out that when it comes to small circuits, you can just find the best small circuits circuit that solves a problem using backpropagation, and this is the miraculous fact on which the rest of ai stands. It is the fact that then you have a circuit and you impose constraints on your circuits. On your circuit using data, you can find a way to satisfy these constraints, these constraints using backdrop by iteratively making small changes to the weights of your neural network until its predictions satisfy the data.

Speaker 1:          04:13          Well, this means is that the computational problem that sober backpropagation is extremely profound. It is circuit search. Now we know that you can solve with solid all this, but you can solve it sometimes and you can solve it that those times where we have a practical data set, it is easy to design artificial data sets for which you not find the best neural network, but in practice it seems to be not a problem. Are you going to think of training neural network as solving an equation? In many cases where you have a large number of equation terms like this, Fox, Scifi equals way, so you've got your parameters and they are present all your degrees of freedom and you use gradient descent to push the information from these equations into the parameters of satisfy them all. And you can see that the neural network, let's say one with 50 layers is basically a parallel computer that is given 50 times steps to run and you can look quite a lot to be the 50 slash 50 times step, sort of very, very powerful.

Speaker 1:          05:21          I'm massively parallel computer. So for example, I think it is not widely known that you can learn to sort sort an n beat numbers using a modestly sized neural network with just two hidden layers, which is not bad. It's not self evident. Especially since we've been taught that sorting requires logan parallel steps with a neural network, you can sort successful using only two parallel steps. So there's some things like an obvious going on now these a part of those steps of thresholds for threshold neurons, so they're doing a little bit more work. That's an answer to the mystery, but if you've got 50 such layers, you can do quite a bit of logic, quite a bit of reasoning all inside the neural network. And that's why it works. Given the data, we are able to find the best neural network and because the neural network is deep because it can run computation inside of inside of its layers, the best neural network is worth finding because that's really what you need.

Speaker 1:          06:27          You need something you need to model class which is worth optimizing, but it also needs to be optimized. Mobile and deep neural networks satisfied both of these constraints, and this is why everything works, so this is the basis on which everything else resides. Now, I wanted to talk a little bit about reinforcement learning. Reinforcement learning is a framework. It's a framework of evaluating agents in their ability to achieve goals and complicated stochastic environments. You've got an agent which is plugged into an environment as shown in the figure right here and for any given agent, you can simply run it many times and computers average award. Now, the thing that's interesting about the reinforcement learning framework is that there exist interesting, useful reinforcement learning algorithms. The framework existed for a long time. It became interesting once we realized that good algorithms exist. Now, these are perfect algorithms, but they're good enough to do interesting things and all you want. The mathematical problem is one where you need to maximize the expected reward.

Speaker 1:          07:43          Now, one important way in which the reinforcement learning framework is not quite complete is that it assumes that the reward is given by the environment. You see this picture, the agent sends an action while the reward sends it the observation, and they're both of the observation and the reward backwards. That's what the environment communicates back. One way in which this is not the case in the real world is that we figure out what the reward is from the observation we reward ourselves. We are not told. Environment doesn't say, Hey, here's some negative reward. It's our interpretation over census that lets us determine what the reward is and there is only one real reward in life and this is existence or nonexistence and everything else is a corollary of that. So well, what should the region be? You already know the answers should be a your own network because whenever you want to do something, then it's going to be a neural network and you want the agents to map observations to actions so you let it be parameterized within your own net.

Speaker 1:          08:49          Then you apply learning algorithm, so I want to explain to you how reinforcement learning works. This is model free reinforcement learning. Reinforcement learning has actually been using practice everywhere, but he's also deeply in it. It's very robust. It's very simple. It's also not very efficient, so the way it works is the following. This is literally the one sentence description of what happens. In short, try something new. I'd randomness directions and compare the results to your expectation. If the result surprises you. If you find that the results exceeded your expectation, then change your parameters, but they could those actions in the future. That's it. This is the full idea of reinforcement learning. Try it out, see if you like it, and if you do more of that in the future, and that's it, that's literally, this is the core idea. Now it turns out it's not difficult to formalize mathematically, but this is really what's going on in a neural network, in irregular neural network, I guess you might say, okay, what's the goal? Iran, the neural network. You get an answer, you compare it to the desired answer and whatever difference you have between those, do you send it back? The changing the neural network that's supervised learning and reinforcement learning. You're running your own network, you had a bit of randomness, direction, and then you feel like the result, you're randomness turns into the desired target in effect, so that's it. Trivial.

Speaker 1:          10:28          Now math exists without explaining what these equations mean. The point is not for you to derive them, but just to show that they exist. There are two classes of reinforcement learning algorithms. One of them is the policy gradients where basically what you do is that you take this expression right there, the will expect expected, some of rewards and it just crunched through the derivatives. You expand the terms, Iran do some Algebra and you get the derivative and miraculously the derivative has exactly the form that I told you which is try some actions and if you like them increasing the low probability of the actions. That's literally follows from the math. It's very nice when the intuitive explanation as a one to one correspondence to what you get in the equation. Even though you'll have to take my word for it if you're not familiar with it.

Speaker 1:          11:24          That's the question. At the top there was a different class of reinforcement learning algorithms which is a little bit more difficult to explain. It's called a cue learning based algorithms. They are a bit less stable and beat, more simple efficient and it has the property is that it can learn not only from the data generated by the actor but from any other data as well. So it has, it has some it, it has a different robustness profile. It should be a little bit important, but it's only going to be the legality. So yeah, this is the policy of policy distinction, but it's a little bit technical. So if you find this hard to understand, don't worry about it. If you already know, then you already know it. So now what's the potential of reinforcement learning? Wasn't the promise. What is it actually why? Why should we be excited about it?

Speaker 1:          12:16          Now, there are two reasons the reinforcement learning algorithms have today already useful and interesting, and especially if you have a really good simulation of your world, you could train agents to lots of interesting things, but what's really exciting is if you can build a super amazing sample efficient algorithms, reinforcement learning algorithm, which is give it a tiny amount of data and the algorithm just crunches through it and extract every bit of entropy out of it in order to learn in the fastest way possible. Now, today or algorithms are not particularly efficient. They are data inefficient, but as our field keeps making progress, this will change. Next, I want to dive into the topic of metal learning.

Speaker 1:          13:02          The goal of metal learning. So Methadone is a beautiful idea that doesn't really work, but it kind of works and it's really promising too. It's another promising idea. So what's the dream? We have some learning algorithms, perhaps you could use those learning algorithm is in order to learn to learn. I'd be nice if we could learn to learn, so how can you do that? It will take a system which you train it not on one task but on many tasks and you ask it if it learns to solve these tasks quickly and that may actually be enough. So here's how it looks like. Here's how most traditional method learning look works like a looks like you have a model which is a big neural network, but what you do is that you treat every instead of training cases, you have training tasks and instead of test cases, you have test tasks, so your input maybe instead of just your current test case, it will be all the information about the news, about the test tasks plus the test case, and you'll try to output the prediction reaction for that test case.

Speaker 1:          14:14          So basically you say, yeah, I'm going to give you your 10 examples as part of your input to your model, figured out how to make the best use of them. It's a really straightforward idea. You turn the neural network into the learning algorithm by turning a training task into a training case, so training, task it constraining case. This is learning this one sentence and so there've been several success stories which I think are very interesting. One of the success stories of learning is learning to recognize characters quickly. So they've been a dataset produced by MIT, by Lake at all, and this is a data set. We have a large number of different handwritten characters and people have been able to train extremely strong metal learning system for this task and now the successful and other very successful example of metal learning is neural architecture. Search by is openly from Google where they found a neural architecture that sold one problem, well small problem and then you could generalize it and then it will successfully solve large problems as well.

Speaker 1:          15:29          So this is the kind of the, the small number of bits matter learning. It's like when you learn the architecture or maybe even learn a program, a small program or learning algorithm, and she applied to new tasks. So this is the other way of doing metal learning. So anyway, but the point is what's happening, what's really happening in matter learning in most cases is that you turn a training task into a training case and pretend that this is totally normal, normal, deep learning. That's it. This is the entirety of metal learning. Everything else that just minor details. Next I want to dive in. So now that I finished the introduction section, I want to start discussing different work by different people from open ai and I want to start by talking about mindset experience, replay. There's been a large effort by unrecalled. Richard, I'll develop a learning algorithm for reinforcement learning that doesn't solve just one task, but it solves many tasks and it learns to make use of it's experience in a much more efficient way.

Speaker 1:          16:35          And I want to discuss one problem in reinforcement learning. It's actually, I guess a set of problems which are related to each other, like one really important thing you need to learn to do is to explore your in that you start out in an environment you don't know what to do, what do you do? So one very important thing that has to happen is that you must get rewards from time to time. If you try something and you don't get rewards, then how can you learn? So said that's the kind of the crux of the problem. How do you learn? And relatedly, is there any way to meaningfully benefit from your, from the experience, from your attempts to, from, from your failures. If you try to achieve a goal and you fail, can you still learn from it? You tell you instead of asking your algorithm to achieve a single goal, you want to learn a policy that they can achieve a very large family of goals.

Speaker 1:          17:34          For example, instead of reaching one state, you want to learn a policy that reaches every state on your system. Now, what's the implication? Anytime you do something, you achieved some state. So let's suppose you say I want to achieve state A. I try my best and I ended up achieving state B. I can either conclude while that was disappointing, I haven't flown almost anything. I still have no idea how to, how to achieve state aid. But alternatively I can say, well, wait a second, I've just reached the perfectly good state, which is b, can I learn how to achieve state, be for my attempt to achieve state a and answer is yes, you can, and it just works and I just want to point out this is the one case, there's a small subtlety here which may be interesting to those of you who are very familiar with the distinction between non policy and off policy.

Speaker 1:          18:31          When you try to achieve a, you are on your own policy learning for reaching the state a, but you're doing off policy learning fruition to state b because you will take different actions if you actually try to reach them beat. So that's why it's very important that the algorithm you use here can support of policy learning. But that's a minor technicality at the crux. The crux of the idea is you make the problem easier by ostensibly making it harder by training assistant which can, which aspires to reach, to learn to reach every state, to learn to achieve every goal, to learn to master it's environment. In general, you build a system which always learned something, it learns from success as well as from failure because if it tries to do one thing, one thing and it does something else, it now Australian data for how to achieve.

Speaker 1:          19:23          That's something else we want to show you a video of how this thing works in practice. So one challenge in reinforcement learning systems is the need to shape the reward. So what does it mean? It means that at the beginning of the at the, at the start of learning, then the system doesn't know much, it will probably not achieve your goal, and so it's important that you design your reward function to give it gradual increments to make it smooth and continuous so that even when the system is not very good, it achieves the goal. Now, if you give your state, your system a very sparse report where the reward is achieved only when you reach a final state, very hard for normal reinforcement learning algorithms to solve the problem because naturally you'll never get the reward so you never learn. No reward means no learning. But here, because you learn from failure as well as some success, there's this problem simply doesn't occur. And so this, this, this is nice. I think, you know, let's, let's look at the videos a little bit more. Like it's nice how say it confidently and energetically moves the little green buck. Please target. And here's another one.

Speaker 1:          20:50          Okay. So we can skip the physical. It works on the physical robot as well, but he can skip it. So I think the point is the hindsight central experience, replay algorithm is directionally correct because you want to make use of all your data and not only a small fraction of it. Now one huge question is where do you get the high level states? Where do the high level states come from? Because in the work that I've shown you so far, the system is asking your goal in the states. So I think one thing too will become very important for these kinds of approaches. Is it a presentation, learning and unsupervised learning. Figure out what are the rates, what are the right states? What's the state space of goals that's worth achieving?

Speaker 1:          21:41          Now I want to go through some real method learning results and I'll show you a very simple way of doing to real from simulation to the physical. Robert, with Metta learning, and this is where went panadol was in a an entrance and an a really nice intern project and vain 17, so I think we can agree that in the domain of robotics, it would be nice if you could train your policy in simulation and then somehow this knowledge would carry over to the physical robbed. Now we can build, we can build simulators that are okay, but they can never perfectly match the real world unless you want to have an insanely slow simulator. And the reason for that is that it turns out that stimulating freakers simulating contacts is super hard. And I heard somewhere, correct me if I'm wrong, that simulating friction is NP complete. I'm not sure, but it's like stuff like that. So your simulation is just not going to match reality. There'll be some resemblance, but that's it. How can we address this problem? And I wanted to show you one simple idea.

Speaker 1:          23:03          So let's say one thing. What's one thing that would be nice is that if you could learn a policy, learn a policy that will quickly adapt itself to the real world. Well, if you want to learn a policy that can quickly adapt, we need to make sure that it has opportunities to adapt you in training time. So what do we do? Instead of solving a problem in just one simulator, we add a huge amount of variability to the simulator. We say we will randomize the friction so we will randomize the masses, the length of the different objects and their I guess and dimensions. So you tried to randomize physics, the simulator and lots of different ways, and then importantly you don't tell the policy how we randomized it, so what is it going to do? Then you'll take your policy and you put it in an environment.

Speaker 1:          23:58          Then says, well, this is really, really tough. I don't know what the masses are and I don't know what the frictions are. I need to try things out and figure out where the friction is. As I get responses from the environment. So you build it, you learn a certain degree of adaptability into the policy and it actually works. I just want to show you, this is what happens when you just straight up policy in simulation and deployed on the physical robots and here the goal is to bring the hockey puck towards the red dots and you will see that it will struggle

Speaker 1:          24:37          and there isn't it struggles is because of the systematic differences between the simulator and the real physical Robert. So I could even the basic movement is difficult for the policy because the assumptions of allocated so much. So if you do the training, as I discussed, we train the recurrent neural network policy which learns to quickly infor properties of the simulator in order to accomplish the task. You can give it to the real thing, the real physics and it will do much better. Now this is not a perfect technique, but it's definitely very promising. It's promising whenever you are able to sufficiently randomized simulator.

Speaker 1:          25:19          So he's definitely very nice to see the closed loop nature of the policy. You consider that it would push the hockey puck and he would correct it very, very gently to bring into the goal so that, that was cool. So that was very, uh, that, that, that was a cool application of metal learning. I want to discuss one more application of Methadone, which is learning the hierarchy of actions. And this was working with and by France at all. Actually I'm Kevin France or the intranasal did. It wasn't high school. I mean he wrote this paper. So

Speaker 1:          26:02          one thing that would be nice is if reinforcement learning was hierarchical, if instead of simply taking my corrections, you had some kind of util sub routines that you could deploy. Maybe the term sub routine is a little bit too crude, but if you had some idea of which action primitives are, we're starting to now no one has been able to get actually like real value add from curriculum reinforcement learning. Yet so far all the really cool results or that really convincing is also reinforcement learning does not use it. That's because we haven't quite figured out what's the right way for reinforcement learning. Reinforcement learning. I just want to show you one very simple approach where you use metal learning to learn to the hierarchy of actions. So here's what you do. You have in this specific work, you have a certain yellow, let's say you have a certain number of low level primitive, so let's say you have to of them and you have a distribution of tasks and your goal is to learn low level primitives such that when they are used inside a very brief run of some reinforcement learning algorithm, it will make as much progress as possible.

Speaker 1:          27:30          So the idea is you want to get the greatest amount of progress. You want to learn policies that result in the great. Sorry. You want to learn primitives the result in the greatest amount of progress possible when used inside learning. So this is a mental learning center because any distribution of tasks, and here we've had before, we've had a little maze, he have a distribution of amazes and in this case and the little bug learned three policies which morbid in speaks to direction. And as a result of having this hierarchy, you are able to solve problems really fast, but only when the hierarchies correct. So correct. Called reinforcement learning is still a work in progress. And this was in this work is an interesting proof point of how curricular reinforcement would be like. Hi, how correct. Called reinforcement learning could be like if it worked now I wanted to just spend one slide addressing the limitations of high capacity method learning. The specific limitation is that

Speaker 1:          28:40          the training task distribution has to be equal to the test task distribution and I think this is a real limitation because in reality you the new task that you want to learn to in some ways being fundamentally different from anything you've seen so far. So for example, if you go to school, you learn lots of useful things, but then when you go to work, only a fraction of this, of the things that you've learned carries over. Can you need to learn if he wouldn't have quite a few more things from scratch. So madeloni would struggle with that because it really assumes that the training, the training data is the distribution over the training task has to be equals with distribution over the test asks. That's the limitation, the thing that as we develop better algorithms for being robust when the

Speaker 1:          29:33          test tasks outside of the distribution of the training desk than metal and got much better. Now I want to talk about self play thing. Self plays a very cool topic that's starting to get attention only now and I want to start by reviewing very old work cold td Gammon. It's back from the older way from 1992. So 26 years old now. It was done by Jerry to Sarah. So this work is really incredible because it has so much relevance today. What they did basically they said, okay, let's take two neural networks and lets them let them play against each other, let them play backgammon against each other and let them try. Let them be trained with culinary. So it's a super modern approach and you would think this was a paper from 2017 except with you look at the splots. It shows that you only have 10 hidden units, 20 hidden units, 40 and 84. The difference in colors where you noticed that the largest neural network works best, so in some ways not much has changed and this is the evidence and in fact they were able to beat the world champion and backgammon and they were able to discover new strategies that the best human, a backgammon players weren't a have not noticed and they've determined that the strategy is covered, but ed gamut actually better. So that's pure stuff. Play with cue learning which is which remained dormant until the Deq and work with Atari Buddy of mine.

Speaker 1:          31:21          So now other examples of self blame include Alphago Zero, which was able to learn to beat the world champion in go without using any external data whatsoever. And other results of this vein is by open ai, which is our daughter to bought, which was able to build the world champion on the one, the one version of the game. And so I want to spend a little bit of time talking about the allure of self play and why I think it's exciting. So one important problem, that's it, that's the must face as we try to build truly intelligent systems is what is the task, what are we actually teaching the systems to do, and one very attractive attribute of self is that the agents create the environment by virtue of the agent acting in the environment. The environment becomes difficult for the other agents and you can see here an example of an iguana interacting with snakes, the try to eat it on successfully this time so we can see what will happen in a moment.

Speaker 1:          32:43          The Guan Australian's best and so the fact that you have this arms race between the snakes and the iguanas motivates their development potentially without bound and this is what happened in effective but in biological evolution. Now, interesting work in this direction was done in 1994, but Carl, since there is a really cool video on youtube by Carl scenes, you should check it out, which really kind of shows all the work that he's done. And here you have a little competition between agents where you evolve both the behavior and their morphology. When you Walkman, the agents is trying to gain possession of a green cube and so you can see that the agents create the challenge for each other and that's why they need to develop.

Speaker 1:          33:34          So one thing that we deed and this is work by and sell it up from open ai is we said, okay, well can be demonstrates some unusual results in self play that would really convince us that there is something there. So what we did here is that we created a small, a small ring and you have these two humanoid figures and their goal is just to push each other outside the ring and they don't know anything about wrestling. They don't know anything about standing or balancing each other. They don't know anything about centrals gravity. All they know is that if you don't do a good job, then your competition is going to do a better job. Now, one of the really attractive things about self play is that you always have an opponent that's roughly as good as you are. In order to learn, you need to sometimes in and sometimes lose, like you can't always win. Sometimes you must fail, sometimes you must succeed, so let's see what will happen here. Yeah, so it was able to be so the green human was able to block the ball in a so in a well balanced environment.

Speaker 1:          34:52          The competition is always level. No matter how good you are or how bad you are, you have a competition that makes it exactly, exactly have exactly the right challenge for you in one thing here. So this with your shows, transfer learning, you take the little wrestling humanoid and you take its friend away and you start applying a big, large random forces on it and you see if it can maintain its balance and the answer turns out to be that yes it can because it's been trained against an opponent, it pushes it and so that's why even if it's doesn't understand where the pressure forces being applied on it, it's still able to balance itself. So this is one potentially attractive feature of software environments. The QTC would learn a certain broad set of skills, although it's a little hard to control those with the skills will be. And so the biggest open question with this research is how do you learn agents in a software environment such that they do whatever they do, but then they are able to solve a battery of tasks that is useful for us that is explicitly specified externally. Yeah.

Speaker 1:          36:05          I also want to want to highlight one attribute of self play environments that people observed in our daughter bought and that is that you've seen a very rapid increase in the competence of the Bot. So over the period over the course of maybe five months, we've seen the bottom go from playing totally randomly all the way to the world champion. And the reason for that is that once you have a self clean environment, if you put computing into it, you turn it into data. Self play allows you to turn compute into data and I think we will see a lot more of that as being an extremely important thing to be able to turn compute into essentially data generalization simply because the speed of neural net processors will increased very dramatically over the next few years. So neural net cycles will be cheap and it will be important to make use of this new, of newly found over abundance of cycles.

Speaker 1:          37:03          I also wanted to talk a little bit about the end game of the self blame approach. So one thing that we know about the human brain is that it has increased in size fairly rapidly over the past 2 million years. My theory, the reason I think it happened is because our ancestors got to a point where the thing that's most important for your survival is your standing in the tribe and less the tiger and the lion. Once the most important thing is how you deal with those other things which have a large brain. Then it really helps to have a slightly larger brain. And I think that's what happened and that exists at least one paper from science which supports this point of view. So apparently there has been convergent evolution between social apps and social birds even though in terms of various behaviors, even though the divergence in evolutionary timescale between humans and birds as occurred a very long time ago, and humans and humans, apes and humans, apes and birds have very different brain structure.

Speaker 1:          38:16          So I think what should happen if we succeed, if we successfully follow the path of this approach, is that we should create a society of agents which will have language and theory of mind negotiation, social skills, trade economy, politics and justice system. All these things should happen inside the multiagent environment and it will also be some alignment issue of how do you make sure that the agents we learn behave innovative, want. Now, I want to make speculative digression here, which is I want to make the following observations. If you believe that this kind of society of agents is a plausible place where truly where the full fully general intelligence will emerge and if you accept that our experience with Dota both where we've seen a very rapid increase in competence, will carry over once all the details are right. If you assume both of these conditions, then it should follow that we should see a very rapid increase in the competence of our agents as they live in the society of agents.

Speaker 1:          39:34          So now that we've talked about potentially interesting way of increasing the competence and teaching teaching social skills and language and a lot of things that actually exist in humans as well. We want to talk a little bit about how you convey goals to Asians and the question of conveying goal to eight calls to agents. It's just a technical problem, but it will be important because it is more likely than not that the agents that people train will eventually be dramatically smarter than us. And this is work by, um, are they opening a safety team by Paul Cristiana at all and others. So I'm just going to show you this video which basically explains how the whole thing works. You there is some behavior looking for and you the human gets to see pairs of behaviors and you simply click on the one that looks better and after a very modest number of clicks, you can get this little simulated leg to do backflips.

Speaker 1:          40:57          There you go. He can now do backflips and in this to get this specific behavior, it took about 500 clicks by human annotators. The way it works is that you take all the, so this is a very data efficient reinforcement learning algorithm, but it is efficient in terms of rewards and not in terms of the environment interactions. So what you do here is that you take all the clicks, so you've got your here is one here, which is better than the other. You fit a reward function and numerical reward function to those. So you want to fit a reward function which satisfies those cleats clicks, and then you optimize this reward function with reinforcement learning and it actually works. So this requires 500 bits of information. We've also been able to train him lots of Atari Games using several thousand bits of information. So in all these cases you had human, a human annotators or human judges just like in the previous slide, looking at the pairs of trajectories and clicking on the one that they thought was better and here's an example of an unusual goal where this is a car racing game, but the goal was to ask the agent to train the white car drive right behind the orange car.

Speaker 1:          42:18          So it's a different goal and it was very straightforward to communicate this goal using this approach. So then to finish off alignment is a technical problem. It has to be solved, but of course the determination of the correct goals we want our systems to have. You'll be a very challenging political problem. And on this note, I want to thank you so much for your attention and I just want to say that it will be a happy hour at Cambridge brewing company. 40th five. If you want to chat more about Ai and other topics, please come by. I think that deserves an applause.

Speaker 2:          42:59          Thank you.

Speaker 3:          43:03          So my population is a neural networks at buyer inspired, but backpropagation doesn't look as though it's what's going on in the brain because signals in the brain go one direction down the excellence where as back propagation requires the areas to be publicated backup the wires. So can you just talk a little bit about that whole situation where it looks like the greatest doing something a bit different than our highest successful algorithms algorithm is going to be improved once we figure out what the brain is doing or what is the brain sending signals back even though it's got no obvious way of doing that. What's what's happening in that area?

Speaker 1:          43:42          So that's a great question. So first of all, I'll say that the true answer is that the honest answer is that I don't know, but I have opinions. And so I'll say two things. First of all, given that like if you agree, if me agreed. So rather it is a true fact. Backpropagation solves the problem of circuit search. This problem feels like an extremely fundamental problem. And for this reason I think that it's unlikely to go away. Now you also right that the brain doesn't obviously do backpropagation, although they've been in multiple proposals of how it could be, how it could be doing them, for example, there's been a work by and others where they've shown that if you use that, it's possible to learn a different set of connections that can be used for the backward pass and that can result in successful learning. Now the reason this hasn't been really pushed to the limit by practitioners is because they say, well, I got tf the gradients. I'm just not going to worry about it, but you're right that this is an important issue and you know, one of two things is going to happen. So my personal opinion is that backpropagation is just going to stay with us til the very end and we'll actually build fully human level and beyond systems before we understand how the brain does what it does.

Speaker 1:          45:06          So that's what I believe, but of course it is a difference that has to be acknowledged.

Speaker 4:          45:14          Do you think it was a fair match up for the Dota bought and that person given the constraints of the system?

Speaker 1:          45:21          So I'd say that the biggest advantage computers having games like this, like one of the big advantages is that they obviously have a better reaction time. Although in Dota in particular the number of clicks per second over the top players is fairly small, which is different from starcraft. Starcraft, starcraft is a very mechanically heavy game because if a large number of units and so the top players, they just click all the time. In Dota, every player controls just one hero and so that greatly reduces the total number of actions they need to make. Now still precision matters. I think that we'll discover that later, but I think will really happen is if you will discover that computers have the advantage in any domain or rather every domain. Not yet.

Speaker 4:          46:15          Do you think that the emergent behaviors from the agent we're actually kind of directed because the constraints are already kind of in place. Like so it was kinda forced discover those or do you think that like that was actually something quite novel that like wow, it actually discovered these on its own. Like you didn't actually have bias towards constraining it.

Speaker 1:          46:33          So it's definitely need discover new strategies and I can share an anecdote where our tester, we have a prohibition, would test the Bot and he played against it for a long time and the bottom will do all kinds of things against the player, the human player which were effective. Then at some point that pro decided to play against the better flow pro and he decided to imitate one of the things that they both was doing and this image. But by imitating it, he was able to defeat a better pro. So I think, I think the strategies to discoveries are real and so like it means that there's very little transport, but you know, I would say I think what that means is that because the strategy's discovered by the bulk of the humans, it means that the fundamental game plays the deeply related for, for

Speaker 4:          47:23          a long time now I've heard that the objective of reinforcement learning is to determine policy

Speaker 5:          47:30          that chooses an action to maximize the expected reward, which is what you said earlier. Would you ever want to look at the standard deviation of possible rewards? Does that even make sense?

Speaker 1:          47:42          Yeah, I mean I think for sure, I think it's really application dependent, one of the reasons to maximize the expected reward just because it's easier to design algorithms for it. So you write down this equation of the formula, you do a little bit of derivation, you get something which amounts to a nice looking algorithm. Now I think there exists like really their existing applications where you'd never want to make mistakes and you wanna work on the standard deviation as well. But in practice it seems that the, just looking at the expected reward covers a large fraction of the situation is you'd like to apply this to. Okay, thanks. Cam.

Speaker 5:          48:27          Um, we talked last week about motivations, um, and that has a lot to do with the reinforcement and some of the ideas is that the, uh, our motivations are actually connection with others and cooperation and I'm wondering if there through enough, and I understand it's very popular to have the computers play these competitive games. Um, but is there any use in like having an agent self play collaboratively collaborative games?

Speaker 1:          49:03          Yeah, I think that's an extremely good question. I think one place from which we can get some inspiration is from the evolution of cooperation. I think cooperation be cooperate ultimately because it's much better for you, the person to be cooperative or not. And so I think what should happen if you have a sufficiently open-ended game incorporation of it will be the winning strategy. And so I think we will get cooperation whether it be like it or not.

Speaker 6:          49:43          Hi. Um, you mentioned the complexity of the simulation of friction. I was wondering if you feel that there exist open complexity theoretic problems relevant to relevant to ai or whether it's just a matter of finding good approximations that humans have, the types of problems that humans tend to solve?

Speaker 1:          50:04          Yeah. So complexity theory. Well, like at the very basic level, we know that whatever algorithm we're going to run, he's going to run fairly efficiently on some hardware. So that puts a pretty strict upper bound and the true complexity of the problems you're solving, like by definition, via solving problems which aren't too hard in complexity theoretic sense. Now it is also the case that many of the problems. So while the overall thing that we do is not hard from a complexity theoretic sense and indeed humans cannot solve and be complete problems in general, it is true that many of the like optimization problems that we pose to our algorithms are in intractable in the general case, starting from neural net optimization itself, it is easy to create a family of datasets for a neural network with a very small number of neurons such that find the global optimum is not complete. And so how do we avoid it? Well, we just try grading dissent anyway and somehow it works, but without question like we cannot, we do not solve problems which are truly intractable. So I mean I have the sense of the question.

Speaker 7:          51:20          Hello. It seems like an important sub problem on the path towards Agi will be understanding language. And the state of generative language modeling right now is pretty abysmal. What do you think are the most productive research trajectories towards generative language models?

Speaker 1:          51:38          So I'll first say that you are completely correct that the situation with language is still far from great. Although progress has been made, even without any particular innovations beyond models that exist today, simply scaling up models that exist today on larger data sets is going to go surprisingly far, not even larger datasets, but larger and deeper models. For example, if you train a language model with a thousand layers and it's the same layer, I think it's going to be a pretty amazing language model like we don't have the cycles for week yet, but to think it will change very soon. Now. I also agree with you that there are some fundamental things missing in our current understanding of deep learning which prevent us from really solving the problem that we want. So I think one of these problems, one of the things that's missing is that, or that seems like blatantly wrong, is the fact that we train a model and then you stop training the model and you freeze it.

Speaker 1:          52:41          Even though it's the training process for the magic really happens of the magic is if you think about it, like the training process is the true general part of the whole, of the whole of the whole story. Because you're tends to flow code doesn't care what your data set to optimize. He just says whatever, just give me the data set, I don't care which one solve also the mall. So like the ability to do that feels really special and I think we are not using it at test time. It gets hard to speculate about like things you don't know the answer, but all I'll say is that simply train bigger, deeper language models. They'll go surprisingly far scaling out, but also doing things like training or test stamina in, for instance, time. I think it will be another important boosts the performance.

Speaker 8:          53:30          Hi. Thank you for the talk. Um, so it seems like right now another interesting approach to solving reinforcement learning problems could be to go for the evolutionary evolutionary strategies and although they have their caveats, I wanted to know if I'd open ai particularly you're working on something related and what are, what is your general opinion on them?

Speaker 1:          53:51          So like at present, I believe that something like evolutionary strategies is another great for reinforcement learning. I think that normal reinforcement learning algorithms, especially if it's big policies a better, but I think if you want to evolve a small compact object like like a piece of code for example, I think that would be a place where it's gonna be a seriously worth considering, but this, you know, you've all been a useful piece of code is a cool idea. Hasn't been done yet, so still a lot of work to be done. Before we get there.

Speaker 9:          54:25          Hi. Thank you so much for coming. My question is, you mentioned what is the right goal is a political problem, so I'm wondering if you can elaborate a bit on that and it also, what do you think would be the approach for us to maybe get.

Speaker 1:          54:40          Well? Again, I can't really comment too much because all the thoughts that we have. You now have a few people who are thinking about this full time at open Ai. I don't have enough of this super strong opinion to say anything too definitive. All I can say at the very high level isn't given the size. Like if you go into the future, whenever soon or whenever it's going to happen, when you build a computer, which can do anything better than a human, it's will happen because the brain is physical, being picked on society is going to be completely massive and overwhelming. It's very difficult to imagine even if you try really hard. And I think what it means is that people do care a lot and that's what I was alluding to, the fact that this will be something that many people who care about strongly and like as the impact increases gradually be self driving cars, more automation, I think we will see a lot more people care.

Speaker 9:          55:42          Do we need to have a very accurate model of the physical world and then simulate that in order to have, uh, these agents that can eventually come out into the real world and do something approaching, you know, human level intelligence tasks.

Speaker 1:          55:59          That's a very good question. So I think if that were the case, we'd be in trouble and I am very certain that it could be avoided. So specifically, the real answer has to be that look, you learn to problem solve, we learned to negotiate, you learn to persist, you know, lots of different useful life lessons in the simulation and yes, you learn some physics too, but I ain't go outside to the real world and you have to start over to some extent because many of your deeply held assumptions will be false and amount of the goals, so that's one of the reasons I care so much about never stopped in training. You've accumulated your knowledge. Now we go into an environment for some of your assumptions of let you continued training. We tried to connect the new data to your old data and this is an important requirement from our algorithms, which is already met to some extent, but it will have to be met a lot more so that you can take the partial knowledge is required and go to a new situation, learn some more. Literally the example of you go to school alone, learn useful things, then you go to work. It's not perfect. It's not. You know, you pull your four years of cs in Undergrad is not going to fully prepare you for whatever it is you need to know how to work. It will help somewhat. You'll be able to get off the ground, but there will be lots of new things you need to learn. So that's, that's the spirit of it. I think of those, of the school,

Speaker 10:         57:18          one of the things you mentioned pretty early on in your talk is that one of the limitations of this sort of style of reinforcement learning is there is no self organization so you have to tell it went into the good thing or is it a bad thing and that's actually a problem in neuroscience as well and you're trying to teach a rat to, you know, navigate a maze. You have to artificially tell it what to do. So where do you see moving forward when we already have this problem with teaching, you know, not necessarily learning but also teaching. So where do you see the research moving forward in that respect? How do you sort of introduce this notion of self organization?

Speaker 1:          57:48          So I think without question one really important thing you need to do is to be able to infer the goals and strategies of other agents by observing them. That's a fundamental skill we need to be able to learn to, to embed into the agents. So if, for example, we have two agents, one of them is doing something and the other agents says, well that's really cool. I want to be able to do that too, and then you go on and do that. And so I'd say that this is a very important component in terms of setting the reward of you see what they do, you further reward and now we have a knob which says you see what they're doing now go and try to do the same thing. So let's say this, this is as far as I, as far as I know, this was one of the important ways in which humans are quite different from other animals in the way which in the scale and scope in which we copy the behavior of other humans

Speaker 10:         58:43          mind. If I ask a quick followup or go for it. So that's kind of obvious how that works in this scope of competition, but what about just sort of arbitrary tasks like I'm in a math class with someone and I see someone doing a problem in a particular way and I'm like, oh that's a good strategy. Maybe I should try that out. How does that work in a sort of non competitive environment?

Speaker 1:          59:00          So I think that this movie, I think that that's going to be a little bit separate from the competitive environment, but it will have to be somehow either bake, probably baked in, maybe evolved into the system where if you have other agents doing things, they're generating data which you and the only way to truly make sense of the data that you see used to infer the goal of the agent to strategy their beliefs state. That's important also for communicating with them. If you want to successfully communicate with someone, you have to keep track both of their goal and their beliefs state instead of knowledge. So I think people find that there are many, I guess connections between understanding what other agents are doing, inferring their goals, imitating them and community successfully communicating them. Alright, let's give Eylea and the happy hour hand.

Speaker 2:          59:52          Thank you.