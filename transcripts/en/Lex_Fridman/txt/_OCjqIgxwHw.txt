Speaker 1:          00:00:00       Welcome back to six as zero, nine for deep learning for self driving cars. Today we will talk about autonomous vehicles, also referred to as driverless cars, autonomous cars, robot cars. First, the Utopian view where for many autonomous vehicles have the opportunity to transform our society into a positive direction. One point 3 million people die every year in automobile crashes globally. Thirty five, 38, 40,000 die every year in the United States, so the one opportunity that's huge. That's one of the biggest focus for us here and mit for people who truly care about this is to design the autonomous systems are artificial intelligence systems that say lies and those systems help work with deal with or take away what Nitsa calls the four ds of human folly, drunk, drugged, distracted, and drowsy. Driving autonomous vehicles have the ability to take away drunk driving, distracted, drowsy, and drugged. Eliminated car ownership. So taking shared mobility to another level, eliminating car ownership from the business side has the opportunity to save people money and increase mobility and access making vehicles. Removing ownership makes vehicles more accessible because the cost of getting from point a to point b drops an order to magnitude and the insertion of software and intelligence into vehicles makes those vehicles, makes the idea of transportation. It makes the way we see moving from a to point b, a totally different experience, much like with our smartphone, it makes it a personalized, efficient, and reliable experience. Now for the negative view, for the dystopian view,

Speaker 1:          00:02:35       eliminate jobs, any technology throughout its history, throughout our history of human civilization has always created fear that jobs that rely on the prior technology will be lost. This is a huge fear, especially in trucking because so many people in the United States and across the rely work in the transportation industry, transportation sector, and the possibility that ai will remove those jobs has potential catastrophic consequences.

Speaker 1:          00:03:13       The idea one that we have to struggle with in the 21st century of the role of intelligence systems that aren't human beings being further and further integrated into our lives is the idea that a failure of an autonomous vehicle, even if they're much rare, if the. Even if they're much safer that there is a possibility for an ai algorithm designed by probably one of the engineers in this room will kill a person where that person would not have died if they were in control of the vehicle. The idea of intelligence system, one indirect interaction with a human being killing that human being is one that we have to struggle with on a philosophical, ethical and technological level. Artificial intelligence systems in popular culture, less so in engineering concerns may not be grounded ethically grounded at this time. Much of the focus of building these systems, as we'll talk about today and throughout this course that focuses on the technology, how do we make these things work, but of course, decades out years or decades out, the ethical concern started rising for Rodney Brooks, one of the seminal people from Mit. Those ethical concerns will not be an issue for another several decades, at least five decades, but they're still important. It continues the thought, the idea of what is the role of Ai in our society? When that car gets to make a decision about human life, what is it making that decision based on, especially when it's a black box, what is the ethical grounding of that system? Does it conform with our social norms?

Speaker 1:          00:05:14       Does it go, go, go against them, and there's many other concerns? Security is definitely a big one. A car this not even artificial intelligence based the car, this software base. As they're becoming more and more millions, most of the cars on roads today, I run by millions of lines of source code. The idea that those lines of source code written again by some of the engineers in this room get to decide the life of a human being means then a hacker from outside of the car can manipulate that code to also decide the fate of the human being. That's a huge concern for us from the engineering perspective. The truth is somewhere in the middle we want to find what is the best positive way we can build these systems to transform our society to improve the quality of life of everyone amongst us, but there's a grain of salt to the hype of autonomous vehicles. We have to remember as we discussed in the previous lecture, new will come up again and again. Our intuition about what is difficult and what is easy for deep learning for autonomous systems is flawed. If we use our infuse ourselves. In this example, human beings are extremely good at driving. This will come up again and again. Our intuition has to be grounded in understanding of what is the source of data, what is the annotation, and what is the approach? What is the algorithm, so you have to be careful about using our intuition, extending a decades out and making predictions whether there's towards the Utopian or dystopian view

Speaker 1:          00:07:12       and as we'll talk about some of the advancements of companies working in this space today, you have to take what people say in the media, what the companies say. Some of the speakers that will be speaking at this class say about their plans for the future and their current capabilities. I think us a guide that can provide is when there's a promise of a future technology, future vehicles that are two years out or more. That has to be. That's a very delicate prediction one that is within a year as we'll give a few examples today is skeptical. The real proof comes in actual testing on public roads or in the most impressive, the most amazing. The reality of it is when it's available to consumer purchase. I would like to use Rodney Brooks as a so it doesn't come from my mouth, but I happen to agree his prediction is no earlier than 2032 drivers. Taxi service in a major US city will provide arbitrary pickup and drop off locations fully autonomously. That's 14 years away and by 2045 it will do so in multiple cities across the United States. So think about that, that a lot of the engineers working in this space, a lot of folks are actually building these systems, agree with this idea, and that is the earliest I believe this will happen and Rodney believes, but

Speaker 1:          00:09:08       as all technophobes have been wrong, it could be wrong. This is a map on the x axis, a plot on the x axis of time throughout the 20th century and the adoption rate on the y axis from zero to 100 percent of the various technologies from electricity to cars to radio, the telephone and so on. And as we get closer to today, the technology adoption rate, when it goes from zero to 100 percent, the number of years it takes to adopt that technology is getting shorter and shorter and shorter. As a society, we're better at throwing away the technology evolved and accepting of technology of new. So if a brilliant idea to solve some of the problems were discussing comes along, it could change everything overnight. So let's talk about different approaches to autonomy. We'll talk about sensors afterwards. We'll talk about companies, players in this space, and then we'll talk about ai and the actual algorithms and how they can help solve some of the problems with autonomous vehicles. Levels of autonomy. Here's a useful

Speaker 1:          00:10:30       taxonomies of levels of autonomy, useful for initial discussion, for legal discussion and for policy making and for blog posts. The media reports, but it's not useful. I would argue for design and engineering of the underlying intelligence and the system viewed from a holistic perspective, the entire thing, creating an experience that's safe and enjoyable. So let's go over those levels. The five, the six levels. This is presented by sae report j three zero, one six. The most widely accepted taxonomists nation of autonomy. No automation at level zero. Level one and level two is increasing levels of automation. Level one is cruise control, level two is adaptive cruise control and lane keeping. Level three, I don't know what level three is. There's a lot of people that will explain that. Level three is conditional automation, meaning it's constrained to certain geographical location. I will explain that from an engineer perspective. I'm a personally a little bit confused on where that stands. I'll try to redefine how we should view automation. Level four and level five is high full level automation. Level four is when the vehicle can drive itself fully for part of the time. There's certain areas in which you can take care of everything no matter what. No human inter interaction input safekeeping as required. Level five, automation is the car does everything. Everything.

Speaker 1:          00:12:18       I would argue that those levels aren't useful for designing systems that actually work in the real world. I would argue that there's two systems, but first a starting point that every system to some degree involves a human. It starts with manual control from a human human getting in the car and a human electing to do something so that's the manual control. What we're talking about. When the human engages the system, when the system is first available and the human chooses to turn it on, that's one we have to ai systems, human centered autonomy, when the human is needed is involved and full autonomy. When ai is fully responsible for everything from the legal perspective, that means a two full autonomy means the car they designer, the AI system is liable, is responsible, and for the human center of autonomy, the human is responsible.

Speaker 1:          00:13:27       What does this practically mean for human centered autonomy, and we'll discuss examples of all of these. When a human interaction is necessary. The question then becomes is how often is the system available? Is it available on in traffic conditions, so for traffic, bumper to bumper is available on the highway. Is that sensor based like in the Tesla vehicle, meaning based on the visual characteristics to the scene, the vehicle is confident enough to be able to control, to make control decisions, perception control decisions. The other factor not discussed enough and I think poorly imprecisely discussed when it is, is the number of seconds given to the driver, not guaranteed, but provided as a sort of feature to the driver to take over and the Tesla vehicle in all vehicles on the road today, that time is zero. Zero seconds of guaranteed zero seconds have provided. There is some, there is some room and sometimes it's hundreds of milliseconds, sometimes it's multiple seconds, but really there's no standard of how many seconds you get to say wake up, take control.

Speaker 1:          00:14:51       Then tele operation, something that some of the companies will mention are playing with is when a human being as involved remotely controlling the vehicle remotely, so being able to take over control of the vehicle when you're, uh, when you're not able to control it. So support by human that's not inside the car. That's a very interesting idea to explore. But for the human centered autonomy side, all of those features and not required, they're not guaranteed the human driver, the human inside the car is always responsible. At the end of the day, they must pay attention to a degree that's required to take over. When the system fails and no matter under this consideration, under this level of autonomy, the system will fail at some point. That is the, that is the point. That is the collaboration between human and robot is the system will fail and the human has to catch one that does.

Speaker 1:          00:15:53       And then full autonomy is ai is fully responsible, not that doesn't. Again, as we will present some companies in the marketing material and that pr side of things, they might present that there is significant degrees of autonomy. If you're talking about l three or l four or l five, you have to read between the lines. You're not allowed to have tele operation. If a human is remotely operating the vehicle, a humanist still in the loop, a humanness still evolved. It's still a human centered and autonomy system. You don't get the ten second rule, which is ge just because you give the driver 10 seconds to take control. That somehow removes liability for you. If you say that that's it. As an AI system, I can't take a can't resolve, can't deal, can't control the vehicle in this situation, and you have 10 seconds to take over, that's not good enough.

Speaker 1:          00:16:58       The driver might be sleeping. That driver may have had a heart attack. They're not able to control the vehicle. Full autonomous systems might must find safe harbor. That must get you full. Stop from point a to point b. That point b might be your desired destination or it might be a safe parking lot, but it has to bring you to a safe location. This is a clear definition of the two systems and the human. Of course, as far as our certain. A current conception of artificial intelligence in cars today is a human always overrides the AI system, so we should for the for the in the general case, the human gets to choose to take control. The Ai can't take controls any human except when danger is imminent, meaning sudden crashes like an ab events. We're not yet ready for AI systems to say as a society to say, no, no, no. You're drunk. You can't drive. So beyond the traditional levels from level zero to level five, the starting point is level zero, no automation. All cars start here. Level one, level two and level three, I would argue fall into human senator autonomy systems, a one

Speaker 1:          00:18:25       because they involve some degree of a human. Now four or five to some degree, there's some crossover fall into full autonomy, even though with all four, with way Mo, as you can ask on Friday and anyone cruise uber playing in this space, there's very often a human driver involved. One of the huge accomplishments of waymo over the past month. Incredible accomplishment. We're in Phoenix, Arizona. They drove without the car, drove without a driver. The meaning there was no safety driver to catch. There is no engineer staff member there to catch the car. A human being that doesn't work for Google or Waymo got into that car and got from a to point b without a safety driver. That's an incredible accomplishment and that particular trip was a fully autonomous trip. That is full autonomy when there's no human to catch the car. No Ai presentation is good without cats. So full autonomy. A two system is when you do nothing but write along. Human centered autonomy system is when you have some control. I'm sorry, I had to. So the two paths for autonomous systems, they want an a two in blue. On the left is a one human centered, on the right is a two full autonomy

Speaker 1:          00:20:09       and then blue is from the artificial intelligent perspective is easy, easier, and then red is harder, easier, meaning we do not have to achieve 100 percent accuracy. Harder means everything that's off of 100 percent accuracy, no matter how small has a potential of costing human lives and huge amounts of money for companies.

Speaker 1:          00:20:46       So let's discuss. We'll discuss later in the lecture about the algorithms behind each of these methods and the left and the right, but this summarizes the two approaches, the localization mapping for the car to determine where it's located. For the human centered autonomy, it's easy. It still has to do the perception it has to localize itself within the lane. It has defined all the neighboring pedestrians in the vehicles in order to be able to control the vehicle to some degree, but because the human is there, it doesn't have to do so perfectly when it fails. A humans there to catch it. Seen understanding, perceiving everything in the environment from the camera, from law, whether it's Lidar, radar, ultrasonic, the planning of the vehicle, whether it's just staying within lane, uh, for adaptive cruise control, controlling the longitudinal movement of the vehicle, or it's changing lanes at the Tesla autopilot or higher degrees of automation.

Speaker 1:          00:21:46       All of those movement planning decisions can be made autonomy. When the human is there to catch, it's easier because you're allowed to be wrong. Rarely but wrong. The hard part is getting the human robot interaction piece right. That's next Wednesday lecture as we'll discuss about how deep learning can be used to interact first. Perceive everything about the driver and second to interact with the driver. That part is hard because he can't screw up on that part. You have to make sure you help the driver know where your flaws are so they can take over. If the driver's not paying attention, you have to bring their attention back to the road, back to the interaction. You have to get that piece right because for a flawed system, one that's rarely flawed. The rarities, the challenge in fact has to get the interaction right and then the final piece is communication.

Speaker 1:          00:22:47       The autonomous vehicle, fully autonomous vehicle must communicate extremely well with the external world, with a pedestrian is the Jay Walker's the humans in this world. The cyclists that that communication piece one, at least that is part of a safe and enjoyable driving experience, is extremely difficult on the Ta Waymo vehicle. I wish them luck if they come to Boston. I'm getting from point a to point b because pedestrians will take advantage of vehicle must assert itself in order to be able to navigate Boston streets and that assertion is communication. That piece is extremely difficult for a tesla vehicle for for a a human centered autonomy vehicle. L Two l three. The way you deal with Boston pedestrians is you take over, roll down the window, yells something, and then speed up getting the piece for an artificial intelligence system to actually be able to accomplish something like that.

Speaker 1:          00:23:56       As we'll discuss on the ethics side and the engineering side is extremely difficult. That said, most of the literature and the human factors field and the autonomous vehicle field, anyone that's studied autonomy in aviation and in vehicles is extremely skeptical about a human centered approach they think is deeply irresponsible, is deeply irresponsible because is as argued because human beings, when you give them a technology which will take control part of the time, they'll get lazy. They would take advantage of that technology. They will overtrust that technology. They'll assume a work perfectly always. This is the idea that this is this idea extended beyond further and further means that the better the system gets, the better of the car, it gets a driving itself. The more the humans will sit back and be completely distracted. It will not be able to reengage themselves in order to safely catch when the system fails.

Speaker 1:          00:25:05       This is Chris Urmson, the founder of the Google self driving cars program, and now the co founder of one of the other co founder is a speaker. This class on next Friday, Sterling Anderson have a company called Aurora, a startup. He was one of the big proponents, or the I should say, our opponents of the idea that human centered autonomy could work. They tried it publicly, spoken about the fact that Google, as in the early self driving car program, they've tried shared autonomy. They've tried l two and it failed because they're engineers that people driving their vehicles fell asleep and that's the belief that people have and we'll talk about why that may not be true. There's a fascinating truth in the way human beings can interact with artificial intelligence systems that may work. In this case, as I mentioned, it's the human robot interaction, building that deep connection between human and machine of understanding of communication. This is what we believe happens, so there's a lot of videos like this as a it's, it's fun, but it's also representative of what what society believes happens when automation is allowed to enter the human experience and driving where the human life is a steak that you can become completely disengaged.

Speaker 2:          00:26:46       Hmm.

Speaker 1:          00:26:51       It's kind of. It's kind of a natural thing to think, but the question is, does this actually happened? What actually happens on public roads, the amazing thing that people don't often talk about

Speaker 1:          00:27:11       is that there is hundreds of thousands of vehicles on the road today, equipped with autopilot, Tesla, autopilot that have a significant degree of autonomy. That's data, that's information so we can answer the question what actually happens. So many of the people behind this team of instrumented 25 vehicles, 21 of which are Tesla, autopilot vehicles now with over collected recording everything about the driver, two cameras to hd, cameras on the driver, two cameras on the a one, the camera on the external roadway and collecting everything about the car, including audio, the state, the pulling, everything from the Cambus, the kinematics of the vehicle, Imu, gps, all of that information over now over 300,000 miles, over 5 billion video frames. All as we'll talk about, analyze the computer vision. You extract from that video of the driver of everything they're doing. That level of distraction, the allocation of attention, the drowsiness, emotional states, the hands on, we'll hands off, we'll body pose, I activity, smartphone usage, all of these factors, all of these things that you would think would fall apart when you start letting autonomy into your life. We'll talk about what the initial reality is. That should be inspiring and thought provoking,

Speaker 1:          00:28:50       as I said, three cameras, single board computer recording all the data over a thousand machines in holyoke and distributed computation, running the deep learning algorithms I've had mentioned on these five plus billion video frames going from the raw data to the actionable useful information. The slides are up online if you'd like to look through them, I'll fly through some of them and this is the video of one of thousands of trips. We have an autopilot in our data, a car driving autonomously, large fraction of the time on highways from here to California. I'm here to Chicago, to Florida and all across the United States. We take that data and using the supervised learning algorithms, semi-supervised. The number of frames here is huge for those that work in computer vision. Five billion frames is several orders of magnitude larger than any data set that people are working with in computer vision, actively annotated, so we want to use that data for understanding the behavior of what people are actually doing in the cars and we want to train the algorithms, the do perception and control. A quick summary over 300,000 miles, 25 vehicles. The color is a true to the actual colors of the vehicles, a little fun fact. Tesla model x model less and now a model three,

Speaker 1:          00:30:41       500,500 plus miles a day and growing. Now, most days in 2018 are over a thousand miles a day. This is a quick gps map in red is manual driving across the Boston area and Blue Cyan is autonomous driving. This is giving you the sense of just the scope of this data. This is a huge number of miles with automated driving, several orders of magnitude larger than what Waymo is doing that what cruises doing, what Uber is doing. The miles driven in this data with autopilot confirming what are. Y'All must [inaudible] stated it's 33 percent of miles driven autonomously. This is a remarkable number for those of you who drive and for those of you who are familiar with these technologies, that is remarkable adoption rate, that 33 percent of the miles are driven in autopilot. That means these drivers are getting use out of the system. It's working for them. That's an incredible number. It's also incredible because under the the decades of literature from aviation to automation in vehicles to to Chris Urmson and Waymo, the belief is such high numbers are likely to lead to crashes, two fatalities to at the very least highly irresponsible behavior.

Speaker 1:          00:32:25       Drivers overtrusting the systems and getting in trouble. We can run the glance classification algorithms. Again, this is for next Wednesday discussion. The actual algorithm is the algorithm that tells the region that the driver is looking at and it's comparing road instrument cluster, left rear view center stack, and right. Does the allocation of glance change with autopilot or with manual driving? It does not appear to have any significant noticeable way, meaning you don't start playing chess, you don't start. You don't get in the back seat to sleep, you don't start texting in your smart phone and watching a movie, at least in this Dataset, this promise here for the human centered approach, the observation to summarize this particular data is that people are using it a lot. The percentage of miles and percentage of hours is incredibly high, at least relative to what was it will be expected from these systems and given that there's no crashes, there's no near crashes in autopilot. The road type is mostly highway traveling at high speeds. The mental engagement looked at

Speaker 1:          00:33:47       8,000 trestles of control from machine to human, so human beings taking control of the vehicle saying, you know what? I'm going to take control now. I'm not comfortable with the situation for whatever reason either not comfortable or electing to do something that the vehicle is not able to like turn off the highway, make a right or left turn stop for a stop sign. These kinds of things. Physical engagement, as I said, glance remains the same and what do we take from this? It says something that I'd like to really emphasize this. We've talked to, we talk about autonomous vehicles in this class and the guest speakers who are all on the other side, so I'm representing the human center side. Most all our speakers are focused on the full autonomy side because that's the side roboticist know how to solve. That's the fascinating algorithm nerd side, and that's the side I love as well.

Speaker 1:          00:34:44       It's just my belief stands that the solving the perception control problem is extremely difficult and two, three decades away. So in the meantime we have to utilize the human robot interaction to actually bring these ai systems onto the road to successfully operate and the way we do that counterintuitively is we have to have. We have to let the artificial intelligence systems reveal their flaws. One of the most endearing things to human beings can do to each other. Friends is revealed their flaws to each other. Now, from an automotive perspective, from a company perspective is perhaps not appealing for an ai system to reveal what it sees about the world and would it doesn't see about the world where it succeeds and where it fails,

Speaker 1:          00:35:41       but that is perhaps exactly what it needs to do. In the case of autopilot, the way the very limited, but I believe successful way it's currently doing that is allowing you to use autopilot basically anywhere, so what people are doing is they're trying to engage their turn on autopilot in places where they really shouldn't. Rural rural roads, curvy with terrible road markings with a in heavy rain conditions with snow, with lots of cars driving at high speeds all around. They turn autopilot on to understand, to experience the limitations of the system, to to interact. That human robot interaction is through it's tactile. By turning it on and seeing is it going to work here? How's it going to fail? And the human is always there to catch it. That interaction, that's communication, that intimate understanding is what creates successful integration of ai in the car. Before we're able to solve the full autonomy puzzle. Learn the limitations by exploring it starts with this guy and hundreds of others. If you search on Youtube, first time with the autopilot, the amazing experience of direct transfer, of control of your life to an artificial intelligence system in this case, given control to Tesla autopilot system. This is why in the human centered camp of autonomy, I believe that autonomous vehicles can be viewed as personal robots with which you build, build a relationship or the human robot interaction is the key problem, not the perception control

Speaker 1:          00:37:35       and they're the flaws of both humans and machines must be clearly communicated and perceived perceived because he used the computer vision algorithms to detect everything about the human and communicated because on the displays of the car or even through voice, it has to be able to reveal when it doesn't see different aspects of the scene from the human centered approach, then we can focus on the left, the perception and control side. Perceiving everything about the external environment and controlling the vehicle without having to worry about being 99 point nine, nine, nine, nine, nine percent correct. Approaching 100 percent correct because in the cases where it's extremely difficult, we can let the human catch the system, we can reveal the flaws and let the human takeover on the system can't. So let's get to the sensors, the sources of raw data that we'll get to work with. There's three. There's cameras, so image sensors, rgb infrared, visual data. There's radar and ultrasonic and there's lidar. Let's discuss the strengths first. Discuss really what these sensors are, the strength and weaknesses and how they can be integrated together for sensor fusion, so radar is the trust of the old trusted friend. The sensor that's commonly available in most vehicles that have a degree of autonomy on the left is a visualization of the kind of data on high resolution rate or that's able to be extracted.

Speaker 1:          00:39:38       It's cheap. Both radar which works with electromagnetic waves and ultrasonic, which works with sound waves, sending a wave, letting it bounce off the obstacles, knowing the speed of that wave, being able to calculate the distance to the obstacle based on that, it does extremely well in challenging weather, rain, snow. The downside is a slow resolution compared to the other senses we'll discuss, but it is the one that's most reliable and using automotive industry today and it's the one that's in sense of fusion is always there. Lidar visualized on the right. The downside is it's expensive, but it produces an extremely accurate depth information and a high resolution map of the environment that has 360 degrees of visibility. It has some of the big strengths of radar in terms of reliability, but with much higher resolution and accuracy. The downside is cost. Here's a quick visualization comparing the two of the kind of information and get to work with the. The density and the quality of information with Lidar is much higher and lighter has been the successful source of ground truth. The reliable sensor relied upon on vehicles that don't care about cost

Speaker 1:          00:41:26       and camera. The thing that most people here should be passionate about because machine learning, deep learning, the most ability to have a significant impact there. Why versus cheap, so it's everywhere. Second is the highest resolution, so there's the most, the most highly dense amount of information, which means information is something that can be learned and inferred to interpret the external scene. So that's why it's the best source of data for understanding the scene. And the other reason it's awesome for deep learning is because of the hugeness of data involved, the, it's many orders of magnitude more data available for driving in camera, visible light or infrared than it is in Lidar. Uh, the. And our world is designed for visible light. Our eyes work in similar ways, the cameras at least crudely. So the source data is similar. The lane markings, the traffic size of traffic lights, the other vehicles, the other pedestrians all operate with each other in this rgb space. In terms of visual characteristics, the downside is cameras are bad at depth estimation, it's noisy and difficult even with stereo vision cameras to estimate depth relative to lidar and they're not good and extreme weather and they're not good at least visible light cameras at night.

Speaker 1:          00:43:10       So let's compare the ranges. Here's a plot and meters on the x axis of the range. And Acuity on the y axis with ultrasonic lidar radar and camera passive visual sensor plotted the range of cameras as the greatest this is looking at. We're going to look at several different conditions. This is for clear well lit conditions, so during the day, no rain, no fog, lidar and radar have a smaller range under 200 meters and ultrasonic sensors used mostly for park assistance and these kinds of things. And blind spot warning has terrible range, is designed for extremely close as high resolution distance estimation for extremely close distances here, a little bit small, but looking at up top is clear. Well lit conditions the plow would just looked at and I'm bottom is clear dark conditions, so just a clear night day, no rain but it's night. And on the bottom right is heavy rain. Snow or fog. Vision falls in terms of range and accuracy under dark conditions and in rain, snow or fog radar. Our old trusted friend stay strong. The same range just under 200 meters and at the same acuity, same with sonar. Lighter works well at night, but it does not do well with rain or fog or snow.

Speaker 1:          00:45:01       One of the biggest downsides of lighter other than cost. So here's another interesting way to visualize this that I think is productive far discussion of which sensor will win out. Is that the ell musk prediction of camera or is it the way more prediction of Lidar for lidar in this kind of plot that will look for every single sensor, the greater the radius of the blue, the more successful that sensor is that accomplishing that feature with a bunch of features lined up around the circle, so range for lighter is pretty good, not great, but pretty good. Resolution is also pretty good. It works in the dark. It works in bright light, but it falls apart in the snow. It does not provide color information, texture, information contrast. It's able to detect speed, but the sensor size, at least to date, is huge. The sensor cost, at least to date, is extremely expensive and it doesn't do well in proximity. We're ultrasonic shines. Speaking of which, ultrasonic, same kind of plot does well in proximity detection. It's cheap, the cheapest sensor or the four and census size. You can get it to be tiny. It works in snow, fog and rain, but it's resolution is terrible. It's range is nonexistent and it's not able to detect speed.

Speaker 1:          00:46:42       That's where radar steps up. It's able to detect speed. It's also cheap. It's also small, but the resolution is very low and it's just like lidar is not able to provide texture. Information, call information camera.

Speaker 1:          00:47:03       The sensor costs is cheap. The sensor size is small, not good up close proximity. The range is the longest of all of them. Resolution is the best of all of them. It doesn't work in the dark. It works in bright light, but not always. One of the biggest downfalls of cameras sensors is the sensitivity to lighting variation. It works. It doesn't work in the snow. Fog, rain so suffers much like lidar from that, but it provides rich, interesting textural information. The very kind that deep learning needs to make sense of this world. So let's look at the cheap sensors. Ultrasonic radar and cameras, which is one approach. Putting a bunch of those in the car and fusing them together, the cost there is low. One of the nice ways to visualize using this visualization technique when they're fused together on the bottom, it gives you a sense of them working together to compliment each other's strengths and the question is where the camera or lidar will win out for partial autonomy or full autonomy on the bottom, showing this kind of visualization for a lidar sensor and on top showing this kind of visualization for fused radar, ultrasonic and camera.

Speaker 1:          00:48:42       At least under these considerations, the fusion of the cheap sensors can do as well as lighter.

Speaker 1:          00:48:51       Now, the open question is whether the Lidar and the future of this technology can become cheap and it's ranged, can increase [inaudible]. Then Lidar can win out solid state lidar and a lot of developments with a lot of startup ladder companies are promising to decrease the costs and increase the range of the sensors, but for now we plow long dedication on the camera front. The annotated driving data grows exponentially. More and more people are beginning to annotate and study the particular driving perception and control problems and the very algorithms for the supervisor and semi-supervised and generative networks that we use to work with this data are improving. So it's a race and of course radar and ultrasonic. I was there to help, so companies that are playing in this space, some of them are speaking here

Speaker 1:          00:50:01       waymo in April 2017. They exited their testing, their extensive impressive testing process and allowed the first rider and Phoenix public rider in November 2017 and it's an incredible accomplishment for our company and for an artificial intelligence system in November 2017. No safety driver. So the car truly achieved full autonomy and there are a lot of constraints, but it's full autonomy. It's a step. It's an amazing step in the direction towards full autonomy much sooner than people would otherwise predict. And the miles, 4 million miles driven autonomously by November 2017 and growing quickly growing in terms of full autonomous driving, if I can say so cautiously because most of those miles have a safety driver. So I would argue it's not full autonomy, but however they define full autonomy, it's 4 million miles driven, incredible uber in terms of miles second on that list, they have driven 2 million miles autonomy by December of this of last year, 2017, the quiet player here in terms of not making any declarations of being fully autonomous, just quietly driving in a human centered way. L Two over 1 billion miles in autopilot. Over 300,000 vehicles today are equipped with autopilot technology, with the ability to drive control the car laterally and longitudinally. And if anyone believes the CEO of Tesla, there'll be over 1 million such vehicles by the end of 2018.

Speaker 1:          00:52:11       But no matter what, the 300 thousands and incredible number and the 1 billion miles is an incredible number. Autopilot was first released in September 2014, one of the first systems on the road to do so autopilot. And I caught myself as one of the skeptics in October 2016. Autopilot decided to let go of an incredible work done by Mobileye. Now Intel, we're designing their perception control system. They decided to let go of it completely and start from scratch using mostly deep learning methods that drive px to system from Nvidia and eight cameras. They decided to start from scratch. That's the kind of boldness, the kind of risk taking. They can come with naivety, but in this case it worked incredible. A eight system is going to be released at the end of 2018 and this promising one of the first vehicles that's promising what they're calling l and the definition of l three, a coordinated Thorston Lionheart, the head of the automated driving an Audi, and Audi is one of the function is operating as intended if the customer turns the traffic jam pilot on. Now this l three system is designed only for traffic jams, bumper to bumper traffic under 60 kilometers an hour.

Speaker 1:          00:53:57       If the customer turns the traffic jam pilot on and uses it as intended, and the car was in control at the time of the accident, the driver goes to the insurance company and the insurance company will compensate the victims of the accident and aftermath. They come to us, we will pay them. So that means the car is liable. The problem is under the definition of l, two l three, perhaps there is some truth to this being an l three system. The important thing here is nevertheless less deeply and fundamentally human centered because even as you see here in this demonstration video with a reporter, the car for a poorly understood reason, transfer control to the driver says, that's it, I can't. I can't take care of the situation. You take control.

Speaker 3:          00:54:50       How, how much time do you have in terms of seconds before you really need to know to take over? Well, this is the new thing about level three. With level three, the system allows the driver to give the prompt to take over vico control again ahead of time, which is in this case up to 10 seconds. Okay, so if the traffic jam situation clears up or anything, she failed her in the system careers, everything you might think of, the system still needs to be able to dry automatically because it's a driver has this time to take over. You might ask them, what is new about this? So why is he saying this is the first level three system worldwide on the market when talking about these levels of automation, there's a classification which starts as low as zero, which is basically the driver's doing everything, there's no assistance, nothing.

Speaker 3:          00:55:52       And then it gradually becomes into partly automation and when we're talking about these assistants functions like lay, keeping a distance, keeping a, we're talking about level two assistants functions, which is um, meaning that the driver is obliged to permanently monitored the traffic situation to keep the hands on the wheel even though there's the support and assistance and to intervene immediately if anything is not quite right. So you know that from living assistance systems when the steering is not perfectly in the right lane, you have to intervene and correct immediately. And that is the main difference. Now we gotta take over the crest. So what, so let's, let's talk about what

Speaker 1:          00:56:42       that means. This is still a human center system is still struggles. It's still must solve the human robot interaction problem

Speaker 1:          00:56:52       and there's many others playing in the space and they on the full autonomy side, Waymo, uber, GM cruise Yutani. The CTO, which we'll speak here on Tuesday, optimists ride is annuity voyage, the CEO of which we'll speak here next Thursday, and Aurora not listed, the the founder of which we'll speak here next Friday, and the human centered autonomy side. The reason I am a speaking about us so much today is we don't have any speakers. I'm the speaker. The Tesla autopilot is for several years now doing incredible work on that side. We're also working with Volvo pilot assist as a lot of different approaches. They're more conservative. Interesting. The audio traffic jam assist, as I mentioned, the eight being released at the end of this year. I'm the Mercedes drive politest system. The eclass an interesting vehicle that I got to drive quite a bit as the Cadillac Super Cruise. The ct six, which is very much constrained geographically to highway driving and the loudest. Proudest of the mall. George Hotz of the. I opened pilot. Let's just leave that there, so where can ai help?

Speaker 1:          00:58:26       We'll get into the details of the coming lectures on each individual component. I'd like to get some examples. The key areas, problem spaces that we can use machine learning to solve from data is localization and mapping, so being able to localize yourself in the space, very first question, that robot and use to answer, where am I seeing understanding, taking the scene in and interpreting that scene, detecting all the entities in the scene, detecting the class of those entities in order to then do movement planning to move around those entities. And finally, driver's state essential element for the human robot interaction. Perceive everything about the driver, everything about the pedestrians and the cyclists and the cars outside the human element of those, the human perception side. So first the, where am I? Visual adometry using cameras sensors, which is really where, once again, deep learning is most, uh, the vision sensor is the most amenable to learning based approaches. And visual adometry is using camera to localize yourself. To answer the where am I question the traditional approaches slam detect features in the scene and track them through time from frame to frame. And from the movement of those features are able to estimate a thousands of features. Tracking, estimate the location, the orientation of the vehicle or the camera.

Speaker 1:          01:00:11       Those methods was stereo. Vision first requires taking two camera streams on distorting them. Competing disparity map from the different perspectives, the two camera computing, the matching between the two, the feature detection, the sift to fast or any of the methods of extracting non deep learning methods of extracting features, strong detectable features that can be tracked through from frame to frame, tracking those features and estimating the trajectory, the orientation of the camera. That's the traditional approach to visual odometry in the recent years since 2015, but most success in the last year has been the end to end. Deep learning approaches either Stereo monocular cameras. Deep Vo is one of the most successful. The ntn methods is taken a sequence of images extracting with a CNN from each image, the central features from each image, and then using rnn recurrent neural network to track over time the trajectory, the pose of the camera image to pose and to end. Here's the visualization on a kitty dataset using DPO. Again, taking the video up on the top right as an input and estimating, what's visualized is the position of the vehicle in red is the estimate based again end to end with a CNN and rnn. The in red is the estimate in blue is the ground truth in the Kitty Dataset,

Speaker 1:          01:01:55       so this removes a lot of the modular parts of Slam, a visual adometry and allows it to be end to end, which means it's learnable, which means it gets better with data. That's huge

Speaker 1:          01:02:11       and that's vision alone. This is one of the exciting opportunities for ai or people working in ai is the ability to use a single sensor and perhaps the most inspiring because that sensor is similar to our own the sensor that we ourselves use of our eyes to use that alone as the primary sensor to control a vehicle. That's really exciting and the fact that deep learning that the vision visible light is the most amenable to deep learning approaches makes this particular an exciting area for deep learning research seen understanding of course, who can do a thousand slides on this? Traditionally, object detection, pedestrians, vehicles, there is a bunch of different types of classifiers and feature extraction is hard like features and deep learning has basically taken over and dominate every aspect of scene, interpretation, perception, understanding, tracking, recognition, classification, detection problems

Speaker 4:          01:03:15       and audio. Can't forget audio that we can use audio as source of information, whether that's detecting honks or in this case using the audio of the tires, microphones on the tires to determine, visualize. There's a spectrogram of the audio coming in. For those of you who are particularly have a particularly tuned ear, can listen to the different audio coming in here of wet road and drive road after the rain so there's no rain, but the road is nevertheless wet and detecting that as extremely important for vehicles because they still don't have traction. Control is that have poor control in road to road surface, tired road surface connection, and being able to detect that from just audio is a very interesting approach.

Speaker 1:          01:04:15       Finally or not. Finally, next for the perception control side. Finally is the movement planning. Getting from a to point b from point a to point b. traditional approaches, the optimization based approach, determine the optimal control, try to reduce the problem, formalize the problem in a way that's amenable to optimization based approaches. There's a lot of assumptions that need to be made. Once those assumptions are made, you're able to determine to generate thousands or millions of possible trajectories and have an objective function would determine which of the trajectories to take. Here's a race car optimizing how to take a turn at high speed

Speaker 4:          01:05:02       learning reinforcement, learning the application you'll know or it's three enforcement learning is particularly exciting for both the control and the planning side, so that's where the two of the competitions were doing in this class coming into play, the simplistic two dimensional world of deep traffic and the high high speed moving. I risked world of deep crash. We'll explore those tomorrow.

Speaker 1:          01:05:42       Tomorrow's lecture is on deeper enforcement learning and finally driver's state detecting everything about the driver and then interacting with them. On the left in green are the easier problems on the right in red are the harder problems in terms of perception, in terms of how amenable they are to deep learning methods. Body pose estimation is a very well studied problem. We have extremely good detectors for estimating the pose, the hands, the elbows, the shoulders, every aspect, visible aspect of the body. Head pose, the orientation of the head or extremely good at that, and as we get smaller and smaller in terms of size, blink rate, blink duration, Ipos and bling dynamics start getting more and more difficult. All of these metrics, all of these metrics extremely important for detecting things like drowsiness or as components of detecting emotion or where people are looking and driving where your head is turned is not necessarily where you're looking in regular life.

Speaker 1:          01:06:51       Non-Driving life. When you look somewhere, you usually turn your head to look with your eyes in driving your head office. They still or moves very subtly. Your eyes do a lot more moving. It's the kind of a effect it would described as the lizard owl effect. Some fraction of people, a small fraction are owls, meaning they move their head a lot and some people, most people are lizards moving eyes to allocate their attention. The problem with eyes is from the computer vision perspective, they're much harder to detect in lighting variation in the real world conditions, they get harder and we'll discuss how to deal with it. Of course, that's where deep learning steps up and really helps with real world data. Cognitive load. We'll discuss as well as the meeting of the cognitive load of the driver to give. A quick clip is this is the driver glance I've seen before estimating the very most important problem on driver state side is determining whether they're looking on road or off road.

Speaker 1:          01:08:02       It's the dumbest, simplest, but most important aspect. Are they looking at are they in the seat and looking on the road or are they not? That's driver glance classification. Not estimating the Xyz geometric orientation where they're looking, but actually binary class classification on road or off road body pose estimation, determining of the hands on wheel or not determining if the body alignment is standard is good for seatbelt, for safety. This is one of the important things for autonomous vehicles. If there's an imminent danger to the driver, the driver should be asked to return to a position that is safe for them in a in case of a crash driver. Emotion

Speaker 1:          01:08:53       on the top is a satisfied on the bottom as a frustrated driver, they self report as satisfied. This is what the voice based navigation. One of the biggest sources of frustrations for people in cars is voice based navigation. Trying to tell an artificial intelligence system using your voice alone where you would like to go. Huge source of frustration. One of the interesting things in our large data set that we have from the effective computing perspective is determining which of the features are most commonly associated with frustrated voice based interaction, and that's a smile is shown. There is the counterintuitive notion that emotion in particular emotion and the car is very context dependent, that smiling is not necessarily a sign of happiness and the stoic board look of the driver up top is not necessarily a reflection of unhappiness is indeed a 10 out of 10 in terms of satisfaction with the experience if he has ever been satisfied with anything happens to be Dan Brown, one of the amazing engineers in our team, cognitive load estimating from the eye region and sequences of images and three d convolutional neural networks taking in a sequence of images from the eye, looking at the blink dynamics in the eye position to determine the cognitive load from zero to two, how deep thought you are.

Speaker 1:          01:10:25       Two paths to autonomous future. Again, I would like to maybe for the last time, but probably not argue for the one on the left because our brilliant much smarter than me. Guest speakers will argue for the one on the right. The human centered approach allows us to solve the problems in 99 percent accuracy of localization, seen understanding, movement planning. Those are the problems we're taking on this class. The scene segmentation and we'll talk about on Thursday the control that we'll talk about tomorrow and then drive our state that we'll talk about next Wednesday. These problems can be solved with deep learning today. The problems in the right, solving them to close to 100 percent accuracy are extremely difficult and maybe decades away. Because for full autonomy to be here, we have to solve this situation. I've shown this many times octet triomphe. We have to solve this situation. I give you just a few examples. What do you do have to solve this situation?

Speaker 1:          01:11:44       I sorta subtler situation here is a, is it busy crosswalk where no autonomous vehicle will ever have a hope of getting through unless it asserts itself and I, there's a couple of vehicles here that kind of nudged ourselves through or at least when they have the right of way. Don't necessarily nudge, but don't hesitate when a pedestrian is present. An ambulance flying by, even though if you use a trajectory. So as a pedestrian and intent modeling algorithm to predict the momentum of the pedestrian, uh, to estimate where they can possibly go, you would then Thomas vehicle will stop. But these vehicles don't stop. They assert themselves, they move forward. Now for full autonomy system, this may not be the last time I showed this video, but because it's taking full control, it's following a reward, function and objective function and all of the problems, the ethical and the ai problems that arise, like this close runner problem will arise. So we have to solve those problems. We have to design that objective function. So with that, I'd like to thank you and encourage you to come tomorrow because you get a chance to participate in deep traffic, deep reinforcement learning competition. Thank you very much.