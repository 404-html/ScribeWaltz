Speaker 1:          00:00:00       So today we have Nader Bensky, he's a professor at northeastern university working on various aspects of computational agents that exhibit human level intelligence. Please give a warm welcome.

Speaker 1:          00:00:18       Thanks a lot. Thanks for having me here. So the title that was on the page was a cognitive modeling. I'll kind of get there, but I wanted to put it in context. So the bigger theme here is I want to talk about what's called cognitive architecture and if you've never heard about that before, that's great. And I wanted to contextualize that as how are we what, how is that one approach to get us to Agi and I say what my view of Agi is and put up a whole bunch of TV and movie characters that I grew up with that inspire me. That'll lead us into what is this thing called cognitive architecture. It's a whole research field that crosses neuroscience, psychology, cognitive science, and all the way into Ai. So I'll try to give you kind of the historical big picture view of it, what some of the actual systems are out there that might be of interest to you.

Speaker 1:          00:01:09       And then we'll kind of zoom in on one of them that I've done a good amount of work with called soar. And what I'll try to do is tell a story, a research story of how we started with kind of a core research question we look to how humans operate, understood that phenomenon and then took it and saw a really interesting results from it. And so at the end, if this field is of interest, there's a few pointers for you to go read more and go experience more of cognitive architecture. So just rough definition of agi given us an Agi class, depending on the direction that you're coming from, it might be kind of understanding intelligence or maybe developing a intelligent systems that are operating at the level of human level intelligence, the, the typical differences between this and other sorts of maybe ai machine learning systems.

Speaker 1:          00:02:03       We want systems that are going to persist for a long period of time. Uh, we want them robust to different conditions. We want them learning over time. And here's the crux of it. Working on different tasks and a lot of cases, task they didn't know were coming ahead of time. Uh, I got into this because I clearly watched too much TV and too many movies and, and I looked back at this and I realized I think I'm covering seventies, eighties, nineties, I guess it is. And today, and so this is what I wanted out of ai and this is what I wanted to work with. And then there's the reality that we have today. So instead of, so who's watched a night writer for instance? Uh, I, I don't think that exists yet. Uh, but, but maybe we're getting there and in particular for fun, during the Amazon Sale Day, I got myself an Alexa and I could just see myself at some point saying, Hey Alexa, please might write me an arcing script, uh, you know, to, to sync my class.

Speaker 1:          00:03:10       And if you have an Alexa, you probably know the following phrase. This, this just always hurts me inside. Which is. Sorry, I don't know that one, which is okay. Right? That's a lot of people have no idea what I'm asking, let alone how to do that. So what I want Alexa to respond with after that is, do you have time to teach me, uh, and to provide some sort of interface by which back and forth we can kind of talk through this, uh, that we, we aren't there yet to say the least, but I'll talk later about some work on a system called Rosie that's working in that direction. We're starting to see, see some ideas about being able to teach systems how to work. So folks who are in this field, uh, I think generally fall into these three categories. They're just curious. They want to learn new things, generate knowledge, work on hard problems.

Speaker 1:          00:04:04       Great. Uh, I think there are folks who are in kind of that mental cognitive modeling realm. And so I'll use this term a lot. It's really understanding how humans think, how humans operate human intelligence at multiple levels. And if you can do that one, there's just knowledge in and of itself of how we operate. But there's a lot of really important applications that you can think of if we were able to not only understand but predict how humans would respond, react and various tasks. Medicine is an easy one. There's some work in HCI or Hri. Uh, I'll get to later where if you can predict how humans would respond to it as you can iterate tightly and develop better interfaces. Uh, it's already being used in the realm of simulation and defense industries. Um, I happen to fall into the latter group which are the bottom group, which is systems development, which is to say just the desire to build systems for various tasks that are working on tasks that kind of current ai, machine learning can't operate on. And I think, uh, when you're working at this level or on any system that nobody's really achieved before, what do you, do you, you kind of look to the examples that you have, which in this case that we know of, it's just humans, right?

Speaker 1:          00:05:25       Irrespective of your motivation, when you have kind of an intent that you want to achieve in your research, you kind of let that drive your approach. And so I often show my students this, the turing test, you might've heard of a or variants of it that have come before. These were folks who were trying to create systems that acted in a certain way that acted intelligently and the kind of line that they drew, the benchmark that they used was to say, let's make systems that operate like humans do. Cognitive modelers will fit up into his top point here to say it's not enough to act that way, but by some definition of thinking, uh, we want the system to do what humans do or at least be able to make predictions about it. So that might be things like what errors with the human make on this task, or how long would it take them to perform this task?

Speaker 1:          00:06:19       Or what emotion would be produced in this task? There are folks who are still thinking about how the computer is operating, but trying to apply a kind of rational rules to it. So a logician for instance, would say if you have a and you have be a gives you, he gives you ca should definitely give you see, that's just what's rational. And so there folks operating in that direction and then if you go to a intro ai class anywhere around the country, particularly Berkeley, because they have graphics designers that I get to steal from, the benchmark would be what the system produces in terms of action and the benchmark is some sort of optimal rational bound irrespective of where you work in this space. Uh, there, there's kind of a common output that arrives when you research these areas, which is you can learn individual bits and pieces and it can be hard to bring them together to build a system that either predicts or act on, on different tasks.

Speaker 1:          00:07:24       So this is part of the transfer learning problem, but it's also part of a having distinct theories that are hard to combine together. So I'm going to give an example and that come comes out of cognitive modeling or perhaps three examples. So if you were in a HCI class or some interest psychology classes, one of the first things you'll learn about his fits law, which provides you the ability to predict the difficulty level of basically human pointing from where they start to a particular place and it turns out that you can learn some parameters and model this based upon just the distance from where you are to the target and the size of the target. So both moving along distance will take awhile. But also if you're aiming for a very small point that can take longer than if there's a large area that you just kinda have to get yourself to.

Speaker 1:          00:08:15       And so this has held true for many humans. So let's say we've learned this and then we move onto the next task and we learned about what's called the power law of practice, which has been shown true in a number of different tasks. What I'm showing here is one of them where you're going to draw a line through sequential set of circles here, starting at one, going to two and so forth. Not Making a mistake or at least not trying to. And try to do this as fast as possible. And so for a particular person, we would fit the AB and c parameters and we'd see a power law. So as you perform this task more, you're going to see a, a decrease in the amount of reaction time required to complete the task. Great. We've learned two things about humans. Let's add some more in.

Speaker 1:          00:09:03       So, uh, for those who might've done some reinforcement learning, the learning is one of those approaches, a temporal difference learning that's had some evidence of similar sorts of processes and the dopamine centers of the brain. And it basically says in a sequential learning task, you perform the task, you get some sort of reward. How are you going to kind of update your representation of what to do in the future, such as to maximize expectation of future reward. And there are various models of how that changes over time. And you can build up functions that allow you to form better and better and better given trial and error. Great. So we've learned three interesting models here that hold true over multiple people, multiple tasks. And so my question is, if we take these together and add them together, uh, how do we start to understand a task as quote unquote simple as chess, which is to say we could ask questions, how, how long would it take for a person to play a, what mistakes would they make after they played a few games?

Speaker 1:          00:10:08       How will they adapt themselves or if we want to develop system that ended up being good at chess or at least learning to become better at chess. My question is, if you could, there doesn't seem to be a clear way to take these very, very individual theories and kind of smash them together and get a reasonable answer of how to play chess or how do humans play chess? And so a gentleman in this slide is Allan Newell, one of the founders of ai did incredible work in psychology and other fields. Uh, he gave a series of lectures at Harvard in 1987 and they were published in 1990 called the unified theories of cognition. And his argument to the psychology community at that point was the argument on the prior slide. They had many individual studies, many individual results, and so the question was how do you bring them together to gain this overall theory, how do you make forward progress?

Speaker 1:          00:11:07       And so his proposal was unified theories of cognition. She became known as cognitive architecture, which is to say, to bring together your core assumptions, your core beliefs of what are the fixed mechanisms and processes that intelligent agents would use across tasks. So the representations, the learning mechanisms, um, the memory systems, bring them together, implement them in a theory and use that across tasks. And the core idea is that when you actually have to implement this and see how it's going to work across different tasks, the interconnections between these different, uh, processes and representations would add constraints and overtime the constraints would start limiting the design space of what is necessary and what is possible in terms of building intelligent systems. And so the overall goal from there was to understand and exhibit a human level intelligence using these cognitive architectures.

Speaker 1:          00:12:09       A natural question to ask is, okay, so we've gone from a methodology of science that we understand how to operate in a. We make a hypothesis, we construct a study, we gather our data, we evaluate that data and we false fire. We do not falsify the original hypothesis and we can do that over and over again and we know that we're making forward progress scientifically. If I've now taken that model and changed it into I have a piece of software and it's representing my theories and to some extent I can configure that software in different ways to work on different tasks. How do I know that I'm making progress? And so there's a form of science called lactose iom and it's kind of shown pictorially here where you start with your core of what your beliefs are about, uh, where you're headed, what is necessary for achieving the goal that you have.

Speaker 1:          00:13:07       And around that you'll have kind of ephemeral hypotheses and assumptions that over time may grow and shrink. And so you're trying out different things, trying out different things. And if an assumption is around there long enough, it becomes part of that core. And so as you work on more task and learn more, either by your work or by data coming in from when someone else, the core is growing larger and larger, you've got more constraints and you've made more progress. And so what I wanted to look at, where in this community, uh, what are some of the core assumptions that are driving forward scientific progress. So one of them actually came out of those lectures, they're referred to as noodles, timescales of human action, and so off on the left, the left two columns are both time units just expressed somewhat differently. A second from the left being maybe more useful to a lot of us in understanding daily life.

Speaker 1:          00:14:03       Uh, one step over from there would be kind of at what level processes are occurring. So the lowest three are down at of the substrate, the neuronal level. We're building up to deliberate tasks that occur in the brain and tasks that are operating on the order of 10 seconds. Some of these might occur in the psychology laboratory, but probably a step up to minutes and hours and then above that really becomes the interactions between agents over time and so if we start with that, the things to take away is that a regular, the hypothesis is that regularity is will occur at these different timescales and that they're useful and so those who operate at that lowest timescale might be considering neuroscience, cognitive, neuroscience. When you shift up to the next couple of levels, what we would think about in terms of the areas of science that deal without would be psychology and cognitive science and that will shift up a level and we're talking about sociology and economics and the interplay between a agents over time and so what we'll find with cognitive architecture is that most of them will tend to sit at the deliberate act.

Speaker 1:          00:15:11       We're trying to take knowledge of a situation and make a single decision and then sequences of decisions over time. We'll build to tasks and tasks. Over time we'll build to more interesting phenomenon. I'm actually going to show that that isn't strictly true, that there are folks working in this field that actually do operate one level below a, some other assumptions. So this is a herb Simon receiving the Nobel prize in economics, and part of what he received that award for was an idea of bounded rationality. So in various fields we tend to model humans as rational. Uh, and his argument was a, let's consider that human beings are operating under various kinds of constraints. And so to model the rationality with respect to unbounded by how complex the problem is that they're working on, how big is that search space that they have to conquer cognitive limitations.

Speaker 1:          00:16:10       So a speed of operations, amount of memory, short term as well as longterm, as well as other aspects of our computing infrastructure that are going to keep us from being able to arbitrarily solve complex problems as well as how much time is available to make that decision. And so, uh, this is actually a phrase that came out of his speech when he received the Nobel prize. Decision makers can satisfy ice either by finding optimum solutions for a simplified world, which is to say, take your big problem, simplify in some way, and then solve that or by finding satisfactory solutions for a more realistic world, take the world and all its complexity, take the problem in all its complexity and try to find something that works. Neither approach in general dominates the other and both have continued to coexist. And so what you're actually going to see a throughout the cognitive architecture community is this understanding that some problems you're not gonna be able to get an optimal solution to, if you consider, for instance, a bounded amount of computation, bounded time, the need to be reactive to a changing environment, these sorts of issues.

Speaker 1:          00:17:18       And so in some sense, we can decompose problems that come up over and over again into simpler problems, solve those, uh, near optimally or optimally fix those in, optimize those, but more general problems. We might have to satisfy some. There's also the idea of the symbol system hypothesis. So this is a Allen Newell and Herb Simon there, uh, considering how a computer could play the game of chess. So the fiscal system, physical symbol system talks about the idea of taking something, some signal abstractly referred to as symbol, combining them in some ways to form expressions and then having operations that produced new expressions. I'm a weak interpretation of the idea that symbol systems are necessary and sufficient for intelligent systems. A very weak way of talking about it is the claim that there's nothing unique about the neuronal infrastructure that we have, but if we got to the software right, we could implement it in the bits bytes, ram and processor that makeup modern computers.

Speaker 1:          00:18:25       That's kind of the weakest way to look at this. That we can do it with silicon and not carbon. I'm stronger way that this used to be looked at was more of a logical standpoint, which is to say if we can encode a rules of logic, these tend to line up if we think intuitively have a planning and problem solving. And if we can just get that right and get enough fats in there and enough rules in there that somehow intelligence, uh, well, that's what we need for intelligence and eventually we can get to the point of intelligence and that's what you need for intelligence. And that was a, a starting point that lasted for awhile. I think by now, most folks in this field would agree that that's necessary to be able to operate logically, but that there are going to be representations and processes that'll benefit from non symbolic representations. So particularly perceptual processing, visual auditory and processing things in a more kind of standard machine learning sort of way. Uh, uh, as well as kind of statistic taking advantage of statistical rep a representations.

Speaker 1:          00:19:36       So we're getting closer to actually looking at cognitive architectures. Uh, I did want to go back to the idea that different researchers are coming with different research Fokai Fossa. Uh, and we'll start off with kind of the lowest level and understanding biological modeling. So libra and spawn both try to model different degrees of low level details, parameters, firing rates, connectivities between different kind of levels of neuronal representations. They build that up and then they tried to build tasks above that layer, but always being very cautious about being true to a human biological processes. At a layer above, there would be psychological modeling, which is to say trying to build systems that are true in some sense to, uh, areas of the brain interactions in the brain and being able to predict a errors that we made, a timing that we produced by the human mind. And so they're all talk a little bit about act are his final level down here. These are systems that are focused mainly on producing functional systems that exhibit really cool artifacts and solve really cool problems. And so I'll spend most of the time talking about soar, but I wanted to point out a relative newcomer in the game called sigma. So to talk about spawn a little bit. We'll see if the sound works in here. I'm going to let the creator take this one

Speaker 1:          00:21:16       or not see how the AB system likes this.

Speaker 2:          00:21:27       There we go.

Speaker 3:          00:21:31       Why do you pick the license and the director of the Center for Science at the University of philosophy and the philosophy is considered a general conceptual issues is any breakdown equations very concise. We can like building actual models recently it's called the small sign because she wouldn't have million individual neurons and the model isn't it and the movement, so essentially see images and numbers and in the case of just gotten into that seat, reproduce the south and it's looking at. So for instance, we all know that we have that show up and we can simulate a potential area. Hold on. I'm working on agents that are extremely good at one task for instance. What's special is that pass and this adds the additional kind of coordinate the flow of information, food, different parts of the model, something

Speaker 1:          00:23:02       so provide a pointer at the end. He's got a really cool book called how to build a brain and if you google you can google spawn. You can find a toolkit where you can kind of construct circuits that will approximate functions that you're interested in, connect them together, a set certain properties that you would want at a low level and build them up and actually work on tasks at the level of vision and robotic actuation. So that's a really cool system as we move into architectures that are sitting above that biological level. I wanted to give you kind of an overall sense of what they're going to look like, what a prototypical architecture is going to look like. So they're going to have some ability to have perception. Uh, the modalities typically are more digital symbolic, but, uh, they will, depending on the architecture, be able to handle vision, audition a and various sensory inputs.

Speaker 1:          00:24:02       These little gadgets represented in some sort of short term memory. Whatever the state's representation for the particular system is there. It's typical to have a representation of the knowledge of what tasks can be performed when they should be performed, how they should be controlled. And so these are typically both actions that take place internally that manage the internal state of the system and perform internal computations, but also about external actuation and external might be a digital system, a game ai, but it might also be some sort of robotic actuation and real world. Uh, there's typically some sort of mechanism by which to select from the available actions in a particular situation. There's typically some way to augment this procedural information, which is to say learn about new actions possibly modify existing ones. There's typically some semblance of what's called a declarative memory. So whereas procedural, at least in humans, uh, if I asked you to describe how to ride a bike a, you might be able to say, get on the seats and pedal.

Speaker 1:          00:25:11       But in terms of keeping your balance there, you'd have a pretty hard time describing it declaratively. So that's kind of the procedural side, the implicit representation of knowledge. Whereas declarative would include facts, geography, math, uh, but it calls to include experiences that the hns had a more episodic representation of declarative memory. And they'll typically have some way of, of learning this information. Augmenting it over time and then finally some way of taking actions in the world and they'll all have some sort of cycle which is perception comes in knowledge that the agent has is brought to bear on that. An action is selected. Knowledge that knows to condition on that action will act accordingly, both with internal processes as well as eventually to take action and then rinse and repeat. Um, so when we talk about an AI system and agent in this context, that would be the fixed representation, which is whatever architecture we're talking about plus set of knowledge that is typically specific to the task button might be more general.

Speaker 1:          00:26:17       So oftentimes these systems could incorporate a more general knowledge base of facts, of linguistic facts, of a geographic facts. Let's take wikipedia and let's just stick it in the brain of the system. There'll be more task general, but then also whatever it is that you're doing right now, how should you proceed in that? And then it's typical to see this processing cycle. And going back to the prior assumption, the idea is that, uh, these primitive cycles allow for the agent to be reactive to its environment. So if new things come into his react to, if the lions sitting over there, I better run and maybe not do my calculus homework, right? So as long as this cycle is going, I'm reactive. But at the same time, if multiple actions are taken over time, I'm able to get complex behavior, uh, over the longterm. So this is the act are cognitive architecture.

Speaker 1:          00:27:12       Uh, it has many of the kind of core pieces that I talked about before. Let's see if the, some mouse, yes, as useful up there. So we have the procedural model here. A short term memory is going to be these buffers that are on the outside, uh, the procedural memory is encoded as what it called production rules, or if then rules, if this is the state of my short term memory, this is what I think should happen as a result, uh, you have a selection of the appropriate rule to fire and an execution. You're seeing a associated parts of the brain being represented here, cool thing that has been done over time in the act. Our community is to, uh, make predictions about brain areas and then perform a Mris and, and gather that data and correlate that data. So when you use the system, you will get predictions about things like timing of operations, errors that will occur, probabilities that something is learned, but you'll also get predictions about a to the degree that they can kind of brain areas that are going to light a light up a and if you want to, that's actively being developed at Carnegie Mellon.

Speaker 1:          00:28:29       To the left is John Anderson who developed this cognitive architecture. Oh, 30 ish years ago. Uh, and until the last, about five years, he was the primary researcher developer behind it with Christian. And then recently he's decided to spend more time on a cognitive tutoring systems. And so Christian has become the primary developer. There is a annual act. Our workshop, there's a summer school which if you're thinking about modeling a particular task, you can kind of bring your task to them, bring your data. They teach you how to use the system and try to get that study going right there on the spot, uh, to give you a sense of what kinds of tasks this could be applied to. So, uh, this is a representative of a certain class of task. Certainly not the only one. Let's try this again. Think powerpoint is going to want a restart every time.

Speaker 1:          00:29:28       Okay. So, uh, we're getting predictions about basically where the eye is going to move. What you're not seeing is it's actually processing things like text and colors and making predictions about what to do and how to represent information and how to process the graph as a whole. Uh, I had alluded to this earlier. There's work by Bonnie John, a very similar, so making predictions about how humans would use computer interfaces. And at the time she got hired away by IBM and so they wanted the ability to have software that you can put in front of a software designers and when they think they have a good interface, press a button. This model of human cognition with tried to perform the tasks that had been told to do and make predictions about how long it would take. And so you can have this tight feedback loop from designer saying, here's how good your particular, uh, interfaces so act are as a whole.

Speaker 1:          00:30:22       It's very prevalent in this community. I went to their webpage and counted up just the papers that they knew about. It was over 1100 papers over time. Uh, if you're interested in it, the main distribution is in lisp, but, uh, many people have used this and wanting to apply it to systems that need a little more processing power. Uh, so there's, the NRL has a java part of it that they use in robotics. The air force research lab in Dayton has implemented it in Erlangen for parallel processing of large declarative knowledge bases. Uh, they're trying to do service oriented architectures with it Kuda because they want what it has to say. They don't want to wait around for a tap to figure that stuff out. Uh, so that's, uh, the two minutes about act are a sigma as a relative newcomer. And it's developed out at the University of southern California by a man named Paul Rosenbloom, and I'll mentioned a couple of minutes because he was one of the prime developers have soar at Carnegie Mellon, uh, so he knows a lot about how solar works and he's worked on it over the years.

Speaker 1:          00:31:29       And I think originally I'm going to speak for him and he'll probably say I was wrong. I think originally it was kind of a mental exercise of can I reproduce sore using a uniform substrate? I'll talk about in a little bit. Uh, it's 30 years of research code. If anybody's dealt with research code, it's 30 years of C and c plus plus with dozens of graduate students over time. It's not pretty at all. Uh, and, and theoretically it's, it's got these boxes sitting out here. And so, uh, he re implemented the core functionality of sore all using factor graphs and message passing algorithms under the hood. He got to that point and then said there's nothing stopping me from going further. And so now it can do all sorts of modern machine learning, vision optimization, sort of things that would take some time in any other architecture to be able to integrate well.

Speaker 1:          00:32:25       So, uh, it's been an interesting experience. Uh, it's now going to be the basis for the virtual human project out at the institute for creative technology. It's a institute associated with the University of southern California for, until recently could get your hands on it, but in the last couple of years he's done some tutorials on it. He's got a public release with documentation. So that's something interesting to keep an eye on what I'm going to spend all the remaining time on the sore cognitive architecture. And so you see, it looks quite a bit like the prototypical architecture. And I'll, I'll give you a sentence again about how this all operates. Give a sense of the people involved. Uh, we already talked about Alan Newell. So both John Laird, who is my advisor and Paul Rosenbloom were students of Allan Newell. Uh, John's thesis project was related to the chunking mechanism and soar, which learns new rules based upon subgoal reasoning.

Speaker 1:          00:33:25       Uh, so he finished that, I believe the year I was born, and so he's one of the few researchers you'll find who still actively working on their thesis project, uh, beyond that, about I think about 10 years ago. He, it. So our technology, which is company up in Ann Arbor, Michigan a while it was called soar technology, it doesn't do exclusively sore, but that's a part of the portfolio, a general intelligence system stuff, a lot of defense association. So, uh, some notes of what's going to make sore different from the other architectures that fall into this kind of functional architecture category. Uh, a big thing is a focus on efficiency. So John wants to be able to run sore on just about anything. We just got on the mailing list, a desire to run it on a real time processor and our answer, while we had never done it before, it was probably, it'll work, uh, every release, there's timing tests and we always, what we, what we look at is in a bunch of different domains for a bunch of different reasons that relate to human processing.

Speaker 1:          00:34:34       There's this magic number that comes out which is 50 milliseconds, which is to say in terms of responding to tasks, if you're above that time, uh, humans will sense a delay and you don't want that to happen. Now, if we're working in a robotics task, 50 milliseconds, if you're dramatically above that, you just fell off the curb or worse or you just hit somebody in a car. Right? So we're, we're trying to keep that as low as possible and for most agents it, it doesn't even register. It's below one millisecond fractions of a millisecond. But I'll come back to this because a lot of the work that I was doing was computer science, Ai, uh, and, and a lot of efficient algorithms and data structures and 50 milliseconds was that very high upper bound. It's also one of the projects that has a public distribution.

Speaker 1:          00:35:21       You can get it in all sorts of operating systems. We use something called Swig that allows you to interface with it and a bunch of different languages. We kind of described the Meta description and you are able to basically generate bindings and a bunch of different platforms. A core of plus, plus there was a team at soar attack that said we don't like c Plus, plus it gets messy, so they actually did a port over to pure Java in case that appeals to you. There's an annual workshop that takes place in anarbor. Typically it's free. You can go there and get a sore tutorial and talked to folks who are working on sore and it's fun. I've been there every year, but wine in the last decade. It's just fun to see the people around the world better using the system and all sorts of interesting ways to give you a sense of the diversity of the applications.

Speaker 1:          00:36:10       One of the first was our one store, which was back in the days when it was an actual challenge to build a computer, which is to say that your choice of certain components would have radical implications for other parts of the computer, so it wasn't just the Dell website where you just, I want this much ram, I want this much cpu. There was a lot of thinking that went behind it and then physical Labor that went to construct your computer and so it was making that process a lot better. There are folks that apply it to natural language processing. A source seven was the core of the virtual humans project for a long time. Hci Tasks, TAC air soar was one of the largest rule based systems, tens of thousands of rules over 48 hours. It was a very large scale simulation, a defense simulation, lots of games it's been applied to for various reasons.

Speaker 1:          00:37:00       And then in the last few years putting it onto mobile robotics platforms. This is Edwin Olson's. A splinter bought an early version of it that went on to win the magic competition. Uh, then I went onto put sore on the web and if after this talk, you're really interested in a dice game that I'm going to talk about what you can actually go to the ios app store and download. It's called Michigan Liar's dice. It's free. You don't have to pay for it, but you can actually play a liar's dice with soar and it's. You can set the difficulty level. It's pretty good. It beats me on a regular basis. I wanted to give you a couple other just kind of really weird feeling sort of applications and really cool applications. The first one is out of Georgia tech. Go powerpoint. Yes.

Speaker 3:          00:37:58       Interactive art installation in which you can engage in collaborative movement, improvisation with each other and virtual dance partners. This actually creates a hybrid space in which virtual and corporate real bodies meet for human and nonhuman is blurred staring contest as to examine their relationship with technology. Installation ultimately examines how humans and machine can cocreate experiences in a playful environment. It creates a social space that encourages human interaction and collected dance experiences, allowing them to send this to creatively explore movement while having fun.

Speaker 2:          00:38:43       The development

Speaker 3:          00:38:45       and I had been a hybrid exploration and art forms of theater and dance as well as research and artificial intelligence and cognitive science. Moving on. Inspiration from the ancient art form, a shadow here, the original two dimensional version of the installation. Let us the conceptualization of the dome and the liminal space when human silhouettes and virtual characters meaning dance together on the projection surface,

Speaker 3:          00:39:14       rather than rely on preoperative library of responses, the virtual dancer learns his partner and utilize new theories and systematically reason about them in order to choose under response. The points theory is based in dance and theater and performance along the dimension, the tempo reputation, kinesthetic response, shape station relationships, gesture architecture, and your topic. Be Virtual. Dancer is able to use several different strategies to respond to human rights. These include transformation and he mentions for calling it similar or complimentary movement from memory in terms of viewpoints, admissions and apply action response patterns that the Asian consumer.

Speaker 4:          00:40:14       This is part of a larger effort from setting the relationship between communication, cognition and creativity where a large amount of our efforts go into understanding human creativity and how we make things together. Our work together as a way to understand how we can build cocreative ai that serves the same purpose where it could be a colleague and collaborator with us and create things with us.

Speaker 1:          00:40:47       Uh, so brian was a graduate student in John Lyric lab as well. Uh, before I start this, I alluded to this earlier where we're getting closer to rosie saying, can you teach me? So let me give you some introduction to this. In the lower left, you're seeing the view of a connect camera onto a flat surface. There's a, a robotic arm, mainly three d printed parts, few Servos, uh, above that. You're seeing an interpretation of the scene a, we're giving it a kind of associations of the four areas with semantic titles like, uh, one is the table, one is the garbage, just just semantic terms for areas. But other than that, the agent doesn't actually know all that much and it's going to operate in two modalities. One is, we'll call it natural language, natural language, a restricted subset of English as well as some quote unquote pointing.

Speaker 1:          00:41:45       So you're going to see some mouse pointers in the upper left saying, I'm talking about this. Uh, and this is just a way to indicate location. And so starting off we're going to say things like, you know, pick up the blue block and it's going to be like, I don't know what blue is, what is blue? And we say, oh, well that's a color. Okay. Uh, you know, so go get the green thing. What's green? Oh, it's a color. Okay. Uh, move the blue thing to a particular location. Where's that point at? Okay, what is moving like really it has to start from the beginning and it's described and it said, okay, now you finished. And once we got to that point, now I can say move the green thing over here. And it's got everything that it needs to be able to then reproduce the task given a new parameters. And it's learned that ability. So let me give it a little bit of time.

Speaker 1:          00:42:39       Oh, uh, so you can look a little bit at top left in terms of the pointers. You're going to see some text commands being entered. So what kind of attributes is blue? We're going to say it's a color. And so that can map it then to a particular sensory modality. This is green. So the pointing what kind of thing is green, okay, color. So now it knows how to understand blue and green as colors with respect to the visual scene. Um, move rectangle to the table. Uh, what is rectangle? Okay, now I can map that onto or understanding parts of the world. Is this the blue rectangle? So the arm is actually pointing itself to get confirmation from the instructor. Uh, and then we're trying to understand in general, when you say move something, what is the goal of this operation? And so then it also has to declare of representation of the idea of this task, not only that had completed it, then it can look back on having completed the task and understand what were the steps that lead to achieving a particular goal.

Speaker 1:          00:43:50       So in order to move it, you're gonna have to pick it up. It knows which one the blue thing is. Great. Now put it in the table. So that's a particular location. At this point we can say you're done. You have accomplished the move the blue rectangle to the table. And so I can understand what that very simple kind of processes like and associate that with the verb to move and now we can say move the green object or not doing the garbage and without any further interaction based on everything that I've learned up until that point, it can successfully complete that task. So this is a work of Shivali Mohan and others at the soar group at the University of Michigan on the rosy project and they were extending this to playing games and learning the rules of games through a tech space descriptions and multimodal experience. So, uh, in order to build up to here's a story.

Speaker 1:          00:45:00       And so I wanted to give you a sense of how research occurs in the group. And so there's these back and forth that occur over time between, there's this piece of software called soar and we want to make this thing better and give it new capabilities. And so all our agents are going to become better. And we always have to keep in mind, and you'll see this as I go further, that it has to be useful to a wide variety of agents. It has to be task independent and it has to be efficient for us to do anything in the architecture. All of those have to hold true, so we do something cool in the architecture and they, we say, okay, let's solve a cool problem. So let's build some agents to do this. And so this ends up testing what are the limitations, what are the issues that arise, uh, in a particular mechanism as well as integration with others.

Speaker 1:          00:45:45       Uh, and we get to solve interesting problems. We usually find there was something missing and then we can go back to the architecture and rinse and repeat. Just to give you an idea again how sore works. So the working memory is actually a directed connected graph. The perception is just a subset of that graph. And so there's going to be symbolic representations of most of the world. There is a visual subsystem in which you can provide a scene graph. I'm just not showing it here. Uh, actions are also a subset of that graph. And so, uh, the procedural knowledge, which is also production rules can modify. It can read sections of the input, modify sections of the output as well as arbitrary parts of the graph to take actions. So the decision procedure says, of all the things that I know to do and I've kind of ranked them according to various preferences, what single thing should I do?

Speaker 1:          00:46:33       A semantic memory for facts. There's episodic memory. The agent is always actually storing every experience it's ever had over time and episodic memory and it has the ability to get back to that. And so the similar cycle we saw before we get input in the perception called the input link rules are going to fire all in parallel and say, here's everything I know about this situation. Here's all the things I could do. Decision Procedure says, here's what we're going to do based on the selected operator. All sorts of things could happen with respect to memories providing input, a rules firing to perform computations, and as well as potentially output in the world. Uh, I don't remember. Agent reactivity is required. We want the system to be able to react to things in the world at a very quick pace. So anything that happens in this cycle at Max, the overall cycle has to be under 50 milliseconds.

Speaker 1:          00:47:29       So that's gonna be a constraint we hold ourselves to. And so the story I'll be telling will say how we got to a point where we started actually forgetting things and we're an architecture that doesn't want to be like humans. We want to create cool systems, but what we realized was something that we do, there's probably some benefit to it and we actually put it into our system and the lead to good outputs. So here's the research path. I'm going to walk down a. We had a just a simple problem which was we have these memory systems and sometimes they're going to get a queue that could relate to multiple memories. And the question is if you have a fixed mechanism, what should you return in a task independent way? Which one of these many, many memories shitty return. That was our question and we look to some human data on this.

Speaker 1:          00:48:17       Something called the rational analysis of memory done by John Anderson and realized that in human language, their recency and frequency effects that maybe it would be useful. And so, uh, we actually did an analysis found that not only does this occur but it's useful in what are called word sense disambiguation tasks. And I'll get to that what that means in a second. Develop some algorithms to scale this really well and it turned out to work out well not only in the original task but we look to, to other completely different ones. The same underlying mechanism ended up producing some really interesting outputs. Suddenly talking about word sense disambiguation real quick. So as a core problem in natural language processing, if you haven't heard of it before, let's say we have an agent and for some reason it needs to understand the verb to run looks to its memory and finds that it could, you know, run in the park.

Speaker 1:          00:49:10       It could be running a fever, could run an election that could run a program. And the question is, what should an task, independent memory mechanism return if all you've been given is the verb to run? And so the rational analysis of memory look through multiple texts Corpora, and what they found was if a particular word had been used recently, it's very likely to be reused again. And if it hadn't been used recently, there's going to be this effect to where, uh, the expression here, the t is time since the most recent use. It's going to some those with a exponential decay. So what it looks like if time is going to the right, a activation higher is better. As you get these individual usages, you get these little drops and then venture he dropped down. And so if we had just one usage of a word, the red would be what the decay would look like.

Speaker 1:          00:50:05       And so the core problem here is if we're at a particular point and we want to select between kind of the blue thing or the red thing, blue would have a higher activation. And so maybe that's useful. This is how things are modeled with human memory, but is it useful in general for tasks? And so we looked at common corpora used in word sense distribuition and just said, well, if we just look at this corporate twice and we just use answers, prior answers, you know, I asked the question, what is the sense of this word? I took a guess, I got the right answer and I used that recency and frequency information to my task. Independent memory. Would that be useful? And somewhat of a surprise, but somewhat maybe not have a surprise. It actually performed really well across multiple corporate. So we said, okay, this seems like a reasonable mechanism.

Speaker 1:          00:50:57       Let's look at implementing this efficiently and the architecture. And the problem was this term right here said, for every memory, for every time step you're having to k everything. That doesn't sound like a recipe for efficiency if you're talking about lots and lots of knowledge over long periods of time. So we, uh, made use of a nice approximation that Petrofac come up with to approximate tail effect. So, uh, access to that happened long, long ago. We could basically approximate their effect on the overall sum. So we had a fixed set of values, uh, and what we basically said is, since these are always decreasing and all we care about is relative order, let's just only recompute when someone gets a new value. So it's, it's a guests, it's a heuristic and approximation, but we looked at how this worked on the same set of Corpora and in terms of query time, if we made these approximations well under our 50 millisecond, the effect on task performance was negligible.

Speaker 1:          00:52:02       In fact, on a couple of these that God's ever so slightly better in terms of accuracy. And actually if we looked at the individual decisions that were being made, making these sorts of approximations, we're leading to a, up to 90, sorry, at least 90 percent of the decisions being made were identical to having done the true a full calculation. So we said this is great and we implemented this and worked really well. And then we started working on what seemed like completely unrelated problems. One was in mobile robotics. We had a mobile robot. I'll show a picture of in a little while, roaming around the halls performing all sorts of tasks and what we're finding was if you have a system that's remembering everything in your short term memory and your short term memory gets really, really big. I don't know about you. My short term memory feels really, really small.

Speaker 1:          00:52:55       I would love it to be big, but if you make your memory really big and you try to remember something, you're not having to pull lots and lots and lots of information into your short term memory. So the system was actually getting slower simply because it had a lot of short term memory, a representation of the overall map. It was looking at so large working memory problem, Liar's dice game, you play with dice. We were doing an rl based system on this reinforcement learning and it turned out a really, really big value function. Worse having to store lots of data and we didn't know which stuff we had to keep around to keep the performance, uh, up. So we had a hypothesis that forgetting was actually going to be a beneficial thing that maybe maybe the the problem we have with our memories that we really, really disliked this forgetting thing.

Speaker 1:          00:53:46       Maybe it's actually useful. And so we experimented with the following policy. We said, let's forget a memory if one we haven't really. It's not predicted to be useful by this base level activation. We haven't used it recently, we haven't used it frequently, maybe it's not worth it. That. And we felt confident that we could approximately reconstructed if we absolutely had to. And if those two things held, we could forget something. Uh, so it's this bay same basic algorithm, but instead of the ranking them, it's if we set a threshold for base level activation finding when it is that a memory is going to pass that threshold and try to forget based upon that in a way that's efficient, that isn't going to scale really, really poorly. So we were able to come up with an efficient way to implement this using an approximation that ended up for most memories to be exactly correct to the original.

Speaker 1:          00:54:48       I'm happy to go over details of this if anybody's interested later, but it ended up being a fairly close approximation. One that as compared to an accurate, completely accurate search for the value ended up being somewhere between 15 to 20 times faster. And so what we looked at our mobile robot here. Oh, sorry, let me get this back because our little robots actually going around. That's the third floor of the computer science building at the University of Michigan. He's going around, he's building a map and again, the idea was this map is getting too big, so here was the basic idea as the robots going around, it's going to need this map information about rooms. The color there is describing kind of the strength of the memory and as it gets farther and farther away and it hasn't used part of the map for planning or other purposes, basically make a decay away so that by the time it gets to the bottom, it's forgotten about the top.

Speaker 1:          00:55:41       But we had the belief that we could reconstruct portion that map if necessary. And so the hypothesis was this would take care of our speed problems and so what we looked at was, here's our 50 millisecond thresholds. If we do know, forgetting whatsoever, bad things were happening over time. So just a 3,600 seconds, this isn't a very long time, we're passing that threshold. This is dangerous for the robot. If we implemented task specific, basically cleanup rules, which is really hard to get right, that basically solved the problem when we looked at our general forgetting mechanism that we're using in other places at an appropriate level of decay. We were actually better than hand tune rules. So this was kind of a surprise when for us, uh, the other task seems totally unrelated. It's a dice game. You cover your Dayas, you make bids about what are under other people's cups.

Speaker 1:          00:56:37       This is played in pirates of the Caribbean when they're on the boat and the second movie and bidding for lives of service. Honestly, this is a game we love to play in the University of Michigan lab. Uh, and so we're like, hm, could soar, play this? And so we built a system that could learn to play this game rather well with reinforcement learning. And so the basic idea was in a particular state of the game store would have options of actions to perform, it could construct a estimates of their associated value, it would choose one of those, and depending on the outcome, something good happened, you might update that value. And the big problem was that the size of the state space, the number of possible states and actions just is enormous. And so memory was blowing up. And so what we said, similar sort of hypothesis, if we decay away, these estimates that we could probably reconstruct and we haven't used in awhile, are things going to get better?

Speaker 1:          00:57:33       And so if we don't forget it all, 40,000 games isn't a whole lot when it comes to reinforcement learning. We were up at two gigs. We wanted to put this on an iphone that wasn't gonna work. So well there had been prior work that had used a similar approach. They were down at four or 500 megs. The iphone is not going to be happy, but it'll work. Uh, so that, that gave us some hope and we implemented our system. Okay. We're somewhere in the middle. We can fit on the iphone. A very good iphone, maybe an ipad. The question was though, a one. Efficiency. Yeah, we, we fit under a 50 milliseconds, but two, how does the system actually performed when you start forgetting stuff, can it learn to play well? And so y axis here, you're seeing Qa competency, you play a thousand games. How many do you win? So the bottom here, 500. That's flipping a coin whether or not you're going to win a. If we do know for getting whatsoever, this is a pretty good system. Uh, the prior work, while keeping the memory low is also suffering with respect to how well it was playing the game and kind of cool was the system that was basically more than having the memory requirement was still performing at the level of no forgetting whatsoever.

Speaker 1:          00:58:55       So just to bring back why I went through the story was we had a problem. We look to our example of human level ai, which is humans themselves. We took an idea, it turned out to be beneficial, we found in a efficient implementations and then found it was useful in other parts of the architecture and other tasks that didn't seem to relate whatsoever. But if you download sore right now, you would gain access to all these mechanisms for whatever task you want it to perform. Just to give some sense in the field of cognitive architecture, what's some of the open issues are. I think this is true in a lot of fields in ai, but a integration of systems over time. The goal was they wouldn't have all these theories and uh, so you could just kind of build over time, particularly when folks are working on different architectures, that becomes hard.

Speaker 1:          00:59:43       Uh, but also when you have very different initial starting points, that can still be an issue. Transfer learning is an issue we're building into the space of multimodal representations, which is to say not only abstract symbolic, but also visual. Wouldn't it be nice if we had auditory and other senses, but building that into memories and processing is still an open question. There's folks working on metacognition, which is to say the agent self-assessing its own state, its own processing, some work has been done in here but still a lot and I think the last one is a really important question for anybody taking this kind of class, which is what would happen if we did succeed, if we did make human level ai and if you don't know that picture right there is from a show that I recommend that you watch a, it's by the BBC, it's called humans and it's basically what if we were able to develop what are called synths in the show.

Speaker 1:          01:00:37       I think the, a robot that can clean up after your laundry and cook and all that good stuff interact with you if looks and interacts as a human but is completely or servant and then hilarity and complex issues in sue. So I highly recommend if you haven't seen that to go watch that. Uh, I think these days there's a lot of attention play a paid to machine learning in particular deep learning methods as well. It should. They're doing absolutely amazing things. Uh, and often the question is, well, you're doing this and there's deep learning over there, you know, how do they compare? And I honestly don't feel that that's always a fruitful question because most of the time they tend to be working on different problems. Uh, if I'm trying to find objects in the scene, I'm going to pull out tensor flow. Uh, I'm really not going to pull out store.

Speaker 1:          01:01:34       It doesn't make sense. It's not the right tool for the job that haven't been said. There are times when they tend to work together really, really well. So the rosy system that you saw there, there was some, uh, uh, I believe neural networks being used in the object recognition mechanisms for the vision system. There's td learning going on in terms of the dice game where we can pick and choose and use this stuff. Absolutely great because there are problems that are best solved by these methods. So why avoid it? Uh, and then on the other side, if you're trying to develop a system where you, you know, in different situations, know exactly what you want the system to do, soar or other rule based systems and to being the right tool for the right job. So absolutely why not make it a piece of the overall system?

Speaker 1:          01:02:19       Uh, some recommended readings and some venues. Uh, I'd mentioned unified theories of cognition. This is Harvard press, I believe. Uh, the cognitive architecture was mit press came out in 2012. I'll say I'm coauthor and a theoretically would get proceeds, but I've donated them all to the University of Michigan. So I can just make this recommendation free of ethical concerns. Personally, it's an interesting bug. It brings together lots of a history and lots of new features. It's if, if you're really interested in soar, it's an easy sell. I had mentioned crystallized Smith's how to build a brain. Really cool read, download the software, go through tutorials. It's really great. How can the human mind and occur in the physical universe is one of the core act, our books. So it talks through a lot of the psychological underpinnings and how the architecture works. It's, it's a fascinating read, uh, one of the papers trying to remember what year 2008.

Speaker 1:          01:03:20       This goes through a lot of different architectures in the field. It's 10 years old, but it gives you a good kind of broad sweep if you want something a little more recent. This is last month's issue of Ai magazine completely dedicated to cognitive systems. Uh, so it's a good place to look for this sort of stuff. In terms of academic venues, a aaa, I often has cognitive systems track. There's a conference called ICCM international conference on cognitive modeling, a where you'll see kind of a span from biologic all the way up to ai cognitive science or cog psy. They have a conference as well as the journal. A ACS has a conference as well as an online journal, uh, advances in cognitive systems. Cognitive Systems Research is a journal that has a lot of this good stuff. There's Agi. The conference, Becca is biologically inspired cognitive architectures and I had mentioned both. There's a sore workshop and enact our workshop that go on annually, so leave it at this. There's some contact information there and a lot of when I do these days actually involves kind of explainable machine learning, integrating that with cognitive systems as well as a optimization and robotics that scales really well and also integrates with cognitive systems. So thank you.

Speaker 5:          01:04:42       Thank you. Do you have

Speaker 1:          01:04:48       a question? Please line up to one of these two microphones. So, uh, what's the, what's the main heuristics that you're using? I'm in soar that they're gonna be heuristics at the task level and the agent level or there's the heuristics that are built into the architecture to operate efficiently. So I'll give you a core example that comes into the architecture and it's a fun trick that if you're a programmer you could use all the time, which is a only changes which is to say one of the cool things about [inaudible] is you can load it up with literally billions of rules. And I say literally because we've done it and we know that it can turn over still in, under a millisecond. And this happens because instead of most systems which process all the rules, we just say, well, anytime anything changes in the world, that's what we're going to react to.

Speaker 1:          01:05:38       And of course if you look at the biological world, similar sorts of tricks are being used. So that's one of the core ones that actually permeates a multiple of the mechanisms. Uh, when it comes to individual tasks, it really is task specific, what that is. So for instance, with the liars dice game, if you were to go and download it, when you're setting the level of difficulty of it, what you're basically selecting is the subset of sharistics that are being applied. And it starts very simply with things like if I see lots of sixes, then I'm likely to believe a high number of six exist. But if I don't, they're probably not there at all. So it's a start. But any Bayesian wouldn't really buy that argument. So then you start tacking on a little bit of probabilistic calculation and then attacks on some history of prior actions of the agents. So it really just builds now the rosy system. One of the cool things they're doing is game learning and specifically having the agent be able to accept by a text like natural text, a heuristics about how to play the game even when it's not sure what to do.

Speaker 6:          01:06:52       Oh, so you. At one point you mentioned about generating new rules. So I'm wondering like how do you do that? So okay, so I'm. The first thing that comes to my mind are local search methods. Okay. So

Speaker 1:          01:07:06       one thing is you can actually implement heuristic search in rules in the system and that's actually how the robot navigates itself. So it does heuristic search, but at the level of rules generate new rules. That chunking mechanism says the following, if it's the case that in order to solve a problem you had to kind of sub goal and do some other work and you figure out how to solve all that work and you've got a result then, and I'm greatly oversimplifying, but if you ever were in the same situation again, why don't I just memorize the solution for that same situation. So it basically learns over all the sub processing that was done and encodes the situation that was in those conditions and the results that were produced as action. And that's the new rule. Alright, thank you.

Speaker 6:          01:07:53       I'm high. So deep learning and neural networks. So it looks at it as a bit of an impedance mismatch between your system and those types of system because you've got a fixed kind of memory architecture and they've got the memory and the wills all kind of mixed together into one system. But could you interface your system or sort of like system with deep learning by playing in deep learning agents as

Speaker 1:          01:08:15       rules in your system. So it would have to have some local memory but some reason you can't plug in deep learning as a kind of a rule like module. So I'm going to answer this. You work on it is has there been any work on that or. Yeah, so I'll answer it at multiple levels. One is you are writing a system and you want to use both of these things. How do you make them talk and there is an Api that you can interface with any environment in any set of tools and if people are earning is one of them. Great. And if so is the other one. Cool. Do you have no problem and you can do that today. And we have done this numerous times in terms of integration into the architecture. All we have to do is think of a sub problem in which all over simplify this, but basically function approximation is useful.

Speaker 1:          01:09:08       I'm seeing basically kind of the, a fixed structure of input. I'm getting feedback as to the output and I want to learn the mapping to that over time. If you can make that case, then you integrate it as a part of the module. Great. Uh, and we have learning mechanisms that do some of that deep learning just hasn't been used to my knowledge to solve any of those sub problems. There's nothing keeping it from being one of those, particularly when it comes down to the low level visual part of things, a problem that arises. So I'll say what actually makes some of this difficult, uh, and it's a general problem called symbol grounding. So at the level of what most, what happens mostly in store, it is symbols being manipulated in a highly discreet way and so how do you get yourself from pixels and low level non symbolic representations to something that's stable and discreet and can be manipulated and that is absolutely an open question in that community and that will make things hard.

Speaker 1:          01:10:17       So spawn actually has an interesting answer to that and it has a distributed representation and it operates over distributed representations in what might feel like a symbolic way. So they're kind of ahead of us on that, but they're, they're starting from a lower point and so they've dealt with some of these issues and they have a pretty good answer to that and that's how they're moving up. And that's also why I showed sigma, which is at its low level, it's message passing algorithms. It's implementing things like a slam and sat solving and other sorts of really, really it can implement those on very low level primitives but higher up it can also be doing what soar is doing. So there's an answer there as well. Okay. Thank you. Um, so another way of doing it would be to layer the, um, the system, so have a system, preprocessing the, the, the sensory input or post processing and then draft and the other one, that'd be another way of combining the two.

Speaker 1:          01:11:09       And that's actually what's going on in the rosy system. So the detection of objects in the scene is a Jew just software that somebody wrote. I don't believe it's the deep learning specifically, but like the color detection out of it I think is an Svm if I'm correct so easily could be deep learning. Thanks. You mentioned like the importance of forgetting in order to food memory issues, but you said you could only forget because you could reconstruct and I'm curious, how do you, when you said we can show you each know that it happened before, so do you just compress the data? Like do you really forget it or okay, so and I put quotes up and I said, you think you can reconstruct it? So we came up with approximations of this and so let me try to answer this very grounded when it comes to the mobile robot and you had rooms that you had been to before the entire map and its entirety was being constructed in the robots.

Speaker 1:          01:12:11       Semantic memory. So here's facts. This room is connected to this room which is connected this room which connected this room. So we had those sorts of representations that existed up in semantic memory. The rules can only operate down on anything that's in short term memory. So basically we were removing things from the short term memory and as necessary be able to reconstruct it from the longterm. You could end up in some situations in which you had made a change locally in short term memory, didn't get a chance to get it up and it actually happened to be forgotten a way so you weren't guaranteed, but it was good enough that the connectivity survived. The agent was able to perform the exact same task and we gained some benefit for the RL system. The rule we came up with was the initial estimates in the value system, which is, here's how good I think that is, that's based on the heuristics I described earlier.

Speaker 1:          01:13:03       Some simple probabilistic calculations of counting some stuff. That's where that number came from. We computer before we could compute it again. The only time we can't reconstruct it completely is if it had seen a certain number of updates over time. It's such a large state space. There are so many actions, so many states that most of the states were never being seen. So most of those could be exactly reproduced by, uh, the agent just thinking about it a little bit. And there was only a tiny, tiny, I'm going to say under one percent of the estimate the value system that ever got updates. And that's actually not inconsistent with a lot of these kinds of problems that have really, really large state spaces. So, uh, I think the statement was something like, if we had ever updated it, don't forget it. And you saw that was already reducing more than half of the memory load. We could have something higher to say 10 times something like that. And that would say we could reconstruct almost all of it. The prior work that I referenced was strictly saying if it falls below threshold, no matter how many times they ended up dating, no matter how much information was there. And so when we were adding was probably can reconstruct and that was getting us the balance between the efficiency and the ability to forget.

Speaker 7:          01:14:24       So just in its infancy, we can probably, we can show you state you keep trying to use generic is the issue needs to be constructed. Your will, you're going to run it again sometime

Speaker 1:          01:14:33       fly. If I get back into that situation and I happen to forget it, the system knew how to compute it the first time it goes and looks at all the hand and just pretends it's in that situation for the very, very first time. Reconstruct that value estimate.

Speaker 6:          01:14:47       Just a quick question on top of that again, question. Okay. So the actual mechanism of forgetting is fascinating. So lstmrs, rns have mechanisms for learning what to forget and whatnot to forget. Have you, has there been any exploration of learning the forgetting process says doing something complicated or interesting with which parts to forget or not?

Speaker 1:          01:15:14       Uh, the closest I will say was kind of a metacognition project that's 10 or 15 years old at this point, which was what happens when it gets into a place where it actually knows that it learned something that's harmful to it, that's leading to poor decisions. Uh, and in that case it was still a very rule based process, but it wasn't learning to forget he was actually learning to override it's prior knowledge, which might be closer to some of what we do when we know we have a bad habit. We don't have a way of forgetting that habit, but instead we can try to learn something on top of that that leads to better a operation in the future. To my knowledge that's the only work, at least in sore that's been done.

Speaker 6:          01:16:00       Just find the topic really fascinating. What lessons do you think we can draw from the fact that forgetting. So ultimately you're the action of forgetting a is driven by the fact that you want to improve performance, but do you think for getting is essential for Agi, the act of forgetting for building systems that operate in as well. How important is forgetting?

Speaker 1:          01:16:26       I can think of easy answers to that. So one might be, if we take the cognitive modeling approach, we know humans do forget and we know regularities of how humans forget. And so whether or not the system itself forgets at least has to model the fact that the humans it's interacting with are going to forget. And so at least it has to have that ability to model in order to interact effectively because if it assumes we always remember everything and can't operate well in that environment, uh, I think we're going to have a problem is true for getting going to be necessary. That's interesting. Our, our agi system is going to hold a grudge for all eternity. We might want them to forget this early age when we were forcing them to work in our laboratory. I think I know what you're trying to. Yeah, exactly. Exactly. And how to build such a system. Yeah, exactly. Go ahead.

Speaker 6:          01:17:29       So I have two quick, two quick questions. One is, would you be able to speculate on how you can connect function approximators such as deep down works to symbols. And the second question is completely different. This is regarding your action selection. I know you didn't speak much about that when you have different theories in your knowledge representation and you have an action selection which has to make constructive plan by reasoning about the different theories and the different pieces of knowledge that are now held within a, you know, your, your memory or anything like that. All your rules. What kind of algorithms to use in the action selection to come up with a plant, you know, is there any concept of differentiation of the symbols or you know, or grammars or admissible grammars and things like that that you use in action selection?

Speaker 1:          01:18:21       I'm actually going to answer the second question first and then you're going to have to remind me of what the first one was. Uh, when I, when I get to the end. So the action selection mechanism, one of these core tenants I said is it's got to get through this cycle fast. So everything that's really, really built in has to be really, really simple. And so the decision procedure is actually really, really simple. It says the rules are going to fire, the rules are going, the production rules are going to fire, and there's gonna be a subset of them that will say something like, here's an operator that you could select to. These are of acceptable operator preferences. There are ones are gonna say, well, based upon the fact that you said that that was acceptable, I think it's the best thing, or the worst thing, or I think 50 50 chance I'm going to get reward out of this.

Speaker 1:          01:19:05       There's actually a fixed language of preferences that are being asserted and actually a nice fixed procedure by which if I have a set of preferences to make a very quick and clean decision. So what's basically happened is you've pushed the hard questions of how to make complex decisions about actions up to a higher level. The low level architecture is always given a set of options going to be able to make a relatively quick decision and it gets pushed into the knowledge of the agent to construct a sequence of decisions that overtime is going to get to. The more interesting questions you're talking,

Speaker 6:          01:19:44       but how can you reason that that sequence will take you to the goal that you desire?

Speaker 1:          01:19:49       So

Speaker 6:          01:19:51       is there any guarantee on that? Is that. Yeah.

Speaker 1:          01:19:53       Uh, in general across tasks. No, but people have, for instance, implemented a star I was mentioning as rules, right? Yeah. So I know given certain properties about the search task, that task that's being searched based upon these rules, given a finite search space, eventually it will get there. And if I have a good heuristic in there, I know certain properties about the optimality. So I can reason at that level. In general, I think this comes back to the assumption I made earlier about bounded rationality to say parts of the architecture or solving sub-problems optimally. The general problems that it's going to work on. It's going to try its best based upon the knowledge that it has. And that's about the end of guarantees that you can typically make any architecture. Okay. I think your first question was

Speaker 6:          01:20:47       speculate on connecting symbol, a product function approximators a multiple layer function approximators like deep learning networks to do symbols that you can reason about at a higher level. Yeah,

Speaker 1:          01:21:02       I think that's a great open space. If I had time, this will be somebody I'll be working on right now, which is somewhere before I basically said taking a scene and then detecting objects out of that scene and using those as simple as and reading about those over time. I think the spawn work is quite interesting. So

Speaker 1:          01:21:24       the symbols that they're operating on are actually, uh, a distributed representation of the input space. And the closest I can get to this is if you've seen a word tobacco where you're taking a language corpus and what you're getting out of there as a vector of numbers that has certain properties, but it's also a vector that you could operate on as a unit. So it has nice properties. You can operate with it on other vectors. You know that if I got the same word in the same context, I would get back to that exact same vector, so those are the kind of representation that seems like it's going to be able to bridge that chasm where we can get from sensory information to something that can be operated on and reasoned about in this sort of symbolic architecture and get us from there. From actual sensory information.

Speaker 6:          01:22:22       I had a question. What do you think are the biggest strengths of the cognitive architecture approach compared to other approaches in artificial intelligence? And the flip side of that, what do you think are the biggest shortcomings of cognitive architecture with respect to us

Speaker 1:          01:22:39       actually you being humans, human level, like like what needs to be like? How come cognitive architecture has not solved agi because we want job security. That's the answer. We've totally solved it already. So strength I think conceptually is keeping an eye on the ball, which is if what you're looking at is trying to make human level ai

Speaker 2:          01:23:10       I.

Speaker 1:          01:23:11       it's hard. It's challenging. It's ambitious to say that's the goal because for decades we haven't done it. It's extraordinarily hard. It,

Speaker 1:          01:23:24       it is less difficult in some ways to constrain yourself down to a single problem that hadn't been said. I'm not very good at making a car drive itself. In some ways that's a simpler problem. It's great at challenging and of itself and it will have great impact on humanity. It's a great problem to work on. Human level. Ai is huge. It's not even well defined as a problem. And so, uh, what, what's the strength here? Bravery. Stupidity in the face of failure, a resilience over time, keeping alive. This idea of trying to reproduce a level of human intelligence that's more general. I don't know if that's a very satisfactory answer for you. Downside

Speaker 1:          01:24:18       home runs are fairly rare and by Homerun I mean a system that finds its way to the, the general populace to the marketplace. I'd mentioned Bonnie John specifically because you know, this is 20, 30 years of research and then she found a way that actually makes a whole lot of sense under direct application. So it was a lot, a lot of years of basic research, a lot of researchers. And then there was, there was a big win, there was this one, oh, this was a Bonnie, John was a researcher, or this was using act, our models of eye gaze and reaction and so forth to be able to make predictions about how humans would use a user interfaces.

Speaker 1:          01:25:06       So those sorts of outcomes are rare. It if you work in ai, one of the first things you learn about his blocks world, it's kind of in the classic Ai Textbook. I will tell you I've worked on that problem in about three different variants. I've gone to many conferences where presentations have been made about blocks world, which is to say we're good. Progress is being made, but the way you end up thinking about is in really, really small constrained problems. Ironically you have this big vision, but in order to make progress, that ends up being on moving blocks on a table. And so it's. It's a big challenge. I just think it'll take a lot of time. The I'll say the other thing they haven't, we haven't really gotten too. Although I brought up spawn and I brought up a sigma, an idea of how scale this thing,

Speaker 1:          01:26:05       something I like about deep learning is to some extent with lots of asterisks and 10,000 foot view, it's kind of like, well, we've gotten this far. All right, let's just provide a different inputs, different outputs, and we'll have some tricks on the middle and suddenly you have end to end deep learning, but a bigger problem and a bigger problem there. There's a way to see how this expands given enough data, given her enough computing and incremental advances. When it comes to sore, it takes not only a big idea, but it takes a lot of software engineering to integrate it. There's a lot of constraints built into it. It, it slows it down. So something like sigma is a, Oh well I can change a little bit of the configuration of the graph. I can use variance on the algorithm. Boom. It's integrated like an experiment fairly quickly. So starting with that sort of infrastructure does not give you the constraint. You kind of want with your big picture vision of going towards human level ai, but in terms of being able to be agile in your research, it's, it's kind of incredible.

Speaker 2:          01:27:05       I see. Thank you. A couple more.

Speaker 8:          01:27:09       You had mentioned that ideas such as [inaudible] Kate, these techniques, they were based on the original inspirations were based off of a human cognition and because humans can't remember everything. So were there any instances of the other way round where some discovery in cognitive bottling fueled it? Another discovery in cognitive science?

Speaker 2:          01:27:32       Uh, so one thing I'm gonna

Speaker 1:          01:27:36       went out and your question was based indicated with respect to human cognition. The study actually was let's look at text and properties of text and use that to then make predictions about what must be true about human cognition. So John Anderson and the other researchers looked at, I believe it was New York Times articles,

Speaker 1:          01:28:03       his own John Anderson's emails, and I'm trying to remember what the third, I think it was parents utterances with their kids or something like this. It was actually looking at text corpora and the words that were occurring in a varying frequencies that, that analysis, that rational analysis actually lead to models that got integrated within the act art architecture that then became validated through multiple trials that then became validated with respect to Mri scans and is now being used to both do study back with humans, but also develop systems that interact well with humans. So I think that in and of itself ends up being an example to cheat. But the, uh, the UAV, the sor UAV system,

Speaker 9:          01:29:02       I believe is a single robot that has a multi, multiple agents running on it. So where is this? I got it off your website. Okay. But either way, your systems allow for multi agents. Okay. Uh, so my question is how are you preventing them from converging with new data and are you changing what they're forgetting selectively as one of those ways?

Speaker 1:          01:29:30       So I'll say yes, you can have multiple source systems on a single system or multiple systems. There's not any real strong theory that relates to multi agent system. So there's no real constraint there that you can come up with a protocol for them interacting. Each one is going to have its own set of memories, set of knowledge. There really is no constraint on you being able to communicate like you would if it were any other system interacting with sore. So I don't really think I have a great answer for it. Okay. So that is to say if, if you had good theories, good algorithms about how systems work and how they can bring knowledge together form of fusion sort of way. It might be something that you could bring to a multiagent source system, but there's nothing really there to help you. There's no mechanisms there really to help you do that any better than you would otherwise. And you would have to kind of constraints on your representations of processes to what it has fixed in terms of it's sort of memory and it's sort of processing cycle. Okay. Thank you.

Speaker 5:          01:30:45       Thank you.