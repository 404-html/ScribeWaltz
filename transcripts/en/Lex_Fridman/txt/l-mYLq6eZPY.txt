Speaker 1:          00:00          The following is a conversation with Peter at bill. He's a professor at UC Berkeley and the director of the Berkeley robotics learning lab. He's one of the top researchers in the world working on how we make robots understand and interact with the world around them, especially using imitation and deeper enforcement learning. This conversation is part of the MIT course and artificial general intelligence and the artificial intelligence podcast. If you enjoy it, please subscribe on Youtube, Itunes, or your podcast provider of choice or simply connect with me on Twitter at Lex Friedman, spelled f, r I. D. And now here's my conversation with Peter a bill.

Speaker 2:          00:41          You've mentioned that if there was one person you could meet you be Roger Federer. So let me ask, when do you think will have a robot that fully autonomous, he can beat Roger Federer at tennis, Roger Federer level player a tennis? Well first, if you could make it happen for me to meet Roger, let me know terms, getting a robot to be him in tennis. It's kind of an interesting question because for a lot of the challenges we think about in Ai, the software, it's really the missing piece. But for something like this, the hardware is nowhere near either to really have a robot that can physically run around the Boston dynamics robots, they're starting to get there, but still not really human level ability to do, run around and then swing a racket.

Speaker 2:          01:36          So you think that's a hardware problem? I don't think it's a harder problem only. I think it's a hardware and a software problem. I think it's both. I think they'll, they'll have independent progress. So I'd say the, the hardware maybe in 10 15 years and this way, not grass grass plague. I, I'm not sure what's Carter? Cras or clay. The clay involves sliding, which might be harder to master actually. Yeah, but you, you're not limited to a bipedal. I mean I'm sure there's only going to build a machine. It's a whole different question. Of course if it can, if you can say, okay, this robot can be on wheels, it can move around on wheels and can be designed differently, then I think that that can be done sooner probably down a full human humanoid type of setup. What do you think his swing a rack, so you've worked at a basic manipulation, how hard it do you think is the task of swinging Arachat would uh, be able to hit a nice backhand or a forehand, let's say, let's say we just set up stationary, uh, an a nice robot arm, let's say, you know, standard industrial arm and it can wash the ball calm and then swing the racket.

Speaker 2:          02:50          It's a good question. I'm not sure it would be super hard to do. I mean I'm sure it would require a lot if we do it with reinforcement learning would require a lot of trial and error. It's not going to swing it right the first time around. But yeah, I don't, I don't see why I couldn't see anything. I think it's learnable. I think if you set up a ball machine, let's say on one side and then a robot with a tennis racket on the other side, I think it's learnable and maybe a little bit of pre training and simulation. Yeah, I think that's, I think that's feasible. I think, I think the swing, the racket is feasible. It'd be very interesting to see how much precision it can get. Okay. Cause I mean that's, that's where, I mean some of the human players can hit it on the lines, which is very high precision with spin. This one is, it is, it is an interesting, uh, well the RL can learn to put a spin on the ball. All you got me interested maybe some day we'll set this off your hands

Speaker 1:          03:51          sir. Is basically okay for this problem. It sounds fascinating, but for the general problem of a tennis player, we might be a little bit farther away. What's the most impressive thing you've seen a robot doing? The physical world?

Speaker 2:          04:04          So physically for me it's

Speaker 2:          04:08          the Boston dynamics videos always just ring home and just super impressed. Recently, the robot running up the stairs during the park who are type thing. I mean, yes, we don't know what's underneath. They don't really ride a lot of detail. But even if it's hard code than than the neath, which you might or might not be. Just the physical abilities of doing that park who are at, that's a very impressive. So a lot right there. Have you met spot many or any of those robots in person? It's spot mini. Last year in April at demarce event that Jeff pieces organizes, they brought it out there and it was nicely falling around Jeff, when Jeff left the room, they had it follow him along, which is pretty impressive. So I think

Speaker 1:          04:53          good. There's some confidence to know that there's no learning going on on those robots, the psychology of it. So while knowing that, while knowing there's not, if there's any learning going on, it's very limited. I met spot mini earlier this year and knowing everything that's going on, having one on one interaction. So I've got to spend some time alone and there's a immediately a deep connection on the psychological level. Even though you know the fundamentals, how it works, there's something magical. So do you think about the psychology of interacting with robots and this physical world? Even you just showed me the PR to the robot and there was a little bit something like a face head, a little bit, something like a face. There's something that immediately you draws you to it. Do you think about that aspect of, of the robotics problem?

Speaker 2:          05:45          Well, it's very hard with Brett here. We're giving them a name. Berkeley robot for the elimination of tedious task is very hard to not think of the robot as a person and it like everybody calls him a he for whatever reason, but that also makes it more a person than if it was a it and it's, it seems pretty natural to think of it. That would, this past weekend really struck me of FCM pepper many times on on videos. But then I was at an event organized by, this was by fidelity and they had scripted pepper to help moderate some sessions and at scripted pepper to have the personality of a child a little bit. And it was very hard to not think of it as it's own person in some sense because it was just going to jump in. It would just jump into conversation and in very interactive with moderate we'd be saving, the pepper would just jump in, hold on, how about me?

Speaker 2:          06:40          Can I participate in this? Doing? And just like, okay, this is like like a person. And that was 100% scripted. And even then it was hard not to have that sense of somehow there is something there. So as we have robots interact in this physical world, is that a signal that can be used in reinforcement learning? You've, you've worked a little bit in this direction, but do you think that psychology can be somehow pulled in? So that's a question I would say a lot. A lot of people ask. And I think part of why they ask it is there thinking about how unique are we really still ask people. Like after they see some results, they see a computer play go to say computer do this, that they're like, okay, but can it really have emotion? Can it really interact with us in that way?

Speaker 2:          07:26          And then once you're around robots, you already start feeling it. And I think that kind of maybe mythologically the way that I think of it as if you run something like reinforcement learning, it's about optimizing some objective and there was no reason that the objective couldn't be tied into how much does a person like interacting with this system and why it could not. The reinforcement learning system optimized for the robot being fun to be around and why wouldn't it then naturally become more and more attractive and more and more maybe like a person or like a pet. I don't know what it would exactly be, but more and more have those features and acquire them automatically. As long as you can formalize an objective of what it means to like something. What, how you exhibit, uh, what's the ground truth? How do you, how do you get the reward from human?

Speaker 2:          08:19          Cause you have to somehow collect that information with the new human, but you're, you're saying if you can formulate it as an objective, it can be learned. There is no reason it couldn't emerge through learning. And maybe one way to formulate as an objective you wouldn't have to necessarily score and explicitly. So standard rewards had numbers and numbers are hard to come by. I see. It's a 1.5 or 1.7 on some skill. It's very hard to do for a person, but much easier is for a person to say, okay, what you did the last five minutes was much nicer than when we did the previous five minutes. And that now gives a comparison comparing, in fact there has been some results that, for example, Paul Christian and collaborators had opening. I had the hopper, we Joko hopper one legged robot that flew back flips purely from feedback. I like this better than that. That's kind of equally good. And after a bunch of interactions, it figured out what it was the person was asking for it, namely a backflip. And so I think the same thing, oh, it wasn't trying to do a backflip, it was just getting a score from the comparison score from the person

Speaker 1:          09:21          based on, uh,

Speaker 2:          09:23          person having a mind in their own mind what I wanted to do a back flip. But the robot didn't know what it was supposed to be doing. It just knew them. Sometimes the person said this is better, this is force. And then the robot figure it out what the person was actually after it was a back flip. And I imagine the same would be true for things like a more interactive robots that the robot would figure out over time. Oh, this kind of thing apparently as appreciate it more than his other kind of thing. So

Speaker 1:          09:50          when I first picked up a sudden's, uh, which is sons reinforcement learning book before sort of this deep learning, um, before the reemergence of neural networks is a powerful mechanism for machine learning. Rl seemed to me like magic. It was a as beautiful. So it that seemed like what intelligence is a RL reinforcement learning. So, uh, how do you think we can possibly learn anything about the world when they're award for the actions is delayed, is so sparse. Like where is, why do you think are works? Why do you think he can learn anything under such sparser awards? Whether it's regular reinforcement learning, deep reinforcement learning, it's your intuition.

Speaker 2:          10:40          The counterpart of that is why is RL, why does it need so many samples? So many experiences to learn from. Um, because really what's happening is when you have a sparse reward, you do something maybe for like, I dunno, you'd take a hundred actions and then you get a reward and maybe get like a score of three. And I'm like, okay, three, not sure what that means. You go again and now I get to, and now you know that that sequence of a hundred actions that you did the second time around, somehow it was worse than the sequence of hundred actions he did the first time around. But that's tough to know which one of those were better or worse. Some might have been good and bad and either one, and so that's why I need so many experiences. But once you have enough experiences, effectively RLS teasing that apart, it's turned to say, okay, well what is consistently there? When you get a high reward and what's consistently there, we can get a lower award. And then kind of the magic of it as the policy grant update is to say, now let's update the neural network to make the actions that were kind of precedent when things are good, more likely and make the actions that are present when things are not as good, less likely.

Speaker 1:          11:45          So that's the, that is the counterpoint, but it seems like you would need to run it a lot more than you do. Even though right now people could say that is very inefficient, but it seems to be way more efficient than one would imagine on paper that the, the simple updates to the policy, the policy gradient, that that somehow you can learn is executives just said, what are the common actions that seem to produce some good results that that somehow can learn anything. It seems counterintuitive, at least did they, is there some intuition behind that? So yeah,

Speaker 2:          12:18          I think there's a few ways to think about this. The way I tend to think about it mostly originally when, so when we started working on deep reinforcement learning here at Berkeley, which was maybe 2011, 12, 13, around that time challenge Schulman was a phd student initially. Kind of driving it forward here. And kind of the way we thought about it at the time was if you think about rectified linear units or kind of rectifier type neural networks, um, what do you get? You get something that's piecewise linear feedback control. And if you look at the literature, uh, linear feedback control is extremely successful, can solve many, many problems surprisingly well. I remember for example, when we did the helicopter flight, if you're in a stationary flight regime, not a non station but the station, their flight regime like hover, you can use linear feedback control to stabilize and helicopter, very complex dynamical system.

Speaker 2:          13:15          But the controller is relatively simple. And so I think that's a big part of is that if you do feedback control, even though the system you control can be very, very complex, often relatively simple control architectures can already do a lot. But then also just linear is not good enough. And so one way you can think of this neural networks is that the incidences they tiled a space which people were already trying to do more by hand or with finite state machines. Say this linear controller here, this linear controller here, neural network learns the towel dispenser, Linear Controller here at another leaner controller here. But it's more subtle than that. And so it's benefiting from this linear control aspect is benefiting from the tiling, but it's somehow telling it one dimension at a time. Because if, let's say you have a two layer network, even the hidden layer, you make a transition from active to inactive or the other way around that is essentially one axes but not access align but one direction that you change. And so you have this kind of very gradual tiling of the space. We have a lot of sharing between the linear controllers that towel the space. And that was always my intuition as to why to expect that this might work pretty well. It's essentially leveraging the fact that linear feedback control is so good, but of course not enough. And this is a gradual tiling of the space with leaner feedback controls that share a lot of

Speaker 1:          14:35          expertise across them. So that, that's a, that's really nice intuition. What do you think that scales to the more and more general problems of when you start going up the number of control dimensions, uh, when you start going

Speaker 2:          14:51          down in terms of how often you get a clean reward signal, does that intuition carry forward to those crazy or weird or worlds that we think of as the real world? So I think where things get really tricky in the real world compared to the things we've looked at so far with great success in reinforcement learning is

Speaker 2:          15:16          the timescales, which takes us to an extreme. So when you think about the real world, I mean, I dunno, maybe some student decided to do a phd here. Right, okay. That's, that's a decision that's a very high level decision. But if you think about their lives, I mean any person's life, it's a sequence of muscle fiber contractions and relaxations and that's how you interact with the world and that's a very high frequency control thing, but it's ultimately what you do and how you affect the world until I guess we have brain readings and you can maybe do it differently, but typically that's how you affect the world and the decision of doing a phd, it's like so abstract relative to what you're actually doing in the world. And I think that's where credit assignment becomes just completely beyond what any current RL algorithm can do.

Speaker 2:          16:06          And we need hierarchical reasoning at a level that is just not available at all yet. Where do you think we can pick up hierarchal reasoning? By which mechanisms? Yeah, so maybe let me highlight for that. I think the limitations are of what already was done 2030 years ago. In fact, you'll find reasoning systems that reason over relatively long horizons, but the problem is that that we're not grounded in the real world. So people would have to hand design, uh, some kind of logical dynamical descriptions of the world and that didn't tie into perception and so then tie into real objects and so forth. And so that, that was, that was a big gap. Now with deep learning, we start having the ability to really see with sensors process that and understand what's in the world. And so it's a good time to try to bring these things together.

Speaker 2:          17:04          One, I see a few ways of getting there. One way to get there would be to say deep learning can get bolted on somehow to some of these more traditional approaches. Now bolted on would probably mean you need to do some kind of end to end training where you say, my deep learning processing somehow leads to a representation that in term uses some kind of traditional underlying dynamical, uh, systems that can be used for planning. And that's, for example, the direction of Eve. Tamara and the North Korea attach here have been pushing with causal Infigen and of course other people too that that's one way can we somehow force it into the form factor that is amenable to reasoning. Another direction we've been thinking about for a long time and didn't any progress on was more information theoretic approaches. So the idea there was that what it means to take high level action is to taken choose a latent variable.

Speaker 2:          18:01          Now that tells you a lot about what's going to be the case in the future because that's what it means to, to take a high level auction. I say, okay, what do I decide I'm going to navigate to the gas station? Cause you need to get gas for my car. Well that will not take five minutes to get there. But the fact that I get there, I could already tell that from the high level action at took much earlier, um, that we had a very hard time getting success with. I'm not saying it's a dead end, the silly, but we had a lot of trouble getting that to work. And then we start revisiting the notion of what are we really trying to achieve. Um, what we're trying to achieve is not necessarily how he perceived. We could think about what this hierarchy give us.

Speaker 2:          18:43          Um, what it's we hope it would give us this better credit assignment. Um, Kinda what is better criticized is given, is giving us, it gives us faster learning, right? And so fast to learning is ultimate maybe what we're after. And so that's what we ended up with the RL squared paper on learning to reinforcement learning, which had a time rocky Dwan led. Um, and that's exactly the metal learning approach where we say, okay, we don't know how to design hierarchy. We know what we want to get from it. Let's just end to an optimize for one to get from it and see if it might emerge. And we saw things emerged, the amaze navigation had consistent motion down hallways, which is what you want a hierarchical control should say, I want to go down this hallway. And then when there is an option to take a turn, I can decide whether to take a turn in naught. And repeat even had the notion of where have you been before or not, do not revisit places you've been before. Um, it still didn't scale yet to the real world kind of scenarios I think you had in mind, but it was some sign of life that maybe you can metal learn these hierarchal concepts.

Speaker 1:          19:51          I mean, it seems like, uh, through these metal learning concepts, get at the, what I think is one of the hardest and most important problems of AI, which is transfer learning. So this generalization, how far along this journey towards building general systems are we being able to do transfer learning? Well, so there's some signs that you can generalize a little bit, but do you think we're on the right path or s totally different breakthroughs and needed to be able to transfer knowledge between different learned models?

Speaker 2:          20:31          Yeah, I'm pretty tired on this and that. I think there are some very of the day they're there. There's just some very impressive results already, right? I mean, yes. I would say when even with the initial, kind of a big breakthrough in 2000 with Alex snap, right. Did initial, the initial thing is, okay great, this does better on image net, the hands image recognition, but then immediately thereafter there was of course the notion that wow, what was learned on image net and you and I want to solve a new task, you can fine tune Alex net for new tasks and that was often found to be the even bigger deal that you learned something that was reusable, which was not often the case before. Usually machine learning you learned something for one scenario and that was it and that's really exciting. I mean that's a huge application.

Speaker 2:          21:22          That's probably the biggest success of transfer learning to date in terms of scope and impact. That was a huge breakthrough. And then recently I feel like similar kind of by scaling things up, it seems like this has been expanded upon like people training, given bigger networks they might transfer even better if you looked out for example, some of the opening I results on language models and some of the recent Google results on language models they are learn for just prediction and then they get reused for other tasks. And so I think there is something there where somehow if you train a big enough model and enough things it seems to transfer some deep mine results that I thought were very impressive to unreal results where um, it was learning to navigate mazes in ways where it wasn't just doing reinforcement learning but it had other objectives it was optimizing for.

Speaker 2:          22:16          So I think there's a lot of interesting results already. I think maybe where it's hard to wrap my head around this to which extent l or when they would call something generalization, right? Or the levels of generalization involved in this different tasks. All right, so you draw this by the way, just to frame things I've heard you say somewhere is the difference between learning to master versus learning. To generalize that it's a nice line to think about and it gets, you're saying there's a gray area of what learning to master and learning to generalize where one starts. And the might have heard this, I might never heard it somewhere else. And I think it might've been one of one of your interviews and maybe the one with Yosha Benjamin, I'm not a hundred percent sure, but I liked the example and I'm to not sure who it was.

Speaker 2:          23:08          But the example was essentially if you use current deep learning techniques, what we're doing to predict, um, let's say they're out of motion of, of um, of our planets, it would do pretty well. Um, but then now if a massive new mass enters our solar system, it would proud not predict what will happen. Right. And that's a different kind of channelization that's a generalization that relies on the ultimate, simplest, simplest explanation that we have available today to explain the motion of planets where I was just pattern recognition could predict our current soil system motion. Well, no problem. And so I think that's an example of a kind of generalization that is a little different from what we've achieved so far. And it's not clear if just, you know, regularizing more enforcing it to come up with a simpler, simpler, simpler, simpler. But that's what physics researchers do, right? They say, can I make this even simpler? How simple can I get this? What's the simplest equation that can explain everything? All right. The master equation for the entire dynamics of the universe. We haven't really pushed that direction as hard. And in the planning I would say, I'm not sure if it should be pushed, but it seems a kind of general history you'd get from that, that you don't get in our current methods so far.

Speaker 1:          24:27          So I just talked to Vladimir vap. Nick, for example, who was a statistician, a statistical learning and he kind of dreams of creating these are the a equals e equals mc squared for a learning. Right? The general theory of learning. Do you think that's a fruitless pursuit in a near term in within the next several decades?

Speaker 2:          24:51          I think that's a really interesting pursuit. And uh, and in the following sense in that there is a lot of evidence that the brain is pretty modular. And so I wouldn't maybe think of it as the theory, maybe the underlying theory, but more kind of the principal were there have been findings where people who are blind we'll use the part of the brand usually used for vision for other functions. And even after, uh, some kind of, if people get rewired in some way they might, I'm able to reuse parts of their brand for other functions. And so what that suggests is some kind of modularity and I think it is a pretty natural thing to strive for to see can we find that modularity? Can we find this thing? Of course it's not every, every part of the brain is not exactly the same. Not Everything can be rewired arbitrarily. But if you think of things like the Neocortex, which a pretty big part of the brain that seems fairly modular from what the findings so far, can you design something equally modular and if you can just grow it becomes more capable probably. I think that would be the kind of interesting underlying principle to shoot for that is not unrealistic. Do you think

Speaker 1:          26:10          you prefer math or empirical trial and error for the discovery of the essence of what it means to do something intelligent. So reinforcement learning and bodies, both groups, right? The prove that something converges, prove the bounds. And then at the same time a lot of those successes that are, well let's try this and see if it works. So which do you gravitate towards? How do you think of those two parts of your brain?

Speaker 2:          26:36          So maybe I would prefer we can make progress with mathematics. And the reason maybe I would prefer that is because often if you have something you can mathematically formalize, you can leap frog a lot of experimentation and experimentation takes a long time to get through and a lot of trial and error kind of reinforcement, letting your research process. Um, but you need to do a lot of trial and error before you get to a success. So if we can leap out at, to my mind, that's what the math is about. And hopefully once you do a bunch of experiments, you start seeing a pattern. You can do some derivations that leapfrog some experiments. But I agree with you. I mean in practice a lot of the progress has been such that we have not been able to find the math that allows us to leapfrog ahead and we are kind of making gradual progress, wants to out at time a new experiment here in new experiment there that gives us new insights and gradually building up but not getting to something yet where we're just, okay, here is some equation and now explains how, you know, that would be have been two years of experimentation to get there.

Speaker 2:          27:42          But this tells us what the result's going to be. I'm unfortunately not so much as not so much yet, but your hope is there

Speaker 1:          27:50          in trying to teach uh, robots or systems to do every day task or even in simulation, what, what do you think you're more excited about imitation, learning or self play. So letting robots learn from humans or letting robots plan their own to try to figure out and then their own way and eventually play, eventually interact with humans or solve whatever problem is. What's the more exciting to you? What's more promising you think as a research direction. So

Speaker 2:          28:27          when, when we look at self play, what's so beautiful about it is because back to kind of the challenges and reinforcement learning. So the challenge for me and forced planting, it's getting signal. And if you don't never succeed, you don't get any signal in self play. You're on both sides. So one of you succeeds and the beauty is also one of you fails. And so you see the contrast, you see the one version of me that it better than the other version. And so every time you play yourself you get signal. And so whenever you can turn something into self play, you're in a beautiful situation where you can naturally learn much more quickly than in most other reinforced wanting environments. So I think, I think if somehow we can turn more reinforcement learning problems into self play formulations, that would go really, really far.

Speaker 2:          29:17          So far. South play has been largely around games where the risk Nashville opponents, but if we could do self play for other things and let's say, and I know a robot learns to build a house, I mean that's a pretty advanced thing to try to do for our robot, but maybe tries to build a hut or something. If that can be done through self play, it would learn a lot more quickly if somebody can figure it out. And I think that would be something where it goes closer to kind of the mathematical leapfrogging where somebody figures out a formalism to say, okay, any RL problem by playing this and this idea, you can turn it into self play problem where you get signal a lot more easily. Reality is many problems who don't know how to turn to self play. And so either we need to provide detailed reward that doesn't just reward for achieving a goal, but rewards for making progress.

Speaker 2:          30:02          And that becomes time consuming. And once you're starting to do that, let's say you want a robot to do something, you need to give all this detailed reward. Well, why not just give a demonstration, right? Because why not just show the robot and now the question is how do you show the robot? One way to show is to teleoperate to robot and then the robot really experienced this things and that's nice cause that's really high signal to noise ratio there. Then we've done a lot of that and you teach her about skills in just 10 minutes. You can teach a robot a new basic skill like okay, pick up the bottle and place it somewhere else. That's a skill no matter where the bottle starts, maybe it always goes up to a target or something that's fairly used to teach her about with telly off.

Speaker 2:          30:39          Now what's even more interesting if you can now teach her about through third person learning where the robot watches you do something and doesn't experience it but just watches it and says, okay, well if you're showing me that that means I should be doing this and I'm not going to be using your hand because I don't get to control your hand but I'm going to use my hand. I'd do that mapping. And so that's where I think one of the big breakthroughs has happened this year. This was led by a Chelsea Finn here. Um, it's almost like learning and machine translation for demonstrations where you have a human demonstration and the robot learns to translate it into what it means for the robot to do it. And that was a metal learning from English and learn from one to get the other. Um, and that I think opens up a lot of opportunities to learn a lot more quickly.

Speaker 2:          31:24          So my focus is in autonomous vehicles. Do you think this approach of third person watching is a Ma the, the autonomous driving is amenable to this kind of approach? So for autonomous driving, I would say it's their person is slightly easier. And the reason I'm going to say slightly easier to do with third person is because the hard dynamics are very well understood. So the easier than a first person you mean or easier. So I think that distinction between third person and first person is not a very important distinction for autonomous driving. Yeah, they're very similar because the distinction is really about who turns the steering wheel or maybe I'll, let me put it differently. How it gets from a point where you are now to a point, let's say a couple of meters in front of you and that's a problem that's very well understood and that's the only distinction between third and first person there. Whereas with robot, many places and interaction forces are very complex and it's still a very different thing. Um, for autonomous driving, I think there is still the question imitation versus RL. So imitation gives you a lot more signal. I think we're, imitation is lacking and needs some extra machinery. Is it

Speaker 3:          32:43          okay?

Speaker 2:          32:43          Doesn't and it's normal format. It doesn't think about goals or objectives. And of course there are versions of imitation, learning, inverse reinforcement, any type imitational in which I'm also thinks about goals. I think then we're getting much closer. But I think it's very hard to think of a fully reactive har generalizing. Well if it really doesn't have a notion of objectives to generalize well to the kind of general that you would want, you'd want more than just that reactivity that you get from just behavioral cloning. Slash supervise learning.

Speaker 3:          33:16          Okay.

Speaker 1:          33:17          So a lot of the work, whether it's self play or even imitation learning would benefit significantly from simulation, from effect as simulation. And you're doing a lot of stuff in the physical world and in simulation. Do you have hope for greater and greater power of simulation, Lou being boundless eventually to where most of what we need to operate in the physical world, what could be simulated to degree that's directly transferable to the physical world or are we still very far away from that?

Speaker 2:          33:51          So,

Speaker 3:          33:54          okay.

Speaker 2:          33:54          I think we could even rephrase that question in some sense please. So the power of simulation, right, as similar as get better and better of course becomes stronger and we can learn more assimilation. But there's also another version which is where you said assimilate, it doesn't even have to be that precise as long as it's somewhat representative. And instead of trying to get one simulator that is sufficiently precise to learn in and transfer really well to the real world. I'm going to build many simulators, ensemble of simulators, ensemble of simulators. Not any single one of them is sufficiently representative of the real world such that it would work if you train in there. But if you trained in all of them, then there was something that's good in all of them. The real world will just be, you know, another one I've done, but that's not identical to any one of them, but just another one of them. Now, the sample from the distribution of simulators, we do live in a simulation. So, uh, this is just one, one other one. I'm not sure about that. But yeah, it's definitely a very advanced simulator of it is. Yeah.

Speaker 1:          35:04          Pretty good one. I've talked to us to rustle something. You think about it a little bit too, of course you're like really trying to build these systems, but do you think about the future of AI? A lot of people have concern about safety. How do you think about AI safety as you build robots that are operating in the physical world? What, what is, um, yeah, how do you approach this problem in an engineering kind of way? In a systematic way.

Speaker 3:          35:28          Okay.

Speaker 2:          35:29          So,

Speaker 3:          35:30          okay,

Speaker 2:          35:31          what a robot is doing things to kind of have a few notions of safety to worry about. One is that through about as physically strong and of course could do a lot of damage. Um, same for cars, which we can think of as robots to in some way.

Speaker 2:          35:46          And this could be completely unintentional, so it could be not the kind of longterm AI safety concerns that, okay, AI is smarter than us and now what do we do? But it could be just very practical. Okay. This robot, if it makes a mistake, whether the results going to be, of course simulation comes in a lot there too. Test testing, simulation. It's a difficult question and I'm always wondering like, oh, there's one there. Let's say you look at it, let's go back to driving because a lot of people know driving well of course. Well what do we do to test somebody for driving right to, to get a driver's license? What do they really do? I mean you fill out some tests and then you'd drive and I mean we're a few minutes of Bourbon California. That driving test is just, you drive around the block, pull over you to do a stop sign successfully and then you know, you've pulled over again and you pretty much done and you're like, okay, if a self driving car did die, would you trust it that it can drive?

Speaker 2:          36:46          And I'd be like, no, that's not enough for me to trust it. But somehow for humans we've figured out that somebody being able to do that, it's representative of them being able to do a lot of other things. And so I think somehow for humans we figured out representative tests of what it means. If you can do this where you can really do of course testing, you must, you must don't want to be tested at all times. Self driving cars or robots can be tested more often. Probably you can have replicates that get tested are known to be identical because they use the same neural net and so forth. But still I feel like we don't have this kind of unit tests or proper tests for for robots. And I think there's something very interesting to be thought about there. Especially as you update things, your software improves, you have a better self driving cars, you update it. How do you know it's indeed more capable on everything than what you had before that you didn't have any bad things creep into it. So I think that's a very interesting direction of research that there is no real solution yet except that somehow for us we do, cause we say, okay you have a driving test you passed, you can go on the road now and you must have accents that every like million or 10 million miles, some pretty phenomenal compared to that short test that is being done.

Speaker 1:          38:01          So let me ask a, you've mentioned, you've mentioned that Andrew Ang by example showed you the value of kindness and Tim, do you think the space of a policies, good policies for humans and for AI is populated by policies that with kindness or ones that are the opposite, exploitation, even evil. So if you just look at the Sea of policies we operate under as human beings or if AI system had to operate in this real world, do you think it's really easy to find policies that are full of kindness like when naturally fall into them or is it like a very hard optimization problem?

Speaker 2:          38:48          I mean there is kind of to optimizations happening for humans, right? So for you, most just kind of the very long term optimization, which ever evolution has done for us, and we're kind of predisposed to like certain things. And that's something that's what makes our learning easier. Because I mean, we know things like pain and, uh, hunger and thirst and the fact that we know about those as not something that we were taught that's kind of innate when we're hungry, we're unhappy when we're thirsty, we're unhappy, won't have Penn, were unhappy. And ultimately evolution built that into us to think about this thing. So I think there is a notion that it seems somehow humans evolved in general to prefer to get along and in some ways, but at the same time also to be very territorial and can of centric to their own tribe. Is it like, it seems like that's the kind of space we conversed onto it. I mean, I'm not an expert in anthropology, but it seems like we're very kind of good within our own tribe, but need to be taught to be nice to other tribes.

Speaker 1:          39:54          Well, if you look at Steven Pinker, he highlights is pretty nicely. And, uh, uh, better, better angels of our nature where he talks about violence decreasing over time consistently. So whatever tension, whatever teams we pick, it seems that the long arc of history goes towards us getting along more and more. So. I hope so. Uh, so do you think that, do you think is possible to cheat, teach, Rl base robots, the, this kind of kindness, this kind of ability to interact with humans, this kind of policy even to, let me ask, let me ask a fun one. Do you think it's possible teach RL base robot to love a human being and to inspire that human to love the robot back so to like, uh, RL based a algorithm that leads to a happy marriage?

Speaker 3:          40:47          Okay,

Speaker 2:          40:47          that's an interesting question. Maybe I'll, I'll, I'll, I'll answer it with, with another question, right? Because I mean it's, but I'll come back to it. So another question I can have is, okay. I mean, how, how close to some people's happiness get from interacting with just a really nice,

Speaker 3:          41:06          okay

Speaker 2:          41:07          dog. I mean, dogs, you come home, that's what dogs do. They greet you, they're excited, makes you happy when you come home to your door. It just like, okay, this is exciting. They're always happy when I'm here and if they don't greet you because maybe whatever your partner took them on a trip or something, you, you might not be nearly as happy when you get home. Right. And so the kind of, it seems like the level of reasoning a dog has is, is pretty sophisticated, but then it's still not yet at the level of, of human reasoning. And so it seems like we don't even need to achieve human level reasoning to get like very strong affection with humans. And so my thinking is why not? Right? Why? Why couldn't with an AI, couldn't we achieve the kind of level of affection that humans feel among each other or with friendly animals and so forth? It's a question, is it a good thing for us or not that, that's another thing, right? Because I mean, but I don't see why not. Why not? Um, so Eli Musk says love is the answer. Maybe he should say love is the objective function. And then RL is the answer. Maybe. Peter, thank you so much. I don't want to take up more of your time. Thank you so much for talking today. Well, likewise. Thanks for coming. Bye. Great to have you visit.