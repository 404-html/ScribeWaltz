WEBVTT

1
00:00:00.060 --> 00:00:03.780
Hello world,
it's Saroj and have you ever bought company stocks before?

2
00:00:03.930 --> 00:00:07.320
We're going to train three different machine learning models on stock data.

3
00:00:07.510 --> 00:00:12.510
Then compare their results to see which one has the best predictive ability for

4
00:00:12.541 --> 00:00:17.040
future prices.
Think about the process of creating an investment strategy.

5
00:00:17.160 --> 00:00:20.190
Once you decided that you do want to make an investment,

6
00:00:20.400 --> 00:00:24.300
you need to figure out which companies out there are most likely to give you big

7
00:00:24.301 --> 00:00:25.620
returns.
Generally,

8
00:00:25.621 --> 00:00:28.920
you'd start by doing some research on the company's history and team.

9
00:00:29.100 --> 00:00:33.450
You'd read past news articles on how the company has fared over the years.

10
00:00:33.660 --> 00:00:37.170
You'd look at how stock prices have changed over time.

11
00:00:37.320 --> 00:00:41.310
Maybe observe what others are saying about the future of the company on Twitter,

12
00:00:41.520 --> 00:00:45.930
and finally you'd collect all of this data that you've gathered in your head to

13
00:00:45.931 --> 00:00:48.000
make a prediction about a future price.

14
00:00:48.360 --> 00:00:50.940
If that's not the perfect use case for machine learning,

15
00:00:50.970 --> 00:00:53.520
learning from past data points to predict future ones.

16
00:00:53.730 --> 00:00:57.510
I don't know what is in a very recent paper from Auburn on this topic.

17
00:00:57.540 --> 00:01:01.230
A small group concluded that using data from several sources like Google trends,

18
00:01:01.380 --> 00:01:05.400
Wikipedia and Google correlate resulted in a model capable of assisting in

19
00:01:05.401 --> 00:01:09.570
investment decisions and had a relatively high accuracy for movement prediction,

20
00:01:09.870 --> 00:01:14.520
so we know academics are researching this and we know big banks are definitely

21
00:01:14.521 --> 00:01:15.660
doing this after all,

22
00:01:15.661 --> 00:01:19.980
quantitative analysts have it in their job description to apply methods to risk

23
00:01:19.981 --> 00:01:22.890
management problems,
but is anyone else?
Well,

24
00:01:22.891 --> 00:01:24.960
the Quantopian community is pretty cool.

25
00:01:25.110 --> 00:01:29.460
It's essentially a crowdsource hedge fund that encourages freelance data

26
00:01:29.461 --> 00:01:31.860
scientists to develop models for stock trading.

27
00:01:32.160 --> 00:01:36.000
They've got a bunch of educational resources on their website and data sets

28
00:01:36.001 --> 00:01:39.930
available for training as well.
The idea is that once you sign up,

29
00:01:39.931 --> 00:01:44.760
you can write an algorithm in python using their ide.
Then you have two options.

30
00:01:44.880 --> 00:01:48.450
The first option is submitting it to one of their algorithm contests.

31
00:01:48.660 --> 00:01:52.560
If you can make better predictions than the competitors,
you win a cash prize.

32
00:01:52.770 --> 00:01:56.580
The other is submitting it for an evaluation to see if you can get your model

33
00:01:56.581 --> 00:01:58.410
licensed.
If it's licensed,

34
00:01:58.560 --> 00:02:02.640
then you'll receive a small allocation of funds that your model can use to trade

35
00:02:02.641 --> 00:02:06.390
stocks and you'll get 10% of the net profit it gains.

36
00:02:06.600 --> 00:02:10.800
It's a crowdsourced hedge fund and they built a pretty active community that

37
00:02:10.801 --> 00:02:13.080
discusses different algorithmic strategies.

38
00:02:13.110 --> 00:02:16.440
Quantopian isn't the only one of these DIY hedge funds to pop up.

39
00:02:16.470 --> 00:02:19.830
There's also numerous [inaudible] which allows algorithm contributors to remain

40
00:02:19.831 --> 00:02:22.740
anonymous and prove themselves through weekly tournaments.

41
00:02:23.220 --> 00:02:26.910
It pays in Bitcoin as well.
When it comes to the type of model we could use,

42
00:02:27.000 --> 00:02:31.740
we have a huge assortment to try from reading results from papers is a good

43
00:02:31.741 --> 00:02:35.910
start to see what's been tried before.
Random forests,
linear regressions,

44
00:02:35.911 --> 00:02:36.930
deep neural networks,

45
00:02:36.931 --> 00:02:40.050
and it's not just about using pattern recognition algorithms.

46
00:02:40.230 --> 00:02:44.820
There's also a space for reinforcement learning where a model can learn from its

47
00:02:45.090 --> 00:02:49.530
past predictive ability giving itself a reward only for good behavior,

48
00:02:49.680 --> 00:02:51.750
essentially self improving over time.

49
00:02:51.960 --> 00:02:55.080
So between all the individual different types of models,

50
00:02:55.081 --> 00:02:59.260
we could use the different combinations of them,
all the different data points.

51
00:02:59.261 --> 00:03:03.670
We could feed it both numerical,
textual,
video,
audio based,

52
00:03:03.880 --> 00:03:06.430
and the different time intervals.
We could trade in.

53
00:03:06.700 --> 00:03:09.610
There are truly endless possibilities for us.

54
00:03:09.850 --> 00:03:13.450
The fact that it's such a multivariate problem is the very reason that

55
00:03:13.451 --> 00:03:16.150
investment advisors have been around since the beginning.

56
00:03:16.360 --> 00:03:20.590
We trust their insight that they've collected enough data to make valuable

57
00:03:20.591 --> 00:03:25.591
predictions but better than any human is of course a well tuned machine learning

58
00:03:25.811 --> 00:03:26.401
algorithms,

59
00:03:26.401 --> 00:03:29.890
so let's use the sentiment from news headlines and historical price charts

60
00:03:29.891 --> 00:03:32.680
together to predict future prices in python.

61
00:03:33.100 --> 00:03:35.770
We'll use to machine learning libraries for our problem.

62
00:03:35.920 --> 00:03:39.430
The first is called NLTK or natural language toolkit,

63
00:03:39.640 --> 00:03:43.930
which will help us perform a wide range of text processing options like

64
00:03:43.931 --> 00:03:48.760
classification,
tagging and tokenization.
The second is called psychic learn,

65
00:03:48.880 --> 00:03:51.160
which we'll use for the actual machine learning.

66
00:03:51.580 --> 00:03:55.750
The data set we're using is the adjusted closing price gathered from the past 10

67
00:03:55.751 --> 00:03:58.240
years for Microsoft stock.

68
00:03:58.420 --> 00:04:02.770
We'll use eight of those years for training and two of them for testing as well

69
00:04:02.771 --> 00:04:07.060
as a Dataset of New York Times article headlines about Microsoft for sentiment

70
00:04:07.061 --> 00:04:07.894
analysis.

71
00:04:08.140 --> 00:04:12.280
We can use the pandas data processing tool to combine both data sets into one

72
00:04:12.281 --> 00:04:16.270
data frame that we can then manipulate where the features are,
the date,

73
00:04:16.420 --> 00:04:20.230
the closing price,
and the article headline for that day.
If there is one,

74
00:04:20.530 --> 00:04:23.980
well next one to perform sentiment analysis on these headlines,

75
00:04:24.100 --> 00:04:28.300
so we'll use the sentiment intensity analyzer from nltk to do this.

76
00:04:28.570 --> 00:04:32.350
This will output sentiment scores for four classes of sentiment,
negative,

77
00:04:32.351 --> 00:04:36.070
neutral,
positive and compound,
which is the aggregated score.

78
00:04:36.370 --> 00:04:40.630
The way it does this is by using a lexicon of predefined word sentiments.

79
00:04:40.840 --> 00:04:44.050
There are actually several popular sentiment lexicons out there.

80
00:04:44.290 --> 00:04:48.400
They are annotated manually by humans and pattern recognition algorithms.

81
00:04:48.580 --> 00:04:52.030
Use them to summarize the polarities of entire documents.

82
00:04:52.090 --> 00:04:54.460
We'll add rows for each of the sentiment classes.

83
00:04:54.520 --> 00:04:58.120
Then fill those rows after computing the polarity of each headline.

84
00:04:58.420 --> 00:05:00.130
Now that we've got our features,

85
00:05:00.131 --> 00:05:03.400
we can try out some different models to predict our objective variable,

86
00:05:03.430 --> 00:05:07.390
the price for a given day.
Well,
first try out a random forest model.

87
00:05:07.420 --> 00:05:09.280
This is a collection of decision trees,

88
00:05:09.580 --> 00:05:13.480
a model where an input query is entered at the top and as it traverses down the

89
00:05:13.481 --> 00:05:14.140
tree,

90
00:05:14.140 --> 00:05:18.160
the data gets bucketed into smaller and smaller sets at your passing through a

91
00:05:18.161 --> 00:05:21.430
series of thresholds for some number of trees,
tea,

92
00:05:21.580 --> 00:05:26.080
we sample end cases at random with a replacement to create a subset of the data

93
00:05:26.320 --> 00:05:27.160
at each node.

94
00:05:27.160 --> 00:05:31.720
M predictor variables are selected at random from all the predictor variables.

95
00:05:31.960 --> 00:05:36.280
The one that gives the best split according to the objective function is used to

96
00:05:36.281 --> 00:05:38.380
do a binary split on that node.

97
00:05:38.590 --> 00:05:43.000
And then at the next node we choose another m variables at random from all

98
00:05:43.001 --> 00:05:45.040
predictor variables and do the same.

99
00:05:45.310 --> 00:05:47.710
The second model we'll use is linear regression,

100
00:05:47.740 --> 00:05:51.400
which will draw the line of best fit between our variables.

101
00:05:51.670 --> 00:05:55.150
And the third model we'll use is a multilayer perceptron,

102
00:05:55.210 --> 00:05:56.620
also called a neural network.

103
00:05:56.930 --> 00:06:01.130
This is a series of matrix operations apply to input data capable of learning

104
00:06:01.131 --> 00:06:05.840
both linear and nonlinear functions due to activation functions that are applied

105
00:06:05.841 --> 00:06:08.120
at each layer.
Until we receive an output.

106
00:06:08.121 --> 00:06:12.800
Prediction will input our data into all three initialized models and observe the

107
00:06:12.801 --> 00:06:14.840
results in graphs for each of them.

108
00:06:14.990 --> 00:06:17.030
They'll random forest model doesn't look too nice.

109
00:06:17.060 --> 00:06:20.570
It's pretty off the linear regression model.
It looks a little bit better,

110
00:06:20.571 --> 00:06:21.650
but it's still pretty bad,

111
00:06:21.651 --> 00:06:24.980
and the MLP classifier looks to be the best of all of them.

112
00:06:24.981 --> 00:06:27.860
So let's go with that one.
To recap what we've learned,

113
00:06:27.890 --> 00:06:32.270
stock prices can be predicted by combining several data points from across the

114
00:06:32.271 --> 00:06:32.900
web.

115
00:06:32.900 --> 00:06:37.900
We can use the sentiment of the public company history and past prices as key

116
00:06:38.001 --> 00:06:39.620
data points in this process.

117
00:06:39.830 --> 00:06:43.010
And by using the Nlt K and psyche learning library,

118
00:06:43.100 --> 00:06:46.070
anyone can do this if they've tuned their algorithm properly.

119
00:06:46.430 --> 00:06:50.090
This week's coding challenge is to create a stock price prediction algorithm

120
00:06:50.270 --> 00:06:52.700
using at least four different data sources.

121
00:06:52.880 --> 00:06:56.300
You can find details in the github read me and winners will be announced in one

122
00:06:56.301 --> 00:06:59.210
week.
Please subscribe for more programming videos.
And for now,

123
00:06:59.270 --> 00:07:02.390
I've got to invest in myself,
so thanks for watching.

