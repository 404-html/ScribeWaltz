WEBVTT

1
00:00:00.210 --> 00:00:03.990
If deep mind hired me,
I just reveal their secrets.

2
00:00:03.991 --> 00:00:05.760
So hello world,

3
00:00:05.761 --> 00:00:10.761
it's so Raj and deep mind just dropped a very impressive paper called neural

4
00:00:10.981 --> 00:00:13.320
seen representations and rendering.

5
00:00:13.680 --> 00:00:18.680
Their AI is capable of rendering an entire three d environment from just one or

6
00:00:19.051 --> 00:00:23.760
a few input images and to make it even more impressive it learn how to do this

7
00:00:23.910 --> 00:00:25.080
without any labels.

8
00:00:25.230 --> 00:00:29.790
It just learned from data eight obtained itself from exploring different three d

9
00:00:29.791 --> 00:00:30.624
environments.

10
00:00:30.870 --> 00:00:35.490
I'll explain how it works in this video since there are a lot of applications

11
00:00:35.491 --> 00:00:37.860
for this technology.
For example,

12
00:00:37.861 --> 00:00:42.630
companies like Google and GM are spending billions of dollars on research and

13
00:00:42.631 --> 00:00:44.520
development for self driving cars.

14
00:00:44.550 --> 00:00:48.870
If a self driving car incorporated this technology into it stack,

15
00:00:49.110 --> 00:00:52.860
it would give it yet another signal of what to expect on the road.

16
00:00:53.220 --> 00:00:56.700
After training on millions of hours of Dash Cam footage,

17
00:00:56.910 --> 00:01:01.910
it could create a reliable three d map of what's likely to come up on its path.

18
00:01:02.280 --> 00:01:06.900
Adding to is predictive decision making capability and thus further reducing the

19
00:01:06.901 --> 00:01:09.060
risk of an accident.
Also,

20
00:01:09.061 --> 00:01:11.970
creating a quality game is no simple task.

21
00:01:12.210 --> 00:01:17.210
Often it requires many hours of practice with a whole host of tools to begin the

22
00:01:17.251 --> 00:01:21.120
process of creating an intricately detailed three d world.

23
00:01:21.390 --> 00:01:26.190
This technology could allow anyone to generate a three d game world from a

24
00:01:26.191 --> 00:01:29.250
simple two d drawing or even a real life photo,

25
00:01:29.580 --> 00:01:32.610
letting them iterate on their ideas much faster.

26
00:01:33.000 --> 00:01:37.380
And these three worlds could be used not just for gaming but for virtual and

27
00:01:37.410 --> 00:01:38.610
augmented reality.

28
00:01:38.970 --> 00:01:43.920
The Ar versions could let engineer's design different versions of a component

29
00:01:43.950 --> 00:01:46.800
much faster acting as a design tool.

30
00:01:46.830 --> 00:01:49.890
So the researchers had a straightforward goal,

31
00:01:49.980 --> 00:01:53.640
create an AI that can understand any given scene.

32
00:01:54.060 --> 00:01:57.180
If you or I were placed into any given scene,

33
00:01:57.181 --> 00:02:00.210
be that a Savanna in Africa or Savannah,
Georgia,

34
00:02:00.540 --> 00:02:04.590
we would immediately perceive and interpret everything around us.

35
00:02:04.890 --> 00:02:07.950
We would observe where the nearest Wifi was,

36
00:02:08.130 --> 00:02:12.240
whether it was day or night based on where the sun was positioned,

37
00:02:12.450 --> 00:02:14.310
what type of animals surrounded us.

38
00:02:14.311 --> 00:02:18.330
Like we're Fiki where the nearest village might be based off of footprints.

39
00:02:18.600 --> 00:02:23.550
Our brain with Lauren to form representations of the environment that would

40
00:02:23.551 --> 00:02:27.330
support not only classifications of what we see,

41
00:02:27.600 --> 00:02:30.420
but also motor control,
memory planning,

42
00:02:30.450 --> 00:02:35.430
imagination and rapid skill acquisition all related to the environment.

43
00:02:35.880 --> 00:02:38.640
All of this,
without any teacher telling us these things,

44
00:02:38.760 --> 00:02:41.790
we would do it ourselves.
David Marr,

45
00:02:41.820 --> 00:02:46.820
one of the originators of the field of computational neuroscience suggested in

46
00:02:46.891 --> 00:02:51.891
his influential book vision that there's likely a generative process in the

47
00:02:51.901 --> 00:02:54.810
brain that gives us this incredible ability,

48
00:02:55.050 --> 00:02:59.100
one that doesn't require supervision.
In contrast,

49
00:02:59.110 --> 00:03:04.110
the current most popular computer vision technique is to use deep convolutional

50
00:03:04.630 --> 00:03:09.630
neural networks on big labeled image data sets to learn how to interpret images.

51
00:03:10.840 --> 00:03:15.840
This label centric or supervised approach requires an intensive process of

52
00:03:16.091 --> 00:03:18.430
humans manually labeling images.

53
00:03:18.730 --> 00:03:22.720
Then having a neural network learn the mapping between the input data and the

54
00:03:22.721 --> 00:03:27.160
output label.
Only then can it classify what it sees in an image,

55
00:03:27.340 --> 00:03:28.420
but even then,

56
00:03:28.450 --> 00:03:33.450
this approach doesn't give the AI the ability to really discern what it's seeing

57
00:03:33.671 --> 00:03:37.630
and in what ways different objects in a scene relate to each other.

58
00:03:37.930 --> 00:03:42.370
So the researchers decided to create an AI that internally represented the

59
00:03:42.371 --> 00:03:42.970
environment.

60
00:03:42.970 --> 00:03:47.970
It was placed in more like we would minus our constant existential crises.

61
00:03:48.430 --> 00:03:51.820
The idea then is to give this AI some input data,

62
00:03:51.880 --> 00:03:55.270
ideally two d images of a scene that it's placed in.

63
00:03:55.540 --> 00:04:00.540
Essentially what gets cs along with its position in that scene and use that data

64
00:04:00.941 --> 00:04:03.730
without any kind of label as the training Dataset.

65
00:04:03.970 --> 00:04:08.560
Luckily they had an open source game engine ready called deep mind lab that they

66
00:04:08.561 --> 00:04:11.320
made that can be used as this training dataset.

67
00:04:11.620 --> 00:04:15.670
It's a collection of three d environments that can be used to train an AI agent

68
00:04:15.700 --> 00:04:20.500
in including a simple square room that you can easily very floor and wall

69
00:04:20.501 --> 00:04:24.370
textures of in a more complex,
randomly generated maze,

70
00:04:24.640 --> 00:04:28.810
but it seems like a simple seven by seven square room would be a good starting

71
00:04:28.811 --> 00:04:30.790
point for training.
This AI,

72
00:04:31.090 --> 00:04:36.070
they call their AI the generative query network or Gq n

73
00:04:37.810 --> 00:04:42.130
it consists of two parts,
a representation network and a generation network.

74
00:04:42.430 --> 00:04:47.200
The representation network takes as input what the agent observes inside of the

75
00:04:47.201 --> 00:04:50.590
Three d environment,
essentially a two d image frame.

76
00:04:51.010 --> 00:04:55.630
The representation network then outputs a representation of that input.

77
00:04:55.960 --> 00:04:58.210
The idea is that this representation,

78
00:04:58.211 --> 00:05:01.510
we'll capture the most important elements of the scene,

79
00:05:01.540 --> 00:05:04.090
like the position of objects,
colors,

80
00:05:04.091 --> 00:05:06.880
and the room layout in a compressed way.

81
00:05:07.270 --> 00:05:11.200
It will learn to detect those features.
They aren't hand coded.

82
00:05:11.530 --> 00:05:16.530
Then the generation network will be asked to predict Aka imagine a scene given

83
00:05:17.141 --> 00:05:22.141
both a previously unobserved viewpoint and the seed representation created by

84
00:05:22.661 --> 00:05:23.530
the first network.

85
00:05:23.950 --> 00:05:28.450
The generator essentially learns how to fill in the details given the highly

86
00:05:28.451 --> 00:05:33.451
compressed abstract representation created by the first network in furry likely

87
00:05:34.271 --> 00:05:38.530
relationships between objects and regularities in the environment.

88
00:05:38.920 --> 00:05:43.210
I'd like in the relationship between these two networks to the relationship

89
00:05:43.211 --> 00:05:46.600
between a crime scene witness and a sketch artist.

90
00:05:46.870 --> 00:05:51.760
The witness remembers fragments of a criminal,
their height,
their hair color,

91
00:05:51.790 --> 00:05:54.340
their choice of Linux distro,
and the sketch.

92
00:05:54.341 --> 00:05:58.970
Artists must discern the full picture of the criminal case on a few details,

93
00:05:59.150 --> 00:06:03.440
inferring the likely other traits based on what they're given by the witness.

94
00:06:03.470 --> 00:06:05.300
Put more formally.
First,

95
00:06:05.330 --> 00:06:09.590
the algorithm collects a set of different viewpoints from the training scene.

96
00:06:09.950 --> 00:06:14.660
Each viewpoint is an image each fed sequentially into the representation

97
00:06:14.661 --> 00:06:15.470
network,

98
00:06:15.470 --> 00:06:20.060
which is a convolutional neural network that's known for image classification

99
00:06:20.061 --> 00:06:20.894
tasks.

100
00:06:21.320 --> 00:06:26.320
An image is a matrix of numbers and through a series of matrix operations,

101
00:06:26.540 --> 00:06:30.890
a convolutional network will continually modify that input matrix.

102
00:06:31.310 --> 00:06:33.470
The result is the representation.

103
00:06:33.530 --> 00:06:37.670
It will create as many representations as there are viewpoints.

104
00:06:38.030 --> 00:06:43.030
Then they performed a summation operation on the representations to create a

105
00:06:43.221 --> 00:06:47.900
single representation or are is then fed to the generation network.

106
00:06:48.230 --> 00:06:51.860
For the generation network.
They used a recurrent neural network.

107
00:06:52.010 --> 00:06:56.360
Since they are capable of processing sequences of data during training,

108
00:06:56.361 --> 00:07:01.070
recurrent networks aren't just continuously fed the next data point in a

109
00:07:01.071 --> 00:07:01.904
dataset.

110
00:07:02.090 --> 00:07:06.320
They are also fed bill learned state from the previous time step,

111
00:07:06.590 --> 00:07:09.710
which is what gives them recurrent knowledge of the past.

112
00:07:10.160 --> 00:07:14.750
They learn from what they've learned before allowing for a contextual

113
00:07:14.751 --> 00:07:19.751
understanding that incorporates time into their predictions since they wanted an

114
00:07:20.181 --> 00:07:24.620
agent that could predict the next frame in a sequence of three d environment

115
00:07:24.630 --> 00:07:25.463
frames.

116
00:07:25.550 --> 00:07:30.530
They needed to use a sequence model and the generator network used what's called

117
00:07:30.531 --> 00:07:35.180
a latent variable to mathematically very the outputs a bit.

118
00:07:35.630 --> 00:07:40.130
The generator then generated a likely image for a given viewpoint and that

119
00:07:40.131 --> 00:07:44.870
generated image was compared to the actual viewpoints on error.

120
00:07:44.871 --> 00:07:49.250
Value was computed by computing the difference in these two images

121
00:07:49.340 --> 00:07:50.210
mathematically.

122
00:07:50.690 --> 00:07:55.690
Then they updated both networks using that error to be just a bit more accurate.

123
00:07:56.170 --> 00:08:00.530
The next training iteration via the popular backpropagation technique,

124
00:08:00.950 --> 00:08:02.900
updating the weight values of each.

125
00:08:03.320 --> 00:08:07.970
This optimization strategy meant both the representation network and the

126
00:08:07.971 --> 00:08:10.880
generation network were improved over time.

127
00:08:10.910 --> 00:08:15.860
At the same time as the agent navigated whatever environment it was in.

128
00:08:16.190 --> 00:08:18.920
Making this an end to end approach.

129
00:08:19.040 --> 00:08:23.210
The first trained it on a few simple seven by seven square maps with a few

130
00:08:23.211 --> 00:08:25.370
objects in them over time.

131
00:08:25.371 --> 00:08:29.090
It rapidly learn to predict what an entire map look like,

132
00:08:29.510 --> 00:08:33.980
so they gave it a more complex maze instead and over time it learned how to

133
00:08:33.981 --> 00:08:35.870
represent that as well.

134
00:08:36.050 --> 00:08:39.590
At first it was a bit uncertain of some parts of the map,

135
00:08:39.620 --> 00:08:43.790
but with more observations and buy more.
I made only five total.

136
00:08:44.030 --> 00:08:47.180
It's uncertainty disappeared almost entirely.

137
00:08:47.600 --> 00:08:52.190
Eventually they wanted to use it to control a robotic arm to grab a colored

138
00:08:52.220 --> 00:08:56.970
object in a simulated environment because Yolo,
no,
not the algorithm.

139
00:08:57.630 --> 00:09:02.370
Deep reinforcement learning is a combination of deep learning,
Aka learning,

140
00:09:02.400 --> 00:09:04.860
a mapping and reinforcement learning,

141
00:09:04.890 --> 00:09:08.220
Aka learning from trial and error in an environment.

142
00:09:08.580 --> 00:09:13.470
It's been behind some of the big AI successes of the past few years like Alphago

143
00:09:13.680 --> 00:09:15.450
and Ataris DQ learner.

144
00:09:15.870 --> 00:09:19.860
The idea is that the AI agent learns of policy for playing a game.

145
00:09:20.070 --> 00:09:23.100
My learning directly from pixels from the game frames,

146
00:09:23.400 --> 00:09:27.990
no hints as to what the objective of the game is,
what the controls meet.

147
00:09:28.440 --> 00:09:33.440
The problem with this approach is that it requires a very long training time to

148
00:09:33.451 --> 00:09:34.860
converge to good results.

149
00:09:35.190 --> 00:09:40.170
So they conducted an experiment where they first trained the GQ N to learn how

150
00:09:40.171 --> 00:09:43.170
to represent observations of the environment.

151
00:09:43.560 --> 00:09:46.620
Then they use it's learned representations,

152
00:09:46.740 --> 00:09:51.180
has input to a policy out rhythm that learned how to control the arm.

153
00:09:51.570 --> 00:09:55.020
The representation encapsulate what the AI saw,

154
00:09:55.200 --> 00:09:59.040
the arms joint angles,
the position and the color of the object,

155
00:09:59.190 --> 00:10:04.190
the colors of the walls in a much more compressed way than just using the raw

156
00:10:04.320 --> 00:10:05.370
input pixels.

157
00:10:05.700 --> 00:10:09.810
And because of this they saw that it was substantially more data efficient,

158
00:10:10.050 --> 00:10:14.520
requiring only a quarter of the training time that a raw pixel virgin would

159
00:10:14.521 --> 00:10:17.130
require.
Very impressive indeed.

160
00:10:17.190 --> 00:10:22.190
Gq N is exciting because a major limiting factor on what it can do is computing

161
00:10:22.591 --> 00:10:25.110
power.
If given enough computing power,

162
00:10:25.200 --> 00:10:29.370
who knows what kind of amazingly detailed environments it could generate.

163
00:10:29.400 --> 00:10:33.780
And this is exciting for anybody,
designers,
artists,
engineers,

164
00:10:33.900 --> 00:10:38.730
scientists who could use a tool to help them visualize and create things.

165
00:10:39.120 --> 00:10:40.980
Three things to remember from this video.

166
00:10:41.400 --> 00:10:46.400
Deep minds generative query network learns how to perceive and interpret an

167
00:10:46.591 --> 00:10:51.510
environment without labels.
It consists of a representation network,

168
00:10:51.511 --> 00:10:56.511
which encodes image frames and a generation network that generates them based on

169
00:10:57.001 --> 00:11:00.120
those representations.
And it did surprisingly well,

170
00:11:00.121 --> 00:11:04.800
requiring only a fourth of the training time for a deep reinforcement learning

171
00:11:04.801 --> 00:11:08.610
task that a raw pixel focused algorithm would require.

172
00:11:08.850 --> 00:11:12.680
Ai Is never boring.
If you want to stay up to date on the field,
answer,

173
00:11:12.681 --> 00:11:15.990
survived the AI,
apocalypse it the subscribe button for now.

174
00:11:16.020 --> 00:11:18.900
I've got to keep reading papers,
so thanks for watching.

