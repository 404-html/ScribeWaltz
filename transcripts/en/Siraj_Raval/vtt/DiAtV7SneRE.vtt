WEBVTT

1
00:00:00.090 --> 00:00:03.240
Hello world.
It's the Raj and dynamic programming.

2
00:00:03.270 --> 00:00:06.570
It's one of the most important concepts in computer science.

3
00:00:06.690 --> 00:00:10.710
It's used all over the place in artificial intelligence.
It's used in security,

4
00:00:10.830 --> 00:00:14.520
it's used in distributed systems,
it's used everywhere.
And in this video,

5
00:00:14.521 --> 00:00:18.030
I'm going to teach you how it works by going through several examples.

6
00:00:18.180 --> 00:00:21.630
But before I go through those examples,
before I go over the abstracts,

7
00:00:21.631 --> 00:00:25.290
before I go over the theory,
let's look at this demo I've got here.

8
00:00:25.291 --> 00:00:26.280
And what this demo is,

9
00:00:26.281 --> 00:00:30.240
is it's a visual way for you to see dynamic programming in action.

10
00:00:30.450 --> 00:00:35.190
What it's doing is it's aligning DNA sequences into this matrix and it's a
table,

11
00:00:35.400 --> 00:00:37.590
right?
With rose and with columns.

12
00:00:37.860 --> 00:00:41.640
And what we can do is we can type in a sequence like a,
let's say,

13
00:00:41.700 --> 00:00:44.820
let's say sequence one is going to be a c,
G,

14
00:00:44.821 --> 00:00:48.900
and they're all some variant of HCG GCA,
right?
Those are the three letters.

15
00:00:49.350 --> 00:00:54.300
And then we say,
hey,
GC,
AGC,
and the for sequence too,
we could say,
you know,
Gac,
g,

16
00:00:54.330 --> 00:00:57.540
a,
C,
a,
c,
and so forth,
so on and so forth.

17
00:00:57.541 --> 00:01:00.660
So notice how as I'm constructing these sequences,

18
00:01:00.780 --> 00:01:03.990
the table and the values in that table are getting filled in.

19
00:01:04.260 --> 00:01:07.380
What it's using is what's what's called a top down approach.

20
00:01:07.410 --> 00:01:10.890
There's two types of techniques we can use and dynamic programming,

21
00:01:11.130 --> 00:01:13.950
a top down approach,
and a bottom up approach.

22
00:01:14.040 --> 00:01:15.810
And we're going to talk about each of these in detail.

23
00:01:15.960 --> 00:01:18.090
So what I wanted to do was just kind of show you this,

24
00:01:18.270 --> 00:01:20.880
this a visual demo before we go into the code,

25
00:01:21.090 --> 00:01:24.450
you can find the get hub link in the description of this video if you want to

26
00:01:24.451 --> 00:01:25.580
look at the code.
But,
uh,

27
00:01:25.860 --> 00:01:28.710
it's a good way to visually see what's happening here and it's all written in

28
00:01:28.750 --> 00:01:32.880
javascript.
Okay,
so what is dynamic programming?
Right?
What is this?
Well,

29
00:01:32.881 --> 00:01:35.190
there's this really famous quote by,
I don't know who,

30
00:01:35.191 --> 00:01:40.170
it's not by de dynamic programming the concept because concepts can't speak yet.

31
00:01:40.620 --> 00:01:43.920
Those who cannot remember the past are condemned to repeat it.

32
00:01:43.950 --> 00:01:48.180
That's some sage advice for you and for me as well.
Geez.
Oh my God.
Anyway,

33
00:01:49.470 --> 00:01:54.060
anyway,
so bellman bellman was the dude.
Bellmen made the bellman equation,

34
00:01:54.061 --> 00:01:57.330
which is used all over the place in reinforcement learning.

35
00:01:57.510 --> 00:02:00.840
It's used for the Mark Haub decision processes for responsible for some of the

36
00:02:00.841 --> 00:02:04.020
famous winds off ago.
For example,
in Ai last year,

37
00:02:04.290 --> 00:02:08.010
but he had this very famous quote in the 50s when he created the concept of

38
00:02:08.011 --> 00:02:10.770
dynamic programming,
and I'll quote him,
he said,

39
00:02:11.010 --> 00:02:15.450
dynamic programming amounts to breaking down an optimization problem,
Ding,

40
00:02:15.480 --> 00:02:20.480
ding AI into simpler sub problems and storing the solution to each sub problem

41
00:02:20.760 --> 00:02:24.360
so that each sub problem is only solved once,
right?

42
00:02:24.450 --> 00:02:28.620
So it's taking a big optimization problem.
What is an optimization problem?

43
00:02:28.740 --> 00:02:33.300
It's a problem that looks to minimize or maximize some value iteratively,
right?

44
00:02:33.500 --> 00:02:36.510
All,
all of machine learning is an optimization problem,
right?

45
00:02:36.540 --> 00:02:39.630
All of machine learning is trying to optimize to fit a curve.

46
00:02:39.720 --> 00:02:44.340
It's glorified curve fitting to some data in dynamic programming is taking that

47
00:02:44.341 --> 00:02:48.540
optimization problem and breaking it down into simpler sub problems.
Okay,

48
00:02:48.660 --> 00:02:51.330
so that each sub problem is only solved once.

49
00:02:51.510 --> 00:02:53.730
Once we solve all of those sub problems,

50
00:02:53.850 --> 00:02:57.990
we can concatenate them or put them together to find the optimal solution.

51
00:02:57.991 --> 00:03:02.320
For the larger initial problem.
So what I've got here is this funny little,
uh,

52
00:03:03.000 --> 00:03:06.310
uh,
it's like an XKCD,
but it's not.
It's just a clip.

53
00:03:06.311 --> 00:03:09.570
But what I'm gonna do is I'm going to just animate the dialogue here too,

54
00:03:09.880 --> 00:03:11.290
just to give you some,
you know,

55
00:03:11.320 --> 00:03:13.240
a little bit of fun before we get into the harder stuff.

56
00:03:13.600 --> 00:03:18.460
So there's a woman in a man,
okay,
so here's how it goes.
Oh,
what to wear,

57
00:03:18.520 --> 00:03:21.040
what to wear.
It's such a vast wardrobe.

58
00:03:21.430 --> 00:03:26.110
How will I ever find the right combination?
Perhaps I can help.
Who are you?

59
00:03:26.410 --> 00:03:28.240
I'm here to tell you about dynamics,

60
00:03:28.390 --> 00:03:32.920
the new selection optimization software from algorithmic anomaly.

61
00:03:33.130 --> 00:03:34.720
How does it work?
Well,

62
00:03:34.721 --> 00:03:39.460
we use dynamic programming to recursively create an outfit that you'll look just

63
00:03:39.461 --> 00:03:44.230
stunning in.
Now,
if we want to do this in end squared time,
we'll have to hurry.

64
00:03:44.560 --> 00:03:47.500
Let's begin that shirt and those pants that you're holding,

65
00:03:47.710 --> 00:03:49.440
how good they look together,
how,

66
00:03:49.520 --> 00:03:53.860
how would you rate them on a scale of one to 10 and so she says seven.
He says,

67
00:03:53.920 --> 00:03:56.080
excellent.
I'll enter that into our array.

68
00:03:56.500 --> 00:03:59.560
And then he keeps asking her these questions.
How about those shoes?

69
00:03:59.561 --> 00:04:03.070
With that necklace?
She says,
three how about those other shoes with that shirt?

70
00:04:03.220 --> 00:04:06.700
She says,
six.
He keeps asking me these questions,
this necklace with those pants,

71
00:04:06.760 --> 00:04:10.480
that shirt with those pants to six,
right?
Three hours later there,

72
00:04:10.481 --> 00:04:11.410
I've told you everything,

73
00:04:11.411 --> 00:04:15.940
and now in a few seconds the paradigm will have worked its magic Wallah all
done.

74
00:04:15.970 --> 00:04:19.300
So what do you want to,
where are these exact pants,
this shirt,

75
00:04:19.330 --> 00:04:22.450
those shoes and that necklace later that night,
darling,

76
00:04:22.451 --> 00:04:26.500
you look absolutely dynamic.
Somehow.
I knew he was going to say that,
right?

77
00:04:26.501 --> 00:04:29.920
So what's happening here is we took this larger problem,

78
00:04:29.950 --> 00:04:34.180
which is what is the optimal outfit to wear that's going to impress this dude at

79
00:04:34.181 --> 00:04:35.520
the end,
right?
And what this,

80
00:04:35.550 --> 00:04:39.010
what this a programmer did was he broke it down into sub problems.

81
00:04:39.160 --> 00:04:42.700
What's the optimal shoe and necklace to wear together?
What's the opposite?

82
00:04:42.780 --> 00:04:45.760
Optimal pants and a shirt to wear together.

83
00:04:46.030 --> 00:04:49.930
And then he combined all those solutions to form the perfect larger problem,

84
00:04:49.931 --> 00:04:51.520
which is that larger outfit,
right?

85
00:04:52.330 --> 00:04:56.290
So dynamic programming can be used to solve problems that would take exponential

86
00:04:56.291 --> 00:04:59.530
time and it can solve them in end squared timer and cube time.

87
00:04:59.950 --> 00:05:02.320
And so it's very similar to the divide and conquer approach.

88
00:05:02.321 --> 00:05:04.960
Recall that in divided and conquer or taking a problem,

89
00:05:05.170 --> 00:05:07.270
we're subdividing into smaller problems,
right?

90
00:05:07.271 --> 00:05:09.130
We're dividing it and conquering each of them.

91
00:05:09.310 --> 00:05:13.120
But the difference between divide and conquer and dynamic programming is that in

92
00:05:13.121 --> 00:05:17.620
dynamic programming,
some of these sub problems can overlap.

93
00:05:17.860 --> 00:05:22.330
And what I mean is let's say this,
uh,
you know,
shirt,
necklace,
a dressing example,

94
00:05:22.690 --> 00:05:24.220
we can have one of the sub problems,
B,

95
00:05:24.340 --> 00:05:27.430
what is the optimal combination of a necklace and a shirt.

96
00:05:27.700 --> 00:05:31.360
And another one could be what is the optimal combination of a necklace,

97
00:05:31.390 --> 00:05:32.530
shirt and shoes.

98
00:05:32.680 --> 00:05:36.750
Notice how the necklace shirt and shoes is kind of a sub problem of this other

99
00:05:37.150 --> 00:05:42.010
or a not a sub problem but a super sect problem of this sub problem.

100
00:05:42.100 --> 00:05:44.560
But they're both sub problems in the larger scale of things,
right?

101
00:05:44.680 --> 00:05:46.240
But they overlap is what I'm saying.

102
00:05:47.620 --> 00:05:50.110
So the basic idea is that in dynamic programming,

103
00:05:50.260 --> 00:05:54.280
we're breaking the problem up into sub problems and we use the optimal solutions

104
00:05:54.281 --> 00:05:58.130
to those sub problems to give us the optimal solution to the larger ones.

105
00:05:58.340 --> 00:06:00.980
It's okay if they overlap,
that's,
that's okay.

106
00:06:01.310 --> 00:06:04.880
And it's basically recursion plus using some common sense.

107
00:06:05.030 --> 00:06:08.960
So recursion allows you to express the value of a function in terms of other

108
00:06:08.961 --> 00:06:12.530
values of that function where common sense tells you that if you implement your

109
00:06:12.531 --> 00:06:15.710
functioning in a way that the recursive calls are done and store it for easy

110
00:06:15.711 --> 00:06:17.990
access,
it will make your program faster.

111
00:06:18.200 --> 00:06:21.530
So what we're doing with dynamic programming is we're essentially trading,

112
00:06:21.970 --> 00:06:24.080
we're trading space for time,
right?

113
00:06:24.081 --> 00:06:27.350
So instead of calculating all the states taking a lot of time,
but no space,

114
00:06:27.500 --> 00:06:31.520
we do take up space to store the results of all those sub problems to save time

115
00:06:31.521 --> 00:06:34.490
later.
Right?
So it's a,
it's a trade off.

116
00:06:34.520 --> 00:06:37.970
We're storing some values so we don't have to recompute everything all over

117
00:06:37.971 --> 00:06:39.140
again.
So it's faster.

118
00:06:39.141 --> 00:06:42.200
But the trade off is we're saving more data so we're taking up more space

119
00:06:42.201 --> 00:06:46.340
complexity.
A good time to use this is check out this,
uh,
well,
here's a,

120
00:06:46.341 --> 00:06:48.650
here's an example by the way.
So the,

121
00:06:48.680 --> 00:06:51.380
the algorithm here is repeat the others word,

122
00:06:51.470 --> 00:06:54.800
add your word and pass it on to the next one.
And if you're wrong,

123
00:06:54.801 --> 00:06:57.350
then you have to drink one beer and we start over again.

124
00:06:57.590 --> 00:06:59.720
So this is called dynamic drinking.
It's just a joke,

125
00:06:59.721 --> 00:07:03.650
but it's like another way of understanding this.
The first guy says while drinks,

126
00:07:03.860 --> 00:07:06.830
or the first guy says,
while the next guy says while Bob,

127
00:07:07.000 --> 00:07:08.540
and next time it says while Bob Eight.

128
00:07:08.690 --> 00:07:12.620
So at every iteration we've saved the previous states,
right?

129
00:07:12.620 --> 00:07:15.740
While while Bob think of it as an array,
right?

130
00:07:15.741 --> 00:07:19.100
So we're storing all of these previous states rather than having to recompute

131
00:07:19.101 --> 00:07:22.250
everything all over again.
And eventually this guy at the end,
he's so drunk,

132
00:07:22.251 --> 00:07:25.100
he forgets what the other said.
And so we start over again.

133
00:07:26.390 --> 00:07:30.260
So all of these dynamic programming problems have four steps.

134
00:07:30.500 --> 00:07:34.580
First we show that the problem can be broken down into optimal sub problems,

135
00:07:34.581 --> 00:07:34.970
right?

136
00:07:34.970 --> 00:07:38.930
So you have to think about it like right with in the case of dressing someone up

137
00:07:39.050 --> 00:07:42.950
or in the case of some larger problem that we can sub divided into sub problems

138
00:07:42.951 --> 00:07:47.720
like designing the optimal uh,
layout of the room,
right?

139
00:07:47.870 --> 00:07:50.660
We could,
we could split the room up into subsections,
right?

140
00:07:50.661 --> 00:07:53.960
And they all had their individual and unique features,
right?

141
00:07:54.050 --> 00:07:57.200
The Wall is kind of curved at this end,
but it's like this and Oh,
Hey,

142
00:07:57.201 --> 00:08:00.530
this sofa would fit perfectly into this subsection of the room,
right?

143
00:08:00.680 --> 00:08:02.330
So we divided into sub problems.

144
00:08:02.630 --> 00:08:06.530
Then we recursively defined the value of the solution by expressing it in terms

145
00:08:06.531 --> 00:08:08.990
of optimal solutions of this smaller sub problems.

146
00:08:09.170 --> 00:08:14.150
So the optimal solution to this room design problem would be such that each of

147
00:08:14.151 --> 00:08:19.151
these spaces in terms of square meters or square feet are filled at the,

148
00:08:19.221 --> 00:08:20.900
at their maximum value.

149
00:08:21.260 --> 00:08:26.260
Because the optimal solution for the larger problem is to feel as as many square

150
00:08:26.480 --> 00:08:30.050
meters or feet as we can,
such that the room is completely fitful,
right?

151
00:08:30.290 --> 00:08:33.860
We compute the value of the optimal solution in a bottom up fashion.

152
00:08:33.890 --> 00:08:36.830
I'll talk about that in a second.
And then lastly,

153
00:08:36.831 --> 00:08:41.570
we construct an optimal solution from the computed information,
right?
So,
okay,

154
00:08:41.571 --> 00:08:42.620
so here we go.
With this,

155
00:08:42.830 --> 00:08:47.830
there are two key attributes that we must have for any problem in order to apply

156
00:08:47.931 --> 00:08:49.370
dynamic programming to it.

157
00:08:50.090 --> 00:08:53.270
The first is it has to have an optimal substructure.

158
00:08:53.450 --> 00:08:55.920
What that means is an optimal solution to a problem.

159
00:08:55.921 --> 00:08:59.670
An instance contains optimal solutions to solve problems.

160
00:09:00.420 --> 00:09:02.940
And then we have to have overlapping sub problems,

161
00:09:03.060 --> 00:09:06.540
meaning a recursive solution contains a small number of distinct sub problems

162
00:09:06.780 --> 00:09:11.660
repeated many times.
So let's,

163
00:09:11.661 --> 00:09:13.070
let's talk about this,
right?
So we have,

164
00:09:13.340 --> 00:09:16.370
so every problem has to have two of these features.

165
00:09:16.371 --> 00:09:19.640
It's got to have an optimal substructure such that for each of these sub

166
00:09:19.641 --> 00:09:22.340
problems,
if we find the optimal solutions to them,

167
00:09:22.520 --> 00:09:26.270
we can combine them and then find the optimal solution to the larger problem.

168
00:09:26.420 --> 00:09:30.260
That's the first.
The second is it's got to have overlapping sub problems,
right?

169
00:09:30.261 --> 00:09:33.980
Like shirt,
shoe tie,
and then short tie for it,
right?
For example,

170
00:09:34.010 --> 00:09:38.460
those are overlapping.
So how do we solve this prop?

171
00:09:38.461 --> 00:09:41.300
Once we've identified a problem that has those two key features,

172
00:09:41.480 --> 00:09:44.780
how do we solve it?
Well,
we have two approaches.
The first,

173
00:09:44.840 --> 00:09:47.900
and these are fancy words,
is the tabulation approach.

174
00:09:48.050 --> 00:09:53.050
The second is a memo position approach and what this really means are a bottom

175
00:09:53.061 --> 00:09:56.540
up approach and a top down approach.
So here's the bottom up approach.

176
00:09:56.750 --> 00:09:59.640
Let's say I want to become an amazing coder,
right?
That's,

177
00:09:59.700 --> 00:10:03.230
that's the goal to the bottom up approach would be step one,

178
00:10:03.320 --> 00:10:06.710
I'm going to learn programming step two.
Then I'll start practicing step three.

179
00:10:06.770 --> 00:10:08.180
I'll take part in contests.

180
00:10:08.330 --> 00:10:11.780
Step four I'll practice even more and try to improve step five after working

181
00:10:11.781 --> 00:10:16.370
hard like crazy.
Step six,
I'll be an amazing coder.
That's bottom up,
right?

182
00:10:16.460 --> 00:10:19.350
We're starting at the very bottom.
Sorry,
from the bottom.
Now we're here,
right?

183
00:10:19.400 --> 00:10:22.250
We're sorry.
I'm on the very bottom like Drake and we're going to the top.

184
00:10:22.670 --> 00:10:26.750
Now top down is the opposite.
We start with that final solution.

185
00:10:26.780 --> 00:10:30.350
I will be an amazing coder.
How?
I'll work hard like crazy.

186
00:10:30.470 --> 00:10:32.180
How I'll prices more and more and more.

187
00:10:32.330 --> 00:10:35.240
How I'll take part in contests all the way down to the bottom.

188
00:10:35.540 --> 00:10:37.340
I'm going to learn how to program.
Okay,

189
00:10:38.420 --> 00:10:40.920
now I've got this table that shows some of the,
you know,

190
00:10:41.060 --> 00:10:43.880
key differences and similarities between these two,

191
00:10:44.000 --> 00:10:46.010
which I'm not going to go into right now because there's a lot.

192
00:10:46.250 --> 00:10:50.720
Let's just get into an example,
shall we?
Okay,
so four overlapping sub problems.

193
00:10:50.810 --> 00:10:52.880
Let's take the FIBONACCI sequence as an example.

194
00:10:53.120 --> 00:10:55.950
You've heard of the FIBONACCI sequence,
right?
It's,

195
00:10:55.960 --> 00:10:57.740
it's a sequence of numbers where,

196
00:10:58.010 --> 00:11:02.990
where every number in the sequence is the,
some of the previous two numbers,

197
00:11:03.260 --> 00:11:07.160
right?
So the first one would be one.
There's nothing before it.
So it's zero.

198
00:11:07.220 --> 00:11:11.180
The next one is one.
So one plus one or one plus zero is one.

199
00:11:11.420 --> 00:11:14.180
The third one is two because one plus one is two,

200
00:11:14.330 --> 00:11:18.110
and the fourth one is three because one plus two is three,
right?

201
00:11:18.560 --> 00:11:21.440
It's just that there's just that chain,
right?
It's a recursive chain.

202
00:11:21.441 --> 00:11:25.400
We can solve it.
Recursively so that's the FIBONACCI sequence.

203
00:11:25.790 --> 00:11:27.340
So how do we solve this?
How do we write,

204
00:11:27.800 --> 00:11:30.800
how do we write a programmatic algorithm for this?
Well,

205
00:11:30.801 --> 00:11:33.040
one solution is to do it recursively,
right?

206
00:11:33.050 --> 00:11:36.740
So doing it recursively does not mean doing it.
The dynamic programming.
Wait,

207
00:11:36.770 --> 00:11:39.650
there's a distinction here.
So let's look at the recursive solution.

208
00:11:39.890 --> 00:11:41.990
The recursive solution looks like this in python.

209
00:11:42.290 --> 00:11:43.640
Now let's go over this function here.

210
00:11:43.940 --> 00:11:47.690
The functions is interfered with the parameter and end,
right?
So we,

211
00:11:47.810 --> 00:11:50.480
we input the amount of numbers in the sequence,

212
00:11:50.570 --> 00:11:52.790
we want it to compute and then we have a base.

213
00:11:53.320 --> 00:11:58.090
The base case is if an is less than or equal to one return that return what n is

214
00:11:58.720 --> 00:12:00.100
and then return fib,

215
00:12:00.130 --> 00:12:04.030
n minus one plus and minus two which are both the,
the,

216
00:12:04.031 --> 00:12:05.950
some of the previous two numbers.

217
00:12:07.250 --> 00:12:11.290
So what this looks like is if f is one,
then we're just going to return one.

218
00:12:11.291 --> 00:12:15.880
So then it would construct one.
If F is zero,
then we're going to turn zero,
right?

219
00:12:15.881 --> 00:12:20.020
But then if f is two is going to be this,
it's going to be,
look at this,

220
00:12:20.230 --> 00:12:22.180
look at this recursive tree that's going to be computed.

221
00:12:22.330 --> 00:12:26.260
It's going to be f of one plus F of zero,
which is two if f of three,

222
00:12:26.261 --> 00:12:31.261
if it's f of three it's going to be f of three is going to be f of two plus f of

223
00:12:31.391 --> 00:12:35.650
one and then four f of two we have to then compute recursively the solution for

224
00:12:35.651 --> 00:12:36.940
f one and F of zero.

225
00:12:37.240 --> 00:12:41.620
Now if we do it like this will totally solve FIBONACCI.
However,

226
00:12:41.800 --> 00:12:45.550
we're going to have to call f three two times.

227
00:12:45.551 --> 00:12:49.330
If we do f of three we could have stored the value of ff three instead of

228
00:12:49.331 --> 00:12:52.750
computing it again,
and we could have reuse that old stored value.

229
00:12:52.930 --> 00:12:56.890
We're not storing anything in memory here.
We're recomputing this entire tree.

230
00:12:57.010 --> 00:13:00.910
So notice if f if,
if it was at,
when I say APP,
I'm talking about fib.

231
00:13:01.220 --> 00:13:02.140
IfF was like,

232
00:13:02.170 --> 00:13:06.790
let's say 200 we would have to continually recompute all the branches of this

233
00:13:06.791 --> 00:13:11.050
tree of execution.
Whereas if we saved some of these values iteratively,

234
00:13:11.200 --> 00:13:14.440
then we wouldn't have to do that.
So right?
It's a trade off between time,

235
00:13:14.441 --> 00:13:16.660
complexity and space complexity.

236
00:13:18.160 --> 00:13:21.280
So how can we do this and that dynamic programming way?
Well,

237
00:13:21.281 --> 00:13:25.120
the dynamic programming way looks very similar to the recursive version with a

238
00:13:25.121 --> 00:13:29.890
small modification in that it uses what's called a lookup table before computing

239
00:13:29.891 --> 00:13:32.470
solutions.
How do we do this?
First,

240
00:13:32.500 --> 00:13:36.460
we initialize that lookup table as nil.
There's no values in here.

241
00:13:36.700 --> 00:13:38.710
Whenever we need a solution to a sub problem,

242
00:13:38.830 --> 00:13:43.480
we first look into this lookup table.
If the precomputed value is there,

243
00:13:43.510 --> 00:13:44.740
then we returned that value.

244
00:13:44.920 --> 00:13:49.420
Otherwise we'd calculate the value and put the result in the lookup table so

245
00:13:49.421 --> 00:13:50.980
that it can be reused layer.

246
00:13:51.010 --> 00:13:54.040
This is the dynamic version of it's the memos version.

247
00:13:54.910 --> 00:13:58.630
For this function we have our base case which is the lookup table initialize as

248
00:13:58.631 --> 00:14:02.410
nil.
If the value is not calculated previously,
then calculate it.

249
00:14:02.440 --> 00:14:06.790
Notice that here is the recursive part fib and one with the lookup as its

250
00:14:06.791 --> 00:14:10.750
parameter and in fib and to uh,
with the look of as this parameter.
Again,

251
00:14:10.930 --> 00:14:15.100
we only add those up if the value in the lookup table is empty and that at the

252
00:14:15.101 --> 00:14:19.420
very end we returned the lookup table.
So notice how it is recursive.
However,

253
00:14:19.421 --> 00:14:23.920
we're adding in this extra a lookup table which acts as a store for the

254
00:14:23.921 --> 00:14:26.410
precomputed values in that Fibonacci sequence.

255
00:14:26.590 --> 00:14:28.600
And it's still a top down approach,
right?

256
00:14:28.601 --> 00:14:31.540
Memorize approach because we're starting,
we're starting at the top.

257
00:14:31.630 --> 00:14:34.810
The top is what is the last value in that Fibonacci sequence.

258
00:14:35.020 --> 00:14:37.390
And then let's go to the smaller sub values right

259
00:14:39.370 --> 00:14:42.580
now and now instead of doing top down,
let's do bottom up.

260
00:14:42.610 --> 00:14:46.750
So the tabulated or bottom up approach is building a table in a bottom up

261
00:14:46.751 --> 00:14:49.420
fashion and returning the last entry from that table.

262
00:14:49.660 --> 00:14:53.330
So for the same FIBONACCI number we calculate first it was zero,

263
00:14:53.480 --> 00:14:54.650
then they've won,

264
00:14:54.680 --> 00:14:58.710
then fifth to then fifth three and so on instead of the opposite way,
right?

265
00:14:58.711 --> 00:15:03.340
And so like (535) 025-0150 we're doing it bottom up Lou,

266
00:15:03.350 --> 00:15:06.110
we are literally building the solutions of sub problems bottom up.

267
00:15:07.190 --> 00:15:10.040
So in this example for the tabulated version,

268
00:15:10.460 --> 00:15:12.590
we are starting with F of zero,
right?

269
00:15:12.890 --> 00:15:17.030
Zero Times n plus one and we are adding not subtracting from end because end

270
00:15:17.031 --> 00:15:17.900
starts at that.

271
00:15:18.020 --> 00:15:22.790
At that minimum value and then we have our base case assignment and then we'll

272
00:15:22.791 --> 00:15:26.210
end is going to be how many numbers we want that that the perimeter is the same.

273
00:15:26.450 --> 00:15:29.490
However,
the way we're computing it is different.
We're starting at the,

274
00:15:29.720 --> 00:15:32.930
we're starting at the smallest value and we're adding them up iteratively.

275
00:15:33.020 --> 00:15:36.860
Like in this four loop we're calculating the FIBONACCI and storing those values

276
00:15:36.950 --> 00:15:39.560
inside of this array that we declared or earlier.

277
00:15:39.920 --> 00:15:43.160
So both of these solutions store the sub problems,
right?

278
00:15:43.750 --> 00:15:48.080
The in the memos version,
the top down version,
the table is filled on demand.

279
00:15:48.081 --> 00:15:52.220
While in the tabulated version we start from the first entry in all entries are

280
00:15:52.221 --> 00:15:55.370
filled one by one,
but unlike the tabulated version,

281
00:15:55.580 --> 00:15:59.600
all entries in the lookup table or not necessarily filled in the memory wise

282
00:15:59.601 --> 00:16:00.560
version,
right?

283
00:16:00.560 --> 00:16:05.420
So that is the overlapping sub problems feature that each of these dynamic

284
00:16:05.421 --> 00:16:08.000
programming problems can have.
Now here's the other feature,

285
00:16:08.120 --> 00:16:09.800
the optimal substructure feature.

286
00:16:10.040 --> 00:16:12.890
Now this also applies to the FIBONACCI problem we just looked at,

287
00:16:13.040 --> 00:16:15.050
but I'm going to look at a different problem now,

288
00:16:15.200 --> 00:16:18.860
which really exemplifies this feature of dynamic programs.

289
00:16:18.950 --> 00:16:21.470
So the problem here is the shortest path Algorithm,
right?

290
00:16:21.620 --> 00:16:23.690
We know about shortest path dykes,
stress.

291
00:16:23.720 --> 00:16:26.180
There's a bunch of the traveling salesman,
Tom,

292
00:16:26.300 --> 00:16:28.580
there's a bunch of shortest path algorithms out there,

293
00:16:28.850 --> 00:16:32.150
but we're going to talk about one in particular that's called the Bellman Ford

294
00:16:32.151 --> 00:16:36.980
Algorithm.
So let's say we have a graph,
right?
We have some,
a graph,

295
00:16:37.370 --> 00:16:40.220
a graph with,
with nodes,
with edges,

296
00:16:40.310 --> 00:16:43.640
and we want to find the shortest path between two,
two notes,
right?

297
00:16:43.641 --> 00:16:46.920
So how do we do this?
So this is an example of,

298
00:16:47.070 --> 00:16:50.900
of a problem that has an optimal substructure.
Check this out.

299
00:16:50.901 --> 00:16:54.470
I'm going to read this out.
All right,
here.
Here we go.
If a node x,
right,

300
00:16:54.471 --> 00:16:59.330
we've got to note x lies in the shortest path from a source node.
You right?

301
00:16:59.331 --> 00:17:03.470
We have no tax source source node,
you and a destination node v.
That's it.

302
00:17:03.471 --> 00:17:07.400
That's all we have to remember.
Three nodes,
node x,
a node,

303
00:17:07.401 --> 00:17:08.690
you and then node z.

304
00:17:09.200 --> 00:17:14.200
Then then the shortest path from you to the is a combination of the shortest

305
00:17:15.261 --> 00:17:19.580
path from you to x and x to V.
Makes Sense,
right?
We have three notes.

306
00:17:19.620 --> 00:17:22.460
X is in the middle,
you ex the,
the short,

307
00:17:22.461 --> 00:17:27.350
what all it's saying is the shortest path from you to x and X.

308
00:17:27.350 --> 00:17:29.990
V is equal to the shortest path from you to be.

309
00:17:30.140 --> 00:17:33.020
So if we solve these two up a sub problems,

310
00:17:33.021 --> 00:17:36.650
optimally we will get the optimal solution to the larger problem.

311
00:17:36.800 --> 00:17:40.010
What's the shortest path from UTV?
Right?
So that's an example.

312
00:17:40.160 --> 00:17:43.760
And we can use the Bellman Ford Algorithm to find the shortest path.

313
00:17:43.970 --> 00:17:45.050
So here's how it works.

314
00:17:46.100 --> 00:17:50.280
So we're given a graph and a source for techs I SRC in that graph,

315
00:17:50.400 --> 00:17:52.950
we want to find the shortest path.
Here's how we do it.

316
00:17:53.160 --> 00:17:55.290
First we calculate the shortest distances,

317
00:17:55.440 --> 00:17:58.080
which I have at most one edge in the path.

318
00:17:58.350 --> 00:18:02.790
Then we calculate the shortest path without most two edges and so on.

319
00:18:02.820 --> 00:18:05.550
Three edges for I just five edges,
et cetera.

320
00:18:05.850 --> 00:18:07.950
After the ice iteration of the outer loop,

321
00:18:08.190 --> 00:18:12.150
the shortest paths would that most I edges are calculated and there can be at

322
00:18:12.151 --> 00:18:16.560
maximum B minus one where vias that highest threshold edges in any simple path.

323
00:18:16.740 --> 00:18:21.330
And that is why the outer loop runs a B minus one times,
right?

324
00:18:21.331 --> 00:18:25.980
So here's an example.
We're basically,
we're computing all of these sub sub,

325
00:18:26.190 --> 00:18:30.090
uh,
paths,
and then we're finding the optimal solutions for those.

326
00:18:30.310 --> 00:18:32.850
And then when we find them,
we can just add them together.

327
00:18:32.851 --> 00:18:35.190
And that gives us the optimum path from a to B,

328
00:18:35.220 --> 00:18:37.080
which are going to be our parameters to this model.

329
00:18:38.700 --> 00:18:42.120
So here's the code version of this.
These are our initialization steps.

330
00:18:42.121 --> 00:18:43.020
We don't have to look at these,

331
00:18:43.170 --> 00:18:45.900
but here's where it really the meat of the code where this is happening.

332
00:18:45.930 --> 00:18:50.580
The Bellman Ford equation finds a shortest distances from source to,

333
00:18:50.630 --> 00:18:54.720
uh,
the initial note that were happening that were we were at.

334
00:18:55.110 --> 00:18:58.920
So step one is to initialize a distances from the source to all of the other

335
00:18:58.921 --> 00:19:01.950
vertices.
Then we relax all the edges.

336
00:19:01.970 --> 00:19:04.590
The minus one times I see a simplest,

337
00:19:04.591 --> 00:19:08.610
shortest path from source to any other vertex can have at most that many edges.

338
00:19:08.910 --> 00:19:12.150
So we have this nested loop where we're updating the distance values and the

339
00:19:12.151 --> 00:19:15.660
parent index of the adjacent vertices of the picked vertex.

340
00:19:15.900 --> 00:19:18.720
So consider only those vertices,
which are still in the queue.

341
00:19:19.290 --> 00:19:21.570
And then we check for negative weight cycles and remove them.

342
00:19:21.571 --> 00:19:24.930
So then we are left with the optimal solutions to that larger sub problem.

343
00:19:25.980 --> 00:19:29.580
And then we have us constructing the graph with all of its edges and vertices

344
00:19:29.581 --> 00:19:33.030
and then we print the solution.
But what I bought,
what I mean there's,

345
00:19:33.060 --> 00:19:34.350
there's a ton,

346
00:19:34.500 --> 00:19:38.430
there's literally a ton of dynamic programming problems out there.

347
00:19:38.431 --> 00:19:41.700
There's like 40 50 60 on the sites out there,
you know,

348
00:19:41.701 --> 00:19:43.200
like dynamic programming problems,

349
00:19:44.820 --> 00:19:47.610
problems cause a million.
And one of these,

350
00:19:50.790 --> 00:19:54.240
which I was looking@gigsforgeeks.com check out gifts for Geeks Dotcom.

351
00:19:54.330 --> 00:19:57.680
He's got so many of these dynamic programming problems.
But anyway,
we're,

352
00:19:57.681 --> 00:20:01.500
where were we?
Oh,
okay.
So what do we know about dynamic programming?
It's,
well,

353
00:20:01.530 --> 00:20:06.530
any problem that dynamic programming can be applied to has two key features.

354
00:20:07.380 --> 00:20:09.180
It's got an optimal substructure,

355
00:20:09.240 --> 00:20:12.240
meaning if we can find the optimal solutions to the sub problems,

356
00:20:12.390 --> 00:20:17.390
we can find the optimal solution to the larger problem and it's got overlapping

357
00:20:17.761 --> 00:20:20.750
sub problems,
meaning some of those problems will overlap.
Sure.

358
00:20:20.751 --> 00:20:23.310
Choose tie tie shoes,
right?

359
00:20:23.520 --> 00:20:27.810
They overlap and now where do we apply this?
Well,

360
00:20:27.990 --> 00:20:29.070
in deep learning,

361
00:20:29.370 --> 00:20:33.300
the work horse of deep learning is the backpropagation algorithm.

362
00:20:33.510 --> 00:20:37.770
This is how supervised learning works,
okay,
let's take a look at some,

363
00:20:37.771 --> 00:20:42.040
some code here for this,
right?

364
00:20:42.041 --> 00:20:42.880
So here's a,

365
00:20:43.030 --> 00:20:47.050
here's a code for a simple feed forward neural network that's trying to,

366
00:20:47.530 --> 00:20:51.010
the pattern match between these inputs and outputs,
right?

367
00:20:51.011 --> 00:20:56.011
We have zero zero 1111101100 and then we have it's associated labels zero one

368
00:20:56.951 --> 00:20:57.784
zero zero zero.

369
00:20:57.910 --> 00:21:01.660
We want to find the pattern here is such that we give this network some input

370
00:21:01.661 --> 00:21:05.350
like one zero one arbitrarily and it will know exactly,
oh,

371
00:21:05.410 --> 00:21:09.190
the label is going to be zero or one after training on this very small four

372
00:21:09.191 --> 00:21:12.130
point dataset.
So how do we do this?
Well,

373
00:21:13.120 --> 00:21:17.680
in this training step,
we say we first,
we initialize our weights as a matrix,

374
00:21:17.710 --> 00:21:19.270
right?
We only have one sets of weight.

375
00:21:19.390 --> 00:21:22.060
This is a perceptron and very simple neural network.

376
00:21:22.300 --> 00:21:23.710
We have our activation function,

377
00:21:23.711 --> 00:21:28.030
which allows us to learn nonlinearities and then I'm not going to go into the

378
00:21:28.270 --> 00:21:31.270
total details of backpropagation.
Now I've done this many,

379
00:21:31.271 --> 00:21:35.110
many times a look in my intro to deep learning series.
In fact,
I can just,

380
00:21:35.150 --> 00:21:37.620
you know,
rap about it a little bit right now.
Uh,

381
00:21:37.840 --> 00:21:41.410
my ones and knows map to oes in one's inputs,
add weights,

382
00:21:41.650 --> 00:21:46.510
update gets sums past that shit too.
Must sigmoid function,
get that error.

383
00:21:46.511 --> 00:21:50.080
What's real in prediction?
And that's what I use.
Gradient descent.

384
00:21:50.290 --> 00:21:55.120
It gives predictions and it doesn't pretend update weights and repeat 10,000

385
00:21:55.121 --> 00:22:00.070
times outputs are lit.
I'd be doing just fine if you want to see that actual rap,

386
00:22:00.090 --> 00:22:04.600
a check on my intro to deep learning a playlist and I think it's um,

387
00:22:04.601 --> 00:22:09.030
number two,
how to make a neural network where I actually wrapped that but to,

388
00:22:09.050 --> 00:22:13.960
anyway,
what we're doing here is we are calculating a gradient value,
right?

389
00:22:14.050 --> 00:22:15.880
We're calculating a gradient value,
which is,

390
00:22:16.120 --> 00:22:19.750
which is also called a partial derivative with respect to our weight values.

391
00:22:19.840 --> 00:22:23.980
It's one single value and we are doing that recursively for every weight going

392
00:22:23.981 --> 00:22:28.270
backwards so we feed forward,
we calculate some input data,
applying operation,

393
00:22:28.271 --> 00:22:31.720
right input times,
wait,
activate.
We send it to the next layer,

394
00:22:31.900 --> 00:22:35.490
input times wait activates and it's the next layer.
We get an output.
We,

395
00:22:35.680 --> 00:22:39.850
we calculate the difference between the expected output in the actual output,

396
00:22:39.851 --> 00:22:41.320
the label.
That's our error.

397
00:22:41.410 --> 00:22:44.500
We use that air to then compute a gradient with respect to the weight's going

398
00:22:44.501 --> 00:22:48.160
backward right back where we compute a gradient,
we get that gradient update,

399
00:22:48.580 --> 00:22:50.380
well we update the waste at the very end.

400
00:22:50.760 --> 00:22:53.710
Then we use that creating to compute the grading for the next layer and

401
00:22:53.711 --> 00:22:55.030
recursively for the next layer.

402
00:22:55.240 --> 00:23:00.240
That is dynamic programming because we are storing that grading value to update

403
00:23:00.821 --> 00:23:05.770
our weights later on.
Okay.
That is dynamic programming.
In principle,

404
00:23:05.771 --> 00:23:09.280
we could calculate the partial derivative of the function with respect to any

405
00:23:09.281 --> 00:23:12.940
weight simply by tracing out the nodes downstream from it and calculating the

406
00:23:12.941 --> 00:23:16.240
longer derivative chains manually,
but we're not doing that.

407
00:23:16.450 --> 00:23:20.470
We're using the chain rule and dynamic programming to do it in a much more

408
00:23:20.471 --> 00:23:22.930
efficient way and that is backpropagation.

409
00:23:23.170 --> 00:23:26.050
It would be very tedious to do it the other way.

410
00:23:26.320 --> 00:23:31.320
The key idea is that we can reuse results for an efficiency increase just as we

411
00:23:31.331 --> 00:23:35.650
do for dynamic programming in general.
So it's used in deep learning.

412
00:23:35.860 --> 00:23:36.940
Where else is it used?

413
00:23:36.970 --> 00:23:40.630
It's also used in reinforcement learning and reinforcement learning.

414
00:23:40.631 --> 00:23:43.780
I've got a whole playlist on reinforcement learning as well to search

415
00:23:43.781 --> 00:23:47.090
reinforcement learning playlist Saroj on youtube,

416
00:23:47.510 --> 00:23:52.370
but dynamic programming solved for the optimal false policy or value function by

417
00:23:52.371 --> 00:23:56.660
recursion.
So if we look at the landscape of reinforcement learning algorithms,

418
00:23:56.840 --> 00:24:00.890
we've got policy optimization and we've got dynamic programming and two of the

419
00:24:00.891 --> 00:24:05.891
most important techniques in mark hub decision process optimization or policy

420
00:24:08.031 --> 00:24:12.500
iteration and value iteration and from them spring cue learning and actor critic

421
00:24:12.501 --> 00:24:13.430
method methods.

422
00:24:14.450 --> 00:24:19.010
So dynamic programming is used heavily to find the optimal mark off decision

423
00:24:19.011 --> 00:24:23.700
process.
And this if you,
if you don't,
you know,

424
00:24:23.720 --> 00:24:25.760
if you want to know what a mark of decision processes,

425
00:24:25.910 --> 00:24:29.300
it's a way of framing an environment where an agent can learn from.
Right?

426
00:24:29.480 --> 00:24:32.630
We have a set of states,
we have a set of actions that the agent can take.

427
00:24:32.780 --> 00:24:35.990
We have some transition function.
We have some starting say distribution,

428
00:24:36.280 --> 00:24:39.710
a discount factor if we want to get fancy and then some reward for completing

429
00:24:39.830 --> 00:24:43.970
the correct action,
whatever that objective is.
Lastly,

430
00:24:44.210 --> 00:24:45.920
let's go over some runtime analysis.

431
00:24:46.010 --> 00:24:49.670
How do we analyze the runtime of a dynamic programming problem?
Well,

432
00:24:49.671 --> 00:24:51.740
it always takes this kind of formula,

433
00:24:52.010 --> 00:24:54.740
the preprocessing run time plus the loop,

434
00:24:54.950 --> 00:24:57.200
tons of recurrence plus the post processing,

435
00:24:57.380 --> 00:25:01.850
so it's a runtime of all of these preprocessing or any of the initial steps you

436
00:25:01.851 --> 00:25:03.470
have to take.
Just find the time.

437
00:25:03.471 --> 00:25:07.370
Complexity of that for the loop is how many times is a Lupron?
Is it a,

438
00:25:07.430 --> 00:25:11.270
is it a single loop?
Is it a nested loop?
Is it a nested,
nested,
nested loop,

439
00:25:11.570 --> 00:25:13.580
and then recurrence?
How much time does it take?

440
00:25:13.581 --> 00:25:18.560
The recurrence run in one four loop iteration.
We multiply those two together,

441
00:25:18.770 --> 00:25:20.000
add the post processing,

442
00:25:20.120 --> 00:25:24.230
and that's going to be our overall runtime analysis for a dynamic programming

443
00:25:24.231 --> 00:25:27.170
problem.
I hope that video helped you and if you want to learn more,

444
00:25:27.171 --> 00:25:29.960
please subscribe for more programming videos.
For now,

445
00:25:29.990 --> 00:25:33.260
I've got to find 99 sub problems,
so thanks for watching.

