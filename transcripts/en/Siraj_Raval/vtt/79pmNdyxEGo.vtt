WEBVTT

1
00:00:00.060 --> 00:00:00.781
Hello world,

2
00:00:00.781 --> 00:00:05.781
it's Saroj and in this episode we're going to build our own game bots capable of

3
00:00:06.121 --> 00:00:09.000
beating any Atari game that we give it.

4
00:00:09.240 --> 00:00:14.240
In 2014 Google acquired a small London based startup called deep mind for $500

5
00:00:16.200 --> 00:00:16.230
million.

6
00:00:16.230 --> 00:00:21.230
That is a lot of Benjamin for what seems like pretty simple software.

7
00:00:21.900 --> 00:00:24.240
It was a bot for Atari Games,

8
00:00:24.480 --> 00:00:28.830
but the reason they paid so much for it was because it was one of the first

9
00:00:28.831 --> 00:00:32.310
steps towards general artificial intelligence.

10
00:00:32.640 --> 00:00:37.230
That means an AI that can excel in not one,
but a variety of tasks.

11
00:00:37.500 --> 00:00:40.770
It's capabilities are generalized just like ours are.

12
00:00:41.100 --> 00:00:45.360
There paper was later featured on the cover of nature showing that their

13
00:00:45.361 --> 00:00:50.361
algorithm could be applied to 50 different Atari Games and achieve superhuman

14
00:00:51.181 --> 00:00:54.420
performance.
In all of them,
they called their bots,

15
00:00:54.480 --> 00:00:59.190
the DPU learner,
but before we talk about that,

16
00:00:59.250 --> 00:01:02.070
let's talk about the concept of reinforcement learning.

17
00:01:02.340 --> 00:01:07.170
Supervised and unsupervised learning techniques are well known in the applied AI

18
00:01:07.171 --> 00:01:09.150
community.
You give some model,

19
00:01:09.151 --> 00:01:13.800
a Dataset with labels and have it learned the mapping between the two or a

20
00:01:13.801 --> 00:01:18.801
dataset without labels and try to learn what the labels are by clustering or

21
00:01:19.411 --> 00:01:21.360
detecting the anomaly in the Dataset.

22
00:01:21.720 --> 00:01:26.490
We can use these data sets to create data classifiers or data generators.

23
00:01:26.700 --> 00:01:30.420
But consider this scenario you're playing the game Super Mario Brothers,

24
00:01:30.600 --> 00:01:33.420
awesome game and rather than play it yourself,

25
00:01:33.570 --> 00:01:36.240
you'd like to train an AI to play it for you.

26
00:01:36.480 --> 00:01:38.550
How should we think about this problem?

27
00:01:38.850 --> 00:01:43.020
If we screen captured game sessions from expert players,

28
00:01:43.200 --> 00:01:47.160
we could use the video frames from the game as input to a model.

29
00:01:47.310 --> 00:01:51.150
And the output could be the directions that Mario could move.

30
00:01:51.390 --> 00:01:55.980
This would be a supervised classification problem.
Since we have labels,

31
00:01:56.010 --> 00:01:57.330
the directions to move,

32
00:01:57.660 --> 00:02:01.840
assuming we have lots of data and access to some sick gps,

33
00:02:02.100 --> 00:02:04.830
it makes sense to try out a neural network here,

34
00:02:04.950 --> 00:02:07.110
given video frames in a new game,

35
00:02:07.320 --> 00:02:11.730
it would know how best to navigate to beat the level,
right?

36
00:02:12.210 --> 00:02:16.230
Yeah.
But then we'd need hundreds of hours of gameplay videos to train on,

37
00:02:16.260 --> 00:02:19.980
and it doesn't seem like an elegant solution to this specific problem.

38
00:02:20.280 --> 00:02:24.870
First of all,
we're training a model,
not on a static data set,
but a dynamic one.

39
00:02:25.110 --> 00:02:27.180
The training data is continuous.

40
00:02:27.181 --> 00:02:31.620
New Frames are constantly emerging in this game world,
this environment,

41
00:02:31.680 --> 00:02:34.800
and we want to learn how to act in this world.

42
00:02:34.980 --> 00:02:38.130
Humans learn best by interacting with an environment,

43
00:02:38.280 --> 00:02:42.630
not by watching others interact in it.
Environments are stochastic.

44
00:02:42.660 --> 00:02:44.520
Any number of events can occur.

45
00:02:44.700 --> 00:02:48.450
It seems best to learn by trying out different possibilities.

46
00:02:48.600 --> 00:02:53.190
So rather than framing this problem as solvable by pattern recognition,

47
00:02:53.640 --> 00:02:57.990
let's frame it as solvable through a process of trial and error.

48
00:02:58.230 --> 00:03:02.140
This is the kind problem that reinforcement learning is made for.

49
00:03:02.440 --> 00:03:07.300
We do have a few labels,
a plus one.
Every time Mario does something positive,

50
00:03:07.540 --> 00:03:11.560
they're just not instantly available to us.
They're time delayed.

51
00:03:11.710 --> 00:03:14.410
Instead of calling them labels,
let's call them.

52
00:03:15.880 --> 00:03:20.410
So how do we formalize this process mathematically?
Well,

53
00:03:20.411 --> 00:03:25.411
we start off with an environment where an AI or agent can perform a number of

54
00:03:25.721 --> 00:03:29.050
actions in.
Since environments are unpredictable,

55
00:03:29.170 --> 00:03:31.690
we want to keep track of its current state.

56
00:03:31.870 --> 00:03:36.520
The agent acts in a given state of the environment and based on its actions,

57
00:03:36.550 --> 00:03:41.170
it may or may not receive a reward,
an increase in the score.

58
00:03:41.350 --> 00:03:44.770
We can then represent one full episode of this process.

59
00:03:44.890 --> 00:03:49.890
An episode would be a single game from start to finish as a sequence of states

60
00:03:50.500 --> 00:03:52.540
actions and rewards for the agent.

61
00:03:52.780 --> 00:03:57.730
The probability of each state depends only on the immediately previous states

62
00:03:57.880 --> 00:04:02.200
and the performed action,
but not on the states or actions before that.

63
00:04:02.410 --> 00:04:05.800
This is called the Mark Hoff property in probability theory named after the

64
00:04:05.801 --> 00:04:07.980
Russian mathematician Andre Hoff,

65
00:04:08.230 --> 00:04:11.860
and since our agent is making decisions based on this property,

66
00:04:12.160 --> 00:04:16.330
this process is considered a Mark Cobb decision process.

67
00:04:18.350 --> 00:04:20.590
How could you submitted without asking me first,

68
00:04:20.591 --> 00:04:24.220
it doesn't matter where on the front page of nature.
Hello Luis,

69
00:04:30.390 --> 00:04:35.040
we want our agent to be smart,
a f to plan not just for short term rewards,

70
00:04:35.100 --> 00:04:38.940
but for long term rewards as well.
For our Super Mario example,

71
00:04:38.970 --> 00:04:43.650
stepping on a Cooper would increase our score in the short term,
but that's it.

72
00:04:43.860 --> 00:04:48.030
However,
consuming a star would increase our score in the short term,

73
00:04:48.060 --> 00:04:52.650
but also increase our score and the longterm we could represent the total future

74
00:04:52.651 --> 00:04:57.651
reward for a single episode from a time point t onward like this where we just

75
00:04:57.871 --> 00:05:02.100
summed them all up,
but remember that our environment is unpredictable,

76
00:05:02.190 --> 00:05:06.390
so we can't be sure that we'll get the same rewards in another episode.

77
00:05:06.600 --> 00:05:11.340
The farther into the future we go,
the farther the rewards could diverge.

78
00:05:11.430 --> 00:05:15.660
So we can add a discount factor between zero and one to our equation.

79
00:05:15.900 --> 00:05:18.570
What this means is the more into the future,

80
00:05:18.571 --> 00:05:21.690
the reward is the less we take it into account.

81
00:05:21.900 --> 00:05:24.540
So we want a balanced value.
Ideally,

82
00:05:24.570 --> 00:05:29.100
we want to choose an action that maximizes the discounted future reward.

83
00:05:29.370 --> 00:05:31.770
So how do we do that?
Well,

84
00:05:31.830 --> 00:05:36.330
we can represent this discounted future reward when we perform an action in a

85
00:05:36.331 --> 00:05:40.710
state and continue optimally from that point on as a function.

86
00:05:41.040 --> 00:05:45.270
This function represents the best possible score at the end of a game after

87
00:05:45.271 --> 00:05:47.820
performing a given action in a given state.

88
00:05:48.180 --> 00:05:51.750
It's a measure of the quality of an action in that state.

89
00:05:51.900 --> 00:05:55.080
So we'll call this function Q four quidditch.
No,

90
00:05:55.081 --> 00:05:59.900
I wish for quality whenever is deciding between several possible actions.

91
00:06:00.050 --> 00:06:04.760
The solution is picking the action that has the highest Q value computing.

92
00:06:04.761 --> 00:06:08.150
This Q function is where the learning process comes in.

93
00:06:08.270 --> 00:06:13.270
The maximum future reward for this state and action is the immediate reward plus

94
00:06:14.031 --> 00:06:16.760
the maximum future award for the next state.

95
00:06:17.090 --> 00:06:19.340
This is also called the bellman equation.

96
00:06:19.670 --> 00:06:24.350
We can think of the Q function as a matrix where the states are the rows and the

97
00:06:24.351 --> 00:06:25.850
actions are the columns.

98
00:06:26.120 --> 00:06:30.470
We'll start by initializing the Q matrix randomly and observing the initial

99
00:06:30.471 --> 00:06:31.580
state of the game.

100
00:06:31.890 --> 00:06:36.050
Then we can approximate it through a training process where we first pick an

101
00:06:36.051 --> 00:06:37.610
action,
execute it,

102
00:06:37.760 --> 00:06:42.020
observe whether or not we received a reward because of it and the new state

103
00:06:42.021 --> 00:06:44.180
we're in.
This is called Q learning.

104
00:06:44.210 --> 00:06:48.590
It's used to find the optimal policy for any mark Haub decision process.

105
00:06:48.770 --> 00:06:51.800
It's a very popular algorithm in reinforcement learning.

106
00:06:51.980 --> 00:06:54.950
Since we're any finites markup decision process,

107
00:06:55.100 --> 00:06:58.700
it's been proven to eventually find the optimal policy.

108
00:06:59.060 --> 00:07:01.400
If we apply cue learning to Mario,

109
00:07:01.460 --> 00:07:05.870
then the state would be presented by the location of the enemies on the screen

110
00:07:06.110 --> 00:07:08.990
and the obstacles as well as some other factors,

111
00:07:09.770 --> 00:07:12.140
but that's a game specific state.

112
00:07:12.320 --> 00:07:17.320
What could we represent as a states that could be generalized across many gates?

113
00:07:18.410 --> 00:07:21.980
Well,
one thing that all games share in common are pixels.

114
00:07:22.160 --> 00:07:26.270
If we could somehow convert pixels from the game screen into actions,

115
00:07:26.510 --> 00:07:31.310
we could feed it any game and it would learn the optimal policy for that game.

116
00:07:31.820 --> 00:07:34.610
If we use a convolutional neural network,

117
00:07:34.730 --> 00:07:39.730
we could use game screens as input and output the Q value for each possible

118
00:07:39.831 --> 00:07:40.640
action.

119
00:07:40.640 --> 00:07:45.640
So deep mind use the CNN with three convolutional layers and to fully connected

120
00:07:45.741 --> 00:07:50.720
layers.
Normally you'd also want to use pooling layers which makes the network

121
00:07:50.750 --> 00:07:55.190
insensitive to the location of an object in an image which is perfect for

122
00:07:55.191 --> 00:07:59.540
classification tasks like detecting if a picture has a certain object and it,

123
00:07:59.870 --> 00:08:04.520
but for our use case,
the location of the player and the enemies is crucial.

124
00:08:04.610 --> 00:08:08.580
So we won't use pooling layers.
We'll input for different games,

125
00:08:08.581 --> 00:08:13.581
screens as input so that we have a way of representing speed and direction of

126
00:08:14.571 --> 00:08:19.571
the game characters and the outputs are the Q values for each possible action.

127
00:08:20.030 --> 00:08:23.810
Since we're using a deep network to help approximate the Q function,

128
00:08:24.020 --> 00:08:27.410
we can call this process deep cue learning.

129
00:08:27.770 --> 00:08:32.270
Deep Mind added a few other tricks to this,
but that's really the basic idea.

130
00:08:32.480 --> 00:08:36.710
The best part is that this algorithm can learn how to excel in any day.

131
00:08:37.220 --> 00:08:38.150
To summarize,

132
00:08:38.240 --> 00:08:43.240
we can use reinforcement learning to optimize an agent for sparse time delayed

133
00:08:43.731 --> 00:08:47.420
labels called rewards in an environment mark Haub.

134
00:08:47.421 --> 00:08:52.100
Decision processes are a mathematical framework for modeling decisions using

135
00:08:52.101 --> 00:08:57.101
states actions and rewards and cue is a strategy that finds the optimal action

136
00:08:58.101 --> 00:09:02.670
selection policy for any MDP.
Noah Dell is the wizard of the week.

137
00:09:02.760 --> 00:09:06.210
The challenge was to build Lda and he definitely delivered.

138
00:09:06.420 --> 00:09:11.420
Noah used Lda to mind topics from scientific papers found on nature.com using

139
00:09:11.491 --> 00:09:15.720
it.
He was able to find important key words that summarize all the jargon very

140
00:09:15.721 --> 00:09:18.210
neatly.
Such an awesome use case,
a plus,

141
00:09:18.390 --> 00:09:20.880
and the runner up is he jumped one each on one.

142
00:09:20.881 --> 00:09:23.610
Use Lda to mind topics from news articles,

143
00:09:23.700 --> 00:09:27.180
then classified them with the CNN to predict stock prices.

144
00:09:27.240 --> 00:09:30.780
This week's coding challenge is to implement a cue learning algorithm from

145
00:09:30.781 --> 00:09:35.100
scratch for any game you'd like post those get hub links and the comments,

146
00:09:35.220 --> 00:09:38.580
and I'll give the two best submissions a shutout next week.

147
00:09:38.910 --> 00:09:40.620
I hope you liked this video and if you did,

148
00:09:40.621 --> 00:09:44.580
please hit the subscribe button for now.
I've got to find my Q matrix,

149
00:09:44.700 --> 00:09:45.990
so thanks for watching.

