WEBVTT

1
00:00:00.180 --> 00:00:03.630
Yo check out my friend Daniel hocked ones youtube channel for some cool virus

2
00:00:03.631 --> 00:00:06.280
videos and coming to you straight from San Francisco.
It's

3
00:00:11.290 --> 00:00:12.600
a world,
it's a Raj,

4
00:00:12.601 --> 00:00:16.590
and in this episode we're gonna Learn how machines can learn to detect viruses.

5
00:00:16.650 --> 00:00:20.820
Then we're going to build our own simple antivirus script in python since 2010

6
00:00:20.821 --> 00:00:24.210
the amount of malware that exists on the web has skyrocketed.

7
00:00:24.300 --> 00:00:27.330
Reputable antivirus programs like Norton,
I'm sorry,

8
00:00:27.331 --> 00:00:29.520
I can't even say that with a straight face.
Norton sucks.

9
00:00:29.570 --> 00:00:33.330
I have to constantly upgrade their systems to defend against new threats and

10
00:00:33.331 --> 00:00:35.850
it's not just the security systems that are getting smarter.

11
00:00:35.880 --> 00:00:37.710
It's the viruses as well.
For example,

12
00:00:37.740 --> 00:00:41.640
polymorphic viruses encrypt themselves in a different way every time they infect

13
00:00:41.641 --> 00:00:42.450
a host machine,

14
00:00:42.450 --> 00:00:46.890
making them harder to detect a worm self so it can spread to other computers as

15
00:00:46.891 --> 00:00:47.190
well.

16
00:00:47.190 --> 00:00:51.120
Using a bandwidth and computing of every host machine and in facts along the
way.

17
00:00:51.180 --> 00:00:53.780
Right under your mouse,
I mean knows maybe I should make a warm too.

18
00:00:53.781 --> 00:00:57.390
In fact a bunch of computers and use them as bitcoin miners.
At its core,

19
00:00:57.420 --> 00:01:00.210
buyers detection is a classification problem.

20
00:01:00.300 --> 00:01:04.080
If we can train a program to recognize whether a piece of software either is

21
00:01:04.081 --> 00:01:08.160
malware or is not malware,
we can delete it in a paper released.
Last year,

22
00:01:08.190 --> 00:01:11.850
a group of researchers in Nigeria trained the cane nearest neighbor classifier

23
00:01:11.880 --> 00:01:13.860
to detect viruses on an android phone.

24
00:01:13.950 --> 00:01:18.660
TheK nearest neighbor or an algorithm pups solve a simple problem.
Cancer?
No,

25
00:01:18.690 --> 00:01:22.650
you haven't set a point in n dimensions given a new point,
let's call it a query.

26
00:01:22.680 --> 00:01:25.110
You need to find the k closest points to that query.

27
00:01:25.170 --> 00:01:26.970
Canon finds those closest points,

28
00:01:27.000 --> 00:01:30.810
so it's great for finding similarities between say a set of documents and

29
00:01:30.811 --> 00:01:34.620
because it can find similarities,
it can also help find anomalies.

30
00:01:34.650 --> 00:01:37.200
In the case of this paper,
the anomaly would be a virus.

31
00:01:37.230 --> 00:01:41.310
In order to train the classifier to detect anomalies and needed a set of feature

32
00:01:41.311 --> 00:01:45.510
vectors,
representations of a clean machine,
they ended up using for features,

33
00:01:45.540 --> 00:01:49.350
SMS texts,
calls,
device statuses,
and running processes.

34
00:01:49.410 --> 00:01:51.900
As in they took a set of ds,
labeled them as clean,

35
00:01:51.930 --> 00:01:55.740
so it was a supervised problem and train their classifier on them and model

36
00:01:55.741 --> 00:01:58.980
ended up having a 94% accuracy,
pretty good results.

37
00:01:59.070 --> 00:02:03.120
A more fresh approach from three months ago was aimed specifically at detecting

38
00:02:03.140 --> 00:02:07.170
botnets on android phones botnets form a distributed network of inducted

39
00:02:07.171 --> 00:02:11.460
machines and utilize their computing power for things like sending spam without

40
00:02:11.461 --> 00:02:12.420
the owner's knowledge.

41
00:02:12.480 --> 00:02:15.390
There are two approaches when it comes to malware analysis,

42
00:02:15.450 --> 00:02:19.680
static or code based and dynamic or runtime based.
The static approach,

43
00:02:19.681 --> 00:02:23.100
it looks at software as it is on the machine at dynamic approach.

44
00:02:23.130 --> 00:02:25.440
It looks at ongoing processes on the system.

45
00:02:25.470 --> 00:02:27.870
These guys decided to go for the dynamic approach.

46
00:02:27.900 --> 00:02:30.960
They use a neural net to train on a labeled Bot net dataset.

47
00:02:30.990 --> 00:02:35.130
Then it labeled an unlabeled data set as either bought net or not VOD net.

48
00:02:35.160 --> 00:02:38.790
They didn't took that labeled dataset and trained six different classification

49
00:02:38.820 --> 00:02:42.990
algorithms on it like logistic regression or random forest and a support vector

50
00:02:42.991 --> 00:02:46.740
machine.
They found that a simple logistic regression got the best results.

51
00:02:46.770 --> 00:02:49.950
Who would have thought and they called this framework of mining features

52
00:02:50.040 --> 00:02:53.790
training a classifier and performing dynamic analysis,
smart bond,

53
00:02:53.850 --> 00:02:57.480
extremely original.
But let's talk about a super fresh approach.

54
00:02:57.510 --> 00:03:00.130
A paper released two years in the future.
Just kidding.

55
00:03:00.131 --> 00:03:02.530
I can't literally see into the future yet,

56
00:03:02.590 --> 00:03:07.000
which release just a week ago that used BZ and classification to detect android

57
00:03:07.001 --> 00:03:07.450
malware.

58
00:03:07.450 --> 00:03:11.110
The first reverse engineered a set of clean android apps to map them into

59
00:03:11.111 --> 00:03:12.910
feature vectors like API calls,

60
00:03:12.911 --> 00:03:16.060
Linux system commands and permissions contained in the manifest file.

61
00:03:16.120 --> 00:03:18.850
They didn't train their Basie and classifier on those features.

62
00:03:18.880 --> 00:03:22.780
These IAN classification uses Bayes theorem to measure the likelihood that an

63
00:03:22.781 --> 00:03:26.320
object is of a certain class using feature vectors as inputs.

64
00:03:26.350 --> 00:03:29.830
And the results in the paper showed way better detection rates than traditional

65
00:03:29.831 --> 00:03:31.720
signature based antivirus software.

66
00:03:31.750 --> 00:03:35.770
So there are many different ways to approach virus detection and as software

67
00:03:35.771 --> 00:03:38.230
eats the world,
malware tries to as well.

68
00:03:38.260 --> 00:03:40.870
Viruses can use machine learning as well to avoid detection.

69
00:03:40.871 --> 00:03:44.110
So only one way to fight fire with fire.

70
00:03:44.200 --> 00:03:46.060
That analogy doesn't actually make sense,
does it?

71
00:03:46.090 --> 00:03:49.750
So let's build a script in python that you just psychic learn to train a

72
00:03:49.751 --> 00:03:54.130
classifier to detect if a file is legitimate or malicious in learning that pie

73
00:03:54.250 --> 00:03:57.490
will import the necessary libraries.
Pandas is for data analysis,

74
00:03:57.520 --> 00:03:59.440
not actual pandas.
Sadly none.

75
00:03:59.441 --> 00:04:02.620
Pious for math pickle will help us save our learning features as a byte stream

76
00:04:02.770 --> 00:04:06.670
and psychic learn will help us build,
train and test a machine learning model.

77
00:04:06.700 --> 00:04:09.040
The first thing we want to do is load a data source.

78
00:04:09.070 --> 00:04:12.880
We have a CSV file on our local machine called data dot CSV that contains a

79
00:04:12.881 --> 00:04:16.630
labeled Dataset of PE files labeled as either a legit or malicious and their

80
00:04:16.631 --> 00:04:20.410
associated properties will then print out the total number of features per row.

81
00:04:20.470 --> 00:04:23.770
Then it's time for us to identify which features from our Dataset we will

82
00:04:23.771 --> 00:04:26.800
identify as important for our classifier.
In order to do this,

83
00:04:26.801 --> 00:04:28.720
we use an extra trees classifier.

84
00:04:28.750 --> 00:04:33.190
This fits a number of randomized decision trees on sub samples of the data.

85
00:04:33.250 --> 00:04:37.240
Once we have our important features or print them out and sort them accordingly,

86
00:04:37.300 --> 00:04:40.240
then we'll want to create an array of models.
We're going to test.

87
00:04:40.270 --> 00:04:44.470
Each model on our dataset using are extracted features as inputs and compare

88
00:04:44.471 --> 00:04:45.610
their prediction results.

89
00:04:45.640 --> 00:04:49.570
Whichever model has the best results is the one we will use to detect malware in

90
00:04:49.571 --> 00:04:53.170
our for loop.
We'll test each algorithm out,
fitting it on our feature set.

91
00:04:53.200 --> 00:04:56.680
Then scoring the prediction accuracy or print out each score.

92
00:04:56.710 --> 00:05:00.460
Then calculate a winner by finding the highest prediction accuracy or print out

93
00:05:00.461 --> 00:05:01.000
the winner.

94
00:05:01.000 --> 00:05:04.570
Then save the algorithm weights and features to the classifier folder as a

95
00:05:04.571 --> 00:05:07.510
series of pickle files,
so that's how we train our classifier.

96
00:05:07.570 --> 00:05:10.300
Let's see what it takes to write the main script.
In our main method.

97
00:05:10.360 --> 00:05:13.870
We'll initialize a command line parts too.
When we type in the name of this file,

98
00:05:13.900 --> 00:05:17.710
the argument will be to target file we went to classify as either malicious or

99
00:05:17.711 --> 00:05:19.360
legit.
Then we'll load our classifier,

100
00:05:19.390 --> 00:05:22.510
the one we trained from our classifier folder as well as our features.

101
00:05:22.540 --> 00:05:25.450
Well then extract the byte stream from our input file and extract a set of

102
00:05:25.451 --> 00:05:26.260
features from it.

103
00:05:26.260 --> 00:05:28.930
We'll feed those features to our trained model and it will output a

104
00:05:28.931 --> 00:05:31.180
classification that we then print it to command line.

105
00:05:31.210 --> 00:05:34.660
Let's try this baby out on the command line by feeding at first a legitimate PE

106
00:05:34.661 --> 00:05:37.030
file,
and now a known malicious pdf file.

107
00:05:37.090 --> 00:05:39.490
Malicious links through the codes in the description,

108
00:05:39.550 --> 00:05:42.100
and please hit that subscribe button if you want to see more machine learning

109
00:05:42.101 --> 00:05:45.790
videos.
For now,
I've got to go code up a girlfriend,
so thanks for watching.

