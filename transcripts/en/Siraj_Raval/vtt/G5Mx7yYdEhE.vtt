WEBVTT

1
00:00:00.090 --> 00:00:04.590
Hello world.
It's the Raj and do you want to predict cryptocurrency prices?

2
00:00:04.830 --> 00:00:06.390
Of course you do.
Who wouldn't?

3
00:00:06.450 --> 00:00:09.780
Everybody's talking about cryptocurrency these days and if you understand

4
00:00:09.781 --> 00:00:10.614
machine learning,

5
00:00:10.680 --> 00:00:14.370
why not apply that skill to try to predict some cryptocurrency prices?
Right?

6
00:00:14.850 --> 00:00:18.570
In this video,
I'm going to show you how to predict the price of Bitcoin,

7
00:00:18.690 --> 00:00:22.440
but this can really apply to etherium.
This can apply to any alt coin.

8
00:00:22.441 --> 00:00:24.780
It can apply to any cryptocurrency,

9
00:00:24.781 --> 00:00:28.290
and we're going to use a very simple model built with care os,

10
00:00:28.320 --> 00:00:32.400
a deep learning library that is very popular and I've talked about it before.

11
00:00:32.490 --> 00:00:36.510
We were going to use it to predict future prices.
Now you might be asking,
wait,

12
00:00:36.530 --> 00:00:39.960
does this work?
Are you sure this is going to work?
And let me just tell you this,

13
00:00:40.110 --> 00:00:44.040
JP Morgan,
Morgan Stanley,
all of these big banks,

14
00:00:44.041 --> 00:00:47.850
all of these big hedge funds numerate all of these hedge funds.

15
00:00:48.180 --> 00:00:52.200
I promise you they're using some kind of algorithm to predict future prices.

16
00:00:52.440 --> 00:00:56.730
Will they share it with you?
Definitely not because that is their profit,
right?

17
00:00:56.731 --> 00:01:00.660
That is their secret.
Of course we're not going to share it.
So the answer is yes,

18
00:01:00.661 --> 00:01:01.351
it's possible.

19
00:01:01.351 --> 00:01:06.180
There's millions of variables out there and you can get a greater than 50% a

20
00:01:06.240 --> 00:01:10.620
return if you do this the right way.
Is this the absolute optimal right way?

21
00:01:10.710 --> 00:01:11.970
I don't know,
but it,

22
00:01:12.000 --> 00:01:16.530
the graph looks really nice and I've made sure to really avoid overfitting.
So,

23
00:01:16.710 --> 00:01:19.020
uh,
using dropout and some other techniques that I'll talk about.

24
00:01:19.021 --> 00:01:21.360
So let's just get into this.
Okay.
All right.

25
00:01:21.361 --> 00:01:25.590
So what I'm gonna do is I'm going to show you this code.
Now,
this code,

26
00:01:25.620 --> 00:01:29.280
all of it is available in the description of this video.

27
00:01:29.281 --> 00:01:32.550
So definitely check it out and follow along because I'm going to be analyzing

28
00:01:32.551 --> 00:01:35.730
this code.
I'm going to be talking about different techniques.
Okay?
So let's,

29
00:01:35.731 --> 00:01:38.730
let's do this.
So the first step is for us to import our dependencies,

30
00:01:38.731 --> 00:01:41.940
which I've got right here.
The first dependencies are of course,

31
00:01:41.941 --> 00:01:43.050
all of those carrots.

32
00:01:43.090 --> 00:01:46.590
Submodules remember Ken Ross is the easiest way to get started with deep

33
00:01:46.591 --> 00:01:51.330
learning.
If you haven't searched care os explained Saroj on Youtube.

34
00:01:51.390 --> 00:01:54.060
Great video for you.
But anyway,
in this case,

35
00:01:54.210 --> 00:01:57.580
we're going to use what's called a recurrent network specifically.
Okay.
This is,

36
00:01:57.581 --> 00:02:01.680
no,
this is a long one.
A by directional LSTM recurrent network.

37
00:02:01.770 --> 00:02:03.270
I'll talk about exactly what that is,

38
00:02:03.720 --> 00:02:06.510
but I've talked about it before in my intro to deep learning course,

39
00:02:06.630 --> 00:02:09.840
which you should watch as well if you haven't.
Uh,
but uh,
it's,

40
00:02:09.841 --> 00:02:13.830
it's basically a more advanced version of her recurrent network that takes into

41
00:02:13.831 --> 00:02:17.910
account future,
uh,
values in a sequence instead of just the past.

42
00:02:19.200 --> 00:02:21.810
But [inaudible] is awesome.
It's a great library.

43
00:02:21.960 --> 00:02:26.280
It's got loads of documentations,
great examples and uh,
yeah,

44
00:02:26.281 --> 00:02:29.490
you can basically learn all about deep learning just from reading the chaos

45
00:02:29.491 --> 00:02:32.640
documentation,
which I've got right here.
So it's a great library.
Okay,

46
00:02:32.820 --> 00:02:35.670
so enough of that.
So what I'm doing here for these forts,

47
00:02:35.760 --> 00:02:39.900
first four lines is I'm importing the relevant parts of care os that we're going

48
00:02:39.901 --> 00:02:42.840
to need for this model.
The first part is dense.

49
00:02:42.841 --> 00:02:44.610
Dense means a fully connected layer.

50
00:02:44.611 --> 00:02:48.840
That means all of the nodes in one layer are connected to all of the nodes or

51
00:02:48.841 --> 00:02:53.580
neurons in the next layer,
which usually happens at the end of a deep network.

52
00:02:53.581 --> 00:02:56.370
Usually the last two or three layers are fully connected,

53
00:02:56.371 --> 00:02:59.560
whether it's for convolutional networks or LSTM networks.

54
00:02:59.950 --> 00:03:01.150
I've got the activation,

55
00:03:01.200 --> 00:03:04.720
a sub module which is going to be used for are activation function,
right?

56
00:03:04.720 --> 00:03:08.590
This is our nonlinearity so our network can learn both linear and nonlinear

57
00:03:08.591 --> 00:03:09.041
functions,

58
00:03:09.041 --> 00:03:13.520
which of course the stock graph is definitely nonlinear and then drop out.

59
00:03:13.521 --> 00:03:15.370
Drop out is a technique that I'm going to talk about,

60
00:03:15.371 --> 00:03:18.080
but in this case I'm using it to prevent over fitting.

61
00:03:18.370 --> 00:03:21.940
Then I've got the LSTM class,
which is,
which stands for long short term memory.

62
00:03:21.941 --> 00:03:24.730
I'll talk about all of this by directional is a type of network.

63
00:03:24.820 --> 00:03:29.740
Sequential is a type of graph like in order,
you know,
ABC d,
uh,
instead of like,

64
00:03:29.770 --> 00:03:33.460
you know,
multiple branches like a,
like a directed a cyclic graph or something.

65
00:03:33.670 --> 00:03:36.790
It's more of a single,
it's more of like a like a link,

66
00:03:36.820 --> 00:03:39.880
a singly linked to list in terms of a graph in terms of bi directional.

67
00:03:39.881 --> 00:03:42.220
It's a doubly linked lists,
but we'll,
we'll get into that.
Okay,

68
00:03:42.221 --> 00:03:46.420
so the next part are is psychic learn.
Second learns a great library,

69
00:03:46.421 --> 00:03:50.290
but at this point it's really all of it's functionality has been eclipsed by

70
00:03:50.560 --> 00:03:53.170
great libraries like tensorflow,
Pi torch.

71
00:03:53.260 --> 00:03:57.490
But what it's really gotten going for it is it's a metrics module,
so like this,

72
00:03:57.610 --> 00:04:00.880
you know,
it's loggings like s sk,
learn dot metrics.

73
00:04:01.030 --> 00:04:03.790
It's got so many great sub modules that I use still,

74
00:04:04.030 --> 00:04:06.670
but we're going to be used it too important the mean squared error,

75
00:04:06.671 --> 00:04:08.080
which is going to be our loss function.

76
00:04:08.350 --> 00:04:11.410
This is what we're going to minimize over time to make our network better and

77
00:04:11.411 --> 00:04:14.200
better at predictions.
Lastly,

78
00:04:14.201 --> 00:04:17.620
we've got logging for logging with the training process,
num,

79
00:04:17.621 --> 00:04:22.060
py and math for standard matrix math operations map plot live for plotting like

80
00:04:22.061 --> 00:04:25.810
our graph at the end.
And lastly,
data processing is going to be pandas.

81
00:04:26.110 --> 00:04:29.860
Pandas is the data processing tool that data scientists use.
In fact,

82
00:04:30.100 --> 00:04:33.910
I have seen data scientists put pandas expert in their,
on their resume,

83
00:04:34.120 --> 00:04:36.700
like it's that important,
the library.
So if you haven't seen pandas yet,

84
00:04:36.880 --> 00:04:40.770
definitely check it out.
So,
uh,
this is a huge like infographic,

85
00:04:40.780 --> 00:04:44.230
but let me first read this,
this first step here.
So for data processing,

86
00:04:44.470 --> 00:04:46.990
what we're going to do is we're going to input this data as a CSV file.

87
00:04:46.991 --> 00:04:49.210
And I'll show you this,
this data in a second,

88
00:04:49.211 --> 00:04:52.330
but we're going to time series transformed this data.
So it's you,

89
00:04:52.390 --> 00:04:55.690
it's going to be from number of days by the number of features that was the

90
00:04:55.691 --> 00:04:56.920
original array,
right?

91
00:04:57.190 --> 00:05:00.430
And we're going to convert that to the number of days minus the window size

92
00:05:00.670 --> 00:05:04.030
times the number of days per sample times the number of features.

93
00:05:04.150 --> 00:05:07.300
So we're turning it from a two d array,
two or three d array.

94
00:05:07.330 --> 00:05:10.940
And this is just the preprocessing step.
So we can then feed it into our,
uh,

95
00:05:11.020 --> 00:05:15.610
network,
right?
Uh,
and normalization is what we're,

96
00:05:15.640 --> 00:05:16.480
what we're about to do.

97
00:05:16.481 --> 00:05:21.481
So the reason we want to normalize data is so that our model will be able to

98
00:05:21.881 --> 00:05:26.620
converge faster,
right?
Normalization means,
I mean it means a lot of things,

99
00:05:26.621 --> 00:05:30.280
but check,
now,
let me go to this infographic here,
but we have five,

100
00:05:30.310 --> 00:05:34.450
five rules of data normalization.
The first is eliminating repeating groups.

101
00:05:34.480 --> 00:05:37.540
Anytime you have some values that are being repeated,
eliminate them.

102
00:05:37.810 --> 00:05:41.950
Eliminate redundant data,
eliminate columns not dependent on the key.

103
00:05:42.100 --> 00:05:47.100
So if attributes to not contribute to a description of the key or move them to a

104
00:05:47.381 --> 00:05:48.280
separate table.

105
00:05:48.370 --> 00:05:51.880
So it's kind of compartmentalizing all of these like different features,

106
00:05:52.180 --> 00:05:54.430
isolating independent,
multiple relationships,

107
00:05:54.580 --> 00:05:56.860
and isolating semantically related multiple.

108
00:05:57.710 --> 00:05:59.810
So you can really read the small text later on.

109
00:05:59.811 --> 00:06:03.590
But those are like five key rules for data normalization.
But in general,

110
00:06:03.591 --> 00:06:06.980
remember that data normalization is a step that we always perform in data

111
00:06:06.981 --> 00:06:10.700
preprocessing.
It's just a very important thing to do.
And so in this case,

112
00:06:10.701 --> 00:06:14.270
what we're going to do is we're going to divide each value in the window by the

113
00:06:14.271 --> 00:06:17.360
first value of the window and then subtract one.
So I.
E.

114
00:06:17.361 --> 00:06:21.200
If we had an array like this for three to once normalized,

115
00:06:21.230 --> 00:06:24.860
it would become zero negative 0.25 negative 0.5.

116
00:06:24.980 --> 00:06:28.550
And notice that these values are much smaller and the interval between these

117
00:06:28.551 --> 00:06:30.650
values is much smaller.
So it's kind of dense.

118
00:06:30.651 --> 00:06:34.520
It's kind of condensing this graph so it's smaller and more readable and easier

119
00:06:34.521 --> 00:06:39.500
to converge on.
For our model,
the normalized basis,
we're going to keep them,

120
00:06:39.501 --> 00:06:41.210
that means those original numbers,

121
00:06:41.211 --> 00:06:44.690
just so we can compare the models predictions with the true prices,

122
00:06:44.930 --> 00:06:49.640
we're going to split our data,
90% training,
10% testing.
Okay.
So this data,

123
00:06:49.650 --> 00:06:53.540
what it's going to look like is like this.
It's going to have so many,
uh,

124
00:06:53.750 --> 00:06:56.030
different columns for all of these features.

125
00:06:56.120 --> 00:06:59.480
So you might be wondering how is predicting cryptocurrency any different from

126
00:06:59.481 --> 00:07:03.110
predicting stock prices or any of the other,
you know,
financial predictions?

127
00:07:03.200 --> 00:07:07.430
And the answer is cryptocurrency has its own set of variables that the stock

128
00:07:07.431 --> 00:07:11.020
market doesn't have.
You've got block size,
block,
high annual Hashcode,

129
00:07:11.160 --> 00:07:15.860
Medcalf slaw,
market capitalization,
Hash rate,
you know,
minor revenue value.

130
00:07:15.890 --> 00:07:20.480
All of these variables that are unique to blockchains,
right?
Very,

131
00:07:20.481 --> 00:07:23.960
very cool stuff.
So in terms of where you can get this Dataset,

132
00:07:24.050 --> 00:07:27.140
if you just Google a bitcoin blockchain Dataset,

133
00:07:28.430 --> 00:07:31.280
you've got several.
I got mine from Kaggle,
which is the first link.
Uh,

134
00:07:31.281 --> 00:07:32.570
but you've got several here.
In fact,

135
00:07:32.690 --> 00:07:35.460
you could just pull it from the blockchain API directly.
Uh,

136
00:07:35.510 --> 00:07:39.830
if you search stack overflow,
you've got some people who are uh,
who have,

137
00:07:39.950 --> 00:07:42.530
there we go get bitcoin historical data.
This guy is like,

138
00:07:42.531 --> 00:07:45.080
you can get the whole trade history and CSV format from here.

139
00:07:45.081 --> 00:07:49.130
Api Dot bitcoin charts.
Startcom Quandl is another one.
Uh,

140
00:07:49.131 --> 00:07:53.720
but basically I use Kaggle,
uh,
just because I really liked Kaggle and um,

141
00:07:54.650 --> 00:07:57.290
yeah,
but there's a lot,
you've got a bunch of CSPS for,
you know,

142
00:07:57.291 --> 00:07:58.100
all sorts of things.

143
00:07:58.100 --> 00:08:01.970
The exchanges you can play around with what data you're going to use,
right?

144
00:08:02.120 --> 00:08:06.170
What is it,
what is the exchange volume between different cryptocurrencies?
Uh,

145
00:08:06.200 --> 00:08:06.651
but there's,

146
00:08:06.651 --> 00:08:11.540
there's so much data out there that it is just a crime to not be training some

147
00:08:11.541 --> 00:08:16.010
machine learning models on this data.
So back to this.
So we've got,

148
00:08:16.040 --> 00:08:19.370
we've got lots and lots and lots of data here.
We can look at some of it,

149
00:08:19.371 --> 00:08:22.070
but that's,
that's,
that's the gist of it.
So let's keep going here.

150
00:08:22.430 --> 00:08:25.790
So then comes our loading data a step,
right?

151
00:08:25.791 --> 00:08:29.480
So what we're gonna do is we're going to read the data file using pandas
Builtin,

152
00:08:29.481 --> 00:08:31.760
read CSV function,
and we're going to store this,

153
00:08:31.761 --> 00:08:35.510
that pandas data frame object in this raw data variable.

154
00:08:35.760 --> 00:08:39.080
Then we're going to change all the Zeros to the number before the zero occurs.

155
00:08:39.200 --> 00:08:42.380
So in this way,
we're removing all the unnecessary zero values.

156
00:08:42.381 --> 00:08:45.860
We want very dense data.
So that's what this nested loop does.

157
00:08:46.190 --> 00:08:49.640
We convert that file to a lists for further and easier preprocessing.

158
00:08:49.880 --> 00:08:53.510
And then we take that data variable and we convert it to a three d array.

159
00:08:53.511 --> 00:08:56.910
Remember I said we're converting it from a two d array to a three d array,
right?

160
00:08:57.180 --> 00:09:02.100
And then we fill that three d array with the,
uh,
normalized values,
right?

161
00:09:02.101 --> 00:09:06.030
This is that normalization step.
We keep the unformalized prices as well.

162
00:09:06.031 --> 00:09:09.720
So we can compare the two later on.
We split the data into training.

163
00:09:09.721 --> 00:09:12.480
Remember 90% training,
10% testing.

164
00:09:12.720 --> 00:09:17.430
We shuffle the data and then we create those training and testing of variables

165
00:09:17.640 --> 00:09:20.100
we get the day before,
why tests price,

166
00:09:20.160 --> 00:09:23.370
so there's so we can use that for prediction later on and then we get the window

167
00:09:23.371 --> 00:09:26.670
size and sequenced length.
We return all of those variables,
right?

168
00:09:26.671 --> 00:09:31.620
We computed all of these values and now we can return them,
right?

169
00:09:31.621 --> 00:09:35.790
So now step two,
step two for us is going to be to build our model.

170
00:09:35.820 --> 00:09:40.410
Now recall I said we're going to build a recurrent network.
To be more specific,

171
00:09:40.470 --> 00:09:45.470
we're building a three layer recurrent network with 20% dropout at each layer to

172
00:09:45.661 --> 00:09:48.630
reduce overfitting into training data.
Specifically,

173
00:09:48.631 --> 00:09:52.380
the type of recurrent net we're going to build is called a bidirectional LSTM

174
00:09:52.381 --> 00:09:56.550
network.
The model is going to have about 500,000 trainable parameters.

175
00:09:56.730 --> 00:09:59.760
Don't be alarmed.
This is very normal for deep networks,
right?

176
00:10:00.150 --> 00:10:03.420
Each of those values inside of the matrices of the weights,

177
00:10:03.600 --> 00:10:07.920
those are parameters that are being trained and they are tuneable values that

178
00:10:07.921 --> 00:10:11.460
are getting updated through some optimization scheme.
In this case,

179
00:10:11.461 --> 00:10:14.060
the optimization scheme we'll use is Adam,
right?

180
00:10:14.061 --> 00:10:15.750
So Adam is the optimization scheme.

181
00:10:16.110 --> 00:10:19.110
The loss function is going to be mean squared error.

182
00:10:19.740 --> 00:10:22.980
The linear activation function in this model is going to determine the output of

183
00:10:22.981 --> 00:10:26.730
each neuron in the model and then it's going to use the chaos is sequential

184
00:10:26.790 --> 00:10:30.210
bidirectional LSTM layers.
So now I'm going to go into what I just said.

185
00:10:30.270 --> 00:10:34.740
So recurrent nets are different from feedforward networks because in feedforward

186
00:10:34.741 --> 00:10:38.610
networks,
every,
at every time step,
we're only feeding in the new input data,

187
00:10:38.611 --> 00:10:40.980
right?
You know,
we have,
you know,
let's say like 10 data points,

188
00:10:40.981 --> 00:10:44.940
like every time step we feed in a new one,
Duh Duh,
Duh Duh Duh Duh Duh.

189
00:10:45.060 --> 00:10:47.730
But with recurrent networks,
there's loops,
right?

190
00:10:47.940 --> 00:10:51.390
Instead of just feeding in the PR,
the input data,
right?

191
00:10:51.391 --> 00:10:53.580
The next data in the set of data,

192
00:10:53.790 --> 00:10:57.870
we're also feeding in the hidden state that was learned from the previous time

193
00:10:57.871 --> 00:11:00.540
step,
hence the loop,
right?
Hence the looping part.

194
00:11:00.720 --> 00:11:04.410
We are feeding in the pin stay from the previous time step and the,

195
00:11:04.470 --> 00:11:07.710
the new input data.
So we're concatenating both of those values.

196
00:11:07.711 --> 00:11:11.190
And we're feeding them in and the reason we do this is because when it comes to

197
00:11:11.191 --> 00:11:14.100
sequences of data,
memory matters,
right?

198
00:11:14.220 --> 00:11:15.930
If we have a sequence like one two three four,
five,

199
00:11:15.931 --> 00:11:18.870
six and we want to predict the next number in that sequence,

200
00:11:19.050 --> 00:11:21.330
we have to know that before the next number,

201
00:11:21.331 --> 00:11:23.040
which is going to be eight spoiler alert,

202
00:11:23.310 --> 00:11:26.780
we have to know that the first seven numbers were one through seven and that's

203
00:11:26.790 --> 00:11:29.310
why we feed in the hidden state.
That is learned over time,

204
00:11:29.730 --> 00:11:33.360
but what happens is when we have really long sequences,

205
00:11:33.540 --> 00:11:37.710
whenever we're optimizing this network,
the gradient Spanish slowly,
slowly,

206
00:11:37.711 --> 00:11:38.011
slowly,

207
00:11:38.011 --> 00:11:41.430
slowly as we go from the last layer is back to the first layers and the gradient

208
00:11:41.431 --> 00:11:43.770
is what we use to update all of these weights.

209
00:11:43.950 --> 00:11:47.190
So in order to prevent this vanishing gradient,
so all of the layers,

210
00:11:47.310 --> 00:11:49.890
all the weights in each layer are updated properly.

211
00:11:50.400 --> 00:11:54.430
Someone invented this technique called a short term memory cell,

212
00:11:54.640 --> 00:11:57.760
so the LSTM cell,
it has a lot of features to it,

213
00:11:58.000 --> 00:12:00.070
but I'm not going to talk about all of them right now.

214
00:12:00.160 --> 00:12:03.940
But basically what it does is it's got an input gate of forget gate and an

215
00:12:03.941 --> 00:12:08.350
output gates and these act as valves,

216
00:12:08.410 --> 00:12:13.000
they act as vowels like plumbing valves that can store and locked in memory over

217
00:12:13.001 --> 00:12:16.120
time.
And what this does is it prevents the vanishing gradient problem,

218
00:12:16.240 --> 00:12:19.330
which for us means we can predict longer term sequences,

219
00:12:19.480 --> 00:12:23.590
perfect for predicting cryptocurrency prices over a long period of time.

220
00:12:24.970 --> 00:12:28.510
Now lastly,
it's not just an LSTM recurrent network.

221
00:12:28.540 --> 00:12:31.360
It's a bi directional LSTM recurrent network.

222
00:12:31.540 --> 00:12:33.220
So if you've seen some of my older videos,

223
00:12:33.370 --> 00:12:37.630
the only other time I've talked about a by directional LSTM is when I was

224
00:12:37.631 --> 00:12:40.060
talking about how to create a language translator.

225
00:12:40.061 --> 00:12:45.061
I think it was deep learning number 16 or 15 because Google needed that too to

226
00:12:45.400 --> 00:12:48.700
create a really good,
you know,
state of the art language translation service.

227
00:12:49.060 --> 00:12:50.290
But by Directional Arice,

228
00:12:50.330 --> 00:12:54.940
rns are based on the idea that the output at time t may not only depend on the

229
00:12:54.941 --> 00:12:58.510
previous element in the sequence,
but also future elements.

230
00:12:58.540 --> 00:13:02.260
It's not just about the past,
it's about the future,
right?
So for example,

231
00:13:02.261 --> 00:13:04.090
to predict a missing word in a sequence,

232
00:13:04.270 --> 00:13:07.570
you want to look at both the left and the right context,
right?

233
00:13:07.720 --> 00:13:12.720
Like I went to the gym to get every day.

234
00:13:14.140 --> 00:13:17.620
What was that pause?
Swole.
Right.
Do I want to get,

235
00:13:17.621 --> 00:13:22.390
I went to the gym to get swole when every day,
so I'm looking at every day,

236
00:13:22.600 --> 00:13:24.770
what do I want to do every day I want to go to the gym everyday.

237
00:13:24.790 --> 00:13:27.670
What is it that I want to do every day at the gym?
I want to get swoll right.

238
00:13:27.910 --> 00:13:30.580
That was a ta.
That was a example I just came up with.
Anyway,

239
00:13:30.910 --> 00:13:32.590
let's just keep going.
I'll just keep going here.

240
00:13:32.710 --> 00:13:37.710
So bi-directional rns are good in this case because the price of Bitcoin,

241
00:13:38.560 --> 00:13:40.900
ideally we have,

242
00:13:40.990 --> 00:13:44.260
we have this list of all the prices over time,
right?

243
00:13:44.500 --> 00:13:48.310
And we can predict future prices that are in the past.
You know what I'm saying?

244
00:13:48.311 --> 00:13:50.980
Like as our network has training in one direction,
right?

245
00:13:51.160 --> 00:13:54.190
We also have in a recurrent network training in the other direction.

246
00:13:54.400 --> 00:13:56.410
And then we can then concatenate there,

247
00:13:56.411 --> 00:14:01.300
learned hidden states together to form a more accurate representation of the,

248
00:14:01.301 --> 00:14:02.530
of the,
of the sequence,
right?

249
00:14:02.770 --> 00:14:06.010
So it's essentially two RNN stacked on top of each other.

250
00:14:06.220 --> 00:14:10.360
The output is then computed based on both the hidden,
both hidden states,

251
00:14:10.510 --> 00:14:15.220
so the hidden states of both rns
and for the Adam optimizer.

252
00:14:15.340 --> 00:14:19.330
What I have here is a graph of the loss functions decreasing over time.

253
00:14:19.570 --> 00:14:24.080
And notice how Adam is the fastest one or the best one of all of them on the M

254
00:14:24.081 --> 00:14:25.540
and ist datasets.
Uh,

255
00:14:25.550 --> 00:14:29.110
basically I have a great video on the differences between all of these types of

256
00:14:29.111 --> 00:14:30.220
activation functions.

257
00:14:30.520 --> 00:14:33.430
Just search which activation function should I use on youtube.

258
00:14:33.580 --> 00:14:35.680
And it will be the first link.
Look,
I'm telling you,

259
00:14:35.681 --> 00:14:39.040
I have made so much content on deep learning,

260
00:14:39.280 --> 00:14:42.160
anything that you would ever want to know,
I have content on it.

261
00:14:42.190 --> 00:14:44.650
So now we're going to initialize this model.
Okay?

262
00:14:44.651 --> 00:14:46.870
So when we come to initializing the model,
like I said,

263
00:14:46.871 --> 00:14:48.910
Ken Ross makes it super simple.
Uh,

264
00:14:48.920 --> 00:14:53.920
bi-directional LSTM recurrent network is a relatively complex a model to me,

265
00:14:55.191 --> 00:14:58.130
but with carrots,
we can do this in just about 10 lines.

266
00:14:58.340 --> 00:15:00.230
We'll initialize it as a sequential model.

267
00:15:00.320 --> 00:15:02.840
What had our first recurrent layer with dropout,

268
00:15:03.220 --> 00:15:05.510
and let me talk about dropout as well after this.

269
00:15:05.690 --> 00:15:08.330
So ad then our second recurrent layer,
right?
You're just,

270
00:15:08.360 --> 00:15:11.120
you're literally naming them like human readable,

271
00:15:11.121 --> 00:15:13.940
bi-directional Lstm with those two parameters.

272
00:15:14.030 --> 00:15:17.480
A third recurrent layer on output layer.
Remember I said that to dense,

273
00:15:17.510 --> 00:15:19.280
fully connected layer comes at the end,

274
00:15:19.640 --> 00:15:22.760
and then our activation function and then our loss function at the very end,

275
00:15:22.761 --> 00:15:25.340
which is going to be Adam.
So why do we use dropout?

276
00:15:25.341 --> 00:15:28.730
So drop out was a technique that was invented by Hinton,

277
00:15:28.731 --> 00:15:31.010
which was one of the godfathers of neural networks.

278
00:15:32.210 --> 00:15:37.130
But basically the idea is that some overfitting is a real problem,

279
00:15:37.131 --> 00:15:40.770
right?
When we have very homogenous data,
it overfitting is a,

280
00:15:40.771 --> 00:15:44.380
is a very big problem,
right?
Because when you learn something,

281
00:15:44.400 --> 00:15:45.770
think about grooves in your brain,
right?

282
00:15:45.771 --> 00:15:48.440
If you're learning the same thing over and over and over again and all your

283
00:15:48.441 --> 00:15:49.310
inputs are the same,

284
00:15:49.520 --> 00:15:52.640
you're going to have these very specific grooves in your brain.

285
00:15:52.730 --> 00:15:54.530
So if you have some kind of new data,

286
00:15:54.650 --> 00:15:56.450
your brain's not going to know how to predict it,
right?

287
00:15:56.451 --> 00:16:00.200
Because it's been trained on such homogeneous data.
So what dropout is,

288
00:16:00.201 --> 00:16:04.430
is essentially it's this technique which randomly turns neurons on and off.

289
00:16:04.610 --> 00:16:09.310
And what this does is it forces the data to create new pathways,
right?

290
00:16:09.360 --> 00:16:12.320
Create new pathways between weights.
In this case,
right?

291
00:16:12.410 --> 00:16:16.010
Between essentially Matrix multiplications waits in each layer.

292
00:16:16.190 --> 00:16:19.610
And because it's creating all of these new pathways randomly,
right?

293
00:16:19.610 --> 00:16:22.790
These nodes are turned on and off.
These neurons are turned on and off.

294
00:16:22.820 --> 00:16:27.070
What happens is the weights become more robust to,
um,

295
00:16:27.170 --> 00:16:30.350
more heterogeneous data,
which means you could give it,
you know,

296
00:16:30.351 --> 00:16:33.260
new data that's not like the training set and it would be better able to

297
00:16:33.261 --> 00:16:36.290
generalize.
So it increases the ability to generalize.

298
00:16:36.291 --> 00:16:39.710
That's why we've added dropout here and we can tune,
drop out on and off.

299
00:16:39.711 --> 00:16:41.870
Like we could say,
you know,
20%,
30%,

300
00:16:41.871 --> 00:16:44.840
40% and it's one of those things where you're just going to kind of,
you know,

301
00:16:45.080 --> 00:16:49.430
trial and error it out and see what works best.
Now,

302
00:16:49.760 --> 00:16:51.080
now we're going to train the model,
right?

303
00:16:51.081 --> 00:16:55.570
So we're going to train it with the batch size of 10,
24 for a hundred epochs.

304
00:16:55.720 --> 00:16:59.510
And if we're going to minimize the loss of its training data using the mean

305
00:16:59.511 --> 00:17:04.040
squared error.
And uh,
so now we can look at this.
So for fitting the model,
we say,

306
00:17:04.070 --> 00:17:07.160
okay,
it's time to start recording and just muddled up fit.
That's it.

307
00:17:07.161 --> 00:17:10.160
And we give it all of those variables that we computed at the very beginning,

308
00:17:10.370 --> 00:17:12.410
the training data,
and then we return the model,
right?

309
00:17:12.411 --> 00:17:15.380
So this is our function for fitting the model right here.

310
00:17:17.060 --> 00:17:18.350
And then we test the model,
right?

311
00:17:18.351 --> 00:17:21.260
We do the same thing for testing the model on the testing data.

312
00:17:21.500 --> 00:17:25.460
The models given x values of testing data and will predict normalize prices,

313
00:17:25.461 --> 00:17:29.330
which is the why,
you know,
underscore,
predict.
And so for testing it,
it's,

314
00:17:29.331 --> 00:17:33.200
you know,
it's relatively simple.
We just say,
test the model x test,

315
00:17:33.310 --> 00:17:37.280
create an empty two d array,
fill the two d array with all the predicted values,

316
00:17:37.281 --> 00:17:41.030
and then plot those predicted values and see,
see what turns out at the end,

317
00:17:41.031 --> 00:17:44.030
right?
So we're still riding these functions.
We're going to,

318
00:17:44.300 --> 00:17:46.940
we're going to actually compute them at this at the end,
right?

319
00:17:47.480 --> 00:17:52.050
Step five is to evaluate the in price so we can plot the models predicted change

320
00:17:52.080 --> 00:17:56.370
in price each day against the real change and each each price daily,
right?

321
00:17:56.371 --> 00:17:59.640
So we can basically model the prediction versus the actual price,

322
00:17:59.910 --> 00:18:03.840
which is the real graph that we want to see in the end to see how good our model

323
00:18:03.841 --> 00:18:08.550
is performing.
Then we processed a percent change in price at the very end,

324
00:18:08.910 --> 00:18:09.960
which is the Delta,
right?

325
00:18:09.961 --> 00:18:14.430
The change between what the price that it was before and the price that it is

326
00:18:14.431 --> 00:18:18.900
now.
And then we compare the prediction to the real data.

327
00:18:18.901 --> 00:18:21.240
So when it,
when,
when,
when all is said and done,

328
00:18:21.390 --> 00:18:24.120
we're going to have some true positives,
false positives,

329
00:18:24.240 --> 00:18:26.280
true negatives and false negatives,
right?

330
00:18:26.380 --> 00:18:31.350
Our model is going to predict the real values sometimes and it's going to

331
00:18:31.351 --> 00:18:32.970
predict the fake value sometimes,
right?

332
00:18:33.150 --> 00:18:35.820
And so we want to know how often it's predicting,

333
00:18:35.850 --> 00:18:39.780
predicting real values and how often it's predicting fake or off values.

334
00:18:39.930 --> 00:18:43.020
And we want to minimize those fake or off values as much as we can.

335
00:18:43.470 --> 00:18:45.570
And so to do that we're going to compete,

336
00:18:45.600 --> 00:18:47.640
we're going to minimize our loss function.
So remember,

337
00:18:47.641 --> 00:18:51.210
recall I said that the loss function that we're going to use here is the mean

338
00:18:51.211 --> 00:18:52.200
squared error,
right?

339
00:18:52.201 --> 00:18:54.930
This is a very popular loss function and this is what it looks like.

340
00:18:55.200 --> 00:18:57.540
It's the mean of the squared error.

341
00:18:57.570 --> 00:19:01.620
So what I mean is we take the value returned by the model f and we subtract the

342
00:19:01.680 --> 00:19:04.530
actual value for the data point,
right?
That is the error,
right?

343
00:19:04.740 --> 00:19:08.640
The prediction minus the real value,
that's our error.
We take that air value,

344
00:19:08.641 --> 00:19:13.500
how far off our model's prediction is and we square it and then we add all of

345
00:19:13.501 --> 00:19:16.290
those values up together for every single data point we have.

346
00:19:16.470 --> 00:19:18.660
And then we divide by the number of data points there are.

347
00:19:18.840 --> 00:19:22.230
And that single value is our mean squared error.

348
00:19:22.380 --> 00:19:26.040
And that is what we want to minimize at every single time step while we're

349
00:19:26.041 --> 00:19:28.800
training.
Okay.
Using the Adam optimization scheme,

350
00:19:28.801 --> 00:19:33.480
which is a variant of gradient descent.
So right?

351
00:19:33.481 --> 00:19:36.270
This is just another graph of true positives versus false positives and

352
00:19:36.271 --> 00:19:37.950
negatives and negatives.
But anyway,

353
00:19:37.980 --> 00:19:41.310
when we take these values and we add them and put them on top of each other,

354
00:19:41.460 --> 00:19:45.210
we get these other three uh,
metrics of how good our model is.

355
00:19:45.211 --> 00:19:48.510
Performing precision.
Recall an f one score.

356
00:19:48.630 --> 00:19:53.070
So position to position means how often is our model getting true positive

357
00:19:53.100 --> 00:19:54.870
compared to how often it returns.

358
00:19:54.870 --> 00:19:59.870
A positive recall is how often does the model get a true positive compared to

359
00:19:59.971 --> 00:20:01.230
how often it should have gotten.

360
00:20:01.231 --> 00:20:04.710
The positive and f one score means a weighted average of both of those two

361
00:20:04.711 --> 00:20:09.390
values,
right?
And so we just compute those right here.
See,

362
00:20:09.391 --> 00:20:10.500
it means squared error right here.

363
00:20:10.530 --> 00:20:14.520
Precision recall f one we literally just took those equations and wrote them out

364
00:20:14.521 --> 00:20:19.140
programmatically using just native python.
So now we can put it all together,

365
00:20:19.141 --> 00:20:20.340
right?
We've,
we've compute,

366
00:20:20.341 --> 00:20:23.160
we've created all of these functions and now we can put it all together.

367
00:20:24.100 --> 00:20:26.970
So our first step is to load up all this data,
which we're doing right here.

368
00:20:27.270 --> 00:20:30.300
And then we initialize the model,
right?
So I've already trained it beforehand.

369
00:20:30.301 --> 00:20:33.870
So we can just look at the results just right here in this Jupiter notebook,

370
00:20:34.770 --> 00:20:38.550
right?
But remember it's a three layer by directional LSTM network.

371
00:20:38.790 --> 00:20:41.040
Then we train the model,
right?
So this,

372
00:20:41.041 --> 00:20:44.280
the model is training by just running that fit model function that we created

373
00:20:44.310 --> 00:20:47.800
earlier.
And then we can test the model.
And so here is the graph.

374
00:20:47.801 --> 00:20:51.350
Here is the good stuff that we have.
We have been wanting to see.
All right,

375
00:20:51.360 --> 00:20:56.360
so this is the bitcoin price over time for a period of over 250 days.

376
00:20:57.430 --> 00:21:00.820
Notice how the real price is different from the predicted price,

377
00:21:00.880 --> 00:21:05.680
but just by very little,
which is kind of exciting,
right?
So our model,

378
00:21:05.770 --> 00:21:08.620
it could be a bit over fit.
Let's not,
let's not kid ourselves.

379
00:21:08.770 --> 00:21:12.880
But as we add more features,
as we add more data to this model,

380
00:21:13.090 --> 00:21:15.760
it will become more robust.
We can implement features,

381
00:21:15.970 --> 00:21:18.100
we can implement techniques like dropout,

382
00:21:18.340 --> 00:21:20.230
we can implement techniques like you know,

383
00:21:20.231 --> 00:21:22.180
all sorts of normalization techniques out there.

384
00:21:22.300 --> 00:21:23.800
And I can go into that in a future video,

385
00:21:23.801 --> 00:21:26.470
but there are all these ways that we can make our model more robust.

386
00:21:26.620 --> 00:21:28.840
I have other videos on predicting stock prices,

387
00:21:28.960 --> 00:21:31.750
which some of the learnings can be applied to here as well.

388
00:21:31.900 --> 00:21:35.320
Search predicting stock prices Saroj on youtube and like three videos will show

389
00:21:35.321 --> 00:21:39.850
up.
And then we have one more really important graph and that is predicting the

390
00:21:39.851 --> 00:21:40.720
percent change.

391
00:21:40.930 --> 00:21:45.430
So what is it predicted present percent change from the previous day to the

392
00:21:45.430 --> 00:21:47.050
current day versus a real percent change.

393
00:21:47.200 --> 00:21:50.860
And notice how there is a bit more variance here between the real values and the

394
00:21:50.861 --> 00:21:54.680
predicted value.
So we can,
we can definitely do better here.
Uh,

395
00:21:54.970 --> 00:21:58.450
and then we compare the predictions and the true data and then we get our

396
00:21:58.451 --> 00:22:02.500
statistics at the very,
very end position.
Recall F1 and mean squared error.

397
00:22:02.950 --> 00:22:05.500
Please subscribe for more programming videos.
And for now,

398
00:22:05.530 --> 00:22:08.080
I've got to predict the future.
So thanks for watching.

