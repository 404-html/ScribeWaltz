WEBVTT

1
00:00:05.010 --> 00:00:05.701
Hello world.

2
00:00:05.701 --> 00:00:09.390
It's Saroj and in this episode we're going to talk about tents or board tents.

3
00:00:09.420 --> 00:00:13.410
Board is a data visualization tool that comes packaged with tensorflow.

4
00:00:13.470 --> 00:00:17.250
Tensorflow programs can range from super simple like an addition problem,

5
00:00:17.400 --> 00:00:22.020
two super complex using thousands of computations and they all have two basic

6
00:00:22.021 --> 00:00:24.870
components,
operations and tensors.

7
00:00:25.080 --> 00:00:28.740
The idea is that you create a model that consists of a set of operations,

8
00:00:28.980 --> 00:00:31.170
feed data or tensors into the model,

9
00:00:31.350 --> 00:00:35.310
and the tensions will flow between operations until you get an output tensor,

10
00:00:35.340 --> 00:00:36.270
which is your result.

11
00:00:36.360 --> 00:00:40.260
Tenser board was created as a way to help you understand the flow of tensors in

12
00:00:40.261 --> 00:00:42.720
your model so that you can debug and optimize it,

13
00:00:42.810 --> 00:00:46.440
but we're going to visualize a classifier that can recognize handwritten digits.

14
00:00:46.460 --> 00:00:49.320
In tenser board we use two examples of the classifier.

15
00:00:49.350 --> 00:00:53.400
What is a simple version in 60 lines of python and the other is a more complex

16
00:00:53.401 --> 00:00:55.530
version path.
Don,
I love you.

17
00:00:56.010 --> 00:00:58.350
Don't worry if you don't understand every line of code.

18
00:00:58.351 --> 00:01:01.740
We just want to get a general sense of what's happening under the hood so that

19
00:01:01.741 --> 00:01:06.510
we can later understand the tensor board visualizations.
Here we go.

20
00:01:06.540 --> 00:01:09.000
We'll start by importing tensor flow and our input data class,

21
00:01:09.001 --> 00:01:10.680
which pulls our training data from the web.

22
00:01:10.740 --> 00:01:14.100
Then we'll define two helper functions and it weights were returning a variable

23
00:01:14.101 --> 00:01:17.430
that contains randomly generated wait values along a normal distribution.

24
00:01:17.460 --> 00:01:18.450
Our second helper function,

25
00:01:18.451 --> 00:01:20.940
we'll create our model a three layer feedforward neural network.

26
00:01:20.970 --> 00:01:24.120
We're going to create three layers and to find them via the name scope function.

27
00:01:24.150 --> 00:01:27.760
The named scope function creates high level namespaces for operations in the

28
00:01:27.761 --> 00:01:31.000
tensor flow graph that we will later visualize and each layer we'll run a

29
00:01:31.001 --> 00:01:33.870
dropout function and an activation function and dropout is a function that

30
00:01:33.871 --> 00:01:36.960
forces our neural network to learn several different representations of
patterns.

31
00:01:36.960 --> 00:01:39.690
To that generalization improves the activation function,

32
00:01:39.691 --> 00:01:41.280
creates probabilities out of numbers.

33
00:01:41.310 --> 00:01:42.960
So now that we have our two helper functions,

34
00:01:42.990 --> 00:01:46.050
we can create variables to store our training and testing images as well as our

35
00:01:46.051 --> 00:01:47.280
training and testing labels.

36
00:01:47.310 --> 00:01:50.070
Next we can create input and output placeholders for our data.

37
00:01:50.130 --> 00:01:52.890
Then we'll initialize our weights between each of the three layers using the

38
00:01:52.891 --> 00:01:54.120
function we defined earlier.

39
00:01:54.150 --> 00:01:57.370
We'll define histogram summaries for each of our weights so we can visualize the

40
00:01:57.371 --> 00:01:59.280
distribution of weights later intenser board.

41
00:01:59.310 --> 00:02:02.970
The next step is to add dropout placeholders to are hidden and input layers.

42
00:02:03.030 --> 00:02:05.520
We can then create our model using the variables we've created.

43
00:02:05.550 --> 00:02:07.890
Now that we have our model,
we'll create our cost function.

44
00:02:07.920 --> 00:02:11.190
The cost function is a measure of how good a neural net is with respect to its

45
00:02:11.191 --> 00:02:14.520
given training sample and the expected output.
We want it to decrease over time.

46
00:02:14.580 --> 00:02:16.230
We'll also measure the accuracy of the network.

47
00:02:16.231 --> 00:02:19.110
We want that to increase over time.
Well later.
Visualize both.
Next,

48
00:02:19.111 --> 00:02:21.270
we'll create a session to run our graph computation.

49
00:02:21.300 --> 00:02:24.330
In our session we'll save our model using the summary Writer.
Lastly,

50
00:02:24.360 --> 00:02:27.480
we'll initialize our variables for training and train our model.
That's it.

51
00:02:27.510 --> 00:02:31.230
I know that was really fast.
Unlike series comprehension,
you heard me apple.

52
00:02:31.410 --> 00:02:34.890
Let's visualize that code intense or board and I'll explain it.
More.

53
00:02:34.950 --> 00:02:38.220
Summary operations or how it tends to board acquires data from our tensor flow

54
00:02:38.221 --> 00:02:40.770
runs.
They are functions like TF dot matrix multiply.

55
00:02:40.800 --> 00:02:42.480
We created a number of them in our code here.

56
00:02:42.481 --> 00:02:45.900
In the events tab we can view our scalar summaries,
accuracy and cost.

57
00:02:45.960 --> 00:02:49.320
The x axis shows a time timestamps and the y axis shows the accuracy measure or

58
00:02:49.321 --> 00:02:50.760
the percentage of correct predictions.

59
00:02:50.790 --> 00:02:53.880
Over time we can blow up our graph for a closer look or view a wider range of

60
00:02:53.881 --> 00:02:55.620
data points by expanding the y axis.

61
00:02:55.650 --> 00:02:58.660
We can also drag a rectangle on our graph to zoom in on a certain region if we

62
00:02:58.661 --> 00:03:01.210
like and double click to zoom out.
If we mouse over the chart,

63
00:03:01.240 --> 00:03:03.190
it will produce crosshairs with data values.

64
00:03:03.250 --> 00:03:06.790
We can change how smooth our line is by adjusting the slider.

65
00:03:06.870 --> 00:03:08.330
Istep option shows time steps.

66
00:03:08.350 --> 00:03:11.200
The relative option shows the time relative to the first data point.

67
00:03:11.201 --> 00:03:14.440
That means the number of minutes or hours since the training run was started and

68
00:03:14.441 --> 00:03:16.780
the wall option shows the actual time to runs happen.

69
00:03:16.840 --> 00:03:18.700
We can create more tags as well in the sidebar.

70
00:03:18.730 --> 00:03:21.970
It can either be an entirely new tag or tag that will group a bunch of existing

71
00:03:21.971 --> 00:03:23.620
summaries together into a new category.

72
00:03:23.650 --> 00:03:26.650
Let's switch to a more complex version of our handwritten character classifier

73
00:03:26.651 --> 00:03:29.320
example.
With even more scalar summaries,
we can type in,

74
00:03:29.350 --> 00:03:32.920
it's over 9,000 and since no summary contains those terms,

75
00:03:32.921 --> 00:03:35.410
it becomes a new category.
But if we look under Max and mean,

76
00:03:35.440 --> 00:03:38.440
they both contain summaries for biases.
So if we type in biases,

77
00:03:38.500 --> 00:03:41.740
it'll create a new category that contains both biases from these two existing

78
00:03:41.741 --> 00:03:44.830
categories.
We can also download our graphs in CSV or Jason Format.

79
00:03:44.840 --> 00:03:48.250
For this more complex example,
we have lines for both training and testing data.

80
00:03:48.280 --> 00:03:50.410
That way we can compare runs.
Let's move on to images.

81
00:03:50.440 --> 00:03:51.610
We created an image summary.

82
00:03:51.640 --> 00:03:54.880
This lets us view the images in both the training and testing folder.
Okay,

83
00:03:54.881 --> 00:03:57.820
let's switch back to our simple classifier and move on to the graphs tab which

84
00:03:57.821 --> 00:03:59.200
lets us inspect our TF model.

85
00:03:59.230 --> 00:04:01.960
We can see each of the three layers we created via the name scope function.

86
00:04:02.020 --> 00:04:04.600
Let's double click on one of our layer namespaces to expand it.

87
00:04:04.601 --> 00:04:07.540
The first thing that pops out is the dropout function which we declared inside

88
00:04:07.541 --> 00:04:10.150
of the scope as well as our relu and Matrix multiply functions.

89
00:04:10.180 --> 00:04:13.090
Notice the arrows from both notes pointing to the cost operation.

90
00:04:13.120 --> 00:04:16.330
The direction of the arrows chose the direction that the tensors are flowing are

91
00:04:16.331 --> 00:04:19.960
two placeholder operation notes x and p keep input hidden which is used for the

92
00:04:19.961 --> 00:04:22.510
dropout function service entry points for our tensors.

93
00:04:22.570 --> 00:04:25.240
They move through each of the three layers through the weights into the cost

94
00:04:25.241 --> 00:04:28.480
function every time eventually to the accuracy function and finally to the

95
00:04:28.481 --> 00:04:31.600
output placeholder that gives us a prediction to reduce clutter.

96
00:04:31.630 --> 00:04:35.110
Tensor flow automatically shows nodes with many connections to other nodes in

97
00:04:35.111 --> 00:04:36.730
their own area to view and detail.

98
00:04:36.760 --> 00:04:39.610
If we wanted to we can add them back to the main graph by clicking the add to

99
00:04:39.611 --> 00:04:43.060
main graph button and the detail car Yo um,
bringing it back.

100
00:04:43.090 --> 00:04:44.320
Let's take a look at the sidebar.

101
00:04:44.350 --> 00:04:47.290
We can choose which one we want to display trainer test and show the graph at

102
00:04:47.291 --> 00:04:47.950
each time step.

103
00:04:47.950 --> 00:04:51.280
The color option lets us see at each time step what structures are being used,

104
00:04:51.281 --> 00:04:55.120
which device each operation is running on compute time along a scale and memory

105
00:04:55.121 --> 00:04:58.360
usage.
We can also manually upload a safe tensor flow graph right from the Ui.

106
00:04:58.361 --> 00:05:00.100
If we'd like.
Let's move on to distributions.

107
00:05:00.130 --> 00:05:03.010
We created histogram summaries for all three of our weights between layers and

108
00:05:03.011 --> 00:05:04.810
we can view the distribution of each weight here.

109
00:05:04.840 --> 00:05:06.490
Let's switch to our complex classifier.

110
00:05:06.550 --> 00:05:09.700
The light part shows all the weights across time and the shaded parts shows

111
00:05:09.701 --> 00:05:12.130
those weights that are actually activated during training.

112
00:05:12.160 --> 00:05:15.160
These curves represent percentiles like the Max and mean and median.

113
00:05:15.190 --> 00:05:18.370
The histogram plot allows us to plot any variables we like from our graph.

114
00:05:18.400 --> 00:05:20.980
It's showing how the values of our weights change with training isn't it

115
00:05:21.010 --> 00:05:21.640
beautiful?

116
00:05:21.640 --> 00:05:25.390
Tenser board lets you visualize your data so you can debug and optimize it.

117
00:05:25.420 --> 00:05:28.540
You can create summary operations in your tensorflow program,

118
00:05:28.570 --> 00:05:31.240
which takes tensors as inputs and produces outputs.

119
00:05:31.270 --> 00:05:34.390
Tenser board reads these summaries and displays them visually.

120
00:05:34.450 --> 00:05:38.230
The challenge for this video is to create a tensorflow program that visualizes

121
00:05:38.231 --> 00:05:41.950
audio data in some way and winter.
It gets a shout out for me in two episodes,

122
00:05:41.980 --> 00:05:45.450
more info and links in the description.
Please subscribe for more videos,

123
00:05:45.460 --> 00:05:48.760
and for now I've got to go get some sunlight,
so thanks for watching.

