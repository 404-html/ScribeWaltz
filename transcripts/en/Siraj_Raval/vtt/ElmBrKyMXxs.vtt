WEBVTT

1
00:00:00.150 --> 00:00:04.650
Okay.
When,
when we,
when I go to one,
we're going to start.
Okay.
Cool.
Five,

2
00:00:05.280 --> 00:00:08.460
four,
three,
two,
one.

3
00:00:11.130 --> 00:00:14.250
<v 1>Hello world.
It's Raj.
Welcome to this live session.</v>

4
00:00:14.670 --> 00:00:19.500
Today we're going to build a sequence to sequence model in tensorflow.

5
00:00:19.740 --> 00:00:24.240
And so,
uh,
let me mute this.
Okay,

6
00:00:24.480 --> 00:00:27.780
so we're going to build a sequence,
a sequence model in tensorflow.
Oh my God,

7
00:00:27.781 --> 00:00:31.590
there's so many.
There we go.
Just muting everything we are going to build.

8
00:00:31.591 --> 00:00:32.424
Let me say that again.

9
00:00:32.430 --> 00:00:35.940
We're going to build a sequence to sequence model in tensorflow.

10
00:00:36.000 --> 00:00:40.380
Are we going to do it for a chat bot?
No.
Are we going to do it for a,

11
00:00:40.720 --> 00:00:44.910
uh,
translation system?
No.
Are we going to do it for a,

12
00:00:44.911 --> 00:00:49.410
anything useful?
No.
The point of this is to learn the architecture.

13
00:00:49.470 --> 00:00:50.520
So that's what we're focused on.

14
00:00:50.521 --> 00:00:53.250
We're going to focus on learning the architecture.

15
00:00:53.251 --> 00:00:56.520
It's not about the application this time and I'm rolling up my sleeves because

16
00:00:57.060 --> 00:01:00.750
it's just cool.
It doesn't actually have anything to do with anything,
but uh,

17
00:01:00.770 --> 00:01:05.700
so that's what I'm doing right now and okay.
So that's what we're going to do.

18
00:01:05.701 --> 00:01:08.010
So we're going to have some toy data and you can see right here,

19
00:01:08.011 --> 00:01:10.320
this is the output.
This is what the output should look like.

20
00:01:10.321 --> 00:01:12.990
So we're going to give it some sequence of inputs.

21
00:01:15.770 --> 00:01:19.970
We're going to give it some secrets of inputs like this.
Let me make that bigger.

22
00:01:20.240 --> 00:01:23.120
And it's going to be like that.
That could be a sequence four,
eight,

23
00:01:23.121 --> 00:01:25.310
five could be.
And then the end is just padding.

24
00:01:25.311 --> 00:01:29.560
We just add zeroes at the end randomly.
Okay.
And,
uh,

25
00:01:29.720 --> 00:01:32.150
then we try to predict that same sequence.
That's,

26
00:01:32.151 --> 00:01:36.080
that's the same sequence we want to predict.
So we give it an input sequence,

27
00:01:36.260 --> 00:01:37.220
we encode it,

28
00:01:37.430 --> 00:01:42.320
and then we decode it and then we compare that decoded output to that initial

29
00:01:42.321 --> 00:01:45.770
sequence.
So it's actually going to be different.
And I'm going to show you why,

30
00:01:45.980 --> 00:01:50.500
right?
This is,
this is an example of how memory works in a sequence.

31
00:01:50.501 --> 00:01:53.840
A sequence model,
okay.
Uh,

32
00:01:53.990 --> 00:01:58.820
so as you can see,
it starts off bad,
but eventually at the very,

33
00:01:58.821 --> 00:02:02.860
very end,
the predicted output is the same as the,
uh,

34
00:02:02.930 --> 00:02:06.910
initial input.
Okay?
So,
uh,

35
00:02:06.980 --> 00:02:10.400
that's what we're going to do.
Are going to try to predict that same.

36
00:02:10.401 --> 00:02:14.810
It puts input sequence and in the process,
learn how memory works in a sequence,

37
00:02:14.811 --> 00:02:18.020
a sequence model.
Okay?
So that's what we're going to do.

38
00:02:18.021 --> 00:02:22.310
And the first question is,
well,
what type of,
uh,

39
00:02:22.370 --> 00:02:26.650
this is not an auto encoder,
but what it is is a,
uh,

40
00:02:26.720 --> 00:02:29.840
bi-directional encoder decoder architecture.

41
00:02:29.960 --> 00:02:32.150
So there's the simple encoder decoder architecture,

42
00:02:32.151 --> 00:02:36.770
which we can look at and it looks like this encoder decoder architecture.

43
00:02:37.010 --> 00:02:40.160
And you know,
a lot of initial papers use this.

44
00:02:40.161 --> 00:02:44.150
They use an initial encoder decoder architecture where the,

45
00:02:46.660 --> 00:02:50.260
are you kidding me?
Google?
This is your stuff.
Google.
So it,

46
00:02:50.261 --> 00:02:51.400
it just looked like this,
right?

47
00:02:51.401 --> 00:02:56.110
So you would have a values go one way in the encoder and then they would decode

48
00:02:56.111 --> 00:02:56.944
the other way.

49
00:02:56.950 --> 00:03:00.370
But what we're gonna do is we're going to do an improvement that we're going to

50
00:03:00.371 --> 00:03:03.220
add.
We're going to make the encoder by directional.

51
00:03:03.221 --> 00:03:06.190
Now recall that we talked about this in the language translator,

52
00:03:06.430 --> 00:03:08.620
translator video,
and

53
00:03:10.450 --> 00:03:14.740
we talked about how having a bi-directional encoder gives you the full context

54
00:03:14.741 --> 00:03:16.420
of that input sequence.
You could,
you could,

55
00:03:16.421 --> 00:03:20.260
you could talk about both the future and the past.
So,
so that's,

56
00:03:20.290 --> 00:03:23.620
that's why we're using a bi-directional encoder and uh,

57
00:03:25.270 --> 00:03:28.810
yeah,
that's what we're doing.
And uh,
yeah.
So let's,

58
00:03:28.811 --> 00:03:32.680
let's start building this thing,
shall we?
So let's just drive right into it.
Uh,

59
00:03:32.710 --> 00:03:36.430
but first let me answer two minutes worth of questions and we're going to answer

60
00:03:36.431 --> 00:03:40.210
questions every,
at 15 minute intervals.
So any questions you guys have?
Yes,

61
00:03:40.211 --> 00:03:43.450
I have a new hairstyle,
machine learning deep learning questions.

62
00:03:44.080 --> 00:03:45.760
I finally got time to cut my hair.

63
00:03:45.761 --> 00:03:49.810
I know time is very hard to come by these days,
but that's the way I like it.

64
00:03:50.140 --> 00:03:54.250
What is the best way to identify if a feature in a model is statistically

65
00:03:54.251 --> 00:03:58.180
significant or not for inputs a,
B,
c and output?
Eat?

66
00:03:58.181 --> 00:04:01.960
What is the best way to tell which inputs most contribute to eat?

67
00:04:01.990 --> 00:04:05.080
That is a great question.
So before deep learning,

68
00:04:05.081 --> 00:04:09.460
we had to manually pick what those best features were because we didn't have a

69
00:04:09.461 --> 00:04:12.940
way for our model to learn what is the ideal features to use.

70
00:04:13.060 --> 00:04:16.840
So that's a whole field of engineering that existed before deep learning,

71
00:04:16.990 --> 00:04:19.420
which was called feature engineering.
And the,

72
00:04:19.660 --> 00:04:24.190
the ideal way to pick that was thinking about what features you personally would

73
00:04:24.191 --> 00:04:26.340
use when trying to predict some outputs.

74
00:04:26.340 --> 00:04:30.580
So if it was a classifier and you're trying and you're trying to classify a dog,

75
00:04:30.760 --> 00:04:34.240
you would pick,
well what would I need to know to classify a dog?
Well,

76
00:04:34.360 --> 00:04:37.690
I would look at,
it's what type of dog it is I would look at,

77
00:04:37.691 --> 00:04:39.820
it's the color of its for and you know,

78
00:04:39.821 --> 00:04:41.650
the size of its ears and things like that.

79
00:04:41.651 --> 00:04:43.480
So that's a good rule of thumb to go by.

80
00:04:43.690 --> 00:04:46.750
What are the features you personally would look for?
Uh,

81
00:04:46.751 --> 00:04:51.070
when trying to classify or do another task.
Okay.
Uh,

82
00:04:52.330 --> 00:04:55.540
is this also in line for you,
Udacity nanodegree DL?
Yes.

83
00:04:55.541 --> 00:04:59.560
This is a part of the university nanodegree and these videos are public for you

84
00:04:59.561 --> 00:05:02.710
guys.
Okay.
How about predicting the alphabet?

85
00:05:02.711 --> 00:05:06.340
I have a little snippet that generates a dataset for your previous sequence.

86
00:05:06.341 --> 00:05:07.540
A sequence code.

87
00:05:07.690 --> 00:05:11.860
You can absolutely predict the alphabet that is a 25 character long sequence.

88
00:05:11.890 --> 00:05:15.880
Okay.
And this,
this exact code can be used to predict the alphabet.
Okay.

89
00:05:15.881 --> 00:05:20.250
You can apply this exact code.
Well you would just switch out the data,
but this,

90
00:05:20.290 --> 00:05:23.050
this can be applied to more questions and then we're going to get started with

91
00:05:23.051 --> 00:05:27.690
the code.
Does this model used gr use?
No,
it does not.
But we will use gru.

92
00:05:27.780 --> 00:05:32.780
Gru are being used more and more and I'm not going to say they are objectively

93
00:05:32.861 --> 00:05:33.960
better than Lstm,

94
00:05:34.030 --> 00:05:39.030
but they are turning out to have better outcomes than LSTs in a lot of cases.

95
00:05:39.101 --> 00:05:42.720
Yes.
One more question.
Would you work on neural links?

96
00:05:42.721 --> 00:05:44.380
So I'm very excited about the idea.

97
00:05:44.381 --> 00:05:48.100
We need that to happen for us to keep up with Ai,
for us to merge with it.

98
00:05:48.101 --> 00:05:52.660
That's the goal for us to become the gods and not have,
it becomes some,
you know,

99
00:05:53.350 --> 00:05:57.530
runaway God,
we want to merge with it.
Right.
And become amazing.
So yes,
I would,

100
00:05:57.531 --> 00:06:01.250
I work with them.
No,
because I'm focused on making content for you guys.

101
00:06:01.280 --> 00:06:04.160
So that's,
that's what I'm trying to do and no one else can do what I'm doing.

102
00:06:04.161 --> 00:06:06.980
So that's why I'm doing well in the way that I deal with my personality and

103
00:06:06.981 --> 00:06:10.070
stuff.
So that's what,
that's what my job is.
That's what I'm focused on.

104
00:06:10.130 --> 00:06:13.310
But I will collaborate with neuro link.
I will collaborate with open AI,

105
00:06:13.311 --> 00:06:15.650
I will collaborate with Google.
That's it for questions.

106
00:06:15.651 --> 00:06:18.890
So let's get started with the code.
Okay.
So the first thing we're going to do

107
00:06:20.510 --> 00:06:25.340
is we're going to import our dependencies.
So dependencies,
time.

108
00:06:25.680 --> 00:06:28.370
But see,
and let me know if this is not big enough,
right?

109
00:06:28.371 --> 00:06:31.210
Let me know if the text is not big enough.
Okay.

110
00:06:31.490 --> 00:06:32.323
<v 2>Uh,</v>

111
00:06:34.290 --> 00:06:35.930
<v 1>so,
so these are our dependencies.</v>

112
00:06:35.931 --> 00:06:39.270
So the first thing we're gonna do is we're going to import num Pi to do our

113
00:06:39.300 --> 00:06:43.020
matrix math.
And by the way,
the code is in the description.
Check it out,

114
00:06:43.021 --> 00:06:46.020
follow along as I'm writing this.
Okay.
And I'm going to explain it.

115
00:06:46.021 --> 00:06:50.040
There's a lot of comments.
There's a lot of documentation for this code.

116
00:06:50.041 --> 00:06:54.420
I'd really try to document it as best that I could.
Uh,
they're both okay.

117
00:06:54.421 --> 00:06:59.400
So in markdown and in the comments.
So we're going to,
uh,
important and pie.

118
00:06:59.401 --> 00:07:01.170
And then of course,
tensor flow for ml.

119
00:07:01.680 --> 00:07:05.900
And then we're going to have our helpers are helper is just,
it's,

120
00:07:05.910 --> 00:07:08.730
it's one class and it had only has two functions.

121
00:07:08.910 --> 00:07:12.180
Those functions are formatting the data and,
uh,

122
00:07:12.210 --> 00:07:16.290
generating the random sequence of data generating,
sequenced,
sequential,

123
00:07:16.770 --> 00:07:20.670
generating random sequence data.
And we'll talk about what that data looks like.

124
00:07:21.420 --> 00:07:22.253
Okay?

125
00:07:23.350 --> 00:07:24.183
<v 2>Yeah.</v>

126
00:07:24.300 --> 00:07:28.830
<v 1>So those are our dependencies.
So now what we're going to do is,
oh,</v>

127
00:07:28.980 --> 00:07:31.210
there's one more thing.
We want to,
uh,

128
00:07:31.380 --> 00:07:35.640
run this reset default graph function,

129
00:07:36.300 --> 00:07:39.330
which is going to clear the default graph stack and it's going to reset the

130
00:07:39.331 --> 00:07:40.500
global default graph.

131
00:07:40.530 --> 00:07:43.470
It's just the one of those initial steps that we just have to do.

132
00:07:43.620 --> 00:07:45.240
And we normally wouldn't have to do this,

133
00:07:45.241 --> 00:07:47.430
but we're going to do this and tensorflow,
because like,

134
00:07:47.431 --> 00:07:49.350
if we had multiple graphs,
for some reason,

135
00:07:49.351 --> 00:07:52.290
this would be a good thing if we wanted to clear our cash,

136
00:07:52.650 --> 00:07:55.600
but we're just doing it because it's,
you know,
uh,

137
00:07:56.490 --> 00:08:00.150
it's a good introductory step.
And of course we want to activate our session.

138
00:08:00.240 --> 00:08:03.420
Okay?
We're going to do this at the very beginning.
Um,
and,
uh,

139
00:08:03.450 --> 00:08:07.590
the version of tensorflow that I'm using is 1.0,
okay.

140
00:08:09.240 --> 00:08:12.790
<v 2>Hold on.
Nope.
Do you have.virgin?</v>

141
00:08:14.080 --> 00:08:16.690
<v 1>Okay,
so that's the version of tensorflow.</v>

142
00:08:19.780 --> 00:08:22.560
That's not spam guys.
Okay,
so here we go with this.

143
00:08:22.590 --> 00:08:26.310
So let's start off by defining the vocabulary size.

144
00:08:26.311 --> 00:08:29.670
So recall that for encoder decoder architecture,

145
00:08:29.880 --> 00:08:32.850
we have to have a fixed size input vector.

146
00:08:33.030 --> 00:08:35.340
Now sequences are variable length.

147
00:08:35.341 --> 00:08:38.010
Like how are you versus how are you doing today?

148
00:08:38.190 --> 00:08:41.340
One is only four characters long,
but one is five characters.

149
00:08:41.490 --> 00:08:44.760
So how do we solve this?
Well,
we add padding to the end.

150
00:08:45.000 --> 00:08:49.290
What we do is we add zeros to the end.
So you know,
one sequence could be this,

151
00:08:49.470 --> 00:08:52.860
you know,
one sequence could be like this,
the next sequence could be this,

152
00:08:52.950 --> 00:08:54.900
but they have to be the same length.
So what do we do?

153
00:08:55.080 --> 00:08:57.810
We add zeros just like this.
So they are the same length.

154
00:08:57.811 --> 00:09:01.470
So that's what we're doing right now.
Okay?
350 people live.

155
00:09:01.530 --> 00:09:04.260
That's the way I like it.
Okay,
so let's define,

156
00:09:04.261 --> 00:09:07.230
our pad has zero and our end of sentence as one.

157
00:09:07.350 --> 00:09:12.350
So Eos stands for end of sentence and it's a token that specifies for our model

158
00:09:12.540 --> 00:09:14.550
when our sentence ends.
Okay.

159
00:09:15.360 --> 00:09:19.800
And now we're going to define our vocabulary size.
So you know,
that is the,

160
00:09:19.830 --> 00:09:20.990
we are predicting what are these,

161
00:09:21.010 --> 00:09:24.180
what is the Max Max length for that input sequence.

162
00:09:24.181 --> 00:09:25.590
So we're going to say it's going to be 10.

163
00:09:25.800 --> 00:09:30.570
So in this case it's going to be 10 because we're generating toy data.
But uh,

164
00:09:31.570 --> 00:09:31.800
<v 2>okay.</v>

165
00:09:31.800 --> 00:09:34.740
<v 1>Uh,
in other cases this can be very,
very long.
Okay.</v>

166
00:09:34.741 --> 00:09:38.340
So then we're going to define our input embedding size.

167
00:09:39.210 --> 00:09:40.043
<v 2>Uh,</v>

168
00:09:41.450 --> 00:09:44.360
<v 1>we're just going to be 20,
which is the length of the characters.</v>

169
00:09:45.560 --> 00:09:46.160
<v 2>Okay.</v>

170
00:09:46.160 --> 00:09:50.600
<v 1>And so vocab sizes,
the words and then character length.
And then the,
uh,</v>

171
00:09:50.990 --> 00:09:53.030
and then input in bedding size is the,

172
00:09:53.210 --> 00:09:56.630
we take that input sequence and then we converted to a vector.

173
00:09:56.780 --> 00:09:59.510
And that vector is what we've then feed into the model.

174
00:09:59.511 --> 00:10:01.980
It's not that we just feed this raw,
um,

175
00:10:02.090 --> 00:10:06.080
these raw words directly and we have to convert them to vectors and embedding

176
00:10:06.081 --> 00:10:10.880
vector.
It means the same thing.
And we're going to talk about that.
Okay.
Okay.

177
00:10:10.881 --> 00:10:15.470
So that's it for that.
So the next step is for us to define our hidden units.

178
00:10:15.500 --> 00:10:20.000
So let's,
let's define these hidden units and then talk about what they are.
So

179
00:10:21.530 --> 00:10:24.590
in Kotor,
hidden units and Dakota hidden units,
we have 20 of these.

180
00:10:24.830 --> 00:10:28.310
And for our decoder,
we're going to say we want 20 times two.

181
00:10:28.790 --> 00:10:31.130
So why are we saying 20 times to let's,

182
00:10:31.131 --> 00:10:34.490
let's talk about why we are saying 20 times two.

183
00:10:35.060 --> 00:10:37.970
In the original paper they had a thousand units.

184
00:10:37.971 --> 00:10:41.210
So in the original paper Bites Sutzkever,
uh,
for sequence,

185
00:10:41.211 --> 00:10:43.160
the sequence model where they introduced this model,

186
00:10:43.280 --> 00:10:44.600
the youth a thousand for them,

187
00:10:44.601 --> 00:10:48.410
they use a thousand for both the encoder and then a thousand hidden units for

188
00:10:48.411 --> 00:10:50.870
the decoder.
And generally that's what we want.

189
00:10:50.871 --> 00:10:54.650
We want them to be the same number of hidden units.
But in this case,

190
00:10:54.651 --> 00:10:59.240
I'm saying the decoder is going to be double the size of the encoder.
Why?
Well,

191
00:10:59.630 --> 00:11:01.640
we want it to be a little different.

192
00:11:01.641 --> 00:11:03.650
We want that output value to be a little different.

193
00:11:03.950 --> 00:11:05.900
It's just going to be the same as the input,
right?

194
00:11:05.901 --> 00:11:07.250
And we want it to change a little bit.

195
00:11:07.340 --> 00:11:09.410
So adding hidden units is going to change ever.

196
00:11:09.411 --> 00:11:13.100
So slightly our output so that we can then,
um,
minimize it.
Right?

197
00:11:13.250 --> 00:11:17.120
So that's what we're having it be multiplying by two.

198
00:11:18.800 --> 00:11:19.633
So,

199
00:11:20.840 --> 00:11:21.330
<v 2>okay.</v>

200
00:11:21.330 --> 00:11:22.860
<v 1>Yes.
Great.</v>

201
00:11:25.490 --> 00:11:25.800
<v 2>Okay.</v>

202
00:11:25.800 --> 00:11:29.220
<v 1>So now we're going to define our placeholder.
So placeholders,
time,</v>

203
00:11:29.490 --> 00:11:33.240
placeholders are,
shout it out.
What are they?
They are gateways.

204
00:11:33.241 --> 00:11:36.270
They are gateways for data into our computation graph.

205
00:11:36.810 --> 00:11:38.370
These are the gateways and they are,

206
00:11:39.060 --> 00:11:42.330
they are primitives and tensorflow and they are,

207
00:11:43.550 --> 00:11:44.383
<v 2>mmm,</v>

208
00:11:45.870 --> 00:11:47.400
<v 1>we always have to use them.
Right?</v>

209
00:11:47.401 --> 00:11:49.980
And what we're gonna do is we're going to have three placeholders.

210
00:11:50.190 --> 00:11:54.940
We're going to have placeholders for our inputs,
uh,
our decoder and the targets.

211
00:11:55.270 --> 00:12:00.040
Okay.
So then we're going to define a couple of parameters,
uh,
the size of these.

212
00:12:00.130 --> 00:12:04.120
And then we're going to name them as well because these are names for our

213
00:12:04.121 --> 00:12:05.050
computation graphs.

214
00:12:05.051 --> 00:12:09.400
So we're going to actually talk about a tensor board next live session.
Oops.

215
00:12:09.401 --> 00:12:10.240
I just spilled something.

216
00:12:10.241 --> 00:12:15.241
So this is a good way for us to differentiate between different primitives in

217
00:12:15.940 --> 00:12:17.500
tensorflow.
That's why we had names to it.

218
00:12:17.980 --> 00:12:22.390
And so then we're going to have inputs length,

219
00:12:23.880 --> 00:12:27.480
the length of the inputs.
So actually that's the next place holder,

220
00:12:27.481 --> 00:12:31.920
the length of the inputs.
So we have our inputs,
the length of them,
and then the,

221
00:12:37.190 --> 00:12:40.700
the decoder targets.
So then we have our shape.

222
00:12:41.090 --> 00:12:42.440
The shape is going to be none.

223
00:12:42.950 --> 00:12:47.120
And then we have our data type.

224
00:12:47.690 --> 00:12:51.500
Okay.
So we want them to all be the same data type because that,

225
00:12:51.501 --> 00:12:53.960
that's what we're operating in.
And if we didn't do that,

226
00:12:54.140 --> 00:12:56.840
then we probably have some kind of errors that start popping up.
Okay.

227
00:12:57.170 --> 00:13:01.310
So it's 32 d type is in 32.
And then,
uh,

228
00:13:01.340 --> 00:13:06.340
the name is going to be a encoder inputs encoder inputs length.

229
00:13:11.000 --> 00:13:13.790
Okay.
And then we're going to have,

230
00:13:16.990 --> 00:13:21.310
okay,
so input or
length,

231
00:13:21.370 --> 00:13:26.120
and then we actually have one more.
Uh,

232
00:13:28.890 --> 00:13:33.330
no.
Yeah,
that's fine.
Uh,
Dakota targets.
So Dakota targets are going to be,

233
00:13:33.620 --> 00:13:38.340
you have to a placeholder shape and then none.

234
00:13:38.400 --> 00:13:41.970
None.
Because there are empty.
Right now we,
the,
the uh,

235
00:13:41.971 --> 00:13:45.180
we're going to define them later in a second.
Okay.
Data type.

236
00:13:45.810 --> 00:13:48.240
I do think a Pi torch is going to be hot.

237
00:13:48.241 --> 00:13:52.760
So data type is going to be tf.in 32.
And uh,

238
00:13:53.950 --> 00:13:56.430
got to remember to answer questions after this.

239
00:13:56.431 --> 00:14:01.431
And then finally our a decoder targets.

240
00:14:01.710 --> 00:14:05.310
Okay,
so those are that.
So we're going to get an error right here.
So the,

241
00:14:05.340 --> 00:14:09.660
I knew it.
So it's for this encoder inputs.
Uh,
let's see here.

242
00:14:09.661 --> 00:14:14.640
In Bod Syntax,
let's debug this.
So we have Incode are inputs and then we have,

243
00:14:14.670 --> 00:14:17.460
it's going to be a tensorflow placeholder.
Let's see the whole thing here.

244
00:14:17.610 --> 00:14:20.580
What's going on over here?
We want to maximize that.

245
00:14:20.581 --> 00:14:23.100
And so we have encoder inputs and then we have,

246
00:14:23.130 --> 00:14:27.930
it's a TF flood placeholder with a shape value,
none,
none data type.

247
00:14:28.170 --> 00:14:30.990
And then we have input.
So it's actually not on that.

248
00:14:31.020 --> 00:14:34.120
It's on encoder inputs.

249
00:14:34.121 --> 00:14:38.890
Dot length and the syntax a yes,
that's the one.
Great.

250
00:14:39.550 --> 00:14:43.690
And now for the length it's going to be too,
you have to a placeholder shape,

251
00:14:44.410 --> 00:14:49.110
uh,
none data type
input of length.

252
00:14:49.230 --> 00:14:53.480
So why don't I have w here in my code,
shouldn't have that.
And then,
uh,

253
00:14:53.500 --> 00:14:57.620
someone was saying there's a comma missing from shape,
non equals none,

254
00:14:57.800 --> 00:15:02.690
comma like that.
Okay.
Encoder inputs,

255
00:15:02.691 --> 00:15:05.540
length thought shape.
So in my actual code,

256
00:15:05.541 --> 00:15:09.290
I put a w there for some reason where I put a w like right.

257
00:15:10.850 --> 00:15:15.380
Where'd I put it?
At the end?
That the comma?
Yes.
Hold on.

258
00:15:15.620 --> 00:15:20.210
So I put a w here at the end in my initial code.

259
00:15:21.860 --> 00:15:23.060
And let me add this.
Okay.

260
00:15:25.670 --> 00:15:30.320
See why did I put a w that,
oh,
Suraj.
Here we go.

261
00:15:30.980 --> 00:15:35.600
Okay.
So there is no w great.
Okay.
So now tore embedding layers.

262
00:15:38.030 --> 00:15:41.750
Okay.
Oh,
let me answer two questions.
Okay.
So we are at the 15 minute mark.

263
00:15:41.751 --> 00:15:46.220
Let me answer two questions.
Any questions guys?
Yes,
TFL and 32.

264
00:15:47.480 --> 00:15:51.770
Okay,
so now we're going to define our embeddings.
Uh,
Oh,

265
00:15:51.771 --> 00:15:54.410
and I'm here.
I don't know if I mentioned this at the beginning,

266
00:15:54.411 --> 00:15:57.830
but I'm here at the upload Vr Studios.
Uh,
so check them out,

267
00:15:57.831 --> 00:16:02.720
subscribe to them with Assad.
Balaban Yan totally forgot about saying that.
Uh,

268
00:16:02.721 --> 00:16:06.140
so yeah,
just remember that.
I remember his name.
Okay.
Links down below.

269
00:16:06.500 --> 00:16:10.520
Links down below.
Yes.
So,
um,
and this wouldn't be possible without him.

270
00:16:10.521 --> 00:16:11.900
So big round of applause.

271
00:16:12.200 --> 00:16:15.730
So now what we're gonna do is we're going to define our yes beddings.

272
00:16:17.320 --> 00:16:20.160
So we have our uh,
variable.

273
00:16:24.050 --> 00:16:27.380
Uh Oh and then one question.
So what's best suited for ml,

274
00:16:27.410 --> 00:16:30.860
mac or windows or Linux,
Linux and Mac,

275
00:16:30.950 --> 00:16:33.080
any kind of unix based system.
Why?

276
00:16:33.081 --> 00:16:37.310
Because most of these libraries are they,

277
00:16:37.430 --> 00:16:41.330
when I look at stack overflow errors,
when I look at get hub issues,

278
00:16:41.480 --> 00:16:44.930
it seems that people own windows tend to have more issues because the people who

279
00:16:44.931 --> 00:16:48.410
are writing these libraries are using them on unix based systems.

280
00:16:48.411 --> 00:16:52.270
So it just so happens to be that way.
Um,
yes.

281
00:16:52.630 --> 00:16:56.590
So okay.
So now for our embedding,
so we want to convert our Dec,

282
00:16:56.650 --> 00:16:57.970
our sequences to embeddings,
right?

283
00:16:57.971 --> 00:17:01.210
So we're going to use the tensorflow primitive variable to do this.
Okay.

284
00:17:01.480 --> 00:17:06.280
We have going to,
we're going to initialize an embedding matrix randomly.

285
00:17:06.281 --> 00:17:10.870
So we're going to use tfs built in uniform random function to,
to,
to build this.

286
00:17:10.900 --> 00:17:15.430
And it's going to be the vocab size.
So we're going to,

287
00:17:15.950 --> 00:17:18.390
this is going to be a matrix of values and it's going,

288
00:17:18.460 --> 00:17:22.180
then we're going to fill hit.
We're going to fill it.
Okay.

289
00:17:24.870 --> 00:17:28.800
So now we have our vocab size and then we have our input in bedding size.

290
00:17:32.350 --> 00:17:35.050
And then we have,
what is that?
What is the,

291
00:17:35.110 --> 00:17:38.950
what is the interval that we want to generate from,
from negative one to one?
Uh,

292
00:17:38.970 --> 00:17:42.070
it's a,
it's a distribution from negative one to one of values.

293
00:17:42.280 --> 00:17:47.140
And then we have our uh,
data type.
Of course,
our data type is going to be,

294
00:17:49.710 --> 00:17:50.040
is the

295
00:17:50.040 --> 00:17:53.940
<v 0>audio really out of sync dot float?
I'm looking into that.</v>

296
00:17:54.930 --> 00:17:58.650
We have a Zod looking into that.
Okay.
And then

297
00:18:00.810 --> 00:18:03.420
<v 1>we have an encoder inputs embedded.</v>

298
00:18:03.720 --> 00:18:07.500
So now we're going to take that embedding matrix and we're going to add in the,

299
00:18:07.530 --> 00:18:08.363
um,

300
00:18:13.220 --> 00:18:14.870
inputs.
Okay?

301
00:18:17.870 --> 00:18:18.450
<v 0>Okay.</v>

302
00:18:18.450 --> 00:18:19.320
<v 1>So what did we just do?</v>

303
00:18:19.321 --> 00:18:23.640
So what we just did is we in randomly initialized an embedding matrix that can

304
00:18:23.641 --> 00:18:25.860
fit the inputs sequence.
And then,

305
00:18:27.510 --> 00:18:28.000
<v 0>okay,</v>

306
00:18:28.000 --> 00:18:29.470
<v 1>uh,
once we did that,</v>

307
00:18:29.500 --> 00:18:33.130
we then put our encoder inputs into that embedding matrix.

308
00:18:33.250 --> 00:18:36.490
And that embedding matrix is what we then feed to our encoder.

309
00:18:36.491 --> 00:18:39.010
So let's now build that encoder,
right?

310
00:18:39.011 --> 00:18:43.630
We have our embedding matrix and now we can feed that directly to our encoder.

311
00:18:43.840 --> 00:18:48.040
So for our coder,
so define encoder.
So for our encounter,

312
00:18:48.940 --> 00:18:51.430
tensorflow has so many updates,

313
00:18:51.431 --> 00:18:54.610
like they're all over the place and version changes and API changes,

314
00:18:54.700 --> 00:18:56.080
like all software.

315
00:18:56.320 --> 00:19:00.820
And so the Lstm cell is always changing where it is.
Before it used to be,

316
00:19:00.821 --> 00:19:04.210
I think it was in the LSTM cell,
used to be in,

317
00:19:05.230 --> 00:19:07.300
where was it?
In the previous version,

318
00:19:07.301 --> 00:19:12.220
it was in just a different place and now it's in under opps RNN cell.

319
00:19:12.370 --> 00:19:14.110
And in later versions that will probably change.

320
00:19:14.111 --> 00:19:17.780
So it's always a good idea to keep up with these API changes.
Um,

321
00:19:18.130 --> 00:19:22.060
and to best way to do that is a tensorflow documentation.
Okay.

322
00:19:22.061 --> 00:19:27.061
So we're going to import our LSTM cell and our LSTM states Tupelo.

323
00:19:28.091 --> 00:19:32.200
I'll talk about the second one and why we're importing that in a second.
Okay.

324
00:19:32.201 --> 00:19:35.290
So that's,
we're going to define our encoder.
So our encoder cell,

325
00:19:35.320 --> 00:19:39.550
and it's just one cell is going to be an LSTM cell,
right?

326
00:19:39.551 --> 00:19:43.780
That we just imported.
And we're going to define the number of hidden units,

327
00:19:44.390 --> 00:19:47.800
uh,
as that number we defined previously.

328
00:19:48.400 --> 00:19:51.190
And that's how many,
which was,
uh,

329
00:19:53.320 --> 00:19:55.810
2020 neurons.

330
00:19:56.080 --> 00:19:59.740
20 of these voice of God is awesome.

331
00:19:59.741 --> 00:20:03.940
I'm glad you guys liked the voice of God.
So that's 20 of these encoders cells.

332
00:20:04.360 --> 00:20:08.200
Okay.
And remember each of these is an LSTM itself.

333
00:20:08.230 --> 00:20:12.640
Each neuron is an Lstm.
Okay.
And,
um,

334
00:20:14.200 --> 00:20:18.610
so that's our encoder cell and see deprecation warning,

335
00:20:18.820 --> 00:20:22.990
but it's fine.
It's just a warning,
but it's will soon be deprecated.

336
00:20:23.020 --> 00:20:27.780
I knew it would happen.
So,
so now,
um,
so this is actually like a,

337
00:20:28.180 --> 00:20:29.230
a spatial,

338
00:20:31.160 --> 00:20:35.570
a very hellish spacial line to write out,
which is probably gonna,
you know,

339
00:20:35.750 --> 00:20:37.680
in python,
especially with all of this in tech.

340
00:20:37.681 --> 00:20:40.580
So I'm just going to paste this part and let's talk about what this is.

341
00:20:40.940 --> 00:20:45.560
This is the dynamic RNN.
So when using a standard RNN to make predictions,

342
00:20:45.561 --> 00:20:49.120
we're only taking the past into account,
right?
Um,

343
00:20:49.150 --> 00:20:52.330
so for certain tasks just makes sense whenever we're predicting the next word in

344
00:20:52.331 --> 00:20:53.164
the sequence.

345
00:20:53.170 --> 00:20:57.090
But for some task it would be useful to take both the past and the future into

346
00:20:57.100 --> 00:20:59.890
account.
Okay.
So this is the bi-directional directional part.

347
00:21:00.070 --> 00:21:04.290
We're taking both the past and the future into account.
So let me talk about,
uh,

348
00:21:04.390 --> 00:21:06.940
what we're doing here.
Okay.
So in order to demo this,

349
00:21:06.941 --> 00:21:09.430
I'm going to do some VR to demo this.
Okay.

350
00:21:09.730 --> 00:21:13.810
We're going to talk about how a by directional layer works over time.

351
00:21:14.530 --> 00:21:18.710
Okay.
So let's get some VR going and I'm just going to draw this out in Vr,
uh,

352
00:21:18.790 --> 00:21:19.623
for you guys.

353
00:21:21.870 --> 00:21:22.190
<v 0>Cool.</v>

354
00:21:22.190 --> 00:21:25.190
<v 1>Thanks.
Probably need two of these.
Okay.</v>

355
00:21:25.220 --> 00:21:29.810
So let's see if we could get our VR mode on.
Yup.
We're in our VR mode.

356
00:21:29.840 --> 00:21:33.140
Okay.
So,
so,
um,

357
00:21:33.200 --> 00:21:35.480
so we have our encoder decoder,
right?

358
00:21:35.481 --> 00:21:39.830
So we have our encoder in coder.
This is so fun.

359
00:21:40.070 --> 00:21:41.930
And then we have our decoder

360
00:21:43.520 --> 00:21:46.070
and this is a high level of abstraction of what it is,
right?

361
00:21:46.250 --> 00:21:49.820
We all get this idea that we are taking our encounter and so we're giving it a

362
00:21:49.821 --> 00:21:53.750
sequence,
right?
So we're giving you a sequence like,
you know,
how are you,

363
00:21:53.780 --> 00:21:56.780
how are you?

364
00:21:57.290 --> 00:21:59.510
And we're taking the sequence,

365
00:22:00.830 --> 00:22:03.230
we're actually converting it to are embedding matrix,

366
00:22:04.230 --> 00:22:04.870
<v 0>okay.</v>

367
00:22:04.870 --> 00:22:06.220
<v 1>Arts,
which is a vector.</v>

368
00:22:06.580 --> 00:22:10.210
And then we're feeding the vector into,

369
00:22:11.620 --> 00:22:13.990
right?
So it's,
it's,
here's the steps,
right?
Cause you could see,

370
00:22:13.991 --> 00:22:15.700
you guys can see all of this,
right?
This is dope.
Yup.

371
00:22:15.760 --> 00:22:19.690
So we have our inputs sequence,
we vectorize it and we feed it to our encoder.

372
00:22:19.960 --> 00:22:23.110
And then we want that final hidden state,
right?

373
00:22:23.230 --> 00:22:28.000
We want this final hidden state.
So the hidden state,

374
00:22:28.120 --> 00:22:30.490
we don't feed the output of the encoder to decoder.

375
00:22:30.491 --> 00:22:34.630
Remember we feed the hidden state,
which we'll call h.
So we'll just say hidden,

376
00:22:36.100 --> 00:22:37.270
right?
So this is,

377
00:22:37.300 --> 00:22:41.560
we embed this into a vector and then the encoder further embeds it across as

378
00:22:41.561 --> 00:22:44.530
many as many neurons as we define.

379
00:22:44.830 --> 00:22:49.360
And so because this is bi-directional,
uh,
it's looping over,
uh,

380
00:22:50.080 --> 00:22:53.860
over the sequence from both left and right.
So since it's by bi-directional,

381
00:22:53.980 --> 00:22:58.450
it's looping over the sequence,
both in both left and right directions,

382
00:22:58.630 --> 00:23:02.080
and then what we're doing.
And so now it's time for three d.

383
00:23:02.290 --> 00:23:06.310
So I'm going to go over here.
What we're doing is through time,
and this is,

384
00:23:06.311 --> 00:23:10.510
this represents through time,
we're feeding in the h the hidden states,

385
00:23:10.810 --> 00:23:13.570
and we're also feeding in the pre the previous input.

386
00:23:13.810 --> 00:23:18.340
So this is back propagation through time.
So there's two values.
So BP

387
00:23:19.870 --> 00:23:23.760
through
time,

388
00:23:23.910 --> 00:23:26.490
my handwriting needs to get better in Vr,
but just like,

389
00:23:26.491 --> 00:23:30.240
bear with me for a second.
So,
so I'm,
I'm trying to demo like through time,

390
00:23:30.241 --> 00:23:33.200
so right inputs,
sequence,
vectorize encoder,

391
00:23:33.210 --> 00:23:37.680
get the hidden states and take the hidden state and the previous input and feed

392
00:23:37.681 --> 00:23:42.510
it back into the encoder.
Okay.
And so we feed it back into the encoder and then,

393
00:23:42.660 --> 00:23:47.480
uh,
that's one full time step.
So in one full time step,
once we feed that in,

394
00:23:47.660 --> 00:23:51.860
then we feed that into the decoder.
So we roll it feet,

395
00:23:51.890 --> 00:23:54.530
decoder role,
feats decoder.
Okay.

396
00:23:54.531 --> 00:23:59.531
And so over and over and over again for every word that we have in our input

397
00:23:59.691 --> 00:24:04.090
sequence.
And then we feed it to our decoder,
our d Carter will output a,
um,

398
00:24:05.300 --> 00:24:08.660
uh,
a vector.
And then from that vector,

399
00:24:08.690 --> 00:24:11.680
we convert that into our,
uh,

400
00:24:11.720 --> 00:24:14.810
in this case it's going to be the initial value.
How are you,

401
00:24:14.811 --> 00:24:18.930
but in other cases in chatbots,
it would be,
you know,
I'm doing fine or you know,

402
00:24:18.950 --> 00:24:20.540
translations is going to be French.
Okay.

403
00:24:20.541 --> 00:24:24.890
So that's the high level of what we're doing here.
Hope you guys like that.
Okay.

404
00:24:24.920 --> 00:24:26.840
So back to this.

405
00:24:30.820 --> 00:24:35.140
People love the VR people of the VR.
Okay.

406
00:24:35.410 --> 00:24:39.160
It's super cool,
isn't it?
Okay.
So we have to keep going guys.
So it's super cool.

407
00:24:39.161 --> 00:24:40.780
I know.
So now,

408
00:24:43.110 --> 00:24:43.943
<v 2>uh,</v>

409
00:24:44.580 --> 00:24:46.230
<v 1>here we go with this.</v>

410
00:24:49.130 --> 00:24:53.450
It's a vibe.
It's a vive.
Yeah.
Uh,
great.
So now

411
00:24:56.030 --> 00:25:00.380
let's,
uh,
do what I was just talking about.
So let's,

412
00:25:00.381 --> 00:25:04.350
let's now programmatically do what I was just talking about.
Okay.

413
00:25:04.680 --> 00:25:09.270
So we're going to concatenate bartenders along one dimension.

414
00:25:09.271 --> 00:25:14.130
So we're going to take,
so this is the actual step where we're combining our,

415
00:25:14.610 --> 00:25:15.443
<v 2>uh,</v>

416
00:25:16.510 --> 00:25:19.690
<v 1>this is the bi directional steps.
So this is a bi directional step.</v>

417
00:25:19.920 --> 00:25:22.000
We write down by directional step.
Okay?

418
00:25:22.030 --> 00:25:25.600
So we'll talk about what that means by directional step.
So what we have here,

419
00:25:26.400 --> 00:25:29.380
what we have here is we have our outputs for our encoder.

420
00:25:29.410 --> 00:25:32.150
So we have our encoder outputs and the encoder output.

421
00:25:32.180 --> 00:25:34.990
We're going to use TensorFlow's concatenate function.
To do this,

422
00:25:35.260 --> 00:25:38.440
we're going to have both our forward outputs and our backward outputs.

423
00:25:38.650 --> 00:25:42.460
So our forward outputs are what we s we spit out from this dynamic sell,
right?

424
00:25:42.640 --> 00:25:44.410
We got our forward outputs are backward,

425
00:25:44.411 --> 00:25:47.590
outputs are forward final states in our backward final state.

426
00:25:47.591 --> 00:25:51.700
So their states and outputs for both the encoder for,
sorry,

427
00:25:51.701 --> 00:25:54.070
for both the forward and the backward,

428
00:25:54.800 --> 00:25:55.633
<v 2>uh,</v>

429
00:25:56.500 --> 00:26:00.540
<v 1>parts of the bi-directional RNN.
Okay?
Forward and backward.</v>

430
00:26:00.550 --> 00:26:04.780
FW and BW means forward and backwards.
So we have states for both.

431
00:26:04.900 --> 00:26:07.750
How are you?
And then you are how.
Okay.

432
00:26:07.870 --> 00:26:11.740
And what this does is it allows us to take into account the full context.

433
00:26:11.860 --> 00:26:15.700
So the both the past and the future.
This is definitively better.

434
00:26:15.701 --> 00:26:19.960
I'm telling you right now that it is better to do this every time.

435
00:26:20.350 --> 00:26:23.650
It is more computationally expensive to have a by directional layer.

436
00:26:23.830 --> 00:26:27.850
But just think about it.
You want the future and you want the past,

437
00:26:27.970 --> 00:26:30.760
when you're trying to make a prediction,
it,
it's a word,

438
00:26:30.790 --> 00:26:35.790
a story I sequence is all about what's the story is about what's happening all

439
00:26:36.461 --> 00:26:40.570
over it,
whether it be music,
whether it be art,
words,
numbers,

440
00:26:40.960 --> 00:26:44.670
if there's a pattern,
if there is a pattern book past and the future matters.

441
00:26:44.820 --> 00:26:48.450
So having by directional layers,
although more computationally expensive,

442
00:26:48.750 --> 00:26:52.500
gives us better,
uh,
predictions.
Okay.
So,

443
00:26:54.880 --> 00:26:59.880
so now we're going to take our encoder outputs and we're going to take our in

444
00:27:02.880 --> 00:27:05.310
quarter up forward outputs in our encoder backwards outputs.

445
00:27:05.490 --> 00:27:10.470
And we're going to concatenate them along a along,
uh,
two axes,

446
00:27:10.590 --> 00:27:14.760
access axes.
It's the poor hall.
Uh,
and so,
uh,

447
00:27:15.180 --> 00:27:16.620
that's gonna give us our encoder outputs.

448
00:27:17.010 --> 00:27:20.100
And then what we're going to do is we're going to take the final state,

449
00:27:21.600 --> 00:27:25.980
the final state.
See,
uh,
so that's,
that's for our outputs.

450
00:27:26.010 --> 00:27:29.400
And then we're going to have our concatenate function to take our,

451
00:27:34.860 --> 00:27:35.693
<v 0>okay.</v>

452
00:27:36.470 --> 00:27:39.860
<v 1>Okay.
So we're going to take our encoder forward,</v>

453
00:27:39.890 --> 00:27:44.240
final state dot c and then we're going to take the encoder.

454
00:27:46.200 --> 00:27:47.033
<v 0>Okay.</v>

455
00:27:47.090 --> 00:27:51.290
<v 1>Backwards final states.
Dot C okay.
And so what is this,</v>

456
00:27:51.291 --> 00:27:53.060
what are these words?
What does CN one,
let me,

457
00:27:53.120 --> 00:27:56.150
let me write this owl and then we're going to,
uh,

458
00:28:02.530 --> 00:28:04.450
I'm moving a bit too fast.
Okay.
So

459
00:28:05.230 --> 00:28:06.063
<v 0>hold on.</v>

460
00:28:07.900 --> 00:28:12.900
<v 1>So this specific part I'm going to paste because there's a lot to explain here.</v>

461
00:28:14.080 --> 00:28:15.910
So,
okay,

462
00:28:16.630 --> 00:28:19.510
so we have a final state,

463
00:28:19.511 --> 00:28:22.840
see and we have a final state h and I can slow it down as well.

464
00:28:23.140 --> 00:28:26.710
So what we have here is we have,

465
00:28:26.960 --> 00:28:31.330
so h and c are commonly used to denote output,
value and sell state.

466
00:28:31.510 --> 00:28:35.500
So it's both the output value and the cell state and we want to contaminate both

467
00:28:35.501 --> 00:28:39.050
of those for both our forward and are backwards,
um,

468
00:28:39.940 --> 00:28:42.670
our forward and backward outputs.
Okay.

469
00:28:46.230 --> 00:28:47.063
<v 0>MMM.</v>

470
00:28:50.090 --> 00:28:50.990
<v 1>So that's what we're,</v>

471
00:28:51.050 --> 00:28:55.250
so we're concatenating both of those values and then we get a final state using

472
00:28:55.251 --> 00:28:59.690
the LMS.
And so now this is why we imported LSTM state to pull at the beginning.

473
00:28:59.990 --> 00:29:01.490
This is why we imported it,

474
00:29:01.491 --> 00:29:05.630
because we're going to use that final state c and the final state h.

475
00:29:05.660 --> 00:29:06.290
So that's the,

476
00:29:06.290 --> 00:29:10.670
that's the one I say Cnh I'm talking about the internal state of the cell and

477
00:29:10.671 --> 00:29:11.510
the output value.

478
00:29:11.540 --> 00:29:14.690
We're combining all of those to get that final state for the encoder.

479
00:29:14.840 --> 00:29:19.840
And that this value right here is what we then feed into our decoder.

480
00:29:21.050 --> 00:29:24.470
It is a combination of both our forward or backwards,

481
00:29:24.590 --> 00:29:28.250
our cell state and our output.
We combine it altogether,

482
00:29:28.580 --> 00:29:32.570
and that's what we feed into our decoder.
So now let's,
let's define our decoder.

483
00:29:33.020 --> 00:29:37.760
So our decoder think there's an encoder mistake where you misspelled.

484
00:29:38.240 --> 00:29:41.680
Oh,
e e r at the end.
I'm trying to find it here.

485
00:29:41.980 --> 00:29:43.930
Someone said there's a encoder

486
00:29:46.240 --> 00:29:48.900
to misspelling a bandaid.
I don't see it in this,

487
00:29:49.890 --> 00:29:50.723
<v 2>uh,</v>

488
00:29:52.190 --> 00:29:55.940
<v 1>someone encoder.
Someone said there's an encoder mistake.</v>

489
00:29:55.970 --> 00:30:00.650
I don't see it.
Yeah,
I'm not either or whatever.
So where were we?

490
00:30:00.830 --> 00:30:03.230
So now we're going to define our decoder.
So for our decoder,

491
00:30:07.200 --> 00:30:09.270
for our decoder,
we want to do,

492
00:30:09.300 --> 00:30:12.300
it's going to be very similar but different.

493
00:30:12.301 --> 00:30:14.960
And I'll show you what the difference is.
So let's define it.

494
00:30:15.000 --> 00:30:17.580
So this is going to be similar.
We define it the same way.

495
00:30:17.581 --> 00:30:22.050
It's still an LSTM cell out.
Remember the number of hidden units is different.

496
00:30:22.110 --> 00:30:23.760
It's double that of the,

497
00:30:28.990 --> 00:30:29.823
<v 2>okay.</v>

498
00:30:33.000 --> 00:30:36.480
<v 1>And then,
uh,
we're going to define our</v>

499
00:30:41.170 --> 00:30:45.280
batch size and then our,
we're going to unstack.

500
00:30:45.281 --> 00:30:48.610
So let me talk about what I'm doing right here.
So what I'm doing right here is,

501
00:30:51.400 --> 00:30:55.810
uh,
what I'm doing here is I'm going to do fine our encoder and hour.

502
00:30:58.650 --> 00:30:59.483
<v 2>Yeah.</v>

503
00:30:59.970 --> 00:31:04.270
<v 1>Do you have to unstack.tf dot shape encoder inputs?
Okay.</v>

504
00:31:04.300 --> 00:31:07.600
So let's define our Dakota defining our decoder,

505
00:31:07.990 --> 00:31:10.420
defining our decoder.

506
00:31:11.050 --> 00:31:15.760
So we have our a decoder self and we're going to,

507
00:31:16.150 --> 00:31:18.160
we've defined our decoder cell and so what this,

508
00:31:18.161 --> 00:31:22.030
this step does is what we really care about is the batch size is going to give

509
00:31:22.031 --> 00:31:23.050
us the batch size.

510
00:31:23.230 --> 00:31:27.400
We have all of those inputs and we went to feed it into our decoder in batches

511
00:31:27.610 --> 00:31:32.070
batches because that's how we train,
right?
We feed our data into batches,
uh,

512
00:31:32.110 --> 00:31:37.090
data into our decoder in batches.
Like a subsets.
There are subsets.
Okay.

513
00:31:37.500 --> 00:31:42.080
Uh,
okay,
so,

514
00:31:42.890 --> 00:31:45.800
uh,
right.
So then we're going to have the length of the decoder,

515
00:31:45.950 --> 00:31:47.000
which is going to be

516
00:31:51.180 --> 00:31:55.350
the encoder inputs length plus three.

517
00:31:56.850 --> 00:31:59.820
So why do we have plus three?
Because we have two additional steps.

518
00:31:59.940 --> 00:32:04.440
One for the leading end of sentence token for the decoder inputs.

519
00:32:04.620 --> 00:32:08.100
We want it to be a little bigger because we have an end of sentence token at the

520
00:32:08.101 --> 00:32:11.180
end.
That's just going to help us help our model.
No,
uh,

521
00:32:11.190 --> 00:32:14.760
that this is the end of a sequence.
This is in natural language processing.

522
00:32:14.940 --> 00:32:19.170
We have these end of sentence tokens all the time.
Um,
I'm actually,

523
00:32:19.171 --> 00:32:23.360
I don't think we have models that can,
no,

524
00:32:23.361 --> 00:32:26.420
when the sentence is over yet,
I actually,

525
00:32:26.421 --> 00:32:31.140
I haven't seen a model that doesn't use these,
uh,
either padding or,
um,

526
00:32:31.220 --> 00:32:34.270
some kind of end of sentence token to specify,
you know,

527
00:32:34.280 --> 00:32:36.590
that the sentences over yet,
we,
we need to get there.

528
00:32:36.591 --> 00:32:40.580
We need to get there where we just feed it.
And we don't have to do any of these,

529
00:32:40.581 --> 00:32:43.250
uh,
preprocessing steps,
but we'll get there.
Okay.

530
00:32:45.720 --> 00:32:50.420
Uh,
so now let me answer some questions since we're 15 minutes in.
Uh,

531
00:32:50.790 --> 00:32:54.740
does dot dividing into smaller batches?
Is,

532
00:32:55.580 --> 00:33:00.480
is dividing into smaller batches more computationally expensive?
No,
no,

533
00:33:00.510 --> 00:33:04.920
it's not a,
because it's still the same amount of input data.
Uh,
but what,

534
00:33:04.921 --> 00:33:08.910
what having it,
uh,
in batches does,
is it,

535
00:33:09.900 --> 00:33:10.340
<v 0>okay.</v>

536
00:33:10.340 --> 00:33:13.250
<v 1>It makes our prediction better because,
uh,</v>

537
00:33:13.251 --> 00:33:16.220
we have more iterations that are happening.
So there are more,
uh,

538
00:33:16.430 --> 00:33:19.340
wait updates happening.
It actually,
honestly,

539
00:33:19.341 --> 00:33:23.610
it is a little more computationally expensive,
but it's so,
it's so small.

540
00:33:23.630 --> 00:33:26.420
The difference is so small,
so don't,
don't,
don't worry about that.

541
00:33:27.900 --> 00:33:28.230
<v 0>Okay.</v>

542
00:33:28.230 --> 00:33:33.100
<v 1>Ah,
I appreciate it,
Paul,
for waking up at 4:00 AM in Melbourne for this.
Uh,</v>

543
00:33:33.520 --> 00:33:37.840
okay,
this is a good question.
In what ways are gru is outperforming LSTs?

544
00:33:38.110 --> 00:33:42.060
I recently read a paper that says adding a bias of one to the LSTs.

545
00:33:42.061 --> 00:33:45.400
Forget gate closes the gap between the LSTM and the GRU.

546
00:33:45.580 --> 00:33:50.290
So Gru cells are very similar to LLCs and we're definitely gonna talk about that

547
00:33:50.291 --> 00:33:53.680
in this coming weekly video in three days.
But to sum it up,

548
00:33:53.710 --> 00:33:56.920
so gr use have less gates than LSTs.

549
00:33:57.130 --> 00:34:01.450
There's just a reset gate and there's a,
uh,
update gate.

550
00:34:01.510 --> 00:34:03.280
So whereas Ellis teams have a,
forget gay,

551
00:34:03.310 --> 00:34:06.790
they have three days so it's two gates so it's less computation happening so

552
00:34:06.791 --> 00:34:11.200
it's less computationally expensive and not only is it less computationally

553
00:34:11.201 --> 00:34:11.920
expensive,

554
00:34:11.920 --> 00:34:16.900
it tends to have better results specifically for dynamic memory network,

555
00:34:16.930 --> 00:34:19.090
which is a really,
really cool model type.

556
00:34:19.240 --> 00:34:21.970
And I'm going to start talking about as we get further into this course.

557
00:34:21.971 --> 00:34:26.090
We have four more videos in this course and we are now.
We are now guys,

558
00:34:26.130 --> 00:34:28.150
if you don't understand all of this,
don't worry.

559
00:34:28.390 --> 00:34:30.220
We are at the bleeding edge of deep learning.

560
00:34:30.221 --> 00:34:32.320
This course is called intro to deep learning,

561
00:34:32.530 --> 00:34:36.940
but we went from doing linear regression at the start too.
We are about to go,

562
00:34:37.300 --> 00:34:41.080
we are about to get into generative adversarial networks and we're about to get

563
00:34:41.081 --> 00:34:44.290
into the bleeding edge of all of AI,

564
00:34:44.320 --> 00:34:46.270
which is not even deep learning in the last episode,

565
00:34:46.271 --> 00:34:50.560
which is I'm most excited for,
which is called probabilistic programming,

566
00:34:50.770 --> 00:34:53.350
which is some future stuff and you know how much I liked the future.

567
00:34:53.351 --> 00:34:55.660
So that's going to be awesome.
One more question.

568
00:34:56.820 --> 00:34:57.620
<v 0>Okay.</v>

569
00:34:57.620 --> 00:35:00.900
<v 1>Pace.
Raj,
what model would you use to analyze repetition in speech?</v>

570
00:35:01.410 --> 00:35:05.150
Repetition in speech?
Great question.
Repetition in speak.

571
00:35:05.160 --> 00:35:06.820
What do you mean by analyze?
Repetition is be,

572
00:35:06.821 --> 00:35:11.610
so I've just got to clarify for you.
Uh,
analyzing repetition in speech.
Uh,

573
00:35:11.880 --> 00:35:15.860
so maybe the frequency of repetition.
So how would I do this?
So this does,

574
00:35:15.960 --> 00:35:19.080
this is a good way of thinking about it.
So you would,
um,

575
00:35:19.920 --> 00:35:21.900
you could consider it a classification problem.

576
00:35:21.901 --> 00:35:26.220
So an easy way to do that would be to an easy way computationally to do that

577
00:35:26.370 --> 00:35:30.510
would be to pre label,
uh,
what each sentence is.
So,
you know,
you would say like,

578
00:35:30.511 --> 00:35:35.460
how often does this label,
you know,
x up here versus y and Z and a,
B,

579
00:35:35.461 --> 00:35:40.390
c,
d,
d.
So it'd be a classification problem.
Uh,
and you would,
uh,

580
00:35:40.410 --> 00:35:43.980
use a deep net and you would use a,

581
00:35:45.450 --> 00:35:48.330
you wouldn't even have to use an encoder decoder because you would just define

582
00:35:48.331 --> 00:35:51.450
how many times this label shows up in the data.

583
00:35:51.690 --> 00:35:54.000
So it would be a multi-class classification,

584
00:35:54.001 --> 00:35:58.380
deep net using a supervised learning with labels and,

585
00:35:58.910 --> 00:36:01.710
um,
inputs.
One more question.

586
00:36:01.711 --> 00:36:05.930
Evolutionary strategies that is coming Ganzer are coming.
And uh,

587
00:36:10.230 --> 00:36:14.310
our large batches always worse than smaller batches in terms of prediction,

588
00:36:14.340 --> 00:36:16.980
accuracy,
uh,

589
00:36:17.190 --> 00:36:20.970
our large batches always worse than smaller batches in terms of prediction

590
00:36:20.971 --> 00:36:25.350
accuracy.
That as as far as I've seen,
yes.
But it's not like,

591
00:36:25.380 --> 00:36:29.550
it's not like there's like some uh,
some exponential in terms of like,
you know,

592
00:36:29.551 --> 00:36:30.620
you can never a,
you know,

593
00:36:30.621 --> 00:36:33.930
you can just get infinitely smaller in terms of batches and by infinitely

594
00:36:33.931 --> 00:36:37.430
smaller,
I mean you can have an infinitely large number of batch,

595
00:36:37.560 --> 00:36:41.100
a batch steps.
Uh,
but there's,
there's a,
there's a,

596
00:36:41.880 --> 00:36:42.990
there's a bell curve,
right?

597
00:36:42.991 --> 00:36:45.990
So there's a point where it's at the optimal level of batch step and then it

598
00:36:45.991 --> 00:36:47.390
goes down again.
Remember,

599
00:36:47.400 --> 00:36:51.480
batch steps are one of many hyper parameters and all of machine learning,

600
00:36:51.481 --> 00:36:53.610
all of deep learning,
I'll,
sorry,
all of deep learning.

601
00:36:54.240 --> 00:36:59.240
It's about tuning those hyper parameters and to me what I would like to see,

602
00:36:59.490 --> 00:37:00.870
what I would like to see you guys work on,

603
00:37:00.871 --> 00:37:05.220
what I would like to see more people work on is ways of learning these hyper

604
00:37:05.221 --> 00:37:09.570
parameters.
Because this signing these hyper parameters is so annoying sometimes,

605
00:37:09.571 --> 00:37:13.440
right?
Uh,
because they are so arbitrary,
there's no way for us to really know.

606
00:37:13.860 --> 00:37:15.330
We have to learn them manually.

607
00:37:15.331 --> 00:37:19.050
And it takes up a lot of our time from looking at what is really necessary.

608
00:37:19.051 --> 00:37:20.910
And that is a higher level architecture.

609
00:37:20.911 --> 00:37:24.870
So let's find out easier ways of learning these hyper parameters.
Okay,

610
00:37:24.871 --> 00:37:27.750
so that's it for questions.
Now we're going to,

611
00:37:29.250 --> 00:37:30.450
we've defined our decoder

612
00:37:32.640 --> 00:37:37.260
and the lengths and now we're going to define our weights and biases.
So

613
00:37:38.790 --> 00:37:39.580
<v 0>yeah,</v>

614
00:37:39.580 --> 00:37:42.430
<v 1>hold on.
So our weights and biases are going to be</v>

615
00:37:47.340 --> 00:37:50.340
are,
so our output projection,
so let's define our,

616
00:37:50.520 --> 00:37:55.020
define our weights and biases.
So remember this is all for the Dakota.

617
00:37:55.021 --> 00:37:59.850
We're working on the decoder value.
So for our decoder values,

618
00:38:00.000 --> 00:38:04.530
we have both weights and biases.
And these weights are what we're going to use.

619
00:38:04.531 --> 00:38:06.600
We're,
we're manually defining these.

620
00:38:06.610 --> 00:38:09.420
Remember we didn't define these for the encoder because?

621
00:38:09.840 --> 00:38:14.040
Because tensorflow had those built in for us in that dynamic RNN function.

622
00:38:14.250 --> 00:38:17.340
But we're going to be a little more detailed with our decoder.
Why?

623
00:38:17.610 --> 00:38:21.240
Because we are going to implement attention ourselves.
Okay.

624
00:38:21.241 --> 00:38:23.370
So I want to show you guys how attention works.

625
00:38:23.490 --> 00:38:27.030
So I've kind of skipped on attention before I just said,
you know,
hand wave,

626
00:38:27.150 --> 00:38:28.800
this is attention,
boom.

627
00:38:29.340 --> 00:38:31.470
But now we're going to implement a little bit of what are,
what are,

628
00:38:31.770 --> 00:38:34.770
what is called soft attention and we'll talk about that.

629
00:38:34.771 --> 00:38:39.340
So we're going to find these weights and biases for our decoder manual.
We've uh,

630
00:38:40.840 --> 00:38:44.440
okay,
so where were we,
where were we?
So where were we?

631
00:38:45.640 --> 00:38:49.690
Right.
So these weights are going to be initialized random weight using again,

632
00:38:49.910 --> 00:38:54.490
tension flows,
random uniform function.
We use that a lot.
And then

633
00:38:56.140 --> 00:38:59.830
we want to define the size of that in terms of tensors.
So it's going to be,

634
00:38:59.831 --> 00:39:04.210
the size is going to be a three dimensional tensor.

635
00:39:04.390 --> 00:39:07.930
So we have the number of hidden units,
the vocab size.

636
00:39:10.340 --> 00:39:11.173
<v 0>Okay.</v>

637
00:39:11.300 --> 00:39:16.300
<v 1>And then the distribution from negative one to one given our data type and it's</v>

638
00:39:17.331 --> 00:39:20.000
all in 32 bits and a

639
00:39:22.050 --> 00:39:22.883
<v 0>yeah,</v>

640
00:39:23.140 --> 00:39:26.510
<v 1>right.
So then our bias,
our bias is going to be TFI variable</v>

641
00:39:28.500 --> 00:39:29.860
if that's zeroes.

642
00:39:31.240 --> 00:39:35.500
And then the vocab size followed by the type.
Okay.

643
00:39:35.590 --> 00:39:40.360
So then that's the title.
And let's talk about this.
Okay.

644
00:39:40.750 --> 00:39:42.100
And I'll answer questions in five minutes.

645
00:39:42.101 --> 00:39:45.160
So think about your questions in five minutes.
It's to be the next question mark.

646
00:39:45.161 --> 00:39:49.510
I'll answer them too.
You have to float 32.
Okay.

647
00:39:50.260 --> 00:39:52.570
Okay.
Weights and biases.

648
00:39:54.400 --> 00:39:57.780
These are our weights and biases guys.
And so it's going to be offside.

649
00:39:57.850 --> 00:40:00.070
So the number of hidden units,
so,
right?

650
00:40:00.100 --> 00:40:05.020
Th these weights and the reason we said we put the Duke,
the hidden units as,
uh,

651
00:40:05.290 --> 00:40:05.621
as an,

652
00:40:05.621 --> 00:40:08.950
as an input to this is because the way you have to connect all of those hidden

653
00:40:08.951 --> 00:40:11.020
units to the output,
what the output's going to be.

654
00:40:11.200 --> 00:40:14.680
So we take all of those values and then we multiply it by the way,
it's,

655
00:40:14.681 --> 00:40:18.400
which is a initialized as a random matrix and it's going to learn what the

656
00:40:18.401 --> 00:40:22.240
optimal weight value should be to get that value,
right.
So,

657
00:40:23.590 --> 00:40:24.910
right.
So,
okay.

658
00:40:29.180 --> 00:40:30.013
<v 0>Okay.</v>

659
00:40:30.780 --> 00:40:30.961
<v 1>Okay,</v>

660
00:40:30.961 --> 00:40:35.961
so now we're going to get into the attention part.

661
00:40:36.330 --> 00:40:40.530
Okay.
So,
uh,
let's talk about attention.

662
00:40:40.980 --> 00:40:44.310
So to get attention,
let's see.
Let's see

663
00:40:49.640 --> 00:40:52.490
guys,
if you're lost,
don't worry about it.
This stuff is,

664
00:40:52.491 --> 00:40:55.160
remember the bleeding edge and I'm going to get better.

665
00:40:55.250 --> 00:40:57.710
I'm just going to get better at explaining it.
I'll get better examples.

666
00:40:58.040 --> 00:41:00.760
You're going to get better just by looking at this.
You're getting better crew.

667
00:41:01.040 --> 00:41:03.320
Kudos to you.
Thank you for being here.
By the way.

668
00:41:03.350 --> 00:41:06.140
I need to say thank you to you guys.
You guys are awesome for being here,

669
00:41:06.350 --> 00:41:08.240
for wanting to learn this awesome stuff.

670
00:41:08.420 --> 00:41:11.330
This is the most important stuff in the entire world.

671
00:41:11.780 --> 00:41:14.840
More important than anything else you can be doing in the world.

672
00:41:14.880 --> 00:41:18.200
It is working on AI.
If you are a smart person in the world right now,

673
00:41:18.201 --> 00:41:21.050
more important than politics,
more important than climate change,

674
00:41:21.230 --> 00:41:24.680
more important than anything else in the world.
You should be working on AI.
Why?

675
00:41:24.681 --> 00:41:29.060
Because we are the closest to solving it.
Oh,
across all these problem spaces.

676
00:41:29.270 --> 00:41:31.160
And if we can solve this,
we can solve everything else.

677
00:41:31.190 --> 00:41:34.820
So what we're gonna do is we're going to talk the next steps here

678
00:41:37.770 --> 00:41:40.600
and to do that.
Let me just paste this in because we have a lot to,

679
00:41:40.740 --> 00:41:44.520
to go still and uh,
we don't have time to really type out everything.

680
00:41:44.521 --> 00:41:49.380
So we are now,
now is the padding step.
Remember as I defined the pad and Eos,

681
00:41:49.640 --> 00:41:53.580
uh,
functions before now is now we're going to actually pad those inputs.

682
00:41:53.700 --> 00:41:57.630
So we have,
uh,
we've embedded them those values.

683
00:41:58.180 --> 00:41:58.860
<v 0>Okay.</v>

684
00:41:58.860 --> 00:42:01.920
<v 1>We've embedded those values into our projection matrix,
right?</v>

685
00:42:01.921 --> 00:42:04.230
We took our inputs and we embedded them.

686
00:42:04.650 --> 00:42:08.270
And now what we're going to do is we're going to,

687
00:42:10.410 --> 00:42:11.243
<v 0>yeah,</v>

688
00:42:11.420 --> 00:42:14.060
<v 1>now we're going to add those paddings so we,</v>

689
00:42:14.120 --> 00:42:17.870
so we're going to add the padding's to it.
And that's what,
so the t,

690
00:42:17.960 --> 00:42:22.430
the embedding lookup function of tensorflow,
retrieves the roads of the,

691
00:42:22.520 --> 00:42:27.020
of the parameters tensor,
which is in this case,
the,
uh,
embedding matrix.

692
00:42:27.230 --> 00:42:30.740
And the behavior is similar to using indexing with arrays and num Pi.

693
00:42:31.040 --> 00:42:35.420
So this is essentially just adding the padding and the end of sentence token to

694
00:42:35.421 --> 00:42:36.620
our,
um,

695
00:42:40.650 --> 00:42:42.810
two are uh,
embedding lookup.
Okay.

696
00:42:42.840 --> 00:42:45.600
So that's what the end of sentence and padding steps do.

697
00:42:45.870 --> 00:42:49.740
And now get ready for attention guys,
because I have never like fully,

698
00:42:49.741 --> 00:42:53.490
programmatically a defined attention.
And so now we're going to define attention.

699
00:42:53.850 --> 00:42:57.540
So bear with me.
This is gonna be awesome.
Bear with me.
Okay,

700
00:42:57.541 --> 00:43:01.020
so now we're going to,
now we're going to implement attention.
Okay,

701
00:43:01.021 --> 00:43:05.130
and there's there,
there are two functions here though.
We have to implement.

702
00:43:05.160 --> 00:43:06.870
So for attention.

703
00:43:07.470 --> 00:43:11.790
Now remember for the encoder we defined a that that function,
what was it?

704
00:43:11.791 --> 00:43:13.470
W The function that we defined was,

705
00:43:13.471 --> 00:43:15.840
let's look at it again just so we remember what it was.

706
00:43:16.140 --> 00:43:20.310
It was called bi-directional dynamic coronet.
That functionality,

707
00:43:20.370 --> 00:43:23.880
the looping functionality that I did in Vr a second ago where we're taking the

708
00:43:24.120 --> 00:43:27.810
previous hidden states and the input and feeding both of those backend.

709
00:43:28.020 --> 00:43:31.890
That's all done by this one line.
All of that is done by this one line,

710
00:43:31.891 --> 00:43:36.690
but for the decoder we are going to do that manually because we want to see how

711
00:43:36.691 --> 00:43:39.990
it works.
So this is what we're going to do manually.
We were going to loop.

712
00:43:40.170 --> 00:43:44.850
This is this function,
does that loop manually for us?
So,

713
00:43:45.410 --> 00:43:45.690
<v 0>okay,</v>

714
00:43:45.690 --> 00:43:48.510
<v 1>what we are doing in this is your first saying,</v>

715
00:43:48.630 --> 00:43:51.600
what is the end of sentence step to get that initial input?

716
00:43:51.930 --> 00:43:56.670
What is the initial cell state?
And then what is the loop state?
And uh,

717
00:43:59.120 --> 00:44:02.090
all it does is it's going to initialize these,

718
00:44:02.091 --> 00:44:05.840
these values and then return them because the loop isn't actually happening
here.

719
00:44:05.990 --> 00:44:08.060
The loop happens in the next function.

720
00:44:08.180 --> 00:44:11.690
All we're doing in this function is defining what the end a sentence say.
It is,

721
00:44:11.840 --> 00:44:14.630
what the cell state is,
what the initial output is,

722
00:44:14.631 --> 00:44:17.860
which is going to be non to start off with,
and then return those values to,

723
00:44:18.080 --> 00:44:22.230
to then loop in a second.
So now,
now we're going to loop.

724
00:44:22.770 --> 00:44:25.200
Okay,
so now we're going to

725
00:44:27.940 --> 00:44:28.773
<v 0>loop.</v>

726
00:44:31.640 --> 00:44:33.080
<v 1>Let's get to the looping par.</v>

727
00:44:37.350 --> 00:44:38.760
Let's pace this in as well.

728
00:44:42.290 --> 00:44:46.220
Got a lot to go.
A lot to go.
Okay,
so for our,

729
00:44:46.280 --> 00:44:49.720
so here's our attention mechanism.
So this is considered,
um,

730
00:44:50.030 --> 00:44:54.470
soft attention.
Okay?
It's considered soft as opposed to hard attention.
Uh,

731
00:44:54.500 --> 00:44:58.220
because it's a very trivial form of attention.

732
00:44:58.250 --> 00:45:01.010
I'm going to explain exactly what that is.
Okay.

733
00:45:01.011 --> 00:45:05.150
So let's talk about what we're,
what we're,
what we're doing here.

734
00:45:05.151 --> 00:45:08.210
So we have loop fn initial and uh,

735
00:45:11.020 --> 00:45:14.290
so,
okay,
so let's talk about what we're doing here.

736
00:45:14.980 --> 00:45:17.500
What this does is it's going to get the next,

737
00:45:19.870 --> 00:45:23.140
the next input in the next day.
So remember the loop.
That's what this is doing.

738
00:45:23.141 --> 00:45:27.440
It's transitioning for the loop.
And uh,

739
00:45:28.480 --> 00:45:29.890
so let me talk about what this does,

740
00:45:29.891 --> 00:45:32.260
but I said how would answer questions at the 45 minute mark.

741
00:45:32.261 --> 00:45:33.970
So let me answer some questions right now.
So

742
00:45:34.080 --> 00:45:36.990
<v 3>Paul Brady's got a question and he's asking,</v>

743
00:45:37.000 --> 00:45:40.920
are we going to be exposed into tensorflow,
uh,
in the Udacity course?
All right,

744
00:45:40.950 --> 00:45:44.400
the intro tendon transfer flow or should they start focusing on learning it on

745
00:45:44.401 --> 00:45:45.234
their own way?

746
00:45:45.480 --> 00:45:45.721
<v 1>Yes,</v>

747
00:45:45.721 --> 00:45:48.870
we will be learning tensorflow in the U Udacity course and we will be learning

748
00:45:48.871 --> 00:45:53.760
tensor flow,
um,
just from here on out until,
um,
you know,

749
00:45:53.761 --> 00:45:56.940
anything better comes along which pie torch is starting to get pretty hot.

750
00:45:56.941 --> 00:45:59.340
So also check out py torch,
cause I definitely am,

751
00:45:59.520 --> 00:46:00.930
because their documentation is pretty dope

752
00:46:03.870 --> 00:46:07.110
when we write codes to create a predictive model.
What is the output?

753
00:46:07.170 --> 00:46:11.970
Is it an object,
a dll,
an exe?
Thanks in advance?
Uh,

754
00:46:12.270 --> 00:46:15.840
the output is a set it,

755
00:46:16.380 --> 00:46:19.530
while the output could be anything,
but it is,
um,
it could be numbers,

756
00:46:19.531 --> 00:46:22.560
it could be words,
it could be depending on what you're trying to predict.

757
00:46:22.561 --> 00:46:25.560
You're trying to predict the next word.
Are you trying to pick the next number?

758
00:46:25.710 --> 00:46:28.380
Our Turner trying to predict a label.
Is this a cat or a dog?

759
00:46:28.590 --> 00:46:32.730
But it's going to be a variable in code,
a programmatic variable,

760
00:46:32.880 --> 00:46:35.290
a space in memory that we defined would,

761
00:46:35.291 --> 00:46:37.890
they'd word and programmatically like label.

762
00:46:38.010 --> 00:46:42.480
So it's a set of integer numbers,
but it's not an exe if not some kind of,

763
00:46:42.690 --> 00:46:44.310
uh,
it's not,
it's not a,

764
00:46:44.320 --> 00:46:47.430
it's not a binary executable file that you just run like a program.

765
00:46:47.610 --> 00:46:51.960
It's just a set of,
it's like memory.
So if you were to even print,
you know,

766
00:46:52.020 --> 00:46:55.770
a variable,
it'd be a variable.
Okay.
When we feed words from our,

767
00:46:55.890 --> 00:46:59.250
two more questions,
when we feed words from our texts,
example to the model,

768
00:46:59.251 --> 00:47:01.860
are we,
how are we handling pronouns?
For example,

769
00:47:02.160 --> 00:47:04.680
if we have a text on Sir Albert Einstein,

770
00:47:04.681 --> 00:47:08.710
many of the times we'll refer to him as he,
his,
him,
et cetera.

771
00:47:10.130 --> 00:47:10.770
<v 0>Okay.</v>

772
00:47:10.770 --> 00:47:15.120
<v 1>Okay,
so great question.
So before deep learning,
so again,
before deep learning,</v>

773
00:47:15.121 --> 00:47:17.550
we had to define these things.
We have to,
and this was,

774
00:47:17.580 --> 00:47:21.720
this was an a whole subfield of natural language processing,
which was called,

775
00:47:21.750 --> 00:47:24.990
which is called,
uh,
a part of speech tagging.

776
00:47:24.991 --> 00:47:29.950
So we would manually tag these parts of speech and how do we that while were
you,

777
00:47:30.190 --> 00:47:33.340
there are a bunch of banks like word banks that just,

778
00:47:33.790 --> 00:47:35.830
they're like dictionaries where they have like,

779
00:47:36.100 --> 00:47:39.560
just give it a word and it'll tell you the part of speech.
These are prerecorded.

780
00:47:39.940 --> 00:47:42.550
And so that's,
that's one way to do it.
But with deep learning,

781
00:47:42.910 --> 00:47:47.770
we have to think about this at a higher level where it's just data in data out.

782
00:47:48.070 --> 00:47:49.690
We don't,
don't even,
I know,
you know,

783
00:47:49.691 --> 00:47:53.330
as humans we want to worry about these things,
but we won't have to,

784
00:47:53.350 --> 00:47:56.830
we don't have to because it'll learn what are pronouns more or less.

785
00:47:57.040 --> 00:48:00.640
It's not going to define them as Pronoun.
But internally and it's hidden states,

786
00:48:00.670 --> 00:48:03.610
the more data we feed it,
it will learn what a pronoun is.

787
00:48:03.730 --> 00:48:08.440
And by that I mean when to use it,
when not to use it.
One more question,

788
00:48:13.880 --> 00:48:14.713
<v 2>uh,</v>

789
00:48:18.040 --> 00:48:21.970
<v 1>the bioinformatics question.
What about deep learning in bioinformatics?</v>

790
00:48:22.330 --> 00:48:24.700
What about it?
Uh,
so I guess you're asking about use cases.

791
00:48:24.701 --> 00:48:27.700
So bioinformatics are like,

792
00:48:28.090 --> 00:48:32.240
like high scans and stuff,
right?
Like bio,
uh,

793
00:48:32.410 --> 00:48:35.590
thumb prints and stuff?
Yes,
absolutely.

794
00:48:35.591 --> 00:48:39.690
We could use machine learning for that to learn what to learn,

795
00:48:39.700 --> 00:48:43.690
what a certain biological signature looks like.

796
00:48:43.691 --> 00:48:45.910
Like what is this type of person,

797
00:48:45.911 --> 00:48:50.080
what is their biological signature looked like by giving a data of a certain

798
00:48:50.081 --> 00:48:53.710
type?
Remember it's a label,
it's a class.
It could be a classification problem.

799
00:48:54.010 --> 00:48:57.010
That's kind of generally what I,
what I see bioinformatic,

800
00:48:57.011 --> 00:49:01.150
they use case for deep learning in bioinformatics.
I'm also anomaly detection.

801
00:49:01.300 --> 00:49:06.040
Like,
uh,
maybe somebody,
uh,
is very different.
So,

802
00:49:06.400 --> 00:49:09.910
uh,
somebody who has some off genetics,

803
00:49:10.060 --> 00:49:13.030
some one of their genes is like different than the rest.
Uh,

804
00:49:13.160 --> 00:49:17.860
and it can be used for anomaly detection,
which would be unsupervised clustering.

805
00:49:17.920 --> 00:49:21.610
And for that I would use state of the art generative adversarial networks.

806
00:49:21.790 --> 00:49:24.430
That's it for questions.
Let's get back into this.
Okay.

807
00:49:27.580 --> 00:49:31.840
Uh,
where were we?
So back to this,
to the loop pumps was just,

808
00:49:31.841 --> 00:49:33.290
so here's where the attention mechanism.

809
00:49:33.500 --> 00:49:36.140
So the attention mechanism and the only time I'm going to answer questions after

810
00:49:36.141 --> 00:49:39.320
this is at the very end.
So that's in 10 minutes,
the loop function.

811
00:49:39.321 --> 00:49:41.840
We're going to get the next input.
So what are we doing here?

812
00:49:41.841 --> 00:49:45.500
We're going to get the dot product between the previous output and the weights

813
00:49:45.530 --> 00:49:46.730
and then add the bias.

814
00:49:46.880 --> 00:49:50.450
So Matrix multiply the previous output and the weights and the bias.

815
00:49:50.570 --> 00:49:53.810
And you remember to think about that br thing that I just did,
the Vr Demo,

816
00:49:53.990 --> 00:49:55.070
that's what's happening here.

817
00:49:55.100 --> 00:49:58.790
We're looping it over time and that's going to give us our output logics.

818
00:49:59.000 --> 00:50:01.670
And then we're going to use the Arg Max function.

819
00:50:01.671 --> 00:50:05.030
And this line is attention.

820
00:50:05.990 --> 00:50:07.790
This line is attention.
And what do I mean?

821
00:50:08.060 --> 00:50:12.170
The Arg Max function will return the index with the largest value across the

822
00:50:12.171 --> 00:50:13.280
axis of a tensor.

823
00:50:13.700 --> 00:50:17.480
That largest value is the one that we are picking has our prediction.

824
00:50:17.630 --> 00:50:20.570
So that picking that choosing step is attention.

825
00:50:20.750 --> 00:50:25.280
What you pay attention to.
I remember attention is a very broad term.

826
00:50:25.490 --> 00:50:29.420
It's a very broad term and we could it to pick what is the best value

827
00:50:29.421 --> 00:50:33.380
arbitrarily.
We could generate a probability distribution and say,
you know,

828
00:50:33.381 --> 00:50:35.960
over this certain threshold,
those are the values we want.

829
00:50:36.170 --> 00:50:40.820
Attention mechanisms are words that we use for how we pick what the best value

830
00:50:40.821 --> 00:50:44.990
is from a set by whatever definition of best that we define is.

831
00:50:45.080 --> 00:50:48.770
And that depends on your use case.
So this is the attention mechanism.

832
00:50:48.920 --> 00:50:51.920
Remember it's really not that complicated.
It's just one line.

833
00:50:52.190 --> 00:50:56.660
You attention mechanism has just one line of code.
Okay?
Uh,

834
00:50:57.020 --> 00:50:58.640
so then once we have our prediction,

835
00:50:58.641 --> 00:51:02.510
we're going to embed the prediction for the next input using this embedding look

836
00:51:02.520 --> 00:51:03.680
up layer.
So we,

837
00:51:03.681 --> 00:51:07.180
we have that input and we want to feed it back into our network and that's going

838
00:51:07.181 --> 00:51:10.820
to be our next input.
And then we're going to say this,

839
00:51:10.880 --> 00:51:13.210
this line defines what the,
uh,

840
00:51:14.880 --> 00:51:18.240
what the ending sequence is going to be.
The or the ending scalar.
This,

841
00:51:18.300 --> 00:51:22.590
this tells us that we are done looping.
Uh,
and so then what we do

842
00:51:26.750 --> 00:51:28.040
is we have to,

843
00:51:30.260 --> 00:51:31.093
<v 0>uh,</v>

844
00:51:31.730 --> 00:51:35.960
<v 1>and so this reduce all is the logical end of elements across dimensions of a</v>

845
00:51:35.961 --> 00:51:36.620
tensor.

846
00:51:36.620 --> 00:51:40.070
So we are saying this is going to output a boolean scale or are we done or not?

847
00:51:40.310 --> 00:51:43.250
And then a conditional that says is it if it's done or not,

848
00:51:43.310 --> 00:51:47.080
then continue and get that input value.
And it's just like a,
um,

849
00:51:47.150 --> 00:51:50.440
I remember from data structures and algorithms when we would,
uh,

850
00:51:50.480 --> 00:51:54.260
for binary trees and for,
uh,
for any kind of,

851
00:51:55.580 --> 00:51:57.800
or for any kind of tree like structure,

852
00:51:57.920 --> 00:52:00.920
we would then set the previous day to the current state and then the previous,

853
00:52:01.110 --> 00:52:03.860
um,
leaf to this leaf that that's kind of what we're doing here.

854
00:52:04.130 --> 00:52:07.520
It's the same thing.
It's a,
it's,
it's a data structure.
It is a data structure.

855
00:52:07.730 --> 00:52:09.320
A cell is a data structure.

856
00:52:09.530 --> 00:52:12.650
And the time element is why we're switching the previous to the current.

857
00:52:12.860 --> 00:52:16.090
This is that you should remember this from data structures and algorithms.
Um,

858
00:52:16.170 --> 00:52:18.380
by the way,
if you haven't taken data structures and algorithms,

859
00:52:18.590 --> 00:52:20.260
definitely do that and see my,
uh,

860
00:52:20.510 --> 00:52:24.230
how to succeed in any programming interview video.
Okay.

861
00:52:24.440 --> 00:52:27.770
So we've got to start wrapping up.
Okay.
So I'm getting notes.

862
00:52:27.771 --> 00:52:28.640
We got to start wrapping up.

863
00:52:28.730 --> 00:52:32.180
So what I'm going to do is I'm going to start reading this.
Can we,

864
00:52:32.230 --> 00:52:33.440
we can end in eight minutes.

865
00:52:34.370 --> 00:52:34.910
<v 0>Fine.</v>

866
00:52:34.910 --> 00:52:39.200
<v 1>Five minutes.
Okay.
We have five minutes guys.
This is just a onetime thing.</v>

867
00:52:39.201 --> 00:52:40.430
Next time I'm going to be,
uh,

868
00:52:41.870 --> 00:52:44.990
I won't go over this much over the time I'm allotted,

869
00:52:44.991 --> 00:52:48.090
so don't worry about it guys.
So let's,
let's go.
So we had that out.

870
00:52:48.100 --> 00:52:52.100
So I'm going to be reading off of my code because we are running out of time,

871
00:52:52.101 --> 00:52:54.200
so it don't worry.
We are,
we're,
we're almost done.

872
00:52:54.201 --> 00:52:58.490
We only have a few more lines of code.
Uh,
and so let me explain it.

873
00:52:58.491 --> 00:53:00.110
So that's our looping mechanism.

874
00:53:00.380 --> 00:53:04.100
And so what we're saying is we're doing two loopy and mechanisms,

875
00:53:04.101 --> 00:53:07.850
one for just the first date because we have no data.
We want to fill the data.

876
00:53:08.060 --> 00:53:12.090
And then we do our main looping mechanism,
which is what we just did know.

877
00:53:12.240 --> 00:53:16.100
A cell is not a tree data structure.
It's just ate an analogy I was using.

878
00:53:16.580 --> 00:53:19.700
A tree has nothing to do with this.
It was just an analogy.
Don't worry about it.

879
00:53:20.480 --> 00:53:24.350
So,
so this is our loop,
right?

880
00:53:24.560 --> 00:53:25.630
This is the looping state.
This,

881
00:53:25.950 --> 00:53:28.320
we are doing this manually and if you don't want to do this,

882
00:53:28.470 --> 00:53:32.460
tensorflow has a line of code that does all of this for us.
Okay?

883
00:53:32.490 --> 00:53:35.040
But we want to look at how this works.
And so we are looping,

884
00:53:35.041 --> 00:53:38.040
we're taking that input and the previous time step and we are feeding it back

885
00:53:38.041 --> 00:53:38.874
into the network.

886
00:53:39.120 --> 00:53:42.780
And that's going to give us our decoder output and the Dakota final state.

887
00:53:43.020 --> 00:53:47.070
Do we care about the final state?
No.
We care about the output value.

888
00:53:47.310 --> 00:53:50.190
The only final state we care about is for our encoder.

889
00:53:51.690 --> 00:53:56.360
The only final state
that is true Jordan.
Uh,

890
00:53:56.750 --> 00:53:59.090
why do I have to listen to someone?
It is my channel is true.

891
00:53:59.120 --> 00:54:01.820
I'm getting my own studio.
Don't worry about it.
It's going to happen soon.

892
00:54:02.000 --> 00:54:03.080
We are building up,

893
00:54:03.140 --> 00:54:07.280
we're building guys were building an ml empire and we're going to get there.

894
00:54:07.370 --> 00:54:09.650
Don't worry about it.
Uh,
so

895
00:54:11.560 --> 00:54:15.400
that's our decoder output.
And we want,
what we want to do is that decoder output.

896
00:54:15.610 --> 00:54:18.160
We want to format it into a valid prediction.

897
00:54:18.450 --> 00:54:22.480
We want to format into a valid prediction.
And to do that,

898
00:54:22.840 --> 00:54:25.690
we're going to flatten the matrix and then we're going to get that prediction

899
00:54:25.691 --> 00:54:26.650
value.
Okay?

900
00:54:26.651 --> 00:54:29.860
So we're going to find the matrix and get that prediction value because we want

901
00:54:29.861 --> 00:54:34.861
our tensors to be the same size and shape and the Dakota prediction is going to

902
00:54:34.901 --> 00:54:38.320
be our final prediction value.
Okay?
And we'll,
we'll,
we're going to do,

903
00:54:38.321 --> 00:54:42.080
and I'll talk about what we're going to do is,
what we're going to do is,
um,

904
00:54:46.090 --> 00:54:46.923
<v 0>okay,</v>

905
00:54:47.300 --> 00:54:51.090
<v 1>uh,
we,
we're going to minimize a loss using cross entropy.</v>

906
00:54:51.091 --> 00:54:55.230
So the quarter prediction and the actual value is what we're going to minimize.

907
00:54:55.231 --> 00:54:57.450
And let's talk about that.
And the training we're going to function,

908
00:54:57.451 --> 00:54:59.610
we're going to be using is Adam to do this.

909
00:54:59.910 --> 00:55:04.520
So now he's four our helper functions.
We defined the helper function.
Uh,

910
00:55:04.750 --> 00:55:07.980
the,
this is the data that it's going to generate.
Okay?

911
00:55:07.981 --> 00:55:11.760
It's going to be this sequence of numbers and we're going to continuously

912
00:55:11.761 --> 00:55:15.870
generate that data using this next feed function for all those batches.

913
00:55:16.080 --> 00:55:18.640
We're going to add the end of sentence and pat it,
uh,

914
00:55:18.660 --> 00:55:22.200
and we're going to continuously do that and feed it into our,
uh,
placeholders.

915
00:55:22.530 --> 00:55:25.860
And this is the training staff and we're just a bunch of print statements and

916
00:55:25.861 --> 00:55:28.050
we're saying run the session,
um,

917
00:55:29.140 --> 00:55:33.540
compared to the loss and then print out the values and minimize a loss every

918
00:55:33.541 --> 00:55:36.120
time.
So in one minute,
let me explain exactly what's happening here.

919
00:55:36.180 --> 00:55:39.720
We have our input and our predicted output.

920
00:55:41.870 --> 00:55:42.703
<v 0>Okay?</v>

921
00:55:43.150 --> 00:55:47.590
<v 1>Okay.
So our member,
these are our inputs are all of those generated values.
Four,</v>

922
00:55:47.591 --> 00:55:49.120
eight,
five,
four,
eight,
six.

923
00:55:49.270 --> 00:55:52.720
And we're patting them with Zeros and our predicted value.
Remember because we,

924
00:55:52.950 --> 00:55:56.230
uh,
our predicted value is going to be that decoded output and we,

925
00:55:56.260 --> 00:56:00.700
and what we're minimizing is the difference between the predicted output and the

926
00:56:00.701 --> 00:56:04.750
initial input.
And that's the last that we are minimizing over time.

927
00:56:04.900 --> 00:56:09.580
So eventually it's going to look like this where it's going to be,

928
00:56:09.850 --> 00:56:12.580
the predicted output is going to look exactly like the input.

929
00:56:12.790 --> 00:56:16.840
And I have this plot right here,
um,
that shows just the last minimizing.
Okay.

930
00:56:16.990 --> 00:56:20.250
So we uh,
so that's,
that's it for the code.
Um,

931
00:56:20.470 --> 00:56:23.560
next time I'm going to have more time and I'm going to more,

932
00:56:24.040 --> 00:56:26.220
I'm going to have more time.
Okay.
So,
um,

933
00:56:26.580 --> 00:56:29.530
and the code is all their undocumented and I'm going to add even more examples

934
00:56:29.531 --> 00:56:30.340
to the code for you.

935
00:56:30.340 --> 00:56:33.570
I have two other Python ipython notebooks and I'm going to add to this coach.

936
00:56:33.600 --> 00:56:37.030
You guys get extra help.
Two questions and we're outta here.
So

937
00:56:39.470 --> 00:56:42.350
yes,
there are girls here.
We need more women in machine learning.
Women,

938
00:56:42.351 --> 00:56:44.750
thank you for being here.
We need more women in machine learning.

939
00:56:45.120 --> 00:56:48.020
Spread the word and be nice guys.
Okay.
We,
this is a,

940
00:56:48.140 --> 00:56:53.080
this is a gender equal,
everybody equal opportunity place.
Do you rec?

941
00:56:53.300 --> 00:56:55.910
Oh my God.
Okay.
So do you,

942
00:56:55.970 --> 00:57:00.190
how do you choose learning rate for faster convergence?
Remember,
uh,

943
00:57:00.310 --> 00:57:03.590
Avi Group learning rate is one of those hyper parameters that we want to,

944
00:57:03.710 --> 00:57:06.980
that we kind of guess and check,
right?
Like all hyper parameters.

945
00:57:06.981 --> 00:57:09.770
But if you look at papers,
papers are a good source,

946
00:57:09.950 --> 00:57:13.970
look at their results and copy them and also get hub code and I python notebooks

947
00:57:13.971 --> 00:57:17.720
because you can see the output.
One more question and we're out of here.
Okay.

948
00:57:23.200 --> 00:57:27.520
Is it possible to train a standard RNN on a non reverse translation problem

949
00:57:27.670 --> 00:57:29.140
where sentences are reversed?

950
00:57:33.200 --> 00:57:35.240
Yeah.
Because it would still be a sequence of words,
right?

951
00:57:35.241 --> 00:57:39.000
I don't see why it wouldn't,
it would,
it would still be a sequence of words.
Uh,

952
00:57:39.260 --> 00:57:43.130
you might have to flip the,
the direction.

953
00:57:43.160 --> 00:57:46.910
So maybe you would just have a backwards,
a backwards,
uh,

954
00:57:47.000 --> 00:57:50.300
layer instead of both a forward and backwards there.
Uh,
but yeah,

955
00:57:50.301 --> 00:57:52.310
you could absolutely do that.
It's just a sequence.
Okay.

956
00:57:52.311 --> 00:57:56.950
So that's it for the questions.
Okay.
Thanks guys for showing up.

957
00:57:56.980 --> 00:57:57.910
I appreciate it.

958
00:58:01.520 --> 00:58:05.190
We need to learn how to learn our hyper parameters.
Okay.
Love you guys.

959
00:58:05.460 --> 00:58:07.710
Code's going to happen in,
in,
in at noon.

960
00:58:08.170 --> 00:58:10.920
Special things to upload Vr for the space and to Assad.

961
00:58:10.921 --> 00:58:15.810
Balaban Ian for hosting this.
Uh,
and uh,
please subscribe if you haven't,

962
00:58:15.900 --> 00:58:17.970
tell your friends,
subscribe,
subscribe,
subscribe.

963
00:58:18.120 --> 00:58:22.290
I'm trying to hit a hundred k by April 15th.
That's the goal.
Um,
okay.

964
00:58:22.291 --> 00:58:25.170
And then 500 k by the end of the year.
I'm just saying it right now,

965
00:58:25.171 --> 00:58:29.220
so then I have to do it 500 k by the end of the year.
For now.

966
00:58:29.221 --> 00:58:33.780
I've got to make this empire even bigger.
So thanks for watching.

