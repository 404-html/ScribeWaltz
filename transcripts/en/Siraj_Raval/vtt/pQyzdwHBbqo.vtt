WEBVTT

1
00:00:00.680 --> 00:00:05.470
That feeling when you read a great paper,
but there's no code.
Hello world.

2
00:00:05.500 --> 00:00:10.500
It's the Raj and the practice of actually implementing a technique from a

3
00:00:10.901 --> 00:00:15.901
research paper into code is supremely useful to learn how it all works.

4
00:00:16.880 --> 00:00:20.890
In this video,
we'll implement the model from neural style transfer,

5
00:00:21.100 --> 00:00:26.100
a landmark paper that introduced the idea of applying filters in the style of a

6
00:00:27.221 --> 00:00:30.520
given artist to any image using deep learning.

7
00:00:30.730 --> 00:00:33.400
If we just want the code for the paper,

8
00:00:33.580 --> 00:00:38.580
it's best to first search the web to see if that code already exists.

9
00:00:39.370 --> 00:00:43.960
This saves us a lot of times since implementing it isn't a simple task.

10
00:00:44.230 --> 00:00:49.230
We can find a bunch of research papers using the popular tool archive sanity.

11
00:00:50.200 --> 00:00:54.790
It indexes the latest papers submitted to the Open Journal Archive.

12
00:00:55.150 --> 00:00:59.290
There's also Twitter and Reddit for keeping up to date with the field,

13
00:00:59.500 --> 00:01:03.940
but a lot of time the code isn't linked to the paper.

14
00:01:03.970 --> 00:01:07.870
In a post,
we can use a tool called [inaudible],

15
00:01:07.990 --> 00:01:12.280
which links papers with code to see if the code exists.

16
00:01:12.610 --> 00:01:13.540
If it's not there,

17
00:01:13.541 --> 00:01:18.160
we can go straight to get hub and search for appeal of the keywords from the

18
00:01:18.161 --> 00:01:22.090
paper's title to see if anything promising shows up.

19
00:01:22.390 --> 00:01:25.930
If there's no code there,
well it's time to code it ourselves.

20
00:01:27.190 --> 00:01:30.610
So how do you choose which paper to implement?

21
00:01:30.940 --> 00:01:35.940
Ask yourself what part of the machine learning research pipeline interests you

22
00:01:36.431 --> 00:01:39.850
the most?
Are you really into neural networks?

23
00:01:40.150 --> 00:01:45.150
How about unsupervised learning or attention mechanisms or stochastic models or

24
00:01:46.300 --> 00:01:49.390
evolutionary computing or cell folding cardboard?

25
00:01:50.170 --> 00:01:54.580
You've got to first figure out what makes you excited.

26
00:01:54.700 --> 00:01:55.990
For me personally,

27
00:01:55.991 --> 00:02:00.991
it's either novel optimization techniques or generative models using

28
00:02:01.781 --> 00:02:05.620
probabilistic programming.
List them out in your notes,

29
00:02:05.830 --> 00:02:10.060
then start searching for important papers in that field.

30
00:02:10.420 --> 00:02:14.320
The best paper is the one you actually enjoy reading.

31
00:02:14.590 --> 00:02:16.930
There are a lot of papers out there,

32
00:02:16.931 --> 00:02:19.930
so be sure to pick one that's well written.

33
00:02:20.290 --> 00:02:25.290
Usually these come out of top tier universities or research teams in smaller

34
00:02:25.361 --> 00:02:28.690
universities that have been tackling the problem for years.

35
00:02:30.000 --> 00:02:33.090
I tend to look for papers with an industry focus.

36
00:02:33.360 --> 00:02:38.360
A lot of papers from academia are cryptic and lacking in detail some

37
00:02:39.211 --> 00:02:44.211
intentionally so because their goal is to publish as many papers as possible

38
00:02:44.910 --> 00:02:46.680
that look good on the surface.

39
00:02:46.980 --> 00:02:51.570
Industry focus papers have real life applicability so they are easier to

40
00:02:51.571 --> 00:02:56.260
reproduce.
Go onto our neural style transfer paper.

41
00:02:56.560 --> 00:03:01.560
I've got a great video called how read a research paper that I've linked to in

42
00:03:02.411 --> 00:03:03.550
the video description.

43
00:03:04.300 --> 00:03:09.300
It all boils down to carefully read the paper from start to finish multiple

44
00:03:09.671 --> 00:03:11.260
times as necessary.

45
00:03:11.680 --> 00:03:16.600
There will be a lot or a few terms that you don't understand as you read it.

46
00:03:16.870 --> 00:03:19.810
Make a note of them.
You can look them up later.

47
00:03:20.200 --> 00:03:24.520
If we read the paper a few times and still don't understand the gist of it,

48
00:03:24.730 --> 00:03:29.730
we can follow the tree of citations at the bottom of the page and read relevant

49
00:03:29.891 --> 00:03:34.000
papers and if there's a paywall,
just pirate it.
Because Yolo,

50
00:03:34.750 --> 00:03:39.580
once we've traversed the whole tree of knowledge as all papers are built on

51
00:03:39.581 --> 00:03:40.510
previous knowledge,

52
00:03:40.540 --> 00:03:45.540
we'll be better equipped to interpret this paper before we start building our

53
00:03:45.781 --> 00:03:46.620
model.
Well,

54
00:03:46.630 --> 00:03:51.630
one to first pay attention to the input data that was used by the authors.

55
00:03:52.170 --> 00:03:57.090
If we use a different training set with images that aren't,
say high definition,

56
00:03:57.240 --> 00:04:00.240
but the authors used high definition images,

57
00:04:00.480 --> 00:04:04.260
there's a chance our algorithm won't perform as well as it did for the author.

58
00:04:05.290 --> 00:04:05.520
<v 2>Okay.</v>

59
00:04:05.520 --> 00:04:10.520
<v 0>Our main task will be to understand the variables and operators of the model</v>

60
00:04:11.040 --> 00:04:13.200
that the authors chose to use.

61
00:04:13.590 --> 00:04:18.590
We're essentially translating math equations in the paper into code and data.

62
00:04:19.230 --> 00:04:21.270
So before jumping into the code,

63
00:04:21.570 --> 00:04:26.570
we have to fully understand the equations and processes in these equations.

64
00:04:27.120 --> 00:04:31.920
Notations for variables and operators can change from one mathematical

65
00:04:31.921 --> 00:04:35.940
convention to another and from one research group to another.

66
00:04:36.360 --> 00:04:38.970
We should know what each variable is,

67
00:04:38.971 --> 00:04:41.700
whether it's a scaler or a matrix,

68
00:04:41.701 --> 00:04:45.600
and what every operator is doing on these variables.

69
00:04:45.990 --> 00:04:48.810
A paper is a succession of equations,

70
00:04:48.840 --> 00:04:53.840
so we'll need to know how we'll plug the output of equation one into the input

71
00:04:54.240 --> 00:04:57.540
of equation to once we've read and understood the paper,

72
00:04:57.541 --> 00:05:02.280
it's time to create a prototype.
This can be a very time consuming process.

73
00:05:02.430 --> 00:05:05.250
The more detail we put into it.
So to start off,

74
00:05:05.251 --> 00:05:10.251
let's use the highest level library we can to get something working as fast as

75
00:05:10.351 --> 00:05:11.184
possible.

76
00:05:11.390 --> 00:05:16.110
Cara Ross is a great deep learning library that lets us build neural networks in

77
00:05:16.111 --> 00:05:20.820
python focused on fast experimentation,
good old special k

78
00:05:21.850 --> 00:05:22.683
<v 2>wait,
that's taken</v>

79
00:05:22.960 --> 00:05:24.310
<v 0>the paper details.</v>

80
00:05:24.340 --> 00:05:29.340
A system that generates an image with the same content as a base image but with

81
00:05:29.861 --> 00:05:31.660
the style of a different picture.

82
00:05:31.900 --> 00:05:36.730
So there are three parts to the workflow,
a content extractor,

83
00:05:36.910 --> 00:05:40.690
a style extractor,
and a merger.
In the first part,

84
00:05:40.750 --> 00:05:42.340
the content extractor,

85
00:05:42.550 --> 00:05:46.360
they found a way to separate the semantic content of an image.

86
00:05:46.660 --> 00:05:51.660
It says they used a convolutional neural network called VGG calmness or neural

87
00:05:53.561 --> 00:05:58.561
networks that are well suited for image classification tasks and Vgg 19 was

88
00:05:59.811 --> 00:06:04.811
trained on thousands of images and is capable of classifying images right out of

89
00:06:05.151 --> 00:06:05.984
the box.

90
00:06:06.260 --> 00:06:10.760
It looks like they use the output of one of the hidden layers as a content

91
00:06:10.790 --> 00:06:13.250
extractor.
That makes sense.

92
00:06:13.370 --> 00:06:16.280
The hidden layers of a continent extract,

93
00:06:16.400 --> 00:06:19.970
high level features of an image and the deeper the layer,

94
00:06:19.971 --> 00:06:24.971
the more high level the attributes will be that the layer identifies between

95
00:06:25.071 --> 00:06:29.690
taking an image as input and outputs.
A guess as to what it is.

96
00:06:30.020 --> 00:06:35.020
A CNN is doing transformations to turn the image pixels into an internal

97
00:06:35.301 --> 00:06:37.880
understanding of the content of the image.

98
00:06:38.180 --> 00:06:42.950
We can use one of the intermediate semantic representations in a comnet to

99
00:06:42.951 --> 00:06:45.080
compare the contents of two images.

100
00:06:45.500 --> 00:06:50.240
If we pass two different images through a comnet after being passed through a

101
00:06:50.241 --> 00:06:53.270
few hidden layers,
their representations,

102
00:06:53.271 --> 00:06:55.430
we'll be very close in raw value.

103
00:06:55.760 --> 00:07:00.650
If we pass both the final image and the content image and find the distance

104
00:07:00.651 --> 00:07:04.070
between the intermediate representations of those images,

105
00:07:04.100 --> 00:07:08.240
we have the content loss.
The equation is listed as such.

106
00:07:08.270 --> 00:07:13.190
This summation notation makes the concept look harder than it really is.

107
00:07:13.460 --> 00:07:16.880
We make a list of layers where we want to compute the content loss.

108
00:07:16.910 --> 00:07:21.380
We pass both images through the network until it's at a particular layer in the

109
00:07:21.381 --> 00:07:23.500
list,
take it out of that layer squared.

110
00:07:23.500 --> 00:07:28.010
The difference between each corresponding value in the output and sum them all

111
00:07:28.011 --> 00:07:32.060
up.
We do this for every layer in the list and some of those up.

112
00:07:32.120 --> 00:07:37.120
We're also multiplying each of the representations by some value Elfa called

113
00:07:37.791 --> 00:07:41.150
content weight after finding their differences and squaring.

114
00:07:41.750 --> 00:07:45.980
The second part of the workflow was to extract the style of an image.

115
00:07:46.310 --> 00:07:50.240
It looks like they used the same idea as the content extractor,

116
00:07:50.270 --> 00:07:55.270
meaning they use the output of a hidden layer but they added an additional step.

117
00:07:55.730 --> 00:08:00.730
It used a correlation estimator based on the gram matrix of the filters of a

118
00:08:01.011 --> 00:08:05.390
given hidden layer.
Sounds complicated but if we read on,

119
00:08:05.391 --> 00:08:10.280
it seems like what that does is it destroys the semantics of the image but

120
00:08:10.281 --> 00:08:14.510
preserves its basic components making an excellent texture extractor.

121
00:08:14.600 --> 00:08:19.600
A gram matrix results from multiplying a matrix with the transpose of itself and

122
00:08:19.851 --> 00:08:23.510
because every column is multiplied with every row in the matrix,

123
00:08:23.870 --> 00:08:28.220
we can think of the spatial information that was contained in the original

124
00:08:28.221 --> 00:08:30.800
representations to have been distributed.

125
00:08:30.890 --> 00:08:34.700
This Graham Matrix contains all sorts of information about the image,

126
00:08:34.730 --> 00:08:39.680
the texture,
shapes,
and styles.
Once we have that Graham Matrix,

127
00:08:39.681 --> 00:08:44.480
we can find the distance between the gram matrices of the intermediate

128
00:08:44.481 --> 00:08:49.481
representations of both our image and the style image to find out how similar

129
00:08:50.090 --> 00:08:54.980
they are in style and it's all multiplied by some value Beta known as the style

130
00:08:54.990 --> 00:08:57.000
weight.
For the last part,

131
00:08:57.001 --> 00:09:02.001
they needed to blend the content of one image with the style of another and they

132
00:09:02.251 --> 00:09:05.280
of course framed it as an optimization problem.

133
00:09:05.310 --> 00:09:10.080
As machine learning papers tend to do and in an optimization problem with some

134
00:09:10.081 --> 00:09:14.760
cost function is minimized iteratively during training to achieve a goal.

135
00:09:15.150 --> 00:09:19.680
Their costs function penalize the synthesized image if it's content was not

136
00:09:19.681 --> 00:09:24.660
equal to the desired content and its style was not equal to the desired style.

137
00:09:24.840 --> 00:09:29.490
Both the content and the style loss were added together to get the cost
function.

138
00:09:29.910 --> 00:09:34.410
They then performed backpropagation to minimize the cost by getting the gradient

139
00:09:34.411 --> 00:09:38.790
of the final image and iteratively changing it to look more and more like the

140
00:09:38.791 --> 00:09:40.290
stylized content image.

141
00:09:40.560 --> 00:09:45.560
They use an optimization technique that's terribly named called El bfgs,

142
00:09:45.990 --> 00:09:49.800
which isn't as popular as say,
stochastic gradient descent.

143
00:09:49.980 --> 00:09:51.600
If we do a bit of research,

144
00:09:51.601 --> 00:09:54.420
it looks like it's a second order optimization scheme,

145
00:09:54.450 --> 00:09:59.100
meaning it uses the derivative of the derivative that gets closer to the global

146
00:09:59.101 --> 00:10:02.040
minimum,
but the iteration cost is also bigger.

147
00:10:02.460 --> 00:10:06.240
Looks like this will likely be the term we'll need to spend the most time

148
00:10:06.241 --> 00:10:10.620
learning about,
but first let's create some naming conventions.

149
00:10:10.830 --> 00:10:15.780
We've got a content image,
a style image,
and a final synthesized image.

150
00:10:16.170 --> 00:10:21.150
We can start coding this model in care offs sequentially as a list of steps to

151
00:10:21.151 --> 00:10:23.010
help us organize our thoughts here.

152
00:10:23.310 --> 00:10:28.310
It looks like carrots doesn't use the El bfgs optimizer so we can use psi Pi for

153
00:10:29.461 --> 00:10:33.930
that part.
It's going to be important to document everything here.
As we code,

154
00:10:33.931 --> 00:10:36.210
since there are a lot of moving parts,

155
00:10:36.690 --> 00:10:41.250
we'll define some multidimensional arrays to help us create image variables.

156
00:10:41.520 --> 00:10:44.490
Then concatenate them all into a single tensor.

157
00:10:44.910 --> 00:10:47.880
They first synthesized a white noise image,

158
00:10:47.940 --> 00:10:51.060
then extracted the content and style of it.

159
00:10:51.450 --> 00:10:55.820
We can input our tensor into the VGG 16 model using carrot cost.

160
00:10:56.130 --> 00:11:00.420
They calculated the distance between the content of the image and the original

161
00:11:00.421 --> 00:11:04.410
content image as well as this sense between the style of the image and the

162
00:11:04.411 --> 00:11:05.760
original style image.

163
00:11:06.090 --> 00:11:10.770
We can extract data from specific layers using their numbering for both loss

164
00:11:10.771 --> 00:11:11.604
functions.

165
00:11:11.670 --> 00:11:16.670
Both distances were used to calculate the cost function and thus the gradient as

166
00:11:17.701 --> 00:11:22.380
is the case in machine learning.
If the gradient is zero,
we are done optimizing,

167
00:11:22.381 --> 00:11:27.381
but if it's not we'll run another iteration of optimization that will generate a

168
00:11:28.021 --> 00:11:33.021
new final image that's closer to the content image content wise and closer to

169
00:11:33.271 --> 00:11:38.100
this style image style wise and if the preset number of iterations is achieved

170
00:11:38.370 --> 00:11:41.070
finish.
Otherwise we'll go back to the start.

171
00:11:41.490 --> 00:11:45.630
After a couple of iterations we can check the result in our local directory and

172
00:11:45.631 --> 00:11:47.310
it seems to work well enough.

173
00:11:47.670 --> 00:11:51.720
We can go back and tweak the parameters as necessary to get a result.

174
00:11:51.721 --> 00:11:56.230
We're comfortable with.
Now that we have a prototype version done,
if we want,

175
00:11:56.231 --> 00:12:01.231
we can write a more detailed precise version in pure python or a lower level

176
00:12:01.310 --> 00:12:04.930
deep learning library like tensorflow.
Do you want to be the very best,

177
00:12:04.931 --> 00:12:09.010
like no one ever was.
We'll hit the subscribe button and it'll happen for now.

178
00:12:09.040 --> 00:12:11.590
I've got to use Pi torch,
so thanks for watching.

