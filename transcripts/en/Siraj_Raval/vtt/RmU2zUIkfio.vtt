WEBVTT

1
00:00:00.070 --> 00:00:04.930
Hello world it Saroj and healthcare is the most exciting application of AI

2
00:00:04.931 --> 00:00:09.610
technology.
Using Ai.
We can cure all major diseases,
help doctors,

3
00:00:09.611 --> 00:00:10.600
treat more patients,

4
00:00:10.601 --> 00:00:15.490
and even enhance our own cognitive abilities to superhuman levels.

5
00:00:15.760 --> 00:00:20.760
The school of AI just announced its first global hackathon called health hack at

6
00:00:20.801 --> 00:00:23.860
a launch event in Singapore's National Gallery.

7
00:00:24.070 --> 00:00:29.070
So I flew to Singapore to give a keynote on AI in healthcare at the event as

8
00:00:30.101 --> 00:00:33.460
well as attend a panel discussion on the same topic.

9
00:00:33.790 --> 00:00:38.790
In this video you'll get to see exclusive footage of that keynote and the panel

10
00:00:39.251 --> 00:00:44.230
discussion.
I hope you find it educational and inspirational and if you're new,

11
00:00:44.320 --> 00:00:47.680
subscribe to get notified when I release my videos in Jordan.

12
00:01:04.400 --> 00:01:05.233
<v 1>Okay,</v>

13
00:01:05.440 --> 00:01:09.880
<v 2>let me introduce you to Siraj.
Revolver is the founder of the school of Ai,</v>

14
00:01:10.060 --> 00:01:10.601
as I said,

15
00:01:10.601 --> 00:01:14.920
an international nonprofit with the mission to offer a world class education in

16
00:01:14.921 --> 00:01:17.740
AI to anyone on earth for free.

17
00:01:18.250 --> 00:01:23.250
So his youtube channel of Ai Education content has over 500,000 subscribers and

18
00:01:23.771 --> 00:01:27.160
he partnered with you Udacity to launch their deep learning foundations

19
00:01:27.161 --> 00:01:30.580
nanodegree program.
Besides being an AI educator,

20
00:01:30.880 --> 00:01:33.970
Serratia is also a bestselling author,
public speaker,

21
00:01:34.300 --> 00:01:39.300
data scientist contributed to open source projects like open AI and wrapper.

22
00:01:40.330 --> 00:01:43.140
So please join us in welcoming Serratia vol.

23
00:01:44.860 --> 00:01:46.450
<v 3>Thank you everybody.
Thank you.
Thank you.</v>

24
00:01:47.710 --> 00:01:52.050
<v 4>See how loud this mic is?
Okay,
perfect.
Good.
Very good.
Okay.</v>

25
00:01:52.080 --> 00:01:54.300
I'm very excited to be here in Singapore.
Uh,

26
00:01:54.301 --> 00:01:57.810
yesterday I flew in from Los Angeles.
He was a 17 hour flight.

27
00:01:58.020 --> 00:02:01.650
So I'm here for a reason.
I believe in the people here.
Um,

28
00:02:01.710 --> 00:02:03.780
I think we can do a lot of great things.

29
00:02:03.870 --> 00:02:07.290
This community here in Singapore can do a lot of great things for health care.

30
00:02:07.650 --> 00:02:12.650
And so in this talk I'm going to talk about AI in healthcare specifically.

31
00:02:13.711 --> 00:02:18.450
I'm going to talk about a AI in drug discovery and a few other applications.

32
00:02:18.451 --> 00:02:21.390
I'm going to list three applications,
my thoughts on them,

33
00:02:21.590 --> 00:02:24.150
and then we're going to talk about for each of these applications,

34
00:02:24.210 --> 00:02:28.230
how this would work using technology.
We're going to get into the specifics.
Okay.

35
00:02:29.670 --> 00:02:34.020
So,
um,
before I start,
I want to talk about what,
what I'm most excited about.

36
00:02:34.200 --> 00:02:39.200
What I'm most excited about is the convergence of biology and information.

37
00:02:40.350 --> 00:02:44.340
So if we think about it,
uh,
the bite by gene and the atom,

38
00:02:44.910 --> 00:02:48.180
let's just think about that for a second.
The bite,
the gene and the Adam.

39
00:02:48.630 --> 00:02:51.890
These are three fundamental building blocks,
uh,

40
00:02:51.990 --> 00:02:56.730
for three different subjects.
The bite for digital information,
right?

41
00:02:56.731 --> 00:02:59.920
The bite from the bites comes,
kilobytes,
megabytes,

42
00:02:59.921 --> 00:03:03.670
and then programs and algorithms and everything else.
Data from the gene,

43
00:03:03.760 --> 00:03:07.240
the gene,
you know,
we all have,
we all have DNA genetics.

44
00:03:07.720 --> 00:03:09.790
A lot of it tell,
makes us who we are,

45
00:03:09.791 --> 00:03:13.660
our traits over into how we think.

46
00:03:13.810 --> 00:03:18.790
A lot of that comes from genetics.
And so biology from,
from the,
from DNA,

47
00:03:18.791 --> 00:03:23.440
from genetic spring's life.
And the third is the atom.
So matter writes,

48
00:03:23.710 --> 00:03:26.710
a lot of amazing innovations have come from matter.

49
00:03:26.920 --> 00:03:31.920
And so in the 20th century we saw a lot of incredible innovation segmented in

50
00:03:33.641 --> 00:03:36.910
each of these fields.
So in biology we sell revolutions in healthcare,

51
00:03:37.150 --> 00:03:42.150
in terms of curious for new diseases and ways of dealing with genes such that we

52
00:03:43.241 --> 00:03:46.060
can create new gene therapies,
different things like that.

53
00:03:46.240 --> 00:03:47.710
When it came to the bite.
I mean,

54
00:03:47.830 --> 00:03:51.100
I don't even need to explain all that's happened in computer science in the past

55
00:03:51.220 --> 00:03:55.810
five decades.
And then for the Adam,
uh,
you know,
physics,
geophysics,

56
00:03:55.840 --> 00:03:58.090
I mean earthquake prediction,
there's so much there as well.

57
00:03:58.450 --> 00:04:01.540
So that's what defined progress in the 20th century.

58
00:04:01.720 --> 00:04:03.520
These three fundamental building blocks,

59
00:04:03.640 --> 00:04:08.140
and it's specifically how we humans use those to make innovations.

60
00:04:08.440 --> 00:04:12.550
But I'm here to say that in the 21st century where we are right now,

61
00:04:12.760 --> 00:04:16.390
what's going to define progress is the convergence of these three fundamental

62
00:04:16.391 --> 00:04:17.224
building blocks.

63
00:04:17.500 --> 00:04:22.360
So the one convergence in particular I'm very excited about is the convergence

64
00:04:22.361 --> 00:04:26.980
of the bite and the gene of information and biology.
Because we have,

65
00:04:27.010 --> 00:04:31.270
because of a computer science,
because of data,
because of algorithms,

66
00:04:31.271 --> 00:04:36.190
we have so much biological data that is now available to us.
You know,

67
00:04:36.191 --> 00:04:41.191
I just had a test done and I had blood drawn and it was like phase three vials

68
00:04:41.801 --> 00:04:44.050
and the lady was taken away.
I was like,
hold on,
I want that data.

69
00:04:44.230 --> 00:04:46.240
She didn't even understand what I was talking about.
Like these,

70
00:04:46.241 --> 00:04:48.280
these three fields are segmented there.
They're separate,

71
00:04:48.281 --> 00:04:52.180
but they need to converge because blood,
my blood has so much data.

72
00:04:52.181 --> 00:04:56.680
I want to know about myself and,
and,
and,
and what I'm into,
what,
what,

73
00:04:56.681 --> 00:05:00.310
how I can optimize my life better using what's built into me,

74
00:05:00.311 --> 00:05:04.600
like what types of food I should eat,
what types of activities I should be doing,

75
00:05:04.900 --> 00:05:08.620
what types of climates I'm best suited for.
Things like that.
And Algorithms,

76
00:05:08.650 --> 00:05:12.790
information can help approximate those solutions.
But that's just one example.

77
00:05:12.791 --> 00:05:14.500
We have several coming up.
So that's,

78
00:05:14.501 --> 00:05:18.340
I just wanted to give you a kind of base frame of thought.

79
00:05:18.430 --> 00:05:21.010
As we go into this AI in healthcare space,

80
00:05:21.160 --> 00:05:24.190
we gotta be thinking about the convergence of the bite and the gene in

81
00:05:24.191 --> 00:05:27.280
particular.
And that's what,
that's what AI in healthcare really is.

82
00:05:27.400 --> 00:05:30.550
So first application is drug discovery.
So,
um,

83
00:05:31.030 --> 00:05:35.320
so specifically in the U S it takes about 10 years for a drug to come to market

84
00:05:35.560 --> 00:05:40.540
and it takes tens of millions of the u s dollars.
Um,
and so that is,
I mean,

85
00:05:40.541 --> 00:05:44.230
it's generally long wherever you are in the world because of different

86
00:05:44.231 --> 00:05:47.410
regulations and that just,
that's just how it is.
That's just how it has to be.

87
00:05:47.590 --> 00:05:50.500
It's not like some,
you know,
nefarious plot.
That's just how it is.

88
00:05:50.501 --> 00:05:55.501
Because of the process of having to select the best candidates for,

89
00:05:55.870 --> 00:05:58.840
you know,
some treatment for some disease.
You know,
a lot of,
um,

90
00:05:59.240 --> 00:06:02.870
molecular biologists and,
and people working in cancer therapy,

91
00:06:03.170 --> 00:06:05.160
they have to pick from,
you know,

92
00:06:05.300 --> 00:06:09.500
11 million plus candidates for what could be the ideal cure for whatever disease

93
00:06:09.501 --> 00:06:13.190
or trying to tackle.
And roughly they're doing this by hand.

94
00:06:13.550 --> 00:06:18.550
But AI can look at this information and scale that discovery in a way that we

95
00:06:19.791 --> 00:06:23.780
humans can't write algorithms,
digital digital algorithms,

96
00:06:23.781 --> 00:06:26.840
digital intelligence can look at data in a way that we can't.

97
00:06:27.140 --> 00:06:29.750
And what this can do is it can speed up this process,

98
00:06:29.751 --> 00:06:33.350
this pipeline from basic research to clinical trials,

99
00:06:33.351 --> 00:06:38.270
all three phases to the review process and until it finally becomes a medicine

100
00:06:38.390 --> 00:06:42.200
that can save people's lives.
So,
um,

101
00:06:42.230 --> 00:06:45.560
let's talk about specifically how this would work.
You know,
technically speaking,

102
00:06:45.561 --> 00:06:49.400
right?
Let's,
let's not just talk about buzzwords.
How would this work?

103
00:06:49.520 --> 00:06:53.750
So let's say we have a key,
we have a genetic database for gene therapy.

104
00:06:53.900 --> 00:06:57.290
We would be using a genetic database.
In the case of let's say cancer,

105
00:06:57.291 --> 00:07:00.740
which I think is the most exciting because this is a major unsolved problem.

106
00:07:00.870 --> 00:07:04.000
We would have a list of a data set of possible,
uh,

107
00:07:04.100 --> 00:07:05.890
drugs that could be used to treat,
you know,

108
00:07:05.940 --> 00:07:07.400
whatever cancer you're trying to treat.

109
00:07:08.030 --> 00:07:11.660
And the idea is that how do we find the needle in the haystack in that,

110
00:07:11.810 --> 00:07:14.750
how do we find the one that sticks out from the rest that is going to be perfect

111
00:07:14.751 --> 00:07:17.120
for this specific disease that we are targeting?

112
00:07:17.540 --> 00:07:20.330
So that would be a discriminatory problem.

113
00:07:20.360 --> 00:07:24.560
We are trying to discriminate amongst a bunch of potential drugs to find,

114
00:07:24.561 --> 00:07:26.060
which was the one that could cure the disease.

115
00:07:26.750 --> 00:07:30.830
But what would be more interesting is instead of framing it as a discriminatory

116
00:07:30.831 --> 00:07:33.350
problem,
we could frame it as a generative problem.

117
00:07:33.770 --> 00:07:37.970
What we're trying to generate the cure by using the existing dataset and

118
00:07:37.971 --> 00:07:40.340
applying some sort of distribution to it.

119
00:07:40.340 --> 00:07:43.910
So what I mean by distribution is we take that initial dataset and we think of

120
00:07:43.911 --> 00:07:45.470
that as a bunch of ones and Zeros.

121
00:07:45.620 --> 00:07:50.120
So in machine learning we think of this input as a matrix.
And what a matrix is,

122
00:07:50.121 --> 00:07:53.720
is it just a group of numbers and it's a giant group of numbers can get big,

123
00:07:53.770 --> 00:07:55.160
it can get as big as you want.

124
00:07:55.610 --> 00:07:58.790
And we take this group of numbers and then we apply a series of multiplication

125
00:07:58.791 --> 00:08:01.550
operations to it that that's what AI is.

126
00:08:01.700 --> 00:08:05.750
It's a series of multiplication operations until the initial input,

127
00:08:05.840 --> 00:08:08.240
which is a group of ones and Zeros becomes an output,

128
00:08:08.300 --> 00:08:11.840
which is a different groups of ones and Zeros and from those ones and Zeros,

129
00:08:12.110 --> 00:08:15.110
this translates to a drug,
right?
So think about it,

130
00:08:15.200 --> 00:08:19.730
the bite and the gene DNA blood,
the basic building blocks of life.

131
00:08:19.731 --> 00:08:20.690
This is information.

132
00:08:21.020 --> 00:08:24.830
So we are using information that we create with electronic equipment to find the

133
00:08:24.831 --> 00:08:29.240
information for cures to diseases.
And so if we frame it as a generative problem,

134
00:08:29.390 --> 00:08:32.750
we can generate new types of drugs that would cure potential diseases.

135
00:08:33.050 --> 00:08:34.790
In particular,
a model,

136
00:08:34.820 --> 00:08:39.300
an algorithm that I find to be very promising in this field,
uh,

137
00:08:39.320 --> 00:08:42.830
are they're called generative adversarial networks,

138
00:08:43.010 --> 00:08:44.420
generative adversarial networks.

139
00:08:44.660 --> 00:08:47.900
And the idea behind these networks is that we have two competing models.

140
00:08:48.080 --> 00:08:51.290
One is trying to generate new candidates for a drug in the case of drug

141
00:08:51.291 --> 00:08:55.160
discovery.
The other one is acting as a discriminator.
And it's looking at this,

142
00:08:55.320 --> 00:08:58.620
what it outputs the,
you know,
the,
the group of ones and Zeros.
And it's saying,

143
00:08:58.830 --> 00:09:02.700
is this secure or not?
Is this,
is this valid or not?
And so this,

144
00:09:02.910 --> 00:09:06.810
these two models play a game where one's trying to generate a cure and one is

145
00:09:06.811 --> 00:09:08.760
trying to discriminate whether it's valid or not.

146
00:09:09.030 --> 00:09:12.450
And that's a very high level idea behind these two mathematical models.

147
00:09:12.810 --> 00:09:16.350
By the way,
all of this is math.
All of this has been,
it's just math.

148
00:09:16.560 --> 00:09:20.580
It's matrix math.
That's it.
It's nothing.
It's,
it's,
it's not more than that.

149
00:09:20.610 --> 00:09:24.180
It's just math.
And so people need to understand that when we're talking about,

150
00:09:24.330 --> 00:09:26.340
this is a bit of a tangent,
but it's unnecessary tangent.

151
00:09:26.520 --> 00:09:30.480
If we're talking about AI and apocalyptic AI and all this stuff,

152
00:09:30.930 --> 00:09:34.740
we're talking about the potential for humans to use math,

153
00:09:35.070 --> 00:09:40.070
specifically matrix math to either exploit other humans or to help other humans

154
00:09:40.541 --> 00:09:44.940
empower other humans.
That's,
that's the,
the issue is humans.

155
00:09:45.060 --> 00:09:48.450
It's not matrix math,
it's not AI.
Ai is just matrix math.
Okay,

156
00:09:48.451 --> 00:09:52.930
so that's a tangent back to this.
So,
generative adversarial networks,
um,

157
00:09:53.100 --> 00:09:55.620
they play this game,
generate discriminate,

158
00:09:55.980 --> 00:09:59.100
and I think this is a very useful model to use in the field.

159
00:09:59.310 --> 00:10:03.180
And so in this hackathon,
I'd like to see,
um,
people try this out,
right?

160
00:10:03.181 --> 00:10:07.060
So there's,
this is readily available.
I have several videos on this.
Um,
there,

161
00:10:07.090 --> 00:10:09.420
there's a bunch of free tutorials on the Internet about this.

162
00:10:09.540 --> 00:10:13.680
So if you searched ga and tutorial healthcare and a bunch of great tutorials are

163
00:10:13.681 --> 00:10:14.280
gonna come up.

164
00:10:14.280 --> 00:10:17.370
So that's the first application that I'm very excited for because there's so

165
00:10:17.371 --> 00:10:21.930
many people who have been affected by this.
You know,
high in particular.

166
00:10:21.931 --> 00:10:25.440
I have someone who I care about who has been affected by cancer.
And so it's,

167
00:10:25.450 --> 00:10:28.590
it's both the personal thing and I'm a general thing as well.

168
00:10:28.591 --> 00:10:32.910
So that's a very exciting one.
The second one,
automated diagnosis.
Uh,

169
00:10:32.911 --> 00:10:37.020
so if we think about the process that,
that a doctor uses to diagnose a disease,

170
00:10:37.260 --> 00:10:41.010
they are using their five senses.
They're using their human vision,

171
00:10:41.160 --> 00:10:44.100
biological vision.
They're using their sense of touch,
right?

172
00:10:44.250 --> 00:10:47.430
They're using their sometimes even smell,
um,
different things like that.

173
00:10:47.610 --> 00:10:50.220
And so to try to diagnose what a diseases,
and again,

174
00:10:50.221 --> 00:10:52.050
this is a discriminatory problem,

175
00:10:52.051 --> 00:10:55.650
they have a set that in their head and their biological intelligence,

176
00:10:55.920 --> 00:10:59.280
they have a set of potential diseases that this could be,

177
00:10:59.460 --> 00:11:03.120
and they're trying to discriminate based on the input data,
right?
What,

178
00:11:03.150 --> 00:11:05.910
what diseases could be.
And so if we frame it like this,

179
00:11:05.990 --> 00:11:09.600
the reason I'm repeating this is because we have to frame it as all,
all of it.

180
00:11:09.601 --> 00:11:14.070
This is just information healthcare,
it's all information,
digital,
GE,

181
00:11:14.100 --> 00:11:17.550
genetic information.
It's all information.
And so if we think about that process,

182
00:11:17.551 --> 00:11:22.230
how do we automate this electronically and how do we help help doctors help make

183
00:11:22.231 --> 00:11:25.470
their lives easier?
This is not something that will replace doctors.

184
00:11:25.500 --> 00:11:27.390
It could if we want it to.

185
00:11:27.600 --> 00:11:31.860
But if we frame it as how do we a doctor's ends and make it so that they can

186
00:11:31.861 --> 00:11:33.900
scale the number of people that can help,

187
00:11:34.350 --> 00:11:36.180
then this could be a good thing for doctors.

188
00:11:36.181 --> 00:11:38.970
And this then this could be something that doctors would embrace.

189
00:11:39.150 --> 00:11:43.860
So one example in particular a radiologist's,
right?
So radiologists,

190
00:11:44.040 --> 00:11:46.170
um,
there most of their job,

191
00:11:46.200 --> 00:11:50.550
80% of their job is to look at these images and use their biological vision to

192
00:11:50.551 --> 00:11:53.800
try to decipher whether or not someone has a disease.

193
00:11:54.130 --> 00:11:57.940
Ai has now been proven to be better objectively than radiologists.

194
00:11:57.950 --> 00:12:02.560
Actis what in radiologists could do with this technology is they could scale the

195
00:12:02.561 --> 00:12:05.440
number of people that can be treating at the same time.
Uh,

196
00:12:05.441 --> 00:12:08.650
and a lot of countries across the world,
there's not enough doctors to treat,

197
00:12:08.890 --> 00:12:10.270
you know,
the number of people there are.

198
00:12:10.540 --> 00:12:14.260
So this is just a scaling technology for doctors in that that's one thing that

199
00:12:14.261 --> 00:12:18.610
it does.
So automated diagnosis,
we're using convolutional networks.

200
00:12:18.880 --> 00:12:23.170
Convolutional networks are a type of,
again,
Matrix math combination.

201
00:12:23.430 --> 00:12:24.020
It's just,

202
00:12:24.020 --> 00:12:29.010
it's a particular set of operations between that input data.
You're,

203
00:12:29.050 --> 00:12:29.883
all of these,

204
00:12:29.980 --> 00:12:34.300
all AI is just a different set of operations between four input data.

205
00:12:34.390 --> 00:12:37.150
Sometimes you,
you do three multiplications and then you do a division,

206
00:12:37.420 --> 00:12:39.460
sometimes you do two multiplications and then you do,

207
00:12:39.610 --> 00:12:42.790
you multiply it by a specific type of function,
right?
So it's just,

208
00:12:42.970 --> 00:12:46.750
it's just mixing up these different operations to see what would be the ideal

209
00:12:46.810 --> 00:12:51.160
for whatever our goal is.
And in this case,
our goal would be automated diagnosis.

210
00:12:51.700 --> 00:12:55.600
So the input data in this example here are a series of brain scans for

211
00:12:55.601 --> 00:13:00.400
Alzheimer's.
And so radiologists and um,
you know,
neuroscientists and you know,

212
00:13:00.401 --> 00:13:03.640
different types of brain doctors are looking at this manually with their,

213
00:13:03.700 --> 00:13:06.370
with their biological vision.
But again,
this is information,

214
00:13:06.790 --> 00:13:08.560
this is information we can do this better.

215
00:13:08.980 --> 00:13:13.270
And so automated diagnosis is something that I'm very excited for in particular

216
00:13:13.690 --> 00:13:16.060
and we can use again,
convolutional networks,

217
00:13:16.061 --> 00:13:21.061
which are a type of matrix math operation set that's useful for classifying

218
00:13:22.061 --> 00:13:26.820
images and saying what is in this image?
So if we have a Dataset of um,

219
00:13:27.060 --> 00:13:30.010
all timers,
images with the label,
so you know,

220
00:13:30.011 --> 00:13:34.400
think of it as an excel spreadsheet.
One column is just a bunch of images of,
uh,

221
00:13:34.440 --> 00:13:35.590
of,
of brain scans.

222
00:13:35.710 --> 00:13:40.710
And the other column is just yes or no is does this patient have Alzheimer's or

223
00:13:40.901 --> 00:13:44.680
no?
Right?
So just two columns and that using that Dataset,

224
00:13:44.830 --> 00:13:47.500
we give it to a convolutional network network.
By give it,

225
00:13:47.501 --> 00:13:49.990
I mean convert that into a series of ones and Zeros,

226
00:13:50.290 --> 00:13:52.870
multiply it by whatever model we're choosing to use.

227
00:13:52.870 --> 00:13:55.840
In this case convolutional network,
it's going to give an output.

228
00:13:56.110 --> 00:13:58.810
And the learning process here,
what it's doing well,
it's a learning.

229
00:13:59.590 --> 00:14:02.620
These mathematical models,
they have a set of,
again,

230
00:14:02.621 --> 00:14:06.790
matrices of ones and Zeros that through this optimization strategy,

231
00:14:06.940 --> 00:14:08.740
it's updating itself.
And by updating,

232
00:14:08.741 --> 00:14:12.790
by learning what it's really doing is it's strengthening the connections between

233
00:14:12.910 --> 00:14:17.910
different parts of this input image to know what it means to have Alzheimer's.

234
00:14:18.520 --> 00:14:23.520
And what I mean by saying what it means is what parts of that image contribute

235
00:14:23.861 --> 00:14:27.520
to Alzheimer's,
what w what is the essence of what Alzheimer's is?

236
00:14:27.521 --> 00:14:30.580
And it represents this as a series of ones and Zeros.

237
00:14:30.610 --> 00:14:33.310
It learns this representation.
And so we,

238
00:14:33.330 --> 00:14:36.640
we keep a representation in our head after,
you know,
if,
if we have gone,

239
00:14:36.641 --> 00:14:40.120
if we're a doctrine,
we've gone to metal medical school by looking at many,
many,

240
00:14:40.121 --> 00:14:44.110
many images of this.
And so we can replicate this electronically as well.

241
00:14:44.290 --> 00:14:49.000
So automatic diagnosis is a very exciting application of AI in healthcare and

242
00:14:49.001 --> 00:14:52.400
convolutional networks are just one type of model,
many other types of model.

243
00:14:52.490 --> 00:14:56.210
And this is by far this is not a solved problem at all.

244
00:14:56.450 --> 00:14:58.820
So there's so much all of these,
by the way,

245
00:14:58.970 --> 00:15:03.740
we were literally on the precipice of this new revolution of the intersection of

246
00:15:03.741 --> 00:15:05.120
biology and information.

247
00:15:05.270 --> 00:15:07.580
This is a really exciting time to be in this space right now.

248
00:15:07.610 --> 00:15:11.840
So that brings me to application three,
which is personalized medicine.
I mean,

249
00:15:11.990 --> 00:15:14.570
like I said,
when I got my blood tests done and I was just looking at that,

250
00:15:14.690 --> 00:15:17.300
I was thinking like,
this is data,
this is data,

251
00:15:17.301 --> 00:15:21.980
all of this is data and I want this data because I want to help optimize my
life.

252
00:15:22.010 --> 00:15:25.310
I want to get into,
I want to not have to go to the hospital.

253
00:15:25.550 --> 00:15:29.030
I want to have this preventative assistant telling me what I should and

254
00:15:29.031 --> 00:15:33.820
shouldn't be doing.
Um,
that has my best interest in mind.
You know,

255
00:15:33.830 --> 00:15:37.130
if maybe,
you know,
I'm about to eat something and it's like,
no,
don't do that.

256
00:15:37.160 --> 00:15:39.620
You know,
you should eat this or something like that.
Right?
So this is,

257
00:15:39.740 --> 00:15:41.510
these are just ideas of how this could work.

258
00:15:41.780 --> 00:15:45.380
But the idea behind personalized medicine is doctors are expensive.

259
00:15:45.530 --> 00:15:47.450
Not everybody has access to a doctor.

260
00:15:47.540 --> 00:15:52.070
So if we can automate the whole diagnostic process and to a personal assistant

261
00:15:52.190 --> 00:15:55.550
that everybody has on their phone and everybody has a phone and everybody will

262
00:15:55.551 --> 00:15:59.230
get a phone as the cost of these machines dropped drastically,
um,

263
00:15:59.540 --> 00:16:02.120
8 million people will come online this year,
which is very exciting.

264
00:16:02.240 --> 00:16:04.130
They're going to have access to personalized medicine.

265
00:16:04.280 --> 00:16:08.660
If we can put this technology again,
this set of matrix operations,

266
00:16:08.720 --> 00:16:12.950
mathematical operations on to a server that they can access via a website,

267
00:16:13.130 --> 00:16:17.510
upload their information,
and then they have their own doctor in their pocket.

268
00:16:17.810 --> 00:16:19.340
So that's super exciting as well.

269
00:16:19.490 --> 00:16:24.020
Our goal as an organization at school of Ai is to help solve the 17 what are

270
00:16:24.021 --> 00:16:27.440
called sustainable development goals,
which we'll talk about at the end.

271
00:16:27.560 --> 00:16:28.520
But this is a big one.

272
00:16:28.521 --> 00:16:32.000
This is one of the sustainable development goals outlined by the United Nations.

273
00:16:32.090 --> 00:16:36.650
So there's one more I want to talk about and that's the most exciting to me.

274
00:16:37.010 --> 00:16:40.490
It's also the most futuristic,
but that is synthetic biology.

275
00:16:40.940 --> 00:16:45.860
So again,
the bite and the gene genomic data is really interesting.

276
00:16:46.130 --> 00:16:51.130
There are so many applications to create new types of not just gene therapies.

277
00:16:53.211 --> 00:16:56.410
When we think about genetics,
we think about generally we think as a,

278
00:16:56.440 --> 00:16:57.273
as the public,

279
00:16:57.470 --> 00:17:01.310
we think about using genetics to try to fix some problem that we have.

280
00:17:01.610 --> 00:17:05.210
But what I think is more exciting is not gene therapy,

281
00:17:05.211 --> 00:17:06.680
but gene enhancement.

282
00:17:07.460 --> 00:17:11.690
What if we could use our genetics to create some new type of drug that could

283
00:17:11.780 --> 00:17:15.800
increase our intelligence by orders of magnitude.
And by intelligence,

284
00:17:15.801 --> 00:17:20.801
I mean our ability to learn children are much better learners for language than

285
00:17:21.051 --> 00:17:25.010
we are as adults.
But we can replicate that as adults.

286
00:17:25.011 --> 00:17:27.100
What if we could be like sponges and learn,
you know,

287
00:17:27.110 --> 00:17:30.440
Mandarin and a week with this,
with this new type of drug.

288
00:17:30.620 --> 00:17:34.040
So super intelligence is something that I would be really interested in.

289
00:17:35.030 --> 00:17:38.330
New Types of gene enhancement,
not just intelligence,
but endurance.

290
00:17:38.580 --> 00:17:42.560
I'm reversing aging.
These are things that aren't sounds very science fiction.

291
00:17:42.740 --> 00:17:43.573
But again,

292
00:17:43.580 --> 00:17:47.030
there's so much possibility when it comes to combining biology and information.

293
00:17:47.210 --> 00:17:50.620
Another one that I find in particular,
very exciting.
You know,

294
00:17:50.640 --> 00:17:53.580
I was just on the plane and I watched five movies because it was such a long

295
00:17:53.581 --> 00:17:58.320
flight.
One of them was Jurassic world.
And I was looking at the idea,

296
00:17:58.380 --> 00:18:01.460
I was thinking in my head,
I was like,
oh,
okay.
In this movie they,

297
00:18:01.461 --> 00:18:05.820
the scientists found this sample DNA and they reconstruct it,

298
00:18:05.821 --> 00:18:06.960
a dinosaur from it.

299
00:18:07.320 --> 00:18:11.910
But the reality is that the half life of DNA is something like 520 years.

300
00:18:12.060 --> 00:18:14.370
But dinosaurs were around 60 million years ago.

301
00:18:14.710 --> 00:18:17.100
So it's all of that DNA is gone now.

302
00:18:17.220 --> 00:18:20.430
And so a lot of leaders in that space have said that,
oh,
you know,

303
00:18:20.431 --> 00:18:22.590
we just can't do that because the DNA is gone.

304
00:18:22.830 --> 00:18:26.550
But what they haven't thought about is using,
are using generative models,

305
00:18:26.670 --> 00:18:31.650
not necessarily use that DNA to,
uh,
add different splicing,

306
00:18:31.710 --> 00:18:32.543
whatever's missing,

307
00:18:32.730 --> 00:18:37.650
but to completely regenerate that DNA from scratch using the DNA that we already

308
00:18:37.651 --> 00:18:38.700
have from genomics data.

309
00:18:38.701 --> 00:18:42.540
So think about it as reverse engineering what their DNA would be from the

310
00:18:42.541 --> 00:18:46.350
existing DNA that we have.
And so yeah,
we could have Jurassic Park be real.

311
00:18:46.590 --> 00:18:47.670
I think it's possible.

312
00:18:47.760 --> 00:18:50.250
So I hope to see a startup like that in some at some point,

313
00:18:50.640 --> 00:18:55.470
but also a synthetic biology is very interesting.
It's,
it's super,
super,

314
00:18:55.471 --> 00:18:59.700
like we haven't even begun to think about the possibilities of say molecular

315
00:18:59.701 --> 00:19:01.680
computing.
I mean that,
that's kind of out there.

316
00:19:01.681 --> 00:19:05.220
But use DNA can store something like 200 terabytes.

317
00:19:05.221 --> 00:19:06.510
I'm just kind of guesstimating here,

318
00:19:06.630 --> 00:19:11.630
but like a lot like a single strand of DNA can store and does store terabytes

319
00:19:12.301 --> 00:19:15.660
and terabytes and terabytes of data.
And if we could use,

320
00:19:16.020 --> 00:19:21.020
if we could somehow modify DNA to both store data and to process data,

321
00:19:22.170 --> 00:19:26.190
we would need these machines,
these phones in our pocket anymore.

322
00:19:26.340 --> 00:19:31.110
They would seem like ancient and they will in the future.
And by the future,

323
00:19:31.111 --> 00:19:33.090
I mean,
I hopefully in the next 20 years or so,

324
00:19:33.330 --> 00:19:38.330
but using DNA as not just data but as a mode of computing and for storage is

325
00:19:40.020 --> 00:19:43.110
wildly exciting because think about deep learning,
right?

326
00:19:43.111 --> 00:19:48.111
So in 2012 that's when the deep learning revolution started abusing these 30

327
00:19:49.471 --> 00:19:51.150
year old models.
Um,

328
00:19:51.420 --> 00:19:54.690
all of a sudden they just became amazing at image classification.

329
00:19:54.691 --> 00:19:57.990
All of these other tasks and the reason for that was because we had better

330
00:19:57.991 --> 00:20:00.150
compute and more data.
Those were the two reasons,
right?

331
00:20:00.330 --> 00:20:02.700
Because we have better compute and more data.
These ancient,

332
00:20:02.701 --> 00:20:06.480
not ancient in terms of computer science,
they're ancient 30 year old models.

333
00:20:06.690 --> 00:20:08.130
All of a sudden it worked really well.

334
00:20:08.220 --> 00:20:12.420
So imagine if we had computing that was orders of orders of orders of magnitude

335
00:20:12.421 --> 00:20:15.540
better than the best GPU and Tpu on the planet,

336
00:20:15.810 --> 00:20:20.220
then we could use these existing models and have,
again,
a whole new revolution.

337
00:20:20.221 --> 00:20:24.570
So something like molecular molecular computing and molecular storage has a lot

338
00:20:24.571 --> 00:20:28.710
of potential for creating an entirely new paradigm of computer science and

339
00:20:28.711 --> 00:20:32.310
opening up an entirely new paradigm of Algorithms and possibilities.

340
00:20:32.520 --> 00:20:36.690
And all of this comes from the,
again,
the intersection of the gene and the bite.

341
00:20:36.930 --> 00:20:39.540
By the way,
if you want to learn more about this,
the gene,
the bite the Adam,

342
00:20:39.630 --> 00:20:40.463
I didn't make this up.

343
00:20:40.560 --> 00:20:45.240
This came from a book I read called literally called the gene by Siddhartha

344
00:20:45.240 --> 00:20:47.980
Mukherjee who who's an oncologist.
So definitely read this book.

345
00:20:48.190 --> 00:20:50.740
It's a great book by the way.
This is going to be super useful to you,

346
00:20:50.741 --> 00:20:54.310
like outside of all this,
what I do when I read his,
I read books,

347
00:20:54.550 --> 00:20:58.360
I read books by listening to them as audio books at three x speed.

348
00:20:59.020 --> 00:21:02.340
And so in the past six weeks I've read 13 books.
Um,

349
00:21:02.410 --> 00:21:04.270
and so what I do is I just kind of like sit there.

350
00:21:04.420 --> 00:21:06.400
I'm listening and I'm not really doing anything.

351
00:21:06.401 --> 00:21:10.510
I'm just sitting there and I'm listening and I'm absorbing the information.

352
00:21:10.870 --> 00:21:14.170
And I've had,
and I've said this on my channel and my youtube channel,

353
00:21:14.171 --> 00:21:16.030
but a lot of people were skeptical,

354
00:21:16.150 --> 00:21:18.970
but even now I'm getting people who are saying surrounds you don't know how much

355
00:21:18.971 --> 00:21:21.910
I'm learning now.
Like I was skeptical that I could do this.

356
00:21:21.911 --> 00:21:26.290
Like three x is way too fast and I'm not a native English speaker and you are so

357
00:21:26.291 --> 00:21:27.700
I can't do this.
Well guess what?

358
00:21:28.350 --> 00:21:31.570
You know a lot of my subscribers primarily from India as well,

359
00:21:31.870 --> 00:21:34.810
are able to do this now.
Not a lot like three.
I got three of them,

360
00:21:34.811 --> 00:21:38.090
like tweet it to me that they could,
which made me very happy.
So,

361
00:21:39.190 --> 00:21:42.040
so you got to,
you got to work up to it.
It's like,
it's like,
you know,

362
00:21:42.041 --> 00:21:43.780
you're lifting weights,
you know,
you're not going to be,

363
00:21:43.810 --> 00:21:48.130
I couldn't do three x when I first started,
you know,
I can only do like 1.5 Max.

364
00:21:48.370 --> 00:21:52.960
But if you start at one and then go to 1.5 and do that for like two weeks,

365
00:21:52.961 --> 00:21:56.170
three weeks,
your brain,
your neurons,
your connections,
they're going to,

366
00:21:56.380 --> 00:21:58.360
they're going to update,
they're going to strengthen.

367
00:21:58.990 --> 00:22:01.900
And I see someone shaking their head,
no,
this is real.
This is real.

368
00:22:02.790 --> 00:22:06.340
I'm seeing there's still so much skepticism around the possibility of this,

369
00:22:06.730 --> 00:22:09.850
but it's a real thing.
Try it out.
Don't take my word for it.

370
00:22:09.910 --> 00:22:11.950
Try it out yourself and you'll,
and you'll see.

371
00:22:12.430 --> 00:22:15.970
But this is a great way to taking a lot of information and until we get to the

372
00:22:16.300 --> 00:22:21.300
time where we have devices and maybe Jean enhancements that help us accelerate

373
00:22:22.330 --> 00:22:26.710
how fast we can learn.
This is a great,
um,
midway points,

374
00:22:26.890 --> 00:22:29.320
like listening to things at three x speed video as well.

375
00:22:29.321 --> 00:22:32.830
I do that even my own video.
So these,
these four applications,

376
00:22:32.860 --> 00:22:36.760
there are a part of our mission to help solve these 17 sustainable development

377
00:22:36.761 --> 00:22:38.500
goals outlined by the United Nations.

378
00:22:38.650 --> 00:22:42.850
So the idea is that these are very ambitious goals and the United Nations

379
00:22:43.030 --> 00:22:47.200
outlined by 2030,
which is actually,
you know,
like only 11 years away.
It's,

380
00:22:47.380 --> 00:22:51.310
it's pretty ambitious.
They want to solve all of these no poverty,
zero hunger.

381
00:22:51.520 --> 00:22:54.810
These are like,
these are very ambitious initiatives and,

382
00:22:54.940 --> 00:22:58.510
and we have school of AI believe that with this AI technology,

383
00:22:58.750 --> 00:23:02.770
using the massive data sets that are available to us on the Internet today,

384
00:23:02.950 --> 00:23:06.910
that anybody anymore has access to,
regardless of where you're born or you know,

385
00:23:06.911 --> 00:23:09.940
what your profession is or what your schooling is,
you will,

386
00:23:09.970 --> 00:23:12.490
you are able to make an impact with this technology.

387
00:23:12.491 --> 00:23:16.810
And we believe that with the combined effort of the globe put together people

388
00:23:16.811 --> 00:23:20.140
from across the world,
from different cultures,
different viewpoints,

389
00:23:20.141 --> 00:23:22.420
different perspectives.
If we put this together,

390
00:23:22.540 --> 00:23:26.560
we can come up with these incredible solutions using this technology and we can

391
00:23:26.561 --> 00:23:30.850
accelerate,
um,
the,
the,
the progress towards these goals.

392
00:23:31.090 --> 00:23:35.470
And our goal in particular is to make these happen by 2025 not 20,
30.

393
00:23:35.830 --> 00:23:39.520
And once we do that,
once we're able to solve these goals,

394
00:23:39.521 --> 00:23:42.700
what that means is for the,
for the majority of the world,

395
00:23:42.730 --> 00:23:46.010
as everybody comes online and everybody will be online,

396
00:23:46.011 --> 00:23:50.540
and then within the next decade,
all of humanity,
mostly like 99%,

397
00:23:50.630 --> 00:23:53.810
it's going to be online.
And then we have an entirely new internet culture.

398
00:23:53.870 --> 00:23:56.450
When this happens in everybody's basic needs are,
are met,

399
00:23:56.690 --> 00:24:00.500
then we can get to the stage of creativity and innovation globally and it's just

400
00:24:00.501 --> 00:24:01.730
going to build on itself.

401
00:24:02.000 --> 00:24:04.220
And we're just going to accelerate and accelerate and accelerate.

402
00:24:04.450 --> 00:24:07.730
And it's a very exciting time to be alive in human history right now when we

403
00:24:07.731 --> 00:24:11.120
have the potential for these technologies to solve these incredibly hard

404
00:24:11.121 --> 00:24:15.380
problems and enhance our,
our way of life,
um,
make,

405
00:24:15.440 --> 00:24:16.400
improve our lives,

406
00:24:16.430 --> 00:24:20.450
improve our relationships with people and improve just the way we act in the

407
00:24:20.451 --> 00:24:24.260
world and how we know about all the complexity in the world.

408
00:24:24.470 --> 00:24:29.060
Simplifying it such that we know exactly what to do,
when to do it.

409
00:24:29.270 --> 00:24:33.960
We're more confident.
Uh,
we,
we feel better about ourselves.
Just overall we're,

410
00:24:33.961 --> 00:24:38.120
we're trying to increase the wellbeing,
the health,
the intelligence,

411
00:24:38.210 --> 00:24:41.300
the emotional intelligence of humans using this technology.

412
00:24:41.530 --> 00:24:43.280
And we very much think it's possible.

413
00:24:44.800 --> 00:24:45.633
<v 5>Thank you.</v>

414
00:24:48.040 --> 00:24:51.700
<v 2>So now I'd like to introduce you to our panelists.</v>

415
00:24:51.940 --> 00:24:56.560
So first up we have,
I use Chicana.
So are you sure.

416
00:24:56.561 --> 00:25:01.540
Is Cofounder and CEO of [inaudible] Oh,
I AI,
sorry.

417
00:25:01.541 --> 00:25:04.750
Eight.
Oh,
AI.
An AI solutions from an incubator.

418
00:25:05.050 --> 00:25:09.730
She's advised governments and corporations on AI,
smart cities and Fintech.

419
00:25:09.940 --> 00:25:13.120
And she's on the board of Infocom Media Development Authority,

420
00:25:13.390 --> 00:25:18.280
which is the government agency behind Singapore is a smart Nation Vision Asia

421
00:25:18.281 --> 00:25:23.140
has a phd in information systems and innovation from the London School of

422
00:25:23.140 --> 00:25:26.290
Economics as written two books and multiple articles and technology and

423
00:25:26.291 --> 00:25:30.460
innovation and has been named one of Southeast Asia is groundbreaking female

424
00:25:30.461 --> 00:25:34.780
entrepreneurs by Forbes magazine.
Thank you for joining us today.
Are you sure?

425
00:25:38.020 --> 00:25:40.170
Next up we have doctor Victor Tang.

426
00:25:40.570 --> 00:25:44.920
Victor is the chief digital and information officer at the National Gallery

427
00:25:44.921 --> 00:25:46.990
Singapore where we find ourselves today.

428
00:25:47.380 --> 00:25:51.220
He was the founding director of a star is social and cognitive computing

429
00:25:51.221 --> 00:25:56.221
department and the head of biomedical informatics at the Institute for Infocomm

430
00:25:56.350 --> 00:26:00.430
Research.
He's also held senior roles at companies like Pfizer and SAP research.

431
00:26:00.880 --> 00:26:05.320
Victor has a phd in biochemistry from the National University of Singapore and

432
00:26:05.321 --> 00:26:10.321
his work has won numerous awards including the Mit Tiara 35 award and the World

433
00:26:11.470 --> 00:26:15.030
Economic Forum,
young global leader who's welcome Doctor Victor Tom.

434
00:26:19.620 --> 00:26:23.080
Next up joining us is Charles Coosbay.
Huh.

435
00:26:23.730 --> 00:26:28.350
He leads Accenture's artificial intelligence practice in Southeast Asia.

436
00:26:28.590 --> 00:26:32.970
He has over a decade of experience in AI across industries including Fintech,

437
00:26:33.270 --> 00:26:37.530
advertising,
security,
and the military.
In the past six years,

438
00:26:37.531 --> 00:26:40.050
chars lists founded two AI startups,

439
00:26:40.260 --> 00:26:44.730
one focused on predictive maintenance for the oil and gas and the other on fraud

440
00:26:44.731 --> 00:26:49.530
detection and credit scoring for unbanked populations.
So please welcome Charles.

441
00:26:53.690 --> 00:26:56.600
And last but not least,
we have Dr 10 10.

442
00:26:56.601 --> 00:27:01.601
We is the CEO of the National Super Computing Center in Singapore where he

443
00:27:01.731 --> 00:27:06.731
manages Singapore's first pentascale supercomputer is also the founder of the

444
00:27:06.861 --> 00:27:11.690
bioinformatics center at Nus,
the Asia Pacific bioinformatics network,

445
00:27:11.720 --> 00:27:14.980
the first Internet service provider in Singapore.
He,

446
00:27:14.990 --> 00:27:19.190
he founded it and oversaw the creation of the multilingual internet domain name

447
00:27:19.191 --> 00:27:20.024
system.

448
00:27:20.150 --> 00:27:24.680
And wired magazine has called him one of the people who built the Internet.

449
00:27:25.070 --> 00:27:27.470
So please welcome Dr Tamsen Week.

450
00:27:30.130 --> 00:27:34.870
And of course we'd like to invite Eaker and Sarraj again to join us up on stage.

451
00:27:34.960 --> 00:27:39.960
And our first topic that we're going to be talking about today is ethical Ai.

452
00:27:40.930 --> 00:27:45.930
So that might seem a bit vague to people who are new to this question.

453
00:27:47.350 --> 00:27:50.680
So maybe one of you can can make that more concrete.

454
00:27:50.681 --> 00:27:54.010
What are we talking about when we're talking about Ai and ethics?

455
00:27:54.011 --> 00:27:58.180
Is this like Alexa and Siri getting together to rob a bank or something?

456
00:27:58.181 --> 00:27:59.770
Like what,
what is the problem?

457
00:28:01.200 --> 00:28:04.480
<v 6>It's grabbing Mike.
Maybe I'll,
I'll make a headstart.
Um,</v>

458
00:28:05.060 --> 00:28:08.930
I guess maybe a way of putting the problem is when you look at the way

459
00:28:08.931 --> 00:28:13.490
traditional software programs also we're systems or capabilities are built,

460
00:28:13.940 --> 00:28:18.050
there is a very easy way for us to explain the decisions that are taken by that

461
00:28:18.051 --> 00:28:22.010
solve whether is a deterministic link between the input on the output,
right?
Um,

462
00:28:23.180 --> 00:28:27.560
the moment we perhaps put artificial intelligence into the equation,

463
00:28:28.030 --> 00:28:28.863
um,

464
00:28:29.150 --> 00:28:33.560
such lien gets a bit more blurred and the decisions and the system or making in

465
00:28:33.561 --> 00:28:37.700
the context of think of a classic example that comes to mind is autonomous

466
00:28:37.701 --> 00:28:42.650
driving.
So to autonomous cars are going to be in an unavoidable collision.

467
00:28:43.150 --> 00:28:47.870
Um,
does the system have to make any decisions that affect who survives and who

468
00:28:47.871 --> 00:28:49.100
doesn't survive?
For example,

469
00:28:49.101 --> 00:28:52.700
what are those considerations that are going to be taken into account?

470
00:28:52.730 --> 00:28:56.660
How do those decisions get taken?
What are the parameters?
What are the outcomes?

471
00:28:57.080 --> 00:29:01.640
What,
um,
why do we take the decisions we take?
And how did the system,
uh,

472
00:29:01.700 --> 00:29:05.180
reach the conclusion?
That's perhaps one,
one way of starting the topic.

473
00:29:06.800 --> 00:29:10.340
<v 2>Does anyone have a differing opinion?
Is,
is that,
is that the problem?</v>

474
00:29:10.341 --> 00:29:15.300
Is the problem of inscrutable AI,
AI models that do things.
We,
uh,

475
00:29:15.650 --> 00:29:16.850
maybe you can't take responses.

476
00:29:17.270 --> 00:29:19.550
<v 7>That's definitely one of the problems.
I totally agree.</v>

477
00:29:19.551 --> 00:29:23.840
I think that one can kind of divided into three problems.

478
00:29:23.841 --> 00:29:27.980
One is explainability which is even for AI engineers,

479
00:29:27.981 --> 00:29:30.680
sometimes it's hard to understand why it actually came up with it.

480
00:29:31.100 --> 00:29:32.660
Then his interpretation,

481
00:29:32.690 --> 00:29:37.400
which is for the business user or for the domain expert who's partnering with

482
00:29:37.401 --> 00:29:40.220
the Ai Engineers.
And the third is ethics.

483
00:29:40.520 --> 00:29:44.650
So are you with what it's doing?
And in ethics itself,

484
00:29:44.651 --> 00:29:48.520
there is two ways.
One is I think when you,

485
00:29:48.640 --> 00:29:51.700
you don't create algorithms that are deliberative,

486
00:29:51.760 --> 00:29:56.350
deliberately manipulative such as all the deep fake algorithms and the potential

487
00:29:56.351 --> 00:29:59.740
over there that you keep your data processes,

488
00:29:59.741 --> 00:30:02.080
lineage and transparency in order.

489
00:30:02.440 --> 00:30:07.150
Those are some of the recommendations by IMD A's framework for ethical AI.

490
00:30:07.630 --> 00:30:11.690
And um,
and then the third is trying to really,
um,
you know,

491
00:30:11.740 --> 00:30:16.720
establish global ethics.
And then I guess cultural ethics.

492
00:30:16.721 --> 00:30:21.490
I mean,
different countries may have their own,
um,
limitations may put our,

493
00:30:21.491 --> 00:30:24.550
their own limitations while others may not.
So if you look at Gdpr,

494
00:30:24.551 --> 00:30:25.384
I think it's uh,

495
00:30:25.840 --> 00:30:29.140
I personally think it's a really good standard for the rest of the world to

496
00:30:29.141 --> 00:30:33.020
follow because it really puts human rights as data rights,
uh,

497
00:30:33.160 --> 00:30:37.060
or data rights as human rights,
right at the core of how we approach algorithms.

498
00:30:37.820 --> 00:30:38.653
<v 6>Okay.</v>

499
00:30:38.730 --> 00:30:43.730
<v 4>So is there room then for research in AI to teach us more about global ethics or</v>

500
00:30:44.941 --> 00:30:48.600
cultural ethics are what we want those things to be as a society?

501
00:30:50.530 --> 00:30:53.360
<v 6>I guess a bit of a tough question to big,</v>

502
00:30:53.361 --> 00:30:58.361
but I guess a way of looking at it is that the fact that AI exists and it's

503
00:30:58.431 --> 00:31:02.240
something that is totally unavoidable because I guess it puts a spotlight on

504
00:31:02.241 --> 00:31:06.160
those questions and it is going to force that conversation to take place,
right?

505
00:31:06.260 --> 00:31:10.670
Maybe in a more elevated or intense way that it has been happening so far.

506
00:31:11.270 --> 00:31:11.610
<v 7>I mean,</v>

507
00:31:11.610 --> 00:31:15.360
I think the dumb way to do it is to get either a bunch of lawyers as policy

508
00:31:15.361 --> 00:31:18.000
makers or to get all like engineers together.

509
00:31:18.420 --> 00:31:21.090
Whereas we really need to have a diverse team.

510
00:31:21.091 --> 00:31:26.091
We need more people who are philosophers or artists or people who come from

511
00:31:26.491 --> 00:31:28.410
different domains to get together.

512
00:31:28.830 --> 00:31:33.480
This is literally something that's going to impact society and economy to such

513
00:31:33.481 --> 00:31:37.890
an extent that perhaps it's even rewiring our social contracts with the

514
00:31:37.891 --> 00:31:39.810
government and good corporations.

515
00:31:40.170 --> 00:31:44.670
So it requires a more diversity and the people who address it.

516
00:31:44.870 --> 00:31:46.730
<v 4>Totally agree with that.
Yeah,
that's a great point.</v>

517
00:31:46.731 --> 00:31:51.080
And now we need more diverse a set of people other than engineers making these

518
00:31:51.081 --> 00:31:55.220
very important decisions that are affecting all of us,
all of us that use,

519
00:31:55.310 --> 00:31:56.750
for example,
social networks.

520
00:31:57.410 --> 00:32:02.410
Facebook is the easiest example to make fun of because of the amount of

521
00:32:02.781 --> 00:32:06.410
industrial scale,
manipulation and exploitation that it,

522
00:32:06.590 --> 00:32:09.170
that it does in the name of its business.

523
00:32:09.171 --> 00:32:12.140
And so Facebook is Facebook's newsfeed,
for example.

524
00:32:12.200 --> 00:32:17.200
It's like my favorites algorithm to trash basically because Facebook's newsfeed

525
00:32:17.810 --> 00:32:19.970
is an example of a runaway AI.

526
00:32:20.510 --> 00:32:23.930
It's making decisions that even its creators don't understand at this point

527
00:32:23.931 --> 00:32:28.040
because of the amount of data that it's using to show you what it thinks is

528
00:32:28.041 --> 00:32:32.750
going to maximize your attention.
So their business model is,
is ads right?

529
00:32:32.810 --> 00:32:36.860
Our ads.
So it's revenue.
The revenue they get mostly are from ads.

530
00:32:37.070 --> 00:32:40.070
And so the longer they keep your eyes on screen,
the more money

531
00:32:40.070 --> 00:32:43.250
<v 8>they make.
And this is bad because if we look at,
um,
you know,</v>

532
00:32:43.251 --> 00:32:47.650
a lot of studies of children who are using social networks a lot,
uh,
it,
it,

533
00:32:47.651 --> 00:32:52.100
it causes anxiety and depression.
This is a,
this is a,
this is a proven thing.

534
00:32:52.101 --> 00:32:53.600
It's not just a fake thing.

535
00:32:53.840 --> 00:32:58.730
So being on social networks too much is objectively bad for your mental health

536
00:32:59.000 --> 00:33:03.380
because,
because mainly it's optimizing for your attention.

537
00:33:03.620 --> 00:33:06.980
Whereas ideally it would not be optimizing for your attention.

538
00:33:07.220 --> 00:33:12.110
It would be optimizing for your time well spent.
What would benefit you the most?

539
00:33:12.111 --> 00:33:15.830
What is the content that would help you with whatever your objective in life is?

540
00:33:15.950 --> 00:33:18.800
What would be educational,
what would not be violent,

541
00:33:18.801 --> 00:33:22.490
but instead promote cooperation and empathy and things like that.

542
00:33:22.850 --> 00:33:27.740
And we're not there yet because we just built this system and we just built this

543
00:33:27.740 --> 00:33:31.880
billion person plus scalable Ai Algorithm and trained it on,
um,

544
00:33:32.090 --> 00:33:33.230
maximizing attention.

545
00:33:33.230 --> 00:33:36.500
But maybe in the next phase we're going to clean up what I consider to be

546
00:33:36.680 --> 00:33:40.760
digital pollution and instead make it cleaner,

547
00:33:40.940 --> 00:33:41.773
cleaner data,

548
00:33:41.810 --> 00:33:46.130
cleaner information that is going to optimize for your time well spent.

549
00:33:46.310 --> 00:33:49.940
And I would encourage you all to look up literally time well spent because this

550
00:33:49.941 --> 00:33:54.250
is an actual movement I seek at the end of the day it's you proves also some

551
00:33:54.251 --> 00:33:58.990
things that menu of Wisconsin way existing post pre.

552
00:33:59.240 --> 00:34:03.470
I actually if we talk about credit scoring and other type of application,

553
00:34:03.890 --> 00:34:07.970
well if you consider putting the location of an individual into a decision

554
00:34:07.971 --> 00:34:08.840
making system,

555
00:34:09.080 --> 00:34:12.080
it is not fair because it's not because you're living in a poor neighborhood

556
00:34:12.350 --> 00:34:16.000
that you not someone trustworthy.
And yes,

557
00:34:16.010 --> 00:34:18.890
this is a program that is being raised with AI concern,

558
00:34:18.920 --> 00:34:21.410
but does that mean that he was not existing before?

559
00:34:21.710 --> 00:34:25.040
So maybe at the same time as we're working on those frameworks falls the AI.

560
00:34:25.041 --> 00:34:26.840
So I'm seeing even on the,

561
00:34:26.841 --> 00:34:31.160
on the humanity on the w some seem to consider also because at the end it's kind

562
00:34:31.161 --> 00:34:35.800
of replicating a lot of the things even regarding Facebook feeds well feeds,

563
00:34:35.840 --> 00:34:40.580
Facebook feeds is only replicating what TV China's also doing,

564
00:34:40.581 --> 00:34:45.500
trying to optimize the revenue and a certain way because it's easy to eat to

565
00:34:45.550 --> 00:34:49.130
conserve contents.
So that's a thing.

566
00:34:49.131 --> 00:34:53.060
It's a great question to ask.
And his own not only concerning,
uh,
AI,

567
00:34:53.061 --> 00:34:54.870
it's a society issue here.

568
00:34:55.370 --> 00:34:59.060
<v 2>Yeah,
that's,
that's a good point about,
you know,
these being preexisting issues.</v>

569
00:34:59.061 --> 00:35:04.061
I or there's this now very well publicized phenomenon of judges sentencing more

570
00:35:04.851 --> 00:35:08.930
harshly right before lunch then right after breakfast.

571
00:35:09.200 --> 00:35:12.490
And uh,
so,
you know,
that's the same kind of bias that you can get with,

572
00:35:12.491 --> 00:35:17.491
with humans that you could get with the machines that are trained on bad data or

573
00:35:17.541 --> 00:35:18.970
you know,
uh,
that with,

574
00:35:18.990 --> 00:35:22.490
without those precon without those notions in mind very carefully.

575
00:35:22.760 --> 00:35:24.770
But here again,
we have also to be caught

576
00:35:25.760 --> 00:35:30.760
<v 8>correlation versus causation because if you can get numbers like that,</v>

577
00:35:30.801 --> 00:35:32.930
we had this conversation with my team earlier this week.

578
00:35:33.710 --> 00:35:37.980
You can have a statistics that tells you that you have more murder when you have

579
00:35:37.981 --> 00:35:41.230
more ice creams being sold.
So we also have to be careful on those kinds of

580
00:35:41.510 --> 00:35:46.200
<v 2>right because it's the heat and stress.
Um,
so,
so,
uh,</v>

581
00:35:46.220 --> 00:35:50.730
are you sure.
And,
and Sarah as you were talking about that we need more people.

582
00:35:50.731 --> 00:35:55.700
We,
this is more than just a problem for engineers or lawyers and we need more

583
00:35:55.701 --> 00:35:59.730
diversity and this is something that's going to affect everyone.
So,
uh,

584
00:35:59.840 --> 00:36:04.670
maybe Dr Tami,
you can tell us,
what do you think is an institution like,

585
00:36:04.790 --> 00:36:06.680
like the National Gallery where we are here,

586
00:36:06.890 --> 00:36:11.890
what is that the role of such institutions and helping educate the public and,

587
00:36:12.710 --> 00:36:13.311
and uh,

588
00:36:13.311 --> 00:36:18.200
and helping ensure that we see the more positive sides of those results.

589
00:36:18.201 --> 00:36:19.940
Then the more negative ones.

590
00:36:20.570 --> 00:36:24.320
Do you do or do you see that that institutions like the national guy who have a

591
00:36:24.321 --> 00:36:25.140
role in that,

592
00:36:25.140 --> 00:36:30.140
<v 9>I guess at the application of Ai to improve or optimize our process or we do not</v>

593
00:36:32.421 --> 00:36:36.650
say go into the aspects by I guess a phantom mental problem.

594
00:36:36.651 --> 00:36:41.000
I thought about ethical Ai.
It be customer part,
the culture differences,

595
00:36:42.440 --> 00:36:42.971
for example,

596
00:36:42.971 --> 00:36:46.850
it may be different across our different organizations or even across different

597
00:36:47.160 --> 00:36:50.930
[inaudible] society.
So how do you actually reconcile all these things?

598
00:36:50.931 --> 00:36:54.710
Because when I talk about ethics,
if I take the first two different,

599
00:36:54.711 --> 00:36:57.540
has different meanings to different populations or two different people,

600
00:36:57.800 --> 00:36:58.660
how do we actually know,

601
00:36:59.420 --> 00:37:03.140
get everyone on board to agree on certain size of piece of birth are course of

602
00:37:03.141 --> 00:37:03.974
practice.
Yeah.

603
00:37:04.100 --> 00:37:04.341
<v 2>Yeah.</v>

604
00:37:04.341 --> 00:37:07.790
And I suppose that that's another one of those problems that exists before AI

605
00:37:07.791 --> 00:37:12.140
and will exist afterwards.
Um,
so maybe we can,

606
00:37:12.170 --> 00:37:16.910
maybe we can transition now to the topic of healthcare and,

607
00:37:16.911 --> 00:37:21.830
and some of the implications of AI for healthcare.
Uh,
so,
uh,

608
00:37:21.831 --> 00:37:26.420
we talked,
uh,
so Raj talked about,
um,
the impact of 80 of Ai on,

609
00:37:26.630 --> 00:37:31.490
on a radiology in particular,
but,
and the potential for,
for,

610
00:37:31.570 --> 00:37:33.710
uh,
personal assistants and stuff like that.

611
00:37:33.711 --> 00:37:38.070
What are some of the areas where we're starting to see a impact of AI in

612
00:37:38.180 --> 00:37:41.960
healthcare or in bioinformatics or any of those areas right now?

613
00:37:42.140 --> 00:37:42.261
<v 8>Well,</v>

614
00:37:42.261 --> 00:37:46.160
that seemed the most of the singer Venus brilliantly covered by a Sarraj area.

615
00:37:46.520 --> 00:37:49.700
So she's referred to what had been said before.

616
00:37:49.701 --> 00:37:54.701
But I think in general we can see that a I is e helping at predicting what's

617
00:37:55.941 --> 00:37:58.250
going to happen next day.
Being a,

618
00:37:58.251 --> 00:38:03.251
being able to detect a slightly before doctor d first science of any kind of

619
00:38:04.461 --> 00:38:09.461
cancer or any kind of health issue being about optimizing this is where he

620
00:38:11.480 --> 00:38:12.620
brings a lot of values,

621
00:38:12.890 --> 00:38:17.710
assisting doctors but also assisting you in trying to predict and small sub the

622
00:38:18.140 --> 00:38:22.440
you're all uh,
uh,
the way you leave in order to optimize.
Yes.

623
00:38:23.130 --> 00:38:26.300
<v 2>So you get the average person gets super powers,</v>

624
00:38:26.310 --> 00:38:30.560
essentially gets medical,
uh,
medical superpowers.

625
00:38:31.430 --> 00:38:36.100
<v 8>I don't know if it's super powers,
but it's a statistical,
uh,
knowledge about</v>

626
00:38:36.160 --> 00:38:38.990
<v 9>okay,
if I do that statistical superpowers.
So</v>

627
00:38:41.260 --> 00:38:44.760
adding onto some of the comments,
that's your I shared earlier on,
um,

628
00:38:44.830 --> 00:38:48.550
on cancer being one of the key applications here.
I have,

629
00:38:48.551 --> 00:38:50.020
you look at the Saudi,

630
00:38:50.021 --> 00:38:55.021
I think a lot of us have experienced this in our families and our relatives.

631
00:38:55.320 --> 00:38:57.730
It's,
it's a very complex journey for the patient.

632
00:38:57.731 --> 00:39:01.540
And a journey that that has a lot of different stages in which when you've gone

633
00:39:01.541 --> 00:39:02.171
through that journey,

634
00:39:02.171 --> 00:39:06.370
you can clearly see a lot of aspects in which artificial intelligence can really

635
00:39:06.371 --> 00:39:08.890
play a role in helping early diagnostic,
for example,

636
00:39:08.891 --> 00:39:13.570
certain types of cancer has proven to be the difference between survival or not

637
00:39:13.571 --> 00:39:14.291
survival,
right?

638
00:39:14.291 --> 00:39:18.820
So having that radiology is being able to detect with the help of that,

639
00:39:19.210 --> 00:39:23.320
perhaps super power of having those super computer eyes that are helping the

640
00:39:23.321 --> 00:39:28.321
take something in its earlier form that is totally a big to the human eye.

641
00:39:28.750 --> 00:39:32.750
That will be the difference between life and death for so many people.
Um,

642
00:39:33.290 --> 00:39:36.790
one of the situations how experienced in this particular case was,
um,

643
00:39:37.390 --> 00:39:39.460
when a patient goes through chemo,

644
00:39:40.270 --> 00:39:43.240
you clearly tell that there's something different in that treatment.

645
00:39:43.241 --> 00:39:46.210
Do anything else,
right?
Secondary Effects,

646
00:39:46.570 --> 00:39:48.850
collateral effects of the medication they're giving you,

647
00:39:49.330 --> 00:39:50.620
they will give you a list.

648
00:39:50.650 --> 00:39:55.450
These are the known things and that could or could not happen to you.

649
00:39:55.570 --> 00:39:58.750
This is the phone number you call when you experienced anything else.

650
00:39:58.780 --> 00:40:01.960
So we can learn from it.
And we think about that,
right,

651
00:40:01.961 --> 00:40:05.590
that that sounds like a space where artificial intelligence can start gathering

652
00:40:05.591 --> 00:40:08.780
all that data and we can continue to learn from that.
And it's,
it's,
it's,

653
00:40:09.160 --> 00:40:13.420
it's a disease that puts us all into the unexplored territory perhaps or one

654
00:40:13.421 --> 00:40:16.010
would definitely,
we can see some application.
They're

655
00:40:17.560 --> 00:40:21.990
given a very good overview about the use of AI for a chunk design.

656
00:40:22.370 --> 00:40:23.203
So

657
00:40:26.010 --> 00:40:27.990
design specs in,

658
00:40:28.200 --> 00:40:33.200
we look at using data or weak virus I said needs to trigger new rest points,

659
00:40:33.520 --> 00:40:36.070
but it seems,
for example,

660
00:40:36.400 --> 00:40:41.400
scientists have been using AI AI to identify regions of the five tech Tekken

661
00:40:41.770 --> 00:40:46.600
Chica t cell response.
And so many of these are newer payment,
for example,

662
00:40:46.660 --> 00:40:50.980
hip since multiple sclerosis vaccines answer but food that since my son,

663
00:40:50.981 --> 00:40:53.650
but he's undergoing different stages of our clinical trials,

664
00:40:53.910 --> 00:40:58.910
some in a free spot for HIV interface to treat for no [inaudible].

665
00:41:01.860 --> 00:41:04.480
So these are some of the potential applications tech can be,

666
00:41:04.481 --> 00:41:07.390
I can be used for vaccine design so far track design.

667
00:41:07.670 --> 00:41:12.650
So I just saw actually comment on,
um,
uh,
the importance of the school.
Uh,

668
00:41:12.651 --> 00:41:17.500
organizations like the school of Ai as a lot of education that needs to take

669
00:41:17.501 --> 00:41:22.501
place before the health care community can actually latch onto this extremely

670
00:41:22.511 --> 00:41:24.640
powerful technology.
Of course,

671
00:41:24.641 --> 00:41:28.270
that's with a powerful technology that comes with great responsibility.
And,
and,

672
00:41:28.310 --> 00:41:33.040
and,
and the whole spectrum of,
um,
uh,
ethic ethics,

673
00:41:33.550 --> 00:41:34.330
uh,

674
00:41:34.330 --> 00:41:38.990
<v 10>there has to guide us in the way we apply these incredibly powerful</v>

675
00:41:38.991 --> 00:41:41.900
technologies.
And in particular Ai,

676
00:41:42.080 --> 00:41:47.080
which has the capability of surpassing a human intelligence and also this

677
00:41:47.361 --> 00:41:51.950
possibility that an artificial thing that is divorced from human consciousness

678
00:41:52.310 --> 00:41:57.290
that is able to now predict things far better than anybody the so called super

679
00:41:57.330 --> 00:42:00.410
powers that you described.
And so it's,

680
00:42:00.570 --> 00:42:04.210
it's critically important for us at this juncture of time,
uh,

681
00:42:04.220 --> 00:42:08.220
for us really look at this incredibly powerful technology and,

682
00:42:08.240 --> 00:42:10.150
and put in place a,

683
00:42:10.151 --> 00:42:14.270
an educational process that will help us understand what are the things that we

684
00:42:14.271 --> 00:42:17.750
can apply to and what we shouldn't be doing right?

685
00:42:18.170 --> 00:42:20.700
And these ethical issues that arise,
uh,

686
00:42:21.110 --> 00:42:23.660
touched to the very core of us as human beings.

687
00:42:24.010 --> 00:42:28.850
A human beings are a conscious and we are moral creatures.
And therefore,

688
00:42:28.851 --> 00:42:33.851
if you are envisage an artificial environment where a machine that does not have

689
00:42:34.101 --> 00:42:38.250
consciousness now has super intelligence,
uh,

690
00:42:38.300 --> 00:42:42.590
then that difference now starts to create a lot of,
uh,

691
00:42:42.980 --> 00:42:47.650
impossible and very difficult situations,
uh,
that,
uh,
from,

692
00:42:47.660 --> 00:42:51.500
from day to day activities like deciding fever,

693
00:42:51.740 --> 00:42:56.570
an autonomous vehicle were to crash,
should it crash this way or that way.

694
00:42:56.840 --> 00:42:57.620
And then you've got to,

695
00:42:57.620 --> 00:43:01.610
you've got to make that value judgment in making a pronouncement of a cancer

696
00:43:01.611 --> 00:43:06.080
patient as to what sort of treatment he has to take all of that,
the end for you,

697
00:43:06.350 --> 00:43:10.610
is it ethical to tell a person that,
I'm sorry,
it's incurable.

698
00:43:11.570 --> 00:43:12.403
It's the end.

699
00:43:12.650 --> 00:43:17.270
You have no hope that robs the person of all hope,

700
00:43:18.110 --> 00:43:20.930
but the intelligence says,
yeah,
it's correct.
It's a fact.

701
00:43:21.740 --> 00:43:23.210
Do you want to say that to a person?

702
00:43:23.660 --> 00:43:25.610
Is it ethical for you to say that to a person?

703
00:43:26.660 --> 00:43:31.660
So it's important now for us to understand as a human beings with morality and

704
00:43:33.350 --> 00:43:37.730
consciousness that we are venturing into a scenario where we are talking about

705
00:43:37.820 --> 00:43:39.410
artificial intelligence,

706
00:43:39.500 --> 00:43:44.500
artificial superintelligence that probably is smarter than all of us at this

707
00:43:45.411 --> 00:43:48.770
stage.
And it keeps learning.
Whereas human beings,

708
00:43:48.771 --> 00:43:53.771
we are all born clean slate and we need education now to bring us to the point

709
00:43:54.711 --> 00:43:57.740
where we are smart enough to be able to have this conversation.

710
00:43:58.190 --> 00:44:03.030
So that brings me right back the way the school of Ai has a very critical task

711
00:44:03.031 --> 00:44:07.970
to play,
particularly in the healthcare community.
I'm 29 years,

712
00:44:08.000 --> 00:44:11.240
19th of February at the school of medicine.
Okay.

713
00:44:11.510 --> 00:44:14.960
So you introduced me as being an internet pioneer.
Well,

714
00:44:14.961 --> 00:44:19.060
I'm a professor in the biochemistry department.
By the end,

715
00:44:19.070 --> 00:44:23.870
the last 30 years I've been trying to introduce bioinformatics and along the
way,

716
00:44:23.900 --> 00:44:27.620
no traction.
So I did sightline projects like the Internet,

717
00:44:28.070 --> 00:44:32.100
like super computing,
like multilingual domain names and so

718
00:44:32.100 --> 00:44:36.060
<v 8>on.
And I gained significant traction there.
But in my department,</v>

719
00:44:36.090 --> 00:44:39.920
in my school,
it's still a long way to go and let,

720
00:44:39.980 --> 00:44:41.310
that's the medical school,

721
00:44:41.550 --> 00:44:45.300
let alone the entire establishment of the healthcare system.

722
00:44:45.360 --> 00:44:49.110
It takes a long time.
So even as we are really excited and,

723
00:44:49.111 --> 00:44:52.980
and I'm a technology guy,
right,
and really excited about technology,

724
00:44:53.520 --> 00:44:58.520
but it never underestimate how long it takes for us to actually educate people,

725
00:44:59.340 --> 00:45:04.340
changed the mindset to adopt a technologies in an ethical manner.

726
00:45:05.140 --> 00:45:08.960
<v 2>Oh,
that's really interesting.
Yeah.
Um,
I,
um,
on that,</v>

727
00:45:08.990 --> 00:45:12.430
on that topic of,
of you know,
how,

728
00:45:12.440 --> 00:45:16.100
how it's been taking a long time in the,
in the areas that you're involved with,

729
00:45:16.400 --> 00:45:21.260
is there something specific about healthcare or the healthcare industry that,
uh,

730
00:45:21.261 --> 00:45:26.180
that makes that the case where,
where,
for example,
as the Raj mentioned,

731
00:45:26.181 --> 00:45:27.780
you know,
a lot of these,
um,

732
00:45:27.830 --> 00:45:31.130
these techniques are based on academic papers that have been published for

733
00:45:31.131 --> 00:45:35.240
decades and,
and the research that goes on today,
uh,

734
00:45:35.310 --> 00:45:36.560
especially,
you know,

735
00:45:36.590 --> 00:45:40.790
championed by organizations like open AI is out in the open and researchers out

736
00:45:40.791 --> 00:45:44.810
in the open.
Is there,
does that fit well with healthcare?
It does.

737
00:45:44.811 --> 00:45:46.400
That kind of open open,

738
00:45:46.430 --> 00:45:50.630
does the open source philosophy fit well with healthcare or is there some other

739
00:45:50.810 --> 00:45:53.190
reason why maybe things are it,

740
00:45:53.360 --> 00:45:58.010
it's taking longer as you said to for that adoption to,
to,
to take hold.

741
00:45:58.490 --> 00:46:02.960
<v 8>A lot of healthcare data is private for good reason because it's very sensitive</v>

742
00:46:02.961 --> 00:46:03.590
data.

743
00:46:03.590 --> 00:46:08.270
And so people working with healthcare have to find a way to get this data

744
00:46:08.271 --> 00:46:11.510
finesse their way to be having access to this data.

745
00:46:12.380 --> 00:46:16.970
But what I think is a great startup idea that it's one of many,

746
00:46:16.971 --> 00:46:20.960
but like if I had the time I would be working on this.
Uh,
but what,

747
00:46:20.961 --> 00:46:25.961
what would be a great startup idea is to offer a service to these hospitals and

748
00:46:26.331 --> 00:46:31.250
other healthcare providers that offers to anonymize their data and sell it this

749
00:46:31.251 --> 00:46:36.251
anonymous data directly peer to peer to trusted entities at they vetted.

750
00:46:37.430 --> 00:46:40.670
So that would be like anonymized data as a service.

751
00:46:40.710 --> 00:46:44.510
Hod Us anonymize data as a service for healthcare.

752
00:46:45.530 --> 00:46:50.150
So if you can prove to these and that that's a great kind of startup to do on

753
00:46:50.151 --> 00:46:53.600
the ground because you can physically go to these offices and you know,

754
00:46:53.601 --> 00:46:57.290
work your magic and try to sell this idea and just build your repertoire and

755
00:46:57.291 --> 00:46:59.750
trust over time.
But if you could do that,

756
00:46:59.751 --> 00:47:04.130
then all this anonymized data would become more publicly assessable to people.

757
00:47:04.280 --> 00:47:07.990
So see,
rod,
you talked about your vials of blood,
you donate it,
right?

758
00:47:08.180 --> 00:47:09.440
You actually gave it away.

759
00:47:10.190 --> 00:47:13.820
And the data that's associated with it and the metadata that's associated with

760
00:47:13.821 --> 00:47:17.130
it,
it's actually help in the,
uh,

761
00:47:17.210 --> 00:47:21.890
in a database somewhere in the hospital and the data belongs to them.

762
00:47:21.920 --> 00:47:26.370
You're going to get the copy.
You might get a download at a,

763
00:47:26.371 --> 00:47:30.010
a CD.
Now they stayed,
they,
they give you a CD of all of your scans.
Right?

764
00:47:30.940 --> 00:47:31.361
<v 10>But just,</v>

765
00:47:31.361 --> 00:47:36.361
that's just about it until the medical information of a patient now belongs to

766
00:47:39.310 --> 00:47:42.190
me myself.
Um,
it's a long,

767
00:47:42.191 --> 00:47:46.930
hard road because right now the medical profession keeps that information and

768
00:47:46.931 --> 00:47:51.160
for good reason.
Um,
privacy ECS for example,
in Singapore,

769
00:47:51.161 --> 00:47:56.161
we have had a recent spate of a loss of medical information and by the millions

770
00:47:58.360 --> 00:48:02.690
and the whole issue about,
uh,
what surprised my,

771
00:48:02.710 --> 00:48:07.120
my private medical information and how it impacts on me,

772
00:48:07.121 --> 00:48:10.480
myself to my family members,
or more importantly,

773
00:48:10.481 --> 00:48:14.020
perhaps my insureability,
right?

774
00:48:14.230 --> 00:48:16.980
How would the insurance agents now,
uh,

775
00:48:17.000 --> 00:48:22.000
assess the amount of premium I have to pay if they knew that I was suffering

776
00:48:22.661 --> 00:48:27.640
from this or that genetic disease or my propensity towards a particular

777
00:48:27.641 --> 00:48:32.440
cardiovascular event and so forth.
And with super intelligence,

778
00:48:32.441 --> 00:48:37.270
you could almost predict to a son a very high percentage of accuracy.

779
00:48:37.650 --> 00:48:39.850
So does this legal Albie,
uh,

780
00:48:39.851 --> 00:48:42.110
whether I'm going to develop this other disease and war,

781
00:48:42.111 --> 00:48:46.510
my insurance premium exceed the point where I become uninsurable.
Right?

782
00:48:46.511 --> 00:48:51.511
So these ethical issues now and possibly even a lot of the legal issues now

783
00:48:51.971 --> 00:48:56.971
become entangled and in twine inextricably with all these powerful technologies,

784
00:48:58.320 --> 00:49:00.280
uh,
that,
that we are trying to promote here.

785
00:49:01.350 --> 00:49:02.880
<v 7>Yeah,
I think that's a really good point.</v>

786
00:49:02.881 --> 00:49:07.110
It's one thing to introduce AI to medical students at a hospitals,

787
00:49:07.590 --> 00:49:10.770
but actually it's as important to introduce it to lawyers.

788
00:49:11.100 --> 00:49:13.710
They need to study this as well or business students.

789
00:49:14.100 --> 00:49:18.810
And what my company does is we work with a lot of large enterprises in Asia to

790
00:49:18.811 --> 00:49:21.600
introduced and build AI solutions for them.

791
00:49:21.960 --> 00:49:26.960
And we see a lot of resistance in the beginning because of politics and no Shia,

792
00:49:27.151 --> 00:49:31.770
all of the usual things.
And this idea of privacy that is,

793
00:49:31.890 --> 00:49:35.160
that is not even technically sound sometimes just because they don't even

794
00:49:35.161 --> 00:49:40.161
understand whether something is available or not available or is truly protected

795
00:49:40.681 --> 00:49:44.370
or not.
And I think everybody is just afraid of their own jobs.

796
00:49:44.371 --> 00:49:48.040
They are kind of afraid to take this first step.
So,
um,

797
00:49:48.120 --> 00:49:53.120
making it easier for them with some kind of the repeated pilot or proof of

798
00:49:53.941 --> 00:49:54.631
concept as,

799
00:49:54.631 --> 00:49:58.440
as a lot of companies are trying now and the Singapore government is subsidizing

800
00:49:58.800 --> 00:50:00.450
I think is a good way forward.

801
00:50:00.450 --> 00:50:05.370
And we need to get all small and medium enterprises that are 70% of the backbone

802
00:50:05.371 --> 00:50:08.610
of this country.
We need to get them some basic education.

803
00:50:08.611 --> 00:50:12.630
And then access to the cloud and access to software that they can upload their

804
00:50:12.631 --> 00:50:16.770
data in and began to see some returns on investment.

805
00:50:17.040 --> 00:50:18.330
I think that that's it.

806
00:50:18.450 --> 00:50:21.810
One is the education sector and the other is a business sector really seeing

807
00:50:21.811 --> 00:50:22.980
some return on investment.

808
00:50:23.430 --> 00:50:27.220
The problem is they all expect something they saw in the movies.
I,

809
00:50:27.320 --> 00:50:31.730
there are so many times when I,
when my team we're ride know ride some,

810
00:50:32.030 --> 00:50:36.440
some predictive Algo at them and the data wasn't good,
it was dirty,

811
00:50:36.441 --> 00:50:41.441
Blah Blah Blah and then when I thank God we got like 70% it's amazing and then

812
00:50:41.811 --> 00:50:42.820
they look,
ah,

813
00:50:43.040 --> 00:50:48.040
70% not 99% and it say they're obsessed with is hundred percent 99% number

814
00:50:50.630 --> 00:50:51.600
because they,

815
00:50:51.601 --> 00:50:55.340
they've been going to too many movies are looking at wired magazine and I think

816
00:50:55.341 --> 00:50:56.090
that they,

817
00:50:56.090 --> 00:51:00.290
they need to know it's a process and all of those things are really important to

818
00:51:00.291 --> 00:51:02.900
keep in mind when you're trying to introduce it.

819
00:51:03.090 --> 00:51:06.680
Obviously I'm very interested in seeing how did you say to businesses as well as

820
00:51:06.681 --> 00:51:07.514
to students?

821
00:51:07.700 --> 00:51:09.890
<v 2>Yeah,
I actually,
I'm really interested in that.</v>

822
00:51:09.891 --> 00:51:13.070
In that obsession with the hundred percent thing because that's something I've

823
00:51:13.071 --> 00:51:17.090
come across as,
well maybe this is a bit of a of a distraction,

824
00:51:17.091 --> 00:51:18.920
but let's see if it,
if it strikes a cord.

825
00:51:19.160 --> 00:51:23.600
So I'm personally interested in self driving cars and whenever I talked to

826
00:51:23.601 --> 00:51:25.220
someone about self driving cars,
they say,

827
00:51:25.221 --> 00:51:28.790
well how can you make the self driving car perfect?
And I'm saying,

828
00:51:28.791 --> 00:51:32.120
well I'm not perfect and I can drive a car.

829
00:51:32.360 --> 00:51:35.630
Like why are we not asking that question about like how can I make sure that

830
00:51:35.631 --> 00:51:40.070
you're perfect that driving a car before I let you drive the car?
Right?

831
00:51:40.071 --> 00:51:45.071
So why is it that our expectations of the software that we build is perfection?

832
00:51:45.890 --> 00:51:49.110
Whereas our expectations of ourselves is like as long as we are,

833
00:51:49.510 --> 00:51:53.310
<v 9>it's okay.
I don't think he's is very related to that.</v>

834
00:51:53.311 --> 00:51:56.800
The hype curve we see whenever a new technologies introduced,
right?

835
00:51:56.801 --> 00:51:58.750
There's always like this typical curve off.

836
00:51:58.751 --> 00:52:01.160
You know there's a lot of high becomes a buzzword.
It'd be one,

837
00:52:01.161 --> 00:52:04.840
one syllable and expects the world out of it.
Then this is dissolution men.

838
00:52:04.841 --> 00:52:07.780
When you realize that 100% is not going to happen,

839
00:52:07.781 --> 00:52:11.680
but you may get 70 and eventually we get to the point that you know what 82 is

840
00:52:11.681 --> 00:52:15.130
pretty good.
Right?
And I think if you look at all the applications we're using,

841
00:52:15.131 --> 00:52:17.200
I really like what you just said on the introduction and,
and,

842
00:52:17.201 --> 00:52:21.400
and the fear or sort of the apprehension I think is a journey and improperly

843
00:52:21.401 --> 00:52:23.860
power to the journey to the cloud as well as an example.

844
00:52:23.861 --> 00:52:28.090
I probably more an easier one to two to understand.
Um,

845
00:52:28.120 --> 00:52:31.330
a lot of companies in different degrees in every industry are moving to the

846
00:52:31.331 --> 00:52:34.480
cloud and moving to the cloud as a journey in which they,

847
00:52:34.840 --> 00:52:37.240
as they understand the risks and the advantages,

848
00:52:37.241 --> 00:52:39.580
they feel more and more comfortable moving,
more complex,

849
00:52:39.581 --> 00:52:41.440
more critical workloads to the cloud.

850
00:52:41.440 --> 00:52:45.850
And I think artificial intelligence is just a journey that we see happening in a

851
00:52:45.851 --> 00:52:46.780
very similar way.

852
00:52:46.781 --> 00:52:51.730
And we have a lot of projects in which we are helping implement the FEMAs Chat

853
00:52:51.730 --> 00:52:56.110
Bot,
the femurs.
I'm in customer service and capability,
which is very low risk,

854
00:52:56.230 --> 00:52:58.150
nothing really different.
Critical depends on it.

855
00:52:58.180 --> 00:52:59.830
You can just make someone a bit happier.

856
00:53:00.160 --> 00:53:05.160
You remove friction from the process and moving into using computer vision to

857
00:53:05.291 --> 00:53:09.220
automate a lot of our activities,
but in the interim,
the lower risk tasks,
right?

858
00:53:09.430 --> 00:53:12.640
But we starting to see some traction in places where people are getting more and

859
00:53:12.641 --> 00:53:16.720
more comfortable exploring boarding critical parts of their business in the

860
00:53:16.721 --> 00:53:18.790
hands off of artificial intelligence.

861
00:53:18.791 --> 00:53:21.130
So I guess it's a journey as as as you introduce,

862
00:53:23.380 --> 00:53:25.060
so people just have to get comfortable with

863
00:53:26.010 --> 00:53:29.820
<v 10>yeah.
Like a pair of shoes,
the artificial intelligence.</v>

864
00:53:31.130 --> 00:53:31.963
So,
so,

865
00:53:31.970 --> 00:53:36.690
so victor mentioned about a computational design of vaccines for example.

866
00:53:36.720 --> 00:53:39.900
So recently you heard that uh,
uh,

867
00:53:40.370 --> 00:53:45.370
I was zero tolerance for any drug vaccine that we take.

868
00:53:46.580 --> 00:53:51.120
I,
um,
requires 100%.

869
00:53:51.180 --> 00:53:53.850
You were asking about a hundred percent.
Why is this,

870
00:53:54.350 --> 00:53:59.350
it's important to us if I have a child and I have to decide whether it's going

871
00:53:59.881 --> 00:54:01.200
to take this vaccine or not.

872
00:54:02.190 --> 00:54:06.420
So that has resulted say in the u s where a lot of people are against the east

873
00:54:06.421 --> 00:54:08.100
vaccinations because the are free,

874
00:54:08.101 --> 00:54:10.800
that he could be leading to all kinds of ailments,

875
00:54:10.801 --> 00:54:15.801
like including autism and so on bag by arguably a fairly tenuous,

876
00:54:16.710 --> 00:54:17.880
uh,
facts.

877
00:54:19.710 --> 00:54:23.730
But if there is a risk that my child may get,
I might say no,

878
00:54:24.360 --> 00:54:28.740
but result is recent outbreak of measles,
for example.

879
00:54:29.670 --> 00:54:30.900
So how do you balance that?

880
00:54:31.470 --> 00:54:35.430
So this is where all these serious ethical considerations and legal issues as

881
00:54:35.431 --> 00:54:39.250
well and where the government has to put in a law that says no,
you know,

882
00:54:39.360 --> 00:54:41.430
for the benefit of the entire community,

883
00:54:42.030 --> 00:54:47.030
we are happy to have 1% of people suffering the negative potential negative

884
00:54:47.311 --> 00:54:51.360
effects.
And that's computes to quite a large number.

885
00:54:51.361 --> 00:54:55.230
If you have a million people,
and it's 1% a cycle of people,
you know,

886
00:54:56.220 --> 00:54:58.830
a heck a lot of children that will be affected by it.

887
00:54:59.520 --> 00:55:02.200
So you've got to weigh that against the,
uh,

888
00:55:02.370 --> 00:55:06.840
public health issues versus the,
uh,
uh,
the,
the,
um,
uh,

889
00:55:06.850 --> 00:55:09.630
the lack of hundred percent efficacy of anything.

890
00:55:10.680 --> 00:55:12.390
So these are things that,
uh,

891
00:55:12.420 --> 00:55:17.420
I doubt if any AI system can eventually grapple with maybe a I n f a the

892
00:55:18.790 --> 00:55:23.760
ultimate AI ethical system.
Uh,
but that will be another story for another day.

893
00:55:24.330 --> 00:55:25.163
But it's also,

894
00:55:25.170 --> 00:55:29.070
it is critically important for us when we talk about AI in health care,

895
00:55:29.071 --> 00:55:33.600
that we take this into consideration and,
uh,
um,
and,

896
00:55:33.601 --> 00:55:38.601
and really think through the whole process of how we go about introducing a AI

897
00:55:41.100 --> 00:55:45.290
in a measured,
calibrate that and nuanced manner in healthcare.

898
00:55:45.480 --> 00:55:49.690
<v 0>Oh,
you have an idea for applying AI to healthcare.</v>

899
00:55:49.950 --> 00:55:53.550
Let me know in the comment section and please subscribe for more educational

900
00:55:53.551 --> 00:55:57.810
videos.
For now.
I've got to find a float tank.
So thanks for watching.

