WEBVTT

1
00:00:00.270 --> 00:00:03.240
I don't have to worry about managing my own servers.

2
00:00:04.080 --> 00:00:08.550
There is no spoon.
Hello world.

3
00:00:08.580 --> 00:00:13.580
It's a rod and serverless computing seems to be on every developer's mind these

4
00:00:14.670 --> 00:00:15.000
days.

5
00:00:15.000 --> 00:00:20.000
In this video I'll explain how using Google cloud you can easily build,

6
00:00:20.310 --> 00:00:25.200
train and deploy an APP that can predict the purchasing power of a user.

7
00:00:25.470 --> 00:00:29.640
We can then give them a personalized price for whatever our product is.

8
00:00:30.120 --> 00:00:33.990
Think about all the hassles that a developer has to deal with when it comes to

9
00:00:33.991 --> 00:00:38.520
managing the servers that run their applications.
Provisioning,

10
00:00:38.580 --> 00:00:41.130
deployment,
patching,
monitoring,

11
00:00:41.340 --> 00:00:46.340
all of these little details about the server and operating systems don't

12
00:00:46.351 --> 00:00:49.440
necessarily have to do with the features of the APP,

13
00:00:49.830 --> 00:00:53.220
but are still necessary components to be thinking about.

14
00:00:53.520 --> 00:00:58.520
Serverless computing implies a very high level of abstraction when interacting

15
00:00:58.651 --> 00:01:02.790
with the cloud.
Focus on the code,
not the infrastructure.

16
00:01:03.180 --> 00:01:06.780
It lets developers reduce the entire coding pipeline.

17
00:01:07.350 --> 00:01:11.340
Google cloud has been pushing this concept for a while and they recently

18
00:01:11.341 --> 00:01:15.900
released an internal research tool to the public called colab.

19
00:01:16.170 --> 00:01:21.090
If you go to colab.research.google.com you can create a Jupiter notebook

20
00:01:21.120 --> 00:01:21.953
immediately.

21
00:01:22.080 --> 00:01:27.080
It runs on their cloud infrastructure and you get to use an Nvidia Kad Gpu for

22
00:01:27.721 --> 00:01:30.360
up to 12 hours at a time foe free.

23
00:01:30.690 --> 00:01:34.770
You can install any dependencies using pip and train and test machine learning

24
00:01:34.771 --> 00:01:38.880
models with a zero setup.
It sounds too good to be true,

25
00:01:39.030 --> 00:01:39.900
but it's true.

26
00:01:40.620 --> 00:01:44.280
That's the easiest way to get started with Google cloud,

27
00:01:44.310 --> 00:01:48.900
but let's talk about their other services.
They've got quite a lot.

28
00:01:48.930 --> 00:01:53.930
If we look on the main page and it can be confusing trying to understand what

29
00:01:53.970 --> 00:01:57.480
each service does and what makes it unique from the others.

30
00:01:57.810 --> 00:02:01.560
We can categorize cloud services in general,
like a pyramid.

31
00:02:01.890 --> 00:02:05.580
At the bottom of the pyramid is infrastructure as a service.

32
00:02:05.790 --> 00:02:06.990
It's the lowest level.

33
00:02:06.991 --> 00:02:11.160
A cloud provider can offer CPU operating systems.

34
00:02:11.161 --> 00:02:15.870
You as the customer are in charge of putting the pieces together to run your

35
00:02:15.871 --> 00:02:16.704
business.

36
00:02:16.770 --> 00:02:21.720
You get complete control of all the components from load balancing to hardware

37
00:02:21.750 --> 00:02:24.540
options.
If we move up the pyramid,

38
00:02:24.690 --> 00:02:27.210
we arrive at containers as a service.

39
00:02:27.450 --> 00:02:32.450
Containers help modularize apps so different containers can hold different

40
00:02:32.701 --> 00:02:35.070
services.
You can have a container host,

41
00:02:35.071 --> 00:02:38.430
the front end of your web APP and another host the backend.

42
00:02:38.670 --> 00:02:43.470
You don't have to deal with operating system level dependencies and get

43
00:02:43.471 --> 00:02:47.040
prepackaged libraries ready to use for you.

44
00:02:47.400 --> 00:02:52.290
If we move up the chain of abstraction,
we arrive at platform as a service.

45
00:02:52.620 --> 00:02:54.030
This is where the cloud vendor,

46
00:02:54.031 --> 00:02:59.031
we'll handle all the low level infrastructure details and you can just upload

47
00:02:59.080 --> 00:03:01.030
your app and have it run remotely.

48
00:03:01.420 --> 00:03:05.020
At the very top of the pyramid is software as a service.

49
00:03:05.020 --> 00:03:10.020
These are tailored for end users applications like email and bitcoin faucets

50
00:03:10.201 --> 00:03:14.740
schemes.
So let's talk about what Google cloud offers you in this regard.

51
00:03:15.220 --> 00:03:19.450
Who will compute engine?
Is there infrastructure as a service offering?

52
00:03:19.720 --> 00:03:24.610
You can create virtual machines,
allocate CPU and memory as you like.

53
00:03:24.640 --> 00:03:29.640
It's like building your own computer and handling all the details of running it

54
00:03:30.010 --> 00:03:34.840
minus the dope liquid cooling system.
Of course you can select micro instances,

55
00:03:34.841 --> 00:03:39.841
which is 0.1 cores and one gigs of Ram to massive CPU clusters or even tps with

56
00:03:41.171 --> 00:03:45.250
over 300 gigs of ram custom size virtual machines.

57
00:03:45.251 --> 00:03:50.140
It's all possible.
Google is currently the only provider offering TPU.

58
00:03:50.650 --> 00:03:55.650
A tensor processing unit or TPU is very different architecturally from a Gpu.

59
00:03:56.860 --> 00:03:59.650
A GPU is a processor in its own right.

60
00:03:59.680 --> 00:04:02.620
It's optimized for vectorize numerical code.

61
00:04:02.920 --> 00:04:05.140
A TPU is a coprocessor.

62
00:04:05.170 --> 00:04:10.170
It can't execute code and its own right all code execution takes place on the

63
00:04:10.901 --> 00:04:15.820
CPU,
which feeds a stream of micro operations to the TPU.

64
00:04:16.270 --> 00:04:21.070
Tpes are cheaper and use a lot less power and can thus finish giant prediction

65
00:04:21.071 --> 00:04:23.020
jobs cheaper than a GPU.

66
00:04:23.410 --> 00:04:26.710
A TPU is aren't really great for training networks right now.

67
00:04:26.711 --> 00:04:31.300
They're more focused on executing predictions with them after training and

68
00:04:31.301 --> 00:04:35.590
there's no reason a TPU couldn't run something other than a tensorflow model.

69
00:04:35.740 --> 00:04:39.550
It's just that nobody has written the compilers to do so yet.

70
00:04:39.850 --> 00:04:42.170
They're not as general purpose as GPS.

71
00:04:42.220 --> 00:04:45.070
They're designed specifically for machine learning tasks.

72
00:04:45.190 --> 00:04:48.430
Google's Coopernetti's engine is their container offering.

73
00:04:48.610 --> 00:04:53.610
It allows developers to easily run their docker containers in a fully managed

74
00:04:53.890 --> 00:04:55.510
Coobernetti's environment.

75
00:04:55.810 --> 00:05:00.700
It comprises of a group of Google compute engine instances which run

76
00:05:00.701 --> 00:05:01.600
Coobernetti's.

77
00:05:01.810 --> 00:05:06.810
A master node manages a cluster of docker containers and runs a Kubernetes Api

78
00:05:07.511 --> 00:05:12.511
server to interact with the cluster and perform tasks like servicing API

79
00:05:12.851 --> 00:05:15.100
requests and scheduling containers.

80
00:05:15.550 --> 00:05:20.200
Docker is like a virtual machine but doesn't create a whole virtual operating

81
00:05:20.201 --> 00:05:21.034
system.

82
00:05:21.100 --> 00:05:26.050
It's a way for developers to package up an APP with only the necessary parts it

83
00:05:26.051 --> 00:05:26.860
needs.

84
00:05:26.860 --> 00:05:31.630
Coopernetties was open sourced by Google over a decade ago and builds on their

85
00:05:31.631 --> 00:05:35.020
experience running production workloads at scale.

86
00:05:35.320 --> 00:05:38.470
It's a container centric management environment.

87
00:05:38.680 --> 00:05:43.600
All the computing networking and storage infrastructure is handled by the

88
00:05:43.601 --> 00:05:44.434
service.

89
00:05:44.590 --> 00:05:49.210
It's not an all inclusive platform though all of the default solutions it

90
00:05:49.211 --> 00:05:54.211
provides for things like deployment and scaling are optional and configurable.

91
00:05:54.520 --> 00:05:57.220
It's a good mix of options and ease of use.

92
00:05:57.530 --> 00:06:02.240
The real platform as a service is Google APP engine.
You bring the code,

93
00:06:02.270 --> 00:06:03.440
they handle the rest.

94
00:06:03.740 --> 00:06:08.060
It automatically handles scaling to meet load and demand for users,

95
00:06:08.300 --> 00:06:11.090
so don't worry about sudden spikes in traffic.

96
00:06:11.300 --> 00:06:16.300
You pay exactly for the resource that your APP requires at any given point.

97
00:06:16.940 --> 00:06:21.940
It uses Netties under the hood to handle all of this so you don't have to.

98
00:06:22.520 --> 00:06:27.140
This coupled with their cloud functions make for a powerful combination.

99
00:06:27.500 --> 00:06:31.280
Cloud functions allow you to trigger events via function calls,

100
00:06:31.370 --> 00:06:32.990
so say a user signs up.

101
00:06:33.050 --> 00:06:37.670
A cloud function can be triggered to alert the developers and luckily for us AI

102
00:06:37.671 --> 00:06:38.210
wizards,

103
00:06:38.210 --> 00:06:42.710
Google has a version of APP engine dedicated specifically to machine learning

104
00:06:42.730 --> 00:06:45.380
apps called machine learning engine.

105
00:06:45.680 --> 00:06:50.120
It's already being used in enterprise settings to solve problems like ensuring

106
00:06:50.121 --> 00:06:54.200
food safety and identifying clouds in satellite images.

107
00:06:54.650 --> 00:06:59.450
It allows developers to train machine learning models using multiple frameworks

108
00:06:59.451 --> 00:07:04.190
including tensorflow,
carrots and sidekick learn and once trained.

109
00:07:04.220 --> 00:07:07.460
It also lets you serve that model to users.

110
00:07:07.670 --> 00:07:12.670
You can use multiple frameworks to build multiple models and have them served at

111
00:07:13.251 --> 00:07:15.920
the right time,
update them as necessary.

112
00:07:16.190 --> 00:07:20.900
This is all a part of the tensorflow serving library and while normally you'd

113
00:07:20.901 --> 00:07:23.240
have to think about a lot of those details.

114
00:07:23.510 --> 00:07:26.480
ML engine takes care of a lot of that for you.

115
00:07:26.690 --> 00:07:31.690
And my favorite part is the ability to automatically tune the hyper parameters

116
00:07:32.331 --> 00:07:33.230
for your model,

117
00:07:33.380 --> 00:07:38.060
which can save you lots of time trying to guess and check what the optimal

118
00:07:38.061 --> 00:07:42.350
values should be,
so let's try it out for our demo.

119
00:07:42.351 --> 00:07:46.400
We're going to try and predict the income of a given person.

120
00:07:46.760 --> 00:07:51.760
I created a model based on a census income data set available from the UCI

121
00:07:52.431 --> 00:07:54.470
public repo of data sets.

122
00:07:54.830 --> 00:07:59.480
The data set has many features for each individual including their education,

123
00:07:59.630 --> 00:08:02.200
marital state hours worked,
et cetera.

124
00:08:02.390 --> 00:08:05.630
Since we've already created a Google cloud project,

125
00:08:05.690 --> 00:08:10.640
we can start by creating a cloud storage bucket using the web Ui that we can

126
00:08:10.700 --> 00:08:12.530
easily access in the browser.

127
00:08:12.920 --> 00:08:17.180
We have our downloaded data and we can upload it to the cloud storage bucket

128
00:08:17.181 --> 00:08:21.140
with just a few commands,
both the training and testing data.

129
00:08:21.440 --> 00:08:22.191
With that,

130
00:08:22.191 --> 00:08:27.191
we can run a distributed training job in the cloud including any hyper parameter

131
00:08:27.530 --> 00:08:29.300
tuning as we see fit.

132
00:08:29.690 --> 00:08:32.660
It's using a wide and deep neural network.

133
00:08:32.900 --> 00:08:36.650
The idea here is that linear models are easy to use and understand.

134
00:08:36.680 --> 00:08:41.680
They can memorize the relationship between individual features pretty easily.

135
00:08:42.260 --> 00:08:45.350
Feature engineering results in a lot of derived features.

136
00:08:45.380 --> 00:08:48.740
So linear models can be considered why learning,

137
00:08:49.130 --> 00:08:53.810
but linear models aren't good at generalizing across different features because

138
00:08:53.811 --> 00:08:58.811
they can't see this relationship unless we feed in a set of higher order derived

139
00:08:58.891 --> 00:09:02.280
features that capture this,
but this is labor intensive.

140
00:09:02.610 --> 00:09:05.610
Neural Nets or deep models are better at this,

141
00:09:05.611 --> 00:09:09.960
but there are also prone to overgeneralization and don't do the best job

142
00:09:09.961 --> 00:09:12.930
memorizing specific feature combinations.

143
00:09:13.140 --> 00:09:18.140
So we can use a jointly trained model that combines both wide and deep learning

144
00:09:18.990 --> 00:09:23.730
to produce a more accurate output.
We'll give it a name and an output path,

145
00:09:23.820 --> 00:09:28.530
then submit the training command to have it start running inside of the Gui.

146
00:09:28.560 --> 00:09:32.460
We can see the progress up our job visually.
Once trained,

147
00:09:32.461 --> 00:09:34.350
we can deploy it to production.

148
00:09:34.500 --> 00:09:38.280
I referencing the train model and using the versions create command.

149
00:09:38.760 --> 00:09:43.530
Now we can test our deployed model by retrieving a response for a novel data

150
00:09:43.531 --> 00:09:44.340
point.

151
00:09:44.340 --> 00:09:49.340
Looks like this person likely has an income less than 50,000 USD per year,

152
00:09:49.920 --> 00:09:54.240
so we can offer them a more affordable price for whatever our product is.

153
00:09:54.870 --> 00:09:56.970
I really hope you've found this video useful.

154
00:09:56.971 --> 00:10:01.230
Please subscribe for more programming videos,
and for now I've got to go deeper,

155
00:10:01.440 --> 00:10:02.820
so thanks for watching.

