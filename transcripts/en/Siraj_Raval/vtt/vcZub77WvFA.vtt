WEBVTT

1
00:00:00.320 --> 00:00:01.153
Yes.

2
00:00:02.400 --> 00:00:05.670
<v 1>Let's see who we've got in the room today.</v>

3
00:00:08.450 --> 00:00:11.570
All right,
I'm me.
Go up here

4
00:00:14.350 --> 00:00:15.340
and then

5
00:00:23.690 --> 00:00:28.040
see who we've got.
What does that sound is heard some music.

6
00:00:28.530 --> 00:00:29.130
<v 0>Okay.</v>

7
00:00:29.130 --> 00:00:30.400
<v 1>Oh,
what is that?
What is that?</v>

8
00:00:35.380 --> 00:00:36.213
<v 0>Okay.</v>

9
00:00:36.760 --> 00:00:38.970
<v 1>Hi.
Hold on.
I'm</v>

10
00:00:39.450 --> 00:00:39.800
<v 0>yeah,</v>

11
00:00:39.800 --> 00:00:42.950
<v 1>hearing something.
What does that sound?</v>

12
00:00:46.830 --> 00:00:47.990
What does that sound?

13
00:00:59.180 --> 00:01:02.900
Hi everybody.
I'm about to get in this room in a second.

14
00:01:02.901 --> 00:01:06.650
I'm going to talk to all of you guys.
I'm pulling up this video.

15
00:01:06.980 --> 00:01:11.330
So here we go.
Oh,
I'm hearing a,
an ad.

16
00:01:11.331 --> 00:01:15.680
So it's my noise cancellation.
I everybody,
oh world.
It's to Raj.

17
00:01:15.740 --> 00:01:19.340
And today we're going to create a,

18
00:01:19.910 --> 00:01:23.770
a two layer neural network.
Let me just get this ad out of the way.
All right.

19
00:01:23.780 --> 00:01:28.220
There.
Okay,
here we go.
Okay,
so we,
I'm in the room and I'm ready to go.

20
00:01:28.610 --> 00:01:30.680
Hi everybody.
Okay,
that was crazy.
Okay,

21
00:01:30.681 --> 00:01:35.681
so I am live and there are 83 people watching right now and I'll everybody.

22
00:01:35.691 --> 00:01:40.160
Hi Andre.
Hi David.
Hypercar.
Hi Yasue.
Today is Wednesday,

23
00:01:40.370 --> 00:01:43.910
m and ISC.
No,
it's not him.
And I see,
okay,
so here was what we were going to do.

24
00:01:44.260 --> 00:01:49.260
We are going [inaudible] two layer neural network from scratch that can predict

25
00:01:50.430 --> 00:01:54.720
the XR value.
Okay.
So what does [inaudible] mean?

26
00:01:54.810 --> 00:01:57.300
That means we're just going to give it an array of numbers.
Okay.

27
00:01:57.301 --> 00:02:00.620
So that means it's going to be ones and Zeros.
I'm not going to use Siano,

28
00:02:00.630 --> 00:02:04.820
I'm not going to use tensor flow.
I'm not gonna use anything but num Pi and,

29
00:02:04.920 --> 00:02:07.950
and the time library.
We are building this thing from scratch.

30
00:02:07.980 --> 00:02:11.870
Just a two layer neural network.
Uh,
no psychic,
nothing like,

31
00:02:11.900 --> 00:02:15.690
because people have been asking me to build a neural net from scratch for a

32
00:02:15.691 --> 00:02:19.890
while,
so I'm going to do it.
Okay.
And so disclaimer,
okay.

33
00:02:19.891 --> 00:02:23.250
So like I've made a video on building a neural network before.

34
00:02:23.550 --> 00:02:28.020
I know you did that in kindergarten.
That's awesome.
Uh,
but Parker Ali Baba,

35
00:02:28.110 --> 00:02:30.840
US Mitchell,
I'm goes,
keep shouting out.

36
00:02:31.260 --> 00:02:34.470
I've built a neural network before in a video called building neuron up brick.

37
00:02:34.471 --> 00:02:37.780
Build a neural net in four minutes and it's my most viewed video.
Uh,

38
00:02:37.800 --> 00:02:42.610
but that was like a pretty easy,
you know,
a neural net.
Uh,
this is going to,

39
00:02:42.870 --> 00:02:46.260
this is the first time I'm going to say that this is going to be a little bit

40
00:02:46.290 --> 00:02:50.580
intensive.
It's got to be a little bit of a,
math is going to be a quiet,

41
00:02:50.581 --> 00:02:55.410
quite a bit of math in here.
So this is not going to be a dead simple.

42
00:02:55.411 --> 00:02:58.470
I'm just going to say that for the first time because you guys want to know the

43
00:02:58.500 --> 00:03:02.230
neuro that from scratch.
So like let's just go,
you know what I'm saying?
So,
okay,

44
00:03:02.500 --> 00:03:04.840
here we go.
We're going to do this.
Uh,
and it's,
it's gonna.

45
00:03:04.870 --> 00:03:07.190
It's gonna predict the x value.
Okay,
so it's going to,

46
00:03:07.200 --> 00:03:10.240
we're going to get ones and Zeros and then we're going to generate some random

47
00:03:10.241 --> 00:03:12.070
data and we're going to,
we're going to compare them.

48
00:03:12.070 --> 00:03:14.080
So it's like x or means and computer science.

49
00:03:14.081 --> 00:03:16.410
It's like a exclusive or which means it's,

50
00:03:16.540 --> 00:03:18.670
it's only returns true if they're both different,

51
00:03:18.700 --> 00:03:22.600
like or ones or if it's a one and zero,
oh,
it's a one.
If it's a zero and a one,

52
00:03:22.690 --> 00:03:25.840
oh,
it's a one.
If it's a zero and a zero,
it's a zero.
If it's a one and a one,

53
00:03:25.870 --> 00:03:29.320
it's a one.
It's a zero.
Okay,
so exclusive more.
All right,

54
00:03:29.321 --> 00:03:32.190
so that's what we're going to do a,

55
00:03:32.200 --> 00:03:35.250
and I'm going to start off by doing a five minute Q away like always,

56
00:03:35.260 --> 00:03:38.170
and then we're going to get right into the code.
Okay?
So ask away.

57
00:03:38.171 --> 00:03:43.030
Five minutes starts.
Now.
Where do I work?
I don't work for anybody.

58
00:03:43.090 --> 00:03:47.320
Uh,
although I do have a video coming out for open Ai,

59
00:03:47.410 --> 00:03:51.490
which I'm super psyched about.
Can't say anything about it yet anyway.
Uh,

60
00:03:52.060 --> 00:03:54.700
how do I learn math from machine learning?
Uh,

61
00:03:55.180 --> 00:03:59.410
linear Algebra specifically and statistics.
Uh,

62
00:03:59.470 --> 00:04:01.690
you'd asked if he has some great courses on both of those things.

63
00:04:01.960 --> 00:04:04.690
What do you think of microdosing Lsd?
I think it's a great idea,

64
00:04:04.691 --> 00:04:05.860
but you have to have a friend with you.

65
00:04:06.250 --> 00:04:08.620
What are your thoughts on the Udacity nanodegree in machine learning?

66
00:04:08.890 --> 00:04:12.610
Great courts.
What Linux distro do you recommend who been to,

67
00:04:12.850 --> 00:04:17.050
how long have you been coding?
Six years,
but only seriously like three years.

68
00:04:17.320 --> 00:04:21.610
How can we help you?
Other than with money?
Please promote my videos.
Seriously.

69
00:04:21.611 --> 00:04:23.650
Like get people to subscribe to my videos.

70
00:04:23.651 --> 00:04:28.000
Like I feel like I put all my time and energy to these videos and I don't have

71
00:04:28.001 --> 00:04:29.800
nearly as many subscribers as I want.

72
00:04:29.801 --> 00:04:33.160
So anything you guys can do to get people to subscribe would be super helpful.

73
00:04:33.880 --> 00:04:38.350
When would you use a drone that over a random forest?
Oh Man.

74
00:04:39.580 --> 00:04:42.330
Just go by this simple rule of thumb.
If you,

75
00:04:42.490 --> 00:04:45.880
a deep neural net will outperform any other machine learning method.

76
00:04:46.390 --> 00:04:50.140
Pretty much 90% of the time at least a.

77
00:04:50.141 --> 00:04:52.870
So if you have a lot of data,
use a neural net.

78
00:04:52.871 --> 00:04:55.720
If you don't have a lot of data though,
if you only have like a hundred samples,

79
00:04:55.930 --> 00:05:00.040
then I would look into a random forest big and bill.com does that for you.

80
00:05:01.360 --> 00:05:04.600
Okay.
What's your goal with youtube?
I want to be a public figure.

81
00:05:04.601 --> 00:05:08.500
I want to beat the face of machine learning,
the face of artificial intelligence,

82
00:05:08.501 --> 00:05:10.700
the face of computer science.
Uh,

83
00:05:10.870 --> 00:05:14.710
and I want to use youtube as a platform to get this stuff out there and get

84
00:05:14.711 --> 00:05:17.920
everybody building AI and building intelligence software.

85
00:05:19.650 --> 00:05:21.900
All right.
How can,

86
00:05:21.930 --> 00:05:25.260
can you tell us how long will this live stream at approximately be?

87
00:05:25.261 --> 00:05:28.980
This is going to be about 30 to 40 minutes.

88
00:05:30.090 --> 00:05:30.923
MMM.

89
00:05:33.640 --> 00:05:37.120
What is the coolest Ai ml project you've come up with?
Uh,

90
00:05:37.150 --> 00:05:38.740
I think building an AI composer,

91
00:05:38.741 --> 00:05:42.700
it's my second machine learning for hackers video build an AI composer was like

92
00:05:42.701 --> 00:05:45.850
my favorite project.
Cause you generate music with machine learning.

93
00:05:45.851 --> 00:05:50.260
How cool is that?
Why didn't you try for Masters and Phd?
Well,

94
00:05:50.261 --> 00:05:54.520
I mean my parents definitely wanted me to have a masters in Phd.
Uh,

95
00:05:54.521 --> 00:05:57.270
and they still kind of do and they always will.
Uh,

96
00:05:57.290 --> 00:06:02.290
but I'm just going to keep doing this because I have the time to do this.

97
00:06:04.221 --> 00:06:07.640
I can study on my own and I have good advisors more or less.

98
00:06:07.641 --> 00:06:11.180
So I'm kind of doing a phd,
but it's like not an official,
it's like a,

99
00:06:11.240 --> 00:06:13.160
it's like a new thing and I don't,

100
00:06:13.161 --> 00:06:15.800
I just don't want to have to get with the bureaucracy of academia.

101
00:06:15.980 --> 00:06:20.210
Not to say all research labs are bureaucratic,
but most of them are.

102
00:06:20.730 --> 00:06:21.563
Um,

103
00:06:23.460 --> 00:06:26.280
please try to make these a little early for us here in India.

104
00:06:26.730 --> 00:06:31.110
I can do it 30 minutes earlier and much love to everybody in India.

105
00:06:31.590 --> 00:06:36.360
More d three videos please.
Okay.
I,
I can,
I can,
uh,
yeah.
All right,

106
00:06:36.361 --> 00:06:40.920
good to know.
Bose QC 35 headphones.
Exactly.
It's,
uh,

107
00:06:41.850 --> 00:06:45.960
where did you go to school?
I went to [inaudible] high school in Houston,
Texas.

108
00:06:45.961 --> 00:06:49.260
That's where I grew up.
And then I went to Columbia University in New York City.

109
00:06:49.740 --> 00:06:52.020
Uh,
and yeah,
for Undergrad.

110
00:06:52.500 --> 00:06:55.890
Do you believe academia is the best path for learning machine learning and AI?

111
00:06:56.280 --> 00:07:00.120
Uh,
man,
that's,
that's a great question.
I think any,

112
00:07:00.121 --> 00:07:04.570
anything that can give you the time
and the time and,

113
00:07:04.600 --> 00:07:06.670
and the time to study,

114
00:07:06.700 --> 00:07:10.900
but she running on the Internet is the way to go.
It's so if that means academia,

115
00:07:10.930 --> 00:07:13.540
if that means you've got a scholarship and they're paying for your housing and

116
00:07:13.541 --> 00:07:16.420
they give you a little stipend so you can eat,
that's the best way.

117
00:07:16.480 --> 00:07:18.970
But if you can make money otherwise and then you could just like live like,

118
00:07:18.971 --> 00:07:21.860
you know,
do some contract work,
then you can,
you know,

119
00:07:22.180 --> 00:07:23.290
then that that's what I did.

120
00:07:23.291 --> 00:07:26.070
I just made some money doing contract work for mobile development and I used

121
00:07:26.071 --> 00:07:29.200
that money to just live and study machine learning on the Internet.

122
00:07:29.410 --> 00:07:31.090
I can't do classes in real life.

123
00:07:31.120 --> 00:07:35.500
I can't pay attention to professors who just go on and on and on and they just,

124
00:07:35.970 --> 00:07:38.530
Ah,
you know what I'm saying?
So I just want it to x button,

125
00:07:38.531 --> 00:07:41.710
three x button on life and the Internet is the way people ask me this question

126
00:07:41.711 --> 00:07:44.170
all the time.
Like,
how did you learn machine learning?
What is the way to do this?

127
00:07:44.171 --> 00:07:46.330
And I just tell them every time the internet,

128
00:07:46.600 --> 00:07:48.670
the Internet makes everything obsolete.

129
00:07:48.671 --> 00:07:52.030
Colleges were invented before the Internet and that was the way to do it.

130
00:07:52.031 --> 00:07:54.490
And they were a centralized.
But now with the Internet,

131
00:07:54.491 --> 00:07:56.500
anybody can learn anything online.

132
00:07:56.950 --> 00:08:00.100
So I watched videos very fast and I look at other sources like reddit and

133
00:08:00.101 --> 00:08:00.671
Twitter feeds.

134
00:08:00.671 --> 00:08:03.670
I follow people like policies for new Boston and the whole injuries,

135
00:08:03.760 --> 00:08:06.250
Andreessen Horowitz team.
So I have a lot of data inputs.

136
00:08:06.251 --> 00:08:11.180
I'm going to have one more question and I'm going to get started.
Uh,

137
00:08:14.190 --> 00:08:16.890
what's the best entry point for people that are relatively new to machine

138
00:08:16.891 --> 00:08:18.240
learning and AI?
My video,

139
00:08:18.250 --> 00:08:22.500
start from the very beginning and download the code for each video and run it

140
00:08:22.501 --> 00:08:25.230
locally and get it to work and study it.
I promise you,

141
00:08:25.231 --> 00:08:29.520
you will get to be a pro in no time.
Okay,
so that's it for the questions.

142
00:08:29.521 --> 00:08:30.390
Let's get started.

143
00:08:30.690 --> 00:08:33.570
We're going to build this two layer neural network and it's going to predict the

144
00:08:33.571 --> 00:08:37.680
XR outputs.
All right,
I'm going to help.
So here I go.

145
00:08:37.681 --> 00:08:39.750
I'm going to share my screen and we're going to get started.

146
00:08:41.730 --> 00:08:46.590
Here we go,
screen share desktop start.

147
00:08:47.040 --> 00:08:49.710
All right,
I'm gonna move this out of the way

148
00:08:51.490 --> 00:08:56.040
and boom,
let's see.

149
00:08:56.460 --> 00:08:59.700
You guys can see all this.
All righty,

150
00:09:01.050 --> 00:09:05.640
great,
cool,
cool,
cool,
cool.
All right,

151
00:09:06.030 --> 00:09:07.470
so let's get started.

152
00:09:09.600 --> 00:09:13.440
This is a big enough thing.
Yes.
Okay,

153
00:09:13.770 --> 00:09:18.730
so the two layer neural network.
Okay.

154
00:09:18.850 --> 00:09:22.080
So here we go.
Let's get started.
So the first thing when they do is important,

155
00:09:22.081 --> 00:09:26.260
our dependencies.
The first dependency we want to,
we want to import is a non pie.

156
00:09:26.261 --> 00:09:27.580
Would you start scientific computing,

157
00:09:27.581 --> 00:09:30.880
library num Pi is going to last you all of our matrix multiplication,

158
00:09:31.120 --> 00:09:32.770
which is very important in your own networks.

159
00:09:32.771 --> 00:09:35.260
Matrix multiplication is very important in neural networks.

160
00:09:35.470 --> 00:09:37.510
And I'm gonna explain why.
So that's our first library.

161
00:09:37.511 --> 00:09:42.070
And our next one is gonna be time and time is going to help us,
uh,
time,

162
00:09:42.130 --> 00:09:46.540
time,
how long our training is going to be.
Okay.
So let's,
so that's it.

163
00:09:46.541 --> 00:09:48.610
That's all we're going to,
uh,

164
00:09:49.120 --> 00:09:52.840
that's the only libraries were going to import.
Okay.
So let's just get started.

165
00:09:53.200 --> 00:09:56.530
Let's go ahead and start defining our variables.
Okay.

166
00:09:56.740 --> 00:09:58.320
So the first one we wouldn't want to,

167
00:09:58.450 --> 00:10:00.940
we wanted to find is our number of hidden neurons.

168
00:10:00.970 --> 00:10:05.800
And we're just going to say 10.
Why?
Because we're going to have 10 uh,

169
00:10:05.830 --> 00:10:08.980
values like ones and zeros going array of ones and Zeros.

170
00:10:09.040 --> 00:10:12.280
Then we're going to input into our network and we want to compare it to 10 other

171
00:10:12.281 --> 00:10:15.300
ones and Zeros,
and then he'll put the extra value.
That's,
that's,

172
00:10:15.370 --> 00:10:17.920
that's what the output is going to look like.
Just ones and Zeros.
10 of them.

173
00:10:18.280 --> 00:10:22.030
Okay.
And so now we want artists show the number of inputs.
How many?

174
00:10:22.140 --> 00:10:26.230
10 there it's 10 uh,
neurons and it's 10 inputs.

175
00:10:26.560 --> 00:10:30.130
And then how many outputs do we want to have?
Well,
we want to hop 10.

176
00:10:30.340 --> 00:10:33.760
Glad to have you here.
Good vibes.
All right,

177
00:10:35.020 --> 00:10:39.040
so here we go.
So we want the number of outputs to be 10.
Okay.

178
00:10:39.041 --> 00:10:41.440
And then one more thing.
We want sample data.

179
00:10:41.920 --> 00:10:44.530
How much sample data do we want to have?
Well,

180
00:10:44.531 --> 00:10:48.040
let's just have some really big number and we're just going to pick 10 of them.

181
00:10:48.160 --> 00:10:50.920
So what's a big number that we can pick?
And we're just gonna pick 10 of them.

182
00:10:51.100 --> 00:10:54.730
Let's do,
uh,
this is a neural network for classification.
Yes,

183
00:10:55.090 --> 00:10:57.060
we're going to pick 300.
All right.

184
00:10:57.310 --> 00:11:00.530
300 sample data that we're going to generate.
Okay.

185
00:11:00.540 --> 00:11:02.530
So that's it for our variables.
Okay.

186
00:11:02.531 --> 00:11:04.620
So now we're going to define our hyper parameters.

187
00:11:04.930 --> 00:11:06.700
What are hyper parameter is going to be?
Well,

188
00:11:06.701 --> 00:11:08.200
the first one is going to be the learning Rick.

189
00:11:08.380 --> 00:11:11.560
And the learning rate basically defines how factory,
what our network to move.

190
00:11:11.740 --> 00:11:12.820
And he's a tuning knobs.

191
00:11:12.970 --> 00:11:16.330
We can make it go faster or we can make it go slower depending on what we want.

192
00:11:16.510 --> 00:11:19.780
Okay?
So that's our learning,
right?
And there's one more tuning knob.

193
00:11:19.781 --> 00:11:22.180
There's one more hyper parameter that we want and this model,

194
00:11:22.181 --> 00:11:24.640
and it's called momentum.
Okay?

195
00:11:24.670 --> 00:11:28.990
These are two different variables and we're going to use them both,
uh,
too

196
00:11:29.800 --> 00:11:31.990
to lower the loss function,

197
00:11:32.170 --> 00:11:36.250
which is going to be called Cross entropy as we train our neural net.
Okay.

198
00:11:36.430 --> 00:11:40.480
Those are the only two hyper parameters that we're going to use.
All right?

199
00:11:40.960 --> 00:11:45.760
So those are our two hyper parameters and that's it.

200
00:11:45.820 --> 00:11:50.170
So now we're going to generate a random sample.
Oh,
sorry.

201
00:11:50.230 --> 00:11:51.730
Now we're going to see are

202
00:11:53.290 --> 00:11:57.720
now we're going to seed our random number generators.

203
00:11:57.820 --> 00:11:59.360
What does this mean?
Well,
we're going to,

204
00:11:59.540 --> 00:12:02.650
we're about to generate some random data,
right?
And,
uh,

205
00:12:03.160 --> 00:12:08.160
we want to make sure that every time we run this code,

206
00:12:08.261 --> 00:12:11.020
it generates the same random numbers every time.
Why?

207
00:12:11.021 --> 00:12:14.500
Because we want to test the code,
right?
And so that's what seeding does.
Seating,

208
00:12:14.650 --> 00:12:17.710
make sure that we generate the same random numbers every time we brought our

209
00:12:17.711 --> 00:12:22.510
code.
Okay.
So that's it.
So let's see this shit.

210
00:12:23.470 --> 00:12:26.800
Okay.
So there we go.
You know what?

211
00:12:27.370 --> 00:12:30.850
I'd be a little more descriptive.
Non-Deterministic.
Uh,

212
00:12:30.910 --> 00:12:35.350
see that's the technical term for this in computer science,
right?

213
00:12:35.380 --> 00:12:40.010
And I did a video on,
um,
I mentioned non-determinism and my paper sent the video.

214
00:12:40.240 --> 00:12:44.980
Okay.
So,
um,
so we have that,
can someone say how many people are in the room?
It's,

215
00:12:44.981 --> 00:12:49.690
it's kind of a,
it's,
it's blocked.
Okay.
So we've got the sigmoid.

216
00:12:49.870 --> 00:12:54.130
So,
so now we've seen it our numbers and now we're going to define our activation

217
00:12:54.131 --> 00:12:57.460
function.
The,
uh,
traditional active,
thank you.
Good vibes.

218
00:12:57.610 --> 00:12:59.980
The traditional activation function that we're,

219
00:13:00.010 --> 00:13:04.090
that we use is the sigmoid function.
So let's write that out as a function,
right?

220
00:13:04.330 --> 00:13:08.890
And so sick and white is the function that's running every a neuron in our

221
00:13:08.891 --> 00:13:10.930
network.
So what does sigmoid look like?

222
00:13:10.931 --> 00:13:15.931
So let me just write up this formula and my video should be watching order.

223
00:13:16.300 --> 00:13:21.070
Absolutely.
Uh,
honestly,

224
00:13:21.071 --> 00:13:24.190
if you were to,
if you were to watch all of my videos from like the very,

225
00:13:24.191 --> 00:13:28.330
very start,
like the,
what is bitcoin video you would learn?

226
00:13:28.360 --> 00:13:32.500
So I literally put everything I've learned into these videos.

227
00:13:32.501 --> 00:13:37.010
Like literally everything I've ever,
I am into those videos.
Okay.
So,

228
00:13:37.030 --> 00:13:38.140
so what does a sigmoid function?

229
00:13:38.141 --> 00:13:41.260
The sigmoid function turns numbers until probabilities.
Okay.

230
00:13:41.261 --> 00:13:44.170
They turn numbers into probabilities.
What does that mean?
Well,

231
00:13:44.171 --> 00:13:46.570
in our neural network,
when we have our input data,

232
00:13:46.750 --> 00:13:50.770
it goes through the network and each of the weights is they set a probabilities.

233
00:13:50.830 --> 00:13:51.760
Like which way should it go?

234
00:13:51.761 --> 00:13:56.710
Point by 0.7% this way or 0.6% this way or 0.3% this way.
Output.

235
00:13:56.830 --> 00:13:59.300
Okay.
And so probabilities and they,

236
00:13:59.380 --> 00:14:01.950
these probabilities are updated when we train our network.
So,

237
00:14:02.200 --> 00:14:03.910
and so it gets better and better over time.

238
00:14:04.120 --> 00:14:07.120
So that's what the sigmoid function does every time it hits,
every time.

239
00:14:07.150 --> 00:14:09.160
Our input data kits,
um,

240
00:14:10.390 --> 00:14:15.200
one of those neurons are one of those layers.
Uh,
it's going to,
uh,

241
00:14:15.250 --> 00:14:19.210
turn that number into a probability.
That's all it does.
Okay,
so,

242
00:14:19.240 --> 00:14:22.150
and so traditionally we would just use one activation function,

243
00:14:22.151 --> 00:14:24.040
we call the sigma the activation function,

244
00:14:24.430 --> 00:14:29.230
but this time we're going to use two activation functions.
Why?
Because,

245
00:14:29.890 --> 00:14:31.090
uh,
uh,

246
00:14:31.180 --> 00:14:35.960
it's four x source specifically.
Uh,
the,

247
00:14:35.961 --> 00:14:40.960
the,
the tangent,
the tangent function,
the tangent function is,
is,
is helpful,

248
00:14:41.120 --> 00:14:42.770
uh,
because uh,
it,

249
00:14:42.940 --> 00:14:46.240
it just makes the loss even better than if we were to use just a plain old

250
00:14:46.241 --> 00:14:49.330
sigmoid.
Uh,
and I'm not going to go do like super simple,

251
00:14:49.390 --> 00:14:52.700
so I could just do the white,
but I want to be accurate,
right?

252
00:14:52.850 --> 00:14:56.360
I'm going to make the most accurate and neural network that I can from scratch.

253
00:14:56.870 --> 00:15:01.490
Okay?
So I'm going to use two activation functions.

254
00:15:01.730 --> 00:15:04.030
One is going to be for our,
uh,

255
00:15:04.820 --> 00:15:07.490
first layer and the other is going to be for a second leg.
Okay?

256
00:15:07.550 --> 00:15:09.340
So let me just write that down.
And so this,

257
00:15:09.450 --> 00:15:12.170
whatever it's returning is the actual function itself.

258
00:15:12.410 --> 00:15:15.930
It's the programmatic function of the map function of the,

259
00:15:16.070 --> 00:15:20.300
of the other tangent prime function.
Okay.
So those are two activation functions.

260
00:15:20.690 --> 00:15:24.140
Um,
right.
Okay,
so that's it.

261
00:15:24.170 --> 00:15:28.370
Those are our two activation functions.
So now let's write our training function.

262
00:15:28.400 --> 00:15:31.790
Okay.
So I'm gonna,
I'm gonna,
I'm gonna hit enter.
I'm going to like move up.

263
00:15:31.791 --> 00:15:36.320
So just if you take a screenshot,
okay.
Three,
two,
one.

264
00:15:36.590 --> 00:15:37.161
Here we go.
Okay.

265
00:15:37.161 --> 00:15:39.980
So I'm going to go down to [inaudible] and now I'm going to write my craning

266
00:15:39.981 --> 00:15:41.900
functions like that train.

267
00:15:42.170 --> 00:15:46.450
And so the training function is going to write a awesome,

268
00:15:46.490 --> 00:15:48.890
it doesn't make a different,
pick the one or 1.0 you're,

269
00:15:48.891 --> 00:15:50.000
you're absolutely correct.

270
00:15:50.390 --> 00:15:53.210
So the training function is going to take five creditors.

271
00:15:53.211 --> 00:15:56.780
I'm going to write them and them and explain what these parameters are.
Okay?

272
00:15:57.370 --> 00:16:02.350
Okay.
Bv B.
[inaudible]

273
00:16:04.850 --> 00:16:08.450
okay.
Okay.
So what are we doing here?

274
00:16:08.480 --> 00:16:12.500
So what does the x,
the x is the input data.

275
00:16:12.890 --> 00:16:16.280
The V is our transpose,
which is going to help us perform matrix,

276
00:16:16.281 --> 00:16:20.180
help a multiplication.
The,
the uh,

277
00:16:20.240 --> 00:16:24.860
the uh,
sorry,
let me,
I missed something.
Let me start over.

278
00:16:25.010 --> 00:16:28.280
The X is our input data.
The t is our transpose,

279
00:16:28.281 --> 00:16:30.920
which is going to help us perform a matrix multiplication.

280
00:16:31.490 --> 00:16:33.620
The V and Wrr,
uh,

281
00:16:33.710 --> 00:16:38.570
layers to our network are two layers and BV and BW.
Our biases,

282
00:16:38.720 --> 00:16:42.830
our biases are going to help us make a more accurate prediction.
Okay.

283
00:16:42.920 --> 00:16:46.880
And they're going to be one bias for each of the layers and are in our network.

284
00:16:46.910 --> 00:16:50.500
Okay?
So that's,
those are our inputs.
So let me just write that,
uh,

285
00:16:50.630 --> 00:16:52.550
input data transpose

286
00:16:54.950 --> 00:16:58.520
a layer one,
layer two,

287
00:16:59.360 --> 00:17:03.200
and then by,
okay,
so let's do this.
Do this.

288
00:17:03.260 --> 00:17:07.490
So the first one is going to be,
uh,
forward,
uh,
forward propagation.

289
00:17:07.491 --> 00:17:11.630
So we're going to do matrix multiplication

290
00:17:14.000 --> 00:17:18.830
makers multiply,
uh,
plus our biases.
Okay?
So let's do this.

291
00:17:18.860 --> 00:17:20.320
Okay.
So we're going to have some,

292
00:17:20.570 --> 00:17:23.060
let me just write this and I'm gonna explain it right after,
right this,
okay?

293
00:17:23.240 --> 00:17:27.620
So NP dot.com and do matrix multiplication.
Do the dot product.

294
00:17:28.190 --> 00:17:33.080
And then let me write one more above up and then do the same.

295
00:17:33.081 --> 00:17:36.980
But this time you start tangent function a.
Okay?

296
00:17:38.000 --> 00:17:41.990
So let me explain this.
Okay,
here we go.
I am going to randomly shuffled the data.

297
00:17:42.350 --> 00:17:43.370
Uh,
and

298
00:17:46.880 --> 00:17:51.540
so this is going to be a simple perceptron yes,
top line percent.
Exactly.

299
00:17:52.030 --> 00:17:54.270
You can call it a perceptron.
You can call it a neural,

300
00:17:54.380 --> 00:17:56.880
a feed forward neural network.
Uh,

301
00:17:56.881 --> 00:18:00.720
it's a clever rebranding that they call it all deep learning.
But you know,

302
00:18:00.721 --> 00:18:02.160
this stuff has been around since the 50s.

303
00:18:02.161 --> 00:18:07.080
We just now have this amazing data and compute and that's why it's awesome.
Um,

304
00:18:07.710 --> 00:18:11.430
okay,
so here's what,
here's what we're doing.
This is forward propagation.
Okay.

305
00:18:11.490 --> 00:18:16.320
We're taking the dot product of the input data x and we're putting it into our

306
00:18:16.321 --> 00:18:18.430
first layer fee.
That doc product is,

307
00:18:18.431 --> 00:18:22.410
is computing that matrix multiplication and it's adding the bias in to get that

308
00:18:22.440 --> 00:18:24.360
a,
that a delta value.

309
00:18:24.750 --> 00:18:28.530
Then we're going to perform that first activation function.
Uh,
I,

310
00:18:28.570 --> 00:18:31.440
that's the first time I've been called the Beyonce up neural networks.
Thank you.

311
00:18:31.800 --> 00:18:36.590
It was pretty good actually.
Uh,
okay.
So then we're going,

312
00:18:37.200 --> 00:18:38.033
we're going to do the T,

313
00:18:38.070 --> 00:18:41.310
we're going to perform our activation function on that data.
Okay.

314
00:18:41.610 --> 00:18:46.320
So that's our first um,
operation.
The next one is going to be,

315
00:18:46.370 --> 00:18:50.700
um,
now we're going to have to do our sigmoid function.
Okay.
So,

316
00:18:50.720 --> 00:18:54.030
so we already applied our first activation function analysis by our next one,

317
00:18:54.240 --> 00:18:56.160
which is going to be the,
uh,

318
00:18:56.190 --> 00:18:59.130
we're going to take the value that we just had that we just createdZ ,

319
00:18:59.400 --> 00:19:02.690
that delta.
And then we're going to add that the next layer,
right?

320
00:19:02.700 --> 00:19:05.280
The second layer w from,
we're going to perform the dot product.

321
00:19:05.281 --> 00:19:08.370
So we've computed the first part and then we're going to take that results and

322
00:19:08.371 --> 00:19:11.070
we're going to compute that next layer using the result was in the dot product.

323
00:19:11.580 --> 00:19:15.300
Okay?
So,
oh man.
Okay.
So,
um,

324
00:19:15.440 --> 00:19:17.940
it's now we're going to add our next biases,
right?
The bias for that.

325
00:19:18.050 --> 00:19:19.260
We just need to do this twice,
right?

326
00:19:19.290 --> 00:19:22.990
But we did it once and now we're going to do it then this next time.
So,
uh,

327
00:19:22.991 --> 00:19:25.080
and we're going to calculate that sigmoid,

328
00:19:25.110 --> 00:19:27.990
you think that value that we just had and we're going to call it and why we're

329
00:19:27.991 --> 00:19:32.250
going to call it y.
Okay?
So that drug forward propagation,
so that goes full.

330
00:19:32.490 --> 00:19:37.200
And so what we,
so when we have a feed forward neural network,
it's not recurring,

331
00:19:37.260 --> 00:19:40.350
but we do have something called backwards propagation.
So,

332
00:19:40.440 --> 00:19:43.180
so it goes forward and backward.
It doesn't go loot,

333
00:19:43.230 --> 00:19:46.040
it just goes forward and backwards.
So what does that mean?
We update,

334
00:19:46.080 --> 00:19:49.800
we update our weights one way and then we update them backwards and we just keep

335
00:19:49.801 --> 00:19:54.630
doing that while we're training.
Okay.
So let's do backward propagation backward.

336
00:19:55.030 --> 00:19:59.280
So say those,
say backwards.
Okay.
Um,

337
00:20:00.540 --> 00:20:03.870
so let me just write this and then I'm going to explain it.

338
00:20:03.871 --> 00:20:05.850
It's going to be two lines.
Okay.

339
00:20:08.930 --> 00:20:11.280
Let me get some of that coffee.
And you know what I'm saying?
That stuff helps.

340
00:20:14.600 --> 00:20:18.810
All right.
Dot.
Products.

341
00:20:20.120 --> 00:20:24.690
Um,
eew.

342
00:20:24.970 --> 00:20:28.950
Okay.
Okay.
Um,
so yeah,

343
00:20:28.951 --> 00:20:33.780
these are our two deltas that we're getting
from,

344
00:20:33.840 --> 00:20:36.830
for our backward propagation,
and that's what we use.
Our transpose,

345
00:20:37.070 --> 00:20:41.580
our transports are transposed,
is basically our matrix of,
of weights flipped.

346
00:20:41.880 --> 00:20:43.710
And we flip it because we're going backwards,
right?

347
00:20:43.820 --> 00:20:47.680
We would make the matrix itself backwards,
uh,
values,
and then we,

348
00:20:47.740 --> 00:20:50.380
and then we use that to calculate our backward propagation.

349
00:20:50.590 --> 00:20:54.970
Ultimately we want this he value.
Okay,
and what are we going to use this,
this,

350
00:20:55.270 --> 00:20:58.150
this value for what?
We're going to use it to predict our loss.

351
00:20:58.330 --> 00:21:01.720
And we're going to compare our predicted loss function from our actual loss

352
00:21:01.721 --> 00:21:04.840
function.
And we're going to minimize our loss doing that.
Okay?

353
00:21:04.870 --> 00:21:08.230
So that's the goal.
We want to minimize our loss.
That's how we train.

354
00:21:08.440 --> 00:21:10.610
We want to minimize our loss.
Hi.

355
00:21:10.980 --> 00:21:15.970
Ton of guilt is from Greece.
That's an awesome name.
Okay.
I Love Greens.

356
00:21:16.240 --> 00:21:21.060
Okay,
here.
Here we go.
So let's predict our lungs predict our luxe.

357
00:21:22.300 --> 00:21:25.480
Okay.
So,
so to predict our loss,

358
00:21:25.481 --> 00:21:29.950
we're going to calculate the APP we're going to take,
I mean,
let me,
let me just,

359
00:21:30.490 --> 00:21:33.570
um,
I'm just trying to sell

360
00:21:36.280 --> 00:21:38.830
and cause someone shout out how many people we'd have in the room right now.

361
00:21:39.490 --> 00:21:41.680
Evie,
we want to predict our loss.
Okay?

362
00:21:41.980 --> 00:21:44.470
So what we're going to do is predict our laws.

363
00:21:44.500 --> 00:21:48.010
And these are two deltas and we're using both of these that we're using the Z

364
00:21:48.011 --> 00:21:51.550
value,
okay?
That we predicted from up here at this forum.
Thank you.

365
00:21:51.850 --> 00:21:55.150
From this forward step.
And then we're going to use the,
the x value,

366
00:21:55.151 --> 00:21:59.780
which is our input.
Okay.
MMM.

367
00:22:00.870 --> 00:22:03.330
And so there's that.

368
00:22:04.760 --> 00:22:09.390
And so now we're going to calculate our loss.
Okay,
thanks guys.

369
00:22:09.660 --> 00:22:11.190
So now we're going to calculate our loss.

370
00:22:11.430 --> 00:22:16.200
So this is going to be quite a long life.
Let me just write this out.

371
00:22:16.260 --> 00:22:18.870
And you know,
in tensorflow,
in a bunk of libraries,

372
00:22:18.871 --> 00:22:21.270
a lot function is just a line of code.
You know,

373
00:22:21.271 --> 00:22:24.390
like you've just a loss parameter,
but we're doing this from scratch.

374
00:22:24.420 --> 00:22:28.920
So we're going to actually use the math here.
So let's go ahead and say,
um,

375
00:22:29.800 --> 00:22:31.920
and,
and I have something to say about this as well.

376
00:22:31.921 --> 00:22:35.250
Let me about using math and machine learning and if you should learn it or not,

377
00:22:36.390 --> 00:22:38.520
and what the poll.
Okay,
so let me just do

378
00:22:40.740 --> 00:22:45.180
one minus y.
Okay.
Let me make sure that this is correct.

379
00:22:45.181 --> 00:22:49.650
Loss is the negative of the mean of the transpose.

380
00:22:49.710 --> 00:22:54.480
We take our log function of y and we add one minus of transposed.

381
00:22:54.570 --> 00:22:59.160
Okay?
So this is,
this is,
this is called Cross entropy.
This is a,
this is a,

382
00:23:01.410 --> 00:23:02.243
<v 2>mmm.</v>

383
00:23:04.940 --> 00:23:08.810
<v 1>Mikael,
I would,
I would do a meetup for sure.
Yeah,
I should,
I should,</v>

384
00:23:08.840 --> 00:23:11.120
I should do that.
I actually,
I had a talk yesterday.

385
00:23:11.300 --> 00:23:15.530
One of my fans ask you to have a talk at SF state.
And I did that.
Um,

386
00:23:15.710 --> 00:23:18.350
but there weren't that many people.
I mean there were like 15,
but like,
dude,

387
00:23:18.351 --> 00:23:21.510
there's like 168 people here,
you know what I'm saying?
Live is awesome.
Uh,

388
00:23:21.740 --> 00:23:25.080
but yeah,
if there were enough people,
I would totally do a meetup.
Yeah.
Um,

389
00:23:25.700 --> 00:23:28.460
I'll look into organizing that or if you would look into that,

390
00:23:28.461 --> 00:23:31.610
that would be super helpful as well.
Okay,
so here we go.
So this is cross entropy.

391
00:23:32.480 --> 00:23:35.090
Okay.
So why Cross entropy?
Well,
there's a,

392
00:23:35.150 --> 00:23:38.440
there's a bunch of different loss functions.
There's the mean squared error.
Uh,

393
00:23:38.610 --> 00:23:39.840
there's a bunch of different loss assumptions,

394
00:23:39.910 --> 00:23:44.750
but we're using cross entropy because we're doing classification generally.

395
00:23:44.780 --> 00:23:46.980
If doing any kind of classification tasks,

396
00:23:47.210 --> 00:23:49.700
you want to use the cross entropy function.
Why?

397
00:23:49.730 --> 00:23:52.220
It just tends to give us a better results.
Okay.

398
00:23:52.250 --> 00:23:54.470
And then one more thing about using math and machine learning.

399
00:23:54.860 --> 00:23:57.560
This is a long function and it's kind of difficult to understand.

400
00:23:58.220 --> 00:24:02.570
Do you need to know the math to use machine learning to get good at machine

401
00:24:02.571 --> 00:24:06.380
learning?
And my answer is no,
but it helps.
No you don't.

402
00:24:06.381 --> 00:24:10.130
But it helps because if you ask them of the best javascript programmers in the

403
00:24:10.131 --> 00:24:13.280
world,
right?
If you ask some of the best front end designers in the world,

404
00:24:13.640 --> 00:24:16.250
they make the websites like Airbnb,
like stripe,

405
00:24:16.460 --> 00:24:18.860
like you know the best designers in the world.

406
00:24:19.280 --> 00:24:23.590
How like if you ask them to make a rectangle,
they'll just do like,
you know,
um,

407
00:24:24.150 --> 00:24:27.920
rex,
you know,
make rex or some function.
But if you ask them like the details,

408
00:24:28.160 --> 00:24:31.400
the mathematical details of the pixel values and you know,

409
00:24:31.460 --> 00:24:34.790
all the math that goes into making a rectangle,
they won't know.

410
00:24:35.300 --> 00:24:37.190
All of this is a series of abstractions.

411
00:24:37.250 --> 00:24:39.350
It's just a series and series of abstractions.

412
00:24:39.380 --> 00:24:41.520
Eventually we won't even need to know.

413
00:24:41.830 --> 00:24:45.500
We won't even need to know what cross entropy or,
um,

414
00:24:46.040 --> 00:24:49.220
any of these functions are.
Eventually we won't even need to know programming.

415
00:24:49.221 --> 00:24:51.920
We'll just tell him what we wanted to do.
Right?
So it's just,

416
00:24:51.950 --> 00:24:56.060
it's just a series of abstractions.
We're all building on top of each other.
Um,

417
00:24:56.120 --> 00:25:00.230
and right.
So I,
I don't think it's necessary to know the math to do this,

418
00:25:00.231 --> 00:25:04.010
but it helps.
Okay.
That's,
that's it.
Exactly.
Don't reinvent the wheel.
That's it.

419
00:25:04.480 --> 00:25:08.260
Um,
okay,
so here we go.

420
00:25:08.261 --> 00:25:10.990
So that's a cross entropy rights.
We calculated all that.
So let's,

421
00:25:10.991 --> 00:25:15.160
let's do our last line of business going to return and return our loss function.

422
00:25:15.220 --> 00:25:15.910
Okay.

423
00:25:15.910 --> 00:25:19.630
So we're going to return a lot of function and we're going to return our Yelp to

424
00:25:19.631 --> 00:25:23.520
values and our,
um,
our errors,
right?

425
00:25:23.560 --> 00:25:28.270
They are errors in our deltas.
Okay.
So,
okay,
so that's it.
That's it.
Okay.

426
00:25:28.271 --> 00:25:31.120
So that different training step,
I'm going to screenshot that.

427
00:25:31.121 --> 00:25:36.040
I'm going to go down a little bit more.
Three,
two,
one.
Oh,
okay.
Up,
up,
up,
up,
up,
up,

428
00:25:36.041 --> 00:25:40.810
up.
Up.
All right,
so here we go.
But now let's write a prediction.

429
00:25:40.840 --> 00:25:43.030
Do we have one more fun to drive and then we can get,

430
00:25:43.031 --> 00:25:47.200
go ahead and get started with our actual,
the,
the meat of our code.

431
00:25:47.380 --> 00:25:50.110
So we have one more fun to drive and it's going to be the prediction function.

432
00:25:50.350 --> 00:25:54.670
This is going to predict our value.
Okay,
so let me write down the variables.

433
00:25:55.010 --> 00:25:56.800
Bv and then B,
w.

434
00:25:57.230 --> 00:26:02.020
B.
W.
Okay.
So,

435
00:26:02.410 --> 00:26:07.060
uh,
I Cooley smiley.
Wow.
We got some,
uh,

436
00:26:07.210 --> 00:26:12.160
non ml people in your,
okay,
here we go.
Uh,
everybody say hi to Cooley's smiling.

437
00:26:12.161 --> 00:26:16.390
She was a youtuber that I met in la.
Okay,
so here we go.

438
00:26:16.450 --> 00:26:19.510
So we're going to do a prediction step.
Okay?
So what does that mean?

439
00:26:19.511 --> 00:26:23.620
That means we are going to perform matrix multiplication to,
uh,

440
00:26:23.740 --> 00:26:28.720
predict our value,
our end result.
And to do that,
we're going to use these,
these,

441
00:26:28.810 --> 00:26:33.250
uh,
uh,
variables that we've already calculated.
And p.
Dot.
Dot.

442
00:26:34.920 --> 00:26:39.550
Um,
oh my God,
I need to just say this.

443
00:26:39.551 --> 00:26:44.310
I'm a Ds said when I'm on a date,
I don't need a neural network.
Predict my loss.

444
00:26:45.810 --> 00:26:50.100
That is gold.
That is gold.
That is gold.
Okay.
So,

445
00:26:50.620 --> 00:26:54.910
um,
let's,
let me write this.
Just going to be two lines.

446
00:26:55.160 --> 00:26:58.020
Be Two lines.
All right?
So

447
00:27:00.450 --> 00:27:03.330
let me write that out.
And then I'm going to,

448
00:27:03.410 --> 00:27:07.230
I'm going to explain it like what the heck is happening there?

449
00:27:08.080 --> 00:27:08.913
MMM.

450
00:27:11.760 --> 00:27:16.290
And then we're going to return it and we're going to start sigmoid function too.

451
00:27:16.620 --> 00:27:21.180
Um,
uh,

452
00:27:22.790 --> 00:27:27.580
so it's got to be very then 0.5 adds type.

453
00:27:28.600 --> 00:27:32.420
Okay,
so here we go.
Big Void.

454
00:27:35.030 --> 00:27:39.660
So A and B are both of the final value that we're calculating using those

455
00:27:39.661 --> 00:27:43.770
variables that we've already used.
Internet.
Okay.
And we're using the,

456
00:27:43.830 --> 00:27:46.830
and we're going to return whatever we returned is going to be our prediction.

457
00:27:46.831 --> 00:27:49.650
And what does our prediction prediction going to be?
It's going to be a number,

458
00:27:49.651 --> 00:27:51.630
it's going to be a one or a zero.
Okay.

459
00:27:51.750 --> 00:27:56.620
And we're saying only if the value is greater than 0.5.
Do we return,
uh,

460
00:27:57.130 --> 00:27:58.460
uh,
sorry.

461
00:27:59.220 --> 00:28:02.160
If our value is greater than 0.5 it's going to be a one else is going to be a

462
00:28:02.161 --> 00:28:05.370
zero.
And 0.5 is just a prediction.
It's a probability.

463
00:28:07.160 --> 00:28:12.040
All right.
Jay Haas.
All right,
good point.
Good point.
Okay,
so here we go.

464
00:28:12.190 --> 00:28:16.530
Um,
oh,
thank you ass cut.
All right,

465
00:28:16.531 --> 00:28:18.800
so those are our functions.
Let's go.
Let's go ahead and get started.

466
00:28:18.801 --> 00:28:20.180
So we're going to set up our initial parameters.

467
00:28:20.181 --> 00:28:22.790
So we're going to create our layers.
So let me create a lakes.

468
00:28:23.090 --> 00:28:26.710
Now it's time to create our legs.
Okay,
so let me go ahead and do this.
Um,

469
00:28:29.230 --> 00:28:33.950
okay,
so here we go.
So the equals our first layer,
it's going to be

470
00:28:35.960 --> 00:28:36.531
random.

471
00:28:36.531 --> 00:28:41.531
That more malt scale equals one side's the four

472
00:28:43.840 --> 00:28:44.673
number pin.

473
00:28:48.320 --> 00:28:50.510
So that our first layer,
and now let's make our next,

474
00:28:50.511 --> 00:28:54.500
I remember this is a two layer neural network.
Okay.
Two layers.

475
00:28:54.740 --> 00:28:58.730
And we're going to create them both the same way.
Okay?

476
00:29:00.140 --> 00:29:04.940
And I'm going to explain why we're using these variables has our size right

477
00:29:04.941 --> 00:29:08.930
after I finished talking this out.
Okay.
There we go.
Two layers,

478
00:29:08.960 --> 00:29:11.120
the NW and we're going to,

479
00:29:11.121 --> 00:29:15.680
so the size is basically those net number of the number of employers,

480
00:29:15.681 --> 00:29:19.940
which just number of input values,
which is 10 and the number of hidden layers,

481
00:29:19.941 --> 00:29:24.590
which is 10 as well.
Okay?
So,
so 10 and then 10 hours.
Okay.
Every time.

482
00:29:25.160 --> 00:29:28.430
Whereas a stripe and your hair gets their seat.
I need to

483
00:29:30.020 --> 00:29:34.490
make it more great.
Okay.
It's there for sure though.
Definitely check that out.

484
00:29:34.820 --> 00:29:39.660
Okay,
here we go.
So
here we go.

485
00:29:42.250 --> 00:29:44.030
Here we go.
Um,

486
00:29:46.320 --> 00:29:49.500
we're going to say,

487
00:29:51.270 --> 00:29:55.970
we're going to say
we're going to create our biases.
Okay.
We,

488
00:29:55.971 --> 00:29:57.930
we've used our biases in the parameters,

489
00:29:57.931 --> 00:30:01.620
but we haven't actually initialize our biases.
So let's use our biases.
Okay.

490
00:30:01.650 --> 00:30:04.040
So we're going to say our first bikes is going to,
uh,

491
00:30:05.130 --> 00:30:09.330
use our hidden variable as a parameter.
And then we're going to use our

492
00:30:11.430 --> 00:30:14.550
next bias,
which is going to be,
um,

493
00:30:18.590 --> 00:30:21.470
our next value is going to feed the number of Alpha lenders.
Okay?

494
00:30:23.420 --> 00:30:28.400
Okay.
So those are our biases.
Okay.
And now we're going to generate our data.

495
00:30:28.401 --> 00:30:30.800
We've created our layers,
we can have our biases,

496
00:30:30.920 --> 00:30:35.420
and now we're going to create a variable called parameters,

497
00:30:35.421 --> 00:30:38.210
which are what you input as a parameter.
It,
it's going to be an array.

498
00:30:38.390 --> 00:30:40.820
It's going to be an array of all of our values that we've just created,

499
00:30:41.000 --> 00:30:42.460
our two layers and our biases.

500
00:30:42.570 --> 00:30:45.680
It's just gonna be easier to input it into our training function whenever we

501
00:30:45.681 --> 00:30:49.730
call it.
Um,
my hair is totally open source.

502
00:30:49.760 --> 00:30:53.000
If you guys want to have a silver stripe in your hair,
go for it.
But just like,

503
00:30:53.001 --> 00:30:55.580
you know,
you can credit me if you want to.
I don't,
I don't care.

504
00:30:55.581 --> 00:30:59.390
Everything about me is open towards take my ideas,
take my everything.
Just it's,

505
00:30:59.391 --> 00:31:04.130
I'm open source,
you know what I'm saying?
So parameters and,
oh man,

506
00:31:04.131 --> 00:31:07.100
it's really hard to not reply to comments cause you guys are so awesome.

507
00:31:07.101 --> 00:31:11.210
I don't want to not talk to you.
Right?
Um,
so,
all right,

508
00:31:11.211 --> 00:31:15.110
so we've created our parameters.
Let's generate our data.
Okay.

509
00:31:15.140 --> 00:31:19.750
X equals NP got random.
Dot.
Binomial.

510
00:31:21.230 --> 00:31:26.120
Um,
all right,
so we're going to generate our data.

511
00:31:26.121 --> 00:31:30.520
And so this is why this is where,
uh,
that's that,

512
00:31:30.550 --> 00:31:34.040
that sample of 300 variables,
this is where it's gonna come into play.

513
00:31:34.070 --> 00:31:36.800
This is why we initialize it so that we can generate data.

514
00:31:37.010 --> 00:31:41.180
We're going to generate 300 samples.
Um,
and what,

515
00:31:41.210 --> 00:31:45.360
I'm going to use a learning rate and a second.
And,

516
00:31:47.510 --> 00:31:48.920
and so that,

517
00:31:49.310 --> 00:31:53.360
so we generated our data and so now we're going to calculate our trans folks,

518
00:31:53.630 --> 00:31:58.100
right?
Guess what?
Guess what guys?
Guess what it is time.

519
00:31:58.490 --> 00:32:02.120
It is time to train this ish.
Okay.

520
00:32:02.300 --> 00:32:05.510
It is what I call training times.
It is training time.

521
00:32:05.870 --> 00:32:08.870
This is going to be awesome.
This is our last step is a four loop.

522
00:32:08.871 --> 00:32:12.970
We're going to train.
Okay.
All right.

523
00:32:12.971 --> 00:32:15.310
And I'm going to get better at the video quality and I'm going to get better and

524
00:32:15.311 --> 00:32:18.610
everything.
So just everyone relaxed.
So here we go.
So training time,

525
00:32:18.670 --> 00:32:22.060
we're going to train this for a hundred peacocks.
Okay?

526
00:32:22.090 --> 00:32:25.750
So for epoch with everything pot in the range of a hundred.

527
00:32:25.780 --> 00:32:30.190
So we're gonna do this for 100 bucks.
Okay?
Everybody say hi to each other.

528
00:32:30.191 --> 00:32:31.360
Everybody's cool.
Okay.

529
00:32:31.390 --> 00:32:36.390
So we're going to start off with by initializing are our NP error array,

530
00:32:36.680 --> 00:32:39.410
uh,
which we're going to up.
We're going to add data to in a second.

531
00:32:39.770 --> 00:32:42.950
And then we're going to say our update variable is going to be

532
00:32:45.070 --> 00:32:49.220
the length of our parameters that we just initialize up there.

533
00:32:49.670 --> 00:32:50.503
And

534
00:32:52.770 --> 00:32:57.720
we're going to also initialize our time because we're going to time how long we

535
00:32:57.721 --> 00:33:01.520
want to run this neural net.
Okay.
We're going to time it.

536
00:33:01.521 --> 00:33:03.270
And so that's where that second variable,

537
00:33:03.300 --> 00:33:07.940
that second dependency time comes into play.
Just for this.
Okay,
let's do this.

538
00:33:07.950 --> 00:33:08.640
Okay.

539
00:33:08.640 --> 00:33:13.290
For each data point we want to each data point,

540
00:33:13.291 --> 00:33:17.680
we're going to,
uh,
update our weeks of our network.

541
00:33:17.740 --> 00:33:22.670
Okay.
Um,
we're definitely still fighting.

542
00:33:22.740 --> 00:33:25.130
Um,
we're definitely stuff like,
so

543
00:33:26.290 --> 00:33:30.500
the notion that work is to find the extra value of ones and Zeros.

544
00:33:34.300 --> 00:33:38.530
Okay.
So for each data point we want to calculate,
we want to update our law,

545
00:33:38.531 --> 00:33:40.390
our weight,
our waste.
Okay.

546
00:33:40.391 --> 00:33:45.391
So we're going to say for every value in the range of our input data.

547
00:33:45.970 --> 00:33:49.090
Okay.
So,
and that's,
and how do we say how big our input data is?

548
00:33:49.120 --> 00:33:51.400
We use the shape function of X.
Okay.

549
00:33:51.430 --> 00:33:54.670
So we're going to say we're going to take our loss function.

550
00:33:54.730 --> 00:33:58.480
We're going to talk to like our last and our gradients by doing what?

551
00:33:58.600 --> 00:34:03.600
By using the training function that we just calculated right now.

552
00:34:05.300 --> 00:34:09.610
We just,
uh,
sorry,
initialize and declared and then create.

553
00:34:09.940 --> 00:34:12.820
And that for our printers comes into play.
Just for simplicity's sake,

554
00:34:12.821 --> 00:34:17.260
we're just using grant instead of saying Xb,
BP,
BW,
just say prince.

555
00:34:17.410 --> 00:34:19.690
Great thing about python.
Okay?
So that,

556
00:34:19.700 --> 00:34:23.230
that's us calculating our last ingredients.
Now we want to update our loss.

557
00:34:23.500 --> 00:34:27.580
How do we do that?
Well,
we're going to have two more little loops here.

558
00:34:28.450 --> 00:34:28.601
Okay.

559
00:34:28.601 --> 00:34:33.550
So we're going to say four J in range up the length of the parameters.

560
00:34:36.770 --> 00:34:40.210
Um,
prams,

561
00:34:40.660 --> 00:34:45.660
J minus equals update.

562
00:34:47.860 --> 00:34:51.340
Okay.
Okay.
And so I'm going to do one more,
one more,

563
00:34:51.341 --> 00:34:53.230
and then that's going to be hit.
That's,

564
00:34:53.231 --> 00:34:57.220
that's literally yet I'm going to explain what I'm doing and why I'm,

565
00:35:00.210 --> 00:35:05.140
why I'm doing it.
Okay.
So,
um,

566
00:35:06.010 --> 00:35:08.170
uh,
so okay.

567
00:35:08.171 --> 00:35:11.140
So this is where our learning rate comes into a place and comes into play.

568
00:35:11.141 --> 00:35:13.120
This is where our momentum comes into play.

569
00:35:13.360 --> 00:35:15.790
Both of those hyper parameters that we initialize,

570
00:35:15.880 --> 00:35:19.810
this is where we actually use them.
So learning rates,
times,
gradient,

571
00:35:20.850 --> 00:35:24.590
um,
plus momentum
time,

572
00:35:24.640 --> 00:35:27.190
the update value that we just calculate,

573
00:35:29.590 --> 00:35:32.950
those two four loops are going to help us calculate our loss.

574
00:35:34.000 --> 00:35:36.250
That's going to help us calculate our loss.
Okay.

575
00:35:36.310 --> 00:35:39.060
And then we want to our error with the loss.

576
00:35:40.620 --> 00:35:44.970
That is it.
That's it.
What am I doing here?

577
00:35:45.660 --> 00:35:49.230
Okay,
cool.
Say,
okay,
so that's it.

578
00:35:49.231 --> 00:35:53.520
And now we're going to print that.
That value is 35 minutes in.
Okay.

579
00:35:53.580 --> 00:35:55.830
How many people would we got to hear someone shout it out,
shout it out.

580
00:35:55.870 --> 00:35:58.740
It's now let's print out our results.
Okay,
we're going to print them out

581
00:36:01.160 --> 00:36:05.160
and so I'm going to print it out using present,
be using and so

582
00:36:06.970 --> 00:36:11.740
sorry,
present d and then,
okay,
cool.
Um,

583
00:36:12.190 --> 00:36:16.330
lots is going to be percent 0.8.

584
00:36:17.260 --> 00:36:19.840
We're going to print out,
our lots are going to clean up our time.

585
00:36:19.900 --> 00:36:22.720
Like how long has this thing been going on?
Um,

586
00:36:24.530 --> 00:36:29.530
and then we're going to say percent.

587
00:36:31.010 --> 00:36:35.000
Um,
and then we're going to take eat pock.

588
00:36:35.200 --> 00:36:37.580
We're going to print out our epoch,
we're going to print out our loss.

589
00:36:37.581 --> 00:36:39.560
We just going to be the mean value of our care.

590
00:36:40.010 --> 00:36:42.530
And then are at the time we're just gonna be timed out,

591
00:36:42.531 --> 00:36:47.531
clock and present minus the starting point.

592
00:36:47.750 --> 00:36:50.720
So someone check this and make sure that I have,

593
00:36:50.721 --> 00:36:55.721
don't have any errors in my syntax on this long epoch loss time point.

594
00:36:59.300 --> 00:37:01.790
Right.
Point Four.
Okay.

595
00:37:01.850 --> 00:37:06.350
F S I think that's good.
I think that's good.
Oh,
and so then,

596
00:37:06.530 --> 00:37:11.360
uh,
uh,
that looks a Typo at line 83.
[inaudible] 83.
What does my typo?

597
00:37:11.361 --> 00:37:15.980
My typo is a lens is link.
Thank you.

598
00:37:16.130 --> 00:37:16.963
Okay.

599
00:37:20.950 --> 00:37:23.050
So,
okay,
that's it for a training function.

600
00:37:23.080 --> 00:37:25.330
Let's go ahead and try to predict something.
Okay.
So as say,

601
00:37:25.331 --> 00:37:29.020
tried to predict something,
how do we do that?
Well,
if we say,
okay,

602
00:37:29.021 --> 00:37:33.070
let's say x is going to be our random,
uh,

603
00:37:33.330 --> 00:37:36.040
now you find a binomial value,

604
00:37:36.670 --> 00:37:41.050
a one between 1.5,
and how many of them we want 10 of them.

605
00:37:41.410 --> 00:37:43.030
Okay.
So we're going to try to predict something.

606
00:37:43.031 --> 00:37:47.860
We're going to try to predict the XR prediction.
Okay,

607
00:37:48.490 --> 00:37:52.030
go.
Okay,
well we got to print those,
that x value.
Like,

608
00:37:52.031 --> 00:37:56.170
what are we going to predict?
And then finally,
our prediction,
which we create,

609
00:37:56.171 --> 00:38:00.910
we use our predictive method for,
let's go ahead and do the demo.
This baby.
Okay.

610
00:38:01.270 --> 00:38:06.190
And our framers,
boom.
That's it.
Let's see what is going on here.
Let me,

611
00:38:06.280 --> 00:38:07.720
let me run this.

612
00:38:11.370 --> 00:38:15.380
All right.
Bye.
Can anyone see this?
I can see it.
It's okay.

613
00:38:15.381 --> 00:38:18.380
So let's go ahead and run this.
Okay.

614
00:38:18.381 --> 00:38:21.470
So I've got number of samples is not defined.

615
00:38:22.790 --> 00:38:25.370
Number of samples is not defined.
Okay.

616
00:38:25.400 --> 00:38:30.400
So online 67 number of samples is not defined.

617
00:38:32.030 --> 00:38:36.580
Oh,
did I not define that?
What I did?

618
00:38:36.640 --> 00:38:39.440
Oh,
you know what?
It's a,
it is a,

619
00:38:43.450 --> 00:38:48.280
Oh yes,
it is training.
It is.
Okay.
Okay.
Okay.

620
00:38:48.310 --> 00:38:51.790
Okay.
Okay.
Okay.
Yes,
yes.
Okay.

621
00:38:52.120 --> 00:38:53.680
So what just happened,

622
00:38:54.340 --> 00:38:58.420
what just happened is I only had a single error and I fixed it anyway.

623
00:38:58.480 --> 00:39:03.480
So this train for a hundred steps and you guys are so nice.

624
00:39:04.770 --> 00:39:09.180
You guys are awesome.
Uh,
so
I know,
I know.

625
00:39:09.181 --> 00:39:11.880
I've got to calm down.
I'm just like really excited that this worked.

626
00:39:12.060 --> 00:39:15.450
So what happened?
That first line right here.
Let me,
let me make this bigger,

627
00:39:16.320 --> 00:39:19.260
make it a lot bigger and make it a lot bigger.

628
00:39:19.860 --> 00:39:22.200
That first line is our input data,
right?

629
00:39:22.680 --> 00:39:25.080
And this next line is our x or values.

630
00:39:25.850 --> 00:39:30.540
It's those are our core values.
Okay?
And we basically said,

631
00:39:30.570 --> 00:39:31.620
here's our input data,

632
00:39:31.680 --> 00:39:35.550
here's some randomly generated ones and Zeros and then compute the x shore and

633
00:39:35.551 --> 00:39:37.500
then get better and better over time.
And as you can see,

634
00:39:37.501 --> 00:39:42.080
our loss function is getting better over time.
See if it's your last 0.14,

635
00:39:42.920 --> 00:39:46.800
one,
three and then we up here it's like 36 and so it gets better.
It's timing it,

636
00:39:46.860 --> 00:39:51.240
it's timing the epoch.
When the code works,
it's alive,
it's smart,

637
00:39:51.480 --> 00:39:53.790
it works.
That's our extra value.
Okay,

638
00:39:54.060 --> 00:39:57.540
so I'm going to end this with a five minute Q and a and I'm going to post the

639
00:39:57.541 --> 00:40:00.340
code on get hub and guys please share my videos.

640
00:40:00.341 --> 00:40:04.800
Like I'm trying so hard to hit 50 k subscribers by January 1st so that's the

641
00:40:04.801 --> 00:40:07.170
best thing you can do for me is hit subscribe,
Jake,

642
00:40:07.180 --> 00:40:09.690
your friends and hit subscribe.
I'm going to make so many videos.

643
00:40:09.870 --> 00:40:11.280
I'm like just getting started.
Okay,

644
00:40:11.370 --> 00:40:15.860
so five minute Q and a and then we're out of here.
Thank you.
Moisten in the box.

645
00:40:15.861 --> 00:40:18.830
Thank you.
See you at our Sean.
Um,
what am I trying to predict?
Trump.

646
00:40:18.831 --> 00:40:23.360
I'm trying to break the XR value.
All right.

647
00:40:24.450 --> 00:40:28.800
Can I use a neural network to predict black Friday sales?
Yes,
absolutely.
Um,

648
00:40:28.980 --> 00:40:32.520
just look at a history of past sales and trying to create a future sale.

649
00:40:32.580 --> 00:40:34.680
You want to put that into a spreadsheet either manually or,

650
00:40:34.740 --> 00:40:38.750
I'm sure you could find that somewhere.
Um,
thank you.
Johannes.
Uh,

651
00:40:38.760 --> 00:40:42.690
I have not blocked you a cross entropy in NLP.

652
00:40:43.500 --> 00:40:47.940
Exactly.
What's my favorite color?
A Sapphire blue.

653
00:40:48.150 --> 00:40:50.760
What's your name on gate hub?
Ll sorts.
Now health.

654
00:40:50.790 --> 00:40:54.930
I actually rapped about my name on get hub.
If you looked,
I have not blocked you.

655
00:40:55.700 --> 00:40:59.460
I had rapped about my name on get hub.
Just look at my channel trailer.
It's a,

656
00:41:00.360 --> 00:41:02.880
do you,
do you watch the flash?
I don't,
but I should.

657
00:41:03.240 --> 00:41:06.570
What would be your favorite company to work for?
You know,

658
00:41:07.980 --> 00:41:11.580
man,
I don't know if I could work for a company anymore.
Maybe.

659
00:41:11.581 --> 00:41:13.230
I mean the best one would be open AI,

660
00:41:13.410 --> 00:41:17.960
open AI because it's awesome that that would be the one.
Um,

661
00:41:19.170 --> 00:41:22.560
could you use a neural net went to classify and multidimensional time series a?

662
00:41:22.561 --> 00:41:26.280
Yeah,
no,
for sure.
Absolutely a time series of sequential data.
Right.

663
00:41:26.340 --> 00:41:28.950
So you'd want to use,
uh,
a recurrent neural network.

664
00:41:29.520 --> 00:41:32.790
Are CNN is good for prediction?
Yeah,
I mean for image,
uh,

665
00:41:33.350 --> 00:41:35.870
actually seasons had been used for text prediction as well,

666
00:41:35.871 --> 00:41:38.700
like what's going to be the next and then they'd been using NLP.
Well,

667
00:41:38.701 --> 00:41:42.050
you mind the macro pro 2016 with the touch bar?
Yes,
I have it.
I have it.

668
00:41:42.110 --> 00:41:43.940
I have it right here.
Well,
guess what?

669
00:41:44.000 --> 00:41:48.590
It's got USBC ports and my display doesn't use USBC.

670
00:41:48.860 --> 00:41:51.860
Uh,
there's a little touch bar,
so I had to use my old Mac book.

671
00:41:52.130 --> 00:41:56.900
So that sucks for apple.
Right?
I did say deep mine,

672
00:41:56.930 --> 00:42:00.710
but like,
I mean the two,
I mean,
they're both good.
They're both good,

673
00:42:00.711 --> 00:42:04.100
but I like open AI is openness and democratizing thing.

674
00:42:05.910 --> 00:42:10.340
Value.
How old am I?
I'm 25 years old.
Are you on,
are you an AI or a Syrah?

675
00:42:10.341 --> 00:42:13.430
Geology.
I am myself.
I am Saroj.

676
00:42:13.790 --> 00:42:18.770
I'm also Raj [inaudible].
Raj,
two more questions and we're out of here.

677
00:42:18.950 --> 00:42:22.660
How did you get into cs?
I couch surfed in London.

678
00:42:22.690 --> 00:42:26.830
I met a guy named Alex McCall.
She was the dude who was like,

679
00:42:26.860 --> 00:42:30.640
hello may I'm coding.
And I was like,
Yo,
this text editor you've got here,

680
00:42:30.641 --> 00:42:33.430
it's so pretty and colorful.
I want to do stuff like that.
So yeah,

681
00:42:33.431 --> 00:42:35.440
I'm really envious of you in San Francisco.

682
00:42:35.441 --> 00:42:38.800
You can make a lot of money and do great things.
And from then on I was like,

683
00:42:38.830 --> 00:42:42.160
I got to be a computer scientist.
I was eight,
I was 19 years old.

684
00:42:42.220 --> 00:42:45.250
I was an economics major at the time because all I cared about was money.

685
00:42:45.520 --> 00:42:48.670
And then I was like,
wait a second,
there's more to life than just money.

686
00:42:49.080 --> 00:42:52.390
All right.
So one more question.
Um,

687
00:42:55.210 --> 00:42:55.650
<v 2>okay.</v>

688
00:42:55.650 --> 00:42:56.820
<v 1>When are you</v>

689
00:42:58.400 --> 00:42:59.170
<v 2>okay,</v>

690
00:42:59.170 --> 00:43:04.030
<v 1>uh,
oh,
here's a good one.
Can we use neural networks for Agi?</v>

691
00:43:04.060 --> 00:43:05.080
That's a great question.

692
00:43:06.130 --> 00:43:06.530
<v 2>Okay.</v>

693
00:43:06.530 --> 00:43:10.760
<v 1>I don't,
who knows?
I mean,
yes,
I mean just the idea of a neural network.</v>

694
00:43:10.761 --> 00:43:15.710
Our brain is,
we know it's a neural network,
but like grading,

695
00:43:15.740 --> 00:43:19.250
does our brain use backpropagation does or does it?
Rain news gradient descent.

696
00:43:19.460 --> 00:43:22.640
It's our brain using recurrent networks.
Is it using convolutional?

697
00:43:22.820 --> 00:43:25.370
It's like all these different types of networks we don't know,

698
00:43:25.690 --> 00:43:28.880
but we're getting really close and the state of the art is being broken at an

699
00:43:28.970 --> 00:43:32.870
increasingly fast rate.
Um,
every day,
uh,

700
00:43:32.900 --> 00:43:36.020
deep learning right now is more physics was in the 19 hundreds.

701
00:43:36.410 --> 00:43:41.270
The Einstein's and Newton's and Marie curies and famous,
uh,
uh,

702
00:43:41.660 --> 00:43:45.820
scientists of computer sciences are,
are,
are,
are alive right now.
Oh,
there,

703
00:43:45.850 --> 00:43:48.920
either they're in middle school or elementary school or they're in the field.

704
00:43:49.010 --> 00:43:53.180
So it's a very,
very exciting time for your watching and awesome.

705
00:43:53.360 --> 00:43:57.080
So that's it for this tutorial.
Thanks everybody for watching.

706
00:43:57.170 --> 00:44:00.860
I'm going to have with my video come out very soon on Friday and then a very,

707
00:44:00.861 --> 00:44:01.700
very,
very,

708
00:44:01.701 --> 00:44:06.080
very special video that's going to come out on Monday and it's going to be

709
00:44:06.170 --> 00:44:09.830
awesome.
It's going to be awesome.
Okay.
It's going to be the video.

710
00:44:09.831 --> 00:44:12.800
I've been most excited to make ever.
All right,

711
00:44:12.801 --> 00:44:15.380
so thanks guys for calling out the number of people here.

712
00:44:15.440 --> 00:44:19.820
Thanks everybody for watching this video.
Thanks Brian.
Christina Aditya,
Andre,

713
00:44:19.821 --> 00:44:23.480
Mikael,
a cigar.
Sean,
Ian Elfish,

714
00:44:23.481 --> 00:44:27.920
Danny top 1% multi ship.
[inaudible] you guys are awesome.

715
00:44:27.980 --> 00:44:32.870
Thank you for watching.
Please share my videos.
I love you guys.
And for now,

716
00:44:32.930 --> 00:44:37.790
I've got to write so many video scripts.
Uh,
so thanks for watching.

