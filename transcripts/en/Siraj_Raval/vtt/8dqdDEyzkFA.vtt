WEBVTT

1
00:00:00.210 --> 00:00:03.930
Guys,
I think I found it classify tree of life

2
00:00:06.060 --> 00:00:06.840
hello world,

3
00:00:06.840 --> 00:00:11.490
it's Saroj and the most exciting class of machine learning techniques is called

4
00:00:11.550 --> 00:00:12.900
unsupervised learning.

5
00:00:13.200 --> 00:00:17.640
Teaching machines to learn for themselves without having to be explicitly told

6
00:00:17.641 --> 00:00:22.260
if everything they do is right or wrong is the key to true artificial

7
00:00:22.261 --> 00:00:26.910
intelligence and perhaps the most important research goal of 2019 I mean,

8
00:00:26.940 --> 00:00:31.230
how else are we going to get to fully automated luxury gay space communism?

9
00:00:31.800 --> 00:00:32.581
In this episode,

10
00:00:32.581 --> 00:00:37.080
I'm going to give you a broad overview of this area as well as teach you two of

11
00:00:37.081 --> 00:00:40.440
the most popular unsupervised learning techniques.

12
00:00:40.770 --> 00:00:45.770
Principal Component Analysis and k means clustering in order to save someone's

13
00:00:46.620 --> 00:00:51.510
life.
A patient at a hospital has been suffering from several epilepsy related

14
00:00:51.511 --> 00:00:52.344
seizures.

15
00:00:52.440 --> 00:00:57.440
Luckily we have a Dataset of their neural activity recorded by electrodes that

16
00:00:57.451 --> 00:00:59.250
were inserted into their brain.

17
00:00:59.520 --> 00:01:04.520
The lead surgeon asked us to use unsupervised learning techniques on this neural

18
00:01:04.861 --> 00:01:09.090
data to find out what part of their brain is causing the seizures so they can

19
00:01:09.091 --> 00:01:12.360
perform surgery on it.
Well,
we save the patient's life.

20
00:01:12.390 --> 00:01:15.480
We'll find out at the end of this video and subscribe.

21
00:01:15.510 --> 00:01:18.750
If you want to keep learning about AI technology for free,

22
00:01:18.810 --> 00:01:21.420
we can divide machine learning into two types,

23
00:01:21.480 --> 00:01:25.800
supervised and unsupervised.
There's also reinforcement learning,

24
00:01:25.801 --> 00:01:28.710
but that only applies in a real time environment,

25
00:01:28.770 --> 00:01:33.240
not a static data spreadsheet.
There's also quantum machine.

26
00:01:33.420 --> 00:01:35.280
Can you please keep it simple for once?

27
00:01:38.580 --> 00:01:41.640
Supervised learning is synonymous with pattern matching.

28
00:01:41.670 --> 00:01:44.040
It's done using the ground truth,

29
00:01:44.041 --> 00:01:48.760
meaning we have prior knowledge of what the output values for our input data.

30
00:01:48.761 --> 00:01:51.090
It should be,
you know that hot dog,

31
00:01:51.091 --> 00:01:56.040
not hot dog classifier trope from the popular show silicon valley that's

32
00:01:56.070 --> 00:01:59.610
supervised learning.
My life is literally that show,
so I don't watch it.

33
00:02:00.240 --> 00:02:04.170
The goal is to approximate the relationship between input and output data.

34
00:02:04.440 --> 00:02:09.000
Most machine learning across every industry is done this way.
It's easy,

35
00:02:09.030 --> 00:02:13.410
it's straightforward,
and it tends to perform very well if given enough examples,

36
00:02:13.800 --> 00:02:18.390
but clean,
perfectly labeled datasets aren't always easy to find.
In fact,

37
00:02:18.450 --> 00:02:21.570
80% of the world's data is unstructured.

38
00:02:21.960 --> 00:02:26.700
The goal of unsupervised learning is to automatically find structure in a

39
00:02:26.701 --> 00:02:29.200
Dataset.
This can itself be the goal.

40
00:02:29.201 --> 00:02:33.960
Discovering hidden patterns in data or a means to an end to learn what the most

41
00:02:33.961 --> 00:02:35.190
relevant features are.

42
00:02:35.460 --> 00:02:39.480
We can further subdivide unsupervised learning and to different types of

43
00:02:39.481 --> 00:02:40.314
techniques.

44
00:02:40.500 --> 00:02:44.700
Clustering finds data points similar to each other and groups them together.

45
00:02:45.000 --> 00:02:47.490
If we had any kind of population data,

46
00:02:47.491 --> 00:02:52.410
whether we were a government organization or a startup with a product like diet,

47
00:02:52.411 --> 00:02:56.520
water,
yes,
that's real.
Basically anyone trying to reach a certain set of people.

48
00:02:56.580 --> 00:03:00.970
We want to segment that population into smaller clusters with similar

49
00:03:00.971 --> 00:03:05.710
demographics and purchasing habits so that we could target them most
effectively.

50
00:03:05.770 --> 00:03:07.600
Spending our marketing budget.

51
00:03:08.020 --> 00:03:13.020
Anomaly detection finds the outliers in a collection of data points banks uses

52
00:03:13.061 --> 00:03:14.860
to find fraudulent transactions.

53
00:03:15.250 --> 00:03:18.670
Association finds correlated features between data points.

54
00:03:18.671 --> 00:03:23.671
Then lets us infer other features of a given data point airbnb uses to recommend

55
00:03:24.070 --> 00:03:28.990
other listings you'd probably like and dimentionality reduction reduces the

56
00:03:28.991 --> 00:03:33.790
number of features in a dataset which makes it easier to visualize and
interpret.

57
00:03:34.180 --> 00:03:38.800
Young Lacoon director of AI research at Facebook puts it best with his quote.

58
00:03:39.090 --> 00:03:43.000
If intelligence was a cake,
unsupervised learning would be the cake.

59
00:03:43.150 --> 00:03:47.260
Supervised learning would be the icing on the cake and reinforcement learning

60
00:03:47.261 --> 00:03:48.820
would be the cherry on the cake.

61
00:03:49.210 --> 00:03:52.120
We now know how to make the icing and the cherry,

62
00:03:52.270 --> 00:03:54.520
but we don't know how to make the cake.

63
00:03:54.580 --> 00:03:58.990
Talk about strange but weirdly effective metaphors.
Here's the year young,

64
00:03:59.350 --> 00:04:01.990
so let's take a look at our data to decide what to do with it.

65
00:04:02.200 --> 00:04:06.400
This is a 30 minute long recording of neural data from an epilepsy patient.

66
00:04:06.640 --> 00:04:10.780
A set of electrodes were inserted into the brain of this patient to record the

67
00:04:10.781 --> 00:04:12.730
activity of neurons in real time.

68
00:04:13.120 --> 00:04:17.440
It picked up electrical spikes of neurons and we can see several features here

69
00:04:17.620 --> 00:04:19.420
that relate to the recording devices,

70
00:04:19.421 --> 00:04:23.680
measurements like the channel number frequency and the number of samples.

71
00:04:23.920 --> 00:04:28.420
Let's first visualize this data using digital alchemy like a python.

72
00:04:28.630 --> 00:04:32.560
We went to extract spikes from the signal and to do that we'll find data points

73
00:04:32.620 --> 00:04:37.620
in the signal that are above some predefined threshold and align them at their

74
00:04:37.661 --> 00:04:38.650
peak amplitude.

75
00:04:38.710 --> 00:04:42.730
We can do this with just 100 random spikes and see that there are at least two

76
00:04:42.731 --> 00:04:44.770
types of waveforms in the data.

77
00:04:45.100 --> 00:04:49.240
One group of spikes would they sharp high amplitude peak and a second group with

78
00:04:49.241 --> 00:04:54.040
a broader initial peak and these spikes were likely generated by more than one

79
00:04:54.070 --> 00:04:54.903
neuron.

80
00:04:54.970 --> 00:04:59.140
If we can find a way to group these wave forms into different clusters,

81
00:04:59.290 --> 00:05:03.250
it will help us figure out which spike corresponds to which neurons,

82
00:05:03.430 --> 00:05:06.580
which will help surgeons decide where to perform surgery.

83
00:05:06.640 --> 00:05:08.920
But in order to cluster the wave forms,

84
00:05:08.921 --> 00:05:12.880
we're going to need to decide which features to input to our algorithm.

85
00:05:13.180 --> 00:05:15.460
One possible feature,
it could be,
for example,

86
00:05:15.550 --> 00:05:19.060
the peak amplitude of this spike or the width of the wave form,

87
00:05:19.270 --> 00:05:22.510
but not all features are equally informative and useful.

88
00:05:22.750 --> 00:05:27.280
We need to select the features that represent the spike wave shapes the best and

89
00:05:27.281 --> 00:05:30.100
get rid of the rest.
For our prediction to be accurate.

90
00:05:30.460 --> 00:05:34.990
The way we're going to do that is to use a type of unsupervised learning called

91
00:05:34.991 --> 00:05:39.700
dimensionality reduction of which there are several techniques like brute force,

92
00:05:40.480 --> 00:05:41.170
no,

93
00:05:41.170 --> 00:05:45.790
we're going to use a popular one called principle component analysis or Pca.

94
00:05:46.150 --> 00:05:49.090
PCA finds the principle components of a dataset.

95
00:05:49.390 --> 00:05:52.660
Principal components are the underlying structure in the data.

96
00:05:52.960 --> 00:05:56.200
They are the direction where there is the most variance,

97
00:05:56.201 --> 00:05:58.970
meaning where the data is most spread out.

98
00:05:59.300 --> 00:06:03.710
It's useful to measure data in terms of principal components rather than on a

99
00:06:03.711 --> 00:06:07.880
normal x,
y axis.
Imagine that we had orange of data points,

100
00:06:07.881 --> 00:06:12.881
which we'll denote as tri force symbols as an ode to the princess to find the

101
00:06:13.101 --> 00:06:15.710
direction with the most variants we can find.

102
00:06:15.750 --> 00:06:20.120
The straight line with a data is most spread out when projected onto it.

103
00:06:20.450 --> 00:06:24.080
A vertical straight line with the points projected onto it will look kind of

104
00:06:24.081 --> 00:06:27.680
like this,
not very spread,
so there's a small variance,

105
00:06:27.860 --> 00:06:31.160
likely no principal component here,
a horizontal line,

106
00:06:31.161 --> 00:06:35.840
however with lines projected onto it looks way more spread out a high barriers.

107
00:06:36.140 --> 00:06:39.230
There's no straight line we can draw that has a larger barriers.

108
00:06:39.350 --> 00:06:44.090
Any horizontal one.
Thus the horizontal line is the principal component.

109
00:06:44.180 --> 00:06:48.710
In this example,
to find the principle components,
we use linear Algebra,

110
00:06:48.770 --> 00:06:52.820
one of the mathematical pillars of machine learning,
two concepts.

111
00:06:52.821 --> 00:06:56.810
Here I can vectors which have a direction and eigen values,

112
00:06:56.840 --> 00:07:00.710
which are numbers that tell us how much variance there is in the data in that

113
00:07:00.711 --> 00:07:01.544
direction.

114
00:07:01.640 --> 00:07:05.690
These two concepts come in pairs like Yin and Yang and the eigen vector with the

115
00:07:05.691 --> 00:07:10.691
highest eigen value is the principal component and a three dimensional data set.

116
00:07:10.731 --> 00:07:12.290
There are three variables.

117
00:07:12.500 --> 00:07:16.160
Imagine all the data points lie on a piece of paper size plane.

118
00:07:16.161 --> 00:07:20.930
In this three graph,
when we find the three eigen vectors and values to,

119
00:07:20.931 --> 00:07:24.170
we'll have large eigen values and one of the eigenvectors,

120
00:07:24.171 --> 00:07:28.370
we'll have an eigen value of zero if we rearrange our axes to be along the

121
00:07:28.371 --> 00:07:32.480
eigenvectors rather than the original variables.
Discarding the third one,

122
00:07:32.630 --> 00:07:36.890
we essentially get rid of the useless direction and are able to represent it in

123
00:07:36.891 --> 00:07:41.210
two dimensions.
We can do this in a single line.
Thanks to psychic learn.

124
00:07:41.211 --> 00:07:45.460
We just need to specify how many components we want mine,

125
00:07:45.461 --> 00:07:48.010
myself with 50 features.

126
00:07:48.280 --> 00:07:53.280
Mother m l comes to me predicting just the best ones.

127
00:07:54.240 --> 00:07:55.073
Let it be.

128
00:07:57.460 --> 00:08:01.570
Once we've reduced the dimensionality of our data,
we're ready to perform.

129
00:08:01.571 --> 00:08:05.470
Clustering.
The second type of unsupervised learning.

130
00:08:05.860 --> 00:08:09.100
A popular clustering technique is called k means.

131
00:08:09.370 --> 00:08:13.600
First we choose a number of k random data points from our sample.

132
00:08:13.840 --> 00:08:17.860
These represent the cluster centers and their number equals the number of

133
00:08:17.861 --> 00:08:18.694
clusters.

134
00:08:18.970 --> 00:08:23.970
Then we calculate the distance between all the random cluster centers and any

135
00:08:24.071 --> 00:08:25.090
other data point.

136
00:08:25.330 --> 00:08:29.530
We then assign each data point to the cluster center closest to it.

137
00:08:29.800 --> 00:08:33.910
Since we started with random data points,
it won't give us a great result,

138
00:08:34.150 --> 00:08:39.130
so we repeat the process and instead of using random data points as cluster

139
00:08:39.131 --> 00:08:39.940
centers,

140
00:08:39.940 --> 00:08:44.940
we calculate the actual cluster centers based on the previous random assignment.

141
00:08:45.730 --> 00:08:50.050
This just keeps repeating and with every iteration the data points that switched

142
00:08:50.051 --> 00:08:53.500
clusters go down and we arrive at a global optimum.

143
00:08:53.830 --> 00:08:57.390
We're now in the gang,
a newer version of the Gucci Gang.

144
00:08:57.930 --> 00:08:59.460
A question arises though,

145
00:08:59.461 --> 00:09:04.461
how do we choose the number of clusters we could try running k means multiple

146
00:09:04.501 --> 00:09:08.280
times with different cluster numbers.
When we plot the results,

147
00:09:08.310 --> 00:09:11.880
we can analyze it to see if we chose to many clusters,

148
00:09:11.970 --> 00:09:15.690
too few or just the right amount based on our domain knowledge.

149
00:09:15.930 --> 00:09:20.280
We can expect to find more than two or three separable clusters from a single

150
00:09:20.281 --> 00:09:23.460
electrode recording in our plot seems to confirm this notion.

151
00:09:23.880 --> 00:09:26.790
Another way to decide this is to use the elbow method.

152
00:09:27.150 --> 00:09:31.530
The way that this works is to run k-means several times and increase the number

153
00:09:31.531 --> 00:09:36.531
of clusters every run and during every run we calculate the average distance of

154
00:09:36.601 --> 00:09:38.670
each data point to it's cluster center.

155
00:09:38.760 --> 00:09:43.760
The number of clusters increases and the average intercluster distance decreases

156
00:09:44.250 --> 00:09:48.750
when we reach six clusters in the average distance to the cluster center does

157
00:09:48.751 --> 00:09:52.620
not change anymore and this is called the elbow point.

158
00:09:52.920 --> 00:09:56.250
It gives us a recommendation of how many clusters we should use.

159
00:09:56.520 --> 00:09:57.630
By clustering the data,

160
00:09:57.660 --> 00:10:02.160
we're able to sort the neuron spiked into distinct regions which correlate to

161
00:10:02.161 --> 00:10:03.450
different parts of the brain.

162
00:10:03.810 --> 00:10:08.810
This is going to be supreme helpful for our client at the hospital and we just

163
00:10:08.821 --> 00:10:12.900
use data science to save a patient's life.
Before we popped champagne,

164
00:10:12.960 --> 00:10:15.210
there are three things to remember from this video.

165
00:10:15.510 --> 00:10:19.590
Unsupervised learning helps find previously unknown patterns in a Dataset

166
00:10:19.740 --> 00:10:20.970
without needing a label.

167
00:10:21.300 --> 00:10:25.800
Principal component analysis is a dimensionality reduction technique that helps

168
00:10:25.801 --> 00:10:28.230
find the most relevant features in a dataset.

169
00:10:28.500 --> 00:10:32.850
And k means clustering is the most popular clustering technique.

170
00:10:33.090 --> 00:10:36.460
Grouping similar data points together for further analysis.

171
00:10:36.461 --> 00:10:38.640
And what is your next data science project?

172
00:10:38.641 --> 00:10:41.760
Let me know in the comment section and please subscribe for more programming

173
00:10:41.761 --> 00:10:46.080
videos.
For now,
I've got to find myself,
so thanks for watching.

