WEBVTT

1
00:00:00.090 --> 00:00:04.200
I'm glad I can book a self driving Uber Now.
Hey,
this car is in self driving.

2
00:00:04.430 --> 00:00:05.610
I need to,
we'll never be

3
00:00:10.740 --> 00:00:11.401
hello world.

4
00:00:11.401 --> 00:00:15.210
It's Saroj and in this episode we're going to talk about how self driving cars

5
00:00:15.270 --> 00:00:19.110
work.
Then people in our own self driving car in a simulated environment,

6
00:00:19.170 --> 00:00:22.140
self driving cars aren't in the realm of science fiction anymore.

7
00:00:22.230 --> 00:00:26.790
Real companies like Toyota and Ford have millions of dollars in r and d pouring

8
00:00:26.791 --> 00:00:27.870
into this technology.

9
00:00:27.900 --> 00:00:32.160
Services like Uber and Lyft that currently pay human drivers will soon deploy

10
00:00:32.190 --> 00:00:36.000
entire fleet of self driving cars.
So prepare per skynet,
just kidding.

11
00:00:36.030 --> 00:00:39.030
And two or three years we're going to start seeing hundreds of thousands of self

12
00:00:39.031 --> 00:00:41.490
driving cars being sold to regular consumers.

13
00:00:41.550 --> 00:00:45.030
Get your self driving trucks right here.
But how do they work?
Well,

14
00:00:45.031 --> 00:00:46.920
when we humans are in the driver's seat,

15
00:00:46.921 --> 00:00:50.700
we're observing our environment by receiving an input of our surroundings and

16
00:00:50.701 --> 00:00:54.990
simultaneously processing it in order to make a decision of which way to move

17
00:00:54.991 --> 00:00:55.740
the steering wheel.

18
00:00:55.740 --> 00:01:00.740
This can be translated into a machine problem known as slam slam or simultaneous

19
00:01:00.931 --> 00:01:04.710
localization and mapping and it's something all self driving cars do.

20
00:01:04.770 --> 00:01:09.060
A self driving car is usually outfitted with GPS unit on inertial navigation

21
00:01:09.061 --> 00:01:10.440
system and a range of sensors.

22
00:01:10.470 --> 00:01:14.220
It uses the positional information from the GPS and navigation system to

23
00:01:14.221 --> 00:01:18.240
localize itself and the sensor data to build an internal map of its environment.

24
00:01:18.300 --> 00:01:21.060
Once it has its position and its internal map of the world,

25
00:01:21.150 --> 00:01:25.290
it can use that map to find the optimal path to its destination that avoids any

26
00:01:25.291 --> 00:01:28.050
kind of obstacles,
be that dead babies or Pokemon.

27
00:01:28.140 --> 00:01:30.690
Once the car has determine the optimal path to take.

28
00:01:30.720 --> 00:01:34.350
That decision is then broken down into a series of motor commands which are fed

29
00:01:34.351 --> 00:01:37.800
into the cars,
actuators.
That's a high level description of how they work,

30
00:01:37.801 --> 00:01:41.070
but roads are complex.
It's not just about avoiding obstacles.

31
00:01:41.100 --> 00:01:44.400
There are weather conditions that require changes in the way you accelerate

32
00:01:44.401 --> 00:01:47.910
different types of road signs and situations that you probably couldn't ever

33
00:01:47.911 --> 00:01:48.510
predict.

34
00:01:48.510 --> 00:01:52.470
A recent paper came out just this year called longterm planning for short term

35
00:01:52.471 --> 00:01:55.860
prediction.
These guys proposed a planning algorithm for self driving cars,

36
00:01:55.861 --> 00:02:00.090
specifically one that would be able to make immediate actions to as to optimize

37
00:02:00.091 --> 00:02:01.590
a longterm objective.

38
00:02:01.620 --> 00:02:05.940
An example they used was around about when a car tries to merge in a roundabout,

39
00:02:05.970 --> 00:02:09.600
it should decide on an immediate acceleration or breaking command while the

40
00:02:09.601 --> 00:02:13.410
longterm effect of the command,
it's a success or failure of the merge.

41
00:02:13.470 --> 00:02:17.490
Traditionally planning for self driving cars is done via reinforcement learning.

42
00:02:17.520 --> 00:02:19.800
The car learns to continuously correct.

43
00:02:19.801 --> 00:02:22.890
It's driving capability over time through trial and error.

44
00:02:22.891 --> 00:02:26.940
When training the car or agent observes the state s that is a scene that it

45
00:02:26.941 --> 00:02:30.960
observes and takes an action a and depending on whether or not the action was

46
00:02:30.961 --> 00:02:32.820
good,
however we define good,

47
00:02:32.850 --> 00:02:36.840
it can receive a reward are then it moves to the next state s and the process

48
00:02:36.841 --> 00:02:37.440
repeats.

49
00:02:37.440 --> 00:02:42.060
The goal is to maximize the reward and that depends on a policy which maps state

50
00:02:42.061 --> 00:02:44.070
to action that it learns over time.

51
00:02:44.100 --> 00:02:47.850
The state action value function is called queue and it helps find the optimal

52
00:02:47.851 --> 00:02:48.360
policy,

53
00:02:48.360 --> 00:02:52.200
but it can be super hard to learn Q and an environment as dynamic as roads with

54
00:02:52.201 --> 00:02:56.070
multiple cars.
It's not just a problem of predicting your own cars actions.

55
00:02:56.100 --> 00:02:58.710
You have to be able to predict other cars actions as well.

56
00:02:58.740 --> 00:03:01.150
So to this problem of learning queue,

57
00:03:01.210 --> 00:03:05.080
they used a deeper recurrent neural network to learn a policy and the input to

58
00:03:05.081 --> 00:03:08.200
the neural net was a vector that contains both the predictable part,

59
00:03:08.201 --> 00:03:09.130
the speed of the car,

60
00:03:09.160 --> 00:03:13.420
and an unpredictable part that speed of other cars so it could learn from both.

61
00:03:13.450 --> 00:03:15.040
They applied it to just two features,

62
00:03:15.070 --> 00:03:17.740
adaptive cruise control and merging roundabouts.

63
00:03:17.770 --> 00:03:21.460
But is there a way to make one learning algorithm that can learn everything from

64
00:03:21.461 --> 00:03:22.120
the ground up?

65
00:03:22.120 --> 00:03:26.050
The technical term for this is end to end and an even fresh paper that was

66
00:03:26.051 --> 00:03:27.970
released about three months ago tried it.

67
00:03:28.090 --> 00:03:32.440
A team from Nvidia put three cameras on a car windshield to receive input data

68
00:03:32.470 --> 00:03:36.340
fed this video data to a convolutional neural network and features were learned

69
00:03:36.370 --> 00:03:37.360
by themselves.

70
00:03:37.420 --> 00:03:40.810
They didn't explicitly decompose the problem into sub modules for different

71
00:03:40.811 --> 00:03:41.530
scenarios.

72
00:03:41.530 --> 00:03:45.610
There's CNN matte what it saw from the input directly to steering commands.

73
00:03:45.640 --> 00:03:48.430
It was first trained in a simulation with prerecorded video,

74
00:03:48.460 --> 00:03:51.070
then trained by a human driver.
They got great results,

75
00:03:51.071 --> 00:03:54.880
but it was hard for the paper authors to differentiate the feature extractor

76
00:03:54.881 --> 00:03:57.130
part of the neural network from the controller part,

77
00:03:57.190 --> 00:03:59.200
so it was difficult to test each.

78
00:03:59.230 --> 00:04:03.160
That's why most real world car manufacturers have decided it's not yet possible

79
00:04:03.161 --> 00:04:05.800
to test and verify an end to end system.

80
00:04:05.830 --> 00:04:09.550
They end up just making software where each module is separate and can be tested

81
00:04:09.551 --> 00:04:13.810
on its own.
A hacker named George Hotz built a self driving car in his garage,

82
00:04:13.870 --> 00:04:17.380
which is a couple of cell phone cameras and the total cost turned out to be just

83
00:04:17.381 --> 00:04:20.800
a thousand bucks.
Let's train our own self driving car using cue,

84
00:04:20.801 --> 00:04:23.860
learning to drive itself without running into obstacles.

85
00:04:23.890 --> 00:04:27.220
After we declare our imports,
let's write our training function for our car.

86
00:04:27.221 --> 00:04:30.610
First it'll take in a neural net with a set of hyper parameters as the

87
00:04:30.611 --> 00:04:31.240
parameters.

88
00:04:31.240 --> 00:04:34.660
Then we'll define some variables for the number of frames we want to observe for

89
00:04:34.661 --> 00:04:36.220
both training and testing.
Well,

90
00:04:36.221 --> 00:04:39.190
then define our positional variables for localization.

91
00:04:39.250 --> 00:04:43.150
We'll create a new game instance and get the first state of the game instance.

92
00:04:43.180 --> 00:04:45.460
We'll also set a timer for tracking purposes.

93
00:04:45.520 --> 00:04:47.980
Then when we start building experience replay,

94
00:04:48.010 --> 00:04:51.580
we'll update our positional variables and choose an action depending on the

95
00:04:51.581 --> 00:04:55.150
state randomly.
If the random variable is outside of our constraints,

96
00:04:55.180 --> 00:04:59.410
we'll get the Q values for each action to help us find the optimal policy.

97
00:04:59.440 --> 00:05:02.530
We'll take that action and if it is valid,
we will get a reward.

98
00:05:02.590 --> 00:05:06.130
Once it's done observing the game and building experience replay,

99
00:05:06.160 --> 00:05:08.890
we'll start training,
sampling the experience,
replay memory,

100
00:05:08.980 --> 00:05:10.330
and getting the training values.

101
00:05:10.360 --> 00:05:13.630
It'll then train the model on this batch that is a neural network.

102
00:05:13.690 --> 00:05:16.450
Then update the starting state,
and if the car dies,

103
00:05:16.510 --> 00:05:19.030
logged the distance and reset the car's life.
Finally,

104
00:05:19.031 --> 00:05:22.390
we want to save the model every 25,000 frames and the weights file.

105
00:05:22.450 --> 00:05:24.370
Let's see how it looks in a simulated environment.

106
00:05:24.400 --> 00:05:28.090
It constantly tries to avoid obstacles through a mix of reinforcement learning

107
00:05:28.210 --> 00:05:30.610
and a neural net.
Once you've got it working in the simulator,

108
00:05:30.611 --> 00:05:34.210
you can port it to a real RC car and have it self-drive all over your room.

109
00:05:34.300 --> 00:05:37.390
Links down below for more info death,
subscribe for more ml videos.

110
00:05:37.420 --> 00:05:39.910
I've got to go to descend some gradients,
so thanks for watching.

