WEBVTT

1
00:00:00.210 --> 00:00:00.961
Hello world.

2
00:00:00.961 --> 00:00:05.961
It's a Raj and the question I get asked the most by far is how do I get started

3
00:00:06.511 --> 00:00:11.511
with deep learning and it makes sense to ask that there are so many different

4
00:00:12.301 --> 00:00:14.790
learning paths and tools you could use.

5
00:00:14.970 --> 00:00:18.360
It's hard to just pick one and roll with it.
In this video,

6
00:00:18.480 --> 00:00:23.460
I'm going to explain why you should use a deep learning library called Ross to

7
00:00:23.461 --> 00:00:28.410
build your first deep neural networks and compare it to other options.

8
00:00:28.680 --> 00:00:33.660
Then we'll use carrots to build an APP that generates text in the style of any

9
00:00:33.661 --> 00:00:34.470
given author.

10
00:00:34.470 --> 00:00:38.130
Deep learning only started getting really popular a couple of years ago when

11
00:00:38.310 --> 00:00:43.110
Hinton's team submitted a model that blew away the competition for the large

12
00:00:43.111 --> 00:00:45.210
scale visual recognition challenge.

13
00:00:45.510 --> 00:00:50.340
They're deep neural network was significantly better than all benchmarks.

14
00:00:50.610 --> 00:00:55.610
Illuminati confirmed because it used lots of GPU computation and data.

15
00:00:56.430 --> 00:01:01.230
Others began to take notice and implemented their own deep neural networks for

16
00:01:01.231 --> 00:01:05.340
different tasks resulting in a deep learning renaissance.

17
00:01:05.370 --> 00:01:10.370
Deep learning plays a huge part in the biggest AI success story of 2017 how

18
00:01:10.681 --> 00:01:14.220
Fuego Google's algorithm that mastered the game of go.

19
00:01:14.221 --> 00:01:16.890
Previously thought near impossible.

20
00:01:16.920 --> 00:01:21.210
Similar improvements were made in fields like vision texts and speech

21
00:01:21.211 --> 00:01:23.640
recognition.
Wave Net for example,

22
00:01:23.850 --> 00:01:28.850
was a model that massively sped up improvements to speech to text and text to

23
00:01:28.861 --> 00:01:32.430
speech resulting in lifelike pregenerated audio.

24
00:01:32.540 --> 00:01:36.600
Piano was really the first widely adopted deep learning library.

25
00:01:36.720 --> 00:01:41.720
It was maintained by the University of Montreal but in September of last year

26
00:01:41.850 --> 00:01:46.850
they announced that they would stop developing for the piano in 2018 yes,

27
00:01:47.400 --> 00:01:52.400
different open source python deep learning frameworks have been introduced the

28
00:01:53.400 --> 00:01:56.520
past couple of years and some got lots of traction.
As of now,

29
00:01:56.550 --> 00:02:01.320
tensorflow seems to be the most used deep learning library based on the number

30
00:02:01.321 --> 00:02:05.670
of Github stars and forks as well as stack overflow activity,

31
00:02:05.970 --> 00:02:10.710
but there are other libraries that are growing passionate user basis as well.

32
00:02:11.040 --> 00:02:15.420
Pi Torch is a great example.
It was introduced in January,

33
00:02:15.420 --> 00:02:16.260
2017 by Facebook.

34
00:02:16.500 --> 00:02:20.730
They basically ported the popular towards framework which was written in Lua to

35
00:02:20.731 --> 00:02:23.450
python.
The main driver behind Pi Torch,

36
00:02:23.451 --> 00:02:27.600
his popularity was the fact that it used dynamic computation graphs.

37
00:02:27.960 --> 00:02:32.370
That means they are defined by run instead of the traditional defined and run.

38
00:02:32.700 --> 00:02:36.540
When inputs can vary,
like if we're using unstructured data with text,

39
00:02:36.720 --> 00:02:40.500
this is super useful and efficient.
When it comes to static graphs.

40
00:02:40.680 --> 00:02:44.100
We first draw the graph,
then inject the data to run it.

41
00:02:44.280 --> 00:02:47.130
That's defined Andra for dynamic graphs.

42
00:02:47.190 --> 00:02:52.170
The graph is defined on the fly via the forward computation of the data that's

43
00:02:52.171 --> 00:02:55.920
defined by row,
but in addition to TensorFlow's mainframe work,

44
00:02:56.070 --> 00:03:00.850
several companion for libraries were released including tensor flow fold for

45
00:03:00.851 --> 00:03:05.851
dynamic computation graphs and tensorflow transform for data input pipelines.

46
00:03:05.950 --> 00:03:09.580
The tensorflow team also announced a new eager execution mode,

47
00:03:09.760 --> 00:03:14.680
which works similar to Pi Torches,
dynamic computation graphs,
but wait.

48
00:03:14.920 --> 00:03:18.250
Other tech giants have also been getting in on the game.

49
00:03:18.610 --> 00:03:21.820
Microsoft launched it's cognitive toolkit.
Last year,

50
00:03:22.120 --> 00:03:24.910
Facebook launched cafe to Amazon,

51
00:03:24.911 --> 00:03:27.790
launched mx net deepmind released to Sonnet.

52
00:03:27.940 --> 00:03:32.230
There's also Deeplearning four j d live,
h two o.
Dot.
Ai and spark.

53
00:03:32.530 --> 00:03:37.270
Oh and Facebook and Microsoft announced the Onyx Open format to share deep

54
00:03:37.271 --> 00:03:40.390
learning models across frameworks.
For example,

55
00:03:40.391 --> 00:03:45.040
you can train your model in one framework but then serve it in production in

56
00:03:45.041 --> 00:03:48.910
another one.
I know,
I know,
I know deep AAF overload,
but look,

57
00:03:49.090 --> 00:03:53.440
the best way to learn how some AI concept works is to start building it and

58
00:03:53.441 --> 00:03:54.760
figure it out as you go.

59
00:03:55.030 --> 00:03:59.700
And the best way to do that is by first using a high level library called care

60
00:03:59.701 --> 00:04:04.240
os curiosity.
Effectively an interface that wraps multiple frameworks.

61
00:04:04.480 --> 00:04:08.320
You can use it as an interface to tensorflow,
fianno or CNTK.

62
00:04:08.590 --> 00:04:12.940
It works the same no matter what back end you use.
Francoise Sholay,

63
00:04:12.970 --> 00:04:17.890
a deep learning researcher at Google created it and maintains it.
Last year,

64
00:04:17.980 --> 00:04:22.810
Google announced it was chosen as the official high level API of tensorflow.

65
00:04:23.260 --> 00:04:27.160
When it comes to writing and debugging custom modules and layers,

66
00:04:27.520 --> 00:04:29.620
Pi torch is the faster option.

67
00:04:29.740 --> 00:04:34.480
While Care Os is definitely the fastest track when you need to quickly train and

68
00:04:34.481 --> 00:04:37.810
test a model built from standard layers using care.

69
00:04:37.990 --> 00:04:42.430
The pipeline for building a deep network looks like this,
you define it,

70
00:04:42.640 --> 00:04:46.810
ComPilot fit it,
evaluated,
and then use it to make predictions.

71
00:04:47.080 --> 00:04:50.200
Consider a simple three layer neural network with an input layer,

72
00:04:50.380 --> 00:04:52.030
hidden layer and output layer.

73
00:04:52.420 --> 00:04:57.250
Each of these layers is just a matrix operation input times weight at a bias and

74
00:04:57.251 --> 00:05:00.370
activate the result.
Repeat that twice and get a prediction.

75
00:05:01.000 --> 00:05:05.830
Deep networks have multiple layers.
They can have three,
four,
five,
whatever.

76
00:05:06.040 --> 00:05:07.390
That's why they're called deep,

77
00:05:07.540 --> 00:05:10.960
and these layers don't have to use just one type of operation.

78
00:05:11.260 --> 00:05:15.100
There are all sorts of layers out there for different types of networks,

79
00:05:15.220 --> 00:05:19.870
convolutional layers,
dropout layers,
recurrent layers.
The list goes on,

80
00:05:20.170 --> 00:05:24.610
but the basic idea of a deep neural network is applying a series of math

81
00:05:24.611 --> 00:05:27.130
operations in order to some input data.

82
00:05:27.250 --> 00:05:31.930
Each layer represents a different operation that then passes the result on to

83
00:05:31.960 --> 00:05:36.640
the next layer.
So in a way we can think of these layers as building blocks.

84
00:05:36.850 --> 00:05:39.640
If we can list out all the different types of layers,

85
00:05:39.850 --> 00:05:44.850
we can wrap them into their own classes and then reuse them as modular building

86
00:05:45.041 --> 00:05:47.830
blocks.
That's exactly what care os does.

87
00:05:47.980 --> 00:05:52.150
It also abstracts away a lot of the magic numbers you'd have to input into a

88
00:05:52.151 --> 00:05:56.740
deep network written in say pure tensor flow.
When we define a network,

89
00:05:56.960 --> 00:06:00.590
they're defined as a sequence of layers using the sequential class.

90
00:06:00.890 --> 00:06:03.770
Once we create an instance of the sequential class,

91
00:06:03.920 --> 00:06:07.610
we can add new layers where each new line is a new layer.

92
00:06:07.910 --> 00:06:12.050
We could do this in just two steps or we could do this in one step by creating

93
00:06:12.051 --> 00:06:16.040
an array of layers beforehand and pasting it to the constructor of the

94
00:06:16.041 --> 00:06:17.060
sequential model.

95
00:06:17.180 --> 00:06:20.930
The first layer in the network must define the number of inputs to expect.

96
00:06:21.290 --> 00:06:25.340
The way that this is specified can differ depending on the network type.

97
00:06:25.970 --> 00:06:30.500
Think of a sequential model as a pipeline with your raw data fed in at the

98
00:06:30.501 --> 00:06:33.350
bottom and predictions that come out at the top.

99
00:06:33.620 --> 00:06:37.190
This is helpful in carrots as concept that were traditionally associated with

100
00:06:37.191 --> 00:06:40.430
the layer can also be split out and added as separate layers.

101
00:06:40.431 --> 00:06:44.330
Clearly showing their role in the transform up data from input to prediction.

102
00:06:44.720 --> 00:06:45.351
For example,

103
00:06:45.351 --> 00:06:49.850
activation functions that transform a sum to signal from each neuron in a layer

104
00:06:50.010 --> 00:06:53.870
can be extracted and add it to the sequential class as a layer like object

105
00:06:53.871 --> 00:06:54.860
called activation.

106
00:06:55.220 --> 00:06:59.270
The choice of activation function is most important for the output layer as it

107
00:06:59.271 --> 00:07:02.750
will define the format that predictions will take.
Once we defined our network,

108
00:07:02.780 --> 00:07:03.680
we'll compile it.

109
00:07:03.860 --> 00:07:07.760
That means it transforms a simple sequence of layers into a highly efficient

110
00:07:07.761 --> 00:07:12.761
series of matrix transformed intended to be executed on a GPU or CPU depending

111
00:07:13.401 --> 00:07:16.850
on our configuration setting.
It's a precompute step for the network.

112
00:07:17.090 --> 00:07:21.800
It's required after defining a model compilation requires a number of parameters

113
00:07:21.801 --> 00:07:25.310
to be specified,
specifically tailored to training our network.

114
00:07:25.340 --> 00:07:28.850
The optimization algorithm we use to train the network and the loss function

115
00:07:28.851 --> 00:07:31.520
used to evaluate it are things that we can decide.

116
00:07:31.640 --> 00:07:35.870
This is the art of deep learning.
Once the network is compiled,

117
00:07:35.900 --> 00:07:39.380
it can be fit,
which means adapting the weights on a training dataset.

118
00:07:39.620 --> 00:07:42.980
Fitting the network requires a training data to be specified.

119
00:07:43.220 --> 00:07:47.810
Both a matrix of input patterns x and an array of matching output patterns.

120
00:07:47.810 --> 00:07:52.460
Why the network is trained using the back propagation algorithm and optimize

121
00:07:52.490 --> 00:07:56.120
according to the optimization algorithm and loss function specified when

122
00:07:56.121 --> 00:07:58.100
compiling the model.
Finally,

123
00:07:58.130 --> 00:08:01.160
once we are satisfied with the performance of our fit model,

124
00:08:01.340 --> 00:08:03.620
we can use it to make predictions on new data.

125
00:08:03.920 --> 00:08:07.940
This is as easy as calling the predict function on the model with an array of

126
00:08:07.941 --> 00:08:11.210
new input patterns for our text generation sample.

127
00:08:11.450 --> 00:08:15.260
We'll see that it generates texts in the style of our favorite author just as we

128
00:08:15.261 --> 00:08:17.120
fed it in three points to remember.

129
00:08:17.240 --> 00:08:21.500
There are lots of new competitors that showed up in 2017 for deep learning

130
00:08:21.501 --> 00:08:25.490
libraries,
but care os is still the easiest way to get started.

131
00:08:25.910 --> 00:08:30.680
Pi Torch is getting really popular and is the best way to build models next to

132
00:08:30.681 --> 00:08:35.630
Carol Ross and deep networks are a series of math operations in the form of

133
00:08:35.631 --> 00:08:39.530
layers,
just mix and match them to get different results every time.

134
00:08:39.710 --> 00:08:41.960
The coding challenge winner from the war robots.

135
00:08:41.961 --> 00:08:46.961
Video is Alberto Garcia says he used a proximal policy optimization algorithm to

136
00:08:47.871 --> 00:08:52.490
train an AI to balance a pendulum using the open AI gym environment.

137
00:08:52.760 --> 00:08:53.990
Top notch work,

138
00:08:54.030 --> 00:08:59.030
Alberto and the runner up is Sven near Berger who landed a simulated space x

139
00:08:59.611 --> 00:09:02.940
rocket using PPO.
Such a cool use case.

140
00:09:03.450 --> 00:09:04.890
This week's coding challenge.

141
00:09:04.920 --> 00:09:08.250
It's a used care os to build your own deep neural network,

142
00:09:08.550 --> 00:09:11.130
get help links go into description and coding challenge.

143
00:09:11.131 --> 00:09:13.110
Winners will be announced next week.

144
00:09:13.200 --> 00:09:15.990
Please subscribe for more programming videos.
And for now,

145
00:09:16.020 --> 00:09:19.680
I've got to not use anything made by Microsoft,
so thanks for watching.

