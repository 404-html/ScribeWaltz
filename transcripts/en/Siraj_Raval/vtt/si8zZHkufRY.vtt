WEBVTT

1
00:00:00.160 --> 00:00:00.851
Hello world,

2
00:00:00.851 --> 00:00:04.660
it's Saroj and today we're going to use machine learning to help us understand

3
00:00:04.661 --> 00:00:05.650
our emotions.

4
00:00:05.920 --> 00:00:09.880
Our emotional intelligence distinguishes us from every other known living being

5
00:00:09.881 --> 00:00:13.540
on earth.
These emotions can be simple like when you get so hype,

6
00:00:13.541 --> 00:00:14.910
all you can hear as Gasolina,

7
00:00:14.911 --> 00:00:19.911
Bye Daddy Yankee and we've invented language to help us express them to others,

8
00:00:20.410 --> 00:00:22.330
but sometimes words are not enough,

9
00:00:22.510 --> 00:00:26.590
like some emotions have no direct English translation.
For example,

10
00:00:26.591 --> 00:00:27.910
in German vault iron,

11
00:00:27.911 --> 00:00:31.480
thumb kite is the feeling experience when you are alone in the woods and

12
00:00:31.481 --> 00:00:33.610
connecting with nature and Japanese.

13
00:00:33.900 --> 00:00:38.410
[inaudible] is the awareness of the impermanence of all things and the gentle

14
00:00:38.411 --> 00:00:42.790
sadness at their passing.
Emotions are hard to express,
let alone understand,

15
00:00:43.090 --> 00:00:47.350
but that's where AI can help us and AI could understand us better than we do

16
00:00:47.380 --> 00:00:51.790
analyzing our emotional data to help us make optimal decisions for goals that we

17
00:00:51.791 --> 00:00:52.624
specify,

18
00:00:52.810 --> 00:00:56.830
like a personal life coach slash therapist slash Denzel Washington,

19
00:00:57.430 --> 00:00:58.510
but how would it do this?

20
00:00:58.660 --> 00:01:01.840
There are generally two main approaches to sentiment analysis.

21
00:01:02.050 --> 00:01:04.630
The first one is the lexicon based approach.

22
00:01:04.750 --> 00:01:09.400
We first want to split some given text into smaller tokens,
be that words,

23
00:01:09.401 --> 00:01:13.690
phrases,
or whole sentences.
This process is called tokenization.

24
00:01:13.840 --> 00:01:16.960
Then we count the number of times each word shows up.

25
00:01:17.260 --> 00:01:21.460
This resulting tally is called the bag of words model.
Next,

26
00:01:21.461 --> 00:01:25.210
we'd look up the subjectivity of each word from an existing lexicon,

27
00:01:25.211 --> 00:01:30.211
which is a database of emotional values for words prerecorded by researchers and

28
00:01:30.641 --> 00:01:31.930
what do we have those values?

29
00:01:32.080 --> 00:01:35.560
We can then compute the overall subjectivity of our text.

30
00:01:36.070 --> 00:01:38.370
The other approach uses machinery.

31
00:01:38.740 --> 00:01:42.880
If we have a corpus of say tweets that are labeled either positive or negative,

32
00:01:43.180 --> 00:01:46.690
we can train a classifier on it and then give it a new tweet.

33
00:01:46.691 --> 00:01:51.490
It will classify it as either positive or negative.
So which approach is better?

34
00:01:51.640 --> 00:01:55.570
Don't ask me.
No.
Yeah,
totally.
Ask me.
Well,
using the lexicon is easier,

35
00:01:55.571 --> 00:01:57.760
but the learning approach is more accurate.

36
00:01:57.820 --> 00:02:02.140
There are subtleties in language that lexicons are bad at like sarcasm.

37
00:02:02.470 --> 00:02:05.470
It seems to mean one thing,
but it really means another.

38
00:02:05.860 --> 00:02:10.270
But deep neural nets can understand the subtleties because they don't analyze

39
00:02:10.271 --> 00:02:15.100
texts at face value.
They create abstract representations of what they learned.

40
00:02:15.460 --> 00:02:20.320
These generalizations are called vectors and we can use them to classify data.

41
00:02:20.890 --> 00:02:25.300
Let's learn more about vectors by building a sentiment classifier for movie

42
00:02:25.301 --> 00:02:28.180
reviews,
and I'll show you how to run it in the cloud.

43
00:02:28.660 --> 00:02:31.030
The only dependency will need is TF learn,

44
00:02:31.031 --> 00:02:33.880
and I'm using it since it's the easiest way to get started.

45
00:02:33.881 --> 00:02:37.540
Building deep neural networks will import a couple of helper functions that are

46
00:02:37.541 --> 00:02:40.570
built into it as well,
and I'll explain those when we get to them.

47
00:02:40.900 --> 00:02:44.170
The first step in our process is to collect our dataset up.

48
00:02:44.190 --> 00:02:47.920
Learn has a bunch of preprocessed datasets we can use and we're going to use a

49
00:02:47.921 --> 00:02:50.140
Dataset of IMD being movie ratings,

50
00:02:53.650 --> 00:02:55.490
load it using the load data function.

51
00:02:55.640 --> 00:02:59.000
This will download our Dataset from the wet well name the path where we want to

52
00:02:59.320 --> 00:03:02.320
it.
The extension being pickle,
which means it's a byte street.

53
00:03:02.590 --> 00:03:06.940
This makes it easier to convert to other python objects like lists or tuples.

54
00:03:06.970 --> 00:03:07.720
Later,

55
00:03:07.720 --> 00:03:12.400
we want 10,000 words from the database and we only want to use 10% of the data

56
00:03:12.520 --> 00:03:15.940
for our validation set.
So we'll set the last argument to 0.1.

57
00:03:16.270 --> 00:03:19.810
Load data will return our movie reviews split into a training and testing set.

58
00:03:20.110 --> 00:03:23.800
We can then further split those sets into reviews and labels and set them equal

59
00:03:23.801 --> 00:03:25.030
to x and y values.

60
00:03:25.270 --> 00:03:28.450
Training data is the portion of our model learns from validation.

61
00:03:28.451 --> 00:03:30.190
Data is a part of the training process.

62
00:03:30.400 --> 00:03:32.350
While training data helps us fit our weights,

63
00:03:32.590 --> 00:03:36.670
validation data helps prevent overfitting by letting us tune or hyper parameters

64
00:03:36.700 --> 00:03:41.680
accordingly.
And testing data is what our model uses to test itself by comparing.

65
00:03:41.681 --> 00:03:44.110
It's predicted labels to actual labels,

66
00:03:44.170 --> 00:03:46.270
so tests yourself before you wreck yourself.

67
00:03:46.390 --> 00:03:50.020
Now that we have our data split into sets,
let's do some preprocessing.

68
00:03:50.260 --> 00:03:53.170
We can't just feed text strings into a neural network directly.

69
00:03:53.410 --> 00:03:55.030
We have to vectorize our inputs.

70
00:03:55.240 --> 00:03:58.390
Neural Nets are how algorithms that essentially just apply a series of

71
00:03:58.570 --> 00:04:00.220
computations,
two matrices,

72
00:04:00.221 --> 00:04:04.840
so converting them to numerical representations or vectors is necessary.

73
00:04:05.170 --> 00:04:08.110
The pad sequences function.
We'll do that for our review text.

74
00:04:08.290 --> 00:04:12.580
It'll convert each of you into a matrix and pat hit padding is necessary to

75
00:04:12.581 --> 00:04:15.670
ensure consistency in our inputs.
Dimentionality it.

76
00:04:15.671 --> 00:04:19.030
We'll pat each sequence with the zero at the end which we specify until it

77
00:04:19.031 --> 00:04:23.470
reaches the Max possible sequence length which will set to 100 we also want to

78
00:04:23.471 --> 00:04:27.730
convert our labels to vectors as well and we can easily do that using the two

79
00:04:27.731 --> 00:04:31.480
categorical function.
These are binary vectors with two classes,

80
00:04:31.481 --> 00:04:33.120
one which is positive or zero

81
00:04:33.230 --> 00:04:38.150
<v 2>which isn't negative.
Yo hold up factors got me feeling like feed forward.</v>

82
00:04:38.151 --> 00:04:40.250
Neural nets got me feeling so upset.

83
00:04:40.310 --> 00:04:44.900
Only fixed size inputs like a small number sense so I dropped it and instead I

84
00:04:44.901 --> 00:04:48.830
used for current net which are made for sequences like typed up or written text

85
00:04:48.831 --> 00:04:51.560
that convert my text in to this end up feeding in.

86
00:04:51.830 --> 00:04:55.790
We train in word vectors that are embedded in Redmond more numerically.

87
00:04:55.940 --> 00:05:00.470
We can use generically a one to many mapping that upbringing to you lyrically
on.

88
00:05:00.480 --> 00:05:00.600
We

89
00:05:00.600 --> 00:05:04.860
<v 0>can intuitively define each layer in our network as its own line of code.</v>

90
00:05:05.250 --> 00:05:09.060
First will be our input layer.
This is where we feed data into our network.

91
00:05:09.360 --> 00:05:12.090
The only parameter will specify is the inputs shape.

92
00:05:12.480 --> 00:05:16.380
The first element is the batch size,
which will set to none and then the length,

93
00:05:16.381 --> 00:05:20.760
which is a hundred since we set our max sequence length to 100 our next layer is

94
00:05:20.761 --> 00:05:21.594
our embedding layer.

95
00:05:21.810 --> 00:05:25.200
The first parameter will be the output vector we received from the previous

96
00:05:25.201 --> 00:05:26.400
layer,
and by the way,

97
00:05:26.401 --> 00:05:30.200
for every layer we write we'll be using the previous layers outputs as its

98
00:05:30.210 --> 00:05:33.120
inputs.
This is how data flows to a neural network.

99
00:05:33.121 --> 00:05:38.121
At each layer it's transformed like a seven layer dip of computation will say

100
00:05:38.280 --> 00:05:41.790
mentioned to 10,000 since that's how many words we loaded from our dataset

101
00:05:41.820 --> 00:05:45.960
earlier and the aqua dimension two one 28 which is the number of dimensions of

102
00:05:45.961 --> 00:05:50.730
our resulting embeddings.
Next we'll feed those values to our LSTM layer.

103
00:05:50.910 --> 00:05:53.940
This layer allows our network to remember data from the beginning of the

104
00:05:53.941 --> 00:05:54.774
sequences,

105
00:05:54.810 --> 00:05:59.120
which will improve our prediction will set dropout 2.8 which is a technique that

106
00:05:59.121 --> 00:06:03.320
helps prevent over fitting by randomly turning on and off different pathways in

107
00:06:03.321 --> 00:06:05.450
our network.
Our next layer is fully connected,

108
00:06:05.451 --> 00:06:09.230
which means that every neuron in the previous layer is connected to every
neuron.

109
00:06:09.260 --> 00:06:10.093
In this layer.

110
00:06:10.190 --> 00:06:14.240
We have a set of learned feature vectors from previous layers and adding a fully

111
00:06:14.241 --> 00:06:18.530
connected layer is a computationally cheap way of learning non linear

112
00:06:18.531 --> 00:06:19.700
combinations of them.

113
00:06:20.030 --> 00:06:24.050
It's got two units and it's using these soft max function as its activation

114
00:06:24.051 --> 00:06:24.440
function.

115
00:06:24.440 --> 00:06:28.340
This will take in a vector of values and squash it into a vector of output

116
00:06:28.341 --> 00:06:31.580
probabilities between zero and one that sum to one.

117
00:06:31.880 --> 00:06:34.880
We'll use those values in our last layer,
which is our regression layer.

118
00:06:35.150 --> 00:06:37.820
This will apply a regression operation to the input.

119
00:06:38.090 --> 00:06:41.480
We're going to specify an optimizer method that will minimize a given loss

120
00:06:41.481 --> 00:06:43.340
function as well as the learning rate,

121
00:06:43.460 --> 00:06:46.340
which specifies how fast we want our network to train.

122
00:06:46.670 --> 00:06:48.380
The optimizer will use is Adam,

123
00:06:48.410 --> 00:06:53.150
which performs gradient descent and categorical cross entropy is our loss.

124
00:06:53.270 --> 00:06:56.630
It helps to find the difference between our predicted output and the expected

125
00:06:56.631 --> 00:06:58.670
output.
After building our neural network,

126
00:06:58.671 --> 00:07:02.810
we can go ahead and initialize it using TF learns deep neural net function.

127
00:07:02.840 --> 00:07:05.030
Then we can call our model's fit function,

128
00:07:05.120 --> 00:07:08.600
which we'll launch the training process for our given training and validation

129
00:07:08.601 --> 00:07:09.200
data.

130
00:07:09.200 --> 00:07:12.980
We'll also set show metric to true so we can view the log of accuracy during

131
00:07:12.981 --> 00:07:17.060
training.
So to demo this,
we're going to run this in the cloud using AWS.

132
00:07:17.300 --> 00:07:20.240
What we'll do is use a prebuilt Amazon machine image.

133
00:07:20.480 --> 00:07:24.380
This Ami can be used to launch an instance and it's got every dependency we need

134
00:07:24.430 --> 00:07:28.550
builtin including tensorflow,
Kuta,
lil Wayne's deposition video.

135
00:07:28.760 --> 00:07:32.270
If we click on the orange continue button and we can select the type of instance

136
00:07:32.271 --> 00:07:35.270
we want,
I'll go for the smallest because I'm poor still,

137
00:07:35.271 --> 00:07:38.680
but ideally we'd use a larger instance with gps.

138
00:07:39.050 --> 00:07:41.060
Then we can accept the terms in one click.

139
00:07:41.330 --> 00:07:45.470
Next we'll go to our AWS console by clicking this button and after a while our

140
00:07:45.471 --> 00:07:46.640
instance will start running.

141
00:07:46.880 --> 00:07:49.340
We can copy and paste the public DNS into our browser,

142
00:07:49.520 --> 00:07:53.870
followed by which is the port we specified for access for the password.

143
00:07:53.930 --> 00:07:55.310
We'll use the instance id.

144
00:07:55.370 --> 00:07:59.330
Now we're in our infancy environment built with our Ami and we can play with a

145
00:07:59.331 --> 00:08:04.331
Jupiter notebook hosted on AWS will create a new notebook and paste our code in

146
00:08:04.581 --> 00:08:07.610
there.
Now we can run it and it'll start training just like that.

147
00:08:08.090 --> 00:08:08.960
So to break it down,

148
00:08:08.990 --> 00:08:13.430
there are two main approaches to sentiment analysis using a lexicon,

149
00:08:13.460 --> 00:08:16.820
a prerecorded sentiment or using state of the art,

150
00:08:16.821 --> 00:08:18.740
but more computationally expensive,

151
00:08:18.980 --> 00:08:23.980
deep learning to learn generalize vector representations from words feedforward

152
00:08:25.040 --> 00:08:28.130
nets except fixed size inputs like binary numbers,

153
00:08:28.131 --> 00:08:33.131
but recurrent neural nets help us learn from sequences of data like text and you

154
00:08:33.501 --> 00:08:35.510
can use AWS with a prebuilt.

155
00:08:35.530 --> 00:08:40.310
Am I to easily train your model in the cloud without dealing with dependency

156
00:08:40.311 --> 00:08:41.144
issues?

157
00:08:41.180 --> 00:08:45.650
The coding challenge winner from last week is Ludo born little architected his

158
00:08:45.651 --> 00:08:49.760
neural net so that stacking layers was as easy as a line of code per layer

159
00:08:50.090 --> 00:08:53.080
wizard of the week and the runner up is cgs.

160
00:08:53.150 --> 00:08:57.690
Soon he accurately modified code to reflect multilayer backpropagation.

161
00:08:57.780 --> 00:09:01.770
The coding challenge for this week is to use [inaudible] to train a neural

162
00:09:01.771 --> 00:09:05.790
network to recognize sentiment from a video game or view Dataset that I'll

163
00:09:05.791 --> 00:09:09.540
provide details are in the read me posts or get humbling in the comments and

164
00:09:09.570 --> 00:09:12.810
I'll announce the winner in one week.
Please click that subscribe button.

165
00:09:12.840 --> 00:09:15.840
If you want to see more videos like this,
check out this related video.

166
00:09:16.020 --> 00:09:20.430
And for now I've got to figure out what the FPI torches.
So thanks for watching.

