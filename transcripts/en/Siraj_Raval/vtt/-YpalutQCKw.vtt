WEBVTT

1
00:00:00.090 --> 00:00:00.841
Hello world.

2
00:00:00.841 --> 00:00:05.250
It's the Raj and in machine learning you'll hear the term Monte Carlo a lot.

3
00:00:05.580 --> 00:00:10.170
I'm going to explain what that means and we'll use it to help a house cleaning

4
00:00:10.171 --> 00:00:14.400
robot navigate a virtual living room.
To recap,

5
00:00:14.520 --> 00:00:19.520
last week we talked about how there are two main algorithms for computing

6
00:00:19.710 --> 00:00:23.670
optimal policies for an AI,
namely value,

7
00:00:23.680 --> 00:00:25.740
iteration and policy iteration.

8
00:00:26.100 --> 00:00:31.100
A policy is the set of rules for how an AI chooses actions to take.

9
00:00:31.380 --> 00:00:36.380
We modeled our environment using a mark Haub decision process and we use the

10
00:00:37.321 --> 00:00:42.321
transition model to describe the probability of moving from one state to the

11
00:00:43.591 --> 00:00:44.340
other.

12
00:00:44.340 --> 00:00:49.340
This is the most common way to formalize a reinforcement learning problem where

13
00:00:50.191 --> 00:00:55.191
an agent provides an action to the environment as a function of the state and

14
00:00:56.791 --> 00:01:01.791
reward causing the environment to update and provide a new states and reward to

15
00:01:02.851 --> 00:01:05.460
the agent in a feedback loop.

16
00:01:05.670 --> 00:01:10.670
So far we've assumed that the agent knows what all the elements of the Mark Cobb

17
00:01:11.711 --> 00:01:13.650
decision process are.

18
00:01:13.890 --> 00:01:18.890
We can just compute the solution before ever actually executing an action in the

19
00:01:19.801 --> 00:01:23.850
environment in AI because we can compute the solution.

20
00:01:23.880 --> 00:01:28.880
To this decision making problem before executing the actual decision.

21
00:01:29.400 --> 00:01:32.190
We'd typically call this process planning,

22
00:01:32.490 --> 00:01:37.050
which is the opposite of what NASA is doing for a Mars mission.

23
00:01:37.080 --> 00:01:42.080
Both value iteration and policy iteration algorithms are examples of planning

24
00:01:42.991 --> 00:01:46.410
algorithms,
but reinforcement learning isn't so kind to us.

25
00:01:46.620 --> 00:01:51.620
What makes a problem a reinforcement learning problem instead of a planning

26
00:01:51.811 --> 00:01:56.811
problem is that the agent doesn't know all the elements of the markup decision

27
00:01:57.481 --> 00:02:02.340
process.
So it wouldn't be able to just plan a solution.

28
00:02:02.370 --> 00:02:06.870
It doesn't know how the world will change in response to its actions,

29
00:02:07.050 --> 00:02:07.511
I.
E.

30
00:02:07.511 --> 00:02:12.390
The transition function tea nor what immediate reward it will receive for doing

31
00:02:12.391 --> 00:02:17.391
so I either reward function are the agent has to try taking actions in the

32
00:02:18.421 --> 00:02:23.421
environment and observing what happens until somehow it finds a good policy from

33
00:02:25.291 --> 00:02:25.980
doing so.

34
00:02:25.980 --> 00:02:30.980
So the question then is if the agent doesn't know the transition function or the

35
00:02:31.561 --> 00:02:34.950
reward function can find a good policy?

36
00:02:35.220 --> 00:02:39.030
The answer is yes.
And there are two approaches here.

37
00:02:39.420 --> 00:02:44.420
The first approach is for the agent to learn a model of how the environment

38
00:02:44.791 --> 00:02:47.400
works from its observations.

39
00:02:47.550 --> 00:02:50.370
Then plan a solution using that model.

40
00:02:50.550 --> 00:02:55.550
So if an agent is currently in a state and takes an action than observes the

41
00:02:55.711 --> 00:02:58.920
environment transition to the next state with the,

42
00:02:59.920 --> 00:03:04.510
that information can be used to improve the estimate of T.

43
00:03:04.511 --> 00:03:08.560
And r.
Once the agent has a model for the environment,

44
00:03:08.740 --> 00:03:12.100
it can then use a planning algorithm like policy,

45
00:03:12.101 --> 00:03:17.101
iteration or value iteration with it's learned model to find a policy

46
00:03:17.710 --> 00:03:22.710
reinforcement learning solutions that follow this framework are called model

47
00:03:22.751 --> 00:03:24.280
based algorithms.

48
00:03:24.550 --> 00:03:29.550
This is when an agent exploits a previously learned model to accomplish a task

49
00:03:30.491 --> 00:03:31.180
at hand,

50
00:03:31.180 --> 00:03:36.180
but it turns out that an agent doesn't have to learn a model of the environment

51
00:03:36.940 --> 00:03:38.770
to find a good policy.

52
00:03:39.100 --> 00:03:44.100
Sometimes our agent can simply rely on trial and error experience for actions

53
00:03:45.101 --> 00:03:50.101
election and we can call this model free learning in moto free reinforcement

54
00:03:51.131 --> 00:03:54.820
learning.
The first thing we miss is a transition model.

55
00:03:55.060 --> 00:03:59.800
The second thing we miss is a reward function which gives the agent the reward

56
00:03:59.830 --> 00:04:03.850
associated to a particular state.
There are two approaches here,

57
00:04:04.030 --> 00:04:08.350
a passive and an active approach.
In the passive approach.

58
00:04:08.470 --> 00:04:13.470
We have a policy which the agent can use to move in the environment in state

59
00:04:14.730 --> 00:04:19.660
[inaudible].
The agent always produces the action a given by the policy.

60
00:04:19.930 --> 00:04:23.230
The goal of the agent is to learn the utility function.

61
00:04:23.620 --> 00:04:26.500
This is a case for Monte Carlo for prediction,

62
00:04:26.650 --> 00:04:31.650
but it's also possible to estimate the optimal policy while moving in the

63
00:04:31.811 --> 00:04:36.190
environment.
In this case we are in an active case,

64
00:04:36.240 --> 00:04:40.900
I.
E.
We are applying Monte Carlo for control.
Okay?
Okay.
Okay.

65
00:04:40.960 --> 00:04:43.780
Lots of terms here.
So let's rewind a little bit.

66
00:04:44.290 --> 00:04:48.550
Let's say we've got a four by three tiled floor.
This represents our kitchen.

67
00:04:48.970 --> 00:04:50.710
Don't ask me why it's so tiny.

68
00:04:50.711 --> 00:04:55.510
The virtual rent is too damn high and we want our cleaning robot to clean the

69
00:04:55.511 --> 00:04:56.740
whole kitchen.

70
00:04:57.070 --> 00:05:01.210
This environment has an unknown transition model.

71
00:05:01.240 --> 00:05:06.240
The only information about the environment is the state's availability and since

72
00:05:06.401 --> 00:05:09.670
our robot doesn't have the reward function,

73
00:05:09.940 --> 00:05:14.940
it doesn't know which state contains the charging station and which state

74
00:05:15.880 --> 00:05:17.560
contains the stairs.

75
00:05:17.680 --> 00:05:22.680
Only in the passive case does the robot have a policy that it can follow to move

76
00:05:23.320 --> 00:05:24.250
in the world.

77
00:05:24.490 --> 00:05:29.490
And since the robot doesn't know what's going to happen after each action,

78
00:05:30.280 --> 00:05:34.210
it can only give unknown probabilities to each possible outcome.

79
00:05:34.510 --> 00:05:36.130
In this passive case,

80
00:05:36.190 --> 00:05:41.190
our objective is to use the available information to estimate the utility

81
00:05:41.530 --> 00:05:43.420
utility function.
To do this,

82
00:05:43.421 --> 00:05:47.980
our robot could estimate the transition model moving in the environment and

83
00:05:47.981 --> 00:05:52.540
keeping track of the number of times an action has correctly executed.

84
00:05:52.570 --> 00:05:55.060
Once the transition model is available,

85
00:05:55.090 --> 00:06:00.090
the robot can either use value iteration or policy iteration to get the utility

86
00:06:00.651 --> 00:06:01.220
function.

87
00:06:01.220 --> 00:06:05.660
There are different techniques which can find out that the transition model,

88
00:06:05.690 --> 00:06:10.490
but the problem with this approach is that estimating the values of a transition

89
00:06:10.491 --> 00:06:14.930
model can be expensive for our three by four world.

90
00:06:14.960 --> 00:06:19.960
That means we have to estimate the values for a table of 12 states by 12 states

91
00:06:20.600 --> 00:06:21.860
by four actions.

92
00:06:22.010 --> 00:06:26.960
So there is another technique which directly estimates the utility function

93
00:06:27.230 --> 00:06:32.090
without using the transition model called the Monte Carlo method.

94
00:06:32.120 --> 00:06:37.120
In the 1940s Stanislaw ou alum invented the Monte Carlo method to help with his

95
00:06:37.791 --> 00:06:40.670
experiments.
The idea is simple,

96
00:06:40.790 --> 00:06:45.560
use it randomness to solve problems.
It's used a lot in AI.

97
00:06:45.590 --> 00:06:46.130
In fact,

98
00:06:46.130 --> 00:06:51.130
deep mind used the Monte Carlo method to complete a tree search to find the best

99
00:06:52.521 --> 00:06:54.230
move in the game of go,

100
00:06:54.350 --> 00:06:57.800
which ended up beating the world champion at the game.

101
00:06:57.860 --> 00:07:02.860
It's a useful technique since it lets our agent learn the optimal behavior

102
00:07:03.051 --> 00:07:06.530
directly from its interaction with the environment.

103
00:07:06.590 --> 00:07:08.540
In our cleaning robot scenario,

104
00:07:08.541 --> 00:07:12.770
the robot starts at a state and follows its internal policy.

105
00:07:13.040 --> 00:07:18.040
At each step it records the reward obtained and saves the history of all the

106
00:07:18.651 --> 00:07:22.010
states visited until reaching a terminal state.

107
00:07:22.370 --> 00:07:26.600
We call an episode the sequence of states from the starting state to the

108
00:07:26.601 --> 00:07:30.980
terminal state.
If our robot recorded the following three episodes,

109
00:07:31.160 --> 00:07:35.150
we'd noticed that the robot followed its internal policy,

110
00:07:35.360 --> 00:07:40.360
but an unknown transition model perturbed the trajectory leading to undesired

111
00:07:41.331 --> 00:07:44.060
states.
In the first two episodes,

112
00:07:44.210 --> 00:07:49.210
the robot eventually reached the terminal states obtaining a positive reward,

113
00:07:49.700 --> 00:07:53.750
but in the third episode the robot moved along a wrong path,

114
00:07:53.900 --> 00:07:56.000
reaching the stairs and falling down.

115
00:07:56.090 --> 00:08:00.230
Every occurrence of a state during the episode is called of visit.

116
00:08:00.410 --> 00:08:04.370
Using our discount factor,
we can calculate the return.

117
00:08:04.430 --> 00:08:07.310
For a given state.
After three episodes,

118
00:08:07.311 --> 00:08:09.860
we come out with three different returns.

119
00:08:10.060 --> 00:08:13.190
We are taking the expectation of the returns here,

120
00:08:13.400 --> 00:08:18.400
but we need more than three episodes to properly approximate the utility.

121
00:08:18.680 --> 00:08:21.890
Why is this?
Well,
using Monte Carlo terminology,

122
00:08:22.190 --> 00:08:27.190
we can define s to be a discrete random variable which can assume all the

123
00:08:28.191 --> 00:08:30.830
available states with a certain probability.

124
00:08:30.860 --> 00:08:33.800
Every time our robots steps into a state,

125
00:08:34.010 --> 00:08:38.000
it's like we are picking a value for the random variable S.

126
00:08:38.330 --> 00:08:40.910
For each state of each episode.

127
00:08:41.000 --> 00:08:43.910
We can calculate the return and store it in a list.

128
00:08:44.120 --> 00:08:49.120
Repeating this process for a large number of times is guaranteed to converge to

129
00:08:49.611 --> 00:08:50.720
the true utility.

130
00:08:50.780 --> 00:08:55.780
This is because of the famous theorem known as the law of large numbers,

131
00:08:56.160 --> 00:09:00.540
I.
E.
The more trials the closer to the expected value,

132
00:09:00.541 --> 00:09:01.740
our results will be.

133
00:09:01.800 --> 00:09:06.800
After several iterations we can see that the utility gets more and more accurate

134
00:09:07.350 --> 00:09:10.650
and thus our robot gets better and better at cleaning.

135
00:09:10.740 --> 00:09:14.040
They're just three key points to remember right now.

136
00:09:14.190 --> 00:09:16.170
In model based learning.

137
00:09:16.410 --> 00:09:21.410
The agent exploits a previously learned model to accomplish the task at hand,

138
00:09:22.440 --> 00:09:25.020
whereas in model free learning,

139
00:09:25.170 --> 00:09:30.170
the agent simply relies on some trial and error experience for action selection.

140
00:09:31.770 --> 00:09:36.770
Monte Carlo methods rely on repeated random sampling to obtain numerical results

141
00:09:38.670 --> 00:09:43.670
and for model free learning we can use the Monte Carlo method to either learn

142
00:09:44.910 --> 00:09:49.350
the utility function or to estimate the optimal policy.

143
00:09:49.770 --> 00:09:52.410
The wizard of the week is Justin Francis.

144
00:09:52.650 --> 00:09:57.650
The challenge was to create a simple value iteration algorithm using open Ai's

145
00:09:58.230 --> 00:10:01.440
gym environment and Justin did just that.

146
00:10:01.680 --> 00:10:06.680
He even managed to create a really cool three d graph of state values over time.

147
00:10:07.470 --> 00:10:08.520
Impressive stuff.

148
00:10:08.521 --> 00:10:13.521
Justin and the runner up is suck Chum Sharma who created a deep cue learning Bot

149
00:10:14.460 --> 00:10:16.830
for a PAC man environment.
Great work.

150
00:10:16.890 --> 00:10:21.150
This week's coding challenge is to use the Monte Carlo method to help an AI

151
00:10:21.180 --> 00:10:23.040
navigate in an environment,

152
00:10:23.340 --> 00:10:27.510
posts your get hub links in the comment section and I'll review them personally

153
00:10:27.720 --> 00:10:30.360
announcing the top two entries next week.

154
00:10:30.390 --> 00:10:33.330
Please subscribe for more programming videos,
and for now,

155
00:10:33.360 --> 00:10:37.050
I've got to go compute the optimal policy.
So thanks for watching.

