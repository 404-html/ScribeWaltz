WEBVTT

1
00:00:00.150 --> 00:00:00.931
Hello world.

2
00:00:00.931 --> 00:00:05.790
It's Raj and what's the most efficient way to interact with the database?

3
00:00:06.120 --> 00:00:09.150
Anytime we want to create,
read,
update,

4
00:00:09.180 --> 00:00:11.220
or delete data in a database,

5
00:00:11.250 --> 00:00:14.730
we'll usually do so in the form of SQL queries.

6
00:00:15.180 --> 00:00:19.710
Cql stands for structured query language and it's the standard language use to

7
00:00:19.711 --> 00:00:22.380
communicate with relational databases.

8
00:00:23.520 --> 00:00:28.290
What about Haskell DB?
Each query,

9
00:00:28.291 --> 00:00:32.940
it takes a certain amount of time to compute and ideally we can order our

10
00:00:32.941 --> 00:00:36.840
queries in the most computationally efficient way.

11
00:00:37.290 --> 00:00:42.290
There are several techniques to do this and usually reinforcement learning isn't

12
00:00:42.811 --> 00:00:44.160
considered one of them,

13
00:00:44.400 --> 00:00:49.400
but a recent paper showed that using a deep queue network,

14
00:00:49.650 --> 00:00:54.420
researchers were able to perform queries twice as fast as using standard

15
00:00:54.421 --> 00:00:57.630
techniques.
We'll learn how they did it in this video,

16
00:00:57.631 --> 00:01:02.631
but first we'll need to understand why they used reinforcement learning as

17
00:01:02.941 --> 00:01:05.820
opposed to other more common techniques.

18
00:01:06.000 --> 00:01:10.200
We know that reinforcement learning works great for video games.

19
00:01:10.201 --> 00:01:15.201
Games are the perfect test bed for implementing RL algorithms since they're

20
00:01:15.901 --> 00:01:20.901
constrained environments where time is an element that allows for rapid

21
00:01:21.271 --> 00:01:22.470
experimentation.

22
00:01:22.530 --> 00:01:27.530
The Mark Haub decision process is the mathematical framework of choice for

23
00:01:28.081 --> 00:01:32.340
framing this decision.
Problem that our AI or agent is facing.

24
00:01:32.520 --> 00:01:36.480
It consists of a few variables that define our environment,

25
00:01:36.690 --> 00:01:41.490
our agent,
and how our agent interacts with this environment.

26
00:01:41.940 --> 00:01:43.560
In reinforcement learning,

27
00:01:43.920 --> 00:01:48.920
an agent tries to come up with the best action to take given a state in the

28
00:01:49.411 --> 00:01:54.210
video game Pac man,
the states would be the two D game world we're in.

29
00:01:54.450 --> 00:01:57.720
That includes the surrounding items like enemies,

30
00:01:57.840 --> 00:01:59.580
walls and packed dots.

31
00:01:59.910 --> 00:02:03.360
The action would be moving through this two d space,

32
00:02:03.361 --> 00:02:07.170
which would be going either up,
down,
left or right.

33
00:02:07.440 --> 00:02:09.720
So given the state of our game world,

34
00:02:09.870 --> 00:02:14.730
our agent will need to pick the best action to take in order to maximize

35
00:02:14.731 --> 00:02:15.564
rewards.

36
00:02:15.810 --> 00:02:20.810
We know that eating packed dots gives us positive rewards and getting eaten by a

37
00:02:21.451 --> 00:02:26.451
ghost gives us a negative reward and the possible temper tantrum which we want

38
00:02:26.551 --> 00:02:28.740
to avoid through trial and error.

39
00:02:28.741 --> 00:02:33.741
Our agent will accumulate knowledge of the environment through state action

40
00:02:33.931 --> 00:02:34.764
pairs,

41
00:02:34.980 --> 00:02:39.980
meaning it can tell if there would be a positive or negative reward given a

42
00:02:40.441 --> 00:02:41.910
state action pair.

43
00:02:42.300 --> 00:02:47.220
We can represent this using the Q function where Q stands for quality.

44
00:02:47.640 --> 00:02:51.210
As in it assesses the quality of a given state action pair.

45
00:02:51.480 --> 00:02:56.430
We can actually learn what the optimal Q value will be at any given point,

46
00:02:56.790 --> 00:03:01.150
and this is called Q learning,
uh,
using what's called the bellman equation.

47
00:03:01.300 --> 00:03:06.300
We can write out an equation that relates the value of one state to the value of

48
00:03:06.731 --> 00:03:09.130
another state.
In our environment,

49
00:03:09.580 --> 00:03:14.580
because we are able to relate states across time to each other mathematically

50
00:03:15.220 --> 00:03:17.050
using the bellman equation,

51
00:03:17.380 --> 00:03:22.180
we can use any number of methods to iteratively approximate or Q function,

52
00:03:22.420 --> 00:03:24.880
but in the case of our PAC man environment,

53
00:03:25.000 --> 00:03:28.810
the state action space can get really big.
In fact,

54
00:03:28.811 --> 00:03:33.811
at some point it will no longer be feasible to store all the state action

55
00:03:34.031 --> 00:03:37.810
payers.
Of course,
we could still perform cue learning,

56
00:03:37.990 --> 00:03:41.740
but it'll get harder to approximate the Q function over time.

57
00:03:42.280 --> 00:03:43.630
Luckily for us,

58
00:03:43.660 --> 00:03:48.660
there exists a universal function approximator called a neural network.

59
00:03:49.270 --> 00:03:53.110
If we give it enough input data,
it can learn any function.

60
00:03:53.500 --> 00:03:58.500
If we use a neural network as an agent that predicts the Q value based on the

61
00:03:58.991 --> 00:04:01.150
input state action pair,

62
00:04:01.420 --> 00:04:06.420
then we have a much more tractable solution than storing every possible value

63
00:04:07.151 --> 00:04:08.590
like we did previously.

64
00:04:08.950 --> 00:04:13.950
And to capture all the intricate details of this knowledge present in our queue

65
00:04:14.261 --> 00:04:18.940
table will likely need to add a few hidden layers to our network,

66
00:04:19.150 --> 00:04:21.700
making it a deep neural network.

67
00:04:21.940 --> 00:04:26.860
The extra hidden layers allow the network to internally come up with features

68
00:04:26.980 --> 00:04:30.970
that can help it learn complex functions.
That would have been impossible.

69
00:04:30.971 --> 00:04:32.980
Using a more shallow network.

70
00:04:33.220 --> 00:04:38.220
We can call this whole process deep reinforcement learning and more specifically

71
00:04:38.530 --> 00:04:43.000
deep cue learning.
Now let's bring this theory back to reality.

72
00:04:43.150 --> 00:04:46.060
You and I have neural networks in our head.

73
00:04:46.360 --> 00:04:51.250
Networks of neurons are firing and endlessly different combinations to

74
00:04:51.251 --> 00:04:56.251
approximate functions that help us perform a wide variety of tasks as we go

75
00:04:56.831 --> 00:05:00.400
about our lives and we can consider our reality,
uh,

76
00:05:00.401 --> 00:05:05.260
Mark Haub decision process.
As neural network agents given a state,

77
00:05:05.350 --> 00:05:10.350
we take actions to maximize reward for whatever task we are achieving using our

78
00:05:10.990 --> 00:05:14.500
function approximation capabilities to make predictions.

79
00:05:14.830 --> 00:05:19.540
In that way we can consider our reality deep reinforcement learning.

80
00:05:19.660 --> 00:05:24.040
Anytime we use a neural network to approximate some reinforcement learning

81
00:05:24.041 --> 00:05:28.090
function,
be it a value function,
a policy,

82
00:05:28.420 --> 00:05:32.380
even the model itself,
we can call that deep reinforcement learning.

83
00:05:32.860 --> 00:05:37.840
So how does this apply to the problem of query optimization?
Well,

84
00:05:37.841 --> 00:05:42.841
we know that CQL statements are used to perform a wide variety of tasks related

85
00:05:43.421 --> 00:05:47.800
to the database.
They can update data,
retrieved data,

86
00:05:47.860 --> 00:05:52.090
merged data,
delete data.
Each sequel query has its own function.

87
00:05:52.450 --> 00:05:56.980
Assume we have a database consisting of three tables,
employees,

88
00:05:57.020 --> 00:05:58.820
salaries and taxes.

89
00:05:59.060 --> 00:06:04.060
Let's say we want to calculate the total tax owed by all employees under manager

90
00:06:04.371 --> 00:06:07.460
one we can write out a SQL query that does,

91
00:06:07.461 --> 00:06:12.461
that will compute the tax owed by each employee by selecting their specific

92
00:06:13.161 --> 00:06:15.590
attributes and summing them all up.

93
00:06:15.890 --> 00:06:19.430
This query is going to perform a three relation join.

94
00:06:19.760 --> 00:06:24.760
We can use J to help denote the cost of accessing a base relation.

95
00:06:25.190 --> 00:06:29.570
The cost of each query is the percentage of the total batch cost.

96
00:06:29.870 --> 00:06:34.520
It's the time needed to execute a query.
It's computed different ways,

97
00:06:34.521 --> 00:06:39.521
but almost always takes into account several computation factors such as input,

98
00:06:39.680 --> 00:06:42.350
output,
CPU and communication.

99
00:06:42.680 --> 00:06:47.120
We want to minimize this cost so that we perform our queries as fast as

100
00:06:47.121 --> 00:06:49.820
possible.
Using dynamic programming,

101
00:06:49.821 --> 00:06:54.770
we can iteratively calculate the cost of optimally accessing the three base

102
00:06:54.771 --> 00:06:57.380
relations.
After the first iteration,

103
00:06:57.381 --> 00:07:02.381
we can build off of this information that we previously computed and enumerate

104
00:07:02.600 --> 00:07:04.100
all two relations.

105
00:07:04.400 --> 00:07:08.240
When we compute the best cost to join two relations,

106
00:07:08.450 --> 00:07:13.040
we'll look up the relevant previously computed results and in the third

107
00:07:13.041 --> 00:07:16.640
iteration we'll proceed through the other two relations sets.

108
00:07:16.670 --> 00:07:21.230
Eventually finding the vinyl best costs for joining all three tables.

109
00:07:21.530 --> 00:07:22.311
Once complete,

110
00:07:22.311 --> 00:07:26.660
we'll see that this algorithm has a space and time complexity,

111
00:07:27.110 --> 00:07:29.570
exponential in the number of relations,

112
00:07:29.690 --> 00:07:34.690
which is why it's usually only used for queries between 10 and 20 relations.

113
00:07:35.510 --> 00:07:37.970
When we have more relations than that,

114
00:07:38.090 --> 00:07:41.540
we'll need to use a different query optimization technique.

115
00:07:41.750 --> 00:07:46.400
Instead of solving this join ordering problem using dynamic programming,

116
00:07:46.850 --> 00:07:51.850
what if we formulated this problem as a Mark Cobb decision process and solved it

117
00:07:52.460 --> 00:07:55.460
using reinforcement learning?
If we do that,

118
00:07:55.550 --> 00:07:59.240
the states can be considered the remaining relations to be joint.

119
00:07:59.330 --> 00:08:03.170
The actions would be the valid joints out of the remaining relations.

120
00:08:03.410 --> 00:08:07.880
The next states would be the old remaining relations set with two relations

121
00:08:07.881 --> 00:08:11.240
removed and the resultant joint added.
Lastly,

122
00:08:11.241 --> 00:08:16.241
the reward would be the estimated cost of the new join because we defined these

123
00:08:16.251 --> 00:08:17.870
mark Covidien variables,

124
00:08:18.080 --> 00:08:23.080
we can define a Q function using the bellman equation to describe the longterm

125
00:08:23.300 --> 00:08:27.350
costs of each join and since we've defined a Q function,

126
00:08:27.650 --> 00:08:32.180
we can order joins in a greedy way.
We'd start with the initial query graph,

127
00:08:32.300 --> 00:08:36.920
find the join with the lowest Q value than update the query graph and repeat the

128
00:08:36.921 --> 00:08:37.754
process.

129
00:08:37.910 --> 00:08:42.440
This cue learning algorithm has a computational complexity of n cubed,

130
00:08:42.740 --> 00:08:43.790
although that's high,

131
00:08:43.791 --> 00:08:48.140
that's still much lower than the exponential runtime complexity of dynamic

132
00:08:48.141 --> 00:08:53.050
programming.
In reality though we don't have access to the optimal Q function,

133
00:08:53.051 --> 00:08:56.430
so we need to approximate it.
To do that,

134
00:08:56.460 --> 00:09:01.230
we can use a neural network which would make this deep cue learning to learn the

135
00:09:01.231 --> 00:09:04.920
Q function we need to observe past execution data.

136
00:09:05.370 --> 00:09:10.370
We can use it as training data for our neural network and since we're using a

137
00:09:10.500 --> 00:09:13.410
neural network to represent our Q function,

138
00:09:13.680 --> 00:09:18.680
we need to feed this state and actions into the network as fixed length feature

139
00:09:19.410 --> 00:09:20.243
vectors.

140
00:09:20.340 --> 00:09:24.450
This helps our network perform matrix multiplications gracefully.

141
00:09:24.660 --> 00:09:26.820
As it accepts this kind of format.

142
00:09:27.180 --> 00:09:32.180
We'll use one hot vectors to encode the set of all attributes present in the

143
00:09:32.251 --> 00:09:37.251
query graph and the participating attributes from both the left and right side

144
00:09:37.740 --> 00:09:38.573
of the join.

145
00:09:38.670 --> 00:09:43.230
We'll use a simple two layer fully connected network as our agent and train it

146
00:09:43.231 --> 00:09:47.100
using the standard gradient descent optimization algorithm.

147
00:09:47.430 --> 00:09:51.240
Once trained it will accept a SQL query in plain text,

148
00:09:51.540 --> 00:09:54.780
parse it into an abstract syntax tree form,

149
00:09:54.990 --> 00:09:59.670
feature the tree and use a neural network whenever a candidate join his score.

150
00:09:59.880 --> 00:10:04.880
And because databases are real time used in production environments constantly

151
00:10:05.071 --> 00:10:07.050
being updated.
And changed.

152
00:10:07.380 --> 00:10:12.380
We can periodically retune our network using the feedback from live execution

153
00:10:12.900 --> 00:10:14.940
across all costs models.

154
00:10:14.970 --> 00:10:18.300
Deep Q is competitive with the optimal solution.

155
00:10:18.360 --> 00:10:22.140
Without a priori knowledge of the index structure,

156
00:10:22.500 --> 00:10:27.500
we can safely say that learning based optimizers are more robust than hand

157
00:10:27.691 --> 00:10:32.691
designed algorithms because they can adapt to changes in data workload or cost

158
00:10:33.451 --> 00:10:35.580
models for the largest joints.

159
00:10:35.610 --> 00:10:40.440
Deep wins by up to 10,000 x compared to exhaustive in numeration.

160
00:10:40.860 --> 00:10:43.170
Three points to remember from this video.

161
00:10:43.350 --> 00:10:48.350
Deep reinforcement learning involves using a neural network to approximate

162
00:10:48.660 --> 00:10:51.900
reinforcement learning functions like the Q function.

163
00:10:52.200 --> 00:10:57.030
We can assess the quality or two of state action pairs.

164
00:10:57.240 --> 00:11:01.680
I computing a cute table and cue learning involves approximating the

165
00:11:01.681 --> 00:11:06.570
relationship between state action,
pears and Q values in this table.

166
00:11:06.600 --> 00:11:11.010
Using neural networks,
please subscribe for more programming videos.
And for now,

167
00:11:11.040 --> 00:11:13.550
I've got to take a meeting,
so thanks for watching.

