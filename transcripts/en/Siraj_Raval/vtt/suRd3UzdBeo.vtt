WEBVTT

1
00:00:00.030 --> 00:00:03.360
I start this stream.
When I start this stream,

2
00:00:03.450 --> 00:00:08.280
we're going to do a Kaggle challenge when I started this dream.
Okay,

3
00:00:08.281 --> 00:00:12.870
here we go.
The stream is beginning.
This stream is beginning.
All right guys.

4
00:00:13.110 --> 00:00:17.070
Okay,
here I come.
I'm coming.
I'm coming guys.
I'm coming.

5
00:00:17.640 --> 00:00:22.630
Okay.
Hello world.
It's the Raj and welcome to my live stream.
In this livestream,

6
00:00:22.710 --> 00:00:25.110
I'm going to complete a Kaggle challenge.

7
00:00:25.470 --> 00:00:29.940
The Kaggle challenge that I'm going to complete is called the taxi.

8
00:00:30.540 --> 00:00:33.300
The taxi trip duration challenge is,
this is a,

9
00:00:33.570 --> 00:00:38.570
this is a Kaggle competition with $30,000 worth of prize money.

10
00:00:38.701 --> 00:00:41.040
So it's a,
it's a big deal.
Uh,

11
00:00:41.041 --> 00:00:45.690
and so in this video I'm going to show you how to solve this challenge using a

12
00:00:45.691 --> 00:00:50.370
technique called xg boost,
which I haven't actually talked about before,

13
00:00:50.371 --> 00:00:54.060
surprisingly.
Well.
I talked about it once in my math of intelligence series,

14
00:00:54.061 --> 00:00:58.330
but like that was a while ago.
So what I'm going to do four,

15
00:00:58.380 --> 00:01:02.310
because this video is for my live viewers and from my recorded viewers later on.

16
00:01:02.311 --> 00:01:06.620
So I'm trying to make sure all of the time is used very well.
So,
um,

17
00:01:06.820 --> 00:01:08.840
I'm going to use some amazing tools guys.
I've,
I've,

18
00:01:08.970 --> 00:01:12.300
I'm so excited to show you some tools.
So first of all,
Colab,
by the way,

19
00:01:12.301 --> 00:01:15.540
Google's Colab,
I haven't talked about it.
We're going to talk about that.

20
00:01:15.750 --> 00:01:18.900
We're going to use some amazing libraries.
Uh,
it's gonna be a lot of fun.
Okay,

21
00:01:18.901 --> 00:01:23.890
so I'm just going to say 10 names,
uh,
and then we're going to get started.
Uh,

22
00:01:23.940 --> 00:01:28.060
Alex,
I love you.
Alina Palestinian.
Shivonne punk edge,
Audrey Sabeeth,

23
00:01:28.080 --> 00:01:32.520
three Palestinian Jonas and lacks lifters.
All right,
here we go.

24
00:01:32.521 --> 00:01:34.170
So hello from Hong Kong by the way.
All right,

25
00:01:34.171 --> 00:01:37.380
we've got people from all over the world here.
I'm so excited to do this.
Okay,

26
00:01:37.381 --> 00:01:40.560
so where do we begin?
Let's take a look at this.

27
00:01:40.561 --> 00:01:45.561
So Kaggle is a website that allows you to compete with other data scientists and

28
00:01:47.221 --> 00:01:48.540
you can collect prize money.

29
00:01:48.610 --> 00:01:50.970
It's a great business model that they built over time.

30
00:01:50.971 --> 00:01:54.650
So companies have data sets and they want data scientist who,
you know,

31
00:01:54.720 --> 00:01:57.060
work on these data sets and to complete challenges.

32
00:01:57.061 --> 00:02:01.350
So they give these datasets to Kaggle,
Kaggle supplies the data scientist,

33
00:02:01.351 --> 00:02:03.810
you and us and all of us.
And uh,

34
00:02:03.811 --> 00:02:06.540
they supply some money and then they earned that money through.

35
00:02:06.630 --> 00:02:09.750
It's like a reward for solving the,
you know,
it's a win win situation.

36
00:02:09.751 --> 00:02:13.590
You provide them a prediction,
they'll provide you with some money.
So it's,
it's,

37
00:02:13.591 --> 00:02:15.960
it's a,
it's a lot of,
it's a lot of cool things.
Okay.

38
00:02:15.961 --> 00:02:19.410
So Taggle is awesome.
And,

39
00:02:19.530 --> 00:02:23.820
and what I want to do is to look at this challenge.
$30,000 in prize money.

40
00:02:23.830 --> 00:02:27.390
That's a lot of money.
What they want to do here is,

41
00:02:27.391 --> 00:02:28.530
let's see what they want to do.

42
00:02:28.531 --> 00:02:33.531
What they want to do is predict the time for a ride in New York City.

43
00:02:33.720 --> 00:02:37.990
Okay.
Seems,
seems a simple enough.
Let's look at the data.
What did,

44
00:02:37.991 --> 00:02:41.730
what kind of data do we have?
Your,
okay,
so we have a training,
that CSV file.

45
00:02:42.030 --> 00:02:45.090
We have a test dot CSV file.
Okay.

46
00:02:45.091 --> 00:02:49.230
So we've got two different CSV files and we have a sample submission.
CSP,
Paul,

47
00:02:49.440 --> 00:02:52.250
we have some data fields.
So these are our features.

48
00:02:52.790 --> 00:02:56.970
When we download this Dataset,
we're going to see all of these,
a CSV file,

49
00:02:57.260 --> 00:03:01.930
all of these features in,
in this CSV file.
And the great thing about Kaggle is a,

50
00:03:01.931 --> 00:03:05.170
we can just download it directly just like this.
Okay?
So,

51
00:03:06.390 --> 00:03:08.080
but I'm going to do something even better than that.

52
00:03:08.080 --> 00:03:09.340
So we're going to show you how to,
how to,

53
00:03:09.341 --> 00:03:13.210
how are we going to use this without even needing to download it?
Okay?
So,
um,

54
00:03:13.810 --> 00:03:18.610
what we want to do is predict this feature right here,
trip duration.

55
00:03:18.640 --> 00:03:21.670
That is our label.
That is our y value.

56
00:03:21.970 --> 00:03:24.340
These are going to be our inputs,
right?

57
00:03:24.341 --> 00:03:27.310
Because we know that we want to predict the trip duration.

58
00:03:27.430 --> 00:03:30.250
So dependent on all of these other factors,

59
00:03:30.340 --> 00:03:34.480
these dependent variables or that these independent variables,

60
00:03:34.570 --> 00:03:37.390
we're going to predict the dependent variable,
the trip duration,
okay?

61
00:03:37.391 --> 00:03:38.920
So how are we going to do that?

62
00:03:38.921 --> 00:03:43.270
So normally I would start up a Jupiter notebook and I would start coding and

63
00:03:43.580 --> 00:03:47.050
start installing dependencies locally.
But today,

64
00:03:47.170 --> 00:03:51.040
today I'm going to do this in the browser.
So if you're watching this,

65
00:03:51.041 --> 00:03:54.730
what I want you to do is open up a browser window,
another one,

66
00:03:54.880 --> 00:03:59.880
and type in colab.research.google.com this is going to blow your mind guys.

67
00:04:00.970 --> 00:04:05.110
So this is an internal tool that Google has been using for a while and they open

68
00:04:05.111 --> 00:04:09.550
sourced it and what it allows you to do is build a Jupiter notebooks in the

69
00:04:09.551 --> 00:04:13.180
cloud.
So this,
this is a fully configured python environment.

70
00:04:13.330 --> 00:04:18.330
You can install any dependencies you need and you get 12 hours of GPU training

71
00:04:19.811 --> 00:04:23.140
time on a test on a Kad,
Gpu and Nvidia,

72
00:04:23.350 --> 00:04:26.110
k a GPU for free,

73
00:04:26.740 --> 00:04:31.270
no strings attached for free.
That is amazing.
That is amazing.
Okay,

74
00:04:31.271 --> 00:04:34.750
so let me show you what I mean.
So I literally,
you just go here.
Let me,

75
00:04:34.751 --> 00:04:35.410
let me do it again.

76
00:04:35.410 --> 00:04:40.090
Colab dot research I google.com and I'm going to open a new notebook.
Okay.

77
00:04:40.091 --> 00:04:44.230
It's loading up.
It's,
it's loading up that that that Kuda runtime.

78
00:04:44.231 --> 00:04:47.560
It's got the GPU in the background and I'm going to print out some python.

79
00:04:47.561 --> 00:04:51.400
Hello world.
It's Saroj.
Okay,
let's see what happens here.

80
00:04:52.840 --> 00:04:54.190
Great Pythons working.

81
00:04:54.460 --> 00:04:58.810
So what I'm gonna do is I'm going to solve this challenge from inside of this

82
00:04:59.140 --> 00:05:03.970
colab.
I'm not going to,
uh,
do this locally.
I'm going to do this,
uh,

83
00:05:04.030 --> 00:05:07.060
in,
in the,
in the browser,
right?
All of my dependencies.

84
00:05:07.061 --> 00:05:08.710
I'm going to install those in the browser.

85
00:05:08.860 --> 00:05:12.100
I'm going to do everything in the browser for this Kaggle challenge.
Okay?

86
00:05:12.101 --> 00:05:15.550
So let's,
let's go ahead and get started.
Um,
and uh,

87
00:05:15.610 --> 00:05:18.040
well first let me talk about the tools that I'm going to be using.

88
00:05:18.340 --> 00:05:22.240
So I'm going to be using,
first of all,
the data set we have here are features,

89
00:05:22.480 --> 00:05:25.900
right?
That's one part.
The other part is the compute.
The GPU.

90
00:05:25.901 --> 00:05:30.130
We have that already in the cloud.
Uh,
the other part is the python environment.

91
00:05:30.131 --> 00:05:33.190
We have that in the cloud.
And the last part is our algorithm.

92
00:05:33.190 --> 00:05:36.220
And so we're going to use pandas for data preprocessing.

93
00:05:36.221 --> 00:05:39.010
We know we need to preprocess the CSV file.

94
00:05:39.220 --> 00:05:41.170
We're going to use xg boost for learning,

95
00:05:41.171 --> 00:05:45.370
which is a python library in and of itself.
And we're going to use map,

96
00:05:45.371 --> 00:05:47.830
plot live to visualize that data.
Okay?

97
00:05:47.831 --> 00:05:51.730
So that's how this is gonna go.
Um,

98
00:05:52.720 --> 00:05:54.010
now,
so what are the steps here?

99
00:05:54.011 --> 00:05:57.190
We're going to split it into training and test data.
We're going to do some,
uh,

100
00:05:57.350 --> 00:06:01.640
exploratory data analysis.
I know you guys want to see some Eda.

101
00:06:01.740 --> 00:06:04.820
So we're gonna say we're gonna see what do these features look like,

102
00:06:04.880 --> 00:06:06.080
how long is the trip,

103
00:06:06.200 --> 00:06:09.320
how much overlap between the training and testing data is there.

104
00:06:09.530 --> 00:06:12.380
And then we're going to use xg boost on the model and then we're going to train

105
00:06:12.381 --> 00:06:15.710
it.
Are we to use xg boost on the data?
We're going to train it,

106
00:06:15.740 --> 00:06:18.770
we're going to save it.
Okay.
So before,

107
00:06:18.860 --> 00:06:21.170
so I got to explain how xg boost works,

108
00:06:21.171 --> 00:06:24.050
but let's just start coding and then I'll explain as we go.
Okay.

109
00:06:25.240 --> 00:06:25.720
<v 1>Okay.</v>

110
00:06:25.720 --> 00:06:29.550
<v 0>I cannot marry you.
I am,
I am happily single.</v>

111
00:06:29.551 --> 00:06:34.030
So that's,
that's how that goes.
Thank you though for the offer.
So,
um,

112
00:06:34.290 --> 00:06:37.440
we need to import this data set,
right?

113
00:06:37.441 --> 00:06:41.730
So how are we going to import this Dataset?
Well,
ideally we could just,

114
00:06:42.210 --> 00:06:46.350
you know,
we,
we could import it directly from Kaggle.
So that's one way.

115
00:06:46.351 --> 00:06:47.700
Just using the Cowboy Api.

116
00:06:47.880 --> 00:06:52.410
Another way is for us to download it's is it's for us to uh,
download it,

117
00:06:52.411 --> 00:06:55.680
then upload it to Google drive and import it from there.
So that's what I did.

118
00:06:55.710 --> 00:06:56.820
So I'm going to show you how to do that.

119
00:06:56.821 --> 00:07:01.020
So we're going to have to install one dependency and we can use pip from right

120
00:07:01.021 --> 00:07:05.310
within this.
Um,
this is going to take about 20 minutes more minutes.

121
00:07:05.311 --> 00:07:07.200
So hold your horses.
This is important.

122
00:07:07.201 --> 00:07:09.450
Don't be asking how long this is going to take.
God,

123
00:07:09.820 --> 00:07:11.130
this is some important stuff here.

124
00:07:11.131 --> 00:07:14.250
Go watch a prank video if you don't want to see it as,
no,
I'm just kidding.
Stay,

125
00:07:14.310 --> 00:07:18.780
I'm going to make this fun.
Don't leave,
do not leave.
Okay.
Okay.

126
00:07:19.200 --> 00:07:23.790
So Pi drive is our dependency for us to be able to import data directly from

127
00:07:23.791 --> 00:07:26.040
Google drive.
Okay.
So,
so just remember that part.

128
00:07:26.041 --> 00:07:27.240
I'm going to talk about what I mean.

129
00:07:27.241 --> 00:07:32.241
So I have a few like a sub sub modules from Pi Drive that I'm going to import

130
00:07:33.421 --> 00:07:36.630
like Google off cause we're going to need to authenticate with Google.

131
00:07:36.930 --> 00:07:39.150
I'm going to import the drive of course,

132
00:07:39.151 --> 00:07:42.630
because I need to be able to access Google drive as well.

133
00:07:42.990 --> 00:07:45.540
I need to be able to access colab.

134
00:07:47.650 --> 00:07:47.910
<v 1>Okay.</v>

135
00:07:47.910 --> 00:07:52.740
<v 0>Yes,
Dan's are coming guys.
I know you guys.
I Love Gans you love,
we all love gans,</v>

136
00:07:53.010 --> 00:07:55.560
but there's also an audience that is,

137
00:07:56.040 --> 00:08:01.040
that hasn't had their appetite satisfied when it comes to a Kaggle challenges

138
00:08:02.821 --> 00:08:06.550
and just like machine learning,
not deep learning machine learning.
You know,

139
00:08:06.660 --> 00:08:11.220
I think of course the pointing is amazing.
It's a lot of fun.
Uh,
but,

140
00:08:12.590 --> 00:08:15.440
but uh,
there is a time and place for machine learning.

141
00:08:15.441 --> 00:08:19.310
Sometimes you just want to get a quick and dirty solution out there and you

142
00:08:19.311 --> 00:08:22.410
don't want to have to wait for,
uh,
you know,

143
00:08:22.430 --> 00:08:25.730
processing times that a neural network would take and you don't have a lot of

144
00:08:25.731 --> 00:08:27.800
data.
Uh,
so I think a good,
you know,

145
00:08:27.830 --> 00:08:30.140
exploratory step is to use some machine learning,

146
00:08:30.200 --> 00:08:34.190
specifically xg boost or another ensemble method.
There's bagging,

147
00:08:34.191 --> 00:08:36.620
there's boosting.
We're going to talk about all of this in a second.
Okay.

148
00:08:36.621 --> 00:08:41.120
So that is our Google drive dependency,
um,
list.
Okay.

149
00:08:41.121 --> 00:08:45.170
So now our data dependencies,
data dependencies.
Okay.

150
00:08:45.171 --> 00:08:46.470
So our data dependencies.

151
00:08:46.471 --> 00:08:49.910
And I'm going to answer some questions after I've imported all the dependencies

152
00:08:50.120 --> 00:08:52.430
that I need.
So first of all,
pan does.
Of course,

153
00:08:52.580 --> 00:08:55.220
this is going to allow us to preprocess our dataset.

154
00:08:55.440 --> 00:08:59.610
And of course num Pi is always a go to for any kind of matrix math that we might

155
00:08:59.611 --> 00:09:03.690
have to do in the end.
We also want to be able to visualize this data set.

156
00:09:03.691 --> 00:09:06.320
So we're going to import map,
plot line,
uh,

157
00:09:06.450 --> 00:09:11.450
and I want to be able to also set this parameter format,

158
00:09:13.170 --> 00:09:16.000
plot live such that,
um,

159
00:09:17.010 --> 00:09:20.940
the figure is going to be a certain size.
It's going to fit in my screen.

160
00:09:21.150 --> 00:09:23.910
So I'm going to call,
I'm going to say what is the width and height?

161
00:09:24.150 --> 00:09:28.550
Do I want it to be,
I would say 16 by 10 for this,
uh,
for this screen,
uh,

162
00:09:28.551 --> 00:09:32.220
another data preprocessing library is seaborne.

163
00:09:32.280 --> 00:09:36.930
That's gonna help us visualize a w.
And so psychic learn is a great library.

164
00:09:36.931 --> 00:09:38.970
But what I partake in particular,

165
00:09:39.600 --> 00:09:44.430
what I really liked psych it learn for is uh,
the,
the um,

166
00:09:46.230 --> 00:09:51.230
the training testing split a functionality my gans are showing.

167
00:09:51.361 --> 00:09:54.990
What are you talking about?
My Gans are showing,
yes,
exactly.

168
00:09:55.560 --> 00:09:59.760
A psyche learned up model selection,
import,
train,
test,
split.
I mean this,

169
00:09:59.761 --> 00:10:03.370
this is just like one of those super useful,
um,

170
00:10:04.710 --> 00:10:06.260
functions right here,
t train tests,

171
00:10:06.450 --> 00:10:08.160
cause you're always going to need to be doing that.
Right.

172
00:10:09.030 --> 00:10:12.600
And then of course xg boost because we're going to be using xg boost.

173
00:10:13.110 --> 00:10:16.530
And then,
uh,
what else do I need?
What else do I need?
What else do I need?

174
00:10:16.531 --> 00:10:19.650
I think I need to get,
um,

175
00:10:20.040 --> 00:10:20.873
<v 1>okay.</v>

176
00:10:22.020 --> 00:10:25.330
<v 0>Make sure that Matt Paul live is going to be inline.
Uh,</v>

177
00:10:25.380 --> 00:10:29.340
I need to make sure that it fits inside of my screen.

178
00:10:30.080 --> 00:10:30.550
<v 1>Okay.</v>

179
00:10:30.550 --> 00:10:34.510
<v 0>Oh yeah,
I can make a much bigger fun.
How about that?
That's much better,
isn't it?</v>

180
00:10:34.511 --> 00:10:38.320
Thank you for mentioning that.
Okay,
so,
um,

181
00:10:40.420 --> 00:10:44.470
dot.
Unicode,
uh,
minus,

182
00:10:45.070 --> 00:10:48.910
I make sure that that's false.
So that fits in to my,
uh,

183
00:10:51.060 --> 00:10:54.000
browser.
Okay.
Um,

184
00:10:54.720 --> 00:10:55.120
<v 1>okay.</v>

185
00:10:55.120 --> 00:10:59.330
<v 0>Yeah,
I think I
zoom it's,</v>

186
00:10:59.340 --> 00:11:03.000
it's zoomed in guys.
All right.
Okay.
Okay.
Okay.
Okay.
Okay.
Okay.

187
00:11:04.620 --> 00:11:05.453
Um,

188
00:11:06.030 --> 00:11:09.810
so now what I'm gonna do is I'm going to authenticate.

189
00:11:09.811 --> 00:11:12.110
So I've imported my dependencies.
Let me go ahead and compile that.

190
00:11:16.520 --> 00:11:20.330
Matt [inaudible] live is not fun,
right?
It's Matt plot line.
That's what it was.

191
00:11:31.350 --> 00:11:34.170
Okay.
So now I've done that and um,

192
00:11:36.360 --> 00:11:39.060
now that I've imported that,
I'm going to authenticate.

193
00:11:39.061 --> 00:11:43.140
So now it's time to authentic case.
I'm going to say authenticate me,
my friend,

194
00:11:43.141 --> 00:11:47.670
authenticate user,
and uh,

195
00:11:47.910 --> 00:11:51.990
create a new authentication object using Google off.
Uh,

196
00:11:51.991 --> 00:11:54.850
this is just the authentication flow.
I've got my credential.

197
00:11:54.851 --> 00:11:57.040
So as long as you're,
the great thing about,
you know,

198
00:11:57.100 --> 00:12:00.760
using colab is as long as you're logged into Google,
which I always am,

199
00:12:01.150 --> 00:12:05.500
it's going to be very easy to just authenticate because it's already got your,

200
00:12:05.790 --> 00:12:08.710
uh,
off details.
And lastly,

201
00:12:08.711 --> 00:12:13.180
I'm gonna pull my drive from Google drive jeep off.

202
00:12:13.510 --> 00:12:16.540
Okay?
So let me authenticate then to Kate.

203
00:12:17.650 --> 00:12:17.990
<v 1>Okay?</v>

204
00:12:17.990 --> 00:12:18.970
<v 0>All right.</v>

205
00:12:22.750 --> 00:12:26.740
Do not be sleeping right now.
Okay?
I don't care where you are.
Do not be sleeping.

206
00:12:28.410 --> 00:12:29.070
All right?

207
00:12:29.070 --> 00:12:34.070
So I've authenticated and now I want to access my data.

208
00:12:38.370 --> 00:12:42.930
So where am I going to find this data?
Okay,

209
00:12:43.050 --> 00:12:45.780
I think what I'm going to do is

210
00:12:46.770 --> 00:12:47.603
<v 1>yeah.</v>

211
00:12:47.740 --> 00:12:48.790
<v 0>Say</v>

212
00:12:52.030 --> 00:12:54.190
<v 2>MMM,</v>

213
00:12:54.500 --> 00:12:59.270
<v 3>open notebook.
I have this one.
What does this one,</v>

214
00:13:00.540 --> 00:13:05.320
Dah,
Dah,
Dah,
Dah,
Dah,
Dah,
Dah.
No,
this is nothing.
Open.
Another notebook.

215
00:13:05.330 --> 00:13:09.140
What else would I have here?
What's in here?

216
00:13:10.440 --> 00:13:15.090
<v 0>Nothing of course.
Okay.
So I need to import some data that I have.</v>

217
00:13:15.091 --> 00:13:18.660
So what I'm going to do is I'm going to go to,
um,

218
00:13:19.620 --> 00:13:19.780
<v 1>okay,</v>

219
00:13:19.780 --> 00:13:24.520
<v 0>my drive account,
drive.google.com.
Make sure it's all good in there.</v>

220
00:13:24.521 --> 00:13:29.510
Their tickets.
Your recent,
uh,
what do I got here?
Okay,

221
00:13:29.511 --> 00:13:31.490
so test and train dot CSV.

222
00:13:31.491 --> 00:13:35.720
So what I'm going to do is I'm going to get the shareable link for train dot
CSV.

223
00:13:36.630 --> 00:13:36.950
<v 1>Okay.</v>

224
00:13:36.950 --> 00:13:41.460
<v 0>Okay.
So here's my train dot CSV and here's the ID for it.</v>

225
00:13:41.461 --> 00:13:43.260
So train the,
here's the ids.

226
00:13:43.261 --> 00:13:48.150
These are both CSV files that I import it.
And then here's test,

227
00:13:50.260 --> 00:13:51.093
<v 1>okay?</v>

228
00:13:51.150 --> 00:13:55.110
<v 0>Uh,
where's test test dot CSV right there.
Get sharable link.</v>

229
00:13:55.730 --> 00:13:56.563
<v 3>Okay,</v>

230
00:13:56.910 --> 00:13:58.290
<v 0>so here's test dot CSV.</v>

231
00:13:58.710 --> 00:14:03.350
Now I have both of these ids for both of my datasets.
Now how did I do it?

232
00:14:03.360 --> 00:14:08.070
So import a CSV,
Google colab from Google drive.

233
00:14:08.280 --> 00:14:13.260
So now you're watching me look at stack overflow to remember how to do this.

234
00:14:14.010 --> 00:14:14.710
Okay,

235
00:14:14.710 --> 00:14:16.960
<v 3>so Dah,
Dah,
Dah,
Dah.
How did we do this?</v>

236
00:14:17.980 --> 00:14:19.870
<v 0>We have to say,</v>

237
00:14:21.610 --> 00:14:22.630
<v 3>MMM,</v>

238
00:14:27.110 --> 00:14:29.600
<v 0>how did we do this train and test</v>

239
00:14:31.630 --> 00:14:36.610
<v 3>[inaudible] I did all that.
Uh,</v>

240
00:14:36.670 --> 00:14:38.260
right.
Oh,
Gotcha.

241
00:14:39.140 --> 00:14:44.120
<v 0>Right,
right.
Okay.
Thank you.
Google drive or a person.
And then once I do that,</v>

242
00:14:44.121 --> 00:14:48.440
then I can say,
you know,
import pandas and read it.

243
00:14:50.280 --> 00:14:52.130
Okay?
So what I'll say

244
00:14:52.130 --> 00:14:54.260
<v 1>is a trained,
downloaded,</v>

245
00:14:54.470 --> 00:14:59.360
and now I have got my training data and here's the ID of it.
Okay,

246
00:14:59.720 --> 00:15:04.720
now here is my testing data and here is the idea of it.

247
00:15:05.840 --> 00:15:09.350
Okay?
Hopefully this works.
You know,
I think it's going to work.

248
00:15:09.830 --> 00:15:12.310
Now I'm going to,
I already imported pandas.

249
00:15:12.320 --> 00:15:16.070
So now I can say here's my training data and now print,

250
00:15:16.840 --> 00:15:17.673
uh,

251
00:15:18.210 --> 00:15:20.030
<v 0>DF dot train</v>

252
00:15:22.720 --> 00:15:23.710
<v 1>dot head.</v>

253
00:15:24.960 --> 00:15:26.730
<v 0>Uh,
let me just see if that works.</v>

254
00:15:32.140 --> 00:15:34.450
I hope that works.
I think that's gonna work.

255
00:15:37.880 --> 00:15:41.810
It's importing it from Google drive.
It's converting it into a panda data frame.

256
00:15:42.200 --> 00:15:46.070
And then,
um,
once it's a pandas data frame,

257
00:15:46.400 --> 00:15:49.040
then I can say,
okay,
now take the F.
Dot.

258
00:15:49.041 --> 00:15:53.870
Train and print the head and like print a random row from it.

259
00:15:54.830 --> 00:15:59.300
Yes.
Okay,
good.
Good,
good,
good.
Now,
um,

260
00:15:59.780 --> 00:16:03.470
the F.
Dot.
Test Equals Penn does dot test CSV.

261
00:16:04.430 --> 00:16:08.360
And that is test dot CSV and make sure I've got both.

262
00:16:08.480 --> 00:16:10.610
I've got to book the training data.
I've got both the testing data.

263
00:16:10.660 --> 00:16:14.380
So let's look at this data sets.
Okay.
So here is our data set.

264
00:16:14.381 --> 00:16:16.330
We've loaded it into pandas.

265
00:16:16.530 --> 00:16:17.363
<v 1>MMM.</v>

266
00:16:18.820 --> 00:16:21.760
<v 0>Oh,
we see that it's got to pick up longitude to pick up latitude,</v>

267
00:16:21.761 --> 00:16:24.880
a dropoff longitude.
These are map coordinates.
Okay.

268
00:16:24.881 --> 00:16:27.790
So how can we plot this out?
Right?
This is,

269
00:16:27.820 --> 00:16:32.710
this is map data latitudes and longitudes,
or x,
y coordinates on a tutee a graph.

270
00:16:32.830 --> 00:16:37.300
So we could even graph a lot of this out,
right?
So let's,
let's check this out.

271
00:16:37.540 --> 00:16:38.300
Okay.

272
00:16:38.300 --> 00:16:39.133
<v 1>MMM.</v>

273
00:16:40.000 --> 00:16:43.330
<v 0>All right.
So I've done that.
Now I want to do some data preprocessing.</v>

274
00:16:43.360 --> 00:16:45.910
So let's do some data preprocessing.
Now that I've,
uh,

275
00:16:47.360 --> 00:16:48.180
<v 1>yeah,</v>

276
00:16:48.180 --> 00:16:52.770
<v 0>now that I've done this.
Okay.
So first of all,
what w what is the,
let's see,</v>

277
00:16:52.800 --> 00:16:57.090
what can we do here?
Um,
so we're trying to predict the duration of a trip,
right?

278
00:16:57.091 --> 00:17:01.380
So what if we visualized,
uh,

279
00:17:01.470 --> 00:17:04.470
what the average duration was?
How about let's,
let's try that.
Let's,

280
00:17:04.471 --> 00:17:09.320
let's try that.
Let's try that.
So,

281
00:17:09.500 --> 00:17:14.340
oh,
is it not?
Test Dot CSV.
Oh,
thank you.

282
00:17:14.370 --> 00:17:15.203
Yup.
There we go.

283
00:17:16.810 --> 00:17:17.320
<v 1>Okay.</v>

284
00:17:17.320 --> 00:17:20.350
<v 0>That's my wizards to letting me know what the deal is.
Okay,</v>

285
00:17:20.351 --> 00:17:24.640
so let's visualize how long was the trip?
Let's,
that's our first part.

286
00:17:24.641 --> 00:17:29.530
How long is the average trip?
So what can we do to predict what the average trip?

287
00:17:29.560 --> 00:17:33.580
So we have our training data and what we can do is we can say,

288
00:17:33.610 --> 00:17:37.300
let's see that trip duration,
uh,
feature,

289
00:17:37.330 --> 00:17:42.250
let's use that trip duration feature and well,
use numb pies,

290
00:17:42.280 --> 00:17:46.480
log function to con to regularize that feature.

291
00:17:46.481 --> 00:17:50.880
So it's going to be easier to visualize,
um,
as a logarithm.

292
00:17:51.240 --> 00:17:55.260
Um,
let's,
let's add one just to make sure that it's not a

293
00:17:57.130 --> 00:18:01.720
overshooting the graph.
Um,
and now we're going to create this,
uh,
Matt,
uh,
this,

294
00:18:01.870 --> 00:18:04.090
this plot.
So it's going to be,
first of all,

295
00:18:04.091 --> 00:18:08.740
I want a histogram of the log trip duration.
Okay.

296
00:18:08.741 --> 00:18:10.660
So the log trip

297
00:18:13.330 --> 00:18:18.330
duration and I want to see what those values are.

298
00:18:20.200 --> 00:18:24.580
So I'm going to say,
well,
what's my ex label?
My Ex label is going to be,

299
00:18:24.640 --> 00:18:28.750
um,
what's,
what are,
what are we measuring?
So the log of the trip duration,

300
00:18:28.751 --> 00:18:31.230
which we've,
which we've already,
um,

301
00:18:34.460 --> 00:18:38.270
computed.
Okay.
That didn't,
the log of the trip duration.

302
00:18:38.271 --> 00:18:43.050
I think that's good.
I think that works.
Yeah.
Blogger,

303
00:18:43.051 --> 00:18:45.890
the trip duration.
Then we're going to have the why label B,

304
00:18:45.891 --> 00:18:50.870
the number of training records,
like the,
not the number of data points.

305
00:18:50.871 --> 00:18:54.140
So we want to see the log of the trip duration and let's see if that shows,

306
00:18:54.620 --> 00:18:58.460
it's definitely gonna have an error in a second.
Okay.
What is the syntax here?

307
00:18:58.820 --> 00:19:00.200
The syntax error,

308
00:19:01.330 --> 00:19:02.163
<v 3>uh,</v>

309
00:19:02.540 --> 00:19:07.280
<v 0>is,
where is it?
Line two.
Oh,
Gotcha.
Okay.
Let's see what's,</v>

310
00:19:07.281 --> 00:19:10.790
what's what the deal is here.
Dot.
Values.
Oh,
this,

311
00:19:11.480 --> 00:19:14.360
<v 3>right,
right.
Okay.
Let's try it again.</v>

312
00:19:15.950 --> 00:19:16.700
Okay.

313
00:19:16.700 --> 00:19:21.350
Now my history ground I've got right.

314
00:19:22.140 --> 00:19:22.973
Okay.

315
00:19:23.350 --> 00:19:26.860
<v 0>Now train is not defined.</v>

316
00:19:29.870 --> 00:19:31.040
<v 3>Okay.</v>

317
00:19:35.010 --> 00:19:38.320
<v 0>Right.
Oh,
D F train.
DF train.
That's right.
It was not,</v>

318
00:19:38.370 --> 00:19:40.500
not train D F train.

319
00:19:44.530 --> 00:19:45.363
<v 1>Okay.</v>

320
00:19:45.670 --> 00:19:47.690
<v 0>What do mean train is not the foe,
right?
The F train.</v>

321
00:19:52.300 --> 00:19:53.133
<v 3>Yeah.</v>

322
00:19:54.670 --> 00:19:56.830
<v 0>What do you mean it's not just like,
oh,
I keep using it.
Right.</v>

323
00:19:56.860 --> 00:20:01.760
So every time I use it,
yes.
Okay.
Map,

324
00:20:01.761 --> 00:20:05.840
plot,
live,
check this out,
check this out.

325
00:20:06.140 --> 00:20:09.930
So it seems like there are all kinds,
there's,

326
00:20:09.980 --> 00:20:11.760
there is this average,
right?

327
00:20:11.930 --> 00:20:16.130
Like there is this average trip duration that they all kind of,
um,

328
00:20:16.670 --> 00:20:18.020
there's this,
there's a distribution,

329
00:20:18.080 --> 00:20:20.530
there's a distribution of which it seems like the,

330
00:20:20.531 --> 00:20:24.470
the median is about 6.4.
Um,

331
00:20:24.620 --> 00:20:29.110
and then it's all around there.
So generally the data all seems to be,
uh,

332
00:20:29.360 --> 00:20:31.600
in the,
in the same,
um,

333
00:20:32.220 --> 00:20:32.410
<v 1>yeah.</v>

334
00:20:32.410 --> 00:20:36.700
<v 0>And the same like a vector of values,</v>

335
00:20:36.701 --> 00:20:38.860
like set of values.
Okay.
So,

336
00:20:39.640 --> 00:20:39.980
<v 3>okay.</v>

337
00:20:39.980 --> 00:20:44.880
<v 0>Okay.
Makes Sense.
Makes Sense.
Um,
what else can we do here?
Oh,
you know,</v>

338
00:20:44.881 --> 00:20:48.400
we'll be cool is if we could just visualize all of as like,
uh,

339
00:20:48.430 --> 00:20:51.850
how much overlap there is between the training and the testing data,
right?

340
00:20:51.851 --> 00:20:52.990
So we've got training data,

341
00:20:53.140 --> 00:20:57.220
we've got testing data and we don't want the testing data to be too similar to

342
00:20:57.221 --> 00:21:01.040
the training data,
right?
Because then there,
if our data,
if our model is,

343
00:21:01.070 --> 00:21:04.930
is over fit,
we're not going to be able to know how to,
how to prevent that,
right?

344
00:21:04.931 --> 00:21:09.750
So we want to make sure that there's not a lot of overlap.
So let's sample,

345
00:21:09.770 --> 00:21:13.590
uh,
let's say for like 10,000 data points,
we're going to,
um,

346
00:21:18.120 --> 00:21:22.560
oh,
let's say I'm going to [inaudible] so,

347
00:21:22.900 --> 00:21:27.090
so let me tell you what I'm about to do is plot out the training and the testing

348
00:21:27.091 --> 00:21:30.660
data in terms of their latitudes and longitudes.

349
00:21:30.780 --> 00:21:32.250
And let's just see what this looks like.
Right?

350
00:21:32.251 --> 00:21:35.220
So remember we don't have a map of New York City.
We,

351
00:21:35.221 --> 00:21:39.930
all we have are these data points.
And so if there is enough data and you know,

352
00:21:39.931 --> 00:21:43.590
if there is enough data,
if we don't,
if we were to plot all out,

353
00:21:44.360 --> 00:21:47.480
if we were to plot out all of these little latitude and longitude dots,

354
00:21:48.060 --> 00:21:51.990
theoretically we could make a map of New York City just from those,

355
00:21:51.991 --> 00:21:55.830
from that data.
So let's,
let's try to do that.
So,
mmm,

356
00:21:55.980 --> 00:22:00.870
let's have our law are a variable for our longitude and then we're going to set

357
00:22:00.871 --> 00:22:04.740
a border value.
Okay.
Um,
so I'm just going to like pick some

358
00:22:06.240 --> 00:22:10.560
negative 74,
negative 74.
I don't,
you know,

359
00:22:10.860 --> 00:22:15.510
um,
we're going to have a latitude,
uh,
which is going to be

360
00:22:17.130 --> 00:22:22.070
a little less than that.
40,
40.
Yeah.
Maybe it could be negative 75,
negative 75.

361
00:22:22.100 --> 00:22:24.960
I don't know.
We're,
we'll,
we'll,
we'll fix this later.
Um,

362
00:22:24.980 --> 00:22:27.170
and then we're going to have a plot.

363
00:22:27.171 --> 00:22:29.270
So we have our figure and we have our access.

364
00:22:29.271 --> 00:22:33.980
So using the sub plots function of map,
plot line,
we can,
um,

365
00:22:35.350 --> 00:22:38.650
create this.
Okay.

366
00:22:40.660 --> 00:22:44.170
Of course you will.
Yeah.
It will look like a map.
Of course.
Uh,

367
00:22:46.420 --> 00:22:51.290
number of columns is too,
uh,
don't want to share this.
Yes.

368
00:22:53.840 --> 00:22:58.640
Share the x value,
share the y values and um,

369
00:23:00.140 --> 00:23:04.480
right.
So,
okay.

370
00:23:07.460 --> 00:23:11.510
Okay.
I want to take a second to just answer any questions.
Um,

371
00:23:11.540 --> 00:23:14.930
so ask any questions you'd like and the,
then the chat.
I'm going to answer those.

372
00:23:15.110 --> 00:23:17.200
Meanwhile while I code this.
So I'm going to be like looking,

373
00:23:17.670 --> 00:23:20.700
I questions scatter plot.

374
00:23:20.850 --> 00:23:22.760
You think the true def dot train

375
00:23:24.270 --> 00:23:26.290
pick up a longitude,

376
00:23:27.230 --> 00:23:32.230
dark values up to n for as many as there are a defined and previously.

377
00:23:33.610 --> 00:23:35.260
And um,

378
00:23:41.060 --> 00:23:42.110
now

379
00:23:44.120 --> 00:23:48.830
the f train pickup,
longitude,

380
00:23:49.750 --> 00:23:51.860
pickup latitude

381
00:23:54.140 --> 00:23:57.680
doc values.
Again,
so up to,

382
00:24:00.400 --> 00:24:03.460
okay.
So one question,
one great question is why did I use the log?

383
00:24:03.461 --> 00:24:08.461
So logarithms are a great way for us to measure the relationship,

384
00:24:09.640 --> 00:24:12.850
uh,
between,
um,
different points in a dataset.

385
00:24:12.910 --> 00:24:17.290
So logarithms follow a curve and sometimes,

386
00:24:17.530 --> 00:24:20.350
uh,
data sets can be very,
very,
very large.

387
00:24:20.351 --> 00:24:25.351
So on large timescales we won't be able to see patterns that exist between them

388
00:24:25.600 --> 00:24:29.770
because they're so sparse,
spread a po for so far spread apart.

389
00:24:29.950 --> 00:24:31.680
So when we use the logarithm,

390
00:24:31.780 --> 00:24:34.540
it kind of condenses that data and we can see it in a,

391
00:24:34.810 --> 00:24:38.080
we could see it in a better way.

392
00:24:38.530 --> 00:24:42.490
We can see patterns that we wouldn't exist if we didn't use the log.

393
00:24:42.850 --> 00:24:44.620
And there's a lot of reasons for that.
But,
um,

394
00:24:48.360 --> 00:24:50.760
are you gonna use tensorflow?
No,
I'm going to use xg boost,

395
00:24:50.761 --> 00:24:54.000
which has its own library.
I'm not always gonna use tensorflow guys.

396
00:24:54.030 --> 00:24:57.180
We want to try different libraries.
Right?
There's other things out there.

397
00:24:57.181 --> 00:25:01.440
Pi Torch is another one.
Uh,
tensorflow is amazing though.
Okay.
So,
um,

398
00:25:02.970 --> 00:25:04.830
what kind of one more question.
What kind of,

399
00:25:05.850 --> 00:25:06.190
<v 1>okay,</v>

400
00:25:06.190 --> 00:25:11.190
<v 0>what kind of RN end model will be suitable for c plus plus Code Generation?</v>

401
00:25:11.950 --> 00:25:16.450
Uh,
first of all,
wow,
that's quite an undertaking.

402
00:25:16.451 --> 00:25:18.610
C plus plus cogeneration.
Uh,

403
00:25:21.040 --> 00:25:24.250
you would need a lot of code.
Get hubs.
Api is great for you.

404
00:25:24.251 --> 00:25:28.710
Just pull a bunch of c plus plus code.
Now how would we do this?
You would,
um,

405
00:25:29.620 --> 00:25:33.670
be predicting the next,
uh,
so character level RNN generation.

406
00:25:33.700 --> 00:25:38.380
Andre Carpathians has a great blog post on this.
Um,
you know,

407
00:25:38.830 --> 00:25:43.390
I think this has been tried before like recurrent nets for code generation and

408
00:25:43.480 --> 00:25:47.770
the results are not good.
What I would do is actually try something,
you know,

409
00:25:47.830 --> 00:25:52.090
out there like a convolutional network for text generation.

410
00:25:52.690 --> 00:25:54.190
Not many people have tried that.

411
00:25:54.191 --> 00:25:58.240
Convolutional nets have been used for text classification,
but text generation,

412
00:25:58.241 --> 00:26:03.241
not so much now specifically I would use a type of convolutional network that no

413
00:26:03.941 --> 00:26:06.220
one has ever tried.
So probably like,

414
00:26:07.270 --> 00:26:07.810
<v 1>okay,</v>

415
00:26:07.810 --> 00:26:08.650
<v 0>oh,
you know what,</v>

416
00:26:08.950 --> 00:26:13.950
I use a DC Gan deep convolutional generative adversarial network search a DC Gan

417
00:26:14.620 --> 00:26:17.710
DC gangs have been mostly used for images like Pokemon and things like that.

418
00:26:17.890 --> 00:26:21.130
But I don't think anybody's tried to use that for text generation.

419
00:26:21.131 --> 00:26:25.540
So I would say try that.
Okay.
My Internet speed is amazing.

420
00:26:25.690 --> 00:26:30.400
That's okay.
Blog is magic.
Got It.
So I'm going to,

421
00:26:31.300 --> 00:26:31.640
<v 1>yeah,</v>

422
00:26:31.640 --> 00:26:33.920
<v 0>it's explained that better next time.
Okay.
All right.</v>

423
00:26:33.921 --> 00:26:36.890
Where was I values

424
00:26:38.000 --> 00:26:38.833
<v 1>and</v>

425
00:26:39.490 --> 00:26:41.680
<v 0>what color is going to be?
The color is going to be blue.</v>

426
00:26:41.681 --> 00:26:45.090
So let's have this be blue.
The first access is going to be blue.

427
00:26:47.780 --> 00:26:48.620
<v 3>Uh,</v>

428
00:26:51.520 --> 00:26:54.040
<v 0>one,
uh,
there's going to be one dimension.</v>

429
00:26:54.460 --> 00:26:58.960
The label's going to be the training data and the learning rates is going to be

430
00:26:58.961 --> 00:27:03.010
0.1.
Now that's for that.
And I'm going to do the same.

431
00:27:05.010 --> 00:27:06.150
Oh,
we got a question here.

432
00:27:06.180 --> 00:27:10.410
What made you switch to a nonprofit model for school of Ai?
Great question.

433
00:27:10.980 --> 00:27:11.820
I

434
00:27:12.680 --> 00:27:12.920
<v 3>okay.</v>

435
00:27:12.920 --> 00:27:16.130
<v 0>Did not start this to be charging people</v>

436
00:27:17.120 --> 00:27:17.953
<v 3>mmm.</v>

437
00:27:18.700 --> 00:27:23.410
<v 0>For access to AI.
That just goes against why I even started this and I,</v>

438
00:27:23.590 --> 00:27:25.720
you know,
there's this idea in silicon valley,

439
00:27:25.750 --> 00:27:29.140
you could think about silicon valley is a state of mind of,
of,

440
00:27:29.260 --> 00:27:32.410
of scaling for the sake of scaling is good.

441
00:27:32.500 --> 00:27:35.290
Like that is a good value to have.
So when I,

442
00:27:35.350 --> 00:27:38.320
when I was thinking about ways to scale,
I thought,
well,
I'll just charge,

443
00:27:38.440 --> 00:27:42.850
you know,
students and then I'll use that money to hire people and just scale.

444
00:27:43.090 --> 00:27:47.500
But I am just fine.
I'm doing just fine.
I don't need to do that.
You know,

445
00:27:47.501 --> 00:27:51.390
I tried it.
Uh,
you know,
it was an experiment.
You know,
I,
I mean,

446
00:27:51.440 --> 00:27:52.900
I made some money doing that,

447
00:27:53.170 --> 00:27:57.750
but there are better ways to make money and um,
yeah,
so,

448
00:27:58.560 --> 00:28:03.510
and yeah,
you,
that's why,
that's why,
okay.
That's all I'm going to say,

449
00:28:04.920 --> 00:28:08.820
but I'm just,
I'm just getting started really,
like this is not,
this is just a,

450
00:28:09.660 --> 00:28:14.580
this is,
this was nothing.
Okay.
Acts as one pickup latitude.
Color.
Oh,
this,

451
00:28:14.581 --> 00:28:16.140
this color.
I want it to be green though.

452
00:28:16.380 --> 00:28:20.130
And this is going to be the latitude,
right?

453
00:28:20.170 --> 00:28:21.660
Pick up a latitude.

454
00:28:22.910 --> 00:28:23.610
<v 1>Yeah.</v>

455
00:28:23.610 --> 00:28:25.620
<v 0>Okay.
And that's the value.</v>

456
00:28:26.040 --> 00:28:27.000
<v 3>And then</v>

457
00:28:29.850 --> 00:28:34.100
<v 0>now let's have a legend.
No,
let's not have a legend.</v>

458
00:28:34.101 --> 00:28:37.910
Let's just show this plot.
Now let's just show this plot.
What do we got here?

459
00:28:38.330 --> 00:28:42.270
Of course it's gonna be an error.
What's our,
oh my God.
Wow.

460
00:28:42.500 --> 00:28:43.910
Oh my God.

461
00:28:44.450 --> 00:28:44.630
<v 1>Okay.</v>

462
00:28:44.630 --> 00:28:49.160
<v 0>Oh my God,
guys,
this is the school of Ai.</v>

463
00:28:49.161 --> 00:28:53.630
By the way.
What you are experiencing is the school of Ai Right now.
Um,

464
00:28:53.780 --> 00:28:54.950
it's a state of mind.

465
00:28:55.100 --> 00:28:59.330
We are the school of Ai and soon we're going to have meetups across the world.

466
00:28:59.331 --> 00:29:01.370
I'm going to have point people in different cities.

467
00:29:01.730 --> 00:29:06.320
The plan is just getting started.
By the way,
this is just,
this is just play time.

468
00:29:06.321 --> 00:29:07.250
You know what I mean?
Like this.

469
00:29:07.251 --> 00:29:12.251
This is just like 0.01% of where we're going with this.

470
00:29:14.180 --> 00:29:18.410
So just just think of it as an idea,
a state of mind.
It's a way of life really.

471
00:29:18.411 --> 00:29:22.890
The school have a school of AI as a way of life,
of optimism,
of uh,

472
00:29:23.000 --> 00:29:26.870
of swagger,
of confidence,
of,

473
00:29:27.230 --> 00:29:31.130
of a belief in the power of Ai to do good for the world.
Checkout this.
Um,

474
00:29:32.300 --> 00:29:32.770
<v 1>yeah,</v>

475
00:29:32.770 --> 00:29:36.940
<v 0>the next course coming out in a few weeks.
Guys,
listen,
I've got this on block.</v>

476
00:29:36.941 --> 00:29:40.990
Just let me finish this Kaggle thing and don't be asking me about school of AI

477
00:29:41.200 --> 00:29:44.650
now,
but thank you for asking about it.
I love you guys.
Okay,
so check this out.

478
00:29:44.830 --> 00:29:48.700
We just made,
we literally made Manhattan look at this map.

479
00:29:48.840 --> 00:29:52.090
So on the left is our,
our training data on the right is our testing data.

480
00:29:52.240 --> 00:29:56.500
And if we look at it,
unless like,
you know,
general way,
they look pretty similar.

481
00:29:56.710 --> 00:29:57.820
They look pretty similar,

482
00:29:58.030 --> 00:30:02.590
but that is a map of Manhattan based only on the trip duration of all of these

483
00:30:02.591 --> 00:30:05.770
rides.
Okay.
So that's amazing by the way.
So let's just keep going here.

484
00:30:06.790 --> 00:30:10.750
What else do I want to do in terms of,
uh,
data analysis?
Uh,

485
00:30:10.900 --> 00:30:15.220
let's just say that's it.
Um,
what else are we going to do here?

486
00:30:15.221 --> 00:30:20.120
What are we going to do here?
Okay,

487
00:30:20.121 --> 00:30:22.760
so now I'm going to build this model so I could do more,

488
00:30:22.761 --> 00:30:27.470
but I really just want to get to building this,
this model.

489
00:30:27.471 --> 00:30:30.530
So now come training the model part.
Okay.

490
00:30:30.531 --> 00:30:34.670
So remember how I said x train?
Um,

491
00:30:34.970 --> 00:30:39.860
let's just call it x train x test.
Why train?
Why test.

492
00:30:40.270 --> 00:30:43.670
Remember I said psychic learn has this amazing function.
Train,
test,
split.

493
00:30:43.671 --> 00:30:47.630
Here we go.
We're using it right now.
How easy was that?
We're just going to say,

494
00:30:48.020 --> 00:30:48.853
um,

495
00:30:54.590 --> 00:30:59.390
I need a list of feature name.
So feature names.
Dot Values.

496
00:31:00.680 --> 00:31:03.710
Um,
why?
Uh,
it's,

497
00:31:03.730 --> 00:31:06.560
I'm gonna explain what these variables are in a second.

498
00:31:10.310 --> 00:31:11.143
<v 1>Okay.</v>

499
00:31:12.260 --> 00:31:17.120
<v 0>Test size is 0.2.
Random state is 19.</v>

500
00:31:18.680 --> 00:31:21.620
I don't know.
Okay.
Let me talk about what I,
what I'm,

501
00:31:21.680 --> 00:31:24.020
what I'm doing here by the way.
Okay.
So,
um,

502
00:31:27.090 --> 00:31:30.180
so we need our feature names.
So our feature names,

503
00:31:30.270 --> 00:31:34.440
the names of all of our features,
we're going to pull from our training columns.

504
00:31:34.441 --> 00:31:36.180
So these are,
these are all of our features.

505
00:31:36.470 --> 00:31:38.190
Now we're also going to put a y value.

506
00:31:38.191 --> 00:31:42.180
The y value is going to be the trip duration.

507
00:31:42.210 --> 00:31:46.430
So we're going to use the log for this.
Um,

508
00:31:46.530 --> 00:31:50.130
and so I'm going to say trip duration.
And this is a value right here.

509
00:31:50.730 --> 00:31:55.260
And that's going to be yes,
the F train.
Thank you.

510
00:31:56.000 --> 00:32:00.030
Do you have train?
We've got people on it.
That's what I like to see.

511
00:32:00.031 --> 00:32:03.990
You guys are amazing.
Uh,
trip duration

512
00:32:06.430 --> 00:32:07.900
doc values.

513
00:32:09.500 --> 00:32:10.030
<v 1>Okay.</v>

514
00:32:10.030 --> 00:32:13.330
<v 0>Remember how I did that up there?
I added one.
I'm doing that again.
Okay.</v>

515
00:32:13.540 --> 00:32:16.860
So this is our input data and here are our labels.
Okay?
So that,

516
00:32:16.861 --> 00:32:18.760
that's why I imported both of those.

517
00:32:19.210 --> 00:32:22.510
And so now I've done that.

518
00:32:25.300 --> 00:32:25.730
<v 1>Okay.</v>

519
00:32:25.730 --> 00:32:26.920
<v 0>I've got my uh,
training day,</v>

520
00:32:26.921 --> 00:32:31.921
I've got my testing data and now I'm going to import xg boost.

521
00:32:32.620 --> 00:32:37.410
So xg boost,
which I've already imported.
Um,

522
00:32:39.130 --> 00:32:43.000
and now,
uh,
what do we do?

523
00:32:43.001 --> 00:32:47.160
We say actually boost up parameters are going to,
is we're going to,

524
00:32:47.260 --> 00:32:51.850
we're going to give this xg boost.
It said,
okay,
before we use xg boost,

525
00:32:52.270 --> 00:32:56.200
I got to explain what this is.
So let's get back to the explanation for a second.

526
00:32:56.620 --> 00:32:59.530
Where was I?
Okay,
first of all,

527
00:32:59.560 --> 00:33:02.320
well also be asking questions and I'm gonna be looking at this for questions by

528
00:33:02.321 --> 00:33:05.620
the way.
Okay,

529
00:33:05.680 --> 00:33:09.200
let's talk ensemble for a second,
right?
So when it comes to machine learning,
uh,

530
00:33:09.280 --> 00:33:12.280
ensembles,
there's this idea of ensembles where you have,

531
00:33:12.700 --> 00:33:16.090
you have multiple models that are trained on this data set,

532
00:33:16.390 --> 00:33:18.830
and then you can either combine those clouds,

533
00:33:18.910 --> 00:33:22.000
you always combine those predictions in a certain way.

534
00:33:22.150 --> 00:33:25.050
The way you combine them is different.
Sometimes it's called,
it's,

535
00:33:25.150 --> 00:33:28.210
it's using bagging.
Sometimes it's using boosting,

536
00:33:28.211 --> 00:33:29.710
and I'll talk about each of these,
but right?

537
00:33:29.710 --> 00:33:34.330
So it's just a collection of predictors,
right?
These can be trees.

538
00:33:34.520 --> 00:33:38.930
Um,
so if it was trees,
you could call it a random forest.
It could be,
uh,

539
00:33:39.190 --> 00:33:41.800
it could be a collection of linear regression models.

540
00:33:42.010 --> 00:33:45.460
It can be a collection of logistic regression models.
Um,
but we can,

541
00:33:45.490 --> 00:33:49.150
we can classify ensembling techniques into bagging and boosting.

542
00:33:49.510 --> 00:33:51.750
So what bagging is,
is it's,
it's,

543
00:33:51.760 --> 00:33:56.260
it's an ensembling technique where we built many independent predictor models,

544
00:33:56.261 --> 00:33:59.950
right?
So we have,
let's say a tree,
a decision tree.
We have another decision tree,

545
00:33:59.951 --> 00:34:00.940
we have another decision tree,

546
00:34:00.941 --> 00:34:04.840
we have another decision tree and they're all trained on different parts of the

547
00:34:04.841 --> 00:34:06.760
training data,
right?
So we have,
you know,

548
00:34:06.960 --> 00:34:11.470
say data points one through 100 than data points 101 through 200 and data points

549
00:34:11.471 --> 00:34:15.220
200,
one to 300 for each of those trees,
they'll all,

550
00:34:15.430 --> 00:34:18.520
they'll all be training and then using some technique,

551
00:34:18.610 --> 00:34:20.890
we're going to combine those predictions together.

552
00:34:21.010 --> 00:34:25.620
So these are independent models that we then combine together using,
um,

553
00:34:26.740 --> 00:34:27.820
an equation to it.

554
00:34:27.821 --> 00:34:31.030
It depends like the equation for combining the predictions together depends on

555
00:34:31.031 --> 00:34:33.130
the model that we're using.
Uh,

556
00:34:34.320 --> 00:34:35.153
<v 1>yeah.</v>

557
00:34:36.030 --> 00:34:37.260
<v 0>So there's that.</v>

558
00:34:37.980 --> 00:34:41.160
And then both overfitting which is better.

559
00:34:41.161 --> 00:34:44.220
So actually for overfitting,
uh,

560
00:34:44.760 --> 00:34:48.030
bagging is better that this image answers your question it,

561
00:34:48.031 --> 00:34:52.770
but if we want to reduce the bias and the bias and variance more,

562
00:34:52.771 --> 00:34:56.790
we'll use gray and boosting.
So let me go back here.
So,
um,
where was I?

563
00:34:56.791 --> 00:35:01.170
So that's baggings independent models that are combined to make a final

564
00:35:01.171 --> 00:35:02.004
prediction.

565
00:35:02.010 --> 00:35:05.820
Boosting is another type of ensembling technique where we're not making these

566
00:35:05.821 --> 00:35:08.130
predictions independently but sequentially,
right?

567
00:35:08.131 --> 00:35:12.210
So the models are actually being optimized together.

568
00:35:12.360 --> 00:35:17.130
So one method of doing that is called gradient boosting.
Okay?

569
00:35:17.131 --> 00:35:21.960
So what that means is in terms of gradient boosting,
so check this out,

570
00:35:22.200 --> 00:35:26.570
hold on.
Okay,
so we have some loss function.
Let's say,

571
00:35:26.600 --> 00:35:29.630
let's say mean squared error that we want to minimize.
I'm going to,
okay.

572
00:35:29.631 --> 00:35:33.590
By the way,
if you are here,
don't you dare leave during the math part.

573
00:35:33.591 --> 00:35:35.450
Don't you dare leave.
Second of all,

574
00:35:35.451 --> 00:35:39.840
I'm going to wrap at the end and you can enjoy it.
You can laugh,
whatever,

575
00:35:39.870 --> 00:35:42.890
but don't you dare leave.
Okay.
So here we go.
So,
um,

576
00:35:43.620 --> 00:35:46.770
we need a loss function,
right,
to,
to train any model.

577
00:35:46.771 --> 00:35:50.340
And so if we look at this loss function here,
we'll see that this is,
this is,

578
00:35:50.341 --> 00:35:53.670
this is actually a very simple loss function.
It looks complicated,

579
00:35:53.671 --> 00:35:57.540
but it's not they,
so what this let's just,
let's just segment this.

580
00:35:57.570 --> 00:35:59.220
Let's segment this out.
Okay.

581
00:35:59.221 --> 00:36:02.370
So see the sigma would that he looking thing that's called sigma,

582
00:36:02.500 --> 00:36:05.760
but just let's just think about that part alone.
What that is,

583
00:36:05.761 --> 00:36:10.350
is it's the sum of our,
uh,
errors.
Okay.
So this is some of our errors.

584
00:36:10.740 --> 00:36:13.200
And so our air values are,
you know,

585
00:36:13.201 --> 00:36:16.260
the difference between our prediction and our and our actual label,

586
00:36:16.440 --> 00:36:18.450
and we can find the difference there for all of them.

587
00:36:18.451 --> 00:36:22.560
So sigma means sum them all up,
all of your errors up together.
So that,
that's,

588
00:36:22.740 --> 00:36:26.670
that's,
that's,
that's this part right here.
Then once we have that,

589
00:36:29.930 --> 00:36:34.410
then once we have that,
uh,
we will multiply it by this Alpha value,
uh,

590
00:36:34.430 --> 00:36:37.460
and our learning rates.
And so these can change,
you know,

591
00:36:37.470 --> 00:36:42.020
these are the tuning knobs for our model,
right?
This can be 0.1,
0.2,
0.15,
six,

592
00:36:42.021 --> 00:36:43.190
seven,
eight,
whatever it is.

593
00:36:43.460 --> 00:36:46.760
And there are different techniques to learn what the optimal hyper parameters

594
00:36:46.761 --> 00:36:51.110
for a model should be.
Right?
So there's grid search.
There is um,

595
00:36:52.100 --> 00:36:53.300
you know,
random search.

596
00:36:53.390 --> 00:36:56.090
There are different hyper parameter optimization techniques.

597
00:36:56.091 --> 00:36:57.810
So learning how to learn,
uh,

598
00:36:57.860 --> 00:37:02.860
Bayesean optimization currently is kind of at the forefront of hyper parameter

599
00:37:03.261 --> 00:37:07.280
optimization theory.
A great to research avenue to go down if you want to.
Okay.

600
00:37:07.281 --> 00:37:12.170
So where was I?
So this is us trying to find the mean squared error.

601
00:37:12.320 --> 00:37:17.270
And what we do is we use,
so we're,
we use that error to compute a gradient,

602
00:37:17.271 --> 00:37:20.820
right?
So the gradient tells us how off our model is.
So,

603
00:37:20.821 --> 00:37:25.070
so in terms of the weights,
the coefficients for our model,
whatever the model is,

604
00:37:25.220 --> 00:37:26.660
need to be updated over time.

605
00:37:26.870 --> 00:37:31.190
So we'll use the error to compute the gradient and the gradient tells us how to

606
00:37:31.191 --> 00:37:34.760
update our weight values.
And so when it comes to gradient boosting,

607
00:37:34.850 --> 00:37:38.600
we're not just using the gradient to update a single tree cause we have multiple

608
00:37:38.601 --> 00:37:42.740
trees,
decision trees.
This shit isn't trees look like this.
Check that out.

609
00:37:43.400 --> 00:37:47.390
We're using the grading to update all of them.
Right.
So,
so we train one,

610
00:37:47.391 --> 00:37:50.840
we use the grading to update that and the next one and the next one and the next

611
00:37:50.841 --> 00:37:54.650
one.
So that's trained,
that is used to update the next one.
So it's,
it's,

612
00:37:54.651 --> 00:37:58.280
it's like they're all linked together by this grading that's updating
everything.

613
00:37:59.060 --> 00:38:00.530
And so that's gradient boosting.

614
00:38:00.531 --> 00:38:05.531
And so xg boost is a version of gray and boosting where it is designed for a set

615
00:38:06.411 --> 00:38:08.090
of decision tree specifically.

616
00:38:08.360 --> 00:38:13.360
And xg boost is also like the top a Kaggle model a lot of the time because it's

617
00:38:14.450 --> 00:38:17.000
easy.
It's quick.
Uh,
and um,

618
00:38:17.900 --> 00:38:21.530
although I am convinced at whoever won those competitions using xg boost,

619
00:38:21.710 --> 00:38:26.660
if they just use neural networks,
they would have a,
they would have done better.

620
00:38:26.890 --> 00:38:30.530
Um,
but they didn't have the resources or whatever.
But yeah,

621
00:38:30.531 --> 00:38:33.800
I still believe in deep learning over anything else.
But I think this is,
um,

622
00:38:36.430 --> 00:38:40.960
I think this is a good,
uh,
my hair is different.
It's,
it's silver.

623
00:38:40.990 --> 00:38:43.980
It's silver sea.
This is the original hair that I'm going after.

624
00:38:43.981 --> 00:38:47.260
I just didn't like,
you know,
die for a while.
So it got kinda blonde.

625
00:38:47.620 --> 00:38:52.170
But silver is like my original hair,
you know what I'm saying?
So,
uh,

626
00:38:52.180 --> 00:38:54.040
where was I?
So that's xg boost.

627
00:38:54.041 --> 00:38:57.730
It's a bunch of decision trees that are all optimized together using gradient

628
00:38:57.731 --> 00:39:02.140
descent.
The gradient is updating everything,
all of the trees sequentially.

629
00:39:03.510 --> 00:39:04.343
<v 1>Okay.</v>

630
00:39:04.780 --> 00:39:07.960
<v 0>And not independently.
Okay.
So now,</v>

631
00:39:08.600 --> 00:39:09.433
<v 1>uh,</v>

632
00:39:09.910 --> 00:39:12.430
<v 0>okay,
if you have to go to sleep then definitely go to sleep.</v>

633
00:39:12.431 --> 00:39:16.960
Thank you for tuning in.
Okay,
so now that was that.
What can we do now?

634
00:39:16.961 --> 00:39:20.170
Now I'm going to say,

635
00:39:23.210 --> 00:39:28.010
you know what,
I'm going to use the default parameters for xg boost and uh,

636
00:39:28.550 --> 00:39:30.650
let's just say default prams,

637
00:39:31.600 --> 00:39:34.460
cause there's a lot of different parameters we could talk about sub sampling

638
00:39:34.461 --> 00:39:37.430
lambda and thread a whole bunch of different parameters.

639
00:39:37.620 --> 00:39:41.770
I'm going to be using the default one DF to train a team.

640
00:39:45.240 --> 00:39:46.073
<v 1>Okay.</v>

641
00:39:51.650 --> 00:39:52.483
Uh,

642
00:39:53.430 --> 00:39:55.920
<v 0>okay,
what I'm going to do,</v>

643
00:39:58.890 --> 00:39:59.723
<v 1>okay.</v>

644
00:40:00.220 --> 00:40:02.740
<v 0>So I'm going to run this and then while this is training,</v>

645
00:40:02.741 --> 00:40:06.760
so training is unfortunately going to take a while and I can't just train during

646
00:40:06.761 --> 00:40:08.020
a live stream.
Uh,

647
00:40:08.021 --> 00:40:11.020
so what I'm gonna do is I'm going to show you a model that has been trained

648
00:40:11.030 --> 00:40:14.500
already.
Okay?
Okay.

649
00:40:14.800 --> 00:40:19.750
So over time,
so this is a time,
uh,
this,
this data set changes over time.

650
00:40:19.960 --> 00:40:24.070
So what we can do is we can say on the,
on the rights my rights,

651
00:40:24.100 --> 00:40:28.060
you're seeing the actual pickup density at these times and on the left you're

652
00:40:28.061 --> 00:40:30.850
seeing that predicted pickup density.
Okay?
So this is,

653
00:40:30.880 --> 00:40:32.590
this is using a trained model.

654
00:40:32.591 --> 00:40:37.390
It's predicting where rides are going in real time,

655
00:40:37.660 --> 00:40:40.450
over time.
And this is what happens when you have a trained model,
right?

656
00:40:40.451 --> 00:40:43.990
So once you train the model,
you can visualize it using Matt put live,
et Cetera,

657
00:40:43.991 --> 00:40:46.360
et Cetera,
et cetera.
Okay?
So that's it for this livestream.

658
00:40:46.420 --> 00:40:47.970
I just wanted to go over that.
Um,

659
00:40:48.130 --> 00:40:50.720
the code is going to be in the get hub description.
Uh,

660
00:40:50.740 --> 00:40:55.480
so definitely check that out.
I have more coming out on that.
And lastly,

661
00:40:55.481 --> 00:40:58.900
I'm going to rap.
So let me just wrap instrumental.

662
00:41:01.000 --> 00:41:05.920
Is this going to play here?
No,
it's not.
Okay.
Let me find my phone.
Okay,

663
00:41:05.921 --> 00:41:10.630
hold on.
All Right,
rob.
Instrumental.
Okay.

664
00:41:10.631 --> 00:41:13.330
It's time to wrap and then also answer some questions.

665
00:41:13.540 --> 00:41:15.580
Grid search is time consuming?
Yes it is.

666
00:41:15.670 --> 00:41:18.670
That's why I prefer Basie and optimization.
I don't want to,

667
00:41:21.730 --> 00:41:25.300
okay,
how can I make my own environment like chaos and tensorflow?
Listen,

668
00:41:25.330 --> 00:41:29.500
you don't want to do it yourself.
You want to use Anaconda.

669
00:41:29.501 --> 00:41:31.720
You want to use um,
virtual end.

670
00:41:31.750 --> 00:41:34.010
You don't want to have to do things that want to use docker.

671
00:41:34.160 --> 00:41:35.840
You don't have to mess with dependencies,
right?

672
00:41:35.870 --> 00:41:39.290
We have tools now to automatically install the dependencies you need.

673
00:41:45.030 --> 00:41:49.830
Okay,
so now I need somebody to say a topic cause I'm going to freestyle rap.

674
00:41:50.280 --> 00:41:52.230
Okay.
All right.

675
00:41:52.740 --> 00:41:57.270
Some say someone say someone say a hot topic.
Here we go.

676
00:41:58.260 --> 00:41:59.093
Here we go.

677
00:42:03.810 --> 00:42:08.330
Someone said are the language.
Hey,
okay,
here we go.

678
00:42:10.220 --> 00:42:15.000
Here we go.
I don't like to use our eye drop bars.
I do it.

679
00:42:15.001 --> 00:42:18.060
Then back and back and back and back.
Our Bra.

680
00:42:18.150 --> 00:42:20.310
I'm like Elian Mar.

681
00:42:20.310 --> 00:42:24.030
See a logo is a place I want to go and drive a yacht in the sea,

682
00:42:24.031 --> 00:42:28.350
in the bleed blue man.
It's all good,
man.
I'm like new every day.

683
00:42:28.351 --> 00:42:32.190
I released subsidy.
Yo,
you try to stop me.
It doesn't come back.
I'm like,

684
00:42:32.850 --> 00:42:34.290
are thin yo hall.

685
00:42:34.350 --> 00:42:38.760
He was the guy who was rapping in the Apollo theater man at night.
Okay.

686
00:42:38.761 --> 00:42:41.940
XG boost.
Is it?
That's it.
That's it.
That's it.
That's it.
That's it.

687
00:42:42.810 --> 00:42:46.650
We got a 32nd wrap in guys.
We did this Kaggle competition.
We,

688
00:42:46.770 --> 00:42:50.520
we visualize our data set.
We talked a little bit about xg boost.

689
00:42:50.640 --> 00:42:52.350
We did all of that in 40 minutes.

690
00:42:52.470 --> 00:42:56.970
You guys have learned so much and I am so happy for you.
Thank you for s,

691
00:42:57.000 --> 00:43:00.120
for staying,
uh,
during this live stream.
I'm so excited to have you guys here.

692
00:43:00.300 --> 00:43:03.450
I'm going to try and do this every single week for you guys on Friday.

693
00:43:03.840 --> 00:43:07.320
Please subscribe if you haven't yet.
I'll tell your friends and subscribe.

694
00:43:07.560 --> 00:43:10.140
That's what it's all about.
I'm trying to grow this community,

695
00:43:10.141 --> 00:43:13.440
the school of AI together.
We can do that.
So thank you guys for coming.

696
00:43:14.230 --> 00:43:14.570
<v 1>Okay.</v>

697
00:43:14.570 --> 00:43:18.740
<v 0>For now,
I've got to take a get a plane,
so thanks for watching.</v>

