WEBVTT

1
00:00:01.530 --> 00:00:06.120
I bring you the light of statistics.
Hello world,

2
00:00:06.140 --> 00:00:10.890
it's a Raj and understanding statistics is a really important part of data

3
00:00:10.891 --> 00:00:11.724
science.

4
00:00:11.790 --> 00:00:16.790
There are entire textbooks and graduate level programs dedicated to mastering

5
00:00:17.041 --> 00:00:19.050
this branch of mathematics.

6
00:00:19.170 --> 00:00:22.290
So to give you a brief but relevant overview,

7
00:00:22.500 --> 00:00:27.480
we're going to learn about just three major concepts in statistics that all data

8
00:00:27.481 --> 00:00:30.450
scientists should understand for context.

9
00:00:30.451 --> 00:00:35.451
We'll apply them to a Dataset of loans that were issued to people between 2007

10
00:00:35.880 --> 00:00:39.780
and 2015 to figure out the type of people that received loans,

11
00:00:39.930 --> 00:00:44.340
whether or not they're given credit score was appropriate and whether or not we

12
00:00:44.341 --> 00:00:49.020
can create a model to better predict their credit score.
Using machine learning.

13
00:00:49.140 --> 00:00:53.760
It's hard to understate how crucial statistics is in data science.

14
00:00:53.850 --> 00:00:54.391
In fact,

15
00:00:54.391 --> 00:00:59.391
in 96 the term data science was used for the first time in the title of a

16
00:00:59.971 --> 00:01:04.260
statistical conference called IFCs.
The title was data signs,

17
00:01:04.261 --> 00:01:06.390
classification and related methods.

18
00:01:06.900 --> 00:01:11.610
Data Science started with statistics and has evolved to include concepts like

19
00:01:11.611 --> 00:01:14.100
machine learning and artificial intelligence.

20
00:01:14.610 --> 00:01:19.610
Statistics is a collection of procedures and principles for gaining information

21
00:01:19.741 --> 00:01:23.250
in order to make decisions when faced with uncertainty.

22
00:01:23.370 --> 00:01:25.200
I shouldn't have used it in my last relationship.

23
00:01:25.350 --> 00:01:30.350
It's an extremely valuable skill to understand so much so that's that

24
00:01:30.751 --> 00:01:35.460
institutions can work in virtually any field from business to social science to

25
00:01:35.461 --> 00:01:38.610
medicine in the context of data science.

26
00:01:38.640 --> 00:01:43.170
Think of a data scientist as a person who's better at statistics than any

27
00:01:43.171 --> 00:01:47.160
programmer and better at programming than any statistician.

28
00:01:47.430 --> 00:01:52.260
We can use statistics for so many problems like identifying the risk factors for

29
00:01:52.261 --> 00:01:57.261
a type of cancer or customizing a spam detection system or establishing the

30
00:01:57.811 --> 00:02:02.811
relationship between salary and demographic variables in population survey data.

31
00:02:03.120 --> 00:02:05.910
So let's focus on a very specific problem.

32
00:02:05.940 --> 00:02:10.940
In 2008 there was a sub prime mortgage crisis in the United States,

33
00:02:11.100 --> 00:02:15.630
which was terrible for many companies like when Lehman brothers went down,

34
00:02:15.631 --> 00:02:17.670
down,
down,
down,
down.

35
00:02:18.030 --> 00:02:23.030
But it did create opportunities for new players in the retail credit field

36
00:02:23.670 --> 00:02:28.380
following the credit scarcity that took place briefly during those times peer to

37
00:02:28.381 --> 00:02:32.130
peer lending companies thrived.
According to PWC,

38
00:02:32.131 --> 00:02:37.131
US peer to peer lending platform volumes have grown an average of 84% per

39
00:02:37.921 --> 00:02:42.921
quarter since 2007 lending club is one example of a popular p to p lending

40
00:02:43.450 --> 00:02:48.150
marketplace and we can use readily available data from them to help us figure

41
00:02:48.151 --> 00:02:53.151
out if credit scores for a given set of people are accurate and if we can create

42
00:02:53.281 --> 00:02:55.770
more optimized ones ourselves.

43
00:02:56.100 --> 00:03:01.100
The lending club Dataset contains 887 loan applications over a period of eight

44
00:03:01.931 --> 00:03:06.520
years and we can download it directly from their website.
As you can see,

45
00:03:06.550 --> 00:03:10.300
there are a lot of columns meaning features in this dataset.

46
00:03:10.510 --> 00:03:15.070
For every person we have the amount they were loaned various details about their

47
00:03:15.071 --> 00:03:18.010
background,
like whether or not they own a home,

48
00:03:18.130 --> 00:03:22.150
where they live and of course their credit score.
Using statistics,

49
00:03:22.151 --> 00:03:25.630
we can gain deeper insight into how this data is structured.

50
00:03:25.780 --> 00:03:27.460
And then based on this structure,

51
00:03:27.490 --> 00:03:32.200
we can optimally apply other data science techniques to get even more

52
00:03:32.201 --> 00:03:33.034
information.

53
00:03:33.040 --> 00:03:37.240
So let's start with the first statistics concepts we can use here.

54
00:03:37.270 --> 00:03:38.800
Statistical features,

55
00:03:38.980 --> 00:03:43.300
this is probably the most used statistics concepts in data science.

56
00:03:43.480 --> 00:03:48.340
It's usually the first stats technique we would apply when exploring a Dataset

57
00:03:48.490 --> 00:03:51.430
and includes concepts like bias variants,

58
00:03:51.550 --> 00:03:54.760
mean median percentiles,
and many others.

59
00:03:55.030 --> 00:03:58.300
All of them are easy to understand and implement in code,

60
00:03:58.450 --> 00:04:03.450
but it'll take a while to define every single one of them and you can't download

61
00:04:03.461 --> 00:04:05.260
them into your brain yet.

62
00:04:05.560 --> 00:04:10.560
So I've linked to a detailed cheat sheet in the video description to visualize

63
00:04:11.320 --> 00:04:13.300
some of these statistical features.

64
00:04:13.450 --> 00:04:18.450
Let's create what's called a box plot to examine the relationship between income

65
00:04:19.120 --> 00:04:20.200
and loan amount.

66
00:04:20.530 --> 00:04:25.330
Box plots are a standard way of displaying the distribution of data based on a

67
00:04:25.331 --> 00:04:28.450
few statistical features.
In order to do that,

68
00:04:28.451 --> 00:04:33.451
we'll look at a subset from our data where income is less than 120 k per year.

69
00:04:33.940 --> 00:04:38.620
The reason being that applications with income above this threshold are not

70
00:04:38.621 --> 00:04:43.030
statistically representative of our population.
Out of 880 k loans,

71
00:04:43.060 --> 00:04:46.450
only 10% have annual incomes higher than that.

72
00:04:46.870 --> 00:04:49.300
If we didn't cap our annual income,

73
00:04:49.420 --> 00:04:53.590
we would have a lot of outliers and our box plot wouldn't look as insightful.

74
00:04:53.890 --> 00:04:58.270
Using our box plot,
we can easily visualize some statistical features.

75
00:04:58.480 --> 00:05:01.480
The line in the middle is the median value of the data.

76
00:05:01.660 --> 00:05:06.660
The median is used over the mean since it's more robust to outlier values.

77
00:05:07.000 --> 00:05:09.700
The first quartile is the 25th percentile,

78
00:05:09.730 --> 00:05:14.440
meaning 25% of the points in the data fall below that value.

79
00:05:14.680 --> 00:05:17.680
The third core tile is a 75th percentile,

80
00:05:17.710 --> 00:05:22.390
meaning 75% of the points in the data fall below that value.

81
00:05:22.690 --> 00:05:27.640
The Max and Min values represent the lower and upper ends of our data range.

82
00:05:27.910 --> 00:05:32.910
Box plots are awesome because they demonstrate how we can utilize a sticks,

83
00:05:32.951 --> 00:05:37.030
his physical features instantly.
If a box plot is short,

84
00:05:37.060 --> 00:05:39.610
it means that our data points are generally similar.

85
00:05:39.910 --> 00:05:43.300
Many values are in a small range.
If it's tall,

86
00:05:43.301 --> 00:05:47.890
it implies that our data points are different since the values are more spread

87
00:05:47.891 --> 00:05:51.310
out.
If the median value is closer to the bottom,

88
00:05:51.550 --> 00:05:54.640
we know that most of the data has lower values.

89
00:05:54.700 --> 00:05:56.920
If the median value is closer to the top,

90
00:05:57.050 --> 00:05:59.810
we know that most of the data has higher values.

91
00:06:00.080 --> 00:06:04.910
If the median line is not in the middle of the box,
it means we have skewed data.

92
00:06:05.240 --> 00:06:08.270
We could keep going here.
If the whiskers are super long,

93
00:06:08.271 --> 00:06:12.680
it means our data has a high standard deviation and variance,

94
00:06:12.681 --> 00:06:17.180
which means our values are spread out in highly burying.
As you can see,

95
00:06:17.181 --> 00:06:22.040
we can get a lot of information from just a few simple statistical features that

96
00:06:22.041 --> 00:06:26.600
are all easy to calculate.
My friend said he made it up to Taggle is top spot

97
00:06:29.180 --> 00:06:31.460
and all the maid was a brand new box plot.

98
00:06:32.810 --> 00:06:37.370
We'll notice that the core tile distribution of fully paid is very different

99
00:06:37.371 --> 00:06:40.610
from the core tile distribution of charged off,

100
00:06:40.760 --> 00:06:44.600
but it's similar to current ingrained period and issued.

101
00:06:44.870 --> 00:06:49.640
This means that lending club has been more selective with its newer loans.

102
00:06:49.700 --> 00:06:54.700
Also charged off and default statuses hold similarities in terms of the core

103
00:06:54.711 --> 00:06:58.100
tile distribution differing from all the others.

104
00:06:58.310 --> 00:07:02.600
This lets us know that the income variables are important for predicting loan

105
00:07:02.601 --> 00:07:03.410
grades.

106
00:07:03.410 --> 00:07:08.060
If we add another dimension to the analysis by generating the box plots for

107
00:07:08.090 --> 00:07:09.890
income versus loan grade,

108
00:07:10.190 --> 00:07:15.190
we'll find that a graded loans have a median income that superior to other

109
00:07:15.261 --> 00:07:19.100
grades,
but we can't say the same about the other core tiles.

110
00:07:19.430 --> 00:07:24.430
Notice though that f g and be graded loans hold a similar income core tile

111
00:07:24.951 --> 00:07:27.380
distribution lacking consistency.

112
00:07:28.820 --> 00:07:33.820
Maybe income actually isn't that critical when determining lending club's loan

113
00:07:33.831 --> 00:07:36.860
grades.
We'll have to keep investigating here to learn more.

114
00:07:36.950 --> 00:07:41.600
The second important concept from statistics to know is the probability

115
00:07:41.601 --> 00:07:42.470
distribution.

116
00:07:42.770 --> 00:07:47.210
We can define probability as the percent chance that some events will occur.

117
00:07:47.450 --> 00:07:52.450
Usually this is quantified in the range of zero to one where zero means we are

118
00:07:52.461 --> 00:07:57.260
sure that it won't occur and one means work.
Totally sure it will occur.

119
00:07:57.590 --> 00:08:02.060
We can think of a probability distribution as a function that represents the

120
00:08:02.061 --> 00:08:05.840
probabilities of all possible values in an experiment.

121
00:08:06.170 --> 00:08:09.050
There are many different types of distributions,

122
00:08:09.080 --> 00:08:12.020
so much interesting theory here.
For example,

123
00:08:12.050 --> 00:08:17.050
a uniform distribution has a single value which only occurs in a certain range

124
00:08:17.240 --> 00:08:20.510
while anything outside of that range is solely zero.

125
00:08:20.810 --> 00:08:23.330
Think of it as an on or off distribution.

126
00:08:23.690 --> 00:08:28.160
A normal distribution is specifically defined by its mean and standard

127
00:08:28.161 --> 00:08:29.780
deviation.
With this,

128
00:08:29.781 --> 00:08:34.780
we know the average value of our Dataset as well as how spread out it is that

129
00:08:34.781 --> 00:08:39.781
boys soon distribution is like the normal but with the edit factor of skewness.

130
00:08:40.790 --> 00:08:42.050
When skewness is high,

131
00:08:42.051 --> 00:08:45.440
then the spread of the data will be different in different directions.

132
00:08:45.650 --> 00:08:50.090
One direction can be very spread.
While the other could be very concentrated.

133
00:08:50.150 --> 00:08:51.830
There are more distributions.

134
00:08:51.831 --> 00:08:54.830
I just wanted to give you an overview of three important.

135
00:08:55.620 --> 00:09:00.620
If we generate a distribution plot for annual incomes from single applications,

136
00:09:01.260 --> 00:09:06.180
we'll find that it's heavily skewed,
heavily peaked and has a long right tail.

137
00:09:06.210 --> 00:09:10.650
These points are regularly observed in distributions that are fit by a power
law.

138
00:09:10.970 --> 00:09:15.960
A power law is a functional relationship between two quantities or a relative

139
00:09:15.961 --> 00:09:20.730
change in one quantity results in a proportional relative change in the other

140
00:09:20.731 --> 00:09:24.390
quantity independent of the initial size of these quantities.

141
00:09:24.600 --> 00:09:28.140
Basically one quantity berries as a power of another.

142
00:09:28.410 --> 00:09:32.520
We can informally say that we have a power law candidate distribution here.

143
00:09:32.790 --> 00:09:36.450
Most applications are coming from the lower end of the income spectrum.

144
00:09:36.451 --> 00:09:37.920
An interesting observation.

145
00:09:38.130 --> 00:09:43.130
The third concept to know about his Basie and statistics to understand and

146
00:09:43.471 --> 00:09:48.240
stats,
we have to first understand frequency stats.
Everyone,
everyone stay calm.

147
00:09:48.241 --> 00:09:49.560
No gang wars please.

148
00:09:49.590 --> 00:09:54.590
Frequency statistics is the type of stats that involves applying math to analyze

149
00:09:55.021 --> 00:09:59.910
the probability of some event occurring where specifically the only data we

150
00:09:59.911 --> 00:10:02.070
compute on his prior data.

151
00:10:02.100 --> 00:10:05.670
If we had a die and were asked what the chance of rolling a six was,

152
00:10:05.880 --> 00:10:10.200
most people would say it's one and six but what if someone were to tell us that

153
00:10:10.201 --> 00:10:15.201
the specified die given to us was loaded to always land on six frequency stats

154
00:10:16.110 --> 00:10:18.090
only takes into account prior data.

155
00:10:18.480 --> 00:10:22.770
That new evidence that was given to us about the died being loaded is not being

156
00:10:22.771 --> 00:10:26.360
taken into account Bayesians statistics.
However,

157
00:10:26.361 --> 00:10:28.740
it does take into account this evidence.

158
00:10:29.010 --> 00:10:33.000
We can illustrate it by taking a look at base there from where he is.

159
00:10:33.001 --> 00:10:35.610
The evidence and h is the hypothesis,

160
00:10:35.670 --> 00:10:40.110
the probability of the hypothesis given the evidence is equal to the prior

161
00:10:40.111 --> 00:10:44.010
probability multiplied by the likelihood of the evidence.
He,

162
00:10:44.220 --> 00:10:49.220
if the hypothesis is true divided by the priority probability that the evidence

163
00:10:49.381 --> 00:10:53.910
itself is true Bayesians stats takes everything into account,

164
00:10:53.940 --> 00:10:58.560
we use it whenever we feel that our prior data will not be a good representation

165
00:10:58.561 --> 00:11:03.561
of our future data and results based there I'm simple buys complex concepts.

166
00:11:03.870 --> 00:11:07.080
It explains a lot using a few simple variables.

167
00:11:07.260 --> 00:11:11.640
It supports the concept of conditional probability meaning if a occurred,

168
00:11:11.880 --> 00:11:15.010
it played a role in the occurrence of B.

169
00:11:15.010 --> 00:11:19.770
Base theorem can help us predict the probability of someone having a specific

170
00:11:19.771 --> 00:11:21.780
disease,
knowing their age.

171
00:11:21.960 --> 00:11:26.960
It can let us know if an email is spam based on the number of words it's used to

172
00:11:27.091 --> 00:11:28.620
remove uncertainty.

173
00:11:29.040 --> 00:11:33.240
It was even used to help predict the configurations of the enigma machine to

174
00:11:33.241 --> 00:11:36.030
translate the German codes in World War II.

175
00:11:36.090 --> 00:11:38.880
So how do we utilize Basie and statistics here?

176
00:11:39.180 --> 00:11:42.570
One way is to build a Bayesian classifier algorithm,

177
00:11:42.600 --> 00:11:46.710
one that will predict credit scores given other features in the Dataset.

178
00:11:46.950 --> 00:11:51.950
Naive Bayes is a family of algorithms that takes advantage of base theorem to

179
00:11:52.171 --> 00:11:53.920
predict a target variable.

180
00:11:54.220 --> 00:11:58.720
This type of classifier assumes all features are unrelated to each other.

181
00:11:58.960 --> 00:12:00.850
Using a few features we select,

182
00:12:00.970 --> 00:12:05.500
we can predict whether a loan applicant should be accepted or rejected.

183
00:12:05.830 --> 00:12:07.820
The accuracy looks all right.

184
00:12:07.840 --> 00:12:12.310
I'm sure we can try several other classifier models here to find a better

185
00:12:12.311 --> 00:12:15.970
predictor,
but this is a great first step.
As you can see,

186
00:12:15.971 --> 00:12:20.590
statistics is supremely useful in data science and we only covered three key

187
00:12:20.591 --> 00:12:22.540
concepts from statistics.

188
00:12:22.690 --> 00:12:27.340
There are so many more like upsampling and downsampling and dimensionality

189
00:12:27.341 --> 00:12:28.660
reduction.
There's more,

190
00:12:28.960 --> 00:12:32.410
but there are just three things to remember from this video.

191
00:12:32.800 --> 00:12:37.800
Statistical features like bias variants and many others help us explore a

192
00:12:38.141 --> 00:12:40.630
dataset to gain valuable insights.

193
00:12:40.990 --> 00:12:45.610
Probability distributions defined the percent chance that some event will occur

194
00:12:45.730 --> 00:12:50.730
and we can use them to understand the spread of data and hazy and Statistics

195
00:12:51.310 --> 00:12:56.310
expresses probability as a degree of belief in an event which can change as new

196
00:12:56.771 --> 00:13:01.120
information is gathered rather than a fixed value based on frequency.

197
00:13:01.240 --> 00:13:06.240
The coding challenge for this week is to use statistics to perform exploratory

198
00:13:06.881 --> 00:13:11.620
data analysis on this lending club Dataset in the form of a Jupiter notebook.

199
00:13:11.830 --> 00:13:16.630
The top two most detailed reports when I'll give them a shout out next week,

200
00:13:16.690 --> 00:13:20.230
post your get hub links in the comment section.
Details will be in their get hub.

201
00:13:20.231 --> 00:13:21.210
Read meat in the video.

202
00:13:21.860 --> 00:13:24.050
<v 1>What's one thing you like about statistics?</v>

203
00:13:24.051 --> 00:13:27.320
Let me know in the comments section and please subscribe for more programming

204
00:13:27.321 --> 00:13:31.540
videos.
For now,
I've got to predict the target,
so thanks for watching it.

