WEBVTT

1
00:00:00.150 --> 00:00:01.770
Yo google,
I'm gonna let you finish,

2
00:00:01.771 --> 00:00:05.550
but apple has the best hardware of all time.

3
00:00:06.360 --> 00:00:07.140
Hello world.

4
00:00:07.140 --> 00:00:11.490
It's Saroj and apple just announced its new iPhone xs,

5
00:00:11.520 --> 00:00:13.950
so expected demand for it to be massive.

6
00:00:14.340 --> 00:00:19.340
We can use a special reinforcement learning algorithm called policy iteration to

7
00:00:19.771 --> 00:00:24.000
help manage Apple's retail inventory and any others and make sure that the

8
00:00:24.001 --> 00:00:27.690
demand meets supply.
I'll explain how in this video,

9
00:00:28.110 --> 00:00:32.790
let's assume that we are a retail manager for apple in San Francisco and there

10
00:00:32.791 --> 00:00:37.791
are two retail locations that were in charge of both locations,

11
00:00:37.801 --> 00:00:42.801
have different levels of demand in terms of number of iPhone xs purchases per

12
00:00:43.081 --> 00:00:46.770
day,
and to rate of new deliveries from apple HQ.

13
00:00:47.070 --> 00:00:49.020
Since one of our retail locations,

14
00:00:49.021 --> 00:00:54.021
we'll have more demand than supply in terms of new iPhone xs orders delivered

15
00:00:54.541 --> 00:00:55.410
from Hq.

16
00:00:55.740 --> 00:00:59.580
We can move i-phones overnight from one location to the other.

17
00:00:59.970 --> 00:01:04.970
That way we can make sure that we have enough iPhones at each location to

18
00:01:05.341 --> 00:01:09.600
maximize our profit.
There's a reason where a trillion dollar company,
right.

19
00:01:09.720 --> 00:01:14.720
The problem that we're trying to solve here is how many I iPhones should we move

20
00:01:15.060 --> 00:01:17.430
from one location to the next,

21
00:01:17.700 --> 00:01:22.700
given a particular number of i-phones at each retail store and how much should

22
00:01:23.311 --> 00:01:25.500
we expect to earn at both locations?

23
00:01:26.010 --> 00:01:30.000
This problem requires a real time learning strategy.

24
00:01:30.240 --> 00:01:32.670
One that adapts to a dynamic world.

25
00:01:32.970 --> 00:01:37.970
We can use the tried and true Mark Haub decision process framework to express

26
00:01:38.161 --> 00:01:42.690
this problem mathematically.
Then come up with a proper solution.

27
00:01:42.870 --> 00:01:45.690
Once all of our variables are defined,

28
00:01:46.080 --> 00:01:51.080
our reinforcement learning agent will need to learn how best to optimize our

29
00:01:51.211 --> 00:01:53.940
problem using the variables it's given.

30
00:01:54.000 --> 00:01:57.780
A stake here can be considered the number of iPhones at each location.

31
00:01:57.781 --> 00:02:01.320
At the end of the day,
the actions we could take,
our,

32
00:02:01.350 --> 00:02:06.350
the net number of i-phones moved between the two locations overnight.

33
00:02:06.990 --> 00:02:11.520
The maximum possible being five.
Every time an iPhone is bought,

34
00:02:11.550 --> 00:02:16.140
we earn $10 in commissions.
So that can be art reward value.

35
00:02:16.200 --> 00:02:20.820
It's a hard knock life,
I know,
but we also have a cost to moving.

36
00:02:21.060 --> 00:02:26.060
It costs us $2 in shipping fees to move an iPhone overnight from one location to

37
00:02:27.121 --> 00:02:27.954
the next,

38
00:02:28.020 --> 00:02:33.020
so we can consider that a negative reward and one time step in our case means a

39
00:02:33.901 --> 00:02:34.890
full business day.

40
00:02:35.040 --> 00:02:39.750
We'll also use a discount factor of 0.9 to prevent us from looking infinitely

41
00:02:39.751 --> 00:02:40.680
into the future.

42
00:02:41.010 --> 00:02:45.030
We instead look at a certain amount of time into the future.

43
00:02:45.630 --> 00:02:50.630
This measures how far ahead in time a reinforcement learning algorithm looks.

44
00:02:51.360 --> 00:02:56.360
A discount factor that's closer to zero indicates that only rewards in the

45
00:02:56.401 --> 00:03:01.401
immediate future are being considered a one that's closer to one like ours

46
00:03:02.110 --> 00:03:04.990
prioritizes rewards in the distant future.

47
00:03:05.050 --> 00:03:09.940
So that's it for our mark Cobian variables,
but we should define the rest.

48
00:03:10.330 --> 00:03:15.330
Let's say each retail store can only hold 100 iPhones at a time because we're

49
00:03:15.371 --> 00:03:16.300
feeling exclusive.

50
00:03:16.690 --> 00:03:21.690
We can expect an average of three and four iPhone purchases at the first and

51
00:03:21.971 --> 00:03:24.010
second locations respectively,

52
00:03:24.310 --> 00:03:29.310
and we can expect an average of three and two new iPhone deliveries at the first

53
00:03:30.551 --> 00:03:32.500
and second locations respectively.

54
00:03:32.980 --> 00:03:37.980
What that tells us is that the second location we'll have more purchases than

55
00:03:38.171 --> 00:03:43.171
deliveries were as the first location will have an equal number of purchases and

56
00:03:43.421 --> 00:03:44.254
deliveries.

57
00:03:44.590 --> 00:03:49.590
We also know one crucial variable beforehand and that's the state transition

58
00:03:50.231 --> 00:03:51.580
probability function,

59
00:03:51.700 --> 00:03:55.480
which is a key element in any Mark Cobb decision process.

60
00:03:55.900 --> 00:04:00.670
It defines the probability of transitioning from one state to another in a

61
00:04:00.671 --> 00:04:01.750
single step.

62
00:04:02.050 --> 00:04:06.760
We could even list out the transition probabilities and a matrix and consider it

63
00:04:06.790 --> 00:04:08.860
a state transition matrix.

64
00:04:09.520 --> 00:04:13.750
You can imagine that for some problems with potentially millions of possible

65
00:04:13.751 --> 00:04:14.584
states,

66
00:04:14.590 --> 00:04:19.590
this could get way too computationally expensive to compute manually,

67
00:04:20.110 --> 00:04:24.700
but there are ways around that that we'll talk about later on.
In statistics,

68
00:04:24.850 --> 00:04:29.440
the press on distribution can be thought of as an approximation of the true

69
00:04:29.441 --> 00:04:30.274
reality.

70
00:04:30.430 --> 00:04:35.430
It focuses on the number of discrete events or occurrences over a specified

71
00:04:35.621 --> 00:04:39.530
continuum that can be time,
distance,
slang,
et cetera.

72
00:04:39.910 --> 00:04:44.740
We can define a quest on random variable x as a number of events in a given unit

73
00:04:44.741 --> 00:04:45.574
of time,

74
00:04:45.640 --> 00:04:50.110
which is going to be any positive whole value with no upper bound.

75
00:04:50.410 --> 00:04:51.760
In our specific case,

76
00:04:51.790 --> 00:04:56.790
we can represent the mean of the [inaudible] distribution as lambda and set it

77
00:04:57.400 --> 00:05:01.240
equal to the number of occurrences over a given interval,

78
00:05:01.360 --> 00:05:03.760
which depending on the retail store,

79
00:05:03.761 --> 00:05:07.420
we have two is going to be number of iPhones per day.

80
00:05:07.750 --> 00:05:12.750
This way we can mathematically define the expected probability of each outcome,

81
00:05:13.240 --> 00:05:17.620
which is the likelihood of x iPhones to be bought from a particular retail

82
00:05:17.621 --> 00:05:18.454
location.

83
00:05:18.730 --> 00:05:23.260
So because we know all the elements of our Mark Haub decision process

84
00:05:23.530 --> 00:05:26.830
beforehand,
we can use dynamic programming.

85
00:05:27.220 --> 00:05:32.220
We've used value iteration to estimate the optimal value function,

86
00:05:32.560 --> 00:05:36.040
which we could then use to estimate the optimal policy.

87
00:05:36.370 --> 00:05:41.370
But there's another technique called policy iteration that directly computes

88
00:05:42.100 --> 00:05:45.580
that optimal policy.
Let's use that one.
Remember,

89
00:05:45.581 --> 00:05:50.500
policies are a mapping of the actions and agent takes from a particular state.

90
00:05:50.830 --> 00:05:53.440
For example,
if at the end of a business day,

91
00:05:53.470 --> 00:05:58.470
if there are 13 and 17 i-phones in stores one and two respectively,

92
00:05:58.910 --> 00:06:02.570
how many iPhones should the agent move between stores?

93
00:06:02.900 --> 00:06:06.590
There are lots of possible different actions the agent could take.

94
00:06:06.800 --> 00:06:09.260
So in order to solve this problem,

95
00:06:09.440 --> 00:06:14.030
it needs to know which action will result in the highest longterm return,

96
00:06:14.330 --> 00:06:19.330
so how do we determine what gives us the highest value and how do we come up

97
00:06:19.701 --> 00:06:21.620
with a better set of policies?

98
00:06:21.800 --> 00:06:26.390
Step one is to evaluate the policy and this is known as policy evaluation.

99
00:06:26.780 --> 00:06:31.730
It's the process of taking a policy and calculating the return of every single

100
00:06:31.731 --> 00:06:34.880
state ace on following a particular policy.

101
00:06:35.300 --> 00:06:38.780
When we arrive at a true value function for our policy,

102
00:06:38.960 --> 00:06:41.480
we can try and improve it makes sense,

103
00:06:41.481 --> 00:06:46.310
but we can definitely do better than our initialize policy that does nothing.

104
00:06:46.580 --> 00:06:51.580
Policy Improvement then is the act of looking at all actions and agent could

105
00:06:51.741 --> 00:06:55.820
take from a given state and then choosing the action that gives the highest

106
00:06:55.821 --> 00:06:59.870
return.
With respect to our value function or system.

107
00:06:59.871 --> 00:07:03.290
We'll look through these two processes iteratively.

108
00:07:03.620 --> 00:07:08.390
This is policy iteration.
It will calculate the true value function for a policy.

109
00:07:08.750 --> 00:07:13.280
Then improve the policy based on the value function before going back and

110
00:07:13.281 --> 00:07:17.330
refining the value function again,
thus improving the policy.

111
00:07:17.660 --> 00:07:20.360
It'll just keep doing that until it arrives.

112
00:07:20.361 --> 00:07:23.150
At the optimal policy for our solution,

113
00:07:23.450 --> 00:07:28.010
which will consequently also give us the optimal value function in our code will

114
00:07:28.011 --> 00:07:30.950
have an iterator that examines each state,

115
00:07:31.340 --> 00:07:36.340
does a one step look ahead over the action we take as defined by the policy and

116
00:07:37.221 --> 00:07:40.880
all the different ways that our algorithm could take us,

117
00:07:41.210 --> 00:07:44.870
looks at all the successor states that we could end up in.

118
00:07:45.110 --> 00:07:50.110
Then sums the probabilities of being an each state which is going to give us an

119
00:07:50.210 --> 00:07:54.260
updated returned for the particular state we started from.

120
00:07:54.800 --> 00:07:59.800
We can call this update the expected update due to the fact that it's the result

121
00:08:00.080 --> 00:08:03.740
of going through every possible next state instead of a sample.

122
00:08:04.250 --> 00:08:07.400
If we move i-phones from one store to another,

123
00:08:07.520 --> 00:08:12.520
we don't know exactly how many I phones will be bought or delivered and thus how

124
00:08:13.281 --> 00:08:15.500
many phones we'll have at the end of the day.

125
00:08:15.560 --> 00:08:20.560
What we can do though is calculate the probabilities of different outcomes

126
00:08:20.930 --> 00:08:25.930
happening and some those probabilities together to come up with an

127
00:08:26.211 --> 00:08:30.380
approximation.
We will replicate the approximation of it,

128
00:08:30.381 --> 00:08:35.381
the state value from the last iteration with an updated version and do this for

129
00:08:36.321 --> 00:08:41.321
all states positions between when we first start evaluating and when we end up

130
00:08:41.541 --> 00:08:43.700
settling on a true value function.

131
00:08:43.820 --> 00:08:46.790
There are many value functions we'll have to create.

132
00:08:47.180 --> 00:08:52.180
This iterative application of the bellman expectation equation is supremely

133
00:08:52.641 --> 00:08:53.474
useful.

134
00:08:53.570 --> 00:08:58.080
Once we've fitted the function to the random policy we initialize at the

135
00:08:58.081 --> 00:09:01.020
beginning,
which by the way was choosing to not move.

136
00:09:01.021 --> 00:09:05.550
I phones from one store to another,
we're ready for some policy improvement.

137
00:09:05.700 --> 00:09:09.870
Improving the policy involves testing actions at each state and choosing the

138
00:09:09.871 --> 00:09:13.020
best action from them.
And unlike evaluation,

139
00:09:13.021 --> 00:09:17.640
we iterate through all the actions.
Then have a list of returns to look over.

140
00:09:18.090 --> 00:09:21.060
So when we plot out the results from our agent,

141
00:09:21.090 --> 00:09:26.090
we can see a nifty diagram that shows what our policy would do given a certain

142
00:09:26.761 --> 00:09:29.520
number of i-phones in each store.

143
00:09:29.850 --> 00:09:33.540
It wouldn't switch iPhones to other stores generally,

144
00:09:33.541 --> 00:09:38.541
which is made clear by the huge concentration of policies in the middle of the

145
00:09:38.761 --> 00:09:39.594
graph.

146
00:09:39.720 --> 00:09:44.090
But for a number of cases where store one has more iPhones,

147
00:09:44.100 --> 00:09:49.100
then store to the optimal policy would be to move I iPhones to store too and

148
00:09:50.131 --> 00:09:54.270
there are some scenarios where the optimal policy would move I iPhones from

149
00:09:54.271 --> 00:09:59.100
store two to store one but the threshold is higher and if we consider our

150
00:09:59.101 --> 00:10:01.110
problem statement it makes sense.

151
00:10:01.440 --> 00:10:06.440
We already know store two has a higher purchase rate than both its own delivery

152
00:10:06.811 --> 00:10:09.630
rate as well as store ones purchase rate.

153
00:10:10.080 --> 00:10:14.910
It's pretty cool that no matter where we started with either our value function

154
00:10:14.911 --> 00:10:19.800
or our policy,
we ended up with an optimal value function and policy.

155
00:10:20.340 --> 00:10:22.260
All of what we've done so far,

156
00:10:22.350 --> 00:10:27.270
dynamic programming that is assumes we have complete knowledge of the

157
00:10:27.271 --> 00:10:30.810
environment or all possible transitions,

158
00:10:30.930 --> 00:10:35.640
but what kind of technique could we use if we don't block chain?

159
00:10:35.641 --> 00:10:38.130
Just kidding.
That's a topic for the next video.

160
00:10:38.640 --> 00:10:41.550
Three closing points here in dynamic programming,

161
00:10:41.670 --> 00:10:46.670
policy iteration is a modification of value iteration to directly compute the

162
00:10:47.611 --> 00:10:51.210
optimal policy for a given mark cop decision process.

163
00:10:51.720 --> 00:10:54.480
Policy Iteration consists of two steps,

164
00:10:54.840 --> 00:10:57.930
policy evaluation and policy improvement.

165
00:10:58.260 --> 00:11:03.210
And while value iteration is a simpler algorithm,
then policy iteration,

166
00:11:03.420 --> 00:11:07.860
it's more computationally expensive.
You just keep on learning,
don't you?

167
00:11:07.950 --> 00:11:11.670
I love it.
It subscribed to prevent the apocalypse for now,

168
00:11:11.700 --> 00:11:14.790
I've got to iterate on my skills,
so thanks for watching.

