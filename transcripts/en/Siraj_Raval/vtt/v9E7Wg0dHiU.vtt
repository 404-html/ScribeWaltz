WEBVTT

1
00:00:01.050 --> 00:00:03.030
Deep mind released a new paper.

2
00:00:05.700 --> 00:00:08.760
Hello world.
It's Saroj and deep mind.

3
00:00:08.790 --> 00:00:13.710
Just drop a paper titled Neural Arithmetic Logic units.

4
00:00:13.980 --> 00:00:17.490
It's a really cool idea that I'll demo in this video.

5
00:00:17.700 --> 00:00:20.760
Capable of learning a diverse set of tasks,

6
00:00:20.910 --> 00:00:22.740
including tracking time,

7
00:00:22.920 --> 00:00:27.450
translating words to numbers and counting objects in images.

8
00:00:27.630 --> 00:00:28.500
If you're new here,

9
00:00:28.501 --> 00:00:33.450
hit the subscribe button to stay up to date on my latest AI content.

10
00:00:33.870 --> 00:00:38.870
We all know that deep neural networks are amazing at learning very complex

11
00:00:40.260 --> 00:00:41.093
functions.

12
00:00:41.460 --> 00:00:46.460
They can learn seemingly anything and have been applied to a wide range of

13
00:00:47.521 --> 00:00:52.521
contexts from autonomous vehicles to dialogue generation to generating terrible

14
00:00:54.301 --> 00:00:55.170
pickup lines.

15
00:00:55.500 --> 00:01:00.500
But one of the longstanding objections do deep learning is that neural networks

16
00:01:01.830 --> 00:01:04.530
are not able to deal with numbers.

17
00:01:04.860 --> 00:01:09.860
That includes learning numerical functions that extrapolate beyond the training

18
00:01:10.501 --> 00:01:11.190
data.

19
00:01:11.190 --> 00:01:16.140
Storing numbers outside of the range of values seen during training.

20
00:01:16.560 --> 00:01:21.540
Even basic counting has been nearly impossible for them to learn.

21
00:01:21.570 --> 00:01:22.500
Specifically,

22
00:01:22.530 --> 00:01:26.970
modern neural architectures can learn to count in the training data.

23
00:01:27.470 --> 00:01:29.100
In the testing data sets.

24
00:01:29.310 --> 00:01:33.810
They can only count as high as what they saw in the training data.

25
00:01:34.290 --> 00:01:39.150
This is more akin to memorization than generalization.

26
00:01:39.450 --> 00:01:44.450
It's like memorizing a vocabulary of numbers instead of learning about the idea

27
00:01:45.481 --> 00:01:47.940
of numeracy more generally.

28
00:01:48.300 --> 00:01:53.300
What makes Nalu so exciting is that it's a breakthrough in what has historically

29
00:01:54.121 --> 00:01:56.490
been a crux for neural networks,

30
00:01:56.760 --> 00:02:01.760
that they can't extrapolate basic numerical functions to numbers outside of the

31
00:02:02.821 --> 00:02:05.860
range they've been trained with.
Now,

32
00:02:05.861 --> 00:02:10.861
Lou turns out to be an attention mechanism deciding which parts of the input and

33
00:02:11.310 --> 00:02:15.240
output should have certain operations applied to them.

34
00:02:15.570 --> 00:02:19.230
It can learn a function over a small range of numbers.

35
00:02:19.410 --> 00:02:24.410
Then extrapolate that function orders of magnitude later for the first time.

36
00:02:24.900 --> 00:02:29.760
It allows neural networks to learn the general concept of a number.

37
00:02:30.120 --> 00:02:35.120
The authors also noted that this learning bias accurately fits a phenomenon in

38
00:02:35.431 --> 00:02:38.040
nature of numeracy and arithmetic.

39
00:02:38.280 --> 00:02:42.630
Many different types of species from baboons to bees can do this.

40
00:02:42.960 --> 00:02:47.640
Yes.
Even bees can count be you'd.
If I know also,

41
00:02:47.641 --> 00:02:50.250
I just like to make a side note here.

42
00:02:50.490 --> 00:02:55.490
Most algorithmic breakthroughs in AI like this one or just a novel,

43
00:02:56.191 --> 00:03:01.191
clever new combination of differentiable blocks to existing architectures.

44
00:03:02.620 --> 00:03:07.620
That means the barrier to entry to algorithmic research is quite low.

45
00:03:08.140 --> 00:03:12.450
You too can do this more on that in a later video anyways,

46
00:03:12.640 --> 00:03:17.640
to demonstrate that neural networks can't extrapolate well normally they created

47
00:03:18.401 --> 00:03:21.670
a series of neural networks called auto encoders.

48
00:03:21.970 --> 00:03:26.970
Auto encoders are computation graphs that reconstruct the input data learning a

49
00:03:27.701 --> 00:03:32.050
compressed representation of all the input data it's been given,

50
00:03:32.740 --> 00:03:37.740
and this representation is valuable for a very wide range of tasks.

51
00:03:38.020 --> 00:03:40.360
From classification to regression.

52
00:03:40.720 --> 00:03:44.410
Each auto encoder has the same hyper parameter values,

53
00:03:44.560 --> 00:03:47.470
the same learning rates,
the same number of layers.

54
00:03:47.471 --> 00:03:52.471
The only difference was that each had its own unique activation function.

55
00:03:53.770 --> 00:03:58.770
Neural networks are models created with linear Algebra and optimized using

56
00:03:59.501 --> 00:04:00.334
calculus.

57
00:04:00.490 --> 00:04:05.440
If these models didn't have activation functions included in their chain of

58
00:04:05.441 --> 00:04:08.650
operations that they would apply to the input data,

59
00:04:08.920 --> 00:04:12.580
they would only be able to learn linear functions,

60
00:04:12.880 --> 00:04:17.880
but activation functions allow them to learn both linear and nonlinear

61
00:04:18.011 --> 00:04:18.611
functions,

62
00:04:18.611 --> 00:04:23.290
giving them the Almighty universal function approximator title,

63
00:04:23.620 --> 00:04:28.620
they had each Kado encoder in code numbers between negative five and five that

64
00:04:29.261 --> 00:04:32.140
was the training data as noted in the graph.

65
00:04:32.170 --> 00:04:37.170
When the auto encoders were done training and given novel numbers to reconstruct

66
00:04:38.260 --> 00:04:42.220
as long as those numbers were in the range of the training data,

67
00:04:42.490 --> 00:04:46.120
the reconstruction was pretty accurate with a low error,

68
00:04:46.420 --> 00:04:49.000
but as soon as any of the auto encoders,

69
00:04:49.001 --> 00:04:53.710
we're tasked with reconstructing numbers outside of that range,

70
00:04:53.950 --> 00:04:56.620
the error rate increased substantially.

71
00:04:56.830 --> 00:05:01.150
You were supposed to bring convergence to the model,
not leave it in darkness.

72
00:05:01.600 --> 00:05:06.600
Neural networks can generate new drug molecules for diseases but normally can't

73
00:05:08.021 --> 00:05:12.490
count as well as a five year old can.
So as a solution,

74
00:05:12.491 --> 00:05:15.670
the researchers proposed to new acronyms,

75
00:05:15.910 --> 00:05:20.910
I mean models that would be able to learn to represent and manipulate numbers in

76
00:05:22.091 --> 00:05:23.380
a systematic way.

77
00:05:23.710 --> 00:05:28.570
The first model is called the neuro accumulator or NAC,
NAC.

78
00:05:28.900 --> 00:05:32.170
It's supports the ability to accumulate quantities.

79
00:05:32.230 --> 00:05:37.230
Additively a simple idea that helps form the basis for the next model.

80
00:05:37.660 --> 00:05:42.660
The NAC is a special case of a linear layer whose weight matrix w consists of

81
00:05:43.811 --> 00:05:46.600
just negative ones,
Zeros and ones.

82
00:05:46.870 --> 00:05:51.870
It's outputs are additions or subtractions of rose in the input vector instead

83
00:05:52.151 --> 00:05:57.151
of arbitrary rescaling and that prevents the layer from changing the scale of

84
00:05:57.831 --> 00:06:00.530
the representations of the numbers.

85
00:06:00.560 --> 00:06:03.200
When mapping the input to the output,

86
00:06:03.470 --> 00:06:06.950
no matter how many operations are chained together.

87
00:06:07.220 --> 00:06:08.750
So given our input data,

88
00:06:08.751 --> 00:06:13.010
we multiply it by the weight matrix w to get the output.

89
00:06:13.400 --> 00:06:18.400
The weight matrix w is computed by adding the Tan h activation of w hat

90
00:06:20.120 --> 00:06:23.510
multiplied by the sigmoid activation of m hat.

91
00:06:23.930 --> 00:06:25.880
This whole thing is differentiable.

92
00:06:25.881 --> 00:06:29.840
We can compute the derivative with respect to our weights here.

93
00:06:30.170 --> 00:06:35.030
What that means is that we can use the popular gradient descent optimization

94
00:06:35.031 --> 00:06:39.620
strategy from calculus to help improve this network's prediction,

95
00:06:39.650 --> 00:06:44.090
but while addition and subtraction are useful and equally robust,

96
00:06:44.091 --> 00:06:49.091
ability to learn more complex math functions like multiplication would be

97
00:06:49.221 --> 00:06:53.300
desirable as well,
right?
That's where the second model,

98
00:06:53.330 --> 00:06:58.280
the neural arithmetic logic unit or Lou comes into play.

99
00:06:58.460 --> 00:07:02.030
It learns a weighted sum between two sub cells.

100
00:07:02.330 --> 00:07:05.960
One is capable of addition and subtraction,

101
00:07:06.170 --> 00:07:10.910
while the other is capable of the other operations,
multiplication,

102
00:07:11.030 --> 00:07:12.830
division,
and power functions.

103
00:07:13.010 --> 00:07:17.300
It demonstrates how the neck can be extended with gate controlled operations,

104
00:07:17.510 --> 00:07:22.510
which allows for end to end learning of new classes of numerical functions not

105
00:07:23.840 --> 00:07:25.430
seen in the training data.

106
00:07:25.940 --> 00:07:30.940
The nol Lou consists of two neck cells interpolated by a learned sigmoidal gate

107
00:07:32.360 --> 00:07:37.360
g such that if the add subtract sub cells output value is applied with the

108
00:07:38.091 --> 00:07:43.091
weight of one or on the multiply and divide sub cell is zero or off and vice

109
00:07:44.691 --> 00:07:49.040
versa.
So the output a is computed by the first neck.

110
00:07:49.370 --> 00:07:54.370
This stores the result of the Nol lose addition and subtraction operations.

111
00:07:54.770 --> 00:07:59.210
The second neck learns to multiply and divide the output.

112
00:07:59.211 --> 00:08:04.211
Y is computed by applying several operations to the computed variables and

113
00:08:05.211 --> 00:08:07.020
altogether Analou,

114
00:08:07.070 --> 00:08:11.960
which consists of two knacks can learn arithmetic functions consisting of the

115
00:08:11.961 --> 00:08:16.961
major math operations in a way that extrapolates to numbers outside of the range

116
00:08:18.261 --> 00:08:20.870
observed during the training process.

117
00:08:20.900 --> 00:08:25.790
It can be added to existing neural network architectures as a new module from

118
00:08:25.791 --> 00:08:28.910
convolutional networks to recurrent networks.

119
00:08:29.180 --> 00:08:34.180
It's a few new variables and operations cleverly ordered in a way that allows

120
00:08:35.151 --> 00:08:38.360
these networks to learn fundamental arithmetic.

121
00:08:38.450 --> 00:08:42.530
They experimented with the Nalo on a couple of different tasks.

122
00:08:42.860 --> 00:08:45.740
First was learning a simple function,

123
00:08:45.741 --> 00:08:50.741
meaning learning to select relevant inputs and apply different arithmetic

124
00:08:51.261 --> 00:08:55.770
functions to them.
It extrapolated very well to data.

125
00:08:56.130 --> 00:08:56.850
Next,

126
00:08:56.850 --> 00:09:01.320
they trained it to count how many types of handwritten character images it was

127
00:09:01.321 --> 00:09:05.460
shown and it again extrapolated well to new data.

128
00:09:05.820 --> 00:09:10.820
They also trained it to be able to translate a text number expression into a

129
00:09:11.431 --> 00:09:15.210
scalar representation.
A simple mapping that can be learned,

130
00:09:15.240 --> 00:09:19.140
which LSTM networks have shown to be the best at,

131
00:09:19.440 --> 00:09:22.820
but when the Lstm was augmented with the [inaudible],

132
00:09:22.980 --> 00:09:25.020
it achieved even better results.

133
00:09:25.080 --> 00:09:28.470
And what would a machine learning experiment be without sprinkling some

134
00:09:28.471 --> 00:09:30.240
reinforcement learning on it,
right.

135
00:09:30.750 --> 00:09:35.580
While the previous tasks all involved making numeric predictions alone,

136
00:09:35.850 --> 00:09:40.850
what would it system that uses numeric computations internally to help guide a

137
00:09:41.011 --> 00:09:46.011
larger goal look like using a simple grid world environment as a starting point,

138
00:09:47.250 --> 00:09:52.250
they built a reinforcement learning agent that learned how to track the time in

139
00:09:52.381 --> 00:09:56.550
the game.
The agent has the ability to move in different directions,

140
00:09:56.730 --> 00:10:01.730
is given a time and receives a reward if it arrives at a particular location at

141
00:10:02.551 --> 00:10:04.650
that time.
At each time step,

142
00:10:04.651 --> 00:10:09.651
the agent receives as input a pixel representation of the state of the world and

143
00:10:10.231 --> 00:10:14.400
has to select an action to take like move left or move up.

144
00:10:14.760 --> 00:10:19.760
It also receives an integer instruction t that says when exactly the agent

145
00:10:20.281 --> 00:10:22.530
should arrive at the destination.

146
00:10:22.920 --> 00:10:27.920
In the baseline model t was given to the agent as a new input concatenated with

147
00:10:28.201 --> 00:10:30.390
the output of the visual module,

148
00:10:30.600 --> 00:10:35.550
then pass to its LSTM core memory.
In the improved model,

149
00:10:35.580 --> 00:10:40.530
t was passed through a knack and then back into the LSTM.
Of course,

150
00:10:40.531 --> 00:10:43.530
the [inaudible] agent performed better than the baseline.

151
00:10:43.680 --> 00:10:48.680
There are already almost a dozen implementations of Nalo on get hub right now in

152
00:10:49.620 --> 00:10:53.790
many different python frameworks including Pi torch care,

153
00:10:53.791 --> 00:10:58.710
Ross and tensorflow,
and this paper is only a few days old.

154
00:10:58.980 --> 00:11:00.840
Pretty incredible stuff.
Also,

155
00:11:00.841 --> 00:11:04.980
shout out to my friend and paper author Andrew Trask at deep mind for his work

156
00:11:04.981 --> 00:11:05.820
on the paper.

157
00:11:06.000 --> 00:11:10.920
I had a few questions that he helped clarify before I made this video.
Numeracy,

158
00:11:10.950 --> 00:11:15.950
the ability to truly understand what numbers mean and how they interact with

159
00:11:16.231 --> 00:11:19.650
each other.
He's different than just memorizing them,

160
00:11:19.860 --> 00:11:23.460
and now Lou helps give neural nets that capability.

161
00:11:23.730 --> 00:11:28.560
It's very exciting and there's a lot of room to build on this work.

162
00:11:28.620 --> 00:11:32.130
To further increase the capabilities of neural networks,

163
00:11:32.310 --> 00:11:35.130
links to some awesome educational resources,

164
00:11:35.250 --> 00:11:39.120
and the code will be in the video description.
Oh Em,
gee,

165
00:11:39.121 --> 00:11:42.390
you made it to the end.
Hit subscribe if you haven't yet,

166
00:11:42.540 --> 00:11:45.840
and don't forget to connect with me on Twitter,
Instagram,

167
00:11:45.870 --> 00:11:48.750
and Facebook for more educational content.

168
00:11:49.020 --> 00:11:52.830
Those links are right here for now.
I've got to stop computing,

169
00:11:52.890 --> 00:11:54.530
so thanks for watching.

