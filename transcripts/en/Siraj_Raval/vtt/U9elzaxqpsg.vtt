WEBVTT

1
00:00:00.280 --> 00:00:01.670
Hello world,
it's Raj.

2
00:00:01.690 --> 00:00:05.560
And have you ever wanted to create songs using AI?

3
00:00:05.740 --> 00:00:06.910
It's totally possible.

4
00:00:06.911 --> 00:00:10.840
And I've got my friend Taryn here to help me explain how it all works.

5
00:00:10.920 --> 00:00:13.020
<v 1>Thank you for having me at the time.</v>

6
00:00:13.110 --> 00:00:17.190
I've been watching your videos for a while and very excited about this moment in

7
00:00:17.191 --> 00:00:19.840
time,
so thank you.
Hey everybody.

8
00:00:19.841 --> 00:00:24.370
I'm Taryn southern and I've been a youtuber for quite a long time.

9
00:00:24.820 --> 00:00:29.590
Got My start in making music videos on Youtube about 10 years ago and recently

10
00:00:29.591 --> 00:00:32.080
released my first single off of my upcoming album,

11
00:00:32.081 --> 00:00:36.040
which is composed entirely with artificial intelligence as Suraj said.

12
00:00:36.041 --> 00:00:39.850
So really been exciting to work with this new medium.

13
00:00:40.420 --> 00:00:42.910
<v 0>Totally.
It's super cool.
You guys got to check it out.</v>

14
00:00:42.911 --> 00:00:44.740
Links are going to be in the description.
Okay.

15
00:00:44.741 --> 00:00:47.530
So why did you want to create songs using AI?

16
00:00:48.880 --> 00:00:53.300
<v 1>I think it,
it,
it seemed like a great challenge.
You know,
I,
I just,</v>

17
00:00:53.350 --> 00:00:56.470
I liked doing things that I haven't done before.
Um,

18
00:00:56.500 --> 00:00:59.770
music creation has always been a little bit of a mystery to me because I don't

19
00:00:59.771 --> 00:01:03.670
play instruments.
I grew up trying to learn how to play guitar,

20
00:01:03.671 --> 00:01:07.810
but my hands were so small and not very nimble across the guitar that I

21
00:01:07.811 --> 00:01:10.870
basically gave up.
I played a little bit of piano,

22
00:01:10.871 --> 00:01:13.960
but I had this narcoleptic piano teacher who would fall asleep halfway through

23
00:01:13.961 --> 00:01:17.530
the lesson.
So that didn't work out so well either.
So for me,

24
00:01:17.920 --> 00:01:21.390
writing music in my,
in my twenties in adulthood,
um,

25
00:01:21.400 --> 00:01:25.780
really just became about writing by ear.
And so when I found the first pieces,

26
00:01:25.830 --> 00:01:29.590
a few pieces of software that could actually write music using code,

27
00:01:29.591 --> 00:01:32.320
I got really excited because I thought now finally someone like me who doesn't

28
00:01:32.321 --> 00:01:36.130
play instruments could potentially make the music that I hear in my head with

29
00:01:36.131 --> 00:01:40.640
this new collaboration partner.
So it was an exciting challenge and,
uh,

30
00:01:41.070 --> 00:01:42.460
it was like crossing new frontier.

31
00:01:43.930 --> 00:01:46.300
<v 0>Does that,
I mean,
and this new partner being AI.</v>

32
00:01:46.650 --> 00:01:48.990
<v 1>Yup.
And they never get tired or hungry.</v>

33
00:01:53.300 --> 00:01:56.850
And if you don't like their song,
you just throw it out,
throw it out the window.

34
00:01:57.530 --> 00:02:00.530
<v 0>Scott is out of here.
Okay.
So,
so tell us about the process.</v>

35
00:02:00.560 --> 00:02:02.760
You use it to do this with Ai.
So

36
00:02:02.780 --> 00:02:04.820
<v 1>it's different depending on the software.</v>

37
00:02:04.821 --> 00:02:08.600
And there are now a number of different AI software companies out there that can

38
00:02:08.601 --> 00:02:13.550
compose music from Google,
Magenta to ampere to AI music,
I'Veh,

39
00:02:13.730 --> 00:02:17.510
Ibm Watson,
like I'm forgetting some of the main ones I'm collaborating with.

40
00:02:17.750 --> 00:02:22.220
So they all function differently depending on the neural network and how the,

41
00:02:22.500 --> 00:02:26.500
uh,
how the engineer has actually created the interface.
So I will say that the,

42
00:02:26.750 --> 00:02:30.200
the easiest one to use if you just want to get started and understand what kinds

43
00:02:30.201 --> 00:02:34.220
of things AI can actually create musically.
Ampere um,

44
00:02:34.221 --> 00:02:37.010
and that's the one that I used for the single that I released in September

45
00:02:37.011 --> 00:02:37.940
called break free.

46
00:02:38.480 --> 00:02:43.280
The cool thing about amber is it actually produces all of the music.

47
00:02:43.281 --> 00:02:44.550
So what,
what,

48
00:02:44.551 --> 00:02:47.390
what I mean by that is all the instrumentation that you hear in that song is

49
00:02:47.391 --> 00:02:49.970
actually put together using Ai.
It wasn't,

50
00:02:50.150 --> 00:02:54.540
it was not a musical track composed by AI performed by humans.
Um,

51
00:02:54.550 --> 00:02:57.860
so it gives you a sense of what's possible because it's actually really hard to

52
00:02:57.861 --> 00:03:01.480
take random sounds and put them together into something cohesive.

53
00:03:01.481 --> 00:03:06.160
That sounds good.
Um,
so I would start with ampere.
That's,
that's my cat.

54
00:03:06.161 --> 00:03:10.150
Tiggy no,
that's just the Raj has backpack.
Tiggy don't do that.

55
00:03:11.820 --> 00:03:15.460
She's destroying your backpack.
I'll just,
you have an AI make one.

56
00:03:15.461 --> 00:03:17.710
We already have adversarial networks.
Right.
Okay,
perfect.

57
00:03:17.980 --> 00:03:22.980
So if you're looking to do the most basic kind of introductory lesson in AI

58
00:03:24.461 --> 00:03:25.780
music creation,
I would go to amber.

59
00:03:26.020 --> 00:03:29.770
If you are familiar with code and you want to get your hands a little bit dirty,

60
00:03:29.950 --> 00:03:34.840
I would say Google,
Magenta or IBM Watson are great places to go.

61
00:03:35.170 --> 00:03:35.430
Google,

62
00:03:35.430 --> 00:03:38.830
Magenta their software and they have a number of different programs available

63
00:03:38.831 --> 00:03:41.110
for AI music writing that,
two different things.

64
00:03:41.620 --> 00:03:45.230
It's all available on get hub I believe.
I think um,

65
00:03:45.231 --> 00:03:49.570
and Ibm Watson is actually really seeing there a software open source in January

66
00:03:49.840 --> 00:03:54.310
and I've been using that the last month to write stuff and I love it.
It's cool.

67
00:03:54.550 --> 00:03:57.400
So you do need to have knowledge coding knowledge,

68
00:03:57.401 --> 00:04:01.870
you'll be using terminal to write most of your songs.
Um,
yeah,

69
00:04:01.871 --> 00:04:03.760
I had to like for the first couple of weeks on it,

70
00:04:03.761 --> 00:04:06.760
I was calling their developers being like,
wait,
how do I do this thing?

71
00:04:07.030 --> 00:04:09.040
So basically with a program like Watson or Magenta,

72
00:04:09.041 --> 00:04:10.900
you're going to set different musical parameters.

73
00:04:11.050 --> 00:04:15.220
Like what type of song you want the Ai to create.
Is it pop,
is it reggae,

74
00:04:15.221 --> 00:04:19.960
is it a mixture of reggae and pop?
What's the tempo,
what's the,
what's the bpm,

75
00:04:19.961 --> 00:04:24.010
what are the instruments that you want embedded into the track?
Um,

76
00:04:24.070 --> 00:04:29.070
and in some cases you can also give the AI a track or a multitude of tracks to

77
00:04:31.390 --> 00:04:35.290
teach it.
Um,
basic rules based on those tracks that you like.

78
00:04:35.291 --> 00:04:37.480
So if you want to make a song that sounds like the Beatles,

79
00:04:37.780 --> 00:04:41.410
you basically just feed it a whole catalog of Beatles songs.
It'll do its thing.

80
00:04:41.411 --> 00:04:44.840
And then you say,
I want the song that it creates to be a reggae track.

81
00:04:44.841 --> 00:04:49.660
So now you've got a reggae track inspired by the Beatles,
right?

82
00:04:49.661 --> 00:04:52.900
So,
so there are a lot of,
depending on how skilled you are at coding,

83
00:04:52.901 --> 00:04:55.630
there are a lot of ways that you can take these,

84
00:04:55.690 --> 00:04:57.770
these softwares to do really cool things.
Um,

85
00:04:57.880 --> 00:04:59.980
there's a program at Google called incense,

86
00:04:59.981 --> 00:05:03.280
which allows you to actually take two disparate sounds and combine them
together.

87
00:05:03.281 --> 00:05:07.510
So I could actually record my cat tiggy scratching on Suraj his backpack and

88
00:05:07.511 --> 00:05:11.680
then combine that with the sound of nails on a chalkboard to create a truly

89
00:05:11.681 --> 00:05:15.010
delightful,
um,
tiggy decided to say hi,

90
00:05:16.240 --> 00:05:20.260
this is the little runt that's ruining the video.
I can't even fake it.

91
00:05:20.261 --> 00:05:25.060
I don't really like cat stuff.
But essentially,

92
00:05:25.140 --> 00:05:27.640
I mean,
uh,
you,
you can,
you can create a whole,

93
00:05:27.641 --> 00:05:29.790
an entirely new sound using some of these,

94
00:05:29.890 --> 00:05:33.850
these tools and inject that into your musical work.
So,
um,

95
00:05:34.300 --> 00:05:38.200
this all goes to say that they function differently depending on which you

96
00:05:38.201 --> 00:05:43.120
choose to use and your coding knowledge and your musical knowledge.

97
00:05:43.480 --> 00:05:47.530
But it is absolutely untrue that using AI is not a creative process.

98
00:05:47.531 --> 00:05:51.460
It's so creative because there are so many different avenues that you can take

99
00:05:51.461 --> 00:05:56.461
in creating a song and creating new sounds and I actually think that it has the

100
00:05:56.870 --> 00:05:59.960
ability to bring out the super composer and all of us.

101
00:06:00.750 --> 00:06:04.950
<v 0>Totally awesome.
So how does Stanford make this all happen?
Well,</v>

102
00:06:05.010 --> 00:06:08.820
it's proprietary technology,
so we're out of luck.
See you next time.

103
00:06:09.780 --> 00:06:13.500
Just kidding.
We can totally reverse engineer how it's very likely done.

104
00:06:13.530 --> 00:06:16.050
Looking at some of the latest research in deep learning,

105
00:06:16.350 --> 00:06:20.490
we know that one popular way to class machine learning algorithms isn't the

106
00:06:20.491 --> 00:06:24.180
categories of generative and discriminative models.

107
00:06:24.240 --> 00:06:28.230
Discriminative models classify data into sets of categories.

108
00:06:28.231 --> 00:06:31.800
They allow us to differentiate between different types of data,

109
00:06:32.310 --> 00:06:36.930
generative models.
On the other hand,
offer even more exciting possibilities,

110
00:06:37.080 --> 00:06:40.290
creating new data after going through a training phase,

111
00:06:40.380 --> 00:06:44.340
one type of generative model or called the audio synthesis models.

112
00:06:44.520 --> 00:06:48.240
These are useful across the range of applications that we use day to day,

113
00:06:48.241 --> 00:06:50.310
including Texas speech systems.

114
00:06:50.460 --> 00:06:55.290
The kind that Google translate uses to help dictate what you've just translated

115
00:06:55.530 --> 00:06:59.070
or the kind that voice assistance use to respond to our requests.

116
00:06:59.190 --> 00:07:03.870
Generic synthesizers have been used in music extensively and have a long history

117
00:07:03.871 --> 00:07:08.190
in that field from Djs to terrible hip hop and producers like t pain.

118
00:07:08.490 --> 00:07:13.470
They're usually hand designed instruments that that except control signals like

119
00:07:13.471 --> 00:07:14.760
pitch and velocity to shake.

120
00:07:14.761 --> 00:07:19.560
The tone and dynamic of a sound synthesizers have had a huge impact on culture

121
00:07:19.561 --> 00:07:24.000
and music the past few decades as some of the most popular songs out there have

122
00:07:24.010 --> 00:07:24.843
used them.

123
00:07:24.870 --> 00:07:28.860
So the traditional way of doing this well so you some sort of arrangement of

124
00:07:28.861 --> 00:07:32.250
oscillators or an algorithm for sampling playback.

125
00:07:32.580 --> 00:07:37.580
But a newer way to do this is by using a data driven approach that is letting an

126
00:07:38.221 --> 00:07:40.560
algorithm learn from musical data.

127
00:07:40.980 --> 00:07:45.630
It's possible to create new types of expressive and realistic instrumental

128
00:07:45.631 --> 00:07:47.100
sounds using deep learning.

129
00:07:47.610 --> 00:07:52.080
We know that deep learning allows us to learn features from data and in the case

130
00:07:52.081 --> 00:07:56.940
of music,
these features can represent tone,
timbre,
dynamics,

131
00:07:56.990 --> 00:08:01.170
crunk.
Notice all of these can be tunable knobs for music generator.

132
00:08:01.680 --> 00:08:06.680
Google has been at the forefront of a lot of progress in AI generated music and

133
00:08:06.721 --> 00:08:09.390
they have an open source project called Magenta.

134
00:08:09.510 --> 00:08:13.230
Magenta embodies a lot of the recent models that had been published in that

135
00:08:13.231 --> 00:08:16.590
field.
A more recent model just published a few months ago,

136
00:08:16.620 --> 00:08:18.450
used two concepts together.

137
00:08:18.810 --> 00:08:23.810
The first was a wavenet style auto encoder that learns Tim poral hidden nodes to

138
00:08:24.901 --> 00:08:27.060
capture longer term structure.

139
00:08:27.410 --> 00:08:30.860
<v 1>Exactly.
And the second was called incentive,</v>

140
00:08:30.861 --> 00:08:34.940
which is an enormous data set of music that can be used to explore neural audio

141
00:08:34.941 --> 00:08:38.300
synthesis of musical notes.
Just like I mentioned earlier,

142
00:08:38.301 --> 00:08:43.250
so infant contains about 300,000 for second annotated notes from about a

143
00:08:43.251 --> 00:08:44.630
thousand musical instruments,

144
00:08:44.810 --> 00:08:47.990
an order of a magnitude more than any other similar dataset.

145
00:08:48.020 --> 00:08:52.190
So it's pretty powerful.
And like I said,
you can combine sounds of your cat with,

146
00:08:52.970 --> 00:08:56.160
I don't know anything,
which is pretty cool.
Even that created system

147
00:08:56.160 --> 00:08:56.940
<v 0>called wavenet.</v>

148
00:08:56.940 --> 00:09:00.900
It generated raw audio wave forms and waiting that models a conditional

149
00:09:00.901 --> 00:09:05.430
probability of to have a way for him to generate new music that uses all

150
00:09:05.431 --> 00:09:07.920
previous samples and other parameters to do this.

151
00:09:08.220 --> 00:09:10.390
The authors of our audio synthesis paper,

152
00:09:10.410 --> 00:09:14.670
we're inspired by that model to create something similar to weight net.

153
00:09:15.030 --> 00:09:16.770
There were two motivations here.

154
00:09:17.070 --> 00:09:21.090
The first was to be able to create music that had a consistent long term

155
00:09:21.091 --> 00:09:25.380
structure so that the theme would carry out through the end of the piece.

156
00:09:25.620 --> 00:09:27.300
I'm like anything off of Jesus.

157
00:09:27.660 --> 00:09:32.660
The second was to use the learned features or applications like using meaningful

158
00:09:32.701 --> 00:09:36.060
audio interpolations like between different instruments.
For example,

159
00:09:36.061 --> 00:09:40.110
like what what Taryn did in the original wave net architecture.

160
00:09:40.140 --> 00:09:41.520
At each step of training,

161
00:09:41.850 --> 00:09:46.850
a stack of dilated convolutions predicted the next sample of audio from a fixed

162
00:09:47.161 --> 00:09:49.950
size input,
a prior sample values.

163
00:09:50.160 --> 00:09:54.810
The joint probability of some audio was factorized as a product of conditional

164
00:09:54.811 --> 00:09:57.060
probabilities between the different ways,

165
00:09:57.330 --> 00:10:00.150
but these researchers removed the need for external conditioning.

166
00:10:00.510 --> 00:10:02.870
Instead it works as an auto encoder,

167
00:10:02.930 --> 00:10:07.110
a generic one taking in raw audio wave form as input from which the encoder

168
00:10:07.111 --> 00:10:08.430
produces an embedded.

169
00:10:08.820 --> 00:10:12.900
Then they shifted the same input and fed it back into the decoder which

170
00:10:12.901 --> 00:10:14.670
reproduced the wit input way form.

171
00:10:14.940 --> 00:10:18.840
So using the joint probability they could parameterize a feature as a latent

172
00:10:18.841 --> 00:10:21.750
variable to train the model.
They use gradient descent.

173
00:10:21.810 --> 00:10:23.910
The most popular optimization scheme out there,

174
00:10:23.911 --> 00:10:28.500
shout degraded a set and it uses the difference between the predicted output and

175
00:10:28.501 --> 00:10:30.300
the actual output as an error value.

176
00:10:30.690 --> 00:10:34.170
Then using that error it calculates a great and using that gradient,

177
00:10:34.290 --> 00:10:38.220
it subsequently updates the weight values of the network over time to give it

178
00:10:38.221 --> 00:10:39.150
better predictions.

179
00:10:39.570 --> 00:10:43.980
It was trained over 1800 thousand iterations are really long time for the

180
00:10:43.981 --> 00:10:44.790
training data.

181
00:10:44.790 --> 00:10:49.170
Even though end synth annotated every single note using its unique pitch and

182
00:10:49.171 --> 00:10:53.220
timber and dynamics,
they decided to further annotate it,

183
00:10:53.250 --> 00:10:57.900
giving each sound a source,
a family of instruments,
and a sonic quality.

184
00:10:58.050 --> 00:10:58.831
Even though training,

185
00:10:58.831 --> 00:11:02.910
this type of deep learning model requires a massive amount of computing power.

186
00:11:02.970 --> 00:11:07.380
You could just download a model from get hub and run it on Google cloud or AWS

187
00:11:07.560 --> 00:11:08.790
is stuff has democratized,

188
00:11:08.791 --> 00:11:12.660
so you have access to computing power if you use the cloud or you could just

189
00:11:12.661 --> 00:11:17.040
pull a model off of the Magenta get hub page or use experiments.google.com and

190
00:11:17.041 --> 00:11:20.520
play with it in your browser.
So three things to remember here.

191
00:11:20.940 --> 00:11:24.870
Generative machine learning models that can learn latent variables from a

192
00:11:24.880 --> 00:11:29.210
dataset can be used to generate new data.
Similar to the training section,

193
00:11:29.700 --> 00:11:34.200
you can use a wave net style auto encoder and the end synth Dataset if you'd

194
00:11:34.201 --> 00:11:38.580
like to experiment with state of the art machine learning from music generation

195
00:11:39.090 --> 00:11:44.090
and wave net is perhaps the most powerful model out there that perform audio

196
00:11:44.191 --> 00:11:49.020
generation.
You have a really smart audience.
It's all about them.

197
00:11:49.150 --> 00:11:51.840
They are very smart.
If you want to see more AI music,

198
00:11:51.841 --> 00:11:55.080
you can follow me at youtube.com/terrane there we go.

199
00:11:55.380 --> 00:11:58.110
Please subscribe for more programming videos,
and I think we gotta go.

200
00:11:58.111 --> 00:12:01.770
We gotta go make some AI music.
It's true.
So thank you for watching.

