WEBVTT

1
00:00:00.090 --> 00:00:01.470
Hello world,
it's Saroj.

2
00:00:01.500 --> 00:00:05.850
And imagine that you've built a game and you want to predict whether or not your

3
00:00:05.851 --> 00:00:08.880
players are going to continue playing next month or not.

4
00:00:09.120 --> 00:00:14.120
You want to classify your players as going to churn or not going to churn and

5
00:00:14.221 --> 00:00:18.210
you've got a single data point and that is the amount of money that that user

6
00:00:18.211 --> 00:00:20.190
has spent in the game this month,

7
00:00:20.430 --> 00:00:25.430
either withdrawn or deposited to buy some in game currency or digital gold or

8
00:00:26.190 --> 00:00:29.430
some kind of items,
right?
And based on that data point alone,

9
00:00:29.431 --> 00:00:32.280
you want to predict whether or not the user is going to continue playing the

10
00:00:32.281 --> 00:00:36.780
game or not.
Now,
normally when predicting customer churn as it's called,

11
00:00:37.110 --> 00:00:38.580
you'd want to use several data points.

12
00:00:38.581 --> 00:00:40.920
You'd want to use a time that the user has logged in,

13
00:00:40.921 --> 00:00:42.750
you'd want to use a bunch of different features,

14
00:00:42.930 --> 00:00:45.240
but for the simple one dimensional example,

15
00:00:45.241 --> 00:00:48.690
we're just going to use that because what matters is the model that we're going

16
00:00:48.691 --> 00:00:52.860
to learn today and the model is a Gaussian mixture model.

17
00:00:53.250 --> 00:00:56.700
That's what the model is called and it is a universally used model for

18
00:00:56.701 --> 00:01:01.560
generative unsupervised learning or clustering,
which is what we're going to do.

19
00:01:02.370 --> 00:01:07.370
It's also called e em clustering or expectation maximization clustering based on

20
00:01:07.411 --> 00:01:10.890
the optimization strategy that we're going to use,
which we'll talk about,

21
00:01:11.250 --> 00:01:15.570
but the idea is very simple for a given dataset and in our case is going to be

22
00:01:15.571 --> 00:01:18.090
an amount of money withdrawn or deposited.

23
00:01:18.480 --> 00:01:23.130
Each point is generated by linearly combining multiple Gaussians.

24
00:01:23.520 --> 00:01:26.670
Okay,
so that's what we're going to do and before we start with this amazing,

25
00:01:26.700 --> 00:01:30.360
amazing example and code,
I want to say one thing.
Okay,

26
00:01:30.450 --> 00:01:34.050
this is a message from Saroj.
This is the message.
The message is,

27
00:01:34.350 --> 00:01:36.810
I had a,
this is a 32nd message.

28
00:01:36.990 --> 00:01:39.840
I went and I met subscribers for this channel.

29
00:01:39.841 --> 00:01:43.590
I met the community here in Amsterdam.
I met 15 or so people.

30
00:01:43.680 --> 00:01:48.680
I went to London this past weekend and I met about 15 or so people and I was

31
00:01:48.751 --> 00:01:52.710
just blown away by the quality of people that have subscribed to this channel.

32
00:01:52.711 --> 00:01:56.610
The community that we have built is incredible.
Seriously.

33
00:01:56.850 --> 00:01:59.070
There are people working for the United Nations.

34
00:01:59.190 --> 00:02:03.130
There are people who have met who are working to create a machine learning

35
00:02:03.180 --> 00:02:05.000
community in Tunisia there are,

36
00:02:05.040 --> 00:02:10.040
there are people who have apps with 500,000 plus users that are trying to stop

37
00:02:10.621 --> 00:02:14.000
smoking.
There are people who are working on global warming.
They are,

38
00:02:14.100 --> 00:02:17.730
they are legitimately working on using machine learning to prevent global

39
00:02:17.731 --> 00:02:21.930
warming and it is so inspiring and it is amazing and inspires me and it

40
00:02:21.931 --> 00:02:26.931
increases my conviction to help you guys get better because it's not about me.

41
00:02:27.690 --> 00:02:31.950
You guys are the real inspiration.
You are my hero,
okay?

42
00:02:32.130 --> 00:02:36.330
You are my hero for watching this,
for giving your attention to machine learning.

43
00:02:36.570 --> 00:02:41.570
We are the most important community in the world because we have an interest in

44
00:02:42.151 --> 00:02:44.610
the most important technology in the world.

45
00:02:44.760 --> 00:02:48.510
We are the most important community in the world and we are the ones who are

46
00:02:48.511 --> 00:02:52.980
going to solve the problems in this world using machine learning in the way that

47
00:02:52.981 --> 00:02:56.220
only we can remember.
We are smart and we are cool.

48
00:02:56.221 --> 00:02:59.830
I have met you guys and I'm going to continue meeting you guys in person.

49
00:02:59.831 --> 00:03:01.000
I'm going to India next month.

50
00:03:01.120 --> 00:03:04.720
I am very excited for that and I'm just going to meet all you guys everywhere.

51
00:03:04.721 --> 00:03:06.700
I'm going to do a world tour at some point.
But anyway,

52
00:03:07.180 --> 00:03:10.690
it's a very exciting time to be in this field and we are the,

53
00:03:10.691 --> 00:03:12.640
we are the real change makers.
Okay.

54
00:03:13.570 --> 00:03:15.220
We're the ones we're going to solve these problems.

55
00:03:15.221 --> 00:03:20.140
So it's amazing that we exist and we are growing at a rate of 500 to 800

56
00:03:20.141 --> 00:03:23.560
subscribers a day.
Reply to people in the comments.

57
00:03:23.740 --> 00:03:27.880
As for people ask people for questions in the slack channel.
We are growing,

58
00:03:27.910 --> 00:03:30.550
startups have been built in the slack channel by the way.

59
00:03:30.670 --> 00:03:34.210
It is incredible what is happening.
It is truly incredible.
Anyway,

60
00:03:34.211 --> 00:03:37.720
I was just excited about my past weekend in London meeting you guys and I'm

61
00:03:37.721 --> 00:03:40.480
going to continue to do so back to this.
Um,

62
00:03:41.290 --> 00:03:44.890
so we're going to build a Gaussian mixture model and first of all,

63
00:03:44.980 --> 00:03:48.100
what is a calcium?
Now,
I know I've kind of talked about this before,
but let's,

64
00:03:48.370 --> 00:03:52.600
let's go into it a little bit more.
A Gaussian is a type of distribution.

65
00:03:52.601 --> 00:03:56.830
It's a very popular and mathematically convenient type of distribution,

66
00:03:57.190 --> 00:04:02.190
but a distribution is a listing of outcomes of an experiment and the probability

67
00:04:02.471 --> 00:04:06.640
associated with each outcome.
So let's say we've got some kind of game,
right,

68
00:04:06.641 --> 00:04:10.150
and you could get a score of one or two,
three,
four,
five,
six.

69
00:04:10.480 --> 00:04:12.910
You could get a set of scores and you could,

70
00:04:12.970 --> 00:04:16.540
and then you want to calculate the frequency or the amount of times that a

71
00:04:16.541 --> 00:04:18.250
person has made this score,
right?

72
00:04:18.370 --> 00:04:21.130
So the tally and the frequency columns are the same thing.

73
00:04:21.790 --> 00:04:26.050
And so what we you could do is you can notice that there is a peak here that the

74
00:04:26.260 --> 00:04:30.790
n the number nine,
right?
So notice how this kind of follows a bell curve.

75
00:04:30.940 --> 00:04:32.860
The frequency goes up and then it goes down.

76
00:04:32.861 --> 00:04:36.850
Now it's not exact because seven is greater than six,
but more or less,

77
00:04:36.851 --> 00:04:39.100
it looks like it follows is a kind of bell curve.

78
00:04:39.280 --> 00:04:41.950
The frequencies go up as a scores go up.

79
00:04:41.980 --> 00:04:44.590
And then it has a peak value and then it goes down again.

80
00:04:44.620 --> 00:04:47.410
And we can represent this using a bell curve,

81
00:04:47.440 --> 00:04:49.930
otherwise known as a Gaussian distribution.

82
00:04:50.200 --> 00:04:54.430
And a Gaussian distribution is a type of distribution.
We're half the data,

83
00:04:54.431 --> 00:04:58.450
falls on the left of it and the other half falls on the right of it.
So it's an,

84
00:04:58.480 --> 00:05:01.750
it's an even distribution and you can notice just by the thought of it

85
00:05:01.751 --> 00:05:06.010
intuitively that has been very mathematically convenient.
Right?

86
00:05:06.250 --> 00:05:10.120
And so what do we need to define a Gaussian or normal distribution?

87
00:05:10.270 --> 00:05:13.960
What we need two variables,
we need a mean,
which is the,
uh,

88
00:05:14.020 --> 00:05:15.850
which is the average of all the data points.

89
00:05:16.030 --> 00:05:18.340
And that's going to define the center of the curve.

90
00:05:18.580 --> 00:05:20.470
And then we need the standard deviation,

91
00:05:20.471 --> 00:05:25.450
which describes how spread out the data is as well.
And once we have that,

92
00:05:25.480 --> 00:05:30.250
we can write out this curve.
And this is used throughout many fields.

93
00:05:30.251 --> 00:05:33.970
And you can think about many examples of where you,
you know,

94
00:05:33.971 --> 00:05:36.580
anytime you have some,
some kind of data that's increasing,

95
00:05:36.581 --> 00:05:37.990
has a peak and decreases,

96
00:05:38.200 --> 00:05:42.520
very likely a Gaussian distribution would be a great distribution to model this

97
00:05:42.521 --> 00:05:46.960
data,
whether it's a car or a roller coaster that's increasing in velocity,

98
00:05:47.410 --> 00:05:52.090
reaches a peak and then decreases or a sound wave acoustic wave forms.

99
00:05:52.330 --> 00:05:54.550
There's a lot of applications of this now.

100
00:05:54.580 --> 00:05:57.890
So that's a Gaussian distribution and I'll show you the formula in a second.

101
00:05:58.490 --> 00:06:02.600
But this is mold.
These are multiple Gaussian distributions,
right?

102
00:06:03.020 --> 00:06:06.470
Where we have multiple means and multiple standard deviations.

103
00:06:06.620 --> 00:06:10.940
And if we have those we can write out,
we can plot out multiple Gaussians,

104
00:06:11.690 --> 00:06:13.490
which is what we're going to do in a second.

105
00:06:13.520 --> 00:06:17.780
But the formula for a Gaussian distribution using the mean and the standard

106
00:06:17.781 --> 00:06:22.490
deviation,
once we calculate those two things,
we can write out this,
this,

107
00:06:22.550 --> 00:06:26.930
uh,
this formula,
this,
this graph.
And this is,

108
00:06:26.990 --> 00:06:30.950
this is the formula for it.
And it's called the probability density function.

109
00:06:30.980 --> 00:06:31.813
That's what it's called.

110
00:06:32.480 --> 00:06:36.290
So we can say for a given point x for a given point x,

111
00:06:36.380 --> 00:06:40.160
we can compute the associated why,
which is,
which are all of those values.

112
00:06:40.161 --> 00:06:44.060
Remember in a one dimensional case,
we know all the x values,

113
00:06:44.240 --> 00:06:48.710
we want to compute the y values and the y values are the probabilities for those

114
00:06:48.711 --> 00:06:52.790
x value.
So then given some new x value,
we can output the,

115
00:06:53.480 --> 00:06:57.590
the probability of that x value being a part of the curve or being a part of the

116
00:06:57.591 --> 00:07:00.530
Dataset or being a part of the class.
In this case.

117
00:07:00.920 --> 00:07:05.920
So we can say one over the standard deviation times the square root of two times

118
00:07:07.161 --> 00:07:12.161
Pi Times the natural number e to the negative x value,

119
00:07:12.891 --> 00:07:17.780
which is the data point minus the mean squared over two times the standard

120
00:07:17.781 --> 00:07:18.950
deviation squared.

121
00:07:18.980 --> 00:07:23.870
So this is a function of a continuous random variable whose integral across a

122
00:07:23.871 --> 00:07:28.100
interval give the probability that the value of the variable lies within the

123
00:07:28.101 --> 00:07:30.320
same interval.
So we have some interval.

124
00:07:31.520 --> 00:07:34.910
So that's a Gaussian distribution and that's the formula for one.

125
00:07:34.940 --> 00:07:37.760
So what does a Gaussian mixture model?
Well,

126
00:07:39.050 --> 00:07:43.790
we sometimes our data has multiple distributions or it has multiple peaks.

127
00:07:43.820 --> 00:07:47.540
It doesn't always just have one peak.
And we can notice that we,
we can,

128
00:07:47.630 --> 00:07:49.880
we can look at our data set and we can say,
well,

129
00:07:49.881 --> 00:07:52.170
it looks like there are multiple peaks happen here.
There,

130
00:07:52.200 --> 00:07:56.930
there are two peak points and the data seems to be going up and down and up and

131
00:07:56.931 --> 00:07:59.450
down twice or maybe three times or four times.

132
00:07:59.930 --> 00:08:03.680
But if we think that there are multiple distributions Gaussian distributions

133
00:08:03.681 --> 00:08:05.180
that can represent this data,

134
00:08:05.750 --> 00:08:09.590
then we can build what's called a Gaussian mixture model.
And what this is,

135
00:08:09.830 --> 00:08:12.680
it is a probability distribution.
It's more a probability.

136
00:08:12.710 --> 00:08:16.910
You could think of it as just a probability distribution that consists of

137
00:08:16.940 --> 00:08:21.560
multiple probability distributions and that are multiple Gaussians.

138
00:08:22.460 --> 00:08:27.140
So four d dimensions where d is the number are the number of features in our

139
00:08:27.150 --> 00:08:27.983
dataset.

140
00:08:28.130 --> 00:08:32.900
The gaussian distribution of a vector x where x equals a number of data points

141
00:08:32.901 --> 00:08:37.901
that we have is defined by the following equation and right.

142
00:08:38.900 --> 00:08:42.690
So you can see how you can set the two d for the number of features we have.
Uh,

143
00:08:42.740 --> 00:08:47.060
this is for the mean and so sigma represents the covariance matrix of the

144
00:08:47.061 --> 00:08:50.480
Gaussian.
So you notice how we're not using the standard deviation.

145
00:08:50.481 --> 00:08:55.160
Instead we're using the covariance matrix when it comes to the Gaussian mixture

146
00:08:55.170 --> 00:08:57.450
model.
And this is what it would look like,
right?

147
00:08:57.451 --> 00:09:00.980
So in the case of two dimensions,
it would look like this.
It would give us a,

148
00:09:01.110 --> 00:09:05.640
a curve for all of our,
uh,
possible,
uh,
data points.

149
00:09:06.030 --> 00:09:07.620
And so why do we use the covariants?
Will?

150
00:09:07.621 --> 00:09:12.621
The covariants is a measure of how changes in one variable are associated with

151
00:09:12.751 --> 00:09:14.820
changes in a second variable,
right?

152
00:09:14.821 --> 00:09:19.821
They co-vary it's not just about independence of variation two variables and how

153
00:09:20.011 --> 00:09:24.840
they change depend on each other.
They co very,
and the covariants well,

154
00:09:24.841 --> 00:09:26.820
to be technical,
it's called the variance.

155
00:09:26.820 --> 00:09:31.820
Covariance matrix is a measure of how these variables relate to each other.

156
00:09:32.400 --> 00:09:34.950
And in that way,
it's very similar to the standard deviation,

157
00:09:35.100 --> 00:09:36.930
except when we have more dimensions,

158
00:09:37.080 --> 00:09:42.080
the covariance matrix as opposed to the standard deviation when we plug it in

159
00:09:42.181 --> 00:09:46.590
here,
uh,
gives us a better,

160
00:09:46.680 --> 00:09:51.630
more accurate result.
And so the way we compute the covariance Matrix,

161
00:09:51.631 --> 00:09:55.070
I've talked about this before when we talked about eigenvectors and eigenvalue

162
00:09:55.080 --> 00:09:56.510
pairs,
but it,

163
00:09:56.670 --> 00:10:00.360
the number of the parameters that it uses are the number of scores in each of

164
00:10:00.361 --> 00:10:03.880
the datasets,
the variance,
the covariants and um,

165
00:10:04.550 --> 00:10:09.540
the s where c is the size of the number of columns in the data,

166
00:10:09.541 --> 00:10:14.400
right?
It's a column by column Matrix.
I will compute this in a second,

167
00:10:14.820 --> 00:10:19.820
but the probability given in a mixture of k Gaussians where k is the number of

168
00:10:20.070 --> 00:10:24.630
distributions that we,
that we believe that there are in our case,

169
00:10:24.780 --> 00:10:26.880
we believe that there are going to be too.
Okay.

170
00:10:26.881 --> 00:10:29.310
Let's just say we think in our data set they're going to be two.

171
00:10:29.460 --> 00:10:34.200
So four K Gaussians to,
in our case we're going to multiply,
uh,

172
00:10:34.260 --> 00:10:39.260
w which is the prior probability of the JF Gaussian Times the value that we just

173
00:10:40.171 --> 00:10:45.171
looked at up here right here where this is the Gaussian distribution of the

174
00:10:46.621 --> 00:10:50.340
vector of our,
of our data points.
And so once we have this value,

175
00:10:50.580 --> 00:10:55.580
then we can say we can multiply it by w where for each for each of our Gaussians

176
00:10:56.190 --> 00:11:01.190
and that is going to give us our probability value x for a given x data points.

177
00:11:02.220 --> 00:11:03.810
And so this is what it looks like,
right?

178
00:11:03.811 --> 00:11:07.140
So if we were to plot multiple Gaussian distributions,

179
00:11:07.290 --> 00:11:12.030
it would be multiple bell curves.
But what we really want is a single continuous,

180
00:11:12.150 --> 00:11:15.240
a curve that consists of multiple bell curves.

181
00:11:15.390 --> 00:11:19.710
And so once we had that huge continuous curve then and given our data points,

182
00:11:19.980 --> 00:11:24.420
it can tell us the probability that it's going to belong to a specific class.

183
00:11:24.630 --> 00:11:27.150
Okay.
And so what class,

184
00:11:27.450 --> 00:11:30.660
how do you tell what class something is given this curve?
Well,

185
00:11:30.661 --> 00:11:35.070
let me go to the bottom here.
Uh,
once we have that best fit mixture,
notice this,

186
00:11:35.071 --> 00:11:39.780
this,
um,
this image right here,
we can tell what class it's in by,

187
00:11:39.781 --> 00:11:42.900
where it falls on the curve for the one dimensional case.

188
00:11:43.110 --> 00:11:45.900
So this would be class one,
this would be class two,

189
00:11:45.990 --> 00:11:47.640
this would be class three like that.

190
00:11:47.880 --> 00:11:51.360
Now in the two dimensional case or three dimensional case,

191
00:11:51.361 --> 00:11:55.150
it's easier because there are circles,
right?
Like k means clustering,
right?

192
00:11:55.151 --> 00:11:57.910
There's circles and that's what they,
that's what they come out to.

193
00:11:57.911 --> 00:12:02.290
If we were to measure probability values just like this for not just one feature

194
00:12:02.320 --> 00:12:05.820
like we're doing for,
for us,
but multiple features like two features.

195
00:12:05.821 --> 00:12:09.010
So if you have probability values for y the y axis,

196
00:12:09.011 --> 00:12:12.080
which is a second feature set,
like maybe the,
the um,

197
00:12:13.360 --> 00:12:18.010
number of,
uh,
other users that this person has friended.
And in this is a,

198
00:12:18.050 --> 00:12:20.410
the probability of the,
you know,

199
00:12:20.800 --> 00:12:23.530
the amount that the user has either deposited or withdrawn,

200
00:12:23.710 --> 00:12:27.730
then we can draw these cluster circles.
But for the one dimensional case,

201
00:12:27.820 --> 00:12:29.440
it looks like that a bell curve.

202
00:12:30.870 --> 00:12:35.080
And so the problem that we're trying to solve here with the GMM is given a set

203
00:12:35.081 --> 00:12:39.100
of data points x,
let's draw from an unknown distribution,

204
00:12:39.101 --> 00:12:42.550
which we're going to assume intuitively is a Gaussian mixture model.

205
00:12:42.551 --> 00:12:46.300
That is that the data's that consists of multiple Gaussians estimate the

206
00:12:46.301 --> 00:12:48.130
parameters of Feta,

207
00:12:48.160 --> 00:12:52.360
which consists of the mean and other values that we'll talk about of the GMM

208
00:12:52.361 --> 00:12:56.530
model that fits that data.
And the solution.
The way we find these,

209
00:12:56.620 --> 00:13:00.040
these parameters of our model,
which is what machine learning is all about.

210
00:13:00.220 --> 00:13:05.220
Mathematical optimization is by maximizing the likelihood p of x,

211
00:13:05.231 --> 00:13:10.231
the probability of x given our parameters and x is the data point that we want

212
00:13:10.241 --> 00:13:14.040
to predict the probability,
the class,
the,
the,
the,

213
00:13:14.070 --> 00:13:18.610
we want to maximize the likelihood that x belongs to a particular class.

214
00:13:18.760 --> 00:13:19.630
We went to find,

215
00:13:20.620 --> 00:13:24.610
the way we do that is to compute the maximum likelihood estimate,
which is,

216
00:13:24.760 --> 00:13:26.410
which is this equation right here.

217
00:13:26.590 --> 00:13:31.180
We want to find the maximum probability value for a given class.

218
00:13:31.420 --> 00:13:31.781
That is,

219
00:13:31.781 --> 00:13:36.781
we want to find the class that this data point x is the most likely to be a part

220
00:13:37.570 --> 00:13:40.450
of.
And so notice how,

221
00:13:40.480 --> 00:13:43.920
so I also added this image to show how,
uh,
you know,

222
00:13:43.930 --> 00:13:46.180
given an Indy two dimensional case,
these uh,

223
00:13:46.181 --> 00:13:49.750
distribution bell curves come out two circles.
If you,

224
00:13:49.751 --> 00:13:53.020
if you have distributions for not just one access or one dimension,

225
00:13:53.021 --> 00:13:57.750
but two dimensions.
So it's,

226
00:13:57.751 --> 00:14:01.390
it's very similar to k that we've talked about.
TheK means algorithm.

227
00:14:01.510 --> 00:14:03.760
It uses the same optimization strategy,

228
00:14:03.880 --> 00:14:06.820
which is the expectation maximization algorithm.

229
00:14:07.090 --> 00:14:12.090
It's similar to k means in that came means k means finds k to minimize x minus

230
00:14:12.670 --> 00:14:14.050
the mean squared,

231
00:14:14.230 --> 00:14:19.230
but the Gaussian mixture model finds k to minimize x minus the mean squared over

232
00:14:19.780 --> 00:14:21.760
the standard deviation over.

233
00:14:22.450 --> 00:14:27.450
And the reason that we add the standard deviation into the mix is because the

234
00:14:27.731 --> 00:14:28.560
denominator,

235
00:14:28.560 --> 00:14:33.070
the standard deviation squared takes variation into consideration,

236
00:14:33.071 --> 00:14:35.500
which it when it calculates its measurement,
what does this mean?

237
00:14:35.980 --> 00:14:40.810
This means that the k means algorithm gives you a hard,
uh,
assignment.

238
00:14:40.990 --> 00:14:42.250
It either says this is going to be,

239
00:14:42.520 --> 00:14:45.940
this data point is a part of this class or it's a part of this class.

240
00:14:45.941 --> 00:14:50.080
Now this is great in a lot of cases we just want that hard assignment.

241
00:14:50.350 --> 00:14:54.260
But in a lot of cases it's better to have a soft assignment.
Instead,

242
00:14:54.530 --> 00:14:56.690
we want the maximum probability,
okay,

243
00:14:56.691 --> 00:15:00.710
this is going to be 70% likely that it's part of this class class a.

244
00:15:00.920 --> 00:15:04.340
But we also want the probability that it's going to be a part of other classes,

245
00:15:04.341 --> 00:15:09.020
right?
It's not just a single output,
it's not a,
it's not a discrete out output.

246
00:15:09.021 --> 00:15:13.370
Instead of is a continuous output is a list of probability values,

247
00:15:14.600 --> 00:15:17.840
right?
It could be a part of multiple distributions.
It could be in the middle,

248
00:15:17.930 --> 00:15:22.640
it could be 60% likely this class with 40% likely this class,
not just classic.

249
00:15:23.030 --> 00:15:26.240
And so that's why we incorporate,
uh,
uh,

250
00:15:26.690 --> 00:15:29.570
that's why we incorporate the standard deviation.
So,

251
00:15:31.180 --> 00:15:35.270
so how has this thing optimized what it's optimized using the expectation

252
00:15:35.271 --> 00:15:36.800
maximization algorithm.

253
00:15:36.950 --> 00:15:41.950
So the basic ideas of the algorithm or to introduce a hidden variable such that

254
00:15:42.081 --> 00:15:45.080
it's knowledge would simplify the maximization of the likelihood.

255
00:15:45.230 --> 00:15:48.530
So we pick some random data points,
right?
And we picked some random data points.

256
00:15:48.740 --> 00:15:53.630
We draw a distribution around that data point.
We then estimate the print,

257
00:15:53.660 --> 00:15:57.290
we didn't update our parameters using that generated distribution.

258
00:15:57.530 --> 00:16:00.680
And then we repeat the process.
And every time we draw a new data point,

259
00:16:00.920 --> 00:16:05.920
it's going to be closer and closer to the data point that best fits the data set

260
00:16:06.951 --> 00:16:07.700
that we have,

261
00:16:07.700 --> 00:16:10.790
such that if we were to draw a distribution around that data points,

262
00:16:11.120 --> 00:16:15.350
it would fit that.
It would fit that Dataset the best.
And so there are two steps.

263
00:16:15.440 --> 00:16:16.400
There's the east step,

264
00:16:16.401 --> 00:16:21.320
the expectation step and the expectations step is to estimate the distribution

265
00:16:21.530 --> 00:16:26.390
of the hidden variable given the data and the current value of the parameters.

266
00:16:26.660 --> 00:16:28.550
And then the m step,
the m step,

267
00:16:28.551 --> 00:16:33.551
the maximizations step is to maximize the joint distribution of the data and the

268
00:16:34.521 --> 00:16:38.420
hidden variable.
So that's the high level.

269
00:16:38.421 --> 00:16:41.510
And we're going to talk about the implement the implementation details in the

270
00:16:41.511 --> 00:16:45.260
code,
but you might be thinking,
well wait a second,
wait a second.

271
00:16:45.261 --> 00:16:47.540
What about gradient descent?

272
00:16:47.930 --> 00:16:50.780
You might be thinking about that because we've talked about grading dissent and

273
00:16:50.781 --> 00:16:55.220
the entire industry loves gradient descent.
And here's the thing,

274
00:16:55.221 --> 00:16:57.980
you can obtain the MLE using gradient descent,

275
00:16:58.190 --> 00:17:02.570
but what is the deal with gradient descent?
Gradient descent is using the,

276
00:17:02.690 --> 00:17:06.980
the derivative,
you are computing the derivative of a node,
right?

277
00:17:06.981 --> 00:17:09.230
A node in some kind of graphical model.

278
00:17:09.380 --> 00:17:12.770
You're computing the derivative and what the derivative tells you is the

279
00:17:13.040 --> 00:17:17.120
direction.
It tells you the direction that your data wants to move in,

280
00:17:17.121 --> 00:17:21.890
what direction to move the parameters data of your model such that the function,

281
00:17:21.891 --> 00:17:24.920
your model is optimized to fit your data.

282
00:17:25.310 --> 00:17:27.980
But what if you can't compute a gradient?

283
00:17:27.980 --> 00:17:30.320
What if you can't compute a derivative?

284
00:17:32.270 --> 00:17:36.680
You can't compute a derivative of a random variable,
right?
This,

285
00:17:36.681 --> 00:17:40.160
this Gaussian mixture model has a random variable.

286
00:17:40.190 --> 00:17:43.920
It is a stochastic model that is,
it is non deterministic.

287
00:17:44.060 --> 00:17:46.910
You can't compute the derivative of a random variable.

288
00:17:47.120 --> 00:17:51.540
That's why we're not using gradient descent in this case because you can't

289
00:17:51.541 --> 00:17:54.030
compete the derivative of a random variable.
Okay?

290
00:17:54.600 --> 00:17:57.180
So there are actually ways to compute.

291
00:17:57.210 --> 00:18:02.210
There are actually ways to compute the derivative of a stochastic model like a

292
00:18:02.251 --> 00:18:05.710
variational auto encoder.
But it's,
it's,
it's a trick.
And,
um,

293
00:18:06.280 --> 00:18:09.540
and do you want to know more about that?
I've got a great video on that search.

294
00:18:09.660 --> 00:18:14.010
Variational auto encoders Saroj on Youtube,
there'll be the first link.

295
00:18:14.520 --> 00:18:17.040
So back to this.
So when should you use this thing,
right when,

296
00:18:17.041 --> 00:18:21.900
when is this actually useful?
Besides customer term anomaly detection.

297
00:18:21.930 --> 00:18:25.530
Think about any case where you are trying to cluster data where you are trying

298
00:18:25.531 --> 00:18:28.980
to classify data that does not have labels.

299
00:18:29.100 --> 00:18:33.480
So one use case is trying to track an object in a video for aim,
right?

300
00:18:33.660 --> 00:18:37.020
If you know the moving objects distribution in the first frame,

301
00:18:37.260 --> 00:18:41.400
we can localize the object in the next frames by tracking its distribution.

302
00:18:41.401 --> 00:18:42.211
That is,
you know,

303
00:18:42.211 --> 00:18:46.440
kind of the probability distribution of all the possible ways that the subject

304
00:18:46.441 --> 00:18:47.160
can move.

305
00:18:47.160 --> 00:18:51.630
And based on that you can create a bounding box around a subject such that in

306
00:18:51.631 --> 00:18:55.470
the next frame it's very likely that that's subjects movement will fit into the

307
00:18:55.471 --> 00:18:59.940
bounding box.
I have some related,
uh,
repositories here,
uh,

308
00:18:59.970 --> 00:19:04.970
for using tensorflow and a Gaussian mixture model to classify song lyrics by

309
00:19:05.611 --> 00:19:09.780
genre.
Very cool use case using tensorflow.
Definitely check it out.

310
00:19:09.781 --> 00:19:13.710
It's called word to Gaussian mixture.
Very cool.
Check that out.

311
00:19:14.210 --> 00:19:16.440
It's got some great instructions and I've got one more,

312
00:19:16.441 --> 00:19:20.850
which is a great general purpose tutorial for Gaussian mixture models.
It's an,

313
00:19:20.910 --> 00:19:22.140
it's a Jupiter notebook.

314
00:19:22.410 --> 00:19:25.380
Definitely check that out as well and I've got some great links for you in the

315
00:19:25.381 --> 00:19:27.990
description.
All right,
so check that out as well.

316
00:19:27.991 --> 00:19:31.350
Got The great graphs and everything.
So let's,
let's look at this code.
Okay,

317
00:19:31.770 --> 00:19:35.940
so in this code we're going to import our dependencies and then test out a

318
00:19:35.941 --> 00:19:36.900
distribution graph.

319
00:19:36.901 --> 00:19:41.901
Let's just see if we can draw out a distribution orthogonal or unrelated to our

320
00:19:42.220 --> 00:19:43.830
Dataset just to see if we can do that.

321
00:19:44.100 --> 00:19:48.360
So we're going to import for dependencies here.
The first one is not plot live,

322
00:19:48.361 --> 00:19:51.420
which is our handy dandy plotting tool.

323
00:19:51.720 --> 00:19:56.070
And the next one is Ma is num py,
which we always use for matrix math.

324
00:19:56.250 --> 00:19:57.810
Then we've got psi Pi,

325
00:19:57.960 --> 00:20:02.940
which is going to help us normalize our data and compute the probability density

326
00:20:02.941 --> 00:20:05.520
function,
which we talked about earlier.
Right?
We show,

327
00:20:05.550 --> 00:20:09.150
I showed you the equation for that and then we have one more seaborne which is

328
00:20:09.151 --> 00:20:12.570
also going to help us plot specifically colors.
Okay.

329
00:20:12.571 --> 00:20:16.290
So once we have our four dependencies,
we can go ahead and start this.

330
00:20:16.410 --> 00:20:21.410
So the first step is for us to plot out a d a Gaussian distribution.

331
00:20:21.901 --> 00:20:25.320
Let's just see if we can do that first.
So we've got um,

332
00:20:25.500 --> 00:20:29.790
let's just say we're going to use non pies Lynn space function to return an

333
00:20:29.820 --> 00:20:32.970
evenly spaced set of numbers over a specified interval,

334
00:20:33.120 --> 00:20:35.580
which is a negative 10 to 10.

335
00:20:35.700 --> 00:20:38.880
We have a thousand of these data points that we want to generate and then look

336
00:20:38.881 --> 00:20:40.770
at,
we have a real data set in a second,

337
00:20:40.771 --> 00:20:43.230
but this is just for the sake of an example.

338
00:20:43.500 --> 00:20:48.500
And so then we want to create a normal continuous random variable.

339
00:20:49.991 --> 00:20:54.010
And that's what these stats dot norm dot pdf that probability distribution

340
00:20:54.011 --> 00:20:57.100
function function is going to help us do.

341
00:20:57.430 --> 00:21:01.150
And so we say a loc equal zero and this specifies the mean.

342
00:21:01.151 --> 00:21:05.110
So a mean of zero and scale is 1.5 and that's the standard deviation.

343
00:21:05.111 --> 00:21:09.250
So we're going to draw a Gaussian distribution using those two parameters.

344
00:21:09.251 --> 00:21:13.390
We're going to plug those in into the probability density density function and

345
00:21:13.391 --> 00:21:16.510
it's going to output the y values for all of our x values,
right?

346
00:21:16.511 --> 00:21:19.390
We gave it our x values,
it's going to apply to all of our y value.

347
00:21:19.391 --> 00:21:20.620
So then to plot it out,

348
00:21:20.660 --> 00:21:24.940
always to do is say use map plot live to plot out all of our x values and all of

349
00:21:24.941 --> 00:21:29.230
our computed why values and then there it is.

350
00:21:29.260 --> 00:21:32.770
So we can write out a Gaussian distribution easily.
We can do that.
Okay,

351
00:21:32.950 --> 00:21:35.890
so let's go,
let's go into our date,
our dataset.

352
00:21:36.340 --> 00:21:39.640
We're also going to import pandas and we're going to read our dataset here.

353
00:21:39.670 --> 00:21:44.200
It's a CSV file and we can read it into a pandas data frame object,

354
00:21:44.201 --> 00:21:48.070
which is very easy to manipulate in memory once we have it there.

355
00:21:48.250 --> 00:21:52.420
And we want to show the first five examples.
And here we are,

356
00:21:52.570 --> 00:21:56.500
we have the first five examples.
Okay?
So for each of these are users,

357
00:21:56.530 --> 00:21:59.290
each of these are different players and the amounts that they,

358
00:21:59.291 --> 00:22:03.910
that that player has spent in a given month this month in bitcoin.

359
00:22:04.000 --> 00:22:07.120
Okay.
Bitcoin.
So,
or it's normalized any currency you can,

360
00:22:07.150 --> 00:22:10.510
it doesn't really matter what currency is in,
but it's in bitcoin.
Okay.

361
00:22:10.511 --> 00:22:12.640
And so it's either a positive number,

362
00:22:12.641 --> 00:22:15.550
it's a negative number that means that the user withdrew that amount.

363
00:22:15.820 --> 00:22:19.090
But the idea is that that's it,
that we've got one feature.

364
00:22:19.090 --> 00:22:22.630
So it's a one dimensional problem and we're going to write out a probability

365
00:22:22.631 --> 00:22:27.160
distribution for that single feature.
And if we do that,

366
00:22:27.610 --> 00:22:30.760
if we do that,
then we're going to be able to predict,
uh,

367
00:22:30.790 --> 00:22:33.820
the class of a given user based on how much this person,

368
00:22:33.850 --> 00:22:38.260
this user has either spent or withdrawn,
whether or not they're going to,
uh,

369
00:22:39.130 --> 00:22:41.980
uh,
turn or not.
Okay?

370
00:22:41.981 --> 00:22:46.720
So that's our data set and now we're going to show the distribution of the data

371
00:22:46.900 --> 00:22:50.860
as a histogram.
So let's,
let's show it as a histogram as well.

372
00:22:51.010 --> 00:22:52.840
So this is where seaborne is going to come in.

373
00:22:52.841 --> 00:22:56.290
We're going to say seaborne do create a distribution plot.

374
00:22:56.560 --> 00:22:59.920
Given our data that we've,
that we've loaded into memory with pandas,

375
00:23:00.220 --> 00:23:02.770
we want 20 bins of this data and

376
00:23:04.300 --> 00:23:09.100
let's plot it out.
Uh,
let's see.

377
00:23:15.500 --> 00:23:16.333
Disc plot.

378
00:23:22.170 --> 00:23:25.770
Okay.
So that's the distribution.
That's the histogram of our data,
right?

379
00:23:25.771 --> 00:23:29.190
So for all of those number of players,
this is what it looks like.

380
00:23:29.191 --> 00:23:32.220
So we could look at this and we can say,
oh,
okay,

381
00:23:32.221 --> 00:23:36.180
so it looks like one Gaussian might fit this,

382
00:23:36.480 --> 00:23:40.980
but two looks better.
Let's,
let's see.
Let's try out one to see.

383
00:23:41.340 --> 00:23:43.380
Okay.
So that's,
I mean,

384
00:23:43.470 --> 00:23:48.410
clearly to two would be better than one,
right?
You can see two peaks here.

385
00:23:48.650 --> 00:23:50.870
So two distributions would fit this data better than one.

386
00:23:50.871 --> 00:23:54.680
So that's why we're going to use a Gaussian mixture model instead of a single

387
00:23:54.681 --> 00:23:58.790
Gaussian,
right?
So we want one that,
a continuous bell curve,

388
00:23:58.940 --> 00:24:01.730
a continuous curve that consist of two bell curves.

389
00:24:02.090 --> 00:24:03.950
So to define this model to two,

390
00:24:03.951 --> 00:24:07.010
normal distributions are going to have five parameters.

391
00:24:07.690 --> 00:24:09.590
Four of them are the,

392
00:24:10.040 --> 00:24:14.270
the first four are the mean and the standard deviation for each of those

393
00:24:14.271 --> 00:24:15.680
distributions.
One and two.

394
00:24:15.860 --> 00:24:19.190
And the fifth one is the probability of choosing one of them.

395
00:24:19.580 --> 00:24:23.750
And so the way we're going to write out this Gaussian mixture model where theta

396
00:24:23.780 --> 00:24:25.100
equals uh,

397
00:24:25.400 --> 00:24:29.030
those four values and the probability of choosing one of them.

398
00:24:29.060 --> 00:24:34.060
W is just like this and this is the probability density function for a Gaussian

399
00:24:35.391 --> 00:24:38.000
mixture model consisting of two Gaussians.

400
00:24:38.001 --> 00:24:42.170
Now we'd look at the probability density function for a single galcion and it

401
00:24:42.171 --> 00:24:47.171
looked like this right here and this is what it looks like for two

402
00:24:48.351 --> 00:24:53.180
distributions,
right?
Right.
We get it so far.
So now we're going to fit this model.

403
00:24:53.181 --> 00:24:56.630
That's our model,
right?
All machine learning models are functions.

404
00:24:56.631 --> 00:25:00.710
There are functions or where we initialize those parameters randomly or using

405
00:25:01.010 --> 00:25:04.280
some kind of,
you know,
smart sampling method or something,

406
00:25:04.520 --> 00:25:06.980
but we initialize those parameters very stupidly.

407
00:25:07.160 --> 00:25:10.190
And then we learn what the best or most optimal ones are.

408
00:25:10.370 --> 00:25:14.420
And in neural networks and a bunch of convex optimization problems,

409
00:25:14.600 --> 00:25:18.230
we generally use gradient descent because we can compute the derivative of

410
00:25:18.231 --> 00:25:21.110
deterministic notes.
But this is a stochastic model.

411
00:25:21.111 --> 00:25:25.820
So we're going to use the popular expectation maximization algorithm,

412
00:25:26.150 --> 00:25:30.170
which is a two step process.
We first,
uh,
perform he,

413
00:25:30.171 --> 00:25:34.670
which is to update our variables.
And then m which is to update the hypothesis.

414
00:25:34.671 --> 00:25:35.720
What do I mean?
Well,

415
00:25:35.721 --> 00:25:40.370
this is an iterative method for finding the MLE that estimates the parameters in

416
00:25:40.371 --> 00:25:44.180
statistical models.
So we start with some initial values performed,

417
00:25:44.181 --> 00:25:45.410
the expectation step,

418
00:25:45.650 --> 00:25:49.880
then the maximisation step check if it's converge or not by some threshold that

419
00:25:49.881 --> 00:25:54.800
we define and if it hasn't continue iteratively and if it has stop.

420
00:25:55.160 --> 00:26:00.160
So the expectation step given the current parameters of the model,

421
00:26:00.350 --> 00:26:05.060
estimate a probability distribution maximisation step given the current data,

422
00:26:05.360 --> 00:26:10.360
estimate the parameters to update the model and repeat so more formally using

423
00:26:10.761 --> 00:26:15.230
the current estimates for the parameters create a function for the expectation

424
00:26:15.231 --> 00:26:18.740
of the log likelihood.
And then from maximisation compute,

425
00:26:18.741 --> 00:26:23.450
the perimeter is maximizing the expected log likelihood down on the east step.

426
00:26:23.840 --> 00:26:27.920
So the end parameter estimates are used to determine the distributions of the

427
00:26:27.921 --> 00:26:30.470
latent variables in the next east step.

428
00:26:31.490 --> 00:26:35.270
So e m is trying to maximize the following function,
uh,

429
00:26:35.440 --> 00:26:38.660
which is what we defined the probability density function up here.

430
00:26:38.870 --> 00:26:42.920
Where x is a directly observed a variable that is our data,

431
00:26:43.830 --> 00:26:44.341
a theta,

432
00:26:44.341 --> 00:26:48.690
our whole of those printers is five parameters of our model in z is not directly

433
00:26:48.691 --> 00:26:53.280
observed that the latent variable that is the random value that we plot right
to,

434
00:26:53.460 --> 00:26:55.860
to continue and it gets more and more smart,

435
00:26:56.040 --> 00:27:01.020
it gets more and more optimal where we plot it as our,
as our model learns.
Okay.

436
00:27:01.021 --> 00:27:04.530
And we xe is a joint distribution on x,

437
00:27:04.531 --> 00:27:08.730
so it's a distribution between the latent variable that we've plotted and the

438
00:27:08.731 --> 00:27:11.280
girl Scouts Ian,
that we've already applauded.

439
00:27:12.030 --> 00:27:15.060
So there are four steps.
If we think about it,
there are four steps.

440
00:27:15.061 --> 00:27:19.320
We first initialize the parameters of our model Feta and then we compute the

441
00:27:19.321 --> 00:27:21.810
best values for z given data.

442
00:27:22.080 --> 00:27:25.830
And then we use the computer values of Czi to compute a better estimate.

443
00:27:25.980 --> 00:27:29.500
Fourth data and then repeat.
So in another way,
another way to say this,

444
00:27:29.520 --> 00:27:31.050
and I'll show you a visual way in a second,

445
00:27:31.590 --> 00:27:35.640
but we want to initialize the parameters of the model either randomly or usually

446
00:27:35.641 --> 00:27:36.474
randomly.

447
00:27:36.600 --> 00:27:40.230
And then we find the posts terrier probabilities of the latent variable.

448
00:27:40.410 --> 00:27:43.050
Given a given the current parameter values.

449
00:27:43.320 --> 00:27:45.500
And then the m step is to re estimate the CR,

450
00:27:45.540 --> 00:27:49.500
the parameter values given the current posterior probabilities.

451
00:27:49.680 --> 00:27:52.980
And we repeat that process again.
So check this out visually,

452
00:27:52.981 --> 00:27:56.460
this is what it looks like.
So we have two Gaussians.

453
00:27:56.461 --> 00:27:59.760
So in this case we have probabilities for two features.

454
00:27:59.760 --> 00:28:03.570
So these are circles instead of those curves,
right?
Instead of those bell curves.

455
00:28:03.690 --> 00:28:05.890
And so we have two Gaussians and they're just randomly,

456
00:28:05.891 --> 00:28:08.130
we generated them and we have our data points.

457
00:28:08.280 --> 00:28:12.090
So what we do is we say for each of those data points,

458
00:28:12.240 --> 00:28:14.190
which galcion generated it,

459
00:28:14.191 --> 00:28:17.400
so we give them all probabilities for each Gaussian.

460
00:28:17.401 --> 00:28:20.220
So that's the east step for each point here,

461
00:28:20.490 --> 00:28:23.740
estimate the probability that each galcion generated.

462
00:28:23.790 --> 00:28:27.480
In our case we have two Gaussians,
right?
So we're going to generate the,

463
00:28:27.540 --> 00:28:31.320
we're going to estimate the probability that each data point is from the first

464
00:28:31.350 --> 00:28:34.800
EI Ei and in the second beat.
Okay.

465
00:28:35.760 --> 00:28:37.050
And so when we have that,

466
00:28:37.290 --> 00:28:42.180
then we compute m the m step is to modify the parameters according to the hidden

467
00:28:42.181 --> 00:28:47.160
variable to maximize the likelihood of the data.
And the hidden variable.

468
00:28:48.000 --> 00:28:52.080
And that's it.
And so every time we do that,
these down Sian curves,

469
00:28:52.200 --> 00:28:54.120
these Gaussian distributions,

470
00:28:54.240 --> 00:28:59.240
these clusters are going to be more and more optimal to fit the data where they

471
00:28:59.341 --> 00:29:03.180
need to,
to classify,
to,
to distinguish between classes.

472
00:29:04.020 --> 00:29:08.870
So back to our data.
So let's define what a Gaussian looks like,
right?
So we,

473
00:29:08.910 --> 00:29:11.340
so,
so we'll give a galaxy in its own class.

474
00:29:11.550 --> 00:29:15.330
And so a Gaussian is initialized by the mean and the standard deviation.

475
00:29:15.331 --> 00:29:19.740
So we calculate that first from our data points and we can do that very easily,

476
00:29:20.010 --> 00:29:21.150
right?
For the mean,

477
00:29:21.151 --> 00:29:24.060
it's just add all the numbers up and then divide by the number of them.

478
00:29:24.061 --> 00:29:26.790
And for the,
the center deviation,
it's uh,

479
00:29:26.880 --> 00:29:31.620
the formula for that is a look up there.
That's the formula for it.

480
00:29:32.010 --> 00:29:34.710
Okay,
so back to this.
So once we have those,

481
00:29:34.711 --> 00:29:38.580
we can compute the probability density function that we looked at up there.

482
00:29:38.760 --> 00:29:43.720
The probability to density function is this,
right?

483
00:29:43.721 --> 00:29:46.960
So we can compute this programmatically,
which I'm going to do right now.

484
00:29:47.320 --> 00:29:51.670
So programmatically that looks like
this,
right?

485
00:29:52.840 --> 00:29:57.830
Just like that same thing.
And so we can say,
uh,
let's

486
00:29:59.520 --> 00:30:02.280
find the Gaussian of best fit for our data.

487
00:30:02.281 --> 00:30:06.300
So we have our data in a data frame and let's find the Gaussian of best fit.

488
00:30:06.660 --> 00:30:07.710
And so this,

489
00:30:07.711 --> 00:30:12.030
these are our ideal means and our ideal standard deviations for our data.

490
00:30:12.330 --> 00:30:14.550
But that's not enough.
We want to write,

491
00:30:14.580 --> 00:30:16.830
it's not enough to just have one that's too easy.

492
00:30:17.010 --> 00:30:20.340
There's no expectation maximization algorithm happening there.

493
00:30:20.460 --> 00:30:23.650
We don't really need it.
But if we were to,
uh,

494
00:30:25.990 --> 00:30:29.410
plot out this galcion very easy.
It's a single Gaussian,
but it's not,

495
00:30:29.440 --> 00:30:31.840
it's not well fit to the data.
We want to have them,
right?

496
00:30:32.170 --> 00:30:36.340
So we're going to keep going here and this is where the expectation maximization

497
00:30:36.370 --> 00:30:39.190
algorithm comes into play.
So

498
00:30:40.790 --> 00:30:44.780
first we initially,
uh,
we randomly assigned k cluster centers.

499
00:30:44.900 --> 00:30:48.170
In our case k are the number of Gaussians,
right?

500
00:30:48.171 --> 00:30:51.650
Not k means k where they are the number of Gaussians too.

501
00:30:52.070 --> 00:30:55.260
And then we iteratively refine these clusters or you know,

502
00:30:55.330 --> 00:30:59.030
a bell curves based on the two steps for the expectation step.

503
00:30:59.180 --> 00:31:01.760
We signed each data point x to both clusters,

504
00:31:01.761 --> 00:31:03.650
the probability with the following probability.

505
00:31:04.190 --> 00:31:06.650
And the maximizations step is to estimate,

506
00:31:06.651 --> 00:31:11.651
to create an estimation of the model parameters given those probabilities and

507
00:31:12.261 --> 00:31:15.350
then repeat them.
So I said the probabilities are,
are,

508
00:31:15.620 --> 00:31:18.440
are maximized for each class.

509
00:31:19.640 --> 00:31:22.460
So then we'll create a class for our Gaussian mixture model.

510
00:31:22.461 --> 00:31:24.920
We've already created a class for our Galcion,
but here's four,

511
00:31:24.921 --> 00:31:29.150
our Gaussian mixture model.
So for a mixture model where there are two Gaussians,

512
00:31:29.360 --> 00:31:33.350
we don't just need one a mean and one standard deviation.

513
00:31:33.620 --> 00:31:36.290
We need four.
In fact,
we need five parameters,
right?

514
00:31:36.291 --> 00:31:39.770
We need the mean and the standard deviation for one Gaussian,

515
00:31:39.890 --> 00:31:42.680
we the mean and the standard deviation for the second Gaussian.

516
00:31:42.860 --> 00:31:47.560
And we need our Dataset,
uh,
as well as well as the,
uh,

517
00:31:47.890 --> 00:31:51.950
a mix,
which is the,
uh,
initial w value.

518
00:31:51.951 --> 00:31:55.580
Remember the w value.
And so we'll initialize both of them well in this shot.

519
00:31:55.640 --> 00:31:58.700
So that's why we created that first galcion class.

520
00:31:58.880 --> 00:32:01.880
So you could easily initialize to Gaussians just like that,

521
00:32:02.000 --> 00:32:04.970
using both means and standard deviations,

522
00:32:06.440 --> 00:32:09.990
how as well as the w value,
right?
Which is going to be updated over time.

523
00:32:09.991 --> 00:32:14.991
The weight value and the weights defines how mixed these distributions are like

524
00:32:16.830 --> 00:32:21.780
right.
The how mixed both of them are.
And so now for the East App and the m step,

525
00:32:21.810 --> 00:32:25.500
so basically we make initial guesses for both the assignments,
uh,

526
00:32:25.520 --> 00:32:29.850
point to the distribution and their parameters and then proceed iteratively.

527
00:32:30.000 --> 00:32:33.120
So once we've initialize the assignments and parameters,

528
00:32:33.390 --> 00:32:37.740
we can alternate between the expectation and the maximizations steps until our

529
00:32:37.741 --> 00:32:39.030
estimates converge.

530
00:32:39.260 --> 00:32:44.090
That means that they do not change much between iterations for a mixture of

531
00:32:44.091 --> 00:32:47.570
Gaussians.
And this is similar to convergence in this came means,
right,

532
00:32:47.571 --> 00:32:50.300
which I've talked about I think a few,
a few lessons ago.

533
00:32:50.510 --> 00:32:54.710
K-Means clustering Saroj search that on Youtube.
So for the East App,

534
00:32:55.730 --> 00:32:58.880
we,
we first initialize a log likelihood as zero,

535
00:32:58.881 --> 00:33:03.380
which we're trying to maximize.
And then we say for each of our data points,
uh,

536
00:33:03.410 --> 00:33:08.240
compute the normalized wait values using both of our data points and these are

537
00:33:08.241 --> 00:33:09.620
the probability values.

538
00:33:09.860 --> 00:33:13.220
And then we compute the denominator by adding both of them together.

539
00:33:13.460 --> 00:33:18.460
We normalize them by dividing both of those weights by the denominator and had

540
00:33:18.681 --> 00:33:20.960
them both into the log likelihood.

541
00:33:20.961 --> 00:33:25.850
But so we compute the log of the sum of both of those values and that gives us

542
00:33:25.851 --> 00:33:30.710
the log likelihood and we were turn the weight to pull that our are our

543
00:33:30.711 --> 00:33:35.510
parameters of our model,
right?
And so once we have that,

544
00:33:35.780 --> 00:33:40.660
we'll,
we'll,
we'll use those as parameters for the maximisation step,
right?

545
00:33:41.120 --> 00:33:44.300
We want to optimize those wait values.
So we'll compute,

546
00:33:44.330 --> 00:33:47.060
so will compute denominators of our weights,
right?

547
00:33:47.150 --> 00:33:51.050
So zip tells us the absolute values of our weight.
So it's all positive,

548
00:33:51.051 --> 00:33:55.030
which just makes things a little more pretty mathematically for,
uh,

549
00:33:55.250 --> 00:33:58.430
for both of them.
And then we'll take the sum of all of those values,

550
00:33:58.431 --> 00:34:02.270
both on the left and the right side and will compute new means.

551
00:34:02.300 --> 00:34:06.110
And new standard deviations is for our new distributions,
right?

552
00:34:06.111 --> 00:34:07.850
We're updating them every time.

553
00:34:08.000 --> 00:34:12.680
We're maximizing the likelihood that they belong to the correct class every

554
00:34:12.681 --> 00:34:15.050
time.
And then a new weight value.

555
00:34:15.051 --> 00:34:19.160
So that is our update step are maximisation step is our update step.

556
00:34:19.730 --> 00:34:20.563
Okay?

557
00:34:20.990 --> 00:34:25.990
And so the probability density function is going to be that function that I

558
00:34:27.261 --> 00:34:31.640
talked about up there,
right when it comes to two Gaussians,
not a single galcion.

559
00:34:32.240 --> 00:34:33.073
And so,

560
00:34:34.390 --> 00:34:35.060
<v 1>okay,</v>

561
00:34:35.060 --> 00:34:38.240
<v 0>so for the fitting process,
now that we know these,
we can say,</v>

562
00:34:38.241 --> 00:34:42.290
well let's try this for five iterations on our data set using the class that we

563
00:34:42.291 --> 00:34:45.260
just built.
Let's train this thing.
So what does training look like?

564
00:34:45.261 --> 00:34:46.670
Train training looks like.

565
00:34:47.000 --> 00:34:49.610
We'll take our Gaussian mixture model where we have fitted,

566
00:34:49.640 --> 00:34:53.780
fitted it to our dataset well,
well that we have given our Dataset to.

567
00:34:54.170 --> 00:34:57.770
And we'll say,
okay,
let's try this thing.
So we'll iterate.

568
00:35:06.670 --> 00:35:07.360
<v 1>Okay.</v>

569
00:35:07.360 --> 00:35:08.530
<v 0>If the log likelihood,</v>

570
00:35:08.980 --> 00:35:12.730
if the current log likelihood is better than the best cycle,

571
00:35:12.731 --> 00:35:17.660
that log likelihood then set the best log likelihood to the uh,

572
00:35:18.730 --> 00:35:19.260
<v 1>yeah,</v>

573
00:35:19.260 --> 00:35:23.050
<v 0>models,
log likelihood,
and then set the best,
um,</v>

574
00:35:23.160 --> 00:35:26.700
wait value to the current weight value.
Okay.

575
00:35:26.701 --> 00:35:29.940
And so we do that for as many iterations as we can,
as we,

576
00:35:29.941 --> 00:35:33.450
as we defined five of them.
And so once we've done that,

577
00:35:34.440 --> 00:35:38.820
notice how our means and our standard and our weight values,

578
00:35:38.821 --> 00:35:42.150
that fifth parameter are all being updated every time.

579
00:35:42.180 --> 00:35:45.660
And that is the expectation maximization process.

580
00:35:45.840 --> 00:35:50.370
And so once we've done that,
then we can look at the mixture,
what is,

581
00:35:50.371 --> 00:35:54.270
what does it look like?
And so this is,
this is the end result,
right?

582
00:35:54.271 --> 00:35:58.140
This is the end result.
So we have one distribution to rule them all,

583
00:35:58.141 --> 00:36:00.270
Lord of the rings style.
We've got a single distribution.

584
00:36:00.690 --> 00:36:04.170
And once we have this distribution,
we can then predict,

585
00:36:04.171 --> 00:36:07.740
given some new data points,
right?
Some new spending amounts,

586
00:36:08.160 --> 00:36:11.550
what is the likelihood and what is the likelihood that it's going to be a part

587
00:36:11.551 --> 00:36:14.730
of a specific class?
And our case is going to be one of two classes,

588
00:36:14.731 --> 00:36:17.250
is there's going to spend,
is this person going to spend or not?

589
00:36:17.580 --> 00:36:20.370
And so that's what it looks like.
And if we had more,
uh,

590
00:36:20.580 --> 00:36:22.680
if we had more features,

591
00:36:22.800 --> 00:36:25.380
we could turn this into a clustering problem with circles,

592
00:36:25.530 --> 00:36:29.460
which would make it a lot easier to look at.
Okay,

593
00:36:29.461 --> 00:36:30.900
so what have we learned here?

594
00:36:30.901 --> 00:36:34.200
Let's summarize a little bit about what we've learned.
Gaussian mixture models.

595
00:36:34.201 --> 00:36:38.850
Take our old friend the Gaussian and add other Gaussians,
right plural.

596
00:36:38.880 --> 00:36:41.520
Sometimes we could have up to as many as we'd like.

597
00:36:41.880 --> 00:36:46.350
And this allows us to model more complex data where there are multiple peaks and

598
00:36:46.351 --> 00:36:51.000
valleys and we fit it using the expectation maximization algorithm,

599
00:36:51.001 --> 00:36:55.710
the e m algorithm,
very popular experts,
very popular optimization strategy.

600
00:36:56.310 --> 00:36:59.940
And it's a series of steps to find good parameter estimates where there are

601
00:36:59.941 --> 00:37:01.110
latent variables.

602
00:37:01.320 --> 00:37:05.310
So we initialize the perimeter estimates randomly given the current parameter

603
00:37:05.311 --> 00:37:08.250
estimates,
find the minimum log likelihood for Z,

604
00:37:08.251 --> 00:37:12.600
which is the data plus the latent variables,
the joint probability distribution,

605
00:37:12.960 --> 00:37:14.760
and then given the current data,

606
00:37:14.761 --> 00:37:18.750
find the better parameter estimates and then repeat that process over and over

607
00:37:18.751 --> 00:37:23.400
again.
And the distributions that are going to be really bad at first and slowly

608
00:37:23.610 --> 00:37:28.290
they're going to converge and they're going to fit our dataset perfectly.
Okay.

609
00:37:29.340 --> 00:37:33.360
And so this can be used beyond the Gaussian mixture models instead think about,

610
00:37:33.420 --> 00:37:36.570
well in this case you have to guess the number of Gaussians,
right?

611
00:37:36.571 --> 00:37:39.450
You have to guess.
Well,
I think there's going to be two by looking at your data,

612
00:37:39.780 --> 00:37:43.230
but what if he didn't have to guess them?
Well,
that in,
in that case,

613
00:37:43.440 --> 00:37:46.950
we could use kernel density estimation,
but that's for a d,

614
00:37:47.010 --> 00:37:49.140
that's for another time.
Before we get there,

615
00:37:49.141 --> 00:37:53.280
it's we had to have built the intuition around Gaussian mixture models,

616
00:37:53.430 --> 00:37:56.370
a great model.
If you know the shape of your data,

617
00:37:56.371 --> 00:37:58.660
more or less intuitively see you next week.

618
00:37:58.950 --> 00:38:02.130
Please subscribe for more programming videos.
And for now,

619
00:38:02.340 --> 00:38:05.670
I've got to fit my curves,
so thanks for watching.

