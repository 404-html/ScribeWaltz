WEBVTT

1
00:00:00.060 --> 00:00:01.710
Is generating novel data.

2
00:00:08.910 --> 00:00:10.230
Hello world,
it's the Raj.

3
00:00:10.231 --> 00:00:13.230
And today we're gonna learn about a special type of neural network called an

4
00:00:13.260 --> 00:00:14.093
auto encoder.

5
00:00:14.160 --> 00:00:17.640
Then we're going to implement our own auto encoder using tensorflow to generate

6
00:00:17.641 --> 00:00:18.630
handwritten digits.

7
00:00:18.660 --> 00:00:22.230
An auto encoder is a simple type of neural network with only three layers.

8
00:00:22.260 --> 00:00:26.190
It's got an input layer,
a hidden layer,
and an output layer just like your mom.

9
00:00:26.250 --> 00:00:27.030
Just kidding.

10
00:00:27.030 --> 00:00:30.660
What makes an auto encoder special is that the output neurons are directly

11
00:00:30.661 --> 00:00:32.460
connected to the input neurons,

12
00:00:32.550 --> 00:00:35.790
and the goal is to get the output values to match the input values.

13
00:00:35.910 --> 00:00:38.250
So if I added an image of a dog to the input layer,

14
00:00:38.340 --> 00:00:41.700
the apple layer would then output that same image.
When we input that image,

15
00:00:41.701 --> 00:00:45.180
it's a vector in end dimensional space,
which is sent to the hidden layer.

16
00:00:45.210 --> 00:00:47.490
After some activation function is applied to it,

17
00:00:47.550 --> 00:00:51.240
this reduces it to an end dimensional space.
So there are less dimensions.

18
00:00:51.270 --> 00:00:54.150
This process is called dimensionality reduction,

19
00:00:54.270 --> 00:00:55.970
and it happens in every neural networks.

20
00:00:55.971 --> 00:00:58.530
So we can think of the first layer of the network as the encoder,

21
00:00:58.620 --> 00:01:00.090
since it's compressing data.

22
00:01:00.270 --> 00:01:04.050
And the last part as a decoder fins and tried to reconstruct the original input

23
00:01:04.140 --> 00:01:06.420
from the smaller representations in the hidden layer.

24
00:01:06.480 --> 00:01:09.120
So when do we use auto encoders all day,
all day?
Well,

25
00:01:09.121 --> 00:01:11.010
one use case is data compression,

26
00:01:11.130 --> 00:01:14.880
kind of like creating a zip file for some dataset that you can later unzip.

27
00:01:15.030 --> 00:01:18.870
Another is image search where you search Google using an image as your search

28
00:01:18.871 --> 00:01:20.340
query instead of text.

29
00:01:20.370 --> 00:01:24.360
It's very likely that Google is using auto encoders to help fuel this use case.

30
00:01:24.390 --> 00:01:27.780
Whenever one of Google's bots crawling the web and finds an image,

31
00:01:27.900 --> 00:01:29.820
it will compress it using an auto encoder.

32
00:01:29.850 --> 00:01:33.540
That would mean Google would have all of its indexed images compressed in the

33
00:01:33.541 --> 00:01:37.320
form of an easily searchable array.
Whenever you search for a new image,

34
00:01:37.360 --> 00:01:40.800
it will compress it and find all the points nearest to it in the compression

35
00:01:40.801 --> 00:01:44.670
space.
Since there is less noise,
then rank them according to their similarity.

36
00:01:44.730 --> 00:01:47.370
There's also the one class classification use case.

37
00:01:47.371 --> 00:01:49.710
That's when we only have a dataset with a single class,

38
00:01:49.740 --> 00:01:52.860
which is called the target class.
When we feed it to an auto encoder,

39
00:01:52.861 --> 00:01:55.140
it'll learn to detect objects of that class.

40
00:01:55.141 --> 00:01:58.020
When it receives an object that doesn't fit that class category,

41
00:01:58.110 --> 00:01:59.820
it'll detect it as an anomaly,

42
00:01:59.880 --> 00:02:02.520
whether it's a deadly virus or fraud attempt system,

43
00:02:02.521 --> 00:02:06.030
downtime or someone who actually doesn't play Pokemon go.
That's right.

44
00:02:06.240 --> 00:02:09.690
Auto encoders are for you and if you want to train a deep network,

45
00:02:09.720 --> 00:02:12.060
you could use what's called a stacked auto encoder.

46
00:02:12.210 --> 00:02:16.320
That is a set of auto encoders where the outputs of each layer our wire to the

47
00:02:16.321 --> 00:02:19.490
inputs of the successive layer.
Once you train it on some data set,

48
00:02:19.530 --> 00:02:23.190
you can use those weights to initialize your deep net instead of randomly

49
00:02:23.191 --> 00:02:24.210
initialized weights.

50
00:02:24.270 --> 00:02:28.380
One of the more recent applications of auto encoders is generating novel yet

51
00:02:28.381 --> 00:02:32.130
similar outputs to our inputs like faces that look really similar but are

52
00:02:32.131 --> 00:02:33.930
different than the input basis.
For this,

53
00:02:33.931 --> 00:02:37.740
we use a newer type of auto encoder called a variational auto encoder,

54
00:02:37.860 --> 00:02:41.490
which learns a distribution around data so it can generate similar but different

55
00:02:41.491 --> 00:02:44.970
outputs.
There are lots of techniques that are used to prevent auto encoders from

56
00:02:44.971 --> 00:02:47.130
successfully reconstructing the input image.

57
00:02:47.220 --> 00:02:51.030
Like denoising where the input is partially corrupted on purpose.

58
00:02:51.060 --> 00:02:54.720
The idea is that if you can reconstruct an image,
despite it being corrupted,

59
00:02:54.820 --> 00:02:56.460
it will be a more robust decoder.

60
00:02:56.461 --> 00:02:59.010
So let's build our own simple autoencoder to learn,

61
00:02:59.140 --> 00:03:01.240
detect hen writing digits using tensor flow,

62
00:03:01.241 --> 00:03:04.510
shall we first of all import tensorflow,
the awesome machine learning library.

63
00:03:04.570 --> 00:03:05.381
Then num Pi,

64
00:03:05.381 --> 00:03:09.490
the [inaudible] scientific computing library will also import our input data,

65
00:03:09.491 --> 00:03:11.980
which is the collection of handwritten character images.

66
00:03:12.010 --> 00:03:14.590
Then we'll define our auto encoder hyper parameters.

67
00:03:14.620 --> 00:03:17.890
We know that each character image is 28 by 28 pixels,

68
00:03:17.920 --> 00:03:21.160
so we'll set the width to 28 then we'll initialize a variable that represents

69
00:03:21.161 --> 00:03:24.610
the number of input notes.
We also want to have 500 nodes in the hidden layer.

70
00:03:24.670 --> 00:03:27.970
I generally like to have two thirds of number of nodes in my hidden layer as my

71
00:03:27.971 --> 00:03:29.410
input layer as a starting point.

72
00:03:29.460 --> 00:03:33.550
Once it purposely corrupt our input data later so that our Dakota gets even more

73
00:03:33.551 --> 00:03:35.560
robust and it's reconstruction over time.

74
00:03:35.590 --> 00:03:39.430
So let's set our corruption level to 0.3 which isn't nearly as high as the u s

75
00:03:39.431 --> 00:03:43.300
government's.
Once we have these variables,
we'll use them to help us build nodes.

76
00:03:43.450 --> 00:03:45.370
We'll start by creating a note for the input data.

77
00:03:45.520 --> 00:03:47.410
Then we'll create a note for the corruption mask,

78
00:03:47.500 --> 00:03:49.780
which will help us reconstruct the original image.

79
00:03:49.810 --> 00:03:52.090
Well then went to create notes for our hidden variables.

80
00:03:52.120 --> 00:03:53.530
After we initialize our weights,

81
00:03:53.620 --> 00:03:57.460
we'll initialize our hidden layer as well as the prime values for each at tidal

82
00:03:57.461 --> 00:04:01.570
weights between the encoder decoder and result in an output value.
After that,

83
00:04:01.571 --> 00:04:05.320
we define our model via a function that takes in our variable parameters.

84
00:04:05.350 --> 00:04:07.810
We'll make sure to get a corrupted version of our input data,

85
00:04:07.840 --> 00:04:11.560
then create our neural net and compute the sigmoid function to create our hidden

86
00:04:11.561 --> 00:04:14.620
state.
Then we'll create our reconstructed input and return that.

87
00:04:14.650 --> 00:04:18.250
So our model will accept an input and return a reconstructed version of it

88
00:04:18.430 --> 00:04:20.470
despite the self imposed data corruption.

89
00:04:20.500 --> 00:04:23.530
So we can utilize this function by building a model graph with it,

90
00:04:23.710 --> 00:04:25.960
which we'll call z.
So now that we have our model,

91
00:04:25.990 --> 00:04:29.350
we need to create a cost function,
which is what we want to minimize over time.

92
00:04:29.530 --> 00:04:32.620
The more we minimize it,
the more accurate our output results will be.

93
00:04:32.680 --> 00:04:34.270
Then we want to create a training algorithm,

94
00:04:34.360 --> 00:04:37.840
which we'll call a train op and use the classic gradient descent algorithm to

95
00:04:37.841 --> 00:04:38.830
help train our model,

96
00:04:38.890 --> 00:04:42.700
which takes the cost function as a parameter so it could continuously minimize

97
00:04:42.701 --> 00:04:45.880
when we train it.
Before we start training our model,
we need to load our data,

98
00:04:45.881 --> 00:04:49.030
so let's read from our local directory.
We'll set one hot to true,

99
00:04:49.120 --> 00:04:52.330
which is a bit widest operation that will make computation faster than

100
00:04:52.331 --> 00:04:55.720
initialize the variables for our images and labels for both our training and

101
00:04:55.721 --> 00:04:56.830
testing data.
Finally,

102
00:04:56.831 --> 00:04:59.980
it will begin our training process by initializing a tensorflow session.

103
00:05:00.070 --> 00:05:03.100
We'll initialize all of our variables first,
then begin our for loop,

104
00:05:03.101 --> 00:05:05.620
which will iterate 100 times.
For every image and label.

105
00:05:05.621 --> 00:05:06.850
We will retrieve an input,

106
00:05:07.000 --> 00:05:11.140
then create a mask using the binomial distribution of our input data and use

107
00:05:11.141 --> 00:05:14.740
that as a parameter to run our tensorflow session using the training algorithm

108
00:05:14.741 --> 00:05:15.790
we defined earlier.

109
00:05:15.910 --> 00:05:19.300
We'll calculate a mask for the outer loop as well and print out the results as

110
00:05:19.301 --> 00:05:21.100
we go along.
Let's take a look at the results.

111
00:05:21.130 --> 00:05:24.160
We can see our score gets better and better over time with training,

112
00:05:24.250 --> 00:05:28.660
and eventually our neural net is able to reconstruct and classify handwritten

113
00:05:28.661 --> 00:05:30.100
characters.
For more information,

114
00:05:30.101 --> 00:05:33.430
check out the links down below and please subscribe because I'm just getting

115
00:05:33.431 --> 00:05:37.240
started for now.
I've got to go boost some gradients,
so thanks for watching.

