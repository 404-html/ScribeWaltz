WEBVTT

1
00:00:02.240 --> 00:00:04.100
Hello world.
Welcome this raw geology.

2
00:00:04.290 --> 00:00:06.720
Today we're going to be building a neural net in four minutes.

3
00:00:06.750 --> 00:00:10.500
Let's get started there like 1,000,001 machine learning models out there,

4
00:00:10.590 --> 00:00:13.890
but neural nets in particular have gotten really popular recently because of two

5
00:00:13.891 --> 00:00:16.440
things,
faster computers and more data.

6
00:00:16.590 --> 00:00:19.410
They'd helped produce some amazing breakthroughs in everything from image

7
00:00:19.411 --> 00:00:21.480
recognition to generating rap songs.

8
00:00:21.660 --> 00:00:25.170
They're really just three steps involved in machine learning,
build it,
train it,

9
00:00:25.171 --> 00:00:27.240
and test it.
Once we build our model,

10
00:00:27.241 --> 00:00:30.780
we can train it against our input and output data to make it better and better

11
00:00:30.781 --> 00:00:31.980
at pattern recognition.

12
00:00:32.220 --> 00:00:35.610
So let's build our model a three layer neural network in python.

13
00:00:35.880 --> 00:00:38.010
We'll want to start off by importing num Pi,

14
00:00:38.160 --> 00:00:41.400
which is my goto library for scientific computing in Python.

15
00:00:41.760 --> 00:00:45.390
Then we'll want to create a function that will map any value to a value between

16
00:00:45.391 --> 00:00:48.000
zero and one.
This is called a sigmoid.

17
00:00:48.210 --> 00:00:51.930
This function will be running every neuron of our network when data hits it.

18
00:00:52.110 --> 00:00:56.310
It's useful for creating probabilities out of numbers.
Once we've created that,

19
00:00:56.340 --> 00:00:58.890
let's initialize our input Dataset as a matrix.

20
00:00:59.130 --> 00:01:01.200
Each row is a different training example.

21
00:01:01.350 --> 00:01:03.360
Each column represents a different neuron,

22
00:01:03.510 --> 00:01:06.840
so we have for training examples with three input neurons each.

23
00:01:07.170 --> 00:01:11.700
Then we'll create our output dataset for examples,
one output neuron each.

24
00:01:11.850 --> 00:01:13.860
Since we'll be generating random numbers in a second,

25
00:01:13.980 --> 00:01:16.020
let's see them to make them deterministic.

26
00:01:16.230 --> 00:01:19.830
This just means give random numbers that are generated the same starting point

27
00:01:19.860 --> 00:01:20.693
or seed,

28
00:01:20.700 --> 00:01:24.090
so that will get the same sequence of generated numbers every time we run our

29
00:01:24.091 --> 00:01:26.310
program.
This is useful for debugging.

30
00:01:26.670 --> 00:01:30.750
Next we'll create our synapse major CS synopsis or the connections between each

31
00:01:30.751 --> 00:01:33.630
neuron in one layer to every neuron in the next layer.

32
00:01:33.780 --> 00:01:37.310
Since we'll have three layers in our network,
we need to snaps matrices.

33
00:01:37.620 --> 00:01:40.530
Each synapse has a random weight assigned to it.
After that,

34
00:01:40.531 --> 00:01:41.760
we'll begin the training code.

35
00:01:42.090 --> 00:01:45.240
We'll create a for loop that iterates over the training code to optimize a

36
00:01:45.241 --> 00:01:48.600
network for the given data set.
We'll start off by creating our first layer.

37
00:01:48.690 --> 00:01:51.690
It's just our input data.
Now comes the prediction step.

38
00:01:51.810 --> 00:01:55.170
We'll perform matrix multiplication between each layer and it's sinaps.

39
00:01:55.470 --> 00:01:59.310
Then we'll run our sigmoid function on all the values in the matrix to create

40
00:01:59.311 --> 00:02:03.090
the next layer.
The next layer contains a prediction of the output data.

41
00:02:03.510 --> 00:02:06.570
Then we do the same thing on that layer to get our next layer,

42
00:02:06.690 --> 00:02:08.400
which is a more refined prediction.

43
00:02:08.610 --> 00:02:11.520
So now that we have a prediction of the output value and layer two,

44
00:02:11.730 --> 00:02:15.510
let's compare it to the expected output data using subtraction to get the error

45
00:02:15.511 --> 00:02:16.140
rate.

46
00:02:16.140 --> 00:02:19.140
We'll also want to print out the average error rate at a set interval to make

47
00:02:19.141 --> 00:02:21.960
sure it goes down every time.
Next,

48
00:02:21.961 --> 00:02:24.840
we'll multiply our error rate by the result of our sigmoid function.

49
00:02:25.020 --> 00:02:28.200
The function is used to get the derivative of our output prediction from layer

50
00:02:28.201 --> 00:02:30.030
to this will give us a Delta,

51
00:02:30.090 --> 00:02:33.060
which we'll use to reduce the error rate of our predictions when we update our

52
00:02:33.061 --> 00:02:34.620
synapses every iteration.

53
00:02:35.100 --> 00:02:38.700
Then we'll want to see how much layer one contributed to the error in layer two.

54
00:02:38.970 --> 00:02:40.650
This is called backpropagation.

55
00:02:40.800 --> 00:02:45.060
We'll get this error by multiplying layer two's Delta by synapse ones transpose.

56
00:02:45.420 --> 00:02:48.480
Then we'll get layer ones delta by multiplying its error.

57
00:02:48.481 --> 00:02:50.310
By the result of our sigmoid function.

58
00:02:50.520 --> 00:02:53.010
The function is used to get the derivative of layer one.

59
00:02:53.280 --> 00:02:55.350
Now that we have deltas for each of our layers,

60
00:02:55.470 --> 00:02:59.010
we can use them to update our sinaps waits to reduce the error rate more and

61
00:02:59.380 --> 00:03:03.460
every iteration.
This is an algorithm called gradient descent.
To do this,

62
00:03:03.461 --> 00:03:06.490
we'll just multiply each layer by a delta.
Finally,

63
00:03:06.520 --> 00:03:09.040
let's print the predicted output,
and there you have it.

64
00:03:09.220 --> 00:03:12.190
Let's run this in terminal and see what we get.
Awesome.

65
00:03:12.310 --> 00:03:15.420
We can see that our error rate decreases every iteration and the predicted

66
00:03:15.430 --> 00:03:18.400
output is very,
very close to the actual output.

67
00:03:18.430 --> 00:03:21.640
There is so much we can do to improve our neural network.
For more information,

68
00:03:21.641 --> 00:03:24.610
check out the links in the description below and please subscribe for more

69
00:03:24.611 --> 00:03:26.470
technology videos.
Thanks for watching.

