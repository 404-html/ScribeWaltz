WEBVTT

1
00:00:04.170 --> 00:00:07.780
I actually didn't play anything.
You just heard AI generated music.
Oh,

2
00:00:07.900 --> 00:00:09.700
a world walkmans rod geology.

3
00:00:09.850 --> 00:00:13.270
And this episode we're going to train a neural network that compose music all by

4
00:00:13.271 --> 00:00:15.100
itself.
Machine generated music.

5
00:00:15.130 --> 00:00:18.430
A technical term for this is music language modeling and it has a long history

6
00:00:18.431 --> 00:00:21.640
of research behind it.
Markov models and restricted Boltzmann machines,

7
00:00:21.850 --> 00:00:24.490
which kind of sounds like something out of half life or bioshock.
Hold on Babe.

8
00:00:24.760 --> 00:00:27.850
I've got to go save the world using the restricted Boltzmann machine music is

9
00:00:27.851 --> 00:00:31.630
how we communicate our emotions and passions and it's completely based on

10
00:00:31.631 --> 00:00:35.800
mathematical relationships,
octaves,
chords,
scales,
keys,

11
00:00:35.980 --> 00:00:37.990
all of it is math.
At the lowest level.

12
00:00:38.110 --> 00:00:40.960
Music is a series of sound waves that create pockets of air pressure.

13
00:00:41.080 --> 00:00:43.780
And the pitch we hear it depends on the frequency of changes in this air

14
00:00:43.781 --> 00:00:44.170
pressure.

15
00:00:44.170 --> 00:00:47.440
She music is the annotation would created to help us map the sounds into a

16
00:00:47.441 --> 00:00:49.030
repeatable set of instructions.

17
00:00:49.300 --> 00:00:52.930
So if machine learning is all about feeding data into models to find patterns

18
00:00:52.931 --> 00:00:56.980
and make predictions,
could we use it to feed in music data to predict new songs,

19
00:00:57.030 --> 00:00:57.940
apps of fruit?
We,

20
00:00:58.000 --> 00:01:01.030
we're going to build an APP that learned how to compose British folk music by

21
00:01:01.031 --> 00:01:04.060
training on a British folk dataset.
I was going to go with American,

22
00:01:04.061 --> 00:01:07.300
but I didn't think it'd be age appropriate.
There's too much sacks and violence.

23
00:01:08.020 --> 00:01:09.730
Get it.
We'll be using tensorflow,

24
00:01:09.731 --> 00:01:13.180
the sickest machine learning library ever to do this and just 10 lines of
python.

25
00:01:13.210 --> 00:01:15.340
We're going to follow our tried and true force that machine learning

26
00:01:15.341 --> 00:01:18.430
methodologies to do this collected data set,
build a model,

27
00:01:18.460 --> 00:01:21.530
train the model and test the model to start off with one to collect our data.

28
00:01:21.730 --> 00:01:23.620
So let's import the URL live module,

29
00:01:23.650 --> 00:01:26.020
which is going to let us download the file from the West.
Once we import it,

30
00:01:26.021 --> 00:01:28.240
we can call the URL retreat methods.
To do just that,

31
00:01:28.450 --> 00:01:30.790
we'll set the parameters to a Lincoln,
a Dataset and a name.

32
00:01:30.791 --> 00:01:31.840
We'll call it a downloaded file.

33
00:01:31.870 --> 00:01:33.760
We're using the Nottingham Dataset for this demo,

34
00:01:33.761 --> 00:01:36.770
which is a collection of a thousand British folk songs in Mitie format.

35
00:01:36.820 --> 00:01:40.600
Video format is perfect for us and codes all the note in time information,

36
00:01:40.690 --> 00:01:44.110
exactly how it would be written in music annotation.
It comes in a zip file.

37
00:01:44.200 --> 00:01:45.460
Want to unzip it as well.

38
00:01:45.520 --> 00:01:47.800
We can do this programmatically using the zip file module.

39
00:01:47.950 --> 00:01:51.190
We'll extract the data from the Zip and place it in the data directory.
Okay,

40
00:01:51.191 --> 00:01:54.340
so we've got our data.
It's time to create the model,
but before we do that,

41
00:01:54.341 --> 00:01:56.950
we need to think about how we want to represent our input data.

42
00:01:57.010 --> 00:01:57.940
There are two things happening.

43
00:01:58.090 --> 00:02:00.630
There's the main tune or melody and then they're the supporting.

44
00:02:00.630 --> 00:02:04.650
Those are harmony.
Let's represent each as a vector and to make things easier,

45
00:02:04.651 --> 00:02:07.930
we'll make two assumptions.
The first is that the melody is monophonic.

46
00:02:08.140 --> 00:02:10.600
That means only one note is played at each time step.

47
00:02:10.601 --> 00:02:14.320
The second is that the harmony at each time step can be classified into a core

48
00:02:14.321 --> 00:02:17.740
class,
so that's two different vectors,
one for melody and one per harmony.

49
00:02:17.770 --> 00:02:20.980
Well then combine them into one big vector and use that as our input.

50
00:02:21.010 --> 00:02:24.550
We can just import our ml helper class and then call the create model method.

51
00:02:24.610 --> 00:02:28.600
To do this,
music plays out over a period of time.
It's a sequence of notes,

52
00:02:28.660 --> 00:02:30.670
so we need to use a sequence learning model.

53
00:02:30.820 --> 00:02:33.850
It has to accept a sequence of notes as an input and output.

54
00:02:33.851 --> 00:02:36.700
A new sequence of notes,
plain old neural nets can't do this.

55
00:02:36.820 --> 00:02:39.580
They except fixed size inputs like an image or a number.

56
00:02:39.820 --> 00:02:42.820
We'll need a special kind of neural network,
a recurrent neural network.

57
00:02:43.120 --> 00:02:47.020
Those can deal with sequences.
Since data doesn't just flow one way,
it loops.

58
00:02:47.050 --> 00:02:50.080
This allows the network to have a kind of short term memory unlike Donald Trump,

59
00:02:50.081 --> 00:02:50.770
but wait,

60
00:02:50.770 --> 00:02:54.070
we want our network to not just remember the most recent music it's heard but

61
00:02:54.100 --> 00:02:57.310
all the music gets heard like a piece of music and have multiple themes in

62
00:02:57.311 --> 00:03:01.000
different parts of it.
Hopeful,
sad and if no one can remember members only.

63
00:03:01.001 --> 00:03:04.270
The most recent part which was cheery,
it's just going to compose cherry stuff.

64
00:03:04.600 --> 00:03:08.170
We need a special type of recurrent neural network called a long short term

65
00:03:08.171 --> 00:03:09.100
memory network.

66
00:03:09.460 --> 00:03:12.640
This type of network can remember things from way back in the sequence of data

67
00:03:12.820 --> 00:03:14.830
and it uses everything I remembered to generate.

68
00:03:14.831 --> 00:03:18.490
New sequences are model will generate the sequences and cord mapping file to a

69
00:03:18.491 --> 00:03:19.720
file and a data folder.

70
00:03:19.900 --> 00:03:23.440
This is a serialized bytes stream representation of our music that we can input

71
00:03:23.441 --> 00:03:25.390
directly into our model to train.
By the way,

72
00:03:25.540 --> 00:03:28.720
every machine learning model has a set of water called hyper parameters.

73
00:03:28.750 --> 00:03:31.780
These are the parameters that we human set for how our bottle operates,

74
00:03:31.810 --> 00:03:34.270
like knobs on a control panel.
How many layers do we want,

75
00:03:34.360 --> 00:03:36.640
how many iterations for training,
how many neurons?

76
00:03:36.670 --> 00:03:40.180
You can just use an existing model with pre tuned hyper parameters to build

77
00:03:40.181 --> 00:03:42.640
something.
Awesome,
so now we're ready to train our model.

78
00:03:42.670 --> 00:03:45.850
We can just call the train model method of our neural net class and do this.

79
00:03:45.890 --> 00:03:48.940
It's we'll get the network to start collecting the input data piece by piece.

80
00:03:48.970 --> 00:03:52.060
It took me about two hours to train us on my 2013 macbook pro,

81
00:03:52.090 --> 00:03:54.850
but you don't have to wait until it's completely done training to test it out.

82
00:03:54.880 --> 00:03:58.420
Just wait until you see the best loss of four encountered saving model message.

83
00:03:58.450 --> 00:03:59.201
Once you see that,

84
00:03:59.201 --> 00:04:03.280
you can type in RNN sample into terminal with the config file flag and pointed

85
00:04:03.281 --> 00:04:06.820
to a newly generated config file in your models folder that will generate a new

86
00:04:06.821 --> 00:04:10.210
song.
You think the newly trained model you've just created the generate music,

87
00:04:10.211 --> 00:04:13.900
we just sampled the melody and harmony at each time step and plug it into our

88
00:04:13.901 --> 00:04:16.840
trained model.
The model will then predict what the next note will be.

89
00:04:17.080 --> 00:04:20.650
The collection of all the predictive notes is our new we generated song.

90
00:04:26.820 --> 00:04:29.700
There's some improvements that could be made for sure the time signature.

91
00:04:29.730 --> 00:04:32.610
It's kind of sporadic and in terms of longterm structure,

92
00:04:32.730 --> 00:04:34.950
there seems to be a lack of repeated themes and phrases.

93
00:04:34.980 --> 00:04:38.250
The solution may well be more data and more computing power.

94
00:04:38.430 --> 00:04:41.280
It usually is when it comes to machine learning with deep neural nets,

95
00:04:41.610 --> 00:04:45.600
but he only can help us learn the fundamental nature of how music works in ways

96
00:04:45.601 --> 00:04:48.180
that we haven't even thought about it.
Links with more info below.

97
00:04:48.240 --> 00:04:51.720
Check him out and please subscribe for more machine learning videos per now I've

98
00:04:51.721 --> 00:04:54.240
got to go fix a runtime error,
so thanks for watching.

