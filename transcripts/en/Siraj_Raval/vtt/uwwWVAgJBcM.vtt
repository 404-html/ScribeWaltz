WEBVTT

1
00:00:14.050 --> 00:00:16.060
See,
let's see everybody

2
00:00:17.060 --> 00:00:17.250
<v 0>okay.</v>

3
00:00:17.250 --> 00:00:19.960
<v 1>Um,
minimize myself.
I don't need to see myself.</v>

4
00:00:19.961 --> 00:00:23.740
I need to see you guys when you go see you guys.

5
00:00:25.610 --> 00:00:30.320
Oh,
I am hired for this live session.

6
00:00:30.890 --> 00:00:34.400
Yo class is in session.
Everybody.

7
00:00:34.600 --> 00:00:38.390
We are about to do some math with a lot of sessions so I'm super excited.

8
00:00:38.540 --> 00:00:42.620
But first of all,
let me take some roll call.
All right,
so let's see.

9
00:00:42.650 --> 00:00:46.450
Calling Brandon Neel David because Sebastian,

10
00:00:46.460 --> 00:00:51.400
Raj Spencer and the rash,
Nico Clement.

11
00:00:53.000 --> 00:00:56.600
Hi Guys.
Michael Benjamin.
Alright,

12
00:00:56.870 --> 00:01:01.370
so that was road call.
Uh,
welcome to this live session,

13
00:01:01.460 --> 00:01:06.350
uh,
for the,
for the deep learning deep learning course.
Okay,

14
00:01:06.351 --> 00:01:11.351
this is going to be so awesome because I had been waiting to do some math and

15
00:01:11.421 --> 00:01:12.410
guess what guys,
guess what?

16
00:01:12.530 --> 00:01:17.190
I bought this a tad to write some math on.

17
00:01:17.240 --> 00:01:20.630
Okay.
And I've never used this before.
The light turns off.
Super excited for this.

18
00:01:20.930 --> 00:01:24.620
I'm going to show you guys the math behind linear regression.

19
00:01:24.621 --> 00:01:28.160
By the end of this video,
you guys are going to,

20
00:01:28.820 --> 00:01:33.530
are going to know like the back of your hand,
how to do linear regression.

21
00:01:33.710 --> 00:01:36.500
That includes gradient descent.
And guess what,

22
00:01:36.501 --> 00:01:40.160
we use gradient descent all over the place and machine learning.
Don't worry.

23
00:01:40.161 --> 00:01:42.740
If you don't know what that is,
I'm going to show it to you.
Okay.

24
00:01:42.860 --> 00:01:45.920
So we're going to deep dive into this.
So,
um,

25
00:01:46.820 --> 00:01:50.910
we're gonna start off with a five minute Q and a like always.
And,
uh,

26
00:01:50.980 --> 00:01:55.070
I think we've got some new Udacity peeps in the house as well,
uh,
through,
uh,

27
00:01:55.090 --> 00:01:57.800
Nico and,
uh,
Max,

28
00:01:57.860 --> 00:02:02.630
who's a d the other instructor for the course.
So,
uh,
you,
Udacity,
you guys,

29
00:02:02.631 --> 00:02:05.810
if you're,
if you're,
I think you're,
you're here a shout out,

30
00:02:05.811 --> 00:02:08.660
like say something so people know who you are.
And,
uh,

31
00:02:08.930 --> 00:02:11.980
so I'm gonna do my five minute Q and a like always.
And,
uh,

32
00:02:12.010 --> 00:02:15.560
I'm going to answer all the questions related to me and you know,
my everything.

33
00:02:15.800 --> 00:02:18.080
But if you have anything,
like you'd ask these specific questions,

34
00:02:18.140 --> 00:02:19.820
they will answer those.
Okay?

35
00:02:19.821 --> 00:02:22.640
So let's start off with a five minute Q and a and then we're gonna get right

36
00:02:22.641 --> 00:02:27.390
into the code.
And Matt.
Okay.
Uh,

37
00:02:27.520 --> 00:02:29.290
do I have to know about partial derivatives?

38
00:02:29.320 --> 00:02:33.580
We are going to do a partial derivative.
Um,
but I'll show you how that works.

39
00:02:33.980 --> 00:02:38.880
Um,
I had to cut off cutie pie to catch this.

40
00:02:38.881 --> 00:02:42.480
Well,
I'm honored.
I'm honored a baby girl.

41
00:02:42.481 --> 00:02:46.400
Let me see that regression.
All right,
so that's not a question though.
Let's,

42
00:02:46.420 --> 00:02:50.220
let's get some,
let's,
let's get some real questions in there.
Some,
some quality,

43
00:02:50.610 --> 00:02:51.960
some quality questions.

44
00:02:53.300 --> 00:02:54.260
<v 0>All right.</v>

45
00:02:58.860 --> 00:03:00.220
<v 1>Would want to check out my vibe</v>

46
00:03:00.250 --> 00:03:02.080
<v 0>AI assistant demo.
Sure.
Yes.</v>

47
00:03:02.140 --> 00:03:04.330
I post to get home link in the comments of one of my videos.

48
00:03:04.331 --> 00:03:07.780
I read all my comments.
I answer all my comments.
See,
I'm not,
I'm not,

49
00:03:07.870 --> 00:03:10.720
I'm not fake.
You know what I'm saying?
I answer all my comments.

50
00:03:10.721 --> 00:03:12.100
I'm here for you guys.

51
00:03:12.430 --> 00:03:16.980
Have I enrolled in his calculus required for linear regression?
Yes.
Um,

52
00:03:17.140 --> 00:03:19.420
a little bit of talk to it,
but I'm going to go through that.
Don't,

53
00:03:19.421 --> 00:03:20.980
don't be afraid by the word capital.
This,
this is,

54
00:03:21.000 --> 00:03:25.150
this is actually very intuitive.
Uh,
um,
okay.

55
00:03:25.710 --> 00:03:29.670
<v 1>Can you mention some details about the upcoming as well?
Uh,
uh,</v>

56
00:03:29.730 --> 00:03:31.370
looking to predict your genre from

57
00:03:40.740 --> 00:03:41.573
all right.

58
00:03:45.880 --> 00:03:49.240
What basic Max will give you?
They don't know.
You need to know basic Algebra.

59
00:03:49.241 --> 00:03:49.571
Okay.

60
00:03:49.571 --> 00:03:53.530
And then we're going to learn the calculus necessary to do this in this video.

61
00:03:53.531 --> 00:03:57.100
Okay.
Our Games,
the future.
Yes.
I mean,

62
00:03:57.101 --> 00:04:01.970
the idea of between generating generative models in general,
uh,

63
00:04:02.110 --> 00:04:06.970
are really exciting because,
uh,
there's,
you can generate things that don't exist.

64
00:04:07.080 --> 00:04:11.470
Um,
and that has a lot of potential for art and culture.

65
00:04:11.920 --> 00:04:16.150
Dan's can change culture,
right?
We can generate music,
we could generate art.

66
00:04:16.151 --> 00:04:20.710
We could generate paintings in ways that humans could best book to understand

67
00:04:20.711 --> 00:04:23.920
math behind ml machine learning.
A probabilistic approach.

68
00:04:23.990 --> 00:04:26.470
That's a pretty good ones.
Just not today.
Or coding to both.

69
00:04:26.471 --> 00:04:31.210
Mostly coatings and you were brushing versus other classifiers like SG DC when

70
00:04:31.211 --> 00:04:35.580
you're brushing is definitely easier.
Uh,
what is no free lunch?
It's a theorem.
Uh,

71
00:04:35.800 --> 00:04:39.910
the no free lunch theorem at a very high level.
It's like,
while you,
you can't,

72
00:04:39.911 --> 00:04:42.430
you can't make assumptions,
you can't make assumptions.
Uh,

73
00:04:42.431 --> 00:04:47.030
whenever you aren't doing anything about related to,
uh,
proving something,

74
00:04:48.160 --> 00:04:50.830
just,
um,
when will you do NLP?
Yo,

75
00:04:50.831 --> 00:04:54.580
I'm going to do so much NLP in this course.
I can't wait for NLP.
It's,

76
00:04:54.670 --> 00:04:57.860
it's coming up.
Will you cover again?

77
00:04:58.030 --> 00:05:00.460
I kind of want to just do gans right now.
You know what I mean?

78
00:05:00.461 --> 00:05:03.040
Like I'm super excited for against,
I will do dance.

79
00:05:06.520 --> 00:05:07.360
Um,

80
00:05:08.470 --> 00:05:11.920
I will give an intuition why to do gradient descent over over.
Yes.

81
00:05:11.921 --> 00:05:15.220
I will explain that linear Algebra is the way to go.
Yes.

82
00:05:15.250 --> 00:05:17.110
What's the difference between psych learn and CF learn?

83
00:05:17.170 --> 00:05:19.060
So I can learn and CF learning?
Great question.

84
00:05:19.061 --> 00:05:22.630
So CFR did say high level wrapper on top of tensorflow?
Uh,

85
00:05:22.650 --> 00:05:27.550
it's very similar looking to the side.
Uh,
but second learning specifically is,

86
00:05:27.910 --> 00:05:31.480
uh,
it did,
it does.

87
00:05:31.560 --> 00:05:35.580
So TF one only focuses on deep neural networks,
so I could learn,

88
00:05:35.590 --> 00:05:39.040
use as a support vector machines and all sorts of other machine running models,

89
00:05:39.220 --> 00:05:42.760
whereas TFR is the same kind of,
uh,
it has the same brevity,

90
00:05:42.940 --> 00:05:47.730
but it focuses only on ignoring networks.
Um,
do you prefer a WeChat?
No.

91
00:05:48.360 --> 00:05:53.220
No?
No.
Oh,
when will you start?
When will you start working on Anaconda?
Uh,

92
00:05:53.221 --> 00:05:57.030
I mean,
I'm mostly almost like they start using docker to containerize things.

93
00:05:57.740 --> 00:06:00.560
Alright.
Wrap for 50 k subs.
Okay.
When you wrap through the DCA stops,

94
00:06:00.561 --> 00:06:04.140
and this time I'm going to uh,
uh,
play an instrumental on,

95
00:06:04.180 --> 00:06:07.700
I'm just going to wrap with no,
you know what I mean?
Don't be discouraged.
Rap,

96
00:06:08.000 --> 00:06:09.770
hip hop,
instrumental on youtube,

97
00:06:09.771 --> 00:06:12.760
whatever tests playing someone say a key word and then we're gonna get started.

98
00:06:12.960 --> 00:06:17.430
Try on hip hop comes from around.
What is this kind of scale?
All right.

99
00:06:17.600 --> 00:06:20.340
Can you just unplugged my mind?
So you got,
yeah,
you guys can see this.

100
00:06:21.930 --> 00:06:22.560
<v 0>Okay.</v>

101
00:06:22.560 --> 00:06:23.900
<v 1>It wasn't music.
What does music?
Dad's</v>

102
00:06:29.810 --> 00:06:33.950
50 cakes.
I got 50 cases.
My mind is still fresh.

103
00:06:34.350 --> 00:06:36.890
Looking at this copy mass looks like the best.

104
00:06:40.020 --> 00:06:42.740
I got a USB four mile by.

105
00:06:43.190 --> 00:06:47.110
I'm going to be waiting back online.
I see it.

106
00:06:47.690 --> 00:06:49.630
It's all fine.
That's all right.

107
00:06:49.640 --> 00:06:54.080
These equations online coming back like Russian radio.

108
00:06:58.490 --> 00:07:01.760
So,
okay,
so that was it.
Oh,
see now we're going to get started with the code.
Okay.

109
00:07:01.880 --> 00:07:03.380
So let's go ahead and do this.

110
00:07:04.160 --> 00:07:04.890
<v 0>Okay.</v>

111
00:07:04.890 --> 00:07:08.140
<v 1>I'm going to start screen sharing and then we're going to get started.
Um,</v>

112
00:07:08.210 --> 00:07:09.043
all right,
here we go.

113
00:07:11.520 --> 00:07:11.900
<v 0>Yeah,</v>

114
00:07:11.900 --> 00:07:16.130
<v 1>here we go.
We'll hang outs.
All right.
And what does hangouts want to do?</v>

115
00:07:16.131 --> 00:07:19.920
Hangouts wants to screen share out.

116
00:07:19.950 --> 00:07:24.910
It's going to screen share entire screen share.
All right?

117
00:07:25.250 --> 00:07:26.570
So I'll minimize this.

118
00:07:28.020 --> 00:07:28.390
<v 0>Okay.</v>

119
00:07:28.390 --> 00:07:30.880
<v 1>And minimize,</v>

120
00:07:30.940 --> 00:07:35.890
and then I'll move this out of the way so I can see what you guys are doing.

121
00:07:36.400 --> 00:07:40.860
A,
okay.
And we're going to code this baby.
Okay.
I'm in the corner here.
I mean,

122
00:07:40.870 --> 00:07:43.660
make sure that what you guys are saying is what I want you to see.

123
00:07:44.230 --> 00:07:45.063
<v 0>MMM.</v>

124
00:07:46.970 --> 00:07:50.780
<v 1>Yes.
What you guys are saying is exactly what I want you to see.
Perfect.</v>

125
00:07:51.330 --> 00:07:52.280
All right.

126
00:07:54.750 --> 00:07:55.583
<v 0>Okay.</v>

127
00:07:55.800 --> 00:07:56.790
<v 1>Okay.
So here we go.</v>

128
00:07:58.010 --> 00:07:58.380
<v 0>Okay.</v>

129
00:07:58.380 --> 00:08:01.190
<v 1>Here's what we're going to do.
Beds.
I mean,
make the state that's a success.</v>

130
00:08:01.550 --> 00:08:02.940
It's a big enough,
right?
Okay.

131
00:08:04.020 --> 00:08:06.870
So in this lesson we're going to do linear regression,
okay?

132
00:08:06.871 --> 00:08:11.730
And what is linear regression right now?
Okay,
so linear regression in this case.

133
00:08:12.260 --> 00:08:16.680
Um,
and let me just make sure everything's working.
Um,
everybody's here.

134
00:08:16.681 --> 00:08:20.430
Electronic working live.
Video's not working.
Okay?
So here's,
here's how it goes.

135
00:08:20.640 --> 00:08:23.520
So we're going to do this.
Okay.
Uh,

136
00:08:23.550 --> 00:08:26.220
so this is going to be called linear regression.
This is when your aggression,

137
00:08:26.250 --> 00:08:30.300
and let me,
let me,
uh,
let me just show you guys the best way to explain it.

138
00:08:30.301 --> 00:08:31.560
It's a show to visual.

139
00:08:31.590 --> 00:08:35.460
So I'll show it to visual what exactly we're going to be doing and to show you

140
00:08:35.461 --> 00:08:39.420
visually,
I will give you a late to this and I will just show it right here.

141
00:08:39.510 --> 00:08:44.110
This is what's happening.
So we have a set of points and these points are,
uh,

142
00:08:44.160 --> 00:08:44.400
these,

143
00:08:44.400 --> 00:08:48.300
these points are the test scores of students and the amount of hours study.
Okay?

144
00:08:48.301 --> 00:08:51.760
So this is what it looks like.
So this right on the right,
this rack here,
uh,

145
00:08:52.190 --> 00:08:53.770
these set of points,
uh,

146
00:08:53.790 --> 00:08:58.740
on the set are the x values are the amount of hours the study and the y values

147
00:08:58.741 --> 00:09:02.070
are the test scores they got.
Okay.
And intuitively to us,

148
00:09:02.071 --> 00:09:04.440
there must be some kind of correlation between these two values.

149
00:09:04.680 --> 00:09:08.880
But we want to prove this programmatically.
We want to prove,
I mean,
sorry,

150
00:09:08.940 --> 00:09:12.300
mathematically we went to prove that there's a relationship and how do we prove

151
00:09:12.301 --> 00:09:15.120
that there was a relationship?
We draw a line of best fit.

152
00:09:15.420 --> 00:09:17.820
So how do we know what that line of best fit is?

153
00:09:17.860 --> 00:09:22.740
That linear regression is what we don't know.
We don't know.
We went to,

154
00:09:23.010 --> 00:09:26.310
we have to find that.
And the way we're going to find the line of best fit,

155
00:09:26.340 --> 00:09:28.920
it's using gradient descent and that process,

156
00:09:28.921 --> 00:09:32.280
that training process looks like this.
We're going to draw on a random line,

157
00:09:32.610 --> 00:09:34.470
compute the error for that line.

158
00:09:34.471 --> 00:09:36.510
And I'll talk about how we're going to compute that error.

159
00:09:36.810 --> 00:09:41.430
And that error value is going to say how,
how wellfit is this line to the data?

160
00:09:41.760 --> 00:09:44.550
And then based on that error is that it's going to act as a company,

161
00:09:44.570 --> 00:09:45.630
it's going to tell us,
well,

162
00:09:45.631 --> 00:09:48.750
how best should you redraw the lines to be closer to the line of best fit.

163
00:09:49.020 --> 00:09:52.560
And we'll keep doing that.
So it will be like draw line,
uh,
compute hair,

164
00:09:52.720 --> 00:09:54.050
throw line,
compute Eric,
draw line,

165
00:09:54.051 --> 00:09:57.750
compute era until eventually the line that we drop is the optimal line that we

166
00:09:57.751 --> 00:10:00.780
should draw.
Okay,
so that's at a very high level.

167
00:10:00.781 --> 00:10:03.390
But now I'm going to go into the code and we're going to talk about this in

168
00:10:03.391 --> 00:10:06.810
detail.
All right,
so let's go ahead and start it.
Um,

169
00:10:07.860 --> 00:10:09.870
so to start off,
to start off,

170
00:10:09.871 --> 00:10:13.680
I'm going to write my main function.
Okay?

171
00:10:13.681 --> 00:10:16.770
So let me move all this stuff out of the way.
It's okay.
Right into the code.

172
00:10:17.400 --> 00:10:21.660
All right,
I'll get right into the code for this.

173
00:10:22.020 --> 00:10:23.520
And guys are also,

174
00:10:23.570 --> 00:10:27.540
if people have questions and I'm not able to answer them because I'm busy doing

175
00:10:27.570 --> 00:10:31.440
separately,
please help me answer questions.
I very much appreciate it.

176
00:10:31.960 --> 00:10:33.510
I very much appreciate it.
Okay,

177
00:10:33.720 --> 00:10:36.320
so let me just show up by writing the main function.
Well,
why am I,

178
00:10:36.360 --> 00:10:40.200
what is the main function here?
Well that's,
that's where the read of the cocoa,

179
00:10:40.260 --> 00:10:43.770
right?
Okay.
So we had the main bumps.
We'll write or run function,

180
00:10:43.830 --> 00:10:46.800
which is where we're going to store all of our logic.
Okay.

181
00:10:46.980 --> 00:10:48.600
So let's write up to run functions.

182
00:10:48.870 --> 00:10:52.390
So the run functions are a chance for us to show what we're doing at high level,

183
00:10:52.860 --> 00:10:55.980
at a high level.
So step one is collect our data,
right?

184
00:10:56.100 --> 00:10:58.860
Always in machine learning.
We want to collect our data.

185
00:10:59.100 --> 00:11:03.090
So we'll get our data points and we're going to do,
oh,
what,

186
00:11:03.091 --> 00:11:05.940
how are we going to collect our data,
right?
Well,
the collect our data,

187
00:11:05.941 --> 00:11:08.580
we have to import the one library that we're using.

188
00:11:08.581 --> 00:11:12.200
I know guys were using eight single library and that library of reasons.
None.

189
00:11:12.910 --> 00:11:14.820
All right.
And we're gonna use this little symbol.

190
00:11:14.821 --> 00:11:18.810
That means you don't have to continually say hi whenever we call it a method or

191
00:11:18.811 --> 00:11:22.350
assumptions.
Okay.
So what is the function we're going to use from Nepal?

192
00:11:22.740 --> 00:11:27.440
So the bunch I'm going to use from num Pi,
sorry.

193
00:11:27.441 --> 00:11:30.560
Right Maine.
Thank you.
Good call.

194
00:11:30.640 --> 00:11:33.590
So the front door when they get from [inaudible] hi,
it's Jen from tests.

195
00:11:33.890 --> 00:11:37.010
And what this is going to do because it's going to get the data points from our

196
00:11:37.150 --> 00:11:40.010
uh,
David,
uh,
data file.
And let me show you guys the data file as well.

197
00:11:40.260 --> 00:11:44.000
But basically we're going to separate,
separate it by the comments.
Okay.

198
00:11:44.360 --> 00:11:46.940
And we're going to get those points.
So what,
what does this,

199
00:11:46.970 --> 00:11:48.200
what does this data look like?
Well,

200
00:11:48.201 --> 00:11:52.910
let me pull up terminal and show you guys exactly what this data looks good.

201
00:11:52.980 --> 00:11:57.970
It looks like eight A.
Dot.
CSV.
Okay,
so let me,

202
00:11:58.040 --> 00:11:59.060
let me zoom out of this.

203
00:12:00.170 --> 00:12:01.003
<v 0>Wow.</v>

204
00:12:01.500 --> 00:12:03.580
<v 1>Do way more do you want him to say?</v>

205
00:12:03.740 --> 00:12:07.770
So these are just the hour study on the left side and then the test scores for a

206
00:12:07.771 --> 00:12:10.740
bunch of students for our intro to computer science class.
Okay.

207
00:12:10.741 --> 00:12:15.630
The hour study and the test scores they got.
Okay.

208
00:12:15.631 --> 00:12:17.460
So that's what we're going to pull.
That's our dataset.

209
00:12:17.550 --> 00:12:21.390
That's what we're going to pull into our uh,
points variable.

210
00:12:21.391 --> 00:12:23.820
So it's a bunch of points.
Is going to contain a bunch of x,

211
00:12:23.821 --> 00:12:28.800
y value parents where x is the amount of our study and why is the test score

212
00:12:29.100 --> 00:12:30.720
okay?
And it's separated by the,

213
00:12:32.740 --> 00:12:32.960
<v 0>come</v>

214
00:12:32.960 --> 00:12:35.390
<v 1>on.
Okay.
So that's,
that's that one.
We've done that.</v>

215
00:12:35.391 --> 00:12:37.900
And you're from Texas essentially,
right in two main leads.

216
00:12:38.000 --> 00:12:41.180
The first loop converts each line of the bond with sequence strings and the

217
00:12:41.181 --> 00:12:44.750
second one is converting each string into the appropriate data tab.
Okay.

218
00:12:44.751 --> 00:12:49.580
So that's step one.
Now,
step two is to define our hyper parameters.
Okay.

219
00:12:49.581 --> 00:12:53.660
In machine learning we have what are called hyper parameters.

220
00:12:53.661 --> 00:12:55.580
These are tuning knobs for our model.

221
00:12:55.700 --> 00:13:00.660
They are basically the parameters that define,
you know,
how our model is,

222
00:13:00.780 --> 00:13:05.260
is analyzing certain data,
how,
how fast it's fitting to the data,
what,
what,

223
00:13:05.360 --> 00:13:07.160
what operations are performing on a day.

224
00:13:07.250 --> 00:13:09.980
There's a whole bunch of hyper parameters.
Um,

225
00:13:11.420 --> 00:13:12.290
thank you for the feedback.

226
00:13:12.890 --> 00:13:17.660
There's a whole bunch of hyper parameters and what we're going to use just the

227
00:13:17.661 --> 00:13:22.170
learning groups.
Now the learning rate is used a lot in machinery and you,

228
00:13:22.390 --> 00:13:24.740
and it basically defines how fast,

229
00:13:25.970 --> 00:13:29.810
how fast should our uh,
model converged.

230
00:13:29.870 --> 00:13:33.440
And converged is a word for convergence.

231
00:13:34.190 --> 00:13:38.300
Convergence means when you get the optimal results,
the optimal model,

232
00:13:38.330 --> 00:13:40.850
like the line of best fit in our case,
that's convergence.

233
00:13:41.240 --> 00:13:43.700
So how fast should we converge?
Well,
you might be thinking,
well,

234
00:13:43.790 --> 00:13:46.460
should the learning rate just the like a million if you went to convert super

235
00:13:46.461 --> 00:13:50.330
fast?
Well No,
like all hyper parameters,
it's a balanced,
okay,

236
00:13:50.331 --> 00:13:53.450
so get the learning rate is too small.
We're going to get slow convergence.

237
00:13:53.630 --> 00:13:57.710
But if it's too big,
then our air function might not decrease.
Okay.

238
00:13:57.740 --> 00:14:02.600
So it might not converge.
So that,
so that's our first hyper parameter.

239
00:14:02.960 --> 00:14:07.700
Our next type of Predator is going to be the initial value for B and the initial

240
00:14:07.701 --> 00:14:10.310
value for m.
And what does bnm well,

241
00:14:10.311 --> 00:14:13.850
what we're going to do is we're going to calculate the slope,
right?

242
00:14:13.851 --> 00:14:16.040
So this looks like y equals mx book.

243
00:14:16.280 --> 00:14:21.170
So this is why I said we only need to know basic Algebra.
This is the formula.

244
00:14:21.200 --> 00:14:26.030
This is the slope formula,
okay?
All lines follow this formula where,

245
00:14:26.031 --> 00:14:30.740
why?
So m is the slope be as a liner set.
X and y are the points.

246
00:14:30.770 --> 00:14:33.740
Okay?
So that's the line.
Okay?
So this is our initial,

247
00:14:34.120 --> 00:14:37.850
the value or initial slope and our,
and our initial y intercepts,

248
00:14:37.880 --> 00:14:41.360
they're going to start off as a zero.
Okay?
So,

249
00:14:42.760 --> 00:14:45.890
and then the last type of parameter is then it's going to be the number of

250
00:14:45.920 --> 00:14:50.690
iterations.
How much,
how much do we want to train this model?
Well,
we have a very,

251
00:14:50.691 --> 00:14:54.860
very dataset.
There's only a hundred points.
Okay?
Um,

252
00:14:55.160 --> 00:14:58.170
and
for that,
we're not going to,

253
00:14:58.190 --> 00:15:00.890
we're not going to iterate a million times or 100,000 times.

254
00:15:00.891 --> 00:15:03.140
We're just going to iterate a thousand times.
Okay.

255
00:15:03.170 --> 00:15:07.110
So that's our type of parameters.
And now step three is going to be too,

256
00:15:08.730 --> 00:15:09.330
<v 0>yeah.</v>

257
00:15:09.330 --> 00:15:12.470
<v 1>Uh,
they are trained on model based print,</v>

258
00:15:12.471 --> 00:15:14.920
our motto trainer model.
Okay.

259
00:15:14.921 --> 00:15:19.530
So the first step is going to be to show this starting gradient descent.

260
00:15:20.110 --> 00:15:23.710
Okay.
At v Equals um,
what does it starting gradient descent.

261
00:15:23.740 --> 00:15:25.000
It's going to be zero,
right?

262
00:15:25.300 --> 00:15:29.590
And then Emma is going to be that starting point is going to for that we'll say

263
00:15:31.300 --> 00:15:35.690
one,
this is just for us to see the difference here.
Okay.
Um,

264
00:15:40.100 --> 00:15:43.810
right dot format initial,
the

265
00:15:47.500 --> 00:15:52.030
initial m I so,
um,
okay,
so what's happening here?

266
00:15:56.490 --> 00:15:57.323
Error

267
00:15:59.250 --> 00:16:02.400
or line given points.

268
00:16:03.360 --> 00:16:07.610
So let me just write this out.
It will be,

269
00:16:11.370 --> 00:16:13.950
and then appointments.
So what is,
what's happening here?

270
00:16:14.040 --> 00:16:15.490
Let's go over what I just wrote here.

271
00:16:15.640 --> 00:16:20.460
So in this line we're going to show the starting a B value to starting m value.

272
00:16:20.461 --> 00:16:23.130
So what is our starting line and separate as our starting slow and what does our

273
00:16:23.131 --> 00:16:24.030
starting error.

274
00:16:24.120 --> 00:16:27.390
And I'm going to show you how we're going to calculate that error and to get

275
00:16:27.391 --> 00:16:30.270
that error given our be,
uh,
given,

276
00:16:30.630 --> 00:16:33.980
given our B and m values,
we have dysfunction.

277
00:16:33.981 --> 00:16:36.060
You're called compute air for Lynette given points.

278
00:16:36.120 --> 00:16:39.180
It's going to take a beat em and the points and it's going to compute the era

279
00:16:39.181 --> 00:16:40.410
for that and it's going to help with that.

280
00:16:40.650 --> 00:16:42.540
So that's going to be our starting point.
Okay.

281
00:16:42.780 --> 00:16:45.600
And then now we're going to actually perform,

282
00:16:45.660 --> 00:16:50.660
are graded descent and it's going to give us the optimal slow B and the optimal

283
00:16:52.680 --> 00:16:57.400
a slope.
I sorry.
It's going to go see optimal slogan,
the optimal wider sense.
So,

284
00:16:57.401 --> 00:17:00.190
because great at the sentence,
uh,
we're gonna have,

285
00:17:00.210 --> 00:17:02.420
we're going to call this method the gradient descent referred.

286
00:17:02.460 --> 00:17:06.630
So given points given an initial BW and initial,

287
00:17:07.370 --> 00:17:11.460
um,
uh,
and so an initial end value

288
00:17:13.310 --> 00:17:14.143
in our learning rate.

289
00:17:14.160 --> 00:17:16.530
So this is where we're going to use all the kinds of printers,
right?

290
00:17:16.531 --> 00:17:19.560
Cause this is where we're training our models.
So a number of iterations.

291
00:17:19.620 --> 00:17:21.740
Those are all the things we need to this.
Okay.

292
00:17:21.750 --> 00:17:24.120
And we're going to define these functions in a second.
Okay.
We're going to,

293
00:17:24.180 --> 00:17:27.660
we're going to go deep dive in and defined exceptions.
Okay.

294
00:17:27.661 --> 00:17:31.500
So then after we've paid our model,
well now we can just try it out,
right?

295
00:17:31.501 --> 00:17:35.100
So let me just copy and paste this.
So now this is not our starting point.

296
00:17:35.190 --> 00:17:38.490
It says now are,
uh,
nd ratings,
any point.

297
00:17:38.950 --> 00:17:41.280
So face or any point,
where are we?

298
00:17:41.281 --> 00:17:46.050
Is this a B as to ms too?
And an Arab is three.

299
00:17:46.051 --> 00:17:48.550
And this is with these numbers just defined,
uh,

300
00:17:50.580 --> 00:17:51.680
what,
uh,

301
00:17:51.870 --> 00:17:56.870
we're going to see at the end after a number of iterations or B and then

302
00:18:02.870 --> 00:18:06.400
four.
And then for computing the air for line at given points,

303
00:18:07.370 --> 00:18:12.040
be that the final value,
the final 10 value,
and then our cost.
Okay.

304
00:18:12.041 --> 00:18:15.450
So,
okay,

305
00:18:15.451 --> 00:18:19.740
so that is high level what's happening here.

306
00:18:19.800 --> 00:18:22.980
So all I did was I just printed out the initial BNN value,
which just nothing.

307
00:18:23.190 --> 00:18:24.680
And then the air.
Uh,

308
00:18:24.690 --> 00:18:28.700
and then I computed the gradient descent and then I print out the final values.

309
00:18:28.770 --> 00:18:32.280
So I'm about to,
I'm about to do this now.
Okay.
So we haven't actually done,

310
00:18:32.760 --> 00:18:33.451
now we're going to do it.

311
00:18:33.451 --> 00:18:36.570
So the first thing I'm going to talk about is how are we going to compete that

312
00:18:36.571 --> 00:18:40.650
here?
Okay.
So let's,
let's,
let's,
um,
right at that first function,

313
00:18:40.651 --> 00:18:41.910
what would that person don't you call?

314
00:18:42.000 --> 00:18:46.710
It was called compute error per line given points.

315
00:18:47.070 --> 00:18:51.630
Okay.
Okay.
So,
and the data set,

316
00:18:51.631 --> 00:18:54.510
I'm going to provide that as well.
Um,
but,
but let's,

317
00:18:54.511 --> 00:18:57.330
let's go ahead and run up this method.
Okay.
So,
so this is the first step.

318
00:18:57.331 --> 00:19:00.270
We're going to write up this method.
You,
Eric,
for Atlanta I can employ.

319
00:19:00.510 --> 00:19:04.560
So excited to show you guys is because I get to use my mac panicked for a
second.

320
00:19:04.650 --> 00:19:09.450
Okay.
So,
so let me,
let me,
right.
Okay,
hold on.
Okay,
here we go.
Okay.
So,

321
00:19:09.510 --> 00:19:13.080
um,
okay,
let me
right.

322
00:19:14.660 --> 00:19:18.550
Okay,
so we got a line here.

323
00:19:19.590 --> 00:19:20.000
<v 0>Okay.</v>

324
00:19:20.000 --> 00:19:24.740
<v 1>Oh Man,
what a great,
what a great line dies is.
Okay,</v>

325
00:19:24.920 --> 00:19:28.610
so this is our plot.
Okay.
And so we've got a bunch of data points here.

326
00:19:28.611 --> 00:19:31.610
We've got a bunch of data points,
right?
It's all over the place.

327
00:19:31.670 --> 00:19:36.670
And what we're gonna do is we're going to draw a random lot through that data.

328
00:19:37.280 --> 00:19:39.440
Okay?
We don't know,
we don't know the line of best fit.

329
00:19:39.441 --> 00:19:42.500
So we're going to go on a random line through the data and then we're going to

330
00:19:42.501 --> 00:19:44.430
compute the error of that line.

331
00:19:44.610 --> 00:19:48.260
And so that error is going to tell us how good our line hits.
Okay?

332
00:19:48.261 --> 00:19:50.630
And so how do we know how good our line is?
Well,

333
00:19:50.631 --> 00:19:53.960
what we're gonna do is we're going to for every single,
um,

334
00:19:54.170 --> 00:19:55.670
why value on that line,

335
00:19:55.830 --> 00:20:00.090
we are going to calculate the distance from each point,
uh,

336
00:20:00.110 --> 00:20:04.010
that data to go along.
Okay?
So all of these distances,

337
00:20:04.670 --> 00:20:08.330
all of these distances,
instance wine,
this is two,
this is three,
this is four,

338
00:20:08.460 --> 00:20:10.760
this is five and six all.
And then,
you know,

339
00:20:10.761 --> 00:20:12.350
we probably have more data points down here,

340
00:20:12.470 --> 00:20:14.250
these distances that this is his life.

341
00:20:14.530 --> 00:20:18.680
And so we're going to take all those distances and once you someday.

342
00:20:19.010 --> 00:20:21.080
And so let me show you the equation for that.
Okay?

343
00:20:21.650 --> 00:20:26.240
So rather than actually writing out this equation,
uh,
like really sloppily,

344
00:20:26.300 --> 00:20:30.870
I'm with the show it to you,
um,
uh,
using this.

345
00:20:31.160 --> 00:20:35.300
Okay?
So,
okay,
so this is the equation.
So let me explain what this is,

346
00:20:35.330 --> 00:20:39.290
what this says.
So we got all those distances,
right?
We got old as distances,

347
00:20:39.470 --> 00:20:42.920
we're going to some of those distances together and that,

348
00:20:42.921 --> 00:20:44.720
and then get the average of that.
But guess what,

349
00:20:44.750 --> 00:20:46.940
we're not just going to have some those values alone.

350
00:20:47.080 --> 00:20:50.200
What do you square those doubts?
And why are we swearing as values?

351
00:20:50.470 --> 00:20:55.270
Because we're squaring those values because we want and we want to first of all,

352
00:20:55.271 --> 00:20:58.780
to be positive.
Uh,
and it doesn't really matter what the actual value is.

353
00:20:58.781 --> 00:21:01.750
It's more about the magnitude of those dogs,
right?
So,

354
00:21:01.890 --> 00:21:03.730
and we went to minimize that magnitude overtime.

355
00:21:03.850 --> 00:21:06.700
So this is the equation for that.
Okay?
So let me explain what this,

356
00:21:06.730 --> 00:21:09.820
what the hell does say is,
okay,
so we're computing the error.

357
00:21:10.150 --> 00:21:13.690
We are computing the error of our line given m and d.

358
00:21:13.691 --> 00:21:16.740
So given em and B,
we're going to compute the care line,

359
00:21:16.840 --> 00:21:21.310
MSR slope and be as our wine upset.
So this e looking thing is called,

360
00:21:21.610 --> 00:21:23.110
it's called sigma notation.

361
00:21:23.140 --> 00:21:26.530
It's a little rich giving you guys a little refresher here this evening.

362
00:21:26.531 --> 00:21:29.020
We're going to see a lot of machine learning.
It's,
it's,
it's,

363
00:21:29.080 --> 00:21:32.260
it's called sigma notation and basically it taught it.

364
00:21:32.280 --> 00:21:36.230
It's a way of describing Capelin the sum of a set of uh,

365
00:21:36.300 --> 00:21:40.390
of a set of values.
Uh,
all right,
so the sum of a set of values,

366
00:21:40.700 --> 00:21:44.080
which is what we're doing,
we're talking to some of a set of points.
So for,

367
00:21:44.140 --> 00:21:45.220
and so it,

368
00:21:45.221 --> 00:21:48.760
the starting point is where I equals one and the ending point heads for an end.

369
00:21:48.870 --> 00:21:50.320
It's for every point.
Okay.

370
00:21:50.321 --> 00:21:54.220
So for every point we went to calculate the difference in,
in why dogs,
right?

371
00:21:54.221 --> 00:21:58.870
So why minus mx plus B,
and why do we say mx plus B?

372
00:21:58.930 --> 00:22:02.980
Because in the flip equation,
y equals mx plus B,
right?

373
00:22:03.130 --> 00:22:06.200
So it's y minus MSSP,
which the SEC,

374
00:22:06.220 --> 00:22:08.740
which essentially boils down to just one.

375
00:22:09.010 --> 00:22:12.120
So it's y minus y square and we'll do,

376
00:22:12.130 --> 00:22:14.650
and then we're doing that for every single point.

377
00:22:14.920 --> 00:22:17.950
And so we're going to add all of those points together.
Okay?

378
00:22:17.951 --> 00:22:19.990
So and then get the average.

379
00:22:19.991 --> 00:22:23.170
And so that's why one over n because we're going to get the average of that.

380
00:22:23.290 --> 00:22:27.980
And that's value.
That value is the error.
Okay?
So at a high level,

381
00:22:28.030 --> 00:22:31.840
that is what that is.
So let's now,
let's programmatically write this out.
Okay.

382
00:22:31.990 --> 00:22:35.440
So we're going through startup by initializing the hair,

383
00:22:36.210 --> 00:22:38.320
initialize it at zero,
okay?

384
00:22:38.321 --> 00:22:41.260
So our total error at the store is just gonna be zero.
There's,

385
00:22:41.400 --> 00:22:46.180
there's not anything that's,
that's um,
uh,

386
00:22:46.270 --> 00:22:50.350
we don't have an area.
Okay.
So then for every point,

387
00:22:51.160 --> 00:22:52.030
every point,
so for,

388
00:22:52.031 --> 00:22:56.620
I've been range of starting at zero and then going for the length of the points,

389
00:22:56.621 --> 00:22:59.770
right?
To all of our data points.
So for every data point that we have,

390
00:23:00.430 --> 00:23:04.540
we're going to say,
let's get the x value.
All right,
so let's pick that x value.

391
00:23:04.541 --> 00:23:09.541
So x equals points I zero and then we're going to get that.

392
00:23:09.691 --> 00:23:13.500
Why di you?
Right?
So then the Wagga,
right?

393
00:23:13.510 --> 00:23:16.660
So I'm just basically programmatically showing what I

394
00:23:19.090 --> 00:23:22.760
just talked about.
Mathematical,
right?
So we,
we've,
we've got the x value,

395
00:23:22.770 --> 00:23:26.890
we've got the Y,
y values,
and we went to,
uh,
compute that distance,
right?

396
00:23:26.980 --> 00:23:30.220
We're going to do this every single time.
We're,
okay.
So,

397
00:23:33.030 --> 00:23:34.710
so then the difference

398
00:23:36.960 --> 00:23:40.200
squareds and then and add it,

399
00:23:41.070 --> 00:23:45.680
add it to the,
to the total.
Okay.

400
00:23:45.710 --> 00:23:50.540
So,
so here's that.
Here's the actual equation,
right?
So we're going to,

401
00:23:50.690 --> 00:23:53.690
we're going to be plus people because it's a summation and we're going to

402
00:23:53.691 --> 00:23:56.690
programmatically show what I just talked about right here,
right?

403
00:23:56.810 --> 00:24:00.740
Y minus mx plus B Square,
okay?

404
00:24:00.830 --> 00:24:03.920
And we're gonna get to some of that.
So why minus

405
00:24:05.570 --> 00:24:08.840
m times x Plus B?

406
00:24:10.300 --> 00:24:14.060
Clear.
Okay.
And we're going to do that for every point.

407
00:24:14.061 --> 00:24:18.260
So this whole interracial group right here,
is that equation,
okay?

408
00:24:18.350 --> 00:24:21.300
Minus the average part.
So that's gonna to give us the total value.

409
00:24:21.590 --> 00:24:26.510
The last part was do averages.
So we'll say total error divided by floats.

410
00:24:26.850 --> 00:24:28.650
The length of the point,
cause we wanted to be a flood

411
00:24:31.530 --> 00:24:35.360
backs is the equation.
That is the equation right there.
Okay?
So,

412
00:24:35.380 --> 00:24:38.230
and then if the average,
the average,

413
00:24:41.380 --> 00:24:44.530
so that,
so this a 10 line function,

414
00:24:44.740 --> 00:24:49.120
just describe what I talked about right here and his math equation.
Okay.
We got,

415
00:24:49.121 --> 00:24:53.500
we saw all the distances between all those points.
As I showed rights here,

416
00:24:53.530 --> 00:24:57.070
we sum them all up,
we squared them,
and then we,
uh,

417
00:24:57.790 --> 00:25:01.700
we got the average and that is our error.
Okay?
And so that it's so,

418
00:25:01.850 --> 00:25:04.510
and we're calculating that because we,

419
00:25:04.750 --> 00:25:09.620
we want a way for us,
a measure of us,
something to minimize over time,

420
00:25:09.621 --> 00:25:12.130
right?
We,
something to minimize.
Every time we redraw our lines,

421
00:25:12.160 --> 00:25:15.730
we want to minimize this error because this era basically is a signal.

422
00:25:15.731 --> 00:25:19.550
It's a compass for us.
It's,
it's telling us this is how bad your line is.

423
00:25:19.720 --> 00:25:22.990
You need to get better.
You need to make me smaller.
I'm really big right now.

424
00:25:22.991 --> 00:25:24.830
Make me smaller.
And that's what Grady,

425
00:25:24.860 --> 00:25:28.240
the sense does that what gradient descent does?
And I'm going to talk to you,

426
00:25:28.300 --> 00:25:31.060
I'm going,
I'm gonna explain how rated descent works in seconds.
But that's,

427
00:25:31.061 --> 00:25:33.100
that's that first option,
right?
So what,
okay,

428
00:25:33.130 --> 00:25:36.850
so what was the second doctor we wrote?
It was called gradient descent runner.

429
00:25:37.240 --> 00:25:41.080
So this is our actual,
uh,
our grading dissent function.
Okay.

430
00:25:41.081 --> 00:25:45.860
So let's now let's write this out.
Okay.
Which is our second of three methods,
um,

431
00:25:46.660 --> 00:25:50.090
before we're done.
Okay.
So gradients percent.

432
00:25:51.570 --> 00:25:52.960
So given a set of points,

433
00:25:53.140 --> 00:25:58.140
do you have a starting value for be given a starting value for and

434
00:26:00.880 --> 00:26:04.080
even our learning rates and given our number of iteration,

435
00:26:04.081 --> 00:26:07.280
if we're going to use all of these things to calculate radiant descent,

436
00:26:07.710 --> 00:26:09.760
we're going to use every single frame.
Okay?

437
00:26:13.060 --> 00:26:17.050
Okay.
So let's get back to starting a,
B and m valid.
Okay.

438
00:26:17.080 --> 00:26:20.380
So the starting value for me,
we're going to say it to be,

439
00:26:20.770 --> 00:26:24.700
and the starting value for,
and we're going to say it too.
Okay.
Simple enough.

440
00:26:25.620 --> 00:26:30.090
And now we're going to perform graded descent.
What is great in this at high 10?

441
00:26:30.100 --> 00:26:33.520
Not wait to explain.
Great to send guys,
I found the perfect analogy,

442
00:26:33.580 --> 00:26:38.110
great insight and I'm really excited.
Okay.
So,
um,
so,
uh,

443
00:26:38.560 --> 00:26:41.640
okay.
So before we,
before I explained that let's just,

444
00:26:41.980 --> 00:26:46.710
I took perform you ratio because you actual math is third and last function to

445
00:26:46.711 --> 00:26:50.160
them about the rights.
So for every single iteration that we are fine,

446
00:26:50.390 --> 00:26:52.470
we're going to perform what's called gradient descent.

447
00:26:52.800 --> 00:26:55.950
So we're going to upbeat P and m with the new,

448
00:26:56.100 --> 00:26:59.970
more accurate e and m by for for me,
uh,

449
00:27:00.080 --> 00:27:04.500
the gradient is right before me this gradient step.
Okay?

450
00:27:06.120 --> 00:27:07.860
So we and M,

451
00:27:08.960 --> 00:27:11.140
we're going to return and by performing this race,

452
00:27:11.170 --> 00:27:13.260
this race that we're not going to explain,

453
00:27:13.261 --> 00:27:18.261
this is the way the map is happening given our current VR current men are the

454
00:27:18.421 --> 00:27:21.200
array of points that we have.
And then finally given to learning group,

455
00:27:22.410 --> 00:27:25.620
we're going to talk like that final value of PNA.
And guess what?

456
00:27:26.100 --> 00:27:27.930
Once this grading dissent is done,

457
00:27:27.960 --> 00:27:31.020
we're going to return that optimal he and head,
right?

458
00:27:31.370 --> 00:27:33.960
And so that's what we talked about at this bottom part,
right?

459
00:27:34.030 --> 00:27:37.190
It returned that optimal BNF value performing right in a sense.

460
00:27:37.650 --> 00:27:41.820
And we then printed it out because that optimal be an end value gave us a line

461
00:27:41.821 --> 00:27:46.350
of best fit.
We plugged them into the y equals mx plus B equals a formula.

462
00:27:46.440 --> 00:27:50.820
It gave us a line of best fit.
So now we're going to write out the gradient step.

463
00:27:50.880 --> 00:27:55.830
Okay.
And this is gradient mother epic descent.
Okay.

464
00:27:55.831 --> 00:27:58.660
So this is how it's going to go dad.
Okay.
All right.

465
00:27:58.860 --> 00:28:01.740
Here's how it's going to go down.
Separating.

466
00:28:02.040 --> 00:28:05.910
So I'm just going to say it's not for the magic,
the magic,
the greatest,

467
00:28:06.990 --> 00:28:11.370
greatest.
Okay.
So that's how excited am I just wrote the greatest points.
Okay.
So,

468
00:28:12.630 --> 00:28:12.901
okay.

469
00:28:12.901 --> 00:28:17.901
So given our DNM values points and the learning rapes,

470
00:28:19.350 --> 00:28:22.030
this actually isn't going to help with them.
So I'll delete that.

471
00:28:22.230 --> 00:28:26.780
So given our learning rates,
okay,
let's perform ray to say,

472
00:28:27.180 --> 00:28:32.100
okay,
what is gradient descent?
Okay,
so let me show you guys,

473
00:28:32.101 --> 00:28:36.560
this is so
okay.
How Big Way?
Um,

474
00:28:37.110 --> 00:28:38.970
describe it.
So we have,

475
00:28:40.940 --> 00:28:44.810
let me just show you this image.
This is going to help a lot.

476
00:28:47.730 --> 00:28:52.440
Okay.
So this is a brass.
Okay.
So let's just look at the graph on the,

477
00:28:52.570 --> 00:28:56.790
um,
I mean it's,
if the same wrap,
it's looking at it from two different angles.

478
00:28:56.820 --> 00:29:00.450
It's the same graph.
Okay.
So let's look at the,
let's look at the one on the left.

479
00:29:00.451 --> 00:29:02.730
Just A,
just to pick one.
It's the same graph though.

480
00:29:03.540 --> 00:29:05.910
We have a bunch of y values and I'm sorry,

481
00:29:05.940 --> 00:29:08.650
a bunch of B values and a bunch of em dots.

482
00:29:08.850 --> 00:29:12.330
And then we have that error right and error.
Then I just talked about,
right.

483
00:29:12.360 --> 00:29:16.050
So given the two d graph would be given,
are any,

484
00:29:16.110 --> 00:29:19.950
every single wide insect we could have given every single m value we could have.

485
00:29:20.010 --> 00:29:24.570
What is the error?
Okay,
so for every y intercept and slope pair,
what is the air?

486
00:29:24.720 --> 00:29:27.420
And so we'll,
we'll find this is a three dimensional graph.

487
00:29:27.930 --> 00:29:32.490
This is a three dimensional graph because of the error value.
Yeah,
it's,
it's,

488
00:29:32.491 --> 00:29:33.570
it's,
it's kind of like,

489
00:29:33.600 --> 00:29:38.550
it starts off high and then I do approach what's called the local minima in our

490
00:29:38.551 --> 00:29:42.040
case,
a local minimum,
which is these small dec dot point at the very bottom.

491
00:29:42.430 --> 00:29:45.760
That is our,
that if we're trying to get to,
okay.
So,

492
00:29:46.510 --> 00:29:46.960
<v 3>okay.</v>

493
00:29:46.960 --> 00:29:51.130
<v 1>Given a set of wine slacks and given us,
given a set of sloth possible,</v>

494
00:29:51.131 --> 00:29:52.420
why does that's impossible.
Slopes.

495
00:29:52.600 --> 00:29:56.410
We went to compute the error for for those two things and if we were to graph

496
00:29:56.411 --> 00:29:59.680
that,
who are to grab the relationship between these three things,

497
00:29:59.860 --> 00:30:00.820
it would look like this.

498
00:30:01.000 --> 00:30:05.500
Now it tends to always look very similar to this in more complex cases.

499
00:30:05.501 --> 00:30:08.260
We have like many minimum,
we have many little valleys,

500
00:30:08.500 --> 00:30:12.670
but what we're trying to do is at that point where the error is smallest and so

501
00:30:12.671 --> 00:30:15.070
how do we get that point where the Arab,
the smallest,
well,

502
00:30:15.071 --> 00:30:19.090
we were going to perform what's called gradient descent to get that smallest

503
00:30:19.091 --> 00:30:22.780
point,
that valley,
that smallest point and a great analogy for this,

504
00:30:22.960 --> 00:30:27.030
a great analogy for this is a bowl surgical.

505
00:30:27.820 --> 00:30:31.450
Okay?
Okay.
In fact,
it's kind of like a bowl.

506
00:30:31.480 --> 00:30:36.480
It's like we drop a ball into a bowl and we want to find that coins were the,

507
00:30:37.090 --> 00:30:41.320
there were the ball stops.
That end point,
the the the,
the lowest point.

508
00:30:41.560 --> 00:30:46.560
That's that DNM value is our optimal line of best fit.

509
00:30:47.530 --> 00:30:51.820
Okay.
And the way we're going to get that is gradient descent and we're going to

510
00:30:51.821 --> 00:30:52.780
descend,
right?

511
00:30:52.781 --> 00:30:56.950
We're descending down the bolts using the gradient and gradients is another word

512
00:30:56.951 --> 00:31:01.240
for slope.
We're going to descend down that goal until we get through iteration.

513
00:31:01.330 --> 00:31:02.740
That's that lowest point.

514
00:31:02.980 --> 00:31:06.250
And gradient descent is used everywhere in machine learning.
Okay.

515
00:31:06.251 --> 00:31:09.370
It is like the optimization method for deep neural networks.

516
00:31:09.520 --> 00:31:11.680
It's not that apparent right now,
but no,

517
00:31:11.681 --> 00:31:16.120
this no and understand grading dissent like the back of your hand because it is

518
00:31:16.121 --> 00:31:18.880
going to be very useful in the future.
Okay.

519
00:31:19.030 --> 00:31:21.970
So I don't know why I'm drinking out of the equation.
That was,

520
00:31:22.110 --> 00:31:23.140
that wasn't necessary.
That was a,

521
00:31:23.150 --> 00:31:26.620
that was the equation for the sum of squared errors that we just talked about.

522
00:31:26.650 --> 00:31:30.980
Some square distances.
Okay.
So so,
so how are we going to calculate that gradient?

523
00:31:31.000 --> 00:31:34.180
The descent?
Well now let's,
let's actually do it.
Okay.
So

524
00:31:36.330 --> 00:31:39.360
for our separate in function,
we'll start off with um,

525
00:31:40.010 --> 00:31:41.700
an initial grading value for our Pete.

526
00:31:42.000 --> 00:31:47.000
So these going to be zero and so then m's gradient if going to be zero as well.

527
00:31:48.200 --> 00:31:51.720
Okay.
You said the starting point for our gradients and Brady means slow.

528
00:31:51.960 --> 00:31:54.830
And so the gradient,
the gradient is going to act like a compass.
It's,

529
00:31:54.930 --> 00:31:58.200
and it's going to always point downhill.
So this is what I mean by,

530
00:31:58.260 --> 00:32:01.710
once we tap into that error,
it's going to act as a compass for us.

531
00:32:01.711 --> 00:32:04.650
It's going to tell us where we should be going,

532
00:32:04.680 --> 00:32:09.510
what direction we should be going,
how we should best redraw our lines.
So for,

533
00:32:11.590 --> 00:32:14.290
okay,
what someone has,
why was the lowest point the best?

534
00:32:14.500 --> 00:32:19.230
The lowest point is the best because it is worth it.
Our error is the smallest.

535
00:32:19.350 --> 00:32:21.910
And when our error is the smallest,
that's,

536
00:32:22.130 --> 00:32:24.060
that's when we'd have the line of best fit.

537
00:32:24.120 --> 00:32:27.780
When the air is the smallest that be an m value,
those two,

538
00:32:27.810 --> 00:32:31.560
what we plug into our slope equation is going to give us the line of best fit.

539
00:32:31.800 --> 00:32:35.250
So that's why we're calculating the error.
Okay?
So,

540
00:32:36.960 --> 00:32:40.160
so for I range zero to the length of points.

541
00:32:42.290 --> 00:32:43.490
Okay.
So we're going to get,

542
00:32:43.580 --> 00:32:47.330
so what we're gonna do is we're going to iterate through every single point on

543
00:32:47.331 --> 00:32:50.330
our scatter plot.
Okay?
So every single data point that we have,

544
00:32:50.390 --> 00:32:52.950
we're going to collect it.
Okay?
So we're going to say,
okay,
what is,

545
00:32:53.030 --> 00:32:56.350
so for the first point,
right first point,

546
00:32:56.360 --> 00:32:58.370
which gives us x and Y.
Dot.

547
00:32:59.860 --> 00:33:01.780
X value and a wider,

548
00:33:03.400 --> 00:33:07.800
so let me also write up a little comment to that starting point once for arc,

549
00:33:08.690 --> 00:33:09.523
okay?

550
00:33:11.890 --> 00:33:16.890
Now we're going to get the direction with respect to the n no,

551
00:33:17.380 --> 00:33:21.910
this is uh,
uh,
the last part,

552
00:33:21.911 --> 00:33:24.920
but it's a very,
very important for part,

553
00:33:25.210 --> 00:33:29.200
and this is where calculus comes into play.
Okay?

554
00:33:29.260 --> 00:33:33.070
So I'm going to talk about how we're doing things.
Okay?

555
00:33:33.100 --> 00:33:36.550
So let me talk about what,
what we're,
what we're about to do.

556
00:33:36.700 --> 00:33:40.600
So what we're going to do is so timid.
So for every single point,

557
00:33:40.930 --> 00:33:42.490
for every single point that we have,

558
00:33:42.670 --> 00:33:46.600
we're going to calculate what's called deep partial derivatives.
Okay?

559
00:33:46.601 --> 00:33:50.770
It's called the partial derivative with respect to be,

560
00:33:50.860 --> 00:33:53.230
and with respect to m,
okay?

561
00:33:53.290 --> 00:33:56.890
And what that's going to do is it's going to give us a direction to go for both

562
00:33:56.920 --> 00:34:01.180
the B.
Dot.
M.
Dot.
Right?
So remember,
I remember in this rack,

563
00:34:01.390 --> 00:34:05.090
we want a direction,
right?
We want to,
we want it to be going down to grading.

564
00:34:05.140 --> 00:34:09.260
And so,
right,
and so and so,
right?

565
00:34:09.261 --> 00:34:12.160
So on this left hand side,
you see this radiant search.

566
00:34:12.940 --> 00:34:16.520
You see the value in the feed values are,
are,
uh,

567
00:34:16.780 --> 00:34:19.780
increasingly in the direction that they should be because Grady at the sec is

568
00:34:19.781 --> 00:34:24.390
essentially a search policy.
Awesome.
We're trying to find that minimum,
uh,
Eric.

569
00:34:24.410 --> 00:34:28.240
Dot.
Okay.
Um,
and what we're going to do to get that is we're going to,

570
00:34:28.420 --> 00:34:32.410
we're going to compute the partial derivative with respect to be in that.
Oh,

571
00:34:32.470 --> 00:34:35.170
and let me show you the equation for the perfect.
Terrific.
Okay.

572
00:34:35.770 --> 00:34:40.360
The partial derivative is going to be right here,

573
00:34:43.580 --> 00:34:48.170
renter.
So this is what the partial derivative does.
The partial derivative,
um,

574
00:34:48.240 --> 00:34:52.320
is so,
um,
little,
um,
uh,

575
00:34:52.340 --> 00:34:55.890
so we call it partial.
Uh,
we call it partial because I,

576
00:34:55.920 --> 00:34:57.770
it's not calling us the whole story,
right?

577
00:34:58.170 --> 00:35:02.360
We say it's partially because we're calculating will calculate for board both B

578
00:35:02.480 --> 00:35:04.400
and.
M.
There are two different things.

579
00:35:04.610 --> 00:35:07.490
And so it's going to give us the tangent line.

580
00:35:07.491 --> 00:35:10.240
So it's going to give us this line as you see right here,
right?

581
00:35:10.360 --> 00:35:14.720
So you just line that line is our direction or we're going to use it to update

582
00:35:14.721 --> 00:35:18.860
our DNM value,
okay?
Okay.

583
00:35:20.090 --> 00:35:22.800
So that's what that is.
And uh,

584
00:35:22.970 --> 00:35:26.750
let me also show you the equation for the partial derivative because we're about

585
00:35:26.751 --> 00:35:27.584
to write it out.

586
00:35:27.680 --> 00:35:32.570
So here's what the equation for the partial derivative with respect to m and d

587
00:35:32.571 --> 00:35:36.950
looks like.
Okay?
There are two different equations,
right?

588
00:35:36.950 --> 00:35:38.220
So let's talk about the one on top.

589
00:35:38.670 --> 00:35:41.130
So this little curvy thing that you see up here,

590
00:35:41.600 --> 00:35:45.300
that that just signifies that this is a partial Gribbon,
but that's,

591
00:35:45.301 --> 00:35:48.120
that's the map signifier that this is a partial driven.
Now,

592
00:35:48.121 --> 00:35:51.750
we talked about sigma notation,
right?
Because it's a summation of values,
right?

593
00:35:51.870 --> 00:35:54.290
And that's what we're doing.
We're Sony,

594
00:35:54.380 --> 00:35:58.550
the partial derivatives for all of our points,
okay?
Uh,

595
00:35:59.010 --> 00:36:02.280
for all of them to compute that radiant value,
okay?
And,

596
00:36:03.590 --> 00:36:08.190
uh,
the partial derivative with respect to m and D is going to look like this.

597
00:36:08.220 --> 00:36:10.990
So let's,
let's write this out.
Okay.
Um,

598
00:36:11.760 --> 00:36:15.050
so the be great and it's going to come with a tube to be great.

599
00:36:15.070 --> 00:36:17.850
It's going to be fussed people's,
uh,
and then what was it?

600
00:36:17.930 --> 00:36:21.540
Let me look at the place again.
Two N,
two over n.

601
00:36:21.560 --> 00:36:24.510
So negative two over again.
All right.

602
00:36:26.600 --> 00:36:27.433
<v 0>And</v>

603
00:36:30.090 --> 00:36:34.350
<v 1>thanks.
Good vibes.
And then we're going to,
and then what was it?</v>

604
00:36:34.351 --> 00:36:38.760
It was y minus,
right?
And you started equations.
These are laws.

605
00:36:38.790 --> 00:36:42.870
There are beautiful laws that always stayed the same.
And then they,
they,
they,

606
00:36:42.950 --> 00:36:45.710
they do,
they give us humans,
uh,
uh,

607
00:36:45.870 --> 00:36:48.120
a way of understanding

608
00:36:50.180 --> 00:36:54.720
the direction that we want to move it.
Okay?
So we current situation.
Okay.

609
00:36:54.721 --> 00:36:56.700
So,
all right,
so then

610
00:37:00.040 --> 00:37:01.720
we'll do the same thing.
And what was the second equation?

611
00:37:01.750 --> 00:37:05.770
It looks pretty much the same.
Minus this.
It didn't,
it doesn't have this x,
right?

612
00:37:05.771 --> 00:37:08.670
And the second one doesn't have this x,
right?
So we'll say,

613
00:37:08.710 --> 00:37:09.820
but it does have this to end.

614
00:37:11.380 --> 00:37:12.213
<v 0>Okay.</v>

615
00:37:12.920 --> 00:37:16.850
<v 1>It does have this to end and then it does have,
mmm.</v>

616
00:37:19.810 --> 00:37:23.150
Uh,
let's see.
Let's have this x.
It does have,

617
00:37:27.930 --> 00:37:30.360
why minus m

618
00:37:32.130 --> 00:37:33.150
times x

619
00:37:35.860 --> 00:37:39.460
Plus E.

620
00:37:41.400 --> 00:37:46.200
Okay.
Okay.
So let me one more time.

621
00:37:46.201 --> 00:37:47.034
So you guys,

622
00:37:47.220 --> 00:37:51.210
it's giving us directions for it to go for both d and m.

623
00:37:52.500 --> 00:37:54.460
And remember the partial there.

624
00:37:54.560 --> 00:37:57.390
It's not telling us the whole story and telling us what direction should we go

625
00:37:57.391 --> 00:37:59.420
for beef and what direction should we go for x?

626
00:37:59.610 --> 00:38:01.080
And it's going to tell the direction.

627
00:38:01.081 --> 00:38:05.250
Remember for a bowl to get to that bottom point where that error is the smallest

628
00:38:05.310 --> 00:38:09.510
right here.
Okay.
So right here where I'm,
where my,
where my mouse is that point,

629
00:38:09.540 --> 00:38:11.040
that point is what we want to get to.

630
00:38:11.280 --> 00:38:14.610
And that's what the parcel driven is going to help us.
So once we,

631
00:38:14.670 --> 00:38:17.220
once we've computed the partial derivatives,
uh,

632
00:38:17.410 --> 00:38:21.340
there's some of them with respect to BNS.
Now we're going to,
um,

633
00:38:23.350 --> 00:38:25.540
now we're going to update our BNN values,
right?

634
00:38:25.541 --> 00:38:28.030
So we're gonna use that to update our P and N.
Dot.
And guess what?

635
00:38:28.031 --> 00:38:32.130
This is our last set,
which just our last step,
using this partial driven,

636
00:38:37.960 --> 00:38:41.050
our partners,
it is right plural.
There's two of them.

637
00:38:41.680 --> 00:38:46.600
And that's gonna give us any MRI.
So we have our current value for me,

638
00:38:46.601 --> 00:38:51.180
whatever it is,
and we
keep stopping every time.

639
00:38:51.520 --> 00:38:54.400
This is where our learning rate comes into play.
Okay.

640
00:38:54.550 --> 00:38:56.710
This is why our learning rate is so important.

641
00:38:56.830 --> 00:39:00.510
Important because it defines the rate at which we're,
uh,

642
00:39:00.610 --> 00:39:05.200
we're updating our brightest 0.01,
right.

643
00:39:06.970 --> 00:39:10.300
And then,
uh,
also our end,
current,

644
00:39:11.880 --> 00:39:12.713
<v 0>yeah.</v>

645
00:39:12.840 --> 00:39:15.280
<v 1>Yes.
Learning rates,</v>

646
00:39:17.490 --> 00:39:22.260
times the m gradients
and then a bit,

647
00:39:22.261 --> 00:39:25.920
and then we'll return those values.
And we're doing this every time,
right?

648
00:39:25.921 --> 00:39:30.740
This is this new me,
a new amplifier final.
Yeah.
It's arts.
The septum timber.

649
00:39:30.850 --> 00:39:35.280
We're,
we're,
we're doing this every iteration,
right?
We're doing this,
uh,
for,

650
00:39:35.970 --> 00:39:37.770
for the number of iterations we had a thousand,

651
00:39:38.100 --> 00:39:41.880
but it's going to return a new bnm value every time.
And guess what guys?

652
00:39:42.280 --> 00:39:46.410
That's it for a coat.
That was it.
So,
so let's go over what we,
what we've done.

653
00:39:46.820 --> 00:39:51.540
Um,
okay.
But let me actually,
let me,
let me check for errors.
Right.

654
00:39:52.550 --> 00:39:53.000
<v 0>Okay.</v>

655
00:39:53.000 --> 00:39:55.370
<v 1>Let me,
let me check for errors and then I'm not answering more questions cause I</v>

656
00:39:55.371 --> 00:39:58.520
wouldn't really want to make sure you guys understand how this works.
Okay.

657
00:39:58.610 --> 00:40:01.610
So let me,
let me,
let me,
I'm done with it.

658
00:40:01.620 --> 00:40:06.500
So I thought it up by what we'll need and is not defined.
Okay.

659
00:40:06.650 --> 00:40:07.250
Oh right.

660
00:40:07.250 --> 00:40:12.250
Just what I did define n and is the number of points of points.

661
00:40:16.280 --> 00:40:21.040
Okay.
So let's go.
Learning rape is not defined where,

662
00:40:21.070 --> 00:40:25.370
where he's learning were not defined.
Learning rate is not defined.

663
00:40:26.450 --> 00:40:31.010
Uh Oh
wait a sec.
Yeah.
Oh yeah.

664
00:40:31.011 --> 00:40:35.390
Right,
right.

665
00:40:36.410 --> 00:40:41.150
Okay.
What else?
What else did that
overflow for?

666
00:40:41.151 --> 00:40:42.230
Double scalers

667
00:40:45.390 --> 00:40:49.410
one 14
air

668
00:40:51.300 --> 00:40:52.650
y minus.

669
00:40:53.890 --> 00:40:54.580
<v 0>Okay.</v>

670
00:40:54.580 --> 00:40:55.413
<v 1>Uh Huh.</v>

671
00:41:00.070 --> 00:41:02.730
Go learning.
Right.
But anyway,
I told her,
okay,

672
00:41:02.820 --> 00:41:07.620
so what's going on here?
Okay,
let's say this.
So yeah,

673
00:41:07.621 --> 00:41:11.820
I mean print out the final guy.
It got our final value.

674
00:41:11.930 --> 00:41:13.470
I'm right here.
Um,

675
00:41:14.430 --> 00:41:18.000
and if we wanted to,
let's see,
hold on a second.

676
00:41:18.390 --> 00:41:21.720
If we want it to,
again,

677
00:41:21.780 --> 00:41:25.980
our backup here just in case.
So,
right.
So let me,
let me,

678
00:41:26.010 --> 00:41:30.980
let me blow this up like way,
way,
way up.
And let me,
let me just separate it.

679
00:41:30.990 --> 00:41:33.530
So this is what our output is going to look like,
right?

680
00:41:33.540 --> 00:41:38.000
So [inaudible] seconds.
Why?
Because our data set was so small,

681
00:41:38.390 --> 00:41:43.280
it okay if David said was so small,
all right.

682
00:41:43.281 --> 00:41:46.410
So
that could happen.

683
00:41:47.040 --> 00:41:50.910
And after a thousand iterations,
we've got the optimal B and m.
Dot.
So right.

684
00:41:50.911 --> 00:41:54.480
And we start off with Dnm is zero and we'd have to lay the error for a random

685
00:41:54.481 --> 00:41:56.640
line that we drew and it wasn't huge.

686
00:41:56.670 --> 00:41:58.640
But eventually after running rating descents,

687
00:41:58.880 --> 00:42:02.970
we've got the optimal optimal m and the optimal and the lowest error point.

688
00:42:02.971 --> 00:42:05.700
We said that that smallest point in the bowl.
And to do that,

689
00:42:05.701 --> 00:42:10.430
we use grading dissent,
uh,
with respect to be an m.
Okay.
So let,

690
00:42:10.450 --> 00:42:11.910
let me go over one last time.

691
00:42:12.000 --> 00:42:15.090
Every single thing that we've just done just to really go over it and then I'll

692
00:42:15.091 --> 00:42:16.590
do my last five minutes to an hour.
Okay.

693
00:42:16.890 --> 00:42:19.260
So we started out by collecting our Dataset,
right?

694
00:42:19.340 --> 00:42:24.210
Our data set was a collection of a t test scores and the amount of power

695
00:42:24.211 --> 00:42:28.090
studies,
right?
The x y value pairs of test scores and the amount of power,
uh,

696
00:42:28.190 --> 00:42:29.370
to variable dataset.

697
00:42:29.670 --> 00:42:33.630
Then we define our hyper parameters for our linear regression or learning rate,

698
00:42:33.631 --> 00:42:35.340
which talks about how fast we should,

699
00:42:35.400 --> 00:42:40.230
we should learn our initial bnm values for the slope equation y equals mx plus
B,

700
00:42:40.320 --> 00:42:43.200
the number of iterations a thousand because our Dataset is pretty small.

701
00:42:43.410 --> 00:42:47.430
And then we ran Brady the sense,
so what the graded descent look like.
Well,

702
00:42:48.230 --> 00:42:51.570
for,
for every iteration,
for a thousand iterations,

703
00:42:51.840 --> 00:42:56.460
we computed the gradients with respect to both d and m and we did that

704
00:42:57.060 --> 00:43:01.770
constantly until we got debt optimal BN handout that gives us that line of best

705
00:43:01.771 --> 00:43:06.020
fit.
Now how do we compute the gradients to do that?
We said,
okay,
we'll,

706
00:43:06.021 --> 00:43:08.520
we'll have a starting point of zero for both of those gradients.

707
00:43:08.760 --> 00:43:12.000
Remember gradient is just another word for slope.
And then,

708
00:43:14.240 --> 00:43:18.040
uh,
we said,
okay.
So for every single point in our scatter plot,

709
00:43:18.240 --> 00:43:21.600
for every single point on a scatter plot for our data,
um,

710
00:43:22.320 --> 00:43:26.180
will compute the partial derivative with respect to both B and m and that,

711
00:43:26.181 --> 00:43:28.680
that those two values are going to give us a direction.

712
00:43:28.770 --> 00:43:31.020
I sent them a direction where we want to go.

713
00:43:31.110 --> 00:43:33.390
How do we get to that lowest point in that bowl,
right?

714
00:43:33.391 --> 00:43:35.700
That three dimensional graph,
that lowest point.

715
00:43:35.940 --> 00:43:40.470
And we use the learning rates to determine how fast,
uh,

716
00:43:40.500 --> 00:43:42.100
we want to update our DNM diet.

717
00:43:42.610 --> 00:43:45.810
We got the difference between the current value and what we had before and we

718
00:43:45.811 --> 00:43:50.220
returned that.
So for every point,
and we did that for a thousand iterations.
Okay.

719
00:43:50.221 --> 00:43:53.820
And that's what it gave us the output,
and it looks like visually,

720
00:43:53.850 --> 00:43:58.470
visually it looks like this,
right?
But the desserts done right?
It's like up,
up,

721
00:43:58.471 --> 00:44:01.890
up,
up,
up,
up,
up,
up,
up,
up,
up.
It's kind of like wheel of fortune,
right?

722
00:44:01.920 --> 00:44:05.520
It starts out fast and it gets slower and slower as it approaches convergence.

723
00:44:05.580 --> 00:44:09.270
The word we use when we have optimal line of best fit convergence.
See,

724
00:44:09.300 --> 00:44:13.590
let me do it one more time just like that.
Okay.

725
00:44:13.800 --> 00:44:16.270
So that was,
that was that.

726
00:44:18.080 --> 00:44:22.970
And now I'm going to screen share and do a last five minute Q and.

727
00:44:22.971 --> 00:44:27.560
A.
All right.
Stop Spring Chair.
Hi everybody.
Okay,

728
00:44:27.890 --> 00:44:28.310
let me,

729
00:44:28.310 --> 00:44:32.160
let me bring you guys back on screen and do my last five and a two and a ask me

730
00:44:32.161 --> 00:44:34.590
anything.
Uh,
and uh,
yeah.

731
00:44:39.600 --> 00:44:44.280
How's it going everybody?
Any questions?

732
00:44:44.281 --> 00:44:45.690
I'm open to questions.

733
00:44:49.040 --> 00:44:52.420
Where did I use [inaudible]?
It's at the very top.
It's right.

734
00:44:52.430 --> 00:44:55.190
What's the practical use of linear regression?
Great question.

735
00:44:55.370 --> 00:44:58.930
Anytime we want to find the relationship between two,
um,

736
00:44:59.440 --> 00:45:03.330
two different variables and then,
you know,

737
00:45:03.331 --> 00:45:05.880
in more complex cases there could be more.
But,
uh,

738
00:45:05.990 --> 00:45:07.700
we want to prove mathematically,
right?

739
00:45:07.730 --> 00:45:12.560
Math is all about proving things in a way that is unfair certifiable that no one

740
00:45:12.561 --> 00:45:16.550
can say,
hey,
that's not true.
Well,
I can prove it mathematically.

741
00:45:16.730 --> 00:45:21.200
So it's a way to show the,
uh,
the,
the relationship between two value parents,

742
00:45:21.201 --> 00:45:25.370
right?
So maybe housing prices and uh,
the,
the time of year,
right?

743
00:45:25.371 --> 00:45:29.330
So what is the real estate market going to look like?
Or,
uh,
you know,

744
00:45:29.331 --> 00:45:32.060
anytime that they're intuitively,
you'd think there was a relationship,

745
00:45:32.120 --> 00:45:34.170
you can prove it with linear regression and,

746
00:45:34.250 --> 00:45:36.950
but really I did this to show gradient descent,

747
00:45:37.220 --> 00:45:41.120
that optimization processes that is very popular in deep learning and we're

748
00:45:41.121 --> 00:45:44.480
going to use that in our deep neural networks and in the end,

749
00:45:44.481 --> 00:45:49.340
the rest of the course.
Okay.
Um,
why aren't you biased towards Google?
I mean,

750
00:45:50.060 --> 00:45:53.840
it's not really,
uh,
yeah,
I mean,
tensorflow is awesome.
It's,
it's not like,

751
00:45:54.890 --> 00:45:57.110
it's not like Google's like Sarraj I want you to talk about it.

752
00:45:57.111 --> 00:46:01.520
I just tend to flow is the best,
uh,
deep learning library out there right now.

753
00:46:02.120 --> 00:46:02.870
That's what,

754
00:46:02.870 --> 00:46:06.000
and of course it wouldn't be because Google knows what they're doing.
They,

755
00:46:06.140 --> 00:46:09.170
they handle billions and billions of queries every day.

756
00:46:09.350 --> 00:46:13.280
They have to be able to do machine learning at scale,
the problems they solve,

757
00:46:13.281 --> 00:46:15.690
problems that no one else has even thought of.
Salts.

758
00:46:15.920 --> 00:46:19.820
And all of those solutions are found in tensorflow for machine learning.

759
00:46:20.030 --> 00:46:24.790
Please make an AI doctor,
you can create a classifier to classify between,
um,

760
00:46:25.160 --> 00:46:28.880
different types of disorders that you see in an x ray.

761
00:46:28.960 --> 00:46:32.450
I that's going to augment doctors at first but eventually replace them.
Uh,

762
00:46:33.110 --> 00:46:36.770
how about fitting a quadratic curve instead of a linear line?
Uh,

763
00:46:36.920 --> 00:46:37.880
we could do that as well.

764
00:46:41.670 --> 00:46:45.960
I'm going to provide a data set and the code,
uh,
I can talk slower.
Sure.

765
00:46:47.250 --> 00:46:50.720
How to find the optimum learning rate.
Uh,
that's a great question.
Um,

766
00:46:50.760 --> 00:46:53.430
there's several methods of doing that,
but that's great intuition.

767
00:46:53.431 --> 00:46:55.210
Sometimes we can,
we can,
uh,

768
00:46:55.320 --> 00:46:57.680
use machine learning to find the optimal hyper parameter.

769
00:46:57.681 --> 00:47:00.640
So it's kind of like machine learning for machine learning.
I all,

770
00:47:00.650 --> 00:47:01.860
we'll talk about that later.

771
00:47:03.990 --> 00:47:07.200
This is the first light section at the Udacity course.
Uh,

772
00:47:09.190 --> 00:47:12.790
teaches calculus.
I'll do more of that in the future.
I'm going to be to keep,

773
00:47:12.820 --> 00:47:16.090
I'm going to keep doing calculus.
Okay.
Um,

774
00:47:16.120 --> 00:47:18.780
two more questions then we're good to go to more.

775
00:47:19.920 --> 00:47:20.450
<v 0>Okay.</v>

776
00:47:20.450 --> 00:47:22.670
<v 1>How would you recommend me to start machine learning?</v>

777
00:47:22.790 --> 00:47:27.790
Watch this series and watch my learn python for data science areas.

778
00:47:28.100 --> 00:47:31.080
Watch my hits,
right.
Attention series.
Watch.
Um,

779
00:47:31.390 --> 00:47:35.750
my machine learning for hackers series.
Watch my videos.
Uh,

780
00:47:35.830 --> 00:47:38.890
why is your view Udacity too expensive?
I didn't decide to price guys.

781
00:47:38.920 --> 00:47:43.450
I tried to get hello with whatever you got paid.
You got paid graders for that.

782
00:47:43.451 --> 00:47:46.720
Okay.
In grading is not cheap human raters.
But look,

783
00:47:46.760 --> 00:47:50.410
all the videos are going to be released here on my channel,
so,
all right,
so

784
00:47:52.030 --> 00:47:56.800
I'm here for you guys.
Okay.
I'm trying to grow my current role myself.

785
00:47:56.801 --> 00:47:58.480
So Raj robble.
Okay.

786
00:48:03.310 --> 00:48:05.750
This was the end.
Okay.
So that's it for the questions.

787
00:48:08.830 --> 00:48:10.950
All right,
so for now I've got to

788
00:48:13.080 --> 00:48:17.910
shoot a fighting scene for my next video.
What?
Yeah,

789
00:48:17.970 --> 00:48:21.360
so thanks for watching.
I love you guys.

790
00:48:22.110 --> 00:48:26.300
I'll post a link in the comments right when I'm done.
All right.

791
00:48:27.080 --> 00:48:29.910
In the video description,
I posted a link and then the Dataset,

792
00:48:30.000 --> 00:48:33.030
everything is going to go into the district description within the hour.

793
00:48:33.180 --> 00:48:34.470
All right.
Bye.

794
00:48:36.550 --> 00:48:36.790
<v 0>Okay.</v>

