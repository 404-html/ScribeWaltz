WEBVTT

1
00:00:01.770 --> 00:00:06.120
Hello world.
It's the Raj.
Good to see everybody.
Let's do this.
We're going to,

2
00:00:06.150 --> 00:00:09.900
we're going to talk about recurrent nets today.
So we're all my homies at,

3
00:00:10.500 --> 00:00:12.450
we're all my homies.
That is my question.

4
00:00:12.560 --> 00:00:15.990
I have so many questions and not enough answers.
So Raj,

5
00:00:15.991 --> 00:00:18.090
how far are we from solving intelligence?

6
00:00:19.320 --> 00:00:22.320
Anywhere from five to 10 years is my best guess.
Let me mute this.

7
00:00:22.380 --> 00:00:26.970
I don't want to hear myself.
I mean I do,
but not do the audio.
Okay.
So we're,

8
00:00:27.000 --> 00:00:30.180
you know,
five to 10 years.
I would say.
I would,
I would hope,

9
00:00:30.570 --> 00:00:34.530
I mean the things are moving so fast.
Yo,
Yo Yo yo,
Yo,
yo Yo.
Okay,

10
00:00:34.620 --> 00:00:39.270
so welcome to Bollywood.
Okay.

11
00:00:39.271 --> 00:00:42.830
Hi Bonnie.
Rosio Henry,
she vom you on Aria.

12
00:00:43.110 --> 00:00:46.110
Tension flow versus by torch are,
yeah,
that's a great question.
I,

13
00:00:46.760 --> 00:00:49.560
I am starting to like Pi Torch and,
and in fact,
uh,
the machine,

14
00:00:49.561 --> 00:00:52.740
when the community is starting to say good things about Pi torch,

15
00:00:52.741 --> 00:00:54.090
that debate is happening right now,

16
00:00:54.091 --> 00:00:56.880
which is just crazy because tensorflow has so much momentum,

17
00:00:57.060 --> 00:01:00.750
but it's a good sign of showing just how fast things change in this field.

18
00:01:00.900 --> 00:01:05.790
Nothing is concrete.
Okay.
Uh,
how should I start?
Mistake.
So yeah,

19
00:01:05.791 --> 00:01:09.000
so five minute Q and a,
right?
So,
hi pocket man and uh,

20
00:01:09.030 --> 00:01:12.270
Vishwas how should I start machine learning?
Watch all of my videos.
Really.

21
00:01:12.271 --> 00:01:15.930
There's this guy who just commented on one of my videos are genetic algorithms

22
00:01:15.931 --> 00:01:20.130
video and he was like,
he was like you and I think his name was Fernando,

23
00:01:20.131 --> 00:01:22.500
but he was like,
you started me on the path for data science.

24
00:01:22.580 --> 00:01:25.950
And now after watching all your videos,
now I understand every line.

25
00:01:25.951 --> 00:01:28.650
And before I didn't know anything,
so really watch my videos,
all of them,

26
00:01:29.130 --> 00:01:33.570
I started to notice me.
Voke hi,
how's it going?
Mohamad hello from Russia.
Moscow.

27
00:01:33.720 --> 00:01:38.290
Uh,
uh,
is that over?
That is like Rooskie the Hobo,
this Levine's cubits.

28
00:01:38.790 --> 00:01:40.800
Ricardo,
sir.
Hamster.
Okay.

29
00:01:40.801 --> 00:01:43.050
So we've got a lot of people throw those questions at me.

30
00:01:43.210 --> 00:01:44.011
There are those questions.
I mean,

31
00:01:44.011 --> 00:01:47.950
because today we are talking about recurrent neural networks and we're going to

32
00:01:47.951 --> 00:01:49.200
float memory aspect.

33
00:01:49.320 --> 00:01:53.490
That's what we're focused on because it is really important to talk about the

34
00:01:53.491 --> 00:01:56.580
memory aspect.
Notice me Senpai I'm here.

35
00:01:57.690 --> 00:02:02.570
You're on a Tesla stock.
Hello?
From Paris,
Poland.
Man,
I've gone to Stein.

36
00:02:02.571 --> 00:02:06.570
Wow.
We have s Hungary.
Oh my God,
this is awesome.
Okay guys,

37
00:02:06.870 --> 00:02:09.240
let's talk question.
Can,

38
00:02:09.260 --> 00:02:13.080
can I start deep learning with no experience with machine learning?
Uh,
yes,

39
00:02:13.081 --> 00:02:15.750
absolutely.
But you do need it.
Some experience with math.

40
00:02:15.751 --> 00:02:19.140
So I would suggest introductory courses in statistics,

41
00:02:19.260 --> 00:02:20.730
calculus and linear Algebra.

42
00:02:20.880 --> 00:02:24.360
But what I'm trying to do is teach those things on the side as well.
So,
you know,

43
00:02:24.870 --> 00:02:29.670
just use those as reference material.
Finland,
Sheila,
Brazil,
Canada,
uh,

44
00:02:30.460 --> 00:02:35.160
Ben,
Ben,
Ben,
Benito.
Um,
Canada.
Maple Syrup.
How's it going?

45
00:02:35.370 --> 00:02:38.500
Justin Trudeau.
Okay.
I'm the only data's guys and I've got,
okay.

46
00:02:38.970 --> 00:02:41.710
Rns are better than life.
No,
but they're close.

47
00:02:41.850 --> 00:02:45.930
What's the difference between a CNN and RNN CNN?
Did you see it?
Okay.

48
00:02:47.120 --> 00:02:51.330
CNS apply standards or a whole different architecture?
You you have,

49
00:02:51.450 --> 00:02:55.460
you're pulling involved,
you've convolution,
convolution,
you have strides.
They're,

50
00:02:55.470 --> 00:02:55.740
they're,

51
00:02:55.740 --> 00:02:59.620
they're focused on images and actually recurrent that's can be used images but

52
00:02:59.621 --> 00:03:03.130
there but we use them most of the time for sequential data.
I really,

53
00:03:03.131 --> 00:03:07.900
everything is sequences in life.
All,
all data is sequential.
Columbia.

54
00:03:08.110 --> 00:03:10.690
Okay.
What's Pi?
Torch by torch is a new library.

55
00:03:10.870 --> 00:03:14.320
It's similar to tensorflow but different and I'm going to definitely,

56
00:03:14.321 --> 00:03:17.320
definitely make a pie torch in five minutes video soon and I know it's going to

57
00:03:17.321 --> 00:03:20.470
get a lot of views but not yet because we are focused on this torch.
Don't worry.

58
00:03:20.471 --> 00:03:22.990
We're going to talk about Pi Torch later and I'm sure it's going to get more and

59
00:03:22.991 --> 00:03:25.510
more popular over time because all the cool kids are talking about it.

60
00:03:25.990 --> 00:03:28.600
You need a statistic about your subscriber nationality.
We are,

61
00:03:28.630 --> 00:03:33.160
we have over 200 countries represented in this community.
200 countries.

62
00:03:33.430 --> 00:03:37.090
It is fast becoming the largest machine learning community in the world.

63
00:03:37.420 --> 00:03:39.070
So it's very exciting to be here right now.

64
00:03:39.310 --> 00:03:43.660
What do you think about profit by Facebook?
So I saw that library and uh,
I mean,

65
00:03:44.460 --> 00:03:46.720
uh,
I have to look into it more.
I didn't really read the details.

66
00:03:47.110 --> 00:03:51.310
Aws or Google cloud ml.
Great question.
So AWS recently went down,

67
00:03:51.340 --> 00:03:54.180
it went down because,
uh,
for whatever reason,

68
00:03:54.181 --> 00:03:58.360
and then all these apps we use just went down because of them because everything

69
00:03:58.361 --> 00:04:01.710
is based off of AWS.
Uh,
but that's just a,

70
00:04:01.720 --> 00:04:03.850
that's just a problem of centralized systems in general.

71
00:04:03.851 --> 00:04:07.540
And it shows why we need decentralized systems.
That's a whole different topic.

72
00:04:07.541 --> 00:04:09.820
I wrote a book on that I could talk on.
I linked about that,

73
00:04:09.821 --> 00:04:11.950
but we're not going to,
but it's comparing those two.

74
00:04:11.951 --> 00:04:16.951
I would say AWS because it's got more people using it and it's,

75
00:04:17.380 --> 00:04:19.650
uh,
it's just,
it's,
I don't know.
I'm,

76
00:04:19.651 --> 00:04:23.110
I'm used AWS and the customer support is also pretty great.

77
00:04:23.470 --> 00:04:25.570
They were subtracting school math.
Uh,

78
00:04:26.260 --> 00:04:30.910
honestly it was English.
English was my favorite subject.
I love language.

79
00:04:30.911 --> 00:04:31.744
I love writing.

80
00:04:31.950 --> 00:04:35.740
I consider myself a technical writer and an engineer and a data scientist.

81
00:04:35.741 --> 00:04:37.870
But a English was my favorite subject in school,

82
00:04:37.900 --> 00:04:40.960
followed by close second math close second was map.
Absolutely.

83
00:04:41.440 --> 00:04:45.130
I remember being super excited by math and having a hard time understanding it

84
00:04:45.131 --> 00:04:48.470
and in middle school,
but I was just so excited about it.
Uh,
that you know,

85
00:04:48.490 --> 00:04:50.590
that that excitement is really what pushes you through that.

86
00:04:50.990 --> 00:04:52.270
The obstacles that you'll face.

87
00:04:52.360 --> 00:04:54.100
One more minute and then we're gonna get started with this code.

88
00:04:54.370 --> 00:04:58.060
It's an eye I python notebook open AI and tensorflow don't compete there.

89
00:04:58.061 --> 00:05:01.880
They're not competing.
They're complementary food.
Why no twitch?
Why?
Um,

90
00:05:01.900 --> 00:05:06.850
because I just like to have it on Youtube because I dunno.
We'll see.
I mean,

91
00:05:07.150 --> 00:05:09.460
if I could,
if someone thinks that did speedy use twitch,
I will,

92
00:05:09.461 --> 00:05:11.920
but I'm not sure what,
how do I build up CD for Google internship,

93
00:05:12.400 --> 00:05:13.360
have projects on get help.

94
00:05:13.361 --> 00:05:18.361
I'm actually going to release a video on job interviews this weekend or next

95
00:05:18.370 --> 00:05:19.270
weekend,
so stay tuned for that.

96
00:05:19.480 --> 00:05:23.990
One more question and then we're going to start it.
Uh,

97
00:05:24.350 --> 00:05:27.830
which one do I want to answer here?
Can I can,

98
00:05:31.730 --> 00:05:33.110
how to use,
okay.
This is a good one.

99
00:05:33.260 --> 00:05:38.260
Parliament said how to know when to use LSTM and when not to LSTM generally

100
00:05:39.021 --> 00:05:42.590
improve our prediction.
Every time.
I mean,

101
00:05:42.650 --> 00:05:46.660
all of the greatest advances coming out of sequential learning involves LSTs

102
00:05:46.670 --> 00:05:50.510
right now in some way.
And Andre Karpov these blog posts is so epic,

103
00:05:50.511 --> 00:05:54.510
the unreasonable effectiveness of neural networks.
And most of that uses,
uh,

104
00:05:56.240 --> 00:06:00.860
LSTM use.
So we don't have to watch live programming at five fts.
Uh,

105
00:06:01.040 --> 00:06:03.860
okay.
So that,
that's a good reason.
Okay.
So I'll think about that for next time.

106
00:06:04.100 --> 00:06:06.110
I'm continuously trying to improve my livestream.
So,
okay,

107
00:06:06.170 --> 00:06:08.930
so you've got 333 people here right now.
So let's get started with this code.

108
00:06:09.170 --> 00:06:12.320
We don't want to keep people waiting.
Okay?
So let's get started with this.

109
00:06:12.680 --> 00:06:14.060
Let's talk about what we're going to do.

110
00:06:14.170 --> 00:06:15.430
Let me just say what we're going to do first.

111
00:06:15.440 --> 00:06:19.190
We're going to build a recurrent nets.
It takes him two sequences,
okay?

112
00:06:19.191 --> 00:06:23.720
So one sequence is a binary sequence,
right?
At ones and Zeros,

113
00:06:24.170 --> 00:06:27.230
and be your arch nemesis.
Neeraj high.

114
00:06:27.500 --> 00:06:31.400
And the other sequence is the same sequence,
which shifted,
okay?
And we'll talk,

115
00:06:31.410 --> 00:06:33.800
we'll talk about how,
what that shift means.
Okay?

116
00:06:33.920 --> 00:06:37.040
But the idea is that we going to map these two sequences together.

117
00:06:37.130 --> 00:06:40.340
We're going to learn the mapping between these two sequences using a recurrent

118
00:06:40.341 --> 00:06:43.010
neural network.
So let's get started with this.
Okay,

119
00:06:44.420 --> 00:06:49.070
let's start broadcasting guys,
we have so much sequential sub to learn right now.

120
00:06:49.071 --> 00:06:51.710
We have,
we're going to dive into recurrent nets.
It's going to be awesome.

121
00:06:51.960 --> 00:06:56.630
Screenshare okay.
Screen share.
Here's our screen that we're going to share it.

122
00:06:56.660 --> 00:07:00.920
Now I have the notebook in the get hub link in the description for this video

123
00:07:00.921 --> 00:07:03.770
because you guys have been asking for that.
So you know,

124
00:07:03.800 --> 00:07:05.680
I thought it would honestly,
I thought it would be funner.

125
00:07:06.710 --> 00:07:09.530
I thought it would be fun or if,
if you guys don't see the code,

126
00:07:09.531 --> 00:07:13.130
but you know what,
you'll see it as a code it,
but you know what,

127
00:07:13.250 --> 00:07:17.930
I'll just give it to anyway,
so that's,
that's that.
All right,
so here we go.

128
00:07:17.931 --> 00:07:20.270
With this here.
Here's the,
here's the,

129
00:07:20.300 --> 00:07:22.730
here's a notebook and I'm going to code parts of it and parts of it are now

130
00:07:22.731 --> 00:07:24.530
going to be coated.
So it's going to be a mix of both.

131
00:07:24.920 --> 00:07:29.540
Let's make it a little bigger.
Okay.
Let make it a little bigger.
Okay.
Or,

132
00:07:29.541 --> 00:07:31.400
or a lot bigger,
one or the other.
Okay.

133
00:07:31.700 --> 00:07:33.140
So that's what we're going to do with that.

134
00:07:33.680 --> 00:07:36.530
Let's get the comments over here and we're going to start building this.

135
00:07:36.531 --> 00:07:39.290
So everybody pull up a notebook.
Okay.

136
00:07:39.291 --> 00:07:43.790
It's in the link and we're going to talk about what this happening,
uh,
as we go,

137
00:07:44.080 --> 00:07:45.980
getting started.
Okay,
so here we go.

138
00:07:46.520 --> 00:07:48.740
What we're going to do is we're going to use a recurrent nets.
Oh.

139
00:07:48.741 --> 00:07:53.240
And also of course,
I need to have my bass up here.
What am I doing?

140
00:07:53.450 --> 00:07:56.300
So we're gonna do is we're going to build a recurrent neural network and we're

141
00:07:56.301 --> 00:07:57.710
going to build it with tensorflow.

142
00:07:58.070 --> 00:08:01.370
We're going to build a recurrent neural network with tensorflow.
Okay.

143
00:08:01.371 --> 00:08:04.430
And we're not going to use any other libraries to build their network.
I mean,

144
00:08:04.431 --> 00:08:05.270
we're going to be some libraries,

145
00:08:05.271 --> 00:08:07.190
but they're not going to be to build the network.

146
00:08:07.220 --> 00:08:12.220
They're just going to be there because you know there to plot the data.

147
00:08:13.100 --> 00:08:15.680
Okay?
Okay.
So let's,
let's get started with this.

148
00:08:17.420 --> 00:08:18.890
So what is a recurrent neural network?

149
00:08:18.920 --> 00:08:22.340
It is a type of neural network that is used for sequential data.

150
00:08:22.520 --> 00:08:25.190
We're going to learn about sequential data right now,

151
00:08:25.400 --> 00:08:28.430
and this is a great paper that I have linked right here that talks about

152
00:08:28.431 --> 00:08:31.460
recurring nets.
Definitely check out that paper after this live stream,

153
00:08:31.461 --> 00:08:32.270
but not right now.

154
00:08:32.270 --> 00:08:36.290
It's probably the most comprehensive paper on neural now on recurrent nets that

155
00:08:36.291 --> 00:08:40.850
I found.
And remember when you read a paper,
remember to read the abstract first.

156
00:08:40.880 --> 00:08:43.820
Okay?
Don't expect to understand everything from a paper,

157
00:08:43.821 --> 00:08:46.940
just read the abstract and then keep on going.
Okay?

158
00:08:47.660 --> 00:08:51.620
And if you understand you have tracks.
That is,
that is,
that is a,
that is a uh,

159
00:08:51.780 --> 00:08:53.120
achievement in and of itself.

160
00:08:53.150 --> 00:08:55.530
So we're going to start off by importing our dependencies,
right?

161
00:08:55.680 --> 00:08:58.890
We're going to import I pythons display function because there's going to be

162
00:08:58.891 --> 00:09:03.000
displaying images.
These images are going to help us a look at what we're doing.

163
00:09:03.240 --> 00:09:06.330
Num Pi is going to help us generate some binary input data.

164
00:09:06.690 --> 00:09:08.370
Tensorflow is obviously for machine learning.

165
00:09:08.371 --> 00:09:10.800
And then map plot line will help us plot this data.

166
00:09:11.070 --> 00:09:14.550
So I'm going to start off by showing this image,
okay?
Now what this is,

167
00:09:14.580 --> 00:09:19.560
is it as an image that shows a recurrent nets that is learning over time.

168
00:09:19.890 --> 00:09:24.670
So we have it's learning over time and these three squares or the states of the,

169
00:09:24.690 --> 00:09:29.490
the hidden state that is that,
that hidden layer in the recurrent nets over time.

170
00:09:29.760 --> 00:09:33.780
So it's this,
it's so it's,
so it changes over time,
right?
Why?
Because we feed it,

171
00:09:33.781 --> 00:09:38.250
not just the input data,
we feed the input data and the previous hidden state.

172
00:09:38.460 --> 00:09:42.790
So when we perform an optimization technique like backpropagation,

173
00:09:42.990 --> 00:09:47.250
it's essentially back propagation through time mode.

174
00:09:47.610 --> 00:09:49.200
We're just getting started.
So you didn't miss anything.

175
00:09:49.350 --> 00:09:53.040
It's back propagation through time.
So it's like back to the future,
you know,

176
00:09:53.041 --> 00:09:56.280
it's like time travel and we'll talk about,
not literally time travel,

177
00:09:56.281 --> 00:09:59.250
but in a way,
because we remember what happened beforehand.

178
00:09:59.430 --> 00:10:03.330
We're going into the past.
Okay.
So,
so that's what,
that's what this is.

179
00:10:03.360 --> 00:10:05.790
So let's get started with this.
Okay.
That's,
that's a definition of it.

180
00:10:05.791 --> 00:10:09.450
Recurring that now what we can do is we can start coding this thing.

181
00:10:09.451 --> 00:10:12.240
So we're going to start off by coding up our hyper parameters.
Okay.

182
00:10:13.260 --> 00:10:14.340
So for a hyper parameters,

183
00:10:14.490 --> 00:10:18.420
we're going to define a number of talks to a number of reports,
which is 100.

184
00:10:18.670 --> 00:10:19.790
Remember so and so.

185
00:10:20.200 --> 00:10:23.850
So whenever we are training a network where you define epochs and inside of

186
00:10:23.851 --> 00:10:26.280
these deposits we have batches.
So we have,

187
00:10:26.281 --> 00:10:29.040
so for like 10 epochs we'll have like five batches.

188
00:10:29.041 --> 00:10:33.450
So it's like 50 total iterations,
right?
10 Times five because it is a,

189
00:10:33.750 --> 00:10:35.910
it is a nested loop,
right?
So for every epoch,

190
00:10:36.000 --> 00:10:37.680
how many batches do we want to claim against?

191
00:10:37.850 --> 00:10:40.770
So we're going to find that number of people I can obviously the more he talks,

192
00:10:41.590 --> 00:10:42.630
the longer the training,

193
00:10:42.750 --> 00:10:46.740
the better our prediction right to it's a trade off between computation time and

194
00:10:46.741 --> 00:10:49.560
accuracy,
like all things with when it comes to machine learning,

195
00:10:49.770 --> 00:10:54.510
truncated back pop length.
So,
so the next uh,
variable,

196
00:10:54.600 --> 00:10:55.730
the concrete it back properly.

197
00:10:55.770 --> 00:10:58.440
I'm going to talk about in detail when we get to it,
but right now let's,

198
00:10:58.441 --> 00:11:03.330
let's skip that.
Okay.
Um,
before our state size,
it's going to be four.

199
00:11:03.390 --> 00:11:05.560
So what is our state side?
The state ties in.

200
00:11:05.561 --> 00:11:07.670
That is the number of neurons in our,

201
00:11:07.880 --> 00:11:11.130
a hidden layer in our hidden layer.
Okay.

202
00:11:11.131 --> 00:11:15.300
So that's what it is because we have a three layer recurrent net that we're

203
00:11:15.301 --> 00:11:19.020
building and that hidden state if what we're going to up update over time and

204
00:11:19.021 --> 00:11:20.130
we're going to talk about how that works,

205
00:11:20.400 --> 00:11:22.500
then we've have an expert is going to be number of classes.

206
00:11:22.620 --> 00:11:25.500
So by number of classes it's into our data is binary.

207
00:11:25.530 --> 00:11:28.410
It's going to be two classes,
but this isn't classification.

208
00:11:28.670 --> 00:11:33.000
It's a sequence to sequence mapping.
The number of classes is one and zero.

209
00:11:33.001 --> 00:11:35.820
So it's two.
Then we're going to talk about the echo step.

210
00:11:37.560 --> 00:11:38.790
The echo step is going to be

211
00:11:40.420 --> 00:11:41.253
<v 2>uh,</v>

212
00:11:41.400 --> 00:11:44.310
<v 0>three.
And so the echo stuff,
I'm going to talk about that one in a second too.</v>

213
00:11:44.311 --> 00:11:46.950
So remember these are the two variables that I'll talk about when we get to
them.

214
00:11:47.130 --> 00:11:50.970
The truncated backpack lane and then uh,
the echo step.

215
00:11:51.270 --> 00:11:54.000
And so the next one is going to be the batch size.
So the backsides,

216
00:11:54.010 --> 00:11:56.740
remember inside of each epoch we have a number of batches.

217
00:11:56.741 --> 00:12:00.400
And so we're going to find it as fun.
And then lastly,
the number of that,

218
00:12:00.401 --> 00:12:02.890
just the number of batches is going to be the total series length.

219
00:12:03.220 --> 00:12:06.670
And we're going to take the quotient right.
So in Python,
this,
this,
um,

220
00:12:08.300 --> 00:12:12.300
a character means potion by the backside and the quotion of that by the

221
00:12:12.301 --> 00:12:13.930
truncated back properly.

222
00:12:13.940 --> 00:12:17.010
And this is going to make sense when I talk about complicated backdrop length in

223
00:12:17.011 --> 00:12:20.820
a second,
but let's go ahead and just type that in first.
Okay.

224
00:12:20.910 --> 00:12:24.630
So that's kind to defined a number of batches we have.
Okay,
so let's start,

225
00:12:24.930 --> 00:12:27.270
let's go ahead and get started with collecting our dataset.

226
00:12:27.510 --> 00:12:30.810
What we're gonna do is we're going to generate this data and this is,
this is a,

227
00:12:30.820 --> 00:12:32.660
this is a net.
This is a good step to talk about.

228
00:12:32.661 --> 00:12:36.570
Like generating the data is a good segue to talk about because a lot of times we

229
00:12:36.571 --> 00:12:39.510
just want to test out our nerves,
our neural net,

230
00:12:39.511 --> 00:12:41.490
but we don't want to test it out on,
you know,

231
00:12:41.491 --> 00:12:44.250
real world data so we can just generate it ourselves using them five.

232
00:12:44.460 --> 00:12:47.340
And there's a lot of strategies to generate data,

233
00:12:47.341 --> 00:12:50.670
but what we're gonna do is we're going to generate a time series data

234
00:12:51.090 --> 00:12:55.800
specifically.
We're going to generate 50 k samples of time series data,

235
00:12:55.830 --> 00:12:57.960
and it's going to be all binaries.
Okay?

236
00:12:58.850 --> 00:12:59.250
<v 2>Okay?</v>

237
00:12:59.250 --> 00:13:01.980
<v 0>So we're going to use the [inaudible] function to do this,</v>

238
00:13:03.160 --> 00:13:03.760
<v 2>okay?</v>

239
00:13:03.760 --> 00:13:06.070
<v 0>Okay.
And we're gonna make sure that compiles,
right?
Okay?</v>

240
00:13:06.071 --> 00:13:08.620
So the num py array function is going to say,
well,

241
00:13:08.621 --> 00:13:11.860
we're going to generate a random distribution of data using the random by choice

242
00:13:11.861 --> 00:13:15.970
function.
And we're going to say we want to,
uh,
classes,

243
00:13:16.420 --> 00:13:18.400
the length of the series that we defined earlier,

244
00:13:18.640 --> 00:13:22.960
and then the probability that each of those classes gets picked,
it's 50,
50,

245
00:13:23.190 --> 00:13:26.890
right?
So it's a 50,
50 chance that each number in the series as it's generated,

246
00:13:27.010 --> 00:13:30.250
it's going to get picked.
That's what that is.
And then,

247
00:13:31.010 --> 00:13:32.750
so that's going to be stored in x.

248
00:13:32.900 --> 00:13:35.180
And then what we're going to do is we're gonna generate our echo.

249
00:13:35.210 --> 00:13:37.460
So let's talk about what this echo means.
So we're going to do,

250
00:13:37.490 --> 00:13:39.980
we're going to shift his three steps to the left.

251
00:13:40.220 --> 00:13:43.340
We're going to ship this time series,
three steps to the left.

252
00:13:43.340 --> 00:13:46.430
And what do I mean by that?
By shifting it three steps to the left,

253
00:13:46.580 --> 00:13:48.320
we're going to pad it with Zeros,

254
00:13:48.620 --> 00:13:51.650
we're going to pat that data would zeros because it's going to be the same

255
00:13:51.651 --> 00:13:55.820
essentially the same exact series,
but shifted to the left by three steps.

256
00:13:56.000 --> 00:13:57.710
And we're going to learn that mapping,
right?

257
00:13:57.830 --> 00:14:02.180
We don't need any testing data because it's going to be the same shift,
right?

258
00:14:02.181 --> 00:14:04.490
So it's two sequences and I'm going to show it with this image.

259
00:14:04.670 --> 00:14:07.970
This image is going to work.
Okay,
so,
so the role function,

260
00:14:08.180 --> 00:14:12.350
the role function is going to say,
uh,
so we have x and then echo steps.
So,

261
00:14:13.850 --> 00:14:15.080
so given like two numbers,

262
00:14:15.170 --> 00:14:19.610
let's say like two and 10 it's going to start to from before 10 and then end

263
00:14:19.611 --> 00:14:19.971
before it.

264
00:14:19.971 --> 00:14:24.230
So it's going to have to sue the outputs of this line right here is going to be,

265
00:14:24.260 --> 00:14:29.000
so if it's like two 10 right?
Access to an echo stuff is 10 it'll be like eight,

266
00:14:29.001 --> 00:14:31.340
nine,
one,
two,
three,
four,
five,
six,
seven.
Okay.

267
00:14:31.341 --> 00:14:34.040
And then the reason we do that is because now we're going to pad the beginning

268
00:14:34.160 --> 00:14:38.090
with Zeros will say why and zero and then echoed step.

269
00:14:38.150 --> 00:14:42.320
And then finally we'll set all of those to zero.
Okay.
And then,

270
00:14:42.800 --> 00:14:45.140
then we're going to do lag by three.

271
00:14:47.980 --> 00:14:51.680
Okay,
so then what we're going to do,
what's going to reshape it,
right?

272
00:14:51.681 --> 00:14:53.360
So we have two more lines.
If this Gary Beta,

273
00:14:53.390 --> 00:14:56.000
before we get started with our actual machine learning steps.

274
00:14:56.150 --> 00:14:59.690
So we're going to reshape this data so that it's formatted properly to feed into

275
00:14:59.691 --> 00:15:00.440
our network.

276
00:15:00.440 --> 00:15:04.040
We want to feed this into our network and you have to format it properly and

277
00:15:04.041 --> 00:15:05.960
numb wise reshape function does this,

278
00:15:06.380 --> 00:15:09.050
he takes a matrix and it reshapes it into a size that we give it.

279
00:15:09.410 --> 00:15:12.500
Now the batch size that we're going to reshape it as.
Okay,

280
00:15:12.650 --> 00:15:17.210
so that's for why that's what x and now,
now we're going to do it for y.
Okay.

281
00:15:19.070 --> 00:15:23.180
This is to have an x and y that are correlated with time.
Exactly.
Okay.

282
00:15:25.190 --> 00:15:26.023
<v 1>Okay.</v>

283
00:15:26.230 --> 00:15:29.770
<v 0>Okay.
So then per batch size we have that and then we have negative one.</v>

284
00:15:30.100 --> 00:15:33.530
It's great.
So that's our x and Y,
and then we're going return those values.

285
00:15:33.850 --> 00:15:35.170
So let's,
let's look at,

286
00:15:35.171 --> 00:15:37.310
let's visualize what this looks like by running this function.

287
00:15:37.311 --> 00:15:39.360
Once there's function that we just generated,

288
00:15:39.610 --> 00:15:42.220
generate data and it's going to be stored in the state of variable.

289
00:15:42.221 --> 00:15:44.860
And then we'll go ahead and protect data.
But let's see what this does.

290
00:15:45.190 --> 00:15:46.080
Let's see what this ducks,

291
00:15:47.040 --> 00:15:47.340
<v 1>okay,</v>

292
00:15:47.340 --> 00:15:48.450
<v 0>yeah.
I'm going to show the data.
Here we go.</v>

293
00:15:50.960 --> 00:15:55.260
NP is not defined because I didn't compile a NP beforehand,
right?

294
00:15:56.130 --> 00:16:00.710
To pile this.
Yes.
Let me compile that.
Boom,
compile.

295
00:16:01.520 --> 00:16:06.470
Okay.
And then compile the hyper parameters and then compile this function.
Okay,

296
00:16:06.471 --> 00:16:09.590
so this is our data,
right?
This is just a bunch of ones and Zeros.

297
00:16:09.650 --> 00:16:14.530
It's sequential data,
right?
And it's formatted to be a two d matrix of points,

298
00:16:14.720 --> 00:16:17.780
and now we can visualize it.
It's all ones and Zeros,
right?

299
00:16:18.140 --> 00:16:21.410
It's all ones and Zeros and to the x.

300
00:16:21.560 --> 00:16:24.410
The x values and the y values are both ones and Zeros.

301
00:16:24.530 --> 00:16:28.550
The x values are the initial ones and Zeros,
the binary data,
the sequential,

302
00:16:28.610 --> 00:16:30.290
the sequence of ones and Zeros.

303
00:16:30.560 --> 00:16:35.120
And the why data is that same exact sequence but shifted over three steps and

304
00:16:35.121 --> 00:16:37.310
there's first resets are then replaced with Zeros.

305
00:16:37.460 --> 00:16:39.530
And we want to learn the mapping between these two points.

306
00:16:39.710 --> 00:16:43.700
The point of this whole exercise is to learn about how memory is incorporated

307
00:16:43.701 --> 00:16:45.350
into a Nerc or recurrent net.

308
00:16:45.590 --> 00:16:47.780
And that's going to be the really cool part and that's what we're going to get

309
00:16:47.781 --> 00:16:50.870
to you.
Okay.
So,
so that's the data that we've generated.
Okay.

310
00:16:50.930 --> 00:16:55.670
So this is an image that shows what this data looks like.
Okay.
Basically it is.

311
00:16:56.600 --> 00:16:59.180
So this is an image of what the data looks like and

312
00:17:00.360 --> 00:17:03.210
it's a reshape data matrix and the Arrow is going to show that.

313
00:17:03.370 --> 00:17:07.170
So showing the time steps,
right?
So for every new row it's a,
it's a,
it's,

314
00:17:07.280 --> 00:17:10.800
it's a new time step.
We can consider it a new time step when we,

315
00:17:11.010 --> 00:17:14.010
when we feed it in,
it's going to be at a different time step.
Okay.
So,

316
00:17:14.260 --> 00:17:17.430
so let's go ahead and get started with our actual machine learning with

317
00:17:17.431 --> 00:17:21.010
tensorflow.
So step two,
step one was to,
uh,

318
00:17:21.640 --> 00:17:23.620
step one was to generate the data.

319
00:17:23.621 --> 00:17:26.560
And now step two is going to be to build this model.

320
00:17:27.340 --> 00:17:30.950
It's going to be to build this model.
So we,
so the the,
so we didn't,
so,

321
00:17:30.980 --> 00:17:35.540
so the point of reshaping the array,
the tensor array,

322
00:17:35.560 --> 00:17:39.130
whichever you want to call it,
will,
so that we can feed it into our placeholder,

323
00:17:39.131 --> 00:17:42.780
which at that gateway that data flows in into tensorflow.
Okay.
So,

324
00:17:42.781 --> 00:17:47.040
so let's get started with our tensorflow portion.
Right?
Okay.
So,

325
00:17:47.100 --> 00:17:50.530
so for building our model,
the first thing we want to is generate a,

326
00:17:50.560 --> 00:17:52.740
or to initialize both of are placeholders.

327
00:17:52.980 --> 00:17:56.940
We're going to have place holders for our x values and for our why doggies.
Okay.

328
00:17:57.030 --> 00:18:00.520
So for our placeholder,
we're going to say,
let's use our float.

329
00:18:00.720 --> 00:18:02.910
So what is a data type or our first place holder?

330
00:18:03.090 --> 00:18:07.470
It's going to be a float 32 because these are 32 bit integers and then we want

331
00:18:07.630 --> 00:18:11.760
the shape of it,
right?
So it's gonna be a two d matrix of five,
uh,

332
00:18:11.850 --> 00:18:15.180
rose and then 15 no,
sorry,
five columns.

333
00:18:15.960 --> 00:18:18.990
And in 15 rows,
right?
Just like,
just like we saw up there,

334
00:18:19.020 --> 00:18:23.200
that's exactly what we saw.
We want to fit it to be that.
Okay.
And

335
00:18:24.780 --> 00:18:28.410
it's going to be truncated back,
proper lens,
truncated backdrop.

336
00:18:29.370 --> 00:18:31.470
And remember,
I'm going to come here to talk about this in a second.

337
00:18:31.471 --> 00:18:35.340
It's truncated back off my butt.
So that's the batch y that Jack's okay.

338
00:18:35.520 --> 00:18:38.370
So we're going to do that for both place holders and in fact I can just copy

339
00:18:38.371 --> 00:18:41.670
this and paste it again because it's the same thing.
Okay.

340
00:18:43.250 --> 00:18:45.080
Are All those showing up now?
You're not missing anything.

341
00:18:45.081 --> 00:18:48.620
Just look at the notebook so you can catch up.
It's in the,
it's in the link.
Okay.

342
00:18:48.621 --> 00:18:52.370
So,
so that's that.
Most of our,
uh,
most of our placeholders,
right?

343
00:18:52.430 --> 00:18:55.730
So remember in tensorflow,
these placeholders are gateways.

344
00:18:55.910 --> 00:18:58.970
This is what we feed our data into.
These are primitive types.

345
00:18:58.971 --> 00:19:03.410
These are primitive.
It's intense.
Your phones please.
Holders,
variables,
constants.

346
00:19:03.500 --> 00:19:05.270
These are the primitives with,
with,
with,

347
00:19:05.290 --> 00:19:09.850
with tensorflow works and these gateways we feed the input data and the labeled

348
00:19:09.851 --> 00:19:14.600
data.
So we can essentially,
we can consider this,
this echo to be a label,
right?

349
00:19:14.700 --> 00:19:18.170
In a way it's,
it's a mapping between two sequences,
the sequence,

350
00:19:18.230 --> 00:19:19.790
the sequence mapping.
Okay?

351
00:19:19.791 --> 00:19:24.791
And we're not using LSTM because we have plenty of time to talk about LSTM in

352
00:19:25.091 --> 00:19:28.360
future episodes.
We're going to talk about art and video and music generation,

353
00:19:28.600 --> 00:19:31.210
and what's the secret to winning with chat bots and all that stuff's going to be

354
00:19:31.211 --> 00:19:33.250
using LSTM and we'll talk about that in detail.

355
00:19:33.520 --> 00:19:37.240
But what we're doing right now is we're just going to generate these two

356
00:19:37.260 --> 00:19:38.170
placeholders.
Okay?

357
00:19:38.290 --> 00:19:41.580
So we have these two placeholders and the next thing we're going to do is we're

358
00:19:41.581 --> 00:19:44.920
going to initialize our state,
right?
So this is,

359
00:19:44.950 --> 00:19:47.260
so this is the great part of this tutorial.

360
00:19:47.290 --> 00:19:48.640
This is what I'm really excited about.

361
00:19:48.910 --> 00:19:52.310
What we're going to do is we're going to show what really is happening and that

362
00:19:52.311 --> 00:19:55.150
states,
right?
It's not some magic,
it's not just right.

363
00:19:55.151 --> 00:19:59.220
We're going to show how what is happening in the memory portion of Becket and

364
00:19:59.221 --> 00:19:59.680
state.

365
00:19:59.680 --> 00:20:03.490
So we're going to initialize a state because we feed in the state every time as

366
00:20:03.491 --> 00:20:06.430
well,
right?
That's the,
that's the great thing about recurrent nets,
right?

367
00:20:06.520 --> 00:20:10.930
We're not just feeding an input data every,
every,
uh,
every iteration of training.

368
00:20:11.080 --> 00:20:15.010
We're also feeding it in the previous hidden state that was learnt.

369
00:20:15.220 --> 00:20:17.730
So this is the gateway for that hidden states.

370
00:20:17.800 --> 00:20:19.870
So in recurrent net we have a placeholder,

371
00:20:19.960 --> 00:20:23.350
not just for the input data and its labels,
but also the,

372
00:20:23.410 --> 00:20:25.870
the state that it learned beforehand.
Right.

373
00:20:25.930 --> 00:20:28.680
So it's got a previous state and occurrence.
Okay.

374
00:20:28.690 --> 00:20:32.580
So that's what our placeholder is for.
What does the type,

375
00:20:32.581 --> 00:20:33.970
it's going to be just like the other two,

376
00:20:34.030 --> 00:20:36.160
it's going to be 32 bits and then we're going to,

377
00:20:36.250 --> 00:20:39.400
the shape of it is going to be the bad side and then the state size.
Right.

378
00:20:39.520 --> 00:20:42.460
And we could find a street size has a number of neurons

379
00:20:44.440 --> 00:20:48.640
that happened at beforehand.
Okay.
So batch size and then state size.

380
00:20:48.940 --> 00:20:51.100
So we've defined these.
Great.

381
00:20:52.160 --> 00:20:52.330
<v 2>Okay.</v>

382
00:20:52.330 --> 00:20:54.190
<v 0>Okay.
And then now of course we haven't talked to her.</v>

383
00:20:54.310 --> 00:20:57.820
So let's see what we got here.
D type.
So you know,
it's,

384
00:20:57.821 --> 00:21:00.400
it's good to see this error.
It's happening in real time.
Right?

385
00:21:00.580 --> 00:21:05.200
So for d type it's placeholder shake five,
four,
float,
float,
32.

386
00:21:05.201 --> 00:21:09.580
What do we got here?
What TF dot.
Float 32 batch size,

387
00:21:09.730 --> 00:21:12.520
state size without placeholder values.

388
00:21:17.550 --> 00:21:18.690
And this one is,
oh,

389
00:21:18.691 --> 00:21:22.920
so you know what this is that this is actually meant 32 because I think all

390
00:21:22.930 --> 00:21:23.763
right.

391
00:21:25.040 --> 00:21:25.873
<v 2>Oh,</v>

392
00:21:27.560 --> 00:21:31.950
<v 0>I great.
Okay,
so that's,
that's the course dates.
Okay.</v>

393
00:21:31.951 --> 00:21:36.060
So now what we're going to do is we're going to build our weights and biases for

394
00:21:36.061 --> 00:21:39.630
this network,
right?
We've got our place holders,
we defined the gateway.

395
00:21:39.780 --> 00:21:42.090
It's the data is ready to be fed into the network.

396
00:21:42.150 --> 00:21:46.680
Now let's define those weights and biases and talk about what is happening.
Uh,

397
00:21:48.300 --> 00:21:49.060
<v 2>okay.</v>

398
00:21:49.060 --> 00:21:50.860
<v 0>Yes.
So we're going to talk about what happens to the state.</v>

399
00:21:50.861 --> 00:21:53.980
It's going to be dope.
It's going to be sewed up because you never really see,

400
00:21:54.040 --> 00:21:56.050
see this pat,
it's part time.
So I'm really excited about that.

401
00:21:56.260 --> 00:21:59.520
But let's go ahead and,
and,
and,
and,
and define our weights.
So the,

402
00:21:59.590 --> 00:22:03.610
so remember because it's a three layer network,
we have two sets of weights,

403
00:22:03.611 --> 00:22:06.460
right?
Those are the weights between the intraday,
right?
The input,

404
00:22:06.600 --> 00:22:09.790
the input from the input layer to them in state.

405
00:22:09.850 --> 00:22:12.400
And then from the hidden state to the Apple Apple Layer,
right?

406
00:22:12.401 --> 00:22:15.250
So there were two weights.
Let's define those two sets of weights.

407
00:22:15.430 --> 00:22:17.640
So the first one is going to be a w.
Dot.

408
00:22:17.641 --> 00:22:20.980
We'll call it w and we're going to use the key,
I've got variable for this.

409
00:22:21.160 --> 00:22:23.230
And why do you use,
do you have that variable?

410
00:22:23.380 --> 00:22:25.530
Because it's that tensorflow primitive,
right?

411
00:22:27.280 --> 00:22:31.720
And variables can be updated over time,
right?
Unlike tensorflow,
constants,

412
00:22:31.840 --> 00:22:35.940
variables can be updated over time.
And that's why we're using a variable.

413
00:22:36.100 --> 00:22:39.130
We're going to randomly initialized it.
It's value,
okay?

414
00:22:39.490 --> 00:22:44.180
And we're gonna use a state size plus one.
And
okay,

415
00:22:44.181 --> 00:22:47.560
so this is,
so this is the distribution of value.

416
00:22:47.570 --> 00:22:50.690
This can be random at random distribution of values.
That's going to update.

417
00:22:50.750 --> 00:22:54.260
Now how,
why do we initialize our weights?
It's random,
right?

418
00:22:54.350 --> 00:22:57.080
Why don't we just set it to zero?
It just,
it tends to,

419
00:22:57.380 --> 00:23:00.800
it we generally in the papers that get the best results,

420
00:23:00.890 --> 00:23:04.760
it's kind of become commonplace to initialize weights is as randomly,
right?

421
00:23:04.850 --> 00:23:07.100
But there could be better,
you know,
ways of doing that in the future.

422
00:23:07.101 --> 00:23:09.400
But right now randomly initializing weights.

423
00:23:09.401 --> 00:23:14.401
It seems to work best so well now define the data type for this.

424
00:23:14.720 --> 00:23:17.240
We're just going to be a CSF flow 32 again.
Okay,

425
00:23:17.660 --> 00:23:22.590
so that's it for our data type is going to be ETF
32

426
00:23:24.940 --> 00:23:25.773
<v 2>okay,</v>

427
00:23:26.910 --> 00:23:27.570
<v 0>so that's,</v>

428
00:23:27.570 --> 00:23:30.690
that's randomly in neutralizing our weights and then we're going to initialize

429
00:23:30.691 --> 00:23:35.691
our biases and so for our body sees why do we even have a bias?

430
00:23:36.120 --> 00:23:40.650
So biocese what they do is they improve convergence and we think about biases in

431
00:23:40.651 --> 00:23:43.230
terms of some typing and talking and teaching at the same time.
This is dope.

432
00:23:43.410 --> 00:23:46.100
I'm really getting better at this.
Guys like doing this every week.
It's,

433
00:23:46.101 --> 00:23:49.210
it's really,
it's just public speaking.
Basically.

434
00:23:49.211 --> 00:23:52.750
I'm just giving a speech every week.
What happens is a biocese,

435
00:23:52.760 --> 00:23:54.980
we're improving convergence.
That's what it does.

436
00:23:54.981 --> 00:23:57.440
So we can think of biases in terms of when your regression,
right,

437
00:23:57.441 --> 00:23:59.600
took me in linear regression biases.

438
00:23:59.870 --> 00:24:02.720
You can think of it as the y equals mx plus B equation,
right?

439
00:24:02.721 --> 00:24:05.630
It's very simple for you about it and some plate linear model.

440
00:24:05.950 --> 00:24:10.940
Harder to think about any law.
Nonlinear mom in a,
in a nonlinear model,

441
00:24:12.550 --> 00:24:13.580
a nonlinear model.

442
00:24:13.790 --> 00:24:17.870
Why is he's still offering that constant anchor points such that it improves

443
00:24:17.871 --> 00:24:21.490
convergence.
But it's not that,
it's not that,
uh,
it's,
it's,
it's,

444
00:24:21.491 --> 00:24:26.230
it's not that it's a,
it's a,
it's a w a y intercept per se.
It's just a,
it's just,

445
00:24:26.300 --> 00:24:28.490
it's just not an anchor point.
So that we,

446
00:24:29.360 --> 00:24:33.000
the value that we predict aren't too far off from,
from what they should be.
And,

447
00:24:33.040 --> 00:24:35.780
and we can talk about more about by biases in a second.

448
00:24:37.580 --> 00:24:38.380
<v 1>Okay.</v>

449
00:24:38.380 --> 00:24:41.780
<v 0>All right.
The delight can be quite long.
So,
so that's it for our,
um,</v>

450
00:24:42.220 --> 00:24:43.720
for our initial set of weights.

451
00:24:43.960 --> 00:24:48.960
So weights and biases one weeks and by teeth one and then wait and buy teeth

452
00:24:49.571 --> 00:24:52.570
too.
Why is this one?
And then weights and biases too,
right?

453
00:24:52.900 --> 00:24:55.990
So weights to biases too.
So,

454
00:24:55.991 --> 00:24:58.340
and then in second sense.
Okay.

455
00:24:58.360 --> 00:25:02.350
And so the different between both of these in second sense and so demurrage

456
00:25:02.380 --> 00:25:05.170
between both of these is going to be the state size,
not plus one,

457
00:25:05.171 --> 00:25:08.740
and then the number of classes.
And then finally the Beta type.

458
00:25:08.920 --> 00:25:13.300
It's going to be set to flood 32 and then the number of classes,
right?

459
00:25:13.310 --> 00:25:16.050
Because it's going to be one or two.
Okay.

460
00:25:16.051 --> 00:25:19.710
Because the output and the reason he's going to be numb classes for this one is

461
00:25:19.711 --> 00:25:24.660
because we were going to output,
uh,
that set of classes.
Right?
Um,
okay.

462
00:25:25.530 --> 00:25:30.420
Okay.
No,
no,
I can slow down.
Don't worry about it.
I'll slow down.
Okay.

463
00:25:31.070 --> 00:25:35.880
So that's that.
Okay.
We defined this.
Okay.

464
00:25:35.881 --> 00:25:39.600
So now this is what,
so we define our ways.
We define our biases.

465
00:25:41.060 --> 00:25:45.110
It's a lot to remember.
It's a lot to remember,
but at the same time,
just,

466
00:25:45.170 --> 00:25:49.310
just taking all this in of regally co continuously is good.

467
00:25:49.311 --> 00:25:52.100
Like there's a lot of things that we're learning even subconsciously that we

468
00:25:52.101 --> 00:25:55.850
don't even know.
Okay?
So it's good to even think,
take these things in week,

469
00:25:56.000 --> 00:25:58.250
even if you don't fully understand it,
you know,
I the to start,

470
00:25:58.370 --> 00:26:02.360
you will slowly start to get it.
Okay?
So,

471
00:26:04.130 --> 00:26:05.960
so we did,
we defined those biases.

472
00:26:05.961 --> 00:26:09.420
Now what we're gonna do is we're going to look at this image,
right?

473
00:26:09.440 --> 00:26:13.170
And so what we're,
so we can think of this as,

474
00:26:13.710 --> 00:26:15.510
we're not so as batches,
right?

475
00:26:15.511 --> 00:26:18.870
So we have a full matrix of binary values,

476
00:26:18.900 --> 00:26:22.500
and we're going to look at this in batches.
We're gonna look at this in batches.

477
00:26:22.501 --> 00:26:24.720
So not all of it,
and we're not going to feed it in all at once.

478
00:26:24.840 --> 00:26:28.580
And so that's what this dotted square over these values is.

479
00:26:28.780 --> 00:26:31.740
It's kind of like looking at the batches.
Okay?

480
00:26:33.300 --> 00:26:35.160
So let's keep going.
So,

481
00:26:36.420 --> 00:26:37.140
<v 1>okay,</v>

482
00:26:37.140 --> 00:26:41.700
<v 0>now what we're going to do is we're going to unpack these columns,</v>

483
00:26:42.110 --> 00:26:44.850
uh,
and so,
so we can feed it into our rank.
Okay?

484
00:26:44.851 --> 00:26:47.370
So unpacking is for what we're going to,

485
00:26:47.371 --> 00:26:52.050
we're going to unpack these columns into,
uh,
a set of lists,

486
00:26:52.200 --> 00:26:54.360
and each of these lists are going to be fed it,
right?

487
00:26:54.361 --> 00:26:58.860
So remember that matrix of ones and Zeros.
We just want a one array,
right?

488
00:26:58.861 --> 00:27:01.010
So at a time we just feed in one of those arrays out.

489
00:27:01.011 --> 00:27:02.820
It's one of the sequences at a time.

490
00:27:03.090 --> 00:27:06.690
Unpack a matrix into,
uh,

491
00:27:07.490 --> 00:27:12.120
a one dimensional array.
I mean,
right?
So,
uh,

492
00:27:12.150 --> 00:27:14.850
so took stuff from inputs here and we're going to do that for both of our,

493
00:27:15.000 --> 00:27:16.670
both of our values.
We're going to do it for arc,

494
00:27:18.530 --> 00:27:20.010
we're going to do it for our you put series,

495
00:27:20.040 --> 00:27:21.660
and then we're going to do it for output series.

496
00:27:22.050 --> 00:27:24.540
And we've defined those placeholders for each,
right?

497
00:27:24.541 --> 00:27:27.360
And so it's gonna be on a single access.
And what we can do,

498
00:27:30.270 --> 00:27:33.900
the end goal here is to map,
uh,
two sequences together,
right?

499
00:27:33.901 --> 00:27:36.750
We have one team within a node.
So you can,
can we want to map them together?

500
00:27:36.751 --> 00:27:39.390
Like what is the mapping?
Like what does that,
what is it correlation,

501
00:27:39.391 --> 00:27:41.910
what is the relationship between these two sequences?

502
00:27:42.150 --> 00:27:44.640
And neural nets are universal.
Approximators.

503
00:27:44.850 --> 00:27:47.410
We can find a mapping between Lauren,

504
00:27:47.430 --> 00:27:52.050
a function that maps between two different values or sequences.

505
00:27:52.590 --> 00:27:57.410
Okay.
So we can find placeholders,
uh,
for both of these.

506
00:27:59.670 --> 00:28:00.750
Uh,
and uh,

507
00:28:01.560 --> 00:28:01.930
<v 2>okay.</v>

508
00:28:01.930 --> 00:28:05.020
<v 0>Right?
So batch y placeholder.
Okay,
great.
And then you try it.</v>

509
00:28:05.320 --> 00:28:07.330
Let me make sure I mean that right?
Okay,
great.

510
00:28:07.660 --> 00:28:11.650
So then backtracks not defined because I need to say a batch x is placeholder

511
00:28:11.950 --> 00:28:14.170
and here we go.
So that was it for that.

512
00:28:14.590 --> 00:28:19.590
And then they said that access equals wine access equals one is the value for

513
00:28:21.191 --> 00:28:25.540
the unpacking of everything.
Unpacking the values.

514
00:28:26.170 --> 00:28:30.790
What are you talking about here?
Access as book one knots in negative one to one.

515
00:28:31.690 --> 00:28:34.670
And here we go.
Let's see

516
00:28:36.720 --> 00:28:39.880
you crazy.
You crazy.
Python interpreter.
Are you crazy?

517
00:28:41.460 --> 00:28:43.680
<v 2>Okay,
we're talking about here.</v>

518
00:28:47.640 --> 00:28:48.870
<v 0>Okay.
So,
um,</v>

519
00:28:49.630 --> 00:28:50.010
<v 2>okay,</v>

520
00:28:50.010 --> 00:28:52.590
<v 0>interesting.
So what's happening here is the,</v>

521
00:28:54.890 --> 00:28:55.780
does how you shaped,

522
00:28:58.730 --> 00:28:59.563
<v 2>let's see,</v>

523
00:29:02.140 --> 00:29:04.600
<v 0>should I debug this guys or should I keep going?
This is your call.</v>

524
00:29:05.100 --> 00:29:07.240
I think I can continue to debug this fire.

525
00:29:07.241 --> 00:29:12.140
I also don't want to waste people's time.
Okay.

526
00:29:12.200 --> 00:29:15.830
So what we're going to do now is we're going to

527
00:29:17.970 --> 00:29:22.010
think about,
it's,

528
00:29:22.190 --> 00:29:26.750
so what is our input here?
Debug.
Excellent.
He bought,
keep going.
Debug.

529
00:29:26.940 --> 00:29:31.550
People are okay.
So deeper.
Awesome.
No debugging,
man.
People are split.

530
00:29:31.551 --> 00:29:34.850
I just got to make a call here.
Keep going.
You bug.
It's helpful.
This debug.
Okay,

531
00:29:34.851 --> 00:29:37.640
so a 50,
let's go.
Let's,
let's see.
Buckets.
So,

532
00:29:39.080 --> 00:29:43.720
so what did we get it?
We gave it batch x and then batch y placeholder.
Oh,
in the,

533
00:29:43.721 --> 00:29:47.170
oh,
so,
um,
so let's see.

534
00:29:47.380 --> 00:29:50.950
So these placeholder values could,
could be updated,
right?
So,

535
00:29:51.950 --> 00:29:52.783
<v 1>okay.</v>

536
00:29:52.960 --> 00:29:57.430
<v 0>Okay.
So what are these placeholder values?
So,
um,
for our placeholder values,</v>

537
00:29:57.940 --> 00:29:58.810
it's update these

538
00:30:00.230 --> 00:30:01.063
<v 2>and then</v>

539
00:30:03.310 --> 00:30:04.720
<v 0>let's see,
do that again</v>

540
00:30:06.380 --> 00:30:09.400
<v 2>again,
again.</v>

541
00:30:10.860 --> 00:30:12.960
<v 0>Great.
Okay.
Okay.
So it was as place holders.</v>

542
00:30:12.961 --> 00:30:15.600
What happened was for the placeholders,
I um,

543
00:30:16.890 --> 00:30:17.470
<v 1>yeah,</v>

544
00:30:17.470 --> 00:30:20.090
<v 0>I,
I need you to define a type.
Okay.
So let's keep going.</v>

545
00:30:20.330 --> 00:30:25.310
Do you blog and send nudes?
Oh my God,
we've got some.
Okay,

546
00:30:25.311 --> 00:30:26.750
so here we go.
So

547
00:30:27.360 --> 00:30:27.630
<v 2>yeah,</v>

548
00:30:27.630 --> 00:30:29.670
<v 0>current back,
split into cloud.
So this is what it looks like,
right?</v>

549
00:30:29.671 --> 00:30:32.880
So we've split,
uh,
are two dimensional,

550
00:30:32.970 --> 00:30:37.650
up our matrix into feasible one dimensional arrays.

551
00:30:37.680 --> 00:30:42.380
That's what we've,
that's what we've done.
Okay.
So
that's all you're done.

552
00:30:42.440 --> 00:30:45.890
So now what we're going to do is we're going to,
let's see,

553
00:30:45.930 --> 00:30:48.800
what does the next day,
now we're going to,
okay,
so this is the cool part.

554
00:30:49.040 --> 00:30:52.880
This is the part that I've been waiting for.
This is the part,
okay.
This,

555
00:30:53.010 --> 00:30:56.060
this is the part right here,
the forward pass.

556
00:30:56.270 --> 00:31:00.710
This is what we learned about memory and how memory is incorporated into a

557
00:31:00.711 --> 00:31:02.990
recurrence net.
That will,
we're about to do.
Okay.

558
00:31:02.991 --> 00:31:07.260
So pay attention to this because this shit is awesome.
So for our four passes,

559
00:31:07.400 --> 00:31:09.680
we're going to start off by defining art placeholders,
right?

560
00:31:09.860 --> 00:31:12.260
So are place holders that we're going to update over time.
Now,

561
00:31:12.261 --> 00:31:16.070
we could do this inside and of our computation graph,

562
00:31:16.460 --> 00:31:20.270
but we're going to do it beforehand.
Now,
I mean,
we could just have a gigantic,
uh,

563
00:31:20.480 --> 00:31:22.130
computation,
right?
But we're going to do this beforehand.

564
00:31:22.460 --> 00:31:26.120
So we'll define our current state that is the state of the Corinthian layer as

565
00:31:26.121 --> 00:31:30.550
the initial state that we defined beforehand.
And it's about recurrent nets.

566
00:31:30.770 --> 00:31:34.310
So what we're going to do is we're going to track this series of states over

567
00:31:34.311 --> 00:31:39.140
time,
and we're going to store it in this array.
So it's not magic guys.

568
00:31:39.141 --> 00:31:42.590
It's not magic how we remember this,
uh,
hidden state.

569
00:31:42.591 --> 00:31:45.170
We store it in an in memory array.

570
00:31:45.560 --> 00:31:49.550
So let's go ahead and do this forward paths,
right?
So remember,

571
00:31:49.551 --> 00:31:50.390
whenever we're tray,

572
00:31:53.010 --> 00:31:55.770
this is why we're current nets are all about,
so what this is,

573
00:31:55.771 --> 00:31:58.470
is it a Ford Pass through the network?
This is bright.

574
00:31:58.471 --> 00:32:02.280
We have a forward pass and then a backward pass.
We,
oh,
we update our weight,

575
00:32:02.520 --> 00:32:05.250
right?
So what we're going to have is our Ford Kaspersky.

576
00:32:05.460 --> 00:32:09.060
So for all of the inputs that we have that we defined beforehand,

577
00:32:09.420 --> 00:32:12.300
we're going to take the Caribbean quits.
Okay?
And we'll,
we're,
we're gonna,

578
00:32:12.301 --> 00:32:12.840
we're gonna,

579
00:32:12.840 --> 00:32:16.460
we're gonna take this input and we're gonna feed it through our network,
right?

580
00:32:16.800 --> 00:32:19.350
And we define our weights and biases.
We can just give it to that.

581
00:32:19.351 --> 00:32:21.990
So we're going to take the current influx and we're going to,
first of all,

582
00:32:21.991 --> 00:32:24.300
we're going to reshape it into eight stock,

583
00:32:24.390 --> 00:32:27.880
into a shape that are recurring that accepts.
So it's going to be,
we're going to,

584
00:32:28.020 --> 00:32:31.690
we're going to reshape that currently,
but into the back sides.
Okay?

585
00:32:33.170 --> 00:32:33.540
<v 1>Okay.</v>

586
00:32:33.540 --> 00:32:34.910
<v 0>Could you,
the giving back side.</v>

587
00:32:36.370 --> 00:32:37.203
<v 2>And</v>

588
00:32:37.520 --> 00:32:39.590
<v 0>so this is the shape one.
Okay?</v>

589
00:32:39.591 --> 00:32:42.180
So this is the shape that we're going to give it to him.
And then,
uh,

590
00:32:42.290 --> 00:32:44.870
what we're going to do guys,
we're going to be so many gaming videos later on.

591
00:32:45.050 --> 00:32:48.620
We just need to get through this course first.
Okay?
And

592
00:32:50.620 --> 00:32:54.660
then what we're going to do is we're gonna mix both to include state and the,
uh,

593
00:32:54.920 --> 00:32:55.960
implicated,
right?

594
00:32:55.961 --> 00:33:00.300
So we're going to say input state and input and state concatenated with this

595
00:33:00.310 --> 00:33:04.060
variable gets it is it's going to be the concatenation,

596
00:33:04.061 --> 00:33:08.530
which is a computer science term for combining two values together.
Great words.

597
00:33:08.531 --> 00:33:09.610
Use it in real life as well.

598
00:33:10.000 --> 00:33:14.380
We're going to combine both the current inputs or does that,
is that current,

599
00:33:14.760 --> 00:33:19.180
uh,
inputs,
uh,
which is the data and the current state.

600
00:33:19.480 --> 00:33:23.200
And so how do we combine these two values to combine these two values?

601
00:33:24.950 --> 00:33:26.390
We're going to use the concat function,

602
00:33:26.510 --> 00:33:30.860
which basically combines two 10 shirts together.
And so,

603
00:33:31.010 --> 00:33:35.750
and then what we do and then what we do okay,
is we're going to perform,
are,

604
00:33:35.780 --> 00:33:40.340
uh,
four task,
major qualifications to calculate our next stage.
So what do we do?

605
00:33:40.460 --> 00:33:43.520
Well,
first of all,
we're going to perform to do this.
We're going to perform a,

606
00:33:44.900 --> 00:33:48.590
let's talk about this.
Let's talk before.
So a nonlinearity,

607
00:33:48.591 --> 00:33:49.970
which is the Tan h function.

608
00:33:50.120 --> 00:33:52.780
But before we squash our data with our nonlinearities,

609
00:33:52.880 --> 00:33:55.640
we're going to perform our matrix multiplication.

610
00:33:55.820 --> 00:33:58.850
That is what is happening at every layer of,
of a network,
right?

611
00:33:58.940 --> 00:34:03.020
It is a series of Ma Matrix operations that are happening,

612
00:34:03.170 --> 00:34:06.050
whether it be multiplying those weights by the input data,

613
00:34:06.110 --> 00:34:08.030
whether it be pooling those values,

614
00:34:08.120 --> 00:34:11.240
whether it be convoluting them in a convolutional nets.

615
00:34:11.700 --> 00:34:16.250
It's all matrix multiplication.
It's all linear Algebra,
okay?

616
00:34:16.280 --> 00:34:19.670
So we're going to perform matrix multiplication and tensorflow has this does,

617
00:34:19.671 --> 00:34:24.671
it's very easy for this with this matrix multiplied status and we're going to

618
00:34:24.801 --> 00:34:26.420
concatenate,
uh,

619
00:34:26.480 --> 00:34:29.660
we're going to take that input and state value that's concatenated and we're

620
00:34:29.661 --> 00:34:32.570
going to perform that matrix multiplication with the weight,
right?

621
00:34:32.571 --> 00:34:35.570
The initial weight.
So that's,
this is what's happening in the forward cast.

622
00:34:35.630 --> 00:34:38.600
At the first part of our recurring that this is what's happening in that first

623
00:34:38.601 --> 00:34:43.070
part.
And
we're going to add the bias,
okay?

624
00:34:43.430 --> 00:34:45.700
And that's what's gonna give us our next stage.
Now,

625
00:34:45.710 --> 00:34:48.380
what are we going to do with this state?
Right?
Well,
we're going to take,

626
00:34:48.740 --> 00:34:50.420
we're going to take that matrix,
multiply value,

627
00:34:50.570 --> 00:34:53.900
and we're going to squash it with a nonlinearity,
which is what we always do,

628
00:34:53.901 --> 00:34:56.870
usually in a,
in a,
in a,
not just a recurring that,

629
00:34:56.871 --> 00:34:59.810
but any kind of neural networks.
We have some matrix multiplication,

630
00:35:00.170 --> 00:35:04.100
then followed by a nonlinear.
So this could be a Tan h function.

631
00:35:04.250 --> 00:35:07.940
It could be a sigmoid function,
it could be,
um,
uh,
you know,
what else is it?

632
00:35:07.941 --> 00:35:10.580
Relu there's a lot of different options.
In this case,

633
00:35:10.581 --> 00:35:12.290
we're gonna use a 10 age function,
okay.

634
00:35:12.770 --> 00:35:16.640
Which is generally we see 10 eights use a lot in recurrent nets specifically.

635
00:35:16.940 --> 00:35:21.340
Okay?
So,
yeah,
we've got a band,
these spammers,

636
00:35:21.380 --> 00:35:22.330
so we're going to shoot.

637
00:35:22.340 --> 00:35:26.300
So sigmoids are great for generating output probabilities.

638
00:35:26.330 --> 00:35:28.140
So whether it's a convolutional net or Trenton,

639
00:35:28.160 --> 00:35:30.680
it generally we see sigmoid at the end,

640
00:35:30.740 --> 00:35:33.470
at the last convolutional block or the last,
uh,

641
00:35:33.560 --> 00:35:35.760
Neulasta layer of a network to just output that,

642
00:35:35.960 --> 00:35:40.230
that value that's between zero and one that describes our output probability.

643
00:35:41.160 --> 00:35:45.650
And sigmoid takes a while to converge.
Tan H is better for these?
Uh,
it's,

644
00:35:45.660 --> 00:35:50.040
it's faster.
Okay.
Okay.
So then,
okay,
so here's the cool part.

645
00:35:50.130 --> 00:35:54.030
So we define it speaks series,
empty list at the beginning.

646
00:35:54.031 --> 00:35:57.450
Right now we're going to append it with the state that we calculate it.

647
00:35:57.750 --> 00:35:59.720
So we're going to append it with a next date value.

648
00:36:00.120 --> 00:36:02.810
We're going to keep doing this,
right?
So eventually this,

649
00:36:02.950 --> 00:36:07.680
this list is going to be filled with a sequence of,
of learn hidden states.

650
00:36:07.860 --> 00:36:10.350
This is the memory aspect of work for neck.

651
00:36:10.500 --> 00:36:13.560
We are remembering every step through times we're going to,

652
00:36:13.561 --> 00:36:16.980
we're going to remember every hidden states values and it's not steady.

653
00:36:16.990 --> 00:36:20.010
It just remembers it and throws it into space.

654
00:36:20.040 --> 00:36:22.380
It stores them in a list,

655
00:36:22.500 --> 00:36:25.440
it stores them in memory and in memory data structure.

656
00:36:26.160 --> 00:36:29.670
And then once we've done that,
we've set the current state to the next state.

657
00:36:29.790 --> 00:36:34.370
So as you recall from data structures and algorithms,
this is very similar to um,

658
00:36:34.650 --> 00:36:39.630
you know,
uh,
a stack or a tree where we set the current to the next,

659
00:36:39.780 --> 00:36:43.610
right?
It's,
it's,
it's kind of similar to that.
It's very similar logic of,

660
00:36:43.611 --> 00:36:45.210
of continuously updating.

661
00:36:46.890 --> 00:36:47.723
<v 2>Okay.</v>

662
00:36:48.390 --> 00:36:51.960
<v 0>So,
so that's what that is.
So that time forward pass guys,</v>

663
00:36:51.961 --> 00:36:52.920
that's our forward pass.

664
00:36:53.250 --> 00:36:57.750
And what what is going to do is it's going to give us all of those hidden state

665
00:36:57.780 --> 00:37:01.440
and it's going to save them in memory.
It's going to save them in memory.
Okay.

666
00:37:01.500 --> 00:37:05.730
So and watched there be an error.
Okay.
Batches.

667
00:37:05.731 --> 00:37:08.100
Size is not defined online.
Eight.

668
00:37:10.160 --> 00:37:14.510
Unfortunately I don't have one line numbers in,
in the Pi Python notebooks,

669
00:37:14.720 --> 00:37:19.250
but uh,
let's see.
Uh,
batches,
size,
batches,
size,

670
00:37:19.251 --> 00:37:20.660
batches,
size,
what do you got your badge.

671
00:37:20.670 --> 00:37:23.030
So I'm going to do a five minute Q and a after this as well.
Okay.

672
00:37:23.031 --> 00:37:26.370
So stick to this and we'll get the primary curious.

673
00:37:26.371 --> 00:37:30.080
So batches size is going to be,
where do we have batch size?

674
00:37:34.110 --> 00:37:34.943
<v 2>Oh,</v>

675
00:37:36.190 --> 00:37:39.970
<v 0>current input.
Oh,
right.
Back side.
Not that to size.
Dog.</v>

676
00:37:41.480 --> 00:37:45.020
State series is not defined states.
Where are we?

677
00:37:45.260 --> 00:37:50.090
So that's where that append,
it's called state series.
Duh.
Okay,

678
00:37:50.570 --> 00:37:55.040
great.
Okay,
so we defined that.
Look at Chris' comment.
Batches.
Typo.
Thank you.

679
00:37:55.370 --> 00:38:00.080
It should be the half size.
It should be.
I mean,
I mean,
thank you.
Great.

680
00:38:00.110 --> 00:38:03.020
So we've done that.
And here's an image of what is happening,
right?

681
00:38:03.110 --> 00:38:05.390
So this is a way of thinking about it,
right?
So we have,

682
00:38:06.200 --> 00:38:10.250
we're concatenating the current inputs with the current state die and the

683
00:38:10.251 --> 00:38:14.180
concatenation occurs to a matrix multiplication operation.

684
00:38:14.360 --> 00:38:17.990
And then we squash it with the nonlinearity.
But before we squash it,

685
00:38:18.230 --> 00:38:20.850
we multiply it by that week though.
Okay.
So it's,

686
00:38:20.851 --> 00:38:23.910
it's a matrix multiplication and then we uh,

687
00:38:24.250 --> 00:38:28.220
squash it with the nonlinearity and we get that next state and we're storing all

688
00:38:28.221 --> 00:38:30.590
of those states through time.
Okay.

689
00:38:31.170 --> 00:38:35.000
This is the state saved for all hidden layers or only for some,

690
00:38:35.140 --> 00:38:39.700
it is saved for all hidden layers.
It is safe for all in layers now.

691
00:38:41.980 --> 00:38:43.300
Okay.
Okay.

692
00:38:46.430 --> 00:38:50.960
But it's,
but,
but not in this case.
Not In this case.

693
00:38:51.020 --> 00:38:55.010
We could,
but that would be very computationally expensive.
Right to store.

694
00:38:55.120 --> 00:38:57.470
If our data was huge for them,
it was huge.

695
00:38:57.530 --> 00:39:01.340
Storing all those hidden states would be very computationally expensive.
So,

696
00:39:01.490 --> 00:39:05.060
and now I'm going to explain why we use that,
that value,

697
00:39:05.061 --> 00:39:08.780
which was truncated that proper length.
So the trunk he departments,

698
00:39:08.900 --> 00:39:12.410
we're only going to save portions of that in state,

699
00:39:12.950 --> 00:39:17.540
that sequence at a time and we're going to discard the old oldest values over

700
00:39:17.541 --> 00:39:21.440
time.
Right?
So it's similar to,
so it's similar to the vanishing,

701
00:39:21.620 --> 00:39:25.130
remember has advantage in gradient problem,
the error becomes

702
00:39:26.720 --> 00:39:27.780
less.
Uh,

703
00:39:28.840 --> 00:39:29.180
<v 1>okay.</v>

704
00:39:29.180 --> 00:39:33.860
<v 0>The error value is
less,
a bit is sent back over time.</v>

705
00:39:33.980 --> 00:39:34.813
So

706
00:39:35.770 --> 00:39:36.050
<v 1>okay,</v>

707
00:39:36.050 --> 00:39:37.490
<v 0>what we're going to do is we're going,
we're not,</v>

708
00:39:37.940 --> 00:39:41.000
we're not going to save all those states because these layers are not going to

709
00:39:41.001 --> 00:39:42.440
be enrolled at the beginning of time.

710
00:39:42.650 --> 00:39:47.420
We're going to truncate at a limited number of time steps.
So it's only going a,

711
00:39:47.580 --> 00:39:50.480
we're going to do this.
It didn't,
it didn't happen yet.
It will.

712
00:39:50.780 --> 00:39:53.270
They will continually do this.
Okay.

713
00:39:56.220 --> 00:39:57.960
Much love Ai Aficionados.
Okay,

714
00:39:57.961 --> 00:40:02.640
so now we're going to do the last step and we're going to minimize the loss.

715
00:40:02.730 --> 00:40:06.660
Okay?
So let's go ahead and we have our hidden states would calculate it.

716
00:40:06.690 --> 00:40:10.560
We've done our forward pass and now we're going to do are backward pass.
Okay?

717
00:40:10.620 --> 00:40:13.550
So this is the learning step.
This is the learning suddenly role.

718
00:40:13.920 --> 00:40:16.710
I always roll up my sleeves for the,
for the fun parts.
So,

719
00:40:16.830 --> 00:40:21.420
so let's calculate our loss first.
So to calculate our loss,
but when you'd say,

720
00:40:21.480 --> 00:40:24.690
look what we're going to define a variable called logics series.

721
00:40:24.900 --> 00:40:28.470
And inside of logic series we're going to say we're going to perform matrix

722
00:40:28.471 --> 00:40:32.940
multiplication between our state value and the next way down,
right?

723
00:40:33.000 --> 00:40:35.880
So we did it for,
so we've done board past,

724
00:40:35.940 --> 00:40:39.600
we've done half a forward ass and now has to do that second part of the four

725
00:40:39.601 --> 00:40:43.720
paths,
right?
Actually we're not that,
we're not fully done with four yet.

726
00:40:43.780 --> 00:40:44.720
We've done that first part right now.

727
00:40:44.721 --> 00:40:47.980
I'm going to do that last part until it gets the output and then we'll do our a

728
00:40:47.981 --> 00:40:50.900
backward pass.
Okay.
We are yet,

729
00:40:50.920 --> 00:40:53.740
we are absolutely doing backpropagation for this and I'll talk about that when

730
00:40:53.741 --> 00:40:55.900
we get to it.
So this is our next step right there.

731
00:40:55.901 --> 00:40:58.580
Our second set of weights and art second set of,
uh,

732
00:40:59.490 --> 00:41:01.360
so this is our second set of biases.

733
00:41:01.361 --> 00:41:06.361
So for every state we have in that states series of risks that we,

734
00:41:07.120 --> 00:41:11.560
that we calculated,
we're going to Tuck it,
calculate the legit serious.

735
00:41:11.561 --> 00:41:15.070
So when gets his sword for logistic transformed.
Okay.

736
00:41:15.071 --> 00:41:16.570
And so what we're gonna do with this,

737
00:41:16.720 --> 00:41:20.200
these are the values that we are going to squash at the end.

738
00:41:20.201 --> 00:41:21.220
These are the outputs.

739
00:41:21.410 --> 00:41:24.340
This is the output values that we're going to squash at the very end,

740
00:41:24.341 --> 00:41:27.730
but they softmax and that's going to give us our predictions.
Okay.

741
00:41:27.940 --> 00:41:29.170
So all of our logic,

742
00:41:29.171 --> 00:41:32.830
we're going to squash with an ending nonlinearities which is going to be a

743
00:41:33.650 --> 00:41:34.850
function.
Okay.

744
00:41:34.851 --> 00:41:38.750
This is our ending nominee area and tensorflow has this great bill can softmax

745
00:41:38.751 --> 00:41:43.130
function for lodge?
It's been logics series.

746
00:41:43.340 --> 00:41:47.870
Okay.
So this is going to be our list of all of our predictions.
Okay.

747
00:41:47.871 --> 00:41:48.704
Over time.

748
00:41:50.450 --> 00:41:51.283
<v 2>Okay.</v>

749
00:41:53.970 --> 00:41:56.700
<v 0>We,
yes,
we always use backpropagation for training.</v>

750
00:41:56.701 --> 00:41:59.280
Backpropagation is the way to train neural networks,

751
00:41:59.340 --> 00:42:03.240
except I've only seen one paper that didn't do it,
that deepmind release,

752
00:42:03.420 --> 00:42:07.530
which was called synthetic gradients and they didn't really use it a more

753
00:42:07.531 --> 00:42:10.440
afterwards,
which was crazy because as the time I was really excited.
I was like,

754
00:42:10.470 --> 00:42:13.880
oh shit,
back propagation is obsolete now.
But like in their,
uh,

755
00:42:14.780 --> 00:42:18.630
in their next papers,
they didn't really use that again.
So we'll see.
So yeah,

756
00:42:18.631 --> 00:42:19.321
that propagation,

757
00:42:19.321 --> 00:42:22.530
we always use backpropagation for now until something better comes along.

758
00:42:22.531 --> 00:42:25.770
But okay.
So let's now let's measure our losses.

759
00:42:25.800 --> 00:42:29.040
We have our predictions across time and now,

760
00:42:30.150 --> 00:42:33.300
now we're going to measure our losses.
So for our losses,

761
00:42:34.790 --> 00:42:35.623
<v 2>okay,</v>

762
00:42:35.860 --> 00:42:40.860
<v 0>we're going to use the sports softmax cross entropy with logics function.</v>

763
00:42:41.291 --> 00:42:45.490
Now talk about long methods names,
right?
Anybody's used objective seat.

764
00:42:45.640 --> 00:42:47.080
You are right at home right now.

765
00:42:47.230 --> 00:42:51.420
We have a very long variable name here that I'd like to explain and we'll let

766
00:42:51.421 --> 00:42:56.410
you feed it.
What it's going to do is going to calculate our losses using okay.

767
00:42:56.680 --> 00:42:57.513
And

768
00:42:57.830 --> 00:42:58.280
<v 2>okay.</v>

769
00:42:58.280 --> 00:43:02.150
<v 0>Okay.
So what do we got here for all of our largest labels for,</v>

770
00:43:03.130 --> 00:43:03.490
<v 2>okay.</v>

771
00:43:03.490 --> 00:43:08.190
<v 0>All of logic
and is it in the zip?</v>

772
00:43:09.220 --> 00:43:09.570
<v 2>Okay.</v>

773
00:43:09.570 --> 00:43:14.570
<v 0>Roger [inaudible] series labels back complicate to update weights back,</v>

774
00:43:15.341 --> 00:43:16.180
complicate the update.

775
00:43:16.260 --> 00:43:20.200
So I was actually asked to speak at this galvanize a conference and it was like

776
00:43:20.230 --> 00:43:23.140
the head as a speaker and this and they had all these like lead data,

777
00:43:23.620 --> 00:43:25.120
lead data scientist at Airbnb.

778
00:43:25.240 --> 00:43:27.760
We data scientists at square and they asked me to rap.
So it's like,

779
00:43:28.080 --> 00:43:30.880
and it's a Roger Apps.
So I was like,
Oh,
let me think about it.

780
00:43:30.881 --> 00:43:34.040
So we'll see how that goes.
Maybe I'm busy right now.
Okay.
So,

781
00:43:34.041 --> 00:43:35.440
so here's what this line does.

782
00:43:36.250 --> 00:43:38.570
<v 2>He's what crystalline does.
Okay.</v>

783
00:43:39.880 --> 00:43:42.810
<v 0>So this is going to return a tensor.
So actually what this does,</v>

784
00:43:42.811 --> 00:43:45.630
it's going to read.
So it's going to do exactly what we did before.

785
00:43:45.690 --> 00:43:50.100
And this line right here,
it's going to compute the Softmax,
uh,
into,

786
00:43:50.101 --> 00:43:52.860
so we're actually doing this twice.
It's going to compute the softmax,

787
00:43:52.870 --> 00:43:56.340
it's going to compute those probabilities for the logic and the labels.

788
00:43:56.400 --> 00:43:59.430
So for a sequence and the,
for both of our sequences,
right?

789
00:43:59.460 --> 00:44:02.040
And then it's been performed cross entropy on them.

790
00:44:02.160 --> 00:44:06.480
And so cross entropy measures the difference between two probability

791
00:44:06.481 --> 00:44:07.650
distributions.
Okay?

792
00:44:07.810 --> 00:44:11.940
Is a method of measuring the difference between two probability distributions

793
00:44:12.150 --> 00:44:14.340
and that's going to,
that's going to be our loss.

794
00:44:14.550 --> 00:44:16.310
That difference is what we want to minimize.

795
00:44:16.330 --> 00:44:20.760
We want to minimize the difference between our output probabilities and our

796
00:44:21.240 --> 00:44:24.630
sequence,
our label sequences,
right?
I'm going to that Echo,
right?

797
00:44:24.840 --> 00:44:26.010
We want to minimize it different.

798
00:44:26.040 --> 00:44:28.680
We're going to minimize it to an optimization function.

799
00:44:28.681 --> 00:44:33.120
Now whenever you think about neural networks,
everything,
it's about optimization.

800
00:44:33.330 --> 00:44:35.970
It's about measuring an error value and optimizing it.

801
00:44:36.000 --> 00:44:38.390
Everything is an optimization.
Everything is optimization.

802
00:44:38.510 --> 00:44:42.000
And in fact in whites you start thinking about everything in terms of losses and

803
00:44:42.001 --> 00:44:45.000
optimizations,
whether it's a relationship with somebody,
like how do you,

804
00:44:45.210 --> 00:44:48.780
how do you minimize your losses?
How do you,
um,
whatever you're doing,

805
00:44:48.781 --> 00:44:52.740
a sequence of steps in life.
You know,
you want to argue an optimizer,

806
00:44:52.741 --> 00:44:54.450
I'm getting a little off track.
So back to this,

807
00:44:55.860 --> 00:44:56.090
<v 1>okay,</v>

808
00:44:56.090 --> 00:44:57.740
<v 0>now we're going to minimize these losses.</v>

809
00:44:57.741 --> 00:45:00.230
So this is going to calculate a list of losses.

810
00:45:00.260 --> 00:45:04.310
And what we're going to do is we're going to use the reduced mean function,

811
00:45:06.520 --> 00:45:10.590
what you were using the produce mean function too.

812
00:45:12.900 --> 00:45:13.690
<v 1>Okay.</v>

813
00:45:13.690 --> 00:45:18.220
<v 0>Compute the average of those losses,
okay?
And you're one value,
okay?</v>

814
00:45:18.400 --> 00:45:21.640
That's our total loss that we've measured.
Okay?
After one,

815
00:45:21.641 --> 00:45:25.860
full forward propagation,
this total loss is the,
is the,

816
00:45:25.870 --> 00:45:28.690
is the output loss that we want to minimize,
okay?

817
00:45:31.650 --> 00:45:32.483
<v 1>Okay.</v>

818
00:45:32.840 --> 00:45:33.673
<v 0>Okay.</v>

819
00:45:34.390 --> 00:45:34.890
<v 1>Okay.</v>

820
00:45:34.890 --> 00:45:35.820
<v 0>The total loss.</v>

821
00:45:35.970 --> 00:45:39.270
Now what we're going to do is we're going to do our optimization set.

822
00:45:39.271 --> 00:45:41.970
No trends or cloak has a bunch of built in,

823
00:45:42.490 --> 00:45:45.690
has a bunch of built in optimization steps.
And

824
00:45:47.400 --> 00:45:51.180
what we're going to do is worth you add or Grad,
no added Grad is a,

825
00:45:51.240 --> 00:45:54.120
is a really cool,
it's really cool.
Um,

826
00:45:55.430 --> 00:45:56.080
<v 1>yeah,</v>

827
00:45:56.080 --> 00:45:59.530
<v 0>let me just type this up.
Total blocks.
Okay.
So that's,</v>

828
00:45:59.531 --> 00:46:03.190
that's the input that we get.
So Adam Grant is a really cool,
uh,

829
00:46:06.030 --> 00:46:07.170
optimization technique.
Okay?

830
00:46:07.171 --> 00:46:09.780
So it's very similar to use the castic gradient descent.

831
00:46:09.810 --> 00:46:12.250
So what happens is there's some features whenever,

832
00:46:12.300 --> 00:46:13.640
so here's why we use Adam gripes.

833
00:46:13.710 --> 00:46:17.280
Sometimes there are features that are very important,

834
00:46:17.490 --> 00:46:19.790
but for s for whatever reason or neural,

835
00:46:19.840 --> 00:46:22.570
that doesn't weigh them properly because it,

836
00:46:22.920 --> 00:46:26.910
because it uses the same learning rates across.
So 0.3 by the way,

837
00:46:26.911 --> 00:46:28.980
is our learning right there.
We're feeding this added right optimize.

838
00:46:29.190 --> 00:46:30.690
So for some reason it doesn't,

839
00:46:30.691 --> 00:46:33.690
the learning rate is the same across all of our features.

840
00:46:33.720 --> 00:46:34.980
So for some reason it doesn't weigh.

841
00:46:34.981 --> 00:46:38.310
Those are very important features properly because it's sparse.

842
00:46:38.490 --> 00:46:42.710
Sometimes sparse features can still be a very important so that,

843
00:46:42.780 --> 00:46:46.500
so that is one of the cruxes of neural networks.
If feature is sparse,

844
00:46:46.590 --> 00:46:49.740
that means there's not a lot of data in a feature,
but it's still important,

845
00:46:49.920 --> 00:46:51.330
then it won't recognize it properly.

846
00:46:51.330 --> 00:46:55.920
So it added ride does is it uses different learning rates for those sparse out

847
00:46:56.040 --> 00:46:59.400
teachers.
And what this does is it gives them more weight.

848
00:46:59.520 --> 00:47:02.850
It gives them more importance in training.
Okay.

849
00:47:02.851 --> 00:47:06.540
And because we have binary data here,
we're going to use out of Brad.
Okay.

850
00:47:06.870 --> 00:47:10.590
And I have a great paper on that,
which is in the,
uh,

851
00:47:11.160 --> 00:47:15.680
in the comments of the link.
Great paper notes on added Grad.
It's actually very,

852
00:47:15.830 --> 00:47:17.340
it's,
it's,
it's not,
it's not very simple.

853
00:47:17.430 --> 00:47:20.970
It's simpler than most optimization papers to read.
So I cling to it.

854
00:47:21.060 --> 00:47:23.690
Definitely check that out after work.
The Outer Brad is,
uh,

855
00:47:23.790 --> 00:47:25.880
basically in a nutshell.
It's a,

856
00:47:26.910 --> 00:47:27.180
<v 1>okay.</v>

857
00:47:27.180 --> 00:47:28.620
<v 0>It gives voice to the little guy.</v>

858
00:47:28.680 --> 00:47:32.890
It gives voice to the little guy because the learning rate is adaptable.
Okay.

859
00:47:33.640 --> 00:47:33.950
<v 2>Okay.</v>

860
00:47:33.950 --> 00:47:35.390
<v 0>Waitstaff receive high gradients,</v>

861
00:47:35.540 --> 00:47:38.990
will have their effective learning rate reduced or weights that were too small

862
00:47:38.991 --> 00:47:43.130
or infrequent updates.
We'll have their effective learning rate increased.
Okay.

863
00:47:43.570 --> 00:47:47.880
So,
but it not like a magic wand.
Right?
It doesn't always work,
but it,
right.
So,

864
00:47:48.000 --> 00:47:50.980
so you,
oh,
we generally want to try out different optimizations,

865
00:47:50.981 --> 00:47:53.390
sex and having one line,
right?

866
00:47:53.391 --> 00:47:55.370
We can just implement our optimization in one line.

867
00:47:55.550 --> 00:47:58.790
It makes it really easy to try out different optimization steps.
Right?

868
00:47:58.970 --> 00:48:03.200
It's all about experimentation and trying out different options and seeing what

869
00:48:03.201 --> 00:48:07.700
works best.
Okay.
So that's it for our loss and then minimizing it.

870
00:48:10.500 --> 00:48:14.910
Okay.
State series is not defined here because,
okay.
Because

871
00:48:16.660 --> 00:48:21.660
state series not states.
Great.
Okay.
So then,
so that's the,

872
00:48:21.680 --> 00:48:25.810
so that's that.
Now this is the visualization stuff.

873
00:48:25.811 --> 00:48:28.570
So I'm actually not going to code out this visualization stuff because it's all,

874
00:48:28.750 --> 00:48:31.210
it's all,
you know,
it's,
it's not machine learning.
Okay?
So I'll skip this part,

875
00:48:31.240 --> 00:48:33.780
but basically this is going to visualize what we see.
Okay.

876
00:48:33.920 --> 00:48:34.541
And what is there to do?

877
00:48:34.541 --> 00:48:38.380
It's going to plot the training and the training output and the current

878
00:48:38.381 --> 00:48:42.370
prediction.
It's going to plot out all three of those things.
Okay?

879
00:48:43.680 --> 00:48:47.850
So that's a visualization step.
Now we're waiting to get to the training,
the,
the,

880
00:48:47.880 --> 00:48:50.850
the computation graph.
Now we've defined the four tasks.

881
00:48:50.940 --> 00:48:53.340
We defined the backpropagation through that and you know,

882
00:48:53.341 --> 00:48:58.330
the loss function and in the back of obligation step,
which is the,
uh,
the uh,

883
00:48:58.670 --> 00:49:01.990
Outer Brad,
right?
So that is the,
that is the optimization step two back property.

884
00:49:02.140 --> 00:49:03.550
Now we're going to train the network,

885
00:49:03.551 --> 00:49:06.520
I'm going to use those values and we've calculated through the forward and

886
00:49:06.521 --> 00:49:09.550
backward pass to do this training step.
Okay?

887
00:49:11.090 --> 00:49:13.990
So in contraflow we always to find a section,
right?

888
00:49:14.170 --> 00:49:18.940
10 sections encapsulate our graph.
I Ho thumb.
So entrance.

889
00:49:18.941 --> 00:49:23.941
So sessions and capsulate our brand and we'll say session not run and what

890
00:49:25.131 --> 00:49:26.720
wouldn't.
And so this is a really stupid step.

891
00:49:26.750 --> 00:49:28.910
I don't know why we have to do this.
I mean,

892
00:49:28.911 --> 00:49:31.430
I don't see why it should just know that we all the variables,

893
00:49:31.431 --> 00:49:34.750
we've initial idea for a part of this graph.
What else would we use them for it?

894
00:49:34.751 --> 00:49:38.570
So you know,
not,
not.
So remember not all code,
all code was written by people.

895
00:49:38.660 --> 00:49:41.900
Nothing is perfect.
Nothing was sent by the gods,
but so,
right?

896
00:49:41.901 --> 00:49:46.550
So it just shows that everything is not perfect.
Okay?
So,

897
00:49:48.170 --> 00:49:49.003
<v 2>okay,</v>

898
00:49:50.370 --> 00:49:51.830
<v 0>so we're going to run this session,
right?</v>

899
00:49:51.831 --> 00:49:54.890
Using the variables that we've initialized beforehand.
Okay?

900
00:49:55.160 --> 00:49:59.240
So now we're going to say,
let's define our plot.

901
00:49:59.550 --> 00:50:01.700
It's going to be an interactive mode.
And then we're going to say,

902
00:50:01.730 --> 00:50:06.490
I'm listening to Sheila as a figure,
initialize that bigger value.

903
00:50:07.450 --> 00:50:10.690
And then we're going to say,
show the graph,
show the graph,

904
00:50:11.170 --> 00:50:13.580
and then we're going to say the loss decrease.
We,

905
00:50:13.590 --> 00:50:17.050
we're going to map a lost over time so we can print them out,
right?

906
00:50:17.051 --> 00:50:20.590
We want to show all the lost.
I used a decreasing over time.

907
00:50:20.740 --> 00:50:22.540
So we'll initialize it as an empty list.

908
00:50:22.720 --> 00:50:25.870
Now we're going to go ahead and do art training,
iteration set.
So right,

909
00:50:26.080 --> 00:50:27.830
so we defined those detox,
right?

910
00:50:27.831 --> 00:50:29.960
So the number of the talks that we had beforehand,

911
00:50:30.380 --> 00:50:35.240
and for each of those reports we're going to generate data to write.

912
00:50:35.241 --> 00:50:38.330
We define that generate,
generate data function beforehand.

913
00:50:38.600 --> 00:50:41.330
And it's when the output,
the sequences,
right this secret,

914
00:50:41.331 --> 00:50:43.610
no one is going to be that initial binary sequence.

915
00:50:43.820 --> 00:50:47.270
And the other one is going to be that copy but echoes,
right?
So it shifted over.

916
00:50:48.020 --> 00:50:52.110
Okay.
And then we're going to initialize our MP and it didn't state,

917
00:50:52.340 --> 00:50:56.780
so our current state is going,
we knows it's going to be initialized as

918
00:50:58.280 --> 00:50:58.640
<v 1>okay.</v>

919
00:50:58.640 --> 00:51:00.270
<v 0>Uh,
an empty matrix.</v>

920
00:51:00.390 --> 00:51:04.720
I'm going to use a num Pi zero function to initialize it and the state science.

921
00:51:04.770 --> 00:51:09.120
Okay,
that's our hidden state.
Now we're going to printout.
That's

922
00:51:14.140 --> 00:51:18.350
okay.
So now we're going to say for each that,
so can each of those detox,

923
00:51:18.490 --> 00:51:20.900
we have a set of batches,
right?
And so he's,

924
00:51:20.910 --> 00:51:25.480
that Jews are going to be inside of a range for all of those batches.

925
00:51:25.690 --> 00:51:27.460
And so I think we defined five batches.

926
00:51:27.670 --> 00:51:29.860
So for five batches in the number of detox,

927
00:51:31.150 --> 00:51:31.540
<v 1>okay,</v>

928
00:51:31.540 --> 00:51:35.410
<v 0>we're going to start refining the starting and ending point for the badges.
Okay?</v>

929
00:51:35.411 --> 00:51:39.640
So starting and ending point for those batches is going to use that truncated

930
00:51:39.641 --> 00:51:43.930
backstop length because we don't want to remember all of the hidden state,
right?

931
00:51:44.110 --> 00:51:47.760
We only want to remember in states,
um,
you know the,

932
00:51:47.800 --> 00:51:51.150
the most recent couldn't say because why?
Because it would be [inaudible].

933
00:51:51.370 --> 00:51:53.590
I'm going to definitely have a Q and a right after.
This is not an eight,

934
00:51:53.600 --> 00:51:56.890
not five minutes.
Okay.
Five more minutes before the two and a guys.
Okay.

935
00:51:57.250 --> 00:51:59.840
Stick with figures.
So for a truncated backup links.

936
00:52:02.560 --> 00:52:03.393
<v 1>Okay,</v>

937
00:52:03.480 --> 00:52:08.070
<v 0>we're going to say,
um,</v>

938
00:52:08.500 --> 00:52:11.680
okay,
so that's a starting point and an ending point.
And then

939
00:52:15.480 --> 00:52:17.940
I want you to find the starting point in the ending point.

940
00:52:19.110 --> 00:52:22.070
And I would've got better.
Dah,
Dah,
Dah.

941
00:52:24.990 --> 00:52:26.760
Okay.
That's why,

942
00:52:27.780 --> 00:52:31.050
why and then starts

943
00:52:33.810 --> 00:52:34.643
and,

944
00:52:39.500 --> 00:52:43.270
okay.
So these are our badges for our y.

945
00:52:52.950 --> 00:52:57.480
Okay.
So I'm reading the comments as well.
Okay.
So,
okay,
so,
um,

946
00:52:57.481 --> 00:52:58.231
so those are a bachelors.

947
00:52:58.231 --> 00:53:01.950
We defined our batches and now we're going to run the computation ground.
So,
uh,

948
00:53:04.950 --> 00:53:09.950
so if our total loss but the finer total loss and then our training staff and

949
00:53:10.451 --> 00:53:13.900
then our current stake.
So remember we define these things beforehand.

950
00:53:13.901 --> 00:53:18.820
So right now what we're doing is we're going to just run what we had beforehand

951
00:53:19.100 --> 00:53:23.800
inside of the computation graph,
leaving the session.run function.
All right,

952
00:53:23.860 --> 00:53:24.693
so then

953
00:53:29.060 --> 00:53:32.860
the fine art,
total loss or training scrap or current state.

954
00:53:33.070 --> 00:53:36.370
And it's going to get these values for each state,
right?

955
00:53:36.371 --> 00:53:41.371
So for each iteration of our training predictions series,

956
00:53:43.450 --> 00:53:44.710
boom,
boom,
boom,
boom,
boom.

957
00:53:46.220 --> 00:53:50.490
And then repeat dexa feedbacks to the feedback is how we feed in those place

958
00:53:50.491 --> 00:53:54.030
holders that we defined beforehand.
It does help once we define as place holders,

959
00:53:54.100 --> 00:53:54.933
we have to

960
00:53:58.750 --> 00:53:59.583
thank you.

961
00:54:01.270 --> 00:54:04.690
So the fee days is how we define those placeholders and how we'd be in this

962
00:54:04.691 --> 00:54:09.310
place.
Holders.
Okay.
So then in our batch x,

963
00:54:13.460 --> 00:54:15.200
okay.
And then batch x.

964
00:54:16.730 --> 00:54:19.530
That's why I told her.
Okay,

965
00:54:19.531 --> 00:54:24.090
we've got three more minutes guys before we get to the Q and a three more

966
00:54:24.091 --> 00:54:27.060
minutes.
Okay.
Stick with me here and it states,

967
00:54:28.770 --> 00:54:29.340
okay,

968
00:54:29.340 --> 00:54:34.340
so we are feeding in the whole all three batches,

969
00:54:35.101 --> 00:54:37.740
right?
So all three placeholder values for the state,

970
00:54:37.860 --> 00:54:41.460
the input data and the output sequence.
Right?
Who did the echo sequence?

971
00:54:41.820 --> 00:54:44.370
This is what we do in recruiting that we don't just feed in the input data.

972
00:54:44.520 --> 00:54:48.140
We also did the,
um,

973
00:54:51.630 --> 00:54:52.463
<v 2>so then,</v>

974
00:54:53.420 --> 00:54:55.760
<v 0>okay,
so we've got five more minutes here.
So let me just,</v>

975
00:54:59.000 --> 00:55:02.230
oh well turn in front of lots lists.
We're going to,
we're going to Upenn,

976
00:55:02.231 --> 00:55:04.980
the losses,
every new blocks that we calculate,
right?

977
00:55:04.981 --> 00:55:09.260
So this is that in lots of stuff we find beforehand and for steaks

978
00:55:10.150 --> 00:55:10.983
<v 2>and then,</v>

979
00:55:11.750 --> 00:55:14.490
<v 0>right.
So then for this part I can just,
you know,</v>

980
00:55:15.140 --> 00:55:18.410
he says it's not printing out values here.

981
00:55:19.560 --> 00:55:20.393
<v 2>Okay.</v>

982
00:55:21.340 --> 00:55:23.170
<v 0>And then we'll just close the box.</v>

983
00:55:26.020 --> 00:55:29.380
<v 2>Okay.
Okay.</v>

984
00:55:30.600 --> 00:55:32.520
<v 0>So let's go ahead and peat bog this.</v>

985
00:55:33.330 --> 00:55:36.840
So for batch x in number of batches per batch,

986
00:55:37.050 --> 00:55:42.000
Idx in number of batches.
Okay.
So let's see what we got for batch IDX.
Uh,

987
00:55:43.320 --> 00:55:45.690
there's no friend people time.
Great.
What else we got here?

988
00:55:46.230 --> 00:55:48.940
Unexpected him that and be debt in feed.

989
00:55:48.980 --> 00:55:53.580
Did you have an unexpected index or sorry.
Okay.

990
00:55:55.460 --> 00:55:59.300
And it states,
so for current states,
so for in its state for currency online,

991
00:55:59.310 --> 00:56:03.080
31 and it states equals current state.

992
00:56:03.081 --> 00:56:07.000
So that actually what this is his current underscore states.
Correct.

993
00:56:07.180 --> 00:56:11.360
Underscored states.
And then
it states,

994
00:56:13.270 --> 00:56:15.410
oh,
it's,
it's underscore current states

995
00:56:19.470 --> 00:56:22.300
and it states,
oh,
and then we need a comma here.

996
00:56:23.810 --> 00:56:24.020
<v 2>Okay,</v>

997
00:56:24.020 --> 00:56:24.853
<v 0>great.</v>

998
00:56:27.540 --> 00:56:28.200
<v 2>Oh wait,</v>

999
00:56:28.200 --> 00:56:32.940
<v 0>no name start is not defined in stark is not defined.</v>

1000
00:56:33.210 --> 00:56:36.390
We talking about what he's talking about on an IDX.

1001
00:56:40.020 --> 00:56:41.280
I didn't find star,
right.

1002
00:56:45.070 --> 00:56:47.680
Start plus truncated folk batch ids.

1003
00:56:51.140 --> 00:56:54.840
All right,
so for nids start x,
great.

1004
00:56:57.130 --> 00:56:57.963
<v 2>Yeah.</v>

1005
00:56:58.010 --> 00:57:02.690
<v 0>What does next year we've got a lot of debugging.
Oh my God.
I'm like,
okay.
So</v>

1006
00:57:03.710 --> 00:57:07.010
<v 2>hold on baby.
Sorry.
Ids,</v>

1007
00:57:08.840 --> 00:57:12.020
we've got,
hold on.
Who did I hear

1008
00:57:16.680 --> 00:57:21.160
<v 0>must be a value for placeholder and shoot five 15 you mo for.
Okay,</v>

1009
00:57:21.161 --> 00:57:23.230
we've got three minutes left here.
So,

1010
00:57:24.740 --> 00:57:25.573
<v 2>so hold on,</v>

1011
00:57:27.930 --> 00:57:30.830
<v 0>let's visualize this.
We've got to talk about the visual laser visualization.</v>

1012
00:57:30.840 --> 00:57:32.130
We've got to fit in the Q and a

1013
00:57:33.300 --> 00:57:37.940
<v 2>last,
just go for it.
Yeah.</v>

1014
00:57:39.030 --> 00:57:44.030
<v 0>In plot is not defined plot I'm talking about applies not to find PLTL Ios.</v>

1015
00:57:45.840 --> 00:57:46.530
<v 2>Okay.</v>

1016
00:57:46.530 --> 00:57:50.760
<v 0>Ooh,
lots is not defined.
It is define,
oh,</v>

1017
00:57:50.820 --> 00:57:53.090
plot is lexicon.
So from slots.

1018
00:57:56.500 --> 00:57:57.333
<v 2>Okay.</v>

1019
00:57:57.600 --> 00:58:00.440
<v 0>Oh yeah.
Cause it's the visualizer.
I didn't,
I didn't compile that visualizer.</v>

1020
00:58:00.600 --> 00:58:05.340
So compile this baby up here and then over here and then do this.

1021
00:58:06.300 --> 00:58:09.950
Okay guys,
it's training.
It's,
it is training now.
Okay.
Placeholder.
Okay.

1022
00:58:09.960 --> 00:58:12.960
So it's training.
Yes.
Okay,
good.
Good.

1023
00:58:14.520 --> 00:58:19.170
Now,
uh,
so actually I want to stop this training because we don't have,
we're not,

1024
00:58:19.210 --> 00:58:21.450
this is going to take awhile,
so we're going to stop the training.

1025
00:58:21.720 --> 00:58:26.210
But what happens is,
okay,
so
actually,

1026
00:58:26.270 --> 00:58:31.040
I mean this graph is kind of confusing here.
So,
so,

1027
00:58:31.130 --> 00:58:35.990
uh,
it's,
it's going to learn the,
the mapping between both of those sequences.

1028
00:58:36.110 --> 00:58:38.500
And so in this,
in this,
this is what's what it's going to output,
right?

1029
00:58:38.540 --> 00:58:41.330
Salsa training.
But this is what the output is going to look like.
Okay.

1030
00:58:41.390 --> 00:58:44.150
So in this first graph on the top left over here,

1031
00:58:44.270 --> 00:58:47.870
you see that the last function is minimizing over time.
Okay?
So that's,

1032
00:58:47.871 --> 00:58:51.710
that's what that,
that loss function is doing.
And now it's these other graphs.

1033
00:58:51.770 --> 00:58:55.810
You see that the blue represents the uh,
input sequence,
right?

1034
00:58:55.811 --> 00:58:57.960
Cause then it's kind of a weird graph,
a way of not,

1035
00:58:58.190 --> 00:58:59.660
it's kind of a weird graph to look at,

1036
00:58:59.661 --> 00:59:03.230
but essentially what's happening here is the blue represents at input sequence.

1037
00:59:03.440 --> 00:59:08.000
The red represents the,
uh,
what is it?
The red represents the echos,

1038
00:59:08.001 --> 00:59:11.140
which is that I'll put sequence and the Greens are valued.

1039
00:59:11.160 --> 00:59:12.950
Everyone at the echo and net is generating

1040
00:59:13.570 --> 00:59:14.403
<v 2>[inaudible].</v>

1041
00:59:14.800 --> 00:59:17.610
<v 0>Okay.
Okay.
So that's what that that's doing.</v>

1042
00:59:17.630 --> 00:59:20.770
So essentially it is remembering the mapping,
just ones and Zeros.

1043
00:59:20.830 --> 00:59:22.140
It's remembering the mapping and buddy.

1044
00:59:22.150 --> 00:59:25.520
And a bit is going to know that mapping so well and we don't need testing data

1045
00:59:25.521 --> 00:59:27.630
because,
uh,
it's,
it's going to be the same rights,

1046
00:59:27.650 --> 00:59:30.530
an echo of the same as the input.
So you don't really need testing.

1047
00:59:30.531 --> 00:59:35.000
They're the point of this and is too worried about the memory and how memory was

1048
00:59:35.001 --> 00:59:37.370
incorporated.
Right.
We did it by hand.
Okay.

1049
00:59:38.210 --> 00:59:42.870
Why are there spikes in the loss function?
Great question.

1050
00:59:43.080 --> 00:59:46.770
There are spikes in the loss function.
Why those spikes in the last one?
No,

1051
00:59:46.771 --> 00:59:50.150
it was because,
uh,
what was it?
It was because we're starting,

1052
00:59:50.160 --> 00:59:54.450
so we're starting on a new epoch and w because we're generating new data.

1053
00:59:54.451 --> 00:59:56.700
So because the Matrix is reshaped,

1054
00:59:56.820 --> 01:00:00.360
the first element in all of the rose is going to be adjacent to the last element

1055
01:00:00.361 --> 01:00:01.320
in the previous row.

1056
01:00:01.560 --> 01:00:04.600
So the first few elements and all the rows are going to have dependencies that

1057
01:00:04.601 --> 01:00:06.060
are not included in the states.

1058
01:00:06.180 --> 01:00:08.490
So the net is going to perform badly on the first batch.

1059
01:00:08.590 --> 01:00:11.670
So it's going to go up and down until it gets to that ending point.
Okay.

1060
01:00:12.690 --> 01:00:15.060
Because of epochs essentially.
Right?
So let's,
uh,

1061
01:00:15.061 --> 01:00:17.100
stop screen sharing and then we'll do questions.

1062
01:00:18.190 --> 01:00:18.680
<v 1>Okay.</v>

1063
01:00:18.680 --> 01:00:23.540
<v 0>Okay.
Hi Guys.
Okay,
let's,
uh,
let's,
uh,
let's talk a bit.</v>

1064
01:00:23.550 --> 01:00:27.560
Let's ending five,
many Qa.
Hey,
hey.
Hey.

1065
01:00:28.960 --> 01:00:31.090
What do you got here?
Darshawn and Luciana.
Okay.

1066
01:00:32.820 --> 01:00:33.653
<v 1>So</v>

1067
01:00:37.230 --> 01:00:39.440
<v 0>why do you use the last state from the previous run?</v>

1068
01:00:39.450 --> 01:00:41.340
I think in this state for the current runs,

1069
01:00:43.140 --> 01:00:45.150
because we are back propagating through time.

1070
01:00:47.290 --> 01:00:51.660
It's hidden state string.
A forward pass.
Okay.
Hi.

1071
01:00:51.661 --> 01:00:53.960
Suraj for time series five years.

1072
01:00:55.520 --> 01:00:59.200
Python in tensorflow.
That's a good question.

1073
01:00:59.201 --> 01:01:02.900
I mean who knows.
I mean who knows.
It could,
you know,

1074
01:01:02.901 --> 01:01:07.410
he couldn't run on binary silicon transistors.
We've already surpassed human,
um,

1075
01:01:07.840 --> 01:01:10.000
human levels on a lot of state of the art problems already.

1076
01:01:10.001 --> 01:01:14.110
So it could very well run on that.
Okay,
thanks Fernando.

1077
01:01:14.650 --> 01:01:18.100
Have you heard about api.ai?
I did a video for them a long time ago.
Yeah.

1078
01:01:18.101 --> 01:01:21.670
Now they are.
They got bought out by Google,
but I know people who work there.
Uh,

1079
01:01:22.240 --> 01:01:25.770
thank you alpha.
Yes,
Kudos to the community.

1080
01:01:25.771 --> 01:01:29.040
The chat is becoming more and more informative each session because,

1081
01:01:29.160 --> 01:01:33.300
because the reason the chat is becoming better is because we are growing at

1082
01:01:33.301 --> 01:01:38.250
4,000 engineers a week.
We have growing very fast.

1083
01:01:38.410 --> 01:01:42.030
We are growing faster and faster.
And part of it is you guys are my videos,

1084
01:01:42.031 --> 01:01:43.800
the quality.
But the other part is you've got,

1085
01:01:43.801 --> 01:01:47.250
this field is growing so fast globally.

1086
01:01:47.251 --> 01:01:49.680
We are all on a rocket ship.
Okay.

1087
01:01:49.681 --> 01:01:54.450
So this community is growing fast and if we define the culture of this community

1088
01:01:54.480 --> 01:01:58.350
early,
this culture of informing each other of helping each other.
Okay.

1089
01:01:58.720 --> 01:02:02.640
It's gonna do,
it's going to propagate forward in all directions.
Okay.

1090
01:02:02.641 --> 01:02:05.760
We are growing so fast.
We're going so fast.

1091
01:02:06.660 --> 01:02:07.100
<v 1>Okay.</v>

1092
01:02:07.100 --> 01:02:10.910
<v 0>Thanks Paolo.
Okay,
so I'll take three more questions before we end this session.</v>

1093
01:02:11.270 --> 01:02:13.580
Session on cue learning Sangram.
Great idea.

1094
01:02:13.581 --> 01:02:16.670
I'm going to have one after this course is well,
not know.

1095
01:02:16.880 --> 01:02:19.860
Actually during this course I'll have a learning session during this course.

1096
01:02:20.070 --> 01:02:23.730
Two more questions.
Uh,
I am behind the Korean.
Okay.

1097
01:02:27.620 --> 01:02:31.500
It's overnight.
Thanks.
She how,
how,

1098
01:02:31.530 --> 01:02:34.860
how impactful do you think an ml specific chip that can let people train

1099
01:02:34.861 --> 01:02:37.380
networks and much faster without needing a centralized system?

1100
01:02:37.470 --> 01:02:42.390
Like you mentioned earlier,
how impactful guys?
Uh,
so training in general,

1101
01:02:42.391 --> 01:02:43.470
there's a huge,

1102
01:02:43.680 --> 01:02:48.660
huge space for opportunity around distributed training services.
So if you can,

1103
01:02:49.680 --> 01:02:54.330
you know,
we all have a lot of computing power that just lies.
It'll our laptops,

1104
01:02:54.420 --> 01:02:55.410
our desktops.

1105
01:02:55.560 --> 01:03:00.090
If someone can leverage this and create a distributed computing network where

1106
01:03:00.091 --> 01:03:01.770
people get paid for

1107
01:03:03.660 --> 01:03:06.630
lending their computing power to tensorflow or whatever,

1108
01:03:06.720 --> 01:03:09.630
it's going to make them a lot of money.
It's going to make people a lot of money.

1109
01:03:09.840 --> 01:03:10.261
And this,

1110
01:03:10.261 --> 01:03:15.261
this is an example of one of those jobs of the future as automation Golfs,

1111
01:03:16.020 --> 01:03:18.630
all Labor based jobs,
we're going to need new jobs.

1112
01:03:18.631 --> 01:03:22.650
And one of these jobs he signs out putting data is to let people use your

1113
01:03:22.651 --> 01:03:26.250
computing power.
As the,
as a necessity of computing power grows over time,

1114
01:03:26.550 --> 01:03:30.570
this is going to be a more and more important thing and it shows why we need

1115
01:03:30.571 --> 01:03:34.350
that because AWS went down and a lot of services that we use went down with it.

1116
01:03:34.470 --> 01:03:37.890
It shows the bad part of centralized systems.
Two more questions actually.

1117
01:03:38.280 --> 01:03:42.990
Hey Sir,
I see and you explained how to do tension felt in distributed mode.
Uh,

1118
01:03:46.650 --> 01:03:49.260
yeah.
Uh,
yeah.
Later on.
Later on.
Not right now.

1119
01:03:50.280 --> 01:03:53.100
Can I narrow that change it's size or structure like hidden layers.

1120
01:03:55.850 --> 01:03:58.760
Hold on.
That question went away.
Two more questions.

1121
01:03:58.761 --> 01:04:03.140
Any plans on making videos related to Alexa play round?
No.
One more question.

1122
01:04:04.600 --> 01:04:06.580
How'd we get the optimized hyper parameter?

1123
01:04:06.730 --> 01:04:10.540
How about RNN for learning hyper parameters too.
So guys right now,

1124
01:04:10.541 --> 01:04:13.600
so if you saw that interview video that really quick questions I did with

1125
01:04:13.601 --> 01:04:15.270
dominate attention on the text flow team.

1126
01:04:15.730 --> 01:04:18.190
He had the same idea and I had the same idea in a lot of people.

1127
01:04:18.191 --> 01:04:21.870
We just all kind of have the same ideas that learning ideal,
uh,

1128
01:04:22.080 --> 01:04:25.240
hyper parameters and learning model architectures and learning,

1129
01:04:25.241 --> 01:04:29.620
which monitor a model to use.
Learning about learning is what's hot right now.

1130
01:04:29.621 --> 01:04:31.570
We're all trying to figure out how to best do that.

1131
01:04:31.571 --> 01:04:35.050
So that is what is everybody's kind of focused on right now learning to learn.

1132
01:04:35.230 --> 01:04:38.590
So it's a great question.
We don't know the answer.
We have some strategies,

1133
01:04:38.591 --> 01:04:40.300
you know,
like rich search and random search,

1134
01:04:40.660 --> 01:04:43.690
but there's a lot of space for improvement.
A lot of discoveries can be made.

1135
01:04:43.720 --> 01:04:47.320
Okay.
So that's it for this session.
Okay.

1136
01:04:47.530 --> 01:04:50.470
And I'm going to go,
but before I go,

1137
01:04:50.471 --> 01:04:53.350
I should probably end this with a wrap because nobody asks for it.

1138
01:04:53.351 --> 01:04:54.184
But I just want to do it.

1139
01:04:54.220 --> 01:04:59.140
Somebody say a topic and they were going to go 32nd route because I wanted to do

1140
01:04:59.141 --> 01:05:03.830
it.
Someone say a whatever subject I see first,

1141
01:05:03.831 --> 01:05:07.970
I'm just going to go for it.
Okay.
Uh,
rap.
How do we use a model?

1142
01:05:07.971 --> 01:05:11.540
We just built a use it.
I mean you could use it with the one line function.
It's,

1143
01:05:12.860 --> 01:05:16.640
I'm learning a lot.
Thank you.
I thank you guys for being here.
Okay.

1144
01:05:16.960 --> 01:05:19.390
Can you get some info?
Intuition.
Backpropagation

1145
01:05:20.070 --> 01:05:20.420
<v 1>yeah.</v>

1146
01:05:20.420 --> 01:05:24.210
<v 0>Well,
one's the nose map to oes and one's inputs.
Go in,
add weights,</v>

1147
01:05:24.211 --> 01:05:28.100
get someone's past that shit to my sigmoid function.
Get that era.

1148
01:05:28.110 --> 01:05:31.320
What's real and prediction and that's why I use gritty yet descent.

1149
01:05:31.680 --> 01:05:33.540
It gives direction and it doesn't pretend.

1150
01:05:33.720 --> 01:05:37.320
Update weights and repeat 10,000 times.
Outputs are lit.

1151
01:05:37.410 --> 01:05:40.440
I'll be doing just fine online as I do it.
Now.

1152
01:05:40.530 --> 01:05:44.910
I do it like in the future because I'm a loser.
Not really.
I'm a winner.

1153
01:05:45.210 --> 01:05:49.140
That's it on from the whole place.
San Francis silly.
Yo,

1154
01:05:49.170 --> 01:05:51.690
stop rapping.
I can't because I got a float.

1155
01:05:51.990 --> 01:05:56.970
Don't be homophobic.
Not.
That was from anyway.
Okay,
so please don't.

1156
01:05:56.971 --> 01:06:00.300
Okay.
All right guys,
that's it for this life session.
Thanks for tuning in.

1157
01:06:00.480 --> 01:06:02.610
We're going to keep going with this.
Every single week.

1158
01:06:02.611 --> 01:06:05.400
We are on a warpath to solve intelligence,

1159
01:06:05.610 --> 01:06:08.220
and we will keep going every week and we will feel the learn.

1160
01:06:08.310 --> 01:06:10.680
We're gonna learn the shit out of everything.
That's going to be awesome.

1161
01:06:10.860 --> 01:06:13.140
So stay tuned.
We've got videos coming out.

1162
01:06:13.200 --> 01:06:16.350
We're going to increase video output is gonna be awesome.
Love you guys.

1163
01:06:16.351 --> 01:06:21.120
And for now,
I've got to minimize my losses,
so thanks for watching.

