WEBVTT

1
00:00:06.650 --> 00:00:07.483
Oh,

2
00:00:08.210 --> 00:00:13.070
<v 1>this incredible cool world walking the Sira geology.
In today's episode,</v>

3
00:00:13.071 --> 00:00:15.200
we're going to talk about building an AI artists.

4
00:00:15.290 --> 00:00:17.540
Art is the embodiment of the human experience.

5
00:00:17.570 --> 00:00:20.420
It's a synthesis of all of our emotions and experiences in life.

6
00:00:20.450 --> 00:00:21.410
You might be asking yourself,

7
00:00:21.440 --> 00:00:24.920
is it really possible to build something that can do this?
The answer is yes.

8
00:00:24.921 --> 00:00:25.521
It's incredible.

9
00:00:25.521 --> 00:00:29.000
We can train a neural net to learn an artist's style and tell it to modify a

10
00:00:29.001 --> 00:00:30.710
picture into a painting.
In that style.

11
00:00:30.740 --> 00:00:33.410
It all started when the Google research team released a blog post called

12
00:00:33.440 --> 00:00:34.370
inception ism.

13
00:00:34.400 --> 00:00:37.790
They trained a deep convolutional neural network on a huge data set of images so

14
00:00:37.791 --> 00:00:40.520
they can start to detect everyday things like dogs and buildings.

15
00:00:40.580 --> 00:00:41.690
Once it was able to do that,

16
00:00:41.720 --> 00:00:44.960
they give it a novel image and asked to detect an object they had learned in the

17
00:00:44.961 --> 00:00:46.250
image.
If it's not something in the image,

18
00:00:46.251 --> 00:00:48.620
I look even slightly similar to what they had already learned.

19
00:00:48.650 --> 00:00:51.170
Say a cloud that kind of looked like a dog.
It would then modify.

20
00:00:51.171 --> 00:00:54.290
The image will look more like a dog.
This resulted in some pretty crazy pictures.

21
00:00:54.320 --> 00:00:57.290
Then another group wrote a similar paper called Neuro Algorithm for artistic

22
00:00:57.291 --> 00:00:57.740
style,

23
00:00:57.740 --> 00:01:00.860
but they repainted in imaging the style of another using famous paintings as the

24
00:01:00.861 --> 00:01:01.430
base image,

25
00:01:01.430 --> 00:01:04.520
so when they trained their neural net on starry night and gave it a novel image,

26
00:01:04.521 --> 00:01:05.121
it would modify.

27
00:01:05.121 --> 00:01:08.010
The image will look more like starry night by are defying all of its features.

28
00:01:08.070 --> 00:01:11.090
We've gone over convolutional neural nets in a previous episode at a high level.

29
00:01:11.120 --> 00:01:13.820
We're going to really deep dive into the code for this one and try to understand

30
00:01:13.821 --> 00:01:15.440
exactly how this process works.

31
00:01:15.470 --> 00:01:17.900
We're going to go through the necessary code to recreate the results from the

32
00:01:17.901 --> 00:01:21.320
neural style paper in python using deep learning library care Os.

33
00:01:21.350 --> 00:01:23.930
Let's get started.
We'll start off by defining our arguments.

34
00:01:23.960 --> 00:01:26.990
When we run our script,
we defined the base image,
the style image,

35
00:01:27.050 --> 00:01:29.090
the output image or creating variables for them.

36
00:01:29.120 --> 00:01:32.810
Then reference a precomputed weights file called Dgg 16 these are just a bunch

37
00:01:32.811 --> 00:01:35.480
of precomputed synapse weights trained to recognize everyday images,

38
00:01:35.510 --> 00:01:37.550
which will later add to our neural net as a starting point.

39
00:01:37.610 --> 00:01:40.460
We'll also want to initialize bullying's that define whether or not to rescale

40
00:01:40.461 --> 00:01:43.250
our image or maintain their aspect ratio based on user input.

41
00:01:43.280 --> 00:01:46.040
Then we initialize our variables for style and content.
Wait,

42
00:01:46.100 --> 00:01:48.540
so what do we mean by style and content weight?
Well,

43
00:01:48.680 --> 00:01:51.410
in the neural style paper they found that when they train a neural net to

44
00:01:51.411 --> 00:01:53.300
recognize the painting style of his artists,

45
00:01:53.301 --> 00:01:55.610
the learn features in the initial layers were style base.

46
00:01:55.640 --> 00:01:58.340
Like you know how when you train a neural net to recognize a dog,

47
00:01:58.370 --> 00:02:00.890
initial areas detect edges and the next one's in tech shapes.

48
00:02:00.890 --> 00:02:02.450
Then more complex shapes and a whole dog.

49
00:02:02.510 --> 00:02:04.940
Well those same layers of abstraction apply to paintings,

50
00:02:04.970 --> 00:02:06.460
but what they found with the initial layers,

51
00:02:06.470 --> 00:02:09.920
the edges and curvature and other low level features equated with style.

52
00:02:09.950 --> 00:02:12.440
They also found the highest few layers were more based on content.

53
00:02:12.441 --> 00:02:13.340
So when the starting night photo,

54
00:02:13.350 --> 00:02:16.580
the higher levels would be that dope son thing and perhaps a collection of
stars.

55
00:02:16.610 --> 00:02:19.310
The lower levels would be the curvature of the night sky and the color scheme

56
00:02:19.340 --> 00:02:22.400
and this way soon it's helped us separate content from style and mirror the

57
00:02:22.401 --> 00:02:24.050
capabilities of our biological vision.

58
00:02:24.080 --> 00:02:27.020
We then need to define how much we want to weigh one or the other because we can

59
00:02:27.050 --> 00:02:30.290
optimize for one of them or both depending on how much we weight each.

60
00:02:30.320 --> 00:02:31.153
We'll get a different output.

61
00:02:31.160 --> 00:02:34.370
Will Sarah image dimensions and then create a tense or representation of our

62
00:02:34.371 --> 00:02:36.350
base image,
style,
image and output image,

63
00:02:36.410 --> 00:02:39.590
and we'll combine all three into a single input.
Tenser and tenser.

64
00:02:39.591 --> 00:02:42.890
It's a multidimensional array.
An example would be like colors,
textures.

65
00:02:43.100 --> 00:02:44.450
Each of those would be raised as well,

66
00:02:44.451 --> 00:02:47.060
so colors would be its own array containing the seven main colors.

67
00:02:47.330 --> 00:02:49.520
Then each of those colors would be an array of sub colors.

68
00:02:49.820 --> 00:02:53.120
We convert our image with single tenser because it's an easily possible data

69
00:02:53.121 --> 00:02:54.530
structure for our neural network.

70
00:02:54.770 --> 00:02:58.130
Tensors help reduce the high dimensionality of our images and that in turn

71
00:02:58.131 --> 00:03:01.360
reduces the complexity.
Now we're going to want to build our model,

72
00:03:01.361 --> 00:03:03.220
which is going to be a convolutional neural network.

73
00:03:03.250 --> 00:03:05.500
We'll add in our input tensor as the first layer.

74
00:03:05.590 --> 00:03:07.180
Then we'll start to finding the other layers.

75
00:03:07.210 --> 00:03:09.970
We're going to add in 31 layers to our neural net.
There are three types.

76
00:03:10.000 --> 00:03:11.170
The convolution two layer,

77
00:03:11.171 --> 00:03:14.230
it means it has a set of learnable filters which have a small receptive field.

78
00:03:14.260 --> 00:03:17.470
The receptive field is a subset of filters that are used to connect the neurons

79
00:03:17.471 --> 00:03:20.560
to a local region in the next layer instead of every single other neuro zero

80
00:03:20.561 --> 00:03:23.890
padding layer helps us control the size of the output volume by padding zeros

81
00:03:23.891 --> 00:03:24.640
across the border.

82
00:03:24.640 --> 00:03:27.790
Then there's the average pooling layer pulling as a concept in is where we take

83
00:03:27.791 --> 00:03:30.430
the input image and split it into a sec or pool of rectangles.

84
00:03:30.460 --> 00:03:33.190
Pulling helps us avoid overfitting and reduce the amount of parameters can

85
00:03:33.191 --> 00:03:36.700
computation by only using a subset of the image as representation rather than

86
00:03:36.730 --> 00:03:39.070
all of it.
The idea is that once a feature has been found,

87
00:03:39.071 --> 00:03:42.340
the exact location isn't as important as it's rough relative location to other

88
00:03:42.341 --> 00:03:44.230
features.
We'll take the average value from the pool.

89
00:03:44.260 --> 00:03:47.260
The activation function is called [inaudible] or rectified linear unit.

90
00:03:47.261 --> 00:03:50.080
The relo function is faster than sigmoid without a huge difference in

91
00:03:50.081 --> 00:03:52.540
generalization accuracy,
so we'll use that to the numbers.

92
00:03:52.540 --> 00:03:55.330
Here are the number of output filters and the length and width of the input

93
00:03:55.331 --> 00:03:57.790
image well named each of our layers as well.
For reference.

94
00:03:57.850 --> 00:04:00.850
Now that we have our model,
we'll add into precomputed weights we called earlier.

95
00:04:00.880 --> 00:04:03.820
Then we're going to define our loss function for the style content and Total

96
00:04:03.821 --> 00:04:06.010
Variation,
Aka the uniformity of the image.

97
00:04:06.011 --> 00:04:09.010
Last functions help us calculate the difference between the expected output and

98
00:04:09.011 --> 00:04:09.790
the actual output.

99
00:04:09.790 --> 00:04:12.940
We're going to take these loss functions and combined them into a single scalar

100
00:04:12.941 --> 00:04:15.310
or number.
Then we'll get the gradients of the generated image,

101
00:04:15.340 --> 00:04:17.200
usually the loss which helps us map the color scheme.

102
00:04:17.230 --> 00:04:20.650
Our last step is to train our model by minimizing the loss using
backpropagation.

103
00:04:20.680 --> 00:04:23.890
The backpropagation algorithm will use in this case is called limited memory

104
00:04:23.891 --> 00:04:24.940
bfgs LBF.

105
00:04:24.941 --> 00:04:27.940
GS helps minimize our loss function and it's space efficient and that it only

106
00:04:27.941 --> 00:04:28.870
stores a few key vectors.

107
00:04:28.870 --> 00:04:30.670
Instead of all of that minimizing the loss function needs,

108
00:04:30.671 --> 00:04:33.550
modifying the output image iteratively so it looks more and more similar to this

109
00:04:33.551 --> 00:04:36.940
artistic style we want Osi Pi gives us a technique as a built in function and

110
00:04:36.941 --> 00:04:39.700
after training is over,
if we can rescale it and save the output image,

111
00:04:39.730 --> 00:04:41.800
that's about it.
This is what happened when I tried out the algorithms.

112
00:04:41.860 --> 00:04:44.680
You could also apply this algorithm frame by frame to video content with more

113
00:04:44.681 --> 00:04:46.630
data and computing power.
It's only going to get better.

114
00:04:46.631 --> 00:04:49.300
I hope that in the next few years we'll be able to generate not just paintings,

115
00:04:49.301 --> 00:04:53.230
but other forms of art like sculptures and interior design and facial
hairstyles.

116
00:04:53.260 --> 00:04:53.981
For more information,

117
00:04:53.981 --> 00:04:57.220
check out the links down below and please subscribe for more ml videos.
For now,

118
00:04:57.250 --> 00:05:00.070
I've got to go fix a compile time error,
so thanks for watching.

