WEBVTT

1
00:00:00.060 --> 00:00:00.751
Hello world,

2
00:00:00.751 --> 00:00:04.770
it's Saroj and today we're going to implement a popular reinforcement learning

3
00:00:04.771 --> 00:00:09.771
technique called policy gradients to help us choose which slot machine to play

4
00:00:09.930 --> 00:00:11.820
that will give us the highest winnings.

5
00:00:14.820 --> 00:00:17.550
We've talked a good bit about supervised learning.

6
00:00:17.580 --> 00:00:22.560
It's where we build an algorithm based on both input and output data,

7
00:00:22.650 --> 00:00:25.020
which is great for classification tasks.

8
00:00:25.350 --> 00:00:28.830
Unsupervised learning is when we don't have a history of outputs.

9
00:00:29.070 --> 00:00:32.100
We have to build an algorithm based on input data alone.

10
00:00:32.370 --> 00:00:36.780
Reinforcement learning is similar in that is also building an algorithm based on

11
00:00:36.781 --> 00:00:40.350
input data alone,
but we framed the problem in a different way.

12
00:00:40.590 --> 00:00:42.300
The algorithm presents a state,

13
00:00:42.450 --> 00:00:46.800
it depends on the input data and it's either rewarded or punished via an action

14
00:00:46.801 --> 00:00:49.290
that it takes and this continues over time.

15
00:00:49.530 --> 00:00:54.060
It learns from the reward or punishment and continually updates itself over the

16
00:00:54.061 --> 00:00:56.070
longterm to maximize the reward.

17
00:00:56.370 --> 00:00:59.910
Reinforcement learning doesn't get as much love these days from the Ai community

18
00:01:00.120 --> 00:01:03.660
as the other two do because some of the most interesting problems right now,

19
00:01:03.750 --> 00:01:07.920
like those in speech recognition and LP and computer vision or areas where it's

20
00:01:07.921 --> 00:01:11.370
hard to define the notion of a longterm reward,

21
00:01:11.480 --> 00:01:14.460
don't hate three and force.

22
00:01:15.630 --> 00:01:19.200
Currently the problems that are best solved by reinforcement learning are either

23
00:01:19.360 --> 00:01:24.360
ploy problems like getting an NPC to get through a level without dying or really

24
00:01:24.721 --> 00:01:29.610
complex problems like self driving cars or folding laundry or understanding my

25
00:01:29.690 --> 00:01:33.930
ex-girlfriend.
Things that you could do in a simulation,
human level tasks.

26
00:01:34.020 --> 00:01:38.160
So a lot of the work in reinforcement learning is theoretical instead of

27
00:01:38.470 --> 00:01:42.870
application-based,
which is just as important.
Don't get me wrong,
however,

28
00:01:42.871 --> 00:01:47.370
there are some real world use cases today Finnair the flight booking company,

29
00:01:47.371 --> 00:01:48.061
for example,

30
00:01:48.061 --> 00:01:52.440
uses RL to decide what action to take for what customers to increase their

31
00:01:52.441 --> 00:01:53.430
lifetime value.

32
00:01:53.640 --> 00:01:58.560
And deep mind reduced Google's cooling Centerville by 40% by using RL to decide

33
00:01:58.561 --> 00:02:02.180
the most efficient energy routing strategy for the lowest cost.

34
00:02:02.580 --> 00:02:06.390
So let's take a look at our problem,
which comes from probability theory.

35
00:02:06.510 --> 00:02:09.690
The problem is that a Gambler,
which we'll call our agent,

36
00:02:09.750 --> 00:02:12.210
has to decide which slot machine to play,

37
00:02:12.360 --> 00:02:16.380
how many times to play each machine and in which order to play them.

38
00:02:16.710 --> 00:02:20.340
Every time that a machine is played,
it outputs a reward value,

39
00:02:20.341 --> 00:02:24.870
which is randomly generated from a probability distribution specific to that

40
00:02:24.871 --> 00:02:25.680
machine.

41
00:02:25.680 --> 00:02:30.120
The goal of the Gambler is to maximize the sum total of rewards earned through a

42
00:02:30.121 --> 00:02:31.410
sequence of Liverpool's.

43
00:02:31.590 --> 00:02:36.450
He iteratively plays one lever per round and observes the associated reward.

44
00:02:36.451 --> 00:02:39.750
To do this,
this is called the multi-armed bandit problem.

45
00:02:39.900 --> 00:02:44.340
The band at being the name of an old style slot machine with an arm or several

46
00:02:44.490 --> 00:02:45.900
on the side that you pull down.

47
00:02:46.140 --> 00:02:50.040
The agent has to make a choice between using machines that are known to produce

48
00:02:50.041 --> 00:02:50.874
good results,

49
00:02:51.090 --> 00:02:55.380
exploitation and trying out new machines that have unknown results but could

50
00:02:55.381 --> 00:02:58.140
give better results than the others.
Exploration.

51
00:02:58.660 --> 00:03:03.310
Exploitation is optimizing decisions based on existing knowledge and exploration

52
00:03:03.460 --> 00:03:05.320
is a company to acquire new knowledge.

53
00:03:05.560 --> 00:03:09.910
It's a tradeoff that all reinforcement learning agents make when optimizing for

54
00:03:09.911 --> 00:03:13.870
a reward value,
and it's particularly relevant in this problem.

55
00:03:14.050 --> 00:03:18.970
So let's start initializing some values.
After importing tensorflow and num py,

56
00:03:18.971 --> 00:03:22.750
the only two libraries will meet to use.
We can define our bandits.

57
00:03:22.960 --> 00:03:24.730
We'll be using a four armed bandit.

58
00:03:24.731 --> 00:03:29.200
That is one slot machine with four levers and we can refer to each arm has

59
00:03:29.201 --> 00:03:29.890
abandoned.

60
00:03:29.890 --> 00:03:33.910
So we'll define our bandits as a list and each of these values will help decide

61
00:03:33.911 --> 00:03:36.820
if a reward is given when pulled the lower the bandit number,

62
00:03:36.821 --> 00:03:40.210
the more likely we'll get a positive reward.
The higher the bandit number,

63
00:03:40.211 --> 00:03:42.340
the more likely we'll get a negative reward.

64
00:03:42.640 --> 00:03:46.450
We want our agent to choose the bandit that will give that reward and we'll

65
00:03:46.451 --> 00:03:49.870
initialize a variable for storing the total number well,

66
00:03:49.871 --> 00:03:52.580
next to find a pull bandit function which given a banded,

67
00:03:52.670 --> 00:03:56.680
you will first generate a random number from a normal distribution with a mean

68
00:03:56.681 --> 00:04:00.460
of zero.
Then compare the parameter value to the generated number.

69
00:04:00.760 --> 00:04:04.780
Depending on the result,
they don't either return a positive or negative reward.

70
00:04:04.990 --> 00:04:09.280
In practice,
this model is used anytime you have a project with a fixed budget,

71
00:04:09.550 --> 00:04:13.630
it can be used to help best allocate resources to maximize success.

72
00:04:13.631 --> 00:04:17.050
Since it's specifically designed to deal with the uncertainty about the

73
00:04:17.051 --> 00:04:21.310
difficulty and payoff of each possibility.
I wish I had this thing in college.

74
00:04:21.480 --> 00:04:25.420
Our agent needs to learn which kind of reward it gets for each possible action

75
00:04:25.510 --> 00:04:29.590
so that it can choose the optimal ones.
It's learning a policy,

76
00:04:29.591 --> 00:04:33.540
so we're going to use a popular method called policy gradients.
To solve this,

77
00:04:33.790 --> 00:04:37.330
we'll use a simple neural net that learns a policy for picking the best actions

78
00:04:37.540 --> 00:04:39.670
and adjusting its weights through gradient descent,

79
00:04:39.880 --> 00:04:42.220
using realtime feedback from the environment.

80
00:04:42.490 --> 00:04:46.480
Since we're only using a single bandit or agent ignores the state of the

81
00:04:46.481 --> 00:04:48.490
environment just like the u s government,

82
00:04:48.730 --> 00:04:51.370
there's only ever a single unchanging state.

83
00:04:51.670 --> 00:04:53.710
If we were introducing multiple bandits,

84
00:04:53.760 --> 00:04:57.730
then our agent would need to take state into account when deciding an action.

85
00:04:57.970 --> 00:05:00.100
We would lord a value function instead,

86
00:05:00.340 --> 00:05:02.890
but let's keep it simple with a single state.

87
00:05:03.480 --> 00:05:08.480
<v 1>When you force the policy actions or qualities.</v>

88
00:05:09.750 --> 00:05:13.770
<v 0>Our policy gradient network consists of a set of weights and each weight</v>

89
00:05:13.771 --> 00:05:18.300
corresponds to each of the possible arms to pull and represents how beneficial

90
00:05:18.301 --> 00:05:22.590
our agent thinks it is.
To pull each arm.
We'll initialize the weights to one,

91
00:05:22.591 --> 00:05:26.250
which means our agent will be optimistic about each arms potential reward.

92
00:05:26.550 --> 00:05:30.450
When we update our network,
we'll use what's called an epsilon greedy policy.

93
00:05:30.810 --> 00:05:34.860
This is a way of selecting random actions with uniform distributions from a set

94
00:05:34.861 --> 00:05:37.350
of available actions.
Using this policy,

95
00:05:37.351 --> 00:05:41.820
either we can select random actions with epsilon probability or we can select an

96
00:05:41.821 --> 00:05:45.690
action with one minus epsilon probability that gives maximum reward in a given

97
00:05:45.691 --> 00:05:49.890
state.
Well,
define epsilon is 0.1.
It's the chance of taking a random action.

98
00:05:49.891 --> 00:05:50.670
Basically,

99
00:05:50.670 --> 00:05:54.180
most of the time our agent will choose the action that corresponds to the

100
00:05:54.181 --> 00:05:55.710
largest expected value,

101
00:05:55.711 --> 00:06:00.470
but sometimes with the it will choose randomly so this way it can try out

102
00:06:00.471 --> 00:06:04.850
different arms to continue learning about them.
Our agent is the neural network.

103
00:06:04.880 --> 00:06:07.790
It's feed forward and only has one set of weights.

104
00:06:08.000 --> 00:06:11.660
We'll initialize them has a tensor where each is a set of one for the number of

105
00:06:11.690 --> 00:06:12.523
bandits.

106
00:06:12.560 --> 00:06:16.040
Then we'll use the Arg Max function to choose the weights with the highest value

107
00:06:16.070 --> 00:06:17.960
and store that as our chosen action.

108
00:06:18.260 --> 00:06:22.610
We now need to establish what this training process looks like since we want to

109
00:06:22.611 --> 00:06:26.810
feed the reward and chosen action into the network to compute the loss and then

110
00:06:26.811 --> 00:06:29.690
use that to update the network.
We'll initialize tensorflow,

111
00:06:29.700 --> 00:06:33.890
placeholders for both the reward and the action values.
Next,

112
00:06:33.891 --> 00:06:38.210
we'll define the responsible way it corresponds to the units in the output
layer,

113
00:06:38.240 --> 00:06:42.050
which corresponded to the chosen action.
When updating the policy,

114
00:06:42.051 --> 00:06:46.430
we want to update the likelihood of the actions we actually took as opposed to

115
00:06:46.460 --> 00:06:47.720
all possible actions.

116
00:06:47.721 --> 00:06:52.130
So this will be a slice of our weights and we can define the size of it as well.

117
00:06:52.250 --> 00:06:55.370
So this is what our policy loss equation looks like.

118
00:06:55.640 --> 00:07:00.170
This is what we want to minimize this character is the policy which we take the

119
00:07:00.171 --> 00:07:04.460
log of and a is the advantage.
This is a critical part of our l.

120
00:07:04.490 --> 00:07:08.690
It's a measure of how much better an action was.
Then some baseline.

121
00:07:08.750 --> 00:07:13.250
There are different ways of deciding what that baseline is and it can get pretty

122
00:07:13.251 --> 00:07:14.084
interesting,

123
00:07:15.090 --> 00:07:18.770
but right now we'll just set it to zero so we can just think of it as just a

124
00:07:18.771 --> 00:07:20.660
reward we receive for each action.

125
00:07:20.960 --> 00:07:25.010
This loss function lets us increase the weight for actions that give a positive

126
00:07:25.011 --> 00:07:29.210
reward value and decrease them for actions that give a negative reward value.

127
00:07:29.270 --> 00:07:31.400
When we define our loss function programmatically,

128
00:07:31.401 --> 00:07:35.030
we can see that it corresponds to the equation where reward hold.

129
00:07:35.031 --> 00:07:36.170
There is the advantage.

130
00:07:36.380 --> 00:07:39.770
We can then optimize it with gradient descent and given learning rates.

131
00:07:40.010 --> 00:07:43.010
When we minimize our loss,
it will return a gradient update.

132
00:07:43.011 --> 00:07:46.160
So for our training step we'll initialize ATF graph.

133
00:07:46.420 --> 00:07:48.290
Then for a given number of episodes,

134
00:07:48.320 --> 00:07:52.040
we'll either try a random action or choose one from our network.

135
00:07:52.280 --> 00:07:56.870
Exploitation versus exploration will receive a reward from our action of picking

136
00:07:56.871 --> 00:07:57.770
one of their benefits.

137
00:07:57.950 --> 00:08:01.340
Then we'll update the network using our gradient wait values relevant to the

138
00:08:01.341 --> 00:08:03.560
action and all wait values.

139
00:08:03.800 --> 00:08:07.850
We can use a feed dick to feed in both the action and the reward will want to

140
00:08:07.851 --> 00:08:11.390
see which reward and which band that we are on during each iteration.

141
00:08:11.391 --> 00:08:14.000
So we'll print them out.
When we compile this,

142
00:08:14.001 --> 00:08:18.200
we can see that bandit fours values increase way faster than the other ones.

143
00:08:18.470 --> 00:08:21.290
It decides what is best and then those all in audit,

144
00:08:21.380 --> 00:08:25.490
we can extend this coke later on so that both state and action affect reward,

145
00:08:25.700 --> 00:08:28.490
which would be considered the contextual bandit problem.

146
00:08:28.550 --> 00:08:30.440
So let's go over what we've learned.

147
00:08:30.650 --> 00:08:35.270
Reinforcement learning is usually applied to toy problems or really complex

148
00:08:35.271 --> 00:08:36.104
problems.

149
00:08:36.140 --> 00:08:41.140
Policy gradient methods are a type of RL technique that optimizes a policy with

150
00:08:41.151 --> 00:08:44.810
respect to the expected long term return using gradient descent.

151
00:08:45.140 --> 00:08:48.650
And we can apply this strategy to the popular multi-arm banded problem,

152
00:08:48.890 --> 00:08:53.030
which asks how to best allocate resources to maximize success.

153
00:08:53.090 --> 00:08:56.190
The coding challenge winner for this video is Mike Mcdermott.

154
00:08:56.550 --> 00:09:01.080
Mike improve the bleeding edge memory network model from my last video to create

155
00:09:01.081 --> 00:09:05.760
a Q and a chat Bot by adding a bidirectional LSTM and time distributed dense

156
00:09:05.761 --> 00:09:08.220
layer to it.
This is seriously amazing stuff.

157
00:09:08.221 --> 00:09:11.340
He could publish as a results to a journal wizard of the week,

158
00:09:11.550 --> 00:09:15.060
and the runner up is Michelle Botcher who also had publishable results and you

159
00:09:15.061 --> 00:09:16.740
can run his code right from the command line.

160
00:09:16.950 --> 00:09:19.950
You guys blew my mind and I bow to you.

161
00:09:20.130 --> 00:09:23.880
The coding challenge for this week is to use policy gradients to solve the

162
00:09:23.881 --> 00:09:27.510
contextual bandit problem.
So state is taken into account,

163
00:09:27.690 --> 00:09:30.780
details are in the read me did humbling scope in the comments and winners are

164
00:09:30.781 --> 00:09:33.330
going to be announced next week.
Please subscribe.

165
00:09:33.331 --> 00:09:37.920
And for now I've got to maximize my arm size.
So thanks for watching.

