WEBVTT

1
00:00:00.210 --> 00:00:04.830
Hey,
I can dream up solutions.
Whoa.
Hello world.

2
00:00:04.831 --> 00:00:09.720
It's arrived and there's a really cool paper that recently came out called world

3
00:00:09.721 --> 00:00:10.554
models.

4
00:00:10.620 --> 00:00:15.620
The AI researchers created an algorithm for a simulated car that let it learn

5
00:00:15.631 --> 00:00:20.631
how to drive on a race track by itself and uniquely this AI teaches itself how

6
00:00:21.391 --> 00:00:24.180
to drive by practicing and its own dreams.

7
00:00:24.480 --> 00:00:28.470
By that I mean a hallucinated version of the world that it creates itself.

8
00:00:28.830 --> 00:00:29.850
Pretty wild stuff.

9
00:00:30.180 --> 00:00:34.680
This paper was well done because it combined several deep RL techniques to

10
00:00:34.681 --> 00:00:36.390
achieve incredible results.

11
00:00:36.660 --> 00:00:41.220
It's the first AI to solve the open AI car racing environment and the

12
00:00:41.221 --> 00:00:44.340
researchers created some great documentation as well.

13
00:00:44.610 --> 00:00:48.930
They found a lot of inspiration from the way humans perceive the world.

14
00:00:49.230 --> 00:00:53.670
We develop a mental model of the world based on what we are able to perceive.

15
00:00:53.970 --> 00:00:58.230
We have this internal model of the world that we build in our head from

16
00:00:58.231 --> 00:00:59.310
everything we see.

17
00:00:59.370 --> 00:01:04.320
It contains a few selected concepts and the relationships between them to

18
00:01:04.321 --> 00:01:06.810
represent a larger,
more complex system.

19
00:01:07.230 --> 00:01:12.230
Our brain is learning to represent both space and time for a given event and we

20
00:01:12.811 --> 00:01:17.430
use that representation to predict the future like what we will likely perceive

21
00:01:17.431 --> 00:01:19.110
or feel.
For example,

22
00:01:19.111 --> 00:01:24.111
we might perform a fast reflex when endanger pro baseball players are able to

23
00:01:24.391 --> 00:01:28.950
hit hundred mile an hour fastballs because they are instinctively able to

24
00:01:28.951 --> 00:01:33.570
predict where the ball will go.
It's all subconscious including steroids.

25
00:01:33.930 --> 00:01:38.490
They don't need to consciously roll out a bunch of possible future scenarios to

26
00:01:38.491 --> 00:01:39.360
form a plan.

27
00:01:39.750 --> 00:01:44.340
Reinforcement learning is the branch of AI concerned with trial and error.

28
00:01:44.640 --> 00:01:49.230
How does an agent learn how to maximize rewards in an environment scenario?

29
00:01:49.520 --> 00:01:54.520
Our l agents can benefit from having a robust representation of the past and

30
00:01:54.661 --> 00:01:56.490
present states of the world,

31
00:01:56.670 --> 00:01:59.940
which helps create a good it predicted model of the future.

32
00:02:00.420 --> 00:02:05.310
The researchers created a model inspired by this biological analog.

33
00:02:05.490 --> 00:02:09.840
They looked at the problem of playing a racing game and divided it into three

34
00:02:09.841 --> 00:02:14.490
different tasks,
visual representation,
modeling and control.

35
00:02:14.700 --> 00:02:18.990
At each time step,
the agent receives an observation from the environment.

36
00:02:19.350 --> 00:02:22.260
In the case of the racing game,
it was game frames.

37
00:02:22.500 --> 00:02:27.120
That vision model was responsible for incoding the observation into a lower

38
00:02:27.121 --> 00:02:30.390
dimensional representation,
a more compact version.

39
00:02:30.630 --> 00:02:34.920
Then they took this representation and fed it into the memory model,

40
00:02:35.010 --> 00:02:39.990
which used the learned representation to learn how the world tends to behave and

41
00:02:39.991 --> 00:02:42.600
make predictions on the next state of the world,

42
00:02:42.970 --> 00:02:46.380
essentially learns a model of the world.
Lastly,

43
00:02:46.410 --> 00:02:51.240
the controller model was spent the representation from both the nm and selects

44
00:02:51.241 --> 00:02:55.770
good actions.
Based on that,
let's look into how each of these components work.

45
00:02:56.040 --> 00:03:00.040
We can consider a single game frame as a high inputs.

46
00:03:00.070 --> 00:03:05.070
It's a two d image frame that's part of a video sequence and they used a model

47
00:03:05.111 --> 00:03:10.111
called a variational auto encoder or VA to learn a representation from this VA.

48
00:03:11.190 --> 00:03:14.710
Ease consist of an encoder and a decoder.
The encoder,

49
00:03:14.711 --> 00:03:17.470
it takes the input and creates a representation.

50
00:03:17.890 --> 00:03:22.510
The decoder takes that representation and tries to reconstruct the input.

51
00:03:22.810 --> 00:03:27.700
What makes it variational and not just a regular auto encoder is that a bit of a

52
00:03:27.701 --> 00:03:31.480
randomness is applied to the representation when decoding,

53
00:03:31.780 --> 00:03:36.220
so the decoded output is always some slight variation of the inputs,

54
00:03:36.460 --> 00:03:41.290
not exactly the inputs.
That's why the VA,
he is considered a generative model.

55
00:03:41.380 --> 00:03:44.110
It can generate new data from what it's trained on.

56
00:03:44.350 --> 00:03:46.960
Just don't train it on NSFW images.

57
00:03:47.290 --> 00:03:51.490
That encoded representation was fed to the memory model directly.

58
00:03:51.760 --> 00:03:55.360
It's a recurrent neural network useful for predicting sequences.

59
00:03:55.690 --> 00:03:58.930
If it's given a sequence of image representations as input,

60
00:03:59.230 --> 00:04:01.330
it will try and predict the next one,

61
00:04:01.510 --> 00:04:05.920
which is essentially a prediction of the future state of the world that the

62
00:04:05.921 --> 00:04:08.050
agencies,
recurrent nets,

63
00:04:08.051 --> 00:04:12.790
produce a feedback loop while training learning not just from the data but from

64
00:04:12.791 --> 00:04:15.340
what it learned in the previous time step.

65
00:04:15.670 --> 00:04:20.560
This optimization technique is called back propagation through time.

66
00:04:20.950 --> 00:04:25.720
The controller model is responsible for determining the next action to take in

67
00:04:25.721 --> 00:04:30.520
order to maximize the expected reward of the agent during a rollout of the

68
00:04:30.521 --> 00:04:31.354
environment.

69
00:04:31.630 --> 00:04:36.250
The controller is a simple single layer feed forward neural network that takes

70
00:04:36.251 --> 00:04:40.150
the representations from both the nm as input.

71
00:04:40.420 --> 00:04:45.200
The flow of the data is such that the raw observation is first processed by the,

72
00:04:45.640 --> 00:04:49.870
the input into c is the output of the concatenate it with EMS.

73
00:04:49.871 --> 00:04:54.871
Hidden state see then outputs an action vector for motor control and then takes

74
00:04:55.031 --> 00:04:59.380
the current output from the and the action as an input to update its own hidden

75
00:04:59.381 --> 00:05:02.230
states to be used in the next time step.

76
00:05:02.440 --> 00:05:07.440
Open Ai released an environment called gym that makes it really easy to train AI

77
00:05:08.471 --> 00:05:12.640
agents in simulated game worlds.
The idea is that the library,

78
00:05:12.641 --> 00:05:16.720
it takes care of integrating different game worlds and basic environments

79
00:05:16.721 --> 00:05:21.721
related configurations and you as a developer can focus on creating an algorithm

80
00:05:22.060 --> 00:05:26.230
to solve that game.
The code for this demo is relatively simple.

81
00:05:26.290 --> 00:05:30.460
We start by retrieving the observation,
the image frame from the environment.

82
00:05:30.820 --> 00:05:34.030
We also initialize our recurrent neural network model.

83
00:05:34.060 --> 00:05:37.690
We then create a wild statement for our training loop.

84
00:05:37.930 --> 00:05:40.780
We'll use the bae to encode the observation.

85
00:05:41.140 --> 00:05:45.370
We'll then use that output Czi as an input to our controller as well as the

86
00:05:45.371 --> 00:05:49.810
hidden state of the recurrent network.
This will give us an action to take.

87
00:05:50.020 --> 00:05:55.020
We'll use the step function Jim provides to execute that action and get back a

88
00:05:55.151 --> 00:05:57.010
new observation,
a reward.

89
00:05:57.140 --> 00:06:01.610
If we got one and a boolean value that represents whether or not the game is

90
00:06:01.611 --> 00:06:02.360
over.

91
00:06:02.360 --> 00:06:07.360
We can use this reward to update our cumulative for ward and use the learn

92
00:06:07.401 --> 00:06:08.720
parameters,
a,

93
00:06:08.721 --> 00:06:13.640
Z and h as input to our recurrent net to create a new hidden state.

94
00:06:13.940 --> 00:06:18.320
We repeat this process until the game is over.
The results were incredible.

95
00:06:18.321 --> 00:06:23.321
It learned how to drive really well and since their world model is able to model

96
00:06:23.421 --> 00:06:28.190
the future they use it's predictive abilities to generate a dreamlike world.

97
00:06:28.580 --> 00:06:32.860
They asked it to produce the probability distribution of the VA ease hidden

98
00:06:32.870 --> 00:06:36.020
state,
which ended up being predicted game frames.

99
00:06:36.290 --> 00:06:38.810
When these game frames are generated by em,

100
00:06:38.870 --> 00:06:43.700
it ends up creating a dreamy game environment which they could put the trained

101
00:06:43.701 --> 00:06:48.701
controller into and watch the car execute driving actions inside of the train

102
00:06:49.911 --> 00:06:54.800
the agent inside its own dream and transferred that policy to the actual

103
00:06:54.801 --> 00:06:55.634
environment.

104
00:06:56.180 --> 00:07:01.040
This approach of training and AI inside of a simulated latent space,

105
00:07:01.100 --> 00:07:03.800
dreamworld has practical applications.

106
00:07:04.190 --> 00:07:09.190
Game engines require using heavy compute resources for rendering game states

107
00:07:09.350 --> 00:07:12.860
onto image frames.
We could avoid wasting cycles,

108
00:07:12.861 --> 00:07:17.420
training an agent in the actual environment and instead train the agent inside

109
00:07:17.450 --> 00:07:19.190
it's simulated environment.

110
00:07:19.610 --> 00:07:24.440
While the human brain can hold decades or even centuries of memories,

111
00:07:24.750 --> 00:07:29.600
neural networks trained with backpropagation have more of a limited capacity.

112
00:07:29.810 --> 00:07:34.810
Future versions of this work could use higher capacity models or use an external

113
00:07:35.120 --> 00:07:38.570
memory module three points to remember from this video.

114
00:07:38.840 --> 00:07:42.980
The world models paper demonstrated that an AI could learn by training and its

115
00:07:43.040 --> 00:07:47.330
own simulated environment.
There are model use,
a vision,

116
00:07:47.360 --> 00:07:52.190
memory and controller module to learn from its environment and its vision.

117
00:07:52.191 --> 00:07:57.191
Model of variational auto encoder is generative able to produce variations of

118
00:07:57.921 --> 00:08:01.520
the input data.
Hi,
one,
a master,
any programming language,

119
00:08:01.550 --> 00:08:06.200
we'll hit subscribe and it'll happen for now.
I've got to travel to Europe again,

120
00:08:06.290 --> 00:08:07.730
so thanks for watching.

