WEBVTT

1
00:00:00.060 --> 00:00:00.751
Hello world,

2
00:00:00.751 --> 00:00:04.770
it's Saroj and today we're going to learn how to generate videos using the

3
00:00:04.771 --> 00:00:07.830
hottest type of model in machine learning.
Right now,

4
00:00:08.040 --> 00:00:10.500
the generative adversarial network.

5
00:00:10.740 --> 00:00:14.760
We'll train our model to generate the alien language from the movie arrival that

6
00:00:14.761 --> 00:00:15.930
we can later animate.

7
00:00:16.230 --> 00:00:21.090
In 2014 everyone was trenching themselves in ice water for the als challenge,

8
00:00:21.120 --> 00:00:22.770
but meanwhile in Canada,

9
00:00:22.890 --> 00:00:27.510
a researcher named Ian Goodfellow published a paper introducing the world to

10
00:00:27.511 --> 00:00:30.870
Ganz and the Ai community loved it down.

11
00:00:30.871 --> 00:00:34.980
Macoun the director of AI at Facebook called it the most interesting idea and

12
00:00:34.981 --> 00:00:36.900
the last two decades in machine learning.

13
00:00:37.230 --> 00:00:40.170
I recently interviewed Ian and ask him a bit about its history.

14
00:00:40.200 --> 00:00:41.760
How did you come up with this idea

15
00:00:41.850 --> 00:00:45.420
<v 1>for guns?
The short story is that I was arguing with my friends in a bar.</v>

16
00:00:48.300 --> 00:00:51.870
The thing that clicked there and the bar was this idea of having the

17
00:00:51.871 --> 00:00:55.470
discriminator continually learn at the same time that the generator is learning.

18
00:00:55.650 --> 00:00:57.630
If both muddles learn,

19
00:00:57.900 --> 00:01:01.470
they're driven to this equilibrium where it becomes impossible to fool the

20
00:01:01.471 --> 00:01:02.050
description.

21
00:01:02.050 --> 00:01:06.910
<v 0>Gans are a type of generative model given some input,
data x and some labels.</v>

22
00:01:06.911 --> 00:01:11.560
Why it learns the joint probability distribution of this data with the joint

23
00:01:11.561 --> 00:01:13.270
probability distribution function.

24
00:01:13.390 --> 00:01:18.070
Given a y you can calculate or generate its respective x and since this is

25
00:01:18.130 --> 00:01:21.190
unsupervised learning,
we have no labels handed to us.

26
00:01:21.340 --> 00:01:26.260
We use the training data as y and the generated data as acts to so given say a

27
00:01:26.261 --> 00:01:30.730
set of three d models of major cities from across the world from Google Earth.

28
00:01:31.030 --> 00:01:34.690
There are thousands of different features or dimensions to this data.

29
00:01:34.900 --> 00:01:37.660
Can we generate an entirely new city from it?

30
00:01:37.750 --> 00:01:41.530
Atlantis is waiting and what constitutes a good city?

31
00:01:41.770 --> 00:01:46.450
Sometimes a single input corresponds to many different correct answers,

32
00:01:46.660 --> 00:01:48.040
each of which is acceptable.

33
00:01:48.370 --> 00:01:52.240
Generative models help us work with multimodal outputs like this.

34
00:01:52.300 --> 00:01:55.300
There's been so much activity centered around gans lately.

35
00:01:55.300 --> 00:01:59.740
They've been used to convert low res images into crisp high res images,

36
00:01:59.920 --> 00:02:04.210
convert hand drawn sketches into photorealistic images and to generate

37
00:02:04.480 --> 00:02:08.680
everything from fashion styles to new product categories.
Apple,

38
00:02:08.681 --> 00:02:12.580
you should probably take a look into that last one by gans cycle.

39
00:02:12.581 --> 00:02:14.350
Gans W gads.
There's a lot.

40
00:02:14.500 --> 00:02:17.650
So the basic idea here is that we have two neural nets.

41
00:02:17.860 --> 00:02:21.700
One is called the generator g and the other is called the discriminator d.

42
00:02:21.970 --> 00:02:23.110
We've got some dataset.

43
00:02:23.140 --> 00:02:28.120
Let's say it's a collection of Pokemon images and we want to generate new images

44
00:02:28.121 --> 00:02:29.030
from this dataset.

45
00:02:29.110 --> 00:02:34.110
That means entirely new Pokemon that have similar attributes to those in our

46
00:02:34.601 --> 00:02:35.530
training data.

47
00:02:35.800 --> 00:02:40.480
The generators job is to try to create fake Pokemon that look really similar to

48
00:02:40.481 --> 00:02:42.650
our training Pokemon.
Ah,

49
00:02:42.940 --> 00:02:47.020
the discriminators job is to classify the generated Pokemon has either real or

50
00:02:47.021 --> 00:02:50.770
fake.
Think of Ge as a magician and D as his audience.

51
00:02:50.980 --> 00:02:54.880
The magician is constantly trying to make the audience believe that he's

52
00:02:54.881 --> 00:02:56.590
illusions are real magic.

53
00:02:56.800 --> 00:03:00.610
The audience boos when it can tell that his tricks fake and applauds,

54
00:03:00.611 --> 00:03:05.340
when it can't tell the difference,
they both improve and in the ideal case,

55
00:03:05.430 --> 00:03:10.260
the magician gets so good that no matter what he's able to full his audience

56
00:03:10.290 --> 00:03:14.820
every time the type of gain will use in this video is called a deep

57
00:03:14.850 --> 00:03:17.310
convolutional Gan or DC.
Again,

58
00:03:17.640 --> 00:03:21.570
this is because both d and g are deep convolutional neural nets,

59
00:03:21.720 --> 00:03:26.190
so the discriminator has several layers of what's called convolution and the

60
00:03:26.191 --> 00:03:28.980
generators got several layers of de convolution.

61
00:03:29.160 --> 00:03:33.750
Convolution is literally impossible to understand in a parallel universe,

62
00:03:33.751 --> 00:03:38.130
but in this one it's pretty easy.
Normally neural net layers are fully connected,

63
00:03:38.131 --> 00:03:42.570
so all inputs are connected to all outputs with many types of data.

64
00:03:42.600 --> 00:03:43.650
This makes sense.

65
00:03:43.770 --> 00:03:48.060
We want all parts of the input data to be able to contribute to all parts of the

66
00:03:48.061 --> 00:03:53.040
output prediction,
but images are considered spatially,
locally correlated.

67
00:03:53.040 --> 00:03:55.230
That means if there's a banana and an image,

68
00:03:55.350 --> 00:03:58.410
it doesn't matter where it is in the image,
it's still a banana.

69
00:03:58.590 --> 00:03:59.730
So we exploit that.

70
00:03:59.790 --> 00:04:04.530
Instead of connecting all of the input data or pixels to all output values,

71
00:04:04.680 --> 00:04:09.360
we use a much smaller filter that we slide across the picture like a flashlight,

72
00:04:09.450 --> 00:04:14.310
so that means that much fewer parameters in a convolutional layer as opposed to

73
00:04:14.311 --> 00:04:16.800
a fully connected one.
Let's look at our code.

74
00:04:16.890 --> 00:04:19.110
We're only going to use care os to build our model.

75
00:04:19.140 --> 00:04:22.740
Pillow helps us do image processing and num Pi will help us perform some

76
00:04:22.741 --> 00:04:25.500
valuable reshaping operations on our images.

77
00:04:25.770 --> 00:04:28.230
Let's get started by first defining our discriminator.

78
00:04:28.260 --> 00:04:31.620
We'll give it its own function.
It's going to be a linear stack of layers,

79
00:04:31.710 --> 00:04:35.670
so we'll define it as sequential and we'll start off with two convolutional

80
00:04:35.671 --> 00:04:36.390
blocks.

81
00:04:36.390 --> 00:04:40.800
That means a convolution to extract the feature map followed by the Tan h

82
00:04:40.830 --> 00:04:45.570
activation function to squash real numbers into a range between negative one and

83
00:04:45.571 --> 00:04:49.440
one which lets our model learn more complex functions than just linear

84
00:04:49.441 --> 00:04:50.274
regression.

85
00:04:50.400 --> 00:04:54.810
Then a pulling wire pulling reduces the dimensionality of each feature map but

86
00:04:54.811 --> 00:04:57.240
retains the most relevant information.

87
00:04:57.660 --> 00:05:00.090
Will flatten the feature map into one dimension.

88
00:05:00.150 --> 00:05:04.440
Then applied to fully connected layers to it.
The last dense layer,

89
00:05:04.530 --> 00:05:08.880
I'll put an end dimensional vector where n is the number of classes we have,

90
00:05:09.000 --> 00:05:12.780
so it would be to,
in our case and by applying a sigmoid to it,

91
00:05:12.960 --> 00:05:16.410
it'll convert the data into probability values for each one.

92
00:05:17.010 --> 00:05:20.350
<v 2>Dan's got a train them.
It's Dng.</v>

93
00:05:21.410 --> 00:05:23.630
Let's use cross and true p and g,

94
00:05:23.770 --> 00:05:28.770
enhance gradients to this and is it real or just pretend.

95
00:05:28.940 --> 00:05:32.150
<v 0>Next we'll define our generator which performs similar operations,</v>

96
00:05:32.151 --> 00:05:36.170
but in the reverse order.
Since it's fed random numbers as its input,

97
00:05:36.350 --> 00:05:40.610
it converts them into an image by first going through to fully connected layers

98
00:05:40.730 --> 00:05:45.200
with their own associated activation functions.
Batch normalization,

99
00:05:45.320 --> 00:05:49.640
we'll apply a transformation that maintains the mean activation close to zero

100
00:05:49.880 --> 00:05:52.640
and the activation standard deviation close to one.

101
00:05:52.880 --> 00:05:56.690
This allows for faster learning and higher overall accuracy.

102
00:05:57.020 --> 00:06:01.400
Then we apply to convolutional blocks that will eventually output an image.

103
00:06:01.700 --> 00:06:04.850
Upsampling will convert our image into a higher resolution.

104
00:06:05.090 --> 00:06:09.560
Now that we've defined models for both D and g,
we can combine them to make,

105
00:06:09.561 --> 00:06:13.880
again pretty easily with care os we can just reuse the same network objects

106
00:06:13.881 --> 00:06:18.080
we've already instantiated and they'll conveniently maintain the same shared

107
00:06:18.081 --> 00:06:20.720
weights.
With the previously compiled models,

108
00:06:21.020 --> 00:06:24.530
we're going to want to freeze waits in the discriminator part of the game when

109
00:06:24.531 --> 00:06:26.230
we back propagate the joint models.

110
00:06:26.230 --> 00:06:30.500
So we'll first set the OS trainable flag to false for each element.

111
00:06:30.501 --> 00:06:32.090
In this part of the network,

112
00:06:32.270 --> 00:06:36.920
we'll also define a low data function which will form our images into vectors

113
00:06:37.040 --> 00:06:41.780
that we can feed to our model by making use of the image and num py libraries.

114
00:06:42.230 --> 00:06:44.210
Okay,
so how do we train this thing?

115
00:06:44.510 --> 00:06:49.100
The goal of training our discriminator is to maximize the of x for every image

116
00:06:49.130 --> 00:06:53.720
from the true data distribution and minimize the effects for every image,

117
00:06:53.870 --> 00:06:55.430
not from the true data distribution.

118
00:06:55.730 --> 00:07:00.210
The goal of training the generator GMC is to create samples that full DDI.

119
00:07:00.500 --> 00:07:04.730
We trained both D and g by taking the gradients of this expression with respect

120
00:07:04.731 --> 00:07:09.140
to their parameters because each player's cost depends on the other players

121
00:07:09.141 --> 00:07:13.130
parameters,
but each player cannot control the other players parameters.

122
00:07:13.370 --> 00:07:18.370
This scenario is most straightforward to describe as a game rather than an

123
00:07:18.500 --> 00:07:19.760
optimization problem.

124
00:07:20.000 --> 00:07:24.950
Specifically a mini Max scape mini Max is a strategy of always minimizing the

125
00:07:24.951 --> 00:07:28.760
maximum possible loss,
which can result from a choice that a player makes.

126
00:07:29.030 --> 00:07:33.350
And the Nash equilibrium is where the optimal outcome of a game is.

127
00:07:33.351 --> 00:07:38.351
One where no player has an incentive to deviate from his chosen strategy after

128
00:07:38.511 --> 00:07:40.310
considering an opponent's choice.

129
00:07:40.370 --> 00:07:44.840
So the way we train Ganz is to find the Nash equilibrium of a mini Max game

130
00:07:44.841 --> 00:07:45.980
between g and D.

131
00:07:46.190 --> 00:07:49.880
We'll load our data and initialize both of our models in our train function.

132
00:07:50.060 --> 00:07:53.810
Then we'll combine them both and initialize to castic gradient descent

133
00:07:53.840 --> 00:07:57.350
optimizers for both of them will define a loss function.

134
00:07:57.380 --> 00:08:02.150
Binary Cross entropy then performed the set of steps outlined in the paper for

135
00:08:02.151 --> 00:08:05.390
every time step.
We sampled data from both distributions,

136
00:08:05.420 --> 00:08:10.160
then update d using our gradients.
Then we update g using our gradients.

137
00:08:10.370 --> 00:08:11.510
Once we're done training,

138
00:08:11.540 --> 00:08:16.040
we can generate some images from GE and we'll see that they do indeed look

139
00:08:16.041 --> 00:08:17.930
pretty similar to our other images.

140
00:08:18.200 --> 00:08:21.440
We can even stitch a bunch of these images together to make a video.

141
00:08:21.680 --> 00:08:24.540
This can be applied to any image or video data set.

142
00:08:24.541 --> 00:08:29.480
Since videos are just collections of images,
we just feed them in one at a time.

143
00:08:29.740 --> 00:08:32.120
All right,
so what are the main takeaways here?

144
00:08:32.450 --> 00:08:36.800
Generative adversarial networks are a framework for generating realistic samples

145
00:08:36.801 --> 00:08:39.920
from random noise.
They consist of two neural nets,

146
00:08:39.950 --> 00:08:44.840
a generator that creates fake samples and a discriminator that tries to judge if

147
00:08:44.841 --> 00:08:49.700
those samples are real or fake,
and we optimize them through back propagation,

148
00:08:49.701 --> 00:08:54.701
which helps find the Nash equilibrium of the mini Max game between both g and D.

149
00:08:55.170 --> 00:08:58.710
First place for last week's coding challenge goes to an ammonia totemic.

150
00:08:58.980 --> 00:09:03.780
Nemanja used a convolutional variational autoencoder to generate videos by

151
00:09:03.781 --> 00:09:05.670
training on my own videos.

152
00:09:06.000 --> 00:09:08.610
He also added a recurrent network to this architecture,

153
00:09:08.611 --> 00:09:12.270
which I haven't seen done before.
Very cool results in the Jupiter notebook.

154
00:09:12.300 --> 00:09:16.120
Definitely check it out.
Was there of the week and the runner up is Neoss.

155
00:09:16.130 --> 00:09:21.130
Muhammad neons used a vie to generate Pokemon and wrote up a really cool blog

156
00:09:21.391 --> 00:09:26.010
post on his whole process.
You guys inspire me and I vow to you.

157
00:09:26.250 --> 00:09:29.730
This week's coding challenge is to use again to generate some video.

158
00:09:30.060 --> 00:09:33.000
Decals are in the read me get humbling SCO in the comment and winters are going

159
00:09:33.001 --> 00:09:34.200
to be announced next week.

160
00:09:34.260 --> 00:09:37.170
Please subscribe for more programming videos and for now I've got to

161
00:09:37.171 --> 00:09:40.650
discriminate between data,
so thanks for watching.

