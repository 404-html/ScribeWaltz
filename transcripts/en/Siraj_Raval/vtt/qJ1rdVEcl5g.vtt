WEBVTT

1
00:00:02.630 --> 00:00:05.500
Hey Andrew,
how's it going?
Pretty good.
How are you?
Pretty good.

2
00:00:05.550 --> 00:00:08.900
Can I ask you 67 questions?
Yeah,
sure.
Let's take a walk.

3
00:00:09.620 --> 00:00:11.690
What is one thing you really like about London?

4
00:00:12.110 --> 00:00:13.740
<v 2>Um,
the music and culture</v>

5
00:00:14.450 --> 00:00:18.170
<v 1>favorite part of the software stack.
Um,
the machine learning side.</v>

6
00:00:19.040 --> 00:00:23.900
You're getting your phd from Oxford.
Describe Oxford life in one word.
Uh,
intense.

7
00:00:24.980 --> 00:00:26.840
What was the last project you worked on?

8
00:00:27.310 --> 00:00:29.620
<v 2>Uh,
an open source project called open mind.</v>

9
00:00:29.670 --> 00:00:32.020
Just looking to decentralize the control of AI.

10
00:00:32.660 --> 00:00:35.720
<v 1>If you could spend an evening with one famous person,
who would it be?</v>

11
00:00:40.890 --> 00:00:41.723
<v 2>Barack Obama.</v>

12
00:00:42.410 --> 00:00:43.850
<v 1>What do you do to relax?</v>

13
00:00:44.570 --> 00:00:45.403
<v 2>Uh,
I play music.</v>

14
00:00:47.100 --> 00:00:48.180
<v 1>Do you miss the US?</v>

15
00:00:48.870 --> 00:00:53.170
<v 2>I do all the time.
Uh,
it's,
uh,
it,</v>

16
00:00:53.610 --> 00:00:55.980
I don't think you realize how important it is to develop relationships in a

17
00:00:55.981 --> 00:00:59.210
place until,
uh,
until you leave it.
And then you know,

18
00:00:59.310 --> 00:01:01.320
those people who are far away.
So yeah,
I miss them all the time.

19
00:01:02.880 --> 00:01:04.200
<v 1>What got you into AI?</v>

20
00:01:04.860 --> 00:01:05.230
<v 2>Um,</v>

21
00:01:05.230 --> 00:01:10.210
I originally wanted to do it mostly for fun and also I thought that software

22
00:01:10.211 --> 00:01:14.050
would be a really compelling way to be able to travel because I like being able

23
00:01:14.051 --> 00:01:17.320
to work from my laptop and at the time I was really interested in playing music

24
00:01:17.350 --> 00:01:21.460
and I was playing with a band and,
and a few other places and uh,
yeah,

25
00:01:21.461 --> 00:01:24.440
I thought it would be a cool place to call it a travel,
uh,
and,

26
00:01:24.441 --> 00:01:26.940
and work on my laptop on a tour bus.
Um,

27
00:01:27.220 --> 00:01:31.930
and then discovered AI through a course at my university and just totally got

28
00:01:31.931 --> 00:01:32.764
hooked.

29
00:01:34.050 --> 00:01:35.730
<v 1>Last thing you made with your hands,</v>

30
00:01:38.660 --> 00:01:39.493
<v 0>a song,</v>

31
00:01:41.810 --> 00:01:43.100
<v 1>last person you emailed.</v>

32
00:01:46.000 --> 00:01:46.833
<v 0>Ooh,</v>

33
00:01:47.320 --> 00:01:52.320
<v 2>I cold emailed a researcher in Tokyo who is doing research on the centralized AI</v>

34
00:01:53.800 --> 00:01:56.920
because the community is so small and I went to get to know everyone who's in
it.

35
00:01:57.660 --> 00:02:00.000
<v 1>What advice would you give yourself 10 years ago?</v>

36
00:02:03.340 --> 00:02:04.173
<v 0>Okay.</v>

37
00:02:05.030 --> 00:02:06.590
<v 2>An open mind to the different opportunities.</v>

38
00:02:06.950 --> 00:02:10.880
Don't commit to hard to just one and um,
yeah,

39
00:02:10.881 --> 00:02:13.610
be ready for the,
the road that life might take you on.

40
00:02:14.290 --> 00:02:17.080
<v 1>What website do you waste time on Twitter?</v>

41
00:02:18.550 --> 00:02:21.790
You get an all expenses paid trip to one city,
where do you go?

42
00:02:21.960 --> 00:02:25.170
<v 2>Uh,
Toronto going next week to the decentralized Ai Conference.</v>

43
00:02:25.800 --> 00:02:27.990
<v 1>Would you visit Mars?
Absolutely.</v>

44
00:02:28.890 --> 00:02:31.020
What is one thing you can't live without?

45
00:02:31.590 --> 00:02:32.830
<v 2>Uh,
my wife for sure.</v>

46
00:02:33.590 --> 00:02:35.930
<v 1>Where do you envision machine learning will take us?</v>

47
00:02:37.250 --> 00:02:40.140
<v 2>Um,
I think that's more a question on,</v>

48
00:02:40.320 --> 00:02:45.320
on how governance and sort of capitalism versus a redistribution of wealth plays

49
00:02:46.381 --> 00:02:47.214
out.
Um,

50
00:02:47.310 --> 00:02:51.390
because AI is mostly just a proxy for power and it's often controlled by the

51
00:02:51.391 --> 00:02:53.430
people who control capital.
Um,

52
00:02:54.300 --> 00:02:59.280
so all that has say it's really hard to tell,
but if history is any indicator,

53
00:02:59.360 --> 00:03:03.340
um,
it's gonna be really hard to keep it from being aggravated into a few kind of

54
00:03:03.341 --> 00:03:04.330
small powerful players.

55
00:03:04.980 --> 00:03:07.380
<v 1>Do you think religion has a place in science?</v>

56
00:03:08.870 --> 00:03:12.190
<v 2>Ah,
I do,
which is a,
a bit of a contrarian view.
Um,</v>

57
00:03:12.470 --> 00:03:17.470
I view religion as a large body of work that is attempting to understand and

58
00:03:17.901 --> 00:03:20.340
record things that are unexplainable.
Um,

59
00:03:20.600 --> 00:03:23.750
whereas science is a corpus of work that attempts to explain things that are

60
00:03:23.751 --> 00:03:26.300
explainable and are repeatable.
Um,

61
00:03:26.301 --> 00:03:28.130
and I think both are ultimately trying to learn.

62
00:03:30.010 --> 00:03:31.490
They're both trying to learn the same thing,

63
00:03:31.491 --> 00:03:35.090
which is where are we and who are we and what really matters.

64
00:03:36.720 --> 00:03:40.310
And I think that if you're trying to figure out what the world is about
yourself,

65
00:03:42.080 --> 00:03:42.980
you should take both into account.

66
00:03:43.660 --> 00:03:45.580
<v 1>What text editor do you use?</v>

67
00:03:46.400 --> 00:03:49.520
<v 2>I usually use sublime text,
but I'm using Adam more lately.
It's pretty cool.</v>

68
00:03:50.080 --> 00:03:52.140
<v 1>What open source library do you really like?</v>

69
00:03:52.390 --> 00:03:53.960
<v 2>Uh,
Pi Torch by torch.</v>

70
00:03:54.800 --> 00:03:56.360
<v 1>What's one of your favorite movies?</v>

71
00:03:56.640 --> 00:03:57.710
<v 2>Um,
Ooh,</v>

72
00:04:01.990 --> 00:04:06.310
Steve used to be my favorite.
Um,
but I really like interstellar.

73
00:04:06.640 --> 00:04:07.690
I think it's awesome.
Yeah.

74
00:04:08.120 --> 00:04:10.580
<v 1>What's one of your hobbies outside of machine learning?</v>

75
00:04:10.930 --> 00:04:12.210
<v 2>It's a plain cannon.</v>

76
00:04:13.260 --> 00:04:15.180
<v 1>What was the first program you ever made?</v>

77
00:04:16.150 --> 00:04:16.720
<v 2>Um,</v>

78
00:04:16.720 --> 00:04:21.700
I was in the second grade and my dad helps me make a simple calculator in c plus

79
00:04:21.701 --> 00:04:25.480
plus and I've put it on a floppy disk and show it to my teacher at school.

80
00:04:25.870 --> 00:04:27.820
<v 1>What's a music genre you're really into?</v>

81
00:04:28.650 --> 00:04:33.190
<v 2>Uh,
movie soundtracks because they make me feel like whatever I'm doing is epic.</v>

82
00:04:34.380 --> 00:04:36.830
<v 1>Is there a programming language that you really don't like it?</v>

83
00:04:37.040 --> 00:04:40.080
<v 2>Um,
is it bad to say Louis</v>

84
00:04:42.140 --> 00:04:44.330
<v 1>programming,
language of choice for machine learning?</v>

85
00:04:44.690 --> 00:04:45.850
<v 2>Uh,
Python for sure.</v>

86
00:04:46.970 --> 00:04:47.900
<v 1>Favorite professor,</v>

87
00:04:48.760 --> 00:04:51.660
<v 2>Dr. Hooper who first got me into AI and programming.
I'm going to go way.</v>

88
00:04:53.270 --> 00:04:56.240
<v 1>What's one area of deep learning that interests you?
A lot?</v>

89
00:04:58.770 --> 00:05:02.120
<v 2>Um,
I'm really interested in figuring out how</v>

90
00:05:03.740 --> 00:05:06.110
this is more in the deep cleaning industry than the deep learning algorithms

91
00:05:06.111 --> 00:05:10.340
itself.
But I think that,
and I tweeted this the other day,

92
00:05:10.341 --> 00:05:13.220
like algorithms are more commodities to the extent that everyone gives them away

93
00:05:13.221 --> 00:05:14.054
for free.

94
00:05:14.150 --> 00:05:18.500
So research into algorithms while it helps push forward what humanity can do,

95
00:05:18.950 --> 00:05:21.830
I think it has less of an impact on what we will do.

96
00:05:22.250 --> 00:05:25.580
And that's more controlled by sort of the data acquisition side of the industry.

97
00:05:26.060 --> 00:05:29.240
So as far as something that I think is the most interesting and has the most

98
00:05:29.241 --> 00:05:32.510
opportunity for social impact,
I don't think it's building the next great,

99
00:05:34.160 --> 00:05:37.640
you know,
layer,
right.
I think it's probably a building,

100
00:05:37.970 --> 00:05:42.970
the next great control structure for the natural resource that the deep learning

101
00:05:43.671 --> 00:05:44.750
consumes,
which is data.

102
00:05:45.730 --> 00:05:48.520
<v 1>What is a misconception that people have about deep learning</v>

103
00:05:49.270 --> 00:05:51.320
<v 2>that it's about the algorithms instead of about the data,
like,</v>

104
00:05:51.321 --> 00:05:55.820
like algorithms just extract information from large data sets.
Um,

105
00:05:56.540 --> 00:06:00.380
and that's probably the greatest lesson that my supervisor has taught me so far.

106
00:06:00.530 --> 00:06:01.400
Uh,
while at Oxford

107
00:06:01.870 --> 00:06:03.730
<v 1>name one person you really admire,</v>

108
00:06:07.710 --> 00:06:08.543
<v 0>right?</v>

109
00:06:10.290 --> 00:06:12.450
<v 2>I really admire the,
um,</v>

110
00:06:13.620 --> 00:06:18.620
this is really controversial but the former CEO of Firefox because I think that

111
00:06:19.770 --> 00:06:20.603
uh,

112
00:06:22.530 --> 00:06:26.370
people are generally rewarded now for not acting on their beliefs and for being

113
00:06:26.371 --> 00:06:30.380
really passive.
Um,
and whereas there's definitely a case for that.
And,

114
00:06:30.410 --> 00:06:32.370
and I'm a very open and accepting individual.

115
00:06:32.660 --> 00:06:35.700
I also have tremendous respect for people who put it all on the line for,

116
00:06:35.730 --> 00:06:39.360
for things that,
that they really believe in.
And I think that he did that.

117
00:06:40.020 --> 00:06:42.090
<v 1>What's the most beautiful thing in the world?</v>

118
00:06:43.850 --> 00:06:44.683
<v 2>A smile.</v>

119
00:06:46.240 --> 00:06:48.610
<v 1>What do you want to work on the most at deep mind?</v>

120
00:06:49.260 --> 00:06:53.950
<v 2>Um,
I'm really interested in long term memory.
So,
so,
uh,</v>

121
00:06:54.250 --> 00:06:57.640
neural nets are really good at modeling short sequences and short range

122
00:06:57.641 --> 00:07:00.580
dependencies and,
and saying,
you know,
given the beginning of a sentence,

123
00:07:00.581 --> 00:07:03.850
finish it coherently,
but they have a really hard time saying,
ah,

124
00:07:03.851 --> 00:07:07.150
I think I've read this before,
you know,
2000 pages ago.

125
00:07:07.780 --> 00:07:10.210
And even finding exactly the same thing again.

126
00:07:10.211 --> 00:07:14.470
So what I'd really like to work on there is his long term memory and episodic

127
00:07:14.471 --> 00:07:16.750
memory,
especially as it pertains to reinforcement learning.

128
00:07:17.650 --> 00:07:20.300
<v 1>What are the people you've met had keep mind so far?</v>

129
00:07:20.640 --> 00:07:25.020
<v 2>Um,
generous,
collaborative.
Some of them are hilarious.</v>

130
00:07:26.970 --> 00:07:30.690
Some of them are the kind of a group of odd balls and they know it and they

131
00:07:30.691 --> 00:07:33.060
don't take themselves too seriously.
Uh,

132
00:07:33.720 --> 00:07:36.390
I remember the one of the first guys I met,
he was wearing

133
00:07:38.540 --> 00:07:42.690
a black kilts,
a pink bow tie,
and had like,
just this crazy haircut.

134
00:07:42.960 --> 00:07:45.720
And I just remember thinking like,
wow,
that guy,

135
00:07:46.050 --> 00:07:49.820
that guy's got to figure it out in that he's happy to be whoever is,

136
00:07:49.870 --> 00:07:54.390
he's going to be wherever he's at and uh,
and work on cool stuff no matter what.

137
00:07:54.391 --> 00:07:59.391
And then I think I grew up in a part of the country in the u s where were sort

138
00:07:59.491 --> 00:08:03.060
of social signals like that would have discounted him from sort of academic

139
00:08:03.061 --> 00:08:07.290
pursuits.
And I that's totally ridiculous.
And I was really,

140
00:08:07.291 --> 00:08:10.740
really glad to see that
some people who,

141
00:08:11.400 --> 00:08:14.970
the group that a lot of people would associate with being a member of the top

142
00:08:15.360 --> 00:08:16.193
right.

143
00:08:18.390 --> 00:08:22.110
Don't disqualify someone just because they have those kinds of social signals
or,

144
00:08:22.111 --> 00:08:25.240
or,
um,
or carry themselves in that way.
Right.

145
00:08:26.400 --> 00:08:28.230
<v 1>What is one thing that excites you?</v>

146
00:08:30.190 --> 00:08:32.330
<v 2>Um,
hanging out with my wife.</v>

147
00:08:33.410 --> 00:08:35.210
<v 1>Best advice you've ever received.</v>

148
00:08:37.830 --> 00:08:38.663
<v 2>MMM.</v>

149
00:08:43.660 --> 00:08:46.300
Can you do something?
You should always know what your goal is

150
00:08:47.890 --> 00:08:50.620
because otherwise you'll never know if he got there.

151
00:08:51.260 --> 00:08:53.150
<v 1>What's the bravest thing you've ever done?</v>

152
00:08:54.360 --> 00:08:58.130
<v 2>That's a very friendly,
um,
I guess</v>

153
00:08:59.060 --> 00:09:02.650
<v 1>get married.
Who's your favorite computer scientist?</v>

154
00:09:02.920 --> 00:09:04.380
That's David computer scientist.

155
00:09:04.940 --> 00:09:09.790
<v 2>Um,
there's a lot of great ones.
It's hard to discriminate.
Um,</v>

156
00:09:10.580 --> 00:09:12.380
I guess Alan Turing is a pretty easy answer on that one.

157
00:09:12.930 --> 00:09:15.480
<v 1>Do you think you'd need a phd to do machine learning?</v>

158
00:09:16.070 --> 00:09:20.390
<v 2>No,
absolutely not.
Um,
and actually with the advent of online education,</v>

159
00:09:21.790 --> 00:09:22.730
there is a lot of,

160
00:09:23.070 --> 00:09:26.450
it was a very strong case to be made that people can learn faster,

161
00:09:26.451 --> 00:09:29.870
just the things they need to know by doing self study.
Um,

162
00:09:30.560 --> 00:09:33.170
and I think that's a really good thing,

163
00:09:33.920 --> 00:09:36.490
but it's also going to upset the establishment,
um,

164
00:09:36.630 --> 00:09:40.580
and an America where education is highly overcapitalized and overhyped,
uh,

165
00:09:40.581 --> 00:09:43.400
there's,
there's a big correction coming.
So no watch out

166
00:09:44.130 --> 00:09:45.570
<v 1>will the singularity happens.</v>

167
00:09:47.340 --> 00:09:52.260
<v 2>Um,
I think that we will definitely get to a place where we can't tell the</v>

168
00:09:52.261 --> 00:09:55.070
difference between whether it's a human or it's not.
Um,

169
00:09:55.470 --> 00:09:56.790
because humans are really good at,

170
00:09:57.570 --> 00:09:59.880
we're really interested in mimicking ourselves.

171
00:09:59.881 --> 00:10:02.920
So if you believe that that's singularity,
then absolutely.
Um,

172
00:10:03.210 --> 00:10:05.190
now whether we'll actually be able to create something that's,
uh,

173
00:10:05.340 --> 00:10:10.080
all powerful being that provides for us and answers all their questions and you

174
00:10:10.081 --> 00:10:13.560
know,
is,
is kind of this all powerful entity.

175
00:10:14.670 --> 00:10:17.100
I think that that's a little bit ridiculous,
mostly because they,
I,

176
00:10:17.101 --> 00:10:20.400
that I've seen learns from data and I don't know that there's enough data out

177
00:10:20.401 --> 00:10:21.770
there at the moment.
So

178
00:10:22.420 --> 00:10:25.630
<v 1>what is one area of machine learning that doesn't get enough love?</v>

179
00:10:28.200 --> 00:10:32.670
<v 2>Uh,
same as before.
Data,
Dataset,
Dataset,</v>

180
00:10:32.860 --> 00:10:35.710
kind of power structures,
which is the natural resource that goes into Ai.

181
00:10:36.960 --> 00:10:40.320
<v 1>What's the most surprising result you've ever gotten from your research?</v>

182
00:10:41.940 --> 00:10:44.370
<v 2>Uh,
bigger is better.</v>

183
00:10:45.520 --> 00:10:46.900
<v 1>What is consciousness?</v>

184
00:10:54.230 --> 00:10:55.063
<v 0>I don't know.</v>

185
00:10:55.740 --> 00:10:57.810
<v 1>What was the last thing that frustrated you?</v>

186
00:10:58.520 --> 00:10:59.353
<v 2>Um,</v>

187
00:11:01.460 --> 00:11:04.790
getting python to package dependencies.
They weren't part of the code.

188
00:11:06.320 --> 00:11:08.240
<v 1>What's your spirit animal?
My spirit animal</v>

189
00:11:10.740 --> 00:11:11.573
<v 0>lion.</v>

190
00:11:12.420 --> 00:11:14.920
<v 1>If you had a time machine,
where would you visit?</v>

191
00:11:18.420 --> 00:11:21.000
<v 0>Either um,</v>

192
00:11:22.660 --> 00:11:23.493
<v 2>the time,</v>

193
00:11:25.240 --> 00:11:28.490
either zero BC or zero d or a

194
00:11:28.730 --> 00:11:31.150
<v 1>since six years because it seems like everything was invented in the 60s.</v>

195
00:11:31.900 --> 00:11:34.040
Tell me a secret about yourself.
Just one

196
00:11:34.340 --> 00:11:36.380
<v 2>secret.
Um,</v>

197
00:11:40.360 --> 00:11:41.990
<v 0>I tried to</v>

198
00:11:42.620 --> 00:11:43.453
<v 2>don't have any secrets.</v>

199
00:11:44.600 --> 00:11:47.210
<v 1>Where would you like to see yourself in the next five years?</v>

200
00:11:48.180 --> 00:11:49.013
<v 2>Um,</v>

201
00:11:52.820 --> 00:11:54.370
I currently trying to figure out,

202
00:11:55.060 --> 00:11:58.570
given the set of skills that I'm trying to acquire and then have the opportunity

203
00:11:58.571 --> 00:12:00.130
to acquire,
uh,

204
00:12:02.050 --> 00:12:06.970
what use of that actually matters and can actually have a real impact other than

205
00:12:07.150 --> 00:12:10.150
making a company a bunch of money or automating a process.
Like,

206
00:12:10.151 --> 00:12:13.750
it just seems like there's a lot of near misses in the AE industry right now for

207
00:12:13.810 --> 00:12:15.530
applying these skills.
So,
um,

208
00:12:16.270 --> 00:12:18.460
that's kind of the question of the hour that I don't have the answer to yet,

209
00:12:18.461 --> 00:12:19.294
but I'm working on it.

210
00:12:20.300 --> 00:12:22.940
<v 1>Who's one person you would love to collaborate with?</v>

211
00:12:25.180 --> 00:12:26.013
<v 0>Hmm.</v>

212
00:12:26.730 --> 00:12:29.130
<v 1>Zuckerberg,
favorite Linux,</v>

213
00:12:29.131 --> 00:12:32.700
distro of boon to what's the point of life?

214
00:12:33.610 --> 00:12:37.420
<v 0>Um,
Ooh,
controversy,
controversial questions.</v>

215
00:12:38.120 --> 00:12:39.460
Um,

216
00:12:44.540 --> 00:12:47.510
love God and love people.
Uh,
and yeah.

217
00:12:48.380 --> 00:12:51.620
<v 1>Have you built an app with machine learning that you've used in your day to day</v>

218
00:12:51.621 --> 00:12:52.454
life?

219
00:12:52.600 --> 00:12:55.030
<v 2>No,
actually got an,
a programming to do APP development,</v>

220
00:12:55.090 --> 00:12:57.370
but I've spent more time on the backend lately,

221
00:12:57.400 --> 00:12:58.480
which is something I'd like to change.

222
00:12:58.750 --> 00:13:00.640
<v 1>What's something you've learned in the past month?</v>

223
00:13:01.610 --> 00:13:02.630
<v 0>In the past month?</v>

224
00:13:06.050 --> 00:13:06.883
Um,

225
00:13:07.610 --> 00:13:09.950
<v 2>learned a lot of new things about homomorphic encryption.
Um,</v>

226
00:13:10.040 --> 00:13:13.400
that's kind of been a pursuit of study over the last six months to a year.
Um,

227
00:13:14.030 --> 00:13:19.030
and I've also learned that there's an incredible amount of knowledge to be,

228
00:13:21.710 --> 00:13:25.100
can you find a lot of new uses if you can find different fields in the

229
00:13:25.101 --> 00:13:28.370
scientific study where people have done a ton of work but they haven't done a

230
00:13:28.371 --> 00:13:31.460
lot of kind of cross work and like homework and encryption,

231
00:13:31.820 --> 00:13:34.860
deep learning and even to some extent federal had learning.
Uh,

232
00:13:34.970 --> 00:13:38.480
and blockchain are like these sort of these things where people have worked

233
00:13:38.481 --> 00:13:42.690
really,
really hard independent kind of silos of research.
Uh,

234
00:13:42.740 --> 00:13:44.970
but there's so much interesting low hanging fruit if you,

235
00:13:44.980 --> 00:13:46.730
if you look at the cross between them.
Um,

236
00:13:46.731 --> 00:13:49.040
and I think it's easy to look at that pursuit and just go,
oh,

237
00:13:49.310 --> 00:13:50.720
buzzwords words or something.
But,

238
00:13:50.750 --> 00:13:55.430
but these are legitimately interesting technologies and there's not,

239
00:13:55.970 --> 00:13:58.130
I think we're only now starting to try to figure out,
okay,

240
00:13:58.131 --> 00:13:59.900
how can we take kind of the power,

241
00:14:00.230 --> 00:14:03.490
the power dynamics that we love about blockchain,
the,

242
00:14:03.610 --> 00:14:08.000
the privacy dynamics that we love about encryption.
Um,
as well as,

243
00:14:08.060 --> 00:14:11.580
as the intelligence of,
of deep learning and AI and,

244
00:14:11.800 --> 00:14:13.070
and build the world we want to live in.

245
00:14:13.690 --> 00:14:17.260
<v 1>What is one really cool use case for machine learning that you haven't yet seen</v>

246
00:14:17.261 --> 00:14:18.094
done?

247
00:14:22.650 --> 00:14:23.220
<v 0>Predict</v>

248
00:14:23.220 --> 00:14:27.560
<v 2>what outcomes are going to happen in my life as a result of the decisions that I</v>

249
00:14:27.561 --> 00:14:28.394
make.

250
00:14:29.300 --> 00:14:31.490
<v 1>Do you think will solve intelligence in your lifetime?</v>

251
00:14:35.050 --> 00:14:35.883
<v 0>I hope so.</v>

252
00:14:36.710 --> 00:14:39.530
<v 1>If you were in the world,
what's one law you'd enact</v>

253
00:14:43.460 --> 00:14:44.870
<v 2>do unto others as you'd have them do unto you?</v>

254
00:14:46.010 --> 00:14:48.230
<v 1>What's a problem that you would love to see solved?</v>

255
00:14:50.780 --> 00:14:51.613
<v 0>Um,</v>

256
00:14:54.220 --> 00:14:55.160
<v 2>close mindedness.</v>

257
00:14:57.880 --> 00:14:59.740
<v 1>What were you like as a kid in grade school?</v>

258
00:15:01.100 --> 00:15:05.040
<v 2>Uh,
I asked way too many questions and totally drove my parents and my teachers</v>

259
00:15:05.250 --> 00:15:06.083
crazy.

260
00:15:06.620 --> 00:15:10.610
<v 1>What resource do you use the most to learn about the latest breakthroughs?</v>

261
00:15:11.040 --> 00:15:12.630
<v 2>Twitter.
Yeah,
for sure.</v>

262
00:15:13.480 --> 00:15:15.430
<v 1>What's something that excites you about the future?</v>

263
00:15:17.270 --> 00:15:21.510
<v 2>Uh,
I'm very hopeful that technology will give us the ability to decentralize</v>

264
00:15:22.230 --> 00:15:27.150
interest,
um,
and,
and sort of what we pursue for the first time ever.
Like I,
I,

265
00:15:27.650 --> 00:15:32.650
it seems like we've had to make this trade off where we have to aggregate power

266
00:15:32.671 --> 00:15:36.420
into these certain structures to sort of push us forward because when

267
00:15:36.421 --> 00:15:40.280
everything's all apart,
does this logistical issue with innovating.
But,
um,

268
00:15:41.640 --> 00:15:44.460
it might be possible with the Internet,
with blockchain in the,

269
00:15:44.461 --> 00:15:48.690
like for us to actually innovate in a way where the interests and the return and

270
00:15:48.691 --> 00:15:49.260
the,

271
00:15:49.260 --> 00:15:52.890
the sort of goals that we're trying to achieve is actually built for the masses

272
00:15:52.891 --> 00:15:54.550
and not just for the people who have capital.

273
00:15:55.340 --> 00:15:56.990
<v 1>If you were to start from scratch today,</v>

274
00:15:57.110 --> 00:16:00.010
what is a learning resource that you'd use to learn machine learning?

275
00:16:01.710 --> 00:16:03.040
<v 2>That's a good question.
Um,</v>

276
00:16:03.490 --> 00:16:07.060
but hopefully the things that I've written because that's why I wrote them.
Uh,

277
00:16:07.770 --> 00:16:12.150
but other than that,
I think the Yaseen and degree is really strong.

278
00:16:12.780 --> 00:16:16.710
Um,
and then I would look for 12 years,
12 year olds on youtube who,

279
00:16:16.980 --> 00:16:21.470
who always have some of the best tutorial.
Cool.
All right,
Travis,

280
00:16:21.520 --> 00:16:25.230
thanks for answering those questions.
Yeah,
man.
All right.
Have a good one.
You too.

