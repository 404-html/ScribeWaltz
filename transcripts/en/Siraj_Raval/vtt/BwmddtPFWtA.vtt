WEBVTT

1
00:00:00.060 --> 00:00:03.810
Hello world,
it's Saroj and today we're going to generate words.

2
00:00:03.870 --> 00:00:08.790
So given some book or movie script or any kind of text corpus,

3
00:00:08.820 --> 00:00:12.600
you can,
it's plug and play so you can give it any kind of text corpus.

4
00:00:12.870 --> 00:00:17.430
It will learn how to generate words in the style of that Corpus of text.

5
00:00:17.700 --> 00:00:19.710
And in this case we're going to give it a book.

6
00:00:19.740 --> 00:00:23.370
The book is called metamorphosis by Franz Kafka,

7
00:00:23.550 --> 00:00:27.510
which was a really crazy weird writer from the 20th century.
Anyway,

8
00:00:27.660 --> 00:00:31.440
his cool dude.
Anyway,
we're going to generate words in the style of that book.

9
00:00:31.470 --> 00:00:34.590
And this can be applied to any texts,
any type of text.

10
00:00:34.591 --> 00:00:39.390
It doesn't just have to be words,
it can be code,
it can be h,
you know,
html,

11
00:00:39.391 --> 00:00:43.260
whatever it is.
But that's what we're going to do.
No libraries,
just num Pi.

12
00:00:43.261 --> 00:00:46.680
So I'm going to go through the derivation,
the forward propagation,

13
00:00:46.860 --> 00:00:50.970
calculating the loss,
all the math.
So get ready for some math.

14
00:00:51.000 --> 00:00:54.510
Put on your linear Algebra and calculus hats.
Okay,

15
00:00:54.720 --> 00:00:58.790
so this is kind of what it looks like.
This first image here,
uh,

16
00:00:58.800 --> 00:01:02.130
and I'm going to actually code it as well.
So it's,
I'm not,

17
00:01:02.131 --> 00:01:03.150
I'm not just going to glaze over.

18
00:01:03.151 --> 00:01:06.590
I'm going to code it so we can see the outputs as I go.
Uh,

19
00:01:06.600 --> 00:01:11.510
but the very end part will be,
uh,
I'm going to code the important parts.

20
00:01:11.520 --> 00:01:16.020
Let me just say that.
Okay.
So,
okay,
so check this out.
Given some techs corpus,

21
00:01:16.021 --> 00:01:17.970
it will predict the next character.

22
00:01:18.030 --> 00:01:21.450
So what you're seeing here is it actually pretty predicting the next word,

23
00:01:21.690 --> 00:01:24.390
but we're going to do a character level recurrent networks.

24
00:01:24.391 --> 00:01:27.780
So that means it's going to generate character by characters.
Okay?

25
00:01:27.781 --> 00:01:32.520
So character by character,
by character,
not word by word by word,
okay.

26
00:01:32.521 --> 00:01:33.354
So,
uh,

27
00:01:33.390 --> 00:01:36.690
it's going to be trained for a thousand iterations and the more you train it,

28
00:01:36.750 --> 00:01:37.740
the better it's going to get.

29
00:01:37.740 --> 00:01:40.800
So if you leave this thing running overnight on your laptop,
uh,

30
00:01:40.810 --> 00:01:44.070
then by the time you wake up,
it'll be really good.
However,

31
00:01:44.071 --> 00:01:47.820
I wouldn't recommend training it on your laptop.
As my song says,

32
00:01:48.000 --> 00:01:52.440
I train my models in the cloud now because my laptop takes longer.
Right?

33
00:01:52.470 --> 00:01:55.950
So what is a recurrent network,
right?
What is this?
What is this thing?

34
00:01:56.040 --> 00:01:59.250
We've talked about feedforward networks and I've got two images here of

35
00:01:59.251 --> 00:02:03.540
feedforward networks.
The first image is the most popular image,
right?

36
00:02:03.780 --> 00:02:08.580
It's that uh,
really funky looking neuronal architecture.

37
00:02:08.850 --> 00:02:09.511
But it's,
you know,

38
00:02:09.511 --> 00:02:12.870
it can be kind of confusing if you think about it because it's not like these

39
00:02:13.110 --> 00:02:18.110
neurons are classes and these classes have links to all of the other neurons.

40
00:02:19.561 --> 00:02:23.430
Like it's some kind of linked,
you know,
massive crazy linked list kind of thing.

41
00:02:23.431 --> 00:02:25.770
No,
it's or tree like thing.
It's not really like that.

42
00:02:25.980 --> 00:02:29.550
What's really happening are a series of matrix operations.

43
00:02:29.610 --> 00:02:34.320
So these neurons are the output of a series of Matrix.

44
00:02:34.470 --> 00:02:39.330
These neurons are actually just numbers that we then activate with an activation

45
00:02:39.331 --> 00:02:43.140
function.
So a better way of looking at it would be as a computation graph,

46
00:02:43.141 --> 00:02:46.260
a more mathematically sound way of looking at it.

47
00:02:46.500 --> 00:02:50.160
So if you have some input and you know the input,
it could be anything.

48
00:02:50.310 --> 00:02:54.540
What you would do is you would multiply the input by the weight Matrix,

49
00:02:54.870 --> 00:02:56.340
add a bias value,

50
00:02:56.430 --> 00:03:00.610
and then activate the result of that and that would be your output that you then

51
00:03:00.611 --> 00:03:04.380
feed into the next layer,
a layer that the,

52
00:03:04.390 --> 00:03:05.650
what you see as these neurons,

53
00:03:05.651 --> 00:03:10.651
a layer is actually just the result of a dot product operation followed by

54
00:03:12.041 --> 00:03:16.030
adding a bias value.
If you want to add a bias,
which you should in practice,

55
00:03:16.031 --> 00:03:17.140
you should add a bias.

56
00:03:17.650 --> 00:03:20.350
I built neural networks without biases before for examples,

57
00:03:20.351 --> 00:03:23.200
but you really should add of bias and I'll talk about why in a second,

58
00:03:23.530 --> 00:03:27.250
but you should add a bias value and then you activate the output of that.

59
00:03:27.251 --> 00:03:28.330
And by activates,

60
00:03:28.510 --> 00:03:32.110
I mean you take the output of that doc product plus bias operation,

61
00:03:32.111 --> 00:03:36.010
the output of that and you feed it into a an activation function,

62
00:03:36.040 --> 00:03:37.720
a nonlinearity,

63
00:03:37.930 --> 00:03:41.530
whether that's a sigmoid or tan age or rectified linear unit.

64
00:03:41.830 --> 00:03:45.820
And the reason we do that is so that our network can learn both linear and

65
00:03:46.180 --> 00:03:50.290
nonlinear function because neural networks are universal function approximators.

66
00:03:50.500 --> 00:03:53.020
If we didn't apply an activation function to it,

67
00:03:53.200 --> 00:03:56.110
it would only be able to learn linear functions.

68
00:03:56.230 --> 00:03:58.900
We went to learn nonlinear and linear functions,

69
00:03:59.050 --> 00:04:01.540
and that's what we apply an activation function to it.

70
00:04:01.810 --> 00:04:05.650
Now a great way to remember this whole thing is to just rap about it.

71
00:04:05.650 --> 00:04:10.510
So input Tom's weight,
add a bias,
activate,
repeat.

72
00:04:10.511 --> 00:04:15.430
Here we go.
Inputs singing with me.
Tom's weight,
add a bias,
activate,

73
00:04:15.431 --> 00:04:19.810
repeat.
And you just do that for every layer.
You just repeat that process.
Okay,

74
00:04:20.080 --> 00:04:24.250
so feedforward networks,
they're great for learning an input output pattern.

75
00:04:24.430 --> 00:04:25.030
What is there?

76
00:04:25.030 --> 00:04:29.710
What is the rule here between the set of inputs and the set of outputs,
right?

77
00:04:29.711 --> 00:04:32.530
And in the end of feed forward network,

78
00:04:32.950 --> 00:04:35.770
and in fact all neural networks,
it's just a one.

79
00:04:35.800 --> 00:04:39.640
It's just one big composite function.
What do I mean by that?

80
00:04:39.670 --> 00:04:43.720
I mean that you can think of a neural network as a giant function,

81
00:04:43.721 --> 00:04:48.400
and inside of that network are smaller functions,
nested functions.

82
00:04:48.401 --> 00:04:51.970
A composite function is a function that consists of other functions.

83
00:04:52.180 --> 00:04:53.890
What do I mean by nested functions?

84
00:04:54.010 --> 00:04:56.740
Remember this computation graph that we just looked at right here.

85
00:04:57.010 --> 00:05:00.580
These are all functions.
Each layer is a function,
right?

86
00:05:00.660 --> 00:05:03.310
And put times weights at a bias activate.

87
00:05:03.311 --> 00:05:07.660
That is a function that you feed the output of two as the input to the next

88
00:05:07.661 --> 00:05:12.400
function.
So what a neural network is.
So the,
the,

89
00:05:12.410 --> 00:05:16.990
the most nested function right in the middle is this most nested function is the

90
00:05:16.991 --> 00:05:21.040
first layer value whose output we then feed to the next layer,

91
00:05:21.041 --> 00:05:24.340
which would be the next function,
whose output we feed to the next layer.

92
00:05:24.400 --> 00:05:28.120
So the largest function,
the,
the most,
the,
the,
the function on the,

93
00:05:28.121 --> 00:05:31.480
on the outside here is then the output layer,
because we're feeding it,

94
00:05:31.481 --> 00:05:35.690
the inputs,
outputs the,
the outputs of what?
Of all that chain,

95
00:05:35.710 --> 00:05:40.660
that chain of computation that already occurred,
right?
So that's what that is.
Um,

96
00:05:40.720 --> 00:05:44.400
so it's a composite function and we would use feed forward nuts anytime.

97
00:05:44.410 --> 00:05:47.980
We have two variables that are related.
Temperature,
location,

98
00:05:47.981 --> 00:05:51.100
hide in way car speed and brand.
These are all mappings.

99
00:05:51.220 --> 00:05:54.790
But what if don't have done,
I'm just adding my own sound effects.

100
00:05:54.940 --> 00:05:59.480
Don't dunt dumb.
What if the ordering of the data mattered?

101
00:05:59.660 --> 00:06:03.650
Right?
What have you had stock prices,
right?
It's a very controversial topic,

102
00:06:03.651 --> 00:06:07.820
but I just,
you know,
the stock price thing gets a lot of views even though I,

103
00:06:07.821 --> 00:06:11.030
you know,
I don't wanna I don't wanna I don't know.

104
00:06:11.031 --> 00:06:13.250
I don't really personally care about finance data,

105
00:06:13.280 --> 00:06:17.090
but I know some of you guys do and you know how I'll probably talk about it more

106
00:06:17.091 --> 00:06:20.780
in the future.
Anyway,
tangent back to this.

107
00:06:20.810 --> 00:06:22.310
What if the time matters,
right?

108
00:06:22.311 --> 00:06:25.970
So stock prices are a great example of when time matters.

109
00:06:27.800 --> 00:06:31.640
You can't just predict a mapping between time and the price,

110
00:06:31.700 --> 00:06:33.260
what happened before,

111
00:06:34.400 --> 00:06:38.780
what the stock prices before are,
what matter to the current stock price.

112
00:06:38.810 --> 00:06:42.380
That's actually debatable in the,
in the context of stock prices,

113
00:06:42.381 --> 00:06:46.070
but it applies to all time series data.
So video,
right?

114
00:06:46.071 --> 00:06:48.680
If you want to generate the next frame in a video,

115
00:06:48.830 --> 00:06:50.880
it matters what frames came before.

116
00:06:51.050 --> 00:06:55.610
You can't just learn a mapping between a frame and the time that that frame

117
00:06:55.611 --> 00:07:00.380
shows up because then what happens is given some new time,

118
00:07:00.530 --> 00:07:03.200
you can't just generate a frame based on nothing else,
right?

119
00:07:03.410 --> 00:07:06.380
It depends on the frames that came before it.
You see what I'm saying?

120
00:07:06.470 --> 00:07:09.140
The sequence matters.
The sequence matters here.

121
00:07:09.260 --> 00:07:11.870
The alphabet or lyrics of a song.

122
00:07:12.050 --> 00:07:17.050
You can't just generate a lyric or an alphabet depending on the index that it's

123
00:07:17.511 --> 00:07:19.640
at.
You've got to know what came before it.

124
00:07:19.910 --> 00:07:24.910
Try to recite the alphabet backwards and so to get into neuroscience for a

125
00:07:25.221 --> 00:07:29.480
second,
try to recite the alphabet backwards.
It's hard,
right?
Z,
y,

126
00:07:29.510 --> 00:07:33.440
x w key cue you.
Okay?
See,
I can't even do it right now.

127
00:07:33.980 --> 00:07:37.970
I'm not going to edit that out.
So or any song,
try to recite a song backwards.

128
00:07:37.971 --> 00:07:41.900
You can because you learned it in a sequence.
It's a kind of conditional memory.

129
00:07:42.080 --> 00:07:45.890
What you remembered depends on what you've,
what you've stored previously,
right?

130
00:07:45.891 --> 00:07:50.840
It's conditional memory in that way and that is what recurrent networks help us

131
00:07:50.841 --> 00:07:54.530
do.
They help us compute conditional memory.
They help us compute.

132
00:07:54.650 --> 00:07:57.500
The next value in a sequence of values.

133
00:07:58.010 --> 00:07:59.930
So that's what we're current networks or are good at it.

134
00:07:59.931 --> 00:08:03.560
That's what they're made for.
And it's not like this is some new technology,

135
00:08:03.590 --> 00:08:05.000
recurrent networks were invented.

136
00:08:05.090 --> 00:08:08.810
They were invented in the eighties neural networks were invented in the 50s but

137
00:08:08.811 --> 00:08:11.870
why is this super hot right now?
Why are you watching this video?

138
00:08:11.960 --> 00:08:16.130
Because with the invention of bigger data and bigger computing power,

139
00:08:16.250 --> 00:08:19.580
when you take these recurrent networks and give them those two things,

140
00:08:19.730 --> 00:08:23.300
they blow almost every other machine learning model out of the water in terms of

141
00:08:23.301 --> 00:08:26.030
accuracy.
So it's just incredible.
But anyway,

142
00:08:26.480 --> 00:08:28.880
this is a picture of a three layer and make it,

143
00:08:30.470 --> 00:08:33.440
this is a picture of a three layer recurrent network,
right?

144
00:08:33.441 --> 00:08:36.590
So you've got your first layer,
which is your input,
your hidden state,

145
00:08:36.770 --> 00:08:39.710
your output layer.
And so that would just be a feed forward network.

146
00:08:39.711 --> 00:08:44.600
But the difference here is that we've got this other layer right here and that

147
00:08:44.601 --> 00:08:48.620
is so what that what that other layer is,
it's not actually another layer.

148
00:08:48.710 --> 00:08:52.220
The difference is that we've added a third weight matrix.

149
00:08:52.370 --> 00:08:54.830
So we've got our first way matrix our second way,

150
00:08:55.550 --> 00:08:57.480
but we've got a third weight matrix.

151
00:08:57.481 --> 00:09:01.170
And that's really what makes it different than a feed forward network is that

152
00:09:01.171 --> 00:09:03.450
we're adding a a third weight matrix.

153
00:09:03.540 --> 00:09:07.080
And what the third weight matrix is doing is it's connecting the current hidden

154
00:09:07.081 --> 00:09:11.010
states.
So the hidden stay at the current time,
step two,

155
00:09:11.011 --> 00:09:13.050
the hidden state and the previous time step.

156
00:09:13.140 --> 00:09:16.560
So it's a recurrent weight matrix and you'll see programmatically and

157
00:09:16.561 --> 00:09:17.850
mathematically what I'm talking about.

158
00:09:17.940 --> 00:09:20.220
But that's really the key bit here for recurrent networks.

159
00:09:20.221 --> 00:09:23.940
That's what makes it unique from a feedforward networks.

160
00:09:24.120 --> 00:09:28.920
And so what this does is whenever we feed in some value,
you know,

161
00:09:28.921 --> 00:09:30.660
cause we are training this network,
right?

162
00:09:30.810 --> 00:09:33.060
We continuously feed it new data points,

163
00:09:33.210 --> 00:09:37.590
data point after data point from our training set.
But for feedforward networks,

164
00:09:37.650 --> 00:09:39.330
we're only feeding in the input.

165
00:09:39.600 --> 00:09:41.730
We're not feeding in the the previous hidden states.

166
00:09:41.910 --> 00:09:44.700
We're only feeding an input after input after input.

167
00:09:44.880 --> 00:09:47.400
And the hidden state is being updated at every time step.

168
00:09:47.670 --> 00:09:50.580
But because we want to remember a sequence of data,

169
00:09:51.180 --> 00:09:55.170
we're going to not just feed in the,
the,
the current data point,
wherever we are.

170
00:09:55.470 --> 00:09:59.400
We're also going to feed it in the previous hidden state.
And that is,

171
00:09:59.650 --> 00:10:02.970
and by that I mean the values that are computed from the previous time.

172
00:10:02.970 --> 00:10:06.960
Step four,
that hidden states,
right?
That set of numbers,
that Matrix.

173
00:10:07.800 --> 00:10:09.960
And so you might be thinking,
wait a second,

174
00:10:10.110 --> 00:10:13.740
why don't we just feed in the input and in the previous input as well from the

175
00:10:13.741 --> 00:10:14.574
previous time step,

176
00:10:14.700 --> 00:10:17.460
why are we feeding in the input and the previous hidden state?

177
00:10:17.670 --> 00:10:22.670
Because input recurrence only remembers what just happened that previous input.

178
00:10:24.210 --> 00:10:26.640
But if you feed it in the previous hidden state,

179
00:10:26.820 --> 00:10:29.730
then what's happening is it can remember that sequence.

180
00:10:29.800 --> 00:10:32.010
It's not just about what came right before.

181
00:10:32.160 --> 00:10:34.470
It can remember everything that came before it.

182
00:10:34.650 --> 00:10:38.070
Because you can think of that hidden state as a kind of like,
uh,

183
00:10:38.280 --> 00:10:41.820
like thinking about like clay that's being molded by every new input.

184
00:10:41.940 --> 00:10:43.650
It's been molded by every new input.

185
00:10:43.800 --> 00:10:48.540
And by feeding that clay that's being molded back into the network,
it's,

186
00:10:48.840 --> 00:10:52.550
it's being,
uh,
it,
it's,
it's learning neural memory.
So it's,

187
00:10:52.551 --> 00:10:55.470
it's a form of neural memory,
conditional memory,

188
00:10:55.740 --> 00:11:00.240
and it can remember sequential data.
So,
right.

189
00:11:00.241 --> 00:11:03.990
So here's another example just to give a few more example before I go into the

190
00:11:03.991 --> 00:11:06.300
code here.
But we have,
so this,

191
00:11:06.301 --> 00:11:09.150
this is a very popular type of a image for recurrent networks.

192
00:11:09.210 --> 00:11:12.810
So what's happening is it's we're feeding in the current input,

193
00:11:13.380 --> 00:11:15.990
calculating a hidden state and in computing an output.

194
00:11:16.170 --> 00:11:20.520
And then for the next time step we're giving it that new data points as well.

195
00:11:20.521 --> 00:11:23.520
And so the Blue Arrow is what's different here compared to a feed forward

196
00:11:23.521 --> 00:11:28.170
network we're feeding in the previous hidden state as was the input to compute

197
00:11:28.171 --> 00:11:33.030
the current hidden states to compute our output,
our y value.

198
00:11:33.270 --> 00:11:36.300
And we're using a loss function to improve our network every time.

199
00:11:36.750 --> 00:11:40.710
And so if you think of what that recurrence looks like,
it looks like this.

200
00:11:40.740 --> 00:11:43.020
So do you remember that feed forward network we just looked at?

201
00:11:43.021 --> 00:11:46.740
The difference here is that we are feeding in the output of the hidden state

202
00:11:46.860 --> 00:11:51.360
back into the input.
The output of this wait times bias,

203
00:11:51.570 --> 00:11:54.940
uh,
activate operation in a layer.
Okay.

204
00:11:55.090 --> 00:11:58.000
So of the formula for recurrent network looks like this,

205
00:11:58.001 --> 00:12:00.910
which basically says at the current hidden state,

206
00:12:00.911 --> 00:12:05.911
h t is a function of the previous hidden state and the current input.

207
00:12:07.000 --> 00:12:10.690
Okay.
And the Feta value right here are the parameters of the function.

208
00:12:10.960 --> 00:12:15.010
So the network learns to use hot as a lossy summary of the task.

209
00:12:15.011 --> 00:12:18.720
Relevant aspects of the past sequence of inputs up to t,

210
00:12:19.240 --> 00:12:24.160
the loss function that we're going to use here is going to be the negative log

211
00:12:24.190 --> 00:12:25.420
likelihood.
Okay.

212
00:12:25.421 --> 00:12:30.280
This is a very popular loss function for recurrent networks like plain old

213
00:12:30.281 --> 00:12:31.150
recurrent networks.

214
00:12:31.300 --> 00:12:35.890
Not using anything fancy like long short term memory cells or by directional,

215
00:12:36.260 --> 00:12:37.780
um,
capabilities.

216
00:12:38.080 --> 00:12:41.830
But the negative log likelihood usually gives us the best output or the best

217
00:12:41.831 --> 00:12:45.940
accuracy for plain old or current networks,
which is why we're going to use it.

218
00:12:46.120 --> 00:12:48.520
And we'll talk about what that consists of in a second.

219
00:12:48.700 --> 00:12:51.820
But our steps for this are going to be the first initialized our weights

220
00:12:51.821 --> 00:12:54.370
randomly like we always do and then give it.

221
00:12:54.490 --> 00:12:58.420
Then we're going to give the model a char pair.
So what is a char pair?

222
00:12:58.421 --> 00:13:01.960
The char pair is going to be the input chars so that's some seeds,
some,

223
00:13:02.110 --> 00:13:06.010
some letter from the train techs that we wanted to give us input as well as a,

224
00:13:06.030 --> 00:13:09.430
as the target char.
And the target chart is going to be our label.

225
00:13:09.670 --> 00:13:11.860
So our label is actually the next char.

226
00:13:11.861 --> 00:13:16.861
So if we take the first two chars from some input texts from some input Corpus,

227
00:13:17.050 --> 00:13:20.710
let's say the word is the,
the input chart would be tea,

228
00:13:20.980 --> 00:13:25.290
then the target charter would be h.
So given t we want to predict h.

229
00:13:25.330 --> 00:13:28.240
So you see how that that target char acts as our,

230
00:13:28.390 --> 00:13:31.210
as our label that we're trying to predict.
And so once we,

231
00:13:31.450 --> 00:13:33.400
once we have that age,

232
00:13:33.670 --> 00:13:36.670
we can compute the most likely next character.

233
00:13:36.850 --> 00:13:40.240
And then compare from our forward pass,

234
00:13:40.450 --> 00:13:44.050
we're going to calculate the probability for every possible next char given that

235
00:13:44.051 --> 00:13:47.080
t according to the state of the model using the parameters.

236
00:13:47.410 --> 00:13:50.950
And then we're going to measure our error as a distance between the previous

237
00:13:50.951 --> 00:13:55.600
probability value and the target char.
So that that's,
that's what axes are,
are,

238
00:13:55.720 --> 00:13:59.410
are,
are labeled the next chart in the sequence.
And we just keep doing that.

239
00:13:59.800 --> 00:14:03.910
So it's a dynamic error,
right?
And so we,
once we have that air value,

240
00:14:04.060 --> 00:14:05.560
we'll use it to help us calculate,

241
00:14:05.830 --> 00:14:10.480
to help us calculate our gradients for each of our parameters to see the impact

242
00:14:10.481 --> 00:14:14.230
they have on the loss.
And that is back propagation through time.

243
00:14:14.231 --> 00:14:17.830
And we call it through through time because we are using that,

244
00:14:17.890 --> 00:14:21.940
that hidden state to hidden state matrix,
that recurrent matrix value.

245
00:14:22.150 --> 00:14:24.100
But otherwise it's just the same.

246
00:14:24.130 --> 00:14:29.030
It's just backpropagation it's called through time because we are applying that

247
00:14:29.031 --> 00:14:33.070
to um,
hidden say to hidden state matrix to it.
Okay.

248
00:14:33.071 --> 00:14:35.650
So then once we have our gradient values,

249
00:14:35.800 --> 00:14:39.610
we're going to update all the perimeters in the direction via the grit in the

250
00:14:39.611 --> 00:14:43.960
right direction to minimize the loss.
That's grading dissent,
br gradients.

251
00:14:43.990 --> 00:14:47.170
And we just keep repeating that process.
So everything is the same here.

252
00:14:47.171 --> 00:14:51.390
Grading dissent as a feed forward network,
Caribbean descent,
um,

253
00:14:51.690 --> 00:14:53.960
calculating an error value,
a Ford pass.

254
00:14:53.961 --> 00:14:57.710
But the difference is that we are connecting the current hidden state to the

255
00:14:57.711 --> 00:15:02.450
previous hidden state and that changes how,
um,
the network learns.

256
00:15:03.260 --> 00:15:05.720
So what are some use cases I talked about?
Time series,

257
00:15:05.721 --> 00:15:10.280
prediction specifically weather forecasting.
Yes.
Stock prices,
traffic volume,

258
00:15:10.430 --> 00:15:14.630
sequential data generation as well.
Music,
video,
audio,

259
00:15:14.631 --> 00:15:18.560
any kind of sequential data.
What is the next,
no,
the next audio wave form.

260
00:15:18.561 --> 00:15:22.310
The next frame in the video.
Okay.
And then for other examples,

261
00:15:22.650 --> 00:15:23.570
I've got great one,

262
00:15:23.630 --> 00:15:28.630
a great one here for binary audition that was originally invented by Trask,

263
00:15:29.240 --> 00:15:32.990
who's a great technical writer.
Definitely check that out.

264
00:15:33.140 --> 00:15:36.350
And so once we understand the intuition behind recurrent networks,

265
00:15:36.500 --> 00:15:41.180
then we can move onto LSTM networks and by directional networks and recursive

266
00:15:41.181 --> 00:15:44.810
networks,
those are more advanced.
Uh,
networks.
And they do,

267
00:15:44.870 --> 00:15:48.650
they solve some problems with recurrent networks.
Before you get there,

268
00:15:48.800 --> 00:15:51.230
you've got to understand recurrent networks.
Okay?

269
00:15:51.260 --> 00:15:52.940
So this code contains four parts.

270
00:15:52.941 --> 00:15:55.310
The first part is for us to load the training data,

271
00:15:55.580 --> 00:15:58.970
then we'll define our network,
then we'll define our loss function.

272
00:15:58.970 --> 00:16:03.200
And the loss function is going to contain both the forward pass and the backward

273
00:16:03.410 --> 00:16:06.980
pass.
So the real meat of the code is going to happen in the loss function.

274
00:16:07.220 --> 00:16:10.970
And what it's gonna do is it's going to return the gradient values that we can

275
00:16:10.971 --> 00:16:15.410
then use to update our weights,
uh,
later on during training.
But the Ma,

276
00:16:15.440 --> 00:16:18.890
the meat of the code is going to happen in the loss function.
And once we've,

277
00:16:19.040 --> 00:16:22.850
we've come,
we've defined that will write a function to then make predictions,

278
00:16:22.851 --> 00:16:26.210
which in this case would be to generate words and we'll train the network as

279
00:16:26.211 --> 00:16:30.440
well.
Okay.
So our first step is going to be a load up our training data.

280
00:16:30.470 --> 00:16:34.130
So to load up our training data,
um,

281
00:16:34.340 --> 00:16:37.880
the F to load up our training data,
I'm going to say,
okay,

282
00:16:37.881 --> 00:16:41.300
so let's define what that,
what that data is by the way.

283
00:16:41.300 --> 00:16:45.380
So if we open this file,
we'll look at it.
Kafka,
Kafka,
Tst,

284
00:16:45.710 --> 00:16:49.400
it's one morning when Gregor Samsa woke up from trouble dreams.
The right.

285
00:16:49.401 --> 00:16:52.160
So this is just a book.
It's a big book,
a big txt file.

286
00:16:52.310 --> 00:16:56.390
That's what the input is going to be.
Okay.
So we'll,

287
00:16:56.391 --> 00:16:58.350
we'll open it up using uh,

288
00:16:58.940 --> 00:17:03.940
the native functions here of Python and it's going to be recursive cause we want

289
00:17:04.421 --> 00:17:05.720
to,
we want all of it.

290
00:17:05.721 --> 00:17:10.370
We'll just read that simple plain txt file and then we're going to say,
okay,

291
00:17:10.371 --> 00:17:15.371
let's get that a list of data points and or chars.

292
00:17:15.611 --> 00:17:20.611
In this case we'll store it in chars and we'll define how big our data is as

293
00:17:20.961 --> 00:17:24.110
well as our vocab vocab size.

294
00:17:24.560 --> 00:17:26.060
And we can say that the,

295
00:17:26.600 --> 00:17:29.840
it's going to be the length of the data that that big text file as well as the

296
00:17:29.841 --> 00:17:32.300
length of the charges.
How many charges do we have?

297
00:17:32.600 --> 00:17:37.220
And we'll print it out for ourselves just so we know how many chairs there are.

298
00:17:37.610 --> 00:17:39.560
And once we've,
we've done that,

299
00:17:39.620 --> 00:17:44.620
we can go ahead and print it out and it's going to tell us how many unique chars

300
00:17:44.681 --> 00:17:45.380
there are,

301
00:17:45.380 --> 00:17:50.380
which matters to us because we want to make a vector of the size of the number

302
00:17:52.111 --> 00:17:53.460
of charges that there are.

303
00:17:53.730 --> 00:17:58.730
So let me go ahead and print that out and it's going to tell us exactly what the

304
00:17:59.101 --> 00:17:59.934
deal is.

305
00:18:00.300 --> 00:18:05.100
And so we've got a character that's how many characters it's got.
It has.

306
00:18:05.280 --> 00:18:09.510
Okay.
So the data has 137 k characters in 80.

307
00:18:09.511 --> 00:18:12.360
One of them are unique.
Okay,
good.
Good to know.
Good to know.

308
00:18:12.630 --> 00:18:16.560
Our next step is to calculate the vocab size.
Okay.

309
00:18:16.561 --> 00:18:20.310
So we're going to calculate the vocab size because we want to be able to feed

310
00:18:20.311 --> 00:18:24.980
back jurors into our network.
We can't just feed in Ra,
uh,
string.
You know,

311
00:18:25.020 --> 00:18:29.730
chars we've got to convert the chars two vectors because a vector is an array

312
00:18:29.970 --> 00:18:33.060
of,
of float values in this case,
or a vector is an array,

313
00:18:33.760 --> 00:18:38.280
a at least of numbers in the context of machine learning.
And so,

314
00:18:38.790 --> 00:18:41.550
uh,
so we'll calculate the vocab size to help us do this.

315
00:18:41.760 --> 00:18:44.160
So we're going to create two dictionaries,

316
00:18:44.460 --> 00:18:48.280
and both of these dictionaries are going to convert the,
um,

317
00:18:49.830 --> 00:18:53.850
both of these dictionaries are going to convert the characters,
two integers,

318
00:18:53.880 --> 00:18:56.550
and then the integers to characters while respectively,

319
00:18:56.790 --> 00:18:59.140
one will convert from character to Integer,
which,

320
00:18:59.160 --> 00:19:03.360
which is the one that I've just written.
And then the next one is going to say,

321
00:19:03.660 --> 00:19:07.530
let's convert the integers to characters.

322
00:19:08.040 --> 00:19:11.610
And once we've done that,
we can go ahead and say,
well,

323
00:19:11.611 --> 00:19:14.070
let's print all of the values that it's,

324
00:19:14.310 --> 00:19:17.790
it's storing because these are our dictionaries that we're gonna use in a second

325
00:19:17.791 --> 00:19:22.260
to convert our values into vectors.
So let's go ahead and print that.

326
00:19:22.620 --> 00:19:27.580
And
what's the deal here?
Oh,

327
00:19:27.581 --> 00:19:31.880
and numerate,
right?
And knew my great,
great,

328
00:19:32.300 --> 00:19:36.680
great.
Right?
So here are our vectors right there.

329
00:19:36.740 --> 00:19:39.320
It's a dictionary or here,
or here are our dictionaries,

330
00:19:39.470 --> 00:19:43.130
one for characters to integers and one for integers to characters.
Okay.

331
00:19:43.131 --> 00:19:45.260
So once we have that now,

332
00:19:48.130 --> 00:19:53.070
so we'd done that already.
And so then we're going to say,

333
00:19:53.520 --> 00:19:56.940
let's create a vector for character.
Hey,

334
00:19:56.941 --> 00:19:59.580
so this is what vectorization looks like for,
for us.

335
00:19:59.790 --> 00:20:02.190
So let's say we want to create a vector for the character a.

336
00:20:02.430 --> 00:20:04.500
So we'll say we'll initialize the vectors empty.

337
00:20:04.501 --> 00:20:08.970
So it's just a vector of zeroes of the size of the vocab.
Okay.

338
00:20:08.971 --> 00:20:13.050
And so of the size of our vocab,
and then we'll say,
okay,
so,

339
00:20:13.200 --> 00:20:15.210
so convert the

340
00:20:17.700 --> 00:20:22.080
not now we're going to do the conversion.
We'll say a char to manager,

341
00:20:22.590 --> 00:20:25.760
so a to the integer.
Uh,
so that's,

342
00:20:25.761 --> 00:20:29.040
so that's gonna be our input is going to give us an integer value and we're

343
00:20:29.041 --> 00:20:33.180
gonna set it to one.
And so what happens is when we print out this vector,

344
00:20:33.390 --> 00:20:37.620
it's going to be a vector of size.
Let's see if I got that right.

345
00:20:37.621 --> 00:20:39.420
So there's gonna be a vector of size.

346
00:20:43.510 --> 00:20:48.250
Hold on.
Oh,
right.
Important on Pie.
Duh.
I forgot.
Importing num Pi.
Yeah.

347
00:20:48.580 --> 00:20:53.410
Okay.
Right.
So it's a vector of size.

348
00:20:53.411 --> 00:20:55.390
How many a unique characters were there?

349
00:20:55.540 --> 00:20:58.660
There were 81 unique characters,

350
00:20:58.780 --> 00:21:02.050
so it's in vector of size 81 and all of those values,

351
00:21:02.051 --> 00:21:07.051
all this elements in the vector are going to be zero except for the one that is

352
00:21:07.300 --> 00:21:10.900
the mapping between a and its respective integer in that dictionary.

353
00:21:11.020 --> 00:21:13.660
So that's how we mapped it.
That's why we created this two dictionaries.

354
00:21:13.750 --> 00:21:16.960
So this is what we would feed in as a,
so we will feed it into of these,

355
00:21:17.110 --> 00:21:18.820
cause remember I said that we have a char pair,

356
00:21:18.970 --> 00:21:22.870
so we'll feed an ae and whatever the next character is as our input,

357
00:21:23.050 --> 00:21:26.110
which would be our input and our label value and the label is our other

358
00:21:26.111 --> 00:21:28.450
character,
our next character.
Okay.

359
00:21:28.451 --> 00:21:31.330
So then for our model parameters we're going to define our network.

360
00:21:31.330 --> 00:21:33.130
Remember it's a three layer network.

361
00:21:33.220 --> 00:21:36.700
We have our input layer are hidden layer and our output layer.

362
00:21:37.120 --> 00:21:39.220
And so all these layers are fully connected.

363
00:21:39.221 --> 00:21:43.870
So that means every value is going to be connected to every other value between

364
00:21:43.871 --> 00:21:48.370
layers.
Okay.
So the way we'll define that is to,
um,

365
00:21:49.270 --> 00:21:52.390
well first let's define our hyper parameters.
I've,
I got to,

366
00:21:52.450 --> 00:21:54.550
we got to define our tuning knobs for the network.

367
00:21:54.700 --> 00:21:59.140
So we want to say that our numbers and have a hundred a hidden neurons for it's

368
00:21:59.200 --> 00:21:59.501
for,

369
00:21:59.501 --> 00:22:03.640
it's a hundred a hundred neurons for its hidden layer and then we're going to

370
00:22:03.641 --> 00:22:05.920
say that we want,
um,

371
00:22:08.590 --> 00:22:12.580
they're,
they're going to be 25 characters that are generated at every time step.

372
00:22:12.581 --> 00:22:13.540
That's our sequence length.

373
00:22:13.810 --> 00:22:17.760
And then our learning rates is going to be this very small number,
uh,

374
00:22:17.920 --> 00:22:21.400
because if it's too slow,
then it's never going to converge.
But if it's too high,

375
00:22:21.550 --> 00:22:25.810
then it will overshoot and it's just never going to converge.
The learning rates,

376
00:22:25.811 --> 00:22:27.730
by the way,
is a,

377
00:22:27.731 --> 00:22:31.480
is how quickly I network abandoned old beliefs for new ones.

378
00:22:31.600 --> 00:22:34.210
So if you're training your neural network on cat dog images,

379
00:22:34.480 --> 00:22:35.710
the lower the learning rate,

380
00:22:35.860 --> 00:22:38.770
the less likely it will be too when given a new dog picture.

381
00:22:38.771 --> 00:22:40.570
If you're just been trying to get on cat pictures,

382
00:22:40.780 --> 00:22:41.830
if you give it a new dog picture,

383
00:22:41.850 --> 00:22:46.150
the less likely it will be to consider that as a part of the training data.

384
00:22:46.300 --> 00:22:49.630
If you're trying to be able to recognize both it,
the lower the learning rate,

385
00:22:49.810 --> 00:22:52.990
the more likely it will consider that dog picture just an anomaly and just kind

386
00:22:52.991 --> 00:22:57.050
of discard that internally.
So it's a kind of way of to tune how,
um,

387
00:22:57.310 --> 00:23:00.370
quickly a network gow abandoned old beliefs for new ones.

388
00:23:00.490 --> 00:23:04.840
That's another way of putting it.
Um,
anyway,
so that's for our hyper parameters.

389
00:23:05.830 --> 00:23:07.480
Now we'll define our model parameters,
right?

390
00:23:07.481 --> 00:23:12.100
We've defined our model parameters and now we can define our uh,
networks,

391
00:23:12.101 --> 00:23:15.370
weight value.
So the first set of weights are going to be from our inputs.

392
00:23:15.371 --> 00:23:19.290
So xow x,
h,
so x is our input.
These,
this is what the,

393
00:23:19.460 --> 00:23:24.130
the terminology is,
right?
So the weights from our input tor hidden states,
right?

394
00:23:24.131 --> 00:23:28.420
So that's going to be initialized randomly using the non Pi's random random

395
00:23:28.450 --> 00:23:32.050
brand and the function.
And it will be a value between the,

396
00:23:32.140 --> 00:23:35.170
the hidden size that we've defined.

397
00:23:35.500 --> 00:23:38.970
And the vocab size because those are the two values,
uh,

398
00:23:39.010 --> 00:23:43.660
that we're dealing with here.
And we'll multiply it by 0.01,

399
00:23:43.661 --> 00:23:47.130
because we want,
we just want to scale it,
uh,

400
00:23:47.150 --> 00:23:50.810
for a character level recurrent network because it's a character level current

401
00:23:50.811 --> 00:23:53.000
network.
So input to hidden states.

402
00:23:54.860 --> 00:23:58.760
And so then we will repeat that process.

403
00:23:59.210 --> 00:24:04.210
But this time for are not from our input to hidden buck for our hidden state to

404
00:24:04.431 --> 00:24:08.990
our next hidden state.
And so that's our recurrent weight matrix right there.

405
00:24:08.991 --> 00:24:13.910
That's a recurrent way matrix.
And so lastly,
we'll have our third weight matrix,

406
00:24:13.911 --> 00:24:16.730
which is our,
um,
what's,
what's our third way matrix.

407
00:24:16.731 --> 00:24:20.270
Our third weight matrix is our,
uh,

408
00:24:20.300 --> 00:24:24.440
hidden states to our outputs value or how put,

409
00:24:24.740 --> 00:24:27.950
and so that's going to be vocab size too,
between,

410
00:24:27.951 --> 00:24:32.330
between the vocab size and the hidden size.
And then we will also,

411
00:24:32.331 --> 00:24:35.510
since we have to bias biases will say

412
00:24:37.040 --> 00:24:40.680
the bias for a hidden state will be initialized as a set of zeroes

413
00:24:43.060 --> 00:24:46.130
of size,
of the hidden size.
Cause it's for our hidden state.

414
00:24:46.550 --> 00:24:50.720
And then we will,
uh,
so that's our hidden bias.

415
00:24:50.960 --> 00:24:55.340
And one more bias.
And that is for our,
our output by it.
That is our output bias.

416
00:24:55.341 --> 00:24:57.590
Also a collection of zeroes.
Uh,

417
00:24:57.620 --> 00:25:02.300
the difference here is that is of the vocab size.
Okay.

418
00:25:02.330 --> 00:25:04.100
And so,
yeah.
Great.

419
00:25:05.950 --> 00:25:06.520
<v 1>Okay.</v>

420
00:25:06.520 --> 00:25:10.390
<v 0>Oh,
let's see what we got here.
Hidden size is not defined</v>

421
00:25:12.230 --> 00:25:16.640
in size is right here.
Compile come about.

422
00:25:17.720 --> 00:25:22.370
What's the deal?
Hidden size is not defined.
Yes it is.

423
00:25:23.090 --> 00:25:23.923
Yes it is

424
00:25:30.760 --> 00:25:33.880
invalid syntax.
Great.

425
00:25:36.080 --> 00:25:38.440
So the function is going to take this as its input,

426
00:25:38.450 --> 00:25:42.710
a list of input chars a list of target chars and the previous hidden state.

427
00:25:43.190 --> 00:25:45.290
And then this function is going to output a loss,

428
00:25:45.320 --> 00:25:49.430
a gradient for each parameter between layers.
And then the last Hayden state.

429
00:25:49.760 --> 00:25:51.020
So what does a forward pass,

430
00:25:51.050 --> 00:25:54.050
so the forward pass and a recurrent network looks like this.

431
00:25:54.170 --> 00:25:59.170
This function describes the forward pass or this function describes how the

432
00:25:59.991 --> 00:26:04.970
hidden state is calculated.
Right?
So so,

433
00:26:05.540 --> 00:26:09.170
so how is the forward pass calculated?
So the forward pass is,

434
00:26:09.171 --> 00:26:12.470
remember it's just a series of matrix operations.
So this is,

435
00:26:12.500 --> 00:26:15.830
this is basically our forward pass right here.
What years looking at right here.

436
00:26:16.370 --> 00:26:21.350
So this first equation right here is,
uh,
what is,

437
00:26:21.410 --> 00:26:23.030
let me make this smaller.
So you can see.

438
00:26:23.300 --> 00:26:27.050
So the way we compute this math operation right here,
this is the forward pass,

439
00:26:27.320 --> 00:26:32.320
is the dot product between the input to hidden state weight matrix and the input

440
00:26:32.631 --> 00:26:37.100
data.
That's this term right here,
plus the dot product between the hidden state.

441
00:26:37.430 --> 00:26:38.263
Um,

442
00:26:38.990 --> 00:26:39.630
<v 1>okay,</v>

443
00:26:39.630 --> 00:26:43.770
<v 0>the hidden state,
the hidden state Matrix and uh,
the state.</v>

444
00:26:43.920 --> 00:26:48.030
And then we add the hidden bias and that's going to give us the hidden state

445
00:26:48.031 --> 00:26:51.570
value at the current time step,
right?
So that's what that represents.

446
00:26:51.840 --> 00:26:54.510
And then we take that value and we feed it,

447
00:26:54.570 --> 00:26:57.420
we compute a dot product with the next way matrix.

448
00:26:57.510 --> 00:27:01.520
And that is a hidden state to the output.
And then we add that out,

449
00:27:01.560 --> 00:27:03.870
that output by US value.

450
00:27:04.080 --> 00:27:08.070
And that's going to give us the unnormal and normalize log probabilities for the

451
00:27:08.071 --> 00:27:12.420
next chars,
which we then squash and to probability values using the,
this,

452
00:27:12.570 --> 00:27:16.680
this function p,
which is actually right here,
p right here.

453
00:27:17.310 --> 00:27:20.160
But I'll talk about that in a second.
Okay,
so that's our forward pass.

454
00:27:20.340 --> 00:27:23.820
And then for our backward pass,
the backward pass is going to be,

455
00:27:25.590 --> 00:27:28.410
before we talked about the backward pass,
let's talk about the loss for a second.

456
00:27:28.410 --> 00:27:30.780
So the loss is a negative log likelihood.

457
00:27:30.900 --> 00:27:35.430
So it's the negative log value of p and P is this function here.

458
00:27:35.610 --> 00:27:40.550
So,
uh,
which is represented programmatically by this,
uh,

459
00:27:40.710 --> 00:27:42.400
right here,
right?
So it's the,
uh,

460
00:27:42.450 --> 00:27:46.740
it's e to the x where x is the output value from the,

461
00:27:47.090 --> 00:27:50.980
that it received divided by the sum of all of the,
uh,

462
00:27:51.150 --> 00:27:54.630
e two the probability values,
okay?

463
00:27:54.720 --> 00:27:56.970
And that's going to give us p a p value,
okay.

464
00:27:57.270 --> 00:28:01.470
And so we take that p value and then we take the negative log of that p value

465
00:28:01.620 --> 00:28:05.370
and that is our loss scaler,
that loss scalar value.

466
00:28:05.940 --> 00:28:08.220
And so once we have that loss,
we're going to,

467
00:28:08.460 --> 00:28:11.220
we're going to perform backpropagation using that loss.

468
00:28:12.420 --> 00:28:17.420
And so the way we compute backpropagation to go over this is by using the chain

469
00:28:17.431 --> 00:28:19.710
rule.
So the chain rule is from calculus.

470
00:28:19.860 --> 00:28:23.670
What we want to do is compute gradients for each of the layers.
Okay?

471
00:28:23.671 --> 00:28:27.150
So for each of the weight major sees,
okay,
given an error value,

472
00:28:27.151 --> 00:28:30.870
we're going to compute the partial derivative of the error with respect to each

473
00:28:30.871 --> 00:28:35.610
way recursively.
So the reason we're using the chain rule is that so,

474
00:28:35.640 --> 00:28:40.050
so because we have three weight matrices,
we have the inputs to hidden,

475
00:28:40.230 --> 00:28:42.450
hidden to outputs and hidden to hidden.

476
00:28:42.630 --> 00:28:45.840
We want to compute a grading values for all three of those.

477
00:28:45.841 --> 00:28:47.130
So that's what this looks like.

478
00:28:47.430 --> 00:28:50.760
We want to compute great and values for all three of those weight matrices.

479
00:28:51.030 --> 00:28:54.360
And the way we're going to do that is to computer loss using the negative log

480
00:28:54.361 --> 00:28:58.710
likelihood and use that loss to compute the partial derivative with respect to

481
00:28:58.711 --> 00:29:01.710
each of these wait major cs.
And once we have those though,

482
00:29:01.711 --> 00:29:04.650
that's our gradient value.
That's the change.
That's the delta.

483
00:29:04.860 --> 00:29:09.270
We can then update all three wait major cities at once and we just keep doing

484
00:29:09.271 --> 00:29:10.290
that over and over again.

485
00:29:10.590 --> 00:29:15.100
So our first gradients of our loss is going to be computed using the,

486
00:29:15.120 --> 00:29:18.330
using this function.
So compute p minus one,

487
00:29:18.510 --> 00:29:21.180
and that's going to give us our first grade dance and we're going to use the

488
00:29:21.181 --> 00:29:25.950
chain rule to,
to backward pass that gradients into each,
uh,

489
00:29:26.910 --> 00:29:29.850
into each weight matrix.
So let me talk about what I mean by this.

490
00:29:29.940 --> 00:29:31.740
So the chain rule.
So remember,

491
00:29:31.890 --> 00:29:35.430
remember how I said neural networks are giant composite functions,
right?

492
00:29:35.520 --> 00:29:39.300
It's a giant composite function.
And what the chain rule lets us do is it less?

493
00:29:39.301 --> 00:29:43.330
That's compute the derivative of a giant of a function as a song,

494
00:29:43.360 --> 00:29:46.810
as the product of derivatives of it's nested functions.

495
00:29:47.080 --> 00:29:49.870
So the chain rule in the case of f of x,
right here,

496
00:29:50.170 --> 00:29:55.170
if f of x is a composite function that that consists of g of h of x,

497
00:29:55.810 --> 00:29:57.790
then the chain rule would be to say,

498
00:29:57.970 --> 00:30:02.970
well let's compute the derivative of g of h of x times a derivative of h of x

499
00:30:03.261 --> 00:30:04.300
said Nesad function.

500
00:30:04.600 --> 00:30:08.890
So you multiply it by the derivative of the inside function and that's we'll

501
00:30:08.891 --> 00:30:12.370
give you the derivative of that bigger function,
okay?

502
00:30:12.371 --> 00:30:15.640
And You keep doing that for as many nested functions as you have.

503
00:30:15.790 --> 00:30:16.990
Here's another example.

504
00:30:17.080 --> 00:30:20.830
If I want to derive the function three x plus one to the fifth,

505
00:30:21.130 --> 00:30:25.590
then I would say,
well this is actually a function.
And the function is,
um,

506
00:30:26.050 --> 00:30:29.890
the outer function,
g of x is three x plus one to the fifth.
So,
oh,

507
00:30:29.891 --> 00:30:33.490
so we're using the power rule.
We take the exponent value,

508
00:30:33.491 --> 00:30:37.120
move it to the Coefficient and subtract one from the exponent.

509
00:30:37.121 --> 00:30:41.680
So then it would be five times three of x plus one to the fourth times the

510
00:30:41.710 --> 00:30:45.550
derivative of the nested function,
which is three of three x plus one.

511
00:30:45.730 --> 00:30:47.140
And that's the chain rule.
And so,

512
00:30:47.300 --> 00:30:49.840
and if we multiply those two derivatives together,

513
00:30:50.020 --> 00:30:54.460
that will give us a derivative of the larger function f of x.

514
00:30:55.630 --> 00:30:59.350
So that same logic applies to neural networks because neural networks are a

515
00:30:59.351 --> 00:31:03.280
composite functions.
So we are,
we're cursively moving this derivative,

516
00:31:03.281 --> 00:31:05.410
this partial derivative value.
By moving,

517
00:31:05.411 --> 00:31:09.430
I mean multiplying dot product or computing the dot product between the partial

518
00:31:09.431 --> 00:31:11.680
derivative,
calculate it at the last layer,

519
00:31:11.830 --> 00:31:15.250
and we're multiplying it by every layer recursively going backward.

520
00:31:15.370 --> 00:31:17.680
This will make more sense as we look at this programmatically.

521
00:31:17.860 --> 00:31:22.090
But that's what's happening here.
And uh,
yeah,

522
00:31:22.091 --> 00:31:24.730
that's what's happening here.
So let's,
let's,
let's,
let's code this out.

523
00:31:25.000 --> 00:31:26.830
By the way,
the bias,

524
00:31:26.860 --> 00:31:31.180
the reason we add a bias is that allows you to move the thing of it as like
this,

525
00:31:31.450 --> 00:31:33.610
you know,
in the y equals mx plus B equation,

526
00:31:33.790 --> 00:31:37.240
it allows you to move the line up and down to better fit the data.
Uh,

527
00:31:37.300 --> 00:31:40.600
without be the line will always go through the origin zero,
zero,
and you might,

528
00:31:40.630 --> 00:31:44.590
you might get a poor fit.
So a bias is kind of like an anchor value.

529
00:31:45.250 --> 00:31:49.900
Any way to define our loss function,
our loss function is going to be.

530
00:31:50.260 --> 00:31:53.590
So we're going to give it our inputs and our targets as its parameters,

531
00:31:53.591 --> 00:31:57.940
as well as the,
uh,
hidden state from the previous time step.
Okay?

532
00:31:57.941 --> 00:32:02.941
So then let's define our perimeters that we're going to store these values in.

533
00:32:02.951 --> 00:32:06.160
So I'm going to define four parameters.
Okay?

534
00:32:06.161 --> 00:32:10.300
These are lists that we're going to store values at every time.
Step in,
okay?

535
00:32:10.301 --> 00:32:14.590
As we compute them.
So these are empty dictionaries.

536
00:32:14.770 --> 00:32:17.950
So ActiveX so excess is going to,

537
00:32:17.980 --> 00:32:22.480
we'll store the one hot encoded input characters for each of the two of the 25

538
00:32:22.481 --> 00:32:26.230
time steps.
So concern this will store the input characters.

539
00:32:26.530 --> 00:32:29.950
Hs is going to soar,
the hidden state outputs.
Okay.

540
00:32:29.980 --> 00:32:34.720
Why has we'll store the target values and ps is going to take the why's and

541
00:32:34.721 --> 00:32:38.890
convert them to normalize probabilities for chars.
Okay,

542
00:32:39.020 --> 00:32:43.150
so then let's go ahead and say uh,

543
00:32:43.460 --> 00:32:48.460
h of s h sorry h s the value of h s is going to be,

544
00:32:50.150 --> 00:32:52.100
the reason we're copying that looks to check this out.

545
00:32:52.490 --> 00:32:55.460
We're going to initialize this with the previous hidden states.

546
00:32:55.760 --> 00:33:00.080
The h s currently with the previous hidden states and using the equal sign would

547
00:33:00.081 --> 00:33:03.170
just create a reference but we want to create a whole separate array.

548
00:33:03.320 --> 00:33:04.910
So that's why we,
we don't,
we don't,

549
00:33:05.510 --> 00:33:10.070
we don't want hs with the element negative one to automatically change if
change,

550
00:33:10.220 --> 00:33:13.790
if age previous has changed.
So we'll create an entirely new copy of it.

551
00:33:14.420 --> 00:33:18.140
And so then we'll initialize our loss has zero and then and that,
okay,

552
00:33:18.141 --> 00:33:23.000
so we'll initialize our loss as zero.
So this is our loss scalar value.

553
00:33:23.330 --> 00:33:25.220
And then we'll go ahead and do the forward pass.

554
00:33:25.221 --> 00:33:27.980
So the forward pass is going to look like this.
Okay,

555
00:33:27.981 --> 00:33:31.280
so we've already looked at it mathematically and now we can look at it

556
00:33:31.281 --> 00:33:36.080
programmatically.
So we'll say,
okay,
so for each value in the range of inputs,

557
00:33:36.081 --> 00:33:40.010
so for the length of inputs,
let's compute a forward pass.

558
00:33:40.070 --> 00:33:42.710
So the forward pass is going to be um,

559
00:33:44.360 --> 00:33:48.200
we were going to start off with a that one of k representation.

560
00:33:48.201 --> 00:33:53.201
We placed a zero vector as the teeth and put and then inside that teeth input we

561
00:33:53.541 --> 00:33:57.950
use the manager in inputs list to set the correct value there.
Okay,

562
00:33:57.951 --> 00:34:00.800
so that's then that second line.
And then once we have that,

563
00:34:00.801 --> 00:34:02.090
we're going to compute the hidden state.

564
00:34:02.091 --> 00:34:05.870
Now remember I showed you the equation before.
We just repeat that equation here.

565
00:34:06.140 --> 00:34:08.360
And then we compute our output just like I showed before.

566
00:34:08.510 --> 00:34:10.100
And then our probabilities,

567
00:34:10.670 --> 00:34:14.120
the probabilities for the next chars once we have our probabilities will compute

568
00:34:14.121 --> 00:34:18.500
are softmax cross entropy loss,
which is the negative log likelihood.

569
00:34:18.680 --> 00:34:22.620
It's also called the cross entropy.
You'll actually see that in tenser flow,

570
00:34:22.621 --> 00:34:26.540
the cross entropy as a predefined function.
But we're computing it by hand here.

571
00:34:27.110 --> 00:34:30.560
And so once we have the forward pass,
now we can compute the backward pass.

572
00:34:30.740 --> 00:34:33.320
We're going to compute the gradient,
value's going backwards.

573
00:34:33.440 --> 00:34:37.640
So initialize empty vectors for these gradient values,
right?
So the gradients.

574
00:34:37.850 --> 00:34:40.010
So these are the gradients are the derivatives,

575
00:34:40.040 --> 00:34:42.530
the derivatives are our gradients is the same thing here.

576
00:34:42.920 --> 00:34:46.550
So we're computing are derivatives with respect to our weight values from x to

577
00:34:46.551 --> 00:34:51.020
age,
from age to age and then from age to y and we'll initialize them as Zeros.

578
00:34:51.350 --> 00:34:53.930
And then as also we also want to derive,

579
00:34:54.260 --> 00:34:58.880
we also want to compute partial derivatives or gradients for these um,
uh,

580
00:34:59.480 --> 00:35:04.180
bias values for our hidden state and our outputs.
And then,
uh,

581
00:35:04.190 --> 00:35:05.240
as well as for our next,

582
00:35:05.241 --> 00:35:08.600
which means the next time step that the hidden state in the next time step

583
00:35:08.601 --> 00:35:12.260
derivatives for all of them.
When we do backpropagation,
we're going to,

584
00:35:12.470 --> 00:35:16.820
we're going to collect our output probabilities and then the derive our first

585
00:35:16.821 --> 00:35:20.300
gradient value.
Now our first grade and value,
it looks like this.

586
00:35:20.660 --> 00:35:22.010
Let me go back up here.
This,
this,

587
00:35:22.130 --> 00:35:26.420
this is how we compute our first great and value with respect to our,
uh,

588
00:35:27.770 --> 00:35:32.360
with our Spec to our loss,
right?
That's a first gradient value.
So,

589
00:35:32.570 --> 00:35:34.900
uh,
we're going to compute the output gradients.

590
00:35:34.990 --> 00:35:37.920
So which is the output times the Hin states transpose.

591
00:35:38.250 --> 00:35:41.520
And we can think of this one.
So check this out right here.

592
00:35:41.760 --> 00:35:46.760
So this is our first partial derivative with r four r are hidden state to why to

593
00:35:46.951 --> 00:35:50.010
our output layer,
that matrix.
And you can,

594
00:35:50.040 --> 00:35:53.850
and so what we do is we compute the dot product between that output and the

595
00:35:53.851 --> 00:35:55.470
transpose of the hidden state.

596
00:35:55.890 --> 00:35:59.910
The reason we use a transpose is we can think of this intuitively as moving the

597
00:35:59.911 --> 00:36:01.620
error backward through the network,

598
00:36:01.621 --> 00:36:05.310
giving us some sort of measure of the error at the output of that layer.

599
00:36:05.550 --> 00:36:10.550
So when we compute the dot product between the transpose of some layers matrix

600
00:36:10.740 --> 00:36:15.300
with the derivative of the next layer that is moving the air backwards,

601
00:36:15.301 --> 00:36:19.260
it's kind of,
it's,
it's backpropagation because the error value,

602
00:36:19.261 --> 00:36:23.070
it's constantly changing.
With respect to every layer that it moves through.

603
00:36:23.250 --> 00:36:25.680
And by multiplying it,
by the transpose of a layer,

604
00:36:25.890 --> 00:36:30.890
the dot product from the partial derivative with the previous layer at times

605
00:36:31.111 --> 00:36:34.800
where we currently are,
it's going to output a gradient value that,
that,

606
00:36:34.860 --> 00:36:39.060
that derivative,
right?
And we'll use that derivative,
uh,

607
00:36:39.150 --> 00:36:42.180
later on to update other values as well.

608
00:36:42.480 --> 00:36:44.790
So we're are,

609
00:36:44.850 --> 00:36:48.120
we're also going to compute the derivative of the output bias and then we're

610
00:36:48.121 --> 00:36:49.950
going to back propagate into h.

611
00:36:51.540 --> 00:36:56.190
So notice how we are continuously performing dot product operations here for

612
00:36:56.191 --> 00:36:57.600
every single layer we have,

613
00:36:57.810 --> 00:37:01.680
we're also back propagating through the 10 h nonlinearity,
right?

614
00:37:01.680 --> 00:37:05.040
So we are competing the derivative value and this is programmatically what the,

615
00:37:05.460 --> 00:37:09.510
what the derivative of Tan h looks like.
And we're using the,
uh,

616
00:37:09.750 --> 00:37:14.250
computer derivatives from the previous layer that we were at at the end of the

617
00:37:14.251 --> 00:37:15.510
network,
the tail end,

618
00:37:15.660 --> 00:37:20.660
as we move through to the beginning as we're using them as values to compute the

619
00:37:21.091 --> 00:37:25.200
dot product of the whole point of computing the dot product.
With these,
uh,

620
00:37:25.230 --> 00:37:29.570
with respect to each of these layers is that we are computing new,
um,

621
00:37:29.610 --> 00:37:33.630
gradient values that we can then use to update our network later on.

622
00:37:33.840 --> 00:37:37.740
So then we use that,
a raw value to update our hidden value.

623
00:37:37.920 --> 00:37:39.360
And then we lastly,
we could,

624
00:37:39.450 --> 00:37:42.750
we compute the derivative of the input to the hidden layer as well as the

625
00:37:43.200 --> 00:37:47.160
derivative of the hidden layer to the hidden layer.
And once we have that,

626
00:37:47.340 --> 00:37:51.840
we can return all of those derivatives are gradient values are our change

627
00:37:51.841 --> 00:37:54.240
values,
we can return all of that.

628
00:37:54.420 --> 00:37:58.110
Now there's also this step right here to mitigate exploding gradients,

629
00:37:58.230 --> 00:38:01.440
which we're not going to go into right now because it's not really necessary.

630
00:38:01.620 --> 00:38:05.550
However,
I will say this,
that,
um,
whenever you have really,

631
00:38:05.551 --> 00:38:09.540
really long sequences of input data like book,
like the Bible,

632
00:38:09.541 --> 00:38:13.950
just a huge book,
then what happens is as the gradient is moving,
by moving,

633
00:38:13.951 --> 00:38:18.060
I mean you're competing the dot product of it for every layer with the current

634
00:38:18.061 --> 00:38:22.440
way matrix,
wherever you're at using the partial derivative,
the,

635
00:38:22.500 --> 00:38:24.810
the value will get smaller and smaller.
There's,

636
00:38:24.840 --> 00:38:28.290
it's a problem with the recurrent networks that's called the vanishing gradient

637
00:38:28.291 --> 00:38:30.300
problem.
Okay.
And so,
uh,

638
00:38:30.360 --> 00:38:32.760
it gets smaller and smaller and there's a way to prevent that.

639
00:38:33.000 --> 00:38:35.650
One way is to clip,
uh,
those,

640
00:38:35.651 --> 00:38:39.610
those values by defining some,
some interval that they can,
that can,

641
00:38:39.640 --> 00:38:43.180
they can reach or another way to use LSTM networks,

642
00:38:43.181 --> 00:38:46.210
which we're not going to talk about.
But anyway,
uh,
yeah,

643
00:38:46.270 --> 00:38:48.340
so that's our forward and backward pass.

644
00:38:48.520 --> 00:38:52.870
We computed that inside of the loss function and we computer our loss as well

645
00:38:52.930 --> 00:38:55.540
right here using softmax cross entropy.

646
00:38:55.930 --> 00:38:59.620
So for as many characters as we want to generate,
we will do this.

647
00:38:59.650 --> 00:39:04.000
So we'll say the Ford pass is just like we did before.
It's the same exact thing.

648
00:39:04.001 --> 00:39:08.200
It's just repeating the code over and over again.
Input Times,
wait,
activate,

649
00:39:08.201 --> 00:39:11.900
repeat,
get the probability values,
pick the one with the highest probability,

650
00:39:12.110 --> 00:39:15.700
create a vector for that word,
customize it for the predicted char,

651
00:39:15.701 --> 00:39:19.220
and then add it to the list.
And we just keep repeating that for as many.
Uh,

652
00:39:19.300 --> 00:39:22.960
n defines how many characters we want to generate so we can generate as many

653
00:39:22.961 --> 00:39:27.100
characters as we want on a train to network and we'll print those out.

654
00:39:28.720 --> 00:39:32.860
Okay,
so then for the training part,
we really competed,
we've completed that,

655
00:39:32.861 --> 00:39:35.560
that meat of that code,
right?
But now for the training part,

656
00:39:35.710 --> 00:39:39.160
we're going to feed the network some portion of the file and then for the loss

657
00:39:39.161 --> 00:39:42.550
function we're going to do a forward pass to calculate all those parameters for

658
00:39:42.551 --> 00:39:46.630
the model for a given input for a given input output pair,
the current char,

659
00:39:46.631 --> 00:39:47.464
the next char,

660
00:39:47.620 --> 00:39:50.470
and then we're going to do a backward pass to calculate all those gradient

661
00:39:50.490 --> 00:39:54.100
values.
And then we're going to update the model using a technique.
It's a,

662
00:39:54.220 --> 00:39:57.820
it's a type of grading dissent technique called Adda Grad,

663
00:39:58.000 --> 00:40:00.630
which is just it just a case of learning rate,
but it's,
it's,

664
00:40:00.820 --> 00:40:03.640
it's great in a sense.
You'll see what I'm talking about.
It's not complicated,

665
00:40:03.940 --> 00:40:04.990
but it's called add grad.

666
00:40:05.410 --> 00:40:08.680
So we're going to create two arrays of Charles from the data file to target.

667
00:40:08.681 --> 00:40:10.600
One is going to be shifted from the input one.

668
00:40:10.720 --> 00:40:13.180
So we basically just shifted by one as you notice here.

669
00:40:13.300 --> 00:40:15.370
So now we have our inputs in our targets,
right?

670
00:40:15.520 --> 00:40:18.370
And these numbers are actually character values in the dictionary,

671
00:40:18.520 --> 00:40:22.940
but they represent,
uh,
they help us,
they help us create directors,
uh,

672
00:40:23.020 --> 00:40:26.620
where the indices here represent the one out of all the Zeros and the zero

673
00:40:26.621 --> 00:40:28.750
vector.
And that's what we feed into our model.

674
00:40:28.870 --> 00:40:31.420
So at a grad is our gradient descent technique.

675
00:40:31.421 --> 00:40:36.280
And the difference here as composed as compared to regular grading dissent is

676
00:40:36.281 --> 00:40:40.390
that we decay the learning rate over time.
And what this does is it helps our,
uh,

677
00:40:40.450 --> 00:40:44.230
network,
uh,
learn more efficiently.
This is the,

678
00:40:44.231 --> 00:40:49.231
this is the equation for Adot Grad where a step size means the same thing as a

679
00:40:49.241 --> 00:40:53.020
learning rate,
but basically the learning rate gets smaller,

680
00:40:53.050 --> 00:40:56.980
smaller during training because we introduced this memory variable that grows

681
00:40:56.981 --> 00:40:59.140
over time to calculate the step size.

682
00:40:59.380 --> 00:41:03.940
And the reason it grows while the step size decreases is because it's inside of

683
00:41:03.941 --> 00:41:06.250
the denominator of this function right here.

684
00:41:06.340 --> 00:41:09.940
This is the programmatic representation of the mathematical equation that you're

685
00:41:09.941 --> 00:41:13.690
looking at right here.
So here's the programmatic implementation of that.

686
00:41:13.900 --> 00:41:17.920
We calculate this memory value,
which is we are gradients,

687
00:41:17.950 --> 00:41:21.490
right of our parameters.
And then we update our,

688
00:41:21.690 --> 00:41:25.630
and then we update our weight Matrix,
um,
condition on the learning rate,

689
00:41:25.690 --> 00:41:28.570
which decays over time via this function right here.

690
00:41:30.790 --> 00:41:33.590
So finally,
so this is really,
this is,
so we've,
we've,

691
00:41:33.650 --> 00:41:36.830
we've done all the math and now it's just implementing it.
So,

692
00:41:38.510 --> 00:41:42.650
um,
so we have our,
uh,
weight matrices here.

693
00:41:42.740 --> 00:41:45.950
We have our memory variables for a Grad and then,
um,

694
00:41:46.010 --> 00:41:49.880
we will say for a thousand iterations will actually a thousand times a hundred

695
00:41:49.881 --> 00:41:54.470
iterations.
We want to feed the loss function,
we want to feed the loss function,

696
00:41:54.471 --> 00:41:58.910
the input vectors to see how this part works.
We're gonna feed the loss function,

697
00:41:58.911 --> 00:41:59.990
our input vectors,

698
00:42:00.230 --> 00:42:03.800
and then we're going to compute a Ford pass using that loss function.

699
00:42:04.190 --> 00:42:05.660
And it's going to compute the loss as well.

700
00:42:05.661 --> 00:42:08.660
It's going to return the loss function or the loss scaler.

701
00:42:08.810 --> 00:42:12.860
It's going to return the derivatives or gradients with respect to all of those

702
00:42:12.980 --> 00:42:17.020
weighed values that we want to update.
And then we're going to,
um,

703
00:42:19.070 --> 00:42:21.920
performed the parameter update using ad or Grad,
right?

704
00:42:21.921 --> 00:42:26.780
So we'll feed all the derivative values to uh,
our ad,
a Grad.
Um,

705
00:42:30.540 --> 00:42:32.500
this is Adam [inaudible].
So whose whole fee,

706
00:42:32.520 --> 00:42:36.870
all those derivative values to our ad Grad function right here.
Okay.

707
00:42:36.871 --> 00:42:40.200
And it's going to update our parameters and basically the learning rate just

708
00:42:40.201 --> 00:42:42.240
decays over time.
That's what the,

709
00:42:42.270 --> 00:42:47.270
that's why Ma'am is calculated to decay the learning rate over time and um,

710
00:42:47.610 --> 00:42:49.830
which just helps with convergence and a,

711
00:42:49.831 --> 00:42:53.490
there's different grading dissent techniques,
Adam on different ones like that,

712
00:42:53.491 --> 00:42:57.990
but,
um,
uh,
momentum.
But yeah,
I had a grad is one of them.
And so once we do that,

713
00:42:57.991 --> 00:42:59.880
we can look at our sample function here,

714
00:42:59.910 --> 00:43:03.930
our sample function and our sample function is going to,
right here,

715
00:43:04.170 --> 00:43:07.170
we're going to keep,
we're going to generate 200 word up,

716
00:43:07.200 --> 00:43:10.890
200 character sentences at a time for,
uh,

717
00:43:10.920 --> 00:43:14.100
for a thousand times a hundred iterations.
So a lot of iteration,

718
00:43:14.120 --> 00:43:18.390
100,000 iterations.
Okay.
So let's go ahead and run this and see what happens.

719
00:43:18.960 --> 00:43:21.270
Okay.
See,
the first iteration is really bad.
Look at that.

720
00:43:21.271 --> 00:43:22.680
It's just like weird characters.
Okay.

721
00:43:22.681 --> 00:43:26.730
But now it's got a more human readable characters.
Okay.
It's getting better now.

722
00:43:26.731 --> 00:43:28.650
It's like he ate left.

723
00:43:28.830 --> 00:43:33.600
Notice how the loss is decreasing very rapidly here as well.
Okay.
And so yeah,

724
00:43:33.601 --> 00:43:37.740
it's getting better over time.
Okay.
So that's it for our network.

725
00:43:38.160 --> 00:43:41.520
And let me stop this and you can feed it anything really,
you can feed it.

726
00:43:41.521 --> 00:43:44.610
Any text file it's going to work with any text file.
Okay.

727
00:43:44.611 --> 00:43:46.290
So we've computed the Ford Pass,

728
00:43:46.291 --> 00:43:50.070
the backward pass that the backward pass is just the chain rule.
Okay.

729
00:43:50.071 --> 00:43:53.790
I've got links to help you out in the description,
but it's just the chain roll.

730
00:43:53.910 --> 00:43:58.110
We're just continuously computed computing derivatives or grading values,

731
00:43:58.410 --> 00:44:00.750
partial derivatives or gradients.
Same thing.

732
00:44:00.900 --> 00:44:02.320
We call them partial because there were,

733
00:44:02.550 --> 00:44:06.990
with respect to each of the weights in the network going backward and we are

734
00:44:06.991 --> 00:44:08.820
moving this error by moving,

735
00:44:08.821 --> 00:44:12.300
we're considering the dot product of each layer.

736
00:44:12.470 --> 00:44:17.190
This matrix by the derivative of the previous layer just continually.

737
00:44:17.191 --> 00:44:21.420
And that's the chain rule.
And if we do this,
we can generate words,

738
00:44:21.540 --> 00:44:26.280
we can generate any type of word.
We want to have given some,
some text corpus.

739
00:44:26.430 --> 00:44:29.760
You can generate Wikipedia articles,
you can generate fake news,

740
00:44:30.060 --> 00:44:35.040
you can generate anything really code.
And yeah,
so also for deep learning,

741
00:44:36.090 --> 00:44:40.650
you might be asking what for deep learning,
which of these layers do we add deep?

742
00:44:40.740 --> 00:44:43.890
Which where do we add deeper layers to?

743
00:44:43.891 --> 00:44:47.730
Do we add a more layers between the input and the hidden state between the

744
00:44:47.731 --> 00:44:51.150
hidden state and the output or between the hidden stay in the hidden say Matrix,

745
00:44:51.330 --> 00:44:54.000
which direction do I had?
Deeper and deeper layers?
Well,

746
00:44:54.001 --> 00:44:58.080
the answer is that it depends.
This is one thing that's being worked on,
but the,

747
00:44:58.140 --> 00:45:01.770
but the idea is that you'll get different results for whatever,
um,

748
00:45:02.430 --> 00:45:05.230
whatever set of matrices that you,
um,

749
00:45:05.490 --> 00:45:09.390
add deeper layers to their different papers on this.
But yes,

750
00:45:09.450 --> 00:45:11.730
adding deeper layers is going to give you better results.

751
00:45:11.731 --> 00:45:14.610
And that's deep learning.
Recurrent nets applied to deep learning.

752
00:45:14.611 --> 00:45:18.660
But this is a simple three layer feed forward network that works really well.

753
00:45:18.840 --> 00:45:22.770
And I would very much encourage you to check out the get help link in the

754
00:45:22.771 --> 00:45:26.970
description and the learning resources to learn more about this.
So yeah,

755
00:45:27.120 --> 00:45:29.580
please subscribe for more programming videos.
And for now,

756
00:45:29.610 --> 00:45:33.300
I've got to do a four year transform,
so thanks for watching.

