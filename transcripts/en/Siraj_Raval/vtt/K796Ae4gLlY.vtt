WEBVTT

1
00:00:10.510 --> 00:00:15.100
Oh,
live stream is starting Sarah.
Okay,
there it is.
Oh we're hold.
It's Raj.

2
00:00:15.101 --> 00:00:19.120
Good to see you guys.
Welcome to this live session.
We're going to,

3
00:00:19.180 --> 00:00:21.850
we are so live right now.
It's not even funny.
We are so,

4
00:00:23.250 --> 00:00:28.030
I mean you myself.
Okay,
great.
Okay.
We got people in the room.

5
00:00:28.180 --> 00:00:32.350
We got a bunch of cool people in the room.
We got Brandon Geo.
I'd be rude.

6
00:00:32.740 --> 00:00:36.250
I'd invite Esteban.
Yo.
Hi everybody.

7
00:00:36.280 --> 00:00:39.970
We Day patchy the pirate.
Okay,

8
00:00:40.450 --> 00:00:44.190
good to see everybody it pawn.
Why Radu.

9
00:00:44.830 --> 00:00:47.950
Okay John.
Hi.

10
00:00:49.000 --> 00:00:50.050
Ooh Korean.

11
00:00:50.080 --> 00:00:53.560
I wish I could read Korean someday when we have a neural lace and weak and

12
00:00:53.730 --> 00:00:57.520
languages.
Not as much of a problem for us.
Okay.
Are you guys okay?

13
00:00:58.090 --> 00:01:00.700
We've got a lot of people in the room right now.
Okay.
Okay.
This is tiny.
Okay,

14
00:01:00.790 --> 00:01:05.790
so today we are going to talk about dimensionality reduction.

15
00:01:06.730 --> 00:01:09.340
Okay.
We are going to go deep on this.

16
00:01:09.400 --> 00:01:14.320
We are going to actually build TCA and I've talked about this in my last weekly

17
00:01:14.321 --> 00:01:18.340
video principle component analysis from scratch.
Okay,

18
00:01:18.341 --> 00:01:21.340
well that's what we're going to do and it's,
it's very useful.

19
00:01:21.370 --> 00:01:24.280
We have a lot of use cases and we're going to compare it to two other

20
00:01:24.580 --> 00:01:28.030
dimentionality reduction methods.
We're going to compare it to t.

21
00:01:28.031 --> 00:01:30.280
S n e n l d a.

22
00:01:30.370 --> 00:01:35.230
So we are in acronym heaven right now.
Okay.

23
00:01:35.590 --> 00:01:39.510
Um,
that's what we're going to do and we are definitely going to do some act.

24
00:01:39.530 --> 00:01:41.680
We are doing some math.
I'm cause guys,

25
00:01:41.710 --> 00:01:46.150
I am the most excited when we do math because math is awesome.
Okay.

26
00:01:46.360 --> 00:01:51.310
And it's not taught well.
Map is not taught well.
So that's what I'm here to do.

27
00:01:51.610 --> 00:01:54.730
And let's start off with a five minute Q and a and then we're going to dive

28
00:01:54.790 --> 00:01:57.240
right into the math in an eye python notebook.

29
00:01:57.250 --> 00:02:01.450
We're going to build PCA from scratch to simplify our Dataset so we can

30
00:02:01.451 --> 00:02:05.140
visualize it.
All right,
so hit me with your best questions,

31
00:02:07.290 --> 00:02:11.820
not math.
Math.
Okay,
here we go.

32
00:02:13.140 --> 00:02:18.080
Come on guys.
Let's see it.
Thank you so dark.

33
00:02:22.120 --> 00:02:25.930
How did you pruning on Alex net or any deep neural network?

34
00:02:26.710 --> 00:02:31.300
I am Newbie and no basic python pruning for the architecture or printing for the

35
00:02:31.301 --> 00:02:34.720
data.
I will do a freestyle.
Um,
first of all,

36
00:02:34.721 --> 00:02:37.240
Alex net is great for transfer learning.
Um,

37
00:02:37.480 --> 00:02:40.660
we're going to talk about transforming it in a later episode.
Are you in the yes.

38
00:02:40.720 --> 00:02:44.320
You like game boy.
I mean when I was 10 years old when it was,
that was the thing.

39
00:02:44.410 --> 00:02:48.250
I want that hairstyle.
Any tips?
Uh,
dye your hair.
It's silver right in the middle.

40
00:02:48.310 --> 00:02:51.280
Take a picture tweeted.
I'll retweet it.
Uh,
agree.

41
00:02:51.281 --> 00:02:53.170
The problem is not the math as a teacher.
Exactly.

42
00:02:54.130 --> 00:02:56.590
If a neural net can figure out the right weights for itself.

43
00:02:56.740 --> 00:02:59.200
What is the purpose of running PCA?

44
00:02:59.890 --> 00:03:03.880
The purpose of PCA is to reduce the dimensionality of our data so we can

45
00:03:03.881 --> 00:03:08.110
visualize it.
It reduces the,
I'll talk about why in a second.
Actually,

46
00:03:08.500 --> 00:03:12.370
what are your thoughts on our use python instead?
More modular.
So Raj,

47
00:03:12.371 --> 00:03:15.610
are you liable?
Tensorflow?
Summit is live right now.
Yes.
First of all,

48
00:03:15.611 --> 00:03:19.210
they didn't even enabled chat guys.
I would be there if they enable chat.

49
00:03:19.211 --> 00:03:20.800
They didn't do that,
so I'm going to watch it later.

50
00:03:20.801 --> 00:03:23.680
They're just spewing facts without there being any community aspects.

51
00:03:23.770 --> 00:03:27.130
Love tensorflow.
Love you guys.
But you guys got an enabled chat.
Okay?

52
00:03:28.110 --> 00:03:28.660
<v 2>Okay.</v>

53
00:03:28.660 --> 00:03:32.530
<v 1>So how can I reduce complexity in imperfect information games?</v>

54
00:03:33.500 --> 00:03:33.890
<v 2>Okay.</v>

55
00:03:33.890 --> 00:03:35.810
<v 1>Imperfect information game.</v>

56
00:03:35.870 --> 00:03:39.140
Reduce complexity in imperfect information game.

57
00:03:39.850 --> 00:03:40.683
<v 2>Uh,</v>

58
00:03:42.210 --> 00:03:43.590
<v 1>please clarify.
What do you mean by that?</v>

59
00:03:43.770 --> 00:03:45.840
How do you think about generative models?
How do,

60
00:03:45.900 --> 00:03:50.100
how do I think about generative models?
Um,
how do I think about it?
I,

61
00:03:51.000 --> 00:03:54.240
I think about it as a distribution.
It's a distribution of possibilities.

62
00:03:54.241 --> 00:03:58.350
It's a curve of possibilities and we give it data and then we kind of generate,

63
00:03:59.130 --> 00:04:02.300
we generate new data based on that distribution and possibilities.

64
00:04:02.301 --> 00:04:03.840
So it's all of the old,
old,

65
00:04:03.841 --> 00:04:06.990
what is generated is related to the data that already exists.

66
00:04:07.140 --> 00:04:09.720
It's not something completely off the off the rails.

67
00:04:10.950 --> 00:04:14.610
How can ml change a common,
a common thing in my life?

68
00:04:15.090 --> 00:04:18.120
How can ml change with common thing in my life?
Okay guys,

69
00:04:19.080 --> 00:04:22.740
we are the most important people in the world.

70
00:04:24.120 --> 00:04:26.730
Okay?
We are the most important community in the world.

71
00:04:26.910 --> 00:04:31.260
The people in this community are going to go on to build the most valuable

72
00:04:31.261 --> 00:04:33.870
startups to make the biggest contributions to ml.

73
00:04:34.530 --> 00:04:36.150
If software is eating the world.

74
00:04:36.180 --> 00:04:40.250
As Marc Andreessen of Andreessen Horowitz says an ml is eating software that

75
00:04:40.251 --> 00:04:43.410
people building ml are the most important people in the world.

76
00:04:43.770 --> 00:04:46.650
Even if you don't know that much about ml,

77
00:04:46.680 --> 00:04:49.500
even if you barely know anything about ml,

78
00:04:49.620 --> 00:04:51.270
the fact that you are in this livestream stream,

79
00:04:51.271 --> 00:04:53.310
the fact that you were even watching this video,

80
00:04:53.311 --> 00:04:57.030
that you have the awareness to know how important it says that you have the

81
00:04:57.031 --> 00:05:00.990
desire to learn,
it puts you in the top bracket of people.
Okay,

82
00:05:01.590 --> 00:05:05.340
we are,
we're going to be the biggest machine when he community in the world.

83
00:05:05.360 --> 00:05:08.340
Okay.
And we are just getting started.
So how can it,

84
00:05:08.370 --> 00:05:10.830
it's a common thing in your life.
You have to

85
00:05:12.430 --> 00:05:15.100
start learning the basics and then inspiration will come.

86
00:05:15.280 --> 00:05:18.220
Once you understand the tools that are,
that are at your disposal,

87
00:05:18.310 --> 00:05:21.700
you will find something that you are passionate about and I refuse to accept

88
00:05:21.701 --> 00:05:25.660
that you don't find anything that you're passionate about because you will.
Okay,

89
00:05:25.661 --> 00:05:26.800
I reject that notion.

90
00:05:28.160 --> 00:05:28.810
<v 2>Yeah,</v>

91
00:05:28.810 --> 00:05:30.190
<v 1>I found you today.
Okay.</v>

92
00:05:30.790 --> 00:05:34.510
Can de-boning be used in time series forecasting type of application?

93
00:05:34.511 --> 00:05:38.680
Like in real time tool breakdown in drilling oil.
Yeah,

94
00:05:38.890 --> 00:05:42.040
deep learning can be used for time series.
An LSTM recurrent neural network.

95
00:05:42.100 --> 00:05:45.940
We'll talk about that later.
Where are you?
I am in San Francisco.

96
00:05:46.120 --> 00:05:51.070
What are challenges?
Deep learning basis
facing pre presently.
Um,

97
00:05:51.180 --> 00:05:55.980
we need to,
okay,
so the major problems are learning with unlabeled data.

98
00:05:56.010 --> 00:05:59.920
That's the biggest right now.
How do we learn an uncertain on unsupervised?

99
00:06:00.240 --> 00:06:05.240
How do we perform unsupervised learning without labels and have it be a useful

100
00:06:06.150 --> 00:06:09.720
our predictions and,
okay,
so that's what that is.
But five minutes are over.

101
00:06:09.721 --> 00:06:12.960
Thanks on top of 1% per for saying that I'm going to do a freestyle and I'm

102
00:06:12.961 --> 00:06:13.501
going to use music.

103
00:06:13.501 --> 00:06:15.870
So someone shout out and topic or one minute freestyle and then we're gonna get

104
00:06:15.871 --> 00:06:19.410
started.
Here we go.
Someone shout out of topic.
Just one word.

105
00:06:21.440 --> 00:06:24.410
Yo,
shout out on topics.

106
00:06:25.220 --> 00:06:28.230
One topic and then we're going to go,
I'm waiting for a topic.

107
00:06:32.090 --> 00:06:36.620
First ones to say a word first.
First one.
I see planning,

108
00:06:38.450 --> 00:06:42.800
Yo
go on to other planets.

109
00:06:42.940 --> 00:06:47.000
I you to understand it.
See when I go there,
it's like I gotta be something else.

110
00:06:47.090 --> 00:06:51.320
I go with the state.
It's like my on weight,
it,

111
00:06:51.350 --> 00:06:54.830
all these other things.
I tried to close,
I tried to float my stage.

112
00:06:54.890 --> 00:06:59.330
Patient goes out of control.
Yo.
When I go past the solar system in the moon,

113
00:06:59.331 --> 00:07:03.780
I told Jack like I'm flying,
I'm a little zoom,
zoom,
zoom.
I go out.

114
00:07:03.840 --> 00:07:08.810
Is the space earth from a machine learning space?
That's what I love to do.

115
00:07:08.930 --> 00:07:13.100
I do it every day.
I do with my weight.
I do eight.
Oh Hey.
Not really.

116
00:07:13.240 --> 00:07:17.600
I do a great,
it's like I'm from Chile.
Not really.
I'm from San Francisco.

117
00:07:18.210 --> 00:07:21.560
That's not a place where I stay at Yo cause I don't want every day.

118
00:07:21.620 --> 00:07:25.160
And I do it when our views tend to flow.
Okay.
So that was it.

119
00:07:25.220 --> 00:07:29.030
My one minute freestyle.
Okay.
And I use music this time.
So let's get,

120
00:07:29.190 --> 00:07:32.000
let's get on with this.
All right,
let's get on with this.
Um,

121
00:07:32.750 --> 00:07:35.320
I'm going to start sharing my screen and we're going to build the,

122
00:07:35.710 --> 00:07:38.870
we're going to do bill PCA.
Okay.
So here we go.
Here we go.

123
00:07:39.470 --> 00:07:41.180
Let's get started with this.

124
00:07:41.910 --> 00:07:42.450
<v 0>Okay.</v>

125
00:07:42.450 --> 00:07:45.690
<v 1>Screenshare and guys,
I want you to code along with me.</v>

126
00:07:45.691 --> 00:07:50.670
So open up an I python notebook and start a coding along with me.

127
00:07:50.700 --> 00:07:51.690
Okay,
so here we go.

128
00:07:55.940 --> 00:07:56.773
<v 0>Yeah.</v>

129
00:07:57.880 --> 00:07:58.870
<v 1>Okay,
here we go.</v>

130
00:08:06.730 --> 00:08:11.690
Here we go.
So
what are we going to do today,
guys?
Let me,

131
00:08:11.691 --> 00:08:13.340
let me,
let me also have myself up.

132
00:08:13.370 --> 00:08:17.030
I'm going to put myself up in a little quick time window like I like I do.

133
00:08:17.031 --> 00:08:20.210
You know how I guys,
how I do right guys?
Okay,

134
00:08:22.140 --> 00:08:22.290
sure.

135
00:08:22.290 --> 00:08:25.500
I am in the little corner of the screen and we're going to get started with our

136
00:08:25.501 --> 00:08:30.150
iPods on notebook.
Okay,
so let me make myself a little smaller so I fit in there.

137
00:08:30.330 --> 00:08:34.830
Okay,
so here we go.
Let's do this.
Let's do this.
So what are we going to do it?

138
00:08:34.831 --> 00:08:38.670
We're going to build TCA.
Okay.
And why do we want to build PCA?

139
00:08:39.840 --> 00:08:43.440
Oh,
so why do we use dimension that why you dimensionality reduction?

140
00:08:43.441 --> 00:08:47.520
What is the reason for that?
What is the purpose?
There are three reasons.
Reason.

141
00:08:47.521 --> 00:08:51.390
One reason why,
okay,
I'll give you a minute to initialize.

142
00:08:51.391 --> 00:08:53.260
I'm just writing up some comments.
So go ahead.

143
00:08:53.340 --> 00:08:55.680
Go ahead and initialize your notebook.
We're going to do from scratch,

144
00:08:55.681 --> 00:08:59.700
we're going to use num Pi and later we're going to use map plot line to plot our

145
00:08:59.701 --> 00:09:00.061
data.
Okay,

146
00:09:00.061 --> 00:09:05.061
so reason one for doing dimensionality reduction is space efficiency.

147
00:09:05.430 --> 00:09:08.890
We want space efficiency,
right?
Lina is huge right there.

148
00:09:09.370 --> 00:09:10.950
We could have 500 gigabytes of data,

149
00:09:10.980 --> 00:09:14.910
but if we find those features that are the most important,
we could reduce the,

150
00:09:15.380 --> 00:09:17.790
the,
the,
the amount of space our data takes.

151
00:09:17.970 --> 00:09:20.700
And cause not everyone has terabytes and terabytes of space.

152
00:09:20.701 --> 00:09:25.470
So that's reason one.
The other reason,
reason too is computing efficiency.

153
00:09:26.760 --> 00:09:31.230
Okay?
Let's data there is the faster our model will run,

154
00:09:31.920 --> 00:09:35.880
right?
Make sense,
right?
The last we have,
uh,
the less data we have,

155
00:09:36.010 --> 00:09:39.870
the more the faster model run.
And reason three,
and my favorite reason,

156
00:09:40.260 --> 00:09:42.840
my favorite reason is visualization.

157
00:09:43.140 --> 00:09:47.250
We cannot visualize hundred dimensional data.

158
00:09:47.530 --> 00:09:51.300
When in dimensions are the same as features.
They are synonyms.
Okay?

159
00:09:51.450 --> 00:09:53.310
So remember that dimensions are features.

160
00:09:53.311 --> 00:09:55.440
So if we have a hundred dimensional data set,

161
00:09:55.620 --> 00:09:59.700
we can reduce the dimensionality to two d or three d and then we can view it.

162
00:09:59.880 --> 00:10:04.880
And why do we want to view it so we can analyze by human eye and it just looks

163
00:10:05.371 --> 00:10:08.100
pretty as well.
Okay.
We can analyze it by human eyes.

164
00:10:08.101 --> 00:10:11.070
There's a lot of insights we can make if we can just look at the data.

165
00:10:11.250 --> 00:10:15.510
So today we're going to build PCA.
Okay.
We're going to build PCA.

166
00:10:15.690 --> 00:10:16.980
We will build PCA

167
00:10:18.810 --> 00:10:23.810
and then compare it to DSME and Lva and I'll talk about these methods.

168
00:10:24.541 --> 00:10:28.890
So,
so three dimensionally reduction methods total.
Okay.

169
00:10:29.110 --> 00:10:30.120
And so that's what we're going to do.

170
00:10:30.121 --> 00:10:32.850
I'm going to talk about the math behind this and we're going to talk about the

171
00:10:32.851 --> 00:10:37.670
math behind this.
Okay?
Okay.

172
00:10:37.671 --> 00:10:39.950
So here we go.
We're going to start off with this.
Okay.
So let's,

173
00:10:39.951 --> 00:10:42.300
let's start off with,
let me make the first,
uh,

174
00:10:43.710 --> 00:10:44.020
<v 0>okay.</v>

175
00:10:44.020 --> 00:10:47.230
<v 1>The first important.
So we're going to build PCA.
Okay.
So here we go.</v>

176
00:10:47.530 --> 00:10:50.200
The first thing we'll do is important.
I'm crying.

177
00:10:50.230 --> 00:10:53.260
We always want to important on five or any kind of map that we wanted to.

178
00:10:53.810 --> 00:10:56.570
Then we're going to introduce a c.

179
00:10:56.890 --> 00:10:59.290
Can anyone tell me why we introduced the seat?

180
00:11:00.280 --> 00:11:04.870
Three seconds to think about it.
Okay.
Because it's good for debugging,

181
00:11:05.180 --> 00:11:08.710
for debugging.
It's,
it makes,
it makes cause we're gonna,

182
00:11:08.740 --> 00:11:13.170
we're gonna randomly generate data and,
and we want to start from the same seed,

183
00:11:13.171 --> 00:11:14.320
that same starting point.

184
00:11:14.470 --> 00:11:19.090
So whenever we tested the randomly generated numbers are always the same.

185
00:11:19.390 --> 00:11:22.240
Okay?
So that's just for debugging.
So it's just,
it's just good practice.

186
00:11:22.241 --> 00:11:24.190
Not Necessary,
but it's good practice.

187
00:11:24.340 --> 00:11:27.250
So now let's go ahead and,

188
00:11:29.770 --> 00:11:31.240
uh,
create two classes.

189
00:11:31.241 --> 00:11:35.050
We're going to create two classes and now we're going to put on our map had
guys,

190
00:11:35.051 --> 00:11:37.810
and this is where I'm so excited because we're going to put on our map tap.

191
00:11:37.990 --> 00:11:41.630
So step one is to create our data datasets.

192
00:11:41.800 --> 00:11:45.370
We're going to create this space that,
okay.
And to do this,
we're going to,

193
00:11:46.210 --> 00:11:48.260
we're going to,
okay.
Okay.

194
00:11:48.530 --> 00:11:53.530
We're going to initialize one variable first and I'm going to call it move back

195
00:11:53.951 --> 00:11:57.940
one,
which stands for a sample mean.
Okay.
So,
the,

196
00:11:58.150 --> 00:12:00.550
so I'll talk about why I'm creating a meat.
So,

197
00:12:00.730 --> 00:12:02.860
so I'm going to use num Pi to do this.

198
00:12:02.920 --> 00:12:06.210
I'm going to say here's a zero uh,

199
00:12:06.400 --> 00:12:09.970
and so this is the sample mean of the data.
Why do we want the mean?

200
00:12:10.030 --> 00:12:13.330
The mean is the average of the day.
That's what it's going to represent.

201
00:12:13.420 --> 00:12:16.480
That's what it's going to represent.
So it's going to be just a matrix of Zeros.

202
00:12:16.660 --> 00:12:21.220
And then I'm going to,
I'm going to create a covariance,
a sample,

203
00:12:21.221 --> 00:12:24.690
covariance,
and I'm going to talk about what these two things are and et cetera.

204
00:12:24.790 --> 00:12:27.370
Okay?
So let me just write this out.
And uh,

205
00:12:28.210 --> 00:12:32.840
so the sample covariance is going to be one zero zero and then,
uh,
let's see,

206
00:12:32.890 --> 00:12:34.900
zero,
one zero.
So,

207
00:12:34.901 --> 00:12:38.530
and then what's the last one we want to fill in that last blank spot just so we

208
00:12:38.531 --> 00:12:42.310
have two zero zero one okay,
so this is the sample.

209
00:12:42.311 --> 00:12:46.120
Covariants why did I initialize these things?
What's the point of that?

210
00:12:46.300 --> 00:12:49.790
There is a very important one point because we were going to use both of these

211
00:12:49.791 --> 00:12:50.540
values,

212
00:12:50.540 --> 00:12:55.520
both the mean and the covariants to generate our first data and we're going to

213
00:12:55.521 --> 00:12:59.810
call it class one sample.
How do we generate this dataset?
Well,

214
00:12:59.840 --> 00:13:04.580
we're going to use one of num Pi's functions called random dot.

215
00:13:04.640 --> 00:13:08.480
Multi-Variate underscored normal.
Uh,

216
00:13:08.500 --> 00:13:10.310
and I'm going to talk about what this does in a second,

217
00:13:10.311 --> 00:13:13.670
but to see these are the parameters we use to generate that data.

218
00:13:13.850 --> 00:13:17.630
So let's talk about what,
what the hell is actually happening here.

219
00:13:18.110 --> 00:13:20.720
And I'm gonna use a transfer to that and let me print it out and I'm going to

220
00:13:20.721 --> 00:13:24.080
talk about it.
Here we go.
So when you print it out and made sure it worked okay?

221
00:13:24.330 --> 00:13:25.163
<v 0>Yeah,</v>

222
00:13:26.450 --> 00:13:28.480
<v 1>I is not defined.
Wait a second.</v>

223
00:13:30.800 --> 00:13:34.430
What line is that?
That is on move back one him that array

224
00:13:37.400 --> 00:13:38.480
in court,
num,
py,

225
00:13:38.590 --> 00:13:43.590
SNMP name NP is not defined to important.

226
00:13:45.890 --> 00:13:46.723
Hold on a second.

227
00:13:52.890 --> 00:13:53.723
<v 0>Okay.</v>

228
00:13:54.500 --> 00:13:57.650
<v 1>Two non keyword arguments are accepted for this.
Okay.
Let's see.</v>

229
00:13:57.651 --> 00:14:01.640
Let's see what's happening here.
Let's see.
Let's see.

230
00:14:02.480 --> 00:14:07.010
None pilot array,
covariance matrix and pot.
I got a red one.
One.

231
00:14:07.320 --> 00:14:11.450
Oh,
okay.
Okay.
I see.
So there has to be another one of these.

232
00:14:12.200 --> 00:14:17.000
Okay,
so good.
Okay,
so now we generate,
so what the hell just happened here?
Right?

233
00:14:17.030 --> 00:14:18.350
So let's talk about what this is.

234
00:14:18.440 --> 00:14:23.060
Let's talk about exactly what I just did it cause this is linear Algebra.
Okay.

235
00:14:23.061 --> 00:14:28.010
This is linear Algebra.
So I generated this multivariate normal.

236
00:14:28.040 --> 00:14:30.950
What is a multivariate normal distribution?

237
00:14:31.070 --> 00:14:34.510
It's the same thing as a Garcia and distribution.
So let me,
um,

238
00:14:34.880 --> 00:14:39.860
so this is a Gaussian distribution.
This is,
this is what are generated data.

239
00:14:39.861 --> 00:14:43.010
It looks like in three d space there are a set of points.

240
00:14:43.100 --> 00:14:47.980
It's a three dimensional matrix of values.
Okay?
So there aren't,

241
00:14:47.981 --> 00:14:51.170
so what this is is it's,
it's a three by 20 matrix.
Okay?

242
00:14:51.171 --> 00:14:54.410
So let me write that down.
It says eight,
three times 20 majors.

243
00:14:54.680 --> 00:14:57.230
There are three columns with,

244
00:14:58.130 --> 00:15:00.820
with 20 rows each.
Okay?

245
00:15:00.821 --> 00:15:05.821
So there's 20 rows I put the is and hey Godsey and distribution is used to

246
00:15:07.461 --> 00:15:12.461
create a distribution of possibilities and we use the mean to define a center

247
00:15:13.371 --> 00:15:15.350
for it.
What does that central point,
right in the middle,

248
00:15:15.351 --> 00:15:17.120
that little red center points.

249
00:15:17.360 --> 00:15:22.360
And we use the covariants as a measure of how much of width of standard

250
00:15:23.361 --> 00:15:27.260
deviation,
how far off from the mean do we want to,
to stretch this data,

251
00:15:27.380 --> 00:15:31.640
this distribution of possibilities.
Okay,
how far do we want to stretch that?

252
00:15:31.850 --> 00:15:34.820
And by the way,
the word covariants,
and we're going to use it a lot,

253
00:15:35.090 --> 00:15:39.590
covariance is a measure of how changes in one variable are associated with

254
00:15:39.591 --> 00:15:42.890
changes in a second variable.
The covariants,

255
00:15:43.070 --> 00:15:47.480
how much does one variable very in relation to another variable.

256
00:15:47.600 --> 00:15:50.820
And why do we even use this,
this value?
Because we want to,

257
00:15:51.710 --> 00:15:55.760
we want to measure how these two variables are related and the variables are

258
00:15:55.761 --> 00:16:00.440
features.
What does that,
what does that connecting value between these variables?

259
00:16:00.650 --> 00:16:02.360
And we're going to use this to,

260
00:16:02.470 --> 00:16:07.250
to create our lower dimensional data.
And I'll talk about why in a second.

261
00:16:07.251 --> 00:16:09.610
Okay.
But that's what that is.
And

262
00:16:10.610 --> 00:16:11.140
<v 0>okay.</v>

263
00:16:11.140 --> 00:16:14.320
<v 1>Uh,
so that's a,
it's a gospel and distribution curve of possibilities.</v>

264
00:16:14.410 --> 00:16:15.970
So that's for our data.
Okay.

265
00:16:19.200 --> 00:16:20.460
So,
okay.

266
00:16:21.060 --> 00:16:21.320
<v 0>Yeah.</v>

267
00:16:21.320 --> 00:16:25.460
<v 1>Oh,
so it doesn't make much sense.
So,
okay,
so,
so geo we aren't,</v>

268
00:16:25.490 --> 00:16:28.810
we are just Jen,
we are generating sample data and we are general,

269
00:16:29.010 --> 00:16:31.670
we're just generating sample data and it's a bunch of,

270
00:16:31.671 --> 00:16:35.750
so this is the data as we as we printed out and it's just across a distribution

271
00:16:35.751 --> 00:16:38.600
of possibilities.
So if we were to pick any point off of this,

272
00:16:38.601 --> 00:16:43.250
they would be like 0.6 5.3 right?
So it's a three dimensional point.

273
00:16:43.420 --> 00:16:46.670
It's a triplet of numbers and it's just a set of those.

274
00:16:46.730 --> 00:16:49.760
So it kind of looks like this,
but that's all it is.
Just it's a table.

275
00:16:49.850 --> 00:16:54.680
The word Matrix is scary,
but a matrix is just a table of values.

276
00:16:54.800 --> 00:16:56.750
That's all that a matrix is.
Okay.

277
00:16:56.900 --> 00:16:59.170
What is the difference between a three dimensional matrix and tenser?

278
00:17:00.600 --> 00:17:01.433
<v 0>MMM,</v>

279
00:17:02.570 --> 00:17:05.810
<v 1>a tensor isn't an end dimensional array.</v>

280
00:17:05.870 --> 00:17:08.480
So a three dimensional matrix is a tensor.

281
00:17:08.630 --> 00:17:11.710
So attention is the most generalized form of,
you know,

282
00:17:11.720 --> 00:17:14.570
so a four dimensional majors to be attentional five dimensional matrix.
Okay,

283
00:17:14.620 --> 00:17:17.990
so let's keep on going.
So next,
that was it for our first sample.
And guess what?

284
00:17:17.991 --> 00:17:19.100
I'm going to do this twice.

285
00:17:19.310 --> 00:17:24.310
I'm going to do this twice so I can basically just copy and paste this because

286
00:17:24.381 --> 00:17:26.900
I'm going to do this twice because I'm gonna create two classes.

287
00:17:27.230 --> 00:17:30.560
I'm going to quit Q classes and so I'm going to paste this twice.

288
00:17:30.620 --> 00:17:35.480
So I'll say two.
Two what?
So this is going to be one,

289
00:17:35.481 --> 00:17:37.700
one,
one.
So there are two different,

290
00:17:37.760 --> 00:17:41.330
so there are two data sets and the moon is going to be different for this one.

291
00:17:41.331 --> 00:17:44.840
Okay,
so the starting point and then I'm going to say class two,

292
00:17:44.841 --> 00:17:49.370
sample renovar two and to,

293
00:17:50.020 --> 00:17:52.230
and why do I went through datasets?
Well,

294
00:17:53.160 --> 00:17:53.730
<v 0>okay.</v>

295
00:17:53.730 --> 00:17:57.560
<v 1>No pluck them.
So it going to,
let's see,
let's see.
Good.
Okay,</v>

296
00:17:57.561 --> 00:18:01.490
so I've got two data sets now,
right?
Okay.
Class one and class two.

297
00:18:01.760 --> 00:18:05.510
These are my two datasets and now we're going to plot them.

298
00:18:06.320 --> 00:18:10.100
So step two is to plot the data so we can look at what this looks like.
Okay?

299
00:18:10.120 --> 00:18:13.430
And it's going to be much more intuitive when we plot plot the data.

300
00:18:13.431 --> 00:18:16.340
So let's start by sliding the data.
All right,

301
00:18:16.790 --> 00:18:20.990
so we're going to import Matt thought lives so we can plug this data and we're

302
00:18:20.991 --> 00:18:22.100
going to call it PLT.

303
00:18:22.580 --> 00:18:27.140
And then we're going to define our variables.
So figure with,

304
00:18:27.240 --> 00:18:30.530
so the finger,
which is going to be,
and then we're going to use the finger side.

305
00:18:30.531 --> 00:18:33.920
There's going to give us a width and the height.
Okay,
go figure.

306
00:18:33.930 --> 00:18:36.740
It's going to be what is the width and the height.

307
00:18:36.920 --> 00:18:40.210
So figured size is eight.
Eight.

308
00:18:43.970 --> 00:18:48.290
Okay.
So then we're going to create three d subplot and these,

309
00:18:49.070 --> 00:18:50.330
so that's the finger.
Okay.

310
00:18:50.331 --> 00:18:55.331
So then we're going to say these are subplots grid parameters in coding as a

311
00:18:56.151 --> 00:18:58.880
single integer.
Okay?
So what am I talking about here?

312
00:18:58.881 --> 00:19:02.690
So once we have our finger to create a subplot and the figure is kind of a

313
00:19:02.691 --> 00:19:06.110
generalized form that we draw our data.
And so hold on a second.

314
00:19:06.440 --> 00:19:08.550
It's going to be really pretty in a second.
I'm going to print this out.

315
00:19:08.551 --> 00:19:09.384
It's gonna be awesome.

316
00:19:09.560 --> 00:19:13.830
So I'm going to say let's add this subplot and then some point has been to start

317
00:19:13.850 --> 00:19:17.480
up.
That's what I meant by that word.
What is one,
one,
one.

318
00:19:17.540 --> 00:19:21.200
These are the subplot Rick Rick parameters encoded as a single integer.

319
00:19:21.290 --> 00:19:23.540
So what this is saying is one,
one,
one it says,

320
00:19:23.541 --> 00:19:28.400
so it's a one by one grid and we'll talk about the first sub plot.

321
00:19:28.550 --> 00:19:33.080
So it could have been anything else that it's just defining the rules for our,

322
00:19:33.300 --> 00:19:37.610
our,
our,
our plot.
What,
what do we want our plot to look like?
Okay,

323
00:19:37.611 --> 00:19:42.590
so it's going to be a three d plot and we're going to say,
and you know what,

324
00:19:42.640 --> 00:19:44.960
uh,
let's see.
Yeah,
it looks,
let's keep going here.

325
00:19:44.961 --> 00:19:48.680
So then the font size is going to be first to find the font size,

326
00:19:48.950 --> 00:19:50.000
the font size.

327
00:19:50.270 --> 00:19:54.290
These are such little trivial things,

328
00:19:54.291 --> 00:19:58.730
but they're necessarily right.
We want to make sure that aren't data looks good,

329
00:19:59.060 --> 00:20:00.920
right?
Whether we're presenting it to people,

330
00:20:01.100 --> 00:20:04.100
whether we are looking at it for ourselves,

331
00:20:04.340 --> 00:20:08.780
we want our data to look good and we're going to plot their samples.
Now,

332
00:20:08.781 --> 00:20:11.270
now that we've defined the rules for our graph,

333
00:20:11.450 --> 00:20:15.800
now it's time to actually plot arc data.
So we'll say,
okay,

334
00:20:15.801 --> 00:20:17.690
so let's apply it using the plot function.

335
00:20:17.930 --> 00:20:21.250
And we're going to plot the first sample.
And

336
00:20:23.360 --> 00:20:27.370
you know what,
this,
you know,
I can just paste.
This is pardon?

337
00:20:27.380 --> 00:20:30.110
Because it's not as important as the machine learning that we're about to do.

338
00:20:30.140 --> 00:20:34.150
So let me just,
let me just,
I took that advice before some of them.
So,

339
00:20:34.151 --> 00:20:38.750
so here's what we're doing.
We're plotting both samples,
class one and class two,

340
00:20:38.780 --> 00:20:40.280
okay.
And we're going to label them.

341
00:20:40.430 --> 00:20:42.170
One is going to be blue and one is going to be green.

342
00:20:42.610 --> 00:20:44.870
And there one is gonna be a little class one and one is going to be able to

343
00:20:44.871 --> 00:20:47.860
class two.
We're going to have a legend and let's show plot.
Let's see.

344
00:20:47.980 --> 00:20:52.240
Hope this works.
Boom.
Okay,
so let's see what happened here.
It said,

345
00:20:52.540 --> 00:20:57.400
can't assign too literal.
So what that means is PLT,

346
00:20:57.401 --> 00:21:00.640
that RC parameter equals legend on font size.

347
00:21:02.110 --> 00:21:06.400
Can't assign to literal.
Okay.
Legend,

348
00:21:06.401 --> 00:21:07.750
dark font size.

349
00:21:08.410 --> 00:21:13.100
Let's see what's going on here and not what lie highlight in line.
Uh,

350
00:21:13.360 --> 00:21:16.630
let's see.
We'll just be in line by the way.

351
00:21:21.560 --> 00:21:26.390
So Act,
start,
figure,
add subplot,
projection,
3-d.
So you're size eight,

352
00:21:26.391 --> 00:21:29.010
eight PLT figure,
RC programs.

353
00:21:30.200 --> 00:21:34.610
What do we got going on here?
Marcy?
Parameter for Plt.

354
00:21:35.120 --> 00:21:37.430
Can't assign to literal.
Interesting.
So what's,

355
00:21:41.240 --> 00:21:45.740
Oh yes,
equal sign.
Yeah,
that's what it was.
Okay.

356
00:21:45.770 --> 00:21:50.480
Populate it here from Num Py.
Inmar plot lied.
Okay,
let's see what's going on.

357
00:21:52.710 --> 00:21:52.981
Anon.

358
00:21:52.981 --> 00:21:57.981
Projection three d unknown projection three d why do we have an unknown

359
00:21:58.471 --> 00:22:00.690
projection of three d?
Let's see,

360
00:22:01.290 --> 00:22:05.850
even unknown projection of three days because see what we've got here.

361
00:22:09.130 --> 00:22:13.660
I will look,
I will look,
I will look.
I am looking now.
RC parameter equals

362
00:22:16.000 --> 00:22:19.420
I do.
Look,
I am looking.
Okay.
Uh,
so let's see.

363
00:22:19.480 --> 00:22:23.410
Bigger size and then also Rosie,
arrogant,

364
00:22:24.040 --> 00:22:25.930
unknown projection for Ed.

365
00:22:32.320 --> 00:22:36.220
So you're a dot pads.
Sell time.
Let's see what's going on here

366
00:22:38.500 --> 00:22:39.333
so far.

367
00:22:42.670 --> 00:22:43.503
<v 0>Hmm.</v>

368
00:22:44.990 --> 00:22:47.200
<v 1>All right,
let's just go for this.</v>

369
00:22:49.860 --> 00:22:51.710
UHMM I know.
Projection Three d

370
00:22:54.880 --> 00:22:56.530
let's see what's going on.

371
00:22:58.690 --> 00:23:03.620
But I didn't do maps all like three d in my earlier code.
So it's interesting.

372
00:23:03.680 --> 00:23:06.920
I mean,
what I would do at this point is probably Google it and stack overflow,

373
00:23:06.921 --> 00:23:11.510
but I'm going to see what is going on.

374
00:23:11.540 --> 00:23:13.550
Let's see,
we have,

375
00:23:14.540 --> 00:23:15.373
<v 0>mmm.</v>

376
00:23:17.370 --> 00:23:22.230
<v 1>Capital d,
capital d for y.
So,
oh,</v>

377
00:23:23.280 --> 00:23:27.780
is that what it is?
Is that what it is?
No,
it's not.
No,
it's not.
Capital d,

378
00:23:28.260 --> 00:23:30.600
lowercase d.
Uh,
so,

379
00:23:34.520 --> 00:23:39.360
so,
okay,
so this is actually what,
there's a lot of noise.
No,

380
00:23:39.361 --> 00:23:42.060
that's cool.
No,
it's,
it's fine.
It's fine.
It's fine.

381
00:23:42.240 --> 00:23:46.550
So it's just gonna it's a,
it's an unknown projection three.

382
00:23:46.700 --> 00:23:49.590
Let me just show you this.
I'm going to keep going.
We don't actually,
um,

383
00:23:51.940 --> 00:23:53.350
so,
okay.
Let me just show you what this looks like.

384
00:23:54.400 --> 00:23:57.420
Let me show you what this looks like.
Here's what it looks like.
Check this out.

385
00:23:57.470 --> 00:24:01.780
Check this out.
When we do this,
this is the projection in three d.

386
00:24:01.840 --> 00:24:04.600
This is what it looks like.
We have two classes in three d.

387
00:24:04.660 --> 00:24:09.130
This is what it looks like in three d space.
Okay.
Our data is valid.

388
00:24:09.131 --> 00:24:12.010
It's just the,
it's just the tools and,

389
00:24:13.250 --> 00:24:14.083
<v 0>okay.</v>

390
00:24:14.170 --> 00:24:15.940
<v 1>That's what it looks like.
It's in three d space.</v>

391
00:24:16.090 --> 00:24:18.790
So we're going to get started with the math because that is the important part.

392
00:24:18.791 --> 00:24:21.790
This is what it looks like in three d and now we're going to actually take our

393
00:24:21.791 --> 00:24:26.080
data and we're going to look at it.
All right,
so let's,
let's keep going here.

394
00:24:27.250 --> 00:24:30.490
Okay,
so we have our data at the data is valid.
It's fine.
Okay,
let's,

395
00:24:30.491 --> 00:24:33.600
let's keep going.
Let's keep going.
So now what?

396
00:24:33.601 --> 00:24:35.410
So that was step two to now we're at step three.

397
00:24:35.411 --> 00:24:39.690
So step three is we're going to merge this data into one big data.

398
00:24:39.840 --> 00:24:44.840
So would you merge the data into one big ass?

399
00:24:45.280 --> 00:24:47.860
They said,
I had to say,
asked him that.
Okay,
so that's what we're going to do.

400
00:24:48.160 --> 00:24:50.830
So how we do it,
we're going to say all samples equals,

401
00:24:50.831 --> 00:24:52.360
and we're going to use numb pies,

402
00:24:52.570 --> 00:24:57.400
concatenate function to do this and make function.
To do this,
we're going to say,

403
00:24:57.640 --> 00:25:01.270
take our first class sample,
take our second class sample,

404
00:25:01.690 --> 00:25:04.450
and on one axis we're going to merge it.

405
00:25:04.660 --> 00:25:07.660
So it's going to be accessed before is once it's all on one access.

406
00:25:07.810 --> 00:25:09.980
What does that mean?
One access?
Well then let me,

407
00:25:10.030 --> 00:25:13.030
let me write this out and then we'll show,
we'll show,
we'll show it.

408
00:25:13.031 --> 00:25:14.390
It looks like we'll show at one x.

409
00:25:14.391 --> 00:25:16.870
This looks like this is now what our data looks like.

410
00:25:17.140 --> 00:25:20.410
It is now a three by 40 dataset.

411
00:25:21.400 --> 00:25:22.930
It's a three by 40 data set and

412
00:25:24.400 --> 00:25:27.640
the Saroj reval company is not taking investments right now.

413
00:25:27.760 --> 00:25:31.480
We are focused on our media division and I'm also focusing on,

414
00:25:31.481 --> 00:25:33.610
I'm going to start building our research division,

415
00:25:33.611 --> 00:25:37.840
which includes you guys later on,
but right now it is just media.

416
00:25:37.960 --> 00:25:41.650
We will be the best media world.
Okay.
Back to this.

417
00:25:41.740 --> 00:25:45.730
So all samples,
that's what it looks like three by 40 days said.
Okay,

418
00:25:45.731 --> 00:25:47.830
so that's our data.
So let's get an also,

419
00:25:47.860 --> 00:25:49.360
this is what the transpose looks like by the way,

420
00:25:49.361 --> 00:25:52.670
because we're going to be using this t.
Dot.
T at PN.

421
00:25:52.770 --> 00:25:54.730
We are going to be using that a lot.
What does that do?

422
00:25:54.790 --> 00:25:59.040
It gives us the transpose and what does that do?
It just flips our data.
It just,

423
00:25:59.300 --> 00:26:03.130
it flips our matrix.
Okay.
So inserting horizontal,
it'll be vertical.

424
00:26:03.340 --> 00:26:05.590
And that's just good for looking at them.
It's good for it.

425
00:26:05.591 --> 00:26:08.800
Sometimes formulas require the transpose,
which they will,

426
00:26:08.860 --> 00:26:13.570
but right now we're just doing it just to have it look aligned so we can without

427
00:26:13.571 --> 00:26:16.780
having to struck our window.
So now that was step.
What was that?

428
00:26:16.810 --> 00:26:19.870
That was step three.
Now we're on step four.
Step four,

429
00:26:21.540 --> 00:26:22.610
the research division.
Okay,

430
00:26:22.760 --> 00:26:26.600
so step four is to compute the dimensional mean vector.

431
00:26:26.870 --> 00:26:31.760
What is that?
Why are we
here?

432
00:26:31.770 --> 00:26:35.600
Why don't you compute?
Why are we competing with me?
Becker?

433
00:26:35.810 --> 00:26:39.590
It will help compute the covariance matrix.

434
00:26:39.710 --> 00:26:44.710
So a lot of this process of reduction for PCA and in general is to compute these

435
00:26:46.801 --> 00:26:48.840
little pieces.

436
00:26:48.930 --> 00:26:53.130
They're all little pieces of a puzzle and we use each piece to help us find the

437
00:26:53.131 --> 00:26:56.190
answers to the next piece.
It's a,
it's a,
it's a process of discovery.

438
00:26:56.220 --> 00:26:58.350
It's an incredible process of discovery.

439
00:26:58.560 --> 00:27:01.710
Each of these are pieces to the puzzle and eventually it's going to lead us to

440
00:27:01.711 --> 00:27:05.460
something that is amazing.
So let's find the mean for each feature.

441
00:27:05.470 --> 00:27:09.990
Now it's right.
So by itself it doesn't seem like it's that important,
right?

442
00:27:09.991 --> 00:27:14.520
But when we use,
we're going to use it to compute our co variants matrix.

443
00:27:14.910 --> 00:27:17.280
So the mean for x.
So we have three features,
right?

444
00:27:17.400 --> 00:27:19.740
We're going to use numb highs knee function to do this,

445
00:27:19.950 --> 00:27:23.610
we're going to say it's getting all of our examples and just that first row.

446
00:27:24.020 --> 00:27:24.853
And that's going to

447
00:27:27.690 --> 00:27:32.490
Joe Joe dimensionality reduction is taking a lot of data with a lot of features

448
00:27:32.491 --> 00:27:33.930
and squashing it.
They're just to,

449
00:27:33.931 --> 00:27:38.750
so instead of a hundred features we have just to,
and that's great for uh,

450
00:27:39.030 --> 00:27:42.280
a lot of things.
So we're going to keep going with this all the same.

451
00:27:42.370 --> 00:27:46.560
So I can just copy and paste this and just replace we numbers,
right?

452
00:27:46.561 --> 00:27:50.910
So many for x me for why we perceive we're taking a three dimensional data set

453
00:27:51.060 --> 00:27:55.890
and we are squashing it to a two dimensional datasets.
The covariants

454
00:27:57.990 --> 00:28:02.330
is the measure of your head.
I got to keep writing this.

455
00:28:02.510 --> 00:28:06.620
So the one,
how two variables vary in relation to each other,

456
00:28:07.460 --> 00:28:10.640
right?
And so now we're going to,
so now we have the meat.
What does the mean,

457
00:28:10.641 --> 00:28:13.970
the mean average value from all of those values.
So what is the average,

458
00:28:14.150 --> 00:28:18.140
if we add all the values together and divide by the number of values and it

459
00:28:18.141 --> 00:28:20.030
gives us the average value.

460
00:28:20.210 --> 00:28:25.210
So we want the average value or mean for each of those features.

461
00:28:27.200 --> 00:28:30.080
And I'm not picking the,
uh,

462
00:28:30.560 --> 00:28:33.530
I'm not picking which features I want men,
she,

463
00:28:34.040 --> 00:28:37.410
so it depends on the method but for,
uh,

464
00:28:37.460 --> 00:28:40.610
so for principal component analysis,

465
00:28:40.670 --> 00:28:44.300
it picks those features that it finds to be the most relevant.

466
00:28:44.301 --> 00:28:45.170
And we're going to talk about that.

467
00:28:45.171 --> 00:28:47.810
I'm going to visually show what I'm talking about in a second.

468
00:28:47.840 --> 00:28:50.840
It's going to make a lot more sense.
Stay with me guys.
Okay.
Stay with me.

469
00:28:51.140 --> 00:28:52.190
We are computing the mean right now.

470
00:28:52.191 --> 00:28:54.980
So now we're going to take this three d mean vectors.

471
00:28:55.040 --> 00:28:57.710
So we're going to create one big vector out of it.
So it's called the mean.

472
00:28:57.890 --> 00:29:01.550
That is one array.
And I'm going to print it so we can see what it looks like.

473
00:29:01.970 --> 00:29:04.610
I'm going to print one array and

474
00:29:05.660 --> 00:29:06.470
<v 2>okay,</v>

475
00:29:06.470 --> 00:29:10.720
<v 1>mean of y and then Z.
Okay.
So those are our three.</v>

476
00:29:10.810 --> 00:29:13.660
We create one vector out of this and then we can just print it.

477
00:29:13.810 --> 00:29:15.700
So print me vector.
Let's see what it looks like.

478
00:29:16.290 --> 00:29:17.123
<v 2>Okay,</v>

479
00:29:19.710 --> 00:29:21.860
<v 1>so you're in love with the sequence.
Okay.
So this is a</v>

480
00:29:22.920 --> 00:29:26.900
<v 2>Oh,
right,
there we go.
Hold on a second.</v>

481
00:29:28.680 --> 00:29:32.020
<v 1>NPRA mean hacks,
right?
Close that,
close that bracket.</v>

482
00:29:33.770 --> 00:29:38.060
It's in texts online.
Eight.
What,
what is it?
Oh,
this one.
Okay.

483
00:29:38.960 --> 00:29:39.860
Remove that.
Yep.

484
00:29:40.850 --> 00:29:44.470
This our mean Bector see these three values are,

485
00:29:45.700 --> 00:29:48.730
what do I mean by squashing it?
Okay,
so,
okay.
So

486
00:29:50.060 --> 00:29:50.640
<v 0>yeah.</v>

487
00:29:50.640 --> 00:29:51.840
<v 1>MMM,
okay.</v>

488
00:29:52.850 --> 00:29:53.140
<v 0>Okay.</v>

489
00:29:53.140 --> 00:29:57.910
<v 1>So if we had a hundred features,
if we had like if we had a Dataset of Pokemon,</v>

490
00:29:57.940 --> 00:30:00.730
okay,
let's say we had a Pokemon data sets and there are a hundred features,

491
00:30:00.820 --> 00:30:03.940
the length of it,
the fire breathing capability of it,

492
00:30:04.120 --> 00:30:08.200
the blink of its tail,
who it loves,
its gender,

493
00:30:08.260 --> 00:30:10.930
all of these features.
By squashing it,

494
00:30:10.931 --> 00:30:15.580
I mean we give this huge hundred dimensional data set of all the attributes of a

495
00:30:15.581 --> 00:30:18.910
pub.
Tomorrow we give it to you,
this algorithm like Pca.

496
00:30:19.270 --> 00:30:21.820
And what is going to do is it's going to squash it.

497
00:30:21.821 --> 00:30:24.940
And what that means is it's going to take those hundred features and find the

498
00:30:24.941 --> 00:30:27.430
two most relevant features from that data.

499
00:30:27.640 --> 00:30:31.750
But it's not the same features that exists.
There are two new features.

500
00:30:32.410 --> 00:30:36.010
There are two new features that they don't,
they don't,
they're not cool.

501
00:30:36.040 --> 00:30:38.320
They're not like a word.
They're not like,
they're not like,
oh,

502
00:30:38.321 --> 00:30:42.460
these are the two features,
tail and head size.
They're entirely new features.

503
00:30:44.680 --> 00:30:48.050
They are entirely new features that they just represent.

504
00:30:48.140 --> 00:30:50.330
And I don't talk about this in a second.
So let's keep going with this.

505
00:30:50.600 --> 00:30:55.600
So now we've computed the mean and now we're going to compute the covariance

506
00:30:55.821 --> 00:30:59.660
matrix using this meter.
So let's talk about the covariance matrix with a little,

507
00:30:59.990 --> 00:31:03.280
uh,
with a,
with a,

508
00:31:03.960 --> 00:31:05.300
with the image.
Okay,

509
00:31:05.301 --> 00:31:09.120
so I have this interactive website that I found I'm going to,

510
00:31:09.260 --> 00:31:12.530
I'm going to link to you guys so you guys can see this.
Okay.

511
00:31:12.770 --> 00:31:16.430
This is the one right now.
This is not the one.
What is the one,
let's see,

512
00:31:16.431 --> 00:31:19.340
what does that,
what does that cove or making it covariance matrix in our,

513
00:31:19.550 --> 00:31:21.140
this is a great link for it.
Okay.

514
00:31:24.290 --> 00:31:25.123
So let's see.

515
00:31:29.000 --> 00:31:29.833
<v 0>Okay,</v>

516
00:31:30.800 --> 00:31:34.370
<v 1>so this is also a great link.
Let me,
let me paste this in as little learning a,</v>

517
00:31:34.630 --> 00:31:38.330
so check out that link.
Right.
Okay.
So what are we doing right now?
Let,
let me,

518
00:31:39.520 --> 00:31:43.160
it's back to this.
We're computing the covariance matrix.

519
00:31:43.300 --> 00:31:47.210
You have a covariance matrix models their relationship between our variables and

520
00:31:47.211 --> 00:31:48.530
in our case,
those are features.

521
00:31:48.650 --> 00:31:53.650
It is the matrix of values that model it model the relationship between all the

522
00:31:54.501 --> 00:31:57.440
different features and it's all gonna come together.
What?
Remember,

523
00:31:57.441 --> 00:32:01.190
these are pieces to the puzzle.
It's all gonna come together.
It measures the,

524
00:32:02.090 --> 00:32:05.700
the relationship between all of these variables,
the word variants.

525
00:32:05.730 --> 00:32:10.610
So a variance it the degree by which a random variable changes with respect to,

526
00:32:10.611 --> 00:32:13.280
it's expected that that's variance.

527
00:32:13.490 --> 00:32:17.540
Variance is a word for describing a value in relation to itself with nothing

528
00:32:17.541 --> 00:32:21.530
else in common,
just itself.
But covariants is the degree.

529
00:32:24.050 --> 00:32:24.883
Oh,
the Saudis ones.

530
00:32:26.410 --> 00:32:29.980
So covariates it the degree by which two different random variables change with

531
00:32:29.981 --> 00:32:31.540
respect to each other.

532
00:32:32.710 --> 00:32:36.360
It measured the relationship between each features.
That's Coburn there.

533
00:32:36.520 --> 00:32:40.400
How are they both during?
So let me,
let me write this out now.
Oh,
so,
oh,

534
00:32:40.401 --> 00:32:42.020
it didn't work.
Let me,
uh,
okay.

535
00:32:42.990 --> 00:32:43.490
<v 0>Okay.</v>

536
00:32:43.490 --> 00:32:48.350
<v 1>Yeah,
let me just,
there we go.
Yep.
So let's,</v>

537
00:32:48.480 --> 00:32:50.740
let's try this again.
We're not live.

538
00:32:51.960 --> 00:32:52.270
<v 0>Okay.</v>

539
00:32:52.270 --> 00:32:57.270
<v 1>No worries.
No worries.
Antiglobulin yeah,
eventually.</v>

540
00:32:57.330 --> 00:33:00.990
Not yet.
I'm a popular night enough but works on great.
Great.
Awesome.
Okay,

541
00:33:00.991 --> 00:33:03.300
so let's,
let's do it.
So there's a calculate the covariance matrix.

542
00:33:03.301 --> 00:33:05.460
So let's calculate the covariance matrix.

543
00:33:05.461 --> 00:33:10.461
So we're gonna start off with an empty array of Zeros and herei three by three

544
00:33:10.921 --> 00:33:15.030
array of Zeros.
And we're going to use high zero song sheets it to do that.

545
00:33:15.270 --> 00:33:20.040
So we have an MPRM Zeros.
Now we're going to say,
okay,
so for each value

546
00:33:22.110 --> 00:33:26.840
in our grade,
I in range of,
all right,
great.
Um,

547
00:33:26.841 --> 00:33:28.770
all samples.
Dot shake.

548
00:33:32.390 --> 00:33:36.260
We're going to say take back covariance matrix and now we're going to add our

549
00:33:36.261 --> 00:33:38.130
values driven.
What are these values look like?

550
00:33:38.131 --> 00:33:41.570
So according to that link I sent you guys even a covariance matrix,

551
00:33:41.870 --> 00:33:45.350
it's not just the covariance,
it's the variance and the coherence.

552
00:33:45.351 --> 00:33:46.184
That's what it looks like.

553
00:33:46.310 --> 00:33:50.350
There is a specific formula as you can see here for creating that nature.

554
00:33:50.440 --> 00:33:54.260
And we are going to programmatically write down this formula.

555
00:33:54.680 --> 00:33:56.690
So it is the same set of rules.

556
00:33:56.691 --> 00:34:00.320
That formula is a set of rules that is always the same.

557
00:34:00.321 --> 00:34:03.530
So we're going to say we're going to keep continuously add value through our

558
00:34:03.531 --> 00:34:07.630
covariance matrix and we're going to say
ORC,

559
00:34:07.790 --> 00:34:12.740
all the samples starting with the ones in this row.

560
00:34:12.741 --> 00:34:16.010
And we're going to iterate through the rough start and this column we're going

561
00:34:16.011 --> 00:34:20.270
to reshape and reshape is a new shape twin,
right?
Without changing its data.

562
00:34:20.540 --> 00:34:25.440
So it just gives us a new shapes are all in the same shape minus the mean back

563
00:34:25.441 --> 00:34:26.960
then and now are you using the mean bastard?

564
00:34:26.961 --> 00:34:31.430
So we take those features and we subtract the mean vector and then we compute a

565
00:34:31.431 --> 00:34:36.230
dot product of it.
And then one more value.
And so the dot product is

566
00:34:38.020 --> 00:34:42.940
the don product is the one we take two major seas and we multiply them together.

567
00:34:43.270 --> 00:34:46.300
And why do we multiply matrices together so we can too.

568
00:34:46.510 --> 00:34:49.060
So it gives us a more generalized value.

569
00:34:49.130 --> 00:34:52.590
And is that there's actually a lot of reasons why we would most part two

570
00:34:52.660 --> 00:34:54.070
matrices together.
Uh,

571
00:34:54.130 --> 00:34:56.940
but in this case we're doing it to give us a covariance makers.

572
00:34:57.170 --> 00:35:00.160
I mean Becker is going to help us compute that.

573
00:35:01.310 --> 00:35:04.570
So let me just out all samples.

574
00:35:07.000 --> 00:35:09.330
Oh,
okay.
Hold on.
I'm sorry.

575
00:35:11.850 --> 00:35:12.683
<v 0>Yeah,</v>

576
00:35:12.770 --> 00:35:17.210
<v 1>paste this so then I can talk about it.
I mean,
great.
Okay.</v>

577
00:35:17.390 --> 00:35:21.470
This is our covariance matrix.
It is a three by three matrix.
Okay.

578
00:35:21.740 --> 00:35:25.730
We use our main vector to help us calculate this and we computer the dot product

579
00:35:25.731 --> 00:35:30.650
between every feature minus the mean teacher minus the mean vector with every

580
00:35:30.651 --> 00:35:34.080
other feature minus the mean vectors.
Trans.

581
00:35:34.640 --> 00:35:39.270
Why do we do the transposed?
That is a part of the equation.

582
00:35:39.271 --> 00:35:42.010
As you can see here,
the transpose with mentioned seed.
This tea,

583
00:35:42.500 --> 00:35:46.530
it is the same set of rules every time.
Okay,

584
00:35:47.100 --> 00:35:50.310
now here is my favorite part.
Now we're getting to my favorite part.

585
00:35:50.610 --> 00:35:53.580
Why do we compute the covariance matrix?
This is the,

586
00:35:53.730 --> 00:35:57.030
now the pieces of the puzzle are going to start coming together.

587
00:35:57.630 --> 00:36:01.260
Now it's going to start making more sense why we computed the mean backyard to

588
00:36:01.261 --> 00:36:02.700
compute the covariance Matrix,

589
00:36:02.790 --> 00:36:07.050
to Compute d eigen factors in eigen values.

590
00:36:07.500 --> 00:36:08.460
That's what we're going to do.
Now.

591
00:36:08.461 --> 00:36:12.570
We're going to compute the eigen vectors and eigen values.
Okay,

592
00:36:13.560 --> 00:36:14.790
so that's next six.

593
00:36:15.030 --> 00:36:20.030
Step six is to compute the eigen vectors and hiking dot eigen values.

594
00:36:22.770 --> 00:36:27.330
Why do we do this?
What is the point of,
I can backtrack and Heidi di and what,

595
00:36:27.360 --> 00:36:29.250
what even is that word?
Well,
first of all,

596
00:36:29.251 --> 00:36:33.480
let me just say I can vectors in [inaudible] values are used throughout all of

597
00:36:33.481 --> 00:36:37.380
engineering.
It's not about computer science is used in electrical engineering.

598
00:36:37.500 --> 00:36:41.490
It's using physics.
It's used in the Google page rank system.

599
00:36:42.180 --> 00:36:45.180
I come back to as an eigenvalues or use the route engineer and no,

600
00:36:45.210 --> 00:36:47.250
so here is what the system,
let me,

601
00:36:47.490 --> 00:36:49.860
let me give you a quick summary of the series of what this is.

602
00:36:49.861 --> 00:36:52.740
But so this is actually a great interactive link.

603
00:36:52.890 --> 00:36:56.190
So definitely click on this link.
I'm about to paste it into the chat.

604
00:36:56.430 --> 00:36:58.580
And it's also going be in the comments when people who are not watching this

605
00:36:58.581 --> 00:37:01.350
live,
check out this link.
Okay.

606
00:37:02.830 --> 00:37:07.830
So I can vectors make understanding transformations easy.

607
00:37:09.040 --> 00:37:12.130
Okay.
They are the acts axes,

608
00:37:12.190 --> 00:37:17.190
the directions along which a transformation acts by stretching or complex or or

609
00:37:17.770 --> 00:37:18.603
compressing.

610
00:37:18.730 --> 00:37:23.560
And I do values give you the factors by which this compression occurs.

611
00:37:23.920 --> 00:37:26.010
Okay.
So yeah,

612
00:37:26.200 --> 00:37:29.200
so basically there's a lot of problems that can be modeled with a linear

613
00:37:29.201 --> 00:37:33.910
transformation,
but it gives you a sense of direction.
So,
so that's a,

614
00:37:33.911 --> 00:37:34.900
that's a general description.

615
00:37:34.901 --> 00:37:39.340
So I think vectors give you a sense of direction as to where this data is going.

616
00:37:39.341 --> 00:37:42.100
So let me,
let me show you this so you can actually move this around.

617
00:37:42.101 --> 00:37:45.100
This is interactive.
I can vector is Jeremy.
Exactly.

618
00:37:45.670 --> 00:37:50.630
So see as I move this,
these sets of points.
Uh,

619
00:37:50.920 --> 00:37:54.940
so this is a,
this is,
uh,
this is a very intuitive explanation on this page,

620
00:37:54.941 --> 00:37:59.230
but essentially an eigen vector is a direction that some data is going in.

621
00:37:59.280 --> 00:38:04.000
And what do I mean by direction?
Direction in this case is like,

622
00:38:06.470 --> 00:38:08.990
so there's actually a,

623
00:38:09.300 --> 00:38:12.360
there's actually another link that,

624
00:38:12.400 --> 00:38:17.030
that that also helped me understand this.
Then I want to paste in here.
Uh,

625
00:38:20.590 --> 00:38:23.230
so hold on a second.

626
00:38:26.700 --> 00:38:27.533
<v 0>Yeah.</v>

627
00:38:27.910 --> 00:38:30.610
<v 1>So um,
hold on a second.
So principal,</v>

628
00:38:34.440 --> 00:38:35.273
I think this is

629
00:38:36.160 --> 00:38:38.050
<v 0>no,
there's another one.
So basically,</v>

630
00:38:39.760 --> 00:38:44.740
<v 1>or I think this guy had it.
Okay.
So this is a great explanation.</v>

631
00:38:44.741 --> 00:38:49.680
So,
so check at this point.
Let me show you this.
This is an amazing,
amazing,
like

632
00:38:51.610 --> 00:38:52.600
it's an amazing room.

633
00:38:53.340 --> 00:38:54.173
<v 0>Okay.</v>

634
00:38:55.280 --> 00:38:58.280
<v 1>Check out these points.
Okay.
These are bunch of different data points there,</v>

635
00:38:58.281 --> 00:39:00.590
a bunch of different data points and we could just,

636
00:39:00.710 --> 00:39:03.410
we want to find the direction of this.

637
00:39:03.430 --> 00:39:06.140
We want to find the principle components of this data.

638
00:39:06.141 --> 00:39:08.600
What is the principal component?
What do I mean by that?
Well,

639
00:39:08.601 --> 00:39:11.330
if we just draw a random line in this data,

640
00:39:11.630 --> 00:39:15.050
it's going to all the how far,
what is the variance,

641
00:39:15.260 --> 00:39:17.000
how far is this data from this line?

642
00:39:17.210 --> 00:39:21.790
And we can draw a red line between every data point and this line.
And so that,

643
00:39:21.791 --> 00:39:24.770
that's cool.
But if we draw a horizontal line,

644
00:39:25.640 --> 00:39:30.010
it shows them the,
the data is even more spread out.
And what,

645
00:39:30.050 --> 00:39:30.883
why is that good?

646
00:39:31.520 --> 00:39:35.000
That that means that this is the principal component of this data.

647
00:39:35.240 --> 00:39:38.210
This is the essence of this data.

648
00:39:38.450 --> 00:39:41.240
This is that point where if we were just given this line,

649
00:39:41.360 --> 00:39:45.200
if we were just given this line and some and some distribution,

650
00:39:45.320 --> 00:39:48.080
we could recreate the data points.
That's what I'm trying to say.

651
00:39:48.320 --> 00:39:51.560
We can then recreate the data points from this lower dimensional.

652
00:39:52.270 --> 00:39:52.570
<v 0>Okay</v>

653
00:39:52.570 --> 00:39:56.170
<v 1>idea this line and this line in this case,</v>

654
00:39:56.171 --> 00:40:00.100
because the principal component and the direction for where should this line go,

655
00:40:00.101 --> 00:40:02.740
where,
what is she going to be horizontal it should it be vertical.

656
00:40:02.800 --> 00:40:04.780
That's what the Eigen vector gives us.

657
00:40:04.960 --> 00:40:08.530
It gives us a sense of direction as to what does that point with the most

658
00:40:08.590 --> 00:40:10.000
Darien's between data points.

659
00:40:10.180 --> 00:40:15.180
And we want the most variance because that's what lets us create these new lower

660
00:40:15.491 --> 00:40:19.000
dimensional features from this date so that we can recreate the data with these

661
00:40:19.001 --> 00:40:20.860
lower dimensional features.
Okay.

662
00:40:22.640 --> 00:40:22.980
<v 0>Okay.</v>

663
00:40:22.980 --> 00:40:27.410
<v 1>So that's what that is.
So where were we?
So let's,</v>

664
00:40:27.440 --> 00:40:30.410
let's compute eigenvectors and eigenvalues.
All right.

665
00:40:34.820 --> 00:40:39.770
Values and
I think I in your doctors,

666
00:40:40.040 --> 00:40:41.930
and we're going to use numb pies,

667
00:40:42.170 --> 00:40:45.330
linear Algebra functions.

668
00:40:45.490 --> 00:40:48.830
I in sell poaching to compute them.

669
00:40:49.400 --> 00:40:53.120
Very useful function to have.
Okay?
So,

670
00:40:54.380 --> 00:40:54.990
<v 0>okay.</v>

671
00:40:54.990 --> 00:40:56.970
<v 1>Worth every value we have.</v>

672
00:40:57.390 --> 00:41:00.390
Let's print out all of those.

673
00:41:01.330 --> 00:41:01.850
<v 0>Okay.</v>

674
00:41:01.850 --> 00:41:03.950
<v 1>Um,
so that's actually gonna get it.
Then we can just print them out.</v>

675
00:41:03.951 --> 00:41:08.240
Now we can just print it out to print.
I can value and print.

676
00:41:08.470 --> 00:41:10.820
I can backwards.
So let's just see what this looks like.
Okay.

677
00:41:12.100 --> 00:41:12.730
<v 0>Okay.</v>

678
00:41:12.730 --> 00:41:14.920
<v 1>Oh,
okay.
And so let me print.</v>

679
00:41:16.730 --> 00:41:18.360
But so there's a space between them.

680
00:41:21.810 --> 00:41:24.880
Don't
say what this is.

681
00:41:27.820 --> 00:41:29.270
These are our eigen values.
An item.

682
00:41:29.440 --> 00:41:33.270
So I can vectors give us the direction and I can dial you skip.

683
00:41:33.271 --> 00:41:38.200
You give us the magnitude of that direction.
These two,
uh,

684
00:41:38.810 --> 00:41:42.950
values are intrinsically correlated.
We need both of them.
Okay.

685
00:41:43.040 --> 00:41:47.450
And so we need both of them.
So what are we going to do to,

686
00:41:47.630 --> 00:41:52.400
to combine them together?
What we are going to,
uh,
we're going to create.

687
00:41:52.780 --> 00:41:55.670
Um,
an I didn't care.

688
00:41:56.690 --> 00:41:59.690
I didn't care from both and sort that.

689
00:42:00.320 --> 00:42:04.640
So when I compare it is if we were to combine both of these items.
Okay.

690
00:42:05.060 --> 00:42:07.820
What do you mean by the direction of what did I mean by the direction of the

691
00:42:07.821 --> 00:42:09.470
data?
So,

692
00:42:10.700 --> 00:42:11.170
<v 0>okay,</v>

693
00:42:11.170 --> 00:42:12.310
<v 1>so like,</v>

694
00:42:15.450 --> 00:42:16.283
<v 0>okay,</v>

695
00:42:16.850 --> 00:42:20.130
<v 1>like what does that point where in the lowest dimensional space we could then</v>

696
00:42:20.160 --> 00:42:24.180
recreate all of those dimensions.
It has to be this,
what is the optimal,

697
00:42:24.240 --> 00:42:25.680
what is the optimal line?

698
00:42:25.681 --> 00:42:29.670
We could draw what is the optimal set of features we could have such that we can

699
00:42:29.700 --> 00:42:31.860
given some formula from these features,

700
00:42:31.861 --> 00:42:36.210
we can then recreate all of the data with the most accuracy,

701
00:42:36.990 --> 00:42:41.880
the direction of where we should go.
Negative positive of should it,
you know.

702
00:42:42.780 --> 00:42:43.110
<v 0>Okay.</v>

703
00:42:43.110 --> 00:42:45.750
<v 1>What should it say to look like?
That is what the principal components,</v>

704
00:42:45.840 --> 00:42:49.870
that was the eigen vectors and I can dial,
use,
give us and that,
that,

705
00:42:49.920 --> 00:42:53.490
that lowest Mitchell point is the principal component.

706
00:42:53.491 --> 00:42:56.490
And we're gonna we're gonna calculate those in a second.
So let's,
let's,

707
00:42:56.491 --> 00:43:01.200
let's create our ids.
Harris.
Yeah,
I can Paris or I can,

708
00:43:01.201 --> 00:43:05.150
parents are going to be refused.
Numb,
Pi's have suit

709
00:43:07.410 --> 00:43:11.500
we're can to use the absolute value
of that.

710
00:43:12.130 --> 00:43:16.720
And
the,

711
00:43:16.760 --> 00:43:20.650
actually let me just,
let me just pick this one in this one.
Can you paste it?

712
00:43:21.120 --> 00:43:24.840
So we have to,
I didn't carers.
This should be eigen values.

713
00:43:25.290 --> 00:43:29.490
Let's talk about what's happening here.
Let's see.

714
00:43:30.220 --> 00:43:34.650
So we get,
we made a list.
I can tell you I can vector two bulls,

715
00:43:35.310 --> 00:43:35.911
two bulls,
right?

716
00:43:35.911 --> 00:43:39.480
Cause there are three dimensions and we use the absolute value function to make

717
00:43:39.481 --> 00:43:42.210
them positive.
Why do we make them positive?

718
00:43:44.370 --> 00:43:47.220
Because it doesn't matter if they're negative or positive.
It's

719
00:43:48.780 --> 00:43:49.830
just making them positive.

720
00:43:49.831 --> 00:43:52.710
Makes it easier because we don't have to deal with any kind of uh

721
00:43:54.720 --> 00:43:55.560
<v 0>okay.
It</v>

722
00:43:55.570 --> 00:43:58.260
<v 1>did.
It's the same as like the idea behind the mean squared error.</v>

723
00:43:58.261 --> 00:44:02.640
The reason we square it because it doesn't,
it's just a magnitude that matters.

724
00:44:02.641 --> 00:44:06.780
It's not if it's positive or negative,
it's just the magnitude that matters.
Okay.

725
00:44:06.781 --> 00:44:10.230
So then we sort them with the sort function and then we reverse it so that it's

726
00:44:10.231 --> 00:44:13.590
going to be in descending,
in decreasing,
in descending order.

727
00:44:13.860 --> 00:44:16.950
And these are the values of the parents.

728
00:44:17.040 --> 00:44:21.420
If we were to combine both of those values together,
this is what it gives us,

729
00:44:21.720 --> 00:44:26.720
these three values or the most important values we are looking for.

730
00:44:28.320 --> 00:44:31.230
I'm going to post this code.
Yeah,
let me,
me,

731
00:44:31.380 --> 00:44:34.110
let me paste this code on pacing and yeah,
totally.
That's a great idea.
Let me,

732
00:44:34.111 --> 00:44:37.740
let me do that.
Um,
I don't get home,
um,

733
00:44:37.920 --> 00:44:41.640
and I haven't actually comment on it yet,
but I'm going to,

734
00:44:41.641 --> 00:44:44.460
so let me just paste that in and then I'm going to,
so here's,

735
00:44:44.461 --> 00:44:45.750
here's what's happening guys.
Okay.

736
00:44:45.810 --> 00:44:49.530
So let me paste this code in and then we're going to keep going.
Check this out.

737
00:44:49.650 --> 00:44:53.130
Boom.
All right,
so that's my messy code.
I haven't written,
read me.

738
00:44:53.131 --> 00:44:57.660
I will in a second.
Okay,
so those are our ID in Paris.
So now we've done that.

739
00:44:58.320 --> 00:45:02.940
So that was that,
that was step seven actually.
No,
no,
that was

740
00:45:04.620 --> 00:45:09.360
step seven.
Step seven was too,
sorry about that.
Okay,
that was step seven.

741
00:45:09.510 --> 00:45:10.860
And now we're going to do step

742
00:45:12.640 --> 00:45:17.060
eight now or would you choose the largest one?
So step eight is to,

743
00:45:22.380 --> 00:45:26.500
what would you stack?
We're going to choose the K id vectors we want.

744
00:45:26.770 --> 00:45:30.750
What do I mean by kit backwards?
So what is the dimensionality of uh,

745
00:45:35.610 --> 00:45:36.443
<v 0>yeah,</v>

746
00:45:37.140 --> 00:45:38.640
<v 1>what needs a dimensionality that we want here?</v>

747
00:45:38.641 --> 00:45:41.280
Because we're going to create a matrix and we have one more step after that.

748
00:45:41.290 --> 00:45:44.430
So there was the idea of one more step after this.
Okay?

749
00:45:46.240 --> 00:45:48.790
So we're going to create a matrix.
Uh,
and

750
00:45:51.340 --> 00:45:52.173
<v 0>yeah,</v>

751
00:45:52.420 --> 00:45:54.190
<v 1>we're going to create a matrix and</v>

752
00:45:58.070 --> 00:46:02.790
we're going to say for our,
I can pears zero one

753
00:46:05.320 --> 00:46:09.880
<v 0>we shape them,
boom,</v>

754
00:46:12.600 --> 00:46:14.850
<v 1>we're going to stack on a res horizontally.
That's what we're doing,
right?</v>

755
00:46:14.851 --> 00:46:17.400
How much stacking or a race in sequence horizontally,

756
00:46:18.180 --> 00:46:21.450
that's what appreciate it every day.
Uh,

757
00:46:21.810 --> 00:46:25.290
we're going to stack them horizontally and how are we going and why are we doing

758
00:46:25.291 --> 00:46:28.230
that?
Where are you using,
how are we doing that?
We're using the h stack function,

759
00:46:28.710 --> 00:46:31.530
okay?
Either way,
guys,

760
00:46:31.620 --> 00:46:34.410
all of this can be implemented in a single line of code.

761
00:46:34.620 --> 00:46:36.580
In a single line of code would say get the loans.

762
00:46:36.660 --> 00:46:39.690
But we aren't doing this from scratch with the Mac because we are awesome.

763
00:46:39.691 --> 00:46:43.540
And we just want to know how this works.
Okay.
It does.

764
00:46:43.541 --> 00:46:48.300
It never hurts to know at least a general idea of how this works.
Okay.

765
00:46:49.290 --> 00:46:51.250
Because we are awesome.
So now,
um,

766
00:46:55.210 --> 00:47:00.210
we're going to talk about reshape pretty one.

767
00:47:03.270 --> 00:47:06.600
Okay.
Print Matrix.
Stop you.
I bet you there,

768
00:47:06.601 --> 00:47:09.540
it's going to be an error in a second,
but that's okay because

769
00:47:13.850 --> 00:47:17.090
I in pairs I can down juice.
Let me just,

770
00:47:21.350 --> 00:47:23.840
<v 0>is that him?
Okay.</v>

771
00:47:26.610 --> 00:47:27.680
<v 1>So this is now h.</v>

772
00:47:27.840 --> 00:47:32.110
This is now a two dimensional matrix.

773
00:47:32.200 --> 00:47:34.990
It's a two dimensional matrix with three values age.

774
00:47:35.110 --> 00:47:37.420
We created this using our,

775
00:47:37.690 --> 00:47:40.960
our eigen vector eigen values two poles.

776
00:47:41.590 --> 00:47:46.060
This can absolutely be a data science interview problem.
Yes,
yes,
it can be.

777
00:47:46.600 --> 00:47:50.180
Um,
so now we're on our last step.
We're on our last step.

778
00:47:50.181 --> 00:47:55.181
So step nine is to transform our data is to transform our data using this,

779
00:47:58.070 --> 00:47:58.903
<v 2>uh,</v>

780
00:47:59.960 --> 00:48:04.960
<v 1>I can pair matrix using this eigen major who are going to transform our original</v>

781
00:48:05.170 --> 00:48:09.810
data set.
So now the pieces of the puzzle are coming together a bit.

782
00:48:10.040 --> 00:48:11.840
The piece of the enemy.
To clarify,
Mikael,

783
00:48:11.990 --> 00:48:14.510
it's not that they're going to ask you a code.
The Saul from scratch,

784
00:48:14.570 --> 00:48:16.610
they'll probably ask you to on the high level idea,

785
00:48:16.790 --> 00:48:20.030
like the mind is known to just lightning like lightning interview,

786
00:48:20.031 --> 00:48:24.710
like a hundred questions about machine learning.
Just off the Bat,
what is PCA?

787
00:48:24.820 --> 00:48:26.570
Like,
what is this?
What is this?
Okay,

788
00:48:26.571 --> 00:48:30.200
so we going to create for my data using this.

789
00:48:31.130 --> 00:48:33.800
So we're going to say take our transformed

790
00:48:38.320 --> 00:48:43.320
because transformed data and calculate the transpose of it times a doc.

791
00:48:45.500 --> 00:48:47.730
And then we do the fence post.
So it's to be the same shape.

792
00:48:47.960 --> 00:48:51.490
And so remember all samples data,
that was our original dataset.

793
00:48:51.660 --> 00:48:56.070
We are computing the dot product between our original Dataset and this new

794
00:48:56.250 --> 00:48:59.940
eigenvector.
I can pair a matrix,

795
00:49:00.010 --> 00:49:03.750
am my computing the dot products.
It's going to give us a two dimensional,

796
00:49:04.400 --> 00:49:05.233
<v 2>uh,</v>

797
00:49:05.510 --> 00:49:10.010
<v 1>two features in set up three features because then any of us two features</v>

798
00:49:10.100 --> 00:49:13.760
instead of three features.
And we can use these two features to plot the data.

799
00:49:13.761 --> 00:49:15.740
We're going to plot it in a second.
Okay.

800
00:49:17.060 --> 00:49:17.893
<v 2>Okay.</v>

801
00:49:17.920 --> 00:49:21.040
<v 1>So and then we'll print out the transformed.
That's it.</v>

802
00:49:21.220 --> 00:49:24.460
It's just one line of code.
It was just one line of code,

803
00:49:25.270 --> 00:49:29.950
one line of code.
This is our new data sample.

804
00:49:30.100 --> 00:49:33.880
We just performed principal component analysis.

805
00:49:34.120 --> 00:49:38.440
We now have two dimensional data instead of three dimensional data.

806
00:49:38.740 --> 00:49:42.100
Now instead of writing out all this code unnecessary,

807
00:49:42.101 --> 00:49:46.000
go just to print it out.
I'm just going to paste it in.
So

808
00:49:48.770 --> 00:49:51.020
this is what it looks like now.
This is our,

809
00:49:51.021 --> 00:49:56.021
now this is our two dimensional data instead of our three dimensional data for

810
00:49:56.541 --> 00:49:58.670
our classes.
Okay,

811
00:50:00.650 --> 00:50:02.270
it is now two dimensions.
Instead of three,

812
00:50:02.271 --> 00:50:05.300
we just performed principle component analysis by hand.

813
00:50:05.690 --> 00:50:10.690
Now instead of writing out the now instead of writing up tsne and

814
00:50:12.490 --> 00:50:13.540
Lda from scratch,

815
00:50:13.690 --> 00:50:16.720
let's just talk about them and compare them because that's the really important

816
00:50:16.721 --> 00:50:19.330
bit and then writing up the math for those two methods.

817
00:50:19.360 --> 00:50:23.320
Going to take quite a while.
So let's talk about a comparison.
Okay,

818
00:50:23.321 --> 00:50:27.500
so we're going to compare PCA versus he has a knee versus Lva.

819
00:50:27.880 --> 00:50:32.880
These are the three most popular dimensionality reduction techniques.

820
00:50:33.380 --> 00:50:37.330
These are three most popular PR techniques out there.
Okay,

821
00:50:39.680 --> 00:50:43.160
let's compare them.
Pca.
Done.
Okay.
So what are the pros?

822
00:50:43.161 --> 00:50:46.410
So why do we use PCA?
Pca?
Oh,

823
00:50:46.690 --> 00:50:51.170
so basically I can sum it to this.
So for the best visualizations,

824
00:50:51.560 --> 00:50:52.900
this is,
this is just something to remember.

825
00:50:52.980 --> 00:50:55.870
If you were to take something out of this session.
No,
that,

826
00:50:55.890 --> 00:51:00.410
so the best visualizations,
uh,
I'm sorry,
let me start off with this.

827
00:51:00.620 --> 00:51:05.450
The best generic dimensionality reduction method is drum roll,

828
00:51:05.660 --> 00:51:08.570
please.
Pca.
There's a lot of reasons for this.

829
00:51:08.571 --> 00:51:09.920
There's a lot of reasons for this book,

830
00:51:09.921 --> 00:51:13.400
but that's Generic dimensionality reduction method is PCA.

831
00:51:14.240 --> 00:51:17.600
But if we have supervised data,
or sorry,

832
00:51:17.660 --> 00:51:22.660
if we have a supervisor and that that is for generally for supervised only for

833
00:51:23.811 --> 00:51:26.150
unsupervised data,
which is most of our data,

834
00:51:26.360 --> 00:51:31.360
but the best method for specifically for supervised data that is data that has

835
00:51:31.371 --> 00:51:33.860
labels is drum roll.
Lda,
no,

836
00:51:33.890 --> 00:51:38.030
what is Lda Lva?
Same for was Lda.

837
00:51:38.420 --> 00:51:42.580
That'll be his,
when you're a discriminant analysis,

838
00:51:42.820 --> 00:51:44.350
linear discriminants analysis.

839
00:51:44.560 --> 00:51:48.350
Now if another apartment,
all that.

840
00:51:48.370 --> 00:51:51.460
But it's the same steps as PCA.

841
00:51:51.550 --> 00:51:55.630
So if the same as same steps as PCA,

842
00:51:55.720 --> 00:51:58.060
except for one difference,
uh,

843
00:51:58.420 --> 00:52:01.090
we compute the mean vectors for the different classes.

844
00:52:01.120 --> 00:52:02.500
Instead of take the whole dataset.

845
00:52:02.620 --> 00:52:05.950
Remember how we got a mean vector for the whole dataset inside of doing that

846
00:52:05.951 --> 00:52:09.970
will compute mean directors for each of the classes will compute mean bachelor's

847
00:52:09.971 --> 00:52:11.530
for each of the classes.

848
00:52:11.680 --> 00:52:15.610
It's the same except we compute mean vectors for each of the classes.

849
00:52:16.460 --> 00:52:16.970
<v 0>Okay.</v>

850
00:52:16.970 --> 00:52:21.970
<v 1>We confuse mean vectors for each of the classes that Lva what is the best for</v>

851
00:52:23.031 --> 00:52:25.710
visualization?
Is he s

852
00:52:29.600 --> 00:52:34.210
visualize a dataset easily.
But essentially there are three steps that tsne.

853
00:52:34.250 --> 00:52:38.420
Okay.
And there's also a great link and I'm going to send you guys in a second.

854
00:52:38.600 --> 00:52:42.650
So worried about this.
Um,
okay.
Uh,

855
00:52:43.090 --> 00:52:45.710
he had money and it is an O'Reilly page.

856
00:52:45.920 --> 00:52:50.090
This is the best explanation for Tsne that I found on the web beside me.

857
00:52:50.270 --> 00:52:54.190
So check this out.
Okay.
So

858
00:52:56.070 --> 00:52:59.160
we compute a similarity matrix between all the feature vectors.

859
00:52:59.430 --> 00:53:01.890
And then we compute a similar matrix for the map points,

860
00:53:01.891 --> 00:53:05.310
which are the projected points where we want to be,
who want,

861
00:53:05.400 --> 00:53:08.210
we have a hundred dimensional data and we want two dimensional data that are,

862
00:53:08.410 --> 00:53:09.860
those are our set of nat points.

863
00:53:10.050 --> 00:53:14.440
And basically we use grading dissent to minimize the distance between two major

864
00:53:14.441 --> 00:53:18.120
cities.
And that resulting matrix is our low dimensional data.

865
00:53:18.330 --> 00:53:20.100
That is it at the highest level possible.

866
00:53:20.250 --> 00:53:24.300
Now we need for a data visualization,
generally in general,

867
00:53:24.301 --> 00:53:27.480
it gives us more accurate data visualization than any other method.

868
00:53:28.290 --> 00:53:33.030
But in general,
remember those three reasons I gave at the beginning?
Me,
let me,

869
00:53:33.060 --> 00:53:35.160
let me,
uh,
stop screen sharing and go back to me.

870
00:53:40.590 --> 00:53:45.180
But in general,
we want to use PCA.
That is our most,

871
00:53:45.640 --> 00:53:47.370
that is our most,
uh,

872
00:53:48.000 --> 00:53:52.890
Jen generic dimensionality reduction techniques.
Okay,
so that was it.
Let's,
uh,

873
00:53:52.980 --> 00:53:56.280
let's do a ending a by many Q and a and then we're,
we're out of here.

874
00:53:58.710 --> 00:54:03.390
Hi Guys.
And by the way,
thanks for watching.
You guys are awesome.
And also,
um,

875
00:54:05.130 --> 00:54:07.860
I assume APP uses k nearest neighbor.
Hi.
It does?
Yes.

876
00:54:07.890 --> 00:54:09.090
I assume APIs and other methods.

877
00:54:09.150 --> 00:54:11.900
There's actually so many dimensionality reduction methods.

878
00:54:11.901 --> 00:54:12.990
There are so many out there,

879
00:54:13.260 --> 00:54:17.160
but what I've done is I picked the three that I think are the most important and

880
00:54:17.161 --> 00:54:20.760
the most opulent.
Okay.
I joked to morning,

881
00:54:22.090 --> 00:54:22.610
<v 0>okay,</v>

882
00:54:22.610 --> 00:54:24.740
<v 1>please increase the frequency of live sessions per week.</v>

883
00:54:25.960 --> 00:54:26.793
<v 2>MMM.</v>

884
00:54:27.090 --> 00:54:29.910
<v 1>I'm going to increase my video output in general.
Um,</v>

885
00:54:30.770 --> 00:54:33.830
that's what I'm focused on right now guys.
You're are you Udacity?

886
00:54:33.831 --> 00:54:37.520
Nanodegree is too costly,
Bro.
Yo,
I tried to get it for free,
but look,

887
00:54:37.850 --> 00:54:40.560
grading is not cheap and,
and they have the old team.
The,

888
00:54:40.561 --> 00:54:42.770
these people are awesome.
They have to pay themselves.

889
00:54:42.771 --> 00:54:46.170
They've got to feed themselves,
you know,
so not all of them use,
you know,

890
00:54:46.180 --> 00:54:49.580
what are like the crooning on Alex Net architecture.

891
00:54:51.470 --> 00:54:53.910
It's actually,
let me get back to you on that.
On the,
in the slack channel.
Okay.

892
00:54:54.470 --> 00:54:56.600
I assume APP increases the dimensionality.

893
00:55:00.460 --> 00:55:04.430
Uh,
okay.
So can we have rap?
Okay.

894
00:55:04.940 --> 00:55:08.090
We do a wrap again.
Does the art improve the speed of training?

895
00:55:08.700 --> 00:55:09.533
<v 2>Uh,</v>

896
00:55:10.220 --> 00:55:13.250
<v 1>yes.
Yes it does.
Because it's less data.
It's less data.</v>

897
00:55:13.251 --> 00:55:16.760
So then because there's less data,
it's time for a model is less.
Okay,

898
00:55:16.880 --> 00:55:21.380
where can I learn data science,
my channel,
watch all my videos,

899
00:55:21.440 --> 00:55:24.920
I've put my life into this channel and I'm just getting started guys.

900
00:55:25.040 --> 00:55:28.760
We are going to be the biggest machine learning community in the world and I'm

901
00:55:28.761 --> 00:55:32.030
talking to you.
We are the most important people in the world.

902
00:55:32.690 --> 00:55:35.180
The biggest companies are going to come out of this community.

903
00:55:35.390 --> 00:55:37.400
We are here to help each other.
We are here.

904
00:55:37.580 --> 00:55:42.200
We're here to help each other learn and grow and get better.
Okay?

905
00:55:43.430 --> 00:55:47.450
We are the future.
Okay?
And I do not accept any of you giving up,
okay?

906
00:55:47.510 --> 00:55:51.230
You ask questions in the comments,
you ask questions and it's slack channel.

907
00:55:51.320 --> 00:55:55.430
You ask questions here,
but you do not.
You do not give up.

908
00:55:55.640 --> 00:55:59.720
Not one here.
Okay.
And you,
you believe in yourself.
I believe in myself.

909
00:55:59.750 --> 00:56:04.460
I have failed so many times.
Guys,
I had failed so many times.
It's so many things.

910
00:56:04.461 --> 00:56:08.090
I had been rejected from jobs.
I have been fired before.
I have,

911
00:56:08.850 --> 00:56:09.220
<v 0>okay.</v>

912
00:56:09.220 --> 00:56:13.240
<v 1>Been so many things and yet I just kept going and that's what I want you guys to</v>

913
00:56:13.241 --> 00:56:15.400
do.
Okay.
We are here too.

914
00:56:16.360 --> 00:56:16.730
<v 0>Yeah.</v>

915
00:56:16.730 --> 00:56:20.120
<v 1>To progress our species and to end to just</v>

916
00:56:22.100 --> 00:56:24.280
make the most awesome stuff possible.

917
00:56:24.310 --> 00:56:27.550
<v 0>Okay,
we need to publish papers.
We are going to publish papers.</v>

918
00:56:27.670 --> 00:56:30.340
We're going to make startups.
We're going to do so much amazing.

919
00:56:30.430 --> 00:56:34.000
So many amazing thing.
You're just getting started.
We are just getting started.

920
00:56:34.001 --> 00:56:38.980
We are a 66,000 strong community of machine learning engineers and

921
00:56:40.370 --> 00:56:45.170
<v 1>and the machine learning sub reddit is 80,000 we're going to hit 100,000 by</v>

922
00:56:45.171 --> 00:56:46.790
April.
First smart mark my words,

923
00:56:47.060 --> 00:56:50.750
we're going to hit 100,000 what makes us the biggest machine learning community?

924
00:56:51.470 --> 00:56:51.980
Okay,

925
00:56:51.980 --> 00:56:56.840
so I'm pumped for this and I hope you guys are as well because we are just

926
00:56:56.841 --> 00:56:59.900
getting started and do not tell yourself you can't do this.

927
00:57:00.050 --> 00:57:02.660
Don't ever tell yourself that you can't do this.

928
00:57:02.661 --> 00:57:06.800
I refuse to accept that the people who actually make valuable contributions

929
00:57:07.010 --> 00:57:09.650
believe in themselves.
So step one,
step zero.

930
00:57:09.710 --> 00:57:12.140
Do you remember all the steps that I just pronounced?
Step one,
do this.

931
00:57:12.440 --> 00:57:16.760
Step zero is believe you can do it.
That's step zero.
Okay,

932
00:57:16.761 --> 00:57:19.190
so remember step zero,
how do I join your slack channel?

933
00:57:19.191 --> 00:57:21.410
I'm going to want to play it in a second.

934
00:57:22.840 --> 00:57:26.710
How do I find such excellent sources for understanding topics on the neck?
I

935
00:57:27.560 --> 00:57:28.050
<v 0>okay.</v>

936
00:57:28.050 --> 00:57:30.250
<v 1>What about,
what's a better answer than I just Google through,</v>

937
00:57:30.290 --> 00:57:33.800
I just go through all the results in Google,
like optical,
like page five.

938
00:57:34.610 --> 00:57:37.250
I basically curate data for you guys.

939
00:57:37.251 --> 00:57:40.820
So I'm looking through all the shit to find the best sources and then I talk

940
00:57:40.821 --> 00:57:45.510
about it.
Exactly.
We are going to make a difference.
I love you guys too.

941
00:57:45.511 --> 00:57:49.140
Okay,
so that's it for the five stars.
Let me do one more question.
Oh yeah.

942
00:57:49.141 --> 00:57:52.600
I said I would do a wrap around.
So someone's brought up someone's right,
uh,
um,

943
00:57:52.740 --> 00:57:57.000
uh,
were uh,
a topic.
What?
Okay guys,
so the robot coat.
Yes.

944
00:57:57.090 --> 00:58:00.330
I talked about the robot last live session and I even bought,

945
00:58:00.390 --> 00:58:01.230
I should've brought it with me.

946
00:58:01.231 --> 00:58:05.100
I bought a real robot kit on Amazon and I'm making this new video and I was

947
00:58:05.101 --> 00:58:08.720
going to,
and I had,
I had the script for it.
But the problem was that,
um,

948
00:58:09.200 --> 00:58:12.600
and this was my feedback from you dad city and they were right in this video

949
00:58:12.601 --> 00:58:14.790
that I'm going to release on Friday.
I'm going to say,

950
00:58:14.820 --> 00:58:18.510
I was saying how to build an Ark.
We know how to talk about motors.

951
00:58:18.600 --> 00:58:22.560
Had a talk about convolutional nets.
I tried to put too much into one video.

952
00:58:22.770 --> 00:58:27.150
So I'm,
I'm taking that robot and I'm not going to do it in this weekly video.

953
00:58:27.270 --> 00:58:29.460
I'm just going to talk about convolutional neural networks,

954
00:58:29.490 --> 00:58:32.730
but I'm going to make a robot it's own video in the future.
Okay.

955
00:58:32.910 --> 00:58:35.610
It's going to be,
I already bought the robot kit.
It was 90 bucks on Amazon.

956
00:58:35.760 --> 00:58:38.970
I'm committed to doing it.
It's going to be its own video.
Okay.

957
00:58:39.390 --> 00:58:44.190
So is there a wrap on tensor flow?
Okay.
Okay.

958
00:58:44.370 --> 00:58:46.620
Tentraflow Oh,
here we go.

959
00:58:50.010 --> 00:58:53.220
Here we go.
And these are ending right?
There we go.
Yeah.

960
00:58:54.380 --> 00:58:54.840
<v 3>Okay.</v>

961
00:58:54.840 --> 00:58:55.890
<v 1>What does this,
because it'd be it.
Sorry.</v>

962
00:58:58.720 --> 00:59:01.760
<v 3>Yo,
Yo,
here we go.</v>

963
00:59:02.440 --> 00:59:03.560
<v 1>I'm on tensor flow.</v>

964
00:59:04.040 --> 00:59:07.070
How long roll and you wait every day cause I'm in a community.

965
00:59:07.071 --> 00:59:11.220
So 3,600 strong machine learning engineers are do event.

966
00:59:11.540 --> 00:59:15.530
I sit back and I drink a beer and coffee and all of that stuff on me.

967
00:59:15.680 --> 00:59:18.140
I keep going man.
I'm like Zombie.

968
00:59:18.240 --> 00:59:22.280
I wake up everyday and do machine learning and then I go to sleep because I'm

969
00:59:22.490 --> 00:59:26.540
fucking journey to do something else.
Not really.
I just do what I want man.

970
00:59:26.690 --> 00:59:30.620
Cause I'm really pretty and really it doesn't even matter what I say because I

971
00:59:30.621 --> 00:59:32.840
know all of you are going to go out and do something.
Okay,

972
00:59:34.160 --> 00:59:38.960
I'm going to end it now going back down town and I'll take a bow.

973
00:59:38.990 --> 00:59:42.230
Okay,
so that was it.
So all right guys,
thanks for showing up in his life session.

974
00:59:42.820 --> 00:59:43.280
Um,

975
00:59:43.280 --> 00:59:48.050
and I love you guys and I'm going to post all the links within the hour.

976
00:59:48.051 --> 00:59:48.884
Like always.

977
00:59:49.010 --> 00:59:53.180
I love you guys and definitely watch the tentraflow live that summit as well

978
00:59:53.181 --> 00:59:57.800
later on in the day or right now.
I know I'm,
I'm going to,
so for now,

979
00:59:58.040 --> 01:00:03.040
I've got to go edit this video and think about this other video that I want to

980
01:00:03.921 --> 01:00:07.220
make on the weekend.
So thanks for watching

981
01:00:08.840 --> 01:00:13.590
in a microbe.
Where's Mike?
What do you have in mind?
Right,
whatever.
See you.

982
01:00:13.600 --> 01:00:13.870
Bye.

