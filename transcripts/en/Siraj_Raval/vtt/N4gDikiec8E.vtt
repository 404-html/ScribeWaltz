WEBVTT

1
00:00:04.950 --> 00:00:09.390
You.
Okay.

2
00:00:11.950 --> 00:00:12.783
<v 1>Deep learning,</v>

3
00:00:13.450 --> 00:00:18.370
<v 2>totally.
Hello world.
Learn about the math needed to do deep learning.</v>

4
00:00:18.680 --> 00:00:22.450
Math is in everything,
not just every field of engineering and science.

5
00:00:22.720 --> 00:00:26.500
It's between every note in a piece of music and hidden in the textures of a

6
00:00:26.501 --> 00:00:28.750
painting.
The painting is no different.

7
00:00:28.960 --> 00:00:32.980
Math helps us define rules for our neural network so that we can learn from our

8
00:00:32.981 --> 00:00:34.300
data.
If you wanted to,

9
00:00:34.301 --> 00:00:37.280
you can use the pointing without ever knowing anything about math.

10
00:00:37.510 --> 00:00:41.590
There are a bunch of readily available API Apis for tasks like computer vision

11
00:00:41.591 --> 00:00:42.760
and language translation,

12
00:00:43.000 --> 00:00:47.440
but if you want to use a library like tensorflow to make a custom model to solve

13
00:00:47.441 --> 00:00:52.030
a problem,
knowing what math terms mean when you see them pop up is helpful.

14
00:00:52.240 --> 00:00:55.660
If you want to advance the field through research,
don't even trip.

15
00:00:55.840 --> 00:00:57.720
You definitely need to know the Max.

16
00:00:57.820 --> 00:01:02.290
The pointing mainly pulls from three branches of math,
linear Algebra,
statistics,

17
00:01:02.380 --> 00:01:04.750
and calculus.
If you don't know any of these topics,

18
00:01:04.780 --> 00:01:08.320
I'd recommend a cheat sheet of the important concepts and I've linked to one for

19
00:01:08.350 --> 00:01:09.610
each in the description.

20
00:01:09.640 --> 00:01:13.450
So let's go over the four step process of building a deep learning pipeline and

21
00:01:13.451 --> 00:01:15.460
talk about how math is used at each step.

22
00:01:15.550 --> 00:01:18.820
Once we'd got a data set that we want to use,
we want to process it.

23
00:01:18.821 --> 00:01:22.810
We can clean the data of empty values,
remove features that are not necessary,

24
00:01:22.990 --> 00:01:24.790
but these steps don't require math.

25
00:01:25.270 --> 00:01:28.000
A step that does though is called normalization.

26
00:01:28.150 --> 00:01:31.600
This is an optional step that can help our model reach convergence,

27
00:01:31.840 --> 00:01:35.350
which is that point when our prediction gives us the lowest error possible

28
00:01:35.530 --> 00:01:38.440
faster.
Since all the values operate on the same scale,

29
00:01:38.470 --> 00:01:40.420
this idea comes from statistics.

30
00:01:40.650 --> 00:01:43.490
<v 1>You have a 17.4% chance of making it straight.</v>

31
00:01:46.550 --> 00:01:48.770
<v 2>They're settled strategies to normalize data,</v>

32
00:01:48.800 --> 00:01:53.150
although a popular one is called mid next scaling.
If we have some given data,

33
00:01:53.210 --> 00:01:55.790
we can use the following equation to normalize it.

34
00:01:55.920 --> 00:01:59.100
We take each value in the list and subtract the minimum value from it.

35
00:01:59.370 --> 00:02:02.250
Then divide that result by the maximum value minus the min value.

36
00:02:02.610 --> 00:02:05.550
We then have a new list of data within the range of zero to one,

37
00:02:05.820 --> 00:02:09.060
and we do this for every feature we have,
so they're all on the same scale.

38
00:02:09.390 --> 00:02:10.620
After normalizing our data,

39
00:02:10.621 --> 00:02:14.040
we have to ensure that it's in a format that our neural network will accept.

40
00:02:14.460 --> 00:02:16.350
This is for linear Algebra comes in.

41
00:02:16.510 --> 00:02:20.940
There are four terms in linear Algebra that show up consistently scalers
vectors,

42
00:02:20.970 --> 00:02:24.690
matrices,
and tensors.
A scalar is just a single number.

43
00:02:24.840 --> 00:02:27.060
A vector is a one dimensional array of numbers.

44
00:02:27.450 --> 00:02:31.520
A matrix is a two dimensional array of numbers and a tensor is an end

45
00:02:31.530 --> 00:02:35.820
dimensional array of numbers,
so a matrix scalar,
a vector and specter.
Wait,

46
00:02:35.821 --> 00:02:40.580
not specter can all be represented as a tenser want to convert our data,

47
00:02:40.581 --> 00:02:43.220
whatever form it's in,
B that images words,

48
00:02:43.221 --> 00:02:48.221
videos into tensors where n is the number of features our data has and defines

49
00:02:48.381 --> 00:02:49.910
the dimensionality of our tensor.

50
00:02:50.200 --> 00:02:54.290
Let's do the three layer feed forward neural network capable of predicting a

51
00:02:54.291 --> 00:02:55.400
binary output.

52
00:02:55.401 --> 00:02:59.670
Given an input as our base example to illustrate some more math going forward.

53
00:03:00.220 --> 00:03:02.320
<v 3>When do we use math and deep learning?</v>

54
00:03:02.321 --> 00:03:07.300
When we normalize during processing learning model parameters by searching and

55
00:03:07.301 --> 00:03:11.710
random weights be initializing tensors flow from input to output,

56
00:03:11.711 --> 00:03:14.110
then measure the era to measure the doubt.

57
00:03:14.230 --> 00:03:16.630
It gives us what's real and what's expected.

58
00:03:16.660 --> 00:03:18.870
That Cup of gate to get costs corrected.

59
00:03:19.110 --> 00:03:23.130
<v 2>Import our only dependency num Pi and initialize our input data and help with</v>

60
00:03:23.131 --> 00:03:26.160
data as matrices.
Once our data is in the right format,

61
00:03:26.161 --> 00:03:28.200
we want to build our deep neural networks.

62
00:03:28.560 --> 00:03:31.080
Deep Nets have what are called hyper parameters.

63
00:03:31.081 --> 00:03:35.670
These are the high level 20 knobs of the network that we define and to help

64
00:03:35.671 --> 00:03:39.330
decide things like how fast our model runs,
how many neurons per layer,

65
00:03:39.510 --> 00:03:43.200
how many hidden layers.
Basically,
the more complex your neural network gets,

66
00:03:43.470 --> 00:03:45.360
the more hyper parameters you'll have.

67
00:03:45.810 --> 00:03:49.260
You can tune these manually using knowledge you have about the problem you're

68
00:03:49.261 --> 00:03:53.700
solving to guests,
probable values and observe the results based on the result.

69
00:03:53.760 --> 00:03:57.060
You can tweak them accordingly and repeat that process iteratively,

70
00:03:57.390 --> 00:04:00.570
but another strategy you could use is random search.

71
00:04:00.690 --> 00:04:02.610
You can identify ranges for each.

72
00:04:02.880 --> 00:04:06.960
Then you can create a search algorithm that picks values from those ranges at

73
00:04:06.961 --> 00:04:10.290
random from a uniform distribution of possibilities,

74
00:04:10.530 --> 00:04:14.730
which means all possible values have the same probability of being chosen.

75
00:04:15.030 --> 00:04:19.140
This process repeats until it finds the optimal hyper parameters.
Yay.

76
00:04:19.141 --> 00:04:22.500
For statistics,
we only have number of [inaudible] as our hyper parameters.

77
00:04:22.501 --> 00:04:24.540
Since we have a very simple neural network,

78
00:04:24.720 --> 00:04:29.280
we use probability to decide our weight values to one common method is randomly

79
00:04:29.281 --> 00:04:33.450
initializing samples of each weight from a normal distribution with a low

80
00:04:33.451 --> 00:04:36.030
deviation,
meaning values are pretty close together.

81
00:04:36.420 --> 00:04:39.840
We'll use it to create a weight matrix with a dimension of three by four since

82
00:04:39.841 --> 00:04:41.040
that's the size of our input,

83
00:04:41.310 --> 00:04:45.120
so every note in the input layer is connected to every node in the next layer.

84
00:04:45.240 --> 00:04:47.640
The Wade values will be in the range of negative one to one.

85
00:04:47.970 --> 00:04:50.910
Since we have three layers,
we'll initialize to weight matrices.

86
00:04:51.090 --> 00:04:53.370
The next set of weights has a dimension four by one,

87
00:04:53.371 --> 00:04:56.880
which is the size of our output.
As data propagates forward in a neural network.

88
00:04:56.910 --> 00:05:00.120
Each layer applies its own respective operation to it,

89
00:05:00.300 --> 00:05:04.050
transforming it in some way until it eventually.
How puts a prediction.

90
00:05:04.230 --> 00:05:06.810
This is all linear Algebra.
It's all tensor Matt.

91
00:05:07.120 --> 00:05:10.530
We'll initialize a for loop to train our network 60,000 iterations.

92
00:05:10.920 --> 00:05:13.620
Then we'll want to initialize our layers.
The first layer,

93
00:05:13.621 --> 00:05:15.240
our input gets our input data.

94
00:05:15.450 --> 00:05:18.780
The next layer computes the dot product of the first layer and the first weight

95
00:05:18.781 --> 00:05:21.390
matrix.
When we multiply two major cs together,

96
00:05:21.540 --> 00:05:23.730
like in the case of applying wait values to input data,

97
00:05:24.000 --> 00:05:25.650
we call that the dot product.

98
00:05:25.950 --> 00:05:29.760
Then it applies a nonlinearity to the result which we've decided is going to be

99
00:05:29.761 --> 00:05:30.540
a sigmoid.

100
00:05:30.540 --> 00:05:34.410
It takes a real value number and squashes it into a range between zero and one.

101
00:05:34.590 --> 00:05:38.040
So that's the operation that occurs in layer one and the same occurs in the next

102
00:05:38.041 --> 00:05:42.510
layer will take that value from layer one and propagated forward to layer to

103
00:05:42.690 --> 00:05:45.810
computing the dot product of it and the next weight matrix.

104
00:05:45.990 --> 00:05:50.610
Then squashing it into output probabilities with our nonlinearity cause we only

105
00:05:50.611 --> 00:05:53.550
have three layers.
This output value is our prediction.

106
00:05:53.880 --> 00:05:55.500
The way we improve this prediction,

107
00:05:55.501 --> 00:05:58.970
the way our network learns is by optimizing our network over time.

108
00:05:59.270 --> 00:06:02.810
So how do we optimize it?
Enter Calculus.
The first prediction,

109
00:06:02.811 --> 00:06:05.360
our model mates will be inaccurate.
To improve it,

110
00:06:05.510 --> 00:06:08.990
we first need to quantify exactly how wrong our prediction is.

111
00:06:09.050 --> 00:06:11.870
We'll do this by measuring the error or cost.

112
00:06:12.050 --> 00:06:16.940
The error specifies how far off the predicted output is from the expected
output.

113
00:06:17.210 --> 00:06:19.010
Once we have the error value,

114
00:06:19.130 --> 00:06:22.220
we want to minimize it because the smaller the error,

115
00:06:22.370 --> 00:06:26.870
the better our prediction training a neural network means minimizing the error

116
00:06:26.871 --> 00:06:29.420
over time.
We don't want to change our input data,

117
00:06:29.450 --> 00:06:32.840
but we can change our weights to help us minimize this error.

118
00:06:33.170 --> 00:06:36.500
If we just brute forced all the possible waits to see what gave us the most

119
00:06:36.501 --> 00:06:40.670
accurate prediction,
it would take a very long time to compute.
Instead,

120
00:06:40.671 --> 00:06:44.600
we want some sense of direction for how we can update our weights such that in

121
00:06:44.601 --> 00:06:49.220
the next round of training,
our output is more accurate to get this direction.

122
00:06:49.221 --> 00:06:53.240
We'll want to calculate the gradient of our error with respect to our week

123
00:06:53.241 --> 00:06:56.730
values.
We can calculate this by using what's called deep derivatives.

124
00:06:56.750 --> 00:07:00.650
In Calculus,
when we set direct to true for our nonlinear function,

125
00:07:00.830 --> 00:07:03.020
it'll calculate the derivative of a sigmoid.

126
00:07:03.230 --> 00:07:05.510
That means the slope of a sigmoid hadn't given point,

127
00:07:05.690 --> 00:07:09.530
which is the prediction values we give it from l two we went to minimize our

128
00:07:09.531 --> 00:07:13.640
error as much as possible and we can intuitively think of this process as

129
00:07:13.641 --> 00:07:17.540
dropping a ball into a bowl where the smallest air value is at the bottom of the

130
00:07:17.541 --> 00:07:19.460
bowl.
Once we dropped the ball in,

131
00:07:19.461 --> 00:07:23.420
we'll calculate the gradient at each of those positions and if the gradient is

132
00:07:23.421 --> 00:07:26.240
negative we'll move the ball to the right.
If it's positive,

133
00:07:26.270 --> 00:07:29.420
we'll move the ball to the left and we're using the gradient to update our

134
00:07:29.421 --> 00:07:30.950
weights accordingly.
Each time.

135
00:07:31.310 --> 00:07:34.970
We'll keep repeating this process until eventually the gradient is zero,

136
00:07:34.971 --> 00:07:37.100
which will give us the smallest error value.

137
00:07:37.460 --> 00:07:41.780
This process is called gradient descent because we are descending our gradient

138
00:07:41.930 --> 00:07:44.960
to approach zero and using it to update our weight values.

139
00:07:44.990 --> 00:07:49.370
Irritably I understand everything now still understand everything,

140
00:07:49.610 --> 00:07:53.600
so to do this programmatically will multiply the derivative we calculated for

141
00:07:53.601 --> 00:07:57.620
our prediction by the error.
This gives us our error weighted derivative,

142
00:07:57.621 --> 00:08:00.890
which we'll call l two delta.
This is a matrix of values,

143
00:08:00.891 --> 00:08:05.690
one for each predicted output and gives us a direction later uses direction to

144
00:08:05.720 --> 00:08:07.970
update this layers associated week values.

145
00:08:08.300 --> 00:08:12.140
This process of calculating the error had a given layer and using it to help

146
00:08:12.141 --> 00:08:15.290
calculate the error weighted gradient so that we can update our weights in the

147
00:08:15.291 --> 00:08:16.220
right direction.

148
00:08:16.460 --> 00:08:20.630
We'll be done recursively for every layer starting from the last back to the

149
00:08:20.631 --> 00:08:21.170
first,

150
00:08:21.170 --> 00:08:25.100
we are propagating our air backwards after we've completed our prediction by

151
00:08:25.101 --> 00:08:27.950
propagating forward.
This is called back propagation,

152
00:08:28.130 --> 00:08:32.240
so we'll multiply the l two delta values by the transpose of its associated

153
00:08:32.241 --> 00:08:34.430
weight matrix to get the previous layers air.

154
00:08:34.550 --> 00:08:38.510
Then use that error to do the same operation as before to get direction values

155
00:08:38.511 --> 00:08:42.560
to update the associated layers,
weights,
so air is minimized.
Lastly,

156
00:08:42.561 --> 00:08:46.550
we'll update the weight matrices for each associated layer by multiplying them

157
00:08:46.551 --> 00:08:49.040
by their respective delta.
When we run our code,

158
00:08:49.041 --> 00:08:53.270
we can see that the air values decreased over time and our prediction eventually

159
00:08:53.271 --> 00:08:54.590
became very accurate.

160
00:08:54.740 --> 00:08:58.020
Sort of it down deep learning borrows from three branches of math,

161
00:08:58.230 --> 00:09:00.930
linear Algebra,
statistics and calculus.

162
00:09:01.140 --> 00:09:04.950
A neural net performs a series of operations on an input tensor to compute a

163
00:09:04.951 --> 00:09:09.420
prediction and we can optimize our prediction by using gradient descent to back

164
00:09:09.421 --> 00:09:10.260
propagate our errors.

165
00:09:10.260 --> 00:09:13.800
Recursively updating our weight values for every layer during training.

166
00:09:14.100 --> 00:09:17.770
The coding challenge winner from the last video is Jovie on Lynn Jovian and

167
00:09:17.780 --> 00:09:20.730
tried out a bunch of different models to predict sentiment from a Dataset of

168
00:09:20.731 --> 00:09:24.570
video game reviews wizard of the week.
And the runner up is Michelle Batu.

169
00:09:24.720 --> 00:09:27.900
He tested out several different recurrent nets and eloquently recorded his

170
00:09:27.901 --> 00:09:31.770
experiments and he's read me the coding challenge for this video is to train a

171
00:09:31.771 --> 00:09:36.240
deep neural net to predict the magnitude of an earthquake and use a strategy to

172
00:09:36.241 --> 00:09:38.340
learn the optimal hyper parameters.

173
00:09:38.600 --> 00:09:41.190
And he tells her in the read me post who can help link in the comments and I'll

174
00:09:41.191 --> 00:09:43.470
announce the winner.
Next video,
please subscribe.

175
00:09:43.471 --> 00:09:46.260
If you want to see more videos like this,
check out this related video.

176
00:09:46.470 --> 00:09:49.170
And for now,
I've got to get my math turns up to a million,

177
00:09:49.320 --> 00:09:50.430
so thanks for watching.

