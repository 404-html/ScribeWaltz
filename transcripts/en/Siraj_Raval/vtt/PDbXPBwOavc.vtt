WEBVTT

1
00:00:05.570 --> 00:00:06.403
Oh,

2
00:00:14.660 --> 00:00:18.300
<v 1>all right.
Hello world.
It's the Raj,
welcome to this session,</v>

3
00:00:18.301 --> 00:00:22.740
this live session today we're going through,
as you can see on the screen here,

4
00:00:23.010 --> 00:00:24.090
let me make it a little bigger.

5
00:00:24.120 --> 00:00:28.470
We're going to defeat the game of Pong using policy gradients.

6
00:00:28.860 --> 00:00:32.040
Okay.
So I know this is the world's smallest pong game,

7
00:00:32.100 --> 00:00:34.590
but just,
uh,

8
00:00:34.680 --> 00:00:37.710
so actually I said I set the size programmatically so I can't just like switch

9
00:00:37.711 --> 00:00:39.450
it right here in real time.

10
00:00:39.451 --> 00:00:43.440
But the point is that it's pong and we're going to use a technique called policy

11
00:00:43.470 --> 00:00:46.890
gradients to solve this.
So policy grades is really cool and I,

12
00:00:46.891 --> 00:00:50.550
I really love this idea and it's one of those ideas that has excited me because

13
00:00:50.551 --> 00:00:53.610
I've been studying this stuff like every single day for the past couple of

14
00:00:53.610 --> 00:00:56.820
months.
And,
you know,
things start to get,
start to get repetitive after a while,

15
00:00:56.850 --> 00:01:01.520
right?
We're,
we're taking the gradient of some,
uh,
of,
of,

16
00:01:01.521 --> 00:01:04.680
of function and then back propagating it consistently.
You know,

17
00:01:04.681 --> 00:01:08.900
most of the advances in deep learning is because of supervised learning where

18
00:01:08.901 --> 00:01:12.390
we're able to what's called differentiate,
um,

19
00:01:12.450 --> 00:01:14.580
a computation graph or a neural network.

20
00:01:14.581 --> 00:01:17.430
And that's all of the major advances have happened because of that.

21
00:01:17.820 --> 00:01:21.960
But policy gradients are really cool because they,

22
00:01:22.260 --> 00:01:25.770
they help us back propagate when nodes are stochastic.

23
00:01:25.890 --> 00:01:29.280
So I'm going to talk about these terms in a second and these are good terms to

24
00:01:29.281 --> 00:01:33.060
know.
Okay.
So it's the castic versus back propagate.
Deterministic.

25
00:01:33.330 --> 00:01:37.080
That's my own voice.
Hi everybody.
Good to see everybody.
Okay,

26
00:01:37.081 --> 00:01:42.081
so now what we're going to do is we're going to write out the code for this and

27
00:01:43.141 --> 00:01:46.980
it's going to be about 130,
140 lines of code in Python.

28
00:01:46.981 --> 00:01:50.430
I'm going to explain it as we go.
Uh,
this is a demo of it running already.

29
00:01:50.431 --> 00:01:51.030
So the PR,

30
00:01:51.030 --> 00:01:56.030
so it's going to take about five to six hours to train if you run it on the

31
00:01:57.541 --> 00:02:01.140
cloud,
on a GPU cluster.
If I were to run this on my Mac book,

32
00:02:01.141 --> 00:02:05.610
it would take probably like three to four nights of full time running.

33
00:02:05.970 --> 00:02:09.900
So that's not what it's for.
It's it's,
it's,
it's not to run locally.
Okay.

34
00:02:10.170 --> 00:02:13.830
But when it's good,
when it gets to the point that it is amazing,

35
00:02:13.831 --> 00:02:15.570
that's when things get really interesting.

36
00:02:15.571 --> 00:02:18.630
It's going to be better than any other type of AI you can think about,

37
00:02:18.631 --> 00:02:23.130
which is amazing because it's not looking at the size of the paddle.

38
00:02:23.131 --> 00:02:24.840
It's not looking at the size of the ball.

39
00:02:24.841 --> 00:02:27.680
It's not looking at the velocity of the ball.
It's not,

40
00:02:27.990 --> 00:02:30.780
we're not feeding him these hard coded rules.

41
00:02:31.140 --> 00:02:34.650
It's learning from sheer pixels alone,
which is amazing.

42
00:02:34.860 --> 00:02:36.150
Just sheer pixels alone.

43
00:02:36.151 --> 00:02:40.440
It's able to do this and it's also better than a deep Q,
which was,

44
00:02:40.441 --> 00:02:44.760
Google's a deep Q was Google's,
I'm sorry.

45
00:02:44.761 --> 00:02:49.761
It was deep minds algorithm that Google bought deep mind for like,

46
00:02:49.800 --> 00:02:54.800
this is what impressed Google enough to pay several hundred million dollars for

47
00:02:55.260 --> 00:02:58.320
deep mind.
The acquisition.
This algorithm right here,

48
00:02:58.590 --> 00:03:03.590
it used a convolutional network to read in the pixels from a game and then it it

49
00:03:04.840 --> 00:03:09.310
output actions that then the agent implemented in the environment,

50
00:03:09.311 --> 00:03:12.580
which was any Atari game,
which was incredible.

51
00:03:12.610 --> 00:03:15.010
There were no hard coded rules for any Atari game.

52
00:03:18.690 --> 00:03:19.960
<v 2>Let's just get 30 seconds.</v>

53
00:03:20.690 --> 00:03:22.580
<v 1>Okay.
Let me know when we're ready to go.</v>

54
00:03:26.810 --> 00:03:30.770
We're back.
Okay.
All right guys,
thanks for waiting.
So,
uh,
there's a question.

55
00:03:30.771 --> 00:03:34.040
I'm going to answer two questions before we get started cause there's so much to

56
00:03:34.041 --> 00:03:36.010
go over and I'm actually,
I'm very excited to,

57
00:03:36.011 --> 00:03:39.170
to start coding this cause this is something that I find particularly

58
00:03:39.171 --> 00:03:42.560
interesting.
The first question comes from Lucas who said,

59
00:03:42.561 --> 00:03:45.020
how do I get started with tensorflow or neural networks?

60
00:03:45.170 --> 00:03:48.350
I find this topic very interesting but I don't know how I can get into it.

61
00:03:48.351 --> 00:03:52.880
Can anyone tell me?
That's a great question.
Uh,
so my answer is very simple.

62
00:03:52.881 --> 00:03:55.760
It's very short.
It is my videos.

63
00:03:56.030 --> 00:04:00.260
I have been making videos on tensorflow and neural networks for the past year

64
00:04:00.530 --> 00:04:02.060
and I make a video every single week.

65
00:04:02.330 --> 00:04:06.860
The amount of writing that goes into my radio videos is massive.

66
00:04:07.510 --> 00:04:11.150
I have covered almost every topic intenser flow in beef bits and pieces

67
00:04:11.151 --> 00:04:13.520
throughout my videos.
And in fact,
I have a playlist on tensorflow.

68
00:04:13.640 --> 00:04:15.620
I also have a separate playlist on neural networks.

69
00:04:15.650 --> 00:04:17.300
On my youtube channel to check out my channel.

70
00:04:17.301 --> 00:04:19.400
I've got two playlists for both of them.
Uh,

71
00:04:19.401 --> 00:04:22.790
and as and this intro to deep learning course as well.
So my channel,

72
00:04:22.791 --> 00:04:25.910
if you want to supplement it with some kind of written documentation,

73
00:04:26.090 --> 00:04:30.130
I would recommend Chris Olah is blog.
That guy is amazing.
Also distill,

74
00:04:30.320 --> 00:04:33.530
which is a new machine wedding publication that open AI and a bunch of other

75
00:04:33.531 --> 00:04:34.281
people are working on.

76
00:04:34.281 --> 00:04:37.550
That makes it really easy to look at machine learning papers with visual

77
00:04:37.551 --> 00:04:40.700
documentation.
Okay.
So one more question and then we're going to get started.

78
00:04:41.300 --> 00:04:45.770
So Neil ass,
when you have,
when do you have to test your classifier?
Hold on.

79
00:04:45.980 --> 00:04:47.630
So when you,

80
00:04:48.740 --> 00:04:52.340
when you test your classifier it,
I'm paraphrasing,

81
00:04:52.730 --> 00:04:55.320
is it a good idea to use synthetic data sets?

82
00:04:55.340 --> 00:04:58.160
Can you say something about synthetic datasets?

83
00:04:58.940 --> 00:05:00.350
So synthetic datasets,

84
00:05:00.410 --> 00:05:04.790
I assume you mean g data sets that are created by humans just for testing and

85
00:05:04.791 --> 00:05:08.030
playing around are good for testing and playing around.

86
00:05:08.031 --> 00:05:12.650
You wouldn't want to use a classifier in production that millions of people are

87
00:05:12.651 --> 00:05:15.910
using or even hundreds of thousands that has only been trade on train,

88
00:05:15.920 --> 00:05:20.180
on synthetic datasets.
You want it to be trained on real world data,
right?

89
00:05:20.181 --> 00:05:24.380
Because you're at,
you're applying it to a real world use case.
Uh,
yeah.

90
00:05:24.470 --> 00:05:26.900
So storage,
simple answer to that.
Let's get started.

91
00:05:26.901 --> 00:05:29.540
We have a lot of things to do.
Okay.
So let's get started.

92
00:05:30.770 --> 00:05:35.150
So the first thing we want to do is stop this because we're about to start

93
00:05:35.151 --> 00:05:40.130
coding and let's do this.
So here's the,
here's the fun part.

94
00:05:40.131 --> 00:05:44.000
I'm not using tensorflow.
There's no tensor flow that's happening right now.

95
00:05:44.150 --> 00:05:48.680
I'm using Matrix,
I'm using num Pi,
which is just good for matrix math.

96
00:05:48.860 --> 00:05:53.540
That means we're going to do forward propagation,
backward propagation by hand,

97
00:05:53.570 --> 00:05:57.470
and differentiation.
Slash.
Backpropagation.
We're doing it all hand.

98
00:05:57.471 --> 00:06:00.680
So get ready for this.
So the first part is non Pi.
Okay?

99
00:06:00.681 --> 00:06:02.690
So that's going to do all of our matrix math.
Okay.

100
00:06:02.900 --> 00:06:05.950
We're gonna use this baby a lot.
And then we've got pickles and pickles.

101
00:06:05.960 --> 00:06:07.650
Great for,
um,

102
00:06:07.910 --> 00:06:12.470
eating also to import into our libraries.
It's good.
It's great for that too,

103
00:06:12.680 --> 00:06:15.350
to serialize our data so that we can use the,

104
00:06:15.351 --> 00:06:20.000
save our model and then load it up later.
Um,
okay,
so save load model.

105
00:06:20.690 --> 00:06:23.780
And then our last library is Jim.

106
00:06:25.980 --> 00:06:29.670
So Jim is great for running game environments.

107
00:06:29.760 --> 00:06:34.200
So let me say a little bit about money.
I released Jim a couple,
you know,

108
00:06:34.290 --> 00:06:36.270
a while ago,
a couple months ago.

109
00:06:36.630 --> 00:06:40.290
And Jim was great because it allows you to train agents,

110
00:06:40.320 --> 00:06:44.970
reinforcement learning agents in game worlds in a whole bunch of different game

111
00:06:44.971 --> 00:06:48.560
worlds.
And you can specify which game world it really easily.
Uh,

112
00:06:48.600 --> 00:06:50.490
these are just a bunch of colored boxes.

113
00:06:50.670 --> 00:06:53.580
So let me wait for that to load and let me,
and then what,

114
00:06:53.581 --> 00:06:55.980
what opening I did is they released another,

115
00:06:56.150 --> 00:06:58.620
a repository called universe.

116
00:06:58.800 --> 00:07:03.800
So university builds on gym with an add way more environments.

117
00:07:04.020 --> 00:07:08.760
So Jim and universe are both compatible.
It's not like universe replaced gym.

118
00:07:08.940 --> 00:07:10.050
They're both compatible.

119
00:07:10.230 --> 00:07:14.910
So a lot of the times you don't actually need to use all of university.

120
00:07:14.920 --> 00:07:18.090
You just want like something super simple.
Go for gym,
uh,

121
00:07:18.091 --> 00:07:21.930
which is what we're doing right now.
We just want to use pawn.
Uh,
but yeah,

122
00:07:21.990 --> 00:07:26.130
and so that's what opening are released and there's a whole also,

123
00:07:26.131 --> 00:07:29.280
there's a whole bunch of algorithms that have been made for Jim because it's

124
00:07:29.281 --> 00:07:33.040
been around for longer and you could look at it at an open AI's websites.
Um,

125
00:07:33.240 --> 00:07:35.230
they've gamified the whole process pretty great.

126
00:07:35.910 --> 00:07:40.190
Those are three dependencies that we need.
Okay.
And then,
alright,

127
00:07:40.230 --> 00:07:42.130
we're not going to get a little Mike fix for a second.

128
00:07:49.020 --> 00:07:50.090
<v 0>Cool.
Cool.</v>

129
00:07:50.170 --> 00:07:54.070
<v 1>Thanks.
All right,
so let's get started with this book.
Before we get started,</v>

130
00:07:54.071 --> 00:07:58.060
let's talk about how does pong work.
Okay.
I mean,

131
00:07:59.050 --> 00:08:00.880
let's just think about it.
I'm like,
what do we need to do?

132
00:08:00.881 --> 00:08:03.430
Let's just think about this intuitively for a second.
Stop everything.

133
00:08:03.850 --> 00:08:07.030
Hold your horses is a glorious day or night,
wherever you are.

134
00:08:07.031 --> 00:08:08.440
And I'm thank you for watching,
by the way.

135
00:08:09.160 --> 00:08:13.240
How do we get our agent to learn how to hit the ball?

136
00:08:13.870 --> 00:08:18.340
Well as humans.
Whenever we go into Pong I what do we were doing something like,

137
00:08:18.341 --> 00:08:23.170
let's just see if I go into a game,
I've never seen pong before.
I look at it,

138
00:08:23.320 --> 00:08:24.880
I see that the agent is,

139
00:08:25.120 --> 00:08:28.840
I see that the AI is hitting a ball after even one iteration.

140
00:08:28.841 --> 00:08:33.841
I know that I need to get this ball to go past the agent because I see my score

141
00:08:34.661 --> 00:08:39.430
go down or his score go up if he hits the ball.
So it's easy for us.

142
00:08:40.060 --> 00:08:40.893
However,

143
00:08:42.340 --> 00:08:46.330
one more technical thing that's got to get fixed.
The mic.

144
00:08:51.310 --> 00:08:54.390
<v 0>Cool.
Do you want to hold it too?
Maybe that for now</v>

145
00:08:54.640 --> 00:08:55.530
<v 2>what's happening is it just,</v>

146
00:08:55.680 --> 00:08:59.520
it's just getting really fuzzy and say they can't quite hear you can't

147
00:09:01.370 --> 00:09:02.203
quite

148
00:09:05.570 --> 00:09:08.970
it now.
Like I just like put it right here.

149
00:09:09.150 --> 00:09:11.690
So now that Mike is on the table,
is it as fun?

150
00:09:15.480 --> 00:09:20.370
Okay.
So now I've got,
just hold her in front of you.
All right guys.

151
00:09:21.220 --> 00:09:22.400
<v 1>All right,
so one second.</v>

152
00:09:22.401 --> 00:09:25.550
I'm holding the mic up where we're going through some technical difficulties but

153
00:09:25.551 --> 00:09:28.600
uh,
is with the Mike and Audio and you know,
live stream.

154
00:09:28.860 --> 00:09:33.500
Can be kind of crazy because they're live.
We're doing it live.
Okay,
so let me,

155
00:09:33.530 --> 00:09:36.900
let me talk a little bit more about this while we fix this issue.
So,
uh,

156
00:09:37.550 --> 00:09:39.800
long humans,
we're really good at this,
but uh,

157
00:09:40.190 --> 00:09:44.120
but we need to think about how to do this in a way that we don't even think

158
00:09:44.121 --> 00:09:46.580
about pong.
We want to focus on the outbreed them.

159
00:09:46.581 --> 00:09:49.640
We don't want to focus on the environment.
So if we think about it,

160
00:09:50.300 --> 00:09:53.600
we're going to do this.
Okay.
How does palm work?
Well,

161
00:09:53.601 --> 00:09:57.800
step one is we even from the game that is this game state,

162
00:09:57.801 --> 00:10:02.390
this is an 80 pixel
game world.

163
00:10:02.690 --> 00:10:05.390
And then we're going to move the paddle either up or down.

164
00:10:05.420 --> 00:10:08.660
It's a binary decision.
So we get some image,

165
00:10:08.840 --> 00:10:10.550
we move our paddle up or down,

166
00:10:10.820 --> 00:10:15.140
and then we make an action and receive a reward and it moves past the AI.

167
00:10:15.590 --> 00:10:20.060
And then we subtract one if we miss the ball and then zero otherwise,

168
00:10:20.061 --> 00:10:23.360
so plus one,
if it goes past the agent,
uh,
minus one,

169
00:10:23.361 --> 00:10:26.600
if we missed the ball and it goes past us and then zero.

170
00:10:26.601 --> 00:10:30.920
Otherwise the agent hit it.
So the three possible actions that we could take,

171
00:10:31.520 --> 00:10:35.420
let's forget about everything else.
That's all we need to know about.
Why.

172
00:10:35.450 --> 00:10:40.330
Because we are focusing on the algorithm.
We're not focusing on care,

173
00:10:40.890 --> 00:10:42.320
not policy.
Great hands.

174
00:10:42.680 --> 00:10:47.530
How can we make the most general algorithm that we can apply to paint?
Dang.
Okay.

175
00:10:47.900 --> 00:10:51.070
So,
and this is a type of green

176
00:10:52.810 --> 00:10:57.400
hold policy gradients and policy gradients are really cool for several reasons

177
00:10:57.401 --> 00:11:00.010
that I'm going to go into in a second.
But

178
00:11:02.580 --> 00:11:06.270
what we're going to do is uh,
we're gonna just start coding this thing.
So

179
00:11:07.960 --> 00:11:10.690
<v 2>when I have a free hand,
let me actually say something else.</v>

180
00:11:10.990 --> 00:11:15.130
So what we're gonna do is we're going to build a two layer neural network.
Okay?

181
00:11:15.131 --> 00:11:16.170
And that these are agent,

182
00:11:16.180 --> 00:11:21.100
it's going to be a two layer neural network and the network is going to read in,

183
00:11:21.120 --> 00:11:24.730
we're going to feed it one input and the input is going to be the game state.

184
00:11:25.030 --> 00:11:28.990
And that is the,
the the pixels,
not that,
not the states,
the game pixel.

185
00:11:28.991 --> 00:11:32.740
So a frame,
we're going to free pizza in frame by frame,
by frame,
by frame,

186
00:11:33.160 --> 00:11:38.050
and then it's going to output a probability value of whether to move up or down.

187
00:11:38.740 --> 00:11:41.080
That's it.
And then we sample from that probability value.

188
00:11:41.290 --> 00:11:46.220
Should we go up or down and
then we say,

189
00:11:46.250 --> 00:11:48.290
okay,
so should we go up or down?

190
00:11:48.590 --> 00:11:52.190
And then from that probability value we can then update our network.
We'll get,

191
00:11:52.320 --> 00:11:57.130
we'll we're,
we're getting the gradient,
which is the policy from that value,

192
00:11:57.160 --> 00:12:00.670
from that distribution as we back propagate and you'll understand more as well

193
00:12:00.880 --> 00:12:04.030
as I start coding this.
Okay.
So let's just go ahead and get started with this.

194
00:12:04.240 --> 00:12:06.910
So the first step is to import our hyper parameters.

195
00:12:07.060 --> 00:12:11.530
So let's go ahead and get started with the hyper parameters.
Okay.

196
00:12:11.531 --> 00:12:16.210
So the hyper parameters in this case are going to be the number of hidden

197
00:12:16.211 --> 00:12:19.360
neurons.
Okay.
There's gonna be 200 hidden neurons,
okay.

198
00:12:19.361 --> 00:12:22.300
For our two layer network.
And

199
00:12:24.120 --> 00:12:24.530
<v 3>okay.</v>

200
00:12:24.530 --> 00:12:26.330
<v 2>Then we're going to import the batch size.</v>

201
00:12:26.331 --> 00:12:30.800
So how many episodes do we want in a parameter update?
We're going to say 10.

202
00:12:31.400 --> 00:12:34.190
Uh,
the learning rate,
uh,
is going to be a

203
00:12:36.050 --> 00:12:39.500
very small.
So one e minus four just means like 0.0,

204
00:12:39.501 --> 00:12:43.910
zero zero one where four is a number of values after the decimal point.

205
00:12:44.270 --> 00:12:46.520
And the reason we have a learning right is for convergence.

206
00:12:46.550 --> 00:12:49.040
If it's too low then it's going to be slow to converge.

207
00:12:49.250 --> 00:12:51.110
If it's too high then it's never going to converge.

208
00:12:51.260 --> 00:12:54.200
We always need a learning rate.
Well we don't always need a learning,
right?

209
00:12:54.201 --> 00:12:57.260
But if you want your model to have accurate values,

210
00:12:57.261 --> 00:13:00.530
then yes you want to learning rate.
Okay,
so then we have gamma.

211
00:13:00.920 --> 00:13:03.080
Gamma is going to be used for,

212
00:13:03.190 --> 00:13:05.990
and this is really cool and this is specific to policy gradients,

213
00:13:06.170 --> 00:13:09.620
but for our discount factor,
what the hell is this?

214
00:13:09.770 --> 00:13:13.370
I will talk about it when we write the method for that later.

215
00:13:13.371 --> 00:13:15.270
Rewards are less important.

216
00:13:15.271 --> 00:13:18.740
So what we're doing here is we are optimizing for the short term.

217
00:13:19.730 --> 00:13:22.520
That means that we are optimizing for short term rewards.

218
00:13:22.521 --> 00:13:24.890
We don't know what's going to happen in the future.
This is,

219
00:13:25.580 --> 00:13:30.380
this isn't a game like legend of Zelda all Corinne up time with many levels and

220
00:13:30.381 --> 00:13:32.780
there's a whole bunch of things that we could do and you know,

221
00:13:32.840 --> 00:13:37.520
moving up a ball this way or picking up a ferry right now could help us beat the

222
00:13:37.521 --> 00:13:42.020
boss five levels of in the future.
This is Pong in Paul.

223
00:13:42.050 --> 00:13:45.320
You just got to get,
get in.
You are in the game.
Once you are in the game,

224
00:13:45.321 --> 00:13:50.030
you are in the game.
So that's what that's for.

225
00:13:50.031 --> 00:13:52.120
And we'll explain it.
I'll explain it when we get to the,
uh,

226
00:13:53.200 --> 00:13:57.650
the actual code.
So now we have our decay rates.

227
00:13:58.400 --> 00:14:01.670
So decay rate is going to be 0.99.

228
00:14:02.000 --> 00:14:05.370
And this is for rms prop,
which means grading dissent.
Remember,

229
00:14:05.400 --> 00:14:08.570
we're going to do this by hand and you don't actually have to do this with

230
00:14:08.571 --> 00:14:12.560
tensorflow.
Tensorflow is great because of automatic differentiation.

231
00:14:12.860 --> 00:14:14.870
That means that it does this by itself,

232
00:14:15.020 --> 00:14:17.750
but we're going to do it by hand because hard mode is on right now.

233
00:14:18.020 --> 00:14:22.460
Seriously hard node in life is long.
Okay.
Um,
so right,
so,

234
00:14:24.330 --> 00:14:25.163
<v 3>okay.</v>

235
00:14:25.180 --> 00:14:28.570
<v 2>And then resume equals false because this is just our way of saying do we want</v>

236
00:14:28.571 --> 00:14:30.280
to start off from a previous checkpoints?
Okay.

237
00:14:30.580 --> 00:14:34.810
So then let's go ahead and initialize our model.
Okay,
so in it model,

238
00:14:35.620 --> 00:14:37.660
right?
Okay,
so for anything,
our model,

239
00:14:37.690 --> 00:14:41.410
and I'm going to answer questions in five minutes,
uh,
is going to,

240
00:14:41.440 --> 00:14:43.570
we're going to say,
okay,
so what is the,

241
00:14:44.780 --> 00:14:45.110
<v 3>okay</v>

242
00:14:45.110 --> 00:14:49.340
<v 2>input dimensionality.
It's going to be an 80 by 80 pixel frame.
Okay?</v>

243
00:14:49.341 --> 00:14:53.360
We're defining that right here.
D 80 biking,
and that's what we just saw.

244
00:14:53.390 --> 00:14:57.680
If I made it bigger than that,
that palm would be bigger.
Okay.

245
00:14:57.980 --> 00:15:01.850
So now what we're gonna do is we're going to say if resume that is,

246
00:15:01.851 --> 00:15:04.760
if we want to load it from a previous [inaudible],
this is just,

247
00:15:04.790 --> 00:15:08.480
this is why we imported pickle.
We're going to say,
let's open.

248
00:15:08.481 --> 00:15:13.481
That's value from our saved file and that loads it.

249
00:15:14.480 --> 00:15:17.840
Okay.
So,
okay,
so that's for resuming.

250
00:15:17.900 --> 00:15:22.640
And then I'm going to also see how everybody's doing over here.
Everybody's good.

251
00:15:22.641 --> 00:15:26.600
Everybody's in the room.
All right.
Once you are in the game,

252
00:15:27.710 --> 00:15:30.800
oh man,
you guys are the best.
Okay.
Okay.

253
00:15:30.801 --> 00:15:33.320
So now

254
00:15:35.330 --> 00:15:39.320
model equals nothing.
So elsewhere you,
we were going to initialize it by hand.

255
00:15:39.470 --> 00:15:41.270
Okay.
So this part is really cool.

256
00:15:41.540 --> 00:15:44.600
So this is something that I haven't done before.
So,
um,

257
00:15:45.320 --> 00:15:48.290
it's time to initialize our weights.
So how do we initialize our weights?

258
00:15:48.291 --> 00:15:52.770
Can anyone tell me?
Think about it.
You're gone.
Because see,

259
00:15:52.780 --> 00:15:57.140
we usually say I get these products,
have notifications that I don't care about,

260
00:15:57.141 --> 00:16:00.350
but I don't have time to remove them because I don't have time for anything but

261
00:16:00.351 --> 00:16:02.450
this.
And that's the way I like it.
Look,
where was I?

262
00:16:02.960 --> 00:16:05.910
We are going to initialize these models pseudo randomly.

263
00:16:06.140 --> 00:16:09.320
So it's going to be smart her,
they just clean all random.

264
00:16:09.321 --> 00:16:12.140
It's called Xavier initialization.
So Dave,

265
00:16:12.141 --> 00:16:13.910
you're initialization and there's a whole bunch,

266
00:16:13.911 --> 00:16:15.230
there's a whole theory behind this.

267
00:16:15.740 --> 00:16:19.850
What essentially what we're doing is we are taking the hidden nodes into account

268
00:16:19.970 --> 00:16:24.260
when we initialized our weights.
So we can say,

269
00:16:28.660 --> 00:16:31.810
let's say for our first set of white,
so we have a two layer network,
right?

270
00:16:31.811 --> 00:16:36.811
So our first set of ways is going to be random but conditioned on are,

271
00:16:37.990 --> 00:16:39.010
are um,

272
00:16:40.240 --> 00:16:44.110
are hidden nodes as well as our model,

273
00:16:44.111 --> 00:16:48.940
our input dimentionality so we're going to say random.
Got Rent,
uh,

274
00:16:49.870 --> 00:16:51.190
and this is the interval

275
00:16:54.220 --> 00:16:59.210
that we're going to be using.
And we're going to say that's

276
00:17:03.680 --> 00:17:06.990
beautiful it from teach to d divided by the square root of heat.

277
00:17:07.370 --> 00:17:08.420
Now why do we do this?

278
00:17:11.240 --> 00:17:15.110
The reason we're doing this is because we want to,
let me,

279
00:17:15.310 --> 00:17:17.690
let me just for the equation for a second cause danger and the foundation

280
00:17:17.691 --> 00:17:18.680
because this is pretty cool.

281
00:17:24.290 --> 00:17:28.470
Xavier initialization.
Okay.

282
00:17:35.360 --> 00:17:36.660
Xavier initialization.

283
00:17:41.170 --> 00:17:45.170
It's pretty cool.
So it's the same formula.
It runs the same way

284
00:17:48.410 --> 00:17:49.243
and,

285
00:17:51.720 --> 00:17:56.660
<v 1>okay.
Right?
So it's just,
we don't actually this,
this is the formula,</v>

286
00:17:56.661 --> 00:17:59.430
what we're sampling from a distribution between h and d divided by the square

287
00:17:59.431 --> 00:18:02.340
root of d.
And that is better than random.

288
00:18:02.490 --> 00:18:05.850
And what it does is we make sure that the weights are not too small,

289
00:18:05.851 --> 00:18:09.150
but not too big,
but also not too big to propagate accurately.

290
00:18:09.360 --> 00:18:11.250
It's kind of like the vanishing gradient problem,

291
00:18:11.540 --> 00:18:15.000
whereas like it's a kind of the same idea where,
um,

292
00:18:15.930 --> 00:18:17.970
if our weights are initially very close to zero,

293
00:18:18.180 --> 00:18:21.540
then what happens is that the signal shrink as it goes through each layer until

294
00:18:21.541 --> 00:18:23.610
it becomes too tiny to be useful.

295
00:18:23.670 --> 00:18:26.610
But if the weights are too big than the signal grows at each layer.

296
00:18:27.420 --> 00:18:32.420
So we do that for this first set of weights and then we do that for the next set

297
00:18:32.521 --> 00:18:35.400
of weights as well.
Okay.
We do it for both sets of weights.

298
00:18:36.000 --> 00:18:37.500
So this is going to be weights too.

299
00:18:37.650 --> 00:18:41.220
And these are in the model is going to be a key value pair where the key is

300
00:18:41.221 --> 00:18:44.250
going to be the,
um,
the,
what is it?

301
00:18:45.510 --> 00:18:46.321
The construct,

302
00:18:46.321 --> 00:18:49.290
whether it be weights or a layer and in the value is going to be the values

303
00:18:49.291 --> 00:18:52.560
inside of it.
The vector,
you know,
the actual values inside of the Matrix.

304
00:18:53.490 --> 00:18:58.490
So we're going to say through h and then square root h.

305
00:18:59.610 --> 00:19:02.490
Okay.
Because now we have a hidden state.
So for the next one,

306
00:19:04.890 --> 00:19:09.540
right?
So the first one hidden c and d because we're inputting that the frame.

307
00:19:09.810 --> 00:19:11.550
But now for the wait,
wait too.
We just,

308
00:19:11.551 --> 00:19:14.160
we're just talking about the hidden wait with the input has already been

309
00:19:14.161 --> 00:19:17.730
computed into some doctor.
Okay.
So that's Xavier initialization.

310
00:19:17.970 --> 00:19:21.180
Remember this is super low level.
We're not using anything but num py right now.

311
00:19:21.181 --> 00:19:24.900
So,
um,
so it's awesome.
Cool.

312
00:19:24.990 --> 00:19:27.290
So now we're going to initialize two more values here,

313
00:19:27.300 --> 00:19:32.010
the gradient buffer and the RMS prop cash,
which I'll explain in a second.

314
00:19:32.250 --> 00:19:35.640
So for our gradient buffer,

315
00:19:35.700 --> 00:19:40.700
we have a collection of Zeros that we're going to initialize.

316
00:19:41.170 --> 00:19:45.750
Uh,
and then we're going to say for each key value pair in the model.

317
00:19:46.680 --> 00:19:49.530
And remember this,
this is great about putting them put,

318
00:19:49.590 --> 00:19:52.780
our model is essentially a dictionary for this use case.
Uh,

319
00:19:52.830 --> 00:19:55.410
we're going to say get the gradient buffer.

320
00:19:55.411 --> 00:19:58.590
And let me minimize that a little bit so we can see all of that.
Okay.

321
00:19:58.591 --> 00:20:01.470
Just like that.
And then we're going to say rms product.
Let me explain.

322
00:20:01.471 --> 00:20:02.730
I'll explain this once I type it out.

323
00:20:02.970 --> 00:20:07.390
We have our RMS prop cash and I'm going to take questions once I write this,

324
00:20:07.391 --> 00:20:11.010
this line out.
Okay.
So save them from them.
Save your questions for then.

325
00:20:11.490 --> 00:20:15.980
I assume that I'm talking too fast already just because my uh,

326
00:20:18.390 --> 00:20:22.110
algorithm has been trained to detect that from feedback,

327
00:20:22.770 --> 00:20:27.270
trial and error of doing this it or items.
Okay.

328
00:20:27.570 --> 00:20:30.780
So what,
what,
what the hell did I just do here?
So what I did,
what,

329
00:20:30.781 --> 00:20:33.930
let me explain this and then I'll answer questions was I define a great and

330
00:20:33.931 --> 00:20:36.120
buffer and then an rms prop cash.

331
00:20:36.121 --> 00:20:39.810
The gradient buffer is going to help us

332
00:20:41.970 --> 00:20:45.450
with uh,
backpropagation.
So it's,

333
00:20:45.451 --> 00:20:50.140
it's a way for us to store our gradients.
Right now it's initialized as Zeros,

334
00:20:50.290 --> 00:20:53.230
but eventually we're going to add a bunch of gradients to it and you'll see that

335
00:20:53.231 --> 00:20:58.030
when we get to backpropagation and rms cash is going to store the value of a

336
00:20:58.031 --> 00:20:58.950
formula.
This,

337
00:20:59.050 --> 00:21:03.640
the RMS prop formula and that is including whatever we multiply it by,

338
00:21:03.670 --> 00:21:07.600
which is going to be some set concen value into this cash.
Again,

339
00:21:07.630 --> 00:21:09.730
will be more clear when we start actually using it.

340
00:21:09.910 --> 00:21:12.920
But right now they're both initialize with Zeros.
Um,

341
00:21:13.600 --> 00:21:17.830
for all the values in our model.
Okay,
so let me answer some questions here.

342
00:21:17.831 --> 00:21:20.620
I'll answer two questions and then we'll see.
We'll keep getting,

343
00:21:20.650 --> 00:21:22.420
we'll get right back into this.

344
00:21:24.220 --> 00:21:29.220
Omar Miranda ask please defines the castic and non and and no stochastic but

345
00:21:30.820 --> 00:21:32.830
what non stochastic.
Okay.

346
00:21:33.410 --> 00:21:36.640
It's the castic means unpredictable.
Okay.

347
00:21:36.641 --> 00:21:40.210
It's kind of like the u s elections.
They're sarcastic.

348
00:21:40.600 --> 00:21:44.680
We had no idea that would happen,
but they're kind of like life.
Life is the tats,

349
00:21:44.681 --> 00:21:49.060
the castic.
If you believe in fate,
that is not the castings.

350
00:21:49.670 --> 00:21:52.540
Fate is deterministic.
We are,
we we,

351
00:21:52.590 --> 00:21:55.120
there's a predetermined outcome that is fate.

352
00:21:55.570 --> 00:21:58.210
Stochastic means we have no idea if it's going to happen or not.

353
00:21:58.280 --> 00:22:02.830
Anytime we have a random number that is the castic.
Anytime we have a sat set,

354
00:22:02.831 --> 00:22:07.831
static number that is deterministic and then the question becomes,

355
00:22:08.140 --> 00:22:09.190
what's going on here?

356
00:22:09.370 --> 00:22:12.520
Do we have a bunch of deterministic values in here or do we have a bunch of

357
00:22:12.580 --> 00:22:15.760
stochastic values?
This goes into the question of free will.

358
00:22:16.030 --> 00:22:18.450
So this is a really cool thing because it,
it,

359
00:22:18.540 --> 00:22:21.850
it's a way for us to think about consciousness and human,

360
00:22:22.150 --> 00:22:25.810
what it means to be human and determinism and free will mathematically.

361
00:22:26.050 --> 00:22:26.681
How cool is that?

362
00:22:26.681 --> 00:22:31.681
So I think that it is the castic what's happening here that we do have free

363
00:22:32.871 --> 00:22:33.670
will,

364
00:22:33.670 --> 00:22:37.420
that we do make decisions that are not predetermined and we're starting to see

365
00:22:37.421 --> 00:22:41.630
deep learning move in that direction.
We're starting to see people ha,
uh,

366
00:22:42.010 --> 00:22:44.950
researchers and developers,
anybody working on deep learning,

367
00:22:45.130 --> 00:22:50.130
adding more stochasticity stochasticity or randomness into their model,

368
00:22:50.440 --> 00:22:53.560
which is really cool because we don't know what's going to happen.

369
00:22:53.560 --> 00:22:57.520
Variational auto encoders are a great example of that.
Okay.

370
00:22:57.521 --> 00:22:59.650
So two more or one more question.
Actually,

371
00:22:59.651 --> 00:23:03.670
I've got a bunch of questions so I'm just going to lightning answer them.
Uh,

372
00:23:05.110 --> 00:23:08.980
how to choose optimal batch size to train a and n,
um,
grid search.

373
00:23:09.860 --> 00:23:13.110
Alvaro Garcia,
how did you choose your hyper PR?

374
00:23:13.210 --> 00:23:16.600
How did you choose your hyper parameters?
Uh,
they have worked.

375
00:23:16.780 --> 00:23:19.810
So this is a good rule of thumb if you don't have a learning method for your

376
00:23:19.811 --> 00:23:22.840
hyper parameters,
use what's worked.
And papers beforehand.

377
00:23:22.990 --> 00:23:26.260
If you are going to use a learning method,
then use grid search.

378
00:23:27.100 --> 00:23:28.990
Jessica says,
uh,

379
00:23:29.230 --> 00:23:32.980
more resources for understanding perceptron weights and biases with and,
and,

380
00:23:32.981 --> 00:23:37.330
or logic.
Uh,
Andrew Trask Google him.

381
00:23:37.331 --> 00:23:42.250
Great resource for Rune has how you define it in the same question and one more

382
00:23:42.251 --> 00:23:43.001
olly asks,

383
00:23:43.001 --> 00:23:47.390
what's the best way to use tensorflow for protein prediction model?

384
00:23:47.391 --> 00:23:52.070
Quality estimation,
uh,
protein prediction model,

385
00:23:52.071 --> 00:23:55.040
quality estimation.
I'm not sure exactly what you mean,

386
00:23:55.041 --> 00:23:58.550
but any kind of prediction for some kind of genetic testing.

387
00:23:58.551 --> 00:24:02.600
I'm assuming as a classification problem you have some data set,

388
00:24:02.720 --> 00:24:06.440
you're trying to classify what type of protein.
I assume something is,

389
00:24:06.980 --> 00:24:08.660
hopefully you have pre labeled data.

390
00:24:08.690 --> 00:24:12.800
If you don't then you went to hand label it with a bunch of people on your team

391
00:24:13.040 --> 00:24:16.460
and then run a classifier on it.
I would say use a,

392
00:24:17.420 --> 00:24:20.540
uh,
prob,
use a

393
00:24:22.340 --> 00:24:23.010
<v 3>okay.</v>

394
00:24:23.010 --> 00:24:26.190
<v 1>Feed forward network.
Uh,
that is</v>

395
00:24:27.140 --> 00:24:27.890
<v 3>okay.</v>

396
00:24:27.890 --> 00:24:32.890
<v 1>Youth transfer learning on a feed forward network that has been pretrained on a</v>

397
00:24:33.591 --> 00:24:36.290
genetic data and there are several out there.

398
00:24:36.291 --> 00:24:40.910
So genetic pretrained transfer network,
Google that.
Okay.
That's it for that.

399
00:24:41.240 --> 00:24:45.620
Um,
yeah.
So back to this.
Now what we're going to do is want to keep going.

400
00:24:46.670 --> 00:24:49.970
Let's define our activation function.
And remember we're doing this by hand.

401
00:24:49.971 --> 00:24:52.490
So we've got quite a bit of coats,
right?
So let's,
let's just,

402
00:24:52.520 --> 00:24:55.520
let's just keep going.
So for activation function,
we're going to use sigmoid.

403
00:24:55.521 --> 00:24:57.290
Now it's not just sigmoid,

404
00:24:57.830 --> 00:25:01.400
we're going to use quite a bit of,
uh,

405
00:25:02.450 --> 00:25:03.283
stuff.

406
00:25:05.260 --> 00:25:08.060
We're going to use another activation function as well called Relu.

407
00:25:08.090 --> 00:25:11.060
That's what I meant to say.
So we have one divided by,

408
00:25:11.061 --> 00:25:14.870
and remember this is always the same per sigmoid and it helps us convert numbers

409
00:25:14.871 --> 00:25:19.730
into,
say with me probabilities.
So this is our squashing.
This is,

410
00:25:19.731 --> 00:25:24.410
this is what converts are vectors into probabilities.
Okay?

411
00:25:24.950 --> 00:25:29.060
And then it's going to be a preprocessing step.
So preprocessing step.

412
00:25:30.080 --> 00:25:34.160
What this does is it's going to take a single game frame as inputs.

413
00:25:34.161 --> 00:25:39.161
So I is going to be the game frame and then it's going to convert it into just

414
00:25:39.411 --> 00:25:44.180
what we need,
which is going to be a,
the paddle,
the paddles,
plural,

415
00:25:44.420 --> 00:25:47.090
the ball.
And that's it.
We don't care about anything else.

416
00:25:47.210 --> 00:25:49.100
So we're going to take an image frame,

417
00:25:49.101 --> 00:25:52.370
which is that first image that we get and we're going to first crop it.

418
00:25:52.490 --> 00:25:56.030
So it's just going to be the part that we need,
which contains the paddle.

419
00:25:56.390 --> 00:25:58.700
And how did I find these values,
these magic numbers?

420
00:25:59.360 --> 00:26:01.850
Because they,

421
00:26:02.210 --> 00:26:05.330
I mean in general you would do this through trial and error.
You would,
you would,

422
00:26:05.600 --> 00:26:08.960
you would see,
you know,
you would try out some sub sample,

423
00:26:08.961 --> 00:26:13.220
some sub sample of your image,
and then display it and see like where am I?

424
00:26:13.221 --> 00:26:17.150
And then closing on it,
right?
So this is the cropping step a.

425
00:26:17.151 --> 00:26:21.380
And then once I cropped it,
I'm going to say,

426
00:26:25.250 --> 00:26:26.083
<v 3>okay,</v>

427
00:26:26.830 --> 00:26:30.460
<v 1>two,
now I'm going to downsample downsample by a factor of two,</v>

428
00:26:30.490 --> 00:26:33.790
which means that I only want a subset of what I'm seeing.

429
00:26:34.120 --> 00:26:36.040
And then I'm going to say,
well,
okay,

430
00:26:36.041 --> 00:26:39.850
so and remember these are magic numbers that have been looked at beforehand.

431
00:26:39.880 --> 00:26:42.160
It's not night.
We just just know somehow.

432
00:26:42.430 --> 00:26:47.430
But what I'm saying is I'm saying at the pixel value 144 if the pixel value is

433
00:26:48.061 --> 00:26:52.500
144 that is the type of background which is like this orange g background for

434
00:26:52.501 --> 00:26:54.210
the sake of the game,
it's,
it doesn't care about that.

435
00:26:54.211 --> 00:26:56.970
So we're going to erase that.
So we're going to set it to zero.

436
00:26:56.971 --> 00:27:01.971
So that means erase background and then right and this is just something in four

437
00:27:02.371 --> 00:27:05.250
games in general that we can do to help our model.

438
00:27:05.251 --> 00:27:08.370
If it's not smart enough to do it itself,
we can just hand code it,

439
00:27:08.430 --> 00:27:10.860
which is what this sample is doing right now.

440
00:27:11.700 --> 00:27:15.000
And we're going to say I equals equals one oh nine and this is going to be the

441
00:27:15.001 --> 00:27:18.060
other background.
So there's,
there are two layers of backgrounds here.

442
00:27:18.420 --> 00:27:20.070
One on one oh nine I'm sure I made an error,

443
00:27:20.071 --> 00:27:23.400
but let me just finish writing up this function and then we'll get to that.

444
00:27:24.010 --> 00:27:27.330
When it's not equal to zero,
we're going to say one.

445
00:27:27.331 --> 00:27:29.160
And so that means that everything else is going to be,

446
00:27:29.610 --> 00:27:32.880
so that means paddles and balls.

447
00:27:34.470 --> 00:27:36.870
So many balls set to one.
Okay,

448
00:27:38.640 --> 00:27:43.640
now we're going to return that value as a flattened duray.

449
00:27:44.040 --> 00:27:48.990
And how do we do that?
Well,
we use the rebel function of num Pi.
Okay.

450
00:27:50.250 --> 00:27:55.170
Which means flatten it into a one dimensional array.
Okay.
So we've got that.

451
00:27:58.230 --> 00:27:58.800
Now it's time.

452
00:27:58.800 --> 00:28:01.290
So we're going to keep writing out some helper functions before we actually

453
00:28:01.291 --> 00:28:03.030
implement it and talk about what each is doing.

454
00:28:03.300 --> 00:28:06.990
Now things will start to get interesting.
Okay,
we've done the easy stuff,

455
00:28:06.991 --> 00:28:10.800
now it's time for the good stuff.
That good,
good,
that good,
good.

456
00:28:10.801 --> 00:28:13.650
And that in this case is called the discount reward.

457
00:28:14.130 --> 00:28:17.370
Since we are optimizing for the short term,
right?

458
00:28:17.440 --> 00:28:21.390
We are not optimizing for the longterm,
we are optimizing for the short term.

459
00:28:21.900 --> 00:28:26.850
And the way that we do this is by implementing this strategy called a discount

460
00:28:26.851 --> 00:28:31.650
reward.
So we're going to uh,
receive a set of rewards.
Okay.

461
00:28:31.651 --> 00:28:36.210
For a bunch of time steps.
So our,
our agent is learning from every time,

462
00:28:36.211 --> 00:28:40.050
every time the ball goes in one direction and it goes past,
you know,

463
00:28:40.051 --> 00:28:43.710
one of our players than the game resets.
Right?
And it just keeps going.

464
00:28:43.740 --> 00:28:46.770
The score doesn't reset,
but the game resets,
the ball resets,

465
00:28:47.190 --> 00:28:50.070
and we're going to get rewards for every time this happens and it's going to be

466
00:28:50.071 --> 00:28:54.450
an array of values and we're going to feed that into this discount reward and

467
00:28:54.451 --> 00:28:58.710
what we're gonna do here in the discount reward as we're going to weigh each of

468
00:28:58.711 --> 00:29:03.390
those rewards differently and we're going to weigh that the most immediate

469
00:29:03.391 --> 00:29:06.630
rewards higher in the later rewards.
Okay?
And you'll,

470
00:29:06.660 --> 00:29:08.490
you'll see what I'm talking about when we implement this.

471
00:29:09.000 --> 00:29:11.310
And when I'm done implementing this,
I'll answer some questions.

472
00:29:12.420 --> 00:29:17.420
So I'm going to say that the discounted reward is going to be Zeros.

473
00:29:20.820 --> 00:29:25.020
Okay?
So it's going to start off as a matrix of Zeros.
Okay.

474
00:29:25.030 --> 00:29:29.130
This is our matrix of discount or rewards that all these values have been

475
00:29:29.131 --> 00:29:31.680
applied to.
And there's a formula for this as well that I'm going to,

476
00:29:32.040 --> 00:29:34.470
I'm going to show,
let me just,
let me actually just show the formula first.

477
00:29:34.650 --> 00:29:39.610
So the formula is going to be zero to all.
Uh,

478
00:29:39.980 --> 00:29:44.980
there's going to be called reinforcement zero to all.

479
00:29:48.850 --> 00:29:51.760
And then it's going to be called a discount reward.

480
00:29:53.220 --> 00:29:54.053
<v 3>Okay?</v>

481
00:29:59.700 --> 00:30:04.560
<v 1>Okay.
So this count reward formula is a better thing to search.
So this is,</v>

482
00:30:04.561 --> 00:30:07.530
this is good.
You guys can see how I Google things.
There we go.

483
00:30:11.040 --> 00:30:15.620
Okay,
where are we at?
Okay,
so that,

484
00:30:15.621 --> 00:30:18.710
I mean that's one way of looking at it,
but you know what I'm,

485
00:30:18.740 --> 00:30:22.550
what I'm going to do is I'm going to say let's go to this guy on Kim's get hub.

486
00:30:22.760 --> 00:30:27.410
It's got hung Kim and what Hung Kim did is there,
there we go.
Here it is.

487
00:30:28.010 --> 00:30:32.480
So here it is.
Great intuitive explanation right here.

488
00:30:32.900 --> 00:30:34.250
Okay.
Right here

489
00:30:35.810 --> 00:30:40.810
we have a discount rate and what we are doing is we are weighing the discounts,

490
00:30:41.060 --> 00:30:44.510
we are weighing the rewards that are immediate,

491
00:30:44.600 --> 00:30:48.710
higher than later rewards exponentially so it exponentially.

492
00:30:49.010 --> 00:30:52.280
So as time steps go forward for every time step,

493
00:30:52.490 --> 00:30:57.490
the rewards are exponentially decrease in value in weight and importance to our

494
00:30:59.271 --> 00:30:59.960
agent.

495
00:30:59.960 --> 00:31:03.560
So we're optimizing for the short term because we don't know what's going to

496
00:31:03.561 --> 00:31:04.530
happen in the long term.
We,

497
00:31:04.670 --> 00:31:07.520
we just care about the now and that's what Pong is all about.
The,

498
00:31:07.521 --> 00:31:11.370
now it's a very fast paced game and um,

499
00:31:12.410 --> 00:31:15.890
right,
so we'll start off by initializing a running average

500
00:31:18.200 --> 00:31:22.540
or a running variable of the additions.
It's going to store the rewards,

501
00:31:22.541 --> 00:31:26.420
sums the stuff,
the,
some of the rewards.
And then we're going to say,
okay,

502
00:31:26.421 --> 00:31:31.421
so for all the values in the range from zero to R.

503
00:31:32.320 --> 00:31:36.810
Dot.
Size.
So for all those rewards,

504
00:31:37.140 --> 00:31:40.170
we're going to say if the reward

505
00:31:41.760 --> 00:31:46.260
is not equal to zero,
then we want to make sure that that running,
that,

506
00:31:46.261 --> 00:31:50.310
that uh,
running addition variable is set to zero.

507
00:31:51.030 --> 00:31:55.560
And now it's time to increment the song.
So remember it's just a sum.

508
00:31:55.590 --> 00:31:59.280
So sigma,
that notation,
or over here.
Now here's the full formula.

509
00:31:59.310 --> 00:32:03.240
That was a step by step.
Remember Sigma notation,
this he think looking thing,

510
00:32:03.480 --> 00:32:08.070
it's not scary.
It's just a,
a notation.
It's a,

511
00:32:08.610 --> 00:32:12.750
it's a symbol that represents the sum over a set of values.
Okay?

512
00:32:12.751 --> 00:32:16.410
It's a sum over a set of values in this case.
Okay?

513
00:32:18.540 --> 00:32:21.150
So now we're going to say,

514
00:32:23.100 --> 00:32:24.620
now we're going to do that actual steps.

515
00:32:24.621 --> 00:32:28.530
So we have our running that running ad times gamma,

516
00:32:28.980 --> 00:32:31.100
which is our,
uh,

517
00:32:31.250 --> 00:32:35.760
this discount reward factor that that helps us weigh.

518
00:32:35.761 --> 00:32:39.320
This is what actually helps us weigh those values in exponentially,
uh,

519
00:32:40.540 --> 00:32:44.900
uh,
decreasing order of importance as time steps,
as time moves forward.

520
00:32:45.800 --> 00:32:46.850
We're talking about time,

521
00:32:46.851 --> 00:32:51.470
we're talking about consciousness and whole bunch of things and we're just,

522
00:32:51.650 --> 00:32:55.430
we're just trying to make an agent that beats pong.
Where are we right now?
So,

523
00:32:55.431 --> 00:32:58.040
and when we're done with that,
okay.
Right.

524
00:32:58.220 --> 00:33:02.750
See I can already predict what you guys and say you got to.
Yes,
I know.
Yes.
Okay.

525
00:33:02.751 --> 00:33:07.550
So then we returned that discounted reward.

526
00:33:07.580 --> 00:33:08.270
Okay.

527
00:33:08.270 --> 00:33:11.900
So now let me answer some questions cause I'm sure there are some questions we

528
00:33:11.901 --> 00:33:14.540
got here are okay.

529
00:33:15.590 --> 00:33:18.860
I'll answer some lightening fast.
Okay.
So palosh asks,

530
00:33:19.070 --> 00:33:23.930
can we have some visualizations on tensor board for this short answer?
No time.

531
00:33:24.410 --> 00:33:28.670
But I will do that in a future livestream stream probably for sports
predictions,

532
00:33:28.671 --> 00:33:30.580
which I know you guys want.
So Rod,

533
00:33:30.590 --> 00:33:35.590
will you make a video about differentiable neural computers said Mona Monarch,

534
00:33:35.780 --> 00:33:39.920
you know,
you know me.
You know,
that's what I'm all about.
Yes,
I will.

535
00:33:39.950 --> 00:33:41.960
You know what I'm all about.
That's coming soon.

536
00:33:42.380 --> 00:33:45.140
I'll actually ask what does short term and longterm mean here?

537
00:33:45.170 --> 00:33:49.580
So short term means what is the,
so whenever we,
uh,

538
00:33:49.760 --> 00:33:53.120
make,
uh,
whenever we get our ball to go past the agent,

539
00:33:53.121 --> 00:33:55.760
if we get a plus one reward,
that's a short term reward.

540
00:33:56.240 --> 00:34:01.240
But if we continually hit the ball and then 30 minutes later,

541
00:34:01.840 --> 00:34:05.330
uh,
there's a reward.
Like we're always getting a set of rewards,
right?

542
00:34:05.331 --> 00:34:08.090
No matter what.
But let's just say we have a reward 30 minutes from now.

543
00:34:08.420 --> 00:34:09.920
Can we trace that reward?

544
00:34:10.070 --> 00:34:13.340
Like how we got that reward all the way back to actions that we took 30 minutes

545
00:34:13.341 --> 00:34:16.880
ago should,
should we,
that's the question.
Should we trace it back?

546
00:34:17.060 --> 00:34:19.130
This does what we do,
you know,

547
00:34:19.400 --> 00:34:24.020
40 times steps beforehand with all these rewards happening matter for the 40

548
00:34:24.021 --> 00:34:28.070
times steps later for a,
that happens way later in the future in this case.
No,

549
00:34:28.071 --> 00:34:30.460
they,
it has nothing to do with that,
that it's,

550
00:34:30.461 --> 00:34:35.060
it's not like incrementally our agent are the AI is adapting to what we've done

551
00:34:35.061 --> 00:34:38.480
before.
It's just the same,
you know,
uh,
opponent.

552
00:34:38.660 --> 00:34:43.430
So short term means did the ball go past the agent or not?

553
00:34:43.550 --> 00:34:48.200
Okay.
One more question.
Uh,
I have,

554
00:34:48.470 --> 00:34:50.130
so Benjamin has,

555
00:34:50.131 --> 00:34:54.380
can you walk through line zero through 25 there was some sound problems in that

556
00:34:54.381 --> 00:34:57.410
range.
Sure.
So high level and we walked through this.
Okay.

557
00:34:57.411 --> 00:35:02.000
So we import it three libraries,
matrix math,
save,
load,
our model,

558
00:35:02.300 --> 00:35:06.620
run our game environment,
hyper parameters here are hidden state size,

559
00:35:06.950 --> 00:35:10.310
how,
how,
how many episodes in a batch.
And we have several batches.

560
00:35:10.670 --> 00:35:15.670
What's our learning rate convergence lower high to make it perfect gamma to help

561
00:35:15.831 --> 00:35:20.360
us,
uh,
way rewards that are happening right now.

562
00:35:20.450 --> 00:35:22.310
Higher than rewards that happen in the future.

563
00:35:22.311 --> 00:35:26.360
Because we are optimizing for the short term decay rate,
which we're going to,

564
00:35:26.420 --> 00:35:30.020
we're going to talk about that resume.
Should we load our model now?

565
00:35:30.110 --> 00:35:33.290
Should we load our model from beforehand or do it now?

566
00:35:33.650 --> 00:35:37.410
Then we initialize our model 80 by 80 pixel a matrix,

567
00:35:37.860 --> 00:35:42.860
load our model now from a saved state or lower Amato from a safe state or load

568
00:35:44.131 --> 00:35:46.320
it right now using Xavier initialization,

569
00:35:46.590 --> 00:35:51.000
which they special smarter initialization than just normal using the hidden

570
00:35:51.001 --> 00:35:56.001
state and the input dimentionality as a inputs define our activation standard

571
00:35:56.131 --> 00:36:00.340
sigmoid to squash values into probabilities.
Preprocessed the data,
the,
the,

572
00:36:00.600 --> 00:36:02.460
the frame.
So it's just the paddle and the ball.

573
00:36:03.090 --> 00:36:06.660
And then I just talked about this.
Okay,
one more question.

574
00:36:06.810 --> 00:36:09.990
What kind of network to use in NLP?
LSTM

575
00:36:11.500 --> 00:36:12.080
<v 0>okay.</v>

576
00:36:12.080 --> 00:36:15.920
<v 1>Short answer.
Yes.
LSTM networks are great for text generation.</v>

577
00:36:15.921 --> 00:36:18.440
They're great for tax classification.

578
00:36:18.680 --> 00:36:23.540
Anything with text l s t m s outperformed almost everything else almost all the

579
00:36:23.541 --> 00:36:28.280
time.
Okay.
Okay.
So now we're going to do uh,

580
00:36:28.310 --> 00:36:32.420
two more helper functions and then we're going to get right into it.

581
00:36:32.421 --> 00:36:35.150
And they are for propagation and backward propagation.

582
00:36:35.151 --> 00:36:38.510
So you're going to get to see what this looks like without TensorFlow's.
Um,

583
00:36:39.140 --> 00:36:43.220
you know,
magic,
the magic of tensorflow.

584
00:36:45.170 --> 00:36:49.420
Okay?
So by the way,
Pi Torch tenser flow,
you guys got,
you guys got to,

585
00:36:49.850 --> 00:36:52.820
you guys got to keep your game up because Pi Torch is coming at you.

586
00:36:53.150 --> 00:36:55.970
You know what I'm saying?
Y'All Jala Koons talking about it.
The community is,

587
00:36:55.971 --> 00:36:57.200
is on that pie.
Torch came,

588
00:36:57.201 --> 00:36:59.630
I'm even about to make a video on that pie torch in five minutes.

589
00:36:59.631 --> 00:37:03.410
So spoiler alert.
Okay,
so for propagation,

590
00:37:04.490 --> 00:37:06.880
the first thing we're gonna do is we're going to get that x value.

591
00:37:06.890 --> 00:37:10.970
That is our input.
That is our game,
uh,
value.
That is our,

592
00:37:11.660 --> 00:37:16.020
what is that?
That is the
pixels.

593
00:37:16.021 --> 00:37:18.540
The pixels for the game.
Okay?
So we're going to say,
okay,

594
00:37:18.541 --> 00:37:21.210
so the first thing we're going to do is we're gonna calculate the hidden states

595
00:37:21.420 --> 00:37:25.050
using just as a dot product.
Remember,
it's just a bunch of linear Algebra.

596
00:37:25.080 --> 00:37:27.630
It's just a matrix multiplication.

597
00:37:27.840 --> 00:37:32.560
The dot product multiplies to weight matrices together.
So we have that,
um,

598
00:37:32.760 --> 00:37:36.780
first weight matrix that we defined and we're multiplying it by what's input.

599
00:37:36.930 --> 00:37:39.870
Okay?
And that's going to give us our hidden state,
which is a vector of values.

600
00:37:40.350 --> 00:37:41.580
Okay.
And then once we have that,

601
00:37:41.630 --> 00:37:46.110
what we're gonna do is we're going to squash it with our first non linearity,

602
00:37:46.200 --> 00:37:50.460
which is going to be relu
and Relu is,

603
00:37:51.780 --> 00:37:52.613
what is it saying?

604
00:37:53.370 --> 00:37:58.370
It's saying take the Max between a zero and then what this value is.

605
00:38:00.300 --> 00:38:03.990
So if the value is less than zero,
then it's going to be zero.

606
00:38:04.020 --> 00:38:07.380
If it's greater than zero,
then it's going to be that value.
And uh,

607
00:38:07.440 --> 00:38:11.730
we do this because Relu is actually a,
is a,
is one of them,

608
00:38:11.760 --> 00:38:15.480
one of the most used activation functions for midway through a network where

609
00:38:15.520 --> 00:38:18.960
sigmoid is used for the end of the network to,
to convert.

610
00:38:18.961 --> 00:38:22.980
Whenever we have into probabilities for classification or whatever else they
use,

611
00:38:22.981 --> 00:38:26.730
case is going to be,
so once we have that value squashed,
we're going to say,

612
00:38:26.731 --> 00:38:30.300
let's take the log probability,
which we're calling here.

613
00:38:30.360 --> 00:38:31.410
It's not actually the log,

614
00:38:31.411 --> 00:38:34.110
we're just calling it that because we're going to use that in a second.

615
00:38:35.200 --> 00:38:40.200
And we're going to take the dot product of the next set of weight matrices times

616
00:38:40.870 --> 00:38:44.260
what we have here,
which is the hidden state.
So this is the second layer.

617
00:38:44.470 --> 00:38:45.790
And once we have that,
we're,

618
00:38:45.791 --> 00:38:49.120
we're ready to output those probabilities to what are we going to use?

619
00:38:49.510 --> 00:38:52.480
We're gonna use our sigmoid function.
Okay.

620
00:38:52.720 --> 00:38:56.680
And that's going to output the probabilities for up or down or stay the same.

621
00:38:58.210 --> 00:39:02.320
And then we're going to sample from those probabilities and decide what we want

622
00:39:02.350 --> 00:39:03.183
to do.

623
00:39:03.310 --> 00:39:08.170
And then you that value the policy to get the gradient values by taking the

624
00:39:08.171 --> 00:39:12.250
partial derivatives with respect to each of those weights as we back propagate.

625
00:39:12.251 --> 00:39:16.330
And we're going to do that in a second.
Okay.
And now we return

626
00:39:17.830 --> 00:39:22.660
p and,
h we got,
we got 15,
and we got,
you know,
we've got 19 minutes.
I'm,
I'm,

627
00:39:22.670 --> 00:39:24.700
I'm on this,
we're,
we're gonna,
we're gonna finish this.

628
00:39:25.000 --> 00:39:29.770
We're going to return the probability,
okay.
Of taking action too.

629
00:39:33.610 --> 00:39:36.940
And Hidden States it state.
Okay.

630
00:39:38.140 --> 00:39:40.420
Get away from me.
There we go.
I just talked to some code.

631
00:39:40.870 --> 00:39:45.360
Now we are going to talk about policy backward.

632
00:39:46.200 --> 00:39:47.110
Okay.
So,

633
00:39:49.380 --> 00:39:50.213
<v 3>okay,</v>

634
00:39:51.020 --> 00:39:52.790
<v 1>now it's time to talk about policy backward.</v>

635
00:39:52.850 --> 00:39:55.370
Let me answer one intermediate question just because I feel like there will be

636
00:39:55.371 --> 00:39:57.980
one.
Uh,
no,
there's not.
Okay,
great.

637
00:39:58.340 --> 00:40:00.680
But let me see what people were saying over here.
Hi,

638
00:40:00.681 --> 00:40:01.670
everybody's chilling over here.

639
00:40:03.390 --> 00:40:04.223
<v 3>Okay,
great.</v>

640
00:40:05.930 --> 00:40:06.800
<v 1>Okay.</v>

641
00:40:08.420 --> 00:40:09.253
<v 3>Where were we?</v>

642
00:40:11.220 --> 00:40:13.800
<v 1>We'll probably go over five minutes is because the audio errors.
Okay.</v>

643
00:40:13.801 --> 00:40:17.610
So now what we're gonna do is we're going to recursively compute air derivatives

644
00:40:17.611 --> 00:40:22.140
for both of those layers.
This is the chain rule programmatically.
Okay?

645
00:40:22.141 --> 00:40:23.400
So this value right here,

646
00:40:23.460 --> 00:40:27.990
e p d log p is going to modulate the gradient with advantage.

647
00:40:28.020 --> 00:40:30.330
So we'll talk about that.
Um,

648
00:40:31.230 --> 00:40:34.770
but basically what we're doing as we're computing the update derivative with

649
00:40:34.771 --> 00:40:37.590
respect to wait two to start off with.
Okay.

650
00:40:37.591 --> 00:40:42.330
So this was all going to make sense once we start coding to sal and then

651
00:40:42.360 --> 00:40:43.640
actually implementing it.
But

652
00:40:46.590 --> 00:40:49.710
so we have our dop product here and the dot product says,

653
00:40:49.770 --> 00:40:53.460
let's take the transpose of the,

654
00:40:55.670 --> 00:40:56.840
and remember,
uh,

655
00:40:56.910 --> 00:41:00.890
eph is array of intermediate hidden state.

656
00:41:00.891 --> 00:41:05.170
So we're feeding it back on a ray of intermediate hidden states for every time

657
00:41:05.240 --> 00:41:08.390
for every,
um,
successful.

658
00:41:09.110 --> 00:41:13.520
Every time the ball goes past the agent,
that's considered a a game.

659
00:41:13.521 --> 00:41:15.350
And then we have several games in an episode.

660
00:41:15.530 --> 00:41:17.840
And we're taking the hidden states from all of those that we,

661
00:41:17.870 --> 00:41:21.920
that we are going to calculate and we're going to say

662
00:41:23.920 --> 00:41:28.920
this is going to give us the derivative with respect to wait to tell and we're

663
00:41:28.921 --> 00:41:30.550
gonna use these to update our weights.

664
00:41:30.880 --> 00:41:33.440
We're just calculating derivatives here and then

665
00:41:36.050 --> 00:41:38.360
now we're going to compute the derivative of the hidden states.

666
00:41:40.720 --> 00:41:41.553
<v 3>Yeah,</v>

667
00:41:41.900 --> 00:41:44.960
<v 1>using the
model</v>

668
00:41:46.700 --> 00:41:49.310
and then we're going to apply our activation to it.

669
00:41:50.090 --> 00:41:54.920
And I'll answer questions right when I finished writing the south
equal zero

670
00:41:57.530 --> 00:42:01.360
k which is relu in this case and p.
Dot.

671
00:42:01.370 --> 00:42:04.310
Hauder product taking the outer multiplication.

672
00:42:04.311 --> 00:42:09.140
We're doing an outer product of these two values and hold on a second.

673
00:42:10.250 --> 00:42:13.970
No,
I'm getting deep.
H

674
00:42:17.090 --> 00:42:20.300
eph lessening.
Okay,
great.
That's what I'm talking about.

675
00:42:21.170 --> 00:42:23.110
And then DW one,

676
00:42:23.660 --> 00:42:28.520
now we're going to compute the derivative with respect to weight one and using

677
00:42:28.521 --> 00:42:31.580
hidden states transpose and the input observation.

678
00:42:32.090 --> 00:42:36.710
And then we finally returned both derivatives to update weights.

679
00:42:36.920 --> 00:42:38.720
Okay.
Return

680
00:42:43.110 --> 00:42:45.360
both of these derivatives to update the weights.

681
00:42:47.780 --> 00:42:49.010
Great thing about key value pairs.

682
00:42:49.011 --> 00:42:51.860
We can just assign them just like this in in one line.

683
00:42:52.400 --> 00:42:55.340
And it's great because we only have two layers.
Right.
Okay.

684
00:42:56.200 --> 00:42:57.033
<v 3>Okay.</v>

685
00:42:59.310 --> 00:43:00.450
<v 1>And now we're going to get started with the code.</v>

686
00:43:00.451 --> 00:43:03.270
But let me answer two questions cause there's inevitably going to be some,

687
00:43:04.380 --> 00:43:08.610
is it possible to create an opening I environment that is not a game.
Absolutely.

688
00:43:08.640 --> 00:43:12.060
That's what universe is great for and that's where they want to move to.

689
00:43:12.061 --> 00:43:16.410
So they'll have some predefined task whether that be sending emails or sending

690
00:43:16.420 --> 00:43:21.150
or clicking through some,
I dunno,
some bullshit like that.
So then we have,

691
00:43:21.180 --> 00:43:26.020
is it,
is it possible to solving yeah.
Solving the,

692
00:43:26.080 --> 00:43:28.450
okay.
I need to

693
00:43:30.190 --> 00:43:34.120
rephrase these questions cause they're not written out correctly possible to

694
00:43:34.121 --> 00:43:34.954
solve.

695
00:43:36.370 --> 00:43:37.170
<v 3>Okay.</v>

696
00:43:37.170 --> 00:43:40.650
<v 1>Is it possible to solve the longest common sub sequence problem using machine</v>

697
00:43:40.651 --> 00:43:45.030
learning?
Yes.
Then how,
okay.
Uh,
so if it's a sequence,

698
00:43:45.031 --> 00:43:48.840
you want to use a recurrent network because it's a time series and because

699
00:43:48.870 --> 00:43:50.670
they're just numeric,
it's just numerical data.

700
00:43:50.671 --> 00:43:54.210
You wouldn't want to use anything fancy like LSTs or gr use.

701
00:43:54.690 --> 00:43:58.560
You just use a standard RNN two more questions.
What is dif?

702
00:43:58.590 --> 00:44:03.590
The difference in handling data that is one d or Two d with something that is

703
00:44:03.780 --> 00:44:06.960
tendi.
Great question.
So,
um,

704
00:44:08.340 --> 00:44:12.630
right,
so 10 days where think about dimensions as features.

705
00:44:12.631 --> 00:44:16.830
So anytime you have 10 features,
those are 10 dimensions.
Uh,
you would want to,

706
00:44:16.920 --> 00:44:18.330
if you're using deep learning,

707
00:44:18.360 --> 00:44:21.030
you could just feed them all into the model just like that.

708
00:44:21.031 --> 00:44:24.810
Because deep networks operate on multidimensional data.
However,

709
00:44:25.020 --> 00:44:28.680
if you're using a standard linear,
a model like linear regression,

710
00:44:28.800 --> 00:44:32.640
you went to squash that dimentionality using dimensionality reduction technique

711
00:44:32.910 --> 00:44:36.480
like principal component analysis or a t.
S.
N.
E.

712
00:44:36.690 --> 00:44:39.210
There's a whole theory of machine learning around that.
One more question.

713
00:44:39.720 --> 00:44:44.410
Why are you finding to derivatives?
We are finding too,
we are finding uh,

714
00:44:45.150 --> 00:44:48.720
so backpropagation and have a great video on backpropagation called

715
00:44:48.721 --> 00:44:51.330
backpropagation in five minutes.
Check it out by the way.

716
00:44:51.840 --> 00:44:56.840
But W backpropagation is essentially recursively computing derivatives with

717
00:44:57.181 --> 00:44:59.310
respect to weights at each layer.

718
00:44:59.520 --> 00:45:03.570
And the reason that we say it's using the chain rule is because we are taking

719
00:45:03.571 --> 00:45:08.571
the values at every layer and using those values to compute that the next set of

720
00:45:09.811 --> 00:45:14.010
derivatives,
partial derivatives which equals the gradients,
Parson derivative,

721
00:45:14.070 --> 00:45:15.120
equal gradient,
same thing,

722
00:45:15.870 --> 00:45:19.560
and then we're using those values that we've updated across the network to then

723
00:45:19.680 --> 00:45:20.513
update our weights.

724
00:45:21.530 --> 00:45:22.040
<v 3>Yeah.</v>

725
00:45:22.040 --> 00:45:22.821
<v 1>One more question actually.</v>

726
00:45:22.821 --> 00:45:25.160
Is it possible to see some [inaudible] plots in real time?

727
00:45:27.560 --> 00:45:28.393
That's a great question.

728
00:45:28.430 --> 00:45:30.980
I don't have one but that would be a great project to do.

729
00:45:30.981 --> 00:45:32.330
So let's get started with the code now,
right?

730
00:45:32.331 --> 00:45:37.331
I mean the actual implementation details because we are,

731
00:45:38.870 --> 00:45:43.100
we've implemented our
help,
our libraries.

732
00:45:43.310 --> 00:45:48.310
Now we're going to get the game started and this is what Jim is good for.

733
00:45:49.070 --> 00:45:52.670
It's also good to get swollen but not that's not this.
That's something else.

734
00:45:53.240 --> 00:45:55.370
And we're going to say,
boy,

735
00:45:55.670 --> 00:45:58.970
we're going to take an observation from the environment and this is why I love

736
00:45:58.971 --> 00:45:59.751
open AI guys,

737
00:45:59.751 --> 00:46:03.950
because all we have to do is run this one function reset and we get the

738
00:46:03.980 --> 00:46:07.790
observation.
And I'm not saying that I'm all about simplicity for the sake of it.

739
00:46:08.000 --> 00:46:09.320
I like hard coding things,

740
00:46:09.410 --> 00:46:13.580
but sometimes when it comes to you pixel values and some games state that I'd

741
00:46:13.581 --> 00:46:14.420
never seen before,

742
00:46:14.630 --> 00:46:19.400
I don't want to have to worry about hand coding this reset method where,

743
00:46:19.460 --> 00:46:21.110
how do I extract an observation,

744
00:46:21.111 --> 00:46:24.200
the Pixel from a game say I just want to focus on the algorithm.

745
00:46:24.260 --> 00:46:26.930
And that's why open AI does what they do.
Do what they do.

746
00:46:27.320 --> 00:46:31.730
That's why they do what they do.
Nike.
Okay.

747
00:46:32.440 --> 00:46:36.680
So,
oh,
okay.
Where were we?
Observation.

748
00:46:37.760 --> 00:46:40.880
I'm,
I'm,
I'm uh,
I'm good.
So,
oh,

749
00:46:42.650 --> 00:46:44.990
I don't know why I'm laughing.
I'm laughing at myself.

750
00:46:45.350 --> 00:46:47.540
So what were you going to do is we're going to say

751
00:46:50.110 --> 00:46:52.090
what we want to calculate motion,
right?

752
00:46:52.900 --> 00:46:57.400
And we can't calculate motion if frames,
our statics frames are static.

753
00:46:57.550 --> 00:46:59.920
We want to calculate motion of the ball.
And then,

754
00:47:00.070 --> 00:47:04.000
and then improve our network based on the motion of the ball.

755
00:47:04.060 --> 00:47:06.970
And how do we do that if frames or static or we're going to calculate the

756
00:47:06.971 --> 00:47:10.660
difference between two frames.
And that difference is what we feed in.

757
00:47:10.900 --> 00:47:14.950
So here's how we calculate the difference.
Um,
we'll do that in a second,

758
00:47:14.951 --> 00:47:19.390
but right now we're just defining this previous ex to a store that previous

759
00:47:19.630 --> 00:47:24.070
value of the difference.
Okay?
So now what we're going to do is we're going to

760
00:47:25.950 --> 00:47:30.950
import some variables to intermediate variables to store the observations,

761
00:47:34.120 --> 00:47:38.290
the hidden state,
the gradient,
and the reward.
So that's what they,

762
00:47:38.770 --> 00:47:43.600
that's what they do,
okay?
And then we have our current reward.

763
00:47:43.630 --> 00:47:45.640
So what is a running reward where we're at?

764
00:47:45.820 --> 00:47:48.370
We're just defining some variables right now before we,
you know,

765
00:47:48.970 --> 00:47:50.440
these aren't hyper parameters,
they're just,

766
00:47:50.770 --> 00:47:54.880
they're there because they don't affect our models.
Architecture.

767
00:47:55.150 --> 00:47:57.090
They're used for,
um,

768
00:47:58.030 --> 00:48:02.380
computing the policy,
the reward,

769
00:48:03.280 --> 00:48:05.020
not the actual network itself.

770
00:48:08.050 --> 00:48:11.380
And then where are we?
The episode number?
Okay?
So let's begin the training.

771
00:48:11.680 --> 00:48:16.210
So began,
began training is training time,
okay,

772
00:48:17.260 --> 00:48:20.710
let's do this.
So for training time

773
00:48:22.870 --> 00:48:26.110
we're going to first uh,
preprocessed the observation.

774
00:48:26.111 --> 00:48:29.950
So get a image,
get frame difference.

775
00:48:29.980 --> 00:48:32.260
I'll just say that get framed difference.
So we're gonna use that.

776
00:48:32.770 --> 00:48:37.510
So we're going to calculate this current value of x using the previous value

777
00:48:38.200 --> 00:48:38.830
later,

778
00:48:38.830 --> 00:48:41.320
but right now we're going to use that and now we're going to use that method,

779
00:48:41.321 --> 00:48:46.090
that preprocessed method that we defined beforehand given an observation and

780
00:48:46.091 --> 00:48:49.720
that's going to make sure that it takes out all the,
like I said before,

781
00:48:49.750 --> 00:48:52.390
the stuff that we don't need and then we're going to say,

782
00:48:52.391 --> 00:48:54.910
well the current value minus,
and this is a different step,

783
00:48:54.970 --> 00:48:57.370
we can literally just say minus.
When I say difference,

784
00:48:57.520 --> 00:49:00.070
I mean it minus the previous value.

785
00:49:01.000 --> 00:49:05.170
If the previous value is not none else,

786
00:49:05.230 --> 00:49:10.000
we're going to say get those Zeros,
initialize it as an empty

787
00:49:11.680 --> 00:49:15.280
matrix and then set the previous to the current value.
It's kind of like a,

788
00:49:16.000 --> 00:49:20.110
it's kind of like a a data structure in and of itself set the previous to the

789
00:49:20.111 --> 00:49:22.450
current and current previous.
You guys know what I'm talking about.

790
00:49:22.451 --> 00:49:26.860
Algorithms class when a one,
so x is going to be our image difference.

791
00:49:26.890 --> 00:49:30.700
We feed it right in.
Okay.
The the X,

792
00:49:30.701 --> 00:49:33.610
not previous ex occurrent x,
but x.
Okay,

793
00:49:33.611 --> 00:49:38.611
so now let's do some forward propagation forward onward to a glorious future,

794
00:49:40.120 --> 00:49:42.250
which is what we're doing.
Okay,

795
00:49:42.251 --> 00:49:47.251
so we're going to say use that policy forward method to remember feeding.

796
00:49:48.490 --> 00:49:51.610
Remember feeding that x value and it's going to give us an output,

797
00:49:52.090 --> 00:49:54.580
a probability as well as they hidden states.

798
00:49:54.700 --> 00:49:59.640
And we're going to sample from that Paul probability and a yes,

799
00:49:59.670 --> 00:50:04.010
it's a,
okay.
It's going to,

800
00:50:04.020 --> 00:50:06.570
we're going to sample from that probability to then update our network with

801
00:50:06.571 --> 00:50:09.660
backpropagation and let me answer one question.

802
00:50:16.880 --> 00:50:18.800
Okay,
so the question is

803
00:50:21.320 --> 00:50:25.600
can we create a neural network for changing the code in a PHP file
automatically?

804
00:50:25.630 --> 00:50:26.920
If so,
will this,

805
00:50:27.320 --> 00:50:30.650
the pattern of the code changed in the past,

806
00:50:31.730 --> 00:50:35.780
changing the code automatically for what purpose?
To optimize it?
Yes.

807
00:50:35.781 --> 00:50:37.370
If you have a label for the code,

808
00:50:38.120 --> 00:50:42.650
if you have some thinking about it as a labeled a supervised classification

809
00:50:42.651 --> 00:50:46.410
problem,
if you have labeled good code,
you need a lot of it too as good or bad.

810
00:50:46.411 --> 00:50:51.190
I think of it as binary.
Yeah.
Okay.
So

811
00:50:52.990 --> 00:50:56.080
now this is this the classic part.
Okay,

812
00:50:56.110 --> 00:50:58.510
so this is the Catholic part in this case is going to be,

813
00:50:58.540 --> 00:51:01.420
let's say we have an action and we're going to say

814
00:51:03.910 --> 00:51:08.080
we're going to sample the action from a uniform,
a uniform distribution,

815
00:51:08.740 --> 00:51:09.910
uh,
which is a,

816
00:51:10.930 --> 00:51:15.490
and then we're going to say it's less than the probability else.
Three.

817
00:51:16.410 --> 00:51:18.580
It's essentially like rolling dice.
Okay.
So what we're doing,

818
00:51:18.581 --> 00:51:21.340
here's where we're saying since it's not a part of the model,

819
00:51:21.341 --> 00:51:24.910
the model is going to be easily differentiable the stochastic part,

820
00:51:25.210 --> 00:51:29.050
the model itself isn't the cast stick,
it's deterministic,
but this part,

821
00:51:29.051 --> 00:51:32.260
the output value,
that sampling of of,
of rewards,

822
00:51:32.740 --> 00:51:35.500
it's going to be to castic.
We're going to randomly pick from them.

823
00:51:35.710 --> 00:51:37.660
Now if this to cast,
this city was in the model,

824
00:51:37.750 --> 00:51:41.740
we have to do a re parameterization technique,
which is super cool.

825
00:51:41.741 --> 00:51:44.830
And I'm making a video on that,
but it's not right now.
Yeah.
Which is super cool.

826
00:51:45.310 --> 00:51:47.920
We're starting to look at deep learning through a Basie and Lens.

827
00:51:48.040 --> 00:51:52.480
That is where the field is moving.
More unpredictability,
more randomness,

828
00:51:53.020 --> 00:51:57.670
a less determinism as we move forward.
Okay,

829
00:51:57.671 --> 00:52:02.671
so now what we're gonna do is we're going to append this d log variable with a

830
00:52:06.431 --> 00:52:08.560
minus the eight y minus the high probability.

831
00:52:08.650 --> 00:52:11.230
So it's a gradient that encourages the action that was,

832
00:52:11.500 --> 00:52:14.950
that was taken to be taken.
Uh,
so

833
00:52:17.970 --> 00:52:18.410
<v 3>okay,</v>

834
00:52:18.410 --> 00:52:19.700
<v 1>let's keep moving forward.</v>

835
00:52:20.300 --> 00:52:23.180
Now we're going to step the environment and get the new measurements,

836
00:52:23.690 --> 00:52:26.480
step the environment.
So we're going to render it.

837
00:52:26.481 --> 00:52:29.180
And this step shows the game constantly.
While true.

838
00:52:29.181 --> 00:52:33.980
It's just going to continuously show the game and then we're going to say,

839
00:52:34.010 --> 00:52:37.670
get the reward,
get the observation,
get the done,
get the info,

840
00:52:37.880 --> 00:52:42.770
and then step.
So what?
No,
this is the standard open AI format.

841
00:52:42.771 --> 00:52:45.560
Whether it's universal gym,
you get the observation,

842
00:52:45.860 --> 00:52:50.210
which is what you've see the reward plus or minus the binary value or zero done.

843
00:52:50.360 --> 00:52:51.550
Which means did the,

844
00:52:51.600 --> 00:52:56.030
the did a reward happened at the game finish for an iteration.

845
00:52:56.031 --> 00:52:57.890
Like there's the ball go forward or whatever it is.

846
00:52:58.310 --> 00:53:01.250
And then info was just for logistics like debugging purposes,

847
00:53:01.251 --> 00:53:05.360
but always do say step and that takes an action in whatever game state it is.

848
00:53:05.990 --> 00:53:09.290
<v 3>Okay.
And uh,</v>

849
00:53:13.250 --> 00:53:16.820
<v 1>okay.
So then we're going to take the reward some</v>

850
00:53:18.890 --> 00:53:21.740
and then we're going to add the reward that we got from it to it.

851
00:53:21.741 --> 00:53:25.860
So the reward is going to store the total all the rewards that we got and we're

852
00:53:25.861 --> 00:53:27.150
going to use that to uh,

853
00:53:30.060 --> 00:53:33.360
to back propagate in a second.
Okay.
And so then,

854
00:53:36.920 --> 00:53:37.753
<v 3>okay,</v>

855
00:53:37.850 --> 00:53:39.200
<v 1>now we're going to record the reward.</v>

856
00:53:40.130 --> 00:53:43.400
It has to be done after we call step to get the reward for the previous action.

857
00:53:44.240 --> 00:53:48.680
Okay?
And now we're going to say if we're done,

858
00:53:49.130 --> 00:53:52.460
if an episode has finished,
then increment that episode number.

859
00:53:52.520 --> 00:53:57.020
And this is just our iterable the variable that we're iterating through to get

860
00:53:57.021 --> 00:53:59.450
that to,
to keep track of where we are.

861
00:53:59.960 --> 00:54:04.010
And now we're going to stack inputs,
hidden states,

862
00:54:04.760 --> 00:54:08.750
actions,
gradients,
rewards.

863
00:54:09.530 --> 00:54:10.370
For this episode,

864
00:54:10.460 --> 00:54:14.450
we're going to stack every single value that we have calculated,
uh,

865
00:54:14.480 --> 00:54:19.220
so that we can later compute a discounted reward backwards through time.

866
00:54:19.520 --> 00:54:23.540
So
remember,
this is a,
uh,

867
00:54:23.720 --> 00:54:25.280
a bunch of,
uh,

868
00:54:26.350 --> 00:54:27.183
<v 3>okay.</v>

869
00:54:27.410 --> 00:54:30.080
<v 1>Games are happening in an episode,
a bunch of,
uh,</v>

870
00:54:31.370 --> 00:54:34.670
plus one scores we could think about it are happening in each episode.

871
00:54:34.940 --> 00:54:37.820
So we'll say if we were to say,

872
00:54:38.390 --> 00:54:43.190
let's stack all the observations and now let's stack all the hidden states and

873
00:54:43.191 --> 00:54:48.110
this function,
just like to stack them all them all in and separate arrays,

874
00:54:48.730 --> 00:54:53.510
um,
those lists or as stacks if we want to be.
Yeah.
Stacks.

875
00:54:54.240 --> 00:54:57.080
Uh,
and then these are all the hidden states that we've computed.

876
00:54:57.260 --> 00:54:59.450
And then all the gradients,
we're just,
we're just,

877
00:54:59.480 --> 00:55:02.990
we're just some summing them all together so that we can use them in a second

878
00:55:03.890 --> 00:55:06.230
and all of them are within an episode.
Okay.

879
00:55:08.210 --> 00:55:11.020
And then we say,
be stack dogs.

880
00:55:12.310 --> 00:55:13.090
<v 3>Okay.</v>

881
00:55:13.090 --> 00:55:16.570
<v 1>These are all of our gradients.
And then we have all of our</v>

882
00:55:20.520 --> 00:55:24.210
rewards.
And then we have,
we're going to reset the array memory.
Uh,

883
00:55:24.480 --> 00:55:28.740
now that we have all of those in our respective stacks,
okay,

884
00:55:29.310 --> 00:55:31.680
now that we have all of those in those respective stacks,

885
00:55:31.950 --> 00:55:36.120
we can move forward and now we're going to compute the discounted reward.
Okay?

886
00:55:36.121 --> 00:55:39.510
So this counted for war computation.
So,

887
00:55:41.390 --> 00:55:46.380
uh,
we have our discounted award that we're going to store in this variable EPR,

888
00:55:46.650 --> 00:55:50.400
okay?
And that,
that is what we've just written out,
that function.

889
00:55:50.640 --> 00:55:52.860
And once we have that,
we're going to standardize that reward.

890
00:55:53.160 --> 00:55:55.800
So it's going to be,
uh,
to be unit normal,

891
00:55:55.830 --> 00:56:00.540
which means it's going to help control the gradient estimators variance,
which,

892
00:56:00.630 --> 00:56:03.090
okay,
so let me explain that once I write that out.

893
00:56:03.240 --> 00:56:06.270
So we're going to say minus equals the mean.

894
00:56:06.360 --> 00:56:09.840
So that's the average value from all of those discounted rewards.

895
00:56:09.900 --> 00:56:11.220
So when average,
once we,

896
00:56:11.550 --> 00:56:14.400
once we've applied this discount function to all of those rewards,

897
00:56:14.610 --> 00:56:19.560
we'll average them all together.
Um,
and that's what these two lines do.
So both,

898
00:56:19.680 --> 00:56:21.000
so,
right?

899
00:56:23.370 --> 00:56:25.890
That's what these two lines in total do some total.

900
00:56:27.240 --> 00:56:32.010
And we're gonna use this discount,
a reward to then compute the,
uh,

901
00:56:32.220 --> 00:56:33.840
gradient values.
So we're going to say

902
00:56:35.280 --> 00:56:37.040
<v 3>P P D</v>

903
00:56:39.520 --> 00:56:41.720
<v 1>can we go 10 minutes over actually,
is that cool?
Yeah,
we'll,
we'll,</v>

904
00:56:41.730 --> 00:56:45.580
we'll go 10 minutes over cause we actually have quite a bit to to finish here

905
00:56:45.940 --> 00:56:47.620
and I want to own and make sure that I talk about this.

906
00:56:47.621 --> 00:56:49.990
I know I'm kind of blowing through this part right now and I'm going to go over

907
00:56:49.991 --> 00:56:52.960
it.
So just so just hold tight.
Um,

908
00:56:54.580 --> 00:56:57.910
so actually what I'm going to do here is I'm going to paste the sand because I

909
00:56:57.911 --> 00:57:00.900
want to explain all of this that's happening here.

910
00:57:00.950 --> 00:57:03.470
So my code is on the get hub by the way.
I am,
I get up.

911
00:57:03.471 --> 00:57:08.471
So let me just paste this part in so I can explain the rest of this and make

912
00:57:08.771 --> 00:57:11.260
sure that we get it all because there's a lot going on here.

913
00:57:12.330 --> 00:57:16.290
<v 3>Okay?
Okay.</v>

914
00:57:17.430 --> 00:57:20.200
Where are we to Todd,
to Todd.

915
00:57:21.050 --> 00:57:25.310
<v 1>And before I started explaining it,
I'll answer one question.
Okay.
So</v>

916
00:57:27.820 --> 00:57:30.320
<v 3>okay.
But that,
that,
that did that</v>

917
00:57:31.370 --> 00:57:33.560
<v 1>boring bookkeeping and all that stuff.</v>

918
00:57:33.680 --> 00:57:37.970
So what we're going to do is,
let me save that.
Let me answer your question.

919
00:57:38.990 --> 00:57:39.823
Question is,

920
00:57:42.550 --> 00:57:44.740
how did you converge Vivec ask,

921
00:57:44.741 --> 00:57:47.530
how did you converge on the steps for preprocessing,

922
00:57:47.531 --> 00:57:51.730
like taking difference of pixel values versus true states or feeding two

923
00:57:51.731 --> 00:57:56.130
consecutive frames?
Oh,

924
00:57:56.131 --> 00:57:57.660
so I didn't converge.
Those are,

925
00:57:57.661 --> 00:58:02.661
those are hand coded a pixel states that like if we were to just guess what the,

926
00:58:03.210 --> 00:58:06.240
so if we were to sample those are pixel values,
four colors.

927
00:58:06.450 --> 00:58:11.450
So we were to say 404140414041 44 would be the RGB value for that orange

928
00:58:11.641 --> 00:58:14.490
background that we then remove because we don't actually care about that.

929
00:58:14.730 --> 00:58:16.710
We care about the ball and we care about the paddles.

930
00:58:17.070 --> 00:58:20.970
So let's talk about what I'm doing here.
So,

931
00:58:22.120 --> 00:58:22.953
<v 3>uh,</v>

932
00:58:25.300 --> 00:58:28.630
<v 1>let's me say this.</v>

933
00:58:31.520 --> 00:58:33.410
Okay.
I'm just gonna start from the beginning.

934
00:58:33.411 --> 00:58:36.260
Let's just start from the beginning and then we'll go forward from there.
Okay.

935
00:58:36.261 --> 00:58:37.094
Cause there's a lot here.

936
00:58:37.860 --> 00:58:38.370
<v 3>Okay.</v>

937
00:58:38.370 --> 00:58:41.490
<v 1>Okay.
So from the beginning,
okay,</v>

938
00:58:41.491 --> 00:58:45.660
we've got our libraries hyper per ams.
Um,
let's see.

939
00:58:45.661 --> 00:58:48.660
What's a good part to start off right here.
So let's say,

940
00:58:48.780 --> 00:58:51.900
let's say we're ready to be pong,
right?
Where we want to be.
Pong.

941
00:58:52.080 --> 00:58:54.360
We've got these values and we want to beat Pong.
Okay.

942
00:58:54.540 --> 00:58:59.040
So we're going to initialize our model as an 80 by 80 pixel matrix of values.

943
00:58:59.041 --> 00:58:59.610
Okay.

944
00:58:59.610 --> 00:59:04.170
And then we initialize our weights a pseudo randomly or not suitor randomly.

945
00:59:04.200 --> 00:59:06.100
The correct term for this would be s

946
00:59:08.590 --> 00:59:13.420
s smarter than random but close to random because they're random.

947
00:59:13.421 --> 00:59:18.421
But there were using these predefined values as bounding values between our

948
00:59:18.581 --> 00:59:21.830
hidden state in our input dimentionality for both of our weights.

949
00:59:21.920 --> 00:59:26.510
And then we're initializing buffers for our gradient and our rms prop this,

950
00:59:26.540 --> 00:59:29.480
these are for backpropagation.
So these are four backpropagation,

951
00:59:30.350 --> 00:59:31.460
the to store values,

952
00:59:31.610 --> 00:59:35.780
intermediate values because backpropagation is a process and he's going to store

953
00:59:35.781 --> 00:59:39.650
values for both our gradients and the computations from uh,

954
00:59:41.230 --> 00:59:44.140
from compute,
from the actual rms proper equation,

955
00:59:44.260 --> 00:59:46.060
which I'm going to show the image of in a second.

956
00:59:46.900 --> 00:59:49.810
We have our activation function sigmoid,
our preprocessing step,

957
00:59:49.811 --> 00:59:50.641
our discount reward.

958
00:59:50.641 --> 00:59:55.641
And so our discount reward is going to weigh the rewards that we get earlier

959
00:59:56.920 --> 01:00:01.540
higher than later rewards because we are optimizing for short term rewards

960
01:00:01.570 --> 01:00:05.190
because Paul is a very fast paced game.
Then recomputing,

961
01:00:05.200 --> 01:00:09.370
r p r a them are competing for propagation,
which is essentially,

962
01:00:09.400 --> 01:00:13.690
remember it's a chain of matrix multiplication.
It's,
it's linear Algebra,
inputs,

963
01:00:13.840 --> 01:00:16.870
times,
layer times,
layer times,
layer times layer.

964
01:00:17.080 --> 01:00:21.130
And all of those outputs are then fed into the next layers and this is the chain

965
01:00:21.131 --> 01:00:24.610
of operations to get that initial,
that final value,
those,

966
01:00:24.850 --> 01:00:29.680
those final sets of weights
that we could then uh,

967
01:00:31.640 --> 01:00:34.760
uh,
get the policy from,
which is the reward.

968
01:00:35.100 --> 01:00:38.870
We're going to get an output probability,
which is going to be either up or down.

969
01:00:39.530 --> 01:00:44.270
Let me show a great image of this.
An image would help here.
Um,

970
01:00:44.300 --> 01:00:47.120
so deep a policy gradients.

971
01:00:51.370 --> 01:00:52.203
<v 0>It's a great one.</v>

972
01:00:53.390 --> 01:00:55.520
<v 1>What's a great image?
So</v>

973
01:00:59.480 --> 01:01:03.190
this is a great image.
Okay,
hold on.

974
01:01:03.640 --> 01:01:04.840
So where are we?

975
01:01:08.670 --> 01:01:09.990
Okay,
cool image right here.

976
01:01:10.470 --> 01:01:13.530
So this is what the network is doing in her forward pass.

977
01:01:13.640 --> 01:01:18.360
We are going to output a set of log problem of we could say log probabilities.

978
01:01:18.600 --> 01:01:21.360
Actually these values can be anything.
They can be 10,
it could be a million.

979
01:01:21.600 --> 01:01:24.870
But all that matters is if it's positive or negative,

980
01:01:25.590 --> 01:01:29.100
it doesn't actually matter what the value is,

981
01:01:29.101 --> 01:01:30.720
the magnitude of that matters.

982
01:01:31.590 --> 01:01:34.680
But we're s we're going to output a set of probabilities and those probabilities

983
01:01:34.681 --> 01:01:37.790
are going to say there's a probability that we should move up versus a problem

984
01:01:38.040 --> 01:01:39.360
probability that we should move down.

985
01:01:39.570 --> 01:01:42.690
And then we're going to sample randomly from that probability distribution and

986
01:01:42.691 --> 01:01:47.520
say,
should we go up or down and then use that,
that sample like,

987
01:01:47.521 --> 01:01:51.360
okay,
let's say best probability is to go up.
That's our action.

988
01:01:51.660 --> 01:01:55.770
We're going to then implement that action and then get a reward value from the

989
01:01:55.771 --> 01:01:56.580
game states.

990
01:01:56.580 --> 01:01:59.940
Remember that environment step function is gonna Return or reward value.

991
01:02:00.330 --> 01:02:04.620
And then we're going to use that reward value to then update our,
uh,
our,

992
01:02:04.860 --> 01:02:09.510
our network by using those as our gradients.
Okay?

993
01:02:09.511 --> 01:02:13.770
So we're going to use those reward values to then update our weights recursively

994
01:02:13.860 --> 01:02:16.950
as we back propagate.
So that's why it's called policy gradients.

995
01:02:17.100 --> 01:02:19.530
They're not gradients from,
uh,
some,

996
01:02:20.160 --> 01:02:22.380
it's not because it's not a labeled supervise problem.

997
01:02:22.710 --> 01:02:27.600
They are grains that we received from the reward value from the reward that we,

998
01:02:28.050 --> 01:02:32.760
we get from our policy.
That is our,
should we take,
should we take a,

999
01:02:33.290 --> 01:02:35.850
uh,
an action here or not and what action should we take?

1000
01:02:35.851 --> 01:02:39.690
What is the policy in this environment and how should we then use the rewards

1001
01:02:39.691 --> 01:02:44.040
that we receive to update our network?
What is the gradient?

1002
01:02:44.130 --> 01:02:48.510
Remember gradients give us a direction to update our network during

1003
01:02:48.511 --> 01:02:53.500
backpropagation.
Okay.
So that's a great image for that.
And then,
uh,

1004
01:02:53.910 --> 01:02:57.990
we're going to say,
let's initializing variables to get started with this.

1005
01:02:58.020 --> 01:03:00.870
And then we're going to say,
okay,
let's begin the training.

1006
01:03:01.020 --> 01:03:02.850
We've got the difference between our frames.

1007
01:03:06.810 --> 01:03:07.300
<v 0>Yeah.</v>

1008
01:03:07.300 --> 01:03:10.030
<v 1>Okay.
Because we want to detect motion,
which is going to be an ex.</v>

1009
01:03:10.240 --> 01:03:11.410
We feed that x value,

1010
01:03:11.411 --> 01:03:15.970
which is a matrix of pixels into our foot policy forward a function,

1011
01:03:16.210 --> 01:03:19.110
which is then going to output a probability of either moving up or down.
It's,

1012
01:03:19.180 --> 01:03:21.280
it's going to output both.
And then we're,

1013
01:03:21.490 --> 01:03:26.490
we are tasked then with choosing which a probability,

1014
01:03:26.880 --> 01:03:30.010
uh,
to then pick.
It's not gonna just going to output one probability.

1015
01:03:30.011 --> 01:03:32.260
It's going to say a probability for up and down.

1016
01:03:32.290 --> 01:03:36.540
And then we have to pick which one.
And then the,

1017
01:03:36.570 --> 01:03:40.600
and this is how we pick,
it's the cast Stickley randomly.
And then we then,

1018
01:03:41.470 --> 01:03:45.340
and then this is going to get better,
not the random part,
but the actual outputs,

1019
01:03:46.120 --> 01:03:48.760
uh,
like we're going to randomly pick,
okay,
this one,
this,

1020
01:03:48.761 --> 01:03:52.820
this is a good probability,
right?
And then when we update our network,
the,

1021
01:03:52.830 --> 01:03:55.080
the magnitude of those probabilities in,

1022
01:03:55.130 --> 01:03:59.650
in either direction for either up or down will increase as to maximize that

1023
01:03:59.651 --> 01:04:00.520
short term reward.

1024
01:04:00.640 --> 01:04:04.120
So it doesn't matter if we're randomly picking from them because the magnitude

1025
01:04:04.150 --> 01:04:08.240
in a certain direction has increased.
So then no matter what your sampling,
what,

1026
01:04:08.470 --> 01:04:11.410
no matter if you're sampling from them randomly or deterministically,

1027
01:04:11.620 --> 01:04:15.250
you're still going to pick the one that is the,
that is the most likely.

1028
01:04:16.600 --> 01:04:19.510
And then we stepped through the environment.
We say,
okay,
what is our award value?

1029
01:04:19.840 --> 01:04:23.400
And then we say,
let's begin,
uh,

1030
01:04:23.500 --> 01:04:28.390
the actual episode.
And we're stacking all the inputs in the hidden states,

1031
01:04:28.391 --> 01:04:31.450
in the actions,
all the values that were computing into Iras.

1032
01:04:31.630 --> 01:04:33.710
Because we're going to use all of them.
Uh,

1033
01:04:33.740 --> 01:04:36.220
were we want to use all of them and we want to,

1034
01:04:36.221 --> 01:04:40.870
it's kind of like work boxing them all together and into one state,

1035
01:04:41.050 --> 01:04:44.980
one Maddis state so that we could then,
uh,
back propagate our network

1036
01:04:46.750 --> 01:04:48.670
and then we're computing the discounted reward.

1037
01:04:48.730 --> 01:04:53.260
Remember optimizing for the short term,
using those values,
um,
or sorry,

1038
01:04:53.261 --> 01:04:58.261
using the a gradiance values and then we're competing the advantage.

1039
01:04:59.471 --> 01:05:02.500
So let me,
so this is the part that I didn't explain yet,
right?

1040
01:05:02.501 --> 01:05:06.940
So the advantage is the quantity that describes how good and action is compared

1041
01:05:06.941 --> 01:05:11.560
to the average of all the actions.
Okay.
So the,

1042
01:05:12.250 --> 01:05:15.610
so this is our grading,
right?
EPD Log.

1043
01:05:15.611 --> 01:05:19.870
Pete is our gradients and discounted EPR is going to act as our advantage

1044
01:05:19.930 --> 01:05:24.250
because it's giving an advantage to the one to the rewards that are short term.

1045
01:05:24.251 --> 01:05:28.480
So the way we do that,
it's just simple matrix,
matrix multiplication.
Okay.

1046
01:05:28.481 --> 01:05:32.240
And then,
and then once we have that,
where we've,
we've,
we've,
uh,

1047
01:05:32.560 --> 01:05:35.590
added that reward that discounted wards not added,

1048
01:05:35.591 --> 01:05:37.180
but multiplied the discounted war two,

1049
01:05:37.181 --> 01:05:41.920
our probability we can then backward propagate.
We've,

1050
01:05:41.921 --> 01:05:43.470
we've taken what we've,
uh,

1051
01:05:43.500 --> 01:05:46.810
our reward from our action in the environment and now we're applying it to that

1052
01:05:46.900 --> 01:05:51.850
backwards policy to get our gradient value.
And then,
uh,

1053
01:05:51.880 --> 01:05:54.790
we're gonna get to the final gradients that we are going to have good to use to

1054
01:05:54.820 --> 01:05:57.670
update our weights using gradient descent.

1055
01:05:57.760 --> 01:06:02.590
And so this is the rms prop step and you can see here that there are some weird

1056
01:06:02.620 --> 01:06:05.080
a matrix.
There's some weird math operations here,

1057
01:06:05.230 --> 01:06:10.230
like w the decay rate times the cash of the RMS prop plus one minus the decay

1058
01:06:10.631 --> 01:06:15.340
rate times the gradient buffer squared.
Well guess what,

1059
01:06:15.580 --> 01:06:18.670
what this is,
is it is the formula for rms prop.

1060
01:06:18.910 --> 01:06:22.240
Now there are a bunch of formulas for gradient descent and all of them are

1061
01:06:22.241 --> 01:06:24.970
different and all of them have different,
different use cases.

1062
01:06:24.971 --> 01:06:28.930
But rms prop is stochastic gradient descent.

1063
01:06:28.931 --> 01:06:31.660
And I should do a video on stochastic gradient descent alone.

1064
01:06:32.050 --> 01:06:35.170
But this is the formula and what we are doing here.
And then you can see here,

1065
01:06:35.320 --> 01:06:39.700
one minus this term times,
uh,
g squared.

1066
01:06:39.910 --> 01:06:43.030
We just implemented this formula and it stays the same programmatically.

1067
01:06:43.031 --> 01:06:47.770
That's what we did and rms prop is,
is,
is,
is,
is this set of

1068
01:06:49.450 --> 01:06:52.330
equations and it's a,
it's a relatively,

1069
01:06:52.331 --> 01:06:57.190
I know it doesn't look simple here because we have to think about this in terms

1070
01:06:57.191 --> 01:06:58.720
of these mathematical terms as well,

1071
01:06:58.900 --> 01:07:02.410
but it's a simple equation and I actually prefer looking at it programmatically

1072
01:07:02.411 --> 01:07:06.250
to the actual equation with,
with all of those terms,

1073
01:07:06.530 --> 01:07:09.490
it's actually more intuitive this way because I look at code most of the time

1074
01:07:09.491 --> 01:07:13.660
anyway and you probably do as well as a developer.
So,
uh,

1075
01:07:13.661 --> 01:07:17.860
we get that gradient value and then we use the decay rate to only update those

1076
01:07:17.861 --> 01:07:19.480
values that,
um,

1077
01:07:21.430 --> 01:07:26.170
that the decay rate,
make sure that as we back propagate over time,

1078
01:07:26.550 --> 01:07:30.360
uh,
it's the values that we,
uh,

1079
01:07:30.490 --> 01:07:35.490
update closer to now are more important than the values that we update later on.

1080
01:07:36.970 --> 01:07:41.050
And the decay rate is a mathematical representation of that and it relates to

1081
01:07:41.051 --> 01:07:42.370
the discounted reward.

1082
01:07:43.510 --> 01:07:46.660
And then we take our model and we update it using the learning rate times that

1083
01:07:46.661 --> 01:07:49.150
value g and then we reset the bat.

1084
01:07:49.210 --> 01:07:51.910
And this is our resetting step once we'd done that.

1085
01:07:51.911 --> 01:07:56.200
But this step right here is the,
is the key step of us updating our models.

1086
01:07:56.680 --> 01:07:57.371
Wait values.

1087
01:07:57.371 --> 01:08:00.190
This is the key step right here using our learning rate and what we've

1088
01:08:00.430 --> 01:08:04.600
calculated with RMS prop.
And this is book bookkeeping of just,
you know,

1089
01:08:04.601 --> 01:08:09.400
putting those values and then,
uh,
how putting them,
okay.
So,

1090
01:08:09.910 --> 01:08:14.740
uh,
I'm going to really comment this.
Get hub read me when I,
um,

1091
01:08:16.790 --> 01:08:21.680
when I do,
when I,
when I do it,
but when I,
when I do it,

1092
01:08:22.040 --> 01:08:26.540
when I do it,
okay.
So look at it.

1093
01:08:26.541 --> 01:08:30.470
It's cool.
It's fun.
It's,
it's,
it's something that is fun to look at.

1094
01:08:30.930 --> 01:08:32.810
Run this on the cloud.
Okay.
If you,

1095
01:08:32.811 --> 01:08:35.950
if you want to actually see it get better or you could run it on your Mac book

1096
01:08:35.960 --> 01:08:39.140
for three days or whatever,
if that's fine too.
But,
but uh,

1097
01:08:39.170 --> 01:08:43.370
but an image on Amazon machine image a bit bucket has a good one.
Um,

1098
01:08:43.400 --> 01:08:46.820
you'd asked the actually has one as well that is pre compiled for you where you

1099
01:08:46.821 --> 01:08:49.690
don't have to install the dependencies.
You can just,
you know,

1100
01:08:49.940 --> 01:08:53.270
run your code directly in a Jupiter notebook in the browser.
That'd be great.

1101
01:08:53.271 --> 01:08:55.430
And I should actually do a video on that and that's coming up soon.

1102
01:08:55.940 --> 01:08:59.330
But this is dope.
Okay.
And this was a lot to take in.
Okay.

1103
01:08:59.331 --> 01:09:01.640
So don't worry if you didn't understand every single thing.

1104
01:09:01.910 --> 01:09:04.220
We just did something pretty complicated.

1105
01:09:04.400 --> 01:09:09.140
We computed backpropagation for propagation,
a grading updates policy,

1106
01:09:09.500 --> 01:09:12.980
uh,
and a policy by hand.
And this is,
and just to,

1107
01:09:12.981 --> 01:09:15.200
just for learning purposes and it's awesome.

1108
01:09:15.201 --> 01:09:18.140
And we're going to do more stuff like this and then later on.
Okay.

1109
01:09:18.141 --> 01:09:20.900
So we're going,
I'm going to start to wrap up with two more questions,

1110
01:09:21.440 --> 01:09:25.850
which is a library to read raw pixels.
Um,
Oh shit,
right?

1111
01:09:26.460 --> 01:09:31.040
Uh,
which is the library to read the raw pixels.
Um,
what was it?

1112
01:09:31.100 --> 01:09:32.390
In this case it was

1113
01:09:36.040 --> 01:09:39.100
boom observation.
Open Ai is going to give you the raw pixels.

1114
01:09:39.400 --> 01:09:40.660
And then one more question.

1115
01:09:40.661 --> 01:09:44.440
Is the input of our model a frame or a sequence of frames video?
I mean,

1116
01:09:44.441 --> 01:09:47.860
will the NMB able to perceive the ball moving or will decide based on a single

1117
01:09:47.861 --> 01:09:50.620
image?
It is the difference between two frames.

1118
01:09:50.740 --> 01:09:55.570
That difference is that is,
it's how we capture motion and we continuously feed,

1119
01:09:55.810 --> 01:09:58.810
um,
the difference between frames.
Okay.
So that's it for this live session.

1120
01:09:58.811 --> 01:10:03.190
Thank you guys for showing up.
I love you guys.
And for now I've got to,

1121
01:10:04.420 --> 01:10:09.370
uh,
stochastically determine my life,

1122
01:10:09.460 --> 01:10:13.510
which is a paradox,
which is also my life.

1123
01:10:13.600 --> 01:10:14.620
So thanks for watching.

