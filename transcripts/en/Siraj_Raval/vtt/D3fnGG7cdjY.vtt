WEBVTT

1
00:00:01.260 --> 00:00:05.260
Deep learning on three d objects.
Finally,
hello world.

2
00:00:05.280 --> 00:00:07.290
It's Saroj and geometric.

3
00:00:07.291 --> 00:00:12.291
Deep learning is an emerging field of machine learning that's able to learn from

4
00:00:12.421 --> 00:00:17.421
complex types of data like graphs and three d objects in this tutorial video

5
00:00:18.421 --> 00:00:23.421
will learn how to use geometric deep learning to classify groups with similar

6
00:00:23.701 --> 00:00:28.701
interests in a social network and apply it to 3d object classification as well.

7
00:00:29.670 --> 00:00:30.930
In the last decade,

8
00:00:30.960 --> 00:00:35.960
deep learning algorithms like convolutional neural networks and recurrent neural

9
00:00:36.091 --> 00:00:41.091
networks have allowed us to achieve unprecedented performance on a broad range

10
00:00:41.701 --> 00:00:45.570
of problems.
We can now classify images,
recognize speech,

11
00:00:45.660 --> 00:00:47.910
translate languages,
generate pickup lines,

12
00:00:47.911 --> 00:00:52.590
and some of these metrics have incredibly already surpassed human capability.

13
00:00:52.920 --> 00:00:55.800
It's been incredible to see the progress that's happened,

14
00:00:55.801 --> 00:01:00.360
but a lot of the algorithmic techniques that have been used to achieve these

15
00:01:00.361 --> 00:01:02.820
breakthroughs are actually pretty old.

16
00:01:03.090 --> 00:01:07.440
What's been the real catalyst for the deep learning revolution so far has been

17
00:01:07.441 --> 00:01:11.940
the availability of new types of datasets that have been generated,

18
00:01:12.180 --> 00:01:16.590
whether it be the Atari Dataset for deep minds,
deep cue learning algorithm,

19
00:01:16.620 --> 00:01:20.100
or the chess game dataset for IBM's deep blue victory.

20
00:01:20.490 --> 00:01:22.680
Also gps and Hinton's blessings.

21
00:01:23.190 --> 00:01:26.370
The data set point is important because in the past few years,

22
00:01:26.490 --> 00:01:31.110
we're seeing more and more of a special type of Dataset.
Three d objects,

23
00:01:31.350 --> 00:01:35.280
thanks to companies that have democratized tools like the connect that are able

24
00:01:35.281 --> 00:01:39.540
to capture three d point clouds of objects,
not just two d images.

25
00:01:39.720 --> 00:01:44.460
We now have a wide variety of three d objects available to train models on.

26
00:01:44.640 --> 00:01:45.480
Once we do that,

27
00:01:45.481 --> 00:01:49.320
we can classify them however we'd like or even generate new objects.

28
00:01:49.350 --> 00:01:52.260
Graphs are another type of emerging datasets.

29
00:01:52.410 --> 00:01:57.410
We can consider social networks as graphs where the characteristics of users can

30
00:01:57.691 --> 00:02:00.480
be modeled as signals on its vertices.

31
00:02:00.760 --> 00:02:02.880
The sensor networks are another example.

32
00:02:02.940 --> 00:02:07.530
Distributed interconnected sensors have readings that can be modeled as time

33
00:02:07.531 --> 00:02:10.800
dependent signals on graph courtesies.
In genetics,

34
00:02:10.830 --> 00:02:15.450
gene expression data is modeled as signals defined in the regulatory network.

35
00:02:15.750 --> 00:02:20.370
Graph models are also used to represent anatomical and functional structure in

36
00:02:20.371 --> 00:02:21.204
the brain.

37
00:02:21.390 --> 00:02:25.200
You would think that we could just feed this type of data to a deep neural

38
00:02:25.201 --> 00:02:29.460
network and assume that it would be able to properly parse it,
right?
Ah,

39
00:02:29.550 --> 00:02:34.170
ah.
Neural networks aren't that great at interpreting this type of data.

40
00:02:34.470 --> 00:02:39.420
The reason being deep learning generally works well on what's called Euclidean

41
00:02:39.421 --> 00:02:44.370
data graphs and three d objects that those are considered non Euclidean
datasets.

42
00:02:44.580 --> 00:02:45.900
Let's break down the difference.

43
00:02:46.050 --> 00:02:51.050
Euclid was a Greek mathematician who wrote a book called the elements over 2000

44
00:02:51.421 --> 00:02:56.421
years ago where he outlined the geometric properties of objects that exist in

45
00:02:56.941 --> 00:02:58.770
flat two dimensional planes.

46
00:02:59.020 --> 00:03:02.320
That's why Euclidean geometry is also known as plain geometry.

47
00:03:02.710 --> 00:03:06.220
Euclidean geometry is pretty straightforward,
pun intended,
lol.

48
00:03:06.520 --> 00:03:11.260
It has its own set of rules like the interior angles of a triangle add up to 180

49
00:03:11.261 --> 00:03:15.220
degrees and two parallel lines will never cross and the shortest distance

50
00:03:15.221 --> 00:03:18.510
between two points will always be a straight line.
You could.

51
00:03:18.511 --> 00:03:23.440
Ian Geometry is still practical and has been used by modern engineers to design

52
00:03:23.441 --> 00:03:28.330
buildings,
predict the location of moving objects,
not die,
and survey land.

53
00:03:28.690 --> 00:03:31.780
We can also use it to help us define data.
For example,

54
00:03:31.810 --> 00:03:35.440
we can consider an image as a function over a two dimensional plane.

55
00:03:35.860 --> 00:03:39.880
This function can help us define the image intensity at a specific coordinate.

56
00:03:39.910 --> 00:03:44.110
On the two d image plane and if we need the value of the image intensity at

57
00:03:44.111 --> 00:03:48.280
another pixel,
a certain number of units along the x direction,

58
00:03:48.550 --> 00:03:51.340
we can just plug that new coordinate set into our function.

59
00:03:51.790 --> 00:03:55.630
This property is very useful if we want to define a convolutions.

60
00:03:55.930 --> 00:03:59.710
Convolutions are one of the main building blocks of convolutional neural

61
00:03:59.711 --> 00:04:03.190
networks.
The type of network as suited for image processing.

62
00:04:03.550 --> 00:04:08.550
The term convolution refers to the mathematical combination of two functions to

63
00:04:08.591 --> 00:04:12.340
produce a third function.
It merges two sets of information.

64
00:04:12.670 --> 00:04:14.230
In the case of a CNN,

65
00:04:14.290 --> 00:04:18.430
the convolution is performed on the input data with the use of a filter to then

66
00:04:18.431 --> 00:04:19.600
produce a feature map.

67
00:04:19.990 --> 00:04:24.460
We execute this convolution by sliding the filter over the input at every

68
00:04:24.461 --> 00:04:28.960
location a matrix multiplication is performed and some of the results onto the

69
00:04:28.961 --> 00:04:29.794
feature map.

70
00:04:29.830 --> 00:04:34.000
It's one of several operations that make these networks work really well on

71
00:04:34.001 --> 00:04:38.350
image data,
but if we curve the image so it becomes a three d object,

72
00:04:38.500 --> 00:04:42.010
we can't just convert solve around the three d shape using vectors like we did

73
00:04:42.011 --> 00:04:43.180
for three d images.

74
00:04:43.330 --> 00:04:46.690
That means we need to redefine the whole notion of a convolution.

75
00:04:47.080 --> 00:04:51.760
Non Euclidean geometry encompasses this type of data,
whether it's a sphere,

76
00:04:51.820 --> 00:04:55.930
a shapeless three mass like ditto in Pokemon or a graph of some kind.

77
00:04:56.350 --> 00:05:00.850
A lot of the most interesting data types lie in this category from protein

78
00:05:00.851 --> 00:05:05.620
interaction networks to knowledge graphs to the entire worldwide web.

79
00:05:05.950 --> 00:05:10.450
The non Euclidean nature of this data implies that there are no familiar

80
00:05:10.451 --> 00:05:15.451
properties like a common system of coordinates vector space structure or shift

81
00:05:15.521 --> 00:05:16.360
in variants.

82
00:05:16.540 --> 00:05:20.740
We need a neural architecture that can learn from non Euclidean data with the

83
00:05:20.741 --> 00:05:25.270
kind of accuracy that CNN give us for it.
Euclidean data enter the graph.

84
00:05:25.271 --> 00:05:29.440
Convolutional network or GCN GCS are very powerful models,

85
00:05:29.441 --> 00:05:33.790
so powerful in fact that even a randomly initialized to layer GCN can produce

86
00:05:33.791 --> 00:05:36.940
useful feature representations of nodes in a given network.

87
00:05:37.270 --> 00:05:41.260
We can think of a general graph based learning problem in the following way.

88
00:05:41.590 --> 00:05:46.060
We're given a set of nodes each with an observed a number of numeric attributes

89
00:05:46.240 --> 00:05:49.090
for each node.
We'd like to predict an output label.

90
00:05:49.240 --> 00:05:52.510
We have labels for some nodes,
but not all nodes were.

91
00:05:52.511 --> 00:05:57.130
Also given a set of weighted edges summarize by an adjacency matrix.

92
00:05:57.170 --> 00:06:01.640
The main assumption is that when predicting the output for a know the attributes

93
00:06:01.641 --> 00:06:06.320
and conductivity of nearby nodes provide useful contextual information,

94
00:06:06.710 --> 00:06:11.180
gcs can solve this problem.
They are neural networks that can operate on graphs.

95
00:06:11.300 --> 00:06:15.430
A good way to imagine what's happening is to consider a neural network that were

96
00:06:15.431 --> 00:06:20.060
seized as input a set of features from all nodes in the local neighborhood

97
00:06:20.090 --> 00:06:24.470
around a node and outputs and estimates of the associated label.

98
00:06:24.860 --> 00:06:28.730
The information from the local neighborhood gets combined over the layers via

99
00:06:28.731 --> 00:06:32.150
the concept of graph convolutions.
The deeper the network,

100
00:06:32.180 --> 00:06:33.800
the larger the local neighborhood.

101
00:06:34.160 --> 00:06:38.480
We can think of it as the generalization of the receptive field of a neuron in a

102
00:06:38.481 --> 00:06:39.590
normal CNN.

103
00:06:39.890 --> 00:06:43.730
This network is applied convolutional Lee across the entire graph,

104
00:06:43.790 --> 00:06:47.840
always receiving features from the relevant neighborhood around each node.

105
00:06:47.960 --> 00:06:52.670
Let's talk more about how these graph convolutions are defined by applying a GCN

106
00:06:52.730 --> 00:06:56.750
to a real world graph.
We have a social network dataset here,

107
00:06:56.751 --> 00:07:01.280
a football enthusiasts that are divided into two groups,
fc Barcelona fans,

108
00:07:01.281 --> 00:07:04.580
and Real Madrid fans,
and this is represented as a graph.

109
00:07:04.880 --> 00:07:08.120
Our Selena is the better team,
but that has nothing to do with this.

110
00:07:08.420 --> 00:07:12.620
Nodes represent members of the network and the edges are there at mutual

111
00:07:12.621 --> 00:07:16.910
relations.
The leader of both groups is denoted by a respective letter.

112
00:07:16.970 --> 00:07:21.440
We can build our graph convolutional network by first initializing it at random

113
00:07:21.441 --> 00:07:23.330
to produce feature representations.

114
00:07:23.660 --> 00:07:28.160
Then we'll stack our GCN layers using the identity matrix as a feature

115
00:07:28.161 --> 00:07:29.030
representation.

116
00:07:29.300 --> 00:07:34.130
So each node is represented as a one hot encoded categoric variable.
Our GCN,

117
00:07:34.131 --> 00:07:37.490
we'll take as input a feature matrix that includes a number of nodes and the

118
00:07:37.491 --> 00:07:39.620
number of input features for each note.

119
00:07:39.680 --> 00:07:43.970
It also takes as input the matrix representation of the graph structure.

120
00:07:44.270 --> 00:07:48.830
Each hidden layer of the network corresponds to a feature matrix where each row

121
00:07:48.831 --> 00:07:52.520
is a feature representation of a node.
At each layer,

122
00:07:52.521 --> 00:07:56.690
these features are aggregated to form the next layers features using a

123
00:07:56.691 --> 00:07:57.800
propagation rule.

124
00:07:58.040 --> 00:08:03.040
So features become increasingly more abstract at each consecutive layer are

125
00:08:03.171 --> 00:08:07.460
simple propagation rule consists of the weight matrix for a layer and a

126
00:08:07.461 --> 00:08:09.950
nonlinear activation function relu.

127
00:08:10.250 --> 00:08:13.910
So the size of the second dimension of the weight Matrix determines the number

128
00:08:13.911 --> 00:08:15.530
of features in the next layer.

129
00:08:15.920 --> 00:08:19.700
This is reminiscent of the convolutional filtering operation from regular

130
00:08:19.701 --> 00:08:21.140
convolutional networks.

131
00:08:21.320 --> 00:08:24.950
In that the weights are shared across nodes in the graph.

132
00:08:24.980 --> 00:08:29.180
We can easily extract the feature representations from the graph and plot it in

133
00:08:29.181 --> 00:08:30.140
a few lines.

134
00:08:30.410 --> 00:08:35.210
It cleanly separated both sides and we haven't even started the training process

135
00:08:35.211 --> 00:08:38.000
yet.
If we start training our output graph,

136
00:08:38.001 --> 00:08:43.001
we'll get way more accurate and this demonstrates the power of gcs pretty dope,

137
00:08:43.311 --> 00:08:47.630
right?
But social network graphs aren't the only type of Euclidean datasets.

138
00:08:47.750 --> 00:08:52.670
We can also use gcs on three d objects if we consider them as point cloud data

139
00:08:52.940 --> 00:08:57.450
point clouds are just a set of points represented 3-d either Xyz locations.

140
00:08:57.450 --> 00:08:59.850
It's what hardware like the connect generates.

141
00:09:00.000 --> 00:09:02.700
It's computing not just pixel data,
but depth as well.

142
00:09:02.701 --> 00:09:04.830
So there's a third dimension involved.

143
00:09:05.040 --> 00:09:09.270
If we consider the points in a point cloud as nodes in a directed graph,

144
00:09:09.450 --> 00:09:11.250
we can apply gcs to them.

145
00:09:11.550 --> 00:09:14.430
There has been several work in this space just this year.

146
00:09:14.431 --> 00:09:19.431
A paper titled Dynamic Graph CNNS use a GCN to improve feature extraction from

147
00:09:20.011 --> 00:09:24.300
point cloud data.
There are several variations here of Splat net point net.

148
00:09:24.510 --> 00:09:28.020
There are different types of graph convolutions and different pooling

149
00:09:28.050 --> 00:09:31.650
architectures designed to work well with point cloud data.

150
00:09:31.890 --> 00:09:36.720
This space is moving fast so expect to see even more improvements in the next

151
00:09:36.720 --> 00:09:37.110
few months,

152
00:09:37.110 --> 00:09:40.710
but we can already use several of the freely available models on get hub

153
00:09:40.711 --> 00:09:43.350
ourselves to build products and services.

154
00:09:43.440 --> 00:09:45.660
There are three things to remember from this video.

155
00:09:45.900 --> 00:09:49.950
Deep learning generally does really well at extracting useful feature

156
00:09:49.951 --> 00:09:52.920
representations from Euclidean datasets.

157
00:09:53.160 --> 00:09:56.430
Euclidean data follows the laws of Euclidean geometry.

158
00:09:56.431 --> 00:10:00.180
It's grid like and follows rules involving straight lines and points,

159
00:10:00.420 --> 00:10:05.190
geometric deep learning models like graph convolutional networks help learn from

160
00:10:05.220 --> 00:10:10.220
non Euclidean data like graphs and three d objects by introducing an ordering of

161
00:10:10.231 --> 00:10:14.850
mathematical operators that are different than in classical convolutional
layers.

162
00:10:14.970 --> 00:10:16.980
In this video,
give you an idea for a startup,

163
00:10:17.010 --> 00:10:18.600
share it with us in the comment section,

164
00:10:18.870 --> 00:10:21.630
and please subscribe for more programming videos.
For now,

165
00:10:21.660 --> 00:10:24.600
I've got to visit the fifth dimension,
so thanks for watching.

