WEBVTT

1
00:00:00.270 --> 00:00:04.800
Hi,
I'd like to buy this phone.
We only accept doge coin.
What?

2
00:00:06.570 --> 00:00:08.250
Hello world.
It's so Raj.

3
00:00:08.280 --> 00:00:12.570
And let's build a simple bitcoin trading bot using machine learning.

4
00:00:12.990 --> 00:00:17.400
Crypto currencies are generally considered high risk investments.

5
00:00:17.430 --> 00:00:22.430
So many traditional finance and Economic Gurus from the famous investor Warren

6
00:00:23.251 --> 00:00:28.251
Buffet's to the Nobel Prize winning economist Joseph Stiglitz have said that

7
00:00:28.411 --> 00:00:33.411
Bitcoin is a scam with no substantial worth that will soon experience a crash as

8
00:00:34.381 --> 00:00:38.460
bad as the 17th century tulip mania that overtook the Netherlands.

9
00:00:38.820 --> 00:00:42.450
If we look at the chart of Bitcoin's price over the years,

10
00:00:42.720 --> 00:00:47.340
we can see that there have been several mini crashes throughout its history.

11
00:00:47.730 --> 00:00:48.630
By definition,

12
00:00:48.631 --> 00:00:53.631
a bubble is a price largely above the real value of an asset and it bursts when

13
00:00:54.961 --> 00:00:56.010
people realize that.

14
00:00:56.370 --> 00:00:59.970
So the question then becomes what is the fair value of Bitcoin?

15
00:01:00.390 --> 00:01:04.590
We know that Bitcoin doesn't require the bank credit mechanism for money

16
00:01:04.591 --> 00:01:09.300
creation.
It uses mathematical algorithms to mint.
It's tokens.

17
00:01:09.600 --> 00:01:13.050
It can also be used where normal money would be impractical,

18
00:01:13.051 --> 00:01:15.600
like in micro transfers between machines,

19
00:01:15.960 --> 00:01:19.920
Aka the Internet of things or trading in multiplayer games.

20
00:01:20.280 --> 00:01:24.390
It's uncensorable and it allows for anonymous transactions.

21
00:01:24.780 --> 00:01:28.350
Bitcoin's current supplies around 17 million bitcoins,

22
00:01:28.351 --> 00:01:33.351
while the current cash in circulation in the U S is one point $5 trillion,

23
00:01:33.571 --> 00:01:38.571
meaning we could currently replace 15% of all us cash with bitcoin.

24
00:01:38.970 --> 00:01:41.940
Clearly it has real value unlike ripple,

25
00:01:42.090 --> 00:01:46.230
but that doesn't mean it's price will continually rise forever.

26
00:01:46.470 --> 00:01:48.480
Who knows what its real value is?

27
00:01:48.481 --> 00:01:51.630
It could be 15,000 or $15 per coin.

28
00:01:52.020 --> 00:01:54.030
It's fair to say though that at this point,

29
00:01:54.031 --> 00:01:59.031
bitcoin must have developed price curve inefficiencies that can be exploited in

30
00:01:59.101 --> 00:02:02.580
a trading system.
We could try momentum investing,

31
00:02:02.610 --> 00:02:04.950
not momentum from gradient descent wizards.

32
00:02:05.160 --> 00:02:09.930
This is an investment strategy that aims to capitalize on the continuous

33
00:02:10.140 --> 00:02:12.840
continuance of existing trends in the market.

34
00:02:13.230 --> 00:02:15.930
The idea is that once a trend is established,

35
00:02:15.931 --> 00:02:20.430
it's more likely to continue in that direction than to move against that trend.

36
00:02:20.760 --> 00:02:25.010
The problem,
however,
is that crypto is far too volatiles,

37
00:02:25.011 --> 00:02:29.460
so conventional model based strategies won't work well.

38
00:02:29.880 --> 00:02:33.690
We need a fast trading trend agnostic strategy.

39
00:02:33.990 --> 00:02:38.550
That means it holds positions for only a few minutes and is not exposed to the

40
00:02:38.551 --> 00:02:39.384
bubble risk.

41
00:02:39.750 --> 00:02:44.750
We need to exploit short term price patterns using an algorithm of our choice,

42
00:02:45.450 --> 00:02:50.430
but before we pick an algorithm,
let's find ourselves a nice big coin Dataset.

43
00:02:50.880 --> 00:02:51.810
Luckily for us,

44
00:02:51.811 --> 00:02:56.790
Kaggle has a fund data set of minute by minute historical prices of Bitcoin.

45
00:02:57.120 --> 00:03:01.750
It includes seven factors that can use including prices and volume,

46
00:03:02.110 --> 00:03:06.820
and before we feed this data into our model,
we need to normalize it.

47
00:03:07.060 --> 00:03:11.680
That means adjusting the values of all of these features that are measured on

48
00:03:11.681 --> 00:03:14.470
different scales to one common scale.

49
00:03:14.800 --> 00:03:19.180
This will later boost our model's performance since the feature values will

50
00:03:19.181 --> 00:03:24.100
relate to each other more so it will be easier to find the relationship patterns

51
00:03:24.101 --> 00:03:25.570
that exist between them.

52
00:03:25.900 --> 00:03:30.070
One easy way to do this is to take a sliding window of a size.

53
00:03:30.071 --> 00:03:35.071
We decide and slide it across the data to make sure that there are no zero or n

54
00:03:35.511 --> 00:03:37.960
a n values.
They're normally,

55
00:03:37.961 --> 00:03:41.260
this would be a pain since we've got multiple dimensions here,

56
00:03:41.261 --> 00:03:44.380
but thanks to the python pandas library,

57
00:03:44.620 --> 00:03:48.190
we can represent each window as a pandas data frame.

58
00:03:48.370 --> 00:03:53.050
Then perform the normalization operation across the whole data frame,

59
00:03:53.080 --> 00:03:57.160
meaning all of its columns for the sake of cleanliness.

60
00:03:57.190 --> 00:04:01.840
Let's give functions their own self contained class as a data loader.

61
00:04:02.140 --> 00:04:05.200
That way we can reuse it later to extract,

62
00:04:05.350 --> 00:04:09.640
transform and load our datasets.
Now that we've done this,

63
00:04:09.670 --> 00:04:14.670
we simply need a model that accepts data of shape m where M is a number of

64
00:04:14.921 --> 00:04:18.430
dimensions in our data,
so what can we choose here?

65
00:04:18.520 --> 00:04:23.290
If we look through the academic literature on the topic of predicting market

66
00:04:23.291 --> 00:04:27.910
trends that hasn't been burned by the big banks will see that all sorts of

67
00:04:27.911 --> 00:04:31.330
models have been used for time series forecasting,

68
00:04:31.331 --> 00:04:33.820
which is what this problem is considered.

69
00:04:34.180 --> 00:04:38.290
Random forests support vector machines,
neural networks,

70
00:04:38.530 --> 00:04:43.530
and seeing as how neural networks tend to outperform most other machine learning

71
00:04:43.901 --> 00:04:48.280
models.
Most of the time when given lots of data and compute,
okay,

72
00:04:48.281 --> 00:04:51.730
deep learning,
we might as well try that model.

73
00:04:52.060 --> 00:04:55.960
Keep in mind that a neural network is a composite function.

74
00:04:56.470 --> 00:05:01.180
Every layer in the network is a function and the network as a whole then becomes

75
00:05:01.181 --> 00:05:05.020
a function of functions for as many layers as there are.

76
00:05:05.470 --> 00:05:10.000
The output of the previous layer is what feeds into the next layer as input

77
00:05:10.030 --> 00:05:12.310
until a final output is computed.

78
00:05:12.850 --> 00:05:16.720
If we train a feed forward neural network on this data,

79
00:05:17.110 --> 00:05:21.820
it will be the case that at every time step it will use a new data point as its

80
00:05:21.821 --> 00:05:22.654
input,

81
00:05:22.810 --> 00:05:27.810
but we are under the hypothesis that each data point is closely related to the

82
00:05:28.151 --> 00:05:30.010
data point that came before it,

83
00:05:30.340 --> 00:05:34.720
that there is some relationship between successive prices in the graph.

84
00:05:35.170 --> 00:05:39.310
So we need a way for our network to be able to understand this.

85
00:05:39.610 --> 00:05:42.910
These data points are not created in isolation.

86
00:05:43.030 --> 00:05:48.030
They are a part of a larger sequence so we can make a change to our model

87
00:05:48.190 --> 00:05:53.170
instead of feeding it every new data point at every time step so that the hidden

88
00:05:53.171 --> 00:05:55.570
layer is continuously updated.

89
00:05:55.900 --> 00:06:00.900
We'll feed it in the current data point as well as the previous learned in

90
00:06:01.190 --> 00:06:02.023
state.

91
00:06:02.150 --> 00:06:07.070
Keep in mind that the hidden state is just a matrix through the learning
process,

92
00:06:07.100 --> 00:06:12.100
it's going to be modified so that wind dot product operations are continuously

93
00:06:12.171 --> 00:06:15.500
applied to it to produce an output.
During inference,

94
00:06:15.860 --> 00:06:18.590
the resulting output will be much more accurate.

95
00:06:18.980 --> 00:06:23.980
This recurrence allows the network to not just learn from the data but to learn

96
00:06:24.051 --> 00:06:28.160
from what its previously learned allowing per sequence learning.

97
00:06:28.790 --> 00:06:31.670
But recurrent networks have a big problem.

98
00:06:32.150 --> 00:06:34.670
They can't remember really long sequences.

99
00:06:34.700 --> 00:06:37.100
So Bollywood movies are out of the question.

100
00:06:37.280 --> 00:06:40.190
This is called the vanishing gradient problem.

101
00:06:40.490 --> 00:06:42.500
During the optimization process,

102
00:06:42.530 --> 00:06:47.150
the gradient is recursively computed and apply to every layer of the network

103
00:06:47.151 --> 00:06:49.880
from the last one all the way back to the first.

104
00:06:50.360 --> 00:06:53.450
And as this gradient is computed layer by layer,

105
00:06:53.690 --> 00:06:58.190
it gets smaller and smaller and because the gradient update gets smaller,

106
00:06:58.580 --> 00:07:01.820
the network isn't able to store the memory of the changes.

107
00:07:01.821 --> 00:07:04.190
It's learning over a long period of time.

108
00:07:04.490 --> 00:07:07.290
That memory slowly diminishes.

109
00:07:07.490 --> 00:07:12.490
So we need to use a special and popular subset of recurrent nets called the long

110
00:07:12.861 --> 00:07:14.600
short term memory networks.

111
00:07:14.960 --> 00:07:19.160
The idea is simple rather than each of the nodes of the network just having a

112
00:07:19.161 --> 00:07:21.080
single activation function.

113
00:07:21.620 --> 00:07:26.540
Each node is a memory cell in that it can store other information so it

114
00:07:26.541 --> 00:07:31.541
maintains its own cell state while rns are fed their previous hidden state and

115
00:07:32.391 --> 00:07:34.220
the current input at every time.

116
00:07:34.220 --> 00:07:39.220
Step an lst m does the same except it also takes in its old self state and

117
00:07:40.791 --> 00:07:42.730
outputs.
It's new self states.

118
00:07:43.160 --> 00:07:46.460
What's happening inside of this memory cell is really cool.

119
00:07:46.490 --> 00:07:49.880
It's digital biology.
There are three steps here.

120
00:07:50.250 --> 00:07:54.080
The cell needs it for debt,
the irrelevant parts of the previous state.

121
00:07:54.410 --> 00:07:57.980
It needs to selectively update itself based on the new input.

122
00:07:58.430 --> 00:08:03.430
It's just seed and it needs to decide what parts of the cell state to output as

123
00:08:04.191 --> 00:08:05.360
a new hidden state.

124
00:08:05.870 --> 00:08:10.850
This is all achieved by using a forget to gate an input gates and an output
gate.

125
00:08:11.240 --> 00:08:14.720
A gate is a way to optionally let data through.

126
00:08:14.810 --> 00:08:19.700
Each is composed of a single sigmoid neural net layer and a multiplication

127
00:08:19.850 --> 00:08:21.200
operation.
That's it.

128
00:08:21.620 --> 00:08:26.510
So firstly a function of the previous hidden state and the new input passes

129
00:08:26.511 --> 00:08:27.920
through the forget gate,

130
00:08:28.220 --> 00:08:32.810
which lets us know what is irrelevant and can be removed from our cell state.

131
00:08:33.260 --> 00:08:36.500
It will output values close to one.
For parts of the cell state,

132
00:08:36.501 --> 00:08:40.580
we want EIP and zero for the ones we want to get rid of.

133
00:08:40.970 --> 00:08:41.630
For example,

134
00:08:41.630 --> 00:08:46.460
if the price starts spiking super high for days because of some major event,

135
00:08:46.790 --> 00:08:50.930
the price before that could be irrelevant.
We can forget that next,

136
00:08:50.960 --> 00:08:54.950
a function of the inputs passes through the input gate and it's added to the

137
00:08:55.230 --> 00:08:56.400
state to update it.

138
00:08:56.880 --> 00:09:00.630
There could be something new we've noticed and we want to add it to the cell

139
00:09:00.631 --> 00:09:03.870
state.
That's why this is there.
And lastly,

140
00:09:03.871 --> 00:09:08.580
the output gate decides what values from the cell state are going to be added to

141
00:09:08.581 --> 00:09:10.260
the hidden state output.

142
00:09:10.680 --> 00:09:15.120
So this will learn the parts of the sequence in the distant past that are worth

143
00:09:15.121 --> 00:09:17.070
remembering and those that are not.

144
00:09:17.430 --> 00:09:22.260
This ability to preserve information in the cell state for long stretches of

145
00:09:22.261 --> 00:09:25.830
time is what makes LSTM networks so special.

146
00:09:25.890 --> 00:09:28.440
They solve the problem of bandaging gradients.

147
00:09:28.740 --> 00:09:32.850
Since the forget gate is essentially the weights and activation function for the

148
00:09:32.851 --> 00:09:36.450
cell state and because the LSTM can learn to set that,

149
00:09:36.451 --> 00:09:41.451
forget gay to one for important things into cell state data can pass through

150
00:09:41.760 --> 00:09:42.690
unchanged.

151
00:09:42.750 --> 00:09:47.220
Notice that our bitcoin Dataset is super big since it's minute by minute.

152
00:09:47.250 --> 00:09:52.250
We can't just load 1 million data windows all have once into care os the train.

153
00:09:52.710 --> 00:09:57.180
That would require a godlike amount of Ram that only Jeff Bezos hats.

154
00:09:57.510 --> 00:10:02.510
A better solution would be to use the Karrass fit generator function to iterate

155
00:10:02.610 --> 00:10:04.440
over date,
out of unknown length,

156
00:10:04.500 --> 00:10:06.690
passing on the next piece every time it's called.

157
00:10:07.020 --> 00:10:11.820
We can train our model on one small chunk of data at a time and thanks to Ken

158
00:10:11.820 --> 00:10:16.820
Ross building an Lstm model requires just a few lines of code on your 15 to be

159
00:10:17.011 --> 00:10:21.300
precise.
When we start training our model,
it will take a few hours,

160
00:10:21.301 --> 00:10:25.200
but afterwards we can try forecasting the bitcoin price.

161
00:10:25.470 --> 00:10:28.500
We can do this either on a point by point basis,

162
00:10:28.530 --> 00:10:32.550
meaning we add one to the tee variable,
then ship the window of true data,

163
00:10:32.580 --> 00:10:34.980
predicting the next point along,

164
00:10:35.070 --> 00:10:37.800
or we do the same but multiple steps ahead.

165
00:10:38.190 --> 00:10:41.430
We can see that one step ahead is doing a reasonable job,

166
00:10:41.460 --> 00:10:45.480
but the predictions seem kind of volatile without doing more tests.

167
00:10:45.510 --> 00:10:49.950
It's hard to decide why this is the case and whether better parameters might fix

168
00:10:49.951 --> 00:10:50.784
this.

169
00:10:50.820 --> 00:10:55.470
Could add in sentiment data from Twitter to see how the crowd affects the price

170
00:10:55.471 --> 00:10:56.130
here.

171
00:10:56.130 --> 00:11:00.810
News sentiment is another option and we could merge the data then normalize it.

172
00:11:00.811 --> 00:11:02.220
So it's all numerical.

173
00:11:02.250 --> 00:11:07.250
Using Penn does a cutting edge technique right now is to use Basie and methods

174
00:11:07.860 --> 00:11:12.720
alongside LSTM networks to compensate for the fact that the factors that

175
00:11:12.721 --> 00:11:16.800
influence price variations,
yeah,
themselves change over time.

176
00:11:17.190 --> 00:11:19.650
Details on all of that in the video description

177
00:11:19.770 --> 00:11:21.600
<v 1>before you crash the stock market,</v>

178
00:11:21.690 --> 00:11:25.230
hit the subscribe button and I'll keep making you better,
faster,
and smarter.

179
00:11:25.620 --> 00:11:29.130
For now,
I've got to invest in some furniture,
so thanks for watching.

