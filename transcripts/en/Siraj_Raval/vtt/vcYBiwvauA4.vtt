WEBVTT

1
00:00:00.720 --> 00:00:03.480
Being human is great.
Nothing else can dress itself.

2
00:00:05.660 --> 00:00:10.660
Hello world it Saroj and this video of a virtual character dressing itself is

3
00:00:11.181 --> 00:00:13.310
making the rounds in the Ai Community.

4
00:00:13.580 --> 00:00:17.930
In a joint collaboration between Google brain and Georgia tech researchers

5
00:00:17.931 --> 00:00:22.790
demonstrated that it's possible to automatically discover dressing techniques,

6
00:00:23.030 --> 00:00:26.930
meaning they managed to get an AI to learn how to dress itself using some

7
00:00:26.931 --> 00:00:27.764
clothing.

8
00:00:27.860 --> 00:00:31.790
It's movements are relatively smooth and what's incredible is that these

9
00:00:31.791 --> 00:00:35.840
movements are learned from scratch.
None of them are hand coded.

10
00:00:36.200 --> 00:00:41.200
They use the bleeding edge technique called trust region policy optimization.

11
00:00:41.331 --> 00:00:44.660
To do this,
and I'll explain how that works later,
but first,

12
00:00:44.930 --> 00:00:48.890
we can use this technology to help improve the gaming experience.

13
00:00:49.160 --> 00:00:53.300
A player can create a custom outfit and a Bot can learn how to dress itself

14
00:00:53.301 --> 00:00:56.210
using it,
allowing for a more immersive experience.

15
00:00:56.510 --> 00:00:59.570
We can also use it to help with the elder care market.

16
00:00:59.840 --> 00:01:04.190
Many people in assisted living facilities require a human to help dress

17
00:01:04.191 --> 00:01:08.300
themselves.
And if a physical robot can understand how to do that,

18
00:01:08.480 --> 00:01:12.860
older humans could instead dress themselves with the help of a personal robot

19
00:01:12.920 --> 00:01:16.310
that understands the intricacies of how it should work,

20
00:01:16.400 --> 00:01:20.120
enabling more independence and dignity.
Also,

21
00:01:20.180 --> 00:01:24.680
as the online shopping experience continues to improve with more people buying

22
00:01:24.681 --> 00:01:28.040
clothes online,
we could use this as a visualization tool,

23
00:01:28.041 --> 00:01:31.070
not only showing you how you'd look in a certain outfit,

24
00:01:31.071 --> 00:01:33.380
but also how it would look to put it on.

25
00:01:33.770 --> 00:01:38.060
And infant AI can learn how to properly move its limbs to dress itself.

26
00:01:38.180 --> 00:01:42.560
It could also use a similar technique to learn how to complete any physical
task,

27
00:01:42.561 --> 00:01:47.060
be it on an assembly line in a factory as a personal training instructor,

28
00:01:47.210 --> 00:01:49.790
or even as a personal Netflix and chill companion.

29
00:01:50.000 --> 00:01:54.290
So let's go through the thought process these researchers went through to create

30
00:01:54.291 --> 00:01:55.940
this incredible algorithm.

31
00:01:56.270 --> 00:02:00.980
We have a starting states where the character is holding a garment and an ending

32
00:02:00.981 --> 00:02:03.560
states where the character is fully clothed.

33
00:02:03.740 --> 00:02:07.760
We want to create an algorithm that will help our character transition from the

34
00:02:07.761 --> 00:02:11.300
starting states to the ending states by dressing itself.

35
00:02:11.600 --> 00:02:16.070
This dressing task can actually be split up into multiple sub tasks.

36
00:02:16.340 --> 00:02:17.060
For example,

37
00:02:17.060 --> 00:02:21.080
dressing a jacket consists of four subtasks pulling this sleeve onto the first

38
00:02:21.081 --> 00:02:23.510
arm,
moving the second arm behind the back,

39
00:02:23.660 --> 00:02:25.850
scooping the second sleep onto the arm,

40
00:02:25.970 --> 00:02:28.850
and lastly returning the body to a rest position.

41
00:02:29.150 --> 00:02:31.520
Luckily no popped callers necessary here.

42
00:02:31.790 --> 00:02:35.870
Since this is a simulated environment where time is a dimension we'll want to

43
00:02:35.871 --> 00:02:37.580
use,
not a supervised learning,

44
00:02:37.730 --> 00:02:42.080
but a reinforcement learning technique to help us for each sub task.

45
00:02:42.230 --> 00:02:46.460
We can formulate it as a separate reinforcement learning problem to learn an

46
00:02:46.490 --> 00:02:47.750
optimal policy.

47
00:02:48.020 --> 00:02:52.250
This policy will know exactly which actions to perform given the state of the

48
00:02:52.251 --> 00:02:54.770
character in order to complete the task.

49
00:02:55.040 --> 00:02:59.650
The mathematical framework that all reinforcement learning techniques use called

50
00:02:59.680 --> 00:03:04.680
the mark Haub decision process or MDP can help us formulate our problem.

51
00:03:04.930 --> 00:03:07.090
And since we have several sub tasks,

52
00:03:07.210 --> 00:03:12.190
we'll create a separate one for each and our MDP isn't fully observable.
Instead,

53
00:03:12.191 --> 00:03:13.720
it's partially observable.

54
00:03:14.020 --> 00:03:18.100
This is because we humans don't have direct perception of the full state of the

55
00:03:18.101 --> 00:03:20.950
world and of ourselves.
During dressing.

56
00:03:21.100 --> 00:03:25.360
We have a limited perception of the state of the garment outside of any haptic

57
00:03:25.361 --> 00:03:27.670
and visual observations.
Also,

58
00:03:27.671 --> 00:03:31.570
the full state space of dressing tasks is very highly dimensional,

59
00:03:31.720 --> 00:03:35.080
which leads to very large policy networks with hundreds of thousands of

60
00:03:35.081 --> 00:03:39.190
variables which would be intractable.
So to replicate this reality,

61
00:03:39.191 --> 00:03:42.460
they designed a compact observation space,
oh,

62
00:03:42.610 --> 00:03:45.370
which is a subspace of the full state space s.

63
00:03:45.760 --> 00:03:48.970
The observation space includes the characters,
joint angles,

64
00:03:48.971 --> 00:03:52.360
garment feature locations,
haptics,
surface information,

65
00:03:52.540 --> 00:03:56.140
and a task vectra that suggests a direction for the end effector,

66
00:03:56.320 --> 00:03:59.980
which is the robot's hand to move based on dressing progress.

67
00:04:00.310 --> 00:04:04.150
The state space consists of the joint angles and the velocities of the human

68
00:04:04.151 --> 00:04:06.730
model,
the vertex positions of the garment,

69
00:04:06.970 --> 00:04:11.020
contact information from the previous simulation step and the values of the

70
00:04:11.021 --> 00:04:14.080
precomputed geodesic field on the garment.

71
00:04:14.380 --> 00:04:17.890
So we have a lot of numerical values to work with here.

72
00:04:17.920 --> 00:04:22.720
We're getting this data fed to us in real time and there's so many possible

73
00:04:22.721 --> 00:04:26.320
actions that our robot could take.
It's not as simple as moving up,

74
00:04:26.321 --> 00:04:28.660
down left or right in a two d video game.

75
00:04:28.930 --> 00:04:31.960
Instead of only having a few set of possible actions.

76
00:04:32.080 --> 00:04:35.320
We have many in this continuous action space.

77
00:04:35.500 --> 00:04:39.640
Actions consist of the set of required changes to get from one state to another.

78
00:04:39.910 --> 00:04:44.260
The reward is a numerical measure of how close the character is to completing

79
00:04:44.261 --> 00:04:48.280
its current task as a function of the state of the character and the garment.

80
00:04:48.670 --> 00:04:52.750
They designed it so that it rewards end effector motion in the direction of the

81
00:04:52.751 --> 00:04:53.680
task vector.

82
00:04:53.740 --> 00:04:58.060
Now that we formulated each sub task and it partially observable MDP,

83
00:04:58.240 --> 00:05:02.380
we need to decide on a suitable RL technique that will help us learn the optimal

84
00:05:02.381 --> 00:05:05.440
policy for each sub task trust region.

85
00:05:05.441 --> 00:05:08.680
Policy optimization is the technique they use and look,

86
00:05:08.681 --> 00:05:13.570
I know it sounds difficult to understand,
but that's because it is.
Luckily,

87
00:05:13.600 --> 00:05:16.090
I'll explain it in detail here.
So here we go.

88
00:05:16.450 --> 00:05:18.610
In a traditional supervised learning problem,

89
00:05:18.611 --> 00:05:21.520
we know what the correct answer for every input would be,

90
00:05:21.700 --> 00:05:25.990
so we can provide reliable feedback to every prediction of our model.
If,

91
00:05:25.991 --> 00:05:26.531
for example,

92
00:05:26.531 --> 00:05:30.700
we had a classifier that output that a plant belonged to the wrong class,

93
00:05:31.000 --> 00:05:35.350
we could just use the error between the prediction and label to optimize our

94
00:05:35.351 --> 00:05:38.560
network over time,
but in reinforcement learning,

95
00:05:38.561 --> 00:05:43.561
we don't always have the same luxury of labeled training data for every input

96
00:05:44.440 --> 00:05:45.760
for dressing our character.

97
00:05:45.760 --> 00:05:49.540
We don't know what the correct action is at a given time step.

98
00:05:49.720 --> 00:05:51.070
We have to learn it ourselves,

99
00:05:51.150 --> 00:05:55.420
but what we do know is whether or not the action we take eventually leads to a

100
00:05:55.421 --> 00:05:58.190
success or failure.
In accomplishing our task,

101
00:05:58.280 --> 00:06:02.660
we can modify our optimization function to focus on increasing the chance of

102
00:06:02.661 --> 00:06:06.470
moves that lead to a favorable outcome while decreasing the chance of making

103
00:06:06.471 --> 00:06:09.050
moves that eventually lead to bad outcomes.

104
00:06:09.380 --> 00:06:13.400
We'll start with an untrained and neural network to represent one of our sub

105
00:06:13.401 --> 00:06:14.690
task policies.

106
00:06:15.110 --> 00:06:19.340
In the paper they used fully connected neural networks consisting of two hidden

107
00:06:19.341 --> 00:06:24.341
layers of 64 nodes each and can age activations with a final linear output layer

108
00:06:25.221 --> 00:06:26.930
to represent each policy.

109
00:06:27.260 --> 00:06:32.060
Our policy network will accept a state and output a probability distribution of

110
00:06:32.061 --> 00:06:32.894
actions.

111
00:06:33.020 --> 00:06:37.010
Our character will complete an entire episode until it's finished completing,

112
00:06:37.011 --> 00:06:41.420
addressing sub task.
Then we can use the collection of states and actions.

113
00:06:41.421 --> 00:06:45.680
We ended up taking over time to help update our network.
Yes,

114
00:06:45.710 --> 00:06:48.590
we're unable to provide the correct label at every step,

115
00:06:48.860 --> 00:06:53.000
but instead we compute the gradient as if the action we took was the correct

116
00:06:53.001 --> 00:06:53.834
action.

117
00:06:53.870 --> 00:06:57.830
The gradient tells us in which numerical direction we should update the weights

118
00:06:57.860 --> 00:07:01.640
of our network so that the error is smaller than next iteration.

119
00:07:01.760 --> 00:07:03.680
If the action led to a win,

120
00:07:03.710 --> 00:07:07.940
we can update the gradients such that we are encouraging our network to take the

121
00:07:07.941 --> 00:07:10.400
same action again,
if it led to a loss,

122
00:07:10.430 --> 00:07:14.720
we instead apply the negative gradients to the network parameters to discourage

123
00:07:14.721 --> 00:07:16.760
the network from taking that action.
Again,

124
00:07:17.180 --> 00:07:20.690
this is considered a simple policy gradient technique,

125
00:07:20.750 --> 00:07:25.310
but should we use a simple policy gradient technique to learn an optimal policy

126
00:07:25.311 --> 00:07:27.350
for each sub task?
No,

127
00:07:27.410 --> 00:07:31.190
because policy gradient techniques suffer from two limiting factors.

128
00:07:31.460 --> 00:07:35.030
The first factor is the sensitive step size.
At each time step,

129
00:07:35.031 --> 00:07:36.530
the network updates some amount,

130
00:07:36.531 --> 00:07:39.650
but it turns out that two largest step leads to a disaster.

131
00:07:39.980 --> 00:07:43.850
Think of it like being on a mountain.
If the new policy goes too far,

132
00:07:43.970 --> 00:07:47.390
it takes an action that may be a meter too far and falls off the cliff.

133
00:07:47.600 --> 00:07:50.570
But if the step is too small,
the model learns too slowly.

134
00:07:50.870 --> 00:07:54.770
The second limiting factor is that it's hard to find a good learning rates in

135
00:07:54.771 --> 00:07:59.210
reinforcement learning.
One that doesn't cause an explosive policy updates.

136
00:07:59.480 --> 00:08:03.980
We want to limit the policy changes and ensure that each change guarantees

137
00:08:03.981 --> 00:08:06.380
improvements in rewards.
To do this,

138
00:08:06.470 --> 00:08:10.220
we need a better optimization method to produce better policies.

139
00:08:10.460 --> 00:08:13.700
We can find hope in what are called trust region methods.

140
00:08:14.090 --> 00:08:18.860
Trust region methods define a region around the current iteration within which

141
00:08:18.861 --> 00:08:23.270
they trust the model to be an adequate representation of the objective function.

142
00:08:23.870 --> 00:08:28.610
They then choose the step to be the approximate minimizer of the model in that

143
00:08:28.611 --> 00:08:32.600
specific region.
So during the optimization procedure,

144
00:08:32.630 --> 00:08:36.320
after we decide the gradient direction,
when doing line search,

145
00:08:36.530 --> 00:08:40.640
we want to constrain our step length to be within a trust region.

146
00:08:40.760 --> 00:08:45.760
So that the local estimation of the gradient remains to be trusted and trust.

147
00:08:45.771 --> 00:08:47.810
Region policy optimization.

148
00:08:47.960 --> 00:08:52.130
We can use a measurement of the difference between the old policy and the

149
00:08:52.160 --> 00:08:55.160
updated policy as a measurement for our trust.

150
00:08:55.950 --> 00:08:59.910
This difference is called the coal black Leibler divergence and it helps us

151
00:08:59.911 --> 00:09:04.260
measure just how much information we lose when we choose an approximation.

152
00:09:04.560 --> 00:09:07.680
Over time.
We minimize this difference and our policy improves.

153
00:09:07.770 --> 00:09:10.920
Learning a control policy for each sub task works well,

154
00:09:10.950 --> 00:09:15.180
but how is a character supposed to know when to start one sub task in relation

155
00:09:15.181 --> 00:09:19.950
to another.
In addition to giving each sub task its own control policy,

156
00:09:20.160 --> 00:09:24.150
the researchers introduced a policy sequencing algorithm that matched the

157
00:09:24.151 --> 00:09:28.440
distribution of output states from one task to the input distribution for the

158
00:09:28.441 --> 00:09:32.520
next task in the sequence and that was the last piece of the puzzle.

159
00:09:32.520 --> 00:09:33.810
Using this algorithm,

160
00:09:33.960 --> 00:09:38.730
the team found that training a single sub task required 24 hours of simulation

161
00:09:38.731 --> 00:09:43.110
time on 36 CPU compute nodes.
After trial and error,

162
00:09:43.140 --> 00:09:47.490
they learned that carefully defining the reward function made a huge difference

163
00:09:47.520 --> 00:09:48.540
in their results.

164
00:09:48.840 --> 00:09:53.400
Their character controller was able to successfully dress under various initial

165
00:09:53.401 --> 00:09:55.650
conditions and moving forward,

166
00:09:55.651 --> 00:09:58.770
they like to incorporate lower body dressing as well,

167
00:09:59.070 --> 00:10:02.490
which would require incorporating balance into the controller.

168
00:10:02.910 --> 00:10:05.160
There are three things to remember from this video.

169
00:10:05.460 --> 00:10:10.460
We can use deep reinforcement learning in real time environments to help robots

170
00:10:10.830 --> 00:10:15.600
learn how to perform any physical task to help their character learned to dress.

171
00:10:15.660 --> 00:10:19.890
These researchers use a technique called trust region policy optimization or

172
00:10:19.891 --> 00:10:24.891
Trpa and Trpa Oh is a policy gradient technique that constrains policy changes

173
00:10:25.321 --> 00:10:29.100
during updates to avoid exploding parades and world is yours.

174
00:10:29.220 --> 00:10:31.740
Please subscribe for more programming videos.
And for now,

175
00:10:31.890 --> 00:10:34.560
I've got to undress myself,
so thanks for watching.

