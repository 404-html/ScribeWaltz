WEBVTT

1
00:00:00.150 --> 00:00:04.410
Hello world,
it's Suraj and deep learning.
It works so well.

2
00:00:04.620 --> 00:00:06.000
Why does it work so well?

3
00:00:06.150 --> 00:00:11.070
The answer is because it uses a technique called back propagation.

4
00:00:11.370 --> 00:00:16.370
And in this video I'm going to demo a newer strategy that optimizes neural

5
00:00:17.191 --> 00:00:22.191
networks that is better than backpropagation and it's called synthetic

6
00:00:22.231 --> 00:00:23.064
gradients.

7
00:00:23.250 --> 00:00:27.870
This technique was released by deep mind a few months ago and it just hasn't had

8
00:00:27.871 --> 00:00:32.400
enough attention paid to it.
This is what happens in research.
A lots,
right?

9
00:00:32.430 --> 00:00:35.790
Some great ideas don't get enough attention to paid paid to them.

10
00:00:35.880 --> 00:00:39.150
So then some youtube or like me has to make a video about them.

11
00:00:39.420 --> 00:00:42.930
So what we're gonna do is we're going to take the simple feed forward neural

12
00:00:42.931 --> 00:00:43.351
network.

13
00:00:43.351 --> 00:00:47.820
It's a simple neural network that learns to predict the output of adding two

14
00:00:47.821 --> 00:00:52.230
binary numbers together.
And first we're going to train it using backpropagation.

15
00:00:52.320 --> 00:00:57.000
And so I'm going to go over back propagation and then I'm going to change the

16
00:00:57.001 --> 00:01:00.390
code.
So instead it uses synthetic gradients.

17
00:01:00.630 --> 00:01:04.980
So this gift that you're looking at right here is an example of the synthetic

18
00:01:04.981 --> 00:01:06.840
gradient strategy.
Uh,

19
00:01:06.870 --> 00:01:10.860
the idea behind synthetic gradients is rather than waiting for your neural

20
00:01:10.861 --> 00:01:14.850
network to go through a full forward and backward pass,

21
00:01:15.390 --> 00:01:19.050
you can just update each layer without having to wait for that using what's

22
00:01:19.051 --> 00:01:23.520
called a synthetic gradient.
These are gradients that are not the real gradient,

23
00:01:23.550 --> 00:01:28.320
the true gradient,
but instead an approximation of what that gradient would be.

24
00:01:28.530 --> 00:01:31.980
So it's a synthetic gradient.
And by doing this,

25
00:01:32.040 --> 00:01:36.840
we can update each layer without having to wait for all the other layers to

26
00:01:36.841 --> 00:01:37.560
updates.

27
00:01:37.560 --> 00:01:42.560
It's a faster way to optimize neural networks and allows us to decouple each

28
00:01:43.291 --> 00:01:45.600
layer from having to wait for all the other layers.

29
00:01:46.110 --> 00:01:50.220
So let's start off by talking about how neural networks learn in a black box

30
00:01:50.221 --> 00:01:52.980
approach.
The neural network gets an input.

31
00:01:53.010 --> 00:01:56.430
That's the input data and some desired output,
right?

32
00:01:56.431 --> 00:02:01.080
So it's got an input in a desired output and it uses that to update its internal

33
00:02:01.081 --> 00:02:05.760
state accordingly,
right?
We have some desired output.
Those are our labels,
right?

34
00:02:05.940 --> 00:02:10.940
All of all of that propagation centers around having some labels for our data.

35
00:02:11.101 --> 00:02:13.800
We've got to have the labels.
If you don't have the labels,

36
00:02:13.980 --> 00:02:16.950
you gotta get the labels right?
And so if we have our labels,

37
00:02:16.951 --> 00:02:19.980
we can train our model.
And so once we've trained it,

38
00:02:20.040 --> 00:02:21.480
then we can perform prediction.

39
00:02:21.510 --> 00:02:26.010
And the prediction process is using just the input and the internal state to

40
00:02:26.011 --> 00:02:31.011
generate the most likely output according to its past training experiences.

41
00:02:31.620 --> 00:02:35.490
So we have some input training data,
we feed it to this learning system,

42
00:02:35.491 --> 00:02:39.510
which is a neural network,
and it Purdue,
and it predicts an output.

43
00:02:39.690 --> 00:02:44.670
We then compare that predicted output to the actual output and we compute some

44
00:02:44.671 --> 00:02:49.350
error value.
We use that error to then update then learning system.

45
00:02:49.351 --> 00:02:52.740
So then every time it gets better and better,
that's just the training process.

46
00:02:53.010 --> 00:02:56.450
And that updating process is back propagation.

47
00:02:57.570 --> 00:03:01.300
So,
uh,
in a more detailed sense,
we have some neural network,
right?

48
00:03:01.301 --> 00:03:04.750
We've got an input layer,
we've got a hidden layer,
and we have an output layer.

49
00:03:05.200 --> 00:03:09.790
And each of,
so each of these are layers,
right?
These three circles are a layer.

50
00:03:09.791 --> 00:03:11.230
These three circles are hey layer.

51
00:03:11.380 --> 00:03:15.850
These last two circles are a layer and each of these circles represents a number

52
00:03:15.851 --> 00:03:20.380
because we are inputting,
let's just say three integers,
right?
A one,

53
00:03:20.410 --> 00:03:22.120
one and a zero.
That's the input.

54
00:03:22.450 --> 00:03:27.450
And the reason that there are these black lines is because we are multiplying

55
00:03:27.611 --> 00:03:31.780
the input.
We are multiplying the layer by the next layer.

56
00:03:31.780 --> 00:03:36.780
So we are multiplying this one value by all of these other values and the next

57
00:03:37.661 --> 00:03:38.230
layer,

58
00:03:38.230 --> 00:03:43.230
because a neural network is input times weight at a bias and then apply a non

59
00:03:43.351 --> 00:03:46.750
linearity to it.
So that's what we just keep doing for a neural network.

60
00:03:46.840 --> 00:03:50.440
We take our input data,
we multiply it by a weight Matrix,

61
00:03:50.441 --> 00:03:51.580
which is the next layer.

62
00:03:51.730 --> 00:03:56.290
We add some bias value and then we apply a nonlinearity to it,

63
00:03:56.291 --> 00:03:58.450
which is a sigmoid function or there are,

64
00:03:58.460 --> 00:04:01.810
there are plenty of nonlinearities that we can apply.
And this is the,

65
00:04:01.811 --> 00:04:04.150
this is the,
this is called forward propagation.

66
00:04:04.151 --> 00:04:09.040
This is how we take some input data and we just apply a series of operations to

67
00:04:09.041 --> 00:04:13.960
that input data until we get an output prediction that we denote here as y hat.

68
00:04:14.290 --> 00:04:16.810
And so that is a whole forward propagation process.

69
00:04:16.990 --> 00:04:21.340
A neural network is just a series of operations that are applied to some input

70
00:04:21.490 --> 00:04:25.090
data,
right?
That's it.
Input Times,
weight added,
Tobias activate,

71
00:04:25.091 --> 00:04:28.360
repeat that process over and over and over again until we get that output

72
00:04:28.361 --> 00:04:31.600
prediction.
And once we have that output prediction,

73
00:04:31.840 --> 00:04:36.250
we could compare it to the actual output,
right?
And so in a very,
very basic case,

74
00:04:36.251 --> 00:04:39.910
we can just subtract the actual output from the predicted output.

75
00:04:40.120 --> 00:04:44.620
And that difference is the error,
right?
We what we want is that error to be zero.

76
00:04:44.621 --> 00:04:46.360
We don't want there to be any difference.

77
00:04:46.480 --> 00:04:51.480
We want our predicted output to be the exact same as our actual output.

78
00:04:52.270 --> 00:04:53.980
But as long as there is an error,

79
00:04:54.070 --> 00:04:57.250
we want to minimize it so it gets smaller and smaller over time.

80
00:04:57.490 --> 00:05:02.230
That is the training process.
And so how do we minimize that error?
Well,

81
00:05:02.260 --> 00:05:03.790
once we compute the error,

82
00:05:03.940 --> 00:05:08.410
we can compute what's called the partial derivative with respect to each layer

83
00:05:08.500 --> 00:05:09.820
going backwards,
right?

84
00:05:09.940 --> 00:05:13.780
So we compute the partial derivative of the error with respect to the weights

85
00:05:13.781 --> 00:05:14.860
for the previous layer.

86
00:05:15.040 --> 00:05:18.250
And what that's gonna do is that's going to give us a gradient value.

87
00:05:18.590 --> 00:05:21.970
Then we take that gradient and we compute the partial derivative of the gradient

88
00:05:22.120 --> 00:05:25.840
with respect to the weights of the next layer or the layer before that.

89
00:05:26.170 --> 00:05:29.350
And we get another gradient and then we take that gradient and we compute the

90
00:05:29.351 --> 00:05:32.860
partial derivative of the gradient with respect to the layer before that.

91
00:05:33.070 --> 00:05:36.040
And so we just keep doing that.
And so if we keep doing that,

92
00:05:36.130 --> 00:05:39.250
we get great values for every single layer.

93
00:05:39.460 --> 00:05:41.770
And then once we've done that for every single layer,

94
00:05:41.950 --> 00:05:45.790
then we could use those gradients to update the weights of each layer.

95
00:05:46.030 --> 00:05:50.860
So you might be asking what is a gradient?
A gradient is a direction.

96
00:05:51.010 --> 00:05:54.880
It's a direction on how to update our weights and it comes from Calculus

97
00:05:55.030 --> 00:05:59.180
calculates is a study of how things change.
That's why it's used in physics,

98
00:05:59.181 --> 00:06:02.000
right.
How do planetary bodies move over time?

99
00:06:02.210 --> 00:06:05.870
How do the weight matrices of a neural network change as we are trying to

100
00:06:05.871 --> 00:06:09.680
minimize an error?
The gradient,
the gradient is a value.

101
00:06:09.710 --> 00:06:13.970
It's a value with the direction.
And what I mean by direction is if we were to,

102
00:06:14.180 --> 00:06:15.860
if we were to graph,
uh,

103
00:06:15.861 --> 00:06:19.690
all the possible weight values that we could have for a single layer versus all

104
00:06:19.710 --> 00:06:22.400
the possible error values that we could have for that same layer,

105
00:06:22.610 --> 00:06:27.410
it would look like a parabola.
And with the gradient is,
is it,
is,

106
00:06:27.740 --> 00:06:28.790
it is,
it is.

107
00:06:28.820 --> 00:06:33.080
It is synonymous with the partial derivative and the partial derivative or the

108
00:06:33.081 --> 00:06:37.760
derivative is a slope of the tangent line to occur at a specific point,
right?

109
00:06:37.761 --> 00:06:40.700
So that's what the gradient gives us.
It gives us that slope of a tangent line.

110
00:06:40.730 --> 00:06:45.440
It's anonymous and the direction that that slope is,
is pointing,

111
00:06:45.650 --> 00:06:50.420
tells us what direction we need to move such that we can get to this point where

112
00:06:50.421 --> 00:06:53.750
the error is the smallest that is the minimum of the parabola.

113
00:06:53.960 --> 00:06:55.280
And once we get to that point,

114
00:06:55.430 --> 00:06:59.510
then we know immediately what the ideal weight value should be.

115
00:06:59.540 --> 00:07:03.380
So the error is smallest.
And we do this for every single layer,
right?

116
00:07:03.560 --> 00:07:06.200
We compute this gradient value for every single layer.

117
00:07:06.410 --> 00:07:09.770
And the training process process is us.
Generally.

118
00:07:09.980 --> 00:07:14.980
It is us gradually decreasing the point that we are at for each a weight matrix

119
00:07:15.560 --> 00:07:19.490
until we are at this point where the error is smallest and all the weight values

120
00:07:19.520 --> 00:07:24.080
of that weight Matrix,
a r r equal to when the error is smallest.

121
00:07:24.320 --> 00:07:28.040
And that is when we are finished training.
That is the optimization process.

122
00:07:28.190 --> 00:07:31.580
And because we are descending the Parabola,
it's called gradient descent.

123
00:07:31.730 --> 00:07:35.120
So gradient descent is a very popular optimization strategy.

124
00:07:35.240 --> 00:07:37.820
It's using neural networks,
it's used all over the place.

125
00:07:38.960 --> 00:07:40.880
But in the context of neural networks,

126
00:07:41.000 --> 00:07:45.920
we call it back propagation because we are back propagating on error gradient

127
00:07:46.040 --> 00:07:48.170
across each layer.
Recursively.

128
00:07:48.350 --> 00:07:52.940
So every layer depends on us having computed the gradient for every single layer

129
00:07:52.941 --> 00:07:56.060
before that,
right?
So every layer is coupled.

130
00:07:56.270 --> 00:08:00.360
It's dependent on us having computed the gradients for every single layer that

131
00:08:00.361 --> 00:08:03.380
hat for every single layer,
uh,
after that.

132
00:08:06.290 --> 00:08:10.760
So what this looks like in code is if we were to look at this in num Pi is if we

133
00:08:10.761 --> 00:08:12.050
were to look at this in python,

134
00:08:12.170 --> 00:08:14.840
what I've got here is a function called generate dataset.

135
00:08:14.990 --> 00:08:19.460
What this function is going to do is it generates a bunch of binary numbers,

136
00:08:19.461 --> 00:08:19.671
right?

137
00:08:19.671 --> 00:08:24.671
Like one zero zero one and then one one one zero it's also going to return the

138
00:08:25.700 --> 00:08:30.380
outputs and those outputs are the sums of those binary or numb numbers.

139
00:08:30.560 --> 00:08:32.570
So if we take two binary numbers,

140
00:08:32.571 --> 00:08:36.260
like one zero zero one zero and then we add another binary number,

141
00:08:36.290 --> 00:08:38.900
like one zero zero one and it's going to have some,

142
00:08:38.930 --> 00:08:42.380
some value that it's also a binary number like ones,
there's a one,

143
00:08:43.160 --> 00:08:46.850
the input for our neural network are going to be these two binary numbers,

144
00:08:46.851 --> 00:08:48.170
concatenate it together.

145
00:08:48.290 --> 00:08:52.790
So they're one big binary number and the predict and the actual output will be

146
00:08:52.791 --> 00:08:55.130
the sum of those two binary numbers.

147
00:08:55.230 --> 00:08:57.930
So we already know what the sum is going to be,
right?
We've,

148
00:08:57.931 --> 00:08:59.670
we've already computed it in dysfunction,

149
00:08:59.970 --> 00:09:04.970
but what our neural networks job is is to learn what that output would be so

150
00:09:05.221 --> 00:09:09.600
that even if we don't have that output,
we can predict what the output would be.

151
00:09:09.780 --> 00:09:13.950
So it's making a prediction by updating its weights and going through this

152
00:09:13.951 --> 00:09:18.030
backpropagation process without even having to add the numbers together,

153
00:09:18.180 --> 00:09:20.700
is just able to predict what the output would be.

154
00:09:22.380 --> 00:09:26.690
So what we have here are this,
the sigmoid is the nonlinearity.

155
00:09:26.850 --> 00:09:30.900
And the reason we apply a nonlinearity to neural networks is because neural

156
00:09:30.901 --> 00:09:35.760
networks are universal function approximators.
They can compute any function,

157
00:09:35.910 --> 00:09:39.520
whether it's linear,
like as like a straight line or nonlinear,

158
00:09:39.540 --> 00:09:42.660
like a very curvy line,
like a parabola.
They can,
they can,

159
00:09:42.690 --> 00:09:46.080
they can approximate any function.
And that's why we use a nonlinear,

160
00:09:46.200 --> 00:09:49.650
so they can approximate both linear and nonlinear functions.

161
00:09:50.010 --> 00:09:51.360
This is the,

162
00:09:51.750 --> 00:09:55.110
this function helped us compute the partial derivative with respect to each

163
00:09:55.111 --> 00:09:57.360
weight going backwards,
right?
So it's the,

164
00:09:57.361 --> 00:10:01.440
it's the derivative of the sigmoid function and that's what's going to help us

165
00:10:01.690 --> 00:10:05.640
compute compute backpropagation.
So this class is a,

166
00:10:05.700 --> 00:10:06.930
is a layer class.

167
00:10:07.490 --> 00:10:11.400
Each layer is initialized by its input dimensions and its output dimensions,

168
00:10:11.580 --> 00:10:12.480
which are the,

169
00:10:12.780 --> 00:10:16.500
the dimensions of the numbers going into it and out of it it's got a

170
00:10:16.501 --> 00:10:20.250
nonlinearity,
which is our sigmoid,
and it's got a nonlinearity derivative,

171
00:10:20.251 --> 00:10:22.620
which helps us compute the gradient value.

172
00:10:22.890 --> 00:10:27.150
So each layer is initialized as a weight matrix of some size that we defined

173
00:10:27.151 --> 00:10:31.200
here.
It's also got a nonlinearity and a nonlinearity derivative,
right?

174
00:10:31.320 --> 00:10:34.920
That's how we initialize a layer.
It's a glorified weight matrix.

175
00:10:36.000 --> 00:10:40.320
Then we've got a forward and backward forward and backward function.

176
00:10:41.390 --> 00:10:45.870
The forward function takes the input it's given and it takes,
it takes,
the input,

177
00:10:46.050 --> 00:10:50.070
multiplies it by the weight multiply,
meaning the dot product,
right?

178
00:10:50.070 --> 00:10:53.580
When we multiply two major seats together,
it's considered the dots product.

179
00:10:53.760 --> 00:10:54.540
That's why we use,

180
00:10:54.540 --> 00:10:57.810
that's why it's considered linear Algebra and not regular Algebra.

181
00:10:57.930 --> 00:11:00.030
That dot products comes from linear Algebra.

182
00:11:00.031 --> 00:11:04.230
Linear Algebra tells us how to multiply to groups of numbers together rather

183
00:11:04.231 --> 00:11:08.220
than just single integers,
which is what Algebra helps us do.
That's what we use,

184
00:11:08.221 --> 00:11:09.690
linear Algebra and deep learning.

185
00:11:09.990 --> 00:11:13.620
And so I do the input times the weight and we apply an activation or a

186
00:11:13.621 --> 00:11:16.980
nonlinearity to it and put times weight out of bias.

187
00:11:16.981 --> 00:11:18.710
We're not adding a bias right now,
uh,
for,

188
00:11:18.711 --> 00:11:21.450
for this simple example and then activate,
right?

189
00:11:21.451 --> 00:11:25.080
And that's going to give us our output.
So we repeatedly apply this forward,

190
00:11:25.440 --> 00:11:28.980
this forward function,
and that's going to compute our forward propagation.

191
00:11:29.520 --> 00:11:30.960
And then for the backwards step,

192
00:11:31.020 --> 00:11:35.040
the Delta is the greatest value for the backwards step.

193
00:11:35.370 --> 00:11:40.050
We take the uh,
partial derivative of the output value,

194
00:11:40.051 --> 00:11:44.040
which is the predicted output and we multiply it by the gradient,
right?

195
00:11:44.040 --> 00:11:47.730
And that's going to give us the,
uh,
the,
the great value for that weight.

196
00:11:48.000 --> 00:11:51.660
And then we returned that.
And then one more function,

197
00:11:51.661 --> 00:11:53.950
which is the update function,
which says,
okay,

198
00:11:53.951 --> 00:11:58.750
you've got your Delta or your grading value,
multiply it by the input and that's,

199
00:11:58.990 --> 00:11:59.823
and then,

200
00:12:00.280 --> 00:12:04.630
and then subtract it from our weight values and multiply and multiply that by an

201
00:12:04.631 --> 00:12:06.310
Alpha value,
which is the learning rate.

202
00:12:06.610 --> 00:12:10.420
And that's going to give us the updated wait after we computed are backward

203
00:12:10.660 --> 00:12:15.400
propagation process.
Okay.
So we start off by saying we don't want this.

204
00:12:15.520 --> 00:12:17.550
We want this to be a deterministic algorithms.

205
00:12:17.551 --> 00:12:20.260
So it's going to compute the same results every time,

206
00:12:20.261 --> 00:12:23.200
which is useful for debugging.
We've got a number of examples,

207
00:12:23.290 --> 00:12:26.350
our output dimensions,
how many times do we want to train a thousand?

208
00:12:26.710 --> 00:12:28.210
And then we generate our Dataset,
right?

209
00:12:28.211 --> 00:12:32.800
Using the function we defined earlier for x and y.
Then we say,
okay,

210
00:12:32.980 --> 00:12:36.010
we've got our three layers and we define them as such,
right?

211
00:12:36.011 --> 00:12:38.290
We have our dimensions for the size of each,

212
00:12:38.410 --> 00:12:41.740
we have our nonlinearity and we have our nonlinearity derivative.

213
00:12:41.770 --> 00:12:45.550
Those are our three layers.
Then for our training process,
we're going to say,

214
00:12:45.551 --> 00:12:50.000
okay,
here are our batches of our x and y,
our training data and our,

215
00:12:50.001 --> 00:12:54.130
our training,
uh,
input data and our output data as well,
right?

216
00:12:54.280 --> 00:12:58.750
The two binary numbers concatenate it together would be a s a batch or a set of

217
00:12:58.810 --> 00:13:03.250
our training data.
And the why would be the expected output,
right?

218
00:13:03.400 --> 00:13:05.740
So what we do is we forward propagate,

219
00:13:05.770 --> 00:13:09.790
taking that first input data as the input to get the output value.

220
00:13:09.940 --> 00:13:11.140
And then we forward get forward,

221
00:13:11.141 --> 00:13:13.840
propagate that output as the input to the next layer.

222
00:13:14.020 --> 00:13:15.820
And that's going to give us the output for that layer.

223
00:13:15.940 --> 00:13:18.040
And then we take the output for that layer and we feed it,

224
00:13:18.041 --> 00:13:20.740
has the input for the next layer that's forward propagation.

225
00:13:21.040 --> 00:13:23.320
And so now we have this output prediction.

226
00:13:23.490 --> 00:13:26.890
When we subtract it from the expect expected outputs,

227
00:13:27.070 --> 00:13:31.060
and that's going to give us the error value and we use that error value to then

228
00:13:31.300 --> 00:13:35.410
as input to the backward propagation for the layer before that to give us a

229
00:13:35.411 --> 00:13:38.800
gradiance values,
right?
The delta,
the change.

230
00:13:39.820 --> 00:13:43.120
And it's using the partial derivative with respect to the weights for the

231
00:13:43.121 --> 00:13:46.960
previous layer to compute the gradient.
And we do that recursively every time.

232
00:13:47.380 --> 00:13:49.630
And once we have all of those gradients,

233
00:13:49.750 --> 00:13:53.950
then we can update every single layer and then we are

234
00:13:55.600 --> 00:13:58.060
done with that.
And so if I compile this,

235
00:13:58.360 --> 00:14:00.820
noticed how the loss decreases every time.

236
00:14:01.060 --> 00:14:03.400
And as the number of iterations increased,

237
00:14:03.490 --> 00:14:07.660
the loss is slowly and slowly and slowly getting smaller and smaller.
Okay?

238
00:14:07.661 --> 00:14:11.800
So that is the,
uh,
back propagation process.
So.
Okay,

239
00:14:11.801 --> 00:14:14.770
so now let's talk about the problem with backpropagation.

240
00:14:14.950 --> 00:14:18.550
So backpropagation is how all of supervised deep learning works.

241
00:14:18.551 --> 00:14:21.700
The majority of deep learning,
we have labels for our data,

242
00:14:21.730 --> 00:14:23.920
but there is a problem here.
So check this out.

243
00:14:23.950 --> 00:14:27.910
This is an example of backpropagation right here at very simple example of a

244
00:14:27.911 --> 00:14:29.350
three layer neural network.

245
00:14:29.500 --> 00:14:34.300
So a layer can only be updated after a full forward and a backward pass has been

246
00:14:34.301 --> 00:14:37.870
made.
So after layer,
the first layer has processed an input.

247
00:14:38.050 --> 00:14:40.420
It updates after the output activations.

248
00:14:40.421 --> 00:14:43.600
These black lines right here have been propagated through the rest of the

249
00:14:43.601 --> 00:14:46.600
network,
generated a loss and the error gradients,

250
00:14:46.601 --> 00:14:50.050
these green lines have back propagated through every single layer.

251
00:14:50.230 --> 00:14:55.130
And what this means is that layer one must wait for a full forward and backward

252
00:14:55.131 --> 00:14:58.400
pass.
Every single layer that comes after that.

253
00:14:58.520 --> 00:15:02.810
So layer one is locked,
it's coupled to the rest of the network.

254
00:15:03.080 --> 00:15:04.940
So for this simple example right here,

255
00:15:04.941 --> 00:15:08.060
or the one that I just demoed programmatically,
it's not a big deal.

256
00:15:08.420 --> 00:15:10.820
But when we get to more complex neural networks,

257
00:15:10.850 --> 00:15:15.740
that's where things start to get hairy.
Right?
For example,
the inception network,

258
00:15:15.741 --> 00:15:20.330
it's a huge convolutional network with 19 layers or the differential neural

259
00:15:20.331 --> 00:15:21.141
computer with,

260
00:15:21.141 --> 00:15:25.760
it's a separate memory unit or you have the neural Turing machine or you've got

261
00:15:25.800 --> 00:15:27.860
bi-directional recurrent networks,

262
00:15:27.861 --> 00:15:32.390
just insanely complicated neural networks with all sorts of moving parts.

263
00:15:32.930 --> 00:15:35.210
When we get to more complex systems,

264
00:15:35.480 --> 00:15:40.040
the time it takes to have to wait for the gradient to be back propagated through

265
00:15:40.041 --> 00:15:44.070
every layer is inefficient.
Ideally,
uh,

266
00:15:44.090 --> 00:15:45.380
we don't have to wait,
right?

267
00:15:45.381 --> 00:15:48.830
So over a big distributed network over multiple machines,

268
00:15:48.950 --> 00:15:51.830
this could take a very long time having to wait for every layer.

269
00:15:52.220 --> 00:15:54.500
So if we decouple every layer,

270
00:15:54.501 --> 00:15:59.300
the connections between them so they can update independently without having to

271
00:15:59.301 --> 00:16:03.470
wait for the other layers to update,
that would be more efficient.
Right?

272
00:16:03.860 --> 00:16:07.280
And so that's the idea behind synthetic gradients,
right?

273
00:16:08.390 --> 00:16:12.250
So normally a neural network compares its predictions.

274
00:16:12.280 --> 00:16:15.320
A data set to the side had an update.
It's weights,
right?

275
00:16:15.500 --> 00:16:19.460
It uses backpropagation to figure out how each way it should move in order to

276
00:16:19.461 --> 00:16:22.070
make a prediction.
But we'd synthetic gradients,

277
00:16:22.220 --> 00:16:27.220
individual layers make a best guess an approximation of what the gradient would

278
00:16:27.891 --> 00:16:32.210
be.
It's not the actual gradient.
It's a synthetic grade,

279
00:16:32.240 --> 00:16:35.420
a fake gray,
a predicted gradient.

280
00:16:36.020 --> 00:16:40.970
And so this best guess that each layer will make into synthetic rating model is

281
00:16:40.971 --> 00:16:42.140
called the synthetic graded.

282
00:16:42.560 --> 00:16:46.070
And the data is only used to help update each layers.
Guesser.

283
00:16:46.071 --> 00:16:49.340
So how are these synthetic gradients generated?

284
00:16:50.150 --> 00:16:51.590
I'm going to talk about that in a second,

285
00:16:51.890 --> 00:16:56.300
but what this does to system of approximating gradients at every layer is that

286
00:16:56.301 --> 00:17:00.380
allows individual layers to learn in isolation without having to wait for the

287
00:17:00.381 --> 00:17:03.410
other layers which increases the speed of training.

288
00:17:04.700 --> 00:17:09.560
So this is kind of what it looks like right here.
So we have some input,
right?

289
00:17:09.561 --> 00:17:13.340
We have some input data,
we apply it to the first layer,

290
00:17:13.341 --> 00:17:15.890
so input times weight out of bias activate,

291
00:17:16.130 --> 00:17:20.060
and then we send the output to the next layer.
Then rather than waiting for that,

292
00:17:20.200 --> 00:17:23.600
uh,
for that data to four,
propagate through every single layer,

293
00:17:23.810 --> 00:17:27.550
compute the error and then backward propagate the gradients again and again and

294
00:17:27.551 --> 00:17:29.030
again until we get that great hands.

295
00:17:29.450 --> 00:17:32.750
And having to wait for that full forward and backward pass before we can update

296
00:17:32.751 --> 00:17:35.810
it.
We just take the input apply.

297
00:17:36.230 --> 00:17:40.580
We just take the input times weight added,
bias activate,
send it out,

298
00:17:41.000 --> 00:17:45.230
and then we use what's called this synthetic gradient generator to immediately

299
00:17:45.290 --> 00:17:48.350
update the,
the wait for that network.

300
00:17:48.500 --> 00:17:51.960
So we take an input to compute and output and then update it without having to

301
00:17:51.961 --> 00:17:56.310
wait for,
for forward and backward pass.
And we do that for every single layer.

302
00:17:58.290 --> 00:18:02.220
So you might be wondering,
okay,
how is this possible?
Right?
This Gif,
how are,

303
00:18:02.250 --> 00:18:07.250
how are you able to generate a synthetic gradients without having,

304
00:18:07.680 --> 00:18:09.840
how are you able to do that?
Well,
the answer,

305
00:18:10.200 --> 00:18:15.200
the answer is the synthetic gradient generator is in fact another neural

306
00:18:15.661 --> 00:18:18.390
network.
It's another neural network.
It's a very,

307
00:18:18.391 --> 00:18:20.960
very simple single layer network,
right?
They,

308
00:18:20.980 --> 00:18:24.070
the deep mind found that even when they use a single layer linear,

309
00:18:24.071 --> 00:18:28.080
a layer as a neural network for the generator,
it still works really well,
right?

310
00:18:28.081 --> 00:18:30.690
So it doesn't have to be some complex neural network,

311
00:18:30.930 --> 00:18:34.920
but basically the synthetic gradient generator is a very simple neural network

312
00:18:35.100 --> 00:18:39.680
that's trained to take the output of a layer and predict what the gradient would

313
00:18:39.681 --> 00:18:41.790
be,
the likely gradient at that layer.

314
00:18:41.970 --> 00:18:45.720
It's simply a neural network that predicts what the gradient would be

315
00:18:47.490 --> 00:18:51.830
when we perform a full forward and backward pass,
then we get the correct grade.

316
00:18:52.110 --> 00:18:56.860
We can compare this to our synthetic gradient so we can train our synthetic

317
00:18:56.861 --> 00:19:00.150
gradient networks by pretending that our true gray ants are coming from this

318
00:19:00.151 --> 00:19:04.030
mythical dataset.
So if we look at this image right here,
uh,

319
00:19:04.080 --> 00:19:09.080
noticed how the gradient for m I plus two back propagates through f one plus f I

320
00:19:11.820 --> 00:19:15.840
plus one and into [inaudible] plus [inaudible] plus one.

321
00:19:16.080 --> 00:19:20.220
So each synthetic grating generator is actually only trained using the synthetic

322
00:19:20.221 --> 00:19:24.000
radiance generated from the next layer.
So for every layer,

323
00:19:24.150 --> 00:19:26.760
it doesn't depend on every single layer before that,
right?

324
00:19:26.820 --> 00:19:30.810
Because back propagation is a recursive process rather than having to wait for

325
00:19:30.811 --> 00:19:35.690
the gradient to be computed for every single layer.
It's only taking the,
the,

326
00:19:35.760 --> 00:19:38.670
it's only taking the computer great from the next layer.

327
00:19:38.910 --> 00:19:43.740
Every layer only depends on the next layer.
That's it.
So the,
the,

328
00:19:43.741 --> 00:19:47.130
the synthetic gradient generator is a neural network and it,
it,

329
00:19:47.170 --> 00:19:52.020
it generates a synthetic radiance and it compare that synthetic gradient to the

330
00:19:52.050 --> 00:19:56.340
actual gradients and an actual gradient is a great end that's computed from the

331
00:19:56.341 --> 00:19:59.400
next layer and it computes an error value using that.

332
00:19:59.700 --> 00:20:02.300
And then it updates its way.
It's using that and it's,

333
00:20:02.350 --> 00:20:06.420
and it sends the s the synthetically generated gradient to that layer.

334
00:20:06.600 --> 00:20:08.250
And that's what updates the layer.

335
00:20:08.280 --> 00:20:12.540
So every single layer is using this synthetic radiant generator to update its

336
00:20:12.541 --> 00:20:14.670
weights.
Only the last layer,

337
00:20:14.820 --> 00:20:18.810
Jen updates its weights using the true gradient value because there is no next

338
00:20:18.811 --> 00:20:21.250
gradient.
It's using the true,
uh,

339
00:20:21.270 --> 00:20:24.030
output label to compute it's gradient.

340
00:20:24.210 --> 00:20:28.350
And so what's happening is because every layer is dependent on the next layer,

341
00:20:28.380 --> 00:20:31.680
just just,
just on the next layer to compute it's synthetic gradients

342
00:20:33.250 --> 00:20:38.250
that what happens is it doesn't magically allow for a neural network to train

343
00:20:38.881 --> 00:20:43.080
without having that the true gradient backpropagation the true gradient is

344
00:20:43.260 --> 00:20:47.380
indeed still percolating backwards.
It's just slower.
You see what I'm saying?

345
00:20:47.590 --> 00:20:51.400
So the,
the true gradient is slowly percolating backwards,

346
00:20:51.401 --> 00:20:53.740
not like the real grading every time,
right?

347
00:20:53.740 --> 00:20:57.000
Because only the last layer is computing the,
uh,

348
00:20:57.070 --> 00:21:00.010
output using the true label of the data.

349
00:21:00.160 --> 00:21:03.890
But every other layer only uses the,
uh,

350
00:21:05.050 --> 00:21:08.230
only updates.
It's weights using this it's own,

351
00:21:08.620 --> 00:21:10.930
it's own respective synthetic gradient generator,

352
00:21:11.110 --> 00:21:14.800
which uses the gradient from the next layer as it's,
as it's true,

353
00:21:14.801 --> 00:21:17.890
great aunts versus it's synthetic gradients,
right?

354
00:21:17.891 --> 00:21:22.891
So the true gradient is slowly propagated backwards.

355
00:21:23.260 --> 00:21:23.780
However,

356
00:21:23.780 --> 00:21:27.760
because we're using the synthetic radiant model to approximate and smooth over

357
00:21:27.761 --> 00:21:31.200
the absence of true gradients,
it's faster.
And so they,

358
00:21:31.230 --> 00:21:34.780
they found that this could be applied to any type of neural network
architecture,

359
00:21:34.900 --> 00:21:38.590
not just feedforward networks,
recurrent networks.
It's an awesome strategy.

360
00:21:38.591 --> 00:21:42.550
I would like to see more of it,
uh,
integrated into major deep learning libraries.

361
00:21:42.820 --> 00:21:46.930
And it allows for the training of distributed networks to be faster and cleaner.

362
00:21:47.980 --> 00:21:52.540
So what I want to do is I want to update our code so that it's now using the

363
00:21:52.541 --> 00:21:55.810
synthetic grade model rather than the backpropagation model.

364
00:21:55.870 --> 00:22:00.700
I'll name this class the decoupled neural interface instead of a layer.
Okay.

365
00:22:00.701 --> 00:22:04.480
And so I'm going to say,
so this stuff stays the same as before.

366
00:22:04.540 --> 00:22:06.400
This doesn't change at all these weights,

367
00:22:06.401 --> 00:22:10.360
the nonlinearity and the nonlinearity derivative,
they don't change.

368
00:22:12.590 --> 00:22:17.030
But what changes is this,
we have some new stuff here.
You have some new stuff,

369
00:22:17.390 --> 00:22:20.480
and that's going to be the weights of the synthetic gradient generator,

370
00:22:22.700 --> 00:22:26.940
which we can initialize randomly,
just like we would for any,
uh,

371
00:22:27.140 --> 00:22:29.930
weight matrix for unregular neural network.

372
00:22:30.020 --> 00:22:33.500
And it's going to be generated using this output dimension size,

373
00:22:34.520 --> 00:22:35.353
just like that.

374
00:22:36.660 --> 00:22:37.493
<v 1>MMM.</v>

375
00:22:40.100 --> 00:22:43.790
<v 0>Just like that.
And we have,
it's,
it's going to have its own learning rate,
right?</v>

376
00:22:43.791 --> 00:22:47.660
So this is the synthetic gradient generator,
which is its own neural network.

377
00:22:48.170 --> 00:22:51.590
So now we want to update this forward propagation step,
right?
So

378
00:22:53.270 --> 00:22:53.800
<v 1>okay</v>

379
00:22:53.800 --> 00:22:55.420
<v 0>to update this for propagation stuff,</v>

380
00:22:55.460 --> 00:22:58.150
here's what it's going to look like when I update it for synthetic gradients.

381
00:22:58.360 --> 00:23:00.790
Okay,
so this used to be just the Ford set,

382
00:23:00.791 --> 00:23:04.300
but now we can update during the Ford pass using synthetic gradients.

383
00:23:04.600 --> 00:23:06.430
So we have some cash input,

384
00:23:06.670 --> 00:23:11.300
we forward propagate that input and then we generate a synthetic gradient.

385
00:23:11.301 --> 00:23:13.690
Be a simple linear transformation,
right?

386
00:23:13.900 --> 00:23:17.950
So we use that weight matrix to compute the a dot product with the output.

387
00:23:18.040 --> 00:23:22.480
And then we get our synthetic gradient,
right input times weight matrix,
no bias.

388
00:23:24.260 --> 00:23:29.000
Then we can update our regular weights using the synthetic gradient.

389
00:23:29.001 --> 00:23:33.170
So we take the partial derivative of the output times is synthetic radiant and

390
00:23:33.171 --> 00:23:36.530
that's going to give us the weight update that we can apply to our real weights

391
00:23:36.531 --> 00:23:39.260
for that layer.
It's called weight synthetic gradient.

392
00:23:39.470 --> 00:23:43.910
And the way we apply it is by multiplying the input times the transpose of the

393
00:23:43.911 --> 00:23:48.280
transpose of the product of that synthetic gradient out,
uh,

394
00:23:48.650 --> 00:23:51.020
synthetic gradient multiplied by learning rate.

395
00:23:51.110 --> 00:23:53.420
And that's how we update the wait for that layer.

396
00:23:53.780 --> 00:23:58.190
And finally we can return the back propagated synthetic gradient and the forward

397
00:23:58.191 --> 00:24:00.070
propagated output.
So we're,

398
00:24:00.071 --> 00:24:03.890
we're turning the back propagated synthetic gradient and we are returning the

399
00:24:03.891 --> 00:24:07.160
four propagated output at the same time,
right?
So,
uh,

400
00:24:07.370 --> 00:24:12.110
each layer gets an input it for propagates that input and at the same time it's

401
00:24:12.140 --> 00:24:15.710
updated would they synthetic gradient value,
which is very cool.

402
00:24:15.830 --> 00:24:18.770
And so in this case,
we don't even need this backward step.

403
00:24:18.890 --> 00:24:21.830
So we're going to replace the both of backward and the update step.

404
00:24:23.140 --> 00:24:23.790
<v 1>Yeah.</v>

405
00:24:23.790 --> 00:24:27.670
<v 0>Uh,
with this new update,
synthetic weights,
a method where we,</v>

406
00:24:28.210 --> 00:24:31.330
this is just like the update method from before accepted operates on this

407
00:24:31.331 --> 00:24:35.650
synthetic weights,
right?
So we are updating the synthetic gradient,
a generators,

408
00:24:35.651 --> 00:24:37.480
weights that separate neural network.

409
00:24:37.510 --> 00:24:39.850
We've already updated our real layers weights.

410
00:24:40.000 --> 00:24:42.250
Now we want to update this synthetic gradient waste,
right?

411
00:24:42.251 --> 00:24:46.690
So what we do is we say we take the synthetic gradient minus the true gradient,

412
00:24:46.830 --> 00:24:51.460
which is the gradient from the next layer.
And we use that to compute the,
the,

413
00:24:51.461 --> 00:24:52.660
the Delta,
the change.

414
00:24:52.690 --> 00:24:57.190
And then we use that Delta to update the weights of our synthetic radiant

415
00:24:57.191 --> 00:25:01.310
generator.
So then what is the difference here?

416
00:25:01.340 --> 00:25:05.930
While the difference now is instead of using each delayer class to define these

417
00:25:05.931 --> 00:25:06.764
layers,

418
00:25:07.930 --> 00:25:08.240
<v 1>okay,</v>

419
00:25:08.240 --> 00:25:12.410
<v 0>we'll use the decoupled neural interface class where we have our input</v>

420
00:25:12.411 --> 00:25:16.610
dimensions as for the size of the input,
the sigmoid and this,

421
00:25:16.660 --> 00:25:20.180
the the activation function,
and then the derivative,
and then the learning rates.

422
00:25:20.390 --> 00:25:22.310
And that's how we define every single layer.

423
00:25:22.480 --> 00:25:24.710
Generating our batches stay the stays the same,

424
00:25:24.711 --> 00:25:29.711
but the forward and the backward pass are changed entirely rather than it being

425
00:25:30.051 --> 00:25:32.420
a separate forward and a backward pass.

426
00:25:32.630 --> 00:25:37.630
We have instead a forward and synthetic update pass and then we can update the

427
00:25:37.911 --> 00:25:40.820
synthetic gradient generators after that.
So

428
00:25:41.750 --> 00:25:42.350
<v 1>yeah,</v>

429
00:25:42.350 --> 00:25:43.790
<v 0>let me just move this out of the way.</v>

430
00:25:44.950 --> 00:25:45.650
<v 1>Okay.</v>

431
00:25:45.650 --> 00:25:47.330
<v 0>Right.
So we have,</v>

432
00:25:47.390 --> 00:25:51.680
for every layer we're doing are we're for propagating the input data from that

433
00:25:51.681 --> 00:25:52.400
layer.

434
00:25:52.400 --> 00:25:56.840
And at the same time we are updating its weights using the synthetic radiant,

435
00:25:57.310 --> 00:26:02.180
uh,
when we do that for every single layer.
Okay.
And so then once we have that,

436
00:26:02.300 --> 00:26:07.070
we want to update the synthetic gradient generators,
weights.
So we,
we say,
okay,

437
00:26:07.160 --> 00:26:10.460
we know what the,
uh,
real output is going to be.

438
00:26:10.550 --> 00:26:15.440
So this layer three Delta is the true gray and we only compute that for the last

439
00:26:15.441 --> 00:26:19.970
layer.
Every other layer is updated using synthetic radiant generator.

440
00:26:20.000 --> 00:26:24.530
It's respective synthetic rating generator.
And
then,

441
00:26:24.531 --> 00:26:29.390
and only then do we update.
Uh,
so that's the old to only the last layer is,

442
00:26:29.660 --> 00:26:33.410
is normally updated.
It doesn't have its own synthetic gradient generator,

443
00:26:33.411 --> 00:26:37.940
but every other layer does.
And so we use that gradient to then update,
uh,

444
00:26:37.970 --> 00:26:42.380
the synthetic weights for each of the generators,
uh,
backwards.

445
00:26:42.740 --> 00:26:45.690
Right?
And so that's going to end,
so,
right.

446
00:26:45.691 --> 00:26:48.750
And so that's how synthetic radiance work in a nutshell.

447
00:26:49.110 --> 00:26:53.460
And I think it's a really cool idea.
I'd like to see more people looking at it.

448
00:26:53.580 --> 00:26:56.740
I like to see more of an applied in deep learning.
Tensorflow.

449
00:26:56.760 --> 00:27:00.990
All the major libraries should,
should have synthetic gradients as an,
as a,

450
00:27:00.991 --> 00:27:04.680
as a Dalit optimization scheme.
And I think we're gonna see more of that.

451
00:27:05.220 --> 00:27:08.160
If you want to learn more about artificial intelligence,
about blockchain,

452
00:27:08.161 --> 00:27:12.390
about some of the coolest technology out there,
hit the subscribe button for now.

453
00:27:12.450 --> 00:27:15.900
I've got to go define proof of proof of proof.
So thanks for watching.

