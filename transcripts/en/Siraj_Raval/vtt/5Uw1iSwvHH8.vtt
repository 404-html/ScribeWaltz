WEBVTT

1
00:00:00.180 --> 00:00:02.420
To
one.

2
00:00:10.110 --> 00:00:14.210
Just
starting.

3
00:00:14.350 --> 00:00:18.640
<v 1>All right.
We are live hello world.
It's to Raj and welcome to this live stream.</v>

4
00:00:18.641 --> 00:00:22.260
I'm so excited to be here with uh,
with my new live stream set up.
Uh,

5
00:00:22.450 --> 00:00:25.990
in this video we're going to be predicting stock prices.

6
00:00:25.991 --> 00:00:30.160
So financial forecasting using tensorflow dot.
Js.
Um,
in this,

7
00:00:30.220 --> 00:00:33.940
so what you're seeing right here is a graph of us predicting stock prices using

8
00:00:33.941 --> 00:00:36.130
what's called a recurrent neural network.

9
00:00:36.370 --> 00:00:39.460
But the actual aim of this video is to build,
um,

10
00:00:39.580 --> 00:00:42.600
a financial forecasting model using a convolutional network.

11
00:00:42.601 --> 00:00:43.870
So we're going to be looking at both.

12
00:00:43.871 --> 00:00:47.660
So I'm actually going to code out a convolutional network,
um,

13
00:00:47.680 --> 00:00:48.850
and the second half of the video,

14
00:00:48.851 --> 00:00:51.400
but in the first half we're going to talk through some math.

15
00:00:51.490 --> 00:00:53.950
We're going to talk through the different models out there for financial

16
00:00:53.951 --> 00:00:54.784
forecasting.

17
00:00:54.820 --> 00:00:57.520
And I'm going to also answer some audience questions because what is a live

18
00:00:57.521 --> 00:00:59.620
stream without some audience questions,
right?

19
00:00:59.970 --> 00:01:00.830
<v 0>So,
um,</v>

20
00:01:02.580 --> 00:01:05.690
<v 1>hello everybody.
Yes,
good to see so many people here.
Um,</v>

21
00:01:05.710 --> 00:01:08.550
I'm going to just like sporadically be answering questions in five minute

22
00:01:08.551 --> 00:01:11.950
intervals,
but really it's,
it's got to be related to the topic,
right?
Um,

23
00:01:12.090 --> 00:01:15.480
and also in terms of a demo,
I've got a great demo for us here in terms of,
uh,

24
00:01:15.510 --> 00:01:18.690
you know,
an,
an actual visual demo.
What this is,

25
00:01:18.691 --> 00:01:22.140
is a nice little web interface that I'm running off of local host.

26
00:01:22.350 --> 00:01:25.830
And what this is doing is it's using a neural network to make predictions based

27
00:01:25.831 --> 00:01:27.570
on just simple binary data,
right?

28
00:01:27.571 --> 00:01:31.470
So this is just an example of us being able to visualize what it looks like,

29
00:01:31.471 --> 00:01:34.310
right?
So what we see here are the actual labels,
right?

30
00:01:34.320 --> 00:01:38.310
The outputs zero one zero one.
And we have our predictions which are,

31
00:01:38.340 --> 00:01:41.440
which are right here inside of this tensor.
Um,

32
00:01:41.550 --> 00:01:44.580
and they look very similar and they're going to get better and better over time,

33
00:01:44.581 --> 00:01:46.290
right?
So that's,
that's the visual demo,

34
00:01:46.470 --> 00:01:49.770
but we're going to be predicting apple stock.
Okay.
So that's,
that's the,

35
00:01:49.800 --> 00:01:52.080
that's the actual aim of this video.
All right.

36
00:01:52.081 --> 00:01:55.440
So I want to start off by just answering two questions and then we're going to

37
00:01:55.441 --> 00:01:59.850
go right into the,
uh,
into the code.
All right,
so the question one is,

38
00:02:00.900 --> 00:02:04.140
should I do a masters from the USA,
uh,

39
00:02:04.170 --> 00:02:08.910
or weights to earn some money here and go for a phd later?
Um,
okay,

40
00:02:08.911 --> 00:02:13.480
so I think
first of all,
if you're going to do a masters or a phd,

41
00:02:13.481 --> 00:02:15.700
it has to be at a really good school,
right?

42
00:02:15.701 --> 00:02:17.680
It's gotta be at like a top 10 school.

43
00:02:17.860 --> 00:02:19.990
And if you're not going to go to a top 10 school,

44
00:02:20.020 --> 00:02:23.890
you might as well just do this yourself using the internet as your university.

45
00:02:24.040 --> 00:02:27.640
I mean,
look at the most popular research institution in the world,
right?

46
00:02:27.670 --> 00:02:30.160
Deep Mind,
if you look at their jobs page,

47
00:02:30.280 --> 00:02:33.580
they don't even have phd listed as a requirement,
right?

48
00:02:33.581 --> 00:02:37.810
What a phd does is it does,
it doesn't just say,
you know,
you are the greatest.

49
00:02:37.811 --> 00:02:38.260
What it's,

50
00:02:38.260 --> 00:02:43.260
what it really says is you have published actual research under the advisory of

51
00:02:43.301 --> 00:02:45.250
a professor or someone who is more knowledgeable.

52
00:02:45.430 --> 00:02:48.670
You can do all of that with the Internet.
You can find an advisor online,

53
00:02:49.060 --> 00:02:53.620
you can find educational resources online.
Right?
So,
so that's my thought on that.

54
00:02:53.621 --> 00:02:55.480
And one more question before we get into the code.

55
00:02:55.920 --> 00:02:56.753
<v 0>MMM.</v>

56
00:02:57.640 --> 00:02:59.800
<v 1>Why Not Python?
Great.
So guys,</v>

57
00:02:59.860 --> 00:03:03.310
javascript is really coming up in the machine learning space right now.

58
00:03:03.430 --> 00:03:07.690
Right now you can build a model in python important to javascript and vice
versa.

59
00:03:07.960 --> 00:03:08.800
And uh,
yeah,

60
00:03:08.801 --> 00:03:12.310
so javascript is really becoming easier and easier to use for machine learning

61
00:03:12.311 --> 00:03:16.330
models.
And you're going to also start to see more developers build models in

62
00:03:16.400 --> 00:03:21.280
javascript.
But most of the complex models are built in python,
right?

63
00:03:21.281 --> 00:03:25.640
So if I want to see someone build Alpha,
go in Javascript,

64
00:03:25.660 --> 00:03:27.670
and once that happens and it will happen,

65
00:03:27.910 --> 00:03:31.870
then I think the playing field will be a lot more,
you know,
level.
Anyway,

66
00:03:31.871 --> 00:03:33.640
back to this,
to the two of them are right?

67
00:03:33.641 --> 00:03:38.641
So time series data is starting to play a larger and larger role in our world,

68
00:03:39.221 --> 00:03:43.810
right?
So examples in crude examples include self driving.
Tesla is right.

69
00:03:43.811 --> 00:03:48.430
So Teslas are constantly creating time series data right there,

70
00:03:48.431 --> 00:03:52.090
mapping out to where they are.
And then these are three d point clouds.

71
00:03:52.180 --> 00:03:56.470
And then over time,
this,
this comes out to be a time series,
right?
So by the way,

72
00:03:56.471 --> 00:03:59.680
at time series is a sequence of data points,
right?

73
00:03:59.681 --> 00:04:01.480
Measuring the same thing over time.

74
00:04:01.660 --> 00:04:05.470
This could be three d data points through a three d map points for Teslas.

75
00:04:05.680 --> 00:04:08.260
This can be stock prices,
this could be smart homes,

76
00:04:08.261 --> 00:04:12.490
measuring temperature changes over time.
This could be open data,

77
00:04:12.491 --> 00:04:17.050
publishing a police departments of crimes happening at certain times.
So time,

78
00:04:17.230 --> 00:04:20.200
time is the keyword here.
As data changes over time,

79
00:04:20.380 --> 00:04:22.810
that measurement is a time series.

80
00:04:23.020 --> 00:04:25.640
So imagine sensors collecting data from three settings,
right?

81
00:04:25.641 --> 00:04:28.120
Do you have a factory?
You have a city and you have a farm.

82
00:04:28.300 --> 00:04:29.620
And so it's measuring temperature.

83
00:04:29.650 --> 00:04:32.950
It's a measuring the number of people in a room.
It's measuring,
you know,

84
00:04:32.980 --> 00:04:36.590
the output of a factory.
All of these are static,
um,

85
00:04:36.610 --> 00:04:40.300
data points that change over time.
And because time is involved,

86
00:04:40.301 --> 00:04:44.440
it's a time series.
Exactly.
Cool.
Right?

87
00:04:44.441 --> 00:04:49.441
So when it comes to financial forecasting or Rema is like the Goto model,

88
00:04:49.511 --> 00:04:53.710
right?
So,
uh,
Rema is called is the Goto model.
Um,

89
00:04:53.740 --> 00:04:57.010
and it stands for auto regressive.
Um,
it's a,

90
00:04:57.011 --> 00:04:59.980
it's an auto regressive integrated moving average.

91
00:05:00.190 --> 00:05:04.750
So the first question here is,
is a Rhema,
a machine learning model?

92
00:05:05.140 --> 00:05:09.040
Our rema I you can consider it a regression model.
It is,

93
00:05:09.250 --> 00:05:12.400
it is a regression model.
Is it a machine learning model?

94
00:05:12.730 --> 00:05:17.730
I would say no because it was handpicked for a specific use case stock

95
00:05:18.460 --> 00:05:22.210
prediction.
Um,
but you can't really use it for things outside of that.
Right?

96
00:05:22.590 --> 00:05:26.860
It was handpicked and hand created for a specific use case.
Whereas a model,

97
00:05:26.861 --> 00:05:27.820
a machine learning model,

98
00:05:27.821 --> 00:05:31.930
like say a support vector machine is made for a more general use case.

99
00:05:31.930 --> 00:05:35.350
You can use support vector machines to classified cat faces and you can use

100
00:05:35.351 --> 00:05:39.490
support vector machines,
the classify stock prices,
you know,
anything really.

101
00:05:39.850 --> 00:05:40.850
But a Rhema,
you know,

102
00:05:40.870 --> 00:05:44.170
have a little algorithmic outline of what our Rima looks like here.

103
00:05:44.320 --> 00:05:48.370
But what it comes down to is us approximating three different variables.

104
00:05:48.670 --> 00:05:52.150
The first variable is the number of auto regressive terms.

105
00:05:52.270 --> 00:05:56.620
The second variable is the non seasonal differences needed for stationarity.

106
00:05:56.840 --> 00:06:01.000
And the third variable is the lag forecast errors in prediction equations.

107
00:06:01.001 --> 00:06:05.210
So all three of these variables have their own associated equations.

108
00:06:05.360 --> 00:06:06.770
So we're calculating all three.

109
00:06:07.010 --> 00:06:10.490
And so it turns out that this has been the most popular model,
uh,
for,

110
00:06:10.520 --> 00:06:12.140
for stock price forecasting.

111
00:06:12.290 --> 00:06:17.120
But I'm not going to go into that because with the advent of deep learning

112
00:06:17.121 --> 00:06:19.370
technology,
very recently,
we can,

113
00:06:19.400 --> 00:06:24.380
we can outperform models like that and there's reasons and there's reason to

114
00:06:24.381 --> 00:06:27.950
believe we can because there's a bunch of machine learning models out there,

115
00:06:27.951 --> 00:06:28.160
right?

116
00:06:28.160 --> 00:06:31.790
But neural networks when given a lot of data and a lot of computing power tend

117
00:06:31.791 --> 00:06:35.150
to outperform all of them most of the time.
So why wouldn't they,

118
00:06:35.151 --> 00:06:38.090
when it comes to financial forecasting and you might be thinking,
well,

119
00:06:38.091 --> 00:06:42.350
why don't I hear more about this?
Because big banks and you know,

120
00:06:42.710 --> 00:06:45.610
hedge funds,
of course,
they're not going to give away these models,
right?

121
00:06:45.650 --> 00:06:48.890
That this is not an open source,
uh,
industry,
right?

122
00:06:48.891 --> 00:06:51.170
Finance is not an open source industry.

123
00:06:51.350 --> 00:06:53.930
We'll talk about bitcoin and all of that later.
But like in general,

124
00:06:53.931 --> 00:06:56.870
traditional finance.
So these are very close source models.

125
00:06:56.871 --> 00:07:01.490
Why am I making a video on this?
Because I want you guys to win and I don't care.

126
00:07:01.610 --> 00:07:06.290
I,
I'm,
I'm,
it's,
it's not about,
you know,
that for me it's about democratizing AI.

127
00:07:06.291 --> 00:07:09.550
So this is just for me,
this is like,
I don't even care about this,
but I,

128
00:07:09.750 --> 00:07:12.590
I don't even care about like predicting stock prices myself.

129
00:07:12.620 --> 00:07:15.200
But I do care about you guys knowing about recurrent networks,

130
00:07:15.201 --> 00:07:18.500
about convolutional networks.
And so this is a medium for me to,

131
00:07:18.620 --> 00:07:23.270
to explain that to you.
Okay?
So why deep learning,
right?
So I,

132
00:07:23.300 --> 00:07:24.440
like I said,
when you,
when you,

133
00:07:24.480 --> 00:07:26.960
when you look at the performance versus the amount of data,

134
00:07:27.080 --> 00:07:30.980
deep learning outperforms all of those other models when it comes to having a

135
00:07:30.981 --> 00:07:34.820
lot of data.
So what is the obvious approach here?

136
00:07:34.821 --> 00:07:38.180
Now this is very exciting,
right?
So what is the obvious approach here?

137
00:07:38.181 --> 00:07:42.050
The obvious approach when it comes to time series prediction is to use a

138
00:07:42.051 --> 00:07:44.390
recurrent neural network.
Okay.
So I'm going to,

139
00:07:44.780 --> 00:07:48.740
I'm going to explain how this works mathematically.
So we have some input data,

140
00:07:48.741 --> 00:07:52.670
right?
So first of all,
first of all,
we have a sequence,
right?

141
00:07:52.671 --> 00:07:56.450
So we have a sequence of numbers.
Let's say zero one,
two,
three,
four,
five,
six,

142
00:07:56.451 --> 00:07:58.940
seven,
eight,
nine,
10 and we want,

143
00:07:58.941 --> 00:08:02.810
we were going to use a first nine numbers to predict what the next number is.

144
00:08:02.810 --> 00:08:06.560
It's going to be 10 and there is some function that describes this sequence.

145
00:08:06.590 --> 00:08:11.590
In our case that function would be f of x equals x plus one,

146
00:08:12.230 --> 00:08:16.160
right?
Where x is the term that we are trying to predict the next term it right?

147
00:08:16.161 --> 00:08:20.330
That that's the function.
And we can use an recurrent network to map out this,

148
00:08:20.540 --> 00:08:24.320
this,
um,
this function and learn what this function is.

149
00:08:24.500 --> 00:08:26.630
There's a function that represents everything in life.

150
00:08:26.631 --> 00:08:29.540
There's a function that represents how much attention you are paying.
To me,

151
00:08:29.541 --> 00:08:32.900
there's a function that represents how loud my voice is in relation to

152
00:08:32.901 --> 00:08:35.930
everything else.
There's a function for beauty as a function for love.

153
00:08:36.230 --> 00:08:39.410
There's a function for everything and we can learn these functions using machine

154
00:08:39.411 --> 00:08:42.230
learning technology,
right?
So when it comes to recurrent networks,

155
00:08:42.440 --> 00:08:46.010
normally would feed forward networks.
We are feeding in every new data point,

156
00:08:46.011 --> 00:08:49.850
right?
So we have term one,
term two term three term four and that's it.

157
00:08:50.060 --> 00:08:53.450
But we're current networks.
They're fed in not just the new data points,

158
00:08:53.660 --> 00:08:57.120
they're fed in the hidden state,
the learned weight value,

159
00:08:57.121 --> 00:09:00.990
that Matrix that's initializes to zero,
zero,
zero,
zero,
right?

160
00:09:01.170 --> 00:09:05.790
Just all zeroes.
That's getting optimized over time.
It's fed that in as well.

161
00:09:05.791 --> 00:09:10.320
So it's the input and the old weight matrix from the last time step that are

162
00:09:10.321 --> 00:09:14.670
both concatenated together,
combined together and then fed to the model.

163
00:09:14.730 --> 00:09:16.170
And that just keeps repeating.

164
00:09:16.380 --> 00:09:20.850
And because the model is fed in the previously learned matrix values from the

165
00:09:20.851 --> 00:09:24.630
weight from the last time step,
it's considered a recurrent network.

166
00:09:24.720 --> 00:09:28.410
So mathematically speaking,
we have our input x of t.
Okay,

167
00:09:28.411 --> 00:09:33.360
so where t is the time variable where we have x of t equals w of t,

168
00:09:33.570 --> 00:09:37.350
which is the input data plus s of t minus one,

169
00:09:37.380 --> 00:09:40.650
where that is the hidden state from the last time step.

170
00:09:40.710 --> 00:09:43.230
So it's fed both together.
That's the input.

171
00:09:43.500 --> 00:09:46.590
And so when it comes to the hidden layers for as many as we have,

172
00:09:46.770 --> 00:09:49.680
and that's what the,
this sigma value means,
right?
This big eat,

173
00:09:49.860 --> 00:09:52.260
it's sigma notation.
It means take this,

174
00:09:52.530 --> 00:09:56.550
this a operation or these sets of variables here and whatever operations there

175
00:09:56.551 --> 00:10:01.410
are and add them up for as many as you have.
So in our case,

176
00:10:01.411 --> 00:10:02.850
that would be the number of data points.

177
00:10:03.090 --> 00:10:06.660
So we're going to take that input value x of t like you we just talked about

178
00:10:06.661 --> 00:10:10.780
here and we're gonna multiply it by the weight value right at the,

179
00:10:10.781 --> 00:10:14.100
at that time step.
And that's going to it.
And the function of that,

180
00:10:14.130 --> 00:10:18.390
that operation is our hidden layer.
Okay?
And that's why it's called SMT.

181
00:10:18.510 --> 00:10:22.770
And that's what up here we say sot minus one because this is our new hidden

182
00:10:22.771 --> 00:10:24.480
layer,
our new hidden state.

183
00:10:24.900 --> 00:10:27.840
And this is our old hidden state from the last time step.

184
00:10:28.200 --> 00:10:31.980
And so when now we have this hidden state,
we can use it to compute an output.

185
00:10:32.100 --> 00:10:34.440
So we take that state SFT,

186
00:10:34.740 --> 00:10:39.740
we multiply it by the last weight Value v ofK and we do that for as many inputs

187
00:10:39.901 --> 00:10:43.500
as we have.
That's why the sigma notation is there.
That's our g of,

188
00:10:43.620 --> 00:10:44.790
that's our g of x.

189
00:10:44.791 --> 00:10:49.260
X is this function and the result is our output and the output is a label or it

190
00:10:49.261 --> 00:10:53.820
could be the next term in the sequence and that's why of t.
Okay,
so that's,

191
00:10:54.120 --> 00:10:56.640
that is a recurrent neural network.
It's,
it's a,

192
00:10:56.700 --> 00:11:01.700
it's a universal function approximator that is learning a function by being fed

193
00:11:01.831 --> 00:11:05.670
in new data points as well as what it has learned in the previous time step.

194
00:11:05.910 --> 00:11:08.850
And this is the Goto strategy for regression models,
right?

195
00:11:08.850 --> 00:11:12.150
Predicting the next data point in a,
in a sequence.

196
00:11:12.660 --> 00:11:15.360
But there is a problem and before I talk about that problem,

197
00:11:15.361 --> 00:11:18.300
I'm going to answer two questions again from the livestream.

198
00:11:20.190 --> 00:11:24.210
Okay.
So the first question is how old are you?
That's not related,

199
00:11:24.211 --> 00:11:27.750
but I'm going to answer it.
I am 27 years old.
Can you believe that?

200
00:11:27.780 --> 00:11:31.980
I'm 27 years old.
What that means is if you are 27 younger,

201
00:11:31.981 --> 00:11:36.570
older age doesn't matter,
right?
Nothing.
We live in an absurd reality,

202
00:11:36.780 --> 00:11:40.410
right?
We're credentials,
a age where you're from.

203
00:11:40.700 --> 00:11:44.610
All of that doesn't matter anymore.
All that matters is how motivated you are.

204
00:11:44.611 --> 00:11:47.730
Cause you got the internet,
you can literally do anything you want.

205
00:11:47.731 --> 00:11:52.470
Now you have the,
you can become a world expert in AI as a college dropout.

206
00:11:53.140 --> 00:11:56.110
You can do anything.
Right?
So that's the reality we're living in.

207
00:11:56.111 --> 00:12:01.090
I'm 27 and one more question about Ai,
specifically 10,

208
00:12:01.091 --> 00:12:03.940
we use reinforcement learning for time series data.

209
00:12:03.941 --> 00:12:07.120
You guys are speaking my language.
Oh,

210
00:12:07.330 --> 00:12:12.330
did someone just donate some money a hundred pounds or your euros?

211
00:12:13.030 --> 00:12:14.380
Ah,
so I got to answer this question.

212
00:12:14.381 --> 00:12:17.930
I didn't even know I ne I enabled this but um,
uh,
what was the first question?

213
00:12:17.970 --> 00:12:20.830
Time series?
Yes you can.
You can.

214
00:12:20.890 --> 00:12:25.450
And I have a video coming out on that tomorrow rupees.
Okay.

215
00:12:25.720 --> 00:12:28.420
Is Learning.
So debunk the demonic asks,

216
00:12:28.450 --> 00:12:32.980
is learning tensor flow required or being able to use care ROSC is fine.

217
00:12:33.250 --> 00:12:37.000
Look up the,
the more you learn,
the better.

218
00:12:37.360 --> 00:12:37.610
<v 2>Yeah.</v>

219
00:12:37.610 --> 00:12:41.120
<v 1>The more you learn,
the better you're going to be.
Okay.
But I would say yes.</v>

220
00:12:41.150 --> 00:12:44.690
Learn tensor flow.
Don't just learn careless.

221
00:12:44.720 --> 00:12:48.890
Start off with care os cause it's easy.
Then learn flow,

222
00:12:49.130 --> 00:12:53.120
then learn.
Uh,
just,
just progressively get harder and harder over time.

223
00:12:53.270 --> 00:12:57.260
Don't just start with like buying a giant Linear Algebra textbook and saying,

224
00:12:57.290 --> 00:12:59.090
I'm going to start learning machine learning today.

225
00:12:59.300 --> 00:13:02.150
And then you're like learning about things that you don't necessarily need for

226
00:13:02.151 --> 00:13:02.990
machine learning.
Right?

227
00:13:03.170 --> 00:13:08.000
Just give yourself these small incremental rewards to keep you going on the

228
00:13:08.001 --> 00:13:11.900
learning path.
This is what I do.
Okay.
And as you get those rewards,

229
00:13:11.901 --> 00:13:15.380
you're going to become more and more confident in this space because in the end,

230
00:13:15.590 --> 00:13:18.910
a degree,
a credential,
a phd,
all that is,

231
00:13:18.911 --> 00:13:23.720
is you being able to tell yourself that you have learned something.
Right?

232
00:13:23.721 --> 00:13:26.810
So as long as you can tell yourself that you've learned something,

233
00:13:26.990 --> 00:13:30.620
you will be more confident to learn the next subject in the future.

234
00:13:33.980 --> 00:13:34.813
<v 2>Uh,</v>

235
00:13:35.140 --> 00:13:36.970
<v 1>right.
Okay.
Anyway guys.
So back to this,</v>

236
00:13:37.570 --> 00:13:39.520
there's a problem with recurrent networks.

237
00:13:39.550 --> 00:13:43.690
The gradients that's computed to optimize the network.
Over time it,

238
00:13:43.810 --> 00:13:47.230
it diminishes advantage is slowly,
right because forward.

239
00:13:47.231 --> 00:13:51.730
So there are two processes to processes in a neural network.
Okay,
I'm going to,

240
00:13:51.731 --> 00:13:55.690
I'm about to go into the math right now so you better hold onto your butts.
Okay.

241
00:13:55.691 --> 00:14:00.070
This is the system.
Good stuff.
When we are forward propagating the gradients,

242
00:14:00.100 --> 00:14:04.540
why we're doing input times weight,
add a bias,
activate right?

243
00:14:05.050 --> 00:14:09.850
Input Tom's weight,
add a bias,
activate and we just repeat that over and over.

244
00:14:09.851 --> 00:14:13.000
And that's,
that's the feed forward process.
Until we get that output value,

245
00:14:13.390 --> 00:14:16.360
we have that output value.
We compare it with our actual label.

246
00:14:16.361 --> 00:14:17.650
We compute an error,

247
00:14:17.920 --> 00:14:21.760
we use the era to compute the partial derivative with respect to our weight

248
00:14:21.761 --> 00:14:26.350
values.
Wait one week,
two week three in the reverse order.
So we go backwards.

249
00:14:26.560 --> 00:14:31.090
We use that partial derivative to construct a gradient.
And a gradient is a,

250
00:14:31.091 --> 00:14:36.091
is a list of partial derivatives that we can use recursively to come to update

251
00:14:37.661 --> 00:14:39.520
those wait values using that gradient.

252
00:14:39.730 --> 00:14:42.640
But the problem is over time that gradient gets smaller and smaller and smaller

253
00:14:42.641 --> 00:14:45.370
and smaller and smaller until it's not.
So the,
the,
the,

254
00:14:45.371 --> 00:14:49.510
the layers at the beginning of the network aren't updated as much as the layers

255
00:14:49.511 --> 00:14:53.630
at the end of the network and is a problem because we want that gradient to not

256
00:14:53.631 --> 00:14:57.410
diminish over time because we want our network to learn sequences.

257
00:14:58.220 --> 00:15:02.480
So someone came up with a solution called an LSTM network.

258
00:15:02.510 --> 00:15:07.430
What this is,
is turning one equation in two,
three equations.
That's what it is.

259
00:15:07.460 --> 00:15:10.070
And we can talk about,
you know,
gates and all of that.

260
00:15:10.071 --> 00:15:12.320
But really it really comes down to is this,

261
00:15:12.950 --> 00:15:15.170
this set of four equations here.

262
00:15:15.200 --> 00:15:17.330
The three equations really just make this one equation.

263
00:15:17.331 --> 00:15:18.560
So I'll talk about this in a second.

264
00:15:19.040 --> 00:15:23.960
What we are really doing and an LSTM network versus a normal recurrent network

265
00:15:24.350 --> 00:15:29.240
is we are replacing one weight with three weights.
The right,

266
00:15:29.241 --> 00:15:30.680
we have that input times.
Wait,

267
00:15:30.681 --> 00:15:35.210
there's a weight that's a matrix at every layer as a group of numbers that are,

268
00:15:35.270 --> 00:15:39.980
that are learning over time.
What an Lstm is,
is three weights.

269
00:15:39.981 --> 00:15:42.860
We have an input gate,
uh,
forget gate and an update gates.

270
00:15:42.861 --> 00:15:45.830
And all of these gates are mini perceptrons.

271
00:15:45.831 --> 00:15:50.030
There many neural networks and they all have their own weights and they're

272
00:15:50.031 --> 00:15:51.590
represented by these equations.

273
00:15:51.980 --> 00:15:55.190
So what we do is when we have an input and it goes into the first layer,

274
00:15:55.191 --> 00:15:59.150
which is an LSTM cell,
this is a cell,
all of this for questions.

275
00:15:59.840 --> 00:16:03.920
We multiply the first wait by the hidden state times the input,

276
00:16:03.950 --> 00:16:08.660
we apply a sigmoid activation function to it and that gives us a value z of t.

277
00:16:09.230 --> 00:16:12.590
Now we take that value,
we,
we do the same for that.
So that's our,

278
00:16:12.800 --> 00:16:15.980
we have an input gates we ever forget gate,
we have an update gate.

279
00:16:16.310 --> 00:16:19.260
So we do that for the input gate.
Then we have a forget gate.
We'd,

280
00:16:19.670 --> 00:16:24.230
we multiplied that waits by the hin state times x and that gives us our sigmoid.

281
00:16:24.560 --> 00:16:27.890
We have,
uh,
the third weight value,
our third gates,

282
00:16:28.400 --> 00:16:33.400
weight value times hidden state times the previously computed value here times

283
00:16:34.271 --> 00:16:38.450
the input.
And we apply the 10 h activation function to it,
not the sigmoid.

284
00:16:38.810 --> 00:16:40.820
And this gives us h of t with the,

285
00:16:43.220 --> 00:16:44.420
with the squiggly over it.

286
00:16:44.870 --> 00:16:49.370
And we use all three of those values here to compute the final hidden state.

287
00:16:49.371 --> 00:16:52.940
So what are we doing?
First of all,
this is confusing.
That's okay.
Um,

288
00:16:53.130 --> 00:16:54.260
I'm going at it right now.

289
00:16:55.340 --> 00:16:58.040
What are we really doing in every layer of a neural network?

290
00:16:58.040 --> 00:17:01.310
We're computing a hidden state.
So that's what this is,
this hidden state.

291
00:17:01.311 --> 00:17:04.040
It's the same result.
We're a computing a Hayden states,

292
00:17:04.370 --> 00:17:08.240
but we're using three wait values instead of one weight value to compute that

293
00:17:08.241 --> 00:17:11.930
hidden state.
And that's what that is.
And we repeat that over and over again.

294
00:17:11.931 --> 00:17:15.710
And what this does is inside of those three weights,

295
00:17:16.700 --> 00:17:21.320
the gradient is being captured,
right?
It's learning what to forget,

296
00:17:21.321 --> 00:17:23.930
what to remember,
what's relevant in the sequence.

297
00:17:24.170 --> 00:17:29.150
And that's the whole idea behind LSTM networks.
So the gradient doesn't diminish,

298
00:17:29.151 --> 00:17:32.420
it doesn't vanish it,
it stays static over time,
which is what we need.

299
00:17:32.421 --> 00:17:35.450
We need full upgrades for all of the weights in our network.

300
00:17:37.070 --> 00:17:40.910
So when it comes to predicting a sequence,
and the sequence is apple stock,

301
00:17:41.680 --> 00:17:43.580
we ha we can have a sliding window,
right?

302
00:17:43.581 --> 00:17:48.581
So we have w which is some interval of prices over time and we say plus one and

303
00:17:50.071 --> 00:17:52.260
that's going to give us the next one.
So given these prices,

304
00:17:52.440 --> 00:17:53.460
what are the next prices?

305
00:17:53.461 --> 00:17:57.990
So we can think of it as sliding a window across a time series.

306
00:17:57.991 --> 00:18:01.770
It's that that is what it is.
So when we look at feed forward network data,

307
00:18:01.800 --> 00:18:03.630
we have the number of examples,
right?

308
00:18:03.631 --> 00:18:07.740
How many data points times the number of inputs by the number of inputs.

309
00:18:08.190 --> 00:18:11.130
But when it comes to recurrent network data,
we don't just have,

310
00:18:11.160 --> 00:18:14.730
we have a third dimension time,
right?
Time matters.

311
00:18:15.030 --> 00:18:17.310
And we have a number of times,
series examples,

312
00:18:17.460 --> 00:18:21.180
the values at every time step and the number up and the number of time steps.

313
00:18:21.270 --> 00:18:24.420
So it's a three dimensional input for this recurrent network data.

314
00:18:25.050 --> 00:18:26.040
It can be more actually,

315
00:18:26.041 --> 00:18:28.680
but it comes down to having time as that third dimension.

316
00:18:28.681 --> 00:18:33.100
And we can squash the dimentionality using a dimensionality reduction technique

317
00:18:33.101 --> 00:18:36.510
to do that.
Um,
okay.
So

318
00:18:38.430 --> 00:18:43.260
we have some smart people in this chat room.
By the way guys,
I'm so proud of,
uh,

319
00:18:43.470 --> 00:18:48.390
just the,
the,
the level of interest in AI and just the,
the um,

320
00:18:49.200 --> 00:18:53.940
yeah,
just you guys are,
yeah,
really impressive people.
Wizards.
Okay.
So,
um,

321
00:18:55.890 --> 00:18:59.220
right,
so we,
we can think about w.
So W is that window,

322
00:18:59.430 --> 00:19:03.930
we can think about it as the sum of all of those values before it.

323
00:19:03.931 --> 00:19:06.300
So w of one is the sun volt values before it.

324
00:19:06.530 --> 00:19:08.790
W two is some of all those before it,
right?

325
00:19:09.540 --> 00:19:13.170
And so inside of our Lstm,
we're taking that input,

326
00:19:14.330 --> 00:19:17.970
we're giving it to the LSTM and it's going to output the next value in that

327
00:19:17.971 --> 00:19:22.830
window.
Wt plus one wait,
w a subscript t plus one.

328
00:19:22.831 --> 00:19:26.320
So the next time step,
uh,

329
00:19:26.370 --> 00:19:29.520
and so that's called backpropagation through time when we're applying

330
00:19:29.521 --> 00:19:32.910
backpropagation to recurrent networks is called back propagation through time.

331
00:19:33.120 --> 00:19:35.310
So I'm going to build a model at the end of this,

332
00:19:35.311 --> 00:19:39.990
but I first want to just show you really quickly a simple LSTM model in python.

333
00:19:39.990 --> 00:19:42.300
We're going to get to the javascript.
I'm literally going to build that,

334
00:19:42.690 --> 00:19:45.090
but I want to show you a simple python model first.

335
00:19:46.290 --> 00:19:49.820
What we do is we create this dataset right here using this data preprocessing

336
00:19:49.830 --> 00:19:52.530
technique.
We have our input data,
we have our output data.

337
00:19:52.531 --> 00:19:55.530
Data x is our input data.
Why are our labels,

338
00:19:55.680 --> 00:20:00.570
we fixed a random seed because these values are generated randomly inside of

339
00:20:00.571 --> 00:20:03.720
this test example,
but we want it to be reproducible.

340
00:20:03.721 --> 00:20:07.560
That means we want the same random values to be generated every time we run this

341
00:20:07.710 --> 00:20:10.860
for testings,
for testing purposes and debugging purposes,

342
00:20:11.580 --> 00:20:15.600
we'll load up our data set will use pandas to process it.
By the way,

343
00:20:15.601 --> 00:20:20.460
Panda's great great's Python Library for data preprocessing will normalize it,

344
00:20:20.461 --> 00:20:21.990
so it's all on the same scale.

345
00:20:21.991 --> 00:20:25.590
We always should have all of our data's on the same scale between zero and one

346
00:20:25.591 --> 00:20:26.370
or whatever,

347
00:20:26.370 --> 00:20:30.450
so that it's easier for a network to find the relationships between all of these

348
00:20:30.451 --> 00:20:31.284
data points.

349
00:20:31.380 --> 00:20:34.740
We don't want one feature to be between zero and a thousand and another feature

350
00:20:34.741 --> 00:20:38.790
to be between zero and one right?
That's that's way too big,
right?

351
00:20:38.820 --> 00:20:42.960
We don't want one feature to be a list of strings and another feature to be

352
00:20:43.320 --> 00:20:47.710
images,
right?
That's completely different.
We want that data to be similar.

353
00:20:47.740 --> 00:20:51.040
We want it so that's where normalization comes in so that our network can see,

354
00:20:51.070 --> 00:20:52.780
hey,
this is all just data.
What does he know?

355
00:20:52.781 --> 00:20:57.070
This is all just numbers that are on the same scale.
I can easily see the,

356
00:20:57.100 --> 00:20:59.140
you know,
the line line of best fit,
you know,

357
00:20:59.141 --> 00:21:03.760
whatever it is between all these features,
we split that data up,

358
00:21:03.790 --> 00:21:06.040
we reshape it,
we feed it,
and here it is.

359
00:21:06.070 --> 00:21:09.490
Here is that complex LSTM network that I just talked about.

360
00:21:09.700 --> 00:21:13.090
We can do this in five lines of care os that's where we are today.

361
00:21:13.120 --> 00:21:17.560
Obviously if you want more detail then you can add in a tensorflow or num py.

362
00:21:17.561 --> 00:21:19.000
If you're going to be a baller,

363
00:21:19.210 --> 00:21:21.880
you're just going to build this straight out of num Pi and then we make a

364
00:21:21.881 --> 00:21:23.770
prediction,
right?
So model dot predict,

365
00:21:23.880 --> 00:21:27.280
train it up mild to predict testing data and then we can graph it out and this

366
00:21:27.281 --> 00:21:29.980
is what it looks like.
Just like this.
It seems,
you know,

367
00:21:29.981 --> 00:21:33.100
the red is our prediction.
Blue is the actual,
uh,

368
00:21:33.900 --> 00:21:34.370
<v 0>okay.</v>

369
00:21:34.370 --> 00:21:35.270
<v 1>The real data,</v>

370
00:21:35.330 --> 00:21:40.320
the time series works well enough now to the,
to the good stuff.

371
00:21:40.321 --> 00:21:41.760
What we've been waiting for it.

372
00:21:41.850 --> 00:21:44.220
So I'm actually not going to use our current network.

373
00:21:44.221 --> 00:21:46.410
I'm going to use a convolutional network.

374
00:21:46.411 --> 00:21:50.400
You might be thinking what Saroj convolutional networks they're used for images.

375
00:21:50.550 --> 00:21:54.810
Why would you use them for sequence classification?
That's,
that's crazy talk.

376
00:21:54.840 --> 00:21:58.080
And before I answer that question,
two questions from the comments.
Here we go.

377
00:22:00.970 --> 00:22:02.630
All right.
Two questions.
Is,

378
00:22:04.160 --> 00:22:08.690
is asynchronous actor critic reinforcement learning practical for trading?

379
00:22:08.870 --> 00:22:12.080
Thank you for your live performance.
It's quite some effort to improvise.

380
00:22:12.081 --> 00:22:16.910
Thank you.
Uh,
yes.
Okay.
So short answer is yes.

381
00:22:17.130 --> 00:22:17.450
Um,

382
00:22:17.450 --> 00:22:22.340
reinforcement learning in general is a very underutilized technique for

383
00:22:22.341 --> 00:22:27.020
financial forecasting and it's a very interesting research problem.
In general.

384
00:22:27.021 --> 00:22:30.530
You have multi-agent,
uh,
multi-agent,
uh,
networks.

385
00:22:30.620 --> 00:22:34.010
You can think about other traders as agents.
You can think about,

386
00:22:34.040 --> 00:22:38.420
there's a lot of different ways to frame this and it reinforcement learning

387
00:22:38.421 --> 00:22:42.290
context that hasn't been before,
where your environment is,
your,

388
00:22:42.500 --> 00:22:44.840
the prices and the algorithms.
And the traders,

389
00:22:44.841 --> 00:22:46.760
the humans are all considered bots.

390
00:22:47.000 --> 00:22:49.880
And you're interacting with this environment inside of this,

391
00:22:49.881 --> 00:22:54.260
what is considered a mark Haub decision process that's partially observable

392
00:22:54.500 --> 00:22:58.070
because you don't know the values of their account balances or you know,

393
00:22:58.100 --> 00:22:59.330
things about them in general.

394
00:22:59.510 --> 00:23:02.750
So that's why it's partially observable and that makes it a very interesting

395
00:23:02.751 --> 00:23:05.540
research problem.
So yes.
Two,

396
00:23:08.670 --> 00:23:11.250
how will you save your trained model?

397
00:23:13.060 --> 00:23:13.893
<v 0>MMM,</v>

398
00:23:15.440 --> 00:23:18.760
<v 1>we are going to save it as a PB file.
Proto buffer,
right?</v>

399
00:23:18.761 --> 00:23:20.170
This is the standard for tensorflow.

400
00:23:20.171 --> 00:23:24.250
It's a standard for care os and it makes it easier to,
um,
uh,

401
00:23:24.310 --> 00:23:28.240
export models between different frameworks and languages.

402
00:23:28.241 --> 00:23:31.780
Even you can train a model in tensorflow with python and then,
uh,

403
00:23:31.930 --> 00:23:35.800
use it intentionally.
Dot.
Js with one line of code and vice versa as well.

404
00:23:36.010 --> 00:23:39.640
Very great for portability back to convolutional networks,
right?

405
00:23:39.641 --> 00:23:44.080
So convolutional networks are generally used for image classification,
right?

406
00:23:44.110 --> 00:23:47.390
And they're used for image classification because they outperform most other

407
00:23:47.391 --> 00:23:51.370
models when it comes to image classification.
And,
uh,

408
00:23:51.440 --> 00:23:56.300
when it comes down to is,
again,
it's a series of matrix operations.
That's it.

409
00:23:56.390 --> 00:23:59.510
That's what all neural networks are.
There are a series of,
you know,

410
00:23:59.511 --> 00:24:02.390
Matrix operations,
add,
subtract,
multiply,
divide,

411
00:24:02.570 --> 00:24:06.560
simple operations between groups of numbers that are called matrices.

412
00:24:06.950 --> 00:24:09.410
Input Times,
wait at Tobias activate,
right?

413
00:24:10.790 --> 00:24:13.910
So here's how a convolutional network works.

414
00:24:15.680 --> 00:24:19.400
So we can think of an image as a matrix,
right?

415
00:24:19.570 --> 00:24:24.080
With RGB values,
right?
Between Zero and two 55 it's a,
it's a,

416
00:24:24.081 --> 00:24:26.680
it's a,
an image is really just a,
um,

417
00:24:26.700 --> 00:24:30.080
a matrix of numbers that specify how red,
how green,

418
00:24:30.081 --> 00:24:34.820
how blue every pixel should be.
Now,
in the context of a convolutional network,

419
00:24:34.910 --> 00:24:37.160
when we feed in these images into the model,

420
00:24:37.310 --> 00:24:39.650
we don't consider it a two dimensional matrix.

421
00:24:39.680 --> 00:24:43.760
We consider it a three dimensional matrix.
Why?

422
00:24:43.910 --> 00:24:48.560
Because the third dimension is RGB,
right?
So RGB is one dimension.

423
00:24:48.950 --> 00:24:53.000
And then the other dimension is the width.
And the third dimension is the height.

424
00:24:53.001 --> 00:24:56.840
So with height,
RGB,
there are three dimensions.

425
00:24:56.900 --> 00:25:01.520
And so that is what a convolutional network is used to.
Three dimensional tensor,

426
00:25:01.820 --> 00:25:04.550
right?
Because a tensor,
there's,
okay,

427
00:25:04.551 --> 00:25:09.230
so little linear Algebra terminology here.
A scalar is one.

428
00:25:09.231 --> 00:25:14.231
Number of vector is two or more numbers.

429
00:25:14.930 --> 00:25:19.520
Uh,
no,
wait,
no,
no,
no.
A matrix.
Okay.
So let me,
let me start over.
Let me start over.

430
00:25:19.940 --> 00:25:24.330
A scalar is one single number.
Uh,
a matrix is to,

431
00:25:24.470 --> 00:25:28.760
is to,
um,
uh,
a row and a column of numbers.

432
00:25:29.030 --> 00:25:33.250
A tensor is a generalizable form of all of them,
right?
So it tends to,

433
00:25:33.251 --> 00:25:37.340
it can be a scalar attention can be a matrix that can be a vector of size.

434
00:25:37.370 --> 00:25:42.350
And to that,
you know,
a hundred dimensional,
um,
it could be a hundred dimensions.

435
00:25:42.351 --> 00:25:45.800
So attention is just the most generalized form of saying all of this.

436
00:25:45.801 --> 00:25:48.170
That's why tensorflow,
it's,
that's why it's called tensor flow,

437
00:25:48.171 --> 00:25:53.171
because tensors inputs are flowing through different operations in a computation

438
00:25:53.991 --> 00:25:55.400
graph,
and that's a neural network.

439
00:25:57.400 --> 00:26:01.070
So we have our input and so in [inaudible],
so here's,
here's how it works.

440
00:26:01.460 --> 00:26:05.630
The convolutional networks have a really,
it's,

441
00:26:05.660 --> 00:26:08.150
it's three operations that are repeated over and over again.

442
00:26:08.510 --> 00:26:11.240
Convolution Relu Pool,

443
00:26:11.360 --> 00:26:15.860
repeat that convolution Relu Pool.
What the convolutional layer is,

444
00:26:15.861 --> 00:26:20.861
is it's a flashlight that slides over that input image and it computes a filter.

445
00:26:22.850 --> 00:26:23.683
Here's what I mean.

446
00:26:26.000 --> 00:26:29.500
So we have our receptive field.
Okay.
So what that is,
is it's a,

447
00:26:29.501 --> 00:26:34.501
it's a matrix and we are continuously performing the dot product on that input

448
00:26:35.030 --> 00:26:39.680
by that filter.
And we're sliding over the image.
So that value plus one,

449
00:26:39.681 --> 00:26:42.320
that value plus one plus this one go down again,

450
00:26:42.350 --> 00:26:46.750
that value plus one plus one plus one.
What's up?
Yeah,

451
00:26:46.780 --> 00:26:50.430
that was one that I plus one,
the value plus one,
right?
And it could be plus two.

452
00:26:50.431 --> 00:26:54.270
That's the stride.
So the stride is the value that says how,
um,

453
00:26:54.420 --> 00:26:57.840
how big are the intervals that I'm sliding this?
It could be four,

454
00:26:57.841 --> 00:26:59.310
it could be eight,
right?

455
00:27:00.480 --> 00:27:04.500
But we're computing the dot product between this randomly initialized filter and

456
00:27:04.501 --> 00:27:09.501
that input valley in that and that input matrix tenser and it's,

457
00:27:09.601 --> 00:27:13.920
it's,
it's computing this,
um,
this outputs,
right?
That,
that's our output filter.

458
00:27:14.310 --> 00:27:17.190
And we repeat that.
So that's a convolutional layer,
right?

459
00:27:17.191 --> 00:27:21.000
So once we have computed that filter,
we apply,

460
00:27:21.030 --> 00:27:24.180
we apply an activation function to,
it's called [inaudible],

461
00:27:24.181 --> 00:27:26.700
which stands for rectified linear units,

462
00:27:27.090 --> 00:27:31.980
which is in the equation for that is y equals Max zero x,

463
00:27:32.400 --> 00:27:36.150
y equals Max,
a parentheses zero comma x.

464
00:27:36.390 --> 00:27:38.820
What that means is it's negating all this,

465
00:27:38.880 --> 00:27:42.660
those negative values and that are,
that could be computed.

466
00:27:42.661 --> 00:27:45.990
So everything's got to be positive.
And what this does is two things.
One,

467
00:27:45.991 --> 00:27:49.530
it makes sure that the network can learn both linear and nonlinear functions.

468
00:27:49.531 --> 00:27:54.531
So it's a universal function approximator and it makes sure that the gradient

469
00:27:55.471 --> 00:27:56.041
doesn't vanish.

470
00:27:56.041 --> 00:27:59.520
So relu was also invented as a way to prevent the vanishing gradient problem

471
00:27:59.521 --> 00:28:03.210
that we talked about in recurrent networks.
And so once we have that Relu,

472
00:28:03.240 --> 00:28:08.130
then we can do pooling.
And so what pooling is,
is us saying,

473
00:28:08.340 --> 00:28:11.430
you know,
once we've,
this is the con con involving part,
right?

474
00:28:11.430 --> 00:28:12.600
This is the flashlight part.

475
00:28:12.780 --> 00:28:16.860
It's multiplying by the dot product by that filter,

476
00:28:16.861 --> 00:28:19.080
by that input over and over and over and over and over again.

477
00:28:19.470 --> 00:28:22.930
And then we have that,
we have the output filter.
And then so what pooling is,

478
00:28:22.931 --> 00:28:27.750
is us saying what is the,
what is the,
what is the best?
I'll talk about best,

479
00:28:27.810 --> 00:28:32.280
what is the best values from every region of this filter that we've computed?

480
00:28:32.520 --> 00:28:33.540
And that's going to be our output.

481
00:28:33.541 --> 00:28:36.330
And what this does is it reduces computational complexity,

482
00:28:36.780 --> 00:28:39.570
which makes the model faster.
And so we see here is like,

483
00:28:39.571 --> 00:28:41.940
let's say this is our filter here with single depth slice.

484
00:28:42.270 --> 00:28:45.060
If we perform Max pooling,
so that's one form of pooling.

485
00:28:45.390 --> 00:28:48.030
We take the maximum value from each region,
region,

486
00:28:48.120 --> 00:28:52.200
and this region it would be six and this region it would be eight,
three,
four,

487
00:28:52.230 --> 00:28:53.220
right?
You see what I'm saying?

488
00:28:54.510 --> 00:28:58.710
And so what's happening is as we move through this convolutional network,

489
00:28:59.070 --> 00:29:03.870
there's,
there are more and more filters that are be computed.

490
00:29:03.871 --> 00:29:05.310
It could be like four,
16,

491
00:29:05.311 --> 00:29:10.311
32 but each of these filters are smaller and denser and they're more specific to

492
00:29:10.861 --> 00:29:13.200
certain features,
right?
So it's,
it's,
it's,

493
00:29:13.320 --> 00:29:17.010
it's this level of abstraction where the biggest filters are like head and then

494
00:29:17.011 --> 00:29:21.690
it goes to like,
you know,
uh,
eyes,
shapes,
curvature,

495
00:29:21.780 --> 00:29:23.490
you know,
like very little details.

496
00:29:23.970 --> 00:29:28.860
And the output then is going to be one of several classes,
right?
So class values,

497
00:29:28.861 --> 00:29:30.660
is this a card?
This is a truck.
Is this a plane?

498
00:29:31.520 --> 00:29:36.120
So that's a convolutional network.
Why apply them to time series data?
Okay.

499
00:29:36.121 --> 00:29:38.430
So two reasons.
One is,

500
00:29:38.431 --> 00:29:41.230
so a recurrent network was invented to look to,

501
00:29:41.290 --> 00:29:44.320
to make sure that we take the context of the previous

502
00:29:45.880 --> 00:29:49.240
sequence into account,
right?
So if we're going to predict the next word in a,

503
00:29:49.250 --> 00:29:50.110
say a sentence,

504
00:29:50.290 --> 00:29:54.340
you know I'm going to the bank to deposit some so you know what I'm going to say

505
00:29:54.341 --> 00:29:57.500
money,
but I'm machine doesn't necessarily know that and we,

506
00:29:57.501 --> 00:30:01.630
and we have to take the context of bank today.
You know me,

507
00:30:01.631 --> 00:30:03.550
what I've done in the past into context.

508
00:30:03.551 --> 00:30:06.430
So the recurrent networks are really great at remembering the past.

509
00:30:06.760 --> 00:30:09.490
They're not great at remembering or not remembering,

510
00:30:09.640 --> 00:30:13.750
but taking the future into context as well.
So if we have some testing data,

511
00:30:13.780 --> 00:30:17.410
we know what the 10 next points should be.
That doesn't matter in a recurrent.

512
00:30:17.411 --> 00:30:18.880
Now recalling the past matters.

513
00:30:19.210 --> 00:30:23.590
So convolutional networks are good at seeing both sides are better than

514
00:30:23.591 --> 00:30:27.610
recurring.
That's taking into account both sides of the timeline.
Second of all,

515
00:30:27.611 --> 00:30:31.510
convolutional networks are faster,
there are less computationally expensive,

516
00:30:31.720 --> 00:30:36.040
so you can run them on big data sets in a much faster time scale.
Third

517
00:30:37.970 --> 00:30:40.940
convolutional networks take into account look locality,
right?

518
00:30:40.941 --> 00:30:42.320
So the locality of features,

519
00:30:42.321 --> 00:30:45.140
because this is a good thing when it comes to images,
the same,

520
00:30:45.200 --> 00:30:49.340
the same feature that matters locally,
like say you know the,

521
00:30:49.400 --> 00:30:53.360
the blackness of my eye also matters here,
right?
So if,
if,

522
00:30:53.390 --> 00:30:57.140
if it sees like white in this shape,
it'll know like locally speaking,

523
00:30:57.141 --> 00:31:00.440
well there's probably black and it sees here,
oh white,
there's probably black.

524
00:31:00.680 --> 00:31:02.090
Whereas you know,

525
00:31:02.150 --> 00:31:06.410
recurrent networks are more generalized into taking it the entire data set into

526
00:31:06.411 --> 00:31:09.740
account.
Whereas convolutional networks focused on local,

527
00:31:10.370 --> 00:31:14.720
on the locality of features.
So when it comes to prices,
for example,

528
00:31:15.050 --> 00:31:15.950
you know what,

529
00:31:16.010 --> 00:31:20.750
what bitcoin's price was five years ago is irrelevant to what its price is going

530
00:31:20.751 --> 00:31:23.060
to be.
Now,
if you think about it right now,
there's a whole,

531
00:31:23.090 --> 00:31:26.330
everything is different,
right?
So that can apply to stocks as well,
right?

532
00:31:26.331 --> 00:31:28.670
So high frequency trading for example,
right?

533
00:31:28.850 --> 00:31:30.890
Making trades that nanosecond timescales,

534
00:31:31.250 --> 00:31:34.670
that is all about locality of where we are,
right?
Right.
Exactly.

535
00:31:34.671 --> 00:31:37.640
Now and what's happening.
And so that's,

536
00:31:37.641 --> 00:31:39.650
that's why convolutional networks are better at,

537
00:31:40.040 --> 00:31:41.810
they have fewer sequential calculations.

538
00:31:41.960 --> 00:31:45.760
They tend to be more computationally efficient and they focus on,
um,

539
00:31:45.830 --> 00:31:49.220
not looking at the data holistically as a whole,

540
00:31:49.400 --> 00:31:52.940
but more capturing those and to be specific temporal relations from the

541
00:31:52.941 --> 00:31:56.450
beginning of the time to the end of the time.
All right,
so,
and here's the,

542
00:31:56.660 --> 00:32:01.370
here's a little
complexity chart.
Okay.

543
00:32:01.371 --> 00:32:04.250
So two more questions then.
I'm going to code code time is coming.
Okay.

544
00:32:04.251 --> 00:32:08.720
So what are the questions here?
Can you share that notebook?
Yes,

545
00:32:08.750 --> 00:32:11.600
it's actually in the video description.
And the second question is

546
00:32:15.090 --> 00:32:18.030
why should we not add even more weights than the three in Lstm?

547
00:32:18.300 --> 00:32:20.050
Because as Andre Carpoff,

548
00:32:20.051 --> 00:32:23.850
he says there is a diminishing return to adding more layers,
right?

549
00:32:23.851 --> 00:32:27.510
So up to a certain points,
generally like three to four or five,
six layers,

550
00:32:27.750 --> 00:32:30.180
you know,
you have marginal returns,

551
00:32:30.300 --> 00:32:33.630
but then after that it's a diminishing return and you're adding computational

552
00:32:33.631 --> 00:32:36.210
complexity because it's all about trade offs.
Okay.

553
00:32:36.211 --> 00:32:39.090
So now let's build our recurrent network.
Okay.

554
00:32:39.110 --> 00:32:42.630
So we're going to build this using tensorflow.
Dot.
Yes.

555
00:32:42.920 --> 00:32:47.360
Now I want to say that we have built out this,
uh,
front end.

556
00:32:47.361 --> 00:32:49.070
So the front end,
I talked about his here.

557
00:32:49.280 --> 00:32:53.420
What I want to do now is actually build the network itself and focus on that

558
00:32:53.421 --> 00:32:56.730
because that's the most important part.
We tend to fill it out.
Jay Asks,
you just

559
00:32:58.550 --> 00:33:02.390
import 10 dot and your html and that's it,
right?
So there's,

560
00:33:02.540 --> 00:33:04.750
you don't even need to install it.
Really.
It's just learning.

561
00:33:04.850 --> 00:33:08.510
As long as you have an internet connection,
it's there.
So we have that.

562
00:33:08.630 --> 00:33:10.910
So let's build our convolution network.
That's step one.

563
00:33:10.911 --> 00:33:12.320
We're going to build our CNN,

564
00:33:12.870 --> 00:33:15.920
we'll call it this constant built CNN that we're going to use later.

565
00:33:17.030 --> 00:33:21.050
So this is going to be a function of the data that we feed the model.
Okay?

566
00:33:21.260 --> 00:33:24.230
And so,
um,
that's what that is.

567
00:33:24.410 --> 00:33:26.660
So what are we going to do inside of this function?
What we're,

568
00:33:26.661 --> 00:33:29.000
what we're going to do,
cause we're going to return a promise.

569
00:33:29.001 --> 00:33:32.450
And you might be saying,
well,
what's a promise?
A promise is a,

570
00:33:32.510 --> 00:33:36.560
it represents the eventual result that we're going to have of any asynchronous

571
00:33:36.561 --> 00:33:41.020
operation,
which is in our case is going to be the,
the data that we are our,

572
00:33:41.021 --> 00:33:43.130
our convolutional network that we,
that we compute.

573
00:33:44.560 --> 00:33:44.900
<v 0>Yeah,</v>

574
00:33:44.900 --> 00:33:46.640
<v 1>so it's a place holder.
Okay.
So</v>

575
00:33:49.110 --> 00:33:51.780
it's a placeholder and then we're going to computer result later on.

576
00:33:54.020 --> 00:33:56.860
And so that promise is value is going to be

577
00:33:58.440 --> 00:34:02.850
a function or neural network and function.

578
00:34:03.000 --> 00:34:07.810
Great New
promise,

579
00:34:09.340 --> 00:34:13.870
function,
resolve,
reject Coco,
Coco.
Now let's start building this thing.

580
00:34:13.871 --> 00:34:16.330
So the first thing we're going to do is we're going to say we have our model

581
00:34:16.331 --> 00:34:20.800
using tensorflow dot sequential because this is a linear stack of layers,

582
00:34:20.801 --> 00:34:23.440
you know,
layer one,
layer two,
layer three.
For more complex models,

583
00:34:23.650 --> 00:34:26.680
we would want a nonlinear stack of layers so it can branch into different

584
00:34:26.681 --> 00:34:30.460
directions.
A lot of models that incorporate attention mechanisms can do this.

585
00:34:30.660 --> 00:34:32.950
Or we're going to start off very simple with the sequence,
you know,

586
00:34:32.951 --> 00:34:35.200
simple sequential,
linear stack of layers.

587
00:34:36.340 --> 00:34:41.020
So now we can start adding these layers using the TF dot layers function,
right?

588
00:34:41.021 --> 00:34:44.760
So each layers,
the layers is a high level API.
It's like careless,

589
00:34:44.770 --> 00:34:47.110
but for Java script,
okay,
so we're going to say,

590
00:34:47.500 --> 00:34:50.350
well our first layer is going to be our convolutional layer,
right?

591
00:34:50.351 --> 00:34:55.030
Because our convolutional networks have these uh,

592
00:34:55.090 --> 00:34:58.600
these layers.
And so the first layer we're going to sit,

593
00:34:58.601 --> 00:35:00.670
we're going to have to specify some of these.

594
00:35:03.220 --> 00:35:03.860
<v 0>Okay.</v>

595
00:35:03.860 --> 00:35:06.740
<v 1>Some of these values,
and I'll talk about them in a second.</v>

596
00:35:07.070 --> 00:35:08.720
We have a filter filters,
we have our strides.

597
00:35:08.721 --> 00:35:11.780
What else do we have to specify for layer,
we have our activation function.

598
00:35:12.080 --> 00:35:14.450
We have par colonel initializer.

599
00:35:15.740 --> 00:35:16.340
<v 0>Okay.</v>

600
00:35:16.340 --> 00:35:18.890
<v 1>And right,
if you look into the tension for the documentation,
all that,</v>

601
00:35:18.891 --> 00:35:22.750
it's there.
And so I'm just kind of like,
you know,

602
00:35:22.751 --> 00:35:26.950
based on what I've seen of doing that,
right?
So it's so far our input shape,

603
00:35:26.951 --> 00:35:29.680
what is our orders are,
what is our data going to look like,
by the way.

604
00:35:29.950 --> 00:35:32.440
So our data is going to look like,
by the way,

605
00:35:32.441 --> 00:35:34.990
I can't believe I didn't even show that this is what our data looks like.

606
00:35:35.350 --> 00:35:40.140
We have the dates and we have the,
the closing price,
right?
So that's,

607
00:35:40.141 --> 00:35:43.470
that's what that is.
So we have one price for the input data.
So given,

608
00:35:43.471 --> 00:35:46.950
so this is our input,
the date and the,
and the,
and the price is the output.

609
00:35:46.980 --> 00:35:47.910
That's what we're predicting.

610
00:35:49.170 --> 00:35:54.170
And so because convolutional networks prefer a 3-d tenser as input because

611
00:35:54.631 --> 00:35:55.560
they're used to images.

612
00:35:55.860 --> 00:35:59.250
What we're gonna do is we're going to turn this date into a three dimensional

613
00:35:59.280 --> 00:35:59.880
tensor,

614
00:35:59.880 --> 00:36:03.570
whereas one dimension is the year one dimension is the month than one dimension

615
00:36:03.571 --> 00:36:08.190
is the day.
And so it's a three d 10 is input.
The label is this,
is this price.

616
00:36:08.860 --> 00:36:09.693
<v 3>Okay.</v>

617
00:36:11.560 --> 00:36:14.530
<v 1>So when it comes to our input shape,
we're going to say,
you know,</v>

618
00:36:14.531 --> 00:36:18.670
assuming that we have that,
um,
the dates

619
00:36:21.400 --> 00:36:25.320
cause we have the data that's being fed into this CNN,
which is,
which are the,
um,

620
00:36:27.890 --> 00:36:30.270
<v 3>you know,
the,
the input data,</v>

621
00:36:31.140 --> 00:36:34.080
<v 1>we're going to say it's going to be the length of the dates,
you know,
buy one.</v>

622
00:36:34.980 --> 00:36:36.320
Now when it comes to our kernel size,

623
00:36:36.321 --> 00:36:40.050
let's say I said we want a hundred of those data points to start off with

624
00:36:40.800 --> 00:36:44.760
filters.
You know,
generally there are multiples of Eight,
eight,
1632.

625
00:36:44.780 --> 00:36:46.920
Well let's start off with the,
with the,
with the small one,
two,

626
00:36:46.980 --> 00:36:48.960
eight strides as well.

627
00:36:48.961 --> 00:36:53.460
There's those generally increase or decrease in size as we as we go further

628
00:36:53.461 --> 00:36:55.230
along.
So you want to start off with a,

629
00:36:55.500 --> 00:36:58.530
with a bigger number and then decrease it over time.
So I'll start off with two.

630
00:36:59.180 --> 00:37:01.140
Um,
activation.
Of course,
like I said,

631
00:37:01.141 --> 00:37:06.090
Relu and Colonel Initializer is called baryons scaling.

632
00:37:06.390 --> 00:37:08.070
What do I mean by variants scaling?

633
00:37:09.270 --> 00:37:11.580
This basically is a weight initialization technique.

634
00:37:11.820 --> 00:37:15.750
It's also called Xavier and some context and it tries to make the variance of

635
00:37:15.751 --> 00:37:20.430
the output of a layer be equal to the variance of the inputs.

636
00:37:20.550 --> 00:37:25.140
And this,
um,
this reduces overfitting,
so it's called Barry and scaling.

637
00:37:25.430 --> 00:37:28.710
Um,
I can make an entire video on Xavier initialization by the way,

638
00:37:28.711 --> 00:37:32.910
but we're going to call it that.
That's our first layer.
Now for a pooling layer,

639
00:37:32.911 --> 00:37:37.230
like we,
we did our convolution,
we did art relu and now we can do are pooling.

640
00:37:38.640 --> 00:37:42.060
We'll do TF dot layers dot Max pooling.

641
00:37:42.150 --> 00:37:45.690
That's our one D and

642
00:37:47.120 --> 00:37:47.953
<v 3>okay.</v>

643
00:37:48.000 --> 00:37:52.200
<v 1>Um,
now we can say,
well,</v>

644
00:37:52.201 --> 00:37:53.490
what's our pooling size is going to be,

645
00:37:53.520 --> 00:37:57.540
we'll start off with a big pooling number and then for the next pooling,

646
00:37:57.570 --> 00:38:01.230
remember we start off big and will we get smaller?
And generally pooling is like,

647
00:38:01.440 --> 00:38:05.520
uh,
they're in intervals of 100,
so I'll do 500.

648
00:38:07.090 --> 00:38:10.620
I'll also answer some questions in a second.
The striding,
like I said,

649
00:38:10.621 --> 00:38:12.350
it's going to be,
oh,

650
00:38:12.470 --> 00:38:16.050
let's just say two again because it's like a part of the same block,
you know,

651
00:38:16.110 --> 00:38:19.320
in convolution Relu cooling,
and then it's going to change over time.

652
00:38:19.860 --> 00:38:23.670
So say strides or to,
and now we'll just repeat this process again.

653
00:38:23.671 --> 00:38:26.070
So we'll say model that ad for the next layer.

654
00:38:27.270 --> 00:38:28.010
<v 3>Yeah.</v>

655
00:38:28.010 --> 00:38:31.040
<v 1>Um,
the kernel size is going to be much,
much smaller.</v>

656
00:38:31.130 --> 00:38:34.550
So let's pick something much smaller.
Five.
What'd I say about this filters?

657
00:38:34.551 --> 00:38:38.140
The filters increased,
right?
Because there are less,
they're more filters,

658
00:38:38.141 --> 00:38:38.974
but they're smaller.

659
00:38:39.220 --> 00:38:42.850
As we progress through the network filter is going to be 16 strides are going to

660
00:38:42.851 --> 00:38:45.700
be one because it's smaller.
Relu and Varian scaling.
Again,

661
00:38:46.000 --> 00:38:49.720
we'll repeat that again with one more pooling layer.
And it's going to be old.

662
00:38:49.721 --> 00:38:52.300
Like I said,
the pooling is going to be smaller.
Let's pick a hundred.

663
00:38:52.440 --> 00:38:53.620
So I was going to be too.

664
00:38:55.580 --> 00:38:58.190
<v 3>And lastly we want to add are,</v>

665
00:38:58.220 --> 00:39:00.950
and I'm going to answer questions right after this.
Uh,
lastly,

666
00:39:00.951 --> 00:39:02.530
we're going to add our,
um,

667
00:39:05.650 --> 00:39:08.980
what are we going to add?
We're going to add our model.
Do you have dot layers.

668
00:39:09.130 --> 00:39:11.040
Dot Dense are

669
00:39:11.140 --> 00:39:12.820
<v 1>um,
fully connected layer?</v>

670
00:39:12.850 --> 00:39:15.340
That's what I'm trying to say are fully connected layer because we're going to

671
00:39:15.341 --> 00:39:19.270
now take all that data and make a single class prediction and that's what our,

672
00:39:20.080 --> 00:39:23.200
that's what our softmax layer is going to do.
It's going to output,
you know,

673
00:39:23.201 --> 00:39:26.350
class predictions,
which is going to be in our case the next price,

674
00:39:27.990 --> 00:39:30.550
<v 3>uh,
so dense and that's going to be</v>

675
00:39:33.850 --> 00:39:37.710
units,
uh,
10.

676
00:39:38.100 --> 00:39:39.750
That's the thing about,
um,

677
00:39:41.610 --> 00:39:44.550
it's fun because you get to see,
you know,

678
00:39:45.120 --> 00:39:48.870
what's going to work and what doesn't work.

679
00:39:50.280 --> 00:39:51.340
Activation is of course,

680
00:39:51.341 --> 00:39:56.341
a softmax function and now we can return all of that as a resolve.

681
00:39:58.200 --> 00:40:00.990
Thanks to the promise that we've made at the beginning.

682
00:40:01.350 --> 00:40:03.360
We're going to turn the model.
We're going to return the data.

683
00:40:03.720 --> 00:40:06.600
We're turn the model,
returned the data,

684
00:40:08.550 --> 00:40:11.460
and that's it.
That's where our bill TNN function.

685
00:40:13.430 --> 00:40:18.150
<v 1>I take two questions.
Ms Comma,
online 12th.</v>

686
00:40:18.151 --> 00:40:19.200
Thank you very much

687
00:40:21.010 --> 00:40:25.030
<v 3>data that dates dot.
Lang.
Thank you.
Boom.</v>

688
00:40:25.690 --> 00:40:26.523
Now where were we?

689
00:40:29.600 --> 00:40:32.480
<v 1>Why don't you use the adaptive theme on sublime tax?
Cause I don't have time.</v>

690
00:40:32.870 --> 00:40:37.720
And can I run these on CPU?
Yes,
you can.
That's the great thing about j s uh,

691
00:40:37.760 --> 00:40:38.840
you can run these on the CPU.

692
00:40:40.300 --> 00:40:42.540
<v 3>All right.
So</v>

693
00:40:46.520 --> 00:40:49.850
<v 1>yeah,
lots of questions.
Okay,
so now where were we?</v>

694
00:40:49.851 --> 00:40:51.320
Now we're going to train this thing,

695
00:40:54.210 --> 00:40:56.890
<v 3>right?
[inaudible] de.
De.
De.
De.
De.
De.</v>

696
00:40:57.890 --> 00:41:00.920
<v 1>So step two is to train our model.
We built our CNN,
now we're going to train it.</v>

697
00:41:00.921 --> 00:41:04.670
So we're going to say CNN equals function model.

698
00:41:04.880 --> 00:41:07.910
Given our data and given the number of cycles or training loops,

699
00:41:09.570 --> 00:41:11.850
<v 3>we're going to say,
let's split out that data.</v>

700
00:41:11.851 --> 00:41:15.060
So we have that input data into its own tensor,
each of them.

701
00:41:15.061 --> 00:41:18.630
So we have our gates,
we have our testing data.

702
00:41:19.170 --> 00:41:22.320
Testing data is going to be t f.
Dot.
10 Zero One d again,

703
00:41:23.070 --> 00:41:27.140
we have hard data dot test times and we have our

704
00:41:28.710 --> 00:41:31.080
how equals model dot get layer.

705
00:41:32.880 --> 00:41:36.680
We want to get that last dense layer so we can make the actual prediction right.

706
00:41:36.681 --> 00:41:41.600
We want to make that prediction in a second.
Colons on 12 1516.
Thank you.

707
00:41:41.601 --> 00:41:43.880
Thank you.
Thank you.
Colin's on 1215,

708
00:41:43.881 --> 00:41:48.881
1615 also 14.

709
00:41:51.470 --> 00:41:54.180
See Guys,
when I'm teaching and I'm coding,
I'm getting,

710
00:41:54.280 --> 00:41:56.120
I'm going to get better at this.
It's been a while.

711
00:41:56.121 --> 00:41:59.840
It's been a year since I've done a live coding stream,
but thank you very much.

712
00:42:00.510 --> 00:42:04.110
Uh,
yeah.
Cool.
Colon,
colon,
colon,
colon,
colon,

713
00:42:05.570 --> 00:42:10.410
her period period or comma,
comma,
comma.
Good.
Yup.

714
00:42:11.100 --> 00:42:15.990
Yup.
Great.
All right.
Where were we?
Uh,

715
00:42:16.790 --> 00:42:16.961
right.

716
00:42:16.961 --> 00:42:21.961
So model helper is going to be our helper function for our model and um,

717
00:42:26.570 --> 00:42:29.280
now we can train this model and this is going to be inside of it,

718
00:42:29.360 --> 00:42:32.630
inside of a promise.
Again,
if it doesn't work,
we have some,

719
00:42:32.900 --> 00:42:36.320
it's kind of like a try catch loop in a way.
Um,

720
00:42:40.720 --> 00:42:44.080
okay.
So we're going to try this out.
Speaking of try catch loops,

721
00:42:44.670 --> 00:42:48.730
we're going to say compile the model and now we built the model.
We want to,

722
00:42:48.760 --> 00:42:53.110
we want to run this thing using the optimizer.
It's the CASAC gradient descent,

723
00:42:53.111 --> 00:42:56.800
which is the Goto optimizer for neural networks.

724
00:42:57.180 --> 00:42:59.980
Our loss function is going to be the very probably the most popular one.

725
00:42:59.981 --> 00:43:01.300
Binary Cross entropy.

726
00:43:02.270 --> 00:43:02.670
<v 0>Okay.</v>

727
00:43:02.670 --> 00:43:07.500
<v 3>We're just going to compute our error value and are learning rate is going to be</v>

728
00:43:07.501 --> 00:43:12.501
a very standard 0.1% most often learning rates are at 1.1% I think about deep

729
00:43:12.721 --> 00:43:17.160
learning by the way guys is we are basically just jumbling up some of these

730
00:43:17.161 --> 00:43:19.560
parameter values and we are learning over time.

731
00:43:19.561 --> 00:43:22.800
So that's the art of deep learning is sang.
I think these are good strides.

732
00:43:22.801 --> 00:43:25.740
I think these are good filters.
I think the thing this'll work,
compile,

733
00:43:25.741 --> 00:43:28.570
run the model and see the results and then change those values as,

734
00:43:28.590 --> 00:43:30.360
as you see necessary as necessary.

735
00:43:30.840 --> 00:43:34.050
And you know sometimes you'll change those values and you'll break the,
you know,

736
00:43:34.051 --> 00:43:36.900
your break the Internet because you just beat everything else.

737
00:43:36.901 --> 00:43:39.360
And that's the really exciting part about deep learning.

738
00:43:41.190 --> 00:43:42.480
And so once we've compiled that model,

739
00:43:42.481 --> 00:43:46.170
we're ready to train it with the fit function.
Okay?

740
00:43:46.171 --> 00:43:47.520
So we're going to say,

741
00:43:49.550 --> 00:43:49.810
<v 0>okay,</v>

742
00:43:49.810 --> 00:43:54.100
<v 3>let's make sure to reshape that input data so that it's in three dimensions.</v>

743
00:43:54.760 --> 00:43:55.593
Um,

744
00:43:57.850 --> 00:44:01.060
and do that one more time.

745
00:44:04.050 --> 00:44:05.680
Dot.
Reshape.
Okay.

746
00:44:06.040 --> 00:44:10.840
One 1960 and I will wrap at the end of this.
So stick around by the way,

747
00:44:12.090 --> 00:44:15.440
one
batch size

748
00:44:18.500 --> 00:44:22.130
and then the number of epochs.
How many cycles do we want to run this for?

749
00:44:28.930 --> 00:44:29.763
<v 0>Okay.</v>

750
00:44:32.400 --> 00:44:35.430
<v 3>And now we can tell.
I know,
I know I got to like fix this up in a second.</v>

751
00:44:35.431 --> 00:44:37.020
Just hold on.
I'm going to,

752
00:44:37.021 --> 00:44:40.680
I'm going to fix all these like little parentheses and brackets,
et Cetera,

753
00:44:40.681 --> 00:44:45.050
et cetera.
We can tell our model like it's running if it's running,

754
00:44:45.060 --> 00:44:48.250
nodded out and you know,
whatever you want to say.
And then,
um,

755
00:44:49.380 --> 00:44:50.340
printout that prediction.

756
00:44:56.460 --> 00:44:57.293
<v 0>Okay.</v>

757
00:45:00.120 --> 00:45:02.850
<v 3>And then since we have a catch loop as well,</v>

758
00:45:05.200 --> 00:45:10.200
we have our catch loop is going to be resolve print whenever the exception is.

759
00:45:11.650 --> 00:45:16.270
That's where our exception,
um,
and now we can execute this.
So we can say,

760
00:45:19.100 --> 00:45:23.330
uh,
prep data

761
00:45:25.340 --> 00:45:28.640
for the function and take the result.

762
00:45:29.330 --> 00:45:32.000
We're going to build our CNN using the result.

763
00:45:34.160 --> 00:45:39.160
And then once we built our CNN built function,

764
00:45:40.970 --> 00:45:45.710
then we can say no,
nope.
How answer questions in a second.

765
00:45:45.830 --> 00:45:50.570
CNN built dot model,
they'll talk data

766
00:45:54.230 --> 00:45:58.460
100 then
function.

767
00:45:59.530 --> 00:46:00.363
<v 0>Yeah.</v>

768
00:46:03.700 --> 00:46:04.533
<v 3>Print.</v>

769
00:46:05.350 --> 00:46:09.940
So it's like that now I will purchase the license at some point.
Okay.

770
00:46:09.941 --> 00:46:12.650
So I probably got some serious,
um,

771
00:46:14.920 --> 00:46:19.390
probably got some serious,
um,
errors here.

772
00:46:19.990 --> 00:46:20.823
Let's see,

773
00:46:32.030 --> 00:46:36.640
Park city
scripts.

774
00:46:37.230 --> 00:46:37.611
We're,
we're,

775
00:46:37.611 --> 00:46:42.611
we see d public CD scripts python to be CNN dot.

776
00:46:48.381 --> 00:46:49.214
Js

777
00:46:50.910 --> 00:46:53.430
in valid syntax online one.

778
00:46:57.190 --> 00:46:58.023
<v 0>Yeah.</v>

779
00:46:59.000 --> 00:47:00.660
<v 3>Oh you know what sometimes with</v>

780
00:47:05.370 --> 00:47:06.360
hold on a second.

781
00:47:09.810 --> 00:47:10.910
Get up time guys.

782
00:47:11.000 --> 00:47:15.380
Cause we are running out of time in this livestream and battery and everything,

783
00:47:15.740 --> 00:47:19.400
but yeah.
Anyway,
I've got,
here's a good humbling for you guys by the way,

784
00:47:20.030 --> 00:47:23.090
which already have here would you guys should have been following along with

785
00:47:23.120 --> 00:47:25.280
anyway,
anyway.
Um,

786
00:47:27.520 --> 00:47:30.310
public scripts.

787
00:47:32.420 --> 00:47:36.170
Boom.
Here we go.
And it's very well commented for you guys as well.

788
00:47:36.320 --> 00:47:40.100
So definitely check this out.
When it comes down to it,
uh,

789
00:47:40.130 --> 00:47:42.620
given either the logical data or the apple data,

790
00:47:42.680 --> 00:47:46.370
it's going to be predicting the next in the next data into time series and we

791
00:47:46.371 --> 00:47:50.900
can wait and there we go.
And so that's for,
that's for a test data.

792
00:47:51.050 --> 00:47:55.190
We're pulling from the Apple Api and then we made this Jason file right here and

793
00:47:55.191 --> 00:47:58.220
then we trained it on that.
Um,
but yeah,
convolutional networks.

794
00:47:58.250 --> 00:48:02.810
Super useful for time series forecasting also.
Uh,
I want you guys to,

795
00:48:04.610 --> 00:48:08.990
I want you guys to,
uh,
say a subject.

796
00:48:08.991 --> 00:48:11.960
I'm going to rap about it.
All right,
so just say some subject.
In the meantime,

797
00:48:11.961 --> 00:48:14.330
I'm going to answer two questions.
Uh,

798
00:48:15.020 --> 00:48:19.070
so Raj need a crowdfunding campaign for a sublime license.
Uh,

799
00:48:19.700 --> 00:48:23.340
yes.
No,
no,
no,
no.
Don't worry about it.
It's kind of like a,

800
00:48:23.400 --> 00:48:27.530
it's kind of like a meme now.
Like I just never get sublime.

801
00:48:27.920 --> 00:48:29.420
And then lastly,

802
00:48:32.960 --> 00:48:33.793
<v 0>yeah.</v>

803
00:48:34.570 --> 00:48:38.410
<v 3>Uh,
what are the questions?
Anyone here from Amsterdam?</v>

804
00:48:38.411 --> 00:48:42.520
I used to live their whole hot tits and should I know TF dot js before seeing

805
00:48:42.521 --> 00:48:45.640
this video?
No,
I think this was simple enough.
As long as you've,

806
00:48:46.180 --> 00:48:48.670
you've recognized,
um,
you know,

807
00:48:48.671 --> 00:48:52.540
Care Ross or tensorflow and you've seen linear models,
then you can,

808
00:48:52.870 --> 00:48:54.820
you can get this.
Um,

809
00:48:58.040 --> 00:49:01.970
right.
So,
thoughts on reinforcement learning for time series.
Of course,
I like it.

810
00:49:01.971 --> 00:49:05.540
Medical image analysis of a great video on that search,
medical image analysis.

811
00:49:06.050 --> 00:49:06.883
Um,

812
00:49:10.760 --> 00:49:15.710
and uh,
stationarity how important is it for Lstm?
Uh,

813
00:49:16.190 --> 00:49:20.540
it's not as important for Lstm as it is for a Rhema.
A rhema focuses on that,

814
00:49:20.541 --> 00:49:25.400
whereas Lstm is more focused on features,
um,
across the entire,

815
00:49:25.401 --> 00:49:28.910
so it's to holistic overview of all of that sequential data.

816
00:49:29.950 --> 00:49:30.783
Um,

817
00:49:31.700 --> 00:49:35.930
rap about Egalitarian technocracy um,

818
00:49:35.960 --> 00:49:39.590
why did I pick the hardest ones to mention?
Seriously Egalitarian.

819
00:49:39.591 --> 00:49:41.690
Technocracy all right,
here we go.

820
00:49:42.620 --> 00:49:43.150
<v 0>Yeah.</v>

821
00:49:43.150 --> 00:49:47.410
<v 3>Here we go.
It's a freestyle.
Turn up the sound.
Oh Shit.
I don't have sound.</v>

822
00:49:47.950 --> 00:49:51.550
Could you play a beat from there?
Anything like,

823
00:49:51.551 --> 00:49:56.290
just like pick a instrumental,
we're about to do this on youtube.
Just search,

824
00:49:56.291 --> 00:49:58.330
like rap,
instrumental.
Um,

825
00:50:01.160 --> 00:50:06.110
okay.
Here we go.
With the,
uh,
the rap.
It's coming in.
Five,

826
00:50:07.450 --> 00:50:10.450
<v 0>four,
three,</v>

827
00:50:12.340 --> 00:50:15.550
two,
one.

828
00:50:17.550 --> 00:50:22.410
All right,
there we go.
Oh,

829
00:50:22.411 --> 00:50:23.244
I got it.

830
00:50:24.970 --> 00:50:26.140
<v 3>Sick trap beat.</v>

831
00:50:33.740 --> 00:50:37.610
<v 1>You say,
I need a technocracy.
I'm gonna give you a democracy.</v>

832
00:50:37.700 --> 00:50:42.290
How much show you how to break out of monotony and show you the life that you

833
00:50:42.291 --> 00:50:44.390
can't see.
I'm trying to go with five,

834
00:50:44.391 --> 00:50:49.220
so free outside and see all my enemies telling me that you can do it.

835
00:50:49.340 --> 00:50:53.690
Hold up.
I'm a break it down.
Show you that governments don't mean anything.

836
00:50:53.990 --> 00:50:57.800
It's the Internet.
We've got a new techno utopia.

837
00:50:57.801 --> 00:51:02.300
It's less like man,
Internet,
Internet.
I'm out of it.
I'm back in it.

838
00:51:02.480 --> 00:51:06.290
You don't even see because I'm beyond it.
That's the for this wrap.

839
00:51:06.291 --> 00:51:09.620
I just wanted to do a little five second demo to keep it interesting guy.

840
00:51:09.621 --> 00:51:13.910
So that's my wrap for this.
Today's video.
I hope you liked this video share.

841
00:51:13.970 --> 00:51:18.860
The best thing you could do for me is to hit the subscribe button and tell your

842
00:51:18.861 --> 00:51:23.360
friends to subscribe.
That is a single metric.
I care about.
Subscribe.

843
00:51:23.480 --> 00:51:27.560
I'm trying.
This audience is growing,
but it's not growing nearly fast enough.

844
00:51:27.680 --> 00:51:29.660
We're trying to get to the top.
You know what I'm saying?

845
00:51:29.670 --> 00:51:33.680
Million subscribers by the end of the year.
That is the goal.
Okay?

846
00:51:33.681 --> 00:51:37.190
It's a very unrealistic goal,
but I'm an unrealistic person.
Okay.

847
00:51:37.191 --> 00:51:40.770
So thank you guys for watching.
I love you guys.
You're the reason I do this.
Um,

848
00:51:40.940 --> 00:51:44.660
and for now,
I've got to go take a plane to Houston to visit my parents,

849
00:51:44.720 --> 00:51:45.770
so thanks for watching.

850
00:51:46.620 --> 00:51:46.680
<v 0>Okay.</v>

