WEBVTT

1
00:00:00.120 --> 00:00:00.811
Hello world,

2
00:00:00.811 --> 00:00:05.040
it's Saroj and let's build a game bought that learns how to get from point a to

3
00:00:05.041 --> 00:00:09.210
point B using a special type of reinforcement learning called Q learning.

4
00:00:09.480 --> 00:00:12.600
Reinforcement learning means that learning by interacting with an environment

5
00:00:12.730 --> 00:00:15.150
through positive feedback or reinforcement,

6
00:00:15.390 --> 00:00:17.370
it's similar to how you give a dog a treat,

7
00:00:17.520 --> 00:00:21.060
but only if it rolls over and it's evolved over the past few decades.

8
00:00:21.240 --> 00:00:25.470
In the late 1950s and American mathematician named Richard Bellman was trying to

9
00:00:25.471 --> 00:00:28.140
solve what he called the optimal control problem.

10
00:00:28.410 --> 00:00:31.920
This describes the problem of designing an agent to minimize some behavior of a

11
00:00:31.921 --> 00:00:33.720
system over time.
Eventually,

12
00:00:33.721 --> 00:00:36.690
he and his colleagues finally discovered a possible solution to it,

13
00:00:36.691 --> 00:00:38.910
which was later called the bellman equation.

14
00:00:39.150 --> 00:00:43.140
It describes the value of a problem at a certain point in time in terms of the

15
00:00:43.141 --> 00:00:46.980
payoff made by previous decisions and it also describes the value of the

16
00:00:46.981 --> 00:00:50.760
remaining decision problems that result from the initial decisions by involving

17
00:00:50.761 --> 00:00:52.530
a systems various states.
In this way,

18
00:00:52.650 --> 00:00:55.290
it broke the problem down into simpler sub problems.

19
00:00:55.560 --> 00:00:58.440
The bellman equation is now used in many,
many fields.

20
00:00:58.650 --> 00:01:00.840
It helps with minimizing flight time for airplanes,

21
00:01:01.070 --> 00:01:02.700
maximizing profits for hedge funds,

22
00:01:02.910 --> 00:01:07.830
minimizing the beef that Soulja boy seems to have with everyone.
No,
no,
no,

23
00:01:07.831 --> 00:01:08.664
no,
no,
no,
no.

24
00:01:11.190 --> 00:01:15.180
So bellman squad was making waves throughout the math community,
but meanwhile,

25
00:01:15.210 --> 00:01:18.960
a psychologist named Edward Thorndyke was trying to understand how learning

26
00:01:18.961 --> 00:01:21.090
works.
By studying the animal kingdom.

27
00:01:21.091 --> 00:01:23.850
He came up with what he called the law of effect,

28
00:01:23.970 --> 00:01:27.690
which states responses that produce a satisfying effect in a particular

29
00:01:27.691 --> 00:01:32.130
situation become more likely to occur again in that situation and responses that

30
00:01:32.131 --> 00:01:35.430
produce a discomforting effect become less likely to occur again in that

31
00:01:35.431 --> 00:01:38.730
situation.
Thanks.
Captain obvious now is actually pretty important.
Discovery.

32
00:01:39.000 --> 00:01:42.750
One of his experiments was putting a cat in a wooden box and observing it while

33
00:01:42.751 --> 00:01:46.230
it tried a bunch of different ways of getting out until it finally hit the lever

34
00:01:46.320 --> 00:01:48.990
that opened the box.
When you put the cat back in the box,

35
00:01:49.060 --> 00:01:51.030
it immediately knew that hit the lever to get out.

36
00:01:51.210 --> 00:01:54.330
And it was able to do that because of the process of trial and error,

37
00:01:54.450 --> 00:01:57.540
which is what thorn deck was essentially describing in the law of effect.

38
00:01:57.600 --> 00:01:58.770
A couple decades later,

39
00:01:58.771 --> 00:02:02.580
a British computer scientists named Chris Watkins stock that perhaps these two

40
00:02:02.581 --> 00:02:05.910
ideas could be combined to create a new type of learning algorithm.

41
00:02:06.090 --> 00:02:08.820
The idea of designing an agent that minimize it,

42
00:02:08.821 --> 00:02:10.440
some behavior of a system over time,

43
00:02:10.620 --> 00:02:14.370
like the bellman equation and does so through the process of trial and error,

44
00:02:14.530 --> 00:02:15.810
similar to the law of effect.

45
00:02:16.050 --> 00:02:18.840
And so he invented a novel reinforcement learning technique.

46
00:02:18.960 --> 00:02:21.120
He called Q learning.
So what is this?

47
00:02:21.420 --> 00:02:24.390
Let's say we had five rooms in a building connected by doors and we'll just

48
00:02:24.391 --> 00:02:26.970
think of everything outside of the building as one big room.

49
00:02:27.330 --> 00:02:29.400
All of spacetime is room five.

50
00:02:30.990 --> 00:02:32.670
We can think of the system as a graph.

51
00:02:32.730 --> 00:02:36.900
Each room is a node and each door is a link like room one has doors to both room

52
00:02:36.901 --> 00:02:38.520
five and three so they're connected.

53
00:02:38.790 --> 00:02:42.030
Our goal is to put an agent in any room and for it to learn how to get to room

54
00:02:42.031 --> 00:02:44.160
five through trial and error.
So the gold room,

55
00:02:44.161 --> 00:02:45.990
his room five does that room five as a goal,

56
00:02:45.991 --> 00:02:49.530
we can associate a reward value with each door,
which is the link between notes.

57
00:02:49.650 --> 00:02:52.830
So doors that lead immediately to the goal room get an instant reward of a

58
00:02:52.831 --> 00:02:55.320
hundred doors,
not directly connected to our goal room,

59
00:02:55.321 --> 00:02:57.270
gets zero reward in cue learning.

60
00:02:57.271 --> 00:03:00.520
The goal is to reach the state with the highest through a set of actions.

61
00:03:00.521 --> 00:03:04.120
So if each room is a state,
each action is represented by an Arrow.

62
00:03:04.330 --> 00:03:07.120
And the mapping of state to action is the agent's policy.

63
00:03:07.121 --> 00:03:10.480
It uses the reward that you as a signal to improve its policy over time.

64
00:03:10.570 --> 00:03:13.840
And it stores what he has learned through experience and what's called the Q

65
00:03:13.841 --> 00:03:14.590
matrix.

66
00:03:14.590 --> 00:03:18.040
The rows represent the possible states and the columns are possible actions

67
00:03:18.041 --> 00:03:19.090
leading to the next state.

68
00:03:19.390 --> 00:03:23.230
It updates the Q matrix over time as it learns the best actions to maximize the

69
00:03:23.231 --> 00:03:25.120
reward.
Seems pretty useful,
right?

70
00:03:25.290 --> 00:03:27.850
Cuellar he's gotta be used everywhere in video games,
right?

71
00:03:28.030 --> 00:03:32.170
Consumer video game box needs to be good but not so good that a human couldn't

72
00:03:32.171 --> 00:03:32.950
beat them.

73
00:03:32.950 --> 00:03:36.850
The bots that used Q learning to master games like chess and checkers and most

74
00:03:36.851 --> 00:03:40.510
recently Atari Games become insanely good at whatever they play.

75
00:03:40.780 --> 00:03:45.580
Academic AI learns while consumer game AI generally just make educated guesses,

76
00:03:45.581 --> 00:03:49.060
it doesn't really learn and its actions are all scripted but as different as

77
00:03:49.061 --> 00:03:50.830
they are.
The two fields are converging.

78
00:03:50.860 --> 00:03:54.970
As we discover more about machine learning.
For example,
in Forza Motor sports,

79
00:03:55.060 --> 00:03:57.130
you can create a drive Atar for yourself.

80
00:03:57.220 --> 00:04:00.970
It's an AI that learns how you drive by observing you and can then imitate your

81
00:04:00.971 --> 00:04:01.804
driving style.

82
00:04:01.840 --> 00:04:04.960
Having adaptive behavior like this will make games more interesting and there's

83
00:04:04.961 --> 00:04:06.370
a lot of potential for more of it.

84
00:04:06.400 --> 00:04:10.630
So let's write out a 10 line high level python script that uses cue learning to

85
00:04:10.631 --> 00:04:14.890
train a Bot to get from point a to point B.
This game is a five by five grid.

86
00:04:15.010 --> 00:04:17.830
Our agent is the Yellow Square and the goal is to find its way to the green

87
00:04:17.831 --> 00:04:19.630
square or the Red Square to end the game.

88
00:04:19.930 --> 00:04:23.590
Each cell represents a state the agent can be in and there are four actions up,

89
00:04:23.591 --> 00:04:24.424
down,
left and right.

90
00:04:24.700 --> 00:04:28.150
Moving a step will give us a reward of negative 0.04 the red cell gives us

91
00:04:28.151 --> 00:04:32.050
negative one and the green cell gives us positive one so we ideally want to get

92
00:04:32.051 --> 00:04:35.470
to the green cell every time the game world is already built for us.

93
00:04:35.471 --> 00:04:39.340
So we'll just start off by importing that at the top.
Then in our main function,

94
00:04:39.341 --> 00:04:42.940
we can go ahead and create a wild statement set to true because we want our

95
00:04:42.941 --> 00:04:44.350
agent to run indefinitely.

96
00:04:44.680 --> 00:04:48.550
Next we'll initialize our bots position from a worldclass and set it to the

97
00:04:48.551 --> 00:04:49.390
agent variable.

98
00:04:49.510 --> 00:04:52.540
So now we want our Bot to pick the right action to take in the game world.

99
00:04:52.540 --> 00:04:57.370
And the question is how do we decide that I got a grid of squares measured five

100
00:04:57.371 --> 00:05:00.240
by five and I'm going to get to green in one piece alive.

101
00:05:00.430 --> 00:05:02.710
I'm going to make a cute matrix.
Initial life.

102
00:05:02.890 --> 00:05:05.130
She'd been every single reward of mine or cod.

103
00:05:05.170 --> 00:05:08.890
Then I'm a pick an action straight how to queue.
Then I'm a do it.
Yeah,

104
00:05:08.891 --> 00:05:13.690
you just fresh and new update you with reward on beat and once I got that I'll

105
00:05:13.691 --> 00:05:15.190
go ahead and repeat.
Yeah.

106
00:05:15.360 --> 00:05:18.300
We'll use our box position as a parameter for the Max Q function,

107
00:05:18.301 --> 00:05:21.900
which we'll choose an action from our Q matrix as well as the potential reward

108
00:05:22.180 --> 00:05:25.170
and then we can perform that action by inputting the action as a parameter to

109
00:05:25.171 --> 00:05:28.290
the do action method,
which will return our bot the action.

110
00:05:28.291 --> 00:05:32.370
We took the reward received any updated bots position after taking the action.

111
00:05:32.730 --> 00:05:34.920
Now we're ready to update our Q matrix,

112
00:05:35.010 --> 00:05:38.580
so we'll use the updated bots position as the perimeter will print out both

113
00:05:38.581 --> 00:05:40.740
parameters to terminal so we can observe the results.

114
00:05:41.100 --> 00:05:45.090
We'll run this script by typing in python learner.py into terminal and it'll pop

115
00:05:45.091 --> 00:05:46.310
up as a Gui.

116
00:05:46.650 --> 00:05:49.950
The Bot will immediately start trying out possible paths to get to the green
one,

117
00:05:50.160 --> 00:05:52.800
and we can observe this score in terminal improving over time.

118
00:05:53.220 --> 00:05:56.820
This bot in particular gets really good really fast,
like in 10 seconds.

119
00:05:56.821 --> 00:05:59.510
It's found the ideal path and it's just going to keep doing it.

120
00:05:59.990 --> 00:06:00.860
So to break it down,

121
00:06:01.010 --> 00:06:04.430
reinforcement learning is the process of learning by interacting with an

122
00:06:04.431 --> 00:06:06.290
environment through positive feedback.

123
00:06:06.620 --> 00:06:10.490
Cure learning is a type of reinforcement learning that minimizes the behavior of

124
00:06:10.491 --> 00:06:12.860
a system over time,
through trial and error.

125
00:06:13.070 --> 00:06:15.110
And it does this by updating its policy,

126
00:06:15.111 --> 00:06:18.290
which is a mapping of state to action based on a reward.

127
00:06:18.410 --> 00:06:21.890
The coding challenge for this video is to modify this code so that the game

128
00:06:21.891 --> 00:06:24.260
world is bigger and has more obstacles.

129
00:06:24.440 --> 00:06:27.650
Let's make it harder for our cue learning Bot to find the optimal strategy.

130
00:06:27.920 --> 00:06:29.330
Details are in the read me poster.

131
00:06:29.331 --> 00:06:32.870
Get humbling in the comments and I'll announce the winner.
Next video.
For now,

132
00:06:32.930 --> 00:06:35.720
I've got to optimize my life.
So thanks for watching.

