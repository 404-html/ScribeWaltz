WEBVTT

1
00:00:00.230 --> 00:00:04.410
Yes,
I beat it that that impress you.
If I built an AI,
beat this for me,

2
00:00:04.411 --> 00:00:05.244
would that impress you?

3
00:00:06.770 --> 00:00:10.140
Hello world walking this Raj Ologie and this episode we're going to build an AI

4
00:00:10.141 --> 00:00:11.550
to beat a bunch of Atari Games.

5
00:00:11.580 --> 00:00:14.820
Games have had a long history of being a testbed for AI ever since the days of

6
00:00:14.821 --> 00:00:15.240
Pong.

7
00:00:15.240 --> 00:00:18.420
Traditionally game programmers have taken a reductionist approach to building
AI.

8
00:00:18.480 --> 00:00:21.690
They've reduced the simulated world to a model and had the AI act on prior

9
00:00:21.691 --> 00:00:26.280
knowledge of that model.
And it worked out for the most part.
I guess not really,

10
00:00:26.310 --> 00:00:28.890
but what if we want to build an AI that can be used in several different types

11
00:00:28.891 --> 00:00:30.870
of game worlds?
All the world models are different,

12
00:00:30.871 --> 00:00:34.080
so we couldn't just beat it in one world model.
Instead of modeling the world,

13
00:00:34.110 --> 00:00:35.340
we need to model the mind.

14
00:00:35.520 --> 00:00:38.670
We want to create an AI that can become a pro at any game we throw at it.

15
00:00:38.730 --> 00:00:40.980
So in thinking about this problem,
we have to ask ourselves,

16
00:00:41.010 --> 00:00:42.720
what is the dopest way of doing this?
Well,

17
00:00:42.721 --> 00:00:46.290
the London based startup deep mine already did this in 2015 deep minds goal is

18
00:00:46.291 --> 00:00:47.190
to create AGI.

19
00:00:47.220 --> 00:00:50.580
That's one algorithm that can solve any problem with human level thinking or

20
00:00:50.581 --> 00:00:50.970
greater.

21
00:00:50.970 --> 00:00:53.820
They reached an important milestone by creating an algorithm that was able to

22
00:00:53.821 --> 00:00:57.780
master 49 different Atari Games with no game specific tuning whatsoever.

23
00:00:57.840 --> 00:01:01.530
The algorithm is called the deep Q learner and it was recently made open source

24
00:01:01.560 --> 00:01:03.450
on get hub.
It only takes two inputs,

25
00:01:03.480 --> 00:01:05.520
the raw pixels of the game and the game score.

26
00:01:05.550 --> 00:01:08.970
That's it based on just that it has to complete its objective,
maximize a score.

27
00:01:09.030 --> 00:01:11.850
Let's dive into how this works since we'll want to recreate the results.
First,

28
00:01:11.910 --> 00:01:14.970
it uses a deep convolutional neural network to interpret the pixels.

29
00:01:15.000 --> 00:01:18.030
This is the type of neural network inspired by how our visual cortex operates

30
00:01:18.031 --> 00:01:20.850
and expects images as inputs,
images,
or high dimensional data.

31
00:01:20.850 --> 00:01:24.090
So we need to reduce number of connections.
Each neuron has to avoid overfitting.

32
00:01:24.390 --> 00:01:26.580
Overfitting by the way,
is when your model is too complex,

33
00:01:26.630 --> 00:01:29.460
there are too many parameters and so it's overly tuned to the data you've given

34
00:01:29.461 --> 00:01:31.680
it and won't generalize well for any new Dataset.

35
00:01:31.830 --> 00:01:33.360
So unlike a regular neural network,

36
00:01:33.370 --> 00:01:37.430
I CNNs layers are stacked in three dimensions and it's makes it easy to connect

37
00:01:37.470 --> 00:01:41.520
each neuron only to neurons and it's local region instead of every single other

38
00:01:41.521 --> 00:01:41.850
neurons.

39
00:01:41.850 --> 00:01:45.390
Each layer acts as a detection filter for the presence of specific features in

40
00:01:45.391 --> 00:01:48.510
an image.
And the layers get increasingly abstract with feature representation.

41
00:01:48.540 --> 00:01:50.580
So the first layer,
it could be a simple feature like edges,

42
00:01:50.610 --> 00:01:53.220
and then the next layer would use those edges to the text and shapes.

43
00:01:53.280 --> 00:01:55.470
And the next one would you use those shapes to detect something even more

44
00:01:55.471 --> 00:01:56.430
complex like Kanye,

45
00:01:56.460 --> 00:01:59.730
these hierarchical layers of distraction or what neural nets do really well,

46
00:01:59.760 --> 00:02:00.390
like really well.

47
00:02:00.390 --> 00:02:03.330
So once it's interpreted the pixels and needs to act on that knowledge in some

48
00:02:03.331 --> 00:02:06.510
way.
In a previous episode we talked about supervised and unsupervised learning,

49
00:02:06.540 --> 00:02:07.373
but wait,

50
00:02:10.230 --> 00:02:13.080
it's called reinforcement learning,
reinforcement learning.

51
00:02:13.081 --> 00:02:14.460
It's all about trial and error.

52
00:02:14.520 --> 00:02:17.910
It's about teaching an AI to select actions to maximize future rewards.

53
00:02:17.940 --> 00:02:20.820
It's similar to how you would train a dog.
If the dog fetches the ball,

54
00:02:20.850 --> 00:02:23.460
you give it a treat.
If it doesn't,
then you withhold the tree.

55
00:02:23.490 --> 00:02:25.410
So while the game is running at each time step,

56
00:02:25.440 --> 00:02:28.920
the Ai Execution Action based on what it observes and may or may not receive a

57
00:02:28.921 --> 00:02:30.810
reward.
If it does receive a reward,

58
00:02:30.870 --> 00:02:34.110
we'll adjust our weights so that the AI will be likely to do a similar action in

59
00:02:34.111 --> 00:02:34.710
the future.

60
00:02:34.710 --> 00:02:37.560
Cue learning is a type of reinforcement learning that learns the optimal action

61
00:02:37.561 --> 00:02:41.370
selection of behavior for the AI without having a prior model of the
environment.

62
00:02:41.550 --> 00:02:42.720
So based on the current game state,

63
00:02:42.721 --> 00:02:45.930
like an enemy spaceship thing and shooting this since the AI will eventually

64
00:02:45.931 --> 00:02:47.520
know to take the action of shooting it,

65
00:02:47.940 --> 00:02:51.120
this mapping of state to action is its policy and it gets better and better with

66
00:02:51.121 --> 00:02:54.390
training.
Deep Q also uses something I'll experience replay,

67
00:02:54.540 --> 00:02:57.780
which means the AI learns from the Dataset of its past policies as well.

68
00:02:57.810 --> 00:03:00.970
So we're going to build our gay bought in just 10 lines of python using a

69
00:03:00.971 --> 00:03:04.030
combination of tensorflow and Jim tensorflow is Google's ml library,

70
00:03:04.031 --> 00:03:07.570
which we'll use to create our CNN and Jim is open AI ml library,

71
00:03:07.571 --> 00:03:10.450
which we'll use to create our reinforcement learning algorithm and set up our

72
00:03:10.451 --> 00:03:12.130
environment.
Oh,
if you haven't heard,

73
00:03:12.160 --> 00:03:16.480
open AI is a nonprofit AI research lab focused on creating Agi and an open

74
00:03:16.481 --> 00:03:19.600
source way.
You've got 1 billion bucks.
Buy It from people like Elon Musk.

75
00:03:20.710 --> 00:03:25.690
Good luck.
Get the best.
Let's start off by importing our dependencies.

76
00:03:25.750 --> 00:03:28.690
Environment is our helper class that will help initialize our game environment.

77
00:03:28.720 --> 00:03:30.430
In our case,
this will be space invaders,

78
00:03:30.431 --> 00:03:33.400
but we can easily switch that out to a whole host of different environments.

79
00:03:33.430 --> 00:03:34.720
Jim is very modular opening.

80
00:03:34.721 --> 00:03:37.480
I wants it to be a gym for AI agents and training and get better.

81
00:03:37.510 --> 00:03:39.760
You can submit your algorithms.
They're cipher and evaluation.

82
00:03:39.880 --> 00:03:42.580
They'll score it against a set of service.
I'd metrics I approved.

83
00:03:42.610 --> 00:03:45.970
We'll also want to import our deep Q network helper class to observe the game in

84
00:03:45.971 --> 00:03:48.280
our training class to initialize a reinforcement learning.

85
00:03:48.310 --> 00:03:49.660
Once we've imported our dependencies,

86
00:03:49.661 --> 00:03:51.700
we can go ahead and initialize our environment well,

87
00:03:51.701 --> 00:03:55.210
set the parameters to space invaders and then initialize our agent using our DQ

88
00:03:55.211 --> 00:03:58.480
and helper class with the environment and environment type at the parameters.

89
00:03:58.510 --> 00:03:59.171
Once we have that,

90
00:03:59.171 --> 00:04:02.080
we can start training by running the trainer class with the agent has the

91
00:04:02.081 --> 00:04:02.914
perimeter first.

92
00:04:02.950 --> 00:04:06.640
This will populate our initial replay memory with 50,000 plays so we have a

93
00:04:06.641 --> 00:04:07.990
little experience and train with.

94
00:04:08.050 --> 00:04:11.920
Then it will initialize our CNN to start reading and pixels and our key learning

95
00:04:11.921 --> 00:04:15.610
algorithm to start updating our agents decisions based on the pixels it
receives.

96
00:04:15.640 --> 00:04:19.180
This is an implementation of the classic agent environment loop.
Each time set,

97
00:04:19.181 --> 00:04:22.090
the agent chooses an action and the environment returns and an observation and a

98
00:04:22.091 --> 00:04:22.630
reward.

99
00:04:22.630 --> 00:04:26.710
The observation is raw pixel data which we can feed into our CNN and the reward

100
00:04:26.711 --> 00:04:29.020
is a number we can use to help improve our next actions.

101
00:04:29.080 --> 00:04:32.240
Jim Neatly returns these parameters to use via the step function which we

102
00:04:32.241 --> 00:04:34.450
brought in the environment helper class,
so we started training.

103
00:04:34.451 --> 00:04:36.910
We can start the game with the play function of our agent.
Object.

104
00:04:36.911 --> 00:04:39.820
We can go ahead and run this and terminal and the space invaders windows should

105
00:04:39.821 --> 00:04:42.730
pop up and we'll start seeing the AI started tending to play the game.

106
00:04:42.760 --> 00:04:45.640
It'll be hilariously bad at first,
but we'll slowly get better with time.

107
00:04:46.780 --> 00:04:49.510
The Ai will get more difficult to defeat the longer you train it,

108
00:04:49.511 --> 00:04:52.900
and ideally you can apply to any game you create,
but more info.

109
00:04:52.901 --> 00:04:55.600
Check out the links down below and please subscribe for more machine learning

110
00:04:55.601 --> 00:04:59.440
videos.
For now,
I've got to go fix a runtime error,
so thanks for watching.

