WEBVTT

1
00:00:00.060 --> 00:00:03.420
Hello world.
It's a Raj and AI in marketing.

2
00:00:03.510 --> 00:00:05.550
That's the copic for today's video.

3
00:00:05.760 --> 00:00:10.680
I want to start off this video with a demo and what this is is an AI writer.

4
00:00:10.710 --> 00:00:14.940
What it does is it takes in as input a keyword.
In my case,

5
00:00:14.941 --> 00:00:16.860
I'll say it's going to be wine.
Okay,

6
00:00:16.861 --> 00:00:20.310
so I want to write an article about red wine and I'm going to send it to my

7
00:00:20.311 --> 00:00:24.990
email address.
It's going to use an AI to then generate an article based on that.

8
00:00:25.020 --> 00:00:27.630
It might take a few minutes,
it might take a few hours,

9
00:00:27.750 --> 00:00:30.840
but luckily I already have the outputs right here for us.

10
00:00:31.080 --> 00:00:36.000
What it does is it cause it's going to generate text based on that keyword and

11
00:00:36.001 --> 00:00:38.610
it's gonna use its sources are going to be the internet,

12
00:00:38.611 --> 00:00:42.630
so it's going to search the Internet for other related articles on the topic.

13
00:00:42.810 --> 00:00:47.810
It's going to take all that text and compile it into one giant text dataset.

14
00:00:48.410 --> 00:00:50.550
It's going to feed that into its AI model,

15
00:00:50.551 --> 00:00:54.780
which is likely an LSTM neural network.
Don't worry if you don't understand that,

16
00:00:54.900 --> 00:00:58.480
I'll talk about that later in the video.
And then it's going to output an,

17
00:00:58.740 --> 00:01:02.520
an article that we see here is going to be a very compressed article,

18
00:01:02.521 --> 00:01:03.450
low compression.

19
00:01:03.630 --> 00:01:08.100
It can be a medium or a high compression article and they can get pretty good.

20
00:01:08.101 --> 00:01:10.050
Okay.
Pretty damn good.
So good.

21
00:01:10.051 --> 00:01:14.850
In fact that it's very likely that you've already write an article or seen some

22
00:01:14.851 --> 00:01:19.470
kind of data that was created entirely by an AI and you didn't even know it.

23
00:01:19.680 --> 00:01:23.820
Very famous brands from Fox to Yahoo to the Associated Press,

24
00:01:23.850 --> 00:01:26.520
all use AI to generate content.

25
00:01:26.670 --> 00:01:30.090
And that's just one of the ways that AI can be used in both marketing,

26
00:01:30.091 --> 00:01:35.070
advertising and the entire marketing funnel,
the pipeline.
So in this video,

27
00:01:35.071 --> 00:01:36.840
I'm going to talk about ways we can do that.

28
00:01:37.020 --> 00:01:40.350
And at the very end we're going to look at some code for different architectures

29
00:01:40.351 --> 00:01:42.090
that we can use to do this.
Okay,

30
00:01:42.091 --> 00:01:47.091
so let's start off with the ways that AI can be used in marketing,

31
00:01:47.671 --> 00:01:51.510
right?
So one way is through audience targeting,
right?

32
00:01:51.511 --> 00:01:54.630
So if you have some startup,
a company,
a brand,

33
00:01:54.810 --> 00:01:58.800
you have an audience of customers,
right?
And if you have some new product,

34
00:01:58.980 --> 00:02:03.780
you want to target those specific subset of your customers that are going to be

35
00:02:03.781 --> 00:02:08.190
most,
uh,
willing to convert into sales.
You don't want to waste your time,

36
00:02:08.310 --> 00:02:12.420
your energy,
your money,
your resources on customers who won't convert,
right?

37
00:02:12.600 --> 00:02:16.440
And so how do you target those exact customers that would be most likely to

38
00:02:16.441 --> 00:02:20.940
convert?
This is a perfect use case for AI and we'll talk about that in a bit.

39
00:02:21.150 --> 00:02:24.060
The second part is content creation,
right?

40
00:02:24.060 --> 00:02:28.470
If you've already targeted those users that are most likely to convert to sales

41
00:02:28.471 --> 00:02:30.000
for your product,
how are then,

42
00:02:30.240 --> 00:02:33.420
how are you supposed to generate content that would be best suited for them?

43
00:02:33.630 --> 00:02:36.750
The ideal,
the easy way is to get a human to do it.

44
00:02:36.930 --> 00:02:39.360
But could you get an AI to do it?
Is the question.

45
00:02:39.361 --> 00:02:43.590
And the answer of course is yes,
and we'll talk about that as well.
Now,

46
00:02:43.650 --> 00:02:47.100
real time optimization is another strategy using Ai.

47
00:02:48.030 --> 00:02:50.490
Let's say you have some marketing campaign.

48
00:02:50.491 --> 00:02:52.110
Let's say it's an email campaign rights,

49
00:02:52.320 --> 00:02:56.160
and you're constantly sending ads out to your users to see how they feel about

50
00:02:56.161 --> 00:02:59.560
the product and you're getting feedback from them,
whether they like it or not.

51
00:02:59.920 --> 00:03:01.570
How do you optimize that content,

52
00:03:01.571 --> 00:03:05.740
your route in real time so that you're learning from what your users like and

53
00:03:05.741 --> 00:03:08.890
what they don't like.
And this has been happening,
like I said,
for a very,

54
00:03:08.891 --> 00:03:11.650
very long time.
And we're going to learn how in this video,

55
00:03:12.940 --> 00:03:15.340
so some B,
two B marketers were asked,

56
00:03:15.400 --> 00:03:20.170
what are these metrics that indicate successful return on Ai Investment?

57
00:03:20.380 --> 00:03:25.380
And across the board that the number one way with 59% of respondents saying this

58
00:03:25.960 --> 00:03:29.140
was,
that gives them better close rates for sales.

59
00:03:29.380 --> 00:03:34.210
So sales equates to revenue.
And revenue is the lifeblood of any company,

60
00:03:34.211 --> 00:03:37.690
right?
You can't have a company if you don't have revenue,
okay?

61
00:03:37.691 --> 00:03:40.390
So if I didn't have revenue,
I couldn't hire people.

62
00:03:40.391 --> 00:03:43.810
I couldn't make videos for you.
I couldn't live.
I've got revenue,
okay?
So,

63
00:03:43.930 --> 00:03:46.990
and I am using AI to help target my content.

64
00:03:47.050 --> 00:03:51.160
So I myself and admitting that I'm using AI to help target and you know,

65
00:03:51.161 --> 00:03:55.180
make sure my content is as,
as targeted as possible.

66
00:03:55.480 --> 00:03:58.960
And they say in the end,
if you have some product that you're selling,

67
00:03:59.080 --> 00:04:03.100
it's going to then help close the rates for sales,
right?
For me,
for me,

68
00:04:03.101 --> 00:04:05.320
that was my decentralized applications course.

69
00:04:05.321 --> 00:04:08.230
That was my only paid offering up to this point.

70
00:04:08.440 --> 00:04:12.010
And I targeted it very well and it resulted in some good sales,

71
00:04:12.250 --> 00:04:14.110
which is gonna help me grow the business later on.

72
00:04:14.950 --> 00:04:16.570
So Ai can be used for that as well.

73
00:04:17.170 --> 00:04:20.440
So let's talk about some startups in the space that are helping to move this

74
00:04:20.441 --> 00:04:23.260
field forward,
right?
So happier is one example.

75
00:04:23.380 --> 00:04:27.730
This is a Taiwan based company and what they do is they predict what audience

76
00:04:27.731 --> 00:04:29.860
members are likely to do next,
right?

77
00:04:29.861 --> 00:04:34.240
So if you have some products or some ecommerce websites and you're selling shoes

78
00:04:34.630 --> 00:04:39.250
and you're selling all sorts of clothing,
let's say if a customer buys shoes,

79
00:04:39.610 --> 00:04:43.210
then it is likely that the customer won't buy another pair of shoes.

80
00:04:43.480 --> 00:04:47.230
But it is likely that they'll buy a pair of socks.
The compliment those shoes,

81
00:04:47.500 --> 00:04:49.840
how can you predict what the customer is going to do next?

82
00:04:50.110 --> 00:04:54.040
And what they do is they provide AI as a service to companies to allow them to

83
00:04:54.041 --> 00:04:56.140
predict what customers are going to do next.

84
00:04:57.490 --> 00:05:02.350
Another example is drawbridge,
right?
So users switch devices all the time,

85
00:05:02.380 --> 00:05:04.750
right?
So they can be on mobile,
that can be on desktop,

86
00:05:04.751 --> 00:05:07.540
that can be on a ps four and ideally we can,

87
00:05:07.720 --> 00:05:12.460
we can target the type of content to the specific device that they're on at the

88
00:05:12.461 --> 00:05:14.590
time.
And so that's what drawbridge does.

89
00:05:14.591 --> 00:05:19.150
It predicts what times that a user is going to be on a specific platform and

90
00:05:19.151 --> 00:05:22.750
then it allows a brand to create content based on that platform.

91
00:05:23.110 --> 00:05:26.620
Another example is inside sales.com right?

92
00:05:26.621 --> 00:05:28.960
So if you have a lot of perspective sales,

93
00:05:29.110 --> 00:05:32.320
you can just target all of them because you're going to be wasting time and

94
00:05:32.321 --> 00:05:36.520
energy and money on targeting this giant segment of users.

95
00:05:36.910 --> 00:05:38.420
When it turns out that the,

96
00:05:38.430 --> 00:05:43.360
the customers that are most likely to convert are probably only 20% and that's

97
00:05:43.361 --> 00:05:44.530
what inside sales does,

98
00:05:44.531 --> 00:05:49.330
is it helps you find that 20% of your customer base that are going to be the

99
00:05:49.331 --> 00:05:53.290
most likely to convert to the sales for your product that you're pushing.

100
00:05:54.520 --> 00:05:57.310
And there's one more I want to talk about.
It's called Persado.

101
00:05:57.440 --> 00:06:02.210
What they do is they will help you create content.

102
00:06:02.450 --> 00:06:02.581
Hey,

103
00:06:02.581 --> 00:06:06.290
we'll help you find the phrases and the words that's going to drive the greatest

104
00:06:06.291 --> 00:06:09.990
action for your audience to convert into sales.
So you could say,
you know,

105
00:06:10.010 --> 00:06:12.860
a text message in this way,
but if you say it in this way,

106
00:06:12.861 --> 00:06:15.980
you reworded a little bit,
it's going to increase sales by this much.

107
00:06:16.130 --> 00:06:20.060
And this is something that we humans try to do.
We use our intuition,
right?

108
00:06:20.061 --> 00:06:24.620
So you know don draper from mad men,
he comes into the boardroom,
he's like,

109
00:06:24.621 --> 00:06:26.780
this is going to work.
And then it works and it's beautiful.

110
00:06:26.840 --> 00:06:31.820
That's not how it works anymore.
If you don't have a database decision,

111
00:06:31.970 --> 00:06:36.760
what are you doing?
Right?
So really good marketers and really good,
um,

112
00:06:36.890 --> 00:06:39.580
you know,
salespeople and advertiser,
not salespeople.

113
00:06:39.590 --> 00:06:44.480
Advertisers tells people as well,
but not in this case,
use AI to make decisions,

114
00:06:44.720 --> 00:06:48.050
okay?
It's not just about intuition alone.
You've got to use it.

115
00:06:48.080 --> 00:06:51.740
You've got to base your decision on the data.
If you don't got the data,

116
00:06:51.890 --> 00:06:56.800
you don't got anything,
right?
So Shit.
So anyway,
he's dope.

117
00:06:58.040 --> 00:06:59.900
Let's start with audience targeting,
right?

118
00:06:59.901 --> 00:07:04.340
So how do we target some segment of the audience that is going to be most likely

119
00:07:04.341 --> 00:07:08.270
to convert for a given ad and then focus all of our time and energy on that

120
00:07:08.271 --> 00:07:12.560
segments.
So we could think about this as a recommender system problem,
right?

121
00:07:12.561 --> 00:07:16.490
So a bunch of sites use recommender systems,
Amazon,
Netflix,
you know,

122
00:07:16.491 --> 00:07:17.930
everybody uses it these days.

123
00:07:18.110 --> 00:07:21.170
And one very common way to build a recommender systems,

124
00:07:21.380 --> 00:07:25.040
it's a huge matrix factorization,
okay?
So here's how it works.

125
00:07:25.280 --> 00:07:28.610
If you have some product,
and in this case we're talking about marketing.

126
00:07:28.820 --> 00:07:31.880
So our product is going to be a,
an ad,
right?

127
00:07:31.881 --> 00:07:35.600
So you have a bunch of ads you've tried out and the users rated all of those ads

128
00:07:35.601 --> 00:07:40.280
on a scale of one to 10 and what this turns out is you have a giant matrix of

129
00:07:40.281 --> 00:07:43.910
users and ads and their ratings for all of those ads.

130
00:07:44.090 --> 00:07:47.570
So it turns out that those users aren't going to rate all of those ads.

131
00:07:47.571 --> 00:07:48.980
They're only going to rate some of them.

132
00:07:49.700 --> 00:07:54.700
So what we do for Matrix factorization is we decompose this giant matrix into

133
00:07:55.221 --> 00:07:56.630
two different matrices.

134
00:07:56.960 --> 00:08:00.560
One is going to be how a user's rates certain features of an ad.

135
00:08:00.830 --> 00:08:05.830
The other is going to be how certain features are rated by users for a specific,

136
00:08:06.220 --> 00:08:10.110
um,
ad.
So two different matrices.

137
00:08:10.940 --> 00:08:15.710
The process of decomposing this matrix into a two different feature matrix sees

138
00:08:16.070 --> 00:08:19.580
is a type of machine learning using neural networks or SPD is.

139
00:08:19.610 --> 00:08:21.050
But I've talked about that before.

140
00:08:21.290 --> 00:08:23.780
If you search Saroj recommender system on youtube,

141
00:08:23.781 --> 00:08:25.070
you'll find a bunch of videos there.

142
00:08:25.280 --> 00:08:27.500
But I want to give you a high level explanation here.

143
00:08:27.770 --> 00:08:31.610
Once we've decomposed that giant matrix into these two smaller matress seats,

144
00:08:31.970 --> 00:08:36.200
we then use the dot product to combine them again in such a way that all of

145
00:08:36.201 --> 00:08:38.150
those blank spaces are filled,
right?

146
00:08:38.151 --> 00:08:42.230
So what we're trying to do is we're trying to find those blank spaces.

147
00:08:42.500 --> 00:08:45.980
If a user has rated a certain ad this way and another ad this way,

148
00:08:46.430 --> 00:08:50.810
how would they rate this ad?
And so this is a prediction.

149
00:08:50.870 --> 00:08:53.720
This is what the AI is doing using Matrix factorization,

150
00:08:57.510 --> 00:09:00.270
popular library to do this has been light FM.

151
00:09:00.270 --> 00:09:04.260
I made a video on that before where it takes a generates user and item

152
00:09:04.261 --> 00:09:08.490
representations by functioning has factorization machines and learning the

153
00:09:08.491 --> 00:09:10.080
linear embeddings for each feature.

154
00:09:10.081 --> 00:09:14.160
It then takes a dot products of each of these two representation vector and gets

155
00:09:14.161 --> 00:09:16.560
a score.
But with deep neural networks,

156
00:09:16.561 --> 00:09:20.790
we can improve on this by creating more meaningful representations.
Right?

157
00:09:21.090 --> 00:09:24.390
Deep neural networks outperformed all other machine learning models when it

158
00:09:24.391 --> 00:09:28.050
comes to learning,
uh,
features.
Okay?
So when it comes to learning features,

159
00:09:28.230 --> 00:09:30.210
deep neural net blow everything else out of the water.

160
00:09:30.240 --> 00:09:35.010
If we have enough data and computing power and so tense or rec is a library that

161
00:09:35.011 --> 00:09:39.090
has a lot of developer activity and highly recommended built on top of

162
00:09:39.091 --> 00:09:41.280
tensorflow that does this.

163
00:09:41.281 --> 00:09:46.260
It allows us to use tensor flow to build deep neural networks for recommendation

164
00:09:46.290 --> 00:09:46.980
engines.

165
00:09:46.980 --> 00:09:51.840
This can be to recommend the optimal ads for your users to recommend products to

166
00:09:51.841 --> 00:09:55.800
recommend whatever it is.
Now,
ideally,
if you're a brand,
if you're a company,

167
00:09:55.950 --> 00:10:00.950
you've got some data set that shows how users feel about certain ads you've done

168
00:10:01.051 --> 00:10:05.820
in the past and based on that you can create new ads that a certain user would

169
00:10:06.030 --> 00:10:08.940
be likely to convert on.
So you have to make sure you have that data.

170
00:10:08.941 --> 00:10:10.860
If you don't have that data,
you're a startup,

171
00:10:10.861 --> 00:10:15.861
you want to build a service for brands and just Google datasets for data's had

172
00:10:16.021 --> 00:10:18.810
ad campaign and then test it out from there.

173
00:10:19.260 --> 00:10:22.350
But the idea is that going beyond Matrix factorization,

174
00:10:22.351 --> 00:10:26.160
we can build neural networks to do this,
right?

175
00:10:26.161 --> 00:10:30.690
And so the process is very similar in that we are learning to matrices and then

176
00:10:30.691 --> 00:10:34.860
we're performing a dot product between those two matrices to create predicted

177
00:10:34.861 --> 00:10:36.300
scores for a given user.

178
00:10:36.420 --> 00:10:40.050
And then we just read off of that Matrix to predict what a certain user would

179
00:10:40.051 --> 00:10:43.320
score a particular ad.
And then if we have some threshold,

180
00:10:43.321 --> 00:10:47.820
like if a user scores would score above an eight out of 10 for this ad,

181
00:10:48.060 --> 00:10:52.410
then deploy this ad to them if it's under that then deployed this ad to them.

182
00:10:52.590 --> 00:10:56.280
Okay.
So it's Kinda like that.
So there are four steps in this process.

183
00:10:56.281 --> 00:10:57.114
The first one,

184
00:10:57.300 --> 00:11:01.770
it's a transform our input data into feature tensors for easy embedding,
right?

185
00:11:01.771 --> 00:11:06.771
We have some input data that's a giant matrix and then we use pandas say to then

186
00:11:07.680 --> 00:11:11.550
convert that into a data frame object.
And once we do that,

187
00:11:11.551 --> 00:11:15.150
we convert them to feature tensors using an algorithm like word two Vec,

188
00:11:15.151 --> 00:11:18.000
which we could use in one line of code or several other ones.

189
00:11:18.210 --> 00:11:19.320
And then once we have that,

190
00:11:19.321 --> 00:11:23.940
we transform the user item feature tensors into future item representations.

191
00:11:24.240 --> 00:11:28.020
We transformed that pair into a prediction and then transform that prediction

192
00:11:28.230 --> 00:11:31.230
and truth value into a loss value for loss function.

193
00:11:31.230 --> 00:11:35.490
Minimize that using gradient descent until we have reached a minimum minimal

194
00:11:35.491 --> 00:11:36.810
loss function value.

195
00:11:36.811 --> 00:11:39.780
And then we can use that model to predict what a certain user would like.

196
00:11:39.781 --> 00:11:44.781
So here is a very simple programmatic example of us using tensor rec to

197
00:11:45.271 --> 00:11:49.230
recommend users.
A specific type of data.
This could be ads,
it could be anything,

198
00:11:49.231 --> 00:11:52.890
right?
So we build a model in a single line,
that's all we imported.

199
00:11:53.290 --> 00:11:56.560
We generate some dummy data,
we fit the [inaudible] model on that data.

200
00:11:56.620 --> 00:12:01.330
We predict,
uh,
what the scores would be would be based on the given data.

201
00:12:01.540 --> 00:12:06.540
And then we use a percentage recall as a evaluation metric to see how good it's

202
00:12:07.391 --> 00:12:12.160
doing on testing datasets.
That's it tend to wreck,
check it out.
Now,

203
00:12:13.180 --> 00:12:18.180
once we have targeted a specific subset of our users that we know exactly,

204
00:12:18.881 --> 00:12:20.740
we've automated that part of the pipeline.

205
00:12:20.950 --> 00:12:25.810
Now we can use humans to create content that is perfectly well suited for them.

206
00:12:26.290 --> 00:12:31.290
Or we could automate the entire pipeline where it's not just us targeting the

207
00:12:31.541 --> 00:12:35.230
users using AI will generate content for those users using Ai.

208
00:12:35.440 --> 00:12:39.400
So the entire process is automated,
right?
So this video is content.

209
00:12:39.401 --> 00:12:43.660
This video could be automated.
I who knows if I'm real or not,
right?

210
00:12:43.840 --> 00:12:47.320
You have no idea unless you met me in real life.
But even then,
who knows?

211
00:12:47.321 --> 00:12:50.140
Maybe I was a Hologram.
Anyway,
content creation,
right?

212
00:12:50.141 --> 00:12:54.610
So LSTM networks are the way to do this for text.
Okay?

213
00:12:54.611 --> 00:12:57.730
So if you search LSTM networks,
Raj,

214
00:12:57.910 --> 00:13:00.820
you find a great video on me explaining how they work in depth,

215
00:13:01.270 --> 00:13:03.940
but I'll give you a high level overview right now.

216
00:13:03.970 --> 00:13:08.800
So recurrent networks are really good at predicting sequences of,
of texts,

217
00:13:08.801 --> 00:13:10.960
right?
So normal feedforward networks,
right?

218
00:13:11.320 --> 00:13:14.290
Normal feedforward networks are not about predicting sequences.

219
00:13:14.291 --> 00:13:17.230
There are about predicting what an output would be for a given input.

220
00:13:17.260 --> 00:13:20.260
It learns the mapping.
But when it comes to a sequence,

221
00:13:20.320 --> 00:13:23.390
what happened before matters to what happens now?
What,

222
00:13:23.391 --> 00:13:25.870
what the word that you had previously,
like hi,

223
00:13:25.871 --> 00:13:29.770
like recording videos about Ai.

224
00:13:29.771 --> 00:13:33.480
Because I love,
you know,
maybe it's tensorflow,

225
00:13:33.490 --> 00:13:36.880
but predict what that word is.
You got to know about the word Ai.

226
00:13:36.881 --> 00:13:39.510
You've got to know about the word video.
You can just know,
you know,

227
00:13:40.390 --> 00:13:42.910
use the previous word.
You've got to use the whole sequence,
right?

228
00:13:42.911 --> 00:13:45.850
So what we're current networks do is at every time step,

229
00:13:46.030 --> 00:13:48.880
it's not just the next data point that's fed into the network.

230
00:13:49.000 --> 00:13:52.690
It's also the previous hidden layer that's learned over time.

231
00:13:52.830 --> 00:13:56.860
It's fed into the time step.
And so at every time step,
not just a new data point,

232
00:13:56.861 --> 00:14:00.430
but the previous hidden layer that was learned is fed into the network.

233
00:14:00.490 --> 00:14:03.520
So it's learning not just the next data point,

234
00:14:03.640 --> 00:14:07.270
but it's learning based on what I already learned before,
if that makes sense.

235
00:14:07.420 --> 00:14:10.900
And that's why it's called a recurrent network because there is this recurrence

236
00:14:10.990 --> 00:14:13.570
or feedback loop that's happening.
And what's,

237
00:14:13.571 --> 00:14:17.020
there's a problem though in that for recurrent networks,

238
00:14:17.350 --> 00:14:20.110
if the sequence is uh,
too long,

239
00:14:20.290 --> 00:14:23.080
then there's going to be a problem called the vanishing gradient problem.

240
00:14:23.200 --> 00:14:26.920
So during optimization backpropagation and we use the gradient,

241
00:14:26.921 --> 00:14:29.980
which is the difference between the real output into predicted output.

242
00:14:30.310 --> 00:14:33.610
We use the gradient to then update all of those layers beforehand.

243
00:14:33.790 --> 00:14:37.180
But what happens in recording networks is that gradient gets smaller and smaller

244
00:14:37.181 --> 00:14:39.460
and smaller.
The further back we go in the network.

245
00:14:39.580 --> 00:14:41.260
And so how do we preserve that gradient?

246
00:14:41.261 --> 00:14:43.270
So all the layers are updated accordingly.

247
00:14:43.480 --> 00:14:47.620
While we need to trap that gradient somehow into what are called long short term

248
00:14:47.621 --> 00:14:48.340
memory cells,

249
00:14:48.340 --> 00:14:53.300
cells that consists of gates values and these trap the gradients in such a way

250
00:14:53.420 --> 00:14:55.190
at the vanishing gradient goes away.

251
00:14:55.340 --> 00:14:57.440
And this allows a network to remember a very,
very,

252
00:14:57.441 --> 00:15:01.100
very long term sequences of data,
like an entire essay,
right?

253
00:15:01.101 --> 00:15:04.460
So they let us then write an entire essay or article.

254
00:15:05.750 --> 00:15:09.020
And so you might be thinking,
how do I build this very complex thing?

255
00:15:09.170 --> 00:15:10.760
And the answer is carrots.

256
00:15:10.761 --> 00:15:14.840
Carrots is a machine learning library built on top of tensorflow and let you

257
00:15:14.841 --> 00:15:19.370
build very complex,
deep neural networks in just a few lines of code.

258
00:15:19.580 --> 00:15:24.320
And Ai marketers can use this to automatically generate content best suited for

259
00:15:24.321 --> 00:15:26.450
a particular subset of their audience.

260
00:15:26.660 --> 00:15:29.330
So the idea is that if you have images or video,

261
00:15:29.600 --> 00:15:33.320
then you want to use generative adversarial networks to generate content.

262
00:15:33.470 --> 00:15:36.800
If you want to generate audio,
you use wave nets and if you want to use,

263
00:15:36.890 --> 00:15:38.060
if you want to generate text,

264
00:15:38.240 --> 00:15:41.690
you use LSTM recurrent networks like I'm talking about here.

265
00:15:41.690 --> 00:15:44.750
And remember I have videos for all of these models is search my name and the

266
00:15:44.751 --> 00:15:45.440
model.

267
00:15:45.440 --> 00:15:49.790
So here's a very simple example of us using care os in under a hundred lines of

268
00:15:49.791 --> 00:15:52.760
code that generates an essay in the style of niche.

269
00:15:53.120 --> 00:15:57.770
So the text is niches writing and then what it does is it takes that input data,

270
00:15:57.771 --> 00:15:59.330
it's formats that data,

271
00:15:59.360 --> 00:16:04.360
it feeds it into a model built in os where every line of code corresponds to a

272
00:16:04.371 --> 00:16:05.690
single layer and the network.

273
00:16:05.810 --> 00:16:08.750
So it's very readable code and it's only three layers long.

274
00:16:09.050 --> 00:16:13.520
We optimize it using rms prop,
which is a type of gradient descent search.

275
00:16:13.521 --> 00:16:18.521
Which activation function should I use Saroj on youtube to find a great video on

276
00:16:19.311 --> 00:16:24.200
all the differences between all of the different activation functions out there.

277
00:16:24.490 --> 00:16:24.710
You know,

278
00:16:24.710 --> 00:16:27.620
I also have a video in which optimization function should you use as well.

279
00:16:27.890 --> 00:16:32.360
But anyway,
once we do that,
we minimize it using a loss function.
And that's it.

280
00:16:32.361 --> 00:16:36.250
And then what it's gonna do is it's going to predict every word,
uh,
that,

281
00:16:36.540 --> 00:16:39.620
that Nietzsche would have said based on what it's learned in the past.

282
00:16:39.920 --> 00:16:42.200
And so we can use this for the entire pipeline,

283
00:16:42.201 --> 00:16:45.080
whether we're targeting users and then once we've targeted them generating

284
00:16:45.081 --> 00:16:48.230
content for them,
and we can do this for the entire marketing pipeline.

285
00:16:48.350 --> 00:16:49.670
There's any,
there's a huge,

286
00:16:49.671 --> 00:16:54.140
huge space for startups to come into this space and create services for big

287
00:16:54.141 --> 00:16:58.670
brands,
for consumers to help them optimize,
to save time,
to save money,

288
00:16:58.671 --> 00:17:01.880
to save resources using the latest technologies.
And look,

289
00:17:02.060 --> 00:17:05.000
even though I said that there are some startups out there that do this
currently,

290
00:17:05.150 --> 00:17:06.380
there's a giant,

291
00:17:06.410 --> 00:17:10.850
massive opportunity for new players in this space and there's a huge need for it

292
00:17:10.880 --> 00:17:13.250
as well.
So I hope you found this video useful.

293
00:17:13.280 --> 00:17:16.790
Please subscribe for more programming videos,
and for now,
I've got to be an AI,

294
00:17:17.060 --> 00:17:18.500
so thanks for watching.

