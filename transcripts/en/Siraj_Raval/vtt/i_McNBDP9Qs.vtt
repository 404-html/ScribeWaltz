WEBVTT

1
00:00:00.060 --> 00:00:00.780
Hello world.

2
00:00:00.780 --> 00:00:05.310
It's a Raj and welcome to my new AI for video games series.

3
00:00:05.490 --> 00:00:06.870
For the next 10 weeks,

4
00:00:07.020 --> 00:00:12.020
I'm going to teach you how a really popular subfield of AI called reinforcement

5
00:00:12.961 --> 00:00:17.961
learning works and we're going to be using all kinds of game environments to

6
00:00:18.241 --> 00:00:23.241
implement the theory behind the algorithms we learn the majority of the recent

7
00:00:24.211 --> 00:00:29.211
advancements in AI have been due to the use of supervised algorithms,

8
00:00:29.520 --> 00:00:34.520
namely neural networks applied to big datasets and using lots of gps for

9
00:00:35.071 --> 00:00:35.910
computation.

10
00:00:36.210 --> 00:00:41.210
Neural Nets have been around since the 50s and they're considered universal

11
00:00:41.700 --> 00:00:43.320
function approximators.

12
00:00:43.560 --> 00:00:47.460
That means that given any set of inputs and outputs,

13
00:00:47.670 --> 00:00:51.210
if given enough examples,
they can learn the function,

14
00:00:51.270 --> 00:00:54.390
the mapping that relates both of them together.

15
00:00:54.660 --> 00:00:59.580
We can then use this function to predict new outputs given some set of inputs.

16
00:01:00.150 --> 00:01:04.560
Everything from medical imaging for hospitals to license plate detection for

17
00:01:04.561 --> 00:01:07.560
governments to identify and crop yields for farmers.

18
00:01:07.800 --> 00:01:12.390
These incredible applications of deep neural nets have been due to the fact that

19
00:01:12.391 --> 00:01:17.391
the data set used to train the algorithms had an associated set of labels.

20
00:01:18.210 --> 00:01:20.760
A lot of times though,
if labels aren't available,

21
00:01:20.910 --> 00:01:25.910
data scientists will hire humans to hand label their datasets using services

22
00:01:26.011 --> 00:01:29.970
like Amazon's mechanical Turk,
but ideally we don't need labels.

23
00:01:29.971 --> 00:01:32.700
We can just train our algorithms on unlabeled data.

24
00:01:32.701 --> 00:01:37.701
Since the vast majority of the world's data does not in fact have labels.

25
00:01:38.070 --> 00:01:40.290
So if we want to train algorithms,

26
00:01:40.350 --> 00:01:43.320
unsupervised meaning no labels,

27
00:01:43.440 --> 00:01:47.730
then we can use techniques like clustering and anomaly detection.

28
00:01:48.060 --> 00:01:49.770
These are fast improving,

29
00:01:49.771 --> 00:01:54.771
but there's also room for another class of learning techniques that are based on

30
00:01:54.781 --> 00:01:58.200
trial and error in an environment setting.

31
00:01:58.470 --> 00:02:01.140
This is called reinforcement learning.

32
00:02:01.380 --> 00:02:04.830
The basic idea is that in reinforcement learning,

33
00:02:04.980 --> 00:02:09.300
the labels are time delayed and instead of calling them labels,

34
00:02:09.420 --> 00:02:10.980
we call them rewards.

35
00:02:11.130 --> 00:02:15.660
While supervised learning tells you how to achieve your goal.

36
00:02:15.690 --> 00:02:20.310
Reinforcement learning tells you how well you achieved the goal.

37
00:02:20.340 --> 00:02:25.260
There are lots of problems settings where the idea of time delayed labels makes

38
00:02:25.261 --> 00:02:26.010
more sense.

39
00:02:26.010 --> 00:02:30.600
Think about if you were tasked with creating an AI that learns how best to

40
00:02:30.601 --> 00:02:33.300
control the temperature in a data center.

41
00:02:33.540 --> 00:02:37.470
How are you going to tell your algorithm what the correct setting of each

42
00:02:37.471 --> 00:02:42.471
hardware component is at any given time step using reinforcement learning,

43
00:02:42.510 --> 00:02:46.650
you can use feedback data such as how much electricity was used at a certain

44
00:02:46.651 --> 00:02:49.110
time period or the average temperature.

45
00:02:49.200 --> 00:02:52.410
This is literally how Google reduced the cost of cooling.

46
00:02:52.440 --> 00:02:57.390
It's data centers by a massive amount,
so it's the real deal.
Unlike magic leap,

47
00:02:58.260 --> 00:03:02.140
the human brain probably implements all three of these learning paradigms

48
00:03:02.141 --> 00:03:02.974
together.

49
00:03:02.980 --> 00:03:07.810
The Neo cortex could be similar to a stack of auto encoders learning low level

50
00:03:07.811 --> 00:03:12.811
perceptual feature detectors close to the sensory input with no correction

51
00:03:12.851 --> 00:03:13.684
signals.

52
00:03:13.690 --> 00:03:18.690
These networks possibly get fine tuned by top down feedback in a supervised

53
00:03:18.850 --> 00:03:23.850
manner using immediate and reliable correction signals and the supervision and

54
00:03:23.981 --> 00:03:28.780
behavioral control mechanisms could be largely learned via reinforcement

55
00:03:28.781 --> 00:03:33.160
learning with plenty of different rewards signals,
but who knows,
right?

56
00:03:33.340 --> 00:03:37.300
The only way to unlock the secrets of intelligence are to do some

57
00:03:37.301 --> 00:03:42.301
experimentation because reinforcement learning revolves around using rewards in

58
00:03:42.581 --> 00:03:46.990
some kind of environment instead of using a premade dataset.

59
00:03:47.200 --> 00:03:50.590
Robotics has been a testbed for RL research for decades.

60
00:03:50.620 --> 00:03:54.340
There have been several successes in getting our l agents to learn to play

61
00:03:54.341 --> 00:03:55.030
sports,

62
00:03:55.030 --> 00:04:00.030
navigate a helicopter autonomously getting robots to Wa and getting them to fold

63
00:04:00.251 --> 00:04:00.850
laundry.

64
00:04:00.850 --> 00:04:05.850
We've created some seriously capable robots that are theoretically able to do

65
00:04:05.980 --> 00:04:08.200
any task a healthy human could do.

66
00:04:08.230 --> 00:04:11.740
But the reason they're still so limited is because of software.

67
00:04:11.770 --> 00:04:15.040
Robotics is a software problem,
not a hardware problem.

68
00:04:15.280 --> 00:04:17.170
In parallel to the robotics world,

69
00:04:17.410 --> 00:04:22.150
game environments have also been a test bed for our l since they are safer than

70
00:04:22.151 --> 00:04:23.080
the real world.

71
00:04:23.140 --> 00:04:27.490
And the barrier to entry is just having a laptop so anyone can test out their

72
00:04:27.491 --> 00:04:31.900
algorithms to of the most popular AI research institutions in the world,

73
00:04:32.050 --> 00:04:37.050
open AI and deep mind extensively use game environments to train and test their

74
00:04:37.391 --> 00:04:41.680
algorithms and their worldclass algorithms like Alpha go zero.

75
00:04:41.740 --> 00:04:46.740
And the Dota two bought both of which be world class players for the first time

76
00:04:46.900 --> 00:04:49.660
ever heavily used reinforcement learning.

77
00:04:49.690 --> 00:04:52.480
So let's say we have some game environment,

78
00:04:52.600 --> 00:04:57.600
the simple game of tic tac toe where the goal is to be the first to successfully

79
00:04:57.671 --> 00:05:02.230
create three in a row.
And we have our AI,
which we'll call an agent.

80
00:05:02.260 --> 00:05:06.730
Our goal is to have this AI learn how to become really good at playing tic tac

81
00:05:06.731 --> 00:05:11.290
toe against humans rather than just hard coding in a bunch of if then
statements,

82
00:05:11.320 --> 00:05:13.390
how can we formalize this problem?

83
00:05:13.420 --> 00:05:17.980
That's where the mathematics of Algebra and probability theory come in.

84
00:05:18.010 --> 00:05:21.550
We can use the variable s to define a set of game states.

85
00:05:21.580 --> 00:05:26.230
These are all the possible configurations the board can be in at any given time

86
00:05:26.231 --> 00:05:29.170
step during the game.
We also have a,

87
00:05:29.171 --> 00:05:32.560
which is a set of actions that our agent can take.
In this case,

88
00:05:32.590 --> 00:05:36.790
it would be the position on the board they'd like to place their Exxon p

89
00:05:36.791 --> 00:05:41.791
represents the probability that a given action a in a given state s will lead to

90
00:05:42.041 --> 00:05:42.874
another state.

91
00:05:43.120 --> 00:05:48.120
It's a measure of how likely a board state will be a certain way after the agent

92
00:05:48.400 --> 00:05:51.880
plays a certain move or represents a reward value.

93
00:05:51.910 --> 00:05:56.470
It's what the agent gets after the game transitions from one state to another

94
00:05:56.471 --> 00:05:58.980
after a an action.
This is the time delayed reward.

95
00:05:59.090 --> 00:06:03.080
We were talking about the signal that will tell the agent that the action it's

96
00:06:03.081 --> 00:06:04.850
taken is either good or bad,

97
00:06:05.090 --> 00:06:09.170
which it can then use to further improve itself.
Lastly,

98
00:06:09.171 --> 00:06:13.400
we'll have a discount factor which represents the difference in importance

99
00:06:13.550 --> 00:06:16.310
between future awards and present rewards.

100
00:06:16.340 --> 00:06:20.390
We can call this formalization a mark Haub decision process.

101
00:06:20.420 --> 00:06:24.620
It's a way of framing a problem where at each time step an agent is in some

102
00:06:24.621 --> 00:06:26.510
state and may choose some action.

103
00:06:26.540 --> 00:06:31.310
This will probabilistically transitioned the agent into the next state and give

104
00:06:31.311 --> 00:06:34.910
some reward as a function of the transition.
Crucially,

105
00:06:34.911 --> 00:06:37.550
the transition obeys the Markov property,

106
00:06:37.610 --> 00:06:42.610
meaning the transition and reward probabilities are only dependent on the pair s

107
00:06:43.130 --> 00:06:47.420
and a and not dependent on the entire history of previous states.

108
00:06:47.450 --> 00:06:48.201
In other words,

109
00:06:48.201 --> 00:06:53.120
the state s should encode all of the important information to be able to make

110
00:06:53.121 --> 00:06:55.550
good decisions on which action to take.

111
00:06:57.050 --> 00:07:01.820
So reinforcement learning is a method of solving this kind of process and we can

112
00:07:01.821 --> 00:07:06.080
break down our El methods that do this into two groups.
Policy,

113
00:07:06.081 --> 00:07:09.950
iteration and value.
Iteration methods,
policy methods,

114
00:07:09.951 --> 00:07:12.140
perform a search in a policy space.

115
00:07:12.410 --> 00:07:15.590
And value methods tried to estimate the value function.

116
00:07:15.680 --> 00:07:20.480
The value function is a function that tries to estimate the longterm utility of

117
00:07:20.481 --> 00:07:25.481
either a state or a state action pair and the agent just selects the Arg Max

118
00:07:26.000 --> 00:07:27.530
action over this function.

119
00:07:27.560 --> 00:07:31.670
It represents how good is a state for an agent to be in.

120
00:07:31.870 --> 00:07:36.870
It is equal to the expected total reward for an agent starting from state s.

121
00:07:37.880 --> 00:07:42.880
The value function depends on the policy by which the agent picks actions to

122
00:07:43.671 --> 00:07:46.700
perform.
Among all possible value functions.

123
00:07:46.880 --> 00:07:51.230
There exists an optimal value function that has higher value than other

124
00:07:51.231 --> 00:07:53.030
functions for all states.

125
00:07:53.240 --> 00:07:58.220
The optimal policy is the policy that corresponds to the optimal value function.

126
00:07:58.250 --> 00:08:02.900
Value function algorithms in reinforcement learning generally follow the current

127
00:08:02.901 --> 00:08:07.490
policy as defined by the current value function and whenever a reward is

128
00:08:07.491 --> 00:08:11.090
received,
the agent propagates it back through its history,

129
00:08:11.150 --> 00:08:14.630
assigning each action some of the reward over time.

130
00:08:14.690 --> 00:08:19.340
This gives the agent a good idea of what states and actions result in good

131
00:08:19.341 --> 00:08:20.150
rewards.

132
00:08:20.150 --> 00:08:25.150
So value iteration computes the optimal state value function by iteratively

133
00:08:25.521 --> 00:08:30.521
improving the estimate of the of s both value iteration and policy iteration

134
00:08:31.310 --> 00:08:36.310
algorithms assume the MDP model is known by the agent comparing to each other.

135
00:08:38.900 --> 00:08:43.900
Policy iteration is computationally efficient as it often takes considerably

136
00:08:44.480 --> 00:08:46.850
fewer number of iterations to converge,

137
00:08:47.000 --> 00:08:51.170
although each iteration is more computationally expensive.

138
00:08:51.680 --> 00:08:55.800
So there are three important points to remember from video.

139
00:08:56.100 --> 00:09:00.870
Reinforcement learning is a technique that lets ais learn to complete an

140
00:09:00.871 --> 00:09:05.220
objective in an environment using time delayed labels,

141
00:09:05.400 --> 00:09:07.320
Aka rewards as a signal.

142
00:09:07.770 --> 00:09:12.770
We can formally call this a mark Haub decision process which relates states'

143
00:09:13.740 --> 00:09:16.470
actions and rewards for an agent.

144
00:09:16.740 --> 00:09:21.740
And two fundamental ways of solving MDPs is by either using a value iteration or

145
00:09:22.741 --> 00:09:24.540
policy iteration algorithm.

146
00:09:25.440 --> 00:09:30.420
This week's coding challenge is to create a simple value iteration algorithm for

147
00:09:30.421 --> 00:09:33.660
an AI using the open AI Jim Environment.

148
00:09:34.020 --> 00:09:36.930
I'll announce the top two submissions next week,

149
00:09:36.990 --> 00:09:40.980
so make sure to post your get hub links in the comment section and I'll review

150
00:09:40.981 --> 00:09:41.820
them personally.

151
00:09:42.360 --> 00:09:46.140
Please subscribe for more programming videos and for now I've got to go hack my

152
00:09:46.141 --> 00:09:48.840
Nintendo switch.
So thanks for watching.

