WEBVTT

1
00:00:00.090 --> 00:00:03.420
Hello world,
it's Saroj and activation functions.

2
00:00:03.480 --> 00:00:07.710
Which one should you use for your neural network?
There's so many out there.

3
00:00:07.920 --> 00:00:11.340
Artificial neural nets are roughly based on our brains neural net,

4
00:00:11.580 --> 00:00:14.550
not in the way that they're made up of a biochemical soup,

5
00:00:14.760 --> 00:00:19.760
but in the way that multiple nodes or neurons are interconnected and signals can

6
00:00:20.251 --> 00:00:21.690
pass through these notes.

7
00:00:21.960 --> 00:00:25.530
It's this hierarchical structure that gives us such amazing results.

8
00:00:25.890 --> 00:00:29.730
Our universe itself can be considered to have a hierarchical structure.

9
00:00:29.940 --> 00:00:34.290
Elementary particles form Adam's,
which form molecules,
which form cells,

10
00:00:34.291 --> 00:00:38.520
then organisms,
planets,
solar systems,
and entire galaxies.

11
00:00:38.880 --> 00:00:41.580
To make sense of this hierarchical complexity,

12
00:00:41.760 --> 00:00:46.140
evolution has settled on a specific kind of structure for our brains that can

13
00:00:46.141 --> 00:00:49.740
represent all these layers of abstraction in an ordered way.

14
00:00:49.890 --> 00:00:53.580
And so when we model this rule of hierarchical layers on computers,

15
00:00:53.790 --> 00:00:58.110
we get very similar results.
Even though the substrate is different,
it's silicon.

16
00:00:58.320 --> 00:01:03.030
So it's incredibly exciting to study the mathematical relations of deep neural

17
00:01:03.031 --> 00:01:05.970
networks and ways of improving them.
Because by doing so,

18
00:01:05.971 --> 00:01:10.380
we're not just coming closer to better results for specific tasks or coming

19
00:01:10.381 --> 00:01:14.850
closer to discovering fundamental laws embedded in our universe.

20
00:01:14.970 --> 00:01:19.890
The basic idea behind how a neural network learns is that we first have some

21
00:01:19.920 --> 00:01:23.880
input data that we've got to vectorize.
Then we feed it into the network,

22
00:01:23.910 --> 00:01:28.800
which means we basically perform a series of matrix operations on this input

23
00:01:28.801 --> 00:01:32.010
data layer by layer,
and the simple case for each layer,

24
00:01:32.011 --> 00:01:35.250
we just multiply the input by the weights at a bias,

25
00:01:35.340 --> 00:01:39.330
apply an activation function to the result and pass the output onto the next

26
00:01:39.331 --> 00:01:40.650
layer to do the same thing.

27
00:01:40.830 --> 00:01:44.220
And we keep repeating that process until we reached the last layer.

28
00:01:44.370 --> 00:01:46.440
The final output value is our prediction.

29
00:01:46.500 --> 00:01:50.310
We find a difference between it and the expected output,
which is the label.

30
00:01:50.460 --> 00:01:54.420
Then use that error value to compute the partial derivative with respect to the

31
00:01:54.421 --> 00:01:58.980
weights in each layer going backwards recursively we then update the weights

32
00:01:58.981 --> 00:02:03.540
with these values and repeat the process until the error is as small as

33
00:02:03.541 --> 00:02:07.560
possible.
And that's leaped dearness deep learning who the,

34
00:02:07.690 --> 00:02:11.790
so you might be asking,
why do we apply these activation functions to our data?

35
00:02:11.940 --> 00:02:15.090
Couldn't we just multiply the input by the weight values,

36
00:02:15.150 --> 00:02:19.410
add a bias and propagate that result forward?
Well,

37
00:02:19.411 --> 00:02:21.330
they do something really important.

38
00:02:21.450 --> 00:02:24.570
They introduce nonlinear properties to our network,

39
00:02:24.690 --> 00:02:28.110
so we can call them nonlinearities.
But why is this a good thing?

40
00:02:28.560 --> 00:02:32.850
Let's take a step back.
And linear function is a polynomial of just one degree,

41
00:02:32.880 --> 00:02:34.950
like y equals two x or y equals x.

42
00:02:35.160 --> 00:02:37.320
So if we were to map these functions on a graph,

43
00:02:37.470 --> 00:02:41.130
they would always form a straight line.
If we added more dimensions,

44
00:02:41.131 --> 00:02:43.860
they would form a plane or a hyperplane,

45
00:02:43.861 --> 00:02:48.030
but their shape would always be perfectly straight with no curves of any kind.

46
00:02:48.240 --> 00:02:51.840
That's what we call them linear.
But every other equation is nonlinear.

47
00:02:51.870 --> 00:02:55.260
Polynomials have higher degrees like y equals two x squared.

48
00:02:55.350 --> 00:02:57.570
Trig functions like sign or co-sign.

49
00:02:57.750 --> 00:03:02.110
Nonlinear functions when produce a line that always has some kind of curvature.

50
00:03:02.380 --> 00:03:06.310
Linear equations are easy to solve,
but they are limited in their complexity.

51
00:03:06.340 --> 00:03:11.050
We want to be able to represent any kind of function with our neural network and

52
00:03:11.051 --> 00:03:14.950
neural networks are considered universal function approximators.

53
00:03:15.220 --> 00:03:18.160
That means that they can compute any function at all.

54
00:03:18.250 --> 00:03:22.870
Almost any process you can imagine can be thought of as a function computation,

55
00:03:23.140 --> 00:03:28.060
trying to name the song that you're hearing,
translating Spanish to English,

56
00:03:28.300 --> 00:03:30.250
punching an evil clone of yourself,

57
00:03:30.460 --> 00:03:34.150
so we need a way to compute not just linear functions,

58
00:03:34.300 --> 00:03:39.010
but nonlinear ones as well.
If we didn't use a nonlinear activation function,

59
00:03:39.011 --> 00:03:41.770
then no matter how many layers are neural network has,

60
00:03:41.920 --> 00:03:45.700
it would still behave just like a single layer network because summing these

61
00:03:45.701 --> 00:03:48.640
layers will give us just another linear function.

62
00:03:48.970 --> 00:03:51.850
This is not strong enough to model many kinds of data though,

63
00:03:52.150 --> 00:03:54.550
but by using a nonlinear activation,

64
00:03:54.700 --> 00:03:58.780
the mapping of the input to the output is nonlinear and we want it to be

65
00:03:58.781 --> 00:04:02.620
differentiable.
That means we're able to calculate the derivative of it.

66
00:04:02.950 --> 00:04:06.040
An example of a differential function is x squared.

67
00:04:06.130 --> 00:04:10.270
Since we can differentiate it to two x,
which is it's derivative.

68
00:04:10.300 --> 00:04:14.260
We needed to be this way so we can perform the backpropagation optimization

69
00:04:14.261 --> 00:04:18.520
strategy where we find a nonlinear Eric radiant to learn complex behavior.

70
00:04:18.790 --> 00:04:23.290
The whole idea behind activation functions is to roughly model the way neurons

71
00:04:23.291 --> 00:04:25.780
communicate in the brain with each other.

72
00:04:25.960 --> 00:04:28.720
Each one is activated through its action potential.

73
00:04:28.870 --> 00:04:32.980
If it reaches a certain threshold,
we know to activate a neuron or not,

74
00:04:33.070 --> 00:04:36.940
the activation function simulates this spike train of the brain's action.

75
00:04:36.941 --> 00:04:41.290
Potential input Tom's weight,
add a bias,

76
00:04:41.800 --> 00:04:45.380
activate input,
Tom's wait,

77
00:04:46.050 --> 00:04:48.040
ad buys activate,

78
00:04:48.190 --> 00:04:51.190
so we could think of a lot of possible activation functions,

79
00:04:51.191 --> 00:04:53.980
but how do we know which one to use?

80
00:04:54.220 --> 00:04:56.380
This choice is dependent on a couple factors,

81
00:04:56.560 --> 00:04:58.420
not including if it just sounds cool.

82
00:04:58.720 --> 00:05:00.730
Let's talk about the three most popular ones.

83
00:05:00.820 --> 00:05:05.820
Sigmoid 10 h and relative sigmoid has the mathematical form of f of x equals one

84
00:05:06.250 --> 00:05:08.320
over one plus e to the negative x.

85
00:05:08.470 --> 00:05:12.580
It takes some number and squashes it into a range between zero and one.

86
00:05:12.820 --> 00:05:16.510
It was one of the first to be used because it could be interpreted as the firing

87
00:05:16.511 --> 00:05:21.460
rate of a neuron where zero means no firing and one means a fully saturated

88
00:05:21.461 --> 00:05:23.650
firing.
It's pretty easy to understand,

89
00:05:23.651 --> 00:05:27.760
but it has two problems that have made it fall out of popularity recently.

90
00:05:28.060 --> 00:05:30.640
The first is that it causes our gradients advantage.

91
00:05:30.880 --> 00:05:34.390
When a neuron's activation saturates close to either zero or one,

92
00:05:34.540 --> 00:05:38.830
the gradient at these regions is very close to zero during backpropagation,

93
00:05:38.890 --> 00:05:42.610
this local gradient will be multiplied by the gradient of this gates output for

94
00:05:42.611 --> 00:05:46.240
the whole objective.
So if the local gradient is really small,

95
00:05:46.360 --> 00:05:50.710
it will make the gradient slowly vanish and close to no signal will flow through

96
00:05:50.711 --> 00:05:53.590
to the neuron to its weights and recursively to its data.

97
00:05:53.800 --> 00:05:57.100
The second problem is that its output isn't zero centered.

98
00:05:57.620 --> 00:05:59.600
It starts from zero and ends up one.

99
00:05:59.870 --> 00:06:03.380
That means the value after the function will be positive and that makes the

100
00:06:03.381 --> 00:06:07.730
gradient of the weights become either all positive or all negative.

101
00:06:08.000 --> 00:06:11.630
This makes the gradient updates go too far in different directions,

102
00:06:11.810 --> 00:06:15.620
which makes optimization harder.
I can control these gradients.

103
00:06:16.730 --> 00:06:19.220
So how do we improve on it?
Well,

104
00:06:19.280 --> 00:06:23.780
there's another activation function called the hyperbolic tangent function or 10

105
00:06:23.781 --> 00:06:24.440
h.

106
00:06:24.440 --> 00:06:28.520
It squashes the real number into a range between negative one and one instead of

107
00:06:28.521 --> 00:06:32.960
zero and one.
So it's output is zero centered,
which makes optimization easier.

108
00:06:33.020 --> 00:06:37.160
So in practice it's always preferred to the sigmoid,
but just like the sigmoid,

109
00:06:37.161 --> 00:06:42.161
it also suffers from the vanishing gradient problem enter Relo or the rectified

110
00:06:42.261 --> 00:06:43.094
linear unit.

111
00:06:43.130 --> 00:06:46.730
This activation function has become really popular in the last few years.

112
00:06:46.970 --> 00:06:48.680
It's just an ax zero x,

113
00:06:48.740 --> 00:06:52.460
which means that the value is zero when x is less than zero and linear with a

114
00:06:52.461 --> 00:06:55.040
slope of one when x is greater than zero.

115
00:06:55.220 --> 00:07:00.220
It was noted that it had a six x improvement in convergence over 10 h and the

116
00:07:00.381 --> 00:07:03.650
landmark image net classification paper by Kurczewski.

117
00:07:03.920 --> 00:07:06.620
A lot of times in computer science we find that the simplest,

118
00:07:06.740 --> 00:07:11.060
most elegant solution is the best,
and this applies to revenue as well.

119
00:07:11.270 --> 00:07:14.810
It doesn't involve expensive operations like Tan h or sigmoid,

120
00:07:14.900 --> 00:07:18.740
so it learns faster and it avoids the vanishing gradient problem.

121
00:07:19.040 --> 00:07:21.530
Almost all deep networks use relevant nowadays,

122
00:07:21.560 --> 00:07:23.570
but it's only used for the hidden layers.

123
00:07:23.810 --> 00:07:28.010
The output layer should use a softmax function for classification does it gives

124
00:07:28.160 --> 00:07:32.750
probabilities for different classes and a linear function for regression since a

125
00:07:32.751 --> 00:07:34.670
signal goes through unchanged.

126
00:07:34.730 --> 00:07:38.420
One problem that Relo has sometimes though is that some units can be fragile

127
00:07:38.421 --> 00:07:40.160
during training and die,

128
00:07:40.460 --> 00:07:44.960
meaning a big gradient flowing through a relo neuron could cause a weight update

129
00:07:45.050 --> 00:07:47.840
that makes it never activate on any data point again,

130
00:07:48.020 --> 00:07:51.770
so then gradients flowing through it will always be zero from that point on.

131
00:07:51.980 --> 00:07:55.670
So a variant was introduced called the leaky relative to fix this problem

132
00:07:55.940 --> 00:07:59.900
instead of the function being zero when x is less than zero and instead has a

133
00:07:59.901 --> 00:08:04.460
small negative slope.
There's also another popular variant called Max Salary,

134
00:08:04.490 --> 00:08:08.000
which is a generalized form of both Relo and leaky Relo,

135
00:08:08.240 --> 00:08:12.000
but it doubles the number of parameters for each neuron.
So there's a trade off.

136
00:08:12.560 --> 00:08:16.880
The original question was what type of activation function should you use in a

137
00:08:16.881 --> 00:08:20.480
neural network?
And the answer is rare,
rarely revenue,
revenue,
revenue.

138
00:08:20.690 --> 00:08:22.340
But if a lot of your neurons die,

139
00:08:22.341 --> 00:08:26.810
then try a variant like the leaky relu or Maxell sigmoid just shouldn't be used

140
00:08:26.811 --> 00:08:28.430
anymore,
nor should tan h.

141
00:08:28.550 --> 00:08:31.040
And although relish should be applied to the hidden layers,

142
00:08:31.250 --> 00:08:35.450
the output layers should use a soft max work classification or a linear function

143
00:08:35.451 --> 00:08:36.380
for regression.

144
00:08:36.650 --> 00:08:40.310
There are other activation functions out there and there's still a lot of room

145
00:08:40.311 --> 00:08:44.570
for improvement in this area.
They are a crucial part of neural networks.

146
00:08:44.600 --> 00:08:49.340
So any new discovery here,
we'll have huge impacts in the field moving forward.

147
00:08:49.370 --> 00:08:52.190
If you liked this video,
hit the subscribe button for more like it.

148
00:08:52.400 --> 00:08:57.070
Check out this related video and for now I've got to take a leak.
Irrelevant.

149
00:08:57.350 --> 00:08:58.520
So thanks for watching.

