WEBVTT

1
00:00:00.420 --> 00:00:04.860
When you use AI,
a crash is just an opportunity.
Hello world.

2
00:00:04.890 --> 00:00:09.890
It's a Raj and can we predict the price of Google stock based on a Dataset of

3
00:00:10.201 --> 00:00:11.130
price history?

4
00:00:11.490 --> 00:00:16.490
I'll answer that question by building a python demo that uses an underutilized

5
00:00:16.561 --> 00:00:20.250
technique and financial market prediction,
reinforcement learning,

6
00:00:20.670 --> 00:00:25.410
but before we look at this problem from a reinforcement learning perspective,

7
00:00:25.710 --> 00:00:30.570
let's talk about the more popular algorithmic trading approach involving

8
00:00:30.600 --> 00:00:33.330
supervised learning so we can compare the two.

9
00:00:33.960 --> 00:00:36.690
If we can predict that the market will go up,

10
00:00:36.750 --> 00:00:41.750
we can buy right now and sell once it's gone up or if we can predict if it will

11
00:00:41.821 --> 00:00:45.690
go down,
we can short then by once the market goes down.

12
00:00:46.080 --> 00:00:49.740
But the problem is what price are we actually predicting?

13
00:00:50.160 --> 00:00:55.160
There's no single price we are buying at the final price we pay depends on the

14
00:00:55.351 --> 00:00:59.640
volume that's available at different levels of the order book including the fees

15
00:00:59.641 --> 00:01:03.990
will need to pay.
A naive approach would be to predict the mid price,

16
00:01:04.170 --> 00:01:08.010
which is the mid point between the best ask and the best bid.

17
00:01:08.280 --> 00:01:11.610
Most researchers do this,
but it's just a theoretical price,

18
00:01:11.611 --> 00:01:14.370
not something we can execute trade orders at,

19
00:01:14.670 --> 00:01:18.660
which means it could differ significantly from the real price we're paying.

20
00:01:19.050 --> 00:01:24.050
We also have to pay a transaction fee for whatever platform we use and because

21
00:01:24.121 --> 00:01:25.980
the market moves so fast,

22
00:01:26.220 --> 00:01:29.250
by the time the order is delivered over the network,

23
00:01:29.520 --> 00:01:31.170
that price could have slipped away.

24
00:01:31.350 --> 00:01:35.280
Supervised models don't take into account network latencies,

25
00:01:35.580 --> 00:01:39.420
these or the amount of liquidity in the best order book levels.

26
00:01:39.690 --> 00:01:43.950
The takeaway here is that in order to make money from a simple price prediction

27
00:01:43.951 --> 00:01:44.784
strategy,

28
00:01:44.970 --> 00:01:49.970
we've got to predict large price movements over longer periods of time or be

29
00:01:50.581 --> 00:01:55.530
smart about our fees and order management.
That's a nontrivial problem.

30
00:01:55.590 --> 00:01:58.710
There's also no policy involved in supervised learning.

31
00:01:58.740 --> 00:02:01.920
We predicted the price would move up and then it did luckily,

32
00:02:02.310 --> 00:02:03.720
but what if the price went down?

33
00:02:03.960 --> 00:02:07.830
What would we have done then sold huddled like a beast.

34
00:02:08.100 --> 00:02:09.960
What if the price went up?
Then down,

35
00:02:10.020 --> 00:02:13.770
how do we choose the threshold of certainty to place an order?

36
00:02:14.070 --> 00:02:17.040
We need more than just an price prediction model.

37
00:02:17.190 --> 00:02:19.350
We also need a rule based policy.

38
00:02:19.740 --> 00:02:22.920
This policy will take as input our price prediction,

39
00:02:23.130 --> 00:02:27.320
then output a decision as to what to do with it as in place,
in order,

40
00:02:27.330 --> 00:02:29.340
do nothing,
dance,
et cetera.

41
00:02:29.700 --> 00:02:34.170
How are we supposed to come up with the policy and how do we optimize the policy

42
00:02:34.171 --> 00:02:36.720
parameters and decision thresholds?

43
00:02:37.050 --> 00:02:40.980
Heuristics and human intuition can only get us so far.

44
00:02:41.280 --> 00:02:44.790
A solution to our problem is to use reinforcement learning.

45
00:02:45.240 --> 00:02:49.200
We can formulate the reinforcement learning problem in the following way.

46
00:02:49.560 --> 00:02:54.560
We have an AI or agent that takes actions inside of an environment.

47
00:02:54.960 --> 00:02:56.250
At each time step,

48
00:02:56.251 --> 00:03:00.490
the agent will receive the input of the current state and take an action,

49
00:03:00.520 --> 00:03:03.520
then receive a reward as well as the next state.

50
00:03:03.790 --> 00:03:07.570
The agent will choose which action to take based on some policy.

51
00:03:07.840 --> 00:03:12.840
Our goal is to find a policy that maximizes the cumulative reward over some time

52
00:03:13.211 --> 00:03:17.560
horizon.
In the context of our problem,
the agent is the trader.

53
00:03:17.740 --> 00:03:21.160
It will decide how and when to make trades on the market.

54
00:03:21.580 --> 00:03:26.230
The environment would be the exchange that we use and that includes the other

55
00:03:26.231 --> 00:03:26.651
agents,

56
00:03:26.651 --> 00:03:31.630
both humanlike like Jordan Belfort and algorithms making trades that are a part

57
00:03:31.631 --> 00:03:34.510
of that exchange.
In terms of the state of the environment,

58
00:03:34.511 --> 00:03:39.100
we can't observe the complete state since we don't know the details of the other

59
00:03:39.101 --> 00:03:43.060
agents in the environment like their account balances or what orders they're

60
00:03:43.061 --> 00:03:47.650
waiting on.
We have what we observe is not the actual state of the environment,

61
00:03:47.651 --> 00:03:50.710
but instead a derivation of it for us.

62
00:03:50.711 --> 00:03:55.690
That observation at each time step is a historical log of all exchange events

63
00:03:55.840 --> 00:04:00.070
received up until that time.
In order for our agent to make decisions,

64
00:04:00.220 --> 00:04:03.910
we'll include a few other features like the current account balance,

65
00:04:04.270 --> 00:04:07.630
the timescale with which we make trades will matter.

66
00:04:07.780 --> 00:04:11.470
Should we do this over a period of days.
How about nanoseconds?

67
00:04:11.710 --> 00:04:16.150
High frequency trading bots make decisions at nanosecond timescales.

68
00:04:16.270 --> 00:04:21.270
Neural networks are popular because they can learn complex functions if given

69
00:04:21.431 --> 00:04:25.810
lots of data and compute,
but they're also relatively slow.

70
00:04:26.050 --> 00:04:29.170
They can't make predictions on nanosecond time scales,

71
00:04:29.200 --> 00:04:32.110
so they can't compete with Hoft Algorithms.

72
00:04:32.350 --> 00:04:37.350
So we want to act on a timescale where we can analyze data faster than any human

73
00:04:37.361 --> 00:04:42.340
possibly could,
but we're being smarter allows us to beat the fast,

74
00:04:42.341 --> 00:04:46.180
but simple Hoft algorithms,
it's a tradeoff.

75
00:04:46.630 --> 00:04:47.980
Reinforcement learning,

76
00:04:48.010 --> 00:04:52.090
trading strategies in general are simpler than the supervise.
The approach.

77
00:04:52.360 --> 00:04:56.950
Instead of hand coding or rule based policy or El,
we'll learn a policy,

78
00:04:57.070 --> 00:05:00.790
no need for us to specify rules and thresholds like sell.

79
00:05:00.791 --> 00:05:05.200
When you are 80% sure of the market will move down then initiate skynet.

80
00:05:05.650 --> 00:05:07.270
It's all a part of the policy.

81
00:05:07.271 --> 00:05:10.270
It learns to optimize for the metric we care about.

82
00:05:10.630 --> 00:05:15.220
And this policy can be parameterized by a deep neural network making it more

83
00:05:15.221 --> 00:05:18.550
powerful than any rules a human trader could come up with.

84
00:05:19.300 --> 00:05:22.810
Because our l agents are trained in a simulation,

85
00:05:22.840 --> 00:05:27.670
we can take into account environmental factors by simulating them as well.

86
00:05:27.910 --> 00:05:32.910
We can simulate latency and give the agent a negative reward if it causes it to

87
00:05:33.131 --> 00:05:36.310
make a mistake.
So let's get to the problem at hand.

88
00:05:36.430 --> 00:05:40.690
We want to develop an AI that will predict the price of Google stock so it can

89
00:05:40.691 --> 00:05:43.360
buy at a low price and sell at a high price,

90
00:05:43.930 --> 00:05:46.300
but this type of prediction isn't easy.

91
00:05:46.300 --> 00:05:50.230
It depends on several parameters like the available stocks,

92
00:05:50.380 --> 00:05:52.780
recent prices and the available budget.

93
00:05:53.170 --> 00:05:57.920
The states in our situation would be a vector that has information about those

94
00:05:57.921 --> 00:06:01.880
features.
Let's say we want the 200 last stock prices.

95
00:06:02.150 --> 00:06:06.950
Our state would then be a 202 dimensional vector and in terms of actions are

96
00:06:06.951 --> 00:06:11.480
agent can either buy,
sell or hold.
We've got our states and action,

97
00:06:11.481 --> 00:06:13.220
but we also need a policy.

98
00:06:13.580 --> 00:06:16.850
A policy for our AI consist of three rules.

99
00:06:17.060 --> 00:06:21.050
Buying a stock at the current price decreases the budget.
Selling of stock,

100
00:06:21.051 --> 00:06:25.010
trades it in for money at the current price and holding does neither of these

101
00:06:25.011 --> 00:06:27.110
things.
It yields no reward.

102
00:06:27.410 --> 00:06:32.410
The goal for our AI is to learn a policy that gains the maximum net worth from

103
00:06:32.691 --> 00:06:37.040
trading in the stock market.
Let's try a technique called Q learning.

104
00:06:37.041 --> 00:06:41.720
So our AI will need to select an action and improve its utility.

105
00:06:41.720 --> 00:06:45.290
Function to learning is a table of values for every state,

106
00:06:45.320 --> 00:06:49.610
the Rose and action,
the columns possible in the environment.

107
00:06:49.850 --> 00:06:51.770
Inside of each cell of the table,

108
00:06:51.771 --> 00:06:56.771
we learn to value for how beneficial it is to take a given action within a given

109
00:06:56.841 --> 00:07:00.860
state.
These are cue or a quagmire values.
Just kidding.

110
00:07:00.890 --> 00:07:05.270
Q stands for quality.
We'll start by initializing the table to be all zeros.

111
00:07:05.330 --> 00:07:09.200
Then as we observe the rewards we obtain for various actions,

112
00:07:09.350 --> 00:07:10.850
we can update it accordingly.

113
00:07:11.150 --> 00:07:15.380
The way we make updates to the cute table is by using the bellman equation.

114
00:07:15.680 --> 00:07:20.680
The states that the expected long term reward for a given action is equal to the

115
00:07:20.691 --> 00:07:25.520
immediate reward from the current action combined with the expected reward from

116
00:07:25.521 --> 00:07:28.700
the best future action taken at the following state.

117
00:07:28.970 --> 00:07:33.950
So we're reusing our own Q table when estimating how do update our table for

118
00:07:33.951 --> 00:07:36.410
future actions.
More formally,

119
00:07:36.500 --> 00:07:41.270
the equation states that the Q value for a given state and action should

120
00:07:41.271 --> 00:07:45.140
represent the current reward plus the maximum discounted future.

121
00:07:45.141 --> 00:07:50.090
Reward expected according to our own table for the next state we could end up

122
00:07:50.120 --> 00:07:53.300
in,
so given a state the decision policy,

123
00:07:53.450 --> 00:07:57.800
we'll calculate the next action to take and improve the Q function from the new

124
00:07:57.801 --> 00:07:59.900
experience of taking an action.

125
00:08:00.500 --> 00:08:04.700
Let's start implementing the decision policy based on which action the AI will

126
00:08:04.701 --> 00:08:08.960
take for buying,
selling,
or holding a stock.
To start with,

127
00:08:08.961 --> 00:08:13.070
we'll create a random decision policy and evaluate its performance.

128
00:08:13.310 --> 00:08:17.660
We can do this in the general decision policy abstract class.

129
00:08:17.900 --> 00:08:22.370
We can then inherit from this superclass to implement a random decision policy.

130
00:08:22.820 --> 00:08:26.870
All it does is pick an action randomly without even looking at the state.

131
00:08:27.200 --> 00:08:30.410
We can definitely do better than this neural network based cue.

132
00:08:30.410 --> 00:08:35.030
Learning has proven to be a good strategy for learning a decision policy.

133
00:08:35.510 --> 00:08:39.890
We can set the hyper parameters from the Q function in the constructor as well

134
00:08:39.891 --> 00:08:42.860
as the number of hidden nodes in the neural networks.

135
00:08:43.130 --> 00:08:48.130
This helps define the input and output tensors and the structure of the network.

136
00:08:48.590 --> 00:08:52.040
It will also define the operations to compute the utility.

137
00:08:52.130 --> 00:08:54.770
Of course our optimizer will be great and descent,

138
00:08:54.800 --> 00:08:59.280
which the most popular in this space,
the results don't look too bad,

139
00:08:59.281 --> 00:09:03.150
and there's a lot of potential to improve this model using reinforcement

140
00:09:03.151 --> 00:09:06.660
learning techniques like Sarsat or policy gradients.

141
00:09:07.260 --> 00:09:12.210
Applying our l two financial predictions is an exciting and largely untapped

142
00:09:12.240 --> 00:09:13.440
area of research.

143
00:09:13.680 --> 00:09:16.680
And I've got to link to all the code for you in the video description,

144
00:09:16.681 --> 00:09:20.250
so check it out.
You made it to the end,
but it's really just the beginning.

145
00:09:20.580 --> 00:09:24.690
It subscribe and let me help you on your journey to be the best for now,

146
00:09:24.720 --> 00:09:27.570
I've got to learn a Q function,
so thanks for watching.

