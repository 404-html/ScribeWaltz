WEBVTT

1
00:00:04.000 --> 00:00:05.890
Hello world.
Welcome to Suraj Ology.

2
00:00:06.070 --> 00:00:09.070
Today we're going to be building sentiment analysis in four minutes.

3
00:00:09.190 --> 00:00:10.300
Let's get started.

4
00:00:11.110 --> 00:00:14.650
Sentiment analysis is the process of determining the opinion or feeling of a

5
00:00:14.651 --> 00:00:17.020
piece of text.
We humans are pretty good at this.

6
00:00:17.200 --> 00:00:19.960
I can look at this tweet and immediately know that it's negative.

7
00:00:20.110 --> 00:00:23.560
It feels like the writer sentiment is one of anger and disgust due to the

8
00:00:23.561 --> 00:00:24.460
negative wording.

9
00:00:24.670 --> 00:00:27.550
Companies across the world have implemented machine learning to do this

10
00:00:27.551 --> 00:00:31.510
automatically.
It's super useful for gaining insight into customer opinions.

11
00:00:31.780 --> 00:00:34.870
Once you understand how the customer feels after analyzing their comments or

12
00:00:34.871 --> 00:00:35.620
reviews,

13
00:00:35.620 --> 00:00:39.550
you can identify what they like and dislike and built things like recommendation

14
00:00:39.551 --> 00:00:43.750
systems.
One more targeted marketing campaigns for them.
In this demo,

15
00:00:43.870 --> 00:00:47.230
we're going to be building a sentiment analysis program in python that will

16
00:00:47.231 --> 00:00:51.400
identify whether a movie review is positive or negative based on the text.

17
00:00:51.430 --> 00:00:54.100
In the review,
we'll get our training and testing data,

18
00:00:54.190 --> 00:00:56.890
a bunch of labeled reviews from a site called Kaggle.

19
00:00:57.220 --> 00:01:00.490
We'll start off by importing our dependencies will import the operating system

20
00:01:00.491 --> 00:01:02.980
module to help us perform command line functions.

21
00:01:03.250 --> 00:01:05.320
Then we'll want to import the psychic learn module,

22
00:01:05.410 --> 00:01:08.890
which is a machine learning library in python with a fast learning curve.

23
00:01:09.250 --> 00:01:11.980
Then we'll import a helper class that will help us clean our data.

24
00:01:12.350 --> 00:01:14.830
Pandas helps us read our data,
CSV files,

25
00:01:14.950 --> 00:01:19.230
and NLTK will be used to remove unnecessary words from our dataset.

26
00:01:19.570 --> 00:01:20.080
All right,

27
00:01:20.080 --> 00:01:24.160
so step one is to just read the data from our heart desk will import the label

28
00:01:24.161 --> 00:01:25.870
training data and the testing data.

29
00:01:26.080 --> 00:01:29.170
Then we'll print out that were first review to the command line to ensure we

30
00:01:29.171 --> 00:01:33.460
read the data set correctly.
Once we've read in our data.
Step two is to clean it.

31
00:01:33.610 --> 00:01:38.140
That means ensure that we remove all the html non letters and stop words.

32
00:01:38.290 --> 00:01:40.480
Stop words are words that are insignificant.

33
00:01:40.630 --> 00:01:45.040
We can download them from the Nlt,K or natural language toolkit library words

34
00:01:45.041 --> 00:01:49.330
like the or two or as since it's hard to analyze emotion from them.

35
00:01:49.630 --> 00:01:53.560
We'll iterate over every review in our training dataset and fill our new queen

36
00:01:53.561 --> 00:01:57.160
review array with the cleaned reviews are helpful.
Class,

37
00:01:57.161 --> 00:02:01.180
we'll do the cleaning for us.
Step three is to create a bag of words.

38
00:02:01.420 --> 00:02:05.710
The bag of words model is a simple numeric representation of a piece of text

39
00:02:05.770 --> 00:02:07.200
that is easy to classify.

40
00:02:07.480 --> 00:02:10.960
We just count the frequency of each word in a piece of tax and create a

41
00:02:10.961 --> 00:02:15.070
dictionary of them.
This is called tokenization in natural language processing.

42
00:02:15.340 --> 00:02:18.910
We'll use the count vectorize or object and the psychic learn package to create

43
00:02:18.911 --> 00:02:22.330
it.
We'll set the Max features to 5,000 to keep things simple,

44
00:02:22.480 --> 00:02:26.800
so our bag of words will contain at Max 5,000 words and their associated

45
00:02:26.801 --> 00:02:27.634
frequencies.

46
00:02:27.820 --> 00:02:31.480
Then we'll use a fit transform method to fit a model to the bag of words and

47
00:02:31.481 --> 00:02:35.110
create the feature vectors.
We can then store the feature vectors in an array.

48
00:02:35.320 --> 00:02:37.510
Step four is to create the classifier.

49
00:02:37.750 --> 00:02:41.200
A classifier is a machine learning model that will be used to classify whether a

50
00:02:41.201 --> 00:02:44.260
piece of tax is positive or negative.
In this example,

51
00:02:44.261 --> 00:02:49.180
our classifier is a random forest consisting of 100 trees or random forest is a

52
00:02:49.181 --> 00:02:52.570
set of decision trees.
Decision trees are graphs,
the model,

53
00:02:52.571 --> 00:02:54.490
the possibilities of certain outcomes.

54
00:02:54.640 --> 00:02:59.560
So let's say a piece of tax has a word hate appear more than 20 times the that

55
00:02:59.561 --> 00:03:03.610
it's negative could be something like 80% then based on other word frequencies,

56
00:03:03.790 --> 00:03:07.720
we increased or decreased that probability accordingly until we get to the leaf

57
00:03:07.721 --> 00:03:10.750
of the tree,
which will be a positive or negative rating.

58
00:03:11.140 --> 00:03:14.530
This is different from a standard regression classifier where if a data point is

59
00:03:14.531 --> 00:03:16.570
on a certain side of the line of best fit,

60
00:03:16.900 --> 00:03:20.890
we can easily classify it or random forest tree is more like a series of lines

61
00:03:21.100 --> 00:03:24.040
one for every tree.
That segments are possibilities.

62
00:03:24.250 --> 00:03:28.240
Once we've mapped all the lines onto the graph and we plot a new data point or

63
00:03:28.241 --> 00:03:29.950
review based on its coordinates,

64
00:03:29.951 --> 00:03:33.610
we can then classify based on whether it's in a positive or negative space,

65
00:03:33.880 --> 00:03:36.460
it's time to test our classifier on our testing data.

66
00:03:36.580 --> 00:03:40.090
So let's format the test data by cleaning the reviews and creating a bag of

67
00:03:40.091 --> 00:03:42.850
words.
Once we have our feature vectors for test data,

68
00:03:42.970 --> 00:03:44.410
we can move on to the last step.

69
00:03:44.740 --> 00:03:48.640
The last step is for our program to correctly classify the reviews in the

70
00:03:48.641 --> 00:03:51.160
testing dataset.
As positive or negative.

71
00:03:51.370 --> 00:03:53.870
We'll use our random forest to make a prediction.
Well,

72
00:03:53.871 --> 00:03:57.460
then take the result and write it to a new CSV file.
That's it.

73
00:03:57.580 --> 00:04:01.300
Let's run our program and see what happens.
Okay.
It printed out the first review.

74
00:04:01.301 --> 00:04:03.310
That means it's correctly reading our dataset.

75
00:04:03.670 --> 00:04:06.850
Then it's going to clean and parts of training set.
Create the bag of words,

76
00:04:07.000 --> 00:04:11.320
train the classifier,
then predict the test labels.
Awesome.
Let's test.

77
00:04:11.321 --> 00:04:15.460
The first three predictions or one is positive and zero is negative.
Let's see.

78
00:04:15.461 --> 00:04:19.720
The first three are one zero and one so positive,
negative,
and positive.

79
00:04:19.900 --> 00:04:22.840
Let's skim these.
It's truly a masterpiece.
Positive.

80
00:04:22.930 --> 00:04:26.500
It's so awful that once you know,
okay,
negative,
awesome.

81
00:04:26.660 --> 00:04:29.140
It looks like it's performing sentiment analysis like a charm.

82
00:04:29.380 --> 00:04:32.650
Sentiment analysis is still an evolving field of machine learning.

83
00:04:32.651 --> 00:04:36.850
There's so many grammatical nuances and misspellings and slangs involved in

84
00:04:36.851 --> 00:04:39.700
human language that we haven't really taken into account,

85
00:04:39.880 --> 00:04:41.860
but we can with more powerful algorithms.

86
00:04:42.070 --> 00:04:45.640
So check out the links in the description below.
For more information and please,

87
00:04:45.641 --> 00:04:48.910
please subscribe for more technology videos.
There's so much I want to make.

88
00:04:49.180 --> 00:04:49.810
Thanks for watching.

