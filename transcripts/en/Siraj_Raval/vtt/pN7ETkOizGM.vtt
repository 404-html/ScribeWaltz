WEBVTT

1
00:00:01.280 --> 00:00:03.890
Hello world gets a Raj and Pong.

2
00:00:04.310 --> 00:00:09.310
<v 1>Everybody's played it at least once and let's see if we can create an AI to beat</v>

3
00:00:09.621 --> 00:00:14.570
the game by using a reinforcement learning technique called policy gradients.

4
00:00:15.110 --> 00:00:19.760
When deepmind beat all those Atari Games using the same algorithm a few years

5
00:00:19.760 --> 00:00:24.760
ago or when they created the AI that beat the best go player in the world last

6
00:00:24.800 --> 00:00:28.940
year,
the Ai community was blown away,
but if you think about it,

7
00:00:28.941 --> 00:00:32.840
the algorithms they used warrant novel at all.
In fact,

8
00:00:32.870 --> 00:00:34.700
they'd been around for decades.

9
00:00:34.910 --> 00:00:39.260
The deep Q learner that beat the Atari Games just use a standard Q learning

10
00:00:39.261 --> 00:00:44.261
algorithm with function approximation and you can find an example of that from

11
00:00:44.271 --> 00:00:49.271
the standard RL book by Sutton in 1998 as well as a convolutional net which has

12
00:00:50.541 --> 00:00:55.541
been around since the 1990s alpha go a strategy called a Monte Carlo tree search

13
00:00:56.930 --> 00:01:00.020
as well as something called a policy gradients approach.

14
00:01:00.320 --> 00:01:03.260
All of these are standard,
well known components.

15
00:01:03.620 --> 00:01:08.150
My point is it's not the algorithms that's been the driver of recent progress in

16
00:01:08.151 --> 00:01:11.360
AI,
but the amount of data and computing power.

17
00:01:11.420 --> 00:01:14.180
We've been able to feed these algorithms recently.

18
00:01:14.480 --> 00:01:18.770
So we've talked about a lot of algorithms I've listed here,

19
00:01:19.040 --> 00:01:23.750
but one we haven't talked about is called policy gradients and we'll use it to

20
00:01:23.751 --> 00:01:26.630
help beat the game of Pong.
You might be asking,

21
00:01:26.690 --> 00:01:29.930
shouldn't we use a deep learner like deep minded?
Well,

22
00:01:29.931 --> 00:01:33.350
it turns out that cue learning isn't that great.
In fact,

23
00:01:33.351 --> 00:01:37.910
most people prefer to use policy gradients as they've been shown to work better

24
00:01:37.940 --> 00:01:40.010
than Q learning when tuned.
Well,

25
00:01:40.430 --> 00:01:44.090
the reason for this is because it's an end to end approach,

26
00:01:44.270 --> 00:01:47.930
meaning it's one system that can learn the entire game.

27
00:01:48.170 --> 00:01:53.170
There's an explicit policy and a principled approach that directly optimizes the

28
00:01:53.511 --> 00:01:54.920
expected reward.

29
00:01:55.400 --> 00:02:00.290
Pong is a great example of a simple reinforcement learning task and we can

30
00:02:00.291 --> 00:02:03.410
represent it as a mark Haub decision process.

31
00:02:03.710 --> 00:02:08.710
Think of this as a graph where each node is a particular game state and each

32
00:02:08.751 --> 00:02:10.700
edge is a possible transition.

33
00:02:11.090 --> 00:02:15.620
Each edge also gives a reward and the goal is to compute the optimal way of

34
00:02:15.621 --> 00:02:20.330
acting in any state to maximize rewards.
You know how the game works?

35
00:02:20.510 --> 00:02:24.050
Two players each gets a paddle and you have to bounce the ball,

36
00:02:24.051 --> 00:02:27.350
pass the other player at a more technical level.

37
00:02:27.410 --> 00:02:32.410
We're going to receive an image frame which is a 210 by 160 by three byte array,

38
00:02:32.990 --> 00:02:37.990
basically pixel values and we want to decide if we move our pedal up or down

39
00:02:38.330 --> 00:02:42.320
based on that.
After every single decision we make the game,

40
00:02:42.321 --> 00:02:44.900
we'll execute our action and give us a reward.

41
00:02:45.170 --> 00:02:50.030
We get a plus one if the ball goes past the opponent and a minus one reward if

42
00:02:50.031 --> 00:02:51.800
we miss the ball or zero.

43
00:02:51.801 --> 00:02:55.910
Otherwise the goal is to move this paddle so that we get lots forward.

44
00:02:56.120 --> 00:03:00.610
Something to keep in mind is that we should probably not make any assumptions

45
00:03:00.760 --> 00:03:04.360
about the way that the game of Pong works.
Why?

46
00:03:04.540 --> 00:03:09.040
Because we actually don't care about pong.
We care about complex,

47
00:03:09.070 --> 00:03:13.030
high dimensional problems are getting a robot to save a person in a natural

48
00:03:13.031 --> 00:03:16.690
disaster,
but if we can get an AI to learn pong,

49
00:03:16.810 --> 00:03:21.010
we can someday get them to do more useful tasks using their ability to

50
00:03:21.011 --> 00:03:22.810
generalize to any task.

51
00:03:23.440 --> 00:03:27.820
Our first step is going to be to create a neural network that represents our AI.

52
00:03:28.000 --> 00:03:30.840
It's job is to learn a policy,
so the network,

53
00:03:30.850 --> 00:03:34.150
we'll take the state of the game and decide what we should do,

54
00:03:34.240 --> 00:03:37.300
move up or down based on that game state,

55
00:03:37.510 --> 00:03:39.280
we'll call it the policy network.

56
00:03:39.430 --> 00:03:44.110
We can just use a two layer neural network that takes the raw image pixels of

57
00:03:44.111 --> 00:03:49.111
the game and produces a single number indicating the probability of going up

58
00:03:49.570 --> 00:03:54.570
every iteration we can sample from this distribution to get the actual move.

59
00:03:54.670 --> 00:03:56.650
Since we only have two layers,

60
00:03:56.740 --> 00:04:01.690
the neurons in the hidden layer will be able to detect various game states like

61
00:04:01.691 --> 00:04:06.040
if the ball is on top or our paddle is in the middle and the neurons in the next

62
00:04:06.041 --> 00:04:10.090
layer can then decide if in each case,
if we should be going up or down.

63
00:04:10.360 --> 00:04:13.030
Think about how difficult this problem is.
For a second,

64
00:04:13.120 --> 00:04:16.060
we're getting about a hundred thousand 800 numbers and forward.

65
00:04:16.061 --> 00:04:17.110
Our policy network,

66
00:04:17.260 --> 00:04:21.490
which can easily involve on order of a million parameters for both layers.

67
00:04:21.760 --> 00:04:24.070
If we decided to go up for a given time step,

68
00:04:24.280 --> 00:04:28.450
the game could return a zero reward and give another hundred thousand 800

69
00:04:28.451 --> 00:04:29.890
numbers for the next frame.

70
00:04:30.160 --> 00:04:34.150
It's possible we could repeat this process a hundred times steps before getting

71
00:04:34.151 --> 00:04:37.840
any non zero reward.
If we finally get a plus one,

72
00:04:37.990 --> 00:04:39.850
how can we tell what made that happen?

73
00:04:40.060 --> 00:04:43.840
Was it some move we played just recently or 40 frames ago?

74
00:04:43.990 --> 00:04:48.500
Maybe it had something to do with frame 12 and then frame 38 which of them

75
00:04:48.501 --> 00:04:53.410
billion knobs of our neural network should we change and how in order to do

76
00:04:53.411 --> 00:04:54.580
better in the future.

77
00:04:55.330 --> 00:05:00.310
This is a common problem in reinforcement learning called credit assignment.

78
00:05:00.610 --> 00:05:04.810
How do we solve this?
Well,
let's think of the easy case first.

79
00:05:04.840 --> 00:05:09.220
Aka supervised learning.
Assume we had labels for every single move.

80
00:05:09.490 --> 00:05:13.780
As in we could feed an image to the network and get some probabilities for two

81
00:05:13.781 --> 00:05:15.160
classes up and down.

82
00:05:15.280 --> 00:05:19.510
We could compare that prediction to a label which would tell us the optimal

83
00:05:19.511 --> 00:05:24.460
thing to do at that time step like go up or down.
Once we find the error,

84
00:05:24.490 --> 00:05:27.160
the difference between the label and the prediction,

85
00:05:27.400 --> 00:05:30.880
we could use backpropagation to update our neural network,

86
00:05:31.060 --> 00:05:35.500
the most common optimization scheme using a gradient vector to update our

87
00:05:35.501 --> 00:05:39.520
weights after every single parameter update for every training time step,

88
00:05:39.730 --> 00:05:43.540
our network would be slightly more likely to predict a move that more closely

89
00:05:43.541 --> 00:05:46.210
relates to the ideal move I.
E.
The label,

90
00:05:46.720 --> 00:05:48.580
but let's get real for a second.

91
00:05:48.820 --> 00:05:51.550
We aren't going to have labels for every single move.

92
00:05:51.610 --> 00:05:55.120
This is where the policy gradients solution comes into play.

93
00:05:55.450 --> 00:05:59.480
It's our policy network is fed an image of a game state and outputs a

94
00:05:59.481 --> 00:06:04.481
probability of going up as say 30% and down as say 70% we can sample an action

95
00:06:06.050 --> 00:06:10.310
from this distribution and execute it in the game like down for example.

96
00:06:10.670 --> 00:06:14.840
But our problem is that we don't know if down is good yet,
but that's okay.

97
00:06:15.110 --> 00:06:17.360
We can just wait a bit and see if it is.

98
00:06:17.540 --> 00:06:20.840
We'll just wait until the end of the game.
Then take the reward.

99
00:06:20.841 --> 00:06:25.841
We got either a plus one if we won or a minus one if we lost and use that number

100
00:06:26.121 --> 00:06:28.220
as the gradient for the action we've taken.

101
00:06:28.520 --> 00:06:30.860
So if going down ended up losing the game,

102
00:06:30.861 --> 00:06:35.660
eventually we can find a gradient that discourages the network to take the down

103
00:06:35.661 --> 00:06:38.180
action for the input in the future.

104
00:06:38.420 --> 00:06:40.880
And that's how policy gradients work.

105
00:06:41.120 --> 00:06:46.120
We have a policy that samples actions and the actions that happen to eventually

106
00:06:46.730 --> 00:06:51.710
lead to game wins get encouraged in the future actions that lead us to losing

107
00:06:51.711 --> 00:06:54.890
the game,
get discouraged for training.

108
00:06:55.040 --> 00:06:59.810
We can initialize the policy network with two sets of weights and play a hundred

109
00:06:59.811 --> 00:07:00.800
games of pong.

110
00:07:01.160 --> 00:07:06.160
Assume each game is made up of 200 frames and we'd made 20,000 decisions for

111
00:07:06.561 --> 00:07:07.880
going up or down.

112
00:07:08.030 --> 00:07:12.410
And for each one we know the parameter gradients which tells us how we should

113
00:07:12.411 --> 00:07:17.411
change the parameter if we wanted to encourage that decision in that state in

114
00:07:17.601 --> 00:07:18.290
the future.

115
00:07:18.290 --> 00:07:23.180
The only missing step is to label every decision we've made as good or bad.

116
00:07:23.420 --> 00:07:28.250
If we won 12 games and lost 88 we can take 200 times 12 decisions we made in the

117
00:07:28.251 --> 00:07:30.320
winning game and do a possible update.

118
00:07:30.530 --> 00:07:35.330
We'll take the other 200 times 88 decisions we made in the losing game and do a

119
00:07:35.331 --> 00:07:37.190
negative updates.
That's it.

120
00:07:37.340 --> 00:07:41.450
The network will now become slightly more likely to repeat actions that worked

121
00:07:41.750 --> 00:07:46.430
and less likely to repeat actions that didn't work,
not that crazy.

122
00:07:46.431 --> 00:07:48.950
Right?
Three things to remember from this video.

123
00:07:49.310 --> 00:07:53.720
None of the algorithms that have achieved state of the art results out of deep

124
00:07:53.721 --> 00:07:55.100
mind are novel.

125
00:07:55.370 --> 00:07:59.450
It's the data and the computing resources we have now that gives them

126
00:07:59.510 --> 00:08:00.890
breakthrough results.

127
00:08:00.980 --> 00:08:05.060
The policy gradients method involves running a policy for a while,

128
00:08:05.150 --> 00:08:09.500
seeing what actions lead to high rewards than increasing their probability

129
00:08:09.590 --> 00:08:11.480
through back propagating gradients.

130
00:08:11.540 --> 00:08:16.340
And the credit assignment problem is a common one in our l where we're not sure

131
00:08:16.341 --> 00:08:21.230
how to decide which action an AI took contributed to the reward he received.

132
00:08:21.530 --> 00:08:24.770
Last week's coding challenge winner is Alex Meiosis dove.

133
00:08:24.950 --> 00:08:29.950
He implemented a simple cue learning Bot using open Ai's gym environment to

134
00:08:29.961 --> 00:08:33.410
solve the frozen lake environment.
Really efficient code.

135
00:08:33.530 --> 00:08:38.530
He did it in just under 40 lines of python and the runner up is Aditya Beer

136
00:08:39.020 --> 00:08:43.580
Parmar who solved the same environment using a greedy action selection
algorithm.

137
00:08:43.970 --> 00:08:48.290
This week's challenge is to use a policy gradients method to solve the game

138
00:08:48.410 --> 00:08:51.710
other than palm posts.
Your get hub links in the comment section,

139
00:08:51.800 --> 00:08:53.540
and I'll announce the winner next week.

140
00:08:53.570 --> 00:08:55.400
Please subscribe for more programming videos.

141
00:08:55.401 --> 00:08:58.730
And for now I've got to go assign some credit.
So thanks for watching.

