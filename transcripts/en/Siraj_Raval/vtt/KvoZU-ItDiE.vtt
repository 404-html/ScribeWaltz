WEBVTT

1
00:00:00.990 --> 00:00:01.300
Okay.

2
00:00:01.300 --> 00:00:05.470
<v 1>Hello?
Can anyone hear me?
Who can hear me?</v>

3
00:00:07.060 --> 00:00:09.970
I am alive.
I know I'm late,

4
00:00:10.240 --> 00:00:15.070
but can you hear me is the question
I am talking,

5
00:00:16.350 --> 00:00:20.690
I am talking.
I'm not sure if anybody,

6
00:00:21.110 --> 00:00:23.750
oh my God.
Holy Shit.
That was loud.

7
00:00:26.810 --> 00:00:27.643
<v 0>Yup.</v>

8
00:00:28.880 --> 00:00:32.600
<v 1>Yeah,
you guys hear me?
Okay,
here we go.
Hi everybody.
Hello world.</v>

9
00:00:32.601 --> 00:00:37.400
It's to Raj and we're today we're going to build a neural network.

10
00:00:38.240 --> 00:00:41.900
Okay.
It's going to be a special type of neural network.
Okay,
awesome.

11
00:00:42.320 --> 00:00:44.570
It's a special type of neural network.
Hi.

12
00:00:45.050 --> 00:00:47.750
That is called an LSTM neural network.

13
00:00:47.840 --> 00:00:51.170
It's like probably the most advanced neural network I could think of.

14
00:00:51.171 --> 00:00:53.750
But to do it in an easy way.
So this is going to be a challenge,

15
00:00:53.751 --> 00:00:57.830
but it's gonna be Super Fun.
We're going to generate,
um,

16
00:00:58.460 --> 00:01:01.460
city names using this in python.

17
00:01:02.660 --> 00:01:05.570
We're going to generate city names,
just random city names.

18
00:01:05.571 --> 00:01:10.070
So we're going to train it on a list of every us city in every single city in

19
00:01:10.071 --> 00:01:14.840
the u s and then it's gonna generate new city names from scratch.

20
00:01:15.380 --> 00:01:19.010
Yes,
to long,
short term memory much.
Mathias,

21
00:01:19.020 --> 00:01:21.240
a neural network is,
uh,

22
00:01:21.280 --> 00:01:26.280
an algorithm that replicates neurons in the brain and you can train it to learn

23
00:01:26.811 --> 00:01:30.440
things.
It's a learning algorithm.
Okay.

24
00:01:30.740 --> 00:01:33.080
So we're just going to get right into,
well,

25
00:01:33.110 --> 00:01:35.870
I want to do a five minutes Q and a and then we can just get started.

26
00:01:38.060 --> 00:01:41.990
Okay.
Hitesh the different types of neural networks.
Okay.
So,

27
00:01:42.280 --> 00:01:44.300
so there's the regular type of neural network,

28
00:01:44.301 --> 00:01:46.760
which is just feed forward and a feed forward.

29
00:01:46.761 --> 00:01:50.510
Neural network data just flows one way from the beginning,

30
00:01:50.511 --> 00:01:52.760
from the input layer all the way to the output layer.

31
00:01:53.030 --> 00:01:56.060
That's a feed forward and all network.
And those things are good for,

32
00:01:56.620 --> 00:01:57.453
<v 0>uh,</v>

33
00:01:59.140 --> 00:02:01.600
<v 1>predicting the next number in a sequence of numbers.</v>

34
00:02:01.780 --> 00:02:05.710
I'm going to build this using 10 TF learn.
Yes.
And tensorflow,
it's a,

35
00:02:05.711 --> 00:02:08.230
it's a library on top of tensorflow.
You Guys gonna,
you're gonna,

36
00:02:08.390 --> 00:02:09.223
you're gonna love this.

37
00:02:09.370 --> 00:02:12.880
A recurrent neural network is a type of neural network where the data flows not

38
00:02:12.881 --> 00:02:16.440
just through it,
but then back again.
So it,
so it's recurrent,

39
00:02:16.441 --> 00:02:19.900
it goes back and forth through that.
And that's good for memory.

40
00:02:20.110 --> 00:02:21.460
So anytime you need to

41
00:02:23.170 --> 00:02:28.170
remember maybe a sequence of music or a sequence of a text or poetry if you want

42
00:02:28.841 --> 00:02:30.850
to generate some things,
some new type of texts.

43
00:02:31.120 --> 00:02:33.660
That's what recurrent networks are good for.
Um,

44
00:02:33.880 --> 00:02:38.490
I can build a neural network from scratch.
It's possible using num Pi,

45
00:02:38.520 --> 00:02:43.000
the scientific computing library.
Um,
and then one more type of neural network.

46
00:02:43.001 --> 00:02:46.150
And the one we're going to do in this video is called an LSTM neural network,

47
00:02:46.270 --> 00:02:50.410
which is a type of recurrent network.
It's called long short term memory.

48
00:02:50.710 --> 00:02:53.620
So it can remember things from way back in the sequence of data.

49
00:02:53.950 --> 00:02:55.750
So it's like even remembers everything,

50
00:02:55.751 --> 00:02:58.210
which is what we're going to need for the city because there's so many different

51
00:02:58.211 --> 00:03:02.080
cities for this,
uh,
city generations.
Let me answer a few more questions.

52
00:03:02.081 --> 00:03:04.720
I've got about three more minutes.
I want to get right into it this time.

53
00:03:05.170 --> 00:03:06.520
How did you learn to build neural networks?

54
00:03:06.521 --> 00:03:09.300
By just taking time and going through the machine learning stuff?

55
00:03:09.301 --> 00:03:13.270
We're going to get up and just downloading a bunch of neural networks from get

56
00:03:13.271 --> 00:03:15.070
hub repositories,
just playing around with them,

57
00:03:15.071 --> 00:03:16.780
seeing how they work for comparing them,

58
00:03:16.840 --> 00:03:19.330
comparing different library increment implementations of it,

59
00:03:19.600 --> 00:03:21.880
and then building my own from scratch using them plot.

60
00:03:23.050 --> 00:03:24.820
Where can you get tensorflow?
Google tensor.

61
00:03:24.880 --> 00:03:29.440
Just Google tensorflow is firstly a non is a pain.
Um,

62
00:03:29.830 --> 00:03:32.510
build it from scratch.
I'm gonna,
I'm gonna.
Um,

63
00:03:33.990 --> 00:03:38.990
build an LSTM from close to scratch is c sharp,

64
00:03:39.151 --> 00:03:41.420
good for neural networks.
Um,

65
00:03:42.090 --> 00:03:46.260
there's the Microsoft CNTK library at which I don't really like that much cause

66
00:03:46.261 --> 00:03:50.910
I don't like Microsoft.
I mean,
no,
you know,
you know.
Anyway,

67
00:03:51.060 --> 00:03:54.870
I'm not gonna apologize.
All right.
All right,
we're going to get to the tutorial.

68
00:03:55.090 --> 00:03:59.730
Um,
yes.

69
00:03:59.731 --> 00:04:03.090
LSTS are definitely more powerful than hidden Markov models.
I mean,

70
00:04:03.091 --> 00:04:04.080
hidden Markov models.

71
00:04:04.320 --> 00:04:07.710
We're kind of the precursor to deep learning and this whole craze going on with

72
00:04:07.711 --> 00:04:08.580
deep learning right now,

73
00:04:08.940 --> 00:04:12.720
deep learning kind of makes hidden Markov models not as not as relevant anymore.

74
00:04:13.220 --> 00:04:17.220
Um,
how to design and build a Dataset.
You just have to,
um,

75
00:04:18.750 --> 00:04:20.610
how do you design and build a data set?

76
00:04:20.820 --> 00:04:24.930
You can just create an excel spreadsheet and then for the x column,
you know,

77
00:04:24.931 --> 00:04:29.931
like put like number of times that I've been laid and then yes,

78
00:04:30.271 --> 00:04:32.460
no,
yes,
no.
For everyday or something.
There you go.

79
00:04:32.461 --> 00:04:35.520
There's your data's that CSV and python can read CSE files very easily.

80
00:04:35.790 --> 00:04:38.120
Right now I'm in San Francisco.
Um,

81
00:04:39.120 --> 00:04:44.120
the goal of this neural network is to generate a city names from scratch,

82
00:04:44.580 --> 00:04:47.070
like new city.
Next,
we're going to train it on city names.

83
00:04:47.470 --> 00:04:49.860
It's going to generate new city names.
Okay.

84
00:04:49.861 --> 00:04:54.420
So I'm just going to get right into it.
Here we go.
Let's see,
screen chair.

85
00:04:57.590 --> 00:04:59.210
I would recommend my videos to get started.

86
00:04:59.211 --> 00:05:01.760
Start from machine learning for hackers.
Number one,
screen share.

87
00:05:02.360 --> 00:05:05.300
So I'm going to go to the desktop.
Okay.

88
00:05:06.770 --> 00:05:08.960
And Holy Shit,
that's a lot of shit.
All right,

89
00:05:08.990 --> 00:05:12.950
so I'm gonna move this and it's going to show me,

90
00:05:14.420 --> 00:05:16.400
and we're gonna make this bigger.

91
00:05:17.150 --> 00:05:21.350
So I'm going to be in the corner and you guys,
your text,

92
00:05:21.351 --> 00:05:23.720
I'm going to put right up here so I can look at it,

93
00:05:24.020 --> 00:05:28.550
your chats while I code this thing.
Okay.
Um,

94
00:05:30.550 --> 00:05:35.440
uh,
okay.
Apart from tensorflow and python,

95
00:05:35.470 --> 00:05:38.650
yes,
there are dependencies I'm going to be showing you guys wished dependencies

96
00:05:38.651 --> 00:05:43.300
we're going to use.
Uh,
let me make sure that everything is working well here.

97
00:05:43.840 --> 00:05:46.690
Okay.
Let me make sure everything's working well.

98
00:05:48.240 --> 00:05:52.330
Make sure my text editors big,
nice and big that are right.
That's what we want.

99
00:05:52.331 --> 00:05:56.680
Oh Shit.
Where did that go?
All right.
There we go.

100
00:05:57.320 --> 00:05:59.570
All right,
let me make sure everything's nice and big.

101
00:05:59.571 --> 00:06:04.400
All of Knights and big texts.
Is that,
um,
is that good in big for you guys?

102
00:06:04.401 --> 00:06:07.920
Is that readable right here?
This,
this level?
Um,

103
00:06:09.250 --> 00:06:09.890
<v 2>okay.</v>

104
00:06:09.890 --> 00:06:12.920
<v 1>This is good.
Right?
All right.
We're just going to get started and if anybody,</v>

105
00:06:12.921 --> 00:06:16.880
suddenly I say we're going to,
this is going to be,
uh,
probably 30 minutes.

106
00:06:16.940 --> 00:06:21.740
I'll make it bigger.
Okay.
Blah,
blah,
blah,
blah.
Okay.

107
00:06:22.070 --> 00:06:26.540
How big?
That's big,
right?
All right.
Here we go.
A bit bigger.
Okay.

108
00:06:26.550 --> 00:06:31.400
That's just big as we're going to go guys.
Okay,
let's get started.

109
00:06:31.970 --> 00:06:36.650
Oh Shit.
Yeah.
Okay.
Here we go.
Uh,
uh,

110
00:06:37.250 --> 00:06:41.210
uh,
okay.
So first things first,

111
00:06:41.211 --> 00:06:43.880
we're going to import our dependencies.
Okay.

112
00:06:44.150 --> 00:06:47.240
Let me bring up my notes for a second.

113
00:06:47.241 --> 00:06:52.170
So I make sure that this is the right shit.
Okay.
So Da da Da.

114
00:06:52.940 --> 00:06:57.340
And we're going to build this in python and it's going to be,
you know,

115
00:06:57.360 --> 00:07:01.100
so many fucking windows up there.
Uh Oh,
sex for Ubuntu for deep learning.

116
00:07:01.101 --> 00:07:04.910
I would say goodbye.
I know I'm on Osx,
but like,
okay,
here we go.

117
00:07:05.800 --> 00:07:07.880
So we're going to start off by importing the first module.

118
00:07:08.360 --> 00:07:12.710
This is going to be called a future.
So the future module is toward,
uh,

119
00:07:13.280 --> 00:07:15.350
it's the link between python two and three.

120
00:07:15.470 --> 00:07:19.550
It's a way for us to use python two or three without there being any kind of

121
00:07:19.551 --> 00:07:23.900
version errors between what we're overusing.
So I'm gonna say from future import,

122
00:07:24.230 --> 00:07:27.980
uh,
absolute important.
I'm going to have a recording of this session for sure.

123
00:07:28.250 --> 00:07:32.060
Division and the print function.
Those are,
that's,

124
00:07:32.370 --> 00:07:36.940
and let me turn on python cause right now it's in plain text.
So I saw on boom,

125
00:07:39.260 --> 00:07:43.490
there we go.
There you go.

126
00:07:44.120 --> 00:07:46.950
So hold on.
That shit.
It's big.

127
00:07:48.100 --> 00:07:52.970
So those are our,
those are our,
uh,
dependencies.
Okay.

128
00:07:53.000 --> 00:07:55.190
So we have one,
we have another one.
Okay.

129
00:07:55.730 --> 00:07:59.640
So the next one is uh oh s so we're going to important oh west.

130
00:07:59.890 --> 00:08:03.500
That allows us to have operating system didn't any functionality.

131
00:08:03.560 --> 00:08:05.210
We're going to use it to read in file paths,

132
00:08:05.211 --> 00:08:06.560
which is where our data set's going to be.

133
00:08:06.561 --> 00:08:08.300
We're going to read in our dataset using this,

134
00:08:08.450 --> 00:08:11.850
which we're going to download it in the script.
Okay.
So that's,
oh asked.
Um,

135
00:08:11.900 --> 00:08:15.950
what else we need to import six and we're going to import something from six

136
00:08:15.951 --> 00:08:19.940
called a moves.
And so let me just write this at the top.
Like fucking Ellis.

137
00:08:20.060 --> 00:08:23.750
Sorry for the language.
I don't know.
I don't care.
No,
no,
no apologies ever.
Oh cm.

138
00:08:23.751 --> 00:08:28.070
To generate city names.
Okay.
That's what this is.
This whole thing.

139
00:08:28.400 --> 00:08:31.250
Okay.
So from six important moves.
Okay.

140
00:08:32.240 --> 00:08:35.540
And so what that's going to do a six to another library for like commonalities

141
00:08:35.541 --> 00:08:39.070
between different python versions,
uh,
and moves is,
uh,

142
00:08:39.140 --> 00:08:41.390
we're going to use that for the URL live module,

143
00:08:41.570 --> 00:08:45.200
which is how we're going to pull data from the Internet.
Okay.
In a second.

144
00:08:45.470 --> 00:08:50.090
So what else?
We're going to import,
um,
SSL,

145
00:08:50.120 --> 00:08:54.980
which is how we're going to long short term memory network,
a long,

146
00:08:56.070 --> 00:08:59.970
short,
long,

147
00:08:59.971 --> 00:09:01.620
short term memory

148
00:09:03.850 --> 00:09:08.360
to generate city
January the city,
let's just say generate.

149
00:09:08.361 --> 00:09:09.194
So if he's okay,

150
00:09:09.930 --> 00:09:12.910
so importance of cell that's going to let us connect to the Internet.
Um,

151
00:09:13.260 --> 00:09:17.140
and then of course we're not in port CF learn.
Um,

152
00:09:18.420 --> 00:09:22.020
we have learned is going through VR machine learning library after we're going

153
00:09:22.021 --> 00:09:25.500
to do use TF learn,
uh,
we're going to work with some operations for our data.

154
00:09:25.501 --> 00:09:29.710
So we're going to say from TF learned 30 minutes.
Exactly.
Thank you.
Um,

155
00:09:29.760 --> 00:09:33.900
from field Florida data utiles work in full screen,

156
00:09:34.320 --> 00:09:39.270
um,
working full screen.
Uh,
no.

157
00:09:39.330 --> 00:09:41.190
Okay,
here we go.
Stuck.

158
00:09:42.060 --> 00:09:46.230
So we're going to import everything from the data utilities.
Okay.
Um,

159
00:09:46.920 --> 00:09:51.600
so okay.
That's,
those are all of our dependencies.
But TF learn,
uh,
that's,

160
00:09:51.601 --> 00:09:54.480
those are all of our dependencies.
That's it.
Now we're going to start coding.

161
00:09:54.510 --> 00:09:58.410
Okay?
So the first thing we want to do is get our data assaults.

162
00:09:59.100 --> 00:10:03.300
Step one,
retrieve the data,
okay?
That's,
this is the data.

163
00:10:06.450 --> 00:10:10.450
Okay?
They're going to,
we're going to get our data.
Okay.
So,

164
00:10:12.120 --> 00:10:14.940
so first things first,
we're going to create the path for our dataset

165
00:10:17.250 --> 00:10:20.480
and we're going to call it US cities.
Dot Text.
Okay.
That center,

166
00:10:20.490 --> 00:10:24.780
we're going to call our dataset.
So if we don't have that file already,
okay?

167
00:10:24.781 --> 00:10:27.390
If we don't have it,
we want to tell our machine that.

168
00:10:27.391 --> 00:10:29.910
So that's where the ois module was going to come in.
We're going to say,

169
00:10:29.940 --> 00:10:32.390
if not [inaudible] Stop Path,
uh,

170
00:10:32.450 --> 00:10:36.670
his file and then the path as the argument.
But TF learning is a,

171
00:10:36.680 --> 00:10:39.810
is a wrapper on top of tensorflow that makes it a little easier to use.

172
00:10:39.811 --> 00:10:43.290
So it's great for learning.
Okay.

173
00:10:43.291 --> 00:10:45.870
So it's not really an add on.

174
00:10:45.871 --> 00:10:48.360
It's just like if I were to write an entire neural network,

175
00:10:48.361 --> 00:10:52.290
maybe like a hundred lines of neural network in tensorflow and TF learn,

176
00:10:52.320 --> 00:10:55.200
I could access that neural network and maybe two lines of code.

177
00:10:55.500 --> 00:10:58.560
And it just says like access neural networks.
So it's like an,

178
00:10:59.090 --> 00:11:02.250
it's a layer of abstraction on top of that complexity of tensorflow.

179
00:11:02.640 --> 00:11:04.680
So the trade off is you don't get that,
you know,

180
00:11:04.681 --> 00:11:07.230
fine grain control over everything.

181
00:11:07.500 --> 00:11:10.140
But the good thing is it's easier to understand.
It's more readable.

182
00:11:10.230 --> 00:11:13.830
Kara us is similar to a TF learning and that is also an,

183
00:11:14.330 --> 00:11:18.690
it's a layer of abstraction over tensorflow,
Angiano and uh,

184
00:11:18.750 --> 00:11:23.640
and a few other libraries.
Okay.
So retrieving the data.

185
00:11:23.790 --> 00:11:28.140
So if,
uh,
so if we don't have that in her file path,
we want to create it,
right?

186
00:11:28.141 --> 00:11:29.480
So we're going to say,
con,

187
00:11:29.520 --> 00:11:34.520
we're going to create a context capsular context that says,

188
00:11:37.240 --> 00:11:38.073
<v 0>MMM,</v>

189
00:11:39.850 --> 00:11:44.140
<v 1>uh,
create
on verified.</v>

190
00:11:44.710 --> 00:11:47.740
Bridget.
I'm too,
I'm too soon into this.
Can make it from scratch right now,

191
00:11:47.770 --> 00:11:51.660
but next livestream,
I will.
Okay.
But just follow along.
We're gonna,
you're gonna,

192
00:11:51.670 --> 00:11:55.930
you're gonna like this.
Create an unverified context.
What does that do?

193
00:11:55.990 --> 00:11:58.420
That just initialize a bunch of key and trucks managers.

194
00:11:58.780 --> 00:12:00.670
Cause we're going to get data from a server in a second.

195
00:12:00.671 --> 00:12:03.920
So that's just some initialization stuff.
Okay,
let me,
um,

196
00:12:04.360 --> 00:12:08.380
let me make this a little smaller.
Okay.
That's all.
That's a little small.
Okay.

197
00:12:08.440 --> 00:12:09.850
So now we're going to say,

198
00:12:11.740 --> 00:12:12.573
<v 0>uh,</v>

199
00:12:13.770 --> 00:12:17.760
<v 1>now we're going to retrieve the actual,
we're going to recruit the actual data.</v>

200
00:12:17.910 --> 00:12:22.890
So we're going to say we're gonna use a URL live module of moves to retrieve our

201
00:12:22.891 --> 00:12:26.610
data.
And uh,
let's see,
URL light module.

202
00:12:26.611 --> 00:12:31.611
We're going to retrieve our data and we're going to say request URL,

203
00:12:35.280 --> 00:12:39.530
retrieve,
make computers from scratch.
Nice.
Okay.

204
00:12:40.140 --> 00:12:44.490
One day I actually have before it's,
it's not a big deal.
Anybody can do it.
Okay.

205
00:12:44.510 --> 00:12:49.470
So I will upload it.
I'm recording it.

206
00:12:49.560 --> 00:12:52.840
So six packages,
good for um,

207
00:12:53.400 --> 00:12:57.980
being a kind of
gateway between python two and three.

208
00:12:58.040 --> 00:13:02.330
So you can write python to code and it works on Python three and vice versa.

209
00:13:02.750 --> 00:13:04.880
Okay.
And so here is what we're going to retreat.

210
00:13:05.120 --> 00:13:07.520
So this is the link to the data set that we're going to retreat.

211
00:13:07.700 --> 00:13:10.080
I have this paper,
I,
let me,
I have this written down somewhere,

212
00:13:10.081 --> 00:13:14.660
so I'm going to paste this in.
It's is the data,
is that okay?
It's a huge link.

213
00:13:14.780 --> 00:13:18.860
It's a huge link.
Okay.
Boom.
This whole thing.

214
00:13:18.990 --> 00:13:23.880
That's our u s cities data set.
Okay.
So I'm just going to say get dataset.

215
00:13:26.570 --> 00:13:30.920
Oh,
just like that.
Make it a little smaller.
Okay,
so that's our dataset.

216
00:13:31.210 --> 00:13:35.730
All right,
we've done that and we've retrieved it.
And uh,

217
00:13:35.820 --> 00:13:40.820
I want to save it to my path and with my context equal to the context I just

218
00:13:42.121 --> 00:13:46.950
created.
Okay.
So boom.
That's it.
That's my dataset.
So now,
okay,

219
00:13:47.070 --> 00:13:50.610
it's time to do some machine learning on this stuff.
Okay.
So,

220
00:13:53.490 --> 00:13:54.323
<v 0>mmm,</v>

221
00:13:55.170 --> 00:13:57.900
<v 1>it's going to be on get hub.
It's going to be on get help.
Don't worry man.</v>

222
00:13:57.920 --> 00:14:02.640
It is such a trip to read comments and code and do it all live at the same time.

223
00:14:02.820 --> 00:14:05.880
This shit is crazy.
I love it though.
I love it.
I love you guys.

224
00:14:05.970 --> 00:14:07.620
You guys are awesome.
Okay,

225
00:14:07.650 --> 00:14:12.360
so max length equals path hypo.

226
00:14:12.390 --> 00:14:17.340
Who spotted that half pipe or Javier Path?
Typo.
Oh,
thank you.
You.

227
00:14:18.060 --> 00:14:22.440
Thank you that ass.
Okay.
So Max length.
So this is the max link.

228
00:14:22.441 --> 00:14:24.780
We want cities to be at city named Max length.

229
00:14:25.020 --> 00:14:28.260
We don't want them to the cities that we generate to be,
um,

230
00:14:28.860 --> 00:14:32.220
line break is a good idea.
Thank you.
Thank you for that.
Uh,

231
00:14:32.420 --> 00:14:35.040
so we don't want our city names to be longer than 20 characters.

232
00:14:35.041 --> 00:14:39.990
So we're going to set this max length.
All right?
I checked the type of,
okay,

233
00:14:39.991 --> 00:14:41.800
so Mattson,
Max length.
Uh,

234
00:14:41.920 --> 00:14:45.900
and so now we're going to vectorize our textbook.
Okay.

235
00:14:45.901 --> 00:14:50.790
So we're going to take vectorize that text file.
What does that mean?

236
00:14:50.990 --> 00:14:55.700
Well,
we have a bunch of import typos.
Hi.

237
00:14:56.290 --> 00:14:58.890
Thanks for coming.
Okay,
so that's rising a text file.

238
00:14:58.920 --> 00:15:02.510
We have a Dataset we want to vectorize it would be when we have a data set of

239
00:15:02.511 --> 00:15:05.690
words,
we want to vectorize it.
So what does that,
what does that mean?

240
00:15:05.691 --> 00:15:07.730
I'm going to get that type of,
wait a second.
What does that mean?

241
00:15:08.090 --> 00:15:11.870
It means take all these words,
all these thousands of words,

242
00:15:11.930 --> 00:15:16.010
and find those commonalities between those words and make those abstractions of

243
00:15:16.011 --> 00:15:19.200
those commonalities.
So if I have a set of words like,
uh,

244
00:15:20.030 --> 00:15:24.350
a raccoon swirl,
pidgin and dog,

245
00:15:24.620 --> 00:15:29.060
the vector that would be created from those eventually would be animal.
It would,

246
00:15:29.120 --> 00:15:32.370
it wouldn't be the specific word animal,
but it would be a representation.

247
00:15:32.371 --> 00:15:36.290
You like a numerical representation that that is all of those things.

248
00:15:38.080 --> 00:15:40.810
Okay?
So it,
so that's what vectors are.

249
00:15:40.840 --> 00:15:44.020
They help us extract huge series,

250
00:15:44.021 --> 00:15:48.460
his series of data into something that we can manipulate later and we can use

251
00:15:48.461 --> 00:15:50.740
these abstractions to generate new data.

252
00:15:50.770 --> 00:15:55.450
So we could use that animal abstraction to that to then generate a new animal

253
00:15:55.540 --> 00:15:59.100
like a wolf.
Okay.
That's the great thing about abstractions.

254
00:16:00.660 --> 00:16:03.870
Thank you,
Emerson.
Okay,
so let's,
let's make our vectors.

255
00:16:03.930 --> 00:16:08.430
So our vector is going to spit back,
um,
are,
uh,
three things.

256
00:16:08.431 --> 00:16:12.270
We're going to call it x,
Y,
and a chart dictionary.
So it's going to,

257
00:16:12.510 --> 00:16:16.320
let me just write,
let me just write this out.
Okay.
So it's going to be called,
um,

258
00:16:16.920 --> 00:16:21.920
text file to semi redundant sequences.

259
00:16:22.860 --> 00:16:27.630
Okay.
That's the math.
That's the function that TF learn provides.

260
00:16:28.080 --> 00:16:30.420
Uh,
and it's,
it's going to create vectors for us.

261
00:16:30.480 --> 00:16:33.540
And it's going to take arguments,
the arguments is going to take is the path.

262
00:16:33.720 --> 00:16:38.460
So where we are,
I know that where we are,
uh,

263
00:16:39.690 --> 00:16:44.100
uh,
where's the data set that we're making vectors from?
So that's the path.

264
00:16:44.460 --> 00:16:49.230
And then the sequence,
the Max length,
which we defined earlier,
uh,
the Mac,

265
00:16:50.040 --> 00:16:52.530
I won't say I will post the repo at the end of the video.

266
00:16:52.680 --> 00:16:57.280
So where we are posting,
uh,
what is the Max length?

267
00:16:57.300 --> 00:17:00.630
We already defined it,
right?
And then finally,
redundant step,

268
00:17:02.300 --> 00:17:06.810
uh,
equals three.
Okay?
So that means,

269
00:17:06.840 --> 00:17:10.740
uh,
how many times do we want to do this?
So we're just going to say three times.

270
00:17:11.100 --> 00:17:14.130
Okay.
And I'll make a line break here.
Okay.

271
00:17:14.220 --> 00:17:17.870
So that's to vectorize and so that,
so what,
what x,

272
00:17:17.871 --> 00:17:21.770
y and uh,
it's going to be probably a 20,

273
00:17:22.160 --> 00:17:27.080
20 more minutes.
Uh,
we'll,
we'll see 20 more minutes.
Okay.
So,
uh,

274
00:17:27.200 --> 00:17:29.280
what is this doing?
This is,
uh,

275
00:17:29.410 --> 00:17:33.980
it is creating a directors from our words and it returns the inputs,

276
00:17:33.981 --> 00:17:37.820
the targets and the dictionary,
the inputs,
that's the words,
the targets,

277
00:17:37.821 --> 00:17:40.880
those are vectors and the dictionary of those things.
Okay.

278
00:17:44.850 --> 00:17:49.710
I am not on Adderall,
but I will admit I am on toffee.
Yo,
you need coffee.
Do this,

279
00:17:49.800 --> 00:17:53.520
this,
this stuff that I do full time.
You know what I mean?
I will admit it.

280
00:17:53.580 --> 00:17:58.410
I loved,
I love coffee.
Couldn't do without coffee.
Okay.
Anyway,

281
00:17:58.960 --> 00:18:01.820
but I have never tried out a row.
I've never tried Adderall.

282
00:18:01.860 --> 00:18:06.740
I have friends who have,
I've never tried it.
Okay.
All right.

283
00:18:06.741 --> 00:18:08.700
Thanks Andy.
Touch,
uh,

284
00:18:11.050 --> 00:18:14.940
just 20 minutes.
Oh,
you are more,
I mean I could go more.
Okay.

285
00:18:14.941 --> 00:18:17.040
The neural network is going to generate city names.

286
00:18:17.041 --> 00:18:21.060
We have a data set of cities and it's going to generate new city names.
Okay.

287
00:18:21.210 --> 00:18:25.350
This shit.
Okay.
Curator,
man.
Shrooms.
I've done shrooms before.

288
00:18:25.590 --> 00:18:26.930
In fact,
uh,

289
00:18:27.030 --> 00:18:31.650
the reason I started the youtube channel is because I had a very,
uh,

290
00:18:32.190 --> 00:18:34.380
good true trip.
Oh my God.

291
00:18:34.381 --> 00:18:38.130
He just admitted that he just admitted that because he doesn't give a fuck what

292
00:18:38.131 --> 00:18:41.910
anyone thinks.
Okay.
Here we go.
Here we go.

293
00:18:42.690 --> 00:18:44.690
Create an Lstm.
Oh yes.

294
00:18:44.691 --> 00:18:49.290
You test a redundant steps equals three means where we're just going to,

295
00:18:49.870 --> 00:18:54.870
uh,
go through three sequences to,
to make that happen.
Okay.

296
00:18:55.620 --> 00:18:56.910
Okay,
here we go.
Here we go.
Here we go.

297
00:18:56.940 --> 00:19:01.940
So now we're going to create our LCM create LSTM uh,

298
00:19:02.970 --> 00:19:07.860
really Jorge.
Holy Shit.
I love it.
I love it.
A Java script.
Maybe someday.
Yeah.

299
00:19:07.950 --> 00:19:10.680
I mean eventually,
hopefully Jorge,

300
00:19:10.681 --> 00:19:13.730
that you quit your job to do something that you liked cause it's your,

301
00:19:13.760 --> 00:19:17.320
it's not like you're,
you just became crazy anyway.
Okay.
Uh,

302
00:19:17.460 --> 00:19:18.600
create LSTM let's do this.

303
00:19:18.630 --> 00:19:21.030
So we're going to create an Ltm and we're going to call it G.
Okay?

304
00:19:21.031 --> 00:19:23.550
We're going to call it g,
uh,
on.
You Dad said,
yeah,

305
00:19:23.730 --> 00:19:27.060
I am working with you Udacity on the self driving car course.
All my God,

306
00:19:27.061 --> 00:19:29.490
I just admitted that,
but it's going to be awesome.
Yeah.

307
00:19:29.491 --> 00:19:32.580
And you know that self driving car nanodegree I'm working on that.
I can't wait.

308
00:19:32.581 --> 00:19:34.230
I'm going to show you guys next week.
Okay,

309
00:19:34.231 --> 00:19:37.380
next weekend I'm going down to mountain view to meet with the team and stuff.

310
00:19:37.381 --> 00:19:38.214
That's gonna be crazy.

311
00:19:38.250 --> 00:19:40.800
They just saw my self driving car in five minute video and then they were like,

312
00:19:40.920 --> 00:19:42.930
oh,
you should work with us.
And I'm like,
oh my God.

313
00:19:43.140 --> 00:19:46.050
So that's going through and it's you,
the guy who like,
you know,

314
00:19:46.260 --> 00:19:50.310
made Google x and stuff.
Anyway,
that's going to be exciting.
Okay,
here we go.

315
00:19:50.370 --> 00:19:53.930
So TF,
learn dot throw.
We're going to create our LSTM and your career all,
yeah,

316
00:19:53.960 --> 00:19:56.430
we're going to create layers.
Okay.
We're going to just layer by layer.

317
00:19:56.610 --> 00:19:57.860
That's what TF learned is.
Wait for it.

318
00:19:57.861 --> 00:20:02.040
You can define entire layers in one line of code.
So it's like layer,
layer,
layer,

319
00:20:02.041 --> 00:20:06.510
layer,
layer,
layer.
Okay.
Um,
I don't care what the government thinks.
Come at me.

320
00:20:06.511 --> 00:20:09.270
If the government is watching and they think they're going to do something,

321
00:20:09.271 --> 00:20:13.590
cause I did some drugs.
Come at me.
Okay.
Come at me.
You can't,
you can't stop me.

322
00:20:13.770 --> 00:20:17.400
Okay,
here we go.
So TF,
learn dot input data.

323
00:20:18.720 --> 00:20:23.700
Okay,
we're going to say the shape I,
so this is our equal layer.
We're getting,

324
00:20:23.701 --> 00:20:27.090
this is our inkjet layer.
Okay?
This is our input.

325
00:20:27.091 --> 00:20:28.650
Like this is where we're going to put the data in.

326
00:20:28.651 --> 00:20:33.090
So we're going to say the shape equals none.
Max Link.

327
00:20:33.120 --> 00:20:34.910
I'm going to just come in and I'm going to,

328
00:20:37.060 --> 00:20:42.000
I'm going to talk about what I'm,
what I'm doing in a second.
So Char,
Idx.

329
00:20:42.120 --> 00:20:45.750
Okay.
Tara IDX.
Okay,
so what is this doing?
It's the saying,

330
00:20:45.810 --> 00:20:48.550
make our input layer and is the size of our input layer.

331
00:20:48.760 --> 00:20:53.320
It's going to be up to 20 characters.
Um,
and I wrote input wrong,
didn't I?

332
00:20:53.500 --> 00:20:56.060
Where input data it.
Okay,

333
00:20:56.230 --> 00:21:00.970
so that's our inco layer and now we're going to create our next layer and our

334
00:21:00.971 --> 00:21:03.250
next layer is an LSTM lik,
okay,

335
00:21:03.460 --> 00:21:07.750
so GF learn dot LSTM she five 12.

336
00:21:07.960 --> 00:21:09.760
That's going to be the size of our matrix.

337
00:21:13.400 --> 00:21:16.580
Thank you.
Ace.
Ace.
I will,
I will get better at.

338
00:21:17.150 --> 00:21:17.983
<v 0>MMM.</v>

339
00:21:18.200 --> 00:21:22.130
<v 1>This stuff to make it more understandable.
Okay,
so return sequence</v>

340
00:21:23.750 --> 00:21:27.830
equals true.
Okay,
so what is this?
This is the LSTM layer.

341
00:21:28.400 --> 00:21:31.220
How was TM layer is basically you've got,

342
00:21:31.760 --> 00:21:34.850
you've got 512 nodes in this layer.

343
00:21:34.880 --> 00:21:37.790
That's that 512 number a layer.
Okay.

344
00:21:37.791 --> 00:21:41.440
You have 512 neurons or nodes in this.
Like it's a law that's,

345
00:21:41.510 --> 00:21:46.430
that's deep learning.
That's the deep part.
Okay.
You got 512 of them.

346
00:21:47.790 --> 00:21:48.623
<v 0>MMM.</v>

347
00:21:48.720 --> 00:21:53.470
<v 1>Thank you Omar.
I got this.
Okay.
512 lakes.
And uh,</v>

348
00:21:53.760 --> 00:21:58.760
the LSTM is basically long short term memory inside of each of those neurons.

349
00:21:59.370 --> 00:22:01.740
You've got a little gateways,

350
00:22:02.520 --> 00:22:05.310
basically an LSTM was created recently.

351
00:22:05.370 --> 00:22:07.610
And basically anything that we put it out,

352
00:22:07.770 --> 00:22:10.700
we throw any kind of data that we throw an LSTM app,

353
00:22:10.800 --> 00:22:13.350
usually LSTM is or what works best to learn.

354
00:22:13.840 --> 00:22:17.310
We're not sure why they're the most,
they're the most complicated,

355
00:22:17.370 --> 00:22:20.320
complicated types of neural networks out there,
but they're also,
uh,

356
00:22:20.380 --> 00:22:24.900
they also provides the best,
most performance results.
Okay.

357
00:22:25.020 --> 00:22:29.190
So we've got four more layers to create.
Okay.
Wifi 12th.
I mean,
it's just,

358
00:22:29.370 --> 00:22:33.200
it's one of those things where it's just kind of become standard.
Like,
you know,

359
00:22:33.210 --> 00:22:35.370
one guy did it and it worked and he got great results.

360
00:22:35.371 --> 00:22:38.850
So let's just all do five 12th.
I mean,
I can do five 13 to your right.

361
00:22:39.400 --> 00:22:40.233
<v 0>MMM.</v>

362
00:22:40.830 --> 00:22:44.940
<v 1>Peter,
you're right.
So it's Peter.
Peter is right.
So it's not,</v>

363
00:22:44.970 --> 00:22:48.830
it's not just what's in the layer.
It's,
it's not just how many nodes are in the,

364
00:22:48.900 --> 00:22:53.220
it's how many layers.
But I guess in terms of wide and deep learning,
it is both.

365
00:22:54.750 --> 00:22:59.050
Lstm means long short term memory network.
Lsms are great for chatbox.

366
00:22:59.230 --> 00:23:02.160
Why are both variable names g?
Because we're going to keep,

367
00:23:02.220 --> 00:23:06.390
we're going to keep using G.
We're going to keep using g as we build our lips.

368
00:23:07.350 --> 00:23:12.030
Okay.
Okay.
Uh,
we're going to keep using g cause we're building our LSTM.

369
00:23:12.031 --> 00:23:16.290
Okay.
So now we've,
we built that,
uh,
and now we're going to create dropdown.
Okay.

370
00:23:16.291 --> 00:23:21.210
So TF,
learn dot dropout.
Okay.
And we're going to say,
gee,
again,

371
00:23:21.480 --> 00:23:25.080
we're going to keep using g.
Okay.
So what does that mean?
What is dropped out?

372
00:23:25.260 --> 00:23:28.020
Okay,
so how does it relate to recurrent neural networks?

373
00:23:28.040 --> 00:23:30.230
And LSTM is a type of recurrent neural networks.

374
00:23:30.231 --> 00:23:32.790
So it is a subset of a recurrent neural network.

375
00:23:33.270 --> 00:23:37.950
I'm not a part of the Alphago team yet,
but,
okay.
So,
um,

376
00:23:38.000 --> 00:23:39.680
it's a type of regard.
So what is dropout?

377
00:23:39.920 --> 00:23:43.280
When we are training our neural network,
when we are training our neural network,

378
00:23:43.730 --> 00:23:46.340
okay.
Data is flowing.
The neural network,
right?

379
00:23:46.610 --> 00:23:50.360
Data is data is flowing through our neural networks.

380
00:23:50.630 --> 00:23:54.950
And when we train it,
sometimes we have something called overfitting,

381
00:23:55.060 --> 00:24:00.060
whether it w w we have something called overfitting and over fitting is when I'm

382
00:24:00.681 --> 00:24:04.150
wearing it only on a CPU,
not a GPU overfitting.
It's when the data,

383
00:24:04.190 --> 00:24:09.190
when the Lord model is fit too well to only the Dataset that we trained and we

384
00:24:09.591 --> 00:24:12.920
can't generalize it to new things.
So to prevent over fitting,

385
00:24:13.040 --> 00:24:17.510
there's a technique called dropouts dropouts and would drop out does is it

386
00:24:17.511 --> 00:24:22.340
randomly turns off some notes,
it randomly turns off notes,
wild training.

387
00:24:22.430 --> 00:24:25.490
So it doesn't go through the same grooves,
you know,

388
00:24:25.491 --> 00:24:28.700
like these grooves that we create.
And then every time,
you know,

389
00:24:28.701 --> 00:24:32.300
neuro pathways are kind of like,
when you think a certain way and you get older,

390
00:24:32.930 --> 00:24:35.900
you have these grooves,
right?
And so it's hard to break out of those groups.

391
00:24:35.990 --> 00:24:39.860
That's kind of a rough analogy,
but like drop out basically turns off notes.

392
00:24:39.890 --> 00:24:41.570
So it has to find new groups that time.

393
00:24:41.780 --> 00:24:45.440
And what that does is it makes it easier to generalize.
Okay.
So that's,

394
00:24:45.620 --> 00:24:48.950
that's as good as my explanation is going to get pregnant.
So that's dropped out.

395
00:24:49.050 --> 00:24:52.310
We're gonna create another LSTM layer.
Okay.

396
00:24:53.150 --> 00:24:57.560
I started frying my CPQ.
I was going to pry my CPU.
Exactly.

397
00:24:57.940 --> 00:25:01.840
Uh,
okay.
So,
um,
KF learn dot dropped out.

398
00:25:01.980 --> 00:25:05.030
So that was our dropout layer.
We're going to create one more LSTM layer.

399
00:25:05.420 --> 00:25:06.800
I'm going to send you five 12 again,

400
00:25:08.630 --> 00:25:12.170
and then we're going to add more dropped out because we want to drop out at

401
00:25:12.171 --> 00:25:14.570
every layer.
Okay.
So another dropout

402
00:25:16.140 --> 00:25:19.070
g five 12.

403
00:25:21.270 --> 00:25:21.880
<v 0>Yeah.</v>

404
00:25:21.880 --> 00:25:26.440
<v 1>Okay.
Uh,
and then,
oh no,
sorry.
Drop out.</v>

405
00:25:26.441 --> 00:25:30.430
0.5 by the way,
0.5.
Will we create dropout?

406
00:25:30.490 --> 00:25:34.720
It's just a coefficient.
It's just a measure of how random do we want to get.

407
00:25:34.721 --> 00:25:37.650
We could say ploy for,
because they 0.6.
Let Art,

408
00:25:37.660 --> 00:25:39.570
I'm going to get to that in a second.
Okay.
I'll be shaking.

409
00:25:39.571 --> 00:25:41.790
I'll spend drop out one more time.
So drop out.

410
00:25:41.800 --> 00:25:46.800
It's a way to randomly turn off nodes in your neural network as it trains.

411
00:25:48.220 --> 00:25:51.580
And what this does is it prevents overfit.
It prevents over fit.

412
00:25:52.120 --> 00:25:56.620
And overfitting means you're,
you're trained model won't work for new datasets.

413
00:25:56.710 --> 00:25:59.560
It's only gonna work for the data set that you trained to make predictions,

414
00:25:59.710 --> 00:26:01.990
but you want to make predictions of data that it hasn't seen before.

415
00:26:02.320 --> 00:26:06.340
And that's why we use dropout.
And,
and uh,
Jeff Hinton,

416
00:26:06.341 --> 00:26:09.490
the guy who was like the godfather of neural nets made it and everyone's been

417
00:26:09.760 --> 00:26:12.310
batshit crazy about it since.
Okay.

418
00:26:13.300 --> 00:26:14.950
Why did I write the last two lines twice?

419
00:26:14.980 --> 00:26:18.520
Because we have multiple layers to this.
Lstm it's not just one light.

420
00:26:21.110 --> 00:26:23.530
Okay.
Uh,

421
00:26:26.970 --> 00:26:31.600
uh,
m zero four,
eight one.
Uh,
that's,
that's good intuition.
But I,

422
00:26:31.670 --> 00:26:35.710
it's,
I don't think it's that,
I think it's like a,

423
00:26:36.380 --> 00:26:37.213
<v 0>it's a,</v>

424
00:26:38.200 --> 00:26:40.310
<v 1>it's a measure of,
of,
it's,
it's,
it's,
it's just a,</v>

425
00:26:40.340 --> 00:26:43.950
it's just a coefficient and I think like later on it's like,

426
00:26:43.980 --> 00:26:47.970
it's changed in a different way.
Anyway,
I got to keep going here.
Okay.
So,
uh,

427
00:26:47.990 --> 00:26:51.180
so that's our drop out layer and then what else is left?
What else was like,

428
00:26:51.210 --> 00:26:52.110
we have our last layer,

429
00:26:52.111 --> 00:26:55.830
we're going to create our last layer are fully connected layer.
Our last layer.

430
00:26:55.831 --> 00:26:59.520
Okay.
So g equals TF,
TF,

431
00:26:59.521 --> 00:27:01.680
learn dot [inaudible]

432
00:27:03.180 --> 00:27:04.013
<v 0>mmm.</v>

433
00:27:05.090 --> 00:27:09.390
<v 1>Do a fully connected,
okay.
Gee length,</v>

434
00:27:09.540 --> 00:27:11.140
the length of the fully connected layer.

435
00:27:11.160 --> 00:27:12.780
It's going to be the size of our dictionary.

436
00:27:13.140 --> 00:27:16.020
And then we're going to create our activation function,

437
00:27:16.200 --> 00:27:20.040
which is called soft mass.
Softmax is a type of,

438
00:27:21.240 --> 00:27:25.500
it's a type of logistic regression.

439
00:27:26.100 --> 00:27:29.160
It's a type of logistic regression that is good for classification,

440
00:27:29.520 --> 00:27:30.211
which is what we're doing.

441
00:27:30.211 --> 00:27:33.960
We're classifying big sequences of text and then we're going to generate new

442
00:27:33.961 --> 00:27:37.410
sequences.
Okay.
I'm going to draw the shit.
You know what?

443
00:27:37.411 --> 00:27:40.320
I'm going to show you guys a visualization that second.
Let,
let me just,

444
00:27:40.920 --> 00:27:41.753
<v 0>uh,</v>

445
00:27:42.820 --> 00:27:43.653
<v 1>uh,
did you,
uh,</v>

446
00:27:43.730 --> 00:27:46.730
I'm going to push it all to get hub and you're going to be able to watch this

447
00:27:46.730 --> 00:27:49.010
later.
So just follow along or now be interactive.

448
00:27:49.011 --> 00:27:52.340
I'm going to watch what you're saying and so,
okay,
so here we go.
So that's it.

449
00:27:52.430 --> 00:27:56.000
We built our neural network.
That's all.
Okay,
we're done now we're done with that.

450
00:27:56.180 --> 00:27:58.310
Now we're going to generate our actual sequences.
Okay.

451
00:27:58.670 --> 00:28:03.490
So generates cities.
Okay.
We're at that part,
right?

452
00:28:03.500 --> 00:28:08.500
That generate cities.
Generates cities.
Okay.

453
00:28:09.580 --> 00:28:10.940
So here we go.
We're going to generate six.

454
00:28:10.941 --> 00:28:15.090
We're going to call this generated file.
M um,
right.
We're good.

455
00:28:15.410 --> 00:28:20.290
Other January fall Amagansett Tsr in dot sequence generate CFR has a sequence

456
00:28:20.291 --> 00:28:25.150
generator for us.
It's made to generate the sequence,
the sports [inaudible].

457
00:28:25.590 --> 00:28:28.820
Okay.
Well,
okay,
good to know.
Um,

458
00:28:29.860 --> 00:28:30.910
you tried is not frozen.

459
00:28:31.690 --> 00:28:32.523
<v 0>Uh,</v>

460
00:28:34.520 --> 00:28:38.480
<v 1>is it a regression problem or in classification problem?
This is a,</v>

461
00:28:38.720 --> 00:28:42.500
this is a regression problem that uses classification early on.

462
00:28:42.680 --> 00:28:47.000
The end result isn't classification is using classification early on to jet to

463
00:28:47.150 --> 00:28:51.910
classify two is classifying words.
His doctor's first.
Okay,
so secret generated.

464
00:28:54.340 --> 00:28:58.760
Why is g um,
wait,
what is important?

465
00:28:58.880 --> 00:29:03.170
Uh,
I'm going to apply at normal hours to tech summarization.

466
00:29:03.470 --> 00:29:07.370
Uh,
why is g interpreted as five separate layers when it's one variable being

467
00:29:07.371 --> 00:29:11.460
overwritten?
Uh,
uh,
because it's,

468
00:29:12.710 --> 00:29:16.160
uh,
no,
well,
it,

469
00:29:16.190 --> 00:29:18.590
we're adding layers to what already exists.

470
00:29:18.890 --> 00:29:23.230
Is there a list of commands NTF learn?
Yes,
it's on the get hub.
Okay.
So,

471
00:29:24.380 --> 00:29:24.591
okay.

472
00:29:24.591 --> 00:29:29.591
So g dictionary equal to tar IDX and then sequence.

473
00:29:30.620 --> 00:29:35.210
Okay.
And then,
uh,
sequence,

474
00:29:36.330 --> 00:29:40.940
uh,
Max length or size.
Nice.
Nice.
Nice line.
No,

475
00:29:41.020 --> 00:29:44.530
not over it and had it too.
It's exactly,
um,

476
00:29:44.620 --> 00:29:49.620
quipped gradiance equals 5.0 I'm going to explain this in a second.

477
00:29:49.661 --> 00:29:51.850
Let me just write this out.
Uh,

478
00:29:52.210 --> 00:29:57.190
not a lot of space in the brain for a lot.
There we go.
Okay.

479
00:29:57.940 --> 00:30:02.290
I can to generate new sequences.
I am,
uh,
Ahmed.
Yes.

480
00:30:02.320 --> 00:30:05.680
How big is your source data?
It's about 20 megabytes.

481
00:30:05.681 --> 00:30:10.450
It's just a list of every us city.
Um,
and uh,

482
00:30:11.080 --> 00:30:15.340
let's see.
Um,
okay,
so,

483
00:30:15.400 --> 00:30:16.360
so what this is doing,

484
00:30:16.420 --> 00:30:19.540
it's going to say generally those sequences put them in our dictionary.

485
00:30:19.640 --> 00:30:24.070
The Max length of 20 a clip,
the gradients at 5.0 that's just a standard number.

486
00:30:24.071 --> 00:30:25.840
We're just going to keep their,
it doesn't matter that much.

487
00:30:26.080 --> 00:30:28.360
And then the checkpoint path model us cities.

488
00:30:28.420 --> 00:30:30.730
So this is going to save checkpoints as be trained,

489
00:30:30.760 --> 00:30:33.850
which means like if something happens to my computer,
like you know,

490
00:30:34.030 --> 00:30:36.280
I download some porn and it destroys everything.

491
00:30:36.340 --> 00:30:38.560
It's going to have those checkpoints save.
So the model,

492
00:30:38.561 --> 00:30:39.760
next time I started training,

493
00:30:40.150 --> 00:30:44.310
it's going to go from that build it from scratch.
And Rico,

494
00:30:44.320 --> 00:30:48.640
that would be so long,
but I will,
I will do that later.
Okay.

495
00:30:48.641 --> 00:30:50.920
So here's our last part,
the training part.
Okay.
That's it.

496
00:30:50.950 --> 00:30:54.850
We have five more lines guys.
We have five more lines.
Okay.
Stick with me here.

497
00:30:54.910 --> 00:30:58.420
So for I in range 40.
Okay.
So now as a training partner,

498
00:30:58.840 --> 00:31:00.550
we're going to say courtesy.

499
00:31:00.550 --> 00:31:04.480
I'm gonna explain what a seed is and it's like the seed random sequence from

500
00:31:06.250 --> 00:31:11.170
text file.
Um,
half Max.

501
00:31:11.590 --> 00:31:14.470
Okay.
I see.
What does a seed do at seed?

502
00:31:14.560 --> 00:31:18.220
Helps us start from the same point every time when we generate new things.

503
00:31:18.370 --> 00:31:21.250
So when we were about to generate a bunch of city names,
right?

504
00:31:21.490 --> 00:31:24.310
And so if we start from the same point,
that's what a seed does.

505
00:31:24.490 --> 00:31:28.340
And let's just start from the same point so that we have a little bit more.
Um,

506
00:31:29.260 --> 00:31:30.220
we have a little bit more

507
00:31:34.330 --> 00:31:37.720
names are more similar than they would than they would otherwise be.
Okay.

508
00:31:38.350 --> 00:31:39.350
Let me just read this.
March.

509
00:31:40.360 --> 00:31:43.750
Could you please do a new line of TF and fully connected?
Yes,
the call.
Thank you.

510
00:31:44.080 --> 00:31:46.720
Okay.
Okay.
So that's our seat.

511
00:31:46.840 --> 00:31:50.950
And now we're going to fit our model are generated cities model.
This is the,

512
00:31:50.951 --> 00:31:53.290
this is,
this is the learning part.
Okay.
So,

513
00:31:53.460 --> 00:31:56.860
so I'm going to take this three inputs.
I'm going to say x,
our input data,

514
00:31:56.890 --> 00:32:01.890
why is what we've created with our vectors or validation set is a culture 0.1

515
00:32:03.070 --> 00:32:07.630
batch size of 128.
So now in defining how much we're going to train,

516
00:32:07.631 --> 00:32:12.070
I'm just going to say 128 for batch size.
Um,
how many POCs do I want?

517
00:32:12.071 --> 00:32:16.790
And he pockets like an episode.
How many episodes run ID is,
uh,

518
00:32:17.010 --> 00:32:20.770
what are we going to call this thing?
Or Can I call it [inaudible]?
Okay,
this is,

519
00:32:20.800 --> 00:32:25.330
that's the train.
The fit is the train.
Okay.
And so now,

520
00:32:26.350 --> 00:32:29.350
so now we're going to print,
uh,

521
00:32:30.470 --> 00:32:32.510
he testing.

522
00:32:32.550 --> 00:32:35.440
We're going to say the casting because we're going to create this terminal and

523
00:32:35.441 --> 00:32:39.380
we're going to say,
um,
print,
uh,
else.

524
00:32:39.381 --> 00:32:44.330
We're going to say print test,
print.
I'm gonna print the generated.

525
00:32:44.360 --> 00:32:49.360
We're going to put the generator to generate 30 temperature equals 1.2,

526
00:32:51.750 --> 00:32:55.970
um,
sequence scene.
And then this we created.

527
00:32:56.600 --> 00:33:00.900
Um,
okay.
So that's it.
And maybe we can do this a few times.

528
00:33:00.960 --> 00:33:03.600
So then with different directors,
what is temperatures?
Meaning?

529
00:33:03.620 --> 00:33:08.500
I'll pretend I have a second.
Okay.
So this time it's with a temperature of 1.0

530
00:33:11.280 --> 00:33:13.770
I 40.
Um,
you could do more than 40.

531
00:33:13.771 --> 00:33:16.410
I just don't have like a huge GPU,

532
00:33:16.411 --> 00:33:19.890
so I'm just like 40 is good enough for right now.

533
00:33:20.280 --> 00:33:24.840
I don't feel like training it for longer.
Okay.

534
00:33:24.900 --> 00:33:29.670
So then that's it.
Okay.
That's it.
Let's see.
Um,
okay.
The end up fit line.
Okay.

535
00:33:32.540 --> 00:33:37.070
Okay.
And a fill line.
Um,
so endorphin basically is,
uh,

536
00:33:37.290 --> 00:33:38.280
taking our,

537
00:33:38.700 --> 00:33:42.900
what we've generated from sequence it from our sequence generator at that model.

538
00:33:42.940 --> 00:33:45.870
Okay.
And model.
And it is,
it is,

539
00:33:46.370 --> 00:33:50.880
it's taking the training data and it's adding into our LSTM.
So that's,

540
00:33:50.881 --> 00:33:55.500
that's the actual training bit.
Do you make enough money out of Youtube?
No,

541
00:33:55.530 --> 00:33:58.750
the answer is no,
but I won't be bought out unless you're deep mind,

542
00:33:58.860 --> 00:34:03.570
so it's all good.
Um,
how did you choose the batch size?

543
00:34:03.810 --> 00:34:08.350
It is in relation to the dimensions or know of samples of data.
Uh,

544
00:34:08.490 --> 00:34:12.300
it's not related.
I just randomly chose one 28.
Shouldn't it be G.
Dot.
Fit.

545
00:34:12.330 --> 00:34:16.950
What's m?
So that's a good question.
So Emma's Emma's the,
the,

546
00:34:17.190 --> 00:34:21.990
um,
model.
But,
um,
so there's,

547
00:34:22.030 --> 00:34:24.420
there's a little bit of magic happening here.
As you can see,

548
00:34:24.421 --> 00:34:26.340
like those two things aren't connected,

549
00:34:26.341 --> 00:34:29.820
but TF learn knows that they're connected.
Um,
and

550
00:34:31.390 --> 00:34:35.410
yeah,
so let's just,
let's just train this.
Um,
okay.

551
00:34:35.411 --> 00:34:39.550
Let's see what happens when we let me for now.
Command line.

552
00:34:40.380 --> 00:34:43.730
Uh,
okay.
Where are we?
Where are we at?
I thought on,

553
00:34:44.380 --> 00:34:48.820
let me make this bigger.
Okay.
Thank you.

554
00:34:48.821 --> 00:34:49.930
Big As hell.

555
00:34:51.860 --> 00:34:52.693
<v 2>Okay.</v>

556
00:34:56.920 --> 00:35:00.640
<v 1>Oh my God,
I have scenarios.
Okay,
here we go.</v>

557
00:35:04.720 --> 00:35:09.370
Okay.
Oh my God.
Okay.

558
00:35:10.140 --> 00:35:10.973
<v 2>Hm.</v>

559
00:35:13.330 --> 00:35:17.350
<v 1>Um,
uh,
okay.</v>

560
00:35:17.351 --> 00:35:19.750
So let me answer some more questions.
Uh,

561
00:35:21.800 --> 00:35:24.620
how does I do and I,
if you don't have to follow,
you don't have the library.

562
00:35:24.621 --> 00:35:25.610
How do you do a,

563
00:35:25.611 --> 00:35:29.960
you have to download the file and then what is the loss function?

564
00:35:29.970 --> 00:35:32.910
So loss function helps us,
um,

565
00:35:33.980 --> 00:35:37.040
the loss function will help us.
It's something we need to minimize.

566
00:35:37.230 --> 00:35:40.710
We train the loss function is something we need to minimize as we train.

567
00:35:41.160 --> 00:35:46.160
And it's basically a way for us to measure how good are our learning algorithm

568
00:35:47.941 --> 00:35:52.380
is doing.
Okay.
So I'm about to compile this thing.
Um,

569
00:35:52.410 --> 00:35:57.410
but I'm running out of time so I'm going to see what is this era.

570
00:35:57.601 --> 00:35:59.270
Let's see.
Um,

571
00:35:59.640 --> 00:36:03.210
this is going to be a lot of shit because the problem is that this tax was so

572
00:36:03.211 --> 00:36:04.890
big that I didn't,

573
00:36:04.950 --> 00:36:08.960
I should have been compiling and checking for errors as I was going.
Um,

574
00:36:09.150 --> 00:36:13.050
but it's okay.
It's okay.
I'm just double checking is,
does anyone have a link?

575
00:36:13.051 --> 00:36:16.170
I'm going to have a link to this video when it is done.
Um,

576
00:36:16.230 --> 00:36:21.230
and right now I am going to print out what's happening here.

577
00:36:21.931 --> 00:36:26.450
Let me go ahead and,
let's see.

578
00:36:26.810 --> 00:36:29.220
No,
I don't want to purchase that shit.
Okay.
So,
all right.

579
00:36:35.260 --> 00:36:39.590
Okay.
Training.
Where is that happening?

580
00:36:39.591 --> 00:36:42.170
Line 33 don't need that.

581
00:36:43.520 --> 00:36:44.353
<v 2>Okay.</v>

582
00:36:48.040 --> 00:36:52.690
<v 1>No module named Gfr.
I fucking kids.
Okay.
I know.
I don't,</v>

583
00:36:52.691 --> 00:36:57.690
I don't register it.
Uh,
yes,
I'm Indian and American both,
uh,

584
00:36:58.150 --> 00:37:01.950
born Duis.
Parents from India.
Uh,
no module Natgeo fine.
Okay.

585
00:37:04.990 --> 00:37:09.010
Give me a second here.
Um,

586
00:37:12.670 --> 00:37:13.503
<v 2>hold on.</v>

587
00:37:18.200 --> 00:37:22.190
<v 1>Okay,
so now it's,
now it's uh,
it's working.
Okay,</v>

588
00:37:22.400 --> 00:37:26.110
so it's training on that data.
What's happening?

589
00:37:26.320 --> 00:37:31.210
It's training on that data and you can see that there's a bunch of like a text

590
00:37:31.211 --> 00:37:34.870
here that's printing out like vectorizing texts,
texts,
total length,

591
00:37:34.900 --> 00:37:39.480
distinct chars total sequences that we didn't cope when we never printed this

592
00:37:39.481 --> 00:37:43.800
stuff.
We never did that.
Okay.
That's because TF learn,
uh,

593
00:37:43.830 --> 00:37:48.010
is doing this itself.
This is a lot of magic happening with CF warmth like these,

594
00:37:48.011 --> 00:37:51.400
these functions like sequence generator,
right?
Sequenced generator.

595
00:37:51.430 --> 00:37:55.960
And then random sequence from text file is a very high level functions.
Okay.

596
00:37:55.990 --> 00:37:56.823
Bigger font.

597
00:37:57.770 --> 00:37:58.603
<v 2>Okay.</v>

598
00:37:59.320 --> 00:38:01.090
<v 1>Hi.
So this is going to take pro.</v>

599
00:38:01.190 --> 00:38:06.110
So let me [inaudible] this is probably going to take like,
uh,
probably,

600
00:38:07.000 --> 00:38:10.070
uh,
20 minutes to fully trained with,
we don't have time for,

601
00:38:10.130 --> 00:38:14.930
so I went ahead and I'm going to pause that and it's,
so here's,

602
00:38:14.931 --> 00:38:17.720
here's an example.
I compile this beforehand.
Okay.

603
00:38:17.780 --> 00:38:21.260
So you can see you like what the temperature is that we row what's happening.

604
00:38:21.470 --> 00:38:25.190
These are new cities that are,
that have been generated.
They don't exist.

605
00:38:25.370 --> 00:38:27.110
They've been generated from the cities that we already have,

606
00:38:27.111 --> 00:38:31.390
like more wood and you know,
CNN.
So rocks.
Okay.

607
00:38:31.420 --> 00:38:34.760
So stuff like that.
And if you want to see the Dataset that we use for this,

608
00:38:34.850 --> 00:38:38.020
let me just take that thing and just going to copy that Dataset.

609
00:38:40.140 --> 00:38:42.440
Yes.
I'm going to ask me anything.
Yeah,
for sure.
Give me a second.

610
00:38:42.441 --> 00:38:46.860
Let me just take this data set and then printed and I'm going to put it into,

611
00:38:47.130 --> 00:38:50.700
um,
my browsers.
Do you guys and see what it looks like.

612
00:38:51.120 --> 00:38:55.830
I'm going to paste it in and I'm going to put it right here.

613
00:38:56.070 --> 00:39:00.000
Okay,
so take a look at this.
This is the Dataset.
It's huge.

614
00:39:00.030 --> 00:39:04.170
It's every single us city.
Okay,
so train on these words.
Okay.

615
00:39:04.171 --> 00:39:08.970
So that's about 20 megabytes.
All right,
so now that's,
that's that.

616
00:39:08.971 --> 00:39:12.630
That's what we have time for today.
I want to keep doing these live streams.

617
00:39:12.660 --> 00:39:15.370
I'm going to do a five minute ama before ending.

618
00:39:15.371 --> 00:39:20.160
So let me just stop screen sharing and go to full screen for a second.

619
00:39:20.430 --> 00:39:24.210
Okay.
I'm going to put this down here,
so see what's going on.
All right.

620
00:39:24.390 --> 00:39:28.080
And I'm going to answer a five minute questions.
Okay.

621
00:39:28.710 --> 00:39:33.360
Any questions?
Okay.
Okay.
Here we go.
Five minutes.
Dope.
Um,

622
00:39:34.950 --> 00:39:38.940
I um,
I want to do a good one too with windows.

623
00:39:38.941 --> 00:39:40.080
I'm scared of losing data.

624
00:39:40.150 --> 00:39:44.010
Use parallels then parallels is great for that parallels download parallels.

625
00:39:45.030 --> 00:39:49.740
Thank you.
What exactly is an epoch?
Epoch is like an episode of training.

626
00:39:50.160 --> 00:39:53.820
So you have like 20 epochs.
That means like 20 episodes of training.

627
00:39:54.720 --> 00:39:56.940
How many hidden nodes there were five,
12 in this one.

628
00:39:57.180 --> 00:40:01.520
Draw the neural network created yet.
Um,
meet you.
Then second,
did I,

629
00:40:01.590 --> 00:40:03.900
did I study a university?
Yeah,
I went to Columbia University.

630
00:40:03.901 --> 00:40:07.140
I studied computer science there.
Whereas you start learning machine learning,

631
00:40:07.141 --> 00:40:08.220
my channel started under,

632
00:40:08.221 --> 00:40:10.650
she only for hackers one go through every single video up til now.

633
00:40:10.890 --> 00:40:13.200
How should I start learning?
Just that,
my video.

634
00:40:13.440 --> 00:40:14.273
<v 0>MMM.</v>

635
00:40:16.910 --> 00:40:19.400
<v 1>Okay.
So for movie lens data for recommendation engine,</v>

636
00:40:19.430 --> 00:40:22.460
I would recommend using tensorflow a deep,

637
00:40:22.490 --> 00:40:24.440
you need a deep neural network.

638
00:40:24.530 --> 00:40:28.160
There's a type of recommendation system called a collaborative filtering system.

639
00:40:28.310 --> 00:40:32.390
If you go to git hub and you type in TF,
Dash,
r e c.
O.
M.
M,

640
00:40:32.590 --> 00:40:35.890
there's a great library for that.
I'm an image recognition application.

641
00:40:35.910 --> 00:40:40.550
See my video called Korean image classifier and intentional flow in five
minutes.

642
00:40:40.551 --> 00:40:43.730
I did that
with Tia and you can,

643
00:40:43.760 --> 00:40:47.330
you can specify the CPU or Gpu using one of the perimeters.

644
00:40:47.331 --> 00:40:50.780
There's a method for that.
If you search CPU or Gpu,
tensorflow and pupil,

645
00:40:50.870 --> 00:40:52.610
it's going to be the second link on stack overflow.

646
00:40:52.611 --> 00:40:55.670
I take any books you can recommend for a python newbie.

647
00:40:55.700 --> 00:41:00.700
It's called learn python the hard way it's called learn python the hard way.

648
00:41:01.580 --> 00:41:05.360
It's the best python book you could read.
Okay.
Uh,

649
00:41:05.390 --> 00:41:09.080
worth area to write your thesis on tensorflow.
I would say,
uh,

650
00:41:10.490 --> 00:41:14.260
what's hot right now?
Probably.
Hmm.

651
00:41:14.670 --> 00:41:17.040
A one shot learning with tensorflow.

652
00:41:17.130 --> 00:41:20.460
One shot learning and I have a video on that and see search my channel one shot

653
00:41:20.461 --> 00:41:21.294
learning.

654
00:41:21.680 --> 00:41:22.513
<v 0>MMM.</v>

655
00:41:24.360 --> 00:41:27.060
<v 1>Let's see.
Implement data compression and noise reduction.</v>

656
00:41:27.090 --> 00:41:28.770
You can do that using an auto encoder.

657
00:41:28.800 --> 00:41:31.380
I have a video on that called build an auto encoder in five minutes.

658
00:41:31.620 --> 00:41:34.100
Can you do video classification model?
Carlos,

659
00:41:34.101 --> 00:41:36.970
that is a great idea and I don't think I've done that yet.
So that,

660
00:41:37.040 --> 00:41:41.140
that'd be cool.
How cool would it be to generate video?
Like random videos?

661
00:41:41.141 --> 00:41:44.410
So you train it on a movie and then they generates new seats.

662
00:41:44.730 --> 00:41:48.230
I that didn't exist.
So like real life scene,
but then it's how cool.

663
00:41:48.231 --> 00:41:51.980
That would be amazing.
Right?
Do I do youtube full time?
Yes,
I'm a fulltime utuber.

664
00:41:52.250 --> 00:41:56.750
Okay.
Um,
you're really interested in theoretical cs.

665
00:41:57.160 --> 00:42:00.220
Um,
I'm not sure if I get him mathematics or computer science.

666
00:42:00.260 --> 00:42:01.700
Go for computer science that way.

667
00:42:01.701 --> 00:42:05.510
You wanted the applied mathematics for computer science.
Hi Omar.
Love you.

668
00:42:05.511 --> 00:42:10.400
Thank you.
Uh,
how would you do it with cans?
Yes.

669
00:42:10.610 --> 00:42:12.920
You're Schwann.
Yes,
I'm India.
How do you do?

670
00:42:19.350 --> 00:42:23.150
UN?
Okay.

671
00:42:24.410 --> 00:42:25.243
Um,

672
00:42:28.360 --> 00:42:30.910
anyone can be a data scientist.
That's when new series is about.

673
00:42:30.940 --> 00:42:32.200
And because you guys are here live,

674
00:42:32.201 --> 00:42:35.640
I'm going to tell you what my next video is about.
I'm still writing descriptive,

675
00:42:35.650 --> 00:42:38.130
but it's coming out in two days.
It's called a,

676
00:42:38.150 --> 00:42:40.840
it's using Twitter for sentiment analysis.

677
00:42:40.900 --> 00:42:43.590
So we're going to be mining Twitter data and we're going to do,

678
00:42:43.591 --> 00:42:47.830
do sentiment analysis that I know my hair is less crazy today.
Right.
That's cool.

679
00:42:47.831 --> 00:42:52.810
That's crazy.
Okay.
Why are Indians good at cs?
I,
you know what it is?

680
00:42:52.811 --> 00:42:57.760
I think it's just like your parents just growing up or just like study,
study,

681
00:42:57.761 --> 00:43:01.780
study,
education,
education and,
and make money.

682
00:43:02.050 --> 00:43:05.080
And that's all like computer science.
Really.
Um,

683
00:43:07.690 --> 00:43:10.550
uh,
you're amazing George.
Sure.

684
00:43:10.570 --> 00:43:13.440
I learned python if I already know job when people as plus Mr Benitez.

685
00:43:13.480 --> 00:43:17.760
Absolutely.
Yeah.
Thank you John.
Uh,

686
00:43:18.080 --> 00:43:22.910
you Udacity do the machine learning nanodegree not that data.

687
00:43:22.910 --> 00:43:26.300
Now panel this one.
Exactly.
It's easy to learn if,
you know,

688
00:43:27.050 --> 00:43:31.670
can we make a peer to peer in neural network?
Yo,
I never even thought about that.

689
00:43:31.700 --> 00:43:35.370
The answer to life.
That's a,
what would that even look like?
That's that's,

690
00:43:35.390 --> 00:43:39.800
that's a good question.
Um,
anyway,

691
00:43:40.910 --> 00:43:44.240
America,
people are lazy.
Well,
we're,
we're not lazy.
I mean,

692
00:43:44.241 --> 00:43:45.650
we're going to Mars in 10 years.

693
00:43:45.651 --> 00:43:49.130
We're going to have a million person calling in 40 years and it's going to be

694
00:43:49.131 --> 00:43:52.210
the whole world.
So we're all going to do it.
Okay.
We'll build a new,

695
00:43:52.280 --> 00:43:55.100
we'll go to the new country and on Mars.
It's gonna be crazy.
Anyway,

696
00:43:55.130 --> 00:43:58.940
I'm going off topic.
What are the spikes behind you?
I'm in a sound room.
Okay.

697
00:43:58.941 --> 00:44:02.780
I'm going to sound rude and this is good for a blocking sound.

698
00:44:02.781 --> 00:44:06.380
I'm working out of a coworking space in San Francisco.
Thanks Karen.

699
00:44:07.010 --> 00:44:10.130
I love Toronto.
Okay,
so that's all for today.
I'm going to go,

700
00:44:10.131 --> 00:44:14.600
I'm going to stop the broadcast.
Um,
uh,
and so one more question.

701
00:44:14.840 --> 00:44:18.410
Did you participate in Kaggle?
I have participant in Kaggle.

702
00:44:18.411 --> 00:44:23.300
Kaggle is a great source to become better at data science.
Look at past projects,

703
00:44:23.301 --> 00:44:25.760
looking at what they did and try to try to

704
00:44:27.680 --> 00:44:30.470
replicate the results.
Me and Ilan.
Yeah,
I'm actually,

705
00:44:30.471 --> 00:44:35.471
I might be meeting you on this week because Greg Brockman the CTO at open AI

706
00:44:35.940 --> 00:44:39.750
said he wants me to come in to talk and so like you aren't comes in once a week.

707
00:44:39.751 --> 00:44:43.110
I know that.
So like there's a chance that I'd be like,
Yo,
Yo Greg,

708
00:44:43.111 --> 00:44:47.190
you want to intro meeting Eli?
There's other,
you know what I mean?
So we'll see.

709
00:44:47.310 --> 00:44:51.660
Anyway,
thank you so much,
guys.
Okay.
All right.
Uh,
I'll do another one soon.

710
00:44:52.260 --> 00:44:56.100
I don't know when,
but thank you so much for watching.
For now.

711
00:44:56.130 --> 00:44:59.910
I've got to go drink some coffee,
so thanks for watching.

