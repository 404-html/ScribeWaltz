WEBVTT

1
00:00:00.060 --> 00:00:04.470
Hello world it Saroj and neuro ordinary differential equations.

2
00:00:04.620 --> 00:00:07.050
I know that sounds complicated.
Don't go anywhere,

3
00:00:07.230 --> 00:00:09.810
but it's a very important to idea.

4
00:00:09.840 --> 00:00:14.280
And guess what the biggest Ai Conference of the year is called neuro trips.

5
00:00:14.310 --> 00:00:19.310
It just happened last month in December of 2018 over 4,854 research papers were

6
00:00:21.091 --> 00:00:23.580
submitted to nerves.
That's a lot of research papers.

7
00:00:23.581 --> 00:00:27.480
And this paper that I'm about to explain to you in detail right now,

8
00:00:27.510 --> 00:00:28.950
won best paper.

9
00:00:29.130 --> 00:00:33.660
It was one of four that won the best paper out of over 4,854 research papers,

10
00:00:33.661 --> 00:00:37.590
people from Nvidia,
researchers from Google,
researchers from Facebook,

11
00:00:37.591 --> 00:00:40.560
all of these big tech companies submitted their best work.

12
00:00:40.561 --> 00:00:43.500
And this paper was one of four.
That one the best.

13
00:00:43.501 --> 00:00:46.800
And I'm gonna explain to you how it works in this video.
Now,

14
00:00:46.801 --> 00:00:51.090
why should you even care?
This is a demo of the,
of the,
of the paper behind me,

15
00:00:51.091 --> 00:00:54.360
by the way.
So you can see,
there's a lot for me to explain here,

16
00:00:54.690 --> 00:00:56.340
but why should you care?
Let's start with that.

17
00:00:56.550 --> 00:01:01.260
This idea can be applied to time series data.
What is time series data?

18
00:01:01.261 --> 00:01:04.440
Any kind of any type of Dataset that depends on time.

19
00:01:04.441 --> 00:01:06.870
So what would be an example?
Finance data,
right?

20
00:01:06.871 --> 00:01:09.660
How do stock prices change over time?
Health care data?

21
00:01:09.661 --> 00:01:12.900
How do patients biometric signals change over time?

22
00:01:13.140 --> 00:01:16.680
Anything that changes over time,
that is,
can be considered time series.

23
00:01:16.681 --> 00:01:18.690
Data time is an important element,
right?

24
00:01:19.350 --> 00:01:22.590
This technique can predict,
uh,

25
00:01:23.010 --> 00:01:26.100
can make predictions on time series data better.

26
00:01:26.310 --> 00:01:28.560
Then recurrent neural networks,

27
00:01:28.561 --> 00:01:32.640
which is kind of the defacto go to type of network to make predictions with time

28
00:01:32.641 --> 00:01:37.050
series data,
including including all variants of recurrent neural networks,

29
00:01:37.500 --> 00:01:41.250
including residual networks.
There's,
there's so much,
there's so much theory.

30
00:01:41.251 --> 00:01:45.240
They're dense networks,
highway networks.
Um,
there's a lot,

31
00:01:45.660 --> 00:01:48.480
but we have something to focus on in this video.

32
00:01:48.510 --> 00:01:52.140
It's called neuro ordinary differential equations.
Okay,

33
00:01:52.320 --> 00:01:56.010
so let's get started with this.
What even happened here,
right?

34
00:01:56.011 --> 00:01:59.670
So of all these papers,
this is the one that got the best paper award.

35
00:01:59.850 --> 00:02:02.790
So what is the basic idea before we talk about calculus?

36
00:02:02.791 --> 00:02:04.920
That's really what I'm hyped to talk about today.

37
00:02:04.950 --> 00:02:09.750
Calculus because this is using integral calculus.
Instead of differentiating,

38
00:02:09.930 --> 00:02:13.140
we're going to integrate.
If you don't know calculus,
don't worry.

39
00:02:13.141 --> 00:02:17.970
I'm going to try to put that prerequisite knowledge into this video as well,

40
00:02:18.060 --> 00:02:21.780
because I want this video to be for everybody,
not just for researchers,

41
00:02:21.960 --> 00:02:25.440
not just for AI enthusiasts,
but for everybody.
Even if you've not coded before.

42
00:02:25.441 --> 00:02:29.700
So let's see how,
how,
how,
well,
I can do this.
Okay,
so here,
here we go.

43
00:02:29.970 --> 00:02:34.860
So here's the basic idea.
Neural networks are function approximators.

44
00:02:34.890 --> 00:02:38.790
Neural networks are a series of matrix operations.
So there's,

45
00:02:38.820 --> 00:02:41.670
there are a series of operations,
right?
Like add,
subtract,
multiply,

46
00:02:41.671 --> 00:02:45.390
divide that we apply to matrices.
Matrices are groups of numbers,
right?

47
00:02:45.391 --> 00:02:48.420
So we have an input matrices,
we apply these operations to it,

48
00:02:48.421 --> 00:02:52.500
and then we get an output.
And this output is the prediction.
And so what we,

49
00:02:52.501 --> 00:02:54.570
the way we think about these neural networks,

50
00:02:55.740 --> 00:02:57.210
the way this is a picture by the way,

51
00:02:57.330 --> 00:03:00.790
the way we think about these networks is we group them into what are called

52
00:03:00.791 --> 00:03:04.390
discreet layers.
So layers are in the network.

53
00:03:04.391 --> 00:03:06.160
They're just a block of operations.

54
00:03:06.280 --> 00:03:10.570
And so we can actually make a rap song out of this,
right?
So input times weight,

55
00:03:10.950 --> 00:03:15.940
add a bias,
activate repeat input times weight,
add a bias,

56
00:03:16.150 --> 00:03:19.750
activate repeat.
And so we just keep doing that over and over and over again.

57
00:03:19.780 --> 00:03:23.740
Input data times the weight Matrix,
add a bias,

58
00:03:23.800 --> 00:03:27.400
and then apply that to an activation function,
which could be several.

59
00:03:27.970 --> 00:03:30.490
And then we just repeat that over and over again.
And then we get the output,

60
00:03:30.491 --> 00:03:34.030
right?
So what this image shows is a neural network.
Okay?

61
00:03:34.031 --> 00:03:36.190
And it shows the Matrix operations that are occurring.

62
00:03:36.191 --> 00:03:39.100
These lines represent multiplication,

63
00:03:39.550 --> 00:03:42.790
operations apply to all of these numbers such that we get the next layer.

64
00:03:42.880 --> 00:03:46.840
So this idea of layers is very persistent in neural networks.

65
00:03:47.020 --> 00:03:49.560
Neural networks have layers,
right?
So that's,

66
00:03:49.561 --> 00:03:53.620
that's the basic idea of neural networks.
So what this paper does is it says,

67
00:03:53.621 --> 00:03:56.470
instead of having these be discreet layers,
that means,
you know,

68
00:03:56.471 --> 00:03:59.890
we have one layer and then we have a second layer rights individually

69
00:03:59.891 --> 00:04:04.540
independent grouped layers blocks,
right?
Just separate,
discreet.

70
00:04:04.780 --> 00:04:09.430
Instead of doing that,
let's consider a neural network as a continuous function.

71
00:04:09.880 --> 00:04:13.690
That is a function that is both infant test Mully big and infant testimony.

72
00:04:13.700 --> 00:04:17.380
Small,
right?
This is a continuous function,
right?
There's no grouping,

73
00:04:17.381 --> 00:04:19.960
there's no blocks.
It's just one function forever,

74
00:04:19.961 --> 00:04:22.240
within forever without rights in one direction.

75
00:04:22.241 --> 00:04:25.750
That's a little inside joke for people who follow my channel regularly

76
00:04:25.751 --> 00:04:27.820
subscribed,
by the way,
cause you're,
I promise you,

77
00:04:27.970 --> 00:04:30.940
you're not gonna be getting this kind of lecture anywhere else.
Subscribed.
Okay,

78
00:04:30.941 --> 00:04:33.460
so here we go.
That's what this paper does.
It,

79
00:04:33.470 --> 00:04:37.900
it groups neural networks into this category of continuous functions.

80
00:04:38.080 --> 00:04:39.640
So instead of having single layers,

81
00:04:40.120 --> 00:04:45.120
we just have the entire network be one continuous block of computation.

82
00:04:46.090 --> 00:04:50.830
And what this means is no more specifying the number of layers beforehand,
rate.

83
00:04:50.850 --> 00:04:51.041
You know,

84
00:04:51.041 --> 00:04:54.640
normally you have to decide how many layers do I want my neural network to have,

85
00:04:55.450 --> 00:04:59.980
right?
And once you do that,
then you can build the network.
But with this idea,

86
00:05:00.010 --> 00:05:03.280
instead of specifying the number of layers beforehand,

87
00:05:03.460 --> 00:05:08.460
we specified the desired accuracy and it will learn how to train itself within

88
00:05:09.011 --> 00:05:13.270
that margin of error.
Now,
this is still in the early stages.
By the way,
the team,

89
00:05:13.271 --> 00:05:16.890
it's at University of Toronto,
um,
at the vector institutes,

90
00:05:17.590 --> 00:05:22.090
they are still working on this and it's not perfect.
It is a research paper.

91
00:05:22.091 --> 00:05:23.620
It wasn't meant for production,

92
00:05:23.770 --> 00:05:27.100
but it is a very promising direction for 2019 for AI.

93
00:05:27.340 --> 00:05:30.870
And I think that all AI enthusiasts should read this paper or at least watching

94
00:05:30.871 --> 00:05:31.171
the video.

95
00:05:31.171 --> 00:05:34.330
So direct them to this video because this is going to be easier than reading the

96
00:05:34.331 --> 00:05:36.910
paper,
which was a,
I had to,

97
00:05:36.940 --> 00:05:41.560
I had to review quite a lot of integral calculus,
but that was a lot of fun.
Okay,

98
00:05:41.561 --> 00:05:45.160
so why does this matter?
Time series data is,
is why that matters.
Okay,

99
00:05:45.161 --> 00:05:46.330
so why does this matter?

100
00:05:46.990 --> 00:05:51.300
Because it means faster testing time than recurrent networks.

101
00:05:51.301 --> 00:05:53.890
So just faster neural networks.
If we do it this way,

102
00:05:54.010 --> 00:05:58.910
instead of considering our network as discrete blocks of discreet layers,

103
00:05:58.911 --> 00:06:02.960
we consider it as a continuous function.
And once we do that,

104
00:06:03.350 --> 00:06:06.390
we can have faster testing time.
So we're,

105
00:06:06.391 --> 00:06:09.440
we're making a trade off between precision and speed.

106
00:06:09.710 --> 00:06:14.630
It also means that we get more accurate as results for time series predictions.

107
00:06:14.660 --> 00:06:19.250
I he continuous time models,
what can be considered time series data.

108
00:06:19.251 --> 00:06:23.330
Tell me finance,
right?
How does,
how do stock prices change over time?

109
00:06:23.480 --> 00:06:27.410
Health Care,
how does,
how does the patient's biometric data change over time?

110
00:06:27.500 --> 00:06:30.980
How does what change over time?
All of that is time series data.

111
00:06:30.981 --> 00:06:34.550
And the thing about neural networks is that with these discrete layers,

112
00:06:34.670 --> 00:06:39.500
they expect the intervals for these,
for these time series data sets to be fixed,

113
00:06:39.620 --> 00:06:42.080
right?
So these are predictable,
fixed intervals.

114
00:06:42.081 --> 00:06:45.620
Every day we're taking a recording or every hour or every second.

115
00:06:45.980 --> 00:06:48.260
But that's not how the real world operates,
right?

116
00:06:48.261 --> 00:06:52.820
So one example would be healthcare patients and hospitals aren't always

117
00:06:52.821 --> 00:06:57.821
regularly recording a patient's biometric data every hour or every day.

118
00:06:57.831 --> 00:07:02.240
It's irregular.
And so neural networks are bad at predicting.

119
00:07:02.530 --> 00:07:02.930
Uh,

120
00:07:02.930 --> 00:07:07.640
the are are bad at predicting an output for time series data that is irregular.

121
00:07:07.790 --> 00:07:11.360
But this is a promising direction.
This ode network,

122
00:07:11.361 --> 00:07:14.750
which is ordinary differential equation network that these guys have invented.

123
00:07:15.050 --> 00:07:17.450
And one more point,
why else is this matter?

124
00:07:17.451 --> 00:07:18.920
So two points I've already talked about.

125
00:07:18.921 --> 00:07:22.760
Faster testing time and more accurate results for time series predictions.

126
00:07:22.761 --> 00:07:23.900
I he continuous time models.

127
00:07:23.901 --> 00:07:28.070
And the third point is this opens up a whole new realm of mathematics for

128
00:07:28.100 --> 00:07:29.870
optimizing neural networks,
right?

129
00:07:30.770 --> 00:07:34.640
If we frame it as what's called a differential equation,
which I'm gonna explain,

130
00:07:34.641 --> 00:07:36.500
don't worry if you don't get that,
which I'm gonna explain,

131
00:07:36.740 --> 00:07:41.030
if we frame it this way as a continuous differential equation,
then we could use,

132
00:07:41.031 --> 00:07:44.240
instead of,
you know,
gradient descent,
which were used to only,

133
00:07:44.450 --> 00:07:48.020
we can use an entirely new class of optimizations solvers,

134
00:07:48.230 --> 00:07:50.330
which are called differential equations solvers.

135
00:07:50.330 --> 00:07:53.780
And there's a hundred plus years of theory behind these that we haven't really

136
00:07:53.781 --> 00:07:56.840
applied to neural networks yet in a mainstream way.

137
00:07:57.050 --> 00:08:00.500
And so there's a lot of promise there.
Um,
and Oh,
lastly,

138
00:08:00.501 --> 00:08:03.590
we can compute gradients with a constant memory costs.

139
00:08:03.591 --> 00:08:05.450
So every time you're adding layers to your network,

140
00:08:05.690 --> 00:08:08.540
you are linearly increasing the amount of memory that it's using,
right?

141
00:08:08.750 --> 00:08:12.590
But with this idea,
you have a constant memory cost.
So in this video,

142
00:08:12.591 --> 00:08:15.850
the concept that we're going to cover our basic neural network theory,
rec,

143
00:08:15.890 --> 00:08:19.190
residual neural network theory,
ordinary differential equations,

144
00:08:19.490 --> 00:08:23.930
then ode networks,
Oilers method,
the ad joint method.
Um,

145
00:08:23.960 --> 00:08:28.750
and then we'll have a conclusion.
So that's a lot.
So sit down and enjoy.

146
00:08:28.770 --> 00:08:29.930
This is going to be amazing.
Here we go.

147
00:08:29.990 --> 00:08:33.500
So neural networks are popular types of machine learning models,
right?

148
00:08:33.710 --> 00:08:36.020
Neural networks are built with linear Algebra,

149
00:08:36.021 --> 00:08:40.160
meaning they are matrices with Matrix operations and they are optimized using

150
00:08:40.161 --> 00:08:43.880
calculus.
Calculus is the study of how things change over time.

151
00:08:43.881 --> 00:08:46.340
It's using physics a lot.
We'll get into this in a second.

152
00:08:46.880 --> 00:08:49.670
So neural networks consists of a series of layers which are just matrix

153
00:08:49.671 --> 00:08:54.590
operations and each layer introduces a little bit of error that compounds

154
00:08:54.710 --> 00:08:58.320
through the network as we propagate forward.
So that's one diagram.

155
00:08:58.321 --> 00:09:03.030
Now here is a more mathematically accurate diagram of what's happening in a

156
00:09:03.031 --> 00:09:07.680
neural network.
We have our input data.
This is a lot.
So we have our input data,

157
00:09:07.681 --> 00:09:09.960
right?
This would be an image,
video,
text,
whatever it is.

158
00:09:10.320 --> 00:09:14.400
And then what we do is we take that input data and we multiply it by ww is the

159
00:09:14.401 --> 00:09:18.210
weight matrix.
And so we take the input data,
we multiplied by our weight,

160
00:09:18.450 --> 00:09:22.740
we add a bias,
be see this be,
and then we activate.
Okay,

161
00:09:22.741 --> 00:09:25.830
so f is the activation function.
So,
so look right here where my,

162
00:09:25.831 --> 00:09:29.670
where my mouse is circling around,
we have our input data,

163
00:09:29.671 --> 00:09:33.570
which is p times the weight,
add the bias and then take that,

164
00:09:33.780 --> 00:09:38.160
the result of that,
the some of that and applies activation function to activate.

165
00:09:38.370 --> 00:09:40.020
And that is the output of the first layer.

166
00:09:40.021 --> 00:09:43.530
We take that output and we feed it into the next layer and we are going to do it

167
00:09:43.531 --> 00:09:44.490
again.
And we do it again.

168
00:09:44.550 --> 00:09:48.300
So we could think of a neural network as one giant composite function of

169
00:09:48.301 --> 00:09:51.840
functions within functions.
So the output is gonna be a function of a function,

170
00:09:51.841 --> 00:09:54.420
of a function,
of a function for as many layers as there are.

171
00:09:55.800 --> 00:10:00.000
And so in terms of code,
here is our activation function.

172
00:10:00.030 --> 00:10:01.290
It's a sigmoid function.

173
00:10:01.291 --> 00:10:04.590
And nonlinearity I have a million videos on how neural networks work.
So,

174
00:10:04.591 --> 00:10:08.580
I'm not going to go into that in detail right now,
but just basic idea.

175
00:10:08.730 --> 00:10:12.600
We have our input data,
we have our labels,

176
00:10:12.601 --> 00:10:15.990
we want to find the mapping between both.
And so for our training loop we do,

177
00:10:16.050 --> 00:10:19.500
we perform forward propagation input times,
wait,
alibis activates,

178
00:10:20.040 --> 00:10:24.270
and then we compute the partial derivative with of the error with respect to our

179
00:10:24.271 --> 00:10:26.130
weights and then propagate that backwards.

180
00:10:26.131 --> 00:10:31.080
That's backpropagation Google or Youtube Saroj backpropagation in five minutes

181
00:10:31.081 --> 00:10:34.410
to see how that works.
Okay,
now let's get into this.

182
00:10:34.680 --> 00:10:38.220
So the idea is that to reduce compounded error,

183
00:10:38.490 --> 00:10:41.040
let's just stack more layers,
right?
It's more layers,

184
00:10:41.490 --> 00:10:46.110
the more layers of deeper than network,
he deep learning.
And so,

185
00:10:46.170 --> 00:10:49.080
um,
is this your machine learning system?
Yes.

186
00:10:49.081 --> 00:10:52.260
You just pour the data into this big pile of linear Algebra than collect the

187
00:10:52.261 --> 00:10:54.780
answers on the other side.
What if the answers are wrong?

188
00:10:54.781 --> 00:10:59.781
Just sir the pile until they start looking writes that in essence is how deep

189
00:11:00.061 --> 00:11:02.340
learning research goes.
Let's be real everybody.

190
00:11:03.000 --> 00:11:08.000
So there's a lot of theory behind how many layers to at your network.

191
00:11:08.100 --> 00:11:12.120
But the basic idea is that the number of layers deeply affects the output of the

192
00:11:12.121 --> 00:11:14.370
network.
Too few could cause under fitting.

193
00:11:14.371 --> 00:11:18.030
So meaning your line won't fit the data too many and it could cause overfitting

194
00:11:18.031 --> 00:11:21.930
meaning your line will overfit the data.
Um,
but so you want to get adjust,
right?

195
00:11:21.931 --> 00:11:25.410
So the line perfectly fits your dataset.
So neural networks,
all of this stuff,

196
00:11:25.411 --> 00:11:29.880
it's glorified,
um,
line fitting to data in a way.

197
00:11:31.830 --> 00:11:34.860
So there's so many different rules of thumb for how many layers you should be

198
00:11:34.861 --> 00:11:37.920
choosing.
You know,
here are three bullet points.
Here are three examples.

199
00:11:39.090 --> 00:11:44.090
And so when we use a higher level library like care os to build a deeper

200
00:11:45.871 --> 00:11:47.010
network,
right?
So we're going to,

201
00:11:47.100 --> 00:11:50.550
let's say we'll build an LSTM network and care os right here.

202
00:11:50.730 --> 00:11:53.680
And so for each line,
each line represents a single layer.

203
00:11:53.800 --> 00:11:58.240
If we want it to make this network deeper,
then we can just say model dot had

204
00:11:59.890 --> 00:12:04.540
LSTM
and then 32 or however many

205
00:12:06.250 --> 00:12:07.540
dimensions we want it to have.

206
00:12:07.810 --> 00:12:11.860
And so we could just keep on stacking them like this stack,
stack,
stack,
stack,

207
00:12:11.861 --> 00:12:14.860
stack,
stack,
stack,
stack,
stack,
stack on to level up.
You see what I'm saying?

208
00:12:14.861 --> 00:12:18.580
We keep on stacking them,
uh,
if we want it to be deeper and deeper.

209
00:12:19.390 --> 00:12:20.170
<v 1>Okay.</v>

210
00:12:20.170 --> 00:12:21.280
<v 0>And so there's,
there's a,</v>

211
00:12:21.281 --> 00:12:24.280
there's a threshold here where adding more isn't gonna do anything.

212
00:12:25.570 --> 00:12:28.930
So solution to this was proposed by Microsoft,
by the way,
Microsoft,

213
00:12:28.931 --> 00:12:31.150
you guys are killing it.
Like let's go.
I'm,

214
00:12:31.160 --> 00:12:34.600
I'm proud of you guys for just being super open source and you know,

215
00:12:34.601 --> 00:12:38.650
onyx and all this stuff.
So keep going.
So in December,
2015,

216
00:12:38.651 --> 00:12:42.820
Microsoft proposes idea of,
of what are called residual networks.
Okay.

217
00:12:42.821 --> 00:12:47.260
And so they won the image net classification competition using these residual

218
00:12:47.261 --> 00:12:51.640
networks and residual networks had the best accuracy in the competition.

219
00:12:51.641 --> 00:12:54.550
So if we look at it,
be Google,
they beat everybody.
Um,
this is,

220
00:12:54.551 --> 00:12:57.040
these were the results.
And so this is Microsoft right here.

221
00:12:57.041 --> 00:13:01.270
And so the smaller bar is better because it means a lower error.
Okay.

222
00:13:01.271 --> 00:13:05.320
So resonates,
beat everything else.
And they had the best accuracy.

223
00:13:05.321 --> 00:13:09.040
And what they were able to do is they were able to train networks that we're up

224
00:13:09.041 --> 00:13:12.250
to a thousand layers deep while avoiding vanish and gradients,

225
00:13:12.251 --> 00:13:15.880
meaning avoiding that lower accuracy that occurs with deeper networks.

226
00:13:16.000 --> 00:13:20.500
And six months later their publication already had more than 200 references.

227
00:13:20.501 --> 00:13:24.580
That's a big deal.
So people really liked this idea.
And so the,
the,

228
00:13:24.581 --> 00:13:28.930
the way residual networks work very simple,
really very simple.

229
00:13:28.931 --> 00:13:32.170
Like it's just too simple.
You know how we,

230
00:13:32.190 --> 00:13:35.290
we just keep feeding the output of the previous layer,
the next layer.

231
00:13:36.040 --> 00:13:40.180
So the only change we're making for residual networks is we're t,

232
00:13:40.210 --> 00:13:42.400
we're not just feeding in the output of the previous layer,

233
00:13:42.401 --> 00:13:45.730
we're feeding in the output of the previous layer and the input of that layer.

234
00:13:45.731 --> 00:13:50.080
So we add that,
so f of x plus x,
and that's what we feed into the next layer.

235
00:13:50.530 --> 00:13:52.030
And then we do that again and again and again.

236
00:13:52.031 --> 00:13:55.930
And so what this does is it creates these skip connections or we're not just

237
00:13:55.931 --> 00:13:57.640
fitting in the output of the previous layer.

238
00:13:57.760 --> 00:14:01.720
We're also putting feeding in the input of the previous layer to the next layer.

239
00:14:02.380 --> 00:14:06.970
So network and decide how deep it wants to be.
So that's,

240
00:14:07.000 --> 00:14:09.910
so this would,
this is an example,
right?
So resonates.

241
00:14:10.970 --> 00:14:11.490
<v 1>Okay.</v>

242
00:14:11.490 --> 00:14:15.240
<v 0>Versus you know,
other types of networks.</v>

243
00:14:16.730 --> 00:14:21.680
So instead of hoping each stack of layers directly fits the underlying mapping,

244
00:14:21.860 --> 00:14:25.190
we can explicitly let these layers fit a residual mapping.

245
00:14:25.191 --> 00:14:29.600
So we cast it from of x two F of x Plus X.
Okay?

246
00:14:29.900 --> 00:14:32.290
So it's,
it's very simple like I said,
so,
uh,

247
00:14:32.450 --> 00:14:36.860
we add the output of the activation function,
which is f.
So x is the input.

248
00:14:36.861 --> 00:14:38.300
So x would be input to the network.

249
00:14:38.750 --> 00:14:43.100
F of x is going to be the activation function applied to x,

250
00:14:43.430 --> 00:14:46.540
and then we add x again.
And that's going to be the output for that.

251
00:14:46.820 --> 00:14:50.240
That's going to be the output and also input for the next layer.
Okay?

252
00:14:50.720 --> 00:14:53.540
So f is the function of the cake layer and it's activation.

253
00:14:53.541 --> 00:14:58.541
So this is the formula for a residual network right here.

254
00:14:58.850 --> 00:15:02.630
Okay?
This is what's going to act as the input to the next layer.

255
00:15:02.631 --> 00:15:05.480
Not just the output from the last layer,
but the input from the last layer.

256
00:15:05.480 --> 00:15:09.710
We add both.
That's it.
That's it.
Now,
here's the cool stuff.

257
00:15:10.400 --> 00:15:15.260
So what if we recast this equation by just adding in some constant,

258
00:15:15.500 --> 00:15:18.680
let's say the constant is one,
right?
There's already a constant one.
It just,

259
00:15:18.681 --> 00:15:21.440
we can't see it,
but let's just name it some letter.
Let's name an age,

260
00:15:21.441 --> 00:15:24.660
we can name it.
Anything.
Well,
we wait,
we could name it V,
we can emit cue.

261
00:15:24.850 --> 00:15:28.880
Let's call it h and let's say h equals one.
So we know that h equals one,

262
00:15:29.090 --> 00:15:31.730
and we'll say eight.
So we'll add it to our equation.

263
00:15:31.910 --> 00:15:34.760
So our original equation is x of,

264
00:15:35.220 --> 00:15:39.590
so x equals x plus F of x.
So we'll just add h in there.

265
00:15:39.591 --> 00:15:43.040
So it's x equals x plus h times f of x,
where H is one.

266
00:15:43.041 --> 00:15:46.160
So it doesn't really do anything.
It's just it's just there and it's still true.

267
00:15:46.161 --> 00:15:50.180
Now,
why did we do this?
Because if we cast it in this way,

268
00:15:50.480 --> 00:15:52.100
this equation right here,

269
00:15:52.130 --> 00:15:57.130
x equals x plus h times f of x is the formula for what's called Oilers method

270
00:15:58.610 --> 00:16:03.020
for solving ordinary differential equations when h equals one.

271
00:16:03.500 --> 00:16:07.160
So you might be thinking,
what are you talking about?
What is Oilers method?

272
00:16:07.340 --> 00:16:08.540
What are differential equations,

273
00:16:08.541 --> 00:16:12.560
and what do they have anything to do with neural networks?
So hold that thought.

274
00:16:12.980 --> 00:16:14.630
Let's go get some code first.
Okay.

275
00:16:14.631 --> 00:16:18.500
So this code right here is a normal layer.
Okay?

276
00:16:18.501 --> 00:16:22.100
This is a normal layer in a convolutional neural network meant men for image

277
00:16:22.101 --> 00:16:25.520
classification rights.
So we have a normalization,
um,

278
00:16:25.790 --> 00:16:30.140
operation activation,
convolution,
normalization,
activation,
convolution,

279
00:16:30.710 --> 00:16:34.820
right?
So let's modify that to be residual.

280
00:16:34.850 --> 00:16:37.520
What did we change?
We captured x,
the input,

281
00:16:37.521 --> 00:16:39.890
and we set it to Rez because we want to use that in a second.

282
00:16:40.250 --> 00:16:44.960
And so remember x is the,
is the input to this layer.

283
00:16:45.110 --> 00:16:47.600
And we're also,
when we,
when we give it to the next layer,

284
00:16:48.200 --> 00:16:49.280
here's the other difference.

285
00:16:49.550 --> 00:16:53.240
We're giving it the output of the activation function and the input.

286
00:16:53.540 --> 00:16:56.240
And that's what we feed to the next layer,
right?
So this is a different,

287
00:16:56.241 --> 00:16:59.240
these two lines right here.
Okay.
So back to the question.

288
00:16:59.241 --> 00:17:02.510
I just wanted to show you that what is the significance between residual

289
00:17:02.511 --> 00:17:05.930
networks and ordinary differential equations?
Okay,

290
00:17:06.380 --> 00:17:11.380
so a differential equation is an equation that tells us the slope without

291
00:17:11.721 --> 00:17:15.590
specifying the original function,
who's derivative we are taking.

292
00:17:15.920 --> 00:17:17.930
Let's just go into this.
Okay,
we've got some calculus.

293
00:17:18.350 --> 00:17:22.790
So if we have a function right,
with any kind of function sine cosine,

294
00:17:23.270 --> 00:17:28.250
x cubed plus seven x,
any function,
if we graph it,
if we graph that function,

295
00:17:28.251 --> 00:17:30.110
it could do this.
I can do this.
You can do this.

296
00:17:30.980 --> 00:17:35.980
If we wants to find the slope of the tangent line to any points on that

297
00:17:37.550 --> 00:17:42.050
function,
that slope is the derivative,
okay?

298
00:17:42.410 --> 00:17:47.300
The derivative of the function f of x evaluated at x equals a at any points,

299
00:17:47.480 --> 00:17:51.870
gives the slope of the curve at that point.
Why does this matter?

300
00:17:51.871 --> 00:17:53.940
Because if we derive a function at any point,

301
00:17:53.970 --> 00:17:57.540
it gives us a slope and that slope tells us the direction that that function is

302
00:17:57.541 --> 00:18:00.720
moving at that specific point in time.
Now,

303
00:18:00.750 --> 00:18:03.360
that's the derivative now.
So,
right.

304
00:18:03.361 --> 00:18:07.890
So any function we can compute a derivative of,
right?
So x squared,

305
00:18:07.950 --> 00:18:11.620
the derivative is two x because what do we do?
We take that,
we take the power to,

306
00:18:11.640 --> 00:18:14.740
we move into the coefficient and we subtract one,
uh,

307
00:18:14.880 --> 00:18:18.300
from that power and that gives us a derivative.
It's that simple.
So x cubed,

308
00:18:18.450 --> 00:18:22.410
three to the x,
and then we do three minus one is two,

309
00:18:22.411 --> 00:18:24.630
so three x square,
so x to the fourth.

310
00:18:24.660 --> 00:18:27.620
The derivative is don't look for execute.

311
00:18:28.010 --> 00:18:31.530
I'm not even gonna look now x to the fifth.
Five x to the fourth.
Okay,

312
00:18:31.560 --> 00:18:35.790
six to the x to the sixth,
six x to the fifth.
Okay.

313
00:18:35.970 --> 00:18:38.940
That's the derivative.
And we use a driven a lot in machine learning,
right?

314
00:18:38.941 --> 00:18:42.750
The derivative helps us compute the gradients that tells us how to update our

315
00:18:42.751 --> 00:18:43.584
network,

316
00:18:44.790 --> 00:18:49.200
but we don't use a lot or are integrals.
Now the integral is the,

317
00:18:49.210 --> 00:18:51.190
is the opposite of the derivative.

318
00:18:51.191 --> 00:18:55.590
So assume that we have the derivative and we want to find the original function.

319
00:18:55.890 --> 00:18:57.090
How do we do that?

320
00:18:57.270 --> 00:19:00.720
How do we go from the derivative back to the original function?
What we just do,

321
00:19:00.721 --> 00:19:05.010
the reverse,
we take that coefficient,
we put it in the,
in the power,
right?

322
00:19:05.011 --> 00:19:08.130
The exponent,
and that gives us,
and that's called integration.

323
00:19:08.131 --> 00:19:12.240
So taking a function,
finding the derivative is called differentiation,

324
00:19:12.360 --> 00:19:15.930
but taking the derivative and finding the original function is called

325
00:19:15.931 --> 00:19:17.910
integration,
where we're using the integral.

326
00:19:18.120 --> 00:19:20.370
And so this is a symbol for the integral right here.

327
00:19:22.410 --> 00:19:26.940
So differential equations,
okay.
Are there are two types.

328
00:19:26.941 --> 00:19:31.020
There's ordinary differential equations and there are partial differential

329
00:19:31.021 --> 00:19:31.854
equations.

330
00:19:32.280 --> 00:19:36.390
So ordinary differential equations involve one or more ordinary derivatives of

331
00:19:36.391 --> 00:19:39.570
unknown functions where it's partial differential equations evolve,

332
00:19:39.810 --> 00:19:42.300
one or more partial derivatives of unknown function.

333
00:19:42.301 --> 00:19:45.870
So the idea is that in a an ordinary differential equation,

334
00:19:45.871 --> 00:19:48.990
we are taking the derivative of one independent variable.

335
00:19:49.230 --> 00:19:52.620
Whereas in partial differential equations,
we're taking the partial derivative.

336
00:19:52.710 --> 00:19:54.870
So there are multiple independent variables,
right?

337
00:19:54.871 --> 00:19:58.830
We are differentiating with respect to one of those variable because they're all

338
00:19:58.831 --> 00:20:01.590
independent.
But in an ordinary differential equation,

339
00:20:01.740 --> 00:20:03.090
there's one independent variable.

340
00:20:04.350 --> 00:20:07.320
So we want to solve the differential equation.

341
00:20:07.321 --> 00:20:11.730
So what a differential equation does is it sets the derivative equal to some,

342
00:20:12.150 --> 00:20:14.370
some value.
It could be,
you know,

343
00:20:14.660 --> 00:20:18.720
the Dui dx equals f of x plus three,
you know?
Right?

344
00:20:18.721 --> 00:20:22.920
So it gives us that.
So we set the derivative to some value,
right?

345
00:20:22.921 --> 00:20:26.490
So let's say,
here we go,
here's,
here's a perfect example.

346
00:20:26.491 --> 00:20:28.890
If we have some function,
right,
some function in the world,

347
00:20:28.891 --> 00:20:29.730
we want to find the derivative.

348
00:20:29.750 --> 00:20:32.700
When we say that derivative plus some random number,

349
00:20:32.701 --> 00:20:34.500
like three or four or five equals one,

350
00:20:35.040 --> 00:20:39.120
that's a differential equation because we can use a differential equation solver

351
00:20:39.150 --> 00:20:40.470
method like Oilers method,

352
00:20:40.471 --> 00:20:43.800
which we'll get to or some other method to then find the original function.

353
00:20:43.801 --> 00:20:47.650
So the whole point of all of this,
of partial,
ordinary,

354
00:20:47.651 --> 00:20:50.890
all of this is to find the original function to approximate the function.

355
00:20:51.070 --> 00:20:53.950
And in general,
that's what we're doing in machine learning.
And in general,

356
00:20:53.951 --> 00:20:56.590
that's what we're doing in science and technology and math and everything.

357
00:20:56.740 --> 00:21:01.030
And we are trying to find the function and the function tells us everything.

358
00:21:01.031 --> 00:21:03.760
The function tells us everything.
Math tells us everything.

359
00:21:04.390 --> 00:21:05.560
Not to get too poetic here.

360
00:21:07.350 --> 00:21:08.183
<v 2>So</v>

361
00:21:08.760 --> 00:21:12.570
<v 0>we use this a lot in physics,
right?
So if we have ideas,</v>

362
00:21:12.630 --> 00:21:16.200
if we have concepts like displacement and velocity and acceleration,

363
00:21:16.530 --> 00:21:19.530
if we take the displacement and we differentiate,
we get the velocity.

364
00:21:19.531 --> 00:21:21.300
If we differentiate that we get the acceleration.

365
00:21:21.301 --> 00:21:24.360
If we take the acceleration and we integrate that,
we get the velocity.

366
00:21:24.361 --> 00:21:27.180
If we integrate that we get to the displacement.
So you see what I'm saying?

367
00:21:27.840 --> 00:21:30.380
It's very useful in physics and,
and,

368
00:21:30.400 --> 00:21:33.480
and these different fields to both differentiate and integrate.

369
00:21:33.810 --> 00:21:36.090
So I'm going to bring this back into machine learning.
So one more example,

370
00:21:36.120 --> 00:21:40.680
two x has several,
uh,
integral as possible integrals and it has one derivative.

371
00:21:40.681 --> 00:21:43.050
All of these expressions.

372
00:21:45.210 --> 00:21:48.360
So here's an example of a differential equation,
right?
So we have some,

373
00:21:48.570 --> 00:21:49.920
a derivative right here,
right?

374
00:21:49.950 --> 00:21:54.480
Dyd X equals x plus y over five,
okay.

375
00:21:54.481 --> 00:21:56.460
And so what we do is we say,

376
00:21:56.490 --> 00:22:01.490
let's take that derivative and compute the original function using this ordinary

377
00:22:02.671 --> 00:22:06.930
different differential equation solver.
And that will give us our solution.

378
00:22:06.931 --> 00:22:10.110
So this result in graph,
let me just compile that.

379
00:22:11.010 --> 00:22:15.720
This result in graph is our original function until all we knew was a

380
00:22:15.721 --> 00:22:17.280
differential equation right here.

381
00:22:17.520 --> 00:22:19.810
And using an ordinary differential equation solver,

382
00:22:19.830 --> 00:22:22.290
we were able to find that original function.

383
00:22:22.530 --> 00:22:26.940
Now let's apply this to ordinary differential equation networks.

384
00:22:26.941 --> 00:22:31.020
Now consider it a very simple ode.
We'll call them odysseys from now on.
Okay.

385
00:22:31.030 --> 00:22:34.380
Then that's what it stands for.
Consider a very simple ode from physics.

386
00:22:34.500 --> 00:22:36.960
So let's say we want to model the position of a marble.

387
00:22:36.961 --> 00:22:39.630
Let's say we have a marble,
right when we just want to roll it on the floor,

388
00:22:39.631 --> 00:22:41.370
we're going to roll this marble on the floor.

389
00:22:41.640 --> 00:22:46.110
And what we want to do is we want to predict where it's a position is eventually

390
00:22:46.111 --> 00:22:50.280
going to be using,
uh,
it's philocity and using time.

391
00:22:50.281 --> 00:22:54.300
So these are the three variables that we are considering here.
Time,
velocity,

392
00:22:54.301 --> 00:22:55.530
and position.
Okay?

393
00:22:55.770 --> 00:23:00.180
And so we can model the relationship between these variables using this
function.

394
00:23:00.181 --> 00:23:04.200
So we can calculate it's philocity by differentiating,
it's positioned.

395
00:23:04.201 --> 00:23:08.100
So the derivative of the position is it's philosophy and we can represent that

396
00:23:08.101 --> 00:23:11.280
using this equation right here.
So

397
00:23:12.050 --> 00:23:16.220
Oilers method solves this problem by following the physical intuition.

398
00:23:16.520 --> 00:23:19.910
So the a position at a time very close to the president depends on my current

399
00:23:19.911 --> 00:23:23.390
velocity and position.
So let's say we're traveling at five meters per second.

400
00:23:23.420 --> 00:23:26.870
Okay?
We're traveling at five meters per second and we traveled for one second,

401
00:23:27.050 --> 00:23:27.261
right?

402
00:23:27.261 --> 00:23:31.310
So if we're traveling for five meters per second and we traveled for one second,

403
00:23:31.520 --> 00:23:35.510
how far have we traveled?
Five Times one,
one second,
five.

404
00:23:35.570 --> 00:23:36.800
We've traveled five meters.

405
00:23:37.250 --> 00:23:41.720
And so we can model this using this formula right here.
Okay.
So,
uh,

406
00:23:41.810 --> 00:23:46.810
the position is going to be equal to where we are x of t plus age,

407
00:23:47.300 --> 00:23:51.620
the velocity times the derivative of the position.
And that's going to give us,

408
00:23:51.680 --> 00:23:55.850
so it's going to be five times the number of seconds we've been traveling plus

409
00:23:55.851 --> 00:23:58.790
wherever we were previously.
Okay?
And,

410
00:23:58.820 --> 00:24:02.330
but since we know that x prime of t equals f of x,

411
00:24:02.480 --> 00:24:06.230
we can rewrite this initial equation as this.
Okay?

412
00:24:06.320 --> 00:24:11.320
So x of t plus h is going to be equal to the current position plus the velocity

413
00:24:14.661 --> 00:24:18.500
times,
uh,
the number of seconds that we've traveled.
Okay?

414
00:24:18.590 --> 00:24:21.290
So we are computing how far were traveling by taking the,

415
00:24:21.410 --> 00:24:23.690
wherever we're starting from,
wherever we're starting from.

416
00:24:23.930 --> 00:24:25.230
And then taking that starting point,

417
00:24:25.340 --> 00:24:29.900
adding how far we've traveled by multiplying the velocity by the time that we've

418
00:24:29.901 --> 00:24:34.250
traveled.
And so what this looks like then,
if we squint at this,

419
00:24:34.251 --> 00:24:38.450
it looks at,
we can,
this is the formula for Oilers method yet again.

420
00:24:38.640 --> 00:24:41.670
And which also looks like the formula for residual layer.

421
00:24:41.671 --> 00:24:46.671
So we are connecting physics to neural networks by framing the residual network

422
00:24:48.051 --> 00:24:52.310
equation as a differential equation,
just like we did in physics.

423
00:24:52.370 --> 00:24:55.880
So differential equations can represent neural networks.

424
00:24:55.881 --> 00:24:59.240
Neural networks can be represented as differential equations.

425
00:24:59.390 --> 00:25:00.530
So what does this mean?

426
00:25:00.620 --> 00:25:05.620
It means that we can create new models for optimizing these networks using

427
00:25:05.781 --> 00:25:10.460
different numerical approaches to solving,
um,
ordinary differential equations,

428
00:25:10.610 --> 00:25:13.130
which there are over a hundred years of theory behind.

429
00:25:13.580 --> 00:25:15.860
Over a hundred years of theory are behind ods.

430
00:25:16.010 --> 00:25:19.700
And we haven't applied any of that to neural networks in general.
I mean,

431
00:25:19.730 --> 00:25:23.780
there's some papers,
I mean,
there's,
you know,
it's not in a mainstream way.

432
00:25:23.781 --> 00:25:25.520
We haven't seen anything in production like this.

433
00:25:25.760 --> 00:25:29.960
And what this means is it is the possibility of creating arbitrarily deep neural

434
00:25:29.961 --> 00:25:34.310
networks and training of these deep networks can be improved by considering the

435
00:25:34.311 --> 00:25:38.780
stoke called stability of the underlying ode and as numerical discretization.

436
00:25:38.810 --> 00:25:43.400
So two more points to create these arbitrarily deep networks with the finite

437
00:25:43.460 --> 00:25:44.420
memory footprint.

438
00:25:44.900 --> 00:25:49.900
We designed these neuro networks based on Odsp and so great and to sense the the

439
00:25:50.181 --> 00:25:52.790
optimization technique we all know and love in neural networks,

440
00:25:52.791 --> 00:25:56.750
the most popular optimization technique can be viewed at it's applying Oilers

441
00:25:56.751 --> 00:26:01.730
method for solving ods to gradient flow,
which can be,

442
00:26:02.420 --> 00:26:05.740
which is right here.
It can be represented right here.
So you might be thinking,

443
00:26:05.810 --> 00:26:08.670
so what's the difference here for not applying it to gradient flow because there

444
00:26:08.671 --> 00:26:12.160
are not discreet layers isn't instead of continuous function,
what?

445
00:26:12.210 --> 00:26:14.840
What then does an Ode net look like?

446
00:26:15.380 --> 00:26:20.380
So an ode is a function that describes a change of some system over through

447
00:26:20.751 --> 00:26:22.700
time.
Thank you calculus,
right?

448
00:26:22.701 --> 00:26:26.660
We're using the derivative to find the original function.
And in this setting,

449
00:26:26.661 --> 00:26:28.490
time is a continuous variable.

450
00:26:28.491 --> 00:26:33.050
Now imagine neural networks as this continuous system.
So what happens then?

451
00:26:33.051 --> 00:26:36.770
If we do that is that time really it becomes something more like the depth of

452
00:26:36.771 --> 00:26:39.950
the network and know that there are usually a discrete number of layers,

453
00:26:39.980 --> 00:26:43.380
but in this case there's a continuous number of layers.

454
00:26:43.381 --> 00:26:47.040
So we can either think of this as having waste that are a function of the depth

455
00:26:47.400 --> 00:26:50.130
or as having shared weights across layers,

456
00:26:50.220 --> 00:26:52.830
but adding the depth as an extra input to f.

457
00:26:52.920 --> 00:26:55.230
So anywhere you can put a Rez residual network,

458
00:26:55.231 --> 00:27:00.000
you can also put an ode network and each Odey he block can be used to replace a

459
00:27:00.001 --> 00:27:02.610
whole stack of Rez blocks.
In fact,

460
00:27:02.611 --> 00:27:06.900
they had an example in using the M and ist handwritten character Dataset as an

461
00:27:06.901 --> 00:27:08.280
image classification example,

462
00:27:08.490 --> 00:27:12.720
and they were placed six Rez blocks with one single ode block.
So again,

463
00:27:12.750 --> 00:27:17.730
traditional deep nets input input to a layer.
It gives us the hidden stay hidden,

464
00:27:17.731 --> 00:27:20.440
say go to the next layer,
gives the next,
hey is that go next to that gives,

465
00:27:20.441 --> 00:27:25.020
given that one resonates,
we do the same,
but we add the original input.
Okay.

466
00:27:25.500 --> 00:27:27.480
That's,
that's how that works.
So,

467
00:27:28.090 --> 00:27:28.460
<v 1>okay,</v>

468
00:27:28.460 --> 00:27:31.010
<v 0>if we model our networks like this,</v>

469
00:27:31.160 --> 00:27:33.950
we can use ordinary differential equations solvers,

470
00:27:33.980 --> 00:27:38.810
like Oilers method at solve the trajectory of a system by taking small steps in

471
00:27:38.811 --> 00:27:41.630
the direction of the system dynamics and adding them up,

472
00:27:41.780 --> 00:27:45.320
which allows for better training methods.
Okay.
So

473
00:27:47.010 --> 00:27:49.020
let's cover Oilers method.
Just one more time.

474
00:27:49.021 --> 00:27:51.750
So the Blue Line represents the function,
right?

475
00:27:51.751 --> 00:27:54.180
And so what we want to do is we want to approximate this function.

476
00:27:54.181 --> 00:27:56.430
So what we do is we say,
okay,

477
00:27:56.460 --> 00:28:00.030
let's take this first point on the line and find the derivative of it.
Okay,

478
00:28:00.180 --> 00:28:03.390
it's going to give us a tangent line and we take a small step in that direction.

479
00:28:03.720 --> 00:28:06.990
And then we say,
let's do it again.
What is the derivative of the function?
Okay?

480
00:28:06.991 --> 00:28:10.020
And let's give it another tangent line and another one and another one.

481
00:28:10.021 --> 00:28:11.910
We just keep doing that over and over and over again.

482
00:28:12.630 --> 00:28:16.080
And so what we can do is we can roughly approximate what that function is by

483
00:28:16.081 --> 00:28:20.520
continuously deriving,
um,
the points on that line.

484
00:28:20.730 --> 00:28:22.260
And that's Oilers method.
And so it's good.

485
00:28:22.310 --> 00:28:26.040
It's gonna give us a rough approximation.
So Oilers method isn't that great.

486
00:28:26.041 --> 00:28:27.480
It's a very primitive solver.

487
00:28:27.600 --> 00:28:31.410
But the point is to show that we can frame these networks as differential

488
00:28:31.411 --> 00:28:36.030
equations.
And so,

489
00:28:36.110 --> 00:28:40.130
um,
what happens is in a residual network,
uh,
there's,

490
00:28:40.200 --> 00:28:43.170
there's a discreet sequence of finite transformations,

491
00:28:43.620 --> 00:28:48.620
whereas in an ode network there is a vector field which continuously transforms

492
00:28:48.781 --> 00:28:51.480
this state.
And so this represents a difference between them there,

493
00:28:51.481 --> 00:28:54.390
their vector spaces,
right?

494
00:28:54.391 --> 00:28:58.800
And so there have been over 120 years of developments for efficient and accurate

495
00:28:58.801 --> 00:29:02.580
Ode solvers.
Let's apply those to neural networks and see what happens.

496
00:29:02.581 --> 00:29:07.020
Now that's the next step here.
So how,
so how then do we solve this thing?
If we,

497
00:29:07.021 --> 00:29:09.270
if we frame a neural network as an ode,

498
00:29:09.930 --> 00:29:14.310
how do we solve it using an Ode solver?
Do we use Oilers method?
We could,

499
00:29:14.311 --> 00:29:16.320
but we could use a better one and said,

500
00:29:16.710 --> 00:29:21.300
let's use what's called the ad joint method,
which dates back to 1962.
Okay.

501
00:29:21.301 --> 00:29:24.240
It's like an instantaneous analog of the chain roll.
Okay.

502
00:29:24.270 --> 00:29:27.870
So the thing about the ad joint method is that there is not a lot of content,

503
00:29:27.871 --> 00:29:30.900
if any,
about it on the Internet that is accessible.

504
00:29:30.901 --> 00:29:34.650
So I'm going to do the best that I can to explain this and I myself need to

505
00:29:34.651 --> 00:29:35.970
continuously learn about this.

506
00:29:35.971 --> 00:29:38.970
And even the authors of this paper are still figuring out

507
00:29:40.150 --> 00:29:43.120
how this is going to work.
They believe and a lot of people believe in,

508
00:29:43.121 --> 00:29:45.860
I think that this is going to be as big as gans were,
um,

509
00:29:46.060 --> 00:29:50.530
generative adversarial networks.
Okay.
So the [inaudible] method.

510
00:29:51.400 --> 00:29:55.480
So the basic idea is that we are computing three integrals.

511
00:29:55.780 --> 00:30:00.400
We're competing three integrals,
okay?
So we're computing the ad joined,

512
00:30:00.520 --> 00:30:03.790
which captures,
had the loss function changes with respect to the hidden state.

513
00:30:03.791 --> 00:30:06.430
That's one we're computing,
um,

514
00:30:07.300 --> 00:30:10.960
the hidden state backwards in time together.
And then we're a third integral.

515
00:30:10.961 --> 00:30:15.010
Tell us how the laws changes with the parameters theta,
which are the weights.

516
00:30:15.070 --> 00:30:19.150
So there are three integrals and we combine these three integrals into a single

517
00:30:19.151 --> 00:30:21.700
call to an ode solver.
Give it to that.

518
00:30:21.701 --> 00:30:25.060
It's going to concatenate the original states,
the ad joint states.

519
00:30:25.061 --> 00:30:29.200
And the other partial derivatives into a single vector and that will then solve

520
00:30:29.201 --> 00:30:31.540
our equation.
So here is what the algorithm looks like.

521
00:30:31.900 --> 00:30:35.350
We compute a gradient with respect to the time or tea,

522
00:30:35.351 --> 00:30:38.920
whatever ti is going to be.
We define an initial augmented safe.

523
00:30:39.220 --> 00:30:41.230
Then we defined the dynamics of that,
augment that,

524
00:30:41.231 --> 00:30:45.550
say concatenate the time to read it as together solve the reverse time ode and

525
00:30:45.551 --> 00:30:49.960
then return the gradients and an update our weights.
Okay,
so that's quite a lot.

526
00:30:49.961 --> 00:30:52.510
But in the code,
let's look at some code.

527
00:30:54.190 --> 00:30:57.340
This ode solver was applied to the Ford propagation of the network.

528
00:30:57.341 --> 00:30:59.440
So they have a code that's applied to time series data.

529
00:30:59.441 --> 00:31:03.010
They have code that's applied to em and ist.
They have several code examples.

530
00:31:03.310 --> 00:31:05.260
And so I've,
I've actually even,
you know,

531
00:31:05.261 --> 00:31:07.960
I've downloaded the code and we can run it right here,

532
00:31:10.210 --> 00:31:14.080
super easy.
Just need to insult high torch and what it's going to do it,

533
00:31:14.081 --> 00:31:19.081
it's going to approximate this function using a continuous neural network,

534
00:31:19.690 --> 00:31:21.520
neural network and Odie network.

535
00:31:22.030 --> 00:31:26.890
And it's using an ode solver to Ford propagate the,

536
00:31:26.891 --> 00:31:29.320
the data.
It's still using gradient descent.

537
00:31:29.680 --> 00:31:33.640
But it's using this [inaudible] method to Ford propagate the data and it's

538
00:31:33.641 --> 00:31:35.530
approximating with this line looks like.

539
00:31:35.531 --> 00:31:38.920
And so we can see how it PR approximating both the trajectory.

540
00:31:39.190 --> 00:31:42.880
I'm a different version of that graph and then the learned vector field and it's

541
00:31:42.881 --> 00:31:47.350
training over time.
Okay.
But um,
so I just want to show that as an example.

542
00:31:47.830 --> 00:31:50.710
Okay.
So you can run this code and I've got the code,
you know,

543
00:31:50.711 --> 00:31:52.780
it's like right here.
Okay.

544
00:31:52.990 --> 00:31:57.580
But I took out all of these like unnecessary bits just so for learning purposes

545
00:31:57.581 --> 00:32:00.010
and I've written it here.
So let's just go through this.

546
00:32:00.220 --> 00:32:01.930
Here's a normal residual block,
right?

547
00:32:01.931 --> 00:32:03.790
We already looked at an example of this before,

548
00:32:03.820 --> 00:32:07.540
but here's a normal residual block,
you know,
activation function,

549
00:32:07.541 --> 00:32:10.810
downsampling convolutional network pass,
right?

550
00:32:11.620 --> 00:32:14.710
That's a normal residual block.
Now here is an ode function.

551
00:32:14.711 --> 00:32:18.500
So what we do is we are again defining these,
um,

552
00:32:19.450 --> 00:32:22.330
neural net layers,
convolution normalize,
activate,

553
00:32:22.810 --> 00:32:24.660
and then we are again normal,
uh,

554
00:32:24.670 --> 00:32:29.500
forward propagation looks very similar to a regular neuro layer.

555
00:32:29.770 --> 00:32:34.280
Now in the Ode Block we encapsulate that function.
So we've call it OT,

556
00:32:34.290 --> 00:32:37.780
he flunked.
And then in the forward propagation method,

557
00:32:38.260 --> 00:32:41.420
here is our solver right here,
the ad joint method,

558
00:32:42.320 --> 00:32:44.990
which they created in Pi Torch by the way,
check out their library

559
00:32:46.080 --> 00:32:49.740
and we input that Ode function to this solver for forward propagation.

560
00:32:49.741 --> 00:32:51.960
And that's really the key difference right there.

561
00:32:52.920 --> 00:32:57.690
And so in the main method we add these pooling layers and so we say,
okay,

562
00:32:57.691 --> 00:32:58.560
here are layers.

563
00:32:58.561 --> 00:33:01.890
It's going to be a single ode block that's given this ode function.

564
00:33:02.160 --> 00:33:05.070
Let's have a fully connected layer at the end and build our model.

565
00:33:05.071 --> 00:33:09.870
Our model is a sequential model consisting of these down sampled layers are

566
00:33:09.871 --> 00:33:13.320
feature layers,
which is our single continuous ode block.

567
00:33:13.470 --> 00:33:16.920
And then our fully connected layer that the end to output that prediction.

568
00:33:18.760 --> 00:33:20.890
And then we go on as we normally would,

569
00:33:20.891 --> 00:33:23.020
we declare our gradient descent optimizer,

570
00:33:23.260 --> 00:33:25.030
which is going to be stochastic gradient descent.

571
00:33:25.360 --> 00:33:29.880
We initialize our training data,
we apply our training data to the model,

572
00:33:29.910 --> 00:33:31.840
we get our prediction,
we compute our error.

573
00:33:31.841 --> 00:33:35.230
What's the difference between the prediction and the actual and we back

574
00:33:35.231 --> 00:33:39.340
propagate a using the optimizer and that's it,
right?

575
00:33:39.341 --> 00:33:43.510
So that's the difference here.
So let's summarize what just happened here.

576
00:33:43.940 --> 00:33:45.910
And all networks are a popular type of DNL model.

577
00:33:46.180 --> 00:33:49.690
They're built with linear Algebra matrices and optimize using calculus,

578
00:33:49.810 --> 00:33:53.140
graded descent.
You know,
all these different solvers.
They're consistent.

579
00:33:53.200 --> 00:33:56.920
They consist of a series of discreet layers,
which are just matrix operations.

580
00:33:57.040 --> 00:34:00.010
And each layer introduces a little bit of air to get around this error.

581
00:34:00.250 --> 00:34:02.530
Microsoft introduced residual networks,

582
00:34:02.531 --> 00:34:06.490
which is that skip connection idea f of x plus x equals the input to the next

583
00:34:06.491 --> 00:34:07.270
layer.

584
00:34:07.270 --> 00:34:12.270
And so what the researchers found was like this equation for a residual network

585
00:34:13.241 --> 00:34:15.400
resembles an ordinary differential equation.

586
00:34:15.760 --> 00:34:18.430
So why don't we use ordinary differential equations solvers,

587
00:34:18.431 --> 00:34:22.330
like wheeler's method or the Agilent method to then solve a neural network and

588
00:34:22.331 --> 00:34:24.820
instead have continuous layers.
And when they did that,

589
00:34:25.000 --> 00:34:28.750
it actually worked and it had a better prediction result than a recurrent

590
00:34:28.751 --> 00:34:30.400
network did for approximating

591
00:34:33.170 --> 00:34:34.190
this function right here.

592
00:34:34.460 --> 00:34:38.150
And what happens then is we can apply it to irregular time series data in the

593
00:34:38.151 --> 00:34:42.900
future,
um,
to have better predictions.
We can have faster testing time.
Um,

594
00:34:42.950 --> 00:34:47.060
and we can apply a whole new class of mathematical operations that are dedicated

595
00:34:47.061 --> 00:34:49.100
to ods,
to neural networks.

596
00:34:49.250 --> 00:34:52.640
And that just opens up a whole new branch of AI possibility,
which is amazing.

597
00:34:54.010 --> 00:34:54.730
And,
um,

598
00:34:54.730 --> 00:34:58.360
we can compute the gradient with a constant memory costs instead of constantly

599
00:34:58.361 --> 00:35:03.040
second layers.
We have one continuous ode,
Blah.
Okay.
So I hope that made sense.

600
00:35:03.070 --> 00:35:04.210
I hope that excited you.

601
00:35:04.211 --> 00:35:07.210
I've got links to everything that I've talked about here and helpful links in

602
00:35:07.211 --> 00:35:10.270
the video description.
Subscribe if you haven't yet.
Um,

603
00:35:10.330 --> 00:35:11.890
thank you for watching and for now,

604
00:35:11.950 --> 00:35:16.660
I've got to go make some more videos and play my course and music video.

605
00:35:16.720 --> 00:35:17.800
So thanks for watching.

