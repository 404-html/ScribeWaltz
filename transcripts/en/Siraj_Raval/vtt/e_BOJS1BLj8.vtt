WEBVTT

1
00:00:00.210 --> 00:00:00.990
Hello world,

2
00:00:00.990 --> 00:00:05.990
it's Saroj and neuroscience and machine learning to different fields that are

3
00:00:06.241 --> 00:00:08.010
actually very much connected.

4
00:00:08.220 --> 00:00:11.970
Today I'm going to talk about how neuroscience and machine learning are very

5
00:00:11.971 --> 00:00:12.750
connected.

6
00:00:12.750 --> 00:00:16.560
I'm going to talk about how recent advances in neuroscience will move machine

7
00:00:16.561 --> 00:00:20.250
learning forward.
And before I talk about that,
I want to show you a demo.

8
00:00:20.310 --> 00:00:24.690
In this demo is an interactive network visualization tool for exploring

9
00:00:24.691 --> 00:00:28.260
functional brain connectivity.
It's called spectrum is.

10
00:00:28.350 --> 00:00:32.480
And after this video you're probably going to want to look at more neuroimaging

11
00:00:32.490 --> 00:00:35.040
techniques and this is a great way to get started.

12
00:00:35.041 --> 00:00:39.840
I found this repository on hub and basically what it allows you to do is

13
00:00:39.870 --> 00:00:43.530
simulate a brain in Silico,
but it's a very,
you know,
it's,
it's,

14
00:00:43.531 --> 00:00:45.720
it's not the full brain obviously,
but it's a,

15
00:00:45.900 --> 00:00:50.900
it's a good model of how neuroimaging tools can be simple and it was created

16
00:00:50.911 --> 00:00:54.330
using d three dot.
Js Very cool visualization library.

17
00:00:54.331 --> 00:00:57.990
You can hit start and you can see the network just,
you know,
move it gets,
it's,

18
00:00:58.070 --> 00:01:00.580
it's very,
it's very cool.
But,
uh,

19
00:01:00.690 --> 00:01:02.490
I just wanted to show you that really quickly and you can,
you know,

20
00:01:02.491 --> 00:01:05.070
switch the timing,
the frequency of connections,

21
00:01:05.250 --> 00:01:08.400
all these different things you get,
you get all of these different mappings of,

22
00:01:08.800 --> 00:01:11.790
of how the neurons are moving,
how they're interacting in the network.
Uh,

23
00:01:11.820 --> 00:01:13.920
it's pretty cool.
So check out that after this,

24
00:01:13.921 --> 00:01:15.870
the link to that is going to be in the video description.

25
00:01:16.170 --> 00:01:17.790
But let's get started with this.

26
00:01:17.791 --> 00:01:20.310
I've divided this talk into three different parts,

27
00:01:20.580 --> 00:01:22.330
the history of neuroscience in Ai,

28
00:01:22.570 --> 00:01:27.390
the contemporary neuroscience and AI and the future of neuroscience and AI.
Okay,

29
00:01:27.391 --> 00:01:30.900
so let's start off with the history and obviously I've got this super generic

30
00:01:30.930 --> 00:01:35.280
image that is shown everywhere,
but it's the idea of this image is saying,
Hey,

31
00:01:35.281 --> 00:01:39.540
we've got this neural network.
And what it was inspired by is the human brain.

32
00:01:39.900 --> 00:01:44.430
Okay?
So at the dawn of the computer age,
when computer science was,
was invented,

33
00:01:44.431 --> 00:01:48.810
when it was starting up,
all of AI research was neuroscience.

34
00:01:48.811 --> 00:01:52.290
Everything was based off of neuroscience.
It was the same thing.

35
00:01:52.470 --> 00:01:56.010
Every single discovery came from neuroscience when it came to AI.

36
00:01:56.910 --> 00:02:01.910
And the early pioneers of this field straddled both neuroscience and AI.

37
00:02:01.980 --> 00:02:02.580
It was,
they were,

38
00:02:02.580 --> 00:02:06.840
they were connected to both collaborations between all of these different fields

39
00:02:06.900 --> 00:02:09.960
was highly productive.
So let's,
let's look at some examples here.

40
00:02:10.260 --> 00:02:14.070
How can I talk about computer science without talking about touring?
Right?

41
00:02:14.190 --> 00:02:18.300
Touring was the man Turing test.
Okay.
He's,
he invented the Turing test.

42
00:02:18.450 --> 00:02:19.410
You've ended a bunch of stuff.

43
00:02:19.411 --> 00:02:22.170
But he's in a lot of ways the father of computer science.

44
00:02:22.470 --> 00:02:24.630
But the idea behind the Turing test was,

45
00:02:24.720 --> 00:02:29.400
let's see if a human can tell if he's talking to either a human or a machine,

46
00:02:29.670 --> 00:02:32.970
and if he can't tell,
then the computer has passed the Turing test.
Right.

47
00:02:33.660 --> 00:02:36.960
Touring was very,
very,
very much into the brain.
He said,

48
00:02:36.961 --> 00:02:40.230
we are not interested in the fact that the brain has the consistency of cold

49
00:02:40.231 --> 00:02:42.180
porridge.
That was his quote.

50
00:02:42.181 --> 00:02:45.360
But what he meant was he was very interested in how the brain worked.

51
00:02:45.361 --> 00:02:49.140
He was very certain that it was a very complex organ and his view is shared by

52
00:02:49.141 --> 00:02:54.090
many in that we cannot represent the brain using anything but math because it's

53
00:02:54.091 --> 00:02:57.780
such a complex system,
right?
Think about the brain has a graph,
right?

54
00:02:57.781 --> 00:03:01.150
Think of that.
Think of it as a graph.
And if you think about as a graph,

55
00:03:01.180 --> 00:03:03.940
you have these neurons nodes connected to each other.

56
00:03:04.300 --> 00:03:08.410
It's this common tutorial explosion,
right?
In terms of a search space.

57
00:03:08.710 --> 00:03:11.440
When you take a signal and you probably get it forward,

58
00:03:11.530 --> 00:03:15.100
it's going to be split up into hundreds if not millions of different signals for

59
00:03:15.101 --> 00:03:18.550
different neurons.
And this is a common tutorial explosion,
right?

60
00:03:18.970 --> 00:03:23.050
There's no way to describe this via colors or you know,

61
00:03:23.120 --> 00:03:27.910
regions other than by sheer beautiful math,
right?
And if you think about math,

62
00:03:27.911 --> 00:03:31.060
right?
There's this idea of this,
this mythical book,

63
00:03:31.061 --> 00:03:32.980
the book of math that when you die,

64
00:03:32.981 --> 00:03:37.981
God is able to show you this book and it has all of the theorems for everything

65
00:03:38.351 --> 00:03:42.450
listed out,
right?
And so when you die,
you are able to see,
oh,
this was a theorem,

66
00:03:42.460 --> 00:03:43.293
this was the theorem.

67
00:03:43.300 --> 00:03:47.080
And over time we humans have been able to discover some of those theorems.

68
00:03:47.081 --> 00:03:50.710
But there are a lot more we haven't discovered.
Now imagine a book of Ai,
right?

69
00:03:50.711 --> 00:03:52.600
In a book of Ai Neural networks.

70
00:03:52.601 --> 00:03:54.970
And deep learning would be one of those theorems.

71
00:03:55.120 --> 00:03:59.920
But there are way more that we still have to understand and to to find,

72
00:03:59.921 --> 00:04:04.450
right to discover neuroscience is a great way to do that too.
Right?
So touring,

73
00:04:04.570 --> 00:04:08.500
he had this computer program that he invented at the University of Manchester

74
00:04:08.620 --> 00:04:12.310
that accepted a single input,
right?
A single numbers input and output,

75
00:04:12.340 --> 00:04:14.770
a single number.
And he wagered,

76
00:04:14.800 --> 00:04:17.710
and this is another famous thing by touring the tourings wager,

77
00:04:17.860 --> 00:04:22.370
that it would take someone a thousand years to figure out how that blackbox

78
00:04:22.570 --> 00:04:26.890
worked,
how that number converted from one from one integer integer to another.

79
00:04:27.280 --> 00:04:30.700
And then he said he,
he extrapolated from that idea and he said,
well,

80
00:04:30.701 --> 00:04:34.240
if you think about the brain as that black box,
it's going to take a really,

81
00:04:34.241 --> 00:04:37.030
really long time to figure out how that works.
And he's,

82
00:04:37.031 --> 00:04:40.330
so he was very skeptical that we would be able to figure out how the brain

83
00:04:40.331 --> 00:04:43.930
worked,
even though most of his work involved trying to recreate the,

84
00:04:43.960 --> 00:04:45.040
the brain in Silico.

85
00:04:45.070 --> 00:04:48.430
But if turning could have just seen the modern Sukkot supercomputers that we

86
00:04:48.431 --> 00:04:51.190
have,
the advances in neuroimaging techniques that we have,

87
00:04:51.191 --> 00:04:55.280
he would have been blown away,
right?
So,
uh,
yeah,
it's,

88
00:04:55.281 --> 00:04:59.110
it's definitely possible to recreate the brain in,
in,
in Silico.

89
00:04:59.890 --> 00:05:04.180
But let's keep going here.
So Nicola can pits two other neuroscientists,
right?

90
00:05:04.840 --> 00:05:08.530
They invented the model of the single neuron.
Now this is where it all started.

91
00:05:08.531 --> 00:05:08.760
They,

92
00:05:08.760 --> 00:05:12.940
they wanted to create some sort of mathematical model that represented how a

93
00:05:12.941 --> 00:05:16.360
neuron works.
And this was a very basic model.
The model said this,

94
00:05:16.750 --> 00:05:18.430
let's take some inputs,
right?
Some,

95
00:05:18.431 --> 00:05:21.280
some set of numbers x like this represented here.

96
00:05:21.580 --> 00:05:25.510
Let's apply some sort of function to that.
And output is one single integer,

97
00:05:25.511 --> 00:05:29.050
right?
That is something like what a neuron does.
It's accepting these inputs,

98
00:05:29.051 --> 00:05:30.220
these signals,
right?

99
00:05:30.310 --> 00:05:33.280
It's doing something to those signals and then it's out putting a signal and

100
00:05:33.281 --> 00:05:37.240
that signal propagates to other neurons.
And that creates this network,

101
00:05:37.300 --> 00:05:42.130
this neural network of neurons,
right?
And then they kept going,
right?

102
00:05:42.130 --> 00:05:45.430
So then hop deal took,
took this idea and he said,
let me take them a call.

103
00:05:45.431 --> 00:05:47.650
It pits neuron,
let me combine them together.

104
00:05:47.770 --> 00:05:51.400
So the outputs of one neuron are fed as the inputs of the next neuron and let's

105
00:05:51.401 --> 00:05:52.660
just keep doing that over and over again.

106
00:05:52.900 --> 00:05:55.510
And so this was called the hot dealed network,
right?

107
00:05:55.511 --> 00:05:59.660
So this was the first recurrent artificial network.
He connected Macola pits,

108
00:05:59.661 --> 00:06:03.260
neurons together,
then came Hinton,
right?
So Hinton said,
okay,

109
00:06:03.261 --> 00:06:05.030
we've got these layers of neurons.

110
00:06:05.150 --> 00:06:08.870
What is the best way to optimize these neurons so that it can complete its

111
00:06:08.871 --> 00:06:09.860
objective?
Right?

112
00:06:10.040 --> 00:06:13.520
And so he invented with his colleagues the back propagation technique,

113
00:06:13.730 --> 00:06:16.160
which said,
we've got some output prediction,

114
00:06:16.340 --> 00:06:20.240
let's compute the error based on what their label is.
That's the era value.

115
00:06:20.420 --> 00:06:22.550
Let's computer gradient.
The gradient says,

116
00:06:22.580 --> 00:06:25.700
how much do you update the weights of the network over time and what to update

117
00:06:25.701 --> 00:06:28.190
those weights incrementally backwards in the,
in the,

118
00:06:28.191 --> 00:06:31.730
in the reverse direction as we forward propagated the input data.

119
00:06:32.840 --> 00:06:34.250
And this is called backpropagation.

120
00:06:36.440 --> 00:06:41.440
And Hinton was also a huge proponent of a neuroscience level of thinking when

121
00:06:41.481 --> 00:06:43.820
applied to neural networks and machine learning,

122
00:06:44.090 --> 00:06:46.790
right during the AI winter when everyone said he was crazy,

123
00:06:46.940 --> 00:06:50.600
he stuck to the idea of neural networks being a way forward in AI.

124
00:06:50.601 --> 00:06:52.580
And it turned out and you know,

125
00:06:52.700 --> 00:06:56.120
as seen in the deep learning revolution of late that he was right.

126
00:06:56.420 --> 00:07:00.410
He was also a big proponent of neuroscience inspired ideas like dropout,

127
00:07:00.440 --> 00:07:04.820
turning neurons on and off randomly to pro to increase the generalization

128
00:07:04.821 --> 00:07:09.080
ability of a network of nonlinearities to allow a network to learn both linear

129
00:07:09.081 --> 00:07:13.370
and nonlinear functions,
the idea of hierarchical and layered networks,
et Cetera,

130
00:07:13.371 --> 00:07:16.610
et cetera.
And so all of this leads up until now.

131
00:07:17.060 --> 00:07:19.890
The problem with now is if you were to ha,

132
00:07:19.940 --> 00:07:21.890
if you were to learn machine learning right now,

133
00:07:22.670 --> 00:07:25.310
if you were to learn about all the different types of models,

134
00:07:25.340 --> 00:07:28.520
support vector machines,
random forest decision trees,

135
00:07:29.120 --> 00:07:32.990
it's not very clear that all of this,
these things are inspired by neuroscience.

136
00:07:32.990 --> 00:07:34.610
And in a lot of ways they're not.

137
00:07:34.640 --> 00:07:39.470
So there's this disconnect between neuroscience and AI that didn't exist before.

138
00:07:39.950 --> 00:07:40.790
And,
uh,
what's,

139
00:07:40.791 --> 00:07:44.840
what's happened is because the discoveries in neuroscience have just grown and

140
00:07:44.841 --> 00:07:48.320
grown and the discoveries in AI have grown and grown over time,

141
00:07:48.650 --> 00:07:52.250
these two fields have become so complex that it's hard for any one person to

142
00:07:52.251 --> 00:07:54.920
become an expert in both fields,
let alone one.

143
00:07:55.310 --> 00:07:58.730
So it's hard to make any discoveries that,
that intertwine these two.

144
00:07:59.240 --> 00:08:03.590
But the human brain is the only proof we have that this type of intelligence

145
00:08:03.591 --> 00:08:07.880
works.
So it's,
it's,
it's,
yes,
it's hard,
but it's essential.
It's essential to do

146
00:08:09.500 --> 00:08:12.920
so.
Um,
Maher,
uh,
this other,
this,
this,

147
00:08:12.950 --> 00:08:16.730
this scientist had this idea of describing a biological system in three

148
00:08:16.731 --> 00:08:19.760
different parts.
So one part was the implementation,

149
00:08:19.761 --> 00:08:21.880
the hardware of how you know this,

150
00:08:21.881 --> 00:08:24.710
this software is running this wet where you can call it.

151
00:08:25.040 --> 00:08:26.540
Then there's the algorithmic part,

152
00:08:26.720 --> 00:08:29.600
what representations can implement such computations.

153
00:08:29.870 --> 00:08:32.060
And on top of that is the computational park.

154
00:08:32.330 --> 00:08:36.740
Why do things work the way they do equations on top of those initial algorithms?

155
00:08:36.950 --> 00:08:40.820
It's hard for us to work on the implementational part because creating hardware

156
00:08:40.821 --> 00:08:43.670
requires a lot of capital,
right?
Not everybody's got that money.

157
00:08:44.060 --> 00:08:48.890
So we need to focus on the algorithmic and the computational parts and anyone

158
00:08:48.891 --> 00:08:53.630
with a laptop and Gpu in the cloud.
So this is being democratized can do that.

159
00:08:53.840 --> 00:08:58.680
So now let's talk about neuroscience.
Okay.
And and how it's played into Ai.

160
00:08:59.010 --> 00:09:02.910
So there are four key areas in AI that had been affected by contemporary

161
00:09:02.911 --> 00:09:05.610
neuroscience.
The first is attention.
Okay.

162
00:09:05.611 --> 00:09:09.960
So the brain does not learn by implementing a single global optimization

163
00:09:09.961 --> 00:09:13.200
principle within a uniform and undifferentiated neural network.

164
00:09:13.470 --> 00:09:17.580
What I mean by that is it's not like we have some giant LSTM network running

165
00:09:17.581 --> 00:09:20.220
everywhere,
right?
It's not some giant recurrent network.

166
00:09:20.250 --> 00:09:23.100
We have different modules that are good for doing different things.

167
00:09:23.220 --> 00:09:26.340
So one part of the brand could be doing something like a hot build network and

168
00:09:26.341 --> 00:09:29.670
another one could be an LSTM network.
And another one could be,
you know,

169
00:09:29.700 --> 00:09:31.110
a self organizing map.

170
00:09:31.320 --> 00:09:34.110
And so there are different modules that are good for different things.

171
00:09:34.230 --> 00:09:37.590
And so up until now,
for convolutional networks in particular,

172
00:09:37.591 --> 00:09:40.710
which are networks that are good for working with images,
the,

173
00:09:40.770 --> 00:09:45.570
they worked directly on entire images or video frames with equal priority given

174
00:09:45.571 --> 00:09:47.250
to all the image pixels.

175
00:09:47.550 --> 00:09:51.900
But we learned that the primate visual system works differently rather than

176
00:09:51.901 --> 00:09:56.700
processing all the inputs in parallel.
Visual attention shifts strategically.

177
00:09:56.760 --> 00:10:00.870
That's the key point here.
Strategically among locations and objects,

178
00:10:00.900 --> 00:10:05.370
centering processing resources and represents [inaudible] coordinates on a

179
00:10:05.371 --> 00:10:09.150
series of regions in turn.
And so recent Ai,
um,

180
00:10:09.180 --> 00:10:13.720
discoveries have implemented this idea of an attention mechanism,
uh,

181
00:10:13.721 --> 00:10:15.690
focusing on the parts that are most specific.

182
00:10:15.960 --> 00:10:19.680
And usually the way they do this is by having some weights for this attention

183
00:10:19.681 --> 00:10:22.710
mechanism that are updated via gradient descent over time.

184
00:10:22.950 --> 00:10:27.150
And so there are differentiable weights and so the network learns where best to,

185
00:10:27.180 --> 00:10:31.680
to focus its attention on,
in whatever the input data is as it updates over time.

186
00:10:32.400 --> 00:10:34.920
Another key area is episodic memory.

187
00:10:34.921 --> 00:10:39.120
So the difference between semantic and episodic memory is this semantic memory

188
00:10:39.121 --> 00:10:44.040
is very concrete.
It's like,
you know,
this is an apple.
I know that this is a,

189
00:10:44.041 --> 00:10:45.570
you know,
this is a bad,
this is a,
you know,

190
00:10:45.630 --> 00:10:48.540
whatever episodic memory is more continuous.

191
00:10:48.541 --> 00:10:52.200
It's about an event that has happened in the past.
And we have that,
you know,

192
00:10:52.201 --> 00:10:55.980
we've,
we've learned this from neuroscience that we have both types of memory.

193
00:10:57.240 --> 00:10:58.890
It's one of the canonical,
uh,

194
00:10:59.120 --> 00:11:02.520
themes in neuroscience that we have that are intelligent behavior,

195
00:11:02.521 --> 00:11:04.800
relies on multiple memory systems.

196
00:11:04.800 --> 00:11:09.540
It's not just one type of memory and a so deep mind recently,
you know,

197
00:11:10.110 --> 00:11:12.420
created this deep cue learning algorithm,
right?

198
00:11:12.421 --> 00:11:13.920
So this is the reason that Google bought them.

199
00:11:14.280 --> 00:11:18.420
What they did was they were inspired by the idea of episodic memory and they

200
00:11:18.421 --> 00:11:21.570
created this algorithm called deep to that used,

201
00:11:21.571 --> 00:11:25.800
experienced replay directly inspired by the idea of episodic memory.

202
00:11:25.890 --> 00:11:27.840
And here is the pseudo code for that,
right?

203
00:11:27.841 --> 00:11:31.890
So we initialize some replay memory d and this can be considered a data store,

204
00:11:31.891 --> 00:11:33.390
like think of it as an array.
Okay.

205
00:11:33.690 --> 00:11:37.680
And then we initialize some action value function that we update over time.

206
00:11:38.070 --> 00:11:41.030
And then during the training process,
the agent selects a,

207
00:11:41.050 --> 00:11:45.690
an action from some probability distribution.
It executes that action,

208
00:11:45.840 --> 00:11:47.490
it gets a reward and then it,

209
00:11:47.520 --> 00:11:51.690
it stores that transition inside of the episodic memory and then it performs

210
00:11:51.691 --> 00:11:55.770
grading the to update its waits and it repeats and every time it,

211
00:11:55.771 --> 00:11:57.820
it re re does this for a new episode.

212
00:11:57.940 --> 00:12:01.750
It's sampling not just from this random distribution but from the episodic

213
00:12:01.751 --> 00:12:04.120
memory that it's stored over time.
And so the,

214
00:12:04.300 --> 00:12:06.940
so this replay memory episodic memory is same thing.

215
00:12:06.941 --> 00:12:11.930
In this case it's kind of like this external memory store,
right?
And so it was,

216
00:12:12.250 --> 00:12:16.300
it was inspired by it and it was critical to his success over time.

217
00:12:16.570 --> 00:12:21.570
This thing was able to succeed in like 16 or more a different Atari Games.

218
00:12:22.271 --> 00:12:25.960
The same algorithm,
which is incredible if you think about it,
the same algorithm,

219
00:12:26.110 --> 00:12:30.100
16 different novel environments.
All it was fed was the pixels of the game.

220
00:12:30.460 --> 00:12:33.850
That is very incredible.
The third is working memory,
right?

221
00:12:33.851 --> 00:12:35.560
It operates over a few seconds.

222
00:12:35.710 --> 00:12:40.420
Temporary storage manipulates information and focus his attention while you're

223
00:12:40.421 --> 00:12:43.870
working on something,
you're,
you're using some memory on that,
right?

224
00:12:43.990 --> 00:12:45.880
This is very similar to episodic memory.

225
00:12:46.210 --> 00:12:50.680
We know from neuroscience that we can maintain and manipulate information with

226
00:12:50.681 --> 00:12:52.930
an active store.
No one has working memory.

227
00:12:53.320 --> 00:12:58.150
So one way that we've implemented that in AI is through the idea of long,

228
00:12:58.151 --> 00:13:00.670
short term memory LSTM cells,
right?

229
00:13:00.700 --> 00:13:04.330
These are cells that have certain gates and implicate a forget gate.

230
00:13:04.450 --> 00:13:08.450
And what these gates actually are our perceptrons,
there are many,
uh,

231
00:13:08.590 --> 00:13:10.600
neural networks that have weight values.

232
00:13:10.870 --> 00:13:13.540
And as we differentiate the LSTM network,

233
00:13:13.690 --> 00:13:15.760
these weights are also updated over time.

234
00:13:15.970 --> 00:13:20.380
And what happens is the gradient is trapped in these weights.
So when,
when we're,

235
00:13:20.381 --> 00:13:23.560
when we're,
when a recurrent network is reading in some sequence,

236
00:13:23.561 --> 00:13:26.050
whether that be text or numbers,
over time,

237
00:13:26.150 --> 00:13:30.460
the gradient slowly vanishes as it goes more and more back in the network.

238
00:13:30.760 --> 00:13:34.630
And the problem with this is it's not updating the weights at the end,

239
00:13:34.810 --> 00:13:35.950
at the input of the network,

240
00:13:35.980 --> 00:13:38.530
as much as is updating the weights at the end of the network.

241
00:13:38.800 --> 00:13:41.050
And this is a problem if there's gotta be an equal update.

242
00:13:41.140 --> 00:13:44.440
So LSTM networks help prevent that from happening by storing,

243
00:13:44.470 --> 00:13:46.240
by locking in that memory over time.

244
00:13:46.690 --> 00:13:50.920
And this was directly inspired by neuroscience,
the idea of working memory.

245
00:13:52.060 --> 00:13:57.060
But the problem with LSTM cells are they take the function of sequence control

246
00:13:57.610 --> 00:14:02.610
by controlling what to do next and memory storage and stores them together.

247
00:14:03.130 --> 00:14:08.020
When we know that the brain working memory in the brain separates these two

248
00:14:08.021 --> 00:14:08.854
concepts,

249
00:14:08.860 --> 00:14:13.420
so updated an updated version is called the differential neural computer.

250
00:14:13.510 --> 00:14:16.600
Okay.
So the idea is that there is an external memory store,

251
00:14:16.601 --> 00:14:20.530
which is essentially a matrix and we have some input data and then it outputs a

252
00:14:20.531 --> 00:14:21.040
prediction.

253
00:14:21.040 --> 00:14:24.700
And in the middle it's updating not just the weights of the network but the

254
00:14:24.730 --> 00:14:25.900
external memory store.

255
00:14:26.110 --> 00:14:30.400
So normally the weights of the network or the memory,
but if we,

256
00:14:30.490 --> 00:14:33.310
if we take that memory and we create,
uh,
uh,
you know,
we,

257
00:14:33.520 --> 00:14:35.140
we create an external memory store,

258
00:14:35.320 --> 00:14:38.470
we can separate these storage of memory from the processing of the network

259
00:14:38.471 --> 00:14:42.940
itself.
And this let it do things that an LSTM could not do.

260
00:14:43.120 --> 00:14:47.500
A wide range of complex memory and reasoning tasks like finding the shortest

261
00:14:47.501 --> 00:14:50.650
path through a graph,
like structure,
like a subway map for example.

262
00:14:52.070 --> 00:14:55.670
The fourth and last thing from contemporary neuroscience is continual learning,

263
00:14:55.671 --> 00:14:58.310
right?
We are constantly learning things,
right?

264
00:14:58.470 --> 00:15:02.630
We're learning how to record youtube videos.
We're learning how to read a book.

265
00:15:02.631 --> 00:15:05.210
We're learning how to run,
we're learning how to do karate.

266
00:15:05.300 --> 00:15:07.130
We're able to learn how to do all these things.

267
00:15:07.131 --> 00:15:10.010
We're able to learn how to do all these different things with one brain.

268
00:15:10.580 --> 00:15:13.910
But there's this problem in neural networks called catastrophic forgetting,

269
00:15:14.180 --> 00:15:17.960
where if we take a neural network and we say,

270
00:15:18.560 --> 00:15:20.930
learn how to do x,
right?
Whatever that objective is.

271
00:15:20.931 --> 00:15:24.350
And then we take that same neural network and we say,
okay,
now learn how to do y.

272
00:15:25.430 --> 00:15:25.940
What?

273
00:15:25.940 --> 00:15:29.300
Whatever it's learning for that second training iteration is going to overwrite

274
00:15:29.301 --> 00:15:32.690
the weights.
Eight learned for the,
for the first one.
And this is a problem,
right?

275
00:15:32.840 --> 00:15:33.980
We don't want that to happen.

276
00:15:34.880 --> 00:15:39.880
So there is this synaptic consolidation were connections between neurons are

277
00:15:40.731 --> 00:15:44.630
less likely to be overwritten if they have been important in previously aren't

278
00:15:44.660 --> 00:15:47.060
tasks.
So,
uh,

279
00:15:47.390 --> 00:15:52.130
one way to prevent that is this idea of elastic way consolidation,
right?

280
00:15:52.430 --> 00:15:53.960
We can use,
first of all,
by the way,

281
00:15:53.990 --> 00:15:58.780
we can use advances in neuroimaging to further study how these,
these,
um,

282
00:15:58.910 --> 00:16:03.700
connections are happening in the brain.
The idea behind elastic wait,

283
00:16:03.701 --> 00:16:08.570
consolidation is that it's,
it's slowing down.

284
00:16:08.571 --> 00:16:12.830
Learning in a subset of network weights identified as important to the previous

285
00:16:12.831 --> 00:16:17.820
task,
thereby anchoring these parameters to previously found solutions.
So it,

286
00:16:17.830 --> 00:16:20.930
it,
it allows the network to learn multiple different tasks.

287
00:16:21.110 --> 00:16:22.430
And I'm not saying it does it perfectly,

288
00:16:22.431 --> 00:16:26.540
but it's one step inspired by neuroscience that lets a neural network learn

289
00:16:26.570 --> 00:16:29.720
different tasks over time.
So now let's get to the future,
right?

290
00:16:29.721 --> 00:16:33.710
Five areas of AI that neuroscience will improve.
And I've got this image here,

291
00:16:33.740 --> 00:16:34.640
symbolic concepts,

292
00:16:34.641 --> 00:16:37.910
computation and there's a lot of ways to divide all this stuff up.
You know,

293
00:16:37.911 --> 00:16:40.610
there's,
there's,
there's a lot of ways but uh,
okay.

294
00:16:40.611 --> 00:16:44.510
So the first thing is understanding physical reality.
Okay.

295
00:16:44.570 --> 00:16:49.250
Human infants we have learned are able to interact with the physical world in a

296
00:16:49.251 --> 00:16:52.910
way.
They have these core concepts built in like space and number.

297
00:16:53.150 --> 00:16:58.100
Like how did the concept of space and object,
Nis,
all these things.
Now,

298
00:16:58.130 --> 00:17:02.450
if you think about robots,
right?
If you think about robots in simulations,

299
00:17:02.480 --> 00:17:03.690
they work great,
right?
If you,

300
00:17:03.691 --> 00:17:06.800
if you have a robot pick up an object in a simulation,
it works great.

301
00:17:07.370 --> 00:17:10.820
Just try to take those learnings and apply them in a real robot and the real

302
00:17:10.821 --> 00:17:15.110
world,
exact same setting.
I promise you it won't work.
Okay?

303
00:17:15.830 --> 00:17:20.810
Almost all the time it never works because when it comes to the physical world,

304
00:17:21.110 --> 00:17:23.990
our current models are just not good for some reason.

305
00:17:24.200 --> 00:17:27.770
So we've got to figure out how,
why,
why that is happening,
right?

306
00:17:27.890 --> 00:17:30.600
Why does it work so well in simulation?
If we,

307
00:17:30.601 --> 00:17:33.530
if we have all of those variables together,
you know,
in,
in,

308
00:17:33.710 --> 00:17:37.700
you know how the arm is moving,
the physics,
the inverse kinematics,
the gravity,

309
00:17:37.820 --> 00:17:41.540
yet it just doesn't work in real life.
Why?
We got to figure that out.

310
00:17:41.570 --> 00:17:43.790
And that's a way that neuroscience can help.

311
00:17:44.420 --> 00:17:47.240
There's this idea of the interaction network.
It's very recent.

312
00:17:47.241 --> 00:17:51.510
It's a very recent model that I want to learn how this bowl is bouncing,
right?

313
00:17:51.810 --> 00:17:56.070
And so what it does is it separates relational reasoning with object reasoning.

314
00:17:56.100 --> 00:17:56.431
And that's,

315
00:17:56.431 --> 00:17:59.430
that's as much as I'm going to get into on that cause there's a whole video I

316
00:17:59.431 --> 00:18:00.570
can make on how that works.

317
00:18:00.690 --> 00:18:04.800
The larger point I'm trying to make here is that any task that seems very
simple,

318
00:18:05.250 --> 00:18:07.440
if you really think about that problem for a while,

319
00:18:07.441 --> 00:18:10.800
you realize that there are so many different sub problems there,
right?

320
00:18:10.980 --> 00:18:13.170
So if you think about a problem a lot,
there are sub problems,
right?

321
00:18:13.171 --> 00:18:16.980
It's not just reasoning,
it's relational reasoning,
it's object reasoning.

322
00:18:17.220 --> 00:18:20.850
And as we study this,
more and more will realize what those sub problems are,

323
00:18:20.910 --> 00:18:22.890
which will help us in the larger scheme of things.

324
00:18:23.490 --> 00:18:27.360
No what the overarching solution to those problems are.
Okay.

325
00:18:27.361 --> 00:18:29.520
So check out this interaction network paper as well.

326
00:18:29.521 --> 00:18:34.410
I've got a link to it in the description.
Um,
efficient learning,

327
00:18:34.411 --> 00:18:36.750
right?
Building Metadata,
Meta knowledge,
right?

328
00:18:36.930 --> 00:18:41.930
We are able to look at something and after only a few examples know exactly what

329
00:18:42.361 --> 00:18:43.620
to classify it as,
right?

330
00:18:43.680 --> 00:18:47.760
You show me a picture of something I've never seen before and very,
very likely.

331
00:18:47.760 --> 00:18:51.150
Show me another picture of something similar and I will know exactly.
However,

332
00:18:51.151 --> 00:18:53.430
with deep learning,
we've got to show this deep neural network,

333
00:18:53.490 --> 00:18:57.750
hundreds of thousands of sometimes millions of images for to be able to
classify.

334
00:18:57.840 --> 00:19:00.420
And this is not how the human brain works,
arguably.

335
00:19:00.690 --> 00:19:04.050
And so the idea of metal learning is very important and it will be more

336
00:19:04.051 --> 00:19:05.610
important in the future of AI,

337
00:19:06.120 --> 00:19:09.420
which is also very closely related to transfer learning,
right?

338
00:19:09.630 --> 00:19:13.290
So normally we would say,
okay,
here's a neural network to learn this.

339
00:19:13.470 --> 00:19:16.290
Now here's a neural network to learn this and here's the neural network to learn

340
00:19:16.291 --> 00:19:16.860
this.

341
00:19:16.860 --> 00:19:20.160
But what if we could have the same neural network apply what is learned from

342
00:19:20.161 --> 00:19:23.610
some previous task to this task?
That's the idea of transfer learning,

343
00:19:23.970 --> 00:19:25.650
which we're really,
really good at.

344
00:19:26.550 --> 00:19:29.350
So there was a recent report from this paper,
uh,

345
00:19:29.580 --> 00:19:34.020
from two years ago that said that neural code is thought to be important in the

346
00:19:34.021 --> 00:19:35.310
representation of nap.

347
00:19:35.310 --> 00:19:39.960
Like spaces might be critical for act trek reasoning in more general domains.

348
00:19:40.250 --> 00:19:43.410
Uh,
aloe centric means map like spaces in this context.

349
00:19:43.411 --> 00:19:48.411
But the idea is that aloe centric coding systems and code the location of one

350
00:19:48.541 --> 00:19:51.450
object or its parts with respect to other objects,

351
00:19:51.690 --> 00:19:54.570
egocentric is in relation to the self to other things.

352
00:19:54.810 --> 00:19:59.810
But what this finding was was that the location information that we encode about

353
00:20:00.660 --> 00:20:04.530
objects and how they relate is important for abstract reasoning in general

354
00:20:04.531 --> 00:20:05.250
domains.

355
00:20:05.250 --> 00:20:09.360
And this is a very new finding and we can apply that to AI and a lot of ways,

356
00:20:12.210 --> 00:20:15.840
uh,
right.
So lastly,
I'm going to talk about imagination.

357
00:20:15.870 --> 00:20:19.320
Well actually two more things I want to talk about imagination and planning,

358
00:20:19.321 --> 00:20:21.870
right?
So deep Q was awesome,
right?

359
00:20:21.871 --> 00:20:24.810
But the problem with the Deq was it was reactive.

360
00:20:25.020 --> 00:20:29.040
It was seeing how the environment reacted to what his actions were and then it

361
00:20:29.041 --> 00:20:31.230
reacted to that.
It wasn't proactive,

362
00:20:31.231 --> 00:20:34.230
it wasn't sitting there planning out all these different trajectories.

363
00:20:34.470 --> 00:20:38.310
But what we've learned is that the h that is that we do this,
right?

364
00:20:38.520 --> 00:20:41.850
Humans plan out things,
whether it's consciously or subconsciously,

365
00:20:42.030 --> 00:20:45.750
we plan out how things are going to happen in her head before we execute that

366
00:20:45.751 --> 00:20:48.610
action.
And so we have this imagination,
right?

367
00:20:48.611 --> 00:20:51.180
This is essentially imagination happening.
Uh,

368
00:20:51.220 --> 00:20:54.190
there's this great architecture by deep mine,
very,
very recent.

369
00:20:55.150 --> 00:20:57.460
It's called [inaudible].
Um,
but,
uh,

370
00:20:57.461 --> 00:20:59.830
I could also make an entire video on how that works.

371
00:21:00.100 --> 00:21:02.820
I've got the link to the paper here if you,
if you'd like to see it.
But,
uh,

372
00:21:03.490 --> 00:21:08.260
the idea is that it's using this external module for imagination and in a single

373
00:21:08.270 --> 00:21:12.040
rollout,
it's making these predictions.
So for neuroscience,
for Ai,

374
00:21:12.041 --> 00:21:15.310
the idea of imagination is going to be very,
very important in the future.

375
00:21:15.310 --> 00:21:20.310
And we can learn a lot about how this works by looking at how humans plan out

376
00:21:20.351 --> 00:21:24.580
and simulate actions in their head before they actually execute those actions.

377
00:21:24.610 --> 00:21:27.820
Now,
last thing I want to talk about our virtual brain analytics.

378
00:21:28.270 --> 00:21:33.250
So deep learning has been thought of as being a black box and that makes sense.

379
00:21:33.370 --> 00:21:36.970
Deep neural networks are very complex with sometimes a millions of parameters,

380
00:21:36.971 --> 00:21:40.060
right?
All these different numbers in the matrices.
If our weights,

381
00:21:40.330 --> 00:21:42.700
how are we supposed to visualize that?
Right?
And there've been,
you know,

382
00:21:42.860 --> 00:21:45.970
people who've said,
okay,
we've cracked open the black box,
but let's be real.

383
00:21:45.971 --> 00:21:47.140
In a lot of ways we haven't,

384
00:21:47.141 --> 00:21:51.310
there's so many different types of neural architecture is that it's hard to say.

385
00:21:51.400 --> 00:21:54.790
We have just totally figured out how neural networks work.
It's not true.

386
00:21:54.910 --> 00:21:56.320
We still have a long ways to go,

387
00:21:56.590 --> 00:22:00.910
but what we can do is we can apply findings in neuroscience to that too,

388
00:22:00.911 --> 00:22:04.570
to help with that problem.
The idea of two photon imaging,
for example,

389
00:22:04.780 --> 00:22:09.160
allows us to noninvasively mapped out how neurons are interacting in our,
in our,

390
00:22:09.190 --> 00:22:11.230
in our brain.
And this is very,
very recent.

391
00:22:11.650 --> 00:22:15.430
If we can take that and then we can apply that in Silico that idea of

392
00:22:15.460 --> 00:22:19.300
neuroimaging,
we can better understand how our neural networks are working,
uh,

393
00:22:19.400 --> 00:22:20.320
on our computers.

394
00:22:20.350 --> 00:22:24.310
Visualizing brain states through dimensionality reduction is commonplace in

395
00:22:24.311 --> 00:22:27.490
neuroscience and it can be applied more in AI research.

396
00:22:27.850 --> 00:22:30.970
So this is going to be very important as the complexity of neural networks

397
00:22:30.971 --> 00:22:34.480
increases over time.
And as soon as we create this,

398
00:22:34.510 --> 00:22:37.750
it's going to be not just good for AI research,
for AI ethics,

399
00:22:37.930 --> 00:22:41.790
for Ai Productivity.
Why is this AI doing this?
What,

400
00:22:41.840 --> 00:22:44.110
why did I come up with a solution that I did?

401
00:22:44.290 --> 00:22:47.290
It's going to be important for both business,
for ethics,
for research,

402
00:22:47.291 --> 00:22:49.570
for everything,
so more explainable solutions.

403
00:22:49.840 --> 00:22:52.690
And we can look at neuroscience and neuro neuroimaging as a way to help with

404
00:22:52.691 --> 00:22:54.610
that.
So that's the end of this video.

405
00:22:54.611 --> 00:22:56.500
I want it to go over some things that you know could,

406
00:22:56.640 --> 00:23:01.000
could get you thinking about how the brain inspires AI and how it will inspire

407
00:23:01.001 --> 00:23:04.300
AI in the future.
Check out this spectrum is a repository.

408
00:23:04.301 --> 00:23:06.670
Link to it in the description and hope you like this video.

409
00:23:06.790 --> 00:23:09.190
Please subscribe for more programming videos.
And for now,

410
00:23:09.250 --> 00:23:11.950
I've got to study my brain.
So thanks for watching.

