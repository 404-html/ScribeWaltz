WEBVTT

1
00:00:00.040 --> 00:00:04.870
Data is sacred.
Every hour of every day.
A new sensor is connected to the web.

2
00:00:05.140 --> 00:00:07.690
Every trip,
every memory,
every creation,

3
00:00:07.840 --> 00:00:12.460
every discovery joins the ever-growing web work building together amidst a world

4
00:00:12.461 --> 00:00:17.350
of ever growing skepticism and falsehood.
Data is true.
It's transparent,

5
00:00:17.500 --> 00:00:18.250
provable.

6
00:00:18.250 --> 00:00:22.030
Most of it is unstructured streams of raw numbers being a mass that dizzying

7
00:00:22.031 --> 00:00:24.310
rates,
but by applying intelligence to it,

8
00:00:24.550 --> 00:00:26.800
we can find the patterns and connections that matter.

9
00:00:27.070 --> 00:00:29.290
We can find the meaning hidden in the numbers.

10
00:00:29.590 --> 00:00:31.960
The Economist says more for you is less for me,

11
00:00:31.990 --> 00:00:35.050
but the lover knows more for you is more for me too,

12
00:00:35.320 --> 00:00:37.060
and when we visualize the right data,

13
00:00:37.250 --> 00:00:41.110
it gives us that most precious feeling at the intersection of art and science.

14
00:00:41.140 --> 00:00:41.940
Wonder

15
00:00:41.940 --> 00:00:42.661
<v 1>hello world.</v>

16
00:00:42.661 --> 00:00:46.620
It's Saroj and today we're going to learn how to preprocess a Dataset.

17
00:00:47.010 --> 00:00:50.460
Yes,
preparing data is one of the most important,

18
00:00:50.461 --> 00:00:53.700
yet most overlooked parts of the machine winning pipeline.

19
00:00:53.940 --> 00:00:57.990
A lot of introductory tutorial is just have you import a preprocessed version of

20
00:00:57.991 --> 00:01:02.280
a Dataset like handwritten characters or movie ratings and just a single line of

21
00:01:02.281 --> 00:01:04.440
code.
We go,
we're all these,
not that easy.

22
00:01:04.770 --> 00:01:08.310
Once you've decided what problem you're trying to solve or you have a question

23
00:01:08.311 --> 00:01:11.350
that you want the answer to,
it's time to find the right Dataset.

24
00:01:11.600 --> 00:01:14.390
<v 0>I want to know what happened to the plans they sent you.</v>

25
00:01:14.570 --> 00:01:16.030
<v 1>I stored them on Microsoft Azure.</v>

26
00:01:17.730 --> 00:01:21.530
The predictions your deep net makes are only as good as the data you give it.

27
00:01:21.980 --> 00:01:23.240
Garbage in,
garbage out,

28
00:01:23.690 --> 00:01:26.510
so you want to make sure your data is relevant to your problem.

29
00:01:26.810 --> 00:01:30.590
There are tons of resources to find publicly available datasets and I've linked

30
00:01:30.591 --> 00:01:34.940
to some in the description.
The defacto standard format for data is CSV.

31
00:01:35.240 --> 00:01:38.990
Most software packages out there deal with data in that format and you can

32
00:01:38.991 --> 00:01:41.600
convert your data into CSV format just as easily.

33
00:01:41.960 --> 00:01:44.360
There's so much we could potentially do to our data,

34
00:01:44.390 --> 00:01:48.440
but there are three key preprocessing steps for every dataset will cover

35
00:01:48.770 --> 00:01:51.200
cleaning,
transformation and reduction.

36
00:01:51.470 --> 00:01:54.440
We're going to look at three different datasets and go through these steps for

37
00:01:54.441 --> 00:01:58.880
each one to get them ready to be fed into a model.
So let's start with our first.

38
00:01:59.030 --> 00:02:01.370
The first Dataset will use his music based.

39
00:02:01.490 --> 00:02:05.210
This data was collected from a game called tag attune in the game to players

40
00:02:05.211 --> 00:02:08.330
listen to a song and tack it with Shauna is an instruments that they think are

41
00:02:08.331 --> 00:02:09.770
relevant when the song is over.

42
00:02:09.771 --> 00:02:13.700
The player who had the most correct tags gets a point,
so I would win every time.

43
00:02:14.480 --> 00:02:18.020
Our data set has 25,000 songs with the correctly labeled tax.

44
00:02:18.110 --> 00:02:21.530
We want to train a model on this data so that given a new song,

45
00:02:21.560 --> 00:02:23.570
you know correctly classify,
it's Chaunra.

46
00:02:23.690 --> 00:02:26.210
We'll import pandas to help us parse this dataset.

47
00:02:26.450 --> 00:02:30.290
Then the read CSV function will let us store the data in the two dimensional

48
00:02:30.291 --> 00:02:32.690
pandas.
Data structure known has a data frame.

49
00:02:33.080 --> 00:02:36.560
Data frames are easily modifiable and we'll call our variable new data.

50
00:02:36.890 --> 00:02:40.790
Let's explore this data first shall we will display the first five rows using

51
00:02:40.791 --> 00:02:44.420
the head function with five as a parameter.
So basically each row is numbered,

52
00:02:44.510 --> 00:02:48.680
has an id and then either a one or zero next to a tag to indicate whether or not

53
00:02:48.681 --> 00:02:50.150
the given MP3 has that tech.

54
00:02:50.350 --> 00:02:53.750
It seems simple enough we can use the info function to get some more data.

55
00:02:53.900 --> 00:02:57.740
Only 38 megs.
So for our cleaning step,
is there anything we need to do?

56
00:02:58.100 --> 00:03:00.700
Not really.
Each has a simple binary tag.

57
00:03:00.701 --> 00:03:04.210
It's consistent and luckily our data does not have empty values,

58
00:03:04.660 --> 00:03:08.590
but my soul does.
We can move right onto the transformation step.

59
00:03:08.830 --> 00:03:12.490
What are some modifications we can make to this data that will make it easier

60
00:03:12.491 --> 00:03:14.380
for our model to understand?
Well,

61
00:03:14.410 --> 00:03:18.730
notice how a lot of the tags are pretty similar sounding like female singing

62
00:03:18.850 --> 00:03:23.470
female vocals.
We can generalize these features into one feature called female.

63
00:03:23.680 --> 00:03:27.010
Let's create a two dimensional list of synonyms that we find in our data.

64
00:03:27.280 --> 00:03:30.910
Then we can merge them and drop all the other columns except for the first one

65
00:03:31.090 --> 00:03:32.830
for each synonym list in our matrix.

66
00:03:32.980 --> 00:03:36.220
Let's get the Max values from each of the features and add them all to our

67
00:03:36.221 --> 00:03:38.080
person and I'm in our data frame object,

68
00:03:38.320 --> 00:03:40.810
which will effectively merge the values into one column.

69
00:03:41.080 --> 00:03:43.390
Then we'll drop the rest of the features from the data frame.

70
00:03:43.600 --> 00:03:47.290
Now we've got more generalized features.
Next for the reduction step.

71
00:03:47.320 --> 00:03:49.900
What can we remove from this data that's not necessary?

72
00:03:50.080 --> 00:03:53.290
Everything seems pretty solid,
so let's go ahead and split it into training,

73
00:03:53.291 --> 00:03:56.560
validation and testing sets that we can feed it into our model.

74
00:03:56.740 --> 00:03:58.240
Notice how in this example,

75
00:03:58.241 --> 00:04:01.210
I'm not thinking about which features to use and which not to.

76
00:04:01.540 --> 00:04:05.650
Before deep learning,
we had to pick the right features to use to feed our model,

77
00:04:05.860 --> 00:04:09.940
but deep neural nets learn high level features from whatever features we give
it,

78
00:04:10.210 --> 00:04:13.960
it decides for itself what is relevant to the problem from a Dataset

79
00:04:14.050 --> 00:04:16.960
architecture.
Engineering is the new feature engineering.

80
00:04:17.080 --> 00:04:20.920
The second data set we'll use is a collection of network connections and they're

81
00:04:20.921 --> 00:04:22.660
labeled normal or abnormal.

82
00:04:22.870 --> 00:04:25.750
The abnormal connections are intruders trying to break in.

83
00:04:26.110 --> 00:04:29.860
We want to be able to classify a connection given the set of other features.

84
00:04:30.100 --> 00:04:33.730
When we look at this data,
it seems pretty dense,
no missing values.

85
00:04:33.731 --> 00:04:35.740
Nothing really jumps out as an outlier,

86
00:04:35.860 --> 00:04:39.350
so let's skip the cleaning step and move right on to transforming it.

87
00:04:39.640 --> 00:04:42.790
Our numerical features are all operating on different scales,

88
00:04:42.791 --> 00:04:46.990
so we should normalize them to ensure each feature is treated equally by our

89
00:04:46.991 --> 00:04:50.380
model.
After storing our data into a pen as data frame it's,

90
00:04:50.381 --> 00:04:53.320
I can't learn has a handy sub module called standard scaler,

91
00:04:53.440 --> 00:04:55.090
which we'll import that initialize.

92
00:04:55.420 --> 00:04:57.940
After that we're ready to move on to our reduction step.

93
00:04:58.150 --> 00:05:02.080
We got a lot of features and there are probably a lot that are highly
correlated.

94
00:05:02.320 --> 00:05:06.370
We could use a technique called dimensionality reduction to reduce the number of

95
00:05:06.371 --> 00:05:07.204
features we have.

96
00:05:07.480 --> 00:05:11.320
This will also let us visualize our data in twoD or Three d space.

97
00:05:11.650 --> 00:05:14.620
This doesn't mean that our model will be more accurate necessarily,

98
00:05:14.800 --> 00:05:19.780
just that our data is easier to read.
One method of doing this is called PCA,

99
00:05:19.810 --> 00:05:23.770
which stands for Porsche Club of America,
way prong definition,

100
00:05:23.920 --> 00:05:25.810
principal component analysis.

101
00:05:26.700 --> 00:05:29.230
<v 2>David,
it's got so many teachers have squash,
a mint</v>

102
00:05:30.300 --> 00:05:31.133
<v 1>creatures</v>

103
00:05:31.630 --> 00:05:35.270
<v 2>are so normalized.
Then our correlation matrix size pool,</v>

104
00:05:35.290 --> 00:05:38.640
Eigen vectors and I's sort them.

105
00:05:39.100 --> 00:05:40.780
How many dimensions do I want?

106
00:05:41.500 --> 00:05:46.500
I select that many items up front projection matrix from them and use it to turn

107
00:05:47.171 --> 00:05:50.170
my data three dimensional,
plot them so I could judge them.

108
00:05:50.390 --> 00:05:52.010
<v 1>So let me summarize this process again.</v>

109
00:05:52.250 --> 00:05:56.120
Let's say we had four features and we wanted to reduce them to adjust to using

110
00:05:56.121 --> 00:06:00.290
PCA.
There are five steps to this.
The first is to normalize the data.

111
00:06:00.291 --> 00:06:02.000
Once we have it stored in a variable,

112
00:06:02.240 --> 00:06:05.840
then we'll want to compute a covariance matrix.
To construct this,

113
00:06:05.841 --> 00:06:09.710
we compute the covariants between each feature with every other feature.

114
00:06:09.770 --> 00:06:12.050
So we'd subtract the mean from the feature Matrix,

115
00:06:12.290 --> 00:06:16.340
calculate the transpose and multiply it by the feature matrix minus the meat.

116
00:06:16.700 --> 00:06:20.420
Then we take that whole value and divide it by the number of features minus one.

117
00:06:20.900 --> 00:06:23.300
This gives us our covariance matrix.

118
00:06:23.600 --> 00:06:28.070
Next we'll perform eigen decomposition on it to get the eigenvectors and

119
00:06:28.071 --> 00:06:31.190
eigenvalues.
I again,
isn't it such a fun word to say?

120
00:06:31.400 --> 00:06:34.070
Hiking vectors are the principle components of a dataset.

121
00:06:34.100 --> 00:06:37.530
They give us the directions along which heart transformation act,

122
00:06:37.640 --> 00:06:39.890
the eigenvalues give us the magnitude of each.

123
00:06:39.980 --> 00:06:43.130
We'll sort both in descending order,
then create a matrix out of that.

124
00:06:43.190 --> 00:06:46.850
We'll use this matrix to transform our original Beecher matrix via the dot

125
00:06:46.851 --> 00:06:47.510
product.

126
00:06:47.510 --> 00:06:51.560
We could then plot our data in two d space and use these principal components to

127
00:06:51.561 --> 00:06:54.530
replace are many features.
Let's look at one more dataset.

128
00:06:54.590 --> 00:06:58.130
This time for airline prices,
for flights between New York and Paris,

129
00:06:58.430 --> 00:07:00.980
we want to predict the ticket price from just a departure date.

130
00:07:01.220 --> 00:07:02.900
We've got departure and arrival dates,

131
00:07:03.080 --> 00:07:06.470
airports and flight prices up to 120 days before departure.

132
00:07:06.920 --> 00:07:09.530
Notice how we've got quite a few missing values in our data,

133
00:07:09.531 --> 00:07:13.520
so for our cleaning step,
we could remove these values,
build them with Zeros,

134
00:07:13.670 --> 00:07:16.250
build them with the average price across all the days,

135
00:07:16.490 --> 00:07:18.770
or try to predict them using a learning algorithm.

136
00:07:18.860 --> 00:07:22.220
Let's go ahead and calculate the average price for each row across all days

137
00:07:22.400 --> 00:07:25.910
using the mean function and then we'll iterate through the data and if it's no,

138
00:07:25.940 --> 00:07:29.120
we'll replace it with the mean price,
then we can smooth our data.

139
00:07:29.150 --> 00:07:32.960
That means finding outliers in it that we can remove,
define these.

140
00:07:32.961 --> 00:07:36.620
We could run clustering or regression algorithms on certain values to find the

141
00:07:36.621 --> 00:07:39.450
outliers and then remove them or just remove them by eye.

142
00:07:39.830 --> 00:07:43.250
Since our data set is small,
let's do the latter.
No need to reduce our data.

143
00:07:43.251 --> 00:07:45.350
This seems like a good set.
Let's break it down.

144
00:07:45.380 --> 00:07:48.980
There are three steps to preprocessing a dataset cleaning,

145
00:07:49.130 --> 00:07:50.870
transformation and reduction.

146
00:07:51.170 --> 00:07:53.870
Deep learning learns to relevant features from our data.

147
00:07:54.020 --> 00:07:57.980
So architecture engineering is the new feature and generic and principal

148
00:07:57.981 --> 00:08:02.120
component analysis is a popular dimensionality reduction techniques that can be

149
00:08:02.121 --> 00:08:03.470
implemented with psychic learn.

150
00:08:03.800 --> 00:08:07.340
The winner of the coding challenge from the last video is Charles David Blot

151
00:08:07.580 --> 00:08:08.490
Charles David Hughes,

152
00:08:08.491 --> 00:08:12.500
just non Pi to build a three layer neural net capable of predicting an

153
00:08:12.530 --> 00:08:16.320
earthquake and he used a random search strategy to find the optimal hyper

154
00:08:16.321 --> 00:08:20.900
parameters for his model wizard of the week and the runner up is city Jack
Grove.

155
00:08:21.050 --> 00:08:24.590
He use tensorflow for his prediction using just three inputs.

156
00:08:24.740 --> 00:08:28.580
The coding challenge for this video is to use a dating dataset to predict if

157
00:08:28.581 --> 00:08:31.130
someone gets a match based on their personality traits.

158
00:08:31.370 --> 00:08:34.400
Details are in the read me your get humbling in the comments and I'll announce

159
00:08:34.401 --> 00:08:36.290
the winner.
Next video,
please subscribe.

160
00:08:36.291 --> 00:08:38.930
If you want to see more videos like this checkup,
this related video,

161
00:08:39.170 --> 00:08:42.200
and for now I'm going to predictive roses.
Really smell like poo poo poo.

162
00:08:42.410 --> 00:08:43.700
So thanks for watching.

