WEBVTT

1
00:00:00.090 --> 00:00:04.740
Hello world it Saroj and math equations.
How would the name of Pythagoras,

2
00:00:04.741 --> 00:00:06.690
are you supposed to read these things?

3
00:00:06.780 --> 00:00:11.220
I'm going to cover some crucial tips that you can use to be able to read and

4
00:00:11.221 --> 00:00:13.350
understand any math equation.

5
00:00:13.680 --> 00:00:17.990
A lot of people think math is difficult and their rights.

6
00:00:18.240 --> 00:00:20.340
That is inherently difficult.

7
00:00:20.580 --> 00:00:25.140
It's not as easy to relate it to your life like another topic like economics or

8
00:00:25.170 --> 00:00:28.950
sociology or Doug.
You dancing think a bit like a foreign language.

9
00:00:29.070 --> 00:00:33.330
If you're a native English speaker and you come across a Japanese children's

10
00:00:33.331 --> 00:00:37.290
book for the first time,
no matter how basic it is,

11
00:00:37.380 --> 00:00:42.030
all the squiggles will look very strange and you won't understand a single
thing,

12
00:00:42.270 --> 00:00:44.940
but if you want to learn to read Japanese,

13
00:00:45.210 --> 00:00:50.210
you need to have committed to memory a few hundred symbols and several hundred

14
00:00:50.281 --> 00:00:54.930
words from those.
Your mind will build a more general understanding.

15
00:00:55.200 --> 00:00:56.700
When it comes to math.

16
00:00:56.850 --> 00:01:00.930
You also need to learn new symbols like Greek letters.

17
00:01:01.050 --> 00:01:05.550
There are quite a lot of them.
It's important to recognize the names of them.

18
00:01:05.670 --> 00:01:08.580
When you see them else,
when you see any of them your mind,

19
00:01:08.581 --> 00:01:12.000
we'll just label them squiggly thing much too ambiguous.

20
00:01:12.090 --> 00:01:17.070
You also need to learn new words like function and derivative and new grammar,

21
00:01:17.071 --> 00:01:21.000
a way to write equations in a logical and consistent way.

22
00:01:21.180 --> 00:01:24.180
So before you can understand math formulas,

23
00:01:24.420 --> 00:01:27.900
you need to learn what each of the symbols are and what they mean.

24
00:01:27.990 --> 00:01:32.370
Focus on the math terminology and learn how equations are structured.

25
00:01:32.760 --> 00:01:37.650
Sigma notation for example,
represents a longer some,
but this is an obvious.

26
00:01:37.651 --> 00:01:42.450
When you look at it you have to add another dimension of thought to what you

27
00:01:42.451 --> 00:01:44.850
see.
It's an abstract concept,

28
00:01:45.090 --> 00:01:48.480
but once you can understand this way of thinking,

29
00:01:48.660 --> 00:01:51.450
you can apply it to other equations as well.

30
00:01:51.690 --> 00:01:56.190
A little bit of effort on learning the basics goes a long way when it comes to

31
00:01:56.191 --> 00:01:56.970
math.

32
00:01:56.970 --> 00:02:01.740
A great course that will help you a lot with this is called introduction to

33
00:02:01.741 --> 00:02:06.570
mathematical thinking by Stanford University.
You can find it on Coursera.

34
00:02:06.600 --> 00:02:09.120
It's totally free and I highly recommend it.

35
00:02:09.450 --> 00:02:13.770
When I say math is like a separate language,
I really mean it.

36
00:02:13.980 --> 00:02:18.980
Symbols put together are able to conjure up concepts in the mind of a well

37
00:02:19.591 --> 00:02:21.420
versed reader.
In fact,

38
00:02:21.450 --> 00:02:26.450
a lot of times it's best to understand math at a conceptual level than trying to

39
00:02:26.641 --> 00:02:30.600
translate it to English.
A lot can be lost if you do that.

40
00:02:30.990 --> 00:02:33.600
One great analogy is the last supper.

41
00:02:33.780 --> 00:02:36.720
If you examine it very closely in three different parts,

42
00:02:36.990 --> 00:02:38.910
you'll be able to describe each part,

43
00:02:39.150 --> 00:02:42.600
but the whole painting has an entirely different meaning.

44
00:02:42.870 --> 00:02:47.220
You'll see the detail texture and color,
but miss the portrait completely.

45
00:02:47.490 --> 00:02:51.030
A math equation is similar in that it tells a story.

46
00:02:51.330 --> 00:02:55.110
Try to see what the story is before you delve into the details.

47
00:02:55.320 --> 00:02:56.970
You can go in for a closer look.

48
00:02:57.000 --> 00:02:59.830
Once you have built a framework for understanding,

49
00:02:59.950 --> 00:03:04.570
programmers can be particularly bad at math.
We're trained to think linearly.

50
00:03:04.600 --> 00:03:05.500
It's a process,

51
00:03:05.501 --> 00:03:09.880
whereas a math expression is an entire concept and can not necessarily be

52
00:03:09.881 --> 00:03:11.530
written in a single line of code.

53
00:03:11.860 --> 00:03:15.790
Math is more like natural language than a programming language.

54
00:03:16.180 --> 00:03:18.220
Programming languages have a single,

55
00:03:18.310 --> 00:03:23.310
well-defined fixed syntax or particular grammatical constructs always have the

56
00:03:24.641 --> 00:03:25.474
same meanings,

57
00:03:25.630 --> 00:03:30.630
but math is a collection of rules and conventions sometimes in violates others

58
00:03:31.571 --> 00:03:33.730
less so with lots of idioms,

59
00:03:33.820 --> 00:03:38.820
some of which are mutually incompatible and lots of variation between dialects,

60
00:03:39.400 --> 00:03:42.070
meaning conventions within various fields,

61
00:03:42.100 --> 00:03:44.800
so there's not really a formal specification.

62
00:03:44.801 --> 00:03:49.450
You just have to keep reading and writing the language and allow yourself to

63
00:03:49.480 --> 00:03:52.030
absorb it through practice.

64
00:03:52.270 --> 00:03:57.270
Learning math isn't something you just do and then finish it's discipline.

65
00:03:57.611 --> 00:04:01.030
It's a way of thinking and expression for the universe itself.

66
00:04:01.270 --> 00:04:04.270
No matter what you think of it,
it doesn't change.

67
00:04:04.390 --> 00:04:07.750
It just is and you never completely learn it.

68
00:04:07.840 --> 00:04:10.930
Just a small subset of what is essentially infinity.

69
00:04:11.320 --> 00:04:15.760
The deeper you dive into it,
the more you realize just how little you know.

70
00:04:16.030 --> 00:04:21.010
Computer Science is full of math,
discrete math,
probability,
theory,

71
00:04:21.130 --> 00:04:22.090
number theory.

72
00:04:22.210 --> 00:04:27.010
These are used in general programming when it comes to the math of computer

73
00:04:27.011 --> 00:04:27.550
science,

74
00:04:27.550 --> 00:04:32.080
specifically MIT opencourseware has a great course on this.

75
00:04:32.260 --> 00:04:35.020
Videos,
assignments and solutions all in one,

76
00:04:35.080 --> 00:04:39.340
check it out and when we start getting to the really cool parts of computer

77
00:04:39.341 --> 00:04:42.820
science like machine learning,
cryptography,
robotics,

78
00:04:42.821 --> 00:04:44.860
quantum computing and animation,

79
00:04:45.130 --> 00:04:47.950
math starts being used even more heavily.

80
00:04:48.340 --> 00:04:50.260
So if you're interested in say,

81
00:04:50.320 --> 00:04:54.160
machine learning and want to get up to speed on the math of it,

82
00:04:54.430 --> 00:04:59.110
the first step is to learn the formulas you already understand.

83
00:04:59.350 --> 00:05:03.460
All math requires earlier math.
It just build on top of itself.

84
00:05:03.670 --> 00:05:08.590
That is all the new things you are learning now depend on what you learned last

85
00:05:08.590 --> 00:05:09.760
week,
last semester,

86
00:05:09.820 --> 00:05:14.050
last year and all the way back to the numbers you learned as a little kid.

87
00:05:14.290 --> 00:05:19.180
Maybe you understand how a two d graph works where it's possible to plot the

88
00:05:19.181 --> 00:05:23.260
relationship between two variables and draw the line of best fit.

89
00:05:23.320 --> 00:05:28.000
The equation of any straight line is called a linear equation and can be written

90
00:05:28.001 --> 00:05:29.980
as y equals mx plus B,

91
00:05:29.981 --> 00:05:33.400
where m is the slope of the line and B is the y intercept.

92
00:05:33.640 --> 00:05:37.960
This is useful for linear regression problems where we tried to predict the next

93
00:05:37.961 --> 00:05:39.730
data point in the sequence.

94
00:05:40.030 --> 00:05:44.830
Once you've learned some of the formulas for some of the concepts you already

95
00:05:44.831 --> 00:05:49.750
know,
it's useful to find a cheat sheet of the charts that you'll need to know.

96
00:05:49.810 --> 00:05:53.410
I've got a link to a really good one for you in the video description.

97
00:05:53.500 --> 00:05:55.600
Repetition is key to learning.

98
00:05:55.750 --> 00:05:59.840
If the only time you see your math formulas is when you open a textbook,

99
00:06:00.110 --> 00:06:03.440
there is a good chance that they're going to be unfamiliar when you need to

100
00:06:03.441 --> 00:06:04.970
start from scratch each time.

101
00:06:05.000 --> 00:06:09.740
A lot of the times you'll see the same concept over and over and over again.

102
00:06:09.890 --> 00:06:13.910
When it comes to machine learning,
sometimes it's written in different ways.

103
00:06:14.090 --> 00:06:18.530
It often happens that you think you know and understand a formula and then

104
00:06:18.531 --> 00:06:21.290
you'll see it written in another way and panic.

105
00:06:21.380 --> 00:06:25.460
One major example of that would be gradient descent.

106
00:06:25.670 --> 00:06:29.450
The most popular optimization strategy for deep neural networks.

107
00:06:29.720 --> 00:06:34.010
Same concept,
but it's written in different ways a lot of the time,

108
00:06:34.190 --> 00:06:39.190
so it's good to come to an intuition around a few key mathematical concepts that

109
00:06:39.231 --> 00:06:41.600
apply to your specific field like ml.

110
00:06:41.870 --> 00:06:46.610
I recommend my own course right here on youtube called the math of intelligence

111
00:06:46.730 --> 00:06:50.720
to help you with this.
You'll find it in the form of a playlist.
Okay,

112
00:06:50.840 --> 00:06:51.500
you're ready.

113
00:06:51.500 --> 00:06:56.150
Let's go over to popular equations from machine learning and breakdown how to

114
00:06:56.151 --> 00:06:58.670
read them to develop some more intuition.

115
00:06:59.150 --> 00:07:03.470
The first is the formula for the Euclidean distance.
First of all,

116
00:07:03.500 --> 00:07:06.050
what's the goal of this equation?
Let's figure that out.

117
00:07:06.260 --> 00:07:07.640
In the context of this paper,

118
00:07:07.910 --> 00:07:12.590
it looks like the authors are trying to produce some sort of measurement that

119
00:07:12.591 --> 00:07:17.591
shows how different to learn features are in an n dimensional feature space.

120
00:07:19.160 --> 00:07:23.930
That way they can tell if these concepts are somewhat related or not related at

121
00:07:23.931 --> 00:07:27.950
all.
It looks like there aren't any constants being used here.

122
00:07:28.040 --> 00:07:32.990
Just variables and each of these variables represents a coordinate 0.2 per

123
00:07:32.991 --> 00:07:37.400
feature.
It's subtracting the coordinates of one feature from another.

124
00:07:37.610 --> 00:07:40.340
Then squaring the result,
summing them up,

125
00:07:40.520 --> 00:07:42.740
and then taking the square root of the whole thing.

126
00:07:42.890 --> 00:07:46.760
The result is a single scalar value that represents the distance.

127
00:07:46.940 --> 00:07:50.930
We can use this for sentiment analysis,
generating similar words,

128
00:07:51.080 --> 00:07:53.780
lots of different applications.
Cool.
Right.

129
00:07:53.930 --> 00:07:58.460
All right now onto another popular equation called logistic regression.

130
00:07:58.790 --> 00:08:00.290
In general,
in machine learning,

131
00:08:00.440 --> 00:08:05.150
we're trying to come up with a function that can predict for future inputs based

132
00:08:05.151 --> 00:08:10.151
on the experience it has gained through the pass inputs and their outputs via a

133
00:08:10.220 --> 00:08:12.170
training set.
From this paper,

134
00:08:12.200 --> 00:08:16.280
we can glean that the authors are attempting to perform a multi-class

135
00:08:16.340 --> 00:08:20.150
classification with probabilities for each of the classes.

136
00:08:20.480 --> 00:08:25.430
Logistic Regression is coming up with a probability function that can give us

137
00:08:25.431 --> 00:08:30.020
the chance for an input to belong to any one of the various classes.

138
00:08:30.140 --> 00:08:32.450
Look at our training data.
There are only two classes,

139
00:08:32.600 --> 00:08:34.400
a binary classification problem.

140
00:08:34.730 --> 00:08:38.780
We need to come up with a probability function that takes in an input and

141
00:08:38.781 --> 00:08:41.330
returns the probability of Class A or B.

142
00:08:41.690 --> 00:08:45.020
This probability function is the sigmoid function.

143
00:08:45.260 --> 00:08:48.500
One great way to understand it is to just graph it.

144
00:08:48.650 --> 00:08:50.780
A picture is worth a thousand words,
right?

145
00:08:51.050 --> 00:08:55.220
Since the probability of any event to happen is between zero and one,

146
00:08:55.470 --> 00:08:59.820
this function seems fit to be used as a probability function for logistic

147
00:08:59.821 --> 00:09:04.440
regression.
We know of the constant e,
but what's data?
Well,

148
00:09:04.560 --> 00:09:07.920
the error function in logistic regression looks kind of like this.

149
00:09:07.921 --> 00:09:11.850
Don't be afraid where m is the number of elements in the training set.

150
00:09:11.970 --> 00:09:15.960
Why is either one or zero and h of x is nothing but the sigmoid function.

151
00:09:16.290 --> 00:09:20.850
Since sigmoid is a function of theta,
then Jay is a function of data,

152
00:09:20.910 --> 00:09:23.010
so our error is a function of theta.

153
00:09:23.190 --> 00:09:28.170
We minimize Jay over data and find out the values of Feta for which our error is

154
00:09:28.171 --> 00:09:32.880
minimized.
This is done using gradient descent.
Another very useful formula.

155
00:09:32.940 --> 00:09:36.000
Once we have feta and our probability function s ready,

156
00:09:36.150 --> 00:09:39.930
we can feed it any input data and it will give us a probability value.

157
00:09:39.990 --> 00:09:43.890
This equation can be applied to so many different industry problems,

158
00:09:44.010 --> 00:09:48.300
spam classification,
probability of someone voting,
classifying words,

159
00:09:48.360 --> 00:09:51.720
speech recognition,
and a lot of times it's written a little differently,

160
00:09:51.721 --> 00:09:56.460
but the concept is always the same.
Remember math is its own language.

161
00:09:56.520 --> 00:09:58.050
Memorize some of the symbols.

162
00:09:58.230 --> 00:10:03.230
Keep a formula sheet handy and build an intuition by repetitively learning and

163
00:10:03.631 --> 00:10:07.650
applying new concepts across a wide range of applications.

164
00:10:07.830 --> 00:10:09.780
Soon you'll be able to see a new problem.

165
00:10:09.930 --> 00:10:14.930
Know exactly how to categorize it into its proper type like optimization or

166
00:10:15.271 --> 00:10:20.070
boolean logic,
and understand why the equation you read works for it.

167
00:10:20.100 --> 00:10:24.150
So get on that math grind wizard.
I'm rooting for,
you know,

168
00:10:24.151 --> 00:10:27.750
coding challenge this week.
Subscribe if you want to learn now,

169
00:10:27.810 --> 00:10:31.440
let's spread this AI power for now.
I've got to learn more.

170
00:10:31.590 --> 00:10:32.850
So thanks for watching.

