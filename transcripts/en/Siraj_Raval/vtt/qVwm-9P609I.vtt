WEBVTT

1
00:00:01.510 --> 00:00:02.343
Okay.

2
00:00:06.200 --> 00:00:07.033
<v 1>Sure.</v>

3
00:00:07.500 --> 00:00:08.333
<v 0>Okay.</v>

4
00:00:20.970 --> 00:00:23.790
<v 1>Hello everybody.</v>

5
00:00:25.970 --> 00:00:30.930
Let's see.
Let me move,
make this work.
Oh,

6
00:00:30.950 --> 00:00:35.210
we're all bits or Raj?
I guys,
good to see you.

7
00:00:35.211 --> 00:00:39.000
Let me mute myself.
Okay,
there we go.
All right,

8
00:00:39.800 --> 00:00:42.710
we've got people coming in the room.
Hi everybody.
Hi.

9
00:00:42.711 --> 00:00:46.190
Pressed in high stray Leandro.
Ivan,

10
00:00:47.860 --> 00:00:49.520
uh,
Nico Vosshall.

11
00:00:50.360 --> 00:00:53.290
Nyshawn Sebastian.

12
00:00:54.240 --> 00:00:58.460
All right,
got to kill it with that fresh tensorflow.

13
00:00:59.710 --> 00:01:02.870
Uh,
Shuba deep.
It is in an ice tea this time.
Um,

14
00:01:02.900 --> 00:01:06.800
but I will get fresher stuff later.
All right,
so,
okay.
Um,

15
00:01:06.830 --> 00:01:11.630
in this livestream we are going to,
uh,

16
00:01:12.180 --> 00:01:17.180
we are going to create a classifier in tensorflow for m and ist.

17
00:01:18.980 --> 00:01:21.800
Okay.
What does that mean?
We're going to create a classifier in tensorflow,

18
00:01:22.070 --> 00:01:25.640
no TF learn,
just tensorflow,
straight up tensorflow.

19
00:01:25.910 --> 00:01:30.910
And we're going to do this for a handwritten character digits like images.

20
00:01:32.450 --> 00:01:36.150
So we're going to give it an image of like the number six and our,
our,

21
00:01:36.220 --> 00:01:39.530
our model is going to be to look at that and say,
oh,
this is a six.

22
00:01:39.710 --> 00:01:44.120
So it's going to be able to read,
uh,
the image,
the number that is in the image,

23
00:01:44.290 --> 00:01:47.480
and we're going to do this in straight up tensor flow.
All right.
Um,

24
00:01:47.720 --> 00:01:51.410
so that's what we're gonna do and we're going to start off with five minutes of

25
00:01:51.411 --> 00:01:54.110
Q and a and then I'll get right into the code.
All right,

26
00:01:54.290 --> 00:01:56.480
so five minutes starts now.

27
00:02:07.250 --> 00:02:10.820
Um,
that's cool.
Map,
right?
Yeah,
I,
uh,

28
00:02:10.821 --> 00:02:15.080
I love that map because it's really colorful.
Um,
okay.
Um,

29
00:02:19.280 --> 00:02:23.750
Openai universe.
That is a question.
Did Google contact you?
They did.
Actually.

30
00:02:23.751 --> 00:02:27.230
I'm meeting them next week.
Um,
we'll see what happens.
I mean,

31
00:02:27.231 --> 00:02:30.560
I'm sure there's something we can do together.
Uh,

32
00:02:32.030 --> 00:02:35.390
this is my apartment.
Yup.
How,
how to study chemistry,

33
00:02:36.990 --> 00:02:41.080
uh,
watch chemistry videos on Youtube.
Well,

34
00:02:41.330 --> 00:02:46.040
that's not my,
I'm not into,
why are you so awesome?
I just believe in myself.

35
00:02:46.370 --> 00:02:49.700
How does this differ from the example in the docs?
Uh,
I mean,

36
00:02:49.730 --> 00:02:53.750
this is kind of like a custom version.
How many layers will your model have?

37
00:02:53.780 --> 00:02:55.310
It's going to have four layers.

38
00:02:55.580 --> 00:02:59.240
It's going to be a convolutional neural network with four layers.
Uh,

39
00:02:59.320 --> 00:03:02.860
what is the dopest way to build AI to detect fake articles on the Internet?

40
00:03:03.220 --> 00:03:06.280
That's actually a hard problem that we are working on right now.

41
00:03:06.460 --> 00:03:11.050
Detecting if something is fake or not.
Like,
uh,
a validity test is actually like,

42
00:03:11.590 --> 00:03:16.350
um,
I mean,
what does that even look like?
Like if you had a supervisor,

43
00:03:16.360 --> 00:03:20.410
if you had a set of articles that were labeled as real and a set that labeled is

44
00:03:20.411 --> 00:03:24.700
false,
I mean,
as fake,
you could,
you could do it that way.
But even then,

45
00:03:24.701 --> 00:03:29.200
like if a fake article was good enough,
like it would just pass.
So we needed,

46
00:03:29.201 --> 00:03:33.340
we needed a vet.
We need a very,
very,
very strong,
robust model for that.

47
00:03:33.700 --> 00:03:35.500
And like,
you know,

48
00:03:35.501 --> 00:03:39.460
Google auto reply level amount of data that it's trained on.

49
00:03:39.780 --> 00:03:43.000
Will you use LSTM?
Uh,
yes.
I will.
Um,

50
00:03:45.530 --> 00:03:49.280
Mac book or windows PC Mac book.
We'll scholar be by python.
No.

51
00:03:52.170 --> 00:03:56.770
Um,
did you lift your job?
Why?
I,
we used to work at Twilio.
Um,

52
00:03:57.500 --> 00:03:58.860
uh,
but I want to do this full time.

53
00:03:59.850 --> 00:04:02.040
Why don't you make a video about NLP in particular,

54
00:04:02.041 --> 00:04:06.360
the entity type recognition problems?
Uh,
I have several videos and an LPA.

55
00:04:06.520 --> 00:04:09.280
I'm going to make more though.
What is your laptop specs?

56
00:04:09.281 --> 00:04:10.520
I just got the new Mac book.

57
00:05:27.380 --> 00:05:29.240
I got lost in the neural network.
Um,

58
00:05:33.330 --> 00:05:37.890
okay,
so let's,
let's go ahead and,
uh,
do this.
You guys can see me,
right?

59
00:05:39.080 --> 00:05:43.320
No,
no,
no.
I'm still here.
Just someone safe that they can see me.
I just had to,

60
00:05:43.321 --> 00:05:45.750
I just had a problem for a second.
Um,

61
00:05:47.850 --> 00:05:52.020
all right.
Let's see.
I'm still broadcasting.
Okay.
I'm still broadcasting.

62
00:05:53.510 --> 00:05:57.170
Audio is back.
No,
no,
they,
they can't see me.
Okay.

63
00:05:58.730 --> 00:06:03.660
Um,
came off

64
00:06:06.250 --> 00:06:10.170
right.
There we go.
All right.
Okay.
There we go.
We're

65
00:06:11.920 --> 00:06:15.010
plus code.
Uh,
and so now you guys can see me.
Okay,
here we go.
Okay,

66
00:06:15.011 --> 00:06:17.980
so we've got people watching and we're going to screen share.
Okay,

67
00:06:18.220 --> 00:06:22.260
so screen sharing time.
Um,
sure.

68
00:06:22.740 --> 00:06:23.570
Okay,

69
00:06:23.570 --> 00:06:25.710
<v 2>boom.
Um,</v>

70
00:06:27.740 --> 00:06:29.720
<v 1>all right,
so here we go.
And uh,</v>

71
00:06:33.130 --> 00:06:35.840
all right,
you guys can see everything dropped out for a bit.

72
00:06:35.841 --> 00:06:38.900
I'm going to make this bigger.
Okay,

73
00:06:38.901 --> 00:06:43.160
so now we've got 161 people watching and this we're,
okay.
So here we go.

74
00:06:43.190 --> 00:06:47.360
You guys can see.
All right.
Beautiful.
Right?
All right,
so let's do this.

75
00:06:47.361 --> 00:06:50.300
So we're going to import our first library,
which is the future library.

76
00:06:50.301 --> 00:06:53.470
Why future is going to let us,
uh,

77
00:06:57.480 --> 00:07:02.420
so future is going to let us,
there's lag,
right?
Okay.
Is there still lag?
Um,

78
00:07:03.550 --> 00:07:05.280
yeah,
I'm gonna,
I'm gonna get a,
I'm going to,

79
00:07:05.370 --> 00:07:08.970
I'm going to get a better connection next time I'm lacking.
Okay.
Hold on.

80
00:07:09.910 --> 00:07:10.743
Uh,

81
00:07:12.450 --> 00:07:14.130
<v 2>hold on.
Damn it.</v>

82
00:07:24.300 --> 00:07:25.133
<v 0>Too much.</v>

83
00:07:33.700 --> 00:07:36.040
<v 1>Audio is good.
Okay.
Bad video quality.</v>

84
00:07:38.240 --> 00:07:40.580
<v 2>Um,
hold on.</v>

85
00:07:42.250 --> 00:07:46.120
<v 1>Okay,
so,
um,
it's still lagging.
Okay,</v>

86
00:07:46.121 --> 00:07:48.750
so the video is lagging.
Um,

87
00:07:49.440 --> 00:07:50.273
<v 0>okay.</v>

88
00:07:51.190 --> 00:07:55.390
<v 1>I'm not sure and too much lag.
Okay.</v>

89
00:07:55.391 --> 00:07:58.480
So there's too much lag and

90
00:08:01.220 --> 00:08:06.110
video lags a lot.
Okay.
So just try to restart it.

91
00:08:06.140 --> 00:08:10.610
Okay.
Um,
let's see.
Let's see.
Let's see,
let's see.

92
00:08:10.910 --> 00:08:15.800
Restart.
I can restart.
Restart the one.

93
00:08:15.801 --> 00:08:16.634
Actually

94
00:08:23.350 --> 00:08:25.860
let,
I see there's a little,
I'm not sure.
Okay.

95
00:08:35.340 --> 00:08:39.630
About,
um,
one 23.
Okay,

96
00:08:39.660 --> 00:08:44.220
so one 23.
Oh,

97
00:08:44.380 --> 00:08:47.220
so everybody,
it's lagging,
right?
That's what's happening right now.
It's lagging.

98
00:08:47.430 --> 00:08:51.540
That's what's happening right now.
The stream is lagging.
All right.

99
00:08:51.541 --> 00:08:53.590
So I'm just going to keep going,
um,

100
00:08:54.000 --> 00:08:57.150
because I can't help but the lag,

101
00:08:57.151 --> 00:09:00.390
so I'm just going to keep on going and,
uh,

102
00:09:02.040 --> 00:09:06.120
it's still lagging.
Okay.
Um,
sound is okay,

103
00:09:06.121 --> 00:09:11.010
but the video is lagging.
Uh,
it's breaking.

104
00:09:14.570 --> 00:09:16.390
I'm just going to keep going.
I don't give up.
Okay.
So,

105
00:09:16.420 --> 00:09:20.360
so for future we're going to import the print print function.
Um,

106
00:09:23.790 --> 00:09:24.530
all right,
great.

107
00:09:24.530 --> 00:09:26.550
We're going to import the print function and we're going to import the cancer

108
00:09:26.590 --> 00:09:27.170
floor.

109
00:09:27.170 --> 00:09:29.960
Those are two library that we're going to inquiry and future's going so that we

110
00:09:29.961 --> 00:09:33.080
can have the print function from python two and we'll get,

111
00:09:33.170 --> 00:09:37.310
we could do it for a python three.
Okay.
So,
so there's that.

112
00:09:38.330 --> 00:09:43.250
And uh,
so now we're going to,
uh,
so,

113
00:09:43.251 --> 00:09:47.330
so we had that.
Now we're going to import our dataset.
Okay.
Um,
it's fine.

114
00:09:47.331 --> 00:09:51.410
All right.
No lag.
Close your browser tabs.
All right,
good.
Good call.

115
00:09:51.411 --> 00:09:54.380
Actually when the clothes that boom,
boom,
boom.

116
00:09:57.840 --> 00:10:01.830
So,
uh,
so,
okay.
So we're going to import our data set,
right?

117
00:10:01.831 --> 00:10:05.000
So the first thing we're going to do is we're going to import the exam.

118
00:10:05.070 --> 00:10:07.260
Our example data set of BM and ice tea.

119
00:10:07.500 --> 00:10:09.780
And I'm going to talk about what format we're going to,

120
00:10:09.810 --> 00:10:12.390
we're going to import this ass.
Okay,
so we're going to import,

121
00:10:12.600 --> 00:10:16.440
this is our helper class input data input data is going to do is it's going to

122
00:10:16.441 --> 00:10:18.000
pull our data from the web,

123
00:10:18.230 --> 00:10:21.990
are Emma and Ist art are handwritten character digits are images.

124
00:10:22.260 --> 00:10:24.720
It's going to pull that from the web and it's going to store it in our folder

125
00:10:24.960 --> 00:10:26.760
and it's going to format,
it's going to format it,

126
00:10:26.761 --> 00:10:30.840
so that's good so that we can use it in our code.
Okay,
so good.
Finally,

127
00:10:30.841 --> 00:10:34.150
there's no leg.
All right.
And I'm sorry about the lag guys.
Um,

128
00:10:34.320 --> 00:10:36.990
I'm going to not have this ever happen again.
All right,

129
00:10:37.020 --> 00:10:39.180
so I'm going to work it out of a place where there is no law.

130
00:10:39.600 --> 00:10:43.380
So we're going to create a variable called name an ist using our input data
help,

131
00:10:43.470 --> 00:10:46.140
uh,
help her class.
And we're going to,

132
00:10:47.100 --> 00:10:49.930
we're going to use the read data datasets method.

133
00:10:50.400 --> 00:10:51.990
We're going to read it from wherever it's safe.

134
00:10:52.020 --> 00:10:55.670
So it's going to save it to the temp folder are our data folder in our temp

135
00:10:55.680 --> 00:10:58.740
folder.
And we're going to,
and we're going to

136
00:11:00.390 --> 00:11:03.600
say one hot people's true.
What is one hot equals true means?

137
00:11:04.050 --> 00:11:06.510
That means that let's talk,
let's talk about this for a second.

138
00:11:06.680 --> 00:11:10.500
What is one hot encoding?
Okay,
so this shows up a lot.
One hot encoding,

139
00:11:10.890 --> 00:11:12.660
one hot encoding is a way of,

140
00:11:13.200 --> 00:11:17.700
of of formatting data of representing data.
So that it,

141
00:11:18.270 --> 00:11:22.650
so that it,
uh,
so that it's more machine readable.
So what would be an example?

142
00:11:23.440 --> 00:11:27.960
So if I had an array like car car,

143
00:11:28.200 --> 00:11:30.840
if I included this,
normally I could just say like,

144
00:11:30.870 --> 00:11:35.670
I could represent this as one zero two one.
Okay,
I could,

145
00:11:35.700 --> 00:11:39.060
I couldn't say that,
but um,
what is this?

146
00:11:39.061 --> 00:11:41.130
This looks like tooth is the greatest value.

147
00:11:41.400 --> 00:11:46.050
It looks like tooth is greater than one because the value that I'm representing

148
00:11:46.051 --> 00:11:50.880
it as is,
is greater.
But this is wrong.
Like the tooth isn't actually greater.

149
00:11:50.881 --> 00:11:53.680
I just need a way to differentiate between different,
right?

150
00:11:53.890 --> 00:11:56.370
So a better way of representing these would be,
uh,

151
00:11:56.380 --> 00:12:01.380
as binary digits so that I could represent health is one zero zero zero and then

152
00:12:01.931 --> 00:12:06.640
I could represent car as zero,
one zero zero and then I could represent tooth as,

153
00:12:06.641 --> 00:12:11.640
you know what I'm saying?
So there are different ways of representing,
uh,

154
00:12:12.060 --> 00:12:16.240
of,
of representing data and it's using this kind of binary format.

155
00:12:16.240 --> 00:12:18.670
So it's a way that lets us encode data.

156
00:12:18.671 --> 00:12:22.810
So it's more machine readable and it's good for regression in classification,
but

157
00:12:27.510 --> 00:12:30.690
no,
uh,
it,
it doesn't say that one is greater than the other.

158
00:12:30.750 --> 00:12:34.110
So that's what we call it.
That's what we say.
One hot okay.
To and coding it.

159
00:12:34.111 --> 00:12:37.650
But there's not like,
it's,
it's,
it's not greater than the other.
Okay.

160
00:12:37.770 --> 00:12:41.730
So that's one hub doctors.
Okay.
So that's us getting our dataset import data.

161
00:12:42.330 --> 00:12:46.440
All right,
so next step is to,
uh,
get the love you too,
Michael.

162
00:12:46.770 --> 00:12:50.760
Next step is to get the hyper parameters.
All right,
so hyper parameters.

163
00:12:51.180 --> 00:12:55.140
These are our tuning knobs and I'm going to talk about each of them.
So,
okay.
So,

164
00:12:55.190 --> 00:12:57.210
um,
the first one I'm going to import,
he's called the learning rate.

165
00:12:57.211 --> 00:12:58.830
Let me explain what I'm doing with here.

166
00:12:59.950 --> 00:13:00.783
<v 2>MMM.</v>

167
00:13:02.270 --> 00:13:05.330
<v 1>So why am I sending it to 0.001?
Okay.</v>

168
00:13:05.480 --> 00:13:07.610
So as data flows through our neural network,

169
00:13:07.760 --> 00:13:11.420
we were going to apply an activation function,
uh,
at each layer.

170
00:13:11.630 --> 00:13:15.200
So what this function is going to do is it's going to transform our data in some

171
00:13:15.201 --> 00:13:20.200
way and we're later going to use,
we're later going to use,
um,

172
00:13:20.870 --> 00:13:24.590
that transformed data to update the weighted connections between the layers

173
00:13:24.830 --> 00:13:27.250
until what the learning rate does is it's,

174
00:13:27.251 --> 00:13:30.320
it's what we apply to that weight updating process.

175
00:13:30.500 --> 00:13:33.730
So the greater the learning rate,
the faster our network trains,

176
00:13:33.950 --> 00:13:37.850
but the lower the learning rate,
the more accurate our neural network trains.

177
00:13:38.060 --> 00:13:42.740
Okay.
So it's,
it's that trade off between time,
like speed and accuracy,

178
00:13:42.950 --> 00:13:47.840
speed and accuracy.
Um,
and one hot is used in ward representations as well,

179
00:13:48.080 --> 00:13:50.540
and that's what kind of,
what all hyper parameters are.
They're,
they're,

180
00:13:50.570 --> 00:13:52.850
they're kind of trade off between speed and accuracy.

181
00:13:53.210 --> 00:13:55.670
So that our learning rate and now,
um,

182
00:13:57.710 --> 00:14:01.900
now we're going to have our training iteration.
So how,
how,

183
00:14:01.901 --> 00:14:06.560
how much do we want to train?
I,
so we're going to say 200,000 iterations.
Okay.

184
00:14:06.561 --> 00:14:10.430
So generally the more iterations of the better it's going to,

185
00:14:11.150 --> 00:14:14.720
the more iterations that the better our model is going to train.
Okay?

186
00:14:15.080 --> 00:14:19.310
So that's that for our training iterations.
Now we want our high Heba.
Hey guys,

187
00:14:19.311 --> 00:14:22.640
say hi Eva.
She's my friend Irl.
So for batch size,

188
00:14:22.641 --> 00:14:25.010
we want to say backside is going to be 128.
What does that mean?

189
00:14:25.220 --> 00:14:29.060
That means we have 128 samples.
That's the sides of our batch.

190
00:14:29.120 --> 00:14:33.530
The batch is what we train.
Okay.
Um,
all right.
So,
um,

191
00:14:34.730 --> 00:14:38.900
there's that and we're going to send our display that to 10.
What does that mean?

192
00:14:39.110 --> 00:14:41.960
Well,
how often do we want to display what's happening while we're training?

193
00:14:42.140 --> 00:14:45.680
Let's say every,
every 10 iterations.
So it's,
so that's what we're going to say.

194
00:14:45.681 --> 00:14:48.950
What to say every 10 iterations.
Okay.
So there's that.

195
00:14:49.430 --> 00:14:54.430
And now we want to create our network for amateurs are network parameters.

196
00:14:55.940 --> 00:15:00.490
All right,
so for network parameters,
uh,
we want to say how many,
uh,

197
00:15:00.840 --> 00:15:04.820
uh,
what is the size of our image?
Let's say seven 84.
What does that mean?

198
00:15:05.270 --> 00:15:09.160
I keep doing this minus sign,
let's say seven 84.
Um,

199
00:15:09.260 --> 00:15:12.620
so that means our image shape is going to be a 28 by 28 image,

200
00:15:12.621 --> 00:15:17.350
right at 28 pixel by 28 pixel.
That's our,
that's our,
that's our,

201
00:15:17.540 --> 00:15:21.620
uh,
besides of our image.
So,
so there's that.
And then there are number of classes,

202
00:15:22.120 --> 00:15:26.570
um,
is going to be 10.
What does that mean?
Well,

203
00:15:26.571 --> 00:15:30.170
we have 10 digits,
right?
The digits zero through nine or 10 digits.

204
00:15:30.770 --> 00:15:34.840
Now we're going to,
our last network parameter is called dropout.
Oh,

205
00:15:34.850 --> 00:15:38.960
I cannot wait to explain dropout.
I love dropout.
Okay.
Okay,
here we go.

206
00:15:41.130 --> 00:15:44.280
Dropped out.
Okay.
Is this awesome thing?

207
00:15:44.281 --> 00:15:48.930
So dropout is a technique created by Hinton.
Uh,
Landon,
I'm going to do it.

208
00:15:48.960 --> 00:15:51.810
I'm doing,
I'm going to do another Q one Q and a at the end of this session.

209
00:15:51.811 --> 00:15:53.640
All right,
so let drop out is,

210
00:15:53.641 --> 00:15:58.641
is it's this awesome method invented by Geoffrey Hinton and his team that

211
00:15:59.791 --> 00:16:03.390
prevents overfitting by randomly turning off some neurons during training.

212
00:16:03.570 --> 00:16:04.403
So data is,

213
00:16:04.690 --> 00:16:09.390
is forced to find new paths between the layers to allow for a more generalized

214
00:16:09.391 --> 00:16:11.770
model.
Okay.
So what does that mean?
So,
um,

215
00:16:11.820 --> 00:16:15.600
I think the best analogy I could think of is like old people.
Okay.

216
00:16:15.660 --> 00:16:17.670
So generally they say like the older you get,

217
00:16:17.671 --> 00:16:19.860
the more set in your ways you're are you are,

218
00:16:19.861 --> 00:16:23.520
and there's a kind of scientific truth to this because there are these neural

219
00:16:23.521 --> 00:16:24.690
pathways in our brain,

220
00:16:24.900 --> 00:16:28.530
kind of like grooves that we kind of carbon to our brain that we kind of get

221
00:16:28.531 --> 00:16:32.130
used to thinking about.
Right?
And so that's kind of what it is with dropout.

222
00:16:32.310 --> 00:16:35.790
If you train your neural network has certain way and you don't use dropout,

223
00:16:36.000 --> 00:16:38.820
then those pathways are going to continually be the same way.

224
00:16:39.000 --> 00:16:40.830
So it's going to be to train on the data,

225
00:16:40.831 --> 00:16:44.190
yet it's going to be to fit on the data you trained it on.

226
00:16:44.220 --> 00:16:45.840
It's not going to be able to generalize,
well,

227
00:16:45.960 --> 00:16:47.940
it's not going to be able to predict new data sets.

228
00:16:48.150 --> 00:16:50.880
But what you could do with dropout is while you're training,

229
00:16:51.180 --> 00:16:55.620
what dropout does is it randomly turns off neurons while while the data is

230
00:16:55.621 --> 00:16:57.570
flowing through,
right?
While the data is flowing through,

231
00:16:57.660 --> 00:16:59.640
it's randomly turning off neurons randomly.

232
00:16:59.880 --> 00:17:02.400
And so that the data is forced to find new pathways.

233
00:17:02.610 --> 00:17:05.310
And what this does is it creates a more generalized model.

234
00:17:05.460 --> 00:17:10.050
So the model is then able to train and test on all sorts of data.
Okay.

235
00:17:10.051 --> 00:17:13.770
And drop out is a very,
very successful technique.
Okay.

236
00:17:14.490 --> 00:17:16.230
So that's what dropout is.
I,
there's a little bit long,

237
00:17:16.231 --> 00:17:18.990
but that's what we're going to use for dropout.
And it's a probability.

238
00:17:19.230 --> 00:17:22.020
It's the probability value,
okay.
And like everything else,

239
00:17:22.021 --> 00:17:25.080
it's a trade off and it's all about tonight's about trying and testing what

240
00:17:25.081 --> 00:17:27.340
works best.
Okay.
So,
um,

241
00:17:30.600 --> 00:17:35.020
so,
so that's that.
Okay.
So now we're going to create our,
uh,
our,
uh,

242
00:17:35.150 --> 00:17:39.890
placeholders,
how we're going to,
uh,
how we're going to,
uh,
get the data in there.

243
00:17:39.891 --> 00:17:43.970
So what we're going to say is everybody say hi to Brian.
He's also my friend Irl.

244
00:17:44.270 --> 00:17:47.870
Um,
so we're going to take our placeholder variable are tensorflow placeholder

245
00:17:47.871 --> 00:17:50.640
variable,
and we're going to say nine.

246
00:17:51.360 --> 00:17:55.200
And then the number of inputs.
What does this mean?
This is our gateway.

247
00:17:55.410 --> 00:17:58.980
This is our gateway drop out is not like shrimps kind of.
It is actually.

248
00:17:59.050 --> 00:18:02.290
It actually,
it is a,
that's a,
that's a great point.
So we're going to,

249
00:18:02.330 --> 00:18:03.180
we're going to create,
I'm going to,

250
00:18:03.450 --> 00:18:07.560
let me just type this out and then I'm gonna explain what is happening.

251
00:18:07.830 --> 00:18:11.100
What is happening today.
Okay,

252
00:18:11.101 --> 00:18:14.370
so nine and then the number of classes.
What is this?

253
00:18:14.430 --> 00:18:17.520
What we've created two gateways.
We've created two gateway for our data.

254
00:18:17.730 --> 00:18:20.880
One gateway is for the images are for the images.

255
00:18:21.120 --> 00:18:24.060
The other gateway are for the labels.
So we're going to have an image,

256
00:18:24.061 --> 00:18:27.750
like a number,
like an image of the number zero and then the actual labels zero.

257
00:18:28.430 --> 00:18:31.810
Um,
and so both of those are going to be fed in at the same time.
So,
our,

258
00:18:31.830 --> 00:18:35.520
so our convolutional network is seeing our labels and it's seeing our images at

259
00:18:35.521 --> 00:18:40.290
the same time.
Okay,
well we need one more thing.
It's called keep probability,

260
00:18:40.650 --> 00:18:44.810
which is going.
And so TensorFlow's placeholder op,
uh,
uh,
uh,

261
00:18:45.020 --> 00:18:49.950
tensor flows,
a placeholder.
A object is what is what represents those gateways.

262
00:18:50.040 --> 00:18:52.020
It's a gateway into our computation graph.

263
00:18:52.320 --> 00:18:54.540
So that's how the data flows into our computation graph.

264
00:18:54.840 --> 00:18:57.030
Can we have our placeholders?
Okay.

265
00:18:57.300 --> 00:19:01.500
So we need one more place holder and that's kind of bleed a float 32.

266
00:19:01.650 --> 00:19:05.280
So it's a 32 bit float and that's,
that's what,
that's what are our dropout.

267
00:19:05.460 --> 00:19:07.200
I told her to drop out is going to flow into our network.

268
00:19:07.350 --> 00:19:10.770
So we have three gateways,
actually.
One is for our,
our,
our image.

269
00:19:10.771 --> 00:19:14.310
One is for our label,
and then one is for our dropout.
Okay.

270
00:19:15.960 --> 00:19:17.190
So there's that.
Um,

271
00:19:17.250 --> 00:19:21.720
and now what we want to do is create our convolutional layers.

272
00:19:22.020 --> 00:19:24.840
So let's create our convolution layers and I'm going to explain what's happening

273
00:19:24.841 --> 00:19:28.670
here.
And so what we're gonna do is I'm going to create a,
a,

274
00:19:29.630 --> 00:19:32.460
a function for this.
So it's gonna be a Tutee convolutional layer.

275
00:19:32.640 --> 00:19:35.460
It's going to take a set of or parameters.
Okay.

276
00:19:35.461 --> 00:19:37.680
What are these parameters that I'm,
there have been 40 here.

277
00:19:38.100 --> 00:19:43.100
So what we're going to do is we're going to have these four parameters and uh,

278
00:19:43.230 --> 00:19:45.780
we're going to define our convolutional layer here.

279
00:19:46.200 --> 00:19:49.850
So I'm going to type this out.
And tensorflow has a built in convolutional,
uh,

280
00:19:50.640 --> 00:19:52.710
function,
right?
And,
uh,

281
00:19:52.860 --> 00:19:55.950
so we're going to say what are our men are very our perimeters for this.
Well,

282
00:19:55.951 --> 00:19:57.880
we already imported them,
uh,

283
00:19:57.980 --> 00:20:01.710
in the main method that we just called here and we have something called
strides.

284
00:20:01.711 --> 00:20:03.000
And I'm going to explain it in a second.

285
00:20:03.001 --> 00:20:07.680
Book strides takes in these strides that I've defined previously.

286
00:20:08.220 --> 00:20:12.750
Uh,
one,
one,
one,
one,
and then padding,
it's going to equal same.

287
00:20:13.920 --> 00:20:18.850
And then,
right?
And so then x is,
so let me explain this.
Okay.
So look,

288
00:20:18.990 --> 00:20:21.090
so this is,
so this is why I,

289
00:20:22.610 --> 00:20:27.320
this is why I have um,
a into a,
its own function.

290
00:20:27.330 --> 00:20:30.530
I've quit cause there are two,
there's actually three things I want to do here.

291
00:20:30.890 --> 00:20:34.500
I want to do three.
Here's ago.
Here we go and move to the claims.

292
00:20:34.670 --> 00:20:37.460
So what's happening here?
Okay,
so here's what's happening.

293
00:20:37.490 --> 00:20:38.840
Here's what's happening.
Okay?

294
00:20:39.880 --> 00:20:40.713
<v 0>Yeah.</v>

295
00:20:41.680 --> 00:20:43.840
<v 1>Okay.
So a convolution is so,</v>

296
00:20:43.900 --> 00:20:46.480
so when we're feeding these these images into our network,

297
00:20:46.630 --> 00:20:49.210
a is basically taking a part of that,

298
00:20:49.240 --> 00:20:51.310
taking that image and transforming it in some way.

299
00:20:51.520 --> 00:20:52.990
So that's what convolutional layers do.

300
00:20:53.110 --> 00:20:56.230
They take the image and they process it and they transform it in some way.

301
00:20:56.410 --> 00:20:59.130
So each layer is going to have some,
um,

302
00:21:00.640 --> 00:21:04.330
some representation of that image and it's going to get hierarchically more

303
00:21:04.331 --> 00:21:06.730
abstract.
The higher you go in your network.

304
00:21:06.970 --> 00:21:09.880
So the first layer is going to be a two D layer,
right?

305
00:21:10.030 --> 00:21:12.730
And we're going to add our bias to it.
And what is a bias too?

306
00:21:12.910 --> 00:21:16.450
It's like all other hyper parameters.
A bias is going to,
it's going,

307
00:21:16.451 --> 00:21:19.810
it's a tuning knob.
It's going to,
you know,
I can make it go one way or the other,

308
00:21:19.811 --> 00:21:23.470
but generally what biases does that makes our model more accurate.
Okay.

309
00:21:24.460 --> 00:21:28.450
And,
uh,
so,
so there's that.
And uh,
so,

310
00:21:28.740 --> 00:21:32.020
so what is called convolution is a fancy word for transform.
Okay.
For,

311
00:21:32.021 --> 00:21:35.680
for image transforms and we're going to return the relu function.

312
00:21:35.681 --> 00:21:39.940
Relu is an activation function rectified.
Linear unit is the full name of it,

313
00:21:40.180 --> 00:21:42.910
but basically it is an activation function and we're going to,

314
00:21:42.970 --> 00:21:46.570
and we're going to use it in our convolutional layer.
Okay?
So that's what that,

315
00:21:46.750 --> 00:21:51.460
that's what's happening here.
And then strides are,
uh,
so then strides are the,

316
00:21:51.461 --> 00:21:52.970
uh,
uh,

317
00:21:53.110 --> 00:21:57.930
it's a list of integers and it's going to be a t a t a tensor,
right?
So,

318
00:21:57.950 --> 00:22:02.350
so strides are our tensors and tensor is just means that those just mean data.

319
00:22:02.650 --> 00:22:07.600
Tensors equal data.
All right.
Um,
so that's,
that's one convolutional layer.
Okay.

320
00:22:07.601 --> 00:22:12.470
That I've created that as a function.
Um,
okay.

321
00:22:12.471 --> 00:22:16.790
Thank you eva.
Okay,
so that's our convolutional layer.
And so now,
uh,

322
00:22:16.820 --> 00:22:20.690
I want to create another function called Max pooling layer.
All right.

323
00:22:20.691 --> 00:22:24.290
So then that's pooling layer is what is pooling,
well,

324
00:22:24.380 --> 00:22:27.560
let me talk about what pooling is.
We've,
we've created a convolutional layer,

325
00:22:27.561 --> 00:22:28.940
but let's talk about pooling.
Cause this,

326
00:22:29.150 --> 00:22:33.380
this happens all the time in convolutional nets.
It's like Paccar has a great,

327
00:22:34.430 --> 00:22:38.360
that's a great explanation.
Convolution is like putting filters on an image.

328
00:22:38.390 --> 00:22:42.230
It's a,
it's an increasingly abstract set of,
of,
of filters.
Okay.

329
00:22:42.290 --> 00:22:47.150
So what pooling is,
is a pooling layer takes a small rectangular blocks okay.

330
00:22:47.210 --> 00:22:51.440
From the convolutional layer and its sub samples them to produce a single output

331
00:22:51.441 --> 00:22:55.310
from that block.
So it's taking samples of an image.
It's taking little samples,

332
00:22:55.550 --> 00:22:59.750
pools.
Okay.
Like little pools from an image.
Okay.
And it's,

333
00:23:00.380 --> 00:23:01.300
so they're,

334
00:23:01.330 --> 00:23:04.640
they're going to be Max pooling because we want to take the average or the

335
00:23:04.641 --> 00:23:09.200
maximum of the,
of the learned linear combination of the neurons in the block.

336
00:23:09.590 --> 00:23:14.090
Okay.
Um,
so that's what,
that's what Max pulling is.
So we're going to,

337
00:23:14.150 --> 00:23:15.140
we're just going to do one thing.

338
00:23:15.270 --> 00:23:18.020
If we're going to say we're going to return them to the Max pooling functions.

339
00:23:18.021 --> 00:23:22.250
So tensorflow has a built in Max pooling function and we're going to,

340
00:23:22.300 --> 00:23:26.580
that's what we're going to return.
Okay.
So it's going to be the size,
um,

341
00:23:26.630 --> 00:23:30.320
one k k one,
two that,
uh,

342
00:23:30.321 --> 00:23:35.000
and so that is that that is a Ford be tenser that the 40 tensor.
Okay.

343
00:23:35.001 --> 00:23:39.140
So there are four variables there.
And then same thing about our strikes,

344
00:23:39.170 --> 00:23:43.160
that's going to be another 40 tensor a one K K one.

345
00:23:43.910 --> 00:23:47.480
And padding is going to equal saying,

346
00:23:49.040 --> 00:23:52.700
I'm going too fast.
I will slow down,
I will slow down.
Um,

347
00:23:56.340 --> 00:23:57.173
<v 0>all right,</v>

348
00:23:59.790 --> 00:24:03.910
<v 1>so that is our,
that is our pooling layer.
Um,</v>

349
00:24:03.940 --> 00:24:08.320
and so now let's quit our model.
Okay.
We've created our,
our,
our,
our uh,

350
00:24:08.380 --> 00:24:11.650
definitions here.
Now let's create our model.
Okay,
so create models.

351
00:24:12.490 --> 00:24:16.330
And um,
so we're going to say convolutional nets,

352
00:24:17.240 --> 00:24:20.470
x weights,
biases.
So let me,

353
00:24:20.471 --> 00:24:24.730
let me type out these parameters and what is,
what's going to happen here

354
00:24:29.450 --> 00:24:32.470
or model.
We're going to take our x,
which is our inputs,
our weights,

355
00:24:32.530 --> 00:24:35.380
which are the,
or the connections or synapses between our,

356
00:24:35.470 --> 00:24:37.510
our layers are biases that were,

357
00:24:37.570 --> 00:24:40.090
that's going to affect each of all layer or layers in some way.

358
00:24:40.630 --> 00:24:44.260
And then our dropout.
Okay.
So first before we do anything,

359
00:24:44.261 --> 00:24:47.720
we want to reshape our input data.
So it,
so it is,
uh,

360
00:24:47.830 --> 00:24:50.110
we want to reshape our input data.

361
00:24:50.230 --> 00:24:55.060
So is formatted for our computation graph that we're about to create.
Okay.

362
00:24:55.380 --> 00:25:00.220
Um,
a screen shot went off.
Video is down for you.
Hold on,
hold on,

363
00:25:00.221 --> 00:25:04.380
hold on.
What just happened?
Hold on.
Here we go.
Um,

364
00:25:05.000 --> 00:25:06.850
for camera off turn.
Camera on.

365
00:25:09.180 --> 00:25:10.013
<v 0>All right.</v>

366
00:25:10.900 --> 00:25:15.880
<v 1>Back videos back.
Great video is back man.
It is.
We are on an adventure today.</v>

367
00:25:15.910 --> 00:25:20.120
Okay.
Video is back,
right guys.
Okay.
So I'm going to say,
uh,

368
00:25:20.350 --> 00:25:24.700
I want to reshape my input and,
okay,
great.
And uh,

369
00:25:25.270 --> 00:25:27.520
this is the one time I'm going to have video issues guys.

370
00:25:27.521 --> 00:25:31.420
I'm just recording out of my place today.
I usually regret in my studio,

371
00:25:31.421 --> 00:25:34.390
but it's close today,
but I'm never going to let this happen again.
All right.

372
00:25:34.980 --> 00:25:37.000
I thank you all for showing up because life stream,
I'm going to,

373
00:25:37.010 --> 00:25:41.950
I do this for you guys.
I'm going to keep on doing it.
All right.
So,
um,
so,
um,

374
00:25:43.460 --> 00:25:44.990
we're,
where are we?
Right?
So we're going to,

375
00:25:45.050 --> 00:25:46.820
we're going to reshape our input data,
right?

376
00:25:46.821 --> 00:25:48.500
And what is the shape that we want it to be?
Well,

377
00:25:48.501 --> 00:25:51.050
we can define that as our parameter here.

378
00:25:51.051 --> 00:25:55.860
We can refine what our shape is going to be.
And we want our shape to be a,
uh,

379
00:25:56.180 --> 00:26:00.290
it's going to be 28.
What,
what was her conviction was 28 by eight pixels.

380
00:26:00.620 --> 00:26:04.520
And then,
uh,
our width and height,
we have here a set to one,
right?

381
00:26:04.521 --> 00:26:06.170
So that's how we're going to reshape our input.

382
00:26:06.410 --> 00:26:09.280
Now we can call our compliment leasing a layer.

383
00:26:12.200 --> 00:26:16.610
Uh,
so,
so now we're going to create our convolutional layer,

384
00:26:16.611 --> 00:26:21.080
convolutional layers.
And uh,
what is it?

385
00:26:21.081 --> 00:26:24.170
So we defined our convolutional function up there,
right?

386
00:26:24.171 --> 00:26:26.420
So now we're going to actually use it and guess what?

387
00:26:26.421 --> 00:26:28.340
We haven't actually defined our weights yet.

388
00:26:28.520 --> 00:26:30.350
I'm going to define our weights in a second.

389
00:26:30.590 --> 00:26:34.220
Right now we're defining this function without defining our weight.
Um,

390
00:26:36.430 --> 00:26:37.480
ah,
so now we can,

391
00:26:37.600 --> 00:26:42.600
we can set our biases and our biases are going to be what are our bias is going

392
00:26:42.671 --> 00:26:44.760
to be.
We haven't defined those either.
So we're gonna,

393
00:26:44.820 --> 00:26:49.530
we're just gonna use these as placeholder,
uh,
first.
Um,
and

394
00:26:50.920 --> 00:26:54.550
we're going to,
uh,
let's see.
So that's a convolutional layer.

395
00:26:54.580 --> 00:26:58.790
That's our biases and weights.
So now we have our Max pooling layer.
All right,
so,

396
00:26:58.970 --> 00:27:01.600
so we've created our convolutional layer and then we're going to add to that

397
00:27:01.601 --> 00:27:04.420
convolution and that's pooling.
Um,

398
00:27:05.260 --> 00:27:05.650
<v 0>okay.</v>

399
00:27:05.650 --> 00:27:10.060
<v 1>So then our Max pooling layer is going to take that convolution that we just
had.</v>

400
00:27:10.270 --> 00:27:12.880
So it was going to pick that Max full Tutti function that we just created.

401
00:27:13.120 --> 00:27:16.540
It's going to add another set of weights and

402
00:27:18.250 --> 00:27:20.430
<v 2>um,
uh,</v>

403
00:27:21.200 --> 00:27:25.220
<v 1>and then we're going to have our biases as it's going to be BC to boom,
boom,</v>

404
00:27:25.221 --> 00:27:29.390
boom.
All right,
so that's our convolutional layer and our max pooling layer.
Okay.

405
00:27:29.540 --> 00:27:32.540
And,
uh,
so,
so what happened here?

406
00:27:32.600 --> 00:27:34.130
So we have our accomplish on there and the next moment.

407
00:27:34.250 --> 00:27:37.250
So now that's our first layer.
That's our first layer.
Um,

408
00:27:37.310 --> 00:27:40.490
and now we're going to do our next layer.
Okay.

409
00:27:40.491 --> 00:27:43.960
So our next layer is going to be convolutional two D,

410
00:27:45.470 --> 00:27:50.120
right?
So that was our,
one of our layers.
And now we're going to,

411
00:27:51.780 --> 00:27:52.613
<v 2>uh,</v>

412
00:27:54.660 --> 00:27:57.690
<v 1>now we're going to create our next layer and it's going to take our previous</v>

413
00:27:57.691 --> 00:27:59.460
layer that we just created as an input.

414
00:27:59.850 --> 00:28:04.740
And we're going to say weights are going to be WC two.
And then,

415
00:28:04.770 --> 00:28:07.050
so again,
we're going to define our weights in a second.

416
00:28:07.260 --> 00:28:09.340
We haven't really defined all of them,
uh,

417
00:28:09.360 --> 00:28:10.680
but we're going to define them in a second.

418
00:28:11.100 --> 00:28:15.030
And that's going to be our list of weights.
Uh,
and one more thing.

419
00:28:15.031 --> 00:28:18.600
We need to Max pool this layer too,
right?
We want to Max pool each of our layers.

420
00:28:19.310 --> 00:28:20.143
Um,

421
00:28:23.660 --> 00:28:24.493
<v 2>mmm.</v>

422
00:28:25.490 --> 00:28:27.700
<v 1>All right,
so let me,
let me do that.
Uh,</v>

423
00:28:27.780 --> 00:28:32.780
Max pulled Judy convolutional the k equals two,

424
00:28:33.720 --> 00:28:38.710
and then,
okay,
so,
so that's what we got.

425
00:28:38.770 --> 00:28:41.700
So that's our Max pooling layers.
All right,
so now,
uh,
we,

426
00:28:41.710 --> 00:28:45.340
we have our layers and now we want to create a fully connected layer.
Okay.

427
00:28:45.341 --> 00:28:46.120
So we've got our,

428
00:28:46.120 --> 00:28:48.760
both of our convolutional layers and now we can create a fully connected with

429
00:28:48.970 --> 00:28:50.620
why do we create a fully connected layer?

430
00:28:50.770 --> 00:28:52.870
A fully connected layer is to stay generic layer.

431
00:28:52.871 --> 00:28:54.940
That means like all of the layers are connected to every,

432
00:28:55.150 --> 00:28:57.940
so every neuron in this slit,
in the fully connected layer,

433
00:28:58.090 --> 00:29:00.100
it's connected to every neuron in the previous layer.

434
00:29:00.370 --> 00:29:02.470
So the previous layers are two convolutional layers.

435
00:29:02.620 --> 00:29:04.150
So the convolution layers are taking,

436
00:29:04.240 --> 00:29:08.500
are creating a calm Lucian's or transformations or filters of the image,
right?

437
00:29:08.501 --> 00:29:12.040
And then a fully connected layer isn't,
it's just representing that the image,

438
00:29:12.880 --> 00:29:14.140
the image data,

439
00:29:14.230 --> 00:29:18.100
it's just a representation of image data without it transforming it in a

440
00:29:18.101 --> 00:29:20.210
convolutional way.
So the first thing we want to do,

441
00:29:20.211 --> 00:29:24.790
because we want to reshape it,
right?
We're going to reshape the data for our

442
00:29:24.830 --> 00:29:25.663
<v 2>mmm.</v>

443
00:29:27.810 --> 00:29:30.630
<v 1>Uh,
for our convolutional layer.</v>

444
00:29:31.020 --> 00:29:35.130
And then we want to get the shape,
uh,
and it's going to be as a list.

445
00:29:35.790 --> 00:29:39.000
It's going to be as a list of inputs.

446
00:29:40.620 --> 00:29:41.453
<v 2>MMM.</v>

447
00:29:44.860 --> 00:29:47.110
<v 1>And so,
okay,</v>

448
00:29:47.111 --> 00:29:51.340
so that is our first connected layer.

449
00:29:53.740 --> 00:29:58.190
<v 2>And so now,
um,
I am,</v>

450
00:30:00.260 --> 00:30:03.110
<v 1>I am doing the next one.
Okay.
So,
so the next one is going to be,</v>

451
00:30:03.111 --> 00:30:05.650
we're going to add,
uh,
so,

452
00:30:05.660 --> 00:30:09.110
so this is where the actual matrix multiplication happens.
Okay.
We're gonna,

453
00:30:09.140 --> 00:30:12.650
we're gonna take,
this is what the actress actual matrix multiplication happens.

454
00:30:13.020 --> 00:30:15.310
Um,
this is where the right.
So for cars said,

455
00:30:15.311 --> 00:30:18.080
this is where the actual classification happens.
This is where the,

456
00:30:18.081 --> 00:30:19.540
this is where the Matrix multiply happened.

457
00:30:19.541 --> 00:30:22.340
So all that data that we've been transforming,
this is work.

458
00:30:22.370 --> 00:30:24.530
This is where we kind of combine it together.
Okay?

459
00:30:24.531 --> 00:30:27.380
So what we're going to do is we're going to perform matrix multiplication using

460
00:30:27.381 --> 00:30:30.980
tensorflow.
So technical has a built in matrix multiplication function.

461
00:30:31.370 --> 00:30:35.900
And for our weights,
we want to use the same waste that we had.

462
00:30:36.440 --> 00:30:39.940
Um,
uh,
and then we've got to,

463
00:30:39.990 --> 00:30:43.080
we're going to use our biases as well.
Okay?
So we're gonna use our weight.

464
00:30:43.110 --> 00:30:47.370
I'm going to use our biases,
right?
So there's that and now,
now,

465
00:30:47.640 --> 00:30:52.410
okay,
now that we've done that,
uh,
we're going to apply dropdown.
Okay?

466
00:30:52.420 --> 00:30:53.011
We've,
we've,

467
00:30:53.011 --> 00:30:57.060
we've created our convolution layers are fully connected layers and now we can

468
00:30:57.061 --> 00:31:01.220
apply dropout.
Oh,
you know what?
I guess what,
I forgot something I forgot to.
Uh,

469
00:31:01.230 --> 00:31:04.260
at our activation function,
what does our activation function?

470
00:31:04.370 --> 00:31:09.300
It is Relu and we're going to apply it to that,
that,
uh,
at layer.

471
00:31:09.301 --> 00:31:11.970
Okay.
So now we're going to apply,
dropped out.
So we're going to take,

472
00:31:12.120 --> 00:31:12.730
so guess what?

473
00:31:12.730 --> 00:31:17.460
Tensorflow has a built in dropout function and we define dropout earlier.
Um,

474
00:31:19.540 --> 00:31:23.590
okay.
Line 45 check the parameters.
Um,

475
00:31:24.130 --> 00:31:28.840
let's see.
Convolutional one weights and biases be too.
Yeah,

476
00:31:29.320 --> 00:31:32.740
that works.
Right.
Um,
so there's that.

477
00:31:32.741 --> 00:31:36.940
And so now we're going to say for our output,
um,

478
00:31:37.310 --> 00:31:41.590
mine 45 what's,
what's going on line 45,
line 45 we have max pooling.

479
00:31:42.110 --> 00:31:42.943
<v 2>MMM.</v>

480
00:31:44.440 --> 00:31:48.940
<v 1>Max pooling a convolutional one.
Max Pool Two d.</v>

481
00:31:49.030 --> 00:31:50.260
What is happening over here?

482
00:31:52.570 --> 00:31:56.470
I'm trying to figure this out.
I thought line 46,

483
00:31:57.100 --> 00:31:58.530
uh,

484
00:32:03.990 --> 00:32:08.820
Max pool two D or,
anyway,
I'm 40 is missing brackets.
Oh,

485
00:32:08.821 --> 00:32:12.840
there it is.
Okay,
great.
Thank you.
Okay,
cool.
Okay,
so anyway,
so we applied,

486
00:32:12.841 --> 00:32:14.490
dropped out and now we're going to have our output.

487
00:32:14.600 --> 00:32:17.880
And so what does our output and be output is going to predict our class.

488
00:32:18.060 --> 00:32:20.520
So output is going to predict our class.
Okay.

489
00:32:22.440 --> 00:32:25.210
So we're going to say,
so now that we've,
we're at the bay

490
00:32:29.450 --> 00:32:32.090
for Matrix multiplication on everything that we've calculated beforehand.

491
00:32:32.660 --> 00:32:35.000
Everything that we calculated beforehand,
cause we're going to say our weights,

492
00:32:35.270 --> 00:32:38.880
that's going to be out.
And then our biases.
Um,

493
00:32:43.250 --> 00:32:44.430
we're going to be out

494
00:32:46.690 --> 00:32:48.640
and now we're going to return out.

495
00:32:50.440 --> 00:32:54.400
I'm 47 accomplishment too.
Aww,
thank you.
That's,

496
00:32:54.401 --> 00:32:56.710
thank you Pippa.

497
00:32:56.830 --> 00:33:00.460
I appreciate that bias is out and then we return out.
Okay.

498
00:33:01.280 --> 00:33:05.170
And that good and whatever we're turning here.
That is our class classification.

499
00:33:05.500 --> 00:33:09.700
Okay.
That is our class calcification.
All right.
Um,

500
00:33:09.820 --> 00:33:14.810
and
hold on.
Hold on.
Matrix multiplication to Dah,
Dah,

501
00:33:14.811 --> 00:33:18.690
Dah.
Awesome.

502
00:33:22.220 --> 00:33:27.140
47,
so 47,
college on TV at what's,
what's happening on 47.

503
00:33:27.170 --> 00:33:30.950
Okay.
A 47 is accomplished on Tuesday.
Yes.

504
00:33:31.730 --> 00:33:33.830
Right.
Um,

505
00:33:36.770 --> 00:33:39.040
okay,
so now we were turned out okay.

506
00:33:42.390 --> 00:33:46.670
Convolution on what it is you have to reshape

507
00:33:48.650 --> 00:33:49.483
dates.

508
00:33:54.710 --> 00:33:58.190
All right.
So anyway,
I'm just going to keep going here.
So now,
uh,

509
00:33:58.260 --> 00:34:02.010
now let's create our weights free weights.
Um,

510
00:34:05.170 --> 00:34:09.120
yeah,
let me make my editor a little wider.
Um,

511
00:34:10.010 --> 00:34:13.120
I'm not gonna be able to see the comments fully.
Okay.
Anyway,

512
00:34:13.121 --> 00:34:15.640
so let's create our weights.
Weights

513
00:34:17.380 --> 00:34:20.450
are going to be,
so we're going to create these weights as a dictionary,
right?

514
00:34:20.980 --> 00:34:24.800
It's time for us to create our weights.
And so our weights are going to be,
uh,

515
00:34:25.140 --> 00:34:27.520
I was the for tensor flow variables.
Okay.

516
00:34:27.521 --> 00:34:31.020
So are wasting gonna be a list of four tensorflow variables and uh,

517
00:34:31.180 --> 00:34:36.100
we're going to say,
do you have variable,
do you have that random I will.

518
00:34:36.250 --> 00:34:39.080
Yeah,
I can use sense of boy,
why,
why don't I use tens of warp it is,
okay.
I'll,

519
00:34:39.090 --> 00:34:43.720
I'll use sensor board.
Oh,
when I'm done.
Uh,
right.
So let me,
I'm gonna okay.

520
00:34:43.721 --> 00:34:46.240
So it's going to be kind of hard for me to,

521
00:34:46.390 --> 00:34:48.190
to write this out and explain it the same time.

522
00:34:48.191 --> 00:34:51.520
So what I'm going to do is I'm going to just say,

523
00:34:52.290 --> 00:34:55.180
I'm going to write this out and ban.
I'm going to explain what these weights are.

524
00:34:55.420 --> 00:34:59.860
So these are a list of that list of values and um,

525
00:35:01.650 --> 00:35:02.221
but what does this do?

526
00:35:02.221 --> 00:35:06.820
We're going to say our variable is a TFF random normal.

527
00:35:07.270 --> 00:35:10.750
Uh,
it's what's going to be the same kind of thing,
right?

528
00:35:10.751 --> 00:35:14.920
We're just going to have four tensor flow variables.
And each of these,
uh,

529
00:35:15.220 --> 00:35:19.870
variables,
uh,
is
okay.

530
00:35:20.050 --> 00:35:22.960
Each of these variables is going to be a different value.

531
00:35:25.320 --> 00:35:26.000
<v 0>Okay?</v>

532
00:35:26.000 --> 00:35:29.660
<v 1>Uh,
okay.</v>

533
00:35:29.661 --> 00:35:31.220
And so it's going to be 64.

534
00:35:35.030 --> 00:35:36.110
So,
so there's that.

535
00:35:36.170 --> 00:35:40.500
And then I one got variable.

536
00:35:40.650 --> 00:35:43.120
So here I go.
I'm about to finish writing these up.
Miss,

537
00:35:43.121 --> 00:35:45.510
I'm on my third of a four.
And

538
00:35:47.790 --> 00:35:50.090
do you have top random normal

539
00:35:52.070 --> 00:35:57.020
seven times seven times 64,

540
00:35:58.060 --> 00:36:00.080
uh,
one 24.

541
00:36:02.520 --> 00:36:05.590
What am I doing here?
So that's not an actual,
uh,

542
00:36:07.480 --> 00:36:08.540
in 24.

543
00:36:13.090 --> 00:36:17.590
One more variable guys.
No,
no,
no.
One more variable.
Uh,
don't,
don't worry about it.

544
00:36:17.591 --> 00:36:19.750
One more variable and we're good to go.
Alright,

545
00:36:19.840 --> 00:36:24.640
so CF got variable,
um,

546
00:36:24.940 --> 00:36:28.390
gift up random normal,
right?
It's the same thing.
All of our weights we want.

547
00:36:28.900 --> 00:36:29.561
And then we're going to say,

548
00:36:29.561 --> 00:36:33.790
and I'm going to explain what's happening here in a second.
Okay?
Okay.

549
00:36:33.791 --> 00:36:36.250
So here's what's happening.
Here's what's happening.
Here's what's happening.

550
00:36:36.580 --> 00:36:40.880
We have our first set of weights.
Um,
uh,
it's a five by,

551
00:36:41.010 --> 00:36:42.080
so here's our first set of white.

552
00:36:42.090 --> 00:36:46.050
This is a five by five convolution with one input and 32 outputs.

553
00:36:46.230 --> 00:36:51.180
So it's a five by five that's at width.
And Height and then of of of our input.

554
00:36:51.330 --> 00:36:55.190
And then if there's going to be one input that's going be an image.
And um,

555
00:36:55.840 --> 00:36:56.210
<v 2>okay,</v>

556
00:36:56.210 --> 00:36:59.990
<v 1>32 outputs,
that's the bit that,
that's a number of bits.
Another one,</v>

557
00:36:59.991 --> 00:37:04.030
five by five 32 inputs,
64 outputs.
What are 32 meets?

558
00:37:04.050 --> 00:37:08.450
They're 30 32 different connections going 32 different ways and 64 right?

559
00:37:08.540 --> 00:37:11.570
So it's taking one image and it's splitting it and splitting it like

560
00:37:11.571 --> 00:37:14.420
increasingly through our network.
These are synaptic connections.
Okay.

561
00:37:14.750 --> 00:37:16.940
And so then it's going to be our fully connected layer.

562
00:37:16.970 --> 00:37:20.360
That's the way for a fully connected layer seven by seven by 64 inputs,

563
00:37:20.570 --> 00:37:22.880
1,024 outputs.
Okay.

564
00:37:23.350 --> 00:37:24.183
<v 2>MMM.</v>

565
00:37:24.720 --> 00:37:27.660
<v 1>Lastly,
it's going to take 10 24 inputs at the last layer.</v>

566
00:37:27.810 --> 00:37:29.940
And then this is where we actually predict our class.

567
00:37:30.120 --> 00:37:33.030
The number of classes is going to be 10 so those are our weights.
Okay.

568
00:37:33.420 --> 00:37:37.010
Now let's go ahead and construct our model.
Okay.

569
00:37:37.011 --> 00:37:42.011
So let's construct our model construct model and let me know if it's likely or

570
00:37:42.700 --> 00:37:45.550
not.
Like,
if it's,
let me know if it's not lagging because,
okay.

571
00:37:45.551 --> 00:37:48.610
So I would appreciate that.
So let's construct a model.
Well,
guess what?

572
00:37:48.611 --> 00:37:53.040
We already defined our,
uh,
Anthony Johnson.

573
00:37:53.041 --> 00:37:57.950
That was a thug life moment right there.
Thanks for saying that.
Uh,
uh,

574
00:37:58.250 --> 00:38:01.650
I should go a little slower day.
Yeah.
Uh,
so,

575
00:38:03.540 --> 00:38:04.373
<v 2>uh,</v>

576
00:38:07.160 --> 00:38:09.010
<v 1>the weights are massive,
aren't they?
They weights or Matt.</v>

577
00:38:09.140 --> 00:38:11.030
So now we're going to construct our model.

578
00:38:11.210 --> 00:38:14.210
What are we going to stroke constructed with?
We're going to construct it with,

579
00:38:14.620 --> 00:38:17.680
uh,
our input data x,
our weights.
So we'll,

580
00:38:17.681 --> 00:38:22.370
we just define our biases and our,
uh,
key prom,
which is our,
uh,

581
00:38:22.380 --> 00:38:25.160
dropout.
Okay.
So that's what we are constructing our model now.

582
00:38:25.430 --> 00:38:28.400
Now let's define our optimizer and our loss.

583
00:38:28.640 --> 00:38:31.670
And what can we do that we can start training?
So what is our loss?

584
00:38:31.671 --> 00:38:34.190
We could call it loss or what you'd call it,
costs,
let's call it costs.

585
00:38:34.960 --> 00:38:38.650
We're going to use cancer close reduced me function,
which is,

586
00:38:38.750 --> 00:38:42.940
which is kind of synonymous with reducing the loss.
Okay.
Um,

587
00:38:43.980 --> 00:38:47.590
and what is the type of loss function that we want to use?
Well,
Kaiser flows.

588
00:38:47.591 --> 00:38:51.970
Neural network class has the lost function that is good for image classification

589
00:38:52.390 --> 00:38:56.710
called soft Max.
Cross entropy with logic.

590
00:38:57.070 --> 00:39:00.700
Okay.
And let me explain what the asterisk is.
Okay.

591
00:39:00.701 --> 00:39:04.570
This is a big fancy word right here.
A big fancy mathematical words.

592
00:39:04.720 --> 00:39:05.650
So what does this doing?

593
00:39:05.770 --> 00:39:10.570
This is measuring the probability error and a classification task.
Okay.

594
00:39:10.571 --> 00:39:11.404
It's measuring,

595
00:39:11.410 --> 00:39:15.340
it's measuring the probability error in a class of classification task.
Um,

596
00:39:15.370 --> 00:39:17.680
and it's when the classes are mutually exclusive.

597
00:39:18.010 --> 00:39:21.430
So that means that an image that is at zero and the enemies that are one our

598
00:39:21.790 --> 00:39:26.620
exclusive.
Okay,
so that's our,
that's our,

599
00:39:26.650 --> 00:39:29.990
that's our loss or costs.
Let me just call it cost for simplistic.

600
00:39:30.160 --> 00:39:31.870
And now let's define our optimizer.

601
00:39:32.020 --> 00:39:35.230
What is our optimizer and why do we even need an optimizer?
Well,

602
00:39:35.920 --> 00:39:38.980
whenever we are creating a neural network,
we want to create a,

603
00:39:39.220 --> 00:39:41.920
an optimizer and we're gonna use an optimizer call,
Adam.

604
00:39:42.030 --> 00:39:45.410
And what Adam does is it reduces the loss over time.

605
00:39:45.620 --> 00:39:50.380
Grow gradient descent,
process a gradient descent process.

606
00:39:50.770 --> 00:39:55.590
Okay,
I'm using the latest version of tensorflow and what are,

607
00:39:55.810 --> 00:39:56.860
what is our learning rate?
Well,

608
00:39:56.920 --> 00:39:59.620
it's going to take our learning rate is a parameter which we already define.

609
00:40:00.040 --> 00:40:03.220
Okay.
And what is it going to do?
Well,
it's going to,
and guess what?
There's a,

610
00:40:03.400 --> 00:40:07.840
there's a function for this.
Minimize our cost.
That's what optimizer does.

611
00:40:07.841 --> 00:40:12.430
It minimizes our costs.
And the more we minimize our costs,

612
00:40:12.640 --> 00:40:16.990
the more accurate our model gets.
And,
and why are we using the atom optimizer?

613
00:40:17.140 --> 00:40:19.060
Adam has become really popular recently.

614
00:40:19.240 --> 00:40:23.650
There are other types of optimizers we can use like add or Grad.
Okay.
Um,

615
00:40:23.710 --> 00:40:25.330
but for this case,
we win.

616
00:40:29.470 --> 00:40:32.380
He just said,
I'm going to use tensorflow to visualize these weights at the end.

617
00:40:32.381 --> 00:40:35.050
Yes.
So now that we've created our optimizer enough cost,

618
00:40:35.290 --> 00:40:36.880
now we can evaluate our model.

619
00:40:37.030 --> 00:40:40.480
So what does this look like for a model evaluation?
We want to say,
well,

620
00:40:40.481 --> 00:40:42.400
what is a correct prediction?
What does that look like?

621
00:40:42.760 --> 00:40:46.090
We want to make sure are correct prediction.
We want to see,

622
00:40:47.120 --> 00:40:47.630
<v 0>okay.</v>

623
00:40:47.630 --> 00:40:50.750
<v 1>Okay.
So I'm going a little too fast.
Me,
calm down.</v>

624
00:40:51.260 --> 00:40:55.870
Everybody take a deep breath with me,
right?
Uh,
deep laggy breath with me.
Right?

625
00:40:59.080 --> 00:41:03.490
Okay.
We're calm everybody.
It's all good.

626
00:41:03.700 --> 00:41:06.820
It's all good.
Okay.
So now what are we doing?

627
00:41:06.850 --> 00:41:11.290
We are evaluating a model and we're using to plus equal function,

628
00:41:12.160 --> 00:41:13.780
um,
to man.

629
00:41:16.170 --> 00:41:18.280
We are going to use tensorflow as equal function.

630
00:41:19.550 --> 00:41:24.550
<v 2>You got the difference between the predicted value and um,</v>

631
00:41:28.780 --> 00:41:33.620
uh,
and
uh,
what uh,

632
00:41:35.130 --> 00:41:36.070
<v 1>the test data is.</v>

633
00:41:36.071 --> 00:41:40.780
So we have test data and then we have our predicted value and want to want to

634
00:41:40.781 --> 00:41:43.720
see the difference between that.
Okay.
So that's what we're doing here.

635
00:41:44.650 --> 00:41:46.950
And security.
What is our action,
Chrissy?
This,

636
00:41:46.951 --> 00:41:49.450
he's going to be a difference between our correct park,

637
00:41:49.510 --> 00:41:53.510
correct prediction and we'll whatever we bit,
yeah,

638
00:41:53.800 --> 00:41:58.130
so we're going s so you have to ask a correct prediction and uh,

639
00:41:58.640 --> 00:42:02.620
and that value is going to be a float float 32.
Okay.

640
00:42:02.621 --> 00:42:04.270
So now we can initialize our variables.

641
00:42:04.300 --> 00:42:06.180
So let's initialize our tentacled barracks.

642
00:42:06.280 --> 00:42:10.810
I guess what we always want to do this and tensorflow,
we always want to,
uh,

643
00:42:11.140 --> 00:42:12.900
we always want to,
um,

644
00:42:17.910 --> 00:42:21.840
no video.
Hold on,
hold on.
Oh,
there's no video.
Hold on.
Let me fix that.

645
00:42:23.300 --> 00:42:24.960
<v 2>Boom.
Boom.</v>

646
00:42:27.430 --> 00:42:31.090
<v 1>There we go.
Video is back.
Okay.
So we're,
we're getting through this guys.</v>

647
00:42:31.091 --> 00:42:33.070
We're getting through this.
We're,
we're almost there.
Now.

648
00:42:33.071 --> 00:42:35.000
We're ready to train our graph and we're,
or we're good to go.

649
00:42:35.260 --> 00:42:39.130
So we're going to initialize the variables and the variables are going to be,

650
00:42:39.180 --> 00:42:40.480
so how do,
what are we doing here?

651
00:42:41.080 --> 00:42:43.630
Every time we initialize our tension flow glass graph,

652
00:42:43.870 --> 00:42:47.650
we want to initialize our variables,
okay?

653
00:42:48.520 --> 00:42:50.720
Every time we wanted,
we're going to do that.
So now we,

654
00:42:50.790 --> 00:42:53.800
we've initialize our variables.
We can launch the graph,
the graph base,

655
00:42:53.801 --> 00:42:57.940
our neural network,
but we full,
we call it the computation graph,
okay?
So with,

656
00:42:58.250 --> 00:43:00.550
so how do we do that while we create a tensor flow session?

657
00:43:01.010 --> 00:43:04.510
And a graph is encapsulated by a session.
Okay?

658
00:43:05.830 --> 00:43:10.270
So we're going to say Rhonda session,
and we're going to initialize a session.
Uh,

659
00:43:11.020 --> 00:43:12.430
we're going to initialize a session.

660
00:43:12.870 --> 00:43:14.970
<v 2>Um,
uh,</v>

661
00:43:15.430 --> 00:43:19.960
<v 1>okay?
And we're going to say
equals one.</v>

662
00:43:20.820 --> 00:43:24.100
I said we're going to keep training until we reached maximum iterations.

663
00:43:25.050 --> 00:43:27.640
All right?
So let's,
let's go ahead and do that.
So,

664
00:43:31.330 --> 00:43:36.010
so while,
um,
the step,
so now in this while loop,
we're going to say,

665
00:43:36.011 --> 00:43:37.350
we're going to pick the batch size,

666
00:43:37.740 --> 00:43:42.640
I'm going to say is less than the training iterations.
Um,
let's see,

667
00:43:42.641 --> 00:43:43.890
what is,
what's happening here.
We're gonna,

668
00:43:43.990 --> 00:43:48.430
we're gonna run our optimization and we're going to feed it,

669
00:43:51.020 --> 00:43:51.350
uh,

670
00:43:51.350 --> 00:43:56.120
the values and from our batch and our,

671
00:43:57.170 --> 00:44:01.350
uh,
both of our batches and bar.

672
00:44:01.580 --> 00:44:05.120
What's the last thing?
Our dropout or dropout.
Okay.

673
00:44:05.330 --> 00:44:07.100
So raw and our dropout.

674
00:44:09.960 --> 00:44:11.900
All right.
Um,

675
00:44:15.560 --> 00:44:16.820
so,
okay,
so there's that.

676
00:44:16.821 --> 00:44:19.760
And then we want to print out what's happening here on your printout,

677
00:44:19.761 --> 00:44:23.950
the iteration step and,
uh,

678
00:44:24.380 --> 00:44:25.213
iteration step.

679
00:44:30.870 --> 00:44:31.770
Okay.
So hold on.

680
00:44:31.950 --> 00:44:35.880
So now it's just gonna be a bunk of print statements and man,

681
00:44:35.910 --> 00:44:40.830
it is really laggy this time.
And I'm like,
okay.
So anyway,
uh,
so,
okay,

682
00:44:40.831 --> 00:44:45.600
so we have that.
We've our model,
so,
all right,

683
00:44:45.601 --> 00:44:48.690
so let's see what this looks like.
So Python,
hold on,

684
00:44:50.340 --> 00:44:53.610
hold on.
Maximum.
This is what's happening here.

685
00:44:54.760 --> 00:44:56.140
I Don.
Bye.

686
00:45:00.240 --> 00:45:04.830
Right?
So this is what it looks like.
Um,

687
00:45:05.690 --> 00:45:09.440
I'm going to have to close that quote in the end.
Okay.

688
00:45:09.540 --> 00:45:10.950
So here's what I'm not going to train right now.

689
00:45:10.951 --> 00:45:12.960
So that's going to add more lag because it's computation,

690
00:45:12.961 --> 00:45:15.470
but what it's doing is going to train on our,

691
00:45:15.530 --> 00:45:19.140
in our labeled data set and it's going to test on the,
on the label on the,

692
00:45:19.141 --> 00:45:23.750
on the dailies that we,
we uh,
have his testing data.
And let me,
um,

693
00:45:24.420 --> 00:45:27.820
let me show you guys what I mean by,
uh,

694
00:45:29.130 --> 00:45:33.360
hold on.
I am an ist browser visualization.

695
00:45:33.970 --> 00:45:38.380
I mean I have a,
I have a little,
yeah,

696
00:45:41.380 --> 00:45:44.500
here's what it looks like.
Um,
hold on.

697
00:45:48.130 --> 00:45:50.900
It all right.
Nothing's working.
All right.
Um,

698
00:45:55.600 --> 00:45:59.860
hold on.
Okay guys,

699
00:45:59.960 --> 00:46:02.620
I um,
we have,

700
00:46:02.830 --> 00:46:07.720
there's a lot of lag this time and so what I am doing right now is I'm going to

701
00:46:07.810 --> 00:46:08.770
close out the stream.

702
00:46:08.771 --> 00:46:12.670
I'm going to post the code on get hub and I'm going to do a last,
hold on,

703
00:46:13.150 --> 00:46:17.110
let me get that camera,
turn the camera off,
turn camera on,
make sure it's there.

704
00:46:18.220 --> 00:46:21.040
Hold on,
hold on,
hold on,
come on camera.

705
00:46:24.790 --> 00:46:29.230
All right,
there we go.
So,
okay,
I'm going to do a five minute Q and a now.
Okay.

706
00:46:29.231 --> 00:46:32.260
And then we're going to close out the session and the next time there's going to

707
00:46:32.261 --> 00:46:36.250
definitely be no lag.
All right,
so go ahead and ask a five minute questions.

708
00:46:36.251 --> 00:46:38.740
Q and A.
I'm going to get the,
I'm going to get those.

709
00:46:38.980 --> 00:46:41.380
I'm going to get the code and get help later.
Um,

710
00:46:44.670 --> 00:46:49.470
uh,
let's see.
Uh,
my parents are from India.
I was born in Houston,
Texas.

711
00:46:50.070 --> 00:46:54.630
We need a video on Lstm for audio.
When that's happening on Friday.

712
00:46:55.050 --> 00:46:58.290
Uh,
rob,
I have video on Lstm for audio happening.

713
00:46:58.560 --> 00:47:03.450
What graphics card do you use to train models?
An nvidia titan x would be ideal.

714
00:47:03.960 --> 00:47:08.890
Um,
what color scheme are you using?
And sublime text.

715
00:47:08.920 --> 00:47:12.160
It's the default one.
What school do you stuck?
What school you study?

716
00:47:12.161 --> 00:47:16.330
I studied at Columbia.
Heard about Amazon go.
I think it's a great idea.
Well,

717
00:47:16.360 --> 00:47:18.990
which will be a great force for CV,
computer vision.
Um,

718
00:47:20.140 --> 00:47:22.090
the self driving car course on your destiny.

719
00:47:22.420 --> 00:47:24.880
Can you help me write a neural network to predict if a girl is into me?

720
00:47:24.881 --> 00:47:25.910
How would that work?
Uh,

721
00:47:26.030 --> 00:47:29.350
you'd have to have a label Dataset of girls that are into you and then some of

722
00:47:29.380 --> 00:47:33.130
set of features of why they are into you or maybe maybe unrelated.

723
00:47:33.160 --> 00:47:35.530
That's actually a great quote.
Maybe that could be an unsupervised problem,

724
00:47:35.800 --> 00:47:37.810
girls that are into you versus not.
Well,

725
00:47:37.811 --> 00:47:40.180
you'd have to have a set of girls that you know are into for sure.

726
00:47:40.330 --> 00:47:41.290
And then a set of features.

727
00:47:41.291 --> 00:47:44.140
So you could learn those features or you could just hard code those feature that

728
00:47:44.141 --> 00:47:47.570
would be better.
Um,
maybe the hair type,
what they look like.
Um,

729
00:47:47.590 --> 00:47:48.610
what are the important features,

730
00:47:48.611 --> 00:47:51.160
what type of girl is into maybe their personality.

731
00:47:51.280 --> 00:47:54.950
And then based on that you could kind of predict what other girls would be Ntu.

732
00:47:55.300 --> 00:47:58.000
Um,
if you ha if you were able to extract those features,

733
00:47:58.001 --> 00:48:02.680
Facebook data would actually work for this.
Um,
our region,
San Francisco one day.

734
00:48:02.681 --> 00:48:06.370
Great.
How should it started as a newbie and AI?
Watch my videos.
Uh,

735
00:48:06.371 --> 00:48:09.770
what company do you want to work for?
Uh,
no company,
but I want to,

736
00:48:09.880 --> 00:48:12.550
I love open Ai.
I love them.

737
00:48:12.580 --> 00:48:17.350
And I and I and maybe Google Age,
sex,
height.
I'm 25.
I'm male.

738
00:48:17.351 --> 00:48:22.330
I'm six feet video on string classification.
I have one,
I have several actually,

739
00:48:22.331 --> 00:48:26.380
but I'm going to,
I'm going to do more,
um,
speak something for school,

740
00:48:30.880 --> 00:48:31.713
uh,

741
00:48:34.300 --> 00:48:37.420
and start downloading code for all of my videos and start writing them locally

742
00:48:37.421 --> 00:48:40.840
on your machine whenever you have spare time and try to get your professors to

743
00:48:40.841 --> 00:48:44.440
learn to do more machine learning stuff in all of your computer science class.

744
00:48:44.590 --> 00:48:47.650
But she wanted,
it can be applied to almost every field of computer science.
No,

745
00:48:47.651 --> 00:48:51.070
not almost every field of computer science,
c plus plus or python,

746
00:48:51.071 --> 00:48:54.800
python and um,
no video.
Um,

747
00:48:56.740 --> 00:48:59.590
how long do you sit on the PC and do research?
I'm on my computer all day.

748
00:48:59.700 --> 00:49:02.170
Care Ross is a great thing,
is a great library for starters,

749
00:49:02.171 --> 00:49:07.000
but I would prefer TF learn for starters.
Okay.
Um,
all right.

750
00:49:07.001 --> 00:49:11.980
So what type of music you like?
Mostly hip hop,
uh,
Tupac,
Kanye West,
uh,

751
00:49:12.280 --> 00:49:17.050
a lot of electro and a,
yeah.
What's your favorite project in ml?
I like,

752
00:49:17.110 --> 00:49:19.810
um,
man,
uh,

753
00:49:20.810 --> 00:49:22.960
I like the synthetic gradients paper that,
uh,

754
00:49:23.020 --> 00:49:26.650
deepmind came out with that was really like mind blowing to me.
Okay.
Uh,

755
00:49:26.680 --> 00:49:30.520
sex or python.
Oh man,
that's a hard one.
Uh,

756
00:49:31.310 --> 00:49:33.640
sex is so like,
it just happens then it's gone.

757
00:49:33.641 --> 00:49:37.810
So I would say python if I had to pick.
I know that.
So anyway,

758
00:49:38.020 --> 00:49:41.170
I love terminal.
I love seeding into directories and making directories.

759
00:49:41.171 --> 00:49:45.340
I just love being in a unix shell.
It's so awesome.
And,
uh,
anyway,

760
00:49:46.570 --> 00:49:48.490
what is my motivation to continue learning?
Machine learning?

761
00:49:48.491 --> 00:49:50.470
Machine learning is the solution to all pro.

762
00:49:50.480 --> 00:49:52.150
It is going to help us solve everything.

763
00:49:52.151 --> 00:49:56.020
It is the most important thing in the entire world.
Okay.
Um,

764
00:49:56.200 --> 00:50:00.100
we have to solve intelligence guys.
Uh,
uh,
okay.
Where should I start?

765
00:50:00.101 --> 00:50:01.250
Learning neural networks.
Intensive flow.

766
00:50:01.270 --> 00:50:04.630
Start with my videos and then also tensorflow hasn't great tutorials on their

767
00:50:04.631 --> 00:50:08.050
website.
Um,
do I have a girlfriend?
No,

768
00:50:08.051 --> 00:50:12.000
I am newly single man.
I get this question a lot.
Uh,

769
00:50:12.070 --> 00:50:15.340
but I'm not actually looking for a girlfriend,
so I'm,
my girlfriend is python.

770
00:50:15.370 --> 00:50:19.270
All right,
so open AI is how I found you.
All right.
Anyway,
thank you.

771
00:50:19.420 --> 00:50:22.630
Please do a video on a live video on a current nights.
Okay,
that's it.
Okay,
cool.

772
00:50:22.631 --> 00:50:26.650
I will,
I love you guys.
And next time there is not going to be la as much lag.

773
00:50:26.651 --> 00:50:31.070
I promise you I'm going to,
I want to do these live sessions every week,
okay.

774
00:50:31.580 --> 00:50:33.140
Uh,
and going to,
all right,

775
00:50:33.440 --> 00:50:37.640
and thank you guys so much for showing up resellers song.
I will freestyle.
Okay,

776
00:50:37.641 --> 00:50:42.260
here I go.
Here we go.
What is the topic?
Coffee.
I love coffee.

777
00:50:42.410 --> 00:50:47.210
I do it back every day.
I'm not me.
I'm somebody yells,
my mind is so free.

778
00:50:47.330 --> 00:50:50.780
I seen machine learning because I want to be something more than me.

779
00:50:50.930 --> 00:50:55.930
I want to go and flow and so high in the sky that my mind is so fried from doing

780
00:50:56.121 --> 00:50:59.960
this live.
Okay.
So that's my,
that's my freestyle.
I love you guys.

781
00:51:00.020 --> 00:51:02.600
I've got to go for now.
I've got to

782
00:51:04.890 --> 00:51:08.490
not have a laggy connection,
so thanks for watching.

783
00:51:12.130 --> 00:51:12.790
Have you guys.

