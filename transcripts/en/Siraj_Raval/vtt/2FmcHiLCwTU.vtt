WEBVTT

1
00:00:05.180 --> 00:00:06.610
Oh world.
It's the Raj.

2
00:00:06.690 --> 00:00:10.050
The most popular machine learning library in the world right now is Google's

3
00:00:10.051 --> 00:00:10.884
tensor flow.

4
00:00:10.950 --> 00:00:14.160
We're going to use it to build a classifier that can look at an image of a

5
00:00:14.161 --> 00:00:18.510
handwritten digit and classify what digit it is in under 40 lines of code.

6
00:00:18.511 --> 00:00:22.500
Basic as she is pretty much every single Google product you use machine learning

7
00:00:22.501 --> 00:00:25.650
in some way,
whether it's image search,
image captioning,

8
00:00:25.651 --> 00:00:27.450
translation recommendations,

9
00:00:27.690 --> 00:00:31.650
Google needs machine learning to take advantage of their god like datasets to

10
00:00:31.651 --> 00:00:33.540
give users the dopest experience.

11
00:00:33.570 --> 00:00:36.720
There are three different crowds that use machine learning,
researchers,

12
00:00:36.840 --> 00:00:40.140
data scientists and developers.
Ideally,

13
00:00:40.141 --> 00:00:43.620
they can all use the same tool set to collaborate with each other and improve

14
00:00:43.621 --> 00:00:45.900
their efficiency.
Tensorflow was the solution.

15
00:00:45.901 --> 00:00:47.950
They create it to help solve this problem.
Well,

16
00:00:47.951 --> 00:00:51.150
it doesn't just have a lot of data.
They have the world's largest computer,

17
00:00:51.180 --> 00:00:52.740
so the library was built to scale.

18
00:00:52.770 --> 00:00:57.390
It was made to run on multiple CPU or GPU and even mobile operating systems and

19
00:00:57.391 --> 00:01:01.110
it has several rappers in several languages.
My favorite one is python,

20
00:01:01.230 --> 00:01:04.920
objective C,
who broke my heart.
We have to install tensorflow.
First,

21
00:01:05.010 --> 00:01:08.310
we're going to use pip,
the python package manager to install it.

22
00:01:08.490 --> 00:01:09.301
Once we have pip,

23
00:01:09.301 --> 00:01:13.300
we can create an environment variable that points to the download URL for tensor

24
00:01:13.301 --> 00:01:15.150
flow.
Once we've set the environment variable,

25
00:01:15.240 --> 00:01:19.170
we can download tensorflow via pip install with the upgrade flag and the name of

26
00:01:19.171 --> 00:01:22.770
our environment variable.
Dope.
Now that we have our dependencies installed,

27
00:01:22.890 --> 00:01:23.850
let's get to the code.

28
00:01:24.060 --> 00:01:26.880
We'll start off by importing our handwritten digit Dataset.

29
00:01:27.060 --> 00:01:30.960
The input data class is a standard python class that download that Dataset,

30
00:01:30.961 --> 00:01:34.890
splits it into training and testing data and formats it for our use later on,

31
00:01:34.980 --> 00:01:36.510
and of course we'll import tensorflow.

32
00:01:36.540 --> 00:01:39.870
Now we can set our hyper parameters or tuning knobs for our model.

33
00:01:39.930 --> 00:01:41.370
The first one is the learning rate,

34
00:01:41.430 --> 00:01:44.250
which defines how fast we want to update our weights.

35
00:01:44.280 --> 00:01:48.030
If the learning rate is too big,
our model might skip the optimal solution.

36
00:01:48.060 --> 00:01:49.110
If it's too small,

37
00:01:49.111 --> 00:01:52.470
we might need to many iterations to converge on the best results,

38
00:01:52.500 --> 00:01:56.670
so we'll send it to 0.01 because it's a known decent learning rate for this

39
00:01:56.671 --> 00:01:58.800
problem.
Definitely faster than little Wayne's.

40
00:01:58.830 --> 00:02:01.170
Now we want to create our model in tensorflow.

41
00:02:01.171 --> 00:02:03.690
A model is represented as a data flow graph.

42
00:02:03.750 --> 00:02:06.390
The graph contains a set of nodes called operations.

43
00:02:06.420 --> 00:02:07.920
These are units of computation.

44
00:02:07.921 --> 00:02:11.520
They can be as simple as addition or multiplication and can be as complicated as

45
00:02:11.521 --> 00:02:13.230
some multivariate equation.

46
00:02:13.290 --> 00:02:17.520
Each operation takes in as input a tenser and output to tensor as well.

47
00:02:17.580 --> 00:02:20.580
A tensor is how data is represented in tensorflow.

48
00:02:20.610 --> 00:02:24.540
They are multidimensional arrays of numbers and they flow between operations,

49
00:02:24.570 --> 00:02:27.120
hence the name tensorflow.
Oh,
makes sense.

50
00:02:27.150 --> 00:02:29.910
We'll start by building our model by creating two operations,

51
00:02:29.911 --> 00:02:31.560
both our placeholder operations.

52
00:02:31.620 --> 00:02:35.250
A placeholder is just a variable that we will assign data to at a later date.

53
00:02:35.280 --> 00:02:38.100
It's never initialize and contains no data.
Well defined,

54
00:02:38.101 --> 00:02:40.380
the type and shape of our data as the parameters,

55
00:02:40.381 --> 00:02:43.860
the input images x will be represented by a tutee tensor of numbers.

56
00:02:43.920 --> 00:02:47.790
Seven 84 is a dimensionality of a single flattened Mni ist image.

57
00:02:47.820 --> 00:02:51.810
Finding an image means converting a two d array to a one d array by unstacking

58
00:02:51.811 --> 00:02:55.950
the rose in lining them up.
This is more efficient formatting the output classes.

59
00:02:55.951 --> 00:02:58.140
Why will consist of a Tutee tensor as well,

60
00:02:58.170 --> 00:03:02.320
where each row is a one hot 10 dimensional vector showing which digit class the

61
00:03:02.321 --> 00:03:06.490
corresponding Emond and ist image belongs to and we'll define our weights w and

62
00:03:06.491 --> 00:03:07.960
biases be for our model.

63
00:03:07.990 --> 00:03:11.500
The weights are the probabilities that affect how data flows in the graph and

64
00:03:11.501 --> 00:03:15.490
they will be updated continuously during training so that our results get closer

65
00:03:15.491 --> 00:03:17.410
and closer to the right solution.
That bias,

66
00:03:17.411 --> 00:03:20.560
let's a shift or a regression line to better fit the data well then create a

67
00:03:20.561 --> 00:03:21.730
name scope scopes,

68
00:03:21.731 --> 00:03:24.910
help us organize nodes in the graph visualizer called tensor board,

69
00:03:25.060 --> 00:03:28.480
which will view at the end.
We'll create three scopes in the first scope.

70
00:03:28.481 --> 00:03:32.590
We'll implement our model logistic regression by matrix multiplying the input

71
00:03:32.591 --> 00:03:36.310
images x by the weight matrix w and adding the bias be well.

72
00:03:36.311 --> 00:03:40.150
Then create summary operations to help us later visualize a distribution of our

73
00:03:40.151 --> 00:03:43.270
weights and biases and the second scope we'll create our cost function.

74
00:03:43.450 --> 00:03:46.930
The cost function helps us minimize our error during training and we'll use the

75
00:03:46.931 --> 00:03:49.240
popular cross entropy function as it.

76
00:03:49.300 --> 00:03:52.330
Then we'll create a scalar summary to monitor it during training so we can

77
00:03:52.331 --> 00:03:53.230
visualize it later.

78
00:03:53.290 --> 00:03:57.220
Our last scope is called train and it will create our optimization function that

79
00:03:57.221 --> 00:04:00.770
makes our model improved.
During training.
We'll use the popular gradient descent,

80
00:04:00.771 --> 00:04:01.300
how rhythm,

81
00:04:01.300 --> 00:04:04.900
which takes our learning rate as a parameter for pacing and our cost function as

82
00:04:04.901 --> 00:04:08.410
a parameter to help minimize the error.
Now that we have our grasp bill,

83
00:04:08.500 --> 00:04:12.100
we'll initialize all of our variables and we'll merge all of our summaries into

84
00:04:12.101 --> 00:04:14.680
a single operator because we are extremely lazy.

85
00:04:14.740 --> 00:04:18.220
Now we're ready to launch our graph by initializing a session which lets us

86
00:04:18.250 --> 00:04:22.060
execute our data flow graph well.
Then set our summary writer folder location,

87
00:04:22.090 --> 00:04:26.380
which will later load data from to visualize intenser board training time.

88
00:04:26.410 --> 00:04:30.100
Let's set our for loop for our specified number of iterations and initialize our

89
00:04:30.101 --> 00:04:30.910
average costs,

90
00:04:30.910 --> 00:04:34.480
which will print out every so often to make sure our model is improving during

91
00:04:34.481 --> 00:04:38.020
training.
Well compute our batch size and start training over each example.

92
00:04:38.021 --> 00:04:38.980
In our training data.

93
00:04:39.010 --> 00:04:42.580
Next will fit our model using the batch data and the gradient descent algorithm

94
00:04:42.640 --> 00:04:46.330
for backpropagation will compute the average loss and write logs for each

95
00:04:46.331 --> 00:04:46.990
iteration.

96
00:04:46.990 --> 00:04:50.230
The other summary writer for each displaced step will display our logs to

97
00:04:50.231 --> 00:04:51.550
terminal.
That's it for training.

98
00:04:51.551 --> 00:04:55.630
We can then test the model I comparing our model values to our output values.

99
00:04:55.690 --> 00:04:58.090
We'll calculate the accuracy and printed out for test data.

100
00:04:58.120 --> 00:05:01.120
The accuracy gets better with training and once we've trained and tested our

101
00:05:01.121 --> 00:05:04.690
model,
it will be able to classify novel Emond Ist digits pretty well.

102
00:05:04.720 --> 00:05:07.330
We can then visualize our graph intenser board,

103
00:05:07.390 --> 00:05:09.850
yo pretty colors and stuff in our browser.

104
00:05:09.851 --> 00:05:12.760
We'll be able to view the output of our cost function over time.

105
00:05:12.761 --> 00:05:14.710
Under the events tab under histograms,

106
00:05:14.770 --> 00:05:17.920
we'll be able to see the variance in our biases and weights over time.

107
00:05:17.950 --> 00:05:21.550
Under graphs we can view the actual graph we created as well as the variables

108
00:05:21.551 --> 00:05:22.630
for weights and bias.

109
00:05:22.690 --> 00:05:25.840
We can see the flow of tensors in the form of edges connecting our nodes or

110
00:05:25.841 --> 00:05:29.320
operations.
We can see each of the three scopes we named in our code earlier,

111
00:05:29.350 --> 00:05:30.790
and by double clicking on each,

112
00:05:30.820 --> 00:05:34.240
we can see a more detailed view of how tensors are flowing through each.

113
00:05:34.270 --> 00:05:35.680
Lots of cooling to the description,

114
00:05:35.681 --> 00:05:38.740
and please hit that subscribe button if you want to see more ml videos.
For now,

115
00:05:38.741 --> 00:05:41.440
I've got to go dockerize my environment,
so thanks for watching.

