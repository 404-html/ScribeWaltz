WEBVTT

1
00:00:00.450 --> 00:00:04.600
Been trying to nets and down in losses.
Men,
I feel just like a rock star.

2
00:00:05.520 --> 00:00:06.300
Hello world,

3
00:00:06.300 --> 00:00:11.220
it's arrived and lost functions are a crucial part of the machine learning

4
00:00:11.221 --> 00:00:15.750
pipeline,
but knowing which one to use can be kind of confusing.

5
00:00:16.140 --> 00:00:21.140
I'll explain how and when to use six popular loss functions by using two

6
00:00:21.451 --> 00:00:26.370
examples,
a music genre classifier and a budgeting app for hospitals.

7
00:00:26.760 --> 00:00:31.590
The process of learning from data to find a solution to a question is machine

8
00:00:31.591 --> 00:00:32.340
learning.

9
00:00:32.340 --> 00:00:36.900
Ideally the Datas that we find has labels making it a supervised problem.

10
00:00:37.440 --> 00:00:42.020
The learning process is all about using the training data to produce a predictor

11
00:00:42.040 --> 00:00:42.873
function.

12
00:00:42.990 --> 00:00:47.990
This is a function that takes inputs x and tries to map them to labels why we

13
00:00:48.331 --> 00:00:53.331
went the predictor to approximately work even for examples that it hasn't yet

14
00:00:53.401 --> 00:00:56.610
seen before in the training data that is,

15
00:00:56.640 --> 00:01:01.620
we want it to be as generalized as possible and because we want it to be

16
00:01:01.621 --> 00:01:02.430
general,

17
00:01:02.430 --> 00:01:07.080
this forces us to design it in a principled mathematical way,

18
00:01:07.260 --> 00:01:11.490
which would delight three blue one brown for each data point x,

19
00:01:11.520 --> 00:01:15.030
we start computing a series of operations on it.

20
00:01:15.150 --> 00:01:19.800
Whatever operations our model requires to produce a predicted output,

21
00:01:20.130 --> 00:01:25.130
we then compare that predicted output to the actual output to produce an error

22
00:01:25.171 --> 00:01:30.000
value and that error is what we minimize during the learning process.

23
00:01:30.001 --> 00:01:33.270
Using an optimization strategy like gradient descent.

24
00:01:33.600 --> 00:01:38.600
The way we're actually computing that error value is by using a loss function.

25
00:01:39.540 --> 00:01:44.540
It quantifies how wrong we be if we use the model to make a prediction on x when

26
00:01:45.631 --> 00:01:50.310
the correct output is why we're trying to minimize it.
Unfortunately,

27
00:01:50.311 --> 00:01:55.311
there is no universal loss function that works for all kinds of data.

28
00:01:56.250 --> 00:02:00.600
It depends on a lot of factors like the presence of outliers,

29
00:02:00.950 --> 00:02:03.510
the choice of machine learning algorithm,

30
00:02:03.690 --> 00:02:08.460
the time efficiency of gradient descent and the confidence of predictions.

31
00:02:09.030 --> 00:02:14.030
The idea behind our loss functions originates from the mathematician Claude

32
00:02:15.090 --> 00:02:19.740
Shannon's information theory,
which he proposed in 1948 like a boss.

33
00:02:20.130 --> 00:02:25.130
The goal is to reliably and efficiently transmit a message from a sender to a

34
00:02:25.831 --> 00:02:26.664
recipient.

35
00:02:26.850 --> 00:02:31.740
Digital messages are composed of bits and bits either equals zero or one.

36
00:02:31.950 --> 00:02:35.280
Not all bits are useful though.
Some can be redundant,

37
00:02:35.281 --> 00:02:40.281
so when we communicate a message we want as many relevant bits to be sent as

38
00:02:40.981 --> 00:02:41.820
possible.

39
00:02:41.940 --> 00:02:46.920
In Shannon's theory to transmit one bit of information means to reduce the

40
00:02:46.921 --> 00:02:51.120
recipients uncertainty by a factor of two.
For example,

41
00:02:51.150 --> 00:02:56.150
say that two teams are playing against each other in the World Cup with a 50 50

42
00:02:56.371 --> 00:02:58.920
chance of either team winning.

43
00:02:59.290 --> 00:03:03.490
If a prediction service tells us that one team will win,

44
00:03:03.820 --> 00:03:07.180
they've reduced our uncertainty by a factor of two.

45
00:03:07.570 --> 00:03:11.380
There were two equally likely outcomes.
Now there's just one.

46
00:03:11.920 --> 00:03:16.750
The prediction service sent us a single bit of useful information.

47
00:03:16.810 --> 00:03:19.570
That bit could be a string,
an image.

48
00:03:19.600 --> 00:03:22.660
It could consist of many bytes of data,

49
00:03:22.810 --> 00:03:27.580
but he can still be represented as one bit of useful information.

50
00:03:28.090 --> 00:03:33.010
Suppose there were eight possible teams that could win the World Cup all equally

51
00:03:33.011 --> 00:03:37.000
likely.
If the service tells us the likely team to win,

52
00:03:37.240 --> 00:03:41.230
they're dividing our uncertainty by a factor of eight,

53
00:03:41.410 --> 00:03:46.300
which is two to the power three so they sent us three bits of useful

54
00:03:46.301 --> 00:03:47.134
information.

55
00:03:47.170 --> 00:03:52.170
We can compute the number of bits that were communicated by computing the binary

56
00:03:52.631 --> 00:03:56.200
logarithm of the uncertainty reduction factor,

57
00:03:56.320 --> 00:03:58.690
which is eight in this example,

58
00:03:59.440 --> 00:04:04.180
let's say though that the possibilities are not equally likely,

59
00:04:04.420 --> 00:04:09.420
say one team has a 75% chance of winning and the other 25% if the service says

60
00:04:11.320 --> 00:04:13.630
the less probable team will win,

61
00:04:13.900 --> 00:04:18.900
then the uncertainty has dropped by a factor of four which is two bits of

62
00:04:19.391 --> 00:04:20.230
information.

63
00:04:20.620 --> 00:04:25.240
The uncertainty reduction is the inverse of the events probability.

64
00:04:25.510 --> 00:04:30.190
The log of one over x is equal to the negative log of x,

65
00:04:30.280 --> 00:04:35.280
so the equation to compute the number of bits comes out to minus the binary log

66
00:04:35.831 --> 00:04:40.831
of the probability 25% we can compute the same for the other team.

67
00:04:41.410 --> 00:04:45.850
We can sum up these values to compute an average number of bits.

68
00:04:45.851 --> 00:04:50.170
The service will tell us,
and this is considered the entropy,

69
00:04:50.560 --> 00:04:55.560
it measures the average amount of information that we get from one sample drawn

70
00:04:56.741 --> 00:04:59.290
from a given probability distribution.

71
00:04:59.350 --> 00:05:04.350
P it tells us how unpredictable that probability distribution is.

72
00:05:05.320 --> 00:05:08.800
The more variation in the data and the larger the entropy.

73
00:05:09.190 --> 00:05:12.610
The Cross entropy is the average message length.

74
00:05:12.940 --> 00:05:17.940
We can express this as a function of both the true probability distribution key

75
00:05:19.120 --> 00:05:23.920
and the predicted distribution cue.
If our prediction is perfect,

76
00:05:23.950 --> 00:05:26.860
then the cross entropy is equal to the entropy,

77
00:05:27.190 --> 00:05:32.190
but if the distributions differ than the cross entropy will be crater than the

78
00:05:32.501 --> 00:05:34.690
entropy of by some number of bits.

79
00:05:35.140 --> 00:05:40.140
This amount that the cross entropy exceeds the entropy is called the relative

80
00:05:40.691 --> 00:05:44.500
entropy also called the back Leibler divergence.

81
00:05:44.740 --> 00:05:46.810
We can write this out several ways,

82
00:05:47.170 --> 00:05:52.170
so in the context of ml we can categorize loss functions very broadly into two

83
00:05:52.991 --> 00:05:57.830
types classification and regression loss.
Let's start with classification.

84
00:05:58.160 --> 00:06:02.780
Assume we have a problem where we have a music dataset.

85
00:06:03.440 --> 00:06:06.650
It's a bunch of songs in the form of MP,

86
00:06:06.651 --> 00:06:09.920
three files with labeled genres,
no mumble rap.

87
00:06:09.921 --> 00:06:14.090
Thankfully when we make a prediction using are trained the model,

88
00:06:14.180 --> 00:06:17.900
it'll output a bunch of class probability values,

89
00:06:18.200 --> 00:06:22.250
one for each genre in question for our loss.

90
00:06:22.280 --> 00:06:26.030
Let's try using cross entropy.
Also called log loss.

91
00:06:26.090 --> 00:06:31.090
Sometimes it's the equation we saw earlier and it measures the performance of a

92
00:06:31.341 --> 00:06:36.341
classification model whose output is a probability value between zero and one

93
00:06:37.790 --> 00:06:42.790
the cross entropy loss increases as the predicted probability diverges from the

94
00:06:42.861 --> 00:06:44.030
actual label,

95
00:06:44.390 --> 00:06:49.390
so predicting a probability of say 0.024 when the actual observation label is

96
00:06:49.881 --> 00:06:54.380
one would be bad and it would result in a high loss value.

97
00:06:54.860 --> 00:06:57.860
The ideal model would have a log loss of zero.

98
00:06:58.220 --> 00:07:03.220
If we were to graph out the range of possible loss values given a true

99
00:07:03.411 --> 00:07:08.411
observation like he's rock music equals one we can see that as the predicted

100
00:07:08.841 --> 00:07:13.340
probability approaches one the log loss slowly decreases,

101
00:07:13.610 --> 00:07:16.310
but as the predicted probability decreases,

102
00:07:16.520 --> 00:07:19.100
the log loss increases pretty fast.

103
00:07:19.580 --> 00:07:23.090
This log loss penalizes both types of errors,

104
00:07:23.240 --> 00:07:27.320
but especially those predictions that it's confident are inaccurate.

105
00:07:27.770 --> 00:07:30.740
We can denote it using sigma notation,

106
00:07:30.980 --> 00:07:35.980
so we'll sum up the negative logs of all the predicted probabilities multiplied

107
00:07:37.011 --> 00:07:42.011
by y for as many classes as there are and that will give us our error.

108
00:07:42.080 --> 00:07:46.490
It uses the negative log to give us an easy metric for comparisons.

109
00:07:46.820 --> 00:07:51.590
That's because the positive log of numbers less than one returns negative

110
00:07:51.591 --> 00:07:52.424
values,

111
00:07:52.550 --> 00:07:57.050
which can be confusing to work with when comparing the performance of two

112
00:07:57.051 --> 00:07:58.040
different models.

113
00:07:58.310 --> 00:08:03.310
There's also another commonly used type of loss function in classification tasks

114
00:08:04.191 --> 00:08:08.360
called the hinge loss.
Usually in support vector machines,

115
00:08:08.720 --> 00:08:12.440
it penalizes predictions not only when they are incorrect,

116
00:08:12.530 --> 00:08:15.500
but even when they are correct but not confident.

117
00:08:15.590 --> 00:08:19.820
It penalizes predictions that are really off in a big way.

118
00:08:20.030 --> 00:08:21.980
Those that are just slightly off,

119
00:08:22.010 --> 00:08:26.690
a little less but confidently correct predictions are not penalized at all.

120
00:08:27.170 --> 00:08:29.570
We can formalize this by writing it out.

121
00:08:29.630 --> 00:08:33.350
In the case of binary classification here,
our labels,

122
00:08:33.351 --> 00:08:38.351
why are either one or negative one so the loss is only zero when the signs and

123
00:08:38.601 --> 00:08:43.370
match an age of x is greater than or equal to one for example,

124
00:08:43.400 --> 00:08:48.400
if our score for a specific data point was 0.3 but the label was negative one we

125
00:08:49.221 --> 00:08:51.410
get a 1.3 penalty.

126
00:08:51.440 --> 00:08:56.440
If our score was negative 0.8 as in this instance was predicted to have labelled

127
00:08:56.521 --> 00:09:01.521
negative one we'd still get a penalty of 0.2 but if we predicted negative 1.1

128
00:09:01.771 --> 00:09:02.880
we'd get no penalty.

129
00:09:03.320 --> 00:09:07.140
Hinge loss is easier to compute than the cross entropy loss.

130
00:09:07.170 --> 00:09:12.000
It's faster to train via gradient descent since a lot of the time the gradient

131
00:09:12.001 --> 00:09:17.001
is zero so you won't have to update the weights if you need to make real time

132
00:09:17.191 --> 00:09:19.800
decisions with lesser accuracy.

133
00:09:20.070 --> 00:09:23.280
Depend on the hinge loss over cross entropy,

134
00:09:23.281 --> 00:09:27.990
but if accuracy over speed matters,
go with cross entropy.

135
00:09:28.050 --> 00:09:31.770
It's a trade off the colback lie blurred divergence.

136
00:09:31.800 --> 00:09:36.800
We'll still measure the difference between probability distributions p and Q,

137
00:09:37.170 --> 00:09:42.090
but the difference is that in information theory it focuses on the extra number

138
00:09:42.091 --> 00:09:46.680
of bits needed to encode the data.
That means when applied to our data,

139
00:09:46.710 --> 00:09:50.040
the KL divergence will never be less than zero.

140
00:09:50.280 --> 00:09:53.040
It is only equal to zero if p equals Q.

141
00:09:53.340 --> 00:09:55.530
Now let's switch to a regression problem.

142
00:09:55.710 --> 00:09:58.920
We'll try to find the relationship between two variables,

143
00:09:59.370 --> 00:10:04.370
the amount of maintenance costs for housing a patient at a hospital versus the

144
00:10:06.210 --> 00:10:07.043
number of days they stick.

145
00:10:07.140 --> 00:10:12.140
A popular loss function for regression is to use the mean squared error,

146
00:10:12.180 --> 00:10:14.220
also called the l two loss.

147
00:10:14.580 --> 00:10:19.080
This measures the average amount that the models predictions very from the

148
00:10:19.081 --> 00:10:20.250
correct values,

149
00:10:20.400 --> 00:10:24.120
so we can think of it as a measure of the model's performance.

150
00:10:24.360 --> 00:10:25.410
On the training set,

151
00:10:25.680 --> 00:10:30.680
we calculate the difference between the predicted output and the actual output

152
00:10:30.780 --> 00:10:33.600
square it.
Do that for every data point.

153
00:10:33.720 --> 00:10:36.780
Add them all up and divide by the total number of them.

154
00:10:37.020 --> 00:10:41.970
The reason the square is in there is because it lets our results be quadratic or

155
00:10:41.971 --> 00:10:45.180
convex.
When we plot a quadratic function,

156
00:10:45.210 --> 00:10:48.090
it'll have a u shape with only one minimum.

157
00:10:48.330 --> 00:10:52.560
So when we use an optimization strategy like say gradient descent,

158
00:10:52.920 --> 00:10:57.390
we won't get stuck in a local minimum.
We'll find the global minimum,

159
00:10:57.570 --> 00:11:02.570
which will ultimately help us find the ideal parameter values to optimize the

160
00:11:02.581 --> 00:11:03.480
objective function.

161
00:11:03.660 --> 00:11:08.660
Another popular loss function for regression is called the mean absolute error,

162
00:11:08.850 --> 00:11:10.410
also called El one loss.

163
00:11:10.800 --> 00:11:15.800
This measures the average magnitude of the errors in a set of predictions

164
00:11:15.960 --> 00:11:18.360
without considering their direction.

165
00:11:18.750 --> 00:11:23.580
We take the average over the test sample of the absolute differences between our

166
00:11:23.581 --> 00:11:28.581
prediction and the actual output were all individual differences have equal

167
00:11:28.951 --> 00:11:30.780
weight in squared hair,

168
00:11:30.810 --> 00:11:34.380
we are penalizing large deviations more square,

169
00:11:34.381 --> 00:11:38.400
a big number and it becomes much larger relative to the others,

170
00:11:38.700 --> 00:11:43.700
which means that mean absolute error is more robust to outliers than mean

171
00:11:44.701 --> 00:11:45.720
squared error,

172
00:11:45.870 --> 00:11:49.710
so use it if you have a lot of anomalies in your dataset.

173
00:11:49.780 --> 00:11:52.890
M a d assigns equal weights to the data,

174
00:11:52.900 --> 00:11:56.140
whereas ms he emphasizes the extremes.

175
00:11:56.440 --> 00:12:01.210
The square of a very small number is even smaller and the square of a really big

176
00:12:01.211 --> 00:12:05.710
number is huge.
Lastly,
a loss,
very similar to m.

177
00:12:05.711 --> 00:12:10.540
A E is called the Huber loss.
It's also less sensitive to outliers.

178
00:12:10.541 --> 00:12:12.070
Then the mean squared error.

179
00:12:12.390 --> 00:12:17.140
It's a quadratic for small values and linear for large values.

180
00:12:17.500 --> 00:12:21.430
If we graph out all three of these work,
Russian loss functions,

181
00:12:21.700 --> 00:12:23.860
it will look something like this.

182
00:12:24.040 --> 00:12:29.040
So depending on if your problem is a regression problem or a classification

183
00:12:30.491 --> 00:12:34.120
problem,
you can use one of several loss functions.

184
00:12:34.270 --> 00:12:39.270
Then you can pick a loss function that optimizes for either accuracy or speed.

185
00:12:40.000 --> 00:12:41.110
Hope this video was useful.

186
00:12:41.111 --> 00:12:44.110
I've got some great informational links for you in the video description.

187
00:12:44.111 --> 00:12:44.944
Check them out.

188
00:12:45.200 --> 00:12:48.380
<v 1>It is a loss if you don't hit the subscribe button.</v>

189
00:12:48.470 --> 00:12:51.620
I've got so much more where that came from.
For now,

190
00:12:51.680 --> 00:12:54.620
I'm going to upgrade tensorflow,
so thanks for watching.

