WEBVTT

1
00:00:00.240 --> 00:00:02.160
What type of secret agent are you?

2
00:00:03.720 --> 00:00:07.470
A reinforcement learning agent.
Hello world.

3
00:00:07.500 --> 00:00:09.990
It's a Raj and sensor networks.

4
00:00:10.020 --> 00:00:14.610
In this video we'll learn how to use reinforcement learning to find the most

5
00:00:14.640 --> 00:00:19.640
efficient data routing strategy for a network of connected wireless devices.

6
00:00:20.940 --> 00:00:24.540
Every time packets of data flow from one computer to another,

7
00:00:24.690 --> 00:00:29.690
like from your laptop to a web address like google.com and back to your laptop.

8
00:00:29.971 --> 00:00:32.010
When you say you a webpage,

9
00:00:32.310 --> 00:00:37.110
a number of intermediate devices like routers are involved.

10
00:00:37.530 --> 00:00:40.440
Each time that data passes through a router,

11
00:00:40.470 --> 00:00:45.180
it processes it then sends it along to the next device and the network.

12
00:00:45.510 --> 00:00:50.010
In a multi hop situation,
which is quite common on the Internet,

13
00:00:50.220 --> 00:00:55.220
several routers are involved in getting the requests where you want them to go.

14
00:00:55.620 --> 00:01:00.000
That process of handing off data between devices takes time.

15
00:01:00.300 --> 00:01:02.520
More and more of that happening,

16
00:01:02.670 --> 00:01:07.670
meaning more and more hops adds up to more and more time potentially slowing

17
00:01:08.011 --> 00:01:12.000
down our experience as the hop count increases.

18
00:01:12.450 --> 00:01:17.340
There are lots of different factors that determine the speed in which we can use

19
00:01:17.400 --> 00:01:22.400
certain websites or web based services like how much our roommates loves pirate

20
00:01:22.861 --> 00:01:26.100
Bay and hop count isn't the most important,

21
00:01:26.310 --> 00:01:28.530
but it definitely plays a big role.

22
00:01:28.531 --> 00:01:33.531
So we'll try to route data between devices with the fewest number of hops using

23
00:01:34.650 --> 00:01:35.700
reinforcement learning.

24
00:01:36.280 --> 00:01:41.070
L aims to solve the problem of learning in an environment through trial and

25
00:01:41.071 --> 00:01:46.071
error where time is a dimension and the most common way to frame this problem is

26
00:01:48.061 --> 00:01:53.010
to use the mathematical framework known as a Mark Haub decision process.

27
00:01:53.100 --> 00:01:55.320
Once we formally define our problem,

28
00:01:55.321 --> 00:01:59.520
Mark Cobian style in terms of states actions and rewards,

29
00:01:59.730 --> 00:02:02.190
we'll need to formally define our solution.

30
00:02:02.610 --> 00:02:07.410
A simple way of thinking about our ideal solution is a series of actions that

31
00:02:07.411 --> 00:02:11.790
will need to be learned by the agent in order to complete its goal.

32
00:02:12.090 --> 00:02:12.923
For example,

33
00:02:12.930 --> 00:02:17.930
a network of wireless routers that helps a company transmit secure information

34
00:02:18.990 --> 00:02:21.090
needs to learn several tasks.

35
00:02:21.270 --> 00:02:26.130
It needs to learn how to best route data so that it reaches the right server as

36
00:02:26.131 --> 00:02:27.540
fast as possible.

37
00:02:27.780 --> 00:02:32.160
How to efficiently allocate energy usage amongst its nodes,

38
00:02:32.340 --> 00:02:35.640
how to react to changes in its topology,
et cetera.

39
00:02:36.090 --> 00:02:39.150
The correct actions they will need to take in our example,

40
00:02:39.151 --> 00:02:43.980
it's routing data a specific way through its nodes will depend on the current

41
00:02:43.981 --> 00:02:48.420
situation.
If the network traffic is really high,
for example,

42
00:02:48.450 --> 00:02:52.560
it will need to perform a different set of actions to route data.

43
00:02:52.561 --> 00:02:53.970
Then if it was low,

44
00:02:54.390 --> 00:02:59.130
the reward is always decided in the context of the state that it was decided in

45
00:02:59.170 --> 00:03:01.570
along with the state that comes next.

46
00:03:01.990 --> 00:03:06.990
As long as the agent learns an appropriate action response to any environment

47
00:03:07.121 --> 00:03:11.290
state that it can observe,
we'd have a solution to our problem.

48
00:03:12.340 --> 00:03:15.550
This is where the idea of a policy comes into play.

49
00:03:16.060 --> 00:03:20.740
The most basic type of policy is a mapping from the set of environment states to

50
00:03:20.741 --> 00:03:25.600
the set of possible actions.
We can think of a policy like a simple input,

51
00:03:25.630 --> 00:03:28.570
output,
function input,
any environment state,

52
00:03:28.810 --> 00:03:32.350
and it will output the associated action that the agent will take.

53
00:03:32.650 --> 00:03:36.700
If our agent wants to be able to stay updated on its strategy,

54
00:03:36.730 --> 00:03:38.890
it needs to specify this mapping.

55
00:03:39.190 --> 00:03:42.250
We can call this kind of policy deterministic.

56
00:03:42.340 --> 00:03:46.390
Since the action it will take entirely depends on its input,

57
00:03:46.690 --> 00:03:50.230
which is the state.
In contrast,
a stochastic policy,

58
00:03:50.260 --> 00:03:53.050
we'll let the agent choose actions randomly.

59
00:03:53.350 --> 00:03:57.910
We can define a stochastic policy as a mapping that accepts an environment,

60
00:03:57.911 --> 00:04:02.911
state s and action a then returns the probability that the agent takes action a

61
00:04:03.850 --> 00:04:08.830
while.
In state s the most common way to denote the policy by the way,

62
00:04:08.831 --> 00:04:12.340
is by using the Greek letter.
Pi and Greek is dope.

63
00:04:12.520 --> 00:04:14.950
So in our sensor network problem,

64
00:04:14.980 --> 00:04:19.980
let's assume we just want to be able to transfer a file from one router to

65
00:04:20.411 --> 00:04:23.770
another router using the fewest hops necessary.

66
00:04:23.950 --> 00:04:28.120
And we can think of our network as a grid.
For the sake of simplicity.

67
00:04:28.630 --> 00:04:33.630
A deterministic policy would specify something like whenever a network transfer

68
00:04:33.731 --> 00:04:37.690
requires,
say more than five hops,
reset the process.

69
00:04:37.960 --> 00:04:41.680
Or if the network can achieve a transfer in two hops or less,

70
00:04:41.890 --> 00:04:45.160
record that in an activity log for later analysis.

71
00:04:45.760 --> 00:04:50.710
A stochastic policy would say something like whenever the network requires more

72
00:04:50.711 --> 00:04:51.640
than five hops,

73
00:04:51.700 --> 00:04:56.700
reset the process with a 50% probability or continue routing with a 40%

74
00:04:57.521 --> 00:05:01.240
probability and otherwise continue operating as per normal.

75
00:05:01.390 --> 00:05:04.630
Whenever it's able to achieve a transfer in two hops are less.

76
00:05:04.631 --> 00:05:09.631
Record that in an activity log with a 90% probability otherwise continue

77
00:05:09.671 --> 00:05:10.120
operating.

78
00:05:10.120 --> 00:05:15.120
Normally we can actually express a deterministic policy using the same

79
00:05:15.370 --> 00:05:20.140
mathematical notation that we would generally reserve for us to cast tech policy

80
00:05:20.260 --> 00:05:22.660
if we like as well.
Overall.

81
00:05:22.661 --> 00:05:27.661
Specifying a policy is an important step in the reinforcement learning process,

82
00:05:28.090 --> 00:05:32.650
but figuring out what the optimal policies should be is just as important.

83
00:05:33.160 --> 00:05:37.060
How do we do that?
Use duct tape to find out,

84
00:05:37.090 --> 00:05:41.890
let's return to our sensor network environment and start off with a really bad

85
00:05:41.891 --> 00:05:45.280
policy so we can see what it takes to improve.

86
00:05:45.730 --> 00:05:47.320
In our network of nodes,

87
00:05:47.350 --> 00:05:52.350
we want our agents to be able to send a certain file from one node to another by

88
00:05:53.291 --> 00:05:55.060
routing it through other nodes.

89
00:05:55.330 --> 00:05:59.690
Each visit is considered a hop to make the problem more interesting.

90
00:05:59.691 --> 00:06:03.800
Let's also say that there are certain nodes that should be avoided as they

91
00:06:03.801 --> 00:06:08.801
contain virtual environments that will likely corrupt our file for our starting

92
00:06:09.141 --> 00:06:13.580
policy.
We'll just choose to have our agent visit every state meaning device.

93
00:06:13.581 --> 00:06:18.110
There is,
let's calculate how much of a reward it will get if it does this.

94
00:06:18.380 --> 00:06:22.910
If it starts at the bottom left corner of this environment and collects all the

95
00:06:22.911 --> 00:06:25.310
necessary rewards along the way,

96
00:06:25.550 --> 00:06:29.630
we add them up and the some will turn out to be a single number.

97
00:06:30.080 --> 00:06:33.200
This is actually a formal term in reinforcement learning.

98
00:06:33.530 --> 00:06:38.530
R L agents learn to maximize what's called the cumulative future reward.

99
00:06:39.320 --> 00:06:44.000
The word used to describe cumulative future reward is the return and it's

100
00:06:44.001 --> 00:06:45.980
denoted with uppercase r.

101
00:06:46.550 --> 00:06:49.370
The reward could also be discounted.

102
00:06:49.490 --> 00:06:54.490
A discount factor describes the preference of the agent for the current reward

103
00:06:54.860 --> 00:06:59.240
over future rewards,
but let's assume we're not dealing with that right now.

104
00:06:59.390 --> 00:07:03.920
If we follow this policy by starting at a different state in the environment for

105
00:07:03.921 --> 00:07:08.300
as many as there are,
we'll have computed an important group of values.

106
00:07:08.810 --> 00:07:13.700
We can think of this great of numbers as a function of the environment state for

107
00:07:13.730 --> 00:07:16.340
each state,
it has a corresponding number.

108
00:07:16.580 --> 00:07:21.580
We can refer to this function as the state value function for each state.

109
00:07:22.250 --> 00:07:25.910
The state value function yields the expected return.

110
00:07:26.270 --> 00:07:31.040
If the agent started in that state and then followed the policy for all the time

111
00:07:31.041 --> 00:07:31.760
steps.

112
00:07:31.760 --> 00:07:36.650
The state value function for policy pie is the function of the environment
state.

113
00:07:37.010 --> 00:07:40.430
For every state s it tells us the expected return.

114
00:07:40.460 --> 00:07:45.350
If the agent starts in state ass and then uses the policy to choose its actions

115
00:07:45.351 --> 00:07:46.940
for all time steps,

116
00:07:47.450 --> 00:07:52.370
the state value function will always correspond to a particular policy.

117
00:07:52.580 --> 00:07:54.560
So if we change the policy,

118
00:07:54.590 --> 00:07:58.910
we changed the state value function in a Markov decision process.

119
00:07:58.911 --> 00:08:03.911
We can express the value of any state as the some of the immediate reward plus

120
00:08:04.641 --> 00:08:06.710
the value of the state that follows.

121
00:08:07.160 --> 00:08:12.110
This kind of expression is popularly known as the bellman equation named after

122
00:08:12.111 --> 00:08:15.860
the mathematician and black tie efficient auto Richard Bellman.

123
00:08:16.280 --> 00:08:21.280
This equation is also used in fields as diverse as control theory and economics,

124
00:08:21.801 --> 00:08:26.390
but it's absolutely crucial in reinforcement learning.
In fact,

125
00:08:26.540 --> 00:08:29.630
there are four different kinds of bellman equations,

126
00:08:29.660 --> 00:08:32.660
but we'll just focus on this one right now.

127
00:08:32.960 --> 00:08:37.960
It's used to calculate the value of a given state by considering its options and

128
00:08:38.121 --> 00:08:43.100
we can use it to estimate the best action to take to find the optimal policy.

129
00:08:43.550 --> 00:08:48.440
There is another type of value function we should discuss though the action

130
00:08:48.441 --> 00:08:49.280
value function.

131
00:08:49.670 --> 00:08:53.810
While the state values are a function of the environment state,

132
00:08:54.260 --> 00:08:57.270
the action values are a function of the environment,

133
00:08:57.271 --> 00:09:02.271
state and the agent's action for each state es en action a the action value

134
00:09:03.601 --> 00:09:06.030
function yield the expected return.

135
00:09:06.150 --> 00:09:11.150
If the agent starts in state s then chooses action a and then uses the policy to

136
00:09:11.461 --> 00:09:15.600
choose its actions for all time steps in the state value function.

137
00:09:15.630 --> 00:09:20.630
We kept track of the value of each state using one number in the action value

138
00:09:20.941 --> 00:09:23.850
function.
We'll use four values for each state,

139
00:09:24.030 --> 00:09:27.120
each corresponding to a different action up,
down,

140
00:09:27.121 --> 00:09:30.330
left or right for our agent at the agent wants to move up.

141
00:09:30.360 --> 00:09:33.750
It will then follow the policy until it reaches the terminal state.

142
00:09:34.140 --> 00:09:38.430
We then record the reward.
We then do the same process for left.

143
00:09:38.520 --> 00:09:39.930
It'll follow the policy.

144
00:09:40.260 --> 00:09:45.260
The cumulative reward is our action value as it continually does this for every

145
00:09:45.290 --> 00:09:49.860
action at every state we get our completed action value function.

146
00:09:50.010 --> 00:09:54.960
We need to define the action value function before talking about how the agent

147
00:09:55.140 --> 00:09:57.300
can search for an optimal policy.

148
00:09:57.690 --> 00:10:02.690
The main idea is that the agent interacts with the environment and from this

149
00:10:02.941 --> 00:10:06.720
interaction it estimates the optimal action value function.

150
00:10:07.260 --> 00:10:11.790
The agent will then use that action value function to compute the optimal

151
00:10:11.820 --> 00:10:15.930
policy.
Once we have the optimal action value function,

152
00:10:16.050 --> 00:10:19.830
we can construct the optimal policy for each state.

153
00:10:19.860 --> 00:10:24.000
We need to pick the action that yields the highest expected return.

154
00:10:24.390 --> 00:10:29.340
If we follow the maximum action values for each state will quickly find the

155
00:10:29.341 --> 00:10:33.270
optimal policy,
but that leaves the question,

156
00:10:33.480 --> 00:10:36.690
how do we find the optimal action value function?

157
00:10:36.900 --> 00:10:41.460
It's the intermediary step that's allowed us to find the optimal policy.

158
00:10:41.610 --> 00:10:44.220
And it's also the topic for the next video.

159
00:10:44.610 --> 00:10:47.040
Three things to remember from this video,
though,

160
00:10:47.460 --> 00:10:51.120
there are two types of policies.
Deterministic,

161
00:10:51.210 --> 00:10:55.620
where the action taken entirely depends on the state and stochastic,

162
00:10:55.650 --> 00:10:59.340
which allows for randomness.
To learn an optimal policy.

163
00:10:59.341 --> 00:11:04.150
We need to learn an optimal value function of which there are two kinds,

164
00:11:04.350 --> 00:11:06.450
state action and action value.

165
00:11:06.750 --> 00:11:10.410
And we can compute the value function using the bellmen equation,

166
00:11:10.411 --> 00:11:15.411
which expresses the value of any state as the some of the immediate reward plus

167
00:11:16.681 --> 00:11:18.900
the value of this state that follows.

168
00:11:19.080 --> 00:11:22.020
Please subscribe for more programming videos.
And for now,

169
00:11:22.050 --> 00:11:25.200
I've got to compute the right action.
So thanks for watching.

