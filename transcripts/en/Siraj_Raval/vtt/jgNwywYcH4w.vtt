WEBVTT

1
00:00:00.810 --> 00:00:05.070
Are we ready for Tpu time?
All right,
let's go.
Let's go start streaming TPU time.

2
00:00:05.460 --> 00:00:10.050
We're going live.
We're going live.
We're live.
We're live.
We're live.
Okay.

3
00:00:10.620 --> 00:00:12.360
All right everybody.
Hello world.

4
00:00:12.361 --> 00:00:16.230
It's a Raj and welcome to my live stream on the TPU,

5
00:00:16.231 --> 00:00:19.590
the tensor processing unit in this live stream.

6
00:00:19.591 --> 00:00:24.090
I'm going to talk about the technical details of what a TPU is.

7
00:00:24.420 --> 00:00:27.590
I'm also going to do a short demo of,
uh,

8
00:00:27.660 --> 00:00:31.230
it's going to be a benchmark comparing the CPU to the TPU.

9
00:00:31.530 --> 00:00:35.520
And then I'm going to do another benchmark comparing the GPU to Tpu.

10
00:00:35.790 --> 00:00:38.970
And at the very end we're going to do some natural language processing.

11
00:00:38.971 --> 00:00:40.980
This is a default,
um,

12
00:00:41.370 --> 00:00:45.120
example that was provided by Google and we'll go over that at the very end.
Okay,

13
00:00:45.121 --> 00:00:48.300
so this is a live stream.
Let's start off with a two minute Q and.
A.

14
00:00:48.450 --> 00:00:50.940
This is the structure for this lecture and uh,
that's,

15
00:00:50.941 --> 00:00:54.030
that's how we're going to go.
Hello everybody.
We've got deans in the house.

16
00:00:54.031 --> 00:00:54.421
We have a,

17
00:00:54.421 --> 00:00:58.890
we have a very vibrant community that we're building every single day is another

18
00:00:58.890 --> 00:01:02.670
day.
Uh,
that,
that's,
that's a part that's a part of the school of Ai.
Okay.

19
00:01:02.671 --> 00:01:06.030
So two questions and we'll get right into this.

20
00:01:06.031 --> 00:01:10.800
And I want to preface this by saying this will be a technical lecture.

21
00:01:10.801 --> 00:01:14.160
Okay.
We're talking about systolic arrays.
We're talking about hardware.

22
00:01:14.370 --> 00:01:16.830
It's going to be awesome.
I'm very excited for this lecture.
Okay,

23
00:01:16.831 --> 00:01:20.430
so the first question here is,
um,

24
00:01:23.280 --> 00:01:27.840
a TPU has,
who has used Google's colab.
Uh,

25
00:01:27.990 --> 00:01:31.500
we all have.
And uh,
the second question is,

26
00:01:33.140 --> 00:01:35.720
is it like the USB thing that Intel made?
Yes.

27
00:01:35.750 --> 00:01:37.550
In that it is called an [inaudible],

28
00:01:37.640 --> 00:01:41.480
an application specific integrated circuit.
Okay,

29
00:01:43.020 --> 00:01:47.910
now that's it.
And Hi Chris,
Free Mind Wafa and college guide and Mel's and Pooja.

30
00:01:48.120 --> 00:01:50.940
Hi everybody.
Okay.
Now straight into this,
let's,
let's get into this.

31
00:01:51.030 --> 00:01:54.660
Why did Google make this thing right?
I mean,
there's gps out there,
there's TV,

32
00:01:54.690 --> 00:01:58.650
there's CPU is out there and video is doing a great job shadow to Nvidia.

33
00:01:58.651 --> 00:02:02.280
By the way,
this is no,
this is nothing against Nvidia.
I love Nvidia.

34
00:02:02.400 --> 00:02:05.580
They are awesome.
But this TPU thing is pretty cool.

35
00:02:05.670 --> 00:02:09.720
So what what's happened at Google is that there has been a giant demand for deep

36
00:02:09.721 --> 00:02:12.690
learning over the years.
Let me just make this small.

37
00:02:14.160 --> 00:02:14.993
<v 1>Okay.</v>

38
00:02:16.490 --> 00:02:19.040
<v 0>That's a good question,
but okay.
So,</v>

39
00:02:19.780 --> 00:02:20.410
<v 1>okay.</v>

40
00:02:20.410 --> 00:02:24.250
<v 0>The demand for neural networks and deep learning has grown over the years at</v>

41
00:02:24.251 --> 00:02:28.060
Google and Tpu development actually started in 2013,

42
00:02:28.090 --> 00:02:32.280
which is five years ago.
This is a long time ago,
um,
in,
in,

43
00:02:32.281 --> 00:02:36.370
in the machine learning world and it's been used in production since 2015.
Okay.

44
00:02:36.371 --> 00:02:40.870
So deep learning has been super important at Google.
All of their products,

45
00:02:40.871 --> 00:02:45.850
the ones that we know and use everyday search translate photos,
Gmail,

46
00:02:45.910 --> 00:02:50.610
um,
uh,
the Google assistant,
all of these products use deep learning.
Okay.

47
00:02:50.640 --> 00:02:54.040
So what deep learning is,
is we have all of these machine learning models,
right?

48
00:02:54.190 --> 00:02:56.580
We have random for us,
we have uh,

49
00:02:57.270 --> 00:03:00.880
just a support vector machines and one of them is a neural network.

50
00:03:00.910 --> 00:03:02.620
And we give what we give a neural network,

51
00:03:02.650 --> 00:03:06.400
a lot of data and a lot of computing power.
We call that deep learning.

52
00:03:06.490 --> 00:03:11.200
And that has outperformed almost every other machine learning model almost every

53
00:03:11.201 --> 00:03:14.470
time.
Like 99% of the time.
And so they use it.

54
00:03:14.471 --> 00:03:17.860
It's so crucial to all of their products,
right?
So that's the key here.

55
00:03:17.861 --> 00:03:21.580
Deep learning is absolutely crucial to Google.
Okay.

56
00:03:21.581 --> 00:03:26.560
So why did they make their own chips?
Right?
We can do deep learning on a CPU.

57
00:03:26.590 --> 00:03:29.980
We can do deep learning on a Gpu.
Well,

58
00:03:29.981 --> 00:03:33.400
there's been so much progress in machine learning over the years,
right?

59
00:03:33.430 --> 00:03:36.070
And just the past few years,
every single benchmark,

60
00:03:36.730 --> 00:03:39.400
whether it comes to image classification,
whether it comes to um,

61
00:03:39.730 --> 00:03:43.570
video generation,
whether it comes to,
you know,
so many different use cases,

62
00:03:43.690 --> 00:03:47.440
deep learning has outperformed everything else.
And we seen these bench,

63
00:03:47.470 --> 00:03:51.410
we've seen new state of the art results in each of these subfields of,
of,

64
00:03:51.500 --> 00:03:55.420
of machine learning.
And so why deep learning?
Well,
if we,

65
00:03:55.421 --> 00:03:59.770
if we were to plot out the amount of data and the performance of a model,
uh,

66
00:04:00.050 --> 00:04:03.070
the red line here is our traditional learning algorithms.

67
00:04:03.070 --> 00:04:05.290
They kind of plateau after a while,
right?

68
00:04:05.410 --> 00:04:08.860
The performance can only get so good based on how much data you give it,

69
00:04:09.010 --> 00:04:13.270
but neural networks,
and that's what the acronym and n stands for neuro networks.

70
00:04:13.300 --> 00:04:16.120
If you give them a lot of data,
they will start outperforming everything else.

71
00:04:16.121 --> 00:04:18.880
As you see here,
the small,
the medium and the large.

72
00:04:20.740 --> 00:04:24.550
So it's a game changer.
Okay,
so and,
and what our neural networks,
well,

73
00:04:24.551 --> 00:04:28.120
I have so many videos on what neural networks are,
but briefly,

74
00:04:28.450 --> 00:04:32.110
neural networks are models created with linear Algebra.

75
00:04:32.140 --> 00:04:36.730
So Linear Algebra is the name of the gay.
Okay.
Linear Algebra is awesome.

76
00:04:36.970 --> 00:04:41.500
Neural networks are just a chain of matrix operations applied to input.

77
00:04:41.530 --> 00:04:45.610
Input data is actually a wrap for this input times weight,

78
00:04:45.970 --> 00:04:46.840
add a bias,

79
00:04:47.110 --> 00:04:52.000
activate repeat input times weight add a bias activate.

80
00:04:52.001 --> 00:04:54.910
So that just keeps going and going and going at the basic level,
right?
So,

81
00:04:55.510 --> 00:04:57.700
and we can think of that input data,
image,
video,

82
00:04:57.701 --> 00:04:59.980
whatever it is as a matrix of numbers.

83
00:05:00.130 --> 00:05:03.640
And we give it to this black box neural network that's doing this opera series

84
00:05:03.641 --> 00:05:07.900
of operations and we get a result,
right?
So if it's an image,

85
00:05:08.050 --> 00:05:10.490
an image is actually a UN Un un,

86
00:05:10.600 --> 00:05:13.690
a matrix of pixel values between zero and two 55.

87
00:05:14.170 --> 00:05:18.670
And we feed it to this very simple,
um,
uh,
neural network here,

88
00:05:18.671 --> 00:05:22.030
which is just the equation y equals mx plus B,

89
00:05:22.031 --> 00:05:24.610
where m is actually the,
uh,
wait,

90
00:05:25.000 --> 00:05:29.230
not so it's w so y equals w x plus B,
where B is a constant,

91
00:05:29.231 --> 00:05:30.250
it's a bias value.

92
00:05:30.610 --> 00:05:35.610
And so we are just taking that input image a preprocessor preprocessing it such

93
00:05:35.981 --> 00:05:39.880
that it becomes a vector,
multiplying it by buyer weight,
adding a bias,

94
00:05:39.940 --> 00:05:42.940
and then adding an activation function,
which is actually not listed here.

95
00:05:43.060 --> 00:05:47.020
And the output is our prediction.
Now that,
like I said,

96
00:05:47.021 --> 00:05:51.640
they're just a series of matrix operations,
but it turns out,

97
00:05:51.730 --> 00:05:53.200
so when it comes to hardware,

98
00:05:53.260 --> 00:05:57.190
hardware is tailored current CPS and gps,

99
00:05:57.410 --> 00:06:01.580
they're tailored for specific types of operations,
right?
Multiply.

100
00:06:01.610 --> 00:06:02.750
There's a specific,

101
00:06:02.780 --> 00:06:07.780
there's a specific literal piece of hardware dedicated to multiplication and

102
00:06:08.001 --> 00:06:12.170
another one to division and another one.
So all of these math operations,
well,

103
00:06:12.171 --> 00:06:15.820
it turns out for neural networks in particular,
what we're doing is we're com,

104
00:06:16.030 --> 00:06:20.240
we're continually performing the same set of operations over and over and over

105
00:06:20.241 --> 00:06:24.440
again.
So when it comes down to is a giant Matrix,

106
00:06:24.441 --> 00:06:26.720
c matrix full of numbers,

107
00:06:27.200 --> 00:06:29.930
all of them are being multiplied in parallel.
So it's,

108
00:06:29.931 --> 00:06:34.580
so it all boils down to multiply a bunch of numbers together and add the
results.

109
00:06:34.700 --> 00:06:37.070
So we can think of this as a single operation,

110
00:06:37.340 --> 00:06:40.730
which at which we can call multiply,
accumulate or Mac.

111
00:06:41.150 --> 00:06:46.070
And so if there was a hardware level construct dedicated to a multiply

112
00:06:46.071 --> 00:06:50.060
accumulate operation,
then we could start going,

113
00:06:50.210 --> 00:06:53.090
then we could start computing even faster.
Right?
So let's,
let's,

114
00:06:53.150 --> 00:06:57.890
we're slowly building on,
the idea is here that got Google to create a TPU.

115
00:06:58.700 --> 00:07:03.530
And so Moore's law and vetted by Gordon Moore,
it's a founder of,
of,
uh,

116
00:07:03.830 --> 00:07:08.720
of IBM in the 50s.
He said that the computing power would double every two years.

117
00:07:09.020 --> 00:07:13.280
Um,
and that so far has held true,
but the,
so not IBM,

118
00:07:13.281 --> 00:07:17.750
Intel and so was,
was it actually Intel more?
Was it in or IBM,

119
00:07:17.751 --> 00:07:21.620
I've got to check my facts here.
I always mess up these.
These,
uh,

120
00:07:23.390 --> 00:07:28.340
more was intel.
There we go.
Oh,
uh,
anyway,
great.
Until at 50.

121
00:07:28.341 --> 00:07:30.980
I got the 50 part.
Right.
Anyway,
back to this.
This is live by the way.

122
00:07:31.460 --> 00:07:34.120
I'm so many things can go wrong here because it's live,
but that's,

123
00:07:34.121 --> 00:07:38.090
that's how we do.
So Moore's law is plateauing and so,
uh,

124
00:07:38.120 --> 00:07:41.360
we've got to find a way around this.
Well,

125
00:07:41.361 --> 00:07:44.480
Google definitely has to cause all of their products and profits depend on it,

126
00:07:44.481 --> 00:07:49.460
right?
So how do we get past the limits of Moore's law?

127
00:07:49.610 --> 00:07:53.390
The limits of what hardware can do for us,
for deep learning.

128
00:07:54.560 --> 00:07:56.450
So now we're going to get a little more technical here.
Okay,

129
00:07:56.451 --> 00:08:00.740
so a CPU is a scaler machine.
So scalar is a single numbers.

130
00:08:00.800 --> 00:08:02.870
These are linear Algebra terms.
Like,
like I said,

131
00:08:02.871 --> 00:08:07.130
this whole talk is going to consist of only linear Algebra.
There's no calculus,

132
00:08:07.310 --> 00:08:08.960
there's nothing else.
By the way,

133
00:08:09.020 --> 00:08:12.890
neural networks are models built with linear Algebra and then optimize with

134
00:08:12.891 --> 00:08:14.840
calculus.
We'll just talk about calculus.

135
00:08:14.841 --> 00:08:19.130
Like for 1% at the very end to talk about a certain type of optimizer specific

136
00:08:19.131 --> 00:08:23.270
to the TPU called a cross Shard optimizer.
We'll talk about that at the end.

137
00:08:23.630 --> 00:08:28.100
So a CPU is a scalar machine.
It accepts single values,

138
00:08:28.160 --> 00:08:31.250
so it processes instructions one step at a time.

139
00:08:31.550 --> 00:08:35.450
So they can perform matrix operations,
but they're not in parallel.

140
00:08:35.480 --> 00:08:38.660
They're sequential and Moore's law is coming to an end.

141
00:08:38.900 --> 00:08:41.330
So that's a CPU.
Now,

142
00:08:41.331 --> 00:08:45.440
a CPU has multiple cores and each of these cores contains,
um,

143
00:08:45.560 --> 00:08:50.460
arithmetic logic units.
It's got a control structure.
It's got cash memory,
um,

144
00:08:50.540 --> 00:08:54.200
it's got,
um,
a dram.
It's got memory,

145
00:08:54.201 --> 00:08:58.680
it's got memory with it.
Uh,
what a GPU is,
is,

146
00:08:58.710 --> 00:09:02.400
is it's the,
it's similar to a CPU except it has multiple cores,

147
00:09:02.401 --> 00:09:07.200
like hundreds of cores.
So while a CPU has four cores,
a GPU has hundreds,

148
00:09:07.201 --> 00:09:09.840
of course.
Now,
why does this matter to us?

149
00:09:11.040 --> 00:09:15.210
So gps were designed for three d game rendering,

150
00:09:15.211 --> 00:09:19.110
which involves a lot of parallel processing,
right?
Shading,

151
00:09:19.230 --> 00:09:21.180
ray tracing a pixel,

152
00:09:21.181 --> 00:09:24.760
values that are all these things that are happening in Wa at once in these three

153
00:09:24.761 --> 00:09:27.900
Ed dames.
Uh,
they require parallelism.

154
00:09:28.140 --> 00:09:33.140
So what gps do is they are optimized for parallel operations on numbers.

155
00:09:34.020 --> 00:09:36.150
Okay.
So to get a little more technical,

156
00:09:36.151 --> 00:09:40.500
here are a list of attributes for both as CPU and a Gpu.

157
00:09:40.501 --> 00:09:44.400
And we'll just talk about to a CPS or for low compute density,

158
00:09:44.520 --> 00:09:47.940
whereas gps are for high compute density by density.

159
00:09:47.941 --> 00:09:52.550
We're talking about groups of numbers being operated on simultaneously,
right?

160
00:09:52.560 --> 00:09:57.300
So matrices,
vectors,
whereas CPS are sequential.

161
00:09:57.480 --> 00:10:01.500
Okay.
So to get more technical we can think of,
if we,

162
00:10:01.501 --> 00:10:04.020
if we think of a CPU as a scalar machine,

163
00:10:04.021 --> 00:10:07.260
we can then think of a GPU as a vector machine.

164
00:10:07.440 --> 00:10:12.440
We're a vector is a one dimensional array of numbers that can be operated on at

165
00:10:13.891 --> 00:10:14.820
the same time.

166
00:10:16.200 --> 00:10:19.860
And so GPU is our general purpose chips,
right?

167
00:10:19.861 --> 00:10:23.130
They don't just perform a vector operations,

168
00:10:23.280 --> 00:10:25.440
they can really do any kind of operation,

169
00:10:25.441 --> 00:10:30.060
although they are optimized for vector operations.
So

170
00:10:31.830 --> 00:10:36.600
how do we improve upon a GPU that does well with the vector operations?

171
00:10:36.990 --> 00:10:37.740
Well,
let's just,

172
00:10:37.740 --> 00:10:42.240
let's just look at what we're doing with the GPU here very quickly,
how it works.

173
00:10:42.360 --> 00:10:45.690
Uh,
and then we'll go into 10 TPU specific code.

174
00:10:45.870 --> 00:10:48.360
So we're going to just perform this simple matrix operation here.

175
00:10:48.361 --> 00:10:51.780
So check this out.
I just want you to look at this matrix operation.

176
00:10:52.950 --> 00:10:55.560
What this is,
it's,
it's a major operation that says,

177
00:10:55.590 --> 00:10:59.640
multiply this scalar number two by this matrix.
Okay?

178
00:10:59.850 --> 00:11:03.930
And so what linear Algebra.
Okay,
so just quick primer here.

179
00:11:04.020 --> 00:11:07.440
So Algebra is the,
is a,
is a mathematical,
uh,

180
00:11:07.470 --> 00:11:10.860
it's a branch of math that it's dealing with scalar numbers,
right?

181
00:11:10.860 --> 00:11:11.880
Single numbers.

182
00:11:12.240 --> 00:11:17.240
But when it comes to operating on groups of numbers at the same time matrices,

183
00:11:18.810 --> 00:11:23.220
we need a different set of rules,
right?
So do we multiply the two by the 10,

184
00:11:23.221 --> 00:11:28.140
then add that to 10 by the four?
Or do we do or do we,
you two times,
right?

185
00:11:28.141 --> 00:11:29.190
So there's not a very,

186
00:11:29.250 --> 00:11:32.820
if we just look at this without looking at the rules of linear Algebra will be

187
00:11:32.821 --> 00:11:35.790
kind of confused.
But when we see this diagram,
we see,
oh,
okay.

188
00:11:35.940 --> 00:11:40.410
Linear Algebra tells us that when we multiply a scalar by a matrix,

189
00:11:40.470 --> 00:11:43.680
we're just taking that scalar and multiplying it by each of those values.

190
00:11:43.830 --> 00:11:47.460
And that's the output value.
So that's one of the rules of linear Algebra,

191
00:11:47.461 --> 00:11:48.570
of which there are many.

192
00:11:49.650 --> 00:11:54.580
So let's do this in a mx net,
which is a very short,

193
00:11:57.180 --> 00:12:02.000
just want to make sure everything's all good.
Good.

194
00:12:02.030 --> 00:12:06.110
Okay.
Everybody's in here so that,
okay,
so let's do a very short demo in mx net.

195
00:12:06.111 --> 00:12:08.420
It's just a few lines of code and I just want to show,

196
00:12:08.421 --> 00:12:11.870
and this is basically Kudo by the way.
I'm so excited for this.
Okay,

197
00:12:11.871 --> 00:12:16.730
so this is basically Kuda.
Let me just,
um,
install mx net and wallets installing.

198
00:12:16.760 --> 00:12:19.370
I'll explain what is happening here.
Um,

199
00:12:20.510 --> 00:12:24.820
import as Amex.
Let's make sure that we are doing this on the GPU first.

200
00:12:24.821 --> 00:12:29.330
So the runtime is going to be the GPU.
Good.
It's the GPU.

201
00:12:29.630 --> 00:12:31.640
Now let's just,
let's install this.

202
00:12:31.641 --> 00:12:35.300
So what this is [inaudible] is this open source library.
Um,

203
00:12:35.480 --> 00:12:37.070
let me just say I like tensor flow better,

204
00:12:37.130 --> 00:12:40.170
but I'm just doing this because it's a very simple three line of code,
um,

205
00:12:40.310 --> 00:12:45.310
example that we can use to demonstrate what a matrix operation looks like on the

206
00:12:45.441 --> 00:12:48.520
Gpu.
And so what mx net is doing in,

207
00:12:48.630 --> 00:12:50.940
in this case is it is a very,

208
00:12:51.020 --> 00:12:55.610
we're going to create a single object called it called an n d array,
right?

209
00:12:55.611 --> 00:13:00.200
An end dimensional array.
And so that is the,
that is the,
uh,

210
00:13:00.260 --> 00:13:03.200
construct the object in mx net that we're gonna,

211
00:13:03.260 --> 00:13:06.890
that's gonna represent a matrix that we're going to use to perform this

212
00:13:06.891 --> 00:13:11.630
operation in three lines of code.
Okay?
So while this is loading,
in fact,

213
00:13:11.960 --> 00:13:15.800
let's just,
uh,
let's do this.
So what we're gonna do is we're going to say a,

214
00:13:15.801 --> 00:13:20.801
the result is going to equal mx dot nd on end dimensional array dot ones.

215
00:13:22.100 --> 00:13:25.880
Now,
one's means that this is going to be a two by three matrix.

216
00:13:25.881 --> 00:13:28.760
So this is going to be a two by three matrix of just ones.

217
00:13:28.790 --> 00:13:32.000
All those values are ones,
it's a two by three matrix.

218
00:13:32.300 --> 00:13:37.100
And all those values are once we're going to do this on the Gpu,

219
00:13:37.670 --> 00:13:41.120
uh,
and then we're going to say B is equal to a,

220
00:13:41.150 --> 00:13:44.780
so that entire matrix times the scale or number two.

221
00:13:44.930 --> 00:13:49.490
And let's just add one and then we'll return that back as a num Pi Array.

222
00:13:49.640 --> 00:13:52.970
So this is Amex net,
but it's basically Kuda.

223
00:13:52.971 --> 00:13:56.000
It's a very thin wrapper over Kudo,
which is the,

224
00:13:56.690 --> 00:14:00.650
which is Nvidia's GPU specific programming language.
Kuta is awesome.

225
00:14:00.830 --> 00:14:05.690
It's all about optimizing a,
it's super hard.
Um,
but I really like Kuda.
Um,

226
00:14:06.140 --> 00:14:09.700
I just want to say that anyway,
so
we have,

227
00:14:09.940 --> 00:14:14.210
we have installed,
um,
still going.

228
00:14:14.211 --> 00:14:16.670
So it's going to take a while and then we can do this.
So like I said,

229
00:14:16.700 --> 00:14:19.970
we're taking a two by three matrix of once saying on the GPU,

230
00:14:19.971 --> 00:14:24.230
multiply it by the scalar number two and return the result as a,
as a matrix,

231
00:14:24.231 --> 00:14:26.720
as a num Pi,
a matrix.
And here it is.

232
00:14:26.900 --> 00:14:31.670
So that is a very simple GPU operation that we just performed.
Okay.

233
00:14:31.700 --> 00:14:32.570
So I just wanted to show that.

234
00:14:32.571 --> 00:14:37.250
And now let's do a little Q and a before we get into the tensor.
The TPU part.

235
00:14:38.000 --> 00:14:42.000
Um,
everything's a okay.
Thank you.

236
00:14:43.600 --> 00:14:46.030
Um,
any questions?

237
00:14:51.150 --> 00:14:51.983
<v 2>Okay.</v>

238
00:14:52.830 --> 00:14:57.000
<v 0>I don't understand a thing in our ELL math.
Is it okay?
Um,
yes,</v>

239
00:14:57.030 --> 00:14:58.350
it's okay.
I mean,

240
00:14:58.380 --> 00:15:02.550
everybody starts out not knowing a thing about whatever topic they're learning.

241
00:15:02.551 --> 00:15:05.040
So of course that's okay.
We look,

242
00:15:05.090 --> 00:15:08.700
we're trying to grow the amateur research community here and we are just at the

243
00:15:08.701 --> 00:15:09.750
very beginning.

244
00:15:09.890 --> 00:15:14.280
We are at the very beginning of this revolution of growing the amateur research

245
00:15:14.281 --> 00:15:16.110
community.
We at school of Ai,

246
00:15:16.111 --> 00:15:19.680
I deeply believe that anybody can do research it,

247
00:15:20.010 --> 00:15:22.320
they don't necessarily have to be a part of academia.

248
00:15:22.470 --> 00:15:24.810
They don't necessarily have to be a part of the industry.

249
00:15:25.560 --> 00:15:28.830
We believe in growing the independent amateur research community,

250
00:15:28.831 --> 00:15:33.330
and we think that's with the democratization of tools like Colab,

251
00:15:33.331 --> 00:15:35.850
like TPU is like,
you know,
all these technologies,

252
00:15:35.910 --> 00:15:40.350
it's gonna become easier and easier for everyday developers to make valuable

253
00:15:40.351 --> 00:15:44.580
contributions to the field.
So it doesn't matter.
It's all about growth.

254
00:15:44.640 --> 00:15:47.490
I'm growing,
I'm learning this thing could just totally fail.
Right?

255
00:15:47.640 --> 00:15:50.940
The TPU in Colab is actually,
it's kind of like Beta ish.

256
00:15:51.120 --> 00:15:53.190
It was like it was messing up for me last night.

257
00:15:53.191 --> 00:15:57.180
So who knows if this is even going to work right.
I this could totally fail.

258
00:15:57.181 --> 00:16:02.160
So we're all learning and growing at the same time.
One more question.
Can you,

259
00:16:02.250 --> 00:16:06.930
does TPU support only tensorflow?
Yes.
It only supports tensorflow.

260
00:16:07.530 --> 00:16:11.550
Great.
That's it.
Now the TPU itself.

261
00:16:11.551 --> 00:16:15.780
So Google actually has built three generations of these.
TPU is now.
Okay.

262
00:16:15.840 --> 00:16:17.850
So the first one was in 2015.

263
00:16:18.030 --> 00:16:21.190
The second one was in 2017 and the third one,

264
00:16:21.380 --> 00:16:23.160
it just came out this year.

265
00:16:23.550 --> 00:16:28.550
And also a TPU pod is what they call a cluster of these tps are combined

266
00:16:29.341 --> 00:16:31.410
together.
It's called a TPU pod.
Okay.

267
00:16:31.920 --> 00:16:32.753
<v 1>Okay.</v>

268
00:16:32.810 --> 00:16:35.990
<v 0>So it took them 15 months to make Tpu v one.</v>

269
00:16:36.170 --> 00:16:40.490
And that is actually very fast because what the TPU is,

270
00:16:40.670 --> 00:16:44.780
is it's an application specific integrated circuit or a sick.
Now,

271
00:16:44.781 --> 00:16:47.960
if you're familiar with Bitcoin,
a six are used to mine.

272
00:16:47.990 --> 00:16:50.930
Bitcoin not GPU is generally why?

273
00:16:51.050 --> 00:16:56.050
Because a six are custom made chips designed specifically,

274
00:16:56.310 --> 00:16:58.310
uh,
in that case to mine bitcoin.

275
00:16:58.460 --> 00:17:01.760
Now can we apply the same logic to deep learning?
Of course.

276
00:17:02.030 --> 00:17:07.030
What if we create an created an application specific integrated chip on a sick,

277
00:17:07.130 --> 00:17:11.600
not a general purpose chip specifically for the Matrix operations that neural

278
00:17:11.601 --> 00:17:16.220
networks require,
what would we call that?
Well,
we would call it a TPU,

279
00:17:16.221 --> 00:17:17.930
a tensor processing units.

280
00:17:18.110 --> 00:17:23.000
Now the downside of this are that tps are inflexible.
A six are inflexible.

281
00:17:23.060 --> 00:17:25.780
Once you build that chip using say the,

282
00:17:26.540 --> 00:17:30.290
let's say for the Matrix accumulate operation,
the Mac,
right.

283
00:17:30.291 --> 00:17:31.700
Specifically for neural networks.

284
00:17:31.790 --> 00:17:35.540
Now it's not going to be able to do individual like different different

285
00:17:35.541 --> 00:17:39.230
operations.
It's only going to be able to do that hard coded operation.

286
00:17:39.231 --> 00:17:41.420
The Mac that we hard coded into it,
right?

287
00:17:41.570 --> 00:17:45.710
So it's a specific chips specifically I said specific like three times out

288
00:17:45.711 --> 00:17:47.960
specifically for deep learning.

289
00:17:48.600 --> 00:17:49.100
<v 2>Yeah.</v>

290
00:17:49.100 --> 00:17:50.300
<v 0>And there had been 10</v>

291
00:17:50.300 --> 00:17:52.640
<v 2>and so there,
here's a great image of it.
I should have been showing this.</v>

292
00:17:52.910 --> 00:17:57.080
The multiply add accumulate.
Um,
step looks like this,
right?

293
00:17:57.081 --> 00:18:02.081
So we have our multiplier where most point a times B we add our bias and then we

294
00:18:02.631 --> 00:18:04.550
accumulate the results and then we have our output.

295
00:18:04.551 --> 00:18:09.440
So that's kind of a diagram of what the Mac,
uh,
structure looks like.

296
00:18:11.740 --> 00:18:15.850
<v 0>And so Google decided they needed,
they needed to make a matrix machine,
right?</v>

297
00:18:15.851 --> 00:18:19.110
GPS are vector machines,
they can also be applied to maitre season.

298
00:18:19.120 --> 00:18:24.040
Cpu is our scalar machines.
But Matrix,
these are two dimensional sets of numbers.

299
00:18:24.130 --> 00:18:27.220
And actually it's it,
they need more than just a matrix machine.

300
00:18:27.370 --> 00:18:31.180
They need a tensor machine because what is it tense or can anybody answer the

301
00:18:31.181 --> 00:18:34.120
question?
What is a tensor you watching?

302
00:18:36.940 --> 00:18:41.770
Okay,
so if you said that a tensor is an end dimensional array,
you are correct.

303
00:18:42.010 --> 00:18:46.180
A tensor can be one dimensions,
two dimensions,
three dimensions,
four dimensions,

304
00:18:46.181 --> 00:18:48.070
five,
six hundreds of dimensions.

305
00:18:48.220 --> 00:18:51.400
Because each feature in a data set can be considered a dimension of data.

306
00:18:51.640 --> 00:18:55.210
Now we just can't visualize it with our biological brains,
but,
uh,
that's,

307
00:18:55.211 --> 00:18:56.110
that's a different topic.

308
00:18:57.010 --> 00:18:57.340
<v 2>Okay.</v>

309
00:18:57.340 --> 00:19:00.520
<v 0>So they wanted a design as chips,
specifically for Matrix operations.</v>

310
00:19:00.521 --> 00:19:04.700
And what we're saying here is the,
the TPU chip,

311
00:19:05.900 --> 00:19:10.040
right?
So it's got,
it's got,
um,
four independent chips.
And what this is,

312
00:19:10.070 --> 00:19:11.840
it's a diagram of a single chip.

313
00:19:12.080 --> 00:19:16.640
So each chip has to compute cores called tensor cores just like this.

314
00:19:16.850 --> 00:19:19.490
And inside of each core are two different units.

315
00:19:19.491 --> 00:19:24.491
One is for operating with scalers and vectors and the other is called the mx

316
00:19:24.681 --> 00:19:29.420
whew.
That's specifically dedicated to the multiply accumulate operation that we

317
00:19:29.570 --> 00:19:31.700
talked about or matrix units.
Msu,

318
00:19:32.030 --> 00:19:34.400
it's also got eight gigabytes of on chip memory,

319
00:19:34.401 --> 00:19:39.401
the HBM with each tensor core and they can perform 16,000 operations in each

320
00:19:40.341 --> 00:19:44.000
cycle.
And these cycles are happening in parallel.
We're going to get into this.

321
00:19:44.150 --> 00:19:47.960
They even decided to use a different type of floating point representation

322
00:19:48.080 --> 00:19:52.430
called beef float 16 rather than the standard I Tripoli half precision

323
00:19:52.460 --> 00:19:54.030
representation.
Um,

324
00:19:54.110 --> 00:19:58.880
and so what this allows them to do is execute user computations independently.

325
00:19:59.470 --> 00:20:02.840
Um,
and it's got a high bandwidth interconnect that allows these chips to

326
00:20:02.841 --> 00:20:04.930
communicate with each other directly.
Um,

327
00:20:05.060 --> 00:20:09.710
and we're talking about the TPU pods that I mentioned earlier.
So what's,

328
00:20:09.711 --> 00:20:13.800
what does pipeline looks like is we have a neural network model that we build

329
00:20:13.801 --> 00:20:16.140
with what's called a Tpu estimator.

330
00:20:16.350 --> 00:20:20.580
Estimators are the application level construct in tensorflow associated with

331
00:20:20.610 --> 00:20:24.330
models.
So they built one specifically for the TPU.
Why?

332
00:20:24.390 --> 00:20:28.410
Because when we build one with the TPU estimator it,

333
00:20:28.411 --> 00:20:31.450
we'll then compile a down using Xla,

334
00:20:31.530 --> 00:20:36.480
which is this custom proprietary software that Google has built that allows it

335
00:20:36.481 --> 00:20:37.860
to be run.
Um,

336
00:20:38.770 --> 00:20:39.430
<v 2>yeah,</v>

337
00:20:39.430 --> 00:20:42.670
<v 0>in the form of a systolic array.
Now you might be thinking,
well,</v>

338
00:20:42.700 --> 00:20:45.640
what's a systolic or right.
We're going to get right into that in a second.
Okay.

339
00:20:45.970 --> 00:20:49.570
So the excellent is it is an experimental compiler for the back end of

340
00:20:49.600 --> 00:20:53.830
tensorflow and it turns your tensor flow graph into linear Algebra and it has

341
00:20:53.831 --> 00:20:57.970
backends of its own to run on CPU and GPU and tps.
And it's proprietary.

342
00:20:58.120 --> 00:21:01.930
But you know,
we'll see how that goes.
So the systolic array,

343
00:21:02.050 --> 00:21:03.940
we're just going down the rabbit hole here guys.

344
00:21:03.941 --> 00:21:06.520
We are going so down the rabbit hole before we start coding.

345
00:21:06.730 --> 00:21:09.780
Let me see if there are any,
uh,
questions.
Um,

346
00:21:16.630 --> 00:21:17.463
<v 2>cool.</v>

347
00:21:18.310 --> 00:21:21.190
<v 0>Okay,
so the systolic array,
what this is,
it's,
it's a,</v>

348
00:21:21.430 --> 00:21:23.410
it's a kind of hardware algorithm.

349
00:21:23.411 --> 00:21:27.460
The systolic array is a type of hardware algorithm that describes a pattern of

350
00:21:27.461 --> 00:21:30.940
cells on a chip that computes matrix multiplication.

351
00:21:31.060 --> 00:21:34.180
How did these cells on this chip perform?
The Matrix?

352
00:21:34.200 --> 00:21:37.930
Small operations that are neural network requires.
So systolic,

353
00:21:37.931 --> 00:21:41.410
the word systolic is derived from the,
the,

354
00:21:41.470 --> 00:21:43.360
I think there's Latin for wave,
it's,

355
00:21:43.430 --> 00:21:48.070
it's Latin for wave and it describes how data is moving across the chip.

356
00:21:48.190 --> 00:21:51.880
It's moving across the chip in waves,
like the beating of a human heart,

357
00:21:52.210 --> 00:21:56.900
which is kind of a cool and kind of scary but cool.
Mostly.
Uh,
but um,

358
00:21:56.920 --> 00:21:59.470
so here's an example here,
right?
So we have these two matrices,

359
00:21:59.590 --> 00:22:04.590
we multiply them together and here's our output and this output is going to be

360
00:22:05.411 --> 00:22:08.500
computed using a systolic array algorithm.
Okay.

361
00:22:08.501 --> 00:22:10.630
So for this two by two input,

362
00:22:10.840 --> 00:22:14.560
the each term in the output is going to be the sum of two products,
right?

363
00:22:14.680 --> 00:22:18.160
So a and e we're going to multiply those two together and it's going to be a sum

364
00:22:18.161 --> 00:22:21.430
of a and e Plus B.
And g.
So if you,

365
00:22:21.460 --> 00:22:23.410
if you stare at this image for a while,

366
00:22:23.440 --> 00:22:27.400
it'll start to make sense the ordering with which these operations are
happening.

367
00:22:28.870 --> 00:22:33.820
So let's say that a,
B,
c,
d represent our activations and e,
f,
g,

368
00:22:33.821 --> 00:22:35.830
H,
I,
g h are our weights.

369
00:22:35.860 --> 00:22:39.160
So for our array we're going to first load up the weights like so.

370
00:22:39.161 --> 00:22:43.960
So e f g h are going to be loaded up and our activations are going to go into an

371
00:22:43.990 --> 00:22:47.320
input queue,
which for our example,
we'll go to the left of each row.

372
00:22:47.710 --> 00:22:52.710
So we need to apply our activations to the result of our,

373
00:22:53.380 --> 00:22:56.980
our weight operations.
So we're going to take our input data,

374
00:22:57.160 --> 00:23:01.300
multiply it by our weights,
uh,
and then apply our activations to it.

375
00:23:02.920 --> 00:23:04.510
And we want to do this in parallel.

376
00:23:04.540 --> 00:23:07.840
So what the systolic array algorithm does is it's,

377
00:23:07.870 --> 00:23:12.520
it allows multiple cycles of computations to happen in parallel.

378
00:23:12.670 --> 00:23:16.420
So what's going to happen is each cell will execute the following steps in

379
00:23:16.421 --> 00:23:17.650
parallel four steps.

380
00:23:17.920 --> 00:23:21.510
The first step is to multiply our weight and activation and come.

381
00:23:21.550 --> 00:23:24.580
It's going to be coming in from the left.
And if there's no cell on the left,

382
00:23:24.640 --> 00:23:25.930
take one from the input queue,

383
00:23:26.170 --> 00:23:28.810
add that product to the partial sum coming in from above.

384
00:23:28.960 --> 00:23:32.290
If there are no sell above,
uh,
the partial sum is going to be zero.

385
00:23:32.440 --> 00:23:35.380
Pass the activation to the cell to the right.
If there's no cell to the right,

386
00:23:35.381 --> 00:23:38.830
throw away the activation,
pass the partial sum to the cell on the bottom.

387
00:23:38.831 --> 00:23:40.240
If there's no cell on the bottom,

388
00:23:40.360 --> 00:23:43.270
collect the partial sum as the output you might be thinking.

389
00:23:43.271 --> 00:23:46.730
And those are the four steps of a cycle and many cycles are happening in

390
00:23:46.731 --> 00:23:51.680
parallel.
You might be thinking,
what did you just talk about?
That's a very,

391
00:23:51.740 --> 00:23:53.630
that's a very valid question.

392
00:23:54.430 --> 00:23:54.720
<v 2>Yeah.</v>

393
00:23:54.720 --> 00:23:59.720
<v 0>So I also have a python example specifically for you right here that demos</v>

394
00:24:00.601 --> 00:24:03.600
exactly what a systolic array it looks like.
But we have a lot to cover here,

395
00:24:03.780 --> 00:24:06.540
so we're not going to go over this entire code,
but definitely check it out.

396
00:24:06.570 --> 00:24:11.220
Well documented code,
um,
not written by me,
but very well documented code.

397
00:24:11.221 --> 00:24:16.170
Check it out.
So by these rules,
what,
what tends to happen is it looks like this,

398
00:24:16.380 --> 00:24:20.940
the wave of data is moving to the right and down simultaneously.

399
00:24:20.941 --> 00:24:24.840
So it's like,
it's a way that's coming down like this.
So let,
we actually have a,

400
00:24:25.400 --> 00:24:30.040
uh,
a Gif that demos this.
This is Google's Gif,
right?
So it's like,

401
00:24:30.400 --> 00:24:31.233
watch this.

402
00:24:31.540 --> 00:24:36.030
So it's starting at the top left and the wave of operations is moving.
It's,

403
00:24:36.031 --> 00:24:40.210
it's spreading in the right and downwards,
directions like that.

404
00:24:40.930 --> 00:24:41.763
Okay.

405
00:24:43.000 --> 00:24:43.833
<v 2>So,</v>

406
00:24:44.990 --> 00:24:48.980
<v 0>and so what happens is normally a,
to compute a matrix operation,</v>

407
00:24:49.160 --> 00:24:51.320
it's going to take a sequential solution.

408
00:24:51.321 --> 00:24:55.270
So on a CPU would take n cubed time where this,
uh,

409
00:24:55.280 --> 00:24:59.990
where this takes n cubed cycles,
where n is the number of cycles.
So this is the,

410
00:25:00.020 --> 00:25:04.160
this is the,
uh,
this is the performance where as on a Tpu,

411
00:25:04.161 --> 00:25:08.150
it's going to take three and minus two,
which is a very peculiar,
um,

412
00:25:08.960 --> 00:25:12.200
performance,
uh,
metric,
but that that's what it is.

413
00:25:12.201 --> 00:25:15.950
It's three and minus two as opposed to n cubed,
which is a huge performance,

414
00:25:15.951 --> 00:25:20.120
right?
Especially at scale.
So like I said,
this is what it looks like.

415
00:25:20.121 --> 00:25:21.680
It's going to the right and it's going down.

416
00:25:21.681 --> 00:25:24.920
That's the flow of data on the TPU hardware chip.

417
00:25:26.300 --> 00:25:26.480
<v 2>Okay.</v>

418
00:25:26.480 --> 00:25:30.200
<v 0>So it's just Mac operations multiply,
accumulate,
multiply,
accumulate,
multiply,</v>

419
00:25:30.201 --> 00:25:34.070
accumulate at the same time in parallel,
massively parallel.

420
00:25:34.700 --> 00:25:37.430
And so the high density of the systolic arrays,

421
00:25:37.431 --> 00:25:41.930
let's Google pack in 16,000 of them into a single Amex.
Whew,

422
00:25:42.290 --> 00:25:44.600
that's a lot.
That's a lot.
Um,

423
00:25:44.630 --> 00:25:49.630
and what this translates to for us is a lot more speed and efficiency when we're

424
00:25:49.971 --> 00:25:53.010
training our neural network,
both for inference,
um,

425
00:25:53.240 --> 00:25:56.660
and for serving inference and for training.

426
00:25:58.670 --> 00:26:01.460
So that's it.
The rest of the chip is important and worth going over.

427
00:26:01.760 --> 00:26:06.280
But the core advantage of the TPU is this Msu,
uh,
unit.

428
00:26:06.460 --> 00:26:06.940
It's this,

429
00:26:06.940 --> 00:26:11.940
it's a systolic array matrix multiplication unit designed specifically for

430
00:26:12.560 --> 00:26:14.990
neural networks.
So what are use cases?

431
00:26:14.991 --> 00:26:18.710
So we've seen some benchmarks here and there.
Um,

432
00:26:18.800 --> 00:26:23.800
but I don't think that we need to question whether or not the TPU is a faster

433
00:26:25.040 --> 00:26:29.710
chip than a GPU for neural networks.
Right?
Because Google use,

434
00:26:30.080 --> 00:26:32.840
Google uses it for Google search,
right?

435
00:26:32.930 --> 00:26:34.790
Just think about how massive Google searches.

436
00:26:34.910 --> 00:26:38.450
There's no question that a TPU is faster than a GPU,

437
00:26:38.480 --> 00:26:41.240
specifically for neural networks.
That being said,

438
00:26:41.420 --> 00:26:46.020
do not underestimate Jensen Wong and video because I promise you they are

439
00:26:46.021 --> 00:26:50.410
working on something just as good,
if not better.
So it's,

440
00:26:50.420 --> 00:26:54.630
it's all,
it's all,
it's all love.
It's all family.
We're all in this together.
So,

441
00:26:55.110 --> 00:26:59.970
um,
that's it.
Anyway.
Anyway,
so CPU,
yes.

442
00:27:00.490 --> 00:27:01.140
<v 2>Yeah.</v>

443
00:27:01.140 --> 00:27:06.080
<v 0>CPES are,
what are the,
what are the use cases for each?
We have CPS,
we have gps,</v>

444
00:27:06.110 --> 00:27:09.830
we have tps.
Cpu are good for quick prototyping,

445
00:27:09.831 --> 00:27:13.190
very simple models.
Think linear regression,
right?

446
00:27:13.191 --> 00:27:16.450
Think just very small amounts of data.
Performance as a matter.

447
00:27:16.470 --> 00:27:17.990
You just want to do something really quickly.

448
00:27:18.110 --> 00:27:21.260
See pubes are great for that small thing.
Thanks.
Small,
right?

449
00:27:21.261 --> 00:27:25.720
Small effect of batch sizes.
I,
it's not going to take a long time to train,
um,

450
00:27:26.660 --> 00:27:28.000
like that.
Just like that.
So,

451
00:27:28.240 --> 00:27:33.070
but GPU are good for models that are not written written in tensorflow,

452
00:27:33.250 --> 00:27:37.590
right?
Because tensions,
because you can only run tensorflow code.
Think about the,

453
00:27:37.640 --> 00:27:42.220
the Xla compiler I talked about a 40 purpose.
Um,

454
00:27:43.210 --> 00:27:47.130
so,
so there's that.
And so medium to large models,
great for GPS.

455
00:27:47.530 --> 00:27:48.760
But TPS,

456
00:27:48.970 --> 00:27:53.970
tps are great for tensorflow made models that are dominated by Matrix

457
00:27:54.071 --> 00:27:58.930
computations,
neural networks,
and we're talking giant models that takes,

458
00:27:58.990 --> 00:28:03.340
that takes weeks or months to train.
What would be one great example,

459
00:28:03.790 --> 00:28:05.800
machine translation.
Okay.

460
00:28:05.801 --> 00:28:09.010
I think I remember reading a machine translation paper and I'll take questions

461
00:28:09.011 --> 00:28:10.330
after this.
So start asking questions.

462
00:28:11.320 --> 00:28:13.990
I remember reading a machine translation paper that talked about,

463
00:28:13.991 --> 00:28:15.730
I think it was a bilateral

464
00:28:17.400 --> 00:28:20.310
bilateral recurrent network and it took them a week to train.

465
00:28:20.640 --> 00:28:24.300
Machine translation takes so much data to train.

466
00:28:24.480 --> 00:28:28.740
It took Google a week to train Google on their computer.

467
00:28:28.741 --> 00:28:29.574
That's insane.

468
00:28:29.670 --> 00:28:34.350
So that is a perfect use case for a TPU right now.

469
00:28:34.351 --> 00:28:36.900
Let's take some questions before we get into our benchmarks.

470
00:28:46.440 --> 00:28:47.273
<v 2>Okay.</v>

471
00:28:47.610 --> 00:28:48.390
<v 0>Okay.
Uh,</v>

472
00:28:48.390 --> 00:28:52.920
deepmind's Alphago has brought down from 176 gps to 40 purpose.

473
00:28:53.340 --> 00:28:56.870
Yeah.
So Alphago used tps for sure.
Uh,

474
00:28:56.880 --> 00:29:00.060
nick systolic arrays sound like a very fundamental data structure.

475
00:29:00.061 --> 00:29:03.960
They are a very fundamental algorithm for sure.
Um,

476
00:29:05.040 --> 00:29:05.870
<v 2>okay,</v>

477
00:29:05.870 --> 00:29:06.920
<v 0>one more question.</v>

478
00:29:11.790 --> 00:29:16.170
What else can I keep you do other than specific jobs or is it that a TPU is that

479
00:29:16.171 --> 00:29:20.280
limited and I only know basic Algebra?
What can I do with this math in our l?

480
00:29:20.310 --> 00:29:22.830
Great question.
So um,

481
00:29:23.880 --> 00:29:24.713
<v 2>yeah.</v>

482
00:29:24.730 --> 00:29:25.720
<v 0>First of all,</v>

483
00:29:25.900 --> 00:29:30.410
a TPU art made for neural networks and what else can you do besides their own

484
00:29:30.420 --> 00:29:31.600
networks?
You can,

485
00:29:32.410 --> 00:29:36.220
you can try to do other things and there's some research there.

486
00:29:36.221 --> 00:29:40.240
Actually you can make some valuable contributions by trying new types of models

487
00:29:40.241 --> 00:29:41.980
out,
but they are designed for no one that works

488
00:29:41.980 --> 00:29:44.680
<v 3>for a second question and that's it for the questions for now.</v>

489
00:29:45.880 --> 00:29:48.790
When do we use linear Algebra?
In reinforcement learning?
Look,

490
00:29:49.600 --> 00:29:52.900
you and me or neural networks,

491
00:29:52.930 --> 00:29:56.230
we are neural networks in our head.
Okay.
We are neural networks,

492
00:29:56.950 --> 00:30:00.790
but we are neural networks in the context of a mark called decision process.

493
00:30:00.940 --> 00:30:04.180
This reality is a reinforcement learning process.

494
00:30:04.181 --> 00:30:08.410
We can frame reality mathematically.
We are agents in an environment.

495
00:30:08.470 --> 00:30:12.280
We are in a specific state and we're trying to optimize for the neck.

496
00:30:12.310 --> 00:30:14.560
What is going to be the best next state to be in?

497
00:30:14.561 --> 00:30:17.240
What is the best next state to be in?
Right?
And the,

498
00:30:17.241 --> 00:30:21.250
the way we get there is by performing the next best action that's going to

499
00:30:21.340 --> 00:30:24.220
optimize for a reward value.
So we,

500
00:30:24.250 --> 00:30:27.820
neural networks are inside of a mark Haub decision process.

501
00:30:28.210 --> 00:30:31.450
So reality is deep reinforcement learning.

502
00:30:31.780 --> 00:30:34.270
So what you'll find is that it's not about,
should I do,

503
00:30:34.300 --> 00:30:36.910
should I study RL or should I study,
supervise,

504
00:30:36.940 --> 00:30:41.750
or should or should I study unsupervised?
No,
it's all apart.
It's,

505
00:30:41.920 --> 00:30:45.940
they're all pieces of the same puzzle.
So it all,
it all matters.
Anyway,

506
00:30:47.530 --> 00:30:50.230
let's get into our benchmark here.
So this for this benchmark,

507
00:30:50.231 --> 00:30:53.680
we're going to do a simple ad operation.
Now look no promises.

508
00:30:53.681 --> 00:30:57.060
This is going to work.
He said this is kind of Beta.
Um,
and it didn't,
you know,

509
00:30:57.070 --> 00:30:59.740
it worked and then it didn't work last night.
So we'll see if this works.

510
00:31:00.160 --> 00:31:04.450
But we're going to perform a simple tensorflow based ad operation on the CPU and

511
00:31:04.451 --> 00:31:05.890
then compare that to the TPU.

512
00:31:06.520 --> 00:31:10.630
We can all use the TPU because it's inside of Google colab.
Okay,
so let's,

513
00:31:10.631 --> 00:31:11.560
let's start with this.

514
00:31:12.940 --> 00:31:17.030
So our first step is going to be to import our dependencies.
We have [inaudible],

515
00:31:17.160 --> 00:31:21.190
which is our,
which is four stands for operating system,
which lets us do some,

516
00:31:21.191 --> 00:31:22.900
you know,
input output and you know,

517
00:31:23.830 --> 00:31:26.860
file file retrieval and things like that.

518
00:31:27.560 --> 00:31:28.070
<v 2>Yeah.</v>

519
00:31:28.070 --> 00:31:32.750
<v 3>Tentraflow time num Pi.
Great.
And we'll install these dependencies.</v>

520
00:31:36.080 --> 00:31:39.760
Okay.
Now let's do a CPU benchmark here.
Let's,
let's write out our operations.

521
00:31:39.761 --> 00:31:43.160
So like I said,
this is going to be a very simple ad operation.

522
00:31:43.161 --> 00:31:46.220
We'll call it ad ad op.
Okay.
Add Up.

523
00:31:46.400 --> 00:31:51.400
So ad op is going to take to value to scalar values x and y and it's going to

524
00:31:51.651 --> 00:31:52.484
return the,

525
00:31:52.670 --> 00:31:55.550
it's going to return what happens when we add them together just like that.

526
00:31:55.940 --> 00:31:59.570
And so x and y are actually going to be tensorflow,
placeholders,

527
00:31:59.780 --> 00:32:01.250
what are tensorflow place holders.

528
00:32:01.430 --> 00:32:05.900
They are gateways into our computation graph that we can input data into.

529
00:32:07.350 --> 00:32:11.730
Now I wrote this by the way,
because this is so new that there was not,

530
00:32:12.150 --> 00:32:16.650
there was not like a benchmarking script.
So I just wrote this.
Okay.
So,
um,

531
00:32:18.390 --> 00:32:18.940
<v 2>okay.</v>

532
00:32:18.940 --> 00:32:21.850
<v 3>Um,
so why is also going to be a 10th of a place holder?</v>

533
00:32:21.910 --> 00:32:24.340
And we're going to input data into each of these in a second.

534
00:32:27.880 --> 00:32:28.713
<v 2>So</v>

535
00:32:28.880 --> 00:32:32.270
<v 3>there are a hundred possible values that are going to be here.
Okay.</v>

536
00:32:32.590 --> 00:32:33.980
There are a hundred possible values.

537
00:32:34.190 --> 00:32:38.600
Now will initialize our tensor flow session or computation graph with DTF dot

538
00:32:38.601 --> 00:32:43.490
session.
Remember,
none of this is TPU yet.

539
00:32:43.491 --> 00:32:48.380
We're about to convert it into TPU code.
So inside of our TensorFlow's session,

540
00:32:50.870 --> 00:32:54.490
inside of our tensor flow session,
we're going to start a timer to,

541
00:32:54.520 --> 00:32:57.590
to clocks and performance here.
Now what are we going to do?

542
00:32:57.830 --> 00:33:01.550
We're going to compute a results by saying session.run.

543
00:33:01.551 --> 00:33:03.260
So inside of the computation graph,

544
00:33:03.410 --> 00:33:07.700
run this ad operation on both x and y.

545
00:33:09.770 --> 00:33:14.080
And what are we going to feed in as our data into these placeholders?
Well,

546
00:33:14.090 --> 00:33:17.930
using the feed Dick,
a parameter will say,

547
00:33:17.931 --> 00:33:22.931
well for x we're going to input are a value between zero and 100 using the a

548
00:33:23.481 --> 00:33:24.314
range,

549
00:33:24.770 --> 00:33:29.690
a function of num Pi and for why we're going to do the same thing of value

550
00:33:29.691 --> 00:33:33.680
between a one and 100.

551
00:33:35.380 --> 00:33:36.213
<v 1>Okay,</v>

552
00:33:38.250 --> 00:33:42.960
<v 3>hold on.
Let me make sure that that is going to be back there.
Good.</v>

553
00:33:43.200 --> 00:33:47.910
All right,
so there's that feed Deckland,
make sure it's all good here.

554
00:33:48.870 --> 00:33:51.300
Okay.
Now once we compute that opera,

555
00:33:51.350 --> 00:33:54.060
so basically we're taking two arrays between one and 100.

556
00:33:54.061 --> 00:33:57.450
So it's just one through a hundred each of those values and we're just adding

557
00:33:57.451 --> 00:33:58.260
them all up.

558
00:33:58.260 --> 00:34:01.710
So the result is going to be one plus one is two is going to be the first value.

559
00:34:02.070 --> 00:34:06.090
The next value is going to be a two plus two,
four for two,
four,

560
00:34:06.600 --> 00:34:10.440
six.
That's gonna be the result and it's gonna be a hundred of those in an array.

561
00:34:11.820 --> 00:34:14.810
And uh,
we'll end the timer here.
That's,

562
00:34:14.850 --> 00:34:17.490
that's all we want to do and will compute the elapsed time.

563
00:34:17.491 --> 00:34:21.090
The end mine is a start and we'll print that out.
How much time has elapsed here?

564
00:34:21.450 --> 00:34:25.050
And we'll print out the result as well so we can see what we computed.
Okay.

565
00:34:25.200 --> 00:34:27.300
So let's see if this works.
Of course not.

566
00:34:28.860 --> 00:34:29.110
<v 1>Okay.</v>

567
00:34:29.110 --> 00:34:31.220
<v 3>Oh,
see I already knew.
Okay.
X,</v>

568
00:34:33.480 --> 00:34:34.313
<v 1>right.</v>

569
00:34:37.980 --> 00:34:38.813
Okay.

570
00:34:39.890 --> 00:34:43.280
<v 3>And this one goes like that</v>

571
00:34:47.820 --> 00:34:50.280
feed dicked equals.

572
00:34:53.870 --> 00:34:54.703
<v 1>Okay.</v>

573
00:34:55.830 --> 00:34:59.550
<v 3>Like I said,
zero two,
four,
six,
eight that is running on a CPU.</v>

574
00:34:59.730 --> 00:35:04.230
And so this took 0.01 seconds.
Okay.
Now we're going to come.

575
00:35:04.500 --> 00:35:07.140
We're going to convert this to Tpu code.
You're ready.
You're ready for this.

576
00:35:07.680 --> 00:35:11.340
So that is our TPU.
So let's remember this benchmark.
Your 0.01.

577
00:35:11.370 --> 00:35:13.200
Now let's compare it to the TPU.
Okay.

578
00:35:14.510 --> 00:35:15.220
<v 1>Okay.</v>

579
00:35:15.220 --> 00:35:19.300
<v 3>So how do we do this?
So in Colab,
what we can do is say,</v>

580
00:35:19.390 --> 00:35:21.940
change the runtime to the,

581
00:35:23.570 --> 00:35:28.370
Oh my God,
there it is.
TPU.
Awesome.
Save.

582
00:35:30.900 --> 00:35:34.440
Connecting.
Please connect.
You gotta connect for me.

583
00:35:34.830 --> 00:35:39.540
Do not do not up Google.
Google is watching this.
By the way.

584
00:35:39.541 --> 00:35:42.570
They're here.
They were actually helping me last night on DM,
on Twitter.

585
00:35:42.571 --> 00:35:44.920
They're like,
are you sure about this?
Maybe check this out.

586
00:35:44.950 --> 00:35:46.170
[inaudible] so thank you Google,

587
00:35:46.770 --> 00:35:49.740
because they saw that I was doing this live stream.
Oh,
it's initializing.

588
00:35:50.490 --> 00:35:53.190
Oh my God,
this is going to work.
It's connected.
All right,
so TPU Tom guys.

589
00:35:53.460 --> 00:35:57.600
So TPU address
equals GRPC.

590
00:35:58.980 --> 00:36:03.300
Um,
so in Colab we can actually,
it's the TPU is,

591
00:36:03.330 --> 00:36:04.170
has a name,

592
00:36:04.320 --> 00:36:08.970
so we can call it using the [inaudible] environment variable and we'll say

593
00:36:08.971 --> 00:36:12.840
Colab,
a TPU address.

594
00:36:13.860 --> 00:36:16.170
Okay.
Address,
that's the name.

595
00:36:16.680 --> 00:36:20.820
And once we have that will say,
well,
what is the TPU address?
Let's,

596
00:36:20.821 --> 00:36:25.680
let's see what it is.
TPU address is tip you address.

597
00:36:27.780 --> 00:36:30.120
Now inside of our tent or floats session,

598
00:36:30.600 --> 00:36:35.600
we will initialize a session using the TPU address or the address of where it

599
00:36:35.941 --> 00:36:39.990
is.
And now we can list all of the devices,

600
00:36:39.991 --> 00:36:43.440
which are the cores,
the CPU cores,
um,

601
00:36:44.010 --> 00:36:45.660
in that list of devices.

602
00:36:52.510 --> 00:36:54.550
And I'll print them out at the very end.

603
00:36:58.800 --> 00:37:02.910
Uh,
print TPU devices,

604
00:37:04.470 --> 00:37:07.020
devices.
Now let's just see if this works

605
00:37:12.760 --> 00:37:17.710
really,
but I did define Wes.
Did I not?
Did I not really.
Okay,

606
00:37:17.770 --> 00:37:21.250
fine.
Fine.
All right.
In Porto is,

607
00:37:27.750 --> 00:37:32.370
did I really not?
Okay,
let's just import everything again.
Tentraflow as TF.

608
00:37:39.740 --> 00:37:43.760
Okay.
It's busy.
It's busy,
it's got that cheap,

609
00:37:43.790 --> 00:37:46.460
it's got that CPU,
it's got that TPU.
So there we go.

610
00:37:46.490 --> 00:37:50.090
We are connected to our TPU and now we're going to do this.

611
00:37:50.091 --> 00:37:52.850
So we'll do the same operation we did before.

612
00:37:52.851 --> 00:37:54.110
And I'm going to take questions after this.

613
00:37:54.111 --> 00:37:55.970
So just start asking questions right now guys.

614
00:37:56.480 --> 00:37:59.780
Now I'm just going to do the same thing I did before where I have this ad

615
00:37:59.781 --> 00:38:04.190
operation.
Okay.
But,
um,
the different,

616
00:38:04.191 --> 00:38:06.350
and I'll do the same thing as up here.

617
00:38:09.350 --> 00:38:10.183
<v 2>Okay?</v>

618
00:38:10.870 --> 00:38:15.250
<v 3>By the way.
So when I said I wrote the code,
I wrote the,
I wrote the,
this,</v>

619
00:38:16.210 --> 00:38:20.070
this,
um,
TPU versus CPU benchmark.
It was based off of,
uh,

620
00:38:20.140 --> 00:38:23.620
Google's original code,
but there was no,
like TPU versus CPU version.

621
00:38:23.621 --> 00:38:26.260
There was only a TPU version.
So I converted it between the book,

622
00:38:26.440 --> 00:38:29.740
between both so we can see the difference.
Um,

623
00:38:31.180 --> 00:38:34.240
okay,
so where were we?
So the TPU operations here.

624
00:38:36.500 --> 00:38:39.890
So what we're going to first do,
and here's,
here's a,
here's an important bit,

625
00:38:40.130 --> 00:38:40.651
or we're,

626
00:38:40.651 --> 00:38:44.930
we're going to rewrite that ad operation for the CPU specifically with this

627
00:38:44.931 --> 00:38:48.030
rewrites function.
Now,
like I said before,
um,

628
00:38:48.080 --> 00:38:52.490
tps are running on the systolic array,
a hardware architecture.

629
00:38:52.700 --> 00:38:57.700
So we have a multiply and accumulate operation that is specific to the TPU.

630
00:39:00.560 --> 00:39:04.040
And in order to be able to compile down using the XL,
Hey,

631
00:39:04.070 --> 00:39:05.930
a compile time environment,

632
00:39:06.110 --> 00:39:09.880
we need to convert it into a unex l a friendly,
ah,

633
00:39:09.920 --> 00:39:12.950
format of matrix operations.
And that's what rewrite does.

634
00:39:13.670 --> 00:39:16.870
So once we initialize our tensorflow sessions,
we'll do this in a try.

635
00:39:17.120 --> 00:39:17.960
Finally loop.

636
00:39:18.700 --> 00:39:19.120
<v 1>Okay,</v>

637
00:39:19.120 --> 00:39:21.910
<v 3>what's say session.run and uh,</v>

638
00:39:21.940 --> 00:39:26.320
well the first thing we're going to have to do is initialize the system.

639
00:39:31.110 --> 00:39:34.380
Okay,
that's the first thing we have to do.
And then we'd say,
well,

640
00:39:34.381 --> 00:39:39.381
let's start the timer and we'll print out the results of what happens when we

641
00:39:41.011 --> 00:39:46.011
say run a session using those TPU operations.

642
00:39:47.730 --> 00:39:51.180
We're going to feed in the same deal.

643
00:39:51.181 --> 00:39:53.280
So it's gonna be the num py.
Dot.

644
00:39:53.281 --> 00:39:58.281
K range between Oh oh and 101 in 100 and for why it's going to be end pay n p.

645
00:40:02.451 --> 00:40:02.660
Dot.

646
00:40:02.660 --> 00:40:07.660
A range between one in 100 and right?

647
00:40:08.190 --> 00:40:12.570
That's it.
That,
that I think is it.
So then once we have that now,

648
00:40:14.570 --> 00:40:17.990
now we can end the timer by saying and time.

649
00:40:18.200 --> 00:40:21.290
We'll count the amount of time that has elapsed since we started.

650
00:40:21.530 --> 00:40:26.530
So if elapsed equals and minus start and we'll print the elapsed time.

651
00:40:29.420 --> 00:40:31.160
Now that's it for that.

652
00:40:31.640 --> 00:40:36.620
And then in our finally loop and make sure finally is here.

653
00:40:37.250 --> 00:40:41.940
Finally,
we will,

654
00:40:43.200 --> 00:40:47.010
uh,
shut down the system.
So this is necessary.
We have to,

655
00:40:47.040 --> 00:40:49.650
when we're using the TPU,
we have to shut down.

656
00:40:50.040 --> 00:40:54.600
We used to run this command after we are done with our computations and then we

657
00:40:54.601 --> 00:40:58.240
can close the ship,
the session.
Okay.
Let's,
let's hope this works.

658
00:40:58.241 --> 00:41:02.800
It's not gonna work,
but let's,
let's hope it works.
Of course.
So,

659
00:41:03.670 --> 00:41:04.503
okay.

660
00:41:07.560 --> 00:41:12.390
Why would that be invalid syntax in valid syntax on line 14.

661
00:41:13.250 --> 00:41:14.083
<v 1>Oh,</v>

662
00:41:15.420 --> 00:41:15.781
<v 3>right.
Okay.</v>

663
00:41:15.781 --> 00:41:20.781
So print session.run TPU operations x,

664
00:41:22.410 --> 00:41:26.970
y
one more I think.

665
00:41:27.330 --> 00:41:28.163
Right?

666
00:41:32.690 --> 00:41:37.340
Okay.
I really hope this works.

667
00:41:38.630 --> 00:41:42.190
I really hope this works.
This is what life is all about.
You,

668
00:41:42.210 --> 00:41:46.730
I'll make a fool of myself or okay,
here we go.
Time is not defined.

669
00:41:47.060 --> 00:41:50.780
Yes it is.
Okay,
fine.
So,
um,

670
00:41:52.190 --> 00:41:55.370
I think it's like time is not defined.

671
00:41:55.371 --> 00:41:57.460
So import time to port time

672
00:42:06.100 --> 00:42:07.440
NP,
isn't it?
Okay.
So,

673
00:42:12.250 --> 00:42:15.130
so I guess import tensorflow as well.
Might as well

674
00:42:19.720 --> 00:42:20.553
come on.

675
00:42:24.990 --> 00:42:25.823
Yes.

676
00:42:30.760 --> 00:42:34.270
Really.
It has no attributes such shutdown system.
Oh,

677
00:42:34.271 --> 00:42:35.860
because of this.

678
00:42:43.020 --> 00:42:44.490
Okay.
Yes.

679
00:42:50.540 --> 00:42:52.280
Oh,
TF.
Dot.
Contribute,

680
00:42:52.640 --> 00:42:56.840
contribute dot TPU dot shutdown system

681
00:42:59.540 --> 00:43:01.700
are start asking questions.
I'll take some questions in a second.

682
00:43:02.210 --> 00:43:06.680
And I think this is finally going to work.
Yes.
Good.
Okay.

683
00:43:06.681 --> 00:43:11.681
So 0.016 on the TPU versus where were we before 0.011.

684
00:43:15.080 --> 00:43:19.610
So actually this,
the,
a GPU was faster,

685
00:43:19.611 --> 00:43:24.110
so this is actually,
oh no.
Yeah,
yeah.
The GPU was faster.

686
00:43:24.140 --> 00:43:26.780
So this was actually a GPU versus TPU comparison.

687
00:43:27.140 --> 00:43:32.060
The GP was faster for this extremely small operation,
this ad operation.

688
00:43:32.210 --> 00:43:35.090
So my point here is just due to,
I've done two things.

689
00:43:35.270 --> 00:43:39.500
One to demonstrate that you can use the TPU and to that when it comes to small

690
00:43:39.501 --> 00:43:43.250
models like what we've just done here,
it's not like Tpu is like the be all end.

691
00:43:43.251 --> 00:43:46.490
All right.
It's got a specific use case.

692
00:43:47.240 --> 00:43:49.010
So now let me take some questions.

693
00:43:56.100 --> 00:44:00.680
Um,
Ken tps process,
neural networks made with care.

694
00:44:00.840 --> 00:44:04.330
Yes.
We'll do that at the very end.
One more question.
Um,

695
00:44:08.270 --> 00:44:09.103
<v 2>okay.</v>

696
00:44:09.660 --> 00:44:13.260
<v 3>What is the next big thing after Tpu is,
um,
it's uh,</v>

697
00:44:14.220 --> 00:44:17.430
it's quantum computing,
quantum computing,

698
00:44:17.520 --> 00:44:21.470
so I'll have more to say on that later.
So that's it.

699
00:44:21.480 --> 00:44:26.220
Quantum quantum and at least the unleashed the quantum anyway.
Um,
it's,

700
00:44:26.221 --> 00:44:30.030
it's all about having a wide variety of different types of types of chips and

701
00:44:30.031 --> 00:44:31.980
they all do different things.
Chips are

702
00:44:31.980 --> 00:44:35.700
<v 0>good,
good for specific tasks.
You know,
tps are good for neural networks.</v>

703
00:44:35.730 --> 00:44:37.410
CPS are good for sequential tasks.

704
00:44:37.530 --> 00:44:42.530
GPS are good for rendering and just altogether we can use them for Agi,

705
00:44:43.530 --> 00:44:47.940
artificial general intelligence,
but not not to get,
you know,
anyway,

706
00:44:50.160 --> 00:44:54.390
checkout Jerome linear by the way.
Great.
A philosopher.
So,
uh,

707
00:44:54.391 --> 00:44:57.030
that was it for Q and a and now there,
there are some key,

708
00:44:57.031 --> 00:44:57.864
let's see what time we have.

709
00:44:58.020 --> 00:45:01.770
There are some key TPU functions in tensorflow that I'd like to talk about right

710
00:45:01.771 --> 00:45:04.710
now.
Okay.
So the first one that you should know,
there are three.

711
00:45:05.220 --> 00:45:07.920
The first one is the TPU estimator.
Okay.

712
00:45:07.921 --> 00:45:11.640
So estimator is our application level constructs in tentraflow.
Right?

713
00:45:11.641 --> 00:45:14.220
So they apply to any,
let me just minimize this.

714
00:45:17.950 --> 00:45:19.750
They apply to any kind of model,

715
00:45:19.810 --> 00:45:24.810
but we can use a TPU specific estimators using this command right here or

716
00:45:24.950 --> 00:45:28.300
function right here at tps matter for TPU specific logic.

717
00:45:28.750 --> 00:45:30.970
Now Standard estimators are good for CPS and gps,

718
00:45:30.971 --> 00:45:33.320
but TPU estimators are made for Tpu.

719
00:45:33.940 --> 00:45:35.800
And so this is what it looks like when we build that.

720
00:45:36.340 --> 00:45:39.400
Now notice that one of these parameters is this config parameter.

721
00:45:39.610 --> 00:45:42.310
So using this TPU dot Ron Config,

722
00:45:42.340 --> 00:45:46.960
that's the other TPU specific object that we have to create.

723
00:45:47.290 --> 00:45:50.920
And so we can do that using this tpu.run config function.
And we,

724
00:45:50.980 --> 00:45:54.550
and we give it the evaluation,
uh,
the model directory,

725
00:45:54.551 --> 00:45:57.220
the session configuration and the TPU configuration.

726
00:45:59.310 --> 00:46:03.930
And the third is the cross Shard optimizer.
So like I said,
that 1%,

727
00:46:03.931 --> 00:46:05.910
we're talking about calculus.
It's right now.

728
00:46:06.060 --> 00:46:10.110
So neural networks are models built with linear Algebra and they're optimized

729
00:46:10.111 --> 00:46:12.960
with calculus using gradient descent.
For example,

730
00:46:12.961 --> 00:46:17.400
the most popular optimization strategy in neural networks that I won't go into

731
00:46:17.520 --> 00:46:21.870
just search grades,
the sensorial and you'll find like 50 videos on it.
But,
um,

732
00:46:22.260 --> 00:46:25.950
gradient descent works on tps.

733
00:46:26.000 --> 00:46:30.520
All sorts of optimizers work,
work for neural networks,
work on tps,

734
00:46:30.910 --> 00:46:35.380
uh,
as long as they are made for neural networks,
specifically Adam.
Um,

735
00:46:35.430 --> 00:46:39.720
there's a bunch of optimizers there,
variations of grading dissent.
Um,

736
00:46:40.500 --> 00:46:41.333
but

737
00:46:44.070 --> 00:46:44.903
<v 2>uh,</v>

738
00:46:45.020 --> 00:46:49.220
<v 0>the TPU uses a specific technique called all reduce,
um,</v>

739
00:46:49.250 --> 00:46:52.290
that I need to actually learn more about.
Um,

740
00:46:52.291 --> 00:46:55.450
and I'm not going to pretend to know all the details about all reduced cause

741
00:46:55.451 --> 00:46:58.230
it's um,
it's a very specific technique.
Um,

742
00:46:58.280 --> 00:47:00.940
and we can't just fit literally everything into this video.
And um,

743
00:47:01.010 --> 00:47:02.330
I'll talk about tps more later.

744
00:47:02.690 --> 00:47:07.250
But what we know is that the cross Shard optimizer is made to take an existing

745
00:47:07.251 --> 00:47:09.530
optimizer and optimize it for,

746
00:47:09.590 --> 00:47:12.470
for the all reduced algorithm to aggregate the gradients.

747
00:47:12.471 --> 00:47:16.070
The gradients are the direction.
This,
these skip these,

748
00:47:16.100 --> 00:47:21.020
these values that are vector values that are used to update our weights in

749
00:47:21.021 --> 00:47:22.970
whatever direction we're optimizing for.

750
00:47:23.210 --> 00:47:27.140
And it broadcast the results of each of these gradients to each shard and each

751
00:47:27.141 --> 00:47:30.000
TPU core.
So what

752
00:47:30.000 --> 00:47:34.530
this looks like for us as developers is we will build our optimizer just like

753
00:47:34.531 --> 00:47:35.364
normally,

754
00:47:35.490 --> 00:47:40.080
and then we will wrap it using this TPU dot cross Shard optimizer.

755
00:47:40.110 --> 00:47:44.450
That's the difference.
And then we just do the rest.
And so that's really the,

756
00:47:44.451 --> 00:47:47.790
the main differences here when we're building TPU specific models.

757
00:47:47.970 --> 00:47:50.430
Now of course there are a few other little changes.

758
00:47:50.550 --> 00:47:54.360
So evaluation metrics and tenser board,

759
00:47:54.540 --> 00:47:57.660
you have to use specific TPU code for each of these things.

760
00:47:57.870 --> 00:48:00.390
And the tensorflow documentation is great for that,

761
00:48:00.840 --> 00:48:05.700
but those are the three big ones.
Now,
one more benchmark here.
Okay.

762
00:48:06.120 --> 00:48:10.350
So,
um,
image classification with CFR.
Okay,

763
00:48:10.351 --> 00:48:13.270
so see far as an image classification data set and the,

764
00:48:13.271 --> 00:48:17.820
the problem setting here is given an image,
classify what it is,
a dog,
a cat,

765
00:48:17.850 --> 00:48:21.720
a car,
whatever it is.
It's very standard,
a task and machine learning.

766
00:48:22.410 --> 00:48:27.410
Now the standard neural network architecture to solve this is a convolutional

767
00:48:27.751 --> 00:48:32.640
network.
But for the sake of this demo,
we're just going to use a linear,

768
00:48:32.700 --> 00:48:35.610
a multilayer perceptron.
Okay.

769
00:48:36.240 --> 00:48:37.073
<v 2>Okay.</v>

770
00:48:37.140 --> 00:48:41.040
<v 0>And we'll,
we'll compile it on both the GPU and the Tpu to see the difference in</v>

771
00:48:41.041 --> 00:48:44.970
performance and in the code and,
and,
and how that changes.

772
00:48:45.620 --> 00:48:46.240
<v 2>Okay.</v>

773
00:48:46.240 --> 00:48:47.410
<v 0>So if we look here,</v>

774
00:48:47.411 --> 00:48:50.460
we're sent were we can see the dependencies and so I already have it written out

775
00:48:50.461 --> 00:48:53.830
and I'm going to modify it the live so and hopefully it works.

776
00:48:53.920 --> 00:48:56.940
So what we see here is us importing tentraflow,

777
00:48:57.540 --> 00:49:02.540
we're importing tensor flow and Cara Ross and so we have our basic um,

778
00:49:03.940 --> 00:49:08.320
multilayer perceptron MLP model that we built right here.
Okay?

779
00:49:08.850 --> 00:49:09.140
<v 2>Okay.</v>

780
00:49:09.140 --> 00:49:13.130
<v 0>Input Times weight,
add a bias,
activate input times weight out of buys,
activate.</v>

781
00:49:13.131 --> 00:49:16.190
That just keeps repeating.
Okay?
And we have some special tricks here.

782
00:49:16.410 --> 00:49:17.390
I'm fully connected.

783
00:49:17.391 --> 00:49:21.800
Layer a batch normalization and activation function and drop out.
Okay?

784
00:49:22.040 --> 00:49:25.760
So what we do is once we built our model,
we'll say,
okay,

785
00:49:26.660 --> 00:49:27.493
<v 2>okay,</v>

786
00:49:27.540 --> 00:49:31.710
<v 0>let's get our CFR code using this low data function and let's build our model.</v>

787
00:49:31.711 --> 00:49:35.520
Let's use those two functions we just created and then compile it using Adam,

788
00:49:35.550 --> 00:49:36.930
which is a great descent technique.

789
00:49:37.320 --> 00:49:39.750
And then we'll fit that model on the training and testing data.

790
00:49:39.960 --> 00:49:42.120
Now when we do this on the Gpu,

791
00:49:42.121 --> 00:49:46.830
let's run this code and then we'll modify it for the TPU.
So we'll save that.

792
00:49:48.610 --> 00:49:52.930
Hopefully this works.
You never know.
You never know.
You never know.

793
00:49:55.020 --> 00:49:59.720
Never know.
Okay.
So let's see what happens here.

794
00:49:59.721 --> 00:50:00.650
When we train this thing,

795
00:50:01.010 --> 00:50:04.160
it's going to download this data and let me answer some questions while it's

796
00:50:04.161 --> 00:50:07.820
downloading the c four data.
When is the wrap the rapids?

797
00:50:08.090 --> 00:50:11.870
The wrap is at the very end.
The wrap is a reward for those people.
It's a,

798
00:50:11.871 --> 00:50:16.871
it's a longterm reward for those agents who are optimizing for that reward to

799
00:50:17.631 --> 00:50:22.360
get into reinforcement learning,
uh,
at the very end,
we're almost there.
Okay.

800
00:50:25.210 --> 00:50:25.640
<v 2>Okay.</v>

801
00:50:25.640 --> 00:50:28.130
<v 0>Well,
the free tier of Google colab faster than running</v>

802
00:50:28.130 --> 00:50:33.050
<v 3>it locally.
Um,
that's a good question.
So,
uh,</v>

803
00:50:33.620 --> 00:50:36.890
Google cloud offers a tps as well,

804
00:50:36.920 --> 00:50:41.360
so we can use the Google cloud console to run our models in the cloud.
We don't,

805
00:50:41.390 --> 00:50:44.180
it doesn't have to be with Colab just for teaching purposes.

806
00:50:44.270 --> 00:50:49.070
We're using colab right now,
but um,
it doesn't have to be end.
Yes.
Okay.

807
00:50:49.071 --> 00:50:52.100
So happy birthday.
Harsh.
Okay.

808
00:50:52.430 --> 00:50:55.280
That's it for the questions.
While this is downloading.

809
00:50:55.281 --> 00:50:58.310
So this is actually going to take a while.
So while this takes a while,

810
00:50:58.311 --> 00:51:00.560
I'm actually going to turn this into TPU coat.

811
00:51:01.670 --> 00:51:02.180
<v 2>Okay.</v>

812
00:51:02.180 --> 00:51:04.010
<v 3>Okay.
Let me turn this into Tpu code now.</v>

813
00:51:05.380 --> 00:51:05.780
<v 2>Yeah.</v>

814
00:51:05.780 --> 00:51:08.690
<v 3>So to convert this into TPU code,
what do we do here?</v>

815
00:51:08.960 --> 00:51:11.940
So we're still going to have our model just like before.
Uh,

816
00:51:11.990 --> 00:51:14.870
the difference now is we're going to say let's,

817
00:51:14.930 --> 00:51:18.680
let's do that for first is we're going to apply that cross Shar optimizer that

818
00:51:18.681 --> 00:51:22.070
we talked about before.
So we'll say optimizer equals

819
00:51:23.310 --> 00:51:24.000
<v 2>okay.</v>

820
00:51:24.000 --> 00:51:28.340
<v 3>T F.
Dot.
Train Dot Adam optimizer just like we had,
you know,</v>

821
00:51:28.370 --> 00:51:32.150
right here.
And the learning rate is like,
like it's right here.
One.

822
00:51:34.900 --> 00:51:39.190
Okay.
Now what's the difference?
We're going to create our TPU,

823
00:51:39.330 --> 00:51:43.120
optimize her.
Now how do we do this?
Remember,
like I said before,

824
00:51:43.480 --> 00:51:48.480
we have to use that cross Shard optimizer and wrap our existing optimizer around

825
00:51:52.091 --> 00:51:56.380
that.
And that is what we use to train our model,

826
00:51:56.800 --> 00:51:59.470
the TPU optimizer.

827
00:52:01.390 --> 00:52:04.930
Okay.
Now there's that.
There's that bit.

828
00:52:05.260 --> 00:52:09.370
But before we train our model,
we have one more thing to do.
Um,

829
00:52:09.730 --> 00:52:13.360
for Colab,
which is to say

830
00:52:15.350 --> 00:52:20.350
that we're going to connect in Colab to the TPU that we have.

831
00:52:21.520 --> 00:52:22.353
<v 2>MMM.</v>

832
00:52:24.300 --> 00:52:26.520
<v 3>Inside of this virtual environment</v>

833
00:52:33.490 --> 00:52:36.970
<v 2>and
weed,</v>

834
00:52:37.790 --> 00:52:39.680
<v 3>we want to use a cluster.
Okay.</v>

835
00:52:39.681 --> 00:52:43.400
We want to use a cluster of TPU is not just a single TPU.

836
00:52:43.550 --> 00:52:48.230
So we'll use the cluster resolver.
So what the co cluster resolver is,

837
00:52:48.710 --> 00:52:52.400
is it's letting us access apod,
right?

838
00:52:52.401 --> 00:52:54.680
So TPU pods are clusters.

839
00:52:56.580 --> 00:52:57.260
<v 2>Okay.</v>

840
00:52:57.260 --> 00:53:00.320
<v 3>TPU GRPC Url,
the one we just defined.</v>

841
00:53:01.280 --> 00:53:04.610
And then we'll say,
well,
what is our strategy?

842
00:53:04.611 --> 00:53:09.320
So it turns out that Care Ross has a specific type of distribution strategy,

843
00:53:09.600 --> 00:53:12.110
uh,
for that pod call.

844
00:53:12.140 --> 00:53:17.140
And we can call that using TPU distribution strategy and we'll use the cluster

845
00:53:18.711 --> 00:53:21.290
resolver as the parameter to that.

846
00:53:23.210 --> 00:53:23.830
<v 2>Okay.</v>

847
00:53:23.830 --> 00:53:27.840
<v 3>And now that we've done,
we can say,
let's take our,</v>

848
00:53:29.220 --> 00:53:30.053
<v 1>uh,</v>

849
00:53:30.940 --> 00:53:35.940
<v 3>care Ross model and convert that to a TPU model using the chaos to TPO TPU app,</v>

850
00:53:37.570 --> 00:53:40.550
aptly named function.
And uh,

851
00:53:40.690 --> 00:53:44.110
and we'll use the strategy of the one we defined

852
00:53:45.380 --> 00:53:49.130
<v 1>strategy.
And that's it.</v>

853
00:53:50.150 --> 00:53:50.983
Okay.

854
00:53:51.790 --> 00:53:53.650
<v 3>Um,
so this is actually</v>

855
00:53:55.460 --> 00:53:57.980
going to take a while to download.

856
00:53:58.580 --> 00:54:03.110
So what we'll do actually is pause this.

857
00:54:05.040 --> 00:54:06.060
Okay.
Cause it's going to take,

858
00:54:06.061 --> 00:54:10.620
this is live and we just want to see if this compiles and it's not going to

859
00:54:10.621 --> 00:54:12.900
compile because it's the runtime is Gpu.

860
00:54:13.200 --> 00:54:18.200
But if we set the runtime to Tpu and save that by to connect,

861
00:54:20.000 --> 00:54:23.180
we just want to see if this compiles this TPU code.
Um,

862
00:54:29.250 --> 00:54:31.410
can we fix this and then I'll answer some questions.
Okay,

863
00:54:31.920 --> 00:54:36.630
so invalid syntax where strategy equals strategy.
Oh of course.

864
00:54:40.560 --> 00:54:41.520
<v 1>Winning.
Okay,</v>

865
00:54:42.940 --> 00:54:43.630
<v 3>good.
So that,</v>

866
00:54:43.630 --> 00:54:48.630
so there this TPU that that was us converting it into Tpu Code that this is in

867
00:54:48.911 --> 00:54:50.530
care Os.
Okay.

868
00:54:50.531 --> 00:54:53.680
So we're not going to sit here and wait for it to download the data and train

869
00:54:53.681 --> 00:54:55.960
and you know,
et Cetera.
But that's how that goes.

870
00:54:56.530 --> 00:55:01.530
Now Google has this default train Shakespeare in five minutes example

871
00:55:04.820 --> 00:55:05.930
that we'll find right here.

872
00:55:06.320 --> 00:55:11.320
And what this does is it uses care os to train a recurrent network to generate

873
00:55:12.411 --> 00:55:14.660
speech in the style of Shakespeare.

874
00:55:14.661 --> 00:55:19.430
So the input data or Shakespearian text like these poems loves had led me.

875
00:55:19.431 --> 00:55:22.310
No doms lacquer doesn't even make sense.

876
00:55:22.311 --> 00:55:24.260
But let's just go through this very quickly.

877
00:55:24.530 --> 00:55:27.140
We're downloading the data from Project Gutenberg.

878
00:55:27.470 --> 00:55:31.460
We build that data in Coupeville,
Colab,
we have a specific address for the TPU.

879
00:55:32.660 --> 00:55:36.920
Okay.
We're doing some data transformation.
None of it is TPU specific.

880
00:55:36.921 --> 00:55:40.610
It doesn't seem,
none of this is TPU specific.
We build our model.

881
00:55:40.611 --> 00:55:42.860
This is a long short term memory network.

882
00:55:42.861 --> 00:55:47.861
A type of recurrent network made specifically for longterm sequences like giant,

883
00:55:48.860 --> 00:55:53.750
uh,
Corpus core Pi corpuses of taxed.
And I know,

884
00:55:53.960 --> 00:55:56.870
you know,
lst just search Lstm Saroj for a video on that.

885
00:56:00.540 --> 00:56:04.470
And uh,
yeah,
nothing TPU specific here yet.

886
00:56:04.920 --> 00:56:08.580
When we train the model,
here we go like we did before TPU.

887
00:56:09.170 --> 00:56:14.160
Dot Care Ross to TPU model.
So it's resolving to the Mac,
uh,

888
00:56:14.290 --> 00:56:18.600
multiply,
accumulate architecture of the systolic array architecture for the TPU.

889
00:56:19.770 --> 00:56:22.800
We're fitting it,
they train it,
they make predictions with,

890
00:56:24.160 --> 00:56:24.810
<v 2>okay.</v>

891
00:56:24.810 --> 00:56:28.710
<v 0>None of it as TPU specific.
And here,
here's the hearing the results.
Okay.</v>

892
00:56:28.711 --> 00:56:32.160
So it's actually very simple to use a TPU now and this is a first,

893
00:56:32.250 --> 00:56:35.730
so thank you Google for releasing this.
Let me answer some questions and then,
uh,

894
00:56:35.731 --> 00:56:38.460
that's it for this live stream and we'll wrap as well.

895
00:56:40.820 --> 00:56:41.160
<v 2>Okay.</v>

896
00:56:41.160 --> 00:56:44.490
<v 0>This is a part of move 37?
Yes.
Um,</v>

897
00:56:46.020 --> 00:56:48.540
can we get a link to your colab file?
It's in the video description.

898
00:56:48.780 --> 00:56:49.890
It's an eye python notebook.

899
00:56:49.891 --> 00:56:53.640
You can then upload it to Google colab because it's a backwards compatible.

900
00:56:54.180 --> 00:56:55.500
And last question,

901
00:56:55.501 --> 00:56:59.550
we'll Google make more money through AI than advertising in the future.

902
00:57:01.760 --> 00:57:03.320
Advertising is done by Ai.

903
00:57:03.530 --> 00:57:07.340
Let me just end this with a very important thing that needs to be said.

904
00:57:12.000 --> 00:57:14.730
Ai is everywhere.
Ai is manipulating all of us.

905
00:57:15.030 --> 00:57:19.990
Ai is normally with advertising,
with billboards.
It's a,
it's a,

906
00:57:19.991 --> 00:57:24.270
it's a one way.
Speaking of that with billboards,
it's a one way.
Um,

907
00:57:26.610 --> 00:57:26.920
<v 2>yeah,</v>

908
00:57:26.920 --> 00:57:29.710
<v 0>with billboards and with ads that we see in real life,</v>

909
00:57:30.070 --> 00:57:34.120
it's a one way relationship that we see the billboard and we decide if we want

910
00:57:34.121 --> 00:57:37.300
to buy it or not.
But with the Internet,
with Ai,

911
00:57:37.630 --> 00:57:39.490
when we see an advertisement online,

912
00:57:39.670 --> 00:57:44.110
it's going to measure what we do in re in relation to that advertisement and

913
00:57:44.111 --> 00:57:49.111
then subtly show us something different to manipulate our behavior such that we

914
00:57:50.411 --> 00:57:52.900
will be more likely to click on that ad the next time.

915
00:57:53.200 --> 00:57:58.030
So we're all subtly being manipulated by these AI algorithms because that's how

916
00:57:58.031 --> 00:57:59.320
the,
um,

917
00:58:00.540 --> 00:58:05.340
that's how the business model works for these social media giants.

918
00:58:06.480 --> 00:58:07.860
And that's just how it works right now.

919
00:58:08.100 --> 00:58:11.220
But we've got to get to a world where we are,

920
00:58:11.550 --> 00:58:15.810
our AI algorithms are not optimizing for our attention for the sake of this

921
00:58:15.811 --> 00:58:17.100
third party advertiser,

922
00:58:17.400 --> 00:58:21.240
but instead for our time well spent AI algorithms that benefit us.

923
00:58:21.870 --> 00:58:22.531
And that's a,

924
00:58:22.531 --> 00:58:26.070
that's a new world that we're heading towards and it's a beautiful world and we

925
00:58:26.071 --> 00:58:29.820
all at school of AI envisioned that world and we will get to that world.

926
00:58:29.821 --> 00:58:32.440
I promise you I'm,
but one step at a time.
And,

927
00:58:32.441 --> 00:58:35.580
and thank you for being here and thank you for being a part of this community.

928
00:58:36.090 --> 00:58:40.290
Um,
so there's a lot of exciting things that are coming out of Ai.
You know,

929
00:58:40.560 --> 00:58:44.610
like any power,
like,
like fire,
fire can burn us or it can use,

930
00:58:44.640 --> 00:58:48.300
it can be used to give us warmth and to sustain us to,

931
00:58:48.301 --> 00:58:51.530
in the same way AI can be used to manipulate us and,

932
00:58:51.770 --> 00:58:56.700
and distract us and control us and,
and,
and polarize us.

933
00:58:57.150 --> 00:59:01.320
But it can also be used to bring us together to solve some of the hardest

934
00:59:01.321 --> 00:59:04.830
problems in the world and make the world a better place.

935
00:59:04.890 --> 00:59:09.180
And we at school of AI or determined to make that the latter reality,

936
00:59:09.870 --> 00:59:14.400
our reality.
So thank you for being here.
Uh,
it's,
it's been,
it's been a pleasure.

937
00:59:14.550 --> 00:59:17.400
And let me just end it with a wrap because I sent him because I promised her
rap.

938
00:59:17.700 --> 00:59:20.550
Um,
so going from a little serious talk to a yeah,

939
00:59:22.040 --> 00:59:24.350
more fun talk.
Um,

940
00:59:26.510 --> 00:59:28.510
looks like my youth,
my youtube APP is a,

941
00:59:31.690 --> 00:59:35.080
I'll just go,
I'll just go acapella.
I like.
Okay.
Um,

942
00:59:37.340 --> 00:59:38.173
I actually,
I want to,

943
00:59:38.230 --> 00:59:41.930
I want to close this out at 59 minutes and I just want to leave you with that

944
00:59:41.931 --> 00:59:44.180
thought.
Actually,
let's,
let's not be silly at the end this time.

945
00:59:44.181 --> 00:59:46.070
Let's be a little serious and say,
you know,

946
00:59:46.100 --> 00:59:50.560
this is something that needs to happen.
Um,

947
00:59:50.720 --> 00:59:54.050
that's it.
That's it for this live stream.
Thank you guys for showing up for now.

948
00:59:54.080 --> 00:59:57.380
I've got to go do a bunch of a school of AI work.

949
00:59:57.440 --> 01:00:01.100
I have some great content coming out for you this weekend on how I'm so

950
01:00:01.101 --> 01:00:05.060
productive.
So you better watch out for that.
It's coming out on Sunday or Monday,

951
01:00:05.210 --> 01:00:07.670
and another video as well that I'm going to keep close to the chest.

952
01:00:07.850 --> 01:00:09.530
So thank you guys.
I love you.

