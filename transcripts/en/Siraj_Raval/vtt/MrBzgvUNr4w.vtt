WEBVTT

1
00:00:00.120 --> 00:00:01.650
Oh world,
it's Saroj.

2
00:00:01.800 --> 00:00:05.460
In this video we're going to create a neural network that makes images become

3
00:00:05.461 --> 00:00:08.160
really trippy using python and tensorflow.

4
00:00:08.400 --> 00:00:13.020
We humans have been using psychedelic drugs since prehistoric times that wasn't

5
00:00:13.021 --> 00:00:14.310
a normal mushrooms.

6
00:00:14.760 --> 00:00:18.930
They help manifest parts of our minds to conscious experience that wouldn't

7
00:00:18.931 --> 00:00:19.980
normally otherwise.

8
00:00:20.220 --> 00:00:24.510
One of the most common experiences from psychedelics are trippy visuals.

9
00:00:24.810 --> 00:00:28.710
People who've taken psychedelics repeatedly described experiences of seeing both

10
00:00:28.730 --> 00:00:31.620
open eye and closed I visuals,
but how,

11
00:00:31.830 --> 00:00:35.160
what are these drugs doing in our brain that makes us see things that aren't

12
00:00:35.161 --> 00:00:35.994
really there?

13
00:00:36.150 --> 00:00:39.720
The traditional way for us to find out is to test live human subjects,

14
00:00:39.840 --> 00:00:42.450
tripping balls under an FMRI machine.

15
00:00:42.720 --> 00:00:46.290
Or recently we've got an artificial neural networks to do the same thing.

16
00:00:46.470 --> 00:00:49.350
Google train a neural network on a labeled data set of images,

17
00:00:49.380 --> 00:00:53.280
everything from squirrels to temples and as a train on these images,

18
00:00:53.370 --> 00:00:56.430
it built internal representations and each layer,

19
00:00:56.670 --> 00:01:00.900
eventually the first few layers learned low level features like lines and edges,

20
00:01:01.050 --> 00:01:02.910
and they got progressively more abstract.

21
00:01:03.000 --> 00:01:07.020
So the last few layers when representations or faces and big shapes.

22
00:01:07.050 --> 00:01:10.110
So when we visualize one of the higher level representations at Bell,

23
00:01:10.170 --> 00:01:13.860
we can see that it contains a mixture of features like the eyes of a dog and the

24
00:01:13.861 --> 00:01:14.694
head of a bird,

25
00:01:16.230 --> 00:01:19.200
and when they gave it a novel image and ask not to classify it,

26
00:01:19.320 --> 00:01:23.160
but to maximize a similarity between the image and a representation at a

27
00:01:23.161 --> 00:01:26.400
particular layer.
The result was a very trippy image.

28
00:01:26.490 --> 00:01:30.090
It all sounds very similar to the drug experience,
which is insane.

29
00:01:30.150 --> 00:01:34.380
Our brains are carbon based and they use chemical signals as messengers on

30
00:01:34.381 --> 00:01:37.440
neural network doesn't even exist and physical space at all.

31
00:01:37.560 --> 00:01:42.060
It's an abstract concept being represented on finery silicon transistors.

32
00:01:42.150 --> 00:01:45.480
There's no reason to expect that these two systems would develop the same

33
00:01:45.481 --> 00:01:50.190
mechanism for processing visual information,
even with existing similarities.

34
00:01:50.310 --> 00:01:54.840
Natural selection is very different from using gradient descent to alter weights

35
00:01:54.990 --> 00:01:56.400
of connections between nodes.

36
00:01:56.430 --> 00:02:00.330
Could it be that somehow encoded into the fundamental rules of the universe?

37
00:02:00.331 --> 00:02:03.480
There's this ideal way of doing object recognition.

38
00:02:05.580 --> 00:02:08.280
He's beginning to believe you're damn right.
Morpheus.

39
00:02:08.340 --> 00:02:11.520
We're getting closer to understanding it every day and we can learn a lot about

40
00:02:11.521 --> 00:02:13.440
the brain,
including human development,

41
00:02:13.560 --> 00:02:17.730
treatment for cognitive disabilities and drug effects from studying artificial

42
00:02:17.731 --> 00:02:20.190
neural networks.
After we install our dependencies,

43
00:02:20.370 --> 00:02:24.090
we're going to replicate Google's deep dream code in tensorflow and then test it

44
00:02:24.091 --> 00:02:28.200
out on a novel image num Pi will be used to perform math calculations.

45
00:02:28.260 --> 00:02:30.240
The partial sub module of funk tools.

46
00:02:30.300 --> 00:02:34.110
Let's US create new versions of functions with one or more arguments filled in.

47
00:02:34.380 --> 00:02:35.760
This is good for reusability,

48
00:02:35.790 --> 00:02:39.750
which means less code to write pillow is an imaging library and image will help

49
00:02:39.751 --> 00:02:43.290
us modify our images.
Tensorflow is our machine learning library.

50
00:02:43.500 --> 00:02:45.780
You are alive will let us download data from the web.

51
00:02:45.990 --> 00:02:50.160
Oh s will let us use operating system dependent functionality and zip tools.

52
00:02:50.161 --> 00:02:54.150
Let us run three no,
it'll just let us unzipped files.

53
00:02:54.210 --> 00:02:56.520
Our imports are already at the top of our script,

54
00:02:56.550 --> 00:02:59.380
so we're going to download Google's pretrained neural network.

55
00:02:59.560 --> 00:03:01.330
Create a tensor flow session,

56
00:03:01.430 --> 00:03:04.630
pick a layer in the pretrained network to enhance our input image,

57
00:03:04.840 --> 00:03:07.750
apply our grading ascent to that,
layer it repeatedly,

58
00:03:07.930 --> 00:03:10.270
and then output our newly deep dream image.

59
00:03:10.450 --> 00:03:14.380
Let's start by downloading Google's pretrained neural network called inception.

60
00:03:14.470 --> 00:03:17.920
In our main method,
we'll store link to it in the URL variable.

61
00:03:18.040 --> 00:03:22.030
Create a data directory where we will extract it to then use the ois module to

62
00:03:22.031 --> 00:03:24.760
retrieve the model name and create a local zip file path.

63
00:03:24.940 --> 00:03:26.560
If there is nothing at that path,

64
00:03:26.650 --> 00:03:30.370
we can download it using the URL life module with the URL variable as a

65
00:03:30.371 --> 00:03:32.920
perimeter and store it in the model URL variable.

66
00:03:33.070 --> 00:03:36.790
We'll open our zip file with the WB flag so we can write to it in binary.

67
00:03:36.790 --> 00:03:38.410
Then write the downloaded data to it.

68
00:03:38.590 --> 00:03:40.720
Then we'll extract that using the zip file module.

69
00:03:40.750 --> 00:03:42.910
Now we can create our tensorflow session.

70
00:03:43.060 --> 00:03:46.810
We'll load our intersection graph file into our model fn variable than

71
00:03:46.811 --> 00:03:49.360
initialize a graph.
Using the graph function of tensorflow.

72
00:03:49.660 --> 00:03:52.570
Now we can initialize a session.
Using that graph.

73
00:03:52.750 --> 00:03:56.740
We'll open our existing saved in section graph using the fast g file function

74
00:03:56.890 --> 00:03:59.620
and pointed to the sage grass.
Once we opened it,

75
00:03:59.621 --> 00:04:01.720
we can read that graft and parse it accordingly.

76
00:04:01.810 --> 00:04:05.770
Using the parts from string method of TensorFlow's graph definition module,

77
00:04:05.980 --> 00:04:07.360
we need to define our input.

78
00:04:07.420 --> 00:04:11.440
So we'll create an input tensor using the placeholder method called input with

79
00:04:11.441 --> 00:04:12.790
the size of 32 bits.

80
00:04:12.970 --> 00:04:17.380
Then we'll define the image net mean value of pixels in an image as one 17 by

81
00:04:17.381 --> 00:04:20.410
removing this value from our image,
it will help us with feature learning.

82
00:04:20.560 --> 00:04:24.190
So we will subtract it from our input tenser and store the value in our

83
00:04:24.191 --> 00:04:25.480
preprocessed variable.

84
00:04:25.630 --> 00:04:29.050
Then we'll load the graph deaf variable we initialize with the input as newly

85
00:04:29.051 --> 00:04:30.010
processed tensor.

86
00:04:30.280 --> 00:04:33.760
So now we've got our tensorflow model we've downloaded from the web and we've

87
00:04:33.761 --> 00:04:36.670
loaded it into our session as a graph with a bunch of layers.

88
00:04:36.820 --> 00:04:38.530
It's a convolutional neural network,

89
00:04:38.531 --> 00:04:41.080
the type of neural net that helps recognize images.

90
00:04:41.140 --> 00:04:44.920
Let's load all those layers into an array and store them in our layers object.

91
00:04:44.950 --> 00:04:49.210
So for every tensorflow operation in our graft,
if it's a convolutional layer,

92
00:04:49.330 --> 00:04:53.710
load it into our array.
So we've got layers.
Lay Lay years.
Yeah,

93
00:04:53.711 --> 00:04:57.460
we're balling hard right now.
I know.
Okay,
so each of our convolutional layers,

94
00:04:57.610 --> 00:05:01.360
I'll put tens of hundreds of feature channels to pass data in the graph and we

95
00:05:01.361 --> 00:05:04.450
can collect them all and store them in the feature numbs variable.

96
00:05:04.660 --> 00:05:07.750
Let's print them out and visualize what we've got in terminal first.

97
00:05:07.930 --> 00:05:11.260
We can see our number of layers and the total number of feature channels right

98
00:05:11.261 --> 00:05:14.950
here.
Let's now pick a layer from our model that we're going to enhance.
Well,

99
00:05:14.951 --> 00:05:18.670
pick a lower level layer and pick one of the featured channels to visualize it's

100
00:05:18.671 --> 00:05:22.690
time to load our input image using the pillow image submodules open method and

101
00:05:22.691 --> 00:05:25.840
store it in our image variable.
We'll format it accordingly.

102
00:05:25.870 --> 00:05:27.910
Using num py and perform deep dream on it.

103
00:05:27.940 --> 00:05:31.420
With our render deep dream function with a focus on the layer we selected

104
00:05:31.450 --> 00:05:33.430
earlier in our deep dream function,

105
00:05:33.520 --> 00:05:35.980
we can see a couple of our predefined hyper parameters.

106
00:05:35.981 --> 00:05:38.620
We'll start by defining our optimization objective,

107
00:05:38.680 --> 00:05:40.930
which is to reduce the mean of our input layer.

108
00:05:40.960 --> 00:05:44.720
The gradients function lets us compute the symbolic radiant of our optimize

109
00:05:44.740 --> 00:05:47.020
tensor with respect to our input tensor.

110
00:05:47.050 --> 00:05:51.220
Now we can split our image into a number of octaves and say for each octave,

111
00:05:51.221 --> 00:05:55.360
let's resize it using num py and add it to our array of image octaves and we can

112
00:05:55.361 --> 00:05:59.630
generate details Optiv by OPTIV by iterating through each random shifts or

113
00:05:59.631 --> 00:06:04.160
applied to the image to blur tile boundaries over multiple iterations using the

114
00:06:04.161 --> 00:06:05.630
CALC grade tiled function.

115
00:06:05.900 --> 00:06:09.590
We're essentially applying gradient a sent here to maximize our loss function,

116
00:06:09.740 --> 00:06:13.610
which merges are saved representation in this layer with our input image.

117
00:06:13.700 --> 00:06:16.250
More and more every iteration.
So to break it down,

118
00:06:16.280 --> 00:06:20.270
neural networks are a great test bed for learning about how the brain functions

119
00:06:20.420 --> 00:06:22.250
and responds to certain stimuli.

120
00:06:22.430 --> 00:06:26.510
They store increasingly abstract representations of what they learn in their

121
00:06:26.511 --> 00:06:31.490
layers and we can create trippy images by applying a gradient ascent process to

122
00:06:31.491 --> 00:06:35.840
them based on any chosen layer of a pretrained convolutional neural network.

123
00:06:36.020 --> 00:06:39.590
The winner of the coding challenge from the last video is [inaudible] Tucker

124
00:06:39.590 --> 00:06:43.940
Party.
He created a pretty cool I python notebook to demo his coach and train his

125
00:06:43.941 --> 00:06:46.760
neural net on both price and sentiment data using care.

126
00:06:46.761 --> 00:06:50.060
Awesome bad ass of the week.
And the runner up is Victor Sirano.

127
00:06:50.090 --> 00:06:53.690
Very well documented codes that asks for user input on stock prediction.

128
00:06:53.900 --> 00:06:57.710
The coding challenge for this video is to modify this code so it works not just

129
00:06:57.711 --> 00:07:00.750
on images but video.
He tells her in the read me poster,

130
00:07:00.770 --> 00:07:03.620
get humbling in the comments and I'll announce the winner in the next video.

131
00:07:03.680 --> 00:07:04.250
Also,

132
00:07:04.250 --> 00:07:08.000
I created a slack channel for all of us programming wizards to learn from each

133
00:07:08.001 --> 00:07:11.510
other,
linked to sign up below.
For now,
I've got to say sober,

134
00:07:11.660 --> 00:07:13.040
so thanks for watching.

