WEBVTT

1
00:00:00.160 --> 00:00:04.150
In the sixth century BC,
Pythagoras,
Pythagoras,

2
00:00:04.330 --> 00:00:08.890
propose a concept called the music of the spheres to describe the proportional

3
00:00:08.891 --> 00:00:12.820
movements of celestial bodies like the sun,
moon,
and planets.

4
00:00:13.210 --> 00:00:16.240
This music is not thought of as being literally audible,

5
00:00:16.241 --> 00:00:18.670
but instead a mathematical concept.

6
00:00:18.880 --> 00:00:21.460
Math and music are intrinsically connected.

7
00:00:21.550 --> 00:00:25.990
The field of algorithmic composition dates back to the early days of computer

8
00:00:25.991 --> 00:00:30.940
science.
Translation models take an existing nonmusical medium like a picture and

9
00:00:31.130 --> 00:00:34.180
translated into a sound.
These are usually rule based,

10
00:00:34.181 --> 00:00:38.590
so a rule may be that if it sees a horizontal line and an image,

11
00:00:38.660 --> 00:00:40.960
it will interpret that as a constant pitch.

12
00:00:40.961 --> 00:00:44.260
Whereas a vertical line represents an ascending scale.

13
00:00:44.350 --> 00:00:48.850
Evolutionary models are based off of genetic algorithms through mutation and

14
00:00:48.851 --> 00:00:53.080
natural selection.
Different solutions evolve towards a suitable composition.

15
00:00:53.260 --> 00:00:58.000
Then there's the learning models.
By providing it musical data instead of rules,

16
00:00:58.180 --> 00:01:00.850
we can let it learn for itself how to create music.

17
00:01:01.090 --> 00:01:05.080
We're fast approaching the point where we'll no longer have to wonder how Mozart

18
00:01:05.081 --> 00:01:07.960
or Jimi Hendrix would have composed a piece on a certain topic.

19
00:01:08.290 --> 00:01:12.210
We'll just be able to ask there,
algorithmic counterparts ourselves.
Hello world.

20
00:01:12.211 --> 00:01:16.620
It's Saroj and today we're going to use deep learning to generate some jazz

21
00:01:16.621 --> 00:01:17.454
music.

22
00:01:17.460 --> 00:01:22.460
The first attempt by anyone to use a computer to compose music was by two

23
00:01:22.501 --> 00:01:26.670
American professors at the University of Illinois,
Urbana,
champagne,

24
00:01:26.910 --> 00:01:28.350
hiller and Isaacson.

25
00:01:28.410 --> 00:01:33.410
They program the university's ILIAC computer to compose music and it generated

26
00:01:33.421 --> 00:01:38.100
pitches using random numbers before testing them according to the rules of

27
00:01:38.101 --> 00:01:41.700
classical counterpoint.
So if a pitch didn't it apiece,

28
00:01:41.940 --> 00:01:43.590
another note was generated.

29
00:01:44.010 --> 00:01:48.030
It also relied on probabilities via a Markov chain.

30
00:01:48.270 --> 00:01:52.170
It use the past to determine the probabilities of the future.

31
00:01:52.350 --> 00:01:56.880
The first piece was completed in 1957 and was called the ILIAC suite for string

32
00:01:56.881 --> 00:01:57.714
quartet.

33
00:02:02.720 --> 00:02:03.030
<v 1>Hmm.</v>

34
00:02:03.030 --> 00:02:05.430
<v 0>Although it made headlines in scientific American,</v>

35
00:02:05.431 --> 00:02:08.160
the musical establishment was pretty hostile to them.

36
00:02:08.400 --> 00:02:12.300
They thought it undermined human creativity and didn't include them in their

37
00:02:12.301 --> 00:02:15.570
journals until after Hitler's death.
Nowadays,

38
00:02:15.571 --> 00:02:20.160
there are a ton of amazing generative programs for composers to aid them when

39
00:02:20.161 --> 00:02:22.500
they compose music like iTunes.

40
00:02:23.670 --> 00:02:27.660
Let understand this process by building a model to generate jazz pieces using

41
00:02:27.661 --> 00:02:30.030
care Os.
Well,
first examine the music.

42
00:02:30.031 --> 00:02:34.200
We're going to train our model on our input data is going to be a collection of

43
00:02:34.201 --> 00:02:36.420
piano pieces by American jazz musicians.

44
00:02:36.510 --> 00:02:39.990
Pat Metheny in mid I format mid thigh or musical instrument.

45
00:02:40.020 --> 00:02:43.110
Digital interface is like the digital alphabet for music.

46
00:02:43.320 --> 00:02:47.610
It contains a list of messages that tell an electronic device like a sound card,

47
00:02:47.790 --> 00:02:52.020
how to generate a certain sound so it doesn't store actual sound itself,

48
00:02:52.021 --> 00:02:55.710
which lens to a much smaller file.
Since these messages are a sequence,

49
00:02:55.770 --> 00:02:58.800
we'll use a recurrent network as our sequence learning model.

50
00:02:59.020 --> 00:03:00.190
For each [inaudible] file.

51
00:03:00.191 --> 00:03:03.430
We'll extract the stream of notes for both the melody and the harmony.

52
00:03:03.760 --> 00:03:06.790
The harmony's cords accompany the melodies,
single notes.

53
00:03:07.300 --> 00:03:09.850
Then we'll group them all together by measure number.

54
00:03:10.030 --> 00:03:14.950
So each measure has its own grouping of chords and this measure cord pair is

55
00:03:14.951 --> 00:03:17.080
what we'll call our abstract grammars.

56
00:03:17.410 --> 00:03:21.790
We'll vectorize these inputs by converting them into binary matrices so we can

57
00:03:21.791 --> 00:03:25.000
feed them into our model.
Now we can build our model.

58
00:03:25.300 --> 00:03:28.240
This is going to be a double stack to LSTM network,

59
00:03:28.360 --> 00:03:30.670
so our computation graph will look like this.

60
00:03:31.000 --> 00:03:35.380
The vectorized sequence of notes will be input into the first LSTM cell.

61
00:03:35.590 --> 00:03:39.640
Then we'll apply dropout to help ensure that the model generalized well and

62
00:03:39.641 --> 00:03:41.260
we'll do that process one more time.

63
00:03:41.620 --> 00:03:45.400
Then we'll feed the data to our last fully connected layer,
labeled dense.

64
00:03:45.550 --> 00:03:49.420
Since every neuron in the previous layer is connecting to every neuron in this

65
00:03:49.421 --> 00:03:52.390
layer,
it will mix all the learned signals together.

66
00:03:52.480 --> 00:03:56.710
So our prediction is truly based on the whole input sequence will lastly

67
00:03:56.711 --> 00:04:00.790
transform the result with a softmax activation function into an output

68
00:04:00.791 --> 00:04:04.960
probability for what is likely to next note in the sequence when we build our

69
00:04:04.961 --> 00:04:06.940
first LSTM layer,
by default,

70
00:04:07.120 --> 00:04:10.630
it will only return the last spectrum rather than the entire sequence.

71
00:04:10.780 --> 00:04:15.250
So we set return sequences to true so that it returns the entire sequence,

72
00:04:15.460 --> 00:04:20.110
which is necessary to be able to stack another LSTM later on using to LSTM

73
00:04:20.111 --> 00:04:24.220
layers instead of one allows for a more complex feature representation of the

74
00:04:24.221 --> 00:04:28.330
input,
which means more generalization ability and thus a better prediction.

75
00:04:28.450 --> 00:04:32.110
Recall that recurrent networks are essentially like a series of feedforward

76
00:04:32.111 --> 00:04:33.970
networks that are connected to each other.

77
00:04:34.000 --> 00:04:37.840
The output of each and it's hidden layer is fed into the next one and when we

78
00:04:37.841 --> 00:04:39.700
back propagate with each layer,

79
00:04:39.970 --> 00:04:42.970
the magnitude of the gradients gets exponentially smaller,

80
00:04:42.971 --> 00:04:45.190
which makes the steps also very small,

81
00:04:45.340 --> 00:04:49.270
which results in a very slow learning rate of the weights in the lower layers of

82
00:04:49.271 --> 00:04:50.104
a deep network.

83
00:04:50.170 --> 00:04:54.040
This is the vanishing gradient problem and Lstm recurrent notes help solve that

84
00:04:54.160 --> 00:04:58.120
by preserving the error that can be backed propagated through time and layers.

85
00:04:58.300 --> 00:05:01.300
Let's look a little closer at this process,
but first I got to say

86
00:05:04.930 --> 00:05:05.763
<v 2>again.</v>

87
00:05:09.530 --> 00:05:12.490
S remembers the good stuff that outputs the breast.

88
00:05:15.650 --> 00:05:18.800
<v 0>An LSTM cell consists of three gates.
The input,</v>

89
00:05:18.860 --> 00:05:21.290
forget and output as well as a cell state.

90
00:05:21.680 --> 00:05:23.510
The cell state is like a conveyor belt.

91
00:05:23.511 --> 00:05:27.680
It just lets memory flow across unchanged except for a few minor linear

92
00:05:27.681 --> 00:05:28.514
interactions.

93
00:05:28.760 --> 00:05:32.750
These interactions are the gates we can add or remove memory from the cell state

94
00:05:32.751 --> 00:05:33.860
regulated by them.

95
00:05:34.040 --> 00:05:38.450
They optionally let memory through each is a sigmoid neural net player and a

96
00:05:38.451 --> 00:05:39.950
multiplication operation.

97
00:05:40.310 --> 00:05:44.210
The sigmoid outputs a value between zero and one which describes how much of

98
00:05:44.270 --> 00:05:45.800
each component should be let through,

99
00:05:45.860 --> 00:05:50.390
will represent each of the gates with the following equations where w is the set

100
00:05:50.391 --> 00:05:51.740
of weights at each gate.

101
00:05:52.010 --> 00:05:56.750
The way it's internal memory changes is similar to piping water through a pipe,

102
00:05:56.960 --> 00:06:00.230
so think of memory as water.
It flows into a pipe.

103
00:06:00.500 --> 00:06:04.550
If we want to change the memory flow,
this change is controlled by two valves,

104
00:06:04.730 --> 00:06:07.010
the forget now first if we shut it,

105
00:06:07.011 --> 00:06:11.480
no old memory will be kept if we keep it open,
all old memory will pass through.

106
00:06:11.720 --> 00:06:13.520
The other is the new memory valve.

107
00:06:13.580 --> 00:06:18.440
New memory comes in through a t shaped joint and merges with the old memory and

108
00:06:18.441 --> 00:06:21.830
the amount of new memory that comes in is controlled by this valve.

109
00:06:22.250 --> 00:06:25.560
The input is an old memory and it passes through the forget now,

110
00:06:25.610 --> 00:06:28.040
which is actually a multiplication operation.

111
00:06:28.220 --> 00:06:30.350
The old memory hits the t shape joint pipe,

112
00:06:30.351 --> 00:06:32.360
which represents a summation operation,

113
00:06:32.660 --> 00:06:35.930
new and old memory merge through this operation.
In total,

114
00:06:35.931 --> 00:06:38.180
this updates the old memory to the new memory.

115
00:06:38.510 --> 00:06:41.570
We'll define our loss function as categorical cross entropy.

116
00:06:41.810 --> 00:06:45.950
The Cross entropy between two probability distributions measure the average

117
00:06:45.951 --> 00:06:49.580
number of bits needed to identify an event from a set of possibilities.

118
00:06:49.670 --> 00:06:51.470
Since our data is fed in sequences,

119
00:06:51.560 --> 00:06:55.700
this measures the difference between the real next note and our predicted next

120
00:06:55.701 --> 00:06:59.270
note.
Well,
minimize this loss function using rms prop,

121
00:06:59.510 --> 00:07:02.390
which is an implementation of stochastic gradient descent.

122
00:07:02.750 --> 00:07:07.190
So we'll predict the next note in the sequence over and over again until we have

123
00:07:07.191 --> 00:07:11.420
a sequence of generated notes will translate this into mid thigh format and

124
00:07:11.600 --> 00:07:15.250
write it to a file so we can listen to it.
Let's hear what this sounds like.
It

125
00:07:25.390 --> 00:07:29.080
at least it's better than Kenny G so we're all good.
So to break it down,

126
00:07:29.081 --> 00:07:34.000
we can generate music using l s t m networks to predict sequences of notes.

127
00:07:34.300 --> 00:07:38.740
LSTM consists of three gates.
The input for get an output,
Kate,

128
00:07:38.980 --> 00:07:43.090
and we can think of these gates as valves controlling how memory is stored in

129
00:07:43.091 --> 00:07:45.400
our network to eliminate banishing gradients.

130
00:07:45.670 --> 00:07:49.070
The winner of the coding challenge from the last video is Michelle Batu.

131
00:07:49.330 --> 00:07:52.030
Not only did he performed multiple style transfer,

132
00:07:52.150 --> 00:07:55.570
but he took it a step further by applying it to video as well.

133
00:07:55.780 --> 00:07:58.780
Wizard of the week,
and the runner up is Michael Palka.

134
00:07:58.930 --> 00:08:02.590
He successfully performed multicell transferred through a clever matrix

135
00:08:02.770 --> 00:08:03.603
operation.

136
00:08:03.700 --> 00:08:07.930
The coding challenge for this video is to generate a music clip for a genre that

137
00:08:07.931 --> 00:08:11.920
you choose.
Remember,
it's just a sequence of mid I messages posts.

138
00:08:11.970 --> 00:08:14.860
You're gambling in the comments and I'll announce the winner.
Next video,

139
00:08:14.920 --> 00:08:17.890
please subscribe for more programming videos.
Check out this related video.

140
00:08:18.130 --> 00:08:21.940
And for now I've got to memorize memory cells,
so thanks for watching.

