WEBVTT

1
00:00:00.100 --> 00:00:01.630
Hey Siri,
any songs you'd recommend?

2
00:00:01.800 --> 00:00:05.020
The blacker the berry by Kendrick Lamar could pick why that one?

3
00:00:05.200 --> 00:00:06.350
Because the police,

4
00:00:06.650 --> 00:00:07.251
<v 1>hello world,</v>

5
00:00:07.251 --> 00:00:10.370
welcome to try geology and this episode we're going to build a movie recommender

6
00:00:10.371 --> 00:00:12.380
system recommender systems are all around us.

7
00:00:12.410 --> 00:00:15.530
Almost every single major service on the Internet uses them to show you things

8
00:00:15.531 --> 00:00:16.840
that you'd like based on your interests.

9
00:00:16.841 --> 00:00:19.010
Sometimes it's so subtle you don't even notice.

10
00:00:19.040 --> 00:00:21.770
They help personalize content on the web,
which makes users happy,

11
00:00:21.800 --> 00:00:23.710
which makes companies money rich.

12
00:00:24.830 --> 00:00:28.100
The two most ubiquitous types of recommender systems are content based and

13
00:00:28.101 --> 00:00:30.860
collaborative content based systems focus on each user individually.

14
00:00:30.920 --> 00:00:32.360
It looks at the items you've already expressed,

15
00:00:32.361 --> 00:00:34.790
an interest in your lights and ratings and records,
their keywords,

16
00:00:34.791 --> 00:00:38.000
attributes and tags.
Then your profile has gradually built with these attributes.

17
00:00:38.001 --> 00:00:39.890
Once your profile is built,
the system will start recommending.

18
00:00:39.920 --> 00:00:42.650
I didn't do like with similar attributes to the ones you've already expressed

19
00:00:42.651 --> 00:00:43.130
interest in.

20
00:00:43.130 --> 00:00:46.070
So if you're on an ecommerce site and you buy a bunch of Nickelback tee shirts,

21
00:00:46.100 --> 00:00:48.650
those items will have tags like worst band ever and stupid.

22
00:00:48.650 --> 00:00:51.650
Based on that content.
The system will suggest similarly tagged items for you,

23
00:00:51.651 --> 00:00:53.570
like Cree tee shirts and Walmart brand guitar.

24
00:00:53.600 --> 00:00:54.830
Then they're collaborative systems.

25
00:00:54.831 --> 00:00:56.810
These are the most ubiquitous types of recommender systems.

26
00:00:56.840 --> 00:00:59.990
A collaborative system recommends you items based on what other similar users

27
00:00:59.991 --> 00:01:01.050
have expressed interest in it.

28
00:01:01.070 --> 00:01:03.800
You really like base jumping and constantly by base jumping gear online.

29
00:01:03.830 --> 00:01:06.200
This is the more fine users who have similar purchase history.

30
00:01:06.201 --> 00:01:08.590
It can recommend other items they bought that you have it.

31
00:01:08.630 --> 00:01:10.580
It's likely that you'll be into those products as well.

32
00:01:10.581 --> 00:01:13.940
So we're going to build an APP that can recommend movies you'd like in 10 lines

33
00:01:13.941 --> 00:01:17.360
of c plus plus using Amazon's and newly released ml library called deep
scalable,

34
00:01:17.361 --> 00:01:20.900
sparse tensor network engine or destiny.
We've got to think of a better name.

35
00:01:20.930 --> 00:01:24.020
Our model will be a neural network because depending on how deep we make it and

36
00:01:24.021 --> 00:01:26.510
how much data we feed it,
it's just going to outperform everything else.

37
00:01:26.510 --> 00:01:26.990
Let's be real.

38
00:01:26.990 --> 00:01:29.930
Then we're going to train it in the cloud using AWS because I ain't got time to

39
00:01:29.931 --> 00:01:30.830
train this on my Mac book.

40
00:01:30.831 --> 00:01:33.920
That's an e is what Amazon bill for production use specifically to recommend

41
00:01:33.921 --> 00:01:35.570
products to customers that they might like.

42
00:01:35.571 --> 00:01:38.660
It's optimized for sparse data and multi GPU computation data.

43
00:01:38.661 --> 00:01:41.180
It sparks if it contains a lot of Zeros as not a whole lot of valuable

44
00:01:41.181 --> 00:01:44.450
information.
Rex usually operate on sparse data.
Not everything is connected,

45
00:01:44.451 --> 00:01:47.240
but you can manage to find some valuable links between people in items.

46
00:01:47.270 --> 00:01:49.910
Most central library to implement data parallel training as in it splits

47
00:01:49.911 --> 00:01:52.040
training data across multiple GP use.
It's works,

48
00:01:52.041 --> 00:01:54.230
but there's definitely a trade off between speed and accuracy.

49
00:01:54.260 --> 00:01:55.970
That's any use his model parallel training,

50
00:01:55.971 --> 00:01:58.070
so instead of splitting the data across multiple GP use,

51
00:01:58.100 --> 00:02:01.760
it flips the model across multiple Gpu so all the layers are spread out across

52
00:02:01.761 --> 00:02:04.430
multiple gps on the same server automatically for you,

53
00:02:04.490 --> 00:02:07.220
Amazon had to do this because the weight matrices to had to racks.

54
00:02:07.220 --> 00:02:10.280
That is all the mappings of users and attributes just didn't fit in the memory

55
00:02:10.281 --> 00:02:12.260
of a single GPU.
When it comes to the mount libraries.

56
00:02:12.261 --> 00:02:14.230
Destiny isn't as general purpose as tensorflow,

57
00:02:14.240 --> 00:02:17.060
but it is twice as fast when it comes to dealing with sparse data.

58
00:02:17.120 --> 00:02:19.700
So we'll follow our methodology and collect our data set,
build a model,

59
00:02:19.701 --> 00:02:22.340
train the model and test the model.
We'll call it a retrieved data set method,

60
00:02:22.341 --> 00:02:24.830
but the perimeter as a URL toward downloadable model.
In our case,

61
00:02:24.831 --> 00:02:26.500
we're going to use a sample movie lens data set,

62
00:02:26.510 --> 00:02:29.120
which contains user rating for a lot of different movies and their associated

63
00:02:29.121 --> 00:02:32.040
tags.
Once we have that,
we want to convert it to a format or MLA in rate.

64
00:02:32.070 --> 00:02:34.850
When this case is the net CDF format that's EDF is designed for efficiency

65
00:02:34.851 --> 00:02:37.820
realization of the large array of numbers and it's what destiny expects will

66
00:02:37.821 --> 00:02:40.700
generate it for both the input and output layer of our neural network and we'll

67
00:02:40.701 --> 00:02:42.770
use the name of the downloaded data set as our parameter.

68
00:02:42.800 --> 00:02:44.660
Both of these functions generate a net CDF file,

69
00:02:44.661 --> 00:02:46.880
an index file for neurons and the index fall for features.

70
00:02:46.910 --> 00:02:48.050
Once we generate in our model,

71
00:02:48.080 --> 00:02:50.050
it's time to train our neural networks and destiny.

72
00:02:50.060 --> 00:02:52.490
You build your model in a Jason Fall instead of programmatically.

73
00:02:52.520 --> 00:02:55.190
We can see in the config dot Jason Falls the structure of the neural network.

74
00:02:55.191 --> 00:02:57.950
This is where we set our hyper parameters and most important takeaway here is

75
00:02:57.951 --> 00:03:00.340
that we are creating a three layer of equal with neural network.

76
00:03:00.341 --> 00:03:04.420
I mean data just flows one way with one 128 node hidden layer and our activation

77
00:03:04.421 --> 00:03:07.150
function at each note is the classic sigmoid which turns values into

78
00:03:07.151 --> 00:03:07.840
probabilities.

79
00:03:07.840 --> 00:03:10.390
We can go ahead and run our train function with the batch size and number of

80
00:03:10.420 --> 00:03:14.290
epochs of the parameters or set the batch size or number of examples to two 56

81
00:03:14.291 --> 00:03:17.140
and the number of iPads or runs a 10 once we run this little critter newly

82
00:03:17.141 --> 00:03:19.720
trained model fall called GL dot and see which we can then use to predict

83
00:03:19.721 --> 00:03:20.440
recommendations.

84
00:03:20.440 --> 00:03:22.900
Our last step is to predict recommendation so we can just call the predict

85
00:03:22.901 --> 00:03:25.210
method and set the number of recommendations parameter to 10.

86
00:03:25.240 --> 00:03:27.400
This will place the newly created predictions and the reservoir.

87
00:03:27.430 --> 00:03:29.350
Now that we have our code Betty to be compiled and run,

88
00:03:29.370 --> 00:03:32.440
we want to upload it to AWS.
To start off,
we'll click on the easy to button,

89
00:03:32.441 --> 00:03:34.510
which will take us to Amazon's cloud computing service.

90
00:03:34.540 --> 00:03:37.140
They don't want to make sure when the US east North Virginia regions,

91
00:03:37.141 --> 00:03:39.970
it's payments on creative preconfigured image with dependencies like Kuda and

92
00:03:39.971 --> 00:03:41.950
opening NPR already set up for us in that region.

93
00:03:41.980 --> 00:03:44.710
We'll click on Ami to under the images directory of the left sidebar and the

94
00:03:44.711 --> 00:03:48.130
search for the instance called Ami,
a d six f to e six BC.

95
00:03:48.160 --> 00:03:50.770
It should pop up and then we'll click the blue launch button to spin up an

96
00:03:50.771 --> 00:03:53.680
instance using that image.
Then it'll show us a list of instance types.

97
00:03:53.710 --> 00:03:55.000
Since we want to speed up training time,

98
00:03:55.001 --> 00:03:56.830
let's go ahead and choose the GPU option here.

99
00:03:56.860 --> 00:03:59.590
Then we'll click review and launch and see the final page before we can launch

100
00:03:59.591 --> 00:04:02.280
our instance.
Everything looks good to go,
so let's click launch.

101
00:04:02.290 --> 00:04:04.060
It'll prompt you to create a new key pair.

102
00:04:04.061 --> 00:04:05.920
Go ahead and download it so you have it locally.
This,

103
00:04:05.921 --> 00:04:08.380
we'll have authorized your machine to connect to AWS.

104
00:04:08.410 --> 00:04:10.260
Now that we've successfully launched a GPU instance,

105
00:04:10.270 --> 00:04:11.770
we need to upload our code to it and train it.

106
00:04:11.800 --> 00:04:14.380
I'm a fan of using file Zilla to upload files,
so let's use that.

107
00:04:14.410 --> 00:04:17.290
We'll click the site manager icon,
then pacing the host name.
We've got gotta.

108
00:04:17.291 --> 00:04:20.530
Be sure to set the protocol to SFTP.
Then set the login type of normal,

109
00:04:20.531 --> 00:04:22.720
and the user is called Ubuntu.
Once we set the field,

110
00:04:22.721 --> 00:04:25.390
we can click connect and it will show us all the current files.
In our instance.

111
00:04:25.391 --> 00:04:27.580
Let's go ahead and drag and drop our project in to the root folder.

112
00:04:27.610 --> 00:04:29.530
Now that our code is in our Ece to incense,

113
00:04:29.531 --> 00:04:31.690
we can open up terminal and ssh into it.

114
00:04:31.691 --> 00:04:34.480
We can find the SSH snippet for terminal under the instances section.

115
00:04:34.510 --> 00:04:35.830
Once we click the connect button,
perfect.

116
00:04:35.860 --> 00:04:38.170
Let's just paste this baby right into terminal.
Boom,
we're in.

117
00:04:38.320 --> 00:04:40.180
Let's see the into our directory.
Before we run our code,

118
00:04:40.181 --> 00:04:41.140
we'll need to do two things.

119
00:04:41.150 --> 00:04:45.070
Had the NPAC and NBCC compiler is to our path and make the library.

120
00:04:45.071 --> 00:04:48.340
We can export NPCC and then we're in,
then we'll export NBCC.

121
00:04:48.370 --> 00:04:49.990
Now we can run our script,
and once that's done,

122
00:04:49.991 --> 00:04:51.410
we'll have wrecks and our recs fold.

123
00:04:51.430 --> 00:04:53.920
That's pretty much it that you can scale your neural net accordingly.

124
00:04:53.921 --> 00:04:56.380
Depending on the size of your data.
Check up the links down below,

125
00:04:56.381 --> 00:04:58.150
and please subscribe.
For more machine learning videos.

126
00:04:58.151 --> 00:05:00.220
I've got to go fix a runtime error,
so thanks for watching.

