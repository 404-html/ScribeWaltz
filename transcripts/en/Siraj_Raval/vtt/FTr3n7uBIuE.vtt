WEBVTT

1
00:00:00.120 --> 00:00:01.650
Hello world,
it's a Raj.

2
00:00:01.710 --> 00:00:05.970
And we're going to build a convolutional network using no libraries.

3
00:00:06.000 --> 00:00:10.920
I mean just num Pi,
but no libraries,
no tensor flow,
no pie torch,
none of it.

4
00:00:11.070 --> 00:00:14.070
We're going to look at the math behind it and we're going to build it with just

5
00:00:14.250 --> 00:00:18.810
num Pi for matrix math in python.
Okay?
And what it's going to be able to do,

6
00:00:18.840 --> 00:00:21.450
let me just start off with this demo to start off with.

7
00:00:21.630 --> 00:00:25.530
What it's going to be able to do is recognize any character that you type in,

8
00:00:25.560 --> 00:00:27.870
or not type in,
but draw in with your mouse.

9
00:00:28.080 --> 00:00:32.640
So you could draw a six like that and then hit submit.
It'll start working,

10
00:00:33.120 --> 00:00:35.670
and then it'll say it's a six.
And then if you don't want you to six,

11
00:00:35.671 --> 00:00:38.910
you could say a letter,
like a any number or letter,

12
00:00:38.911 --> 00:00:41.490
it's going to be able to detect slash predict.

13
00:00:41.790 --> 00:00:45.060
So it's gonna be really cool because we basically were wrapping it into a Web

14
00:00:45.061 --> 00:00:49.530
App using the flask web framework.
So it's gonna be,
it's gonna be super awesome.

15
00:00:49.770 --> 00:00:51.420
Okay,
so that's what we're going to do today.

16
00:00:51.690 --> 00:00:55.350
And this is our first neural network that we're building in this course from

17
00:00:55.351 --> 00:00:57.900
scratch.
I mean,
we made one in the weekly video,

18
00:00:57.901 --> 00:01:02.100
but this is the real hardcore convolutional network with all the layers,

19
00:01:02.101 --> 00:01:06.570
all the functions,
everything.
Okay,
so let's start off with what it's inspired by.

20
00:01:06.750 --> 00:01:10.890
Well,
it's inspired by Yan Lacoon,
the genius.
No,
it's not.
So young.

21
00:01:10.891 --> 00:01:15.180
Mccoon is a director of AI at Facebook.
He's a total g.

22
00:01:15.181 --> 00:01:20.181
He is awesome because he was inspired by these original two guys right here who

23
00:01:21.661 --> 00:01:26.661
published a paper in I think 68 or early sixties or seventies but the paper was

24
00:01:27.211 --> 00:01:31.140
on the Mammalian visual cortex.
And the idea they had was,

25
00:01:31.290 --> 00:01:33.840
and so here's a great image of it.
Let me make it a lot bigger.

26
00:01:34.380 --> 00:01:35.880
This has to be a lot bigger.

27
00:01:36.150 --> 00:01:41.150
So the idea they had was that mammals all see in a very similar way.

28
00:01:41.191 --> 00:01:43.230
And that way is hierarchical.

29
00:01:43.350 --> 00:01:47.160
So you have a collection of cells and these cells are neurons and these cells

30
00:01:47.161 --> 00:01:51.570
cluster and,
and these clusters represent different features that are learned.

31
00:01:51.600 --> 00:01:54.390
Okay.
So here in terms of neuroscience,

32
00:01:54.450 --> 00:01:58.470
they call these Clusters v One v two you don't have names for all these clusters

33
00:01:58.471 --> 00:02:02.160
in the brain.
These clusters of neurons before it posterior,

34
00:02:02.280 --> 00:02:03.810
all this neuroscience terminology.

35
00:02:03.811 --> 00:02:06.360
But what we need to know is that at a high level,

36
00:02:06.361 --> 00:02:09.600
what's happening is every time you see something,
a,

37
00:02:10.110 --> 00:02:15.110
a series of clusters or layers of neurons are being activated.

38
00:02:15.520 --> 00:02:16.440
Whenever you see something,

39
00:02:16.441 --> 00:02:18.990
whenever you detect something to pitch to be more accurate.

40
00:02:19.330 --> 00:02:22.290
If I detect a dog or a fee,
you know,
face or whatever,

41
00:02:22.620 --> 00:02:26.520
it's going to be a series of layers or clusters of neurons that fire,

42
00:02:26.700 --> 00:02:30.920
and each of these clusters are going to detect a set of features.
Okay?

43
00:02:30.930 --> 00:02:33.380
And these features are going to be more abstract.
The,

44
00:02:33.381 --> 00:02:35.790
the higher up the hierarchy of clusters.

45
00:02:35.791 --> 00:02:39.540
You could think of it as a vertical hierarchy or even a horizontal hierarchy,

46
00:02:39.541 --> 00:02:43.590
what it doesn't matter.
But the idea is that there is a hierarchy of features.

47
00:02:43.890 --> 00:02:47.550
And at the start,
these features are very simple,
are lines and edges.

48
00:02:47.670 --> 00:02:51.300
But then they get more abstracts and they become shapes and then they become a

49
00:02:52.230 --> 00:02:54.960
more complex shapes.
And then eventually at the,
at the highest level,

50
00:02:54.961 --> 00:02:59.961
at the highest cluster level exist the entire face or the entire dog or whatever

51
00:03:00.371 --> 00:03:04.540
it is.
And this is how the Mammalian visual cortex works.

52
00:03:04.810 --> 00:03:07.000
And so what young mccuin said,

53
00:03:07.001 --> 00:03:11.560
and his team in [inaudible] 98 when they published probably the landmark paper

54
00:03:11.561 --> 00:03:16.030
of convolutional nets,
which is kind of arguable I guess because it Crucell Skis,

55
00:03:16.060 --> 00:03:19.930
image net paper was pretty good and and I think 2012 but anyway,

56
00:03:20.210 --> 00:03:21.040
y'all the coons a g,

57
00:03:21.040 --> 00:03:26.040
I just wanted to say that he had the idea to be inspired by three things,

58
00:03:26.710 --> 00:03:31.570
three features of the human or the Mammalian visual Cortex,
local connections.

59
00:03:31.660 --> 00:03:34.600
And that means the clusters between neurons,
how each neuron,

60
00:03:34.810 --> 00:03:38.590
each set of neurons in a cluster cluster are connected to each other and they

61
00:03:38.591 --> 00:03:42.880
represent some set of features.
And then the idea of layering,
how these,

62
00:03:43.060 --> 00:03:48.060
how there's a hierarchy of features that are learned and spatial invariants what

63
00:03:48.581 --> 00:03:52.150
does this mean?
This word spacial invariants.
It means that whenever you are,

64
00:03:52.151 --> 00:03:54.940
I detect something,
whether it's,
let's say we're going to fighting a shoe,
right?

65
00:03:55.380 --> 00:03:59.560
If you see a shoe,
you know it's a shoe,
right?
If it's a Yeezy,

66
00:03:59.800 --> 00:04:03.520
if it's a,
you know,
Adidas,
whatever it is,
you know,
it's a shoe.

67
00:04:03.610 --> 00:04:07.360
It can be shaped this way or this way.
It can be rotated or transform,

68
00:04:07.600 --> 00:04:11.680
no matter how it varies,
we still can detect that it's a shoe.

69
00:04:11.681 --> 00:04:13.780
We know it's a shoe.
So we are,
it is,

70
00:04:13.960 --> 00:04:17.480
the way it's positioned is it's spatially in barrier.

71
00:04:17.530 --> 00:04:19.120
We can still detect what it is.

72
00:04:19.450 --> 00:04:24.450
And so those three concepts were what inspired the birth of convolutional neural

73
00:04:25.121 --> 00:04:27.580
networks,
programmatic neural networks,

74
00:04:27.700 --> 00:04:32.650
designed to mimic the Mammalian visual cortex.
How cool is that?
That's so cool.

75
00:04:32.980 --> 00:04:36.250
So how does this thing work?
Let's look at how this works.

76
00:04:36.400 --> 00:04:40.810
So we have a set of layers,
okay.
And we'll talk about what these layers mean,

77
00:04:40.811 --> 00:04:41.051
right?

78
00:04:41.051 --> 00:04:45.850
What is layer a layer in each case is a series.

79
00:04:45.880 --> 00:04:49.660
It's,
it's,
it's,
it's a series of operations that we're applying.
Okay?
So let's,

80
00:04:49.661 --> 00:04:53.820
let's talk about this,
right?
So we have some input image.
So let's see.
Let's see.

81
00:04:54.160 --> 00:04:57.520
This is the orange,
that's the image,
and you'll notice by the way,

82
00:04:57.521 --> 00:04:59.890
that this image,
this is a convolutional network,
by the way.

83
00:04:59.891 --> 00:05:01.570
This is what we're building,
okay?

84
00:05:01.840 --> 00:05:05.080
You'll notice that this image right here or this image of the convolutional

85
00:05:05.081 --> 00:05:09.070
network isn't what you normally look at when you think of neural network,
right?

86
00:05:09.220 --> 00:05:11.890
You always see that image of the circles and everything's connected.

87
00:05:11.980 --> 00:05:14.230
So why is it different for convolutional networks?

88
00:05:14.590 --> 00:05:19.540
Because every layer and a convolutional network isn't connected to every,

89
00:05:19.910 --> 00:05:24.640
to every neuron in every layer isn't connected to every other neuron in the next

90
00:05:24.641 --> 00:05:25.690
layer.
Why?

91
00:05:26.860 --> 00:05:29.860
Because that would be too computationally expensive.

92
00:05:29.861 --> 00:05:32.890
I'll go over that in a second,
but the idea is that if you,
if you see here,

93
00:05:33.180 --> 00:05:35.470
there is a part of the image that is connected.

94
00:05:35.471 --> 00:05:40.090
It's this little square of that orange and that is called the receptive field.

95
00:05:40.330 --> 00:05:42.840
Okay,
I'm going to go over all of this.
It's going to make more and more sense.

96
00:05:42.860 --> 00:05:44.140
You're going to be more confused.
It's going to be,

97
00:05:44.290 --> 00:05:47.830
it's going to make more and more sense as I go further and further in depth
here.

98
00:05:47.950 --> 00:05:51.460
So,
so,
so stay with me here.
So we have a receptive field.
Okay.

99
00:05:51.461 --> 00:05:56.020
That is some part of the image that we are focused on.
We are by focused,

100
00:05:56.021 --> 00:06:00.590
I mean that is the of the image that we apply a convolution operation to.

101
00:06:01.040 --> 00:06:05.930
Okay.
And we take that receptive field and we slide it across the image.
Okay?

102
00:06:05.931 --> 00:06:07.970
You're going to see exactly what I'm talking about in a second.

103
00:06:08.180 --> 00:06:11.390
I'm just going get over at a high level.
We slide over the image.

104
00:06:11.391 --> 00:06:15.800
We are applying a dot product between our weight matrix,
add a layer,

105
00:06:15.980 --> 00:06:20.240
and every part of that image iteratively.
Okay?
And so the,

106
00:06:20.241 --> 00:06:21.680
the reason that they look different,

107
00:06:21.681 --> 00:06:24.650
the convolutional networks look different is two reasons.
Really.

108
00:06:24.860 --> 00:06:29.300
The first reason is that not every neuron in each layer is connected to every

109
00:06:29.301 --> 00:06:33.370
other neuron and the next layer.
It's only a part of that because it would be a,

110
00:06:33.610 --> 00:06:34.910
to borrow from discrete math,

111
00:06:35.060 --> 00:06:40.060
a common historial explosion to connect every single pixel value in an image to

112
00:06:40.761 --> 00:06:44.540
every single pixel value in the next layer of features,
right?

113
00:06:44.560 --> 00:06:46.010
There'll be just a huge amount.

114
00:06:46.011 --> 00:06:50.930
So what we do instead is we take a part of that image and we irritably slide

115
00:06:50.931 --> 00:06:53.940
over it.
Okay?
So at a high level,
you understand the sliding part,
right?

116
00:06:53.941 --> 00:06:57.530
I think of it as a flashlight.
Okay.
Think of it.
Think of the,
uh,

117
00:06:57.560 --> 00:07:00.650
the filter at each layer that shines over the receptive field,

118
00:07:00.651 --> 00:07:04.280
that box as a flashlight,
and you're shining over the image and you're,

119
00:07:04.520 --> 00:07:08.930
and you're applying dot products to all of these numbers.
Okay?
Just like that.

120
00:07:08.960 --> 00:07:11.360
Okay.
I'm going to keep going into this.
That was just the highest law.

121
00:07:11.361 --> 00:07:13.280
You're not supposed to understand it all yet.
Okay.
That was,

122
00:07:13.520 --> 00:07:16.880
that was very high level.
We're still going deeper or go indeed,
we're going deep.

123
00:07:16.910 --> 00:07:20.030
Okay.
So check out this beautiful image right here.
Isn't it beautiful?

124
00:07:20.031 --> 00:07:22.640
It's very beautiful.
Also your beautiful for watching this.

125
00:07:22.640 --> 00:07:27.590
So thank you for watching this.
Okay.
So I love my fans so much.

126
00:07:27.591 --> 00:07:29.420
Seriously,
you guys are amazing.
Seriously,

127
00:07:29.421 --> 00:07:34.310
you guys are the reason I do this every week.
Okay,
so I,
by the way,

128
00:07:34.311 --> 00:07:36.290
I want to say one more thing to go on a tangent.

129
00:07:36.950 --> 00:07:40.280
The people who subscribe to my channel,
no one thought they existed.

130
00:07:40.430 --> 00:07:44.810
We are programmers who are smart and we are also cool.

131
00:07:44.930 --> 00:07:48.080
No one thought these people existed but we exist.
Okay.

132
00:07:48.081 --> 00:07:52.220
We are smart and we are cool.
So you are amazing.
Okay.
Anyway,

133
00:07:52.520 --> 00:07:55.760
back to this.
What this is is another way of looking at the network,
right?

134
00:07:55.790 --> 00:07:56.930
We're just looking at different ways.

135
00:07:56.931 --> 00:08:01.820
We're looking at different ways so we can build a spatially in variant image in

136
00:08:01.821 --> 00:08:04.460
our head of what a convolutional network is like,
right?

137
00:08:05.210 --> 00:08:06.530
No matter what the image is,

138
00:08:06.531 --> 00:08:09.800
we're going to learn to recognize what a convolutional network.
When we see one,

139
00:08:09.801 --> 00:08:13.100
I'm just trying to,
you know,
metta applying this logic to what we're learning.

140
00:08:13.520 --> 00:08:18.520
So what happens that each layer we are applying a series of dot products between

141
00:08:18.621 --> 00:08:23.060
the way major cs and the input matrix.
Okay?
And so what happens is,

142
00:08:24.070 --> 00:08:26.180
let's look at a third image.
Okay.
So this is the third image.

143
00:08:26.540 --> 00:08:30.740
What happens is we perform a series of operations,
okay.
At each layer.

144
00:08:31.130 --> 00:08:32.930
And so we could think of of differ,

145
00:08:33.110 --> 00:08:37.730
we could think of splitting up a convolutional network into two separate

146
00:08:37.731 --> 00:08:40.670
categories.
The first category is feature learning,

147
00:08:40.700 --> 00:08:44.030
and that's what's happening at the,
at the,
at the head of the,

148
00:08:44.210 --> 00:08:47.450
the head to the middle,
to almost a tail end of the network.

149
00:08:47.660 --> 00:08:51.200
And at the very tail end is classification.
So there's two parts,

150
00:08:51.500 --> 00:08:54.980
there's the feature learning part,
and then there's the classification part.

151
00:08:55.450 --> 00:08:56.700
And two for the feature learning part.

152
00:08:56.700 --> 00:09:01.230
What happens our three operations over and over and over again.

153
00:09:01.260 --> 00:09:04.110
And we can call them convolutional blocks,

154
00:09:04.140 --> 00:09:06.840
let's just call it an convolutional blocks.
I'm coining the term.

155
00:09:07.110 --> 00:09:09.960
So what happens is we first applied convolution,

156
00:09:10.210 --> 00:09:13.860
then we apply relu or any kind of activation.

157
00:09:14.220 --> 00:09:18.000
And then we applied pooling and we repeat that.
That's us.
That's a single block,

158
00:09:18.001 --> 00:09:22.650
three operations in a single convolutional block.
Okay.
So convolution,

159
00:09:22.651 --> 00:09:27.480
Relu,
pooling,
repeat convolution,
relu pooling,
repeat convolution.
Relu pooling.

160
00:09:27.630 --> 00:09:31.270
Okay.
And it's usually,
you know,
you have three blocks at least unless you're,

161
00:09:31.271 --> 00:09:36.150
you're building inception by Google,
then you have 15,
15 of these.
Uh,
but you,

162
00:09:36.151 --> 00:09:38.580
you know,
you have these convolutional blocks and at the very end,

163
00:09:38.610 --> 00:09:43.610
then you flatten that outputs into a smaller dimensional vector and then you

164
00:09:43.981 --> 00:09:45.480
apply a fully connected layer to it.

165
00:09:45.510 --> 00:09:49.860
So that means that you then connect all the neurons in one layer to the next one

166
00:09:50.070 --> 00:09:54.120
just because we want to then harness all of the learnings that we've learned so

167
00:09:54.121 --> 00:09:56.040
far.
That's why we fully connect at the end.

168
00:09:56.340 --> 00:10:00.600
And then we take those learnings and we squash it into a set of probability

169
00:10:00.601 --> 00:10:03.270
values with our last softmax function.

170
00:10:04.140 --> 00:10:06.360
And then we take the Max value of those probabilities.

171
00:10:06.540 --> 00:10:10.800
And each of these probabilities is a probability for us for specific class that

172
00:10:10.801 --> 00:10:15.750
it could be.
And we take the Max value,
let's say 72% as an we'll say,
okay,

173
00:10:15.751 --> 00:10:19.290
well,
72% for banana and now we know it's a banana.
Okay.

174
00:10:19.291 --> 00:10:23.190
So hopefully you get some of it,
but it's very confusing still.

175
00:10:23.191 --> 00:10:25.920
I know we're about to go even deeper.
Okay.
So get ready for this.

176
00:10:25.921 --> 00:10:30.300
I haven't even started yet,
so I haven't even started yet.
Okay.
So anyway,

177
00:10:31.470 --> 00:10:35.880
step one.
So for step one,
we are preparing a data set of images,
right?

178
00:10:35.881 --> 00:10:39.450
So when you think of an image,
you think of a matrix,

179
00:10:39.451 --> 00:10:42.310
hopefully a matrix of pixel values.
If you don't think of it that way,

180
00:10:42.330 --> 00:10:43.380
think of the thing of it that way.

181
00:10:43.381 --> 00:10:48.060
Now you're thinking of an image as a matrix of pixel values,
roads by columns,

182
00:10:48.061 --> 00:10:50.910
and each of these,
um,
each of these,
uh,

183
00:10:53.030 --> 00:10:57.320
points in the matrix represent a pixel right between zero and two 55,

184
00:10:57.800 --> 00:11:02.450
but it's actually better in terms of convolutional networks to think of an

185
00:11:02.460 --> 00:11:06.890
anemic as a three dimensional matrix.
And you're like,
what?
No,
what?
It's too.
No.

186
00:11:07.000 --> 00:11:10.280
So it's three dimension.
So the first dimension is the length of the image.

187
00:11:10.490 --> 00:11:14.210
The second dimension is the width.
And the third dimension is the depth.
So wait,

188
00:11:14.211 --> 00:11:18.050
what is the depth?
Because the depth represents the channels.

189
00:11:18.080 --> 00:11:21.680
And there are three channels for images,
red,
green,
and blue.

190
00:11:21.710 --> 00:11:24.560
Unless you're talking about gray scale,
then there's black,
then there's,
you know,

191
00:11:24.561 --> 00:11:26.900
black and white,
but we're talking about color images.
Okay.

192
00:11:27.140 --> 00:11:30.380
So there were three channels and you have these dimensions for each of the

193
00:11:30.381 --> 00:11:31.214
channels.

194
00:11:31.250 --> 00:11:35.690
So these values in each of these and each of these two d matrices for,

195
00:11:35.691 --> 00:11:37.880
and there are three of them,
represent the,

196
00:11:38.880 --> 00:11:39.713
<v 1>the</v>

197
00:11:40.800 --> 00:11:45.030
<v 0>amount of redness or the amount of greenness or the amount of blueness between</v>

198
00:11:45.031 --> 00:11:48.060
zero and two 55 so in terms of convolutional nets,

199
00:11:48.180 --> 00:11:51.120
we think of images as three dimensional pixels.
Okay.

200
00:11:51.121 --> 00:11:53.740
So I wanted to say that part.
Okay.
So that's,
that's,

201
00:11:53.770 --> 00:11:56.670
that's what we think of our image as our input image and it,

202
00:11:56.860 --> 00:12:00.700
it has an associated label,
right?
We're talking about supervised learning,

203
00:12:00.701 --> 00:12:04.990
learning the mapping between the input data and the output label dog image,

204
00:12:05.020 --> 00:12:08.470
dog label learned the mapping given a new dog image.
What is a label?
Well,

205
00:12:08.471 --> 00:12:10.360
you just learned it,
right?
So,

206
00:12:10.361 --> 00:12:14.770
and we learn it through backpropagation back propagates the update weights.

207
00:12:14.771 --> 00:12:18.910
Remember the rhyme?
You know what?
It is a,
I haven't wrapped yet in the series,

208
00:12:18.911 --> 00:12:20.920
but I will,
don't worry.
It's coming anyway,

209
00:12:20.921 --> 00:12:25.030
so every image is a matrix of pixel values.
We know this,

210
00:12:25.031 --> 00:12:30.031
we know this are between zero and two 55 and we can use several training

211
00:12:31.031 --> 00:12:33.390
datasets.
There are two really popular ones there.

212
00:12:33.391 --> 00:12:36.790
See farther and there's cocoa and there's a bunch of other ones as well,

213
00:12:36.791 --> 00:12:41.470
but basically these are huge datasets and you can find similar versions of them

214
00:12:41.860 --> 00:12:46.180
and each of these images,
their dogs,
their cars or airplanes there,
people,

215
00:12:46.181 --> 00:12:50.110
whatever,
they all have labels for them.
Ha handmade labels by humans,

216
00:12:50.111 --> 00:12:54.430
which is great for us.
Okay,
so that's,
that's it.
That's step one.

217
00:12:54.431 --> 00:12:56.890
Step one is to get your training data,
which is your images,

218
00:12:56.891 --> 00:13:00.280
which are your images.
Step two is to perform convolution.

219
00:13:00.310 --> 00:13:03.400
Now you might be asking what is convolution?
Well,

220
00:13:03.401 --> 00:13:07.600
I'm here to tell you that convolution is an operation that is dope as f.

221
00:13:07.750 --> 00:13:11.260
Here's why it's dope because it's not just used in computer science and machine

222
00:13:11.261 --> 00:13:14.140
learning.
It's using almost every field of engineering.

223
00:13:14.380 --> 00:13:18.250
Think of convolution as to paint buckets.
You have one paint bucket,
which is red,

224
00:13:18.340 --> 00:13:19.450
and the other one which is blue.

225
00:13:19.600 --> 00:13:23.110
And what you do is just smear it all over yourself.
No,
you don't do that.

226
00:13:23.230 --> 00:13:27.010
Well you do is you take these two paint buckets and you combine them into one

227
00:13:27.011 --> 00:13:29.920
paint bucket and that new paint bucket is going to be a new color,

228
00:13:30.460 --> 00:13:33.160
whatever that combination of colors is,
that's convolution.

229
00:13:33.370 --> 00:13:37.030
Convolution is taking two separate types of data,

230
00:13:37.310 --> 00:13:40.720
two matrices,
and then applying.

231
00:13:40.750 --> 00:13:42.760
And then it's an operation that combines them.

232
00:13:42.761 --> 00:13:46.270
So you could think of convolution as synonymous to combination.
Okay.

233
00:13:46.540 --> 00:13:49.240
And why do we apply?
Why do we say that for convolution networks?

234
00:13:49.480 --> 00:13:54.480
Because what we're doing is we are combining the values for each of these layers

235
00:13:55.210 --> 00:14:00.130
with the input matrix.
So thing of the input,
uh,
as that Matrix,
right?
And so,
well,

236
00:14:00.131 --> 00:14:03.220
it's a three dimensional.
It's a,
it's,
it's a,
it's a,
it's a three d tensor,
right?

237
00:14:03.221 --> 00:14:06.550
But we're applying it to each of these dimensions,
right?
So three of them.

238
00:14:06.551 --> 00:14:11.080
So just think of it as a matrix for right now.
And so what we do is we take this,

239
00:14:11.680 --> 00:14:15.970
uh,
fee.
So at each layer and each layer there is a weight.

240
00:14:16.000 --> 00:14:17.290
So by the way,
okay,

241
00:14:17.650 --> 00:14:21.460
so there's a lot of interchangeable terms and machine learning and it's easy to

242
00:14:21.461 --> 00:14:24.550
get confused here,
but I want to set the record straight for a second.

243
00:14:24.850 --> 00:14:29.850
Wait is the same as feature matrix is the same as feature map is the same as a

244
00:14:32.200 --> 00:14:35.320
filter in this case and for convolutional networks,

245
00:14:35.470 --> 00:14:38.050
so cds or even colonel colonel is a different one.

246
00:14:38.140 --> 00:14:42.010
There's actually five interchangeable terms.
I can see how it can be confusing,

247
00:14:42.250 --> 00:14:46.090
but if you get the basic idea of you have an input Matrix,

248
00:14:46.390 --> 00:14:49.780
which is your image,
and then you'd have a set of matrices,

249
00:14:49.900 --> 00:14:54.560
which are your features that are learned,
you know,
edges,
shapes,

250
00:14:54.680 --> 00:14:58.550
more attract shapes.
That's it.
That's it.
That's all it is.

251
00:14:58.700 --> 00:15:03.700
Matrix dot product matrices that are being multiplied by major cs all the way

252
00:15:04.251 --> 00:15:06.850
through that.
That's all it is.
Majors,
he's going to be multiplied by matrix.

253
00:15:06.851 --> 00:15:08.780
These all the way through or just a chain of them.
Okay.

254
00:15:08.990 --> 00:15:13.610
So what happens for convolution is we take a matrix and we multiply it by all

255
00:15:13.611 --> 00:15:16.190
the values in this matrix at a certain region,
right?

256
00:15:16.310 --> 00:15:18.920
And so this is what I was talking about when I was saying we have as a receptive

257
00:15:18.921 --> 00:15:21.500
field because we don't just multiply it all at once.

258
00:15:21.650 --> 00:15:23.870
We multiplied by a little part of it.
Okay?

259
00:15:23.871 --> 00:15:27.380
The receptive field and we slide it and we can define what that interval is.

260
00:15:27.381 --> 00:15:30.020
That sliding window.
I know I'm talking a lot without coding,

261
00:15:30.021 --> 00:15:32.000
the coding is coming,
believe me,
the cutting is coming.

262
00:15:32.210 --> 00:15:35.120
But just check this out for a second.
We've got to learn this,
uh,

263
00:15:35.150 --> 00:15:38.530
conceptually first.
So we are multiplying the,

264
00:15:38.660 --> 00:15:43.280
the feature matrix by that input image just for every row in every column or

265
00:15:43.281 --> 00:15:44.540
just multiply,
multiply,
multiply them.

266
00:15:44.720 --> 00:15:49.040
And what happens is we have this new matrix that results the output and that

267
00:15:49.070 --> 00:15:52.100
output is considered the convulsed feature.
Okay?

268
00:15:52.101 --> 00:15:56.180
And so what we do is we use that output as the input for them to the next layer.

269
00:15:56.181 --> 00:15:58.790
And we repeat the process over and over and over again.

270
00:15:59.090 --> 00:16:03.250
Obviously there's two more parts here.
There's the activation,
the Relu,

271
00:16:03.260 --> 00:16:05.210
and then there's the pooling,
which I'll talk about as well.

272
00:16:05.211 --> 00:16:08.930
But that's the basic idea between convolution and that's why we call it

273
00:16:09.110 --> 00:16:09.921
convolution.

274
00:16:09.921 --> 00:16:14.921
Because we are combining or consolving the weight matrix or filter or colonel,

275
00:16:15.620 --> 00:16:18.410
whatever you want to call it,
feature map by that input.

276
00:16:18.620 --> 00:16:22.100
We're combining it using the APP and using that output as the input for the next

277
00:16:22.101 --> 00:16:26.150
layer after activating it and,
and pulling it.
Okay,

278
00:16:26.151 --> 00:16:30.680
so that's convolution and also,
um,
right.

279
00:16:30.681 --> 00:16:34.970
So we apply it to all of those dimensions for that,
for that input matrix.
Okay.

280
00:16:34.971 --> 00:16:38.480
And that gives us our activation map or feature map or filter,
right?

281
00:16:38.481 --> 00:16:42.440
So many different interchangeable terms here.
So anyway,

282
00:16:42.441 --> 00:16:46.430
so it's computed using the dot product.
So you might be thinking,
well,
okay,

283
00:16:46.460 --> 00:16:50.660
I see how there is a dot product.
I see how there's matrix multiplication,

284
00:16:50.900 --> 00:16:54.260
but how does that really tell us what features there are?
I still,

285
00:16:54.261 --> 00:16:59.261
you're still not making the connection probably why understandably why this,

286
00:16:59.361 --> 00:17:03.320
these series of major operations help us detect features.
Well,

287
00:17:03.321 --> 00:17:06.380
here's what happens.
What happens is this,

288
00:17:06.381 --> 00:17:09.620
and here's the great thing about matrices and having several of them.

289
00:17:12.110 --> 00:17:16.910
When we learn a filter or a way to whatever you want to call it,

290
00:17:16.940 --> 00:17:19.590
well this is,
you know what,
moving forward,
let's just call it filter.
Okay,
let,

291
00:17:19.610 --> 00:17:21.500
I'm just saying,
let's just call it filter.

292
00:17:21.501 --> 00:17:23.120
Moving forward for the rest of this video,

293
00:17:23.690 --> 00:17:26.900
when we learned a filter over time by training it on mouse mouth pictures,

294
00:17:26.901 --> 00:17:30.710
for example,
a filter's gonna look like this at let's say at the first layer we,

295
00:17:30.830 --> 00:17:33.560
we learn a filter for detecting a curve that looks like this,
right?

296
00:17:33.561 --> 00:17:34.394
This curve right here.

297
00:17:34.580 --> 00:17:37.040
And so what's what this filter is going to look like for the tech thing.

298
00:17:37.041 --> 00:17:40.850
This specific type of curve is,
it's going to be a very sparse filter.

299
00:17:40.851 --> 00:17:45.080
That means there's a lot of zeroes except so there's all these years except for

300
00:17:45.081 --> 00:17:50.040
right here you see this 30 30 30 30 and notice that these values represent the

301
00:17:50.041 --> 00:17:52.830
shape.
They go in this direction of a shape.

302
00:17:53.070 --> 00:17:57.480
And so what happens is when we take this filter and perform the dot product,

303
00:17:57.540 --> 00:18:01.410
you know,
we can involve it with whatever part of the mouse,

304
00:18:01.620 --> 00:18:06.540
if it's over a part of the mouse that matches that feature.
Exactly.

305
00:18:07.050 --> 00:18:10.950
Then we,
when we multiply all of those,
uh,
when we,

306
00:18:11.010 --> 00:18:14.370
when we performed the dot product between all those values and sum them up,

307
00:18:14.760 --> 00:18:17.220
that's the convolution operation right there.
Okay.

308
00:18:17.280 --> 00:18:20.370
Just it's going to be a big number.
Okay.

309
00:18:20.371 --> 00:18:23.160
And so then we know that we've detected a feature because we've,

310
00:18:23.300 --> 00:18:26.670
we multiplied it,
sum it up,
and there's a large number.
And if there's not,

311
00:18:27.810 --> 00:18:29.100
if we multiply,
if,

312
00:18:29.350 --> 00:18:32.310
let's say we had that receptive field over a different part of the mouse and

313
00:18:32.311 --> 00:18:36.450
that that curve doesn't exist,
then it's going to be zero,
right?

314
00:18:36.451 --> 00:18:41.370
Because if you look between these 30,
30,
30 values and that the equivalent,
um,

315
00:18:41.430 --> 00:18:45.600
locations on this pixel representation of the [inaudible] image,

316
00:18:45.780 --> 00:18:50.160
these are zeros.
And so what happens when you multiply zero by 30,
you get zero,

317
00:18:50.310 --> 00:18:53.790
right?
So that's why it's important to make the rest of the,

318
00:18:54.030 --> 00:18:57.600
so the data that's irrelevant.
We want it to be zero,
right in the,

319
00:18:57.840 --> 00:19:02.280
in the feature maps are in the filters that we learn in the filters that we

320
00:19:02.281 --> 00:19:06.870
learn.
We want the irrelevant parts to be zero and in the images.
Okay.

321
00:19:07.580 --> 00:19:11.550
And,
and in the input images.
So I,

322
00:19:11.970 --> 00:19:14.970
so I can actually go even more into convolution,

323
00:19:14.971 --> 00:19:17.280
but it's not really necessary,
but it's,

324
00:19:17.281 --> 00:19:18.750
it is super dope and it's super dope though.

325
00:19:18.960 --> 00:19:20.580
This is a great blog post by the way.

326
00:19:20.581 --> 00:19:23.910
I definitely encourage you to read this blog post.
It's linked in the notebook,

327
00:19:24.180 --> 00:19:27.030
but the stewed Tim Tim,
he goes into these,

328
00:19:27.031 --> 00:19:29.970
this idea of convolution and he talks about how it's applied to all these

329
00:19:29.971 --> 00:19:34.470
different engineering fields and he goes into,

330
00:19:35.070 --> 00:19:37.050
uh,
the formula,

331
00:19:37.350 --> 00:19:41.070
the formula for the convolutional theorem is what he called w is what it's

332
00:19:41.071 --> 00:19:43.860
called.
Okay.
And I'm just gonna go over this at a high level,

333
00:19:44.250 --> 00:19:49.140
but the convolution theorem is this general theorem for discrete,

334
00:19:49.141 --> 00:19:51.900
while there's a discrete version and a continuous version,
right?

335
00:19:51.901 --> 00:19:56.450
The street is if there's one or zero black or white,
you know,
definite,
uh,

336
00:19:56.460 --> 00:19:59.210
classes.
That's something could be,
whereas continuous is,

337
00:19:59.300 --> 00:20:04.300
is if it could be an infinite amount of values between zero and 1.5 0.2 5.7

338
00:20:06.390 --> 00:20:09.900
infinity in that direction.
But here's the,
here's the formula for it.

339
00:20:09.901 --> 00:20:14.100
And so let me make it bigger just really quickly and then we'll get back to it

340
00:20:14.250 --> 00:20:18.000
because it's,
it's really cool.
But the convolution theorem states that

341
00:20:19.800 --> 00:20:24.510
we,
and so in it,
it's a general theorem that can be applied to any,
any,

342
00:20:24.690 --> 00:20:28.580
any set of problems.
But in terms of what's relevant to us is,

343
00:20:28.820 --> 00:20:33.360
is the convolutional theorem applied to a matrix operations.

344
00:20:33.450 --> 00:20:36.060
So what we can do is we can say what it,

345
00:20:36.120 --> 00:20:40.170
what it says is it's the input to times the kernel and it's the dot product.

346
00:20:40.170 --> 00:20:42.720
It's a dot product between two different major seas.

347
00:20:42.900 --> 00:20:46.050
And we performed that for every value in all of those matrices.

348
00:20:46.260 --> 00:20:49.240
And we do that for all of the values that we have and we summed them up
together.

349
00:20:49.241 --> 00:20:51.310
And that's what the sigma term represents.
And we,

350
00:20:51.311 --> 00:20:55.030
and we actually express that right here,
right?
This operation right here,

351
00:20:55.031 --> 00:20:57.310
this multiplication and summation is the same thing,

352
00:20:57.460 --> 00:21:01.390
but it's a more complex way of looking at it or more mathematically accurate
way.

353
00:21:01.840 --> 00:21:04.540
And also the fast 40 a transform is,

354
00:21:04.750 --> 00:21:09.250
is brought up by this and the fast fourier transform.

355
00:21:09.340 --> 00:21:12.670
Take some spatial data and it converts it into 48 space,

356
00:21:12.790 --> 00:21:16.450
which is like a wave form.
And you see this a lot in your day to day life.

357
00:21:16.451 --> 00:21:19.960
Whenever you're looking at,
uh,
some sound,
you know,
you're,

358
00:21:19.961 --> 00:21:22.750
you're listening to some sound and you look at your MP three player and you see

359
00:21:22.751 --> 00:21:26.440
the waves.
That's a,
that's a 48 transform happening.
Uh,
but I won't go into that.

360
00:21:26.441 --> 00:21:29.860
That's,
that's for sound and audio.
But anyway,
it's a really cool a blog post.

361
00:21:29.861 --> 00:21:32.080
Definitely check it out.
Okay,
so back to this.

362
00:21:34.030 --> 00:21:37.270
So we talked about convolution,
now we're going to talk about pooling,
right?

363
00:21:37.450 --> 00:21:41.830
So what is pooling?
So whenever we apply convolution to some image,

364
00:21:41.860 --> 00:21:45.880
what's going to happen at every layer is we're going to get a series of feature

365
00:21:45.881 --> 00:21:50.620
of,
of,
so each of the weights are going to consist of multiple images.

366
00:21:50.830 --> 00:21:55.300
And each of these images are going to be
at every layer,

367
00:21:55.330 --> 00:21:57.730
there's going to be more and smaller images.

368
00:21:57.820 --> 00:22:00.970
So the first few layers are going to be these huge images,
right?

369
00:22:01.210 --> 00:22:03.850
And then at the next few layers are going to be more of those,

370
00:22:03.851 --> 00:22:06.550
but they're gonna be smaller and it's just going to get you just like that.
Okay.

371
00:22:06.700 --> 00:22:09.100
And if then we squash it with some fully connected layer.

372
00:22:09.250 --> 00:22:12.520
So it gets some probability values with a soft Max.
But anyway,

373
00:22:13.120 --> 00:22:17.650
what pooling does,
is it re is it dense?
Is it makes the Matrix,

374
00:22:17.890 --> 00:22:22.780
the major cities that we learn more dense.
Here's what I mean.
So if you,

375
00:22:22.870 --> 00:22:27.520
if you perform convolution between an input and a feature matrix or a weight

376
00:22:27.521 --> 00:22:31.990
matrix or filter,
it's going to result in a matrix,
right?

377
00:22:31.991 --> 00:22:35.350
But this matrix is going to be pretty big.
It's going to be a pretty big matrix.

378
00:22:35.650 --> 00:22:40.650
What we can do is we can take the most important parts of that matrix and pass

379
00:22:41.170 --> 00:22:41.771
that on.

380
00:22:41.771 --> 00:22:46.270
And what that's gonna do is it's going to reduce the computational complexity of

381
00:22:46.271 --> 00:22:50.290
our model.
Okay.
So that's what pooling is all about.
It's a pooling stuff.

382
00:22:50.291 --> 00:22:51.610
So there's different types of pooling.

383
00:22:51.970 --> 00:22:54.520
Max Pooling is the most used type of pooling by the way.

384
00:22:55.510 --> 00:23:00.070
So basically multiply.
So what happens is we,
we strive,
we have some,

385
00:23:00.071 --> 00:23:03.850
we defined some window size and then some strides size.
So how,

386
00:23:03.851 --> 00:23:06.580
what are the intervals that we look at?
And we say,
okay,

387
00:23:06.730 --> 00:23:11.080
so for each of these windows,
let's take the Max value.
So for,
so for uh,

388
00:23:11.140 --> 00:23:14.920
this one right here for six zero hate,
the Max value would be eight.

389
00:23:15.040 --> 00:23:17.830
And so for one,
three,
12,
nine,
it'd be 12,
right?

390
00:23:17.831 --> 00:23:19.900
So we just take the biggest number.
It's really simple actually.

391
00:23:19.990 --> 00:23:23.520
We just take the biggest number and we just do that for all of them.
And so that,

392
00:23:23.521 --> 00:23:24.640
that's what pulling is all about.

393
00:23:24.641 --> 00:23:29.641
And so it's going to just give us that the most relevant parts of the image.

394
00:23:30.341 --> 00:23:33.820
And if you,
if you think of these,
these,
these values and uh,
in the,

395
00:23:34.000 --> 00:23:38.590
in the matrix as pixel intensities,
by taking the maximum intense,

396
00:23:38.860 --> 00:23:42.370
the pixel with the most intensity or that the highest intensity,

397
00:23:42.550 --> 00:23:45.440
we're getting that feature that is the most relevant,
if you see what I'm saying,

398
00:23:45.441 --> 00:23:50.441
it's a least opaque feature to use a term from image,

399
00:23:51.890 --> 00:23:54.150
um,
math.
Anyway,
so we,

400
00:23:54.300 --> 00:23:57.090
so we talked about pooling and we talked about uh,

401
00:23:57.140 --> 00:24:00.620
we talked about activation and so now,

402
00:24:02.640 --> 00:24:06.140
no,
we talked about convolution and we talked about pooling and so now the third

403
00:24:06.141 --> 00:24:09.470
part is normalization or activation.

404
00:24:09.590 --> 00:24:11.660
So remember how I said how it would be,

405
00:24:11.690 --> 00:24:16.100
it's so important that we have these values that are not related to our image.

406
00:24:16.101 --> 00:24:20.300
Be Zero.
We want it to be zero.
So the result is zero.
If the,

407
00:24:20.330 --> 00:24:24.110
if the feature is not detected well,
the way we do that is using relu.

408
00:24:24.380 --> 00:24:28.700
And so relu stance were rectified.
Linear unit.
It's an activation function.

409
00:24:28.730 --> 00:24:30.500
It's an activation function.
Okay.

410
00:24:30.770 --> 00:24:34.640
We use activation functions throughout new neural networks and we use them

411
00:24:34.760 --> 00:24:39.620
because it is that you can also call them nonlinearities because they increase,

412
00:24:39.650 --> 00:24:43.670
they make our model able to learn nonlinear functions,

413
00:24:43.671 --> 00:24:47.300
not just one of your functions but nonlinear function.
So any kind of function,

414
00:24:47.301 --> 00:24:49.880
right?
That universal function approximation through.

415
00:24:49.881 --> 00:24:53.660
And we talked about that activation functions help make this happen.

416
00:24:54.080 --> 00:24:58.550
And so relu is a space is a special kind of activation function that turns all

417
00:24:58.551 --> 00:25:01.280
negative numbers into zero.

418
00:25:01.370 --> 00:25:03.620
So that's why it's going to make the math easier.

419
00:25:03.621 --> 00:25:07.000
It won't make the math break for a convolutional networks who apply Relu.

420
00:25:07.160 --> 00:25:10.580
So basically what we do is for every single pixel value in the,

421
00:25:10.581 --> 00:25:15.050
in the input to this activation function,
we turn it,
if it's a negative,

422
00:25:15.051 --> 00:25:18.110
we just say make a zero.
It's super simple.
It will be one line of code.

423
00:25:18.111 --> 00:25:21.940
You'll see exactly what I'm talking about.
Okay.
And so that's,
that's,

424
00:25:21.950 --> 00:25:26.210
those are our blocks.
So that's how our convolutional blocks work.
However,

425
00:25:26.211 --> 00:25:29.720
there is another step that I didn't talk about that is a nice to have.

426
00:25:29.721 --> 00:25:30.650
And state of the art,

427
00:25:30.651 --> 00:25:33.830
convolutional networks always use it and that's called dropout.

428
00:25:33.950 --> 00:25:37.190
So Geoffrey Hinton,
the guy who invented,
um,

429
00:25:37.760 --> 00:25:41.990
neural networks invented a feature in men at a technical dropout and would drop

430
00:25:41.991 --> 00:25:42.800
out.
Is,

431
00:25:42.800 --> 00:25:47.090
is a good analogy is old people are not old people,

432
00:25:47.091 --> 00:25:49.850
but people who are stuck in their ways.
Let me,
let me,
okay,

433
00:25:49.851 --> 00:25:54.020
so what dropout does is it turns neurons on and off randomly.

434
00:25:54.230 --> 00:25:56.290
What do I mean by that?
That,
I mean,
the,

435
00:25:56.360 --> 00:26:01.360
the matrices for each weight value is converted to zero randomly at some layer

436
00:26:02.031 --> 00:26:04.730
of the network.
And so what happens is by doing this,

437
00:26:04.850 --> 00:26:08.210
our network is forced to learn new representations for the data,

438
00:26:08.360 --> 00:26:10.910
new pathways that that data has to flow through.

439
00:26:11.060 --> 00:26:12.800
It can't always float through this neuron.

440
00:26:12.980 --> 00:26:16.190
And the reason we use it is to prevent over fitting,
right?

441
00:26:16.191 --> 00:26:19.700
We want to prevent over fitting.
We've borne to prevent being to fit to the data.

442
00:26:19.880 --> 00:26:21.500
Think of it as,
you know,
the older you get,

443
00:26:21.501 --> 00:26:24.800
the more set in your ways of thinking you're,
you are right.

444
00:26:24.860 --> 00:26:28.580
And so it's harder to think of new ways of,
of,
of thinking,
right?

445
00:26:28.670 --> 00:26:32.720
Because you're so set in some ways.
So a way to prevent that is to have a novel,

446
00:26:32.930 --> 00:26:34.160
crazy experience,

447
00:26:34.370 --> 00:26:37.520
whether it's skydiving or taking psychedelics or whatever it is.

448
00:26:37.820 --> 00:26:40.640
And what that does is it creates new pathways.
So you're not,

449
00:26:40.670 --> 00:26:44.610
so you're kind of forced,
your brain is forced,
make new pathways,

450
00:26:44.760 --> 00:26:48.600
and this increases your generalization ability and you're not.
So overfit,

451
00:26:49.830 --> 00:26:52.110
that's a very rough abstract analogy.

452
00:26:52.111 --> 00:26:56.520
But basically drop out is not as complex as that sounds dropped out can be done

453
00:26:56.521 --> 00:26:57.430
in three lines of code.

454
00:26:57.431 --> 00:27:00.960
So definitely check out this blog post as well that I've linked.

455
00:27:01.200 --> 00:27:06.200
But what it does is it just randomly picked some neurons in a layer to set to

456
00:27:06.391 --> 00:27:09.360
zero,
right?
So it's just,
it's just three lines.
Okay.

457
00:27:09.361 --> 00:27:11.820
And you can look at it in this notebook,
right?
So that's,

458
00:27:11.821 --> 00:27:14.280
and then our last step is probability conversions.

459
00:27:14.281 --> 00:27:17.070
So we've got this huge set of values,
right?

460
00:27:17.071 --> 00:27:21.060
All these little small images that are represented by this huge output matrix.

461
00:27:21.360 --> 00:27:25.230
And we want to take this huge set of values and make some sense out of it.

462
00:27:25.231 --> 00:27:26.910
We want to make probabilities out of it.

463
00:27:27.150 --> 00:27:30.090
And the way we do that is using a soft Max at the end.

464
00:27:30.300 --> 00:27:33.510
A softmax is a type of function,
and it looks like this,
this,

465
00:27:33.511 --> 00:27:35.160
this is a softmax function right here,

466
00:27:35.370 --> 00:27:39.300
but what we do is we plug these values into the softmax function and it's what

467
00:27:39.301 --> 00:27:41.460
you output a set of probability values,

468
00:27:41.461 --> 00:27:45.960
discreet probability values for each of the classes that we're trying to
predict.

469
00:27:46.230 --> 00:27:49.920
Okay?
And then what we'll do is given all those probability values,

470
00:27:50.070 --> 00:27:54.330
we'll pick the biggest one using Arg Max,
the Arg Max function and num Pi.

471
00:27:54.660 --> 00:27:58.110
And that's going to give us the most likely glass.
Okay.

472
00:27:58.111 --> 00:28:00.120
Those are the seven steps of a fee.

473
00:28:00.180 --> 00:28:04.170
A full forward pass through a convolutional network looks like that.

474
00:28:04.950 --> 00:28:09.660
And so now you might be wondering,
well,
okay,
so how do we train this thing?
Well,

475
00:28:09.690 --> 00:28:13.560
using gradient descent,
right?
And when applied to neural networks,

476
00:28:13.580 --> 00:28:18.530
branded grading dissent is called
backpropagation.

477
00:28:18.720 --> 00:28:21.270
Exactly.
I hope you got that right.
Anyway.
Okay,

478
00:28:21.271 --> 00:28:23.670
so how do we learn these magic numbers,
right?

479
00:28:23.790 --> 00:28:27.630
How do we learn what these weight value should be?
What the features should be?

480
00:28:28.080 --> 00:28:29.910
Backpropagation is how we do it,
right?

481
00:28:30.090 --> 00:28:34.030
And so we've talked quite a bit about backpropagation and gradient descent,

482
00:28:34.050 --> 00:28:36.720
but I'll do a little,
I'll go over it again.
Um,

483
00:28:37.890 --> 00:28:40.770
but the idea is that we have some error that we're computing,
right?

484
00:28:40.950 --> 00:28:45.030
This is super,
this is supervised learning.
We have a huge,
we have a human label,

485
00:28:45.031 --> 00:28:45.870
right?
For some data.

486
00:28:46.020 --> 00:28:50.490
So we put in a dog image or a bicycle image to look at the same it to relate to

487
00:28:50.491 --> 00:28:53.400
this image here we put in a bicycle image and the bike label,

488
00:28:53.610 --> 00:28:57.350
we pass it through the t,
each layer.
Dot.
Product dot product l.
Dot.

489
00:28:57.360 --> 00:29:00.420
Product activation function,
pool dot product repeat,
repeat,

490
00:29:00.500 --> 00:29:04.140
softmax or squash and into probability values.
Pick the biggest one.

491
00:29:04.410 --> 00:29:05.970
And we have some prediction value.

492
00:29:06.330 --> 00:29:09.100
And what we do is we compare the prediction value to the out,

493
00:29:09.150 --> 00:29:13.230
the actual value and we get an error and we take our error and we compute the

494
00:29:13.231 --> 00:29:17.220
partial derivative of the error with respect to each weight value going

495
00:29:17.221 --> 00:29:21.000
backwards in the network.
Okay?
Like this.
Okay.

496
00:29:21.001 --> 00:29:23.700
And so for regression,
we use the mean squared error.

497
00:29:23.701 --> 00:29:25.710
If we're using linear regression regression,

498
00:29:25.980 --> 00:29:28.830
and for classification we use the softmax function.

499
00:29:29.040 --> 00:29:31.680
So remember how in the first neural network we built,

500
00:29:31.681 --> 00:29:36.000
and in their linear regression example,
we used a,
uh,

501
00:29:36.360 --> 00:29:39.840
we use mean squared error to compute the air.
And now we're using the softmax.

502
00:29:40.020 --> 00:29:41.290
So we'll take the oil,

503
00:29:41.291 --> 00:29:44.290
take the partial derivative of the error with respect to our weights,

504
00:29:44.560 --> 00:29:48.430
and then that's going to give us the gradient value that we then update each of

505
00:29:48.431 --> 00:29:51.940
those wait values recursively going backward in the network.

506
00:29:52.120 --> 00:29:55.510
And that's how it learns what those features are,
what the ideal feature,

507
00:29:55.750 --> 00:30:00.520
the weight matrix value should be.
But what about the other,
uh,

508
00:30:00.700 --> 00:30:02.050
what about the other magic numbers?

509
00:30:02.051 --> 00:30:05.350
What about the number of neurons and the number of features and the size of

510
00:30:05.351 --> 00:30:07.930
those features?
And the pooling window size and the window stride,

511
00:30:08.170 --> 00:30:11.260
while those that is an active area of research,

512
00:30:11.320 --> 00:30:14.500
there are best practices for values that you should use for those,

513
00:30:14.680 --> 00:30:18.040
for those hyper parameters,
right.
The tuning knobs of our network.

514
00:30:18.370 --> 00:30:22.810
And Andre carpathy has some great material on this and he's probably the leading

515
00:30:22.811 --> 00:30:25.310
source for convolutional networks right now in terms of um,

516
00:30:25.690 --> 00:30:29.470
written content and uh,
yeah,

517
00:30:29.500 --> 00:30:31.240
I mean this is an active area of research,

518
00:30:31.241 --> 00:30:34.840
finding out what the ideal hyper parameters for our neural network should be and

519
00:30:34.841 --> 00:30:37.180
we're still learning what it should be,
what,
what,
what,
what,

520
00:30:37.210 --> 00:30:40.360
how we can get them rather than just guessing and checking,

521
00:30:40.480 --> 00:30:43.300
which is what we do right now,
which is kind of like,
you know,

522
00:30:44.140 --> 00:30:46.780
not as not as optimal.
Right?
So anyway,

523
00:30:47.230 --> 00:30:49.690
last two things and then we're going to start with the code.

524
00:30:49.840 --> 00:30:52.300
When is a good time to use this?
Well,
we know what to classify images,

525
00:30:52.301 --> 00:30:53.140
we've talked about that,

526
00:30:53.260 --> 00:30:56.980
but you can also use them to generate images and that's for later on.

527
00:30:56.981 --> 00:31:00.400
That's a little more advanced.
But to give you a little spoiler,
a little teaser,

528
00:31:00.580 --> 00:31:02.800
in fact,
this is in my intro to deep learning playlist,

529
00:31:03.140 --> 00:31:04.120
taking a convolutional network.

530
00:31:04.121 --> 00:31:06.910
You flip it and then you call it a d convolutional network,

531
00:31:07.210 --> 00:31:11.440
and then you can take some texts and create an image out of text.

532
00:31:11.620 --> 00:31:12.880
How crazy is that?
Okay.

533
00:31:14.110 --> 00:31:16.810
There's also generative models where you have two networks fighting each other

534
00:31:16.811 --> 00:31:19.540
and you can generate new images.
Whole bunch of really cool,

535
00:31:19.541 --> 00:31:24.310
crazy stuff you can do.
But anyway,
when should you use a convolutional network?

536
00:31:24.340 --> 00:31:28.510
Anytime you have spatial two D or three d data,
what do I mean?
Well,

537
00:31:28.600 --> 00:31:32.350
obviously images are spatial.
The word spacial implies that the space,

538
00:31:32.351 --> 00:31:36.730
the positioning of the data matters.
So sound,

539
00:31:36.731 --> 00:31:40.130
you can apply to sound images or text where the,
the spa,

540
00:31:40.180 --> 00:31:44.980
the position of the text matters,
right?
Because we have a flashlight,
our filter,

541
00:31:45.040 --> 00:31:46.900
and we're involving over an image,
right?

542
00:31:47.980 --> 00:31:50.410
But if you have some data like say customer data,

543
00:31:50.560 --> 00:31:52.720
where if you were to just flip the rows and columns,

544
00:31:52.960 --> 00:31:55.540
it doesn't matter what order they're in,
they're still,
you know,

545
00:31:56.320 --> 00:31:57.370
there's still features.

546
00:31:57.430 --> 00:32:01.180
So a good rule of thumb is if you swap out the rows and columns of your Dataset

547
00:32:01.450 --> 00:32:04.930
and uh,
it's just as useful,
like the space doesn't matter,

548
00:32:05.170 --> 00:32:09.610
then you don't want to use a CNN.
It helps you do.
Okay in a great,
and last thing,

549
00:32:09.611 --> 00:32:12.550
the great example of using CNNs are for robot learning.

550
00:32:12.730 --> 00:32:16.330
You can use a CNN for object detection and then you can use a CNN for grasp

551
00:32:16.420 --> 00:32:19.510
learning and combine the two.
And then you could get a robot that cooks,

552
00:32:19.690 --> 00:32:20.590
which is really cool.

553
00:32:20.770 --> 00:32:24.490
I've got a great tensorflow example and a great adversarial network example.

554
00:32:24.580 --> 00:32:26.350
Okay,
let's go into the code now.

555
00:32:27.490 --> 00:32:31.840
And so what I'm gonna do is I'm going to look at the class for the convolutional

556
00:32:31.841 --> 00:32:35.620
network in them pie as well as the prediction class.
There's two classes here.

557
00:32:35.830 --> 00:32:37.750
Okay.
So these are our three inputs.

558
00:32:37.990 --> 00:32:42.680
Pickle is for saving and loading our serialized model.
What do I mean?

559
00:32:42.681 --> 00:32:47.681
Pickle is python's way of having a platform or language agnostic way of saving

560
00:32:48.081 --> 00:32:50.750
data.
So you can load it up later.
Tensorflow uses it,

561
00:32:50.751 --> 00:32:54.290
a bunch of other libraries uses it as well.
None pies were matrix math.

562
00:32:54.291 --> 00:32:57.770
And when we've got our own little custom class for preprocessing of the data,

563
00:32:57.860 --> 00:33:00.770
because we don't care about that part,
we care about the machine learning part.

564
00:33:01.040 --> 00:33:01.670
Okay,

565
00:33:01.670 --> 00:33:06.670
so let's talk about arc light OCR or object optical character recognition class.

566
00:33:07.490 --> 00:33:08.720
In our initialized function,

567
00:33:08.721 --> 00:33:12.530
we're going to load the weights from the pickle file and in store and then store

568
00:33:12.531 --> 00:33:13.850
all the labels that we've loaded.

569
00:33:14.000 --> 00:33:18.200
We'll define how many rows and columns in an image load up our s convolutional

570
00:33:18.201 --> 00:33:21.860
network using the light CNN function with our saved weights.

571
00:33:21.890 --> 00:33:23.570
So assuming we've already trained our network,

572
00:33:23.571 --> 00:33:27.620
we load it with the saved weights from the pickle file and then we defined a

573
00:33:27.621 --> 00:33:30.560
number of pooling layers.
Okay.
So once we had that,

574
00:33:30.710 --> 00:33:32.480
then we can use this predict function.

575
00:33:32.481 --> 00:33:35.150
So given some new image will reshape the image.

576
00:33:35.151 --> 00:33:38.900
So was in the correct size to perform the dot product between that image and the

577
00:33:38.901 --> 00:33:43.550
first layer of our convolutional network.
And we'll,
we'll,

578
00:33:43.760 --> 00:33:44.750
we'll put it,

579
00:33:44.810 --> 00:33:47.960
go all feed it into our network and it's going to output a prediction

580
00:33:47.961 --> 00:33:51.380
probability for our class.
And we were returning.
Okay.
Super high level.

581
00:33:51.381 --> 00:33:54.290
We haven't even coded our CNN.
That's,
that's our first class.

582
00:33:54.291 --> 00:33:56.720
That's our prediction class.
Now,

583
00:33:56.900 --> 00:33:59.330
now we're going to look at a convolutional network class.

584
00:33:59.570 --> 00:34:00.740
And what I'm going to do as I'm going to,

585
00:34:01.010 --> 00:34:05.090
I'm going to go over the code and I'm going to code some parts of it.

586
00:34:06.840 --> 00:34:09.320
So now we'll look at our convolutional network class,
okay?

587
00:34:09.321 --> 00:34:14.270
So in our initialized function will initialize to list one to store the layers

588
00:34:14.271 --> 00:34:16.670
that we've learned,
the weights of each layer,

589
00:34:16.940 --> 00:34:20.000
and then the size of the pooling area for Max pooling.
Okay,

590
00:34:20.510 --> 00:34:23.780
we'll load up our weights,
uh,
from our pickle file just like this.

591
00:34:24.200 --> 00:34:27.260
And then we have our predict function.
Now in our predict function,

592
00:34:27.261 --> 00:34:31.190
that's where the real magic is happening,
right?
Let's code what this looks like.

593
00:34:31.191 --> 00:34:35.510
So given some input x,
we're going to feed it through all of these layers,
right?

594
00:34:36.650 --> 00:34:40.220
So what happens is we will say,
okay,

595
00:34:40.430 --> 00:34:43.790
so the first layer is going to be a convolutional layer,
okay?

596
00:34:43.791 --> 00:34:46.910
And we're going to define what all of these functions look likes,
look like.

597
00:34:46.911 --> 00:34:49.040
But the first layer is going to be that convolutional layer.

598
00:34:49.250 --> 00:34:52.820
We'll feed in that first image and we'll say,
okay,
well this is the first layer.

599
00:34:52.821 --> 00:34:56.490
So it was a zero layer.
We'll say border mode equals full.
Uh,

600
00:34:56.520 --> 00:34:59.690
and I'll talk about that part later on.
But that's it for that.

601
00:34:59.691 --> 00:35:03.350
And so what happens is x equals this layer.
Okay?
So that's our first layer.

602
00:35:03.351 --> 00:35:07.400
And then our next layer is going to be relu.
So we'll say,
okay,

603
00:35:07.550 --> 00:35:12.550
now let's apply an activation to the outputs of the previous layer.

604
00:35:12.950 --> 00:35:16.010
Okay?
And then we'll set it equal to that.
Okay.

605
00:35:16.040 --> 00:35:19.580
So we'll set the output from the previous leader equal to the input of this

606
00:35:19.581 --> 00:35:24.050
layer.
And then we keep going.
We say,
okay,
so we've got another,
uh,
CNN,

607
00:35:24.080 --> 00:35:28.520
we have another convolutional layer and we do the same thing here.
We say,
okay,

608
00:35:28.700 --> 00:35:32.280
take the output from the previous layer.
We'll define what the uh,

609
00:35:32.420 --> 00:35:34.580
name of this layer is,
as well as the border mode,

610
00:35:34.581 --> 00:35:38.940
which I'll talk about the very end of this.
If a border mode,
which is valid.

611
00:35:39.180 --> 00:35:41.520
And then we say,
okay,

612
00:35:41.521 --> 00:35:44.640
well we'll set the output of that equal to the input of this.

613
00:35:44.641 --> 00:35:45.720
And just keep repeating.

614
00:35:45.900 --> 00:35:50.900
Now it's time for us to apply a nother nonlinearity.

615
00:35:51.241 --> 00:35:54.180
So we'll just go ahead and apply our nonlinearity.
Again,

616
00:35:54.181 --> 00:35:58.800
remember these are convolutional blocks.
Oh,
and we also want to pool.

617
00:35:59.040 --> 00:36:02.730
So also the,
the order with which you can do this varies,
right?

618
00:36:02.731 --> 00:36:05.400
You could do this in different ways and yeah,

619
00:36:05.401 --> 00:36:08.340
so I'm doing it a certain way right now.
You know,
we could change it around,

620
00:36:08.341 --> 00:36:10.890
it would change our result,
but the order,
Matt,

621
00:36:10.980 --> 00:36:15.900
the ordering within the block can be,
can be different.
Okay.
So,
right,

622
00:36:15.901 --> 00:36:20.010
so we're gonna pool.
It's,
we're going to pick the most relevant features from,

623
00:36:20.040 --> 00:36:24.390
from that,
uh,
uh,
from that output.

624
00:36:24.690 --> 00:36:27.100
And then we're going to perform dropout to prevent overfilling.

625
00:36:27.180 --> 00:36:32.040
And we're going to say there's going to be a 0.25% chance that a neuron is going

626
00:36:32.041 --> 00:36:35.100
to be deactivated,
that we'll turn it off,
set it to zero,

627
00:36:35.490 --> 00:36:40.440
and that's our dropout probability value.
And then now we're getting into our,

628
00:36:40.490 --> 00:36:45.430
um,
our,
the second category of our network,
not the feature learning part,

629
00:36:45.431 --> 00:36:48.600
but the classification part.
And we'll say,
okay,
so let's flatten this layer.

630
00:36:48.601 --> 00:36:51.600
Let's reduce the dimensionality of all of that data.

631
00:36:51.780 --> 00:36:56.550
So it's something that we can then
learn from.
And we'll say,

632
00:36:56.551 --> 00:37:00.210
well,
let's,
let's set it equal to seven and then we'll say,

633
00:37:00.790 --> 00:37:05.790
I once again turn that output into our inputs here.
Okay?

634
00:37:05.820 --> 00:37:10.530
And so then we have another dense layer.
We just,
we just keep going with,

635
00:37:10.600 --> 00:37:13.380
or our first dense layer.
And that means we are going to,

636
00:37:13.410 --> 00:37:14.430
it's a fully connected layer.

637
00:37:14.431 --> 00:37:17.670
So we're combining everything that we've learned because we're getting really

638
00:37:17.671 --> 00:37:22.140
close to squashing these values into a set of probability value.

639
00:37:22.141 --> 00:37:24.660
So we want to take all of our learnings and combine them,

640
00:37:24.930 --> 00:37:26.490
what they fully connected layer.

641
00:37:28.310 --> 00:37:32.070
And so we'll combine them with a fully connected layer and then,
uh,

642
00:37:34.020 --> 00:37:38.910
we'll squash it now with our sigmoid or no,
not our sigmoid,
our softmax function.

643
00:37:39.240 --> 00:37:43.530
Okay?
And then that's going to give us our output probability,

644
00:37:43.560 --> 00:37:46.140
and then we're going to say,
well,
which of the probabilities do we want?

645
00:37:46.350 --> 00:37:47.910
We want the Max one,
right?

646
00:37:47.911 --> 00:37:52.710
We want the Max probability and we'll classify it just like that and return that

647
00:37:52.770 --> 00:37:55.020
value.
Okay?
That's the highest level.

648
00:37:55.200 --> 00:37:57.990
And so if you were using carrots or one of these high level libraries,

649
00:37:58.200 --> 00:37:59.880
this is all your code would look like.

650
00:38:00.000 --> 00:38:03.120
But we're going to do is we're going to look at these functions as well.
Okay,

651
00:38:03.420 --> 00:38:04.740
so let's look at these functions.

652
00:38:07.570 --> 00:38:11.190
So we'll start off with the convolutional layer function and have your notebook

653
00:38:11.191 --> 00:38:15.030
open with me as well.
So you could go over this.
The link is in the description.

654
00:38:15.031 --> 00:38:18.090
If you don't know now you know if you don't know now you know.

655
00:38:18.300 --> 00:38:22.320
So for our convolutional layer,
given some input image,
we're going to say,

656
00:38:22.440 --> 00:38:25.680
well we'll store a feature maps and the bias value in these two variables,

657
00:38:25.681 --> 00:38:29.730
features in bias will define how big our filter or patch is going to be.

658
00:38:29.880 --> 00:38:34.410
How many features do we want?
How big is our image,
how many channels RGB.

659
00:38:34.411 --> 00:38:38.050
So three.
And then how many images do we have?
So given those values,

660
00:38:38.170 --> 00:38:40.330
well the find a border mode.
So a border mode.

661
00:38:40.480 --> 00:38:44.230
So is so when you apply it full to border mode.
In this case,

662
00:38:44.500 --> 00:38:48.520
it means that the filter has to go outside the bounds of the input by filter

663
00:38:48.521 --> 00:38:49.690
size divided by two.

664
00:38:49.870 --> 00:38:53.770
The area outside of the input is normally padded with Zeros and the border mode

665
00:38:53.771 --> 00:38:57.130
valid is when you get an output that is smaller than the input because the

666
00:38:57.131 --> 00:39:01.090
convolution is only computed where the input and the filter fully overlap.

667
00:39:02.530 --> 00:39:04.560
Okay.
And they'll give us different,
um,

668
00:39:04.870 --> 00:39:08.770
they'll give us different classification results,
accuracy,
results,

669
00:39:08.920 --> 00:39:10.930
and it's good to test both options.

670
00:39:10.931 --> 00:39:15.120
So what we'll do is we'll initialize our feature matrix for this layer as calm

671
00:39:15.280 --> 00:39:18.100
as can be,
Zeros is going gonna be a bunch of Zeros.
And then we'll say,
okay,

672
00:39:18.101 --> 00:39:21.880
so for every image that we have for every feature in that image,

673
00:39:22.060 --> 00:39:25.690
let's initialize a Convult image as empty.
And then for each channel,

674
00:39:25.700 --> 00:39:27.820
so doing this for each of the three channels,

675
00:39:28.210 --> 00:39:30.220
let's extract a feature from our feature map,

676
00:39:30.580 --> 00:39:32.770
define a channel specific part of our image,

677
00:39:33.100 --> 00:39:37.510
and then perform convolution on our image using that given feature filter.

678
00:39:37.660 --> 00:39:39.700
So notice this convulsed two function.

679
00:39:39.880 --> 00:39:42.670
It's where the actual convolution operation is happening.

680
00:39:43.030 --> 00:39:46.660
This is more of a wrapper for that actual mathematical operation.

681
00:39:46.960 --> 00:39:48.160
So once we have that,

682
00:39:48.161 --> 00:39:51.790
we'll add a bias and a bias acts as our anchor for our network.

683
00:39:51.791 --> 00:39:53.620
It's kind of like the y intercept.
It's kind of like a,

684
00:39:53.790 --> 00:39:56.140
a starting point for our model to exist.

685
00:39:57.280 --> 00:40:00.790
And then we'll add it to our list of Convult features for this,
for this layer.

686
00:40:00.820 --> 00:40:03.310
Okay.
And then we'll return that as the has our feature map.

687
00:40:03.311 --> 00:40:05.920
Ours are set of filter valleys,
our weight matrices.

688
00:40:06.890 --> 00:40:09.430
And so let's look at this Convult Tootie,
uh,
function.

689
00:40:09.431 --> 00:40:11.020
So in our [inaudible] function,

690
00:40:11.021 --> 00:40:14.140
we'll define the tensor dimension of the image and the feature.

691
00:40:14.470 --> 00:40:19.390
We'll get a target dimension.
And then these two lines perform this,

692
00:40:19.630 --> 00:40:24.550
this,
uh,
operation,
this convolutional theorem that we defined right here.

693
00:40:24.790 --> 00:40:29.170
We're performing a dot product between the input and the colonel or feature for,

694
00:40:29.350 --> 00:40:32.860
for all of those,
um,
wait,
now use,

695
00:40:33.070 --> 00:40:36.130
and then we're summing them all up and that's going to be our outputs.

696
00:40:36.400 --> 00:40:40.510
And so the fast 40 ea function in num Pi does this very well.

697
00:40:40.870 --> 00:40:45.070
And so we can just use that as FFT two.
But that's what it's a multiplication.

698
00:40:45.071 --> 00:40:47.230
And in summation operation.
Okay.

699
00:40:47.231 --> 00:40:51.730
And so then we have our target value and then once we have our target value,

700
00:40:52.000 --> 00:40:55.390
we could say,
okay,
let's have a starting point and an ending point.

701
00:40:55.391 --> 00:41:00.190
And our target value is going to be within that range of what we want to return

702
00:41:00.191 --> 00:41:01.390
as the involved feature.
Right?

703
00:41:01.391 --> 00:41:05.950
So we have some bounding box that we went to apply this to.
Okay.

704
00:41:05.951 --> 00:41:08.020
So then,
so we have that.
So what else do we have?

705
00:41:08.021 --> 00:41:12.940
So we started off with our convolutional layer and then we had our relu.

706
00:41:12.941 --> 00:41:13.840
So what does real loose,

707
00:41:13.850 --> 00:41:18.520
really super simple relu Relu is just forgive.

708
00:41:18.550 --> 00:41:21.020
So for,
for some matrix of Zeros,

709
00:41:21.480 --> 00:41:24.460
we'll go through every single pixel value in the input matrix.

710
00:41:24.640 --> 00:41:28.330
And if it's a negative number,
we just turned it into zero.
That's it.
That's Relu.

711
00:41:28.360 --> 00:41:31.480
Okay.
And then so we have the,
we had talked about really,

712
00:41:31.481 --> 00:41:34.220
we've talked about convolution,
we have to talk about pooling.

713
00:41:34.221 --> 00:41:36.110
So what does Max pooling look like?

714
00:41:36.320 --> 00:41:39.320
So given our learn features and our images,

715
00:41:39.620 --> 00:41:43.130
let's initialize our more dense feature list as empty.
And so here's what we do.

716
00:41:44.030 --> 00:41:46.750
We're going to,
we're going to take the max values up,

717
00:41:46.760 --> 00:41:50.180
all of those parts of the input image,
right?
So we're going to say,

718
00:41:50.240 --> 00:41:53.750
we're going to say for each image and for each feature map begin by the row,

719
00:41:53.940 --> 00:41:58.040
the find a starting and ending point,
okay.
Which we defined with our pool size,

720
00:41:58.100 --> 00:42:00.280
hyper parameter.
And so for each column,

721
00:42:00.281 --> 00:42:03.980
so we've got a set of rows and columns for each image.
There's a notice,

722
00:42:03.981 --> 00:42:05.510
a lot of nesting happening here.

723
00:42:05.780 --> 00:42:08.330
We're going to define started hand points for the columns as well.

724
00:42:08.810 --> 00:42:12.500
And then we're going to say define a patch given our defined starting and ending

725
00:42:12.501 --> 00:42:14.600
points,
so some some bounding box.

726
00:42:14.930 --> 00:42:18.320
And then take the Max value from that patch using n and p.
Dot.

727
00:42:18.321 --> 00:42:21.620
Maxx and that patch is what moves around,
right?

728
00:42:22.860 --> 00:42:24.350
For all parts of that image.

729
00:42:24.650 --> 00:42:27.970
And then we returned that and we're going to store all of that in our pooled

730
00:42:27.980 --> 00:42:31.820
features.
A Matrix right here.
And we returned that as the output.

731
00:42:32.060 --> 00:42:35.510
And that's what we pass on in the convolutional network.
Okay?

732
00:42:35.511 --> 00:42:39.650
So that's what Max pooling is.
Okay.
So we've talked about convolution,
Relu,

733
00:42:39.651 --> 00:42:43.250
Max pooling and then drop out.
So for dropouts,

734
00:42:44.750 --> 00:42:45.583
<v 1>yeah,</v>

735
00:42:45.600 --> 00:42:46.110
<v 0>right.</v>

736
00:42:46.110 --> 00:42:50.370
We have our probability value that we define as 0.25 and we just multiply it by

737
00:42:50.371 --> 00:42:51.930
the inputs.
Okay.
And that will,

738
00:42:51.931 --> 00:42:56.550
that's going to do is it's going to turn on or off some part of the matrix into.

739
00:42:56.580 --> 00:43:00.210
So by on and off,
I mean zero,
you'll make it either zero or not zero.

740
00:43:00.390 --> 00:43:01.920
So it'll have,
so then our data,

741
00:43:01.921 --> 00:43:05.970
we'll have to learn to either be multiplied by it or find a different pathway.

742
00:43:06.700 --> 00:43:11.040
And that's for dropout.
And then we talked about dropout and convolution,

743
00:43:11.340 --> 00:43:15.660
flattening,
dense and softmax.
So for flattening,
it's just a,

744
00:43:15.661 --> 00:43:19.950
it's a tensor transformation.
We just reduced the dimensionality of the input.

745
00:43:20.310 --> 00:43:23.040
Okay.
And then for our

746
00:43:25.510 --> 00:43:28.180
dense layer,
our dentists are fully connected layer.

747
00:43:28.300 --> 00:43:32.770
Now this is the generic layer that you would see in a feed forward network input

748
00:43:32.771 --> 00:43:37.150
times.
Wait,
uh,
and then you add a bias,
right?
Which is the dot product.
Right here.

749
00:43:37.151 --> 00:43:40.510
This is,
this is a dense layer.
We take our input times our way out of bias.

750
00:43:40.870 --> 00:43:41.540
So that means we,

751
00:43:41.540 --> 00:43:45.070
we just performed the dot product between the full weight Matrix and the full

752
00:43:45.071 --> 00:43:47.500
way matrix instead of doing it at all the layers,

753
00:43:47.501 --> 00:43:51.790
because that would be way too tuition,
computationally expensive for image data.

754
00:43:52.000 --> 00:43:55.960
We perform it at one fully,
one fully connected or dense layer at the end.

755
00:43:56.170 --> 00:43:59.500
And that's a way for us to combine all of our learnings together so we can then

756
00:43:59.980 --> 00:44:03.250
promptly squash it with a,
a softmax function.

757
00:44:04.690 --> 00:44:09.580
Okay.
So then for our,
uh,
softmax layer,
and then we have classify,

758
00:44:09.910 --> 00:44:14.740
so for our softmax layer,
we will,
uh,
so this is the,

759
00:44:14.770 --> 00:44:18.160
this is the formula for Softmax programmatically speaking.
Uh,

760
00:44:18.190 --> 00:44:22.090
but what it does is going to output a set of probability values and then we'll

761
00:44:22.091 --> 00:44:25.660
classify those values by taking the Arg Max,
the largest probability.

762
00:44:25.780 --> 00:44:27.880
And that is our output.
Okay.

763
00:44:27.881 --> 00:44:32.310
So that is our forward pass through the network.
Okay.

764
00:44:33.780 --> 00:44:34.613
And so,

765
00:44:36.950 --> 00:44:37.640
<v 1>okay,</v>

766
00:44:37.640 --> 00:44:39.740
<v 0>yes,
that is our forward pass through the network.</v>

767
00:44:47.880 --> 00:44:48.640
So backpack,

768
00:44:48.640 --> 00:44:52.230
so backpropagation works pretty much the same way as I've talked about before.

769
00:44:52.231 --> 00:44:55.080
Several Times.
Greatness and backpropagation works the same way.

770
00:44:55.290 --> 00:44:58.530
We take the partial derivative of our error with respect to our weights and the

771
00:44:58.531 --> 00:44:59.071
recursively,

772
00:44:59.071 --> 00:45:03.000
update our weights using that gradient value that we gradient equals partial

773
00:45:03.001 --> 00:45:05.670
derivative equals Delta interchangeable words.

774
00:45:05.850 --> 00:45:09.390
But here's a great simple example right here where we after the forward pass,

775
00:45:09.570 --> 00:45:12.960
we do the same thing in reverse order.

776
00:45:13.140 --> 00:45:16.380
So we calculate the gradient of those weights and then back and then multiply

777
00:45:16.381 --> 00:45:17.610
them by the previous layer.

778
00:45:18.960 --> 00:45:23.670
And then for our javascript portion we are taking the drawing from the user.

779
00:45:23.700 --> 00:45:27.960
Here's the main code for that paint window and a canvas and we are going to say

780
00:45:28.170 --> 00:45:29.820
capture the mouse's positions,

781
00:45:29.821 --> 00:45:33.570
capture all those points in that image with an event listener and they were

782
00:45:33.571 --> 00:45:34.520
going to say on paints.

783
00:45:34.520 --> 00:45:36.930
So whenever they use actually starts moving that painting,

784
00:45:37.110 --> 00:45:40.320
whenever that mouse stops clicking and then the user hits the submit button,

785
00:45:40.530 --> 00:45:44.430
we'll save that snapshot or that image and then feed that into the network.

786
00:45:44.850 --> 00:45:47.370
And that's our flask APP.
We'll define two routes,

787
00:45:47.371 --> 00:45:50.010
one for our home and then one for that image for the network.

788
00:45:50.190 --> 00:45:52.080
We can deploy it to the web.
There's a Heroku APP.

789
00:45:52.081 --> 00:45:55.170
You could definitely check out the link link is in the description as well.

790
00:45:55.260 --> 00:45:57.420
Check out the notebook and yeah,
that's it.

791
00:45:57.421 --> 00:46:00.770
Please subscribe for more programming videos and for now I've got to do a 48

792
00:46:00.771 --> 00:46:03.510
transform,
so thanks for watching.

