WEBVTT

1
00:00:00.130 --> 00:00:04.390
Oh world,
it's Saroj and in this video we're going to make our own chat Bot using

2
00:00:04.391 --> 00:00:08.320
tensorflow.
2016 has been the year of the Chat Bot Messenger.

3
00:00:08.380 --> 00:00:13.340
We chat Skype and a bunch of other popular messaging platforms now host chatbots

4
00:00:13.360 --> 00:00:17.320
that developers have built for them and brands are increasingly using chatbots

5
00:00:17.440 --> 00:00:20.320
to engage their customers because the data doesn't lie.

6
00:00:20.380 --> 00:00:24.940
90% of apps that get downloaded or only used once with a chat bot,

7
00:00:24.970 --> 00:00:26.950
there's no need to download anything.

8
00:00:26.980 --> 00:00:31.390
It lives inside of the chat app that you open up a dozen times a day and

9
00:00:31.391 --> 00:00:34.060
competing for space on your phone home screen is really hard,

10
00:00:34.061 --> 00:00:38.200
but per space on your next most used screen,
your chat app,
that's more doable.

11
00:00:38.290 --> 00:00:42.250
You can now chat with CNN to get the news or chat what they bought to get

12
00:00:42.251 --> 00:00:45.490
flowers delivered to your boo or even chat what they matchmaking

13
00:00:45.600 --> 00:00:48.200
<v 1>but
what,</v>

14
00:00:48.670 --> 00:00:51.400
<v 0>but even though the chat bot space is getting really hot,</v>

15
00:00:51.550 --> 00:00:53.230
it's nowhere near saturated.

16
00:00:53.440 --> 00:00:56.770
Just think of an APP that you like and build a chat Bot for it.

17
00:00:56.920 --> 00:01:01.240
Chatbots are the new apps.
Before deep learning hit the scene a few years ago,

18
00:01:01.480 --> 00:01:03.820
all chatbots had hardcoded responses,

19
00:01:04.030 --> 00:01:08.200
a that would try to predict everything you would say and build a huge list of

20
00:01:08.201 --> 00:01:10.660
responses for every question they could think of.

21
00:01:10.690 --> 00:01:12.400
All of them were pretty terrible.

22
00:01:12.430 --> 00:01:16.480
Deep learning changed everything and still is as new discoveries are being made.

23
00:01:16.750 --> 00:01:19.420
Instead of telling a computer what to do,
you can say,

24
00:01:19.570 --> 00:01:22.120
this is what I want as an outcome.
Make it happen.

25
00:01:22.390 --> 00:01:26.290
Some chatbots that have used deep learning do so by taking different components

26
00:01:26.410 --> 00:01:27.850
and applying it to each of them.

27
00:01:27.910 --> 00:01:32.050
I could create a deep learning based system to interpret the language and

28
00:01:32.051 --> 00:01:35.560
another one to track the state of a conversation and then another one to

29
00:01:35.561 --> 00:01:36.820
generate a response.

30
00:01:37.120 --> 00:01:41.620
Each of these systems would be trained separately to do its own task and the

31
00:01:41.620 --> 00:01:43.810
Chat Bot would collectively use the results from each.

32
00:01:43.990 --> 00:01:46.630
This is unnecessarily complex to build.

33
00:01:48.940 --> 00:01:52.480
Ah,
a better type are called end to end.

34
00:01:52.750 --> 00:01:56.590
These are chatbots that use one system that's trained on one dataset.

35
00:01:56.800 --> 00:02:00.760
They make no assumptions about the use case or the structure of the dialogue.

36
00:02:00.940 --> 00:02:03.550
You just train it on the relevant data and say,

37
00:02:03.640 --> 00:02:07.450
I want you to be able to have a conversation with me about this data and to end

38
00:02:07.451 --> 00:02:09.820
systems are what we should all be striving for.

39
00:02:10.000 --> 00:02:14.320
Intuitively they make sense and they're starting to outperform all other
systems.

40
00:02:14.380 --> 00:02:16.870
So let's talk about how to do this with deep learning.

41
00:02:17.050 --> 00:02:19.840
The most simple type of neural net is feed forward.

42
00:02:19.990 --> 00:02:24.580
That means that as it trains data just flows one way from the input node all the

43
00:02:24.581 --> 00:02:28.240
way to the output node.
It only accepts data that is a fixed size,

44
00:02:28.270 --> 00:02:29.770
like an image or a number.

45
00:02:29.800 --> 00:02:34.780
Give it a labeled dataset like whether or not a temperature is hot or cold and

46
00:02:34.781 --> 00:02:37.960
it'll be able to predict if a given temperature is hot or cold.

47
00:02:37.990 --> 00:02:41.770
But a conversation isn't a fixed size.
It's a sequence of words.

48
00:02:41.800 --> 00:02:44.260
We need a network that can accept sequences as an input,

49
00:02:44.470 --> 00:02:46.660
a recurrent neural net in a recurrent net.

50
00:02:46.661 --> 00:02:50.350
We feed the data back into the input while training it in a recurring loop.

51
00:02:50.470 --> 00:02:54.100
So we're going to build a chat bot intense or float using recurrent neural nets

52
00:02:54.370 --> 00:02:57.190
are steps will be to download our Dataset,
create a model,

53
00:02:57.280 --> 00:03:00.370
train it on that Dataset,
then test out by chatting with it.

54
00:03:00.460 --> 00:03:03.880
The first thing we'll want to do is decide what data set we want to use.

55
00:03:03.910 --> 00:03:07.330
If we were creating a chat bot for a specific use case like customer service,

56
00:03:07.480 --> 00:03:11.110
we want to use a dataset of conversation logs from a real human representative,

57
00:03:11.380 --> 00:03:14.350
but for this demo we just want to make a fun conversational Bot,

58
00:03:14.620 --> 00:03:18.160
so we'll use a movie dialogue.
David said,
compiled by Cornell University.

59
00:03:18.430 --> 00:03:22.540
It contains conversations between characters from over 600 Hollywood movies.

60
00:03:22.600 --> 00:03:26.110
Hopefully transcendence is not included in that list.
Well,

61
00:03:26.111 --> 00:03:28.840
download our Dataset and put it in our data directory.

62
00:03:29.020 --> 00:03:32.440
Next we'll want to split our data into two different sets.
For training,

63
00:03:32.620 --> 00:03:36.130
we'll call one set in code or data and the other set decoder data.

64
00:03:36.190 --> 00:03:39.790
The encoder data will be the text from one side of the conversation.

65
00:03:40.000 --> 00:03:42.160
That decoder data will be the responses.

66
00:03:42.190 --> 00:03:46.840
Then we'll want to tokenize our data and give each token an integer.
Id.

67
00:03:46.840 --> 00:03:51.840
Tokenizing means taking each sentence like a Lomo and chopping it into pieces so

68
00:03:51.911 --> 00:03:56.350
that it's easier for a model to train on and giving each token and associated
id.

69
00:03:56.470 --> 00:04:00.070
We'll make data retrieval faster.
Once our data is properly formatted,

70
00:04:00.130 --> 00:04:03.640
we can create our model.
We can define our own function for this.

71
00:04:03.670 --> 00:04:07.480
That takes our tokenized encoder and decoder data as its parameters.

72
00:04:07.540 --> 00:04:11.650
Our function is going to return TensorFlow's built in sequence to sequence model

73
00:04:11.740 --> 00:04:14.500
with what's called the embedding attention mechanism.

74
00:04:14.690 --> 00:04:16.330
Let's break down what the eff.
This means.

75
00:04:16.390 --> 00:04:20.140
A sequence to sequence model consists of two recurrent neural networks.

76
00:04:20.290 --> 00:04:22.060
One recurrent net is the encoder.

77
00:04:22.090 --> 00:04:25.690
It's job is to create an internal representation of the sentence.

78
00:04:25.691 --> 00:04:28.420
It's given which we can call a context vector.

79
00:04:28.630 --> 00:04:31.600
This is a statistical value that represents that sentence.

80
00:04:31.630 --> 00:04:33.940
The other recurrent net is the decoder.

81
00:04:34.030 --> 00:04:38.320
It's job is to give it a context vector output.
The associated words,

82
00:04:38.530 --> 00:04:42.250
the type of recurrent net we'll be using is called our long short term memory

83
00:04:42.251 --> 00:04:46.030
network.
This type of network can remember words from far back and the sequence,

84
00:04:46.060 --> 00:04:50.140
and because we're dealing with large sequences are attention mechanism helps the

85
00:04:50.141 --> 00:04:54.430
decoder selectively look at the parts of the sequence that are most relevant for

86
00:04:54.431 --> 00:04:56.230
more accuracy.
So our model,

87
00:04:56.231 --> 00:05:00.490
we'll be able to create context vectors for existing questions and responses and

88
00:05:00.491 --> 00:05:04.060
it'll know to associate a certain type of question with a certain type of

89
00:05:04.061 --> 00:05:06.070
response.
So once we create our model,

90
00:05:06.071 --> 00:05:08.890
we can train it by first creating a tensorflow session,

91
00:05:09.040 --> 00:05:11.320
which will encapsulate our computation graph.

92
00:05:11.620 --> 00:05:15.100
Then we'll initialize our training loop and call our sessions run function,

93
00:05:15.190 --> 00:05:16.660
which we'll run our computation rep,

94
00:05:16.720 --> 00:05:19.780
which is our sequence to sequence model and we'll use it as our parameter.

95
00:05:19.840 --> 00:05:23.860
Now we can save our model periodically during training using the TF trained not

96
00:05:23.861 --> 00:05:27.040
saver function.
This will save our model as a checkpoint file,

97
00:05:27.070 --> 00:05:31.000
which we can later load once we're done training using the savers restore

98
00:05:31.001 --> 00:05:34.180
function.
When we run our program,
it'll take a few hours to fully train.

99
00:05:34.240 --> 00:05:38.170
We can periodically test what kind of responses we get from our Bot in terminal

100
00:05:38.290 --> 00:05:42.160
if we'd like.
And as you can see,
responses are pretty meaningless at first,

101
00:05:42.161 --> 00:05:46.330
but as our model improves through training,
eventually it becomes more coherent.

102
00:05:46.450 --> 00:05:47.620
So to break it down,

103
00:05:47.770 --> 00:05:51.910
deep learning allows us to make chatbots that are way more human lives than any

104
00:05:51.911 --> 00:05:56.350
kind of handcrafted Chat Bot we've made before and to end systems are systems

105
00:05:56.351 --> 00:06:00.620
that allow us to use a single model to give us our desired outcome and we can

106
00:06:00.621 --> 00:06:04.820
use sequence to sequence models using two recurrent neural nets to create

107
00:06:04.821 --> 00:06:06.290
conversational chat box.

108
00:06:06.320 --> 00:06:10.220
The winner of the coding challenge from the last video is Georgie petkoff.

109
00:06:10.460 --> 00:06:14.690
He implemented three different methods to estimate a solution to the traveling

110
00:06:14.691 --> 00:06:19.610
salesman problem and benchmark the results in an eye python notebook that ass of

111
00:06:19.611 --> 00:06:21.470
the week and the runner up is mic.

112
00:06:21.471 --> 00:06:25.820
Then Holst he used both the nearest neighbor and simulated annealing algorithm

113
00:06:25.910 --> 00:06:30.080
to estimate a solution.
The coding challenge for this video is to use TF.

114
00:06:30.110 --> 00:06:34.640
Learn to write a script that generates sentences in the style of Lord of the

115
00:06:34.641 --> 00:06:35.474
rings.

116
00:06:37.250 --> 00:06:40.970
Don't take at most 50 lines of code to write this and details are in the read me

117
00:06:41.180 --> 00:06:42.620
post.
You'll get humbling in the comments,

118
00:06:42.740 --> 00:06:45.560
and I'll announce the winner in my video one week from today.

119
00:06:45.650 --> 00:06:49.520
Please hit that subscribe button for now.
I've got a hack snapchat spectacles,

120
00:06:49.580 --> 00:06:50.900
so thanks for watching.

