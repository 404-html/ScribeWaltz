WEBVTT

1
00:00:00.090 --> 00:00:03.660
Hey therapists,
I can't see you anymore.
I've updated my privacy policy.

2
00:00:05.430 --> 00:00:06.091
Hello world.

3
00:00:06.091 --> 00:00:10.500
It's Saroj and if you're interested in a career in data science,

4
00:00:10.590 --> 00:00:15.590
an easy way for you to have a huge competitive advantage over your peers is to

5
00:00:16.501 --> 00:00:19.860
learn about tools that preserve a user's privacy.

6
00:00:20.130 --> 00:00:25.130
I'll demonstrate an example of what I mean by training a model on sensitive

7
00:00:25.141 --> 00:00:30.141
patient data to predict diabetes while preserving patient and on nymity.

8
00:00:30.660 --> 00:00:34.620
But Let me first explain why privacy matters to you personally.

9
00:00:34.950 --> 00:00:39.810
Online services both big and small have become accustomed to collecting and

10
00:00:39.811 --> 00:00:44.700
sharing user data with little to no restriction like it's some kind of wild,

11
00:00:44.701 --> 00:00:49.320
wild west situation.
But all of that is fast changing.
Just this year,

12
00:00:49.321 --> 00:00:54.321
the European Union enacted the general data protection regulation or Gdpr Gdpr

13
00:00:55.371 --> 00:00:58.950
as a set of laws aimed at protecting the data of EU citizens.

14
00:00:59.250 --> 00:01:03.750
Any online service that has users based in the EU has to comply.

15
00:01:03.810 --> 00:01:08.810
It has provisions like if a service has more than 250 employees or deals with

16
00:01:09.811 --> 00:01:11.340
sensitive personal data,

17
00:01:11.520 --> 00:01:16.520
they require a privacy officer and a service requires affirmative consent.

18
00:01:16.741 --> 00:01:20.130
Before saving any tracking cookies or personal data.

19
00:01:20.400 --> 00:01:25.400
Failure to comply with GDPR could result in fines up to 20 million euros and

20
00:01:25.561 --> 00:01:30.270
some companies are already facing massive penalties.
Facebook,

21
00:01:30.570 --> 00:01:32.280
it's not just the EU,

22
00:01:32.281 --> 00:01:37.281
though 107 countries have now put in place legislation to secure the protection

23
00:01:37.681 --> 00:01:41.400
of personal data for consumers.
This is a good thing.

24
00:01:41.730 --> 00:01:46.320
The reason you have value is your data.
It's how you think.
It's what you eat.

25
00:01:46.380 --> 00:01:47.370
It's who you know.

26
00:01:47.400 --> 00:01:52.350
It's how often you listen to desk by seat though everything about you is data.

27
00:01:52.410 --> 00:01:57.240
Data protection laws aren't going anywhere and we'll see more of them in the

28
00:01:57.240 --> 00:01:57.690
coming years.

29
00:01:57.690 --> 00:02:02.690
A crucial skill to learn then for aspiring data scientists is how to preserve

30
00:02:03.421 --> 00:02:08.190
user privacy,
wild training,
machine learning models on sensitive user data.

31
00:02:08.790 --> 00:02:11.790
We can see some real world applications of this already.

32
00:02:11.910 --> 00:02:16.800
Apple started rolling out privacy techniques in Ios 10 they're able to

33
00:02:16.801 --> 00:02:21.150
anonymously collect user data like the words they type that aren't in the

34
00:02:21.151 --> 00:02:26.151
keyboard dictionary and train models on that to improve user experiences.

35
00:02:26.760 --> 00:02:28.530
Another example is Google.

36
00:02:28.680 --> 00:02:33.680
They collect anonymized data in chrome to do studies on malware and in maps to

37
00:02:34.021 --> 00:02:35.640
help predict traffic jams.

38
00:02:36.140 --> 00:02:40.950
Numerate allows data scientists from around the world to train their models on

39
00:02:40.951 --> 00:02:44.970
encrypted financial data that keeps the clients data private.

40
00:02:45.000 --> 00:02:49.140
There's a huge growing space for startups that respect user privacy.

41
00:02:49.500 --> 00:02:54.420
I'm going to cover three important privacy preserving techniques that when put

42
00:02:54.421 --> 00:02:59.421
together form a powerful pipeline that allows us to train un-encrypted model on

43
00:03:00.551 --> 00:03:04.660
encrypted data from multiple users.
Federated Learning,

44
00:03:04.840 --> 00:03:09.010
secure multiparty computation and differential privacy.

45
00:03:09.280 --> 00:03:12.850
I know it sounds super technical,
but this is a technical channel.

46
00:03:12.940 --> 00:03:16.720
Subscribe if you're new.
By the way,
let's start with federated learning.

47
00:03:16.780 --> 00:03:19.060
Before training a machine learning model,

48
00:03:19.090 --> 00:03:22.390
a data scientist must first collect user data.

49
00:03:22.690 --> 00:03:27.640
Users generate this data using devices like their phone or their laptop by

50
00:03:27.641 --> 00:03:29.740
recording events in the real world.

51
00:03:30.010 --> 00:03:34.840
The way this data is usually aggregated is by following a simple server client

52
00:03:34.841 --> 00:03:39.820
model that has come to define most of the worldwide web data is sent from

53
00:03:39.821 --> 00:03:44.821
multiple user devices to a central server that contains the model and the model

54
00:03:45.491 --> 00:03:47.260
then trains on this data.

55
00:03:47.410 --> 00:03:52.410
We can consider this centralized learning since the learning or training process

56
00:03:52.780 --> 00:03:56.590
occurs on a central server.
Federated Learning,
however,

57
00:03:56.630 --> 00:03:58.480
flippity flips this pipeline.

58
00:03:58.600 --> 00:04:02.920
Instead of bringing training data to the model via a central server,

59
00:04:03.040 --> 00:04:07.600
we bring the model to the training data on whichever machine it's located.

60
00:04:07.660 --> 00:04:12.310
This allows the people who create the data to maintain ownership without sending

61
00:04:12.311 --> 00:04:15.490
it off to a machine that they don't control.
Let's see.

62
00:04:15.491 --> 00:04:19.390
An example of how this works using two python libraries.

63
00:04:19.630 --> 00:04:21.140
The first is called [inaudible],

64
00:04:21.440 --> 00:04:26.410
which is a set of tools that enable encrypted privacy,
preserving deep learning.

65
00:04:26.440 --> 00:04:30.520
Definitely give it a star on get hub.
Trask is a privacy beast and a friend.

66
00:04:30.880 --> 00:04:32.860
The second is called Pi Torch,

67
00:04:32.890 --> 00:04:36.820
which is an open source library for training machine learning networks.

68
00:04:36.850 --> 00:04:39.460
I guess something Nice did come out of Facebook.
After all.

69
00:04:39.520 --> 00:04:41.560
After importing both libraries,

70
00:04:41.561 --> 00:04:45.310
we can create a simple toy datasets in just two lines.

71
00:04:45.460 --> 00:04:49.750
That data and the labels for that data we're using the float tents or attribute

72
00:04:49.751 --> 00:04:52.180
of Pi sift here which creates a tensor.

73
00:04:52.480 --> 00:04:57.480
A tensor by the way is an n dimensional matrix where n is a number we define and

74
00:04:58.421 --> 00:05:01.060
it's the format we feed data to neural networks in.

75
00:05:01.300 --> 00:05:03.890
That's why tensorflow flow is called tensorflow.

76
00:05:03.910 --> 00:05:08.320
Tensors flowed through the computation graph as that a data scientist creates.

77
00:05:08.350 --> 00:05:12.910
We can also create a simple linear neural network model in a single line.

78
00:05:13.000 --> 00:05:13.840
Then when we train,

79
00:05:13.841 --> 00:05:18.160
this model will approximate the ideal weight values for the network through the

80
00:05:18.161 --> 00:05:20.230
process known as gradient descent.

81
00:05:20.440 --> 00:05:23.620
I have at least 4 trillion videos on how that works.

82
00:05:23.650 --> 00:05:25.390
Link will be in the video description.

83
00:05:25.480 --> 00:05:29.500
That's centralized learning now to perform federated learning.

84
00:05:29.501 --> 00:05:31.750
We're going to Redo this example,

85
00:05:31.990 --> 00:05:36.220
but this time we have two separate worker nodes who don't know each other,

86
00:05:36.221 --> 00:05:39.790
Jack and Jill that we want to send our model to train on.

87
00:05:40.120 --> 00:05:44.590
Now we're going to create some training data and send it to both of these nodes.

88
00:05:44.770 --> 00:05:47.170
We'll assume they created it themselves.

89
00:05:47.410 --> 00:05:51.190
The way we can access this training data is by using pointers.

90
00:05:51.460 --> 00:05:56.290
Pi Sift represents pointers to tensors that live on remote machines easily.

91
00:05:56.620 --> 00:05:58.250
Once we have these pointers,

92
00:05:58.280 --> 00:06:01.130
we'll store them in a list and start training our model.

93
00:06:01.250 --> 00:06:05.690
It will iterate through each worker's respective dataset and train the model on

94
00:06:05.691 --> 00:06:06.080
them.

95
00:06:06.080 --> 00:06:10.880
So federated learning allows us to train our models while the training data is

96
00:06:10.881 --> 00:06:13.400
distributed across multiple nodes.

97
00:06:13.460 --> 00:06:18.460
The problem though is that we can easily learn about either jack or Jill's data

98
00:06:18.680 --> 00:06:21.770
by reverse engineering the learned weights of our model.

99
00:06:21.830 --> 00:06:23.660
I knew Joe Pushed Jack Down that hill.

100
00:06:23.720 --> 00:06:28.720
The way to fix this is to use a technique called secure multiparty computation

101
00:06:29.181 --> 00:06:30.440
or SMPC,

102
00:06:30.740 --> 00:06:35.330
which provides the ability to compute values of interest from multiple encrypted

103
00:06:35.331 --> 00:06:39.320
data sources without any party having to reveal their private data.

104
00:06:39.650 --> 00:06:43.910
If we have a value,
we'd like to encrypt it split into multiple shares.

105
00:06:44.000 --> 00:06:46.340
These shares are kind of like a private key.

106
00:06:46.341 --> 00:06:49.520
There'll be distributed amongst two or more owners,

107
00:06:49.880 --> 00:06:52.190
and in order to decrypt the variable,

108
00:06:52.220 --> 00:06:56.450
all the owners have to agree to allow decryption.
So in essence,

109
00:06:56.480 --> 00:06:58.460
everyone has their own private key.

110
00:06:58.580 --> 00:07:03.230
It takes the consensus of every party with a share for us to be able to decrypt

111
00:07:03.260 --> 00:07:04.093
the data.

112
00:07:04.100 --> 00:07:09.100
And by far the coolest part about secure multiparty computation is that it has

113
00:07:09.411 --> 00:07:13.790
the ability to perform computation.
While the variables are encrypted,

114
00:07:14.300 --> 00:07:15.530
meaning we can perform,

115
00:07:15.560 --> 00:07:19.430
add or subtract operations on encrypted data.

116
00:07:19.760 --> 00:07:24.200
This can be done locally or remotely as we can perform operations on encrypted

117
00:07:24.201 --> 00:07:27.020
tensors that live across multiple parties.

118
00:07:27.050 --> 00:07:29.420
The worker nodes we've instantiated in our case,

119
00:07:29.630 --> 00:07:32.870
that's right arithmetic on encrypted values.

120
00:07:33.080 --> 00:07:37.700
We can perform computations on values that we can't even see as NBC.

121
00:07:37.701 --> 00:07:42.140
Techniques protect model weights while allowing multiple worker nodes to take

122
00:07:42.141 --> 00:07:46.460
part in the training process with their own dataset.
You know what that means,

123
00:07:46.461 --> 00:07:47.180
right?

124
00:07:47.180 --> 00:07:51.530
It means that since neural networks are a series of math operations,

125
00:07:51.650 --> 00:07:56.200
we can use SMPC to encrypt a neural network model and have it trained on

126
00:07:56.201 --> 00:07:59.450
encrypted data.
After we create our worker nodes,

127
00:07:59.451 --> 00:08:01.430
we can initialize our datasets,

128
00:08:01.490 --> 00:08:05.330
then encrypt both the model and the data in the training loop,

129
00:08:05.360 --> 00:08:06.320
the encrypted model,

130
00:08:06.321 --> 00:08:10.940
we'll be able to learn from encrypted data distributed across multiple machines.

131
00:08:11.180 --> 00:08:13.970
While SMPC encrypts computation,

132
00:08:14.000 --> 00:08:17.630
differential privacy ensures that went a model.
Learn something.

133
00:08:17.750 --> 00:08:21.920
It only learns what it's supposed to learn without accidentally memorizing

134
00:08:21.921 --> 00:08:24.770
private information contained in a Dataset.

135
00:08:25.070 --> 00:08:29.660
Think of differential privacy like a promise between a data scientist and a data

136
00:08:29.661 --> 00:08:34.640
owner that says that the data owner will not be adversely affected in any way by

137
00:08:34.670 --> 00:08:38.240
allowing their data to be used in the training data set,

138
00:08:38.420 --> 00:08:43.310
no matter what other data might be available about that person.
So for example,

139
00:08:43.311 --> 00:08:48.260
SMPC allows us to train a deep neural network while both the model and the data

140
00:08:48.261 --> 00:08:49.280
are encrypted.

141
00:08:49.610 --> 00:08:53.690
Eventually we'll want to use that model somewhere and either decrypt its future

142
00:08:53.691 --> 00:08:57.000
predictions or perhaps even decrypt the model entirely.

143
00:08:57.090 --> 00:09:01.650
Differential privacy ensures that a model's weights and predictions don't

144
00:09:01.651 --> 00:09:06.120
accidentally disclose information about people from the training data.

145
00:09:06.450 --> 00:09:10.530
If,
for example,
a model is trained on diagnostic measurements,

146
00:09:10.710 --> 00:09:15.360
we want it to be able to predict whether a certain diagnostic readout has

147
00:09:15.361 --> 00:09:20.361
diabetes without accidentally learning whether a specific patient named Stanley

148
00:09:20.790 --> 00:09:22.050
Hudson has diabetes.

149
00:09:22.170 --> 00:09:27.000
Some popular examples of differential privacy techniques include randomizing

150
00:09:27.001 --> 00:09:30.630
user ids,
slightly preturbing numerical values,

151
00:09:30.690 --> 00:09:34.410
and injecting noise into datasets.
Using these techniques.

152
00:09:34.411 --> 00:09:39.411
Together we can create a training pipeline that both keeps users data private

153
00:09:39.660 --> 00:09:43.050
and keeps a models of valuable intelligence privates.

154
00:09:43.560 --> 00:09:45.780
There are three things to remember from this video.

155
00:09:46.050 --> 00:09:50.910
Privacy is what lets you keep your value as a functioning member of society.

156
00:09:51.330 --> 00:09:54.210
We can use a combination of federated learning,

157
00:09:54.420 --> 00:09:56.880
secure multiparty computation,

158
00:09:57.060 --> 00:10:02.040
and differential privacy to create intelligent apps that keep both data and

159
00:10:02.041 --> 00:10:03.240
models private.

160
00:10:03.450 --> 00:10:07.530
And [inaudible] is a library that makes it really easy to implement all of this.

161
00:10:07.680 --> 00:10:09.450
How do you feel about the whole privacy debate?

162
00:10:09.480 --> 00:10:12.510
Let me know in the comment section and please subscribe for more programming

163
00:10:12.511 --> 00:10:16.590
videos.
For now,
I've got to encrypt my wallet,
so thanks for watching.

