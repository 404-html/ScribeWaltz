WEBVTT

1
00:00:00.060 --> 00:00:04.470
Simulation ending cheat code up,
up,
down,
down,
left,
right.

2
00:00:05.880 --> 00:00:10.860
Hello world it Saroj and the AI research lab deepmind recently introduced their

3
00:00:10.861 --> 00:00:13.230
newest AI dubbed Alpha Star.

4
00:00:13.590 --> 00:00:18.060
Alpha star was the first AI to defeat a top professional player in the popular

5
00:00:18.061 --> 00:00:21.270
games.
Starcraft two what they final score of five Oh,

6
00:00:21.510 --> 00:00:25.170
it was an exciting event with both sides uncertain about the outcome,

7
00:00:25.171 --> 00:00:30.171
but in the end team liquid's Mana unbelieve accepted defeat back in 2016 when

8
00:00:30.721 --> 00:00:35.490
deep minds Alpha go ended up beating the world champion at the popular game of

9
00:00:35.520 --> 00:00:40.020
go.
A result that experts thought would take decades to accomplish.

10
00:00:40.290 --> 00:00:44.880
It resulted in a flurry of interest in AI technology,
especially in China.

11
00:00:45.330 --> 00:00:50.040
It use a technique called deep reinforcement learning to assess the value of all

12
00:00:50.041 --> 00:00:52.320
possible positions on the board.

13
00:00:52.500 --> 00:00:57.270
Then use a search algorithm called a Monte Carlo tree search to choose its next

14
00:00:57.271 --> 00:01:00.030
move.
Picking the best one last year,

15
00:01:00.031 --> 00:01:04.470
deep mine again stunned the world with the release of a new algorithm dumped

16
00:01:04.471 --> 00:01:09.471
Alpha fold that was able to predict how proteins fold winning first place in an

17
00:01:09.540 --> 00:01:12.900
annual competition made for that specific purpose.

18
00:01:13.110 --> 00:01:17.520
It proved that AI could be used to help solve hard scientific problems,

19
00:01:17.670 --> 00:01:22.670
that these game environments were indeed just a test bed alpha fold use to

20
00:01:23.041 --> 00:01:27.300
residual neural networks to solve a supervised learning problem,

21
00:01:27.301 --> 00:01:32.280
predicting protein properties from genetic sequences.
The job pattern matching.

22
00:01:32.281 --> 00:01:35.910
You did it again,
but now with the release of Alpha Star,

23
00:01:35.911 --> 00:01:37.500
we've got to ask the question,

24
00:01:37.710 --> 00:01:40.590
what are the real world applications of this victory?

25
00:01:40.980 --> 00:01:45.420
Alpha star is more impressive than Alphago and Alpha fold combined.

26
00:01:45.540 --> 00:01:46.980
Here's why.
First of all,

27
00:01:46.981 --> 00:01:51.810
Star craft has been played in professional sports tournaments for more than 20

28
00:01:51.811 --> 00:01:56.130
years.
It's a Saifai universe realtime strategy game,

29
00:01:56.131 --> 00:02:01.020
and the most common way to play is a one B one tournament over five games to

30
00:02:01.021 --> 00:02:04.770
start.
A player chooses one of three different alien races.

31
00:02:05.010 --> 00:02:09.540
Each race has its own distinct set of characteristics and abilities.

32
00:02:09.780 --> 00:02:14.780
One selected a player starts with a number of worker units each which can gather

33
00:02:14.821 --> 00:02:19.140
basic resources to build more units,
structures and technologies.

34
00:02:19.500 --> 00:02:20.940
Using these creations,

35
00:02:20.941 --> 00:02:25.200
players can harvest even more resources and build increasingly more

36
00:02:25.201 --> 00:02:30.201
sophisticated basis and structures as well as new capabilities to defeat the

37
00:02:30.271 --> 00:02:31.110
other player.

38
00:02:31.620 --> 00:02:35.910
It's like building an entire city with an armed military and as such,

39
00:02:36.090 --> 00:02:38.730
a player has used an intelligent strategy to win.

40
00:02:39.180 --> 00:02:42.960
This involves game theory since there's not a single strategy that will win,

41
00:02:43.140 --> 00:02:47.940
there are multiple paths to success and unlike Ingo there is imperfect

42
00:02:47.941 --> 00:02:51.510
information.
A player will actively discover things by scouting.

43
00:02:51.540 --> 00:02:56.100
It's not all known all at once.
The game takes up to an hour to complete.

44
00:02:56.101 --> 00:03:01.101
So actions taken early in the game may not pay off long term and this requires

45
00:03:01.571 --> 00:03:04.330
longterm planning.
It's also all real time.

46
00:03:04.331 --> 00:03:08.510
So it's not like the alternating turns of go and perhaps the most mind blowing

47
00:03:08.570 --> 00:03:10.180
fact is the action space,

48
00:03:10.420 --> 00:03:13.870
which is the number of possible actions in agent can take at any point.

49
00:03:14.170 --> 00:03:18.040
There are hundreds of different units and buildings that have to be controlled

50
00:03:18.070 --> 00:03:18.903
all at once,

51
00:03:19.030 --> 00:03:24.030
which results in a common editorial explosion of possibilities go had tend to

52
00:03:24.191 --> 00:03:26.350
the 170th possible moves,

53
00:03:26.470 --> 00:03:29.440
which is more possibilities and there are atoms in the universe,

54
00:03:29.710 --> 00:03:34.710
but starcraft tend to the 1685th even Putin doesn't have that many subs.

55
00:03:36.400 --> 00:03:41.230
So back to the real world significance of this discovery.
Dennis Hassabis,

56
00:03:41.231 --> 00:03:46.231
the CEO of deep mind stated that the longterm planning and strategizing ability

57
00:03:46.451 --> 00:03:51.340
that Alpha star exhibits can be used for other longterm planning tasks like

58
00:03:51.520 --> 00:03:55.420
weather,
prediction,
climate modeling,
and language understanding.

59
00:03:55.750 --> 00:03:57.430
That makes total sense,

60
00:03:57.640 --> 00:04:01.630
but what else can we use longterm planning algorithms for?

61
00:04:01.870 --> 00:04:06.790
Can we use them to design swarms of self replicating photosynthetic nanobots to

62
00:04:06.791 --> 00:04:11.650
quickly and cheaply suck pollutants from the atmosphere to save our lungs?

63
00:04:11.830 --> 00:04:16.830
How about longterm strategizing for our own lives on assistant that tells us the

64
00:04:16.841 --> 00:04:20.080
steps we need to take in order to accomplish any goal.

65
00:04:20.081 --> 00:04:23.380
We'd like designing drugs to cure all diseases,

66
00:04:23.500 --> 00:04:28.500
creating new strategies to combat the weaponization of social media generating

67
00:04:28.571 --> 00:04:32.650
better documentation of three blue one Brown's python animation library.

68
00:04:32.890 --> 00:04:37.600
We need to think of radically new ideas and AI can help us do that.

69
00:04:37.870 --> 00:04:39.520
We have to find the right datasets,

70
00:04:39.700 --> 00:04:44.440
properly frame our problem mathematically as well as our objective function and

71
00:04:44.441 --> 00:04:49.300
let it work for us.
So let's move into the architecture of how this worked.

72
00:04:49.360 --> 00:04:53.770
Starting with the data Alpha Star used to distinct data sets.

73
00:04:53.920 --> 00:04:58.270
The first was a series of anonymized game replays from expert players.

74
00:04:58.330 --> 00:05:02.500
The largest set ever released since these games were prerecorded.

75
00:05:02.620 --> 00:05:07.620
The result of every action ever taken was known making this part a supervised

76
00:05:07.901 --> 00:05:08.770
learning problem.

77
00:05:09.130 --> 00:05:14.130
The second data set it used was realtime gameplay versus itself and not just one

78
00:05:14.351 --> 00:05:15.220
or two games,

79
00:05:15.221 --> 00:05:20.221
200 years worth of gameplay sped up because we can speed up time in a game world

80
00:05:20.560 --> 00:05:24.700
on computing devices.
Alpha star got pretty good having burst,

81
00:05:24.701 --> 00:05:28.210
been trained on the game replay Dataset.
Then it became really,

82
00:05:28.211 --> 00:05:31.930
really good after playing against itself and the second Dataset,

83
00:05:32.290 --> 00:05:36.790
the team hasn't yet released a paper and the only indication we have as to what

84
00:05:36.791 --> 00:05:40.600
its architecture could be comes from two sentences.
In the blog post.

85
00:05:40.990 --> 00:05:44.950
In the first sentence it says the neural network architecture applies a

86
00:05:44.951 --> 00:05:49.951
transformer torso to the units combined with a deep LSTM core,

87
00:05:50.350 --> 00:05:54.760
an auto regressive policy had with a pointer network and a centralized value

88
00:05:54.761 --> 00:05:55.594
baseline.

89
00:05:55.750 --> 00:06:00.230
The second architecture specific sentence is that the weight update rule isn't

90
00:06:00.260 --> 00:06:02.720
efficient and novel off policy actor,

91
00:06:02.721 --> 00:06:06.170
critic reinforcement learning algorithm with experience,

92
00:06:06.171 --> 00:06:09.860
replay self imitation learning and policy distillation.

93
00:06:10.130 --> 00:06:11.690
That's a lot of concepts,

94
00:06:11.691 --> 00:06:15.380
so I'm going to explain just three of the most important components here,

95
00:06:15.710 --> 00:06:17.030
the transformer network,

96
00:06:17.210 --> 00:06:21.200
the point or network and the multi-agent reinforcement learning setup.

97
00:06:21.590 --> 00:06:26.210
Then I'll give my best educated guess as to how these three components were put

98
00:06:26.211 --> 00:06:28.030
together to create Alpha star

99
00:06:32.530 --> 00:06:33.363
<v 1>draft.</v>

100
00:06:35.410 --> 00:06:38.140
<v 0>Let's first start with the transformer network.</v>

101
00:06:38.290 --> 00:06:43.150
This was a neural network that was first proposed in a paper titled attention is

102
00:06:43.151 --> 00:06:45.040
all you need by Google brain.

103
00:06:45.430 --> 00:06:49.270
Google wanted to make its language translation system more accurate,

104
00:06:49.390 --> 00:06:54.220
so they used a model that uses a technique called attention to do that as well

105
00:06:54.221 --> 00:06:57.460
as speeding up training time at a high level,

106
00:06:57.490 --> 00:07:02.170
a fully trained to transform our network can take an input in one language and

107
00:07:02.200 --> 00:07:05.530
output it in a different language.
If we look closer,

108
00:07:05.531 --> 00:07:10.000
we'll see that it actually consists of two components on encoder model and a

109
00:07:10.001 --> 00:07:10.900
decoder model.

110
00:07:11.350 --> 00:07:16.030
The encoder is actually a stack of encoders and the decoder is a stack of

111
00:07:16.031 --> 00:07:16.864
decoders.

112
00:07:17.020 --> 00:07:21.670
Each one of these encoders consist of a feed forward neural network and what's

113
00:07:21.671 --> 00:07:23.320
called a self attention mechanism.

114
00:07:23.770 --> 00:07:28.660
The inputs to each end coder thus flows first through the self attention layer.

115
00:07:28.690 --> 00:07:30.040
Then to the feed forward network.

116
00:07:30.250 --> 00:07:34.330
By flows I mean matrix multiplication not flows in the hip hop sense.

117
00:07:34.840 --> 00:07:37.510
The decoder has those two components as well,

118
00:07:37.511 --> 00:07:42.070
but it also has an attention layer that helps it focus on relevant parts of the

119
00:07:42.071 --> 00:07:42.904
input data.

120
00:07:43.120 --> 00:07:48.120
The input data is partitioned and each partition is encoded into a vector using

121
00:07:48.191 --> 00:07:49.510
an embedding algorithm.

122
00:07:49.690 --> 00:07:54.130
Each of these embeddings are independently fed through each layer which allows

123
00:07:54.131 --> 00:07:57.970
for parallelization.
This is called multi-headed attention.

124
00:07:58.330 --> 00:08:03.070
Each vector is split into several heads multiplied by weight major sees the

125
00:08:03.071 --> 00:08:07.660
results are concatenated and multiplied by another weight matrix and that's the

126
00:08:07.661 --> 00:08:10.270
output.
These weight values are trained.

127
00:08:10.271 --> 00:08:14.800
It learns the strength of each vector of its relevance in the larger hole.

128
00:08:15.010 --> 00:08:19.780
It knows what to pay attention to for whatever objective function is defined

129
00:08:20.080 --> 00:08:24.310
rather than naively treating the entire input vector as equally relevant.

130
00:08:24.460 --> 00:08:28.180
It learns which parts are most likely the most relevant.

131
00:08:28.540 --> 00:08:32.570
The decoder components are architected in the same way altogether.

132
00:08:32.590 --> 00:08:35.740
The incoders starts by processing the inputs sequence.

133
00:08:35.980 --> 00:08:40.500
Then the outputs of the top encoder is transformed into a set of attention

134
00:08:40.510 --> 00:08:41.343
vectors.

135
00:08:41.500 --> 00:08:45.610
These are used by each decoder and encoder decoder attention layer,

136
00:08:45.850 --> 00:08:49.840
which helps the decoder focus on appropriate parts of the input sequence.

137
00:08:50.200 --> 00:08:53.770
After the encoding basis over the decoding phase begins.

138
00:08:53.950 --> 00:08:58.950
Each component outputs it's results until finally the result is fed into a

139
00:08:59.251 --> 00:09:03.690
softmax layer which outputs a set of probabilities of likely values through

140
00:09:03.691 --> 00:09:06.540
optimization and knowing what the output should be.

141
00:09:06.720 --> 00:09:10.350
This network will learn how to predict the most likely output.

142
00:09:10.440 --> 00:09:14.430
That's the first component.
Let's now move onto the second component,

143
00:09:14.580 --> 00:09:18.120
the auto regressive policy head with a pointer network.

144
00:09:18.410 --> 00:09:22.860
A pointer network is actually pretty similar to a transformer and that it's also

145
00:09:22.861 --> 00:09:25.230
a sequence to sequence model with attention.

146
00:09:25.500 --> 00:09:30.230
What makes a pointer network unique are two things.
The output of a point.

147
00:09:30.231 --> 00:09:34.500
Your network is discreet and corresponds to positions in an input sequence,

148
00:09:34.800 --> 00:09:39.450
and the number of target classes in each step of the output depends on the

149
00:09:39.451 --> 00:09:41.610
length of the input,
which is variable.

150
00:09:42.030 --> 00:09:46.080
Pointer nets are great for problems like sorting words or numbers,

151
00:09:46.260 --> 00:09:47.100
sorry about will soar.

152
00:09:47.130 --> 00:09:51.600
No one loves you as for its auto regressive policy had an auto aggressive model

153
00:09:51.601 --> 00:09:55.560
is one where every inputs sequence depends not only on the input but also

154
00:09:55.561 --> 00:09:56.790
previous outputs.

155
00:09:57.060 --> 00:10:01.350
So the point your network is an auto regressive model that outputs a policy.

156
00:10:01.710 --> 00:10:05.790
What's the policy you ask?
That brings us to the third and final component,

157
00:10:05.940 --> 00:10:08.850
the multiagent reinforcement learning architecture.

158
00:10:09.330 --> 00:10:13.920
The idea behind reinforcement learning is that in some simulated environment

159
00:10:13.921 --> 00:10:18.060
where time is an important element,
an agent will take an action,

160
00:10:18.270 --> 00:10:22.830
receive a reward,
then transitioned to a new state and repeat this process again.

161
00:10:23.520 --> 00:10:26.640
It learns a policy,
a mapping of states to actions.

162
00:10:26.850 --> 00:10:28.950
When the agent is a deep neural network,

163
00:10:28.980 --> 00:10:32.370
it's considered deep reinforcement learning of which there are many different

164
00:10:32.371 --> 00:10:35.820
architectures when there are multiple agents learning at the same time.

165
00:10:35.910 --> 00:10:37.260
It's called multi-agent.

166
00:10:37.290 --> 00:10:42.090
Deep reinforcement learning and a centralized baseline is a value by which all

167
00:10:42.091 --> 00:10:46.770
these decentralized agents can agree is the standard that they can compare their

168
00:10:46.771 --> 00:10:49.530
learnings against to sync the learning process.

169
00:10:49.620 --> 00:10:52.830
A type of deep reinforcement learning algorithm called actor,

170
00:10:52.831 --> 00:10:57.150
critic has one neural network be a critic and that it measures how good the

171
00:10:57.151 --> 00:11:00.270
action taken was and is value based.
The other,

172
00:11:00.271 --> 00:11:04.830
the actor measures how an agent behaves and it is policy based.

173
00:11:05.010 --> 00:11:09.090
So those are the three major ideas of the system,
a transformer network,

174
00:11:09.120 --> 00:11:13.800
a pointer network,
and a multiagent deep reinforcement learning actor,

175
00:11:13.801 --> 00:11:15.870
critic architecture and swag.

176
00:11:15.930 --> 00:11:20.930
I guess what I'm now going to do is make an educated guess as to how these

177
00:11:20.941 --> 00:11:25.941
components were used together and when the transformer network acts as the

178
00:11:26.131 --> 00:11:27.150
critic network.

179
00:11:27.360 --> 00:11:32.010
The actor network is the pointer network and this coupling is fed input data

180
00:11:32.040 --> 00:11:36.300
from the raw game interface,
which is a list of units and their properties.

181
00:11:36.660 --> 00:11:40.230
The output of it is fed to the point your network,
the point network.

182
00:11:40.260 --> 00:11:41.520
Then using the policy.

183
00:11:41.521 --> 00:11:45.540
It's learned thanks to the transformer network and its own weight values,

184
00:11:45.780 --> 00:11:48.210
outputs an action in the first part of training.

185
00:11:48.300 --> 00:11:52.440
It compared it's actions to those of the gamers they've watched during all those

186
00:11:52.440 --> 00:11:55.060
hours of free play,
rewarding or punishing.

187
00:11:55.061 --> 00:11:58.360
It's weight values depending on how similar they were.

188
00:11:58.420 --> 00:12:02.770
Then in the second phase where it played against itself at each iteration,

189
00:12:03.010 --> 00:12:05.380
new versions of itself were replicated.

190
00:12:05.590 --> 00:12:10.330
All of them learning each of them exploring the huge action space of starcraft

191
00:12:10.331 --> 00:12:14.050
game play while ensuring that each competitor performed well against the

192
00:12:14.051 --> 00:12:18.220
strongest strategies and doesn't forget how to defeat earlier ones.
In the end,

193
00:12:18.221 --> 00:12:23.221
the most strategically sound agent is chosen as the final fully trained model to

194
00:12:23.321 --> 00:12:24.370
train Alpha Star.

195
00:12:24.370 --> 00:12:29.370
They built a distributed training setup running for 14 days using 16 TPU for

196
00:12:30.310 --> 00:12:34.930
each agent and the final Alpha Star agent contain the most optimal mixture of

197
00:12:34.931 --> 00:12:39.670
strategies that were discovered and was able to run on a single desktop GPU.

198
00:12:40.150 --> 00:12:44.650
As you can see,
2019 is already turning out to be an exciting year for AI.

199
00:12:44.920 --> 00:12:48.160
There are three things to remember from this video.
Deep minds.

200
00:12:48.190 --> 00:12:53.110
Alpha star Algorithm beat one of the top starcraft two players in the world.

201
00:12:53.350 --> 00:12:56.470
A feet.
Many experts thought would take much longer.

202
00:12:56.890 --> 00:13:00.190
Alpha star used deep reinforcement learning to do so.

203
00:13:00.280 --> 00:13:05.050
The defacto technique for AI in game environments and this technology has the

204
00:13:05.051 --> 00:13:10.051
potential to help us solve problems that require predictions over very longterm

205
00:13:10.210 --> 00:13:13.000
sequences in.
What's a problem you want to solve with AI?

206
00:13:13.030 --> 00:13:16.390
Let me know in the comment section and please subscribe for more technology

207
00:13:16.391 --> 00:13:20.470
videos.
For now,
I'm going to play starcraft,
so thanks for watching.

