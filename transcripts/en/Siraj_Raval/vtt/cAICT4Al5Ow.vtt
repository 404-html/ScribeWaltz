WEBVTT

1
00:00:00.060 --> 00:00:01.530
How do we classify things?

2
00:00:01.740 --> 00:00:05.610
We consider people to be experts in a field if they've mastered classification,

3
00:00:06.000 --> 00:00:08.920
doctors can classify between a good blood sample in a bad way.

4
00:00:09.270 --> 00:00:12.690
Photographers can classify if their latest shot was beautiful or not.

5
00:00:12.990 --> 00:00:16.680
Musicians can classify what sounds good and what doesn't.
In a piece of music,

6
00:00:17.010 --> 00:00:20.550
the ability to classify well,
it takes many hours of training.

7
00:00:20.820 --> 00:00:24.450
We get it wrong over and over again until eventually we get it right.

8
00:00:24.690 --> 00:00:28.500
But with a quality Dataset,
deep learning can classify just as well,

9
00:00:28.560 --> 00:00:32.880
if not better than we can.
We'll use it as a tool to improve our craft,

10
00:00:32.910 --> 00:00:36.450
whatever it is.
And if a job has been not in this,
it'll do it for us.

11
00:00:36.600 --> 00:00:39.180
When we reached the point where we aren't forced to do something,

12
00:00:39.181 --> 00:00:43.050
we don't want to just to survive will flourish like never before.

13
00:00:43.200 --> 00:00:45.030
And that's the world we're aiming for.

14
00:00:45.330 --> 00:00:45.991
<v 1>Hello world,</v>

15
00:00:45.991 --> 00:00:50.430
it's Saroj and today we're going to build an image classifier from scratch to

16
00:00:50.431 --> 00:00:55.290
classify cats and dogs.
Finally we get to work with images I'm filling,

17
00:00:55.291 --> 00:00:56.730
hyping up to do to block arena.

18
00:00:58.920 --> 00:01:01.950
So how does image classification work?
Well,

19
00:01:01.980 --> 00:01:05.400
there were a bunch of different attempts in the eighties and early nineties and

20
00:01:05.401 --> 00:01:07.380
all of them tried to a similar approach.

21
00:01:07.710 --> 00:01:11.400
Think about the features that make up an image and hand code detectors for each

22
00:01:11.401 --> 00:01:13.770
of them,
but there is so much variety out there.

23
00:01:13.771 --> 00:01:18.030
No two apples look exactly the same,
so the results were always terrible.

24
00:01:18.270 --> 00:01:20.730
This was considered a task only we humans can do,

25
00:01:21.030 --> 00:01:24.630
but in 98 or researcher named young lacount introduced a model called a

26
00:01:24.631 --> 00:01:29.550
convolutional neural network capable of classifying characters with a 99%

27
00:01:29.551 --> 00:01:31.500
accuracy which broke every record.

28
00:01:31.770 --> 00:01:36.770
Macoun CNN learned features by itself in 2012 it was used by another researcher

29
00:01:37.060 --> 00:01:40.200
named Alex for Shefsky at the yearly image net competition,

30
00:01:40.410 --> 00:01:44.130
which is basically the annual Olympics of computer vision and it was able to

31
00:01:44.131 --> 00:01:49.131
classify thousands of images with a new record accuracy at the time of 85% since

32
00:01:50.041 --> 00:01:54.240
then,
CNN had been adopted by Google to identify photos and search Facebook for

33
00:01:54.241 --> 00:01:57.600
automatic tagging.
Basically,
they are very hot right now,

34
00:01:57.930 --> 00:02:01.180
but where did the idea for CNN has come from you?

35
00:02:01.290 --> 00:02:04.050
These cabinets all be inspired by a cortex.

36
00:02:04.110 --> 00:02:06.580
Every single thing we see and just the board text.

37
00:02:06.690 --> 00:02:08.910
Some cells fire up the lines and stuff aside.

38
00:02:09.240 --> 00:02:13.920
Neurons in a column organize alive and this helps us class of why what we see.

39
00:02:14.160 --> 00:02:17.400
Cats,
dogs,
pigs,
man,
even Broccoli comp.
Next,

40
00:02:17.401 --> 00:02:22.260
learn dope ass doses from features.
Cross it by me as the king of tee shirts.

41
00:02:22.660 --> 00:02:26.950
Well first one to download our image Dataset from cackle with 1,024 pictures of

42
00:02:26.951 --> 00:02:29.140
dogs and cats.
Each and its own folder.

43
00:02:29.410 --> 00:02:32.320
We'll be using the care os deep learning library for this demo,

44
00:02:32.560 --> 00:02:35.320
which is a high level wrapper that runs on top of tensorflow.

45
00:02:35.560 --> 00:02:39.670
It makes building models really intuitive since we can define each layer as its

46
00:02:39.671 --> 00:02:41.770
own line of code.
First thing's first,

47
00:02:41.771 --> 00:02:44.620
we'll initialize variables for our training and validation data.

48
00:02:44.980 --> 00:02:46.450
Then we're ready to build our model.

49
00:02:46.720 --> 00:02:49.480
We'll initialize a type of model using the sequential function,

50
00:02:49.690 --> 00:02:53.620
which will allow us to build a linear stack of layers so we treat each layer as

51
00:02:53.621 --> 00:02:55.630
an object that feeds data to the next one.

52
00:02:55.870 --> 00:03:00.870
It's like a Conga line kind of know the would be a graph model which would allow

53
00:03:01.271 --> 00:03:05.290
for multiple separate inputs and outputs,
but we're using a more simple example.

54
00:03:05.620 --> 00:03:08.230
Next,
we'll add our first layer,
the convolutional layer.

55
00:03:08.440 --> 00:03:11.560
The first layer of a CNN is always the convolutional layer.

56
00:03:11.800 --> 00:03:15.730
The input is going to be a 32 by 32 by three herea pixel values.

57
00:03:16.000 --> 00:03:18.220
The three refers to RGB values.

58
00:03:18.520 --> 00:03:22.840
Each of the numbers in disarray is given a value from zero to two 55 which

59
00:03:22.841 --> 00:03:25.240
describes the pixel intensity at that point.

60
00:03:25.570 --> 00:03:27.550
The idea is that given this as an input,

61
00:03:27.700 --> 00:03:31.480
our CNN will describe the probability of it being of a certain class.

62
00:03:31.750 --> 00:03:36.070
We can imagine the convolutional layer as a flashlight shining over the top left

63
00:03:36.071 --> 00:03:40.300
of the image.
The flashlights slides across all the areas of the input image.

64
00:03:40.570 --> 00:03:44.230
The flashlight is our filter and the region it shines over is the receptive

65
00:03:44.231 --> 00:03:47.140
field.
Our filter is also an array of numbers.

66
00:03:47.380 --> 00:03:49.840
These numbers are weights had a particular layer.

67
00:03:50.200 --> 00:03:54.820
We can think of that filter as a feature identifier as their filters slides or

68
00:03:54.910 --> 00:03:56.230
involves around the input.

69
00:03:56.410 --> 00:04:00.040
It is multiplying its values with the pixel values in the image.

70
00:04:00.250 --> 00:04:02.710
These are called element wise multiplications.

71
00:04:02.950 --> 00:04:06.940
The multiplications from each region are then summed up and after we've covered

72
00:04:06.970 --> 00:04:09.520
all parts of the image or left the feature map,

73
00:04:09.820 --> 00:04:11.830
this will help us find not buried treasure,

74
00:04:11.831 --> 00:04:14.740
but a prediction which is even better.

75
00:04:14.950 --> 00:04:16.960
Since our weights are randomly initialized,

76
00:04:17.110 --> 00:04:20.680
our filter won't start off being able to detect any specific feature,

77
00:04:20.860 --> 00:04:24.460
but during training,
our CNN will learn values for its filters.

78
00:04:24.670 --> 00:04:28.510
So this first one will learn to detect a low level feature like curves.

79
00:04:28.660 --> 00:04:31.540
So if we placed this filter on a part of the image with a curve,

80
00:04:31.810 --> 00:04:35.770
the resulting value from the multiplication and summation is a big number.

81
00:04:36.130 --> 00:04:38.920
But if we place it on a different part of the image without a curb,

82
00:04:39.100 --> 00:04:43.720
the resulting value is zero.
This is how filters detect features.
Well,

83
00:04:43.721 --> 00:04:44.451
next past,

84
00:04:44.451 --> 00:04:48.790
this feature map through an activation layer called relu or rectified linear

85
00:04:48.791 --> 00:04:51.310
unit.
Relu is probably the name of some alien,

86
00:04:51.311 --> 00:04:56.020
but it's also a nonlinear operation that replaces all the negative pixel values

87
00:04:56.021 --> 00:04:59.260
in the feature map.
With zero,
we could use other functions,

88
00:04:59.261 --> 00:05:02.080
but relu tends to perform better in most situations.

89
00:05:02.410 --> 00:05:05.470
This layer increases the nonlinear properties of our model,

90
00:05:05.471 --> 00:05:09.520
which means our neural net will be able to learn more complex functions than

91
00:05:09.521 --> 00:05:13.870
just linear regression.
After that,
we'll initialize our max pooling layer.

92
00:05:14.200 --> 00:05:17.170
Pooling reduces the dimensionality of each feature map,

93
00:05:17.320 --> 00:05:19.690
but retains the most important information.

94
00:05:20.050 --> 00:05:23.050
This reduces the computational complexity of our network.

95
00:05:23.440 --> 00:05:26.170
There are different types,
but in our case we'll use MACs,

96
00:05:26.290 --> 00:05:29.080
which takes the largest element from the rectified feature map.

97
00:05:29.260 --> 00:05:33.460
Within a window we define and we'll slide this window over each region of our

98
00:05:33.461 --> 00:05:37.750
feature map,
taking the Max values,
so a classic CNN architecture.

99
00:05:37.751 --> 00:05:38.740
It looks like this.

100
00:05:38.980 --> 00:05:42.130
Three convolutional blocks followed by a fully connected layer.

101
00:05:42.400 --> 00:05:44.110
We've initialize the first three layers.

102
00:05:44.260 --> 00:05:46.930
We can basically just repeat this process twice more.

103
00:05:47.170 --> 00:05:51.370
The output feature map is fed into the next convolutional layer and the filter.

104
00:05:51.371 --> 00:05:55.780
In this layer we'll learn to detect more abstract features like pause and dose.

105
00:05:56.080 --> 00:05:59.820
One technique will use to prevent over fitting that point when our model isn't

106
00:05:59.821 --> 00:06:03.140
able to predict labels for novel data is called dropout.

107
00:06:03.440 --> 00:06:08.120
A dropout layer drops out a random set of activations in that layer by setting

108
00:06:08.121 --> 00:06:10.160
them to zero as data flows through it.

109
00:06:10.400 --> 00:06:14.300
To prepare our data for the dropout or first flatten the feature map into one

110
00:06:14.301 --> 00:06:14.990
dimension.

111
00:06:14.990 --> 00:06:18.440
Then we'll want to initialize a fully connected layer with the dense function

112
00:06:18.620 --> 00:06:21.050
and apply relu to it.
After dropout,

113
00:06:21.051 --> 00:06:23.390
we'll initialize one more fully connected layer.

114
00:06:23.660 --> 00:06:27.710
This will output an end dimensional vector where n is the number of classes we

115
00:06:27.711 --> 00:06:31.040
have,
so it would be too,
and by applying a sigmoid to it,

116
00:06:31.130 --> 00:06:34.070
it will convert the data to probabilities for each class.

117
00:06:34.280 --> 00:06:36.380
So how does our network learn?
Well,

118
00:06:36.420 --> 00:06:40.430
one to minimize a loss function which measures the difference between the target

119
00:06:40.431 --> 00:06:43.070
output and the expected output.
To do this,

120
00:06:43.071 --> 00:06:47.060
we'll take the derivative of the loss with respect to the weights in each layer.

121
00:06:47.240 --> 00:06:51.110
Starting from the last to compute the direction we want our network to update

122
00:06:51.410 --> 00:06:54.140
will propagate our loss backwards for each layer.

123
00:06:54.350 --> 00:06:58.220
Then we'll update our wake values for each filter so they can change in the

124
00:06:58.221 --> 00:07:00.800
direction of the gradient.
That will minimize our loss.

125
00:07:01.220 --> 00:07:04.160
We can configure the learning process by using the compile method,

126
00:07:04.370 --> 00:07:07.490
what we'll define our loss as binary cross entropy,

127
00:07:07.640 --> 00:07:11.120
which is the preferred loss function for binary classification problems.

128
00:07:11.330 --> 00:07:16.330
Then our optimizer rms prop which will perform gradient descent and a list of

129
00:07:16.341 --> 00:07:20.060
metrics which we'll set to accuracy since this is a classification problem.

130
00:07:20.450 --> 00:07:23.180
Lastly,
we'll write out our fit function to train the model,

131
00:07:23.181 --> 00:07:26.790
giving it parameters for the training and validation data as well as a number of

132
00:07:26.810 --> 00:07:30.710
epochs to run for each and let's save our weights so we can use our trained

133
00:07:30.711 --> 00:07:31.544
model later.

134
00:07:31.940 --> 00:07:36.710
Overall accuracy comes to be about 70% similar to my attention span and if we

135
00:07:36.711 --> 00:07:39.110
feed our model a new picture of a dog or cat,

136
00:07:39.410 --> 00:07:41.720
it will predict it's labeled relatively accurately.

137
00:07:42.170 --> 00:07:45.590
We could definitely improve our prediction though by either using more pictures

138
00:07:45.830 --> 00:07:50.420
or by augmenting an existing pretrained network with our own network which is

139
00:07:50.421 --> 00:07:53.120
considered transfer learning.
So to break it down,

140
00:07:53.210 --> 00:07:58.070
convolutional neural network are inspired by the human visual cortex and offer

141
00:07:58.071 --> 00:08:02.690
state of the art and image classification CNNs Lauren filters each convolutional

142
00:08:02.691 --> 00:08:05.750
layer that act as increasingly abstract feature detectors.

143
00:08:06.020 --> 00:08:09.020
And with Karastan tensorflow you can build your pretty easily.

144
00:08:09.200 --> 00:08:12.500
But winter of the coding challenge from the last video is Charles David Blot.

145
00:08:12.830 --> 00:08:16.400
He use tensor flow to build a deep net capable of predicting whether or not

146
00:08:16.401 --> 00:08:17.690
someone would get a match or not.

147
00:08:17.870 --> 00:08:21.620
After training on a Dataset and had a pretty three data visualization of his

148
00:08:21.621 --> 00:08:25.550
results wizard of the week and the runner up is la man got clean,

149
00:08:25.610 --> 00:08:27.200
organized and documented code.

150
00:08:27.410 --> 00:08:31.070
The coding challenge for this video is to create an image classifier for two

151
00:08:31.071 --> 00:08:33.690
types of animals.
Instructions are in the read me poster,

152
00:08:33.700 --> 00:08:36.590
get humbling in the comments and I'll announce a winner next Friday.

153
00:08:36.770 --> 00:08:38.840
Please subscribe.
If you want to see more videos like this,

154
00:08:38.841 --> 00:08:42.350
check out this related video.
And for now I've got to upload my mind,

155
00:08:42.650 --> 00:08:43.880
so thanks for watching.

