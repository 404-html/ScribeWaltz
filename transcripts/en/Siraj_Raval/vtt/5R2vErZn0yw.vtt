WEBVTT

1
00:00:00.600 --> 00:00:02.190
Hello world,
it's the Raj.

2
00:00:02.220 --> 00:00:06.720
And how does an AI get from point a to point B in a game world?

3
00:00:06.930 --> 00:00:11.820
That's the question that we're going to answer today using the magic of dynamic

4
00:00:11.821 --> 00:00:13.110
programming,
okay?

5
00:00:13.290 --> 00:00:17.400
This is a type of reinforcement learning and we're going to apply it to the open

6
00:00:17.401 --> 00:00:21.570
AI frozen lake environment that you see behind me.
Basically,

7
00:00:21.750 --> 00:00:25.860
it's a really rough texts version of a frozen lake.
And we've got,

8
00:00:25.861 --> 00:00:26.820
we've got an AI,

9
00:00:26.821 --> 00:00:30.840
an agent that's got to get from the starting point to the ending point without

10
00:00:30.841 --> 00:00:35.190
falling into a hole,
okay?
And there's a,
there's an optimal way of doing this,

11
00:00:35.191 --> 00:00:35.690
right?
There's a,

12
00:00:35.690 --> 00:00:40.690
there's an optimal path that this agent can take such that it doesn't fall into

13
00:00:40.741 --> 00:00:44.790
any holes and it gets from point a to point B as fast as possible.

14
00:00:45.060 --> 00:00:49.050
And we're going to use dynamic programming to help us do that.
Okay?

15
00:00:49.051 --> 00:00:50.700
So let's get right on into it.

16
00:00:50.790 --> 00:00:53.730
The first thing I want to talk about is this environment.

17
00:00:53.790 --> 00:00:58.650
So this is what it looks like,
right?
So you've got,
you've got a grid,

18
00:00:58.651 --> 00:00:58.801
right?

19
00:00:58.801 --> 00:01:03.600
It's a three by four grid or a four by four grid and it represents a frozen
lake.

20
00:01:03.720 --> 00:01:08.430
And we have a,
we have a series of letters,
right?
So s means the starting point,

21
00:01:08.460 --> 00:01:11.670
that's where our agent starts.
F means frozen.

22
00:01:11.671 --> 00:01:14.670
That means that an agent can totally walk on it,
slide on it,

23
00:01:14.671 --> 00:01:18.540
do whatever you want.
It's a frozen lake,
right?
H means a whole,

24
00:01:18.570 --> 00:01:22.020
you do not want to fall into the hole.
And then g means goal.

25
00:01:22.021 --> 00:01:23.370
That's where the agent wants to be.

26
00:01:23.460 --> 00:01:27.750
So it's the goal for the agent is to just travel through all these squares until

27
00:01:27.751 --> 00:01:30.570
it hits g without touching any of the H's.

28
00:01:30.810 --> 00:01:33.360
And it's going to do this through a process of trial and error,

29
00:01:33.361 --> 00:01:38.220
Aka reinforcement learning.
So how do we frame this problem?
Right?

30
00:01:38.221 --> 00:01:43.221
So the mark of decision process is the most used way of,

31
00:01:44.230 --> 00:01:47.400
of formally defining an environment with an agent.

32
00:01:47.460 --> 00:01:49.740
It's the most used way of formally,

33
00:01:49.770 --> 00:01:53.190
mathematically defining a problem setting where we have an agent,

34
00:01:53.370 --> 00:01:54.510
we have an environment,

35
00:01:54.660 --> 00:01:58.170
we have a set of actions that this agent can take in this environment.

36
00:01:58.410 --> 00:02:03.030
And we have a set of states that will occur sequentially as the agent takes new

37
00:02:03.031 --> 00:02:06.720
actions in this environment.
The reason that we define it as such,

38
00:02:06.750 --> 00:02:07.980
instead of just saying,
yeah,

39
00:02:07.981 --> 00:02:10.140
we have an agent and it's in this game world and yeah,

40
00:02:10.141 --> 00:02:13.770
it's going to do some things is because just mathematics is beautiful,
right?

41
00:02:13.771 --> 00:02:14.121
It's,
it's,

42
00:02:14.121 --> 00:02:19.121
it's a way of describing all of these different processes as variables and what

43
00:02:19.291 --> 00:02:22.680
can we do when we break down these processes into variables?
Well,

44
00:02:22.681 --> 00:02:25.620
we can perform all sorts of operations on these variables.

45
00:02:25.740 --> 00:02:28.200
We can find relations between these variables.

46
00:02:28.350 --> 00:02:33.210
We can create formulas and equations that tell us what the optimal value

47
00:02:33.211 --> 00:02:35.700
function for example,
or policy for example,

48
00:02:35.790 --> 00:02:39.450
what are the best set of rules for an agent to perform in order to complete this

49
00:02:39.451 --> 00:02:43.830
objective?
That's the reason that we use what's called a Markov decision process,

50
00:02:43.950 --> 00:02:47.430
which is a mathematical way of describing all the relations between these

51
00:02:47.431 --> 00:02:51.780
variables because it makes things easier later on is we're trying to construct

52
00:02:51.990 --> 00:02:54.450
these functions,
these equations later on.

53
00:02:54.900 --> 00:02:58.230
So in RL,
in reinforcement learning,

54
00:02:58.231 --> 00:03:02.440
we have an agent that's interacting with an environment in some way and at each

55
00:03:02.441 --> 00:03:05.920
time step every second or whatever interval this agent,

56
00:03:05.950 --> 00:03:10.690
our AI is performing some action,
jumping,
moving forward,
hitting a ball,
whatever.

57
00:03:10.960 --> 00:03:15.100
It's going to lead to two things.
One,
it's going to change the environment state.

58
00:03:15.160 --> 00:03:18.070
That means that,
okay,
so Mario moved forward one inch.

59
00:03:18.160 --> 00:03:20.200
The new state is the environment with Mario.

60
00:03:20.201 --> 00:03:25.201
One step forward and the agent possibly receiving a reward or penalty from the

61
00:03:25.541 --> 00:03:29.290
environment that you fall in the hole minus one that you not fall into a hole.

62
00:03:29.291 --> 00:03:33.760
Did you get a coin plus one?
Right?
And the goal of the agent is this governor,

63
00:03:33.761 --> 00:03:37.720
the optimal policy.
So that is the word of the day policy.
Okay,

64
00:03:37.721 --> 00:03:42.190
so policy means what are the actions to do in each state,
right?

65
00:03:42.191 --> 00:03:43.024
That's a policy.

66
00:03:43.150 --> 00:03:46.780
What are the set of actions that this agent could do in each state?

67
00:03:46.781 --> 00:03:49.600
So you could think of it as a table that this agent is looking at.
It's like,

68
00:03:49.601 --> 00:03:53.140
okay,
this is the state of the game.
Oh,
this is the action that I should perform.

69
00:03:53.170 --> 00:03:56.290
Let me do that.
Right?
And we want to learn the optimal policy.

70
00:03:56.290 --> 00:04:01.030
That means the best actions for that given state,
such that the agent is,

71
00:04:01.150 --> 00:04:05.740
uh,
completing its objective as efficiently as possible,
um,

72
00:04:06.040 --> 00:04:09.880
to be more specific,
to maximize the future,
the total future discounted reward.

73
00:04:10.030 --> 00:04:10.863
But we'll get to that.

74
00:04:11.590 --> 00:04:15.430
So mark of decision processes are a way of describing this agent environment

75
00:04:15.431 --> 00:04:19.000
loop in a formal way.
We have [inaudible],
which is a set of states,

76
00:04:19.030 --> 00:04:23.620
the state of the world,
right?
And we can describe states in any number of ways,

77
00:04:23.621 --> 00:04:26.830
right?
The number of Coupas,
the number of,
you know,
uh,

78
00:04:26.860 --> 00:04:30.460
bots that the layout of the grid,
do you know the state,
right?

79
00:04:30.461 --> 00:04:33.550
This is a very abstract way of thinking about all these problems.

80
00:04:33.551 --> 00:04:37.060
That's why it's so generalized.
That's why it's used.
So often we have a,

81
00:04:37.061 --> 00:04:38.050
a set of actions.

82
00:04:38.230 --> 00:04:43.230
We have the probability of a given state given us an action and a previous

83
00:04:43.571 --> 00:04:47.320
state.
This is called our transition function and we want to estimate it,
right?

84
00:04:48.040 --> 00:04:49.960
What is the most likely the next state given,
uh,

85
00:04:50.110 --> 00:04:52.450
an agent takes a certain action in a certain state.

86
00:04:53.080 --> 00:04:55.720
We have our starting state distribution,
which we can skip for now.

87
00:04:55.721 --> 00:04:58.870
We don't need that.
We have a discount factor and we have a reward.

88
00:04:58.900 --> 00:05:02.410
So the reward is the value given some action in a given state,

89
00:05:02.590 --> 00:05:06.400
do we get a plus one?
Do we get a minus one?
Now sometimes right?

90
00:05:06.550 --> 00:05:08.500
Sometimes a reward,

91
00:05:09.520 --> 00:05:12.890
sometimes doing some action will give us a reward.
Uh,

92
00:05:13.010 --> 00:05:17.320
where as if we perform a different action entirely,
we get a longer term reward.

93
00:05:17.321 --> 00:05:19.870
What do I mean?
I mean,
let's say I'm Mario,
right?

94
00:05:19.871 --> 00:05:23.530
And I'm in this game world and I see a star and the star is bouncing towards me

95
00:05:23.800 --> 00:05:25.810
and instead of treating the star,
I choose to,

96
00:05:25.870 --> 00:05:29.290
I choose to step on the Koopa so I get a plus one.
So that's,

97
00:05:29.350 --> 00:05:33.820
that's optimizing for short term reward.
But if instead I decide,
let me,

98
00:05:33.850 --> 00:05:37.180
let me instead get the star and not the coupon,
if I get the star,

99
00:05:37.181 --> 00:05:38.410
I'm not getting any plus one,

100
00:05:38.620 --> 00:05:41.590
but it means that I'm more likely to get plus ones later on.

101
00:05:41.591 --> 00:05:44.230
So instead of optimizing for the short term reward,

102
00:05:44.500 --> 00:05:46.480
I'd be optimizing for the longterm award.

103
00:05:46.660 --> 00:05:51.070
So the discount doctor tells us it's a way of weighing what,
uh,
what,

104
00:05:51.071 --> 00:05:56.020
how,
um,
how optimal and action will be to,
to maximize for reward.

105
00:05:56.470 --> 00:05:59.630
And the agent learns this,
right?
It,

106
00:05:59.631 --> 00:06:03.170
Lauren's what the optimal discount factor will be,
right?

107
00:06:03.171 --> 00:06:07.460
So what we're trying to compute is this policy.
So policies are generally,

108
00:06:07.550 --> 00:06:09.760
they are denoted like this pie symbols.
And the,

109
00:06:09.761 --> 00:06:13.970
the idea is that the policy is a function that takes a current environment state

110
00:06:14.240 --> 00:06:17.330
to return an action.
And it's a noted by this symbol,
right?

111
00:06:17.690 --> 00:06:21.320
So let's differentiate between the two different types of environments that we

112
00:06:21.321 --> 00:06:22.154
could have.

113
00:06:22.640 --> 00:06:26.330
So we could have a deterministic environment or we could have a stochastic

114
00:06:26.331 --> 00:06:27.020
environment.

115
00:06:27.020 --> 00:06:30.050
These are two words that you should just know in general for machine learning

116
00:06:30.051 --> 00:06:32.720
because they're used all over the place.
It's really simple.

117
00:06:32.840 --> 00:06:35.360
Deterministic means you can predict what's going to happen.

118
00:06:35.450 --> 00:06:39.290
It's the cast sick needs.
You can't,
it's random.
Simple as that.
Uh,

119
00:06:39.320 --> 00:06:42.410
but to be a little more specific,
in a deterministic environment,

120
00:06:42.411 --> 00:06:46.370
the next state is completely determined by the current state and the actions

121
00:06:46.371 --> 00:06:49.640
performed by the agent.
That means the agent has a,

122
00:06:50.120 --> 00:06:53.360
has a real effect on what's going to happen in the environment.

123
00:06:53.780 --> 00:06:57.050
Whereas in it's the Catholic environment,
which is real life,
right?

124
00:06:57.051 --> 00:07:00.740
We can't predict what's going to happen,
right?
And outside of a vacuum,

125
00:07:01.310 --> 00:07:04.190
that doesn't matter what an agent does that the environment's going to do,

126
00:07:04.191 --> 00:07:06.200
whatever it's going to do,
you can't predict it,
right?

127
00:07:06.200 --> 00:07:10.910
So clearly it's the CASAC environments are a little harder to learn the optimal

128
00:07:10.911 --> 00:07:14.720
policy for,
but deterministic ones are easier.
So in this video,

129
00:07:14.721 --> 00:07:19.190
we're just going to focus on deterministic environments,
right?

130
00:07:19.191 --> 00:07:21.950
So like I said earlier,
I kind of hinted at this.
This is the,

131
00:07:22.010 --> 00:07:26.630
this is the formula for the total discounted reward.
Okay?
So don't give,

132
00:07:26.640 --> 00:07:29.210
don't get afraid by this math where I'm going to go over it.
And the second.

133
00:07:29.330 --> 00:07:34.250
So the goal of the agent is to pick the best policy that will maximize the total

134
00:07:34.251 --> 00:07:37.430
rewards received from the environment.
So let's,
let's see what this is.

135
00:07:37.580 --> 00:07:38.840
This is a sigma notation.

136
00:07:38.841 --> 00:07:42.860
What this means is it's the sum of all of these values right here together.

137
00:07:43.010 --> 00:07:47.120
So where I equals one up until t or t is the horizon,

138
00:07:47.121 --> 00:07:49.610
the episode length,
right?
Which can be infinity,

139
00:07:49.820 --> 00:07:53.030
write an episode for a game like you know,
from start to finish.

140
00:07:53.420 --> 00:07:56.060
Let's take this discounted reward times the reward.

141
00:07:56.150 --> 00:07:58.730
So what this would look like if we were to expand this equation,

142
00:07:58.940 --> 00:08:01.520
it would be a discount times or award plus a discount,

143
00:08:01.521 --> 00:08:04.840
tons of reward plus a discount times or award,
right?
For,
for,
for,

144
00:08:04.940 --> 00:08:08.750
for each I value.
And so we sum all those rewards up and they're,

145
00:08:08.760 --> 00:08:11.360
this count generally makes the reward smaller and smaller and smaller.

146
00:08:11.510 --> 00:08:14.450
The farther and farther we go into the future or we can flip that and they could

147
00:08:14.451 --> 00:08:15.680
go larger and larger and larger.

148
00:08:16.220 --> 00:08:19.640
And this is what we're trying to maximize for whizzy.
What is the,

149
00:08:20.360 --> 00:08:24.140
we're trying to optimize for what,
how do we,
how do we maximize this value?

150
00:08:24.260 --> 00:08:28.940
What do we optimize for to maximize for this value?
So okay,

151
00:08:28.941 --> 00:08:33.140
so the solution to unmark of decision process is called a policy,
right?

152
00:08:33.320 --> 00:08:36.590
And it's simply specifies the best action to take for each of the states.

153
00:08:37.100 --> 00:08:39.830
But although the policy is what we're after,

154
00:08:39.910 --> 00:08:44.900
what we're actually going to compute is a value function because we can easily

155
00:08:44.901 --> 00:08:48.290
derive the policy from the value function,
right?

156
00:08:48.291 --> 00:08:50.510
So a value function is similar to a policy,

157
00:08:50.810 --> 00:08:53.630
except instead of specifying an action for each state,

158
00:08:53.930 --> 00:08:57.600
it specifies a numerical value for each state,
right?

159
00:08:57.601 --> 00:09:00.750
So policies are very straightforward,
right?
It's,
we're given state,

160
00:09:00.780 --> 00:09:02.490
this is the optimal action we should take.

161
00:09:02.850 --> 00:09:07.680
But for the value function tells us a just a numerical value for each state and

162
00:09:07.681 --> 00:09:11.400
using that value,
we can compute what the optimal policy should be.

163
00:09:11.670 --> 00:09:14.340
So it's kind of,
it's,
so the value function is to,

164
00:09:14.341 --> 00:09:17.970
the policy is a subset of the value function.
And we can derive,

165
00:09:18.000 --> 00:09:21.330
if we know the value function,
we can easily derive what the policy is.

166
00:09:22.080 --> 00:09:25.680
So there are two fundamental methods of solving more carb decision processes

167
00:09:26.160 --> 00:09:27.270
that are model based.

168
00:09:27.300 --> 00:09:30.570
That means that the agent knows the model of the world beforehand.

169
00:09:30.780 --> 00:09:35.270
And these are policy iteration and value iteration,
algorithms and d.

170
00:09:35.370 --> 00:09:38.970
These are considered dynamic programming algorithms,
right?

171
00:09:39.090 --> 00:09:42.780
So they assume that the agent knows what the model of the world looks like later

172
00:09:42.781 --> 00:09:43.261
on.

173
00:09:43.261 --> 00:09:47.970
Will the scourge w we'll discuss model free methods as in the agent doesn't know

174
00:09:47.971 --> 00:09:51.720
what the model of the world is and that is Q learning and we'll get into that in

175
00:09:51.721 --> 00:09:52.554
a later video.

176
00:09:53.100 --> 00:09:56.790
But we're trying to learn what the optimal policy will be and we're going to use

177
00:09:56.791 --> 00:10:00.810
either value iteration or policy iteration to do that,
right?

178
00:10:00.811 --> 00:10:05.640
So let's get into this.
Okay,
so value iteration,
okay,
this is,
this is,

179
00:10:05.641 --> 00:10:08.220
this can be non trivial.
It might take a few tries,

180
00:10:08.221 --> 00:10:10.890
but eventually you're going to get it.
Just keep looking at it over and over.

181
00:10:11.010 --> 00:10:14.100
Look at my notes after this video.
If you don't get it,
look at the code.

182
00:10:14.130 --> 00:10:16.740
It's all there.
You just got to look at it for a while,
a few hours and boom,

183
00:10:16.741 --> 00:10:18.780
you'll get it.
But anyway,
let's,
let's go through this.

184
00:10:18.930 --> 00:10:21.990
These are two separate processes.
So for value iteration,

185
00:10:22.320 --> 00:10:25.200
what we're going to do is we're going to compute the optimal state value

186
00:10:25.201 --> 00:10:29.310
function by iteratively improving the estimate of the value function via

187
00:10:29.311 --> 00:10:32.650
[inaudible],
which is the value function,
right?
So,
um,

188
00:10:32.790 --> 00:10:36.270
the algorithm initializes vos to arbitrary random values,

189
00:10:36.450 --> 00:10:40.980
it repeatedly updates the Q function if,
if we're talking about ACU function,

190
00:10:41.280 --> 00:10:44.610
and then it's guaranteed to converge,
converge on the optimal values.

191
00:10:44.820 --> 00:10:47.610
In our case though,
we're not going to talk about the Q function right now,

192
00:10:47.640 --> 00:10:50.880
we're just going to focus on value iteration without the cue function.

193
00:10:51.030 --> 00:10:52.080
So here's what it looks like.

194
00:10:52.680 --> 00:10:56.100
We start out by choosing an initial estimate of the optimal value function.

195
00:10:56.130 --> 00:10:59.610
Just writes whatever our estimate is,
it could even be zero,
right?

196
00:10:59.790 --> 00:11:03.780
We repeat this process until the change and values is sufficiently small.

197
00:11:03.990 --> 00:11:05.310
For every given state,

198
00:11:05.370 --> 00:11:09.540
we calculate the maximum expected value of neighboring states for each possible

199
00:11:09.541 --> 00:11:13.140
action.
We then use the maximum value from this list.

200
00:11:13.170 --> 00:11:17.790
The Arg Max over that to update the estimate of the optimal value function and

201
00:11:17.791 --> 00:11:21.270
after we do that,
then we can calculate the optimal value function is okay.

202
00:11:21.271 --> 00:11:23.610
If you didn't get that,
I'm going to go over it again in a second.
In a,

203
00:11:23.611 --> 00:11:26.730
in a more planning English way for policy iteration,

204
00:11:26.790 --> 00:11:29.940
we're going to first of all choose an initial policy and a value function.

205
00:11:30.300 --> 00:11:33.210
Then we're going to repeat this process until the policy is stable.

206
00:11:33.300 --> 00:11:37.320
There are two parts here,
policy evaluation and policy improvement.

207
00:11:37.440 --> 00:11:41.370
So for policy evaluation we're going to repeat this process.
For each state.

208
00:11:41.610 --> 00:11:44.640
We're going to calculate the value of neighboring states when taking an action

209
00:11:44.670 --> 00:11:46.260
according to the current policy.

210
00:11:46.500 --> 00:11:50.160
Then update the estimate of the optimal value function and then for policy

211
00:11:50.161 --> 00:11:53.320
improvement,
then we're going to update it to get a new policy,
right?

212
00:11:53.321 --> 00:11:58.321
So Palsy Iteration concenter repeatedly improving the value function estimate

213
00:11:58.630 --> 00:12:02.590
will redefine the policy at each step and compute the value according to the new

214
00:12:02.591 --> 00:12:04.540
policy until the policy converges.

215
00:12:04.900 --> 00:12:09.010
Policy iteration is also guaranteed to converge to the optimal policy and it

216
00:12:09.011 --> 00:12:12.820
often takes the less iterations to converge.
Then the value iteration algorithm,

217
00:12:12.940 --> 00:12:13.773
so check this out.

218
00:12:13.780 --> 00:12:18.280
Here is the best plain English explanation that I can give you for the

219
00:12:18.281 --> 00:12:21.640
difference between these two.
I'm just gonna read this out.
Okay,
just take,
just,

220
00:12:21.641 --> 00:12:24.460
just listen to this.
In a policy iteration algorithm,

221
00:12:24.610 --> 00:12:28.240
you start off with a random policy,
right?
It's just random,
just some number.

222
00:12:28.600 --> 00:12:31.750
Then you'll find the value function of that policy.

223
00:12:31.780 --> 00:12:33.760
This is the policy evaluation step.

224
00:12:34.180 --> 00:12:38.830
Then find a new improved policy based on the previous value function and so on.

225
00:12:39.280 --> 00:12:40.210
In this process,

226
00:12:40.240 --> 00:12:44.440
each policy is guaranteed to be a strict improvement over the previous one,

227
00:12:44.710 --> 00:12:45.970
unless it's already optimal,

228
00:12:46.000 --> 00:12:50.950
in which case we can just stop iterating because now we found the optimal
policy,

229
00:12:51.340 --> 00:12:53.050
right?
But in value iteration,

230
00:12:53.051 --> 00:12:57.340
you start off with a random value function and then find a new improved value

231
00:12:57.341 --> 00:13:02.341
function in an iterative process until reaching the optimal value function and

232
00:13:02.531 --> 00:13:05.560
notice that once we have that value function,
like I said earlier,

233
00:13:05.860 --> 00:13:09.790
we can easily derive the optimal policy from the value function.
Why?

234
00:13:09.791 --> 00:13:13.530
Because it's a variable in the,
in the,
it's one variable that Pi's him.

235
00:13:13.531 --> 00:13:16.750
We'll end the value functions equation,
we can easily drive that.

236
00:13:17.230 --> 00:13:18.370
So what's the difference here?

237
00:13:18.610 --> 00:13:22.870
Policy iteration is generally faster than value iteration as policy convergence

238
00:13:22.871 --> 00:13:27.460
more quickly than value than the value function.
But it depends,
right?

239
00:13:27.461 --> 00:13:30.040
It depends on environment and it's good to try both out.

240
00:13:30.310 --> 00:13:31.390
So let's look at some code here.

241
00:13:32.950 --> 00:13:36.790
So we have open a eyes environments to do this,
right?

242
00:13:36.791 --> 00:13:40.390
So I'm going to go over this code and then I'm going to go over the,

243
00:13:40.840 --> 00:13:43.570
I'm going to go over the high level code and then the lower level of code.

244
00:13:43.571 --> 00:13:45.970
So let's just,
let's just go through this.
Let's just step through this code.

245
00:13:46.360 --> 00:13:49.420
What we're doing is we're building this frozen lake world and we're going to

246
00:13:49.421 --> 00:13:53.680
apply both policy iteration and value iteration algorithms to this code.

247
00:13:54.160 --> 00:13:56.200
So to start off,
we're going to import gym,
right?

248
00:13:56.230 --> 00:14:00.340
Open Ai's gym environment that lets us test out a bunch of different game

249
00:14:00.341 --> 00:14:03.880
environments.
And then we're going to import num Pi to do some matrix math.

250
00:14:04.120 --> 00:14:06.670
So to start off,
we're going to have some action mappings,
right?

251
00:14:06.671 --> 00:14:10.120
Numbers that correlate to keys that the agent can perform like up,
down,

252
00:14:10.121 --> 00:14:14.440
left or right.
Okay.
So in this play episodes function,

253
00:14:14.560 --> 00:14:17.140
we're going to assume that we know the optimal value function,

254
00:14:17.141 --> 00:14:20.590
the optimal policy.
So here's how we would go if we knew those two,
right?

255
00:14:21.220 --> 00:14:24.040
We've already performed value and policy iteration.

256
00:14:24.460 --> 00:14:27.850
So for every episode in the game,
while the game is still running,

257
00:14:27.851 --> 00:14:30.850
select the best action to perform in a current state.

258
00:14:30.851 --> 00:14:34.300
We're taking that right from the policy and we're using Arg Max to find the

259
00:14:34.301 --> 00:14:35.800
greatest value for that action.

260
00:14:35.920 --> 00:14:37.930
The action that's going to give us the greatest value,

261
00:14:38.350 --> 00:14:41.920
then we're going to perform that action and observe how the environment acted in

262
00:14:41.921 --> 00:14:46.060
response.
So the great thing about Jim's environment,
the gym environment,

263
00:14:46.210 --> 00:14:50.530
is that we can just use the step function on a given environment and to perform

264
00:14:50.531 --> 00:14:53.900
an action and it's going to report,
it's going to return the next state.

265
00:14:54.260 --> 00:14:58.460
If there's a reward or not a yes or no boolean did the game terminate or not,

266
00:14:58.580 --> 00:15:01.790
and then some logistics,
some logging info,
that's it.

267
00:15:01.791 --> 00:15:03.890
Then we'll render the environment at every time step.

268
00:15:03.891 --> 00:15:06.320
So we can see what it looks like and we'll summarize.

269
00:15:06.350 --> 00:15:07.940
We'll summarize a total reward.

270
00:15:07.941 --> 00:15:11.660
Remember that equation I showed you about the total reward and then we'll update

271
00:15:11.661 --> 00:15:13.910
the current state as the next state,
right?

272
00:15:13.911 --> 00:15:16.190
Because we just stepped into the next time step.

273
00:15:16.610 --> 00:15:20.270
We'll calculate the number of wins over each episode and we'll get the average

274
00:15:20.271 --> 00:15:24.740
award as a total award divided by the number of episodes and return the number

275
00:15:24.741 --> 00:15:28.950
of wins,
the total reward,
and the average reward.
So that is the law.

276
00:15:29.120 --> 00:15:32.390
That's actually how the agent plays given its,
knows everything,
you know,

277
00:15:32.391 --> 00:15:35.600
the optimal policy and the optimal value function.

278
00:15:36.620 --> 00:15:38.690
So then we have a number of episodes to play.

279
00:15:38.690 --> 00:15:41.990
Let's just say 10,000 and these are both of our solvers,
right?

280
00:15:41.991 --> 00:15:43.610
We're going to have a policy,
iteration solver,

281
00:15:43.611 --> 00:15:47.180
and a value iteration solver that I'm going to go into later.
So,

282
00:15:47.420 --> 00:15:51.350
so for each of these solvers,
load up our environment,
frozen lake,
okay.

283
00:15:51.560 --> 00:15:55.190
And then we're going to search for an optimal policy using policy iteration.

284
00:15:55.220 --> 00:15:58.550
Okay?
And then we're going to print out what that optimal policy is,

285
00:15:58.610 --> 00:16:01.850
and then we're going to compute an optimal policy using policy iteration.

286
00:16:02.060 --> 00:16:05.540
And we're going to use both value and policy iteration for,
because these,

287
00:16:05.541 --> 00:16:09.890
this is looping for each of these solvers.
And then we'll print out the rewards,

288
00:16:09.891 --> 00:16:12.890
the winds,
and everything for us to view later on,
right?

289
00:16:13.160 --> 00:16:17.030
So let's go and see what these two functions look like under DP dynamic

290
00:16:17.031 --> 00:16:21.170
programming.
Okay,
so let's start off with value iteration,
right?

291
00:16:21.380 --> 00:16:25.490
So for value iteration,
we're trying to compute that optimal value function,

292
00:16:25.491 --> 00:16:27.560
right?
And if we've got that value function,

293
00:16:27.680 --> 00:16:30.920
then we can compute the optimal policy.
So here's what we're gonna do.

294
00:16:30.950 --> 00:16:34.040
We've got the environment,
our discount factor,
a theta value.

295
00:16:34.220 --> 00:16:37.790
That data volume is a stopping threshold.
And the Max number of iterations,

296
00:16:37.791 --> 00:16:39.440
we want to perform this right?

297
00:16:39.470 --> 00:16:42.110
How many times do we want to iterate to get that optimal value function?

298
00:16:42.680 --> 00:16:46.280
We're going to start off by initializing our value function randomly rights.

299
00:16:46.550 --> 00:16:50.570
And then for the number of iterations we defined,
we have our stopping condition.

300
00:16:50.810 --> 00:16:52.460
We're going to update each state.

301
00:16:52.700 --> 00:16:56.150
We're going to do one step look ahead to calculate the state action values in

302
00:16:56.151 --> 00:16:59.240
the next time step and that's going to give us this action value.

303
00:16:59.600 --> 00:17:03.380
Then we're going to select the best action to perform based on the highest state

304
00:17:03.381 --> 00:17:04.131
action values.

305
00:17:04.131 --> 00:17:08.810
So we'll perform NP dot Max to find the largest number for the actual value and

306
00:17:08.811 --> 00:17:10.460
that's going to be our best action value.

307
00:17:10.700 --> 00:17:15.380
We'll calculate the change or the Delta right between this current state and the

308
00:17:15.500 --> 00:17:18.950
and our best action value and then we'll update the value function for the

309
00:17:18.951 --> 00:17:21.650
current state.
We found the change,
the difference in those two,

310
00:17:21.800 --> 00:17:24.290
and then we'll check.
This is our stopping conviction,
right?

311
00:17:24.291 --> 00:17:25.700
We'll check if we can,
we can stop.

312
00:17:25.701 --> 00:17:28.640
We've converged and once we have that value right,

313
00:17:28.820 --> 00:17:32.840
then we can create a policy using that optimal value function down here.

314
00:17:33.050 --> 00:17:34.130
So for each state.

315
00:17:34.131 --> 00:17:38.090
So we'll first initialize our policy randomly and then for each state we have,

316
00:17:38.091 --> 00:17:42.170
we'll look ahead one step to find the best action for the state and using our

317
00:17:42.200 --> 00:17:43.370
optimal value function.

318
00:17:43.610 --> 00:17:47.300
Then we'll select the best action based on the highest state action value and

319
00:17:47.301 --> 00:17:52.080
update the policy to perform a better action at a current state.
At the very end,

320
00:17:52.081 --> 00:17:55.740
we were turned the optimal policy and the optimal value function,
right?

321
00:17:55.741 --> 00:17:57.720
So the value function is at the core,

322
00:17:57.721 --> 00:18:00.840
it's at the center and from it we then derive the policy.

323
00:18:01.410 --> 00:18:03.690
So now let's look at policy iteration.

324
00:18:04.480 --> 00:18:05.300
<v 1>Yeah,</v>

325
00:18:05.300 --> 00:18:10.220
<v 0>so for policy iteration,
we start off with the random policy,
right?
Right.</v>

326
00:18:10.221 --> 00:18:13.040
In this case is going to be the number of states times the number of actions

327
00:18:13.041 --> 00:18:15.200
divided by the number of actions,
right?

328
00:18:15.201 --> 00:18:19.400
And so now we have a counter to count how many times we want to evaluate each

329
00:18:19.401 --> 00:18:22.310
policy.
We're going to repeat this until convergence.

330
00:18:22.460 --> 00:18:26.720
We say evaluate the current policy,
right?
Remember there's two steps,
policy,

331
00:18:26.721 --> 00:18:28.820
evaluation,
and then policy improvement.

332
00:18:29.180 --> 00:18:32.390
We're going to evaluate the current policy and that's going to give us a value

333
00:18:32.391 --> 00:18:33.140
function.

334
00:18:33.140 --> 00:18:36.860
We're going to go through each state and try to improve actions that were taken.

335
00:18:37.160 --> 00:18:39.650
We're going to choose the best action in a current state,

336
00:18:39.680 --> 00:18:42.050
under the current policy by performing Arg Max.

337
00:18:42.051 --> 00:18:44.750
What's the Max value in the policy for a given state?

338
00:18:44.810 --> 00:18:46.970
And that's going to be the current action.
Well then look,

339
00:18:46.971 --> 00:18:51.140
one step ahead and evaluate if the current action is optimal and we're going to

340
00:18:51.141 --> 00:18:55.010
try every single possible action in a current state and we're going to select

341
00:18:55.011 --> 00:18:58.730
the better action of the two and the f the action doesn't change,

342
00:18:58.760 --> 00:19:02.270
then that means we found a stable policy,
there's no change in the action.

343
00:19:02.690 --> 00:19:04.880
And then we,
we will update the policy,
right?

344
00:19:04.940 --> 00:19:07.520
Greedily but we'll talk about what greedy means later.

345
00:19:07.610 --> 00:19:11.450
This will update the policy and then we have a convergence criteria at the end,

346
00:19:11.840 --> 00:19:13.460
right?
So that's a difference here,
right?

347
00:19:13.461 --> 00:19:16.610
Value Iteration is focused on the value function first and foremost,

348
00:19:16.850 --> 00:19:20.240
whereas policy iteration is more focused on the policy first and foremost.

349
00:19:20.510 --> 00:19:23.810
And there are two differences.
Sometimes one works better than the other.

350
00:19:23.960 --> 00:19:27.440
Generally,
generally the policy iteration algorithm is faster,

351
00:19:27.620 --> 00:19:29.180
but you got to try out both.
Right?

352
00:19:29.240 --> 00:19:32.750
And both of these are examples of dynamic programming and this,

353
00:19:32.751 --> 00:19:34.280
these are model based methods.

354
00:19:34.450 --> 00:19:37.610
If our agent knows what the world is going to be like,

355
00:19:38.070 --> 00:19:39.960
<v 1>uh,
yeah.</v>

356
00:19:41.550 --> 00:19:45.440
<v 0>So it looks like this,
right?
Uh,
you know,</v>

357
00:19:45.441 --> 00:19:50.441
a better view of this would be just for me to remove this render and then just

358
00:19:52.071 --> 00:19:53.360
look at our results

359
00:19:53.910 --> 00:19:54.750
<v 1>over time.</v>

360
00:19:58.150 --> 00:20:00.090
<v 0>Right?
Here are our policy,
right?
It's,</v>

361
00:20:00.091 --> 00:20:03.780
it's the collection of directions that our agents should move over time using

362
00:20:03.781 --> 00:20:08.200
policy could aeration.
You can see how many policies it's,
it's evaluated,
uh,

363
00:20:08.201 --> 00:20:11.940
and it's just going to keep repeating,
right?
Telling.
And now it's like,
okay,

364
00:20:11.941 --> 00:20:15.510
here's value iteration and see when each of these converged,

365
00:20:15.540 --> 00:20:19.950
you could see the optimal policy derived from value iteration,
and yeah,

366
00:20:20.160 --> 00:20:23.370
that's it.
Please subscribe for more programming videos.
And for now,

367
00:20:23.400 --> 00:20:26.460
I've got to optimize my chest hair,
so thanks for watching.

