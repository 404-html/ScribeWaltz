WEBVTT

1
00:00:00.060 --> 00:00:05.060
This is how I feel when my binary classifier outputs 50% hello world,

2
00:00:06.180 --> 00:00:11.180
it's Saroj and logistic regression is a very popular machine learning technique

3
00:00:12.270 --> 00:00:15.600
that's often misunderstood.
In this video,

4
00:00:15.601 --> 00:00:20.601
we'll build a sentiment analysis app in python that uses a logistic regression

5
00:00:21.360 --> 00:00:26.360
to classify whatever texts that I input into the app as either positive,

6
00:00:26.670 --> 00:00:28.200
negative,
or neutral.

7
00:00:28.530 --> 00:00:33.530
Every week it seems like an exciting new Dataset is made public on websites like

8
00:00:33.901 --> 00:00:36.720
Kaggle,
AWS,
pirate bay.

9
00:00:36.840 --> 00:00:40.500
I'll put a link to some data set websites in the video description.

10
00:00:40.830 --> 00:00:45.060
When observing one of these data sets for the first time,

11
00:00:45.270 --> 00:00:50.270
a novice might trug it off as useless or too complicated to understand,

12
00:00:50.970 --> 00:00:55.970
but AI wizards are delighted because we know that there's so much we can create

13
00:00:57.450 --> 00:01:01.140
and discover with the dataset using machine learning.

14
00:01:01.470 --> 00:01:04.050
Hopefully the Dataset is labeled.

15
00:01:04.110 --> 00:01:08.250
This is a column that describes what each data point is.

16
00:01:08.430 --> 00:01:12.120
Think of it as a summary of what all those signals mean.

17
00:01:12.480 --> 00:01:17.340
This could be a dog and emotion,
a diagnosis and adolescent phase.

18
00:01:17.460 --> 00:01:18.630
So many things.

19
00:01:18.810 --> 00:01:23.310
We can then consider this a supervised learning task and if it doesn't have

20
00:01:23.311 --> 00:01:27.570
labels than it would be considered an unsupervised learning task.

21
00:01:27.930 --> 00:01:32.930
This is more of an exploratory problem where we don't have a target label we're

22
00:01:33.361 --> 00:01:35.370
trying to predict.
Instead,

23
00:01:35.580 --> 00:01:39.000
we use mainly clustering techniques to make discoveries.

24
00:01:39.360 --> 00:01:43.650
Supervised learning can be broken down into two types of problems.

25
00:01:43.770 --> 00:01:45.990
Regression and classification.

26
00:01:46.320 --> 00:01:51.320
Regression is when we want to predict an output value that can have a lot of

27
00:01:51.901 --> 00:01:56.901
different values and usually our output is some rational number.

28
00:01:57.510 --> 00:02:02.400
Classification is when we want to predict an output that can only have some

29
00:02:02.401 --> 00:02:04.920
specific categories.
Now,

30
00:02:04.980 --> 00:02:07.980
depending on the type of problem we have,

31
00:02:08.010 --> 00:02:11.370
we can choose a suitable algorithm to solve it.

32
00:02:11.820 --> 00:02:16.820
One algorithm that is frequently used to solve supervised classification

33
00:02:17.340 --> 00:02:20.430
problems is called logistic regression.

34
00:02:20.700 --> 00:02:24.390
It measures the likelihood occurrence of an event,

35
00:02:24.540 --> 00:02:29.540
so some possible questions that logistic regression could answer include how

36
00:02:30.571 --> 00:02:33.900
likely is it that a user will purchase a subscription?

37
00:02:34.200 --> 00:02:37.230
How likely is it that France will win the World Cup?

38
00:02:37.410 --> 00:02:42.150
How likely is it that getting a certain test score will result in admission to a

39
00:02:42.151 --> 00:02:46.500
good university?
How likely is it that a tweet from Kanye makes sense?

40
00:02:46.740 --> 00:02:51.740
Once our model outputs a class likelihood percentage for a new data points,

41
00:02:51.930 --> 00:02:56.930
we can then use that to classify that data point according to a threshold value

42
00:02:57.840 --> 00:02:58.860
that we decide.

43
00:02:59.230 --> 00:03:04.230
You might be wondering why we use logistic regression and linear regression for

44
00:03:04.661 --> 00:03:05.890
this.
Well,

45
00:03:05.920 --> 00:03:10.920
suppose we had a labeled dataset of cancerous tumor sizes.

46
00:03:11.230 --> 00:03:15.640
Each was either a malignant or not denoted by either a zero or one.

47
00:03:16.060 --> 00:03:18.340
If we use linear regression,

48
00:03:18.460 --> 00:03:22.360
we'd construct the line of best fit for the data and graph it.

49
00:03:22.690 --> 00:03:26.980
We could then decide that all the values to the left of the line are considered

50
00:03:27.040 --> 00:03:31.330
not malignant and all the values on the right as malignant,

51
00:03:31.660 --> 00:03:34.150
but what if there's an outlier in the data,

52
00:03:35.080 --> 00:03:38.710
a data point that is significantly different than the rest.

53
00:03:39.040 --> 00:03:43.690
If we accounted for that outlier point,
when we built our linear regression line,

54
00:03:43.990 --> 00:03:45.430
it would look pretty warped.

55
00:03:45.760 --> 00:03:49.900
It put some positive class examples into the negative class.

56
00:03:50.290 --> 00:03:55.150
If we were to draw out an imaginary class decision boundary that divides

57
00:03:55.270 --> 00:03:59.590
malignant tumors from benign tumors,
it would look off.

58
00:04:00.040 --> 00:04:01.990
It should have been in a different spot,

59
00:04:02.020 --> 00:04:06.760
but a single outlier threw off our linear regression predictions.

60
00:04:06.880 --> 00:04:10.090
We need a way to deal with outliers in the data.

61
00:04:10.390 --> 00:04:15.390
Logistic regression helps us do just that because it uses the standard logistic

62
00:04:16.780 --> 00:04:17.410
function.

63
00:04:17.410 --> 00:04:22.410
This was developed by statisticians to describe properties of population growth

64
00:04:23.140 --> 00:04:28.140
and ecology rising fast and maxing out at the carrying capacity of the host

65
00:04:28.961 --> 00:04:29.860
environment.

66
00:04:31.090 --> 00:04:36.040
It looks like an s shaped curve that takes any real number and maps it into a

67
00:04:36.041 --> 00:04:40.450
value between zero and one but never exactly those limits.

68
00:04:40.720 --> 00:04:45.720
This is described using the equation F of x equals l over one plus e to the

69
00:04:46.571 --> 00:04:51.571
negative k times the difference between x and the x value of the curves.

70
00:04:51.770 --> 00:04:54.910
Midpoints l is the curves and Max value.

71
00:04:55.000 --> 00:04:57.250
He is the natural logarithm base,

72
00:04:57.490 --> 00:05:02.490
a constant value use throughout math to help model growth and k is the steepness

73
00:05:03.431 --> 00:05:04.264
of the curve.

74
00:05:04.540 --> 00:05:09.540
Logistic Regression actually uses a special case of the logistic function called

75
00:05:10.150 --> 00:05:11.380
the sigmoid function.

76
00:05:11.710 --> 00:05:16.710
This is when l equals one because it's using binary classification values for

77
00:05:17.081 --> 00:05:21.460
its prediction.
Either zero or one and k will be set to one as well.

78
00:05:21.490 --> 00:05:23.170
Making a simpler equation.

79
00:05:23.590 --> 00:05:28.590
What logistic regression does is it uses a linear function to represent the

80
00:05:28.721 --> 00:05:29.740
input values.

81
00:05:30.010 --> 00:05:35.010
That's where the input values are combined linearly using weights or coefficient

82
00:05:35.920 --> 00:05:36.753
values,

83
00:05:36.820 --> 00:05:41.710
often referred to as the Greek capitol letter Ada to predict an output value.

84
00:05:41.770 --> 00:05:43.240
Why thanks Greece.

85
00:05:43.660 --> 00:05:47.980
The difference from linear regression though is that the output value being

86
00:05:47.981 --> 00:05:52.981
modeled is a binary values zero or one rather than a numeric value.

87
00:05:53.740 --> 00:05:57.770
We do this by plugging that equation into our sigmoid function,

88
00:05:57.950 --> 00:06:01.310
which gives us the full logistic regression equation.

89
00:06:01.760 --> 00:06:06.740
When we use this model instead of linear regression for our tumor Dataset,

90
00:06:06.890 --> 00:06:11.510
it'll incorporate any outliers into it.
So let's start building our APP,

91
00:06:11.511 --> 00:06:15.770
shall we?
It's a collection of labeled tweets that we can find on Kaggle.

92
00:06:15.830 --> 00:06:19.550
It's got both training and testing CSV files attached.

93
00:06:19.551 --> 00:06:21.890
So once we download the data set,

94
00:06:21.891 --> 00:06:26.660
we can view it ourselves in our Jupiter notebook.
Once we spin it up,

95
00:06:26.661 --> 00:06:28.130
we can import the data.

96
00:06:28.131 --> 00:06:33.131
Preprocessing tool hand does to help load the data into a modifiable data frame

97
00:06:34.250 --> 00:06:36.770
object in memory using the head function.

98
00:06:36.800 --> 00:06:39.740
We can see that the data contains three columns,

99
00:06:39.980 --> 00:06:42.230
the item sentiment of the tweet,

100
00:06:42.260 --> 00:06:47.060
either zero for negative or one for positive and the tweet itself.

101
00:06:47.120 --> 00:06:50.240
Notice how these tweets are all different sizes.

102
00:06:50.270 --> 00:06:54.590
Some of them are much longer than the rest on important data.

103
00:06:54.591 --> 00:06:59.591
Preprocessing step for us is to remove any unnecessary noise from our dataset.

104
00:07:00.710 --> 00:07:02.330
When it comes to text,

105
00:07:02.450 --> 00:07:07.280
there are a bunch of words that don't really contribute to the sentiment of the

106
00:07:07.281 --> 00:07:10.670
phrase as a whole.
These are called stop words.

107
00:07:10.700 --> 00:07:15.700
Examples include the is an of using the nltk libraries stopped board collection.

108
00:07:18.110 --> 00:07:23.110
We can iterate through the Dataset and find any of the stop words that do exist

109
00:07:23.420 --> 00:07:26.270
and remove them once they're gone.

110
00:07:26.300 --> 00:07:31.300
We can go one step further in reducing noise by reducing art vocabulary size and

111
00:07:32.781 --> 00:07:36.830
consolidating similar words.
For example,
swag,

112
00:07:36.890 --> 00:07:38.240
swagger and swag.

113
00:07:38.241 --> 00:07:43.241
Delicious all had the same stem and we can just replace all of those instances

114
00:07:43.850 --> 00:07:48.850
with the stem swag and Ltk has got a handy stem module to help us do this

115
00:07:48.981 --> 00:07:52.340
automatically and we'll also tokenize our words,

116
00:07:52.460 --> 00:07:57.460
meaning break each phrase down into individual words that form a larger vector.

117
00:07:57.920 --> 00:08:01.840
Train,
train,
train.
I'm clean and data sets.
Yeah,

118
00:08:02.240 --> 00:08:06.170
trained it on my laptop.
Now it's on the intimate.
Yeah,

119
00:08:07.040 --> 00:08:11.570
we can programmatically right out our own logistic regression class that we can

120
00:08:11.571 --> 00:08:13.130
use on the data later.

121
00:08:13.430 --> 00:08:18.430
It'll use to hyper parameters that we initialize when we call it first the

122
00:08:19.131 --> 00:08:19.811
learning rate,

123
00:08:19.811 --> 00:08:24.590
which controls how much we are adjusting the weights of our network during

124
00:08:24.591 --> 00:08:28.820
training.
If it's too small,
then optimization will be too slow,

125
00:08:29.000 --> 00:08:33.920
but if it's too large,
our algorithm may fail to converge on the best result.

126
00:08:34.250 --> 00:08:37.550
Second,
a number of training iterations we'll use.

127
00:08:37.820 --> 00:08:39.950
We can first write out the sigmoid function.

128
00:08:39.951 --> 00:08:43.910
Since we already know the formula for our loss function,

129
00:08:44.060 --> 00:08:48.230
the way we measure how bad our prediction is compared to the real label,

130
00:08:48.500 --> 00:08:50.090
let's use cross entropy.

131
00:08:50.390 --> 00:08:53.660
This can be divided into two separate loss functions.

132
00:08:53.661 --> 00:08:58.470
One of them for when x equals one and one for when x equals zero.

133
00:08:58.680 --> 00:09:01.500
When we look at the loss function graphs for both of them,

134
00:09:01.710 --> 00:09:04.350
we can see the benefit of taking the logarithm.

135
00:09:04.680 --> 00:09:09.680
These smooth functions are always increasing or always decreasing and make it

136
00:09:10.321 --> 00:09:14.640
easy to calculate the gradient for our optimization process.

137
00:09:14.940 --> 00:09:18.180
When we compress these functions into one,

138
00:09:18.390 --> 00:09:20.670
we get our final loss function.

139
00:09:20.700 --> 00:09:25.500
It's like a function megazord Oh and one more helper function before our

140
00:09:25.501 --> 00:09:30.420
training loop at intercept.
This will add a decision boundary to our graph.

141
00:09:30.750 --> 00:09:34.860
In our fit function,
we'll initialize the weights of our network as theta.

142
00:09:35.100 --> 00:09:39.120
Then the specified number of iterations will compute the linear function by

143
00:09:39.121 --> 00:09:41.460
multiplying our input by our weight matrix.

144
00:09:41.610 --> 00:09:45.060
Then use that as input to our sigmoid function to compute.

145
00:09:45.090 --> 00:09:49.260
The outputs will then compute the gradients by multiplying the transpose of our

146
00:09:49.261 --> 00:09:53.940
input data I the difference between our prediction and actual label.

147
00:09:54.180 --> 00:09:56.340
Once we multiply it by the learning rate,

148
00:09:56.400 --> 00:10:01.050
we'll adjust our weight value theta to help make a better prediction.
Next round.

149
00:10:01.560 --> 00:10:03.270
After a few minutes of training,

150
00:10:03.271 --> 00:10:07.560
we can visualize our graph in the notebook and see that tweets are pretty

151
00:10:07.561 --> 00:10:10.170
accurately separated by our line.

152
00:10:10.470 --> 00:10:15.470
We can create a simple html UI where we take the text input and feed it to our

153
00:10:16.081 --> 00:10:19.980
python model to make a prediction based on what it outputs.

154
00:10:19.981 --> 00:10:22.950
We can set a threshold for it,
happy or sad,

155
00:10:23.130 --> 00:10:27.030
and then have an Emoji that represents that show up on screen.

156
00:10:27.270 --> 00:10:29.970
We performed a binary classification here,

157
00:10:29.971 --> 00:10:34.560
but logistic regression can be used for multiple classes as well.

158
00:10:34.830 --> 00:10:36.150
That's for another video.

159
00:10:36.420 --> 00:10:40.290
There are three summary points you should remember from this video.

160
00:10:40.710 --> 00:10:45.240
Logistic regression measures the relationship between categorical dependent

161
00:10:45.241 --> 00:10:49.200
variables and one or more independent variables.

162
00:10:49.470 --> 00:10:54.470
It's more robust than linear regression to outliers in the data and using python

163
00:10:56.611 --> 00:10:58.770
and any freely available dataset.

164
00:10:58.860 --> 00:11:03.240
You can build your own logistic regression model in a few lines of code.

165
00:11:03.720 --> 00:11:08.010
Please subscribe for more programming videos.
And for now,
I've got to digress,

166
00:11:08.190 --> 00:11:09.540
so thanks for watching.

