WEBVTT

1
00:00:00.060 --> 00:00:01.530
Hello world,
it's Saroj.

2
00:00:01.560 --> 00:00:05.400
And what happens when we encounter data with no labels?

3
00:00:05.550 --> 00:00:06.840
Can we still learn from it?

4
00:00:07.200 --> 00:00:11.700
We're going to use a really cool model called a variational autoencoder to

5
00:00:11.701 --> 00:00:16.230
generate unique images after training on a collection of images with no labels.

6
00:00:16.530 --> 00:00:20.640
Most of the recent successes in deep learning have been due to our ability to

7
00:00:20.641 --> 00:00:24.240
train neural networks on huge sets of cleanly labeled data.

8
00:00:24.420 --> 00:00:27.510
So if you have a set of inputs and the respective target labels,

9
00:00:27.810 --> 00:00:31.710
you can try and learn the probability of a particular label for a particular

10
00:00:31.711 --> 00:00:36.090
input.
Intuitively this makes sense.
We're just learning a mapping of values.

11
00:00:36.330 --> 00:00:40.470
But the hottest area of research right now is learning from the raw unlabeled

12
00:00:40.471 --> 00:00:44.580
data that the world is absolutely brimming with unsupervised learning.

13
00:00:44.820 --> 00:00:48.090
One type of model that can do this is called an auto encoder.

14
00:00:48.240 --> 00:00:52.860
Auto encoders sequentially deconstruct input data into hidden representations.

15
00:00:53.010 --> 00:00:57.330
Then use those same representations to sequentially reconstruct outputs that

16
00:00:57.331 --> 00:00:58.710
resembled their originals.

17
00:00:58.920 --> 00:01:01.890
Auto encoding is considered a data compression algorithm,

18
00:01:01.891 --> 00:01:06.240
but their data specific so it can only compress data as similar to what it's

19
00:01:06.241 --> 00:01:08.720
been trained on.
They're also relatively lossy.

20
00:01:08.730 --> 00:01:12.720
So the decompressed outputs will be a bit degraded compared to the original

21
00:01:12.721 --> 00:01:17.310
inputs.
Usually a well known compression now wear them like jpeg does better.

22
00:01:17.580 --> 00:01:20.610
So what are they useful for?
Absolutely nothing.
Just kidding.

23
00:01:21.120 --> 00:01:25.500
One major use case is data denoising where we train an auto encoder to

24
00:01:25.501 --> 00:01:28.470
reconstruct the input from a corrupted version of it,

25
00:01:28.471 --> 00:01:32.460
so that given some similar data that's corrupted,
it can de noise it.

26
00:01:32.730 --> 00:01:36.750
Another is to generate similar but unique data,
which is what we'll do.

27
00:01:36.960 --> 00:01:41.550
There are a bunch of different auto encoder types and each has its own unique

28
00:01:41.551 --> 00:01:46.320
implementation details.
But let's dive into one I find particularly interesting.

29
00:01:46.350 --> 00:01:48.810
The variational autoencoder whore VAE.

30
00:01:49.080 --> 00:01:52.650
While deep learning is a great way to approximate complex functions,

31
00:01:52.820 --> 00:01:56.730
Basie and inference offers a unique framework to reason about uncertainty.

32
00:01:56.970 --> 00:02:01.470
It's an approach to statistics in which all forms of uncertainty or express in

33
00:02:01.471 --> 00:02:03.750
terms of probability at any time.

34
00:02:03.751 --> 00:02:07.770
There is the evidence for and against something which leads to a probability,

35
00:02:07.800 --> 00:02:11.100
a chance that it is true.
When you learn something new,

36
00:02:11.160 --> 00:02:14.730
you have to fold this new evidence into what you already know to create a new

37
00:02:14.731 --> 00:02:18.900
probability.
Basie in theory,
describes this process mathematically.

38
00:02:23.070 --> 00:02:27.150
The idea for the Vae came when these two ideas merged,

39
00:02:27.330 --> 00:02:30.930
so looking at it through a Bayesean Lens,
we treat the inputs,

40
00:02:31.050 --> 00:02:34.500
hidden representations and reconstructed outputs of Aba.

41
00:02:34.501 --> 00:02:39.390
He as probabilistic random variables within a directed graphical model,

42
00:02:39.510 --> 00:02:44.160
so it contains a specific probability model of some data acts and latent or

43
00:02:44.161 --> 00:02:48.510
hidden variables.
Z.
We can write out the joint probability of the model.
This way,

44
00:02:48.750 --> 00:02:51.120
given just a character produced by the model,

45
00:02:51.240 --> 00:02:54.900
we don't know which setting of the latent variables generated the characters.

46
00:02:54.960 --> 00:02:59.830
We've built variation into our model.
It's inherently stochastic.

47
00:03:00.310 --> 00:03:03.040
Looking at it through a deep learning lens.
Aba,

48
00:03:03.050 --> 00:03:07.450
he consist of an encoder decoder and lost function,
so given some input x,

49
00:03:07.630 --> 00:03:10.330
let's say we've gotten 28 by 28 handwritten digit image,

50
00:03:10.331 --> 00:03:13.930
which is seven 84 dimensions where each pixel is one dimension.

51
00:03:14.200 --> 00:03:17.350
It will encode it into a latent or hidden representation space,

52
00:03:17.530 --> 00:03:21.730
which is way less than seven 84 we can then sample from a golf Sian probability

53
00:03:21.731 --> 00:03:24.880
density to get noisy values of the representations.

54
00:03:24.910 --> 00:03:26.500
Let's start writing this out programmatically.

55
00:03:26.650 --> 00:03:29.680
After importing the libraries and finding our hyper parameters,

56
00:03:29.800 --> 00:03:31.840
we'll initialize our encoder network.

57
00:03:31.960 --> 00:03:35.560
It's job is to map inputs to our latent distribution parameters.

58
00:03:35.800 --> 00:03:40.030
We'll take the input data and send it through a dense fully connected layer with

59
00:03:40.031 --> 00:03:44.290
our choice of nonlinearity to squash the dimentionality,
which will be removed.

60
00:03:44.590 --> 00:03:48.670
Then we convert the input data into two parameters.
In a latent space,

61
00:03:48.850 --> 00:03:52.390
we've predefined the size of using dense,
fully connected layers.
Again,

62
00:03:52.600 --> 00:03:54.250
Z mean and z log sigma.

63
00:03:54.400 --> 00:03:58.810
The decoder takes Z as its input and help puts the parameters to the probability

64
00:03:58.811 --> 00:04:03.070
distribution of the data.
Let's say each pixel is either one or zero.

65
00:04:03.071 --> 00:04:04.240
Since it's black and white,

66
00:04:04.570 --> 00:04:09.570
we can use a newly distribution since it defines a success as a binary value to

67
00:04:09.701 --> 00:04:12.470
represent a single pixel,
so the decoder,

68
00:04:12.480 --> 00:04:17.380
it gets the Leighton representation of a digit as input and outputs seven 84

69
00:04:17.381 --> 00:04:20.620
Bernoulli parameters.
One for each of the pixels,

70
00:04:20.800 --> 00:04:22.510
we're going to use these two variables.

71
00:04:22.511 --> 00:04:27.160
We have here to randomly sample news similar points from the late normal

72
00:04:27.161 --> 00:04:30.340
distribution by defining a sampling function.

73
00:04:30.550 --> 00:04:33.280
Epsilon refers to a random normal tensor.

74
00:04:33.460 --> 00:04:36.280
Once we have z we can feed it to our decoder.

75
00:04:36.430 --> 00:04:41.260
The decoder will map these latent space points back to the original input data.

76
00:04:41.500 --> 00:04:45.730
We'll initialize it with two fully connected layers and their own respected

77
00:04:45.731 --> 00:04:50.170
activation functions and because the data is extracted from a small

78
00:04:50.171 --> 00:04:52.000
dimensionality to a larger one,

79
00:04:52.240 --> 00:04:55.180
some of it is lost in the reconstruction process,

80
00:04:55.240 --> 00:04:59.620
but how much we need a measure of this and that will be our last function.

81
00:04:59.950 --> 00:05:04.950
<v 1>I want to generate some data printed dick typically and so mom model is the</v>

82
00:05:08.120 --> 00:05:11.350
castic probability

83
00:05:11.420 --> 00:05:14.420
<v 0>for a vie.
The loss function looks like this.</v>

84
00:05:14.480 --> 00:05:17.240
The first term measures the reconstruction loss.

85
00:05:17.270 --> 00:05:20.300
If the decoder output is bad at reconstructing the data,

86
00:05:20.301 --> 00:05:23.810
well that would consequently incur a large cost here.

87
00:05:24.030 --> 00:05:26.480
The next term is the regular advisor.

88
00:05:26.840 --> 00:05:31.550
Regularizing in this case means keeping the representations of each digit as

89
00:05:31.551 --> 00:05:35.630
diverse as possible.
If a two was written by two different people,

90
00:05:35.750 --> 00:05:39.140
we could end up with very different representations.
That's a bad thing.

91
00:05:39.560 --> 00:05:42.770
We only want one since they both wrote the same number.

92
00:05:42.830 --> 00:05:47.660
We penalize bad behavior this way and it makes sure similar representations are

93
00:05:47.661 --> 00:05:48.740
close together.

94
00:05:48.890 --> 00:05:53.690
So our total loss function is defined as the sum of our reconstruction term and

95
00:05:53.691 --> 00:05:58.370
the KL divergence regularization term.
All right.
This is the dopest part.

96
00:05:58.670 --> 00:06:02.990
The way we normally want to train this model is to use grading dissent to

97
00:06:03.020 --> 00:06:07.190
optimize a loss with respect to the parameters of both the encoder and the

98
00:06:07.191 --> 00:06:08.810
decoder.
But wait,

99
00:06:09.020 --> 00:06:12.950
how are we going to take derivatives with respect to the parameters of ace to

100
00:06:12.951 --> 00:06:15.650
castic or randomly determined to variable?

101
00:06:16.070 --> 00:06:19.430
We've built randomness into the model itself.

102
00:06:19.730 --> 00:06:24.530
Gradient descent usually expects that a given input always returns the same

103
00:06:24.560 --> 00:06:26.600
output for a fixed set of parameters.

104
00:06:26.601 --> 00:06:31.490
So the only source of randomness would be the inputs.
What do we do?
Cowboy,

105
00:06:32.590 --> 00:06:34.220
we don't reprint it her ass.

106
00:06:35.210 --> 00:06:40.210
We want to re parameterize samples so that the randomness is independent of the

107
00:06:40.791 --> 00:06:41.624
parameters,

108
00:06:41.720 --> 00:06:46.610
so we define a function that depends on the parameters deterministically and we

109
00:06:46.611 --> 00:06:50.840
inject randomness into the model by introducing a random variable.

110
00:06:51.080 --> 00:06:54.380
Instead of the encoder generating a vector of real values,

111
00:06:54.650 --> 00:06:59.210
it will generate a vector of means and a vector of standard deviations.

112
00:06:59.480 --> 00:07:04.010
Then we can take derivatives of the functions involving z with respect to the

113
00:07:04.011 --> 00:07:05.870
perimeters of its distribution.

114
00:07:06.140 --> 00:07:10.760
We'll define our models optimizer as rms prop which performs gradient descent

115
00:07:10.970 --> 00:07:13.280
and it's lost function as the one we defined.

116
00:07:13.580 --> 00:07:17.210
Then we'll train it by importing our digital images and feeding them into our

117
00:07:17.211 --> 00:07:21.410
model.
For a given number of epochs and batch size.
For our demo,

118
00:07:21.411 --> 00:07:24.920
we can plot out the neighborhoods of the different classes on a two d plane.

119
00:07:25.250 --> 00:07:29.750
Each colored cluster represents a digit representation and close clusters are

120
00:07:29.751 --> 00:07:31.760
digits that are structurally similar.

121
00:07:31.970 --> 00:07:34.760
We can also generate digits by scanning the latent plane,

122
00:07:34.970 --> 00:07:39.650
sampling late points at regular intervals and generating the corresponding digit

123
00:07:39.740 --> 00:07:40.940
for each of these points.

124
00:07:41.060 --> 00:07:45.470
So the three key takeaways here are that variational auto encoders allow us to

125
00:07:45.471 --> 00:07:48.080
generate data by performing unsupervised learning.

126
00:07:48.470 --> 00:07:50.330
They are a product of two ideas,

127
00:07:50.420 --> 00:07:53.840
Basie and inference and deep learning and we can back propagate through our

128
00:07:53.841 --> 00:07:58.280
network but performing a re parameterizing step that makes randomness

129
00:07:58.340 --> 00:08:01.760
independent of the parameters we derive our gradients from.

130
00:08:02.000 --> 00:08:04.730
Mike Mcdermott is last week's coding challenge winner.

131
00:08:04.850 --> 00:08:08.960
He cleverly applied the multi armed bandit problem to the stock market each

132
00:08:08.961 --> 00:08:13.040
bought he created used a different investment strategy based on reinforcement

133
00:08:13.041 --> 00:08:17.210
learning to profit and his eye python notebook is a great example of what well

134
00:08:17.211 --> 00:08:22.211
documented code looks like wizard of the week and the runner up is sgs joure.

135
00:08:22.430 --> 00:08:26.540
He successfully took state into account which made it the contextual bandit

136
00:08:26.541 --> 00:08:29.060
problem.
Dope submission skies.
I vow to both of you.

137
00:08:29.180 --> 00:08:33.500
The coding challenge for this video is to use a vie to generate something other

138
00:08:33.501 --> 00:08:34.490
than digital images.

139
00:08:34.730 --> 00:08:37.610
Details are in the read me get humbling SCO in the comments and winters are

140
00:08:37.611 --> 00:08:39.020
going to be announced next week.

141
00:08:39.050 --> 00:08:41.240
Please subscribe if you want to see more programming videos.

142
00:08:41.241 --> 00:08:44.750
Took up this related video and for now I've got to de noise my hair,

143
00:08:46.100 --> 00:08:47.420
so thanks for watching.

