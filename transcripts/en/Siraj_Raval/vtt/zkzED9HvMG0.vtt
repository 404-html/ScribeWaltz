WEBVTT

1
00:00:00.240 --> 00:00:03.120
Alexa,
get me a new outfit.
Yes.

2
00:00:03.290 --> 00:00:07.200
Saroj Nice.
Hello world.

3
00:00:07.201 --> 00:00:10.260
It's Saroj and let me show you how to build,

4
00:00:10.290 --> 00:00:15.290
train and serve an APP on AWS that predicts if customers are unhappy using a

5
00:00:16.441 --> 00:00:17.670
given mobile carrier.

6
00:00:17.880 --> 00:00:21.990
That way we can prevent them from leaving by giving them incentives to stay.

7
00:00:22.380 --> 00:00:24.690
You've probably heard about AWS.

8
00:00:24.720 --> 00:00:29.670
It's Amazon's gigantic collection of cloud computing services that offer

9
00:00:29.671 --> 00:00:32.670
developers and businesses,
databases,

10
00:00:32.700 --> 00:00:36.390
storage management tools,
analytics,
networking,

11
00:00:36.660 --> 00:00:38.790
deployment options,
Jeff Vasos.

12
00:00:38.820 --> 00:00:43.820
Basically everything you need to create and scale an APP to millions of users.

13
00:00:44.610 --> 00:00:49.610
Amazon currently commands a third of the cloud computing market that's three

14
00:00:49.951 --> 00:00:53.520
times more than it's next biggest competitor,
Microsoft.

15
00:00:53.880 --> 00:00:57.630
But why use the cloud in the first place?
Well,

16
00:00:57.660 --> 00:01:02.660
if you're doing some machine learning locally and try to load a Dataset into

17
00:01:03.451 --> 00:01:07.800
memory,
chances are you've seen an error like this before.

18
00:01:08.040 --> 00:01:13.040
That's because Ram was exhausted on your machine and the operating system

19
00:01:13.140 --> 00:01:17.370
couldn't allocate another,
say 500 megabytes of Ram.

20
00:01:17.670 --> 00:01:21.000
One possible solution could be to upgrade your ram,

21
00:01:21.210 --> 00:01:26.210
but another is to use a virtual machine in the cloud with more ram and Cpu

22
00:01:27.930 --> 00:01:32.760
because hundreds of thousands of customers are aggregated in the cloud.

23
00:01:33.060 --> 00:01:37.140
Aws can achieve higher economies of scale,

24
00:01:37.260 --> 00:01:42.260
which translates to lower pay as you go prices for the developer and you don't

25
00:01:42.511 --> 00:01:45.270
have to think too much about to computing power.

26
00:01:45.510 --> 00:01:48.840
Usually you either have too much or too little,

27
00:01:48.870 --> 00:01:53.870
but the cloud can give you just as much as you need whenever you need it.

28
00:01:55.050 --> 00:01:58.890
If we try and sign up for AWS on the website,

29
00:01:58.950 --> 00:02:03.750
it will briefly ask us for credential details and we can have an account set up

30
00:02:03.751 --> 00:02:07.500
pretty fast and they'll ask us for a credit or debit card.

31
00:02:07.530 --> 00:02:11.070
They won't charge unless we authorize it.
After this,

32
00:02:11.071 --> 00:02:16.071
we can see the AWS dashboard with a listing of every service it provides,

33
00:02:16.351 --> 00:02:21.351
and this can be overwhelming at first to any developer who first sees it.

34
00:02:21.990 --> 00:02:25.680
Since we're doing machine learning,
we know that we need compute,

35
00:02:25.830 --> 00:02:28.710
so let's go over some of these compute options.

36
00:02:28.740 --> 00:02:32.820
Starting with Ece to back when Soulja boy was cranking that.

37
00:02:33.030 --> 00:02:37.890
If we wanted to create an APP that could be accessible to people on the
internet,

38
00:02:38.070 --> 00:02:40.830
we'd need to buy or rent a server,

39
00:02:41.040 --> 00:02:44.430
but if our server got less traffic than we expected,

40
00:02:44.640 --> 00:02:48.150
we have a huge loss in revenue with easy to,

41
00:02:48.151 --> 00:02:50.040
we don't have to worry about that.

42
00:02:50.320 --> 00:02:55.320
Isi stands for elastic computing and it's a concept that represents the ability

43
00:02:56.221 --> 00:03:01.221
to automatically scale up and the amount of compute necessary for an

44
00:03:02.081 --> 00:03:02.950
application.

45
00:03:03.330 --> 00:03:08.290
Ece Two is a collection of globally distributed Linux servers,

46
00:03:08.500 --> 00:03:13.500
likely running a customized version of Red Hat enterprise Linux that allows

47
00:03:14.201 --> 00:03:19.201
customers to create virtual machines that use any one of a variety of operating

48
00:03:19.871 --> 00:03:23.020
systems from windows to free.
BSD.

49
00:03:23.260 --> 00:03:27.370
A virtual machine is a program that acts as a virtual computer.

50
00:03:27.370 --> 00:03:32.370
It runs on top of an operating system and has its own operating system.

51
00:03:33.190 --> 00:03:37.630
We can start with an easy two instance that fits our needs and configure it to

52
00:03:37.631 --> 00:03:42.631
scale up according to the traffic with whatever dependencies we need installed

53
00:03:42.701 --> 00:03:43.510
on it.

54
00:03:43.510 --> 00:03:48.460
But while he c two gives us a computing instance with an operating system,

55
00:03:48.730 --> 00:03:53.730
sometimes we don't need all of the tools that come with that operating system

56
00:03:54.520 --> 00:03:55.660
for our application.

57
00:03:55.720 --> 00:03:59.770
That's where the ECE two container service becomes useful.

58
00:04:00.010 --> 00:04:05.010
Containers let us only use the necessary libraries and tools to make our service

59
00:04:05.891 --> 00:04:10.891
work so it's more lightweight and efficient and if we don't want to configure

60
00:04:11.230 --> 00:04:15.980
any of the details of our easy two instance,
we can use light.

61
00:04:16.440 --> 00:04:21.370
It's a preconfigured easy two instance that makes deployment even faster.

62
00:04:21.730 --> 00:04:26.730
It's a trade off between ease of use and configurability and right in the middle

63
00:04:27.551 --> 00:04:30.250
of those two is elastic beanstock,

64
00:04:30.460 --> 00:04:33.700
which handles a lot of the deployment details for you,

65
00:04:33.970 --> 00:04:38.970
but also gives you more configurability than say light sail lambda lets you run

66
00:04:39.791 --> 00:04:44.791
code in response to events like changes to your Dataset or say image resizing

67
00:04:45.791 --> 00:04:50.791
that's necessary before a customer can classify an image they upload to your

68
00:04:51.101 --> 00:04:51.934
service.

69
00:04:52.270 --> 00:04:57.270
It executes your code only when it's needed and scales automatically from a few

70
00:04:57.561 --> 00:05:00.850
requests per day to thousands per second.

71
00:05:01.270 --> 00:05:05.110
And those are just the compute services.
There's a lot more.

72
00:05:05.290 --> 00:05:10.060
I've linked to this really great article that explains each in detail for you in

73
00:05:10.061 --> 00:05:14.200
the video description.
Now remember we're trying to do machine learning,

74
00:05:14.260 --> 00:05:18.280
not make some personal website and luckily for us,

75
00:05:18.340 --> 00:05:23.140
Amazon recently released a service called sage maker that makes the whole

76
00:05:23.141 --> 00:05:27.520
process much easier.
Sage maker,
let's machine learning,

77
00:05:27.521 --> 00:05:29.020
engineers build,

78
00:05:29.050 --> 00:05:32.710
train and deploy machine learning models easily.

79
00:05:33.250 --> 00:05:38.250
The build module provides us with a hosted environment to work with our Dataset,

80
00:05:39.010 --> 00:05:43.630
experiment with any algorithms and visualize our output.

81
00:05:43.660 --> 00:05:48.660
It provides fully managed easy two instances running Jupiter notebooks that let

82
00:05:48.791 --> 00:05:53.560
us explore training data,
preprocess it,
all of this in the cloud.

83
00:05:53.590 --> 00:05:54.423
Immediately.

84
00:05:54.790 --> 00:05:59.000
These notebooks come preloaded with Kuda tensorflow high torch.

85
00:05:59.330 --> 00:06:03.530
Basically all the tools you need to start training models in the cloud.

86
00:06:03.980 --> 00:06:08.000
It also provides its own algorithms in the form of an Sdk,

87
00:06:08.240 --> 00:06:13.240
both supervised like xg boost and logistic regression and unsupervised like

88
00:06:14.300 --> 00:06:17.990
principal component analysis and k means clustering.

89
00:06:18.050 --> 00:06:20.180
So let's get started with our APP.

90
00:06:20.450 --> 00:06:25.450
Our dataset is publicly available on Kaggle and contains about 3000 data points.

91
00:06:26.540 --> 00:06:31.540
Each data point has 21 attributes and describes the profile of a customer of an

92
00:06:32.060 --> 00:06:34.010
unknown us mobile operator,

93
00:06:34.011 --> 00:06:38.180
including their plan type and the amount of calls they make.

94
00:06:38.420 --> 00:06:42.470
So we'll first need to create an s three bucket to store.

95
00:06:42.471 --> 00:06:47.471
The data set will be using in s three stands for simple storage solutions.

96
00:06:48.260 --> 00:06:51.890
It manages data using an object storage architecture.

97
00:06:52.340 --> 00:06:57.340
Objects are called buckets and are the basic storage unit of s three we'll name.

98
00:06:57.891 --> 00:06:59.120
It's something that will later.

99
00:06:59.121 --> 00:07:03.170
Remember now to our notebook with the click of a button,

100
00:07:03.290 --> 00:07:08.290
we can create our own notebook instance that runs on AWS using sage and maker.

101
00:07:09.590 --> 00:07:11.710
We'll select the default instance type,

102
00:07:11.840 --> 00:07:16.190
create a new role for ourselves and spin up our notebook.

103
00:07:16.700 --> 00:07:18.440
Once we open up our notebook,

104
00:07:18.441 --> 00:07:23.390
we can test out the installation of several popular machine learning frameworks

105
00:07:23.510 --> 00:07:26.210
by importing them.
Then running that code.

106
00:07:26.570 --> 00:07:30.980
Let's start by specifying the s three bucket we just created as it's what we'll

107
00:07:30.981 --> 00:07:35.660
want to use for training data as well as the role we created for ourselves.

108
00:07:35.930 --> 00:07:40.610
Next will import.
The python libraries will need to predict churn,

109
00:07:40.940 --> 00:07:44.340
including of course sage maker in our data set.

110
00:07:44.360 --> 00:07:48.020
The last attribute is the one that matters most to us,

111
00:07:48.170 --> 00:07:51.830
whether or not the customer left the service or termed.

112
00:07:52.340 --> 00:07:55.610
It's a binary attribute and we'll use it as our label.

113
00:07:55.880 --> 00:07:59.990
It's what we want to learn the mapping of from the rest of the attributes,

114
00:07:59.991 --> 00:08:02.540
the input data to this,
our label.

115
00:08:02.630 --> 00:08:06.080
This is now considered a binary classification problem.

116
00:08:06.200 --> 00:08:09.590
Let's explore this dataset a bit using pandas.

117
00:08:09.770 --> 00:08:14.300
We can see how frequently each feature appears and using map plot live.

118
00:08:14.420 --> 00:08:17.870
We can visualize a histogram of the numeric features.

119
00:08:18.230 --> 00:08:23.230
Looks like only 14% of customers have turned and most of the numeric features

120
00:08:23.961 --> 00:08:25.640
are nicely distributed.

121
00:08:26.060 --> 00:08:30.590
It looks like they are evenly distributed geographically and more likely than

122
00:08:30.591 --> 00:08:33.560
not to have an international plan.
Also,

123
00:08:33.590 --> 00:08:38.360
it seems like some of our features have 100% correlation with one another,

124
00:08:38.480 --> 00:08:40.820
which means that we don't need them all.

125
00:08:40.850 --> 00:08:44.720
We can just remove some of them to avoid data redundancy.

126
00:08:45.290 --> 00:08:49.820
Now for our algorithm,
let's use one called xg boost,

127
00:08:49.880 --> 00:08:51.950
which is a bunch of decision trees.

128
00:08:52.280 --> 00:08:56.730
It actually goes by lots of different names like gradient boosted trees,

129
00:08:57.000 --> 00:09:01.830
multiple additive regression trees,
stochastic gradient boosting.

130
00:09:01.890 --> 00:09:06.690
Boosting is an ensemble technique where new models are added to correct the

131
00:09:06.720 --> 00:09:09.270
errors made by existing models.

132
00:09:09.630 --> 00:09:14.630
Models are added sequentially until no further improvements are possible.

133
00:09:15.240 --> 00:09:19.980
Gradient boosting is a method where new models are created that predict the

134
00:09:19.981 --> 00:09:24.981
errors of previous models and then added together to make the final prediction.

135
00:09:25.380 --> 00:09:30.380
It's using gradient descent to minimize the loss when adding new models.

136
00:09:30.900 --> 00:09:35.430
This method supports both regression and classification problems.

137
00:09:35.880 --> 00:09:40.880
Since it seems like there are some variables in our dataset where both high and

138
00:09:40.891 --> 00:09:45.390
low values can predict turn.
If we were to use linear regression,

139
00:09:45.420 --> 00:09:48.060
we need to generate some polynomial terms.

140
00:09:48.360 --> 00:09:53.360
It'd be better to use gradient boosted trees since they naturally account for

141
00:09:53.671 --> 00:09:58.671
non linear relationships between features and target variables while also

142
00:09:59.131 --> 00:10:02.820
accommodating complex interactions between features.

143
00:10:03.330 --> 00:10:07.860
We can start by regularizing our data so that our categorical features are

144
00:10:07.861 --> 00:10:10.020
converted into numeric features.

145
00:10:10.500 --> 00:10:15.500
Then we can split the data into training validation and test sets as this helps

146
00:10:16.501 --> 00:10:17.890
us prevent over fitting.

147
00:10:18.030 --> 00:10:21.720
This is when our motto can't generalize well enough to new data points.

148
00:10:22.170 --> 00:10:23.310
When we start training,

149
00:10:23.311 --> 00:10:27.630
we need to specify the location of the xg boost algorithm containers,

150
00:10:27.990 --> 00:10:30.780
and because we're training in the CSV format,

151
00:10:30.960 --> 00:10:35.960
we'll create s three inputs that are training function can use as a pointer to

152
00:10:36.211 --> 00:10:40.890
files.
In s three we can now decide on the values of our hyper parameters,

153
00:10:41.070 --> 00:10:44.700
how deep do we want each tree within the algorithm to go,

154
00:10:44.940 --> 00:10:46.440
how many boosting rounds?

155
00:10:46.710 --> 00:10:50.520
We can start by guessing these numbers and later on improve them.

156
00:10:50.910 --> 00:10:51.900
After training,

157
00:10:51.930 --> 00:10:56.930
we can deploy our model to a hosted end point and once we have that end point,

158
00:10:57.210 --> 00:11:02.210
we can run inference really easily as simple as making an http post request.

159
00:11:03.330 --> 00:11:05.490
When we run our model on our test data,

160
00:11:05.640 --> 00:11:09.690
we can see that it's mostly correct at predicting the customers who've turned.

161
00:11:09.930 --> 00:11:12.570
I hope you liked the video and it's time to start scaling.

162
00:11:12.870 --> 00:11:17.220
Please subscribe for more computer science videos and for now I've got a turn,

163
00:11:17.340 --> 00:11:18.720
so thanks for watching.

