WEBVTT

1
00:00:00.090 --> 00:00:01.530
Hello world,
it's the Raj.

2
00:00:01.531 --> 00:00:05.670
And today we're going to learn how to deploy a tensorflow model in production.

3
00:00:05.940 --> 00:00:08.850
And now I've never really talked about this before incredibly.

4
00:00:09.060 --> 00:00:13.140
I've talked about how to ride models,
how to train models,
how to test models,

5
00:00:13.290 --> 00:00:16.500
but I've never talked about how to actually deploy them in production so that

6
00:00:16.501 --> 00:00:20.790
you can use them at scale and a bunch of people can use them like hundreds of

7
00:00:20.790 --> 00:00:22.200
thousands or even millions of people.

8
00:00:22.540 --> 00:00:27.540
And now this is super important because this is the way to have your app be used

9
00:00:27.631 --> 00:00:28.680
by many people,
right?

10
00:00:28.860 --> 00:00:31.650
Whether it be in a mobile client that you can call the server,

11
00:00:31.830 --> 00:00:35.790
or if you just want to make a web app.
Both cases,
this applies to both cases.

12
00:00:35.820 --> 00:00:36.061
Okay?

13
00:00:36.061 --> 00:00:40.740
So what I'm gonna do is I'm going to talk about the architecture around this

14
00:00:40.741 --> 00:00:45.480
technology.
It's called tensorflow.
Serving is a separate library from tensorflow,

15
00:00:45.630 --> 00:00:48.900
but you use it with tensorflow.
I'm going to talk about how it works.

16
00:00:49.110 --> 00:00:53.100
And then what we're going to do is we're going to deploy a simple web app that

17
00:00:53.101 --> 00:00:58.101
lets a user upload photos and it's going to return a classification of what's in

18
00:00:58.291 --> 00:01:00.720
that photo via age.
Jason Response.
Okay.

19
00:01:00.721 --> 00:01:05.070
So let's start off with an explanation and then we're going to go over the code

20
00:01:05.071 --> 00:01:08.700
and then we're going to go through this step by step process of what it means to

21
00:01:08.730 --> 00:01:12.930
upload a trained model to a server and then call it from a client.
Okay?

22
00:01:12.960 --> 00:01:17.790
So that's what we're gonna do today,
deploying a tensorflow model in production.

23
00:01:18.090 --> 00:01:21.570
Okay,
so this is what the basic architecture looks like.
I mean,

24
00:01:21.571 --> 00:01:24.300
I have several images of what the architecture looks like,

25
00:01:24.540 --> 00:01:25.830
but this is one of them.
Okay,

26
00:01:25.831 --> 00:01:30.240
let me make sure that it's big enough to see just like that.
Cool.

27
00:01:30.600 --> 00:01:32.880
So here's what it looks like.
So first of all,

28
00:01:32.881 --> 00:01:34.680
let me talk a little bit about tensorflow serving.

29
00:01:34.681 --> 00:01:38.040
So tensorflow is the open source machine learning library that we all know and

30
00:01:38.041 --> 00:01:42.960
love and attention float serving was built to handle it on a server.

31
00:01:42.961 --> 00:01:43.980
And what do I mean by handle?

32
00:01:44.190 --> 00:01:48.870
I mean handling all the infrastructure that goes into building a model,
using it,

33
00:01:49.110 --> 00:01:53.670
uh,
versioning,
different versioning,
the model,
maintaining the lifecycle,

34
00:01:53.940 --> 00:01:58.110
uh,
dealing with,
uh,
which model you want to use.
Uh,

35
00:01:58.500 --> 00:02:02.340
taking the output from one model and making it the input for another model.

36
00:02:02.490 --> 00:02:06.600
It's actually pretty complex.
I mean,
Google uses this in production,
right?

37
00:02:06.601 --> 00:02:08.250
So it's gotta be pretty complex.

38
00:02:08.251 --> 00:02:12.360
Google's got like billions of requests probably every single day,
right?

39
00:02:12.361 --> 00:02:15.420
So we know that this is a battle tested approach.

40
00:02:15.750 --> 00:02:18.060
Now I know like Pi Torch is super hot right now.

41
00:02:18.090 --> 00:02:20.190
I'm like in Pi towards everybody's liking Pi Torch.

42
00:02:20.460 --> 00:02:22.530
But Pi Torch was meant for research,
right?

43
00:02:22.531 --> 00:02:26.970
Tentaflow was meant for production and it's,
it's been battle tested by Google.

44
00:02:26.971 --> 00:02:31.650
So if Google can use it,
we can use it.
Okay.
So in terms of the architecture,

45
00:02:31.651 --> 00:02:32.880
this is kind of what it looks like.

46
00:02:33.030 --> 00:02:35.070
There's a lot of different components that we're going to go over,

47
00:02:35.071 --> 00:02:39.750
but at a high level,
once you,
so this is all like,
just think of this,
this,
uh,

48
00:02:39.780 --> 00:02:42.390
these two green boxes at as the server,

49
00:02:42.391 --> 00:02:44.460
this is what's happening on the server server side.

50
00:02:45.000 --> 00:02:48.390
And then this blue box is the client and the client could be,
you know,

51
00:02:48.450 --> 00:02:49.960
a mobile app or uh,

52
00:02:50.140 --> 00:02:53.760
somebody's desktop for the Terminal v a variety of things.

53
00:02:54.090 --> 00:02:57.420
But so what's happening on the server?
Well,
we've got these models,
right?

54
00:02:57.610 --> 00:03:01.660
That could be the Model v one and V two.
They could be the same model,

55
00:03:01.661 --> 00:03:04.900
but different versions of it,
like newer version and an older version,

56
00:03:05.080 --> 00:03:08.160
or they could be two entirely separate models.
It doesn't matter.
It,

57
00:03:08.410 --> 00:03:12.340
it considers them both ways.
Right.
And you can,
you can tell a like,

58
00:03:12.520 --> 00:03:15.280
I want it to have multiple versions of one model.
Or You could say,

59
00:03:15.281 --> 00:03:18.220
I want multiple models,
right?
It depends on what you want to do.

60
00:03:18.221 --> 00:03:22.300
But we have a file system that's on the server that stores these models.

61
00:03:22.630 --> 00:03:27.190
And what happens is the library,
we'll load up a model.

62
00:03:27.490 --> 00:03:30.910
Okay.
And then it's going to see which version of the model that we want to use.

63
00:03:30.970 --> 00:03:32.860
All of these things we're going to specify,
right?
We're,

64
00:03:32.861 --> 00:03:34.420
we're going to specify programmatically.

65
00:03:34.840 --> 00:03:36.760
And then once it's got that latest version,

66
00:03:36.761 --> 00:03:39.130
let's just say we want to use the latest version of that model,

67
00:03:39.310 --> 00:03:41.080
then it's what you serve that model.

68
00:03:41.081 --> 00:03:44.620
And what do I mean by serve that that model then becomes the front facing

69
00:03:44.650 --> 00:03:48.760
interface to which any client can request via what's called a signature.

70
00:03:48.760 --> 00:03:49.720
And we'll talk about all this,

71
00:03:49.870 --> 00:03:53.260
but it's going to request it via a signature id and it's going to return the

72
00:03:53.261 --> 00:03:57.700
results of that model.
In this case,
it's going to be classification.
So,

73
00:03:57.730 --> 00:03:59.560
and so that's what the,
that's what the client does.

74
00:03:59.561 --> 00:04:01.210
And you see this term right here,

75
00:04:01.211 --> 00:04:05.980
grpc a grpc request and GRPC response.

76
00:04:06.150 --> 00:04:10.930
And you might be wondering,
what the Hell is this?
So Rpc,
uh,
is,

77
00:04:10.931 --> 00:04:14.110
let me,
let me pull this up.
RBC versus http.
This is a,

78
00:04:14.130 --> 00:04:18.130
this is a really good link for this,
but basically you've heard of rest,
right?

79
00:04:18.131 --> 00:04:20.590
So rest is like the way that we,

80
00:04:21.370 --> 00:04:24.400
we work with http API.
So eight.
So,
okay,

81
00:04:24.401 --> 00:04:27.340
so http is the protocol of the Internet,
right?

82
00:04:27.341 --> 00:04:31.210
It's how data flows across the web.
The most popular way.

83
00:04:31.211 --> 00:04:34.120
There's also like bit torrent and like other protocols,

84
00:04:34.480 --> 00:04:37.900
but the most popular way is http,
right?
All websites run on it.

85
00:04:38.110 --> 00:04:41.200
And RPC is a protocol that can,

86
00:04:41.650 --> 00:04:45.070
that is a PR is a protocol that can run on top of http.

87
00:04:45.220 --> 00:04:48.190
It's really not complicated at all.
I know it's like all these acronyms,

88
00:04:48.430 --> 00:04:51.760
but it's actually not complicated.
Like,
uh,
so,
uh,

89
00:04:52.120 --> 00:04:57.040
rest is like the default way,
right?
Mvc Web Apps,
node apps,
you know,
flask,

90
00:04:57.130 --> 00:04:58.720
they all use rest.
And what is rest?

91
00:04:58.810 --> 00:05:01.810
Well it's got these set of verbs and so the only difference,

92
00:05:02.050 --> 00:05:07.050
well the main difference with RPC and rest are that the verbs differ.

93
00:05:07.600 --> 00:05:11.560
So for rest you've got like get post,
put,
patch,
delete.

94
00:05:11.860 --> 00:05:16.720
Whereas with RPC you only have get and post and if you want to delete or do

95
00:05:16.721 --> 00:05:20.200
something else,
you to specify that inside of the request.

96
00:05:20.230 --> 00:05:23.080
So if you want to delete,
you would post a delete.

97
00:05:23.290 --> 00:05:27.160
And so one of the parameters of the post would be like a delete request.

98
00:05:27.310 --> 00:05:30.130
That's it.
And then there's like a bunch of like,
you know,
it's semantics,

99
00:05:30.131 --> 00:05:33.670
like beauty and design ability concerns.
It doesn't really matter.

100
00:05:33.671 --> 00:05:38.671
The point is that you can run this over http via an API just like anything else.

101
00:05:39.700 --> 00:05:40.930
But that's what RPC is.

102
00:05:41.110 --> 00:05:45.280
And GRPC is a type of RPC that's uh,
you know,

103
00:05:45.281 --> 00:05:48.640
tweaked in a certain way.
It's,
it's,
it's,
it's basically negligible.

104
00:05:48.641 --> 00:05:50.750
We don't have to,
we actually have to worry about it.
It's,
it's,

105
00:05:50.760 --> 00:05:54.010
it's happening under the hood,
but that's the high level architecture.

106
00:05:54.011 --> 00:05:58.400
You have a client a request,
uh,
it's,
it's either request a,

107
00:05:58.730 --> 00:06:03.050
it either gets or posts some data and then the server's going to return it.
Um,

108
00:06:03.160 --> 00:06:07.310
and the server contains the model.
So,
okay.

109
00:06:07.311 --> 00:06:11.150
So that's the super high level.
And then we've got this,
I've got this,
uh,

110
00:06:11.210 --> 00:06:12.170
nvidia graphic,

111
00:06:12.171 --> 00:06:16.730
but I wanted to show what inferences because TensorFlow's tentraflow serving is

112
00:06:16.731 --> 00:06:21.140
meant for inference for inference and inference means managing models,

113
00:06:21.260 --> 00:06:24.500
giving versioned access,
a reference counted lookup tables,
I.
E.

114
00:06:24.680 --> 00:06:29.600
Then HDP interface via RPC.
So we have an untrained network,

115
00:06:29.601 --> 00:06:33.590
right?
And then we train it.
The has some training data that is a training stage.

116
00:06:33.860 --> 00:06:36.530
And then once it's done training,
we have a trained model,

117
00:06:36.710 --> 00:06:40.700
then we can test it on our own data to make sure that it's classifying
correctly.

118
00:06:40.850 --> 00:06:42.020
And when we're done with that,

119
00:06:42.170 --> 00:06:45.710
then we can perform inference and inference is when we're applying the

120
00:06:45.711 --> 00:06:50.711
capability of this model to some new dataset that the consumer would probably be

121
00:06:50.721 --> 00:06:54.680
using,
right?
So that's what this is for.
It's for running inference on our models.

122
00:06:54.681 --> 00:06:57.470
That is for production use.
Okay.
So

123
00:06:59.060 --> 00:07:03.260
that's what it's for.
And it can serve multiple models simultaneously,
right?

124
00:07:03.261 --> 00:07:07.550
So we'll talk about how it does this,
but if this is great for ab testing,
right?

125
00:07:07.700 --> 00:07:09.410
Let's say you've got one trained model,

126
00:07:09.411 --> 00:07:13.280
let's say one is like a convolutional neural net with like 16 layers.

127
00:07:13.460 --> 00:07:14.810
It's performing really well,

128
00:07:14.930 --> 00:07:18.800
but then you've got this other kind of like experimental model that's like us,

129
00:07:18.830 --> 00:07:23.030
a convolutional and recurrent net mixed together and you want to test it out,

130
00:07:23.090 --> 00:07:26.180
test out which one works best.
You could put them on this,

131
00:07:26.300 --> 00:07:30.380
you could place them on the server and then have your half tentraflow serving,

132
00:07:30.381 --> 00:07:34.010
serve them the a client and then it could try out different versions of it.

133
00:07:34.220 --> 00:07:38.750
Or You could even have the different versions of the same model and then test

134
00:07:38.751 --> 00:07:42.170
out different versions of it.
So okay,

135
00:07:42.171 --> 00:07:47.000
so the one bad thing about this is that it's written in c plus plus tensorflow

136
00:07:47.001 --> 00:07:48.620
serving is written in c plus plus.

137
00:07:48.710 --> 00:07:51.320
So tensorflow is written in c plus plus as well.

138
00:07:51.500 --> 00:07:56.300
Because when you are making something industrial grade life for hundreds of

139
00:07:56.300 --> 00:07:57.133
thousands,
millions of requests,

140
00:07:57.440 --> 00:08:00.260
c plus plus is still like the most trusted way to do it.

141
00:08:00.410 --> 00:08:05.410
But tensorflow has got python bindings and sodas tensorflow serving but not like

142
00:08:05.451 --> 00:08:10.370
enough.
So we still have to deal with some c plus plus a.
But that's okay.
We will,

143
00:08:10.371 --> 00:08:15.110
we'll get through it.
Okay,
so now to get a little more specific,

144
00:08:15.111 --> 00:08:15.381
right,

145
00:08:15.381 --> 00:08:20.060
to get a little more specific with what this looks like programmatically we have

146
00:08:20.210 --> 00:08:22.520
this is,
this is kind of like what the architecture looks.
Let me,

147
00:08:22.550 --> 00:08:26.900
let me move this over here.
So there are four major components.

148
00:08:26.960 --> 00:08:30.530
Let me make this look a little smaller.
There are four major components here.

149
00:08:30.830 --> 00:08:34.370
Okay?
So we've got server bowls,
loaders,

150
00:08:34.430 --> 00:08:38.540
sources and managers,
and this is all happening within tensorflow serving.

151
00:08:38.541 --> 00:08:42.290
So remember this is separate from tensorflow for for the most part,

152
00:08:42.320 --> 00:08:45.320
if you just have one to train a model and put it on a server,

153
00:08:45.500 --> 00:08:47.840
you don't actually have to deal with any of this.
All you have to,

154
00:08:48.140 --> 00:08:51.770
all you have to worry about is creating a servable and it's going to do all the

155
00:08:51.771 --> 00:08:55.770
rest for you.
But if you want to,
you know,
have more fine grained control.

156
00:08:55.800 --> 00:08:59.310
It's good to know what these variables are.
Okay?
So if you don't care about this,

157
00:08:59.311 --> 00:09:04.040
just skip forward.
But if you care about this,
if you want to deal with,
um,

158
00:09:04.170 --> 00:09:07.530
different versions of your model,
if you want to make sure that,
uh,

159
00:09:07.590 --> 00:09:11.490
the lifecycle of your model is handled well,
then this is an important part.

160
00:09:11.730 --> 00:09:14.010
So we've got,
let's start off with sources,
right?

161
00:09:14.011 --> 00:09:17.430
So sources basically contained service goals.

162
00:09:17.460 --> 00:09:22.440
So a source is basically the gateway.
Think of it like a gateway.
You know,

163
00:09:22.441 --> 00:09:24.870
like in the tensor flow graph we have placeholders.

164
00:09:25.080 --> 00:09:30.060
So that's kind of a rough analogy as a gateway for data into the server,

165
00:09:30.120 --> 00:09:30.990
into the server,
right?

166
00:09:30.990 --> 00:09:35.160
That the tensorflow serving server as a source takes into serving bowls.

167
00:09:35.310 --> 00:09:38.760
That's more or less.
So the circles are the real fun part,
right?

168
00:09:38.761 --> 00:09:43.230
So you see here we have a source and it's,
it's pointing upward to a loader.

169
00:09:43.470 --> 00:09:45.290
And so it's what it,

170
00:09:45.300 --> 00:09:49.650
what a source contains is it contains server bowls and they sortable is the

171
00:09:49.651 --> 00:09:51.630
central abstraction of tensorflow,
right?

172
00:09:51.840 --> 00:09:54.810
They are the objects that clients use to perform computation.

173
00:09:55.050 --> 00:09:59.880
So I certainly will,
can contain one model.
It can contain multiple models.

174
00:10:00.000 --> 00:10:04.650
You can have one model perceivable or you can have one servable with like 20

175
00:10:04.651 --> 00:10:09.030
different models or you could have one servable with a different versions of the

176
00:10:09.031 --> 00:10:13.680
same model,
right?
So there's a lot of ways that you can architect a servable,

177
00:10:13.681 --> 00:10:17.610
but that's like the central,
the central class.
These are four classes,

178
00:10:17.611 --> 00:10:19.980
cervicals loaders,
sources and managers.

179
00:10:20.190 --> 00:10:24.570
But server are like the central class around which everything works around.
So,

180
00:10:24.810 --> 00:10:29.460
uh,
an analogy to tensorflow for a sort of a bowl would be like the computation

181
00:10:29.461 --> 00:10:33.030
graph,
like TF dot graph.
Everything is there to support the TF graph.

182
00:10:33.240 --> 00:10:35.670
And so servable is kind of like that for the server.

183
00:10:36.390 --> 00:10:39.660
So we've got a source and we feed it a servable.

184
00:10:39.661 --> 00:10:42.090
So we'll have some model and then we like,

185
00:10:42.330 --> 00:10:44.370
and then we will look at this programmatically,

186
00:10:44.400 --> 00:10:47.880
but basically we put a model in a servable,
put it into a source.

187
00:10:48.000 --> 00:10:51.660
The source feeds it to a loader.
And so this is the loader right here.

188
00:10:51.870 --> 00:10:56.580
And what the loader does is it just,
it's,
it's basically a placeholder.
Uh,
not,

189
00:10:56.581 --> 00:11:00.630
not a placeholder in terms of the gateway,
but it's a not a,
not a placeholder.

190
00:11:00.631 --> 00:11:03.300
A better word would be kind of like temporary storage,

191
00:11:03.480 --> 00:11:08.160
temporary storage for the manager to then come and manage that model's
lifecycle.

192
00:11:08.340 --> 00:11:10.080
So a manager is really cool.

193
00:11:10.140 --> 00:11:13.700
So a manager manager handles a full lifecycle observables.

194
00:11:13.740 --> 00:11:17.940
That means it decides when to load the model,
went to serve it for a client,

195
00:11:17.970 --> 00:11:22.500
went to unload it,
and it listens to the sources and tracks all the versions.

196
00:11:22.740 --> 00:11:24.960
So a source,
I forgot to say this part.

197
00:11:25.110 --> 00:11:29.130
So a source actually tracks different versions of the model,
right?
So when you,

198
00:11:29.190 --> 00:11:33.390
when you,
when you put a servable inside of a source,
then it's,
it's,

199
00:11:33.391 --> 00:11:36.510
it's got versions built in and what the source does is it is,

200
00:11:36.511 --> 00:11:40.400
it points to different versions and then it tells a loader when the loader loads

201
00:11:40.401 --> 00:11:43.710
it,
hey,
these are the different versions,
here's where they are in memory.

202
00:11:43.830 --> 00:11:47.050
And then the manager can come in and say,
right here,
this dynamic manager,

203
00:11:47.051 --> 00:11:49.170
it could say,
okay,
here's the version I want.

204
00:11:49.171 --> 00:11:51.810
And so that's what the word inspired version means,
right?

205
00:11:51.990 --> 00:11:55.150
Once we have our loader,
we feed the aspired version,
whatever we want,

206
00:11:55.330 --> 00:11:58.690
and we specify that in the request to the manager and the manager is going to be

207
00:11:58.691 --> 00:12:01.240
like,
okay,
I want this version.
Okay.

208
00:12:01.540 --> 00:12:06.060
And so and so once the manager knows what version of the model to syrup,

209
00:12:06.090 --> 00:12:10.870
it will serve it via this sortable handle.
And that's,
that's how it goes.
Right.

210
00:12:10.871 --> 00:12:12.820
And so the,
so you have,

211
00:12:12.880 --> 00:12:16.020
you can have streams of service bowls and what do I mean by streams?
It's,

212
00:12:16.021 --> 00:12:17.050
it's basically like get,

213
00:12:17.170 --> 00:12:19.870
you know how when you like commit constantly you have like new versions.

214
00:12:19.871 --> 00:12:21.220
It's just like that for a model.

215
00:12:21.490 --> 00:12:26.490
So basically this is your way of being able to deploy a model to production and

216
00:12:26.561 --> 00:12:29.800
then continue working on it locally.
And then if you have some new version of it,

217
00:12:29.801 --> 00:12:32.140
you can then just upload that new virgin to the,

218
00:12:32.200 --> 00:12:35.620
to the server and then it's going to save the state of the old version of the

219
00:12:35.621 --> 00:12:37.780
model and also serve the new version.

220
00:12:37.960 --> 00:12:41.380
And sometimes you might want to revert back to the old version and you can do so

221
00:12:41.530 --> 00:12:44.730
via its signature.
And we'll talk about that.
But,
uh,

222
00:12:45.040 --> 00:12:47.980
it's basically a two step process.
We're about to go into the code,

223
00:12:48.340 --> 00:12:51.700
but it's basically a two step process a source creates.

224
00:12:51.730 --> 00:12:54.610
So we put a cervical in a source,

225
00:12:54.700 --> 00:12:58.660
the source creates a loader and the loaders are sent as inspired version to the

226
00:12:58.661 --> 00:13:03.550
manager,
which then loads them and serves them to client requests.
Okay.
So I mean,

227
00:13:03.551 --> 00:13:05.620
that's a lot to take in all at once,
right?

228
00:13:05.621 --> 00:13:10.110
So what we should do is we should look at,
uh,

229
00:13:10.660 --> 00:13:14.620
what it means to create a model,
right?

230
00:13:14.621 --> 00:13:18.340
So or create a servable,
right?
So let's,

231
00:13:18.341 --> 00:13:22.780
let's just like go over this really quickly,
right?
So what this is,
let me,

232
00:13:23.140 --> 00:13:26.020
Ooh,
okay.
Okay.
So check this out.

233
00:13:27.310 --> 00:13:31.600
What this code is,
it's,
it's like 160 lines.
We're not going to go over all of it.

234
00:13:31.601 --> 00:13:34.690
We're just going to go over like some of the basic bits here so we can

235
00:13:34.691 --> 00:13:37.540
understand how these four classes will interact with our model.

236
00:13:37.810 --> 00:13:39.610
This is really simple code.
All right?

237
00:13:39.611 --> 00:13:44.530
All it does is it trains an Mni as t model to recognize,
you know,

238
00:13:44.560 --> 00:13:47.950
it's to recognize handwritten character digits,
right?
You have some characters,

239
00:13:47.951 --> 00:13:48.160
you know,

240
00:13:48.160 --> 00:13:51.520
images one through nine and they've got labels associated labels one through

241
00:13:51.521 --> 00:13:55.120
nine and we went over to the mapping between the labels and the images,
right?

242
00:13:55.420 --> 00:13:57.940
And so once it,
once our model is trained on this data,

243
00:13:58.030 --> 00:14:00.070
then if we feed it some random image,
like a,
you know,

244
00:14:00.071 --> 00:14:02.050
an image of the number three,
it can say,

245
00:14:02.080 --> 00:14:04.480
oh that's a three because it learned the mapping.

246
00:14:04.720 --> 00:14:09.550
So let's go over what this looks like.
We've got our imports,
right.

247
00:14:09.550 --> 00:14:13.360
This is tensorflow and tensorflow serving.
We've got some training flags.

248
00:14:13.630 --> 00:14:15.350
And then in our main function,
you know,
we,

249
00:14:15.440 --> 00:14:18.220
we basically specify how much we want to train and you know,

250
00:14:18.221 --> 00:14:21.910
different training options.
But you know,

251
00:14:21.930 --> 00:14:25.150
it's very similar process to all sorts of machine learning.
We read the,

252
00:14:25.210 --> 00:14:26.440
we first read the data,

253
00:14:26.770 --> 00:14:29.350
thankfully tensorflow has this read data sets function built in.

254
00:14:29.500 --> 00:14:33.640
We established our session and then we format our data that's so that it can be

255
00:14:33.641 --> 00:14:37.990
fit into our placeholders.
Then we build our model,
right?
Very simple model.

256
00:14:38.020 --> 00:14:42.480
It looks like it's a single layer network,
writes x,
y,
WB,
you know,

257
00:14:42.490 --> 00:14:46.120
one set of weights and biases.
And then we squash it with some,
uh,

258
00:14:46.121 --> 00:14:50.790
with a soft Max.
And we're using a cross entropy as our loss and,

259
00:14:50.830 --> 00:14:53.120
and dissent to minimize across entropy.

260
00:14:53.450 --> 00:14:58.450
And what that's gonna do is it's going to build a perceptron that we can then

261
00:14:58.881 --> 00:15:01.010
train.
So once we train this model,

262
00:15:01.340 --> 00:15:04.220
we're going to use a minibatch gradient descent,
so every,

263
00:15:04.430 --> 00:15:08.210
so we're going to train it on batches of 50 training examples at a time.

264
00:15:09.290 --> 00:15:12.650
And it's going to,
when it's done training,
we'll print out done training.

265
00:15:12.770 --> 00:15:15.290
So that has nothing to do with tensorflow serving.

266
00:15:15.350 --> 00:15:16.640
That's all just like tensorflow,
right?

267
00:15:16.641 --> 00:15:19.580
This is just how you would build and train a model.

268
00:15:19.670 --> 00:15:21.500
Now here's the tension flow serving part,
right?

269
00:15:21.680 --> 00:15:24.170
So once we built this model locally,
we've trained it,

270
00:15:24.350 --> 00:15:26.510
now it's time for us to save it,
right?

271
00:15:26.511 --> 00:15:30.320
Because we want to take this model that we've trained locally and we want to,

272
00:15:30.370 --> 00:15:33.800
oh actually we could train it locally,
but it could also be on the server.

273
00:15:33.920 --> 00:15:35.630
So for all intensive purposes,

274
00:15:35.810 --> 00:15:38.900
let's just say that this is going to be on the server.
Okay?

275
00:15:38.901 --> 00:15:43.850
So what we'll do is we'll say,
uh,
we'll say,

276
00:15:43.851 --> 00:15:45.380
okay,
so let's,

277
00:15:47.060 --> 00:15:50.690
let's define where we want to save it.
So we'll create an export path.

278
00:15:50.691 --> 00:15:55.340
Like we want to save it right here.
And once we do that,
then we can say,
okay,

279
00:15:55.370 --> 00:15:57.950
we've got,
you know,
we've got to saved path for this model.

280
00:15:58.220 --> 00:16:00.470
And this could be on the server or on the client,
right?

281
00:16:01.190 --> 00:16:03.140
Cause you might want to train it on the server as well.

282
00:16:03.320 --> 00:16:05.930
But the point is that we've trained a model and now we want to save it.

283
00:16:06.170 --> 00:16:10.130
So then we'll create a servable from our model.

284
00:16:10.131 --> 00:16:13.040
So like we've got this model and we want to create a servable.

285
00:16:13.280 --> 00:16:15.970
And that's what we do with this method right here.
This,
uh,

286
00:16:16.040 --> 00:16:18.980
saved model builder and a sort of bubble is a class,
right?

287
00:16:18.981 --> 00:16:22.890
It's a class and it's how we feed this model to our,
uh,

288
00:16:23.130 --> 00:16:26.570
tensorflow serving main server class function,

289
00:16:26.630 --> 00:16:29.420
our main tensorflow serving class,
right?

290
00:16:29.421 --> 00:16:32.930
So then it could do all of that manager loading sources,
all that stuff.

291
00:16:33.290 --> 00:16:36.140
But we save our model just like this,
using the saved model builder,

292
00:16:36.350 --> 00:16:38.840
it's going to save a snapshot of it so we can later,

293
00:16:38.900 --> 00:16:41.570
it could later be loaded for inference and it can,

294
00:16:41.571 --> 00:16:45.440
we can save as many versions as we want.
We said that we wanted to do,
you know,

295
00:16:45.441 --> 00:16:47.030
we,
we defined the model version,

296
00:16:47.270 --> 00:16:50.180
but we can create an entire stream of these models.
Like we want,
you know,

297
00:16:50.300 --> 00:16:52.220
30 different versions.
But to keep it simple,

298
00:16:52.430 --> 00:16:54.500
we're just going to save one version of the model.
It's good,

299
00:16:54.740 --> 00:16:58.880
it's gotta be one survivable.
And once we do that,

300
00:16:59.540 --> 00:17:03.000
then what it's,
what is going to do is,
uh,
the,
the,
the,

301
00:17:03.050 --> 00:17:06.650
the main serving file is it's going to create a source out of it.

302
00:17:06.830 --> 00:17:10.970
And the source is going to house state.
So there could be multiple service roles,

303
00:17:11.150 --> 00:17:12.200
multiple versions,

304
00:17:12.380 --> 00:17:15.140
and then it's going to create a loader and it's going to use this function,

305
00:17:15.141 --> 00:17:18.740
the TF dot saved model,
loader dot load to load that source.

306
00:17:18.980 --> 00:17:23.020
And then a manager class is going to decide how to handle its lifecycle,

307
00:17:23.170 --> 00:17:25.460
when to serve it,
when not to.
Okay.

308
00:17:25.490 --> 00:17:30.110
So all of this for is going to be in the main class,

309
00:17:30.230 --> 00:17:35.060
but all we've done done here is we've created a servable so that we can then

310
00:17:35.150 --> 00:17:38.930
load this up via a the serving file,

311
00:17:38.931 --> 00:17:42.890
the main tensorflow serving file.
And so what we're doing right now is,

312
00:17:42.980 --> 00:17:45.590
so here's a,
here's,
here's the,
here's another very important part.

313
00:17:45.800 --> 00:17:50.160
We are creating a signature map.
And so the signature map is how we,

314
00:17:50.161 --> 00:17:54.000
these are parameters that we can embed into our [inaudible] class.

315
00:17:54.120 --> 00:17:59.120
So that are 10 truckloads servings are tensorflow serving main class knows which

316
00:18:00.391 --> 00:18:04.920
model to load up.
So we're going to basically give identities,
ids,

317
00:18:05.100 --> 00:18:09.870
Ids,
our signatures to all of our components.
We'll give it to our inputs,

318
00:18:09.990 --> 00:18:11.130
we'll give it to our outputs,

319
00:18:11.250 --> 00:18:16.020
we'll give it to our class itself and some build info.

320
00:18:16.170 --> 00:18:19.770
And so were all of these,
this is basically Metadata,
right?

321
00:18:19.771 --> 00:18:23.880
We're giving a bunch of Metadata to the server in the form of a servable so it

322
00:18:23.881 --> 00:18:25.920
knows what model it is and it also knows,

323
00:18:26.160 --> 00:18:31.160
you know what the ideas are for all of its components so that later on whether

324
00:18:31.501 --> 00:18:34.830
on the server or on the client,
we can reference those parts.

325
00:18:34.950 --> 00:18:37.560
Maybe we want to pull apart like we want to see the output,

326
00:18:37.740 --> 00:18:39.600
maybe we want to pull the whole model.

327
00:18:39.720 --> 00:18:43.110
Maybe we want to take the output from one model and feed it to it as the input

328
00:18:43.111 --> 00:18:45.510
to a different model.
That's what signatures are for.

329
00:18:45.630 --> 00:18:49.530
So we define it as metadata and so that's why we use service goals instead of

330
00:18:49.531 --> 00:18:52.860
just like straight up models.
By abstracting that a little bit more,

331
00:18:52.980 --> 00:18:56.060
we can add metadata to it so that you know are managed.

332
00:18:56.060 --> 00:18:59.880
Both our manager on the server can know what to interact with as well as our

333
00:18:59.881 --> 00:19:04.260
client.
Our client can know which model and which input,
which output to,

334
00:19:04.470 --> 00:19:07.260
to manipulate.
Okay.
So,

335
00:19:07.980 --> 00:19:09.930
and then we add them all into that builder.

336
00:19:10.040 --> 00:19:13.530
You have the ad metrograph and variables function and then save it.
Right.

337
00:19:13.531 --> 00:19:17.580
And once we saved it,
then we can call it from our main server file.

338
00:19:18.270 --> 00:19:18.660
Okay.

339
00:19:18.660 --> 00:19:22.920
And so I just wanted to go over that to like quickly show how those four main

340
00:19:22.921 --> 00:19:27.390
classes would interact with a simple trained model.
Okay.
So,

341
00:19:28.860 --> 00:19:31.800
so let's do this.
Let's build this.
Okay.
So let me,

342
00:19:31.830 --> 00:19:36.030
let me start out by saying like,
this is,
this is a,
uh,
it's definitely doable.

343
00:19:36.060 --> 00:19:39.450
It's definitely doable,
but it's not like some walking the park to run this.

344
00:19:39.451 --> 00:19:42.240
I had to like try this out a couple of times before Dylan,

345
00:19:42.300 --> 00:19:44.550
before having it work successfully.

346
00:19:44.790 --> 00:19:48.870
But there's a lot of like possible errors that can happen due to,

347
00:19:48.871 --> 00:19:53.800
of course dependencies.
But,
uh,
if you,
and you know,
for whatever reason it's,

348
00:19:53.801 --> 00:19:58.440
it's that way,
but,
uh,
it's going to take some work to get this to work.

349
00:19:58.470 --> 00:20:01.840
Let me just say I,
that's all I'm trying to say.
But if you put in the time,
uh,

350
00:20:01.950 --> 00:20:05.460
then you can easily do it.
Okay.
So let's do it right now.

351
00:20:05.461 --> 00:20:08.570
And so some of these commands are going to take awhile.

352
00:20:08.670 --> 00:20:11.340
So I'm gonna like edit forward through those commands cause they can take like

353
00:20:11.341 --> 00:20:15.300
20 minutes or 30 minutes to,
to load up all those dependencies,
right?

354
00:20:15.600 --> 00:20:20.400
So before we,
so before we do anything,
let's set up our development environment,

355
00:20:20.460 --> 00:20:22.580
right?
So we,

356
00:20:22.590 --> 00:20:27.270
what we want to do is we want to download this tensorflow serving repo,
right?

357
00:20:27.540 --> 00:20:31.080
And what we're gonna do is we're going to,
uh,

358
00:20:32.280 --> 00:20:32.881
create,

359
00:20:32.881 --> 00:20:36.060
create a docker image for all of those dependencies because it's got a lot of

360
00:20:36.061 --> 00:20:38.460
dependencies.
So we don't have to use docker,

361
00:20:38.550 --> 00:20:41.940
but we will use docker because it's easier.
So docker,

362
00:20:41.941 --> 00:20:43.560
for those of you who don't know,

363
00:20:43.740 --> 00:20:46.950
is basically like a super lightweight virtual machine.
So what is it virtual,

364
00:20:47.680 --> 00:20:52.680
have you ever used parallels on like Linux or Mac based and to run windows?

365
00:20:54.100 --> 00:20:56.410
So sometimes you want to run,
run a game,
right?

366
00:20:56.530 --> 00:20:58.720
So you would download a virtual machine,

367
00:20:58.870 --> 00:21:02.740
which is basically an entire operating system just to run that game.

368
00:21:02.741 --> 00:21:06.820
Because that game has dependencies that are operating system specific.

369
00:21:07.840 --> 00:21:12.840
But sometimes we don't want to have to download an entire operating system just

370
00:21:12.881 --> 00:21:14.950
because we want to run one APP.
Right?

371
00:21:15.010 --> 00:21:19.600
So what would be better if we had a lightweight virtual machine that was super,

372
00:21:19.601 --> 00:21:20.410
super small,

373
00:21:20.410 --> 00:21:24.340
but it contained all the dependencies we need for the APP that we're trying to

374
00:21:24.341 --> 00:21:26.320
build.
And that's what docker is.

375
00:21:26.650 --> 00:21:30.370
Docker basically con contains all the dependencies that we need.

376
00:21:30.371 --> 00:21:33.250
So like let's say one APP needs python too,

377
00:21:33.430 --> 00:21:37.120
but another APP needs python three and we,
and we had,

378
00:21:37.180 --> 00:21:39.520
we only have one version of python on our system.

379
00:21:39.910 --> 00:21:44.410
We could use docker to contain self contained,
one of those apps,

380
00:21:44.740 --> 00:21:48.760
download python to all the other dependencies and then those versions wouldn't

381
00:21:48.850 --> 00:21:52.990
affect any of the versions on the rest of our system.
It's all self contained.

382
00:21:53.140 --> 00:21:54.100
So it's super cool.

383
00:21:54.340 --> 00:21:59.340
I should be using docker a lot more often and I will eventually,

384
00:22:00.220 --> 00:22:01.360
uh,
but yeah,

385
00:22:02.290 --> 00:22:05.680
that's a little refresher or tutorial on what docker is.

386
00:22:05.920 --> 00:22:08.490
And here's some installed instructions.
It's in the,
it's in the,
uh,

387
00:22:08.660 --> 00:22:12.040
I python notebook.
Okay.
So let's go ahead and download this baby.

388
00:22:12.370 --> 00:22:16.660
So the first step is to clone the repo.
Oh,
hold on.

389
00:22:17.410 --> 00:22:18.730
Okay.
Sound effects.

390
00:22:19.150 --> 00:22:22.760
Let's first clone the repo and we'll put that,

391
00:22:22.900 --> 00:22:26.480
we'll use this recursive flag because it's got a few,
uh,

392
00:22:26.800 --> 00:22:30.610
dependency repos that we will need.

393
00:22:31.220 --> 00:22:31.870
And so it's gonna,

394
00:22:31.870 --> 00:22:36.730
it's gonna basically clone all of those and it could take awhile.
Okay.

395
00:22:36.731 --> 00:22:41.050
And so once we have that,
we can then CD into that serving directory.
Okay.

396
00:22:41.051 --> 00:22:44.950
And so here we are,
we have tensorflow serving,
so we've got that file locally.

397
00:22:45.250 --> 00:22:48.400
And so this file has a lot of dependencies that we're going to need,
right?

398
00:22:48.640 --> 00:22:53.620
And so what we can do
is we can say it,
so this,
this,

399
00:22:53.621 --> 00:22:58.570
this file has a docker file or this repo has a,

400
00:22:59.470 --> 00:23:02.260
um,
has a docker file.

401
00:23:02.590 --> 00:23:06.200
And what the docker file does is it says,
Hey,
these are the dependents,

402
00:23:06.370 --> 00:23:08.290
these are the dependencies that you're going to need.

403
00:23:08.500 --> 00:23:10.960
So let's just see what that docker file looks like.
See,

404
00:23:11.170 --> 00:23:12.760
so here's what the docker file looks like.

405
00:23:12.761 --> 00:23:17.620
It's saying you got to get all these dependencies,
right?
You got to then,

406
00:23:17.650 --> 00:23:18.310
uh,
you know,

407
00:23:18.310 --> 00:23:22.870
if you don't have pip and saw that you got to get the GRPC client for running

408
00:23:22.871 --> 00:23:24.010
those http requests,

409
00:23:24.180 --> 00:23:27.640
you got to set up Bazell so we don't want to have to do this ourselves,
right?

410
00:23:27.640 --> 00:23:30.850
So that's what this docker file is going to do.
It's going to do all this,

411
00:23:30.970 --> 00:23:32.590
create a docker image out of it,

412
00:23:32.591 --> 00:23:37.210
and then we can run our docker image and then self contained in that all of our

413
00:23:37.211 --> 00:23:40.180
dependencies are going to be there and we can run a recode is normal,
right?

414
00:23:40.300 --> 00:23:44.770
By the way,
Bazell is Google's tool for downloading,
um,
dependencies.

415
00:23:44.800 --> 00:23:48.050
Very useful,
very useful tool.
But yeah,
that,

416
00:23:48.060 --> 00:23:51.380
that's what the docker file looks like internally.
And so once we have that,

417
00:23:52.280 --> 00:23:55.190
we can build it.
So let's build this.

418
00:23:55.191 --> 00:23:59.570
It's going to create a docker image with all the required dependencies for us.

419
00:23:59.840 --> 00:24:03.150
So let's go ahead and do that.
Boom.

420
00:24:11.250 --> 00:24:15.030
Okay,
great.
Successfully built.
And so now we've done that.

421
00:24:15.031 --> 00:24:17.490
Now we can go ahead and run the container locally.

422
00:24:17.700 --> 00:24:20.340
So this is going to do is it's going to run our doc,

423
00:24:20.520 --> 00:24:24.420
it's going to run this docker image that we created using the docker demon.
So,

424
00:24:24.480 --> 00:24:28.770
or the Damon,
however you say it.
Daemon Daemon,
I'd never know.
But yeah,

425
00:24:28.771 --> 00:24:32.880
the docker Daemon,
I'm going to say Damon.
Um,
cause it's not evil.
It's great.

426
00:24:33.060 --> 00:24:37.810
So let's go ahead and run it into docker Daemon and we're going to specify,
uh,

427
00:24:37.860 --> 00:24:41.220
the bill that we just defined,
right?
We called it tensorflow container.

428
00:24:41.460 --> 00:24:45.950
And so now we can run it.
Okay.

429
00:24:48.000 --> 00:24:50.250
Error Response.
That's cause I have docker already running.

430
00:24:50.251 --> 00:24:53.910
So let me quit docker and then rerun it docker.

431
00:24:55.180 --> 00:25:00.000
Okay,
no time for that.
And then see what's going on here.

432
00:25:00.030 --> 00:25:02.070
Dockers running.
Okay.

433
00:25:15.730 --> 00:25:19.450
Okay,
so now,
okay,
we are in the docker container,
right?

434
00:25:19.720 --> 00:25:23.680
We can see what's in here.
I make sure this is bigger.
Boom.
Okay.

435
00:25:23.860 --> 00:25:28.390
Make that a lot bigger.
Okay,
so we've created our docker container.
Here it is.

436
00:25:28.600 --> 00:25:32.560
And now that we're in this container,
we've got all of our,
uh,

437
00:25:32.620 --> 00:25:36.760
we've initialize it using our docker image,
our docker file,

438
00:25:36.910 --> 00:25:39.590
and now we can clone our repo into,
and now that we're here,

439
00:25:39.650 --> 00:25:41.980
that now that we're in here,
so we'll do the same thing.

440
00:25:41.981 --> 00:25:44.740
Now that we're inside of our image and this time we're cloning it,

441
00:25:44.741 --> 00:25:48.700
not for the docker file,
we're cloning it for the related files,
right?

442
00:25:48.701 --> 00:25:50.560
The actual tentraflow serving,
right?

443
00:25:50.650 --> 00:25:53.110
We're in the image and now we can download those files to,

444
00:25:53.120 --> 00:25:57.310
it's going to take awhile.
Okay.
So then once it's downloaded,
uh,

445
00:25:57.580 --> 00:26:01.570
via get,
now we can CD intuit what,
see what we got here.

446
00:26:01.571 --> 00:26:04.360
We've got a CD into it and then configure it.
Okay.

447
00:26:04.361 --> 00:26:08.080
And what that's going to do is it's going to use Google's basil bill tool.

448
00:26:08.350 --> 00:26:12.910
It's going to use Google's Bazell build tool from inside our container,

449
00:26:13.030 --> 00:26:15.700
and then it's going to download all of those third party dependencies.

450
00:26:16.000 --> 00:26:17.650
So we'll CD into serving,

451
00:26:19.450 --> 00:26:24.100
and then we're going to run the configure.
Uh,
let's see,

452
00:26:24.190 --> 00:26:27.820
CD serving,
then tensorflow,
tensorflow,

453
00:26:28.090 --> 00:26:32.020
and then configure it.
Boom.
Okay.

454
00:26:33.580 --> 00:26:36.790
It's going to ask or specify the location of a bunch of things.

455
00:26:41.850 --> 00:26:46.620
Do I want Mkl?
No.
Uh,
no.

456
00:26:46.650 --> 00:26:49.260
Default.
Default.

457
00:26:51.720 --> 00:26:54.240
Default.
I mean,
oh shit.

458
00:26:57.130 --> 00:27:00.850
Dammit.
Let me Redo this.
Configure.
Okay.

459
00:27:01.540 --> 00:27:06.070
Um,
MKL sport default.
Default.

460
00:27:06.100 --> 00:27:10.720
Default.
Yes.
I want Google cloud platform.
Yes.
Hadoop

461
00:27:13.180 --> 00:27:17.320
don't care about the experimental compiler verbs.
Um,

462
00:27:17.740 --> 00:27:22.300
don't care.
Open cl don't care.
Kuda normally,
yes,

463
00:27:22.301 --> 00:27:27.220
but for all intensive purposes where this tutorial,
I don't care.
NPI,
no.
Okay.

464
00:27:27.250 --> 00:27:30.880
And that is going to do it.
Okay.
So then while that runs,

465
00:27:31.210 --> 00:27:34.360
the dependencies that we got that we're going to need our tentraflow serving the

466
00:27:34.361 --> 00:27:39.310
library and d pretrained inception model,
right.
The inception model is going to,

467
00:27:39.610 --> 00:27:42.340
let me show you what the inception model is,
by the way.
Basically,

468
00:27:42.341 --> 00:27:47.200
it's a huge convolutional neural network that won the 2014 image net competition

469
00:27:47.290 --> 00:27:48.490
with state of the art results.

470
00:27:48.700 --> 00:27:52.780
They train it on hundreds of thousands of images and this is like a compressed

471
00:27:52.781 --> 00:27:55.180
view of it,
right?
It's the actual view is up here,

472
00:27:55.210 --> 00:27:58.000
but then the compressed view is up here.
It's got a bunch of layers,

473
00:27:58.001 --> 00:27:59.860
like put like a hundred layers.

474
00:28:00.100 --> 00:28:03.760
But basically we don't have to train our whole model,
right?

475
00:28:03.761 --> 00:28:06.220
We could if we wanted to,
but we're going to use transfer learning.

476
00:28:06.221 --> 00:28:10.630
We're going to use this pre trained model for our own use case.
Okay.
So that,

477
00:28:10.660 --> 00:28:12.250
that means we don't have to deal with the training part,

478
00:28:12.280 --> 00:28:16.700
at least for this example.
Okay.
And so then once we have that are,

479
00:28:16.930 --> 00:28:20.440
once we have our,
uh,
Bazell dependencies built,

480
00:28:20.680 --> 00:28:24.880
we can then test it out by,
uh,
uh,
sorry,

481
00:28:24.940 --> 00:28:26.210
once we have our

482
00:28:29.930 --> 00:28:34.780
repo configured,
then we can build our dependencies using Bazell.
Right?

483
00:28:34.781 --> 00:28:38.360
So we'll say Bazell build,
and that's going to take 20 to 50 minutes.

484
00:28:38.420 --> 00:28:41.360
It's going to take a while to build those dependencies,
okay?

485
00:28:41.600 --> 00:28:43.700
And then we can run our model server,
right?

486
00:28:43.701 --> 00:28:46.220
So this is going to take a while to do a base.
We'll build,

487
00:28:46.230 --> 00:28:51.230
but it's done configuring and now we can do Bazell build and it's going to take

488
00:28:51.631 --> 00:28:55.280
a while.
Okay.
And so once it's built,
we have our models server built.

489
00:28:55.430 --> 00:28:57.440
Now we're gonna move on to the second dependency.

490
00:28:57.441 --> 00:29:00.230
And the second dependency is downloading inception,
right?

491
00:29:00.231 --> 00:29:05.231
So we're going to curl that into this repo and it's going to pull it.

492
00:29:06.170 --> 00:29:10.130
It's going to download it just like that.
And while it's downloading,
we can see,

493
00:29:10.131 --> 00:29:11.270
well,
what's the next step?

494
00:29:12.870 --> 00:29:13.250
<v 1>Yeah,</v>

495
00:29:13.250 --> 00:29:18.080
<v 0>the next step is to run it and the server locally is to just run it,</v>

496
00:29:18.081 --> 00:29:22.010
right?
We want to run.
So,
so look at this,
um,
command here.

497
00:29:22.130 --> 00:29:25.890
So while that downloads,
we'll look at the command.

498
00:29:25.891 --> 00:29:28.580
So what we're going to do is once it's downloaded,
we're going to untarnished,

499
00:29:28.610 --> 00:29:33.130
we're gonna,
you know,
uncompress it,
and then we're going to,
uh,

500
00:29:33.950 --> 00:29:37.850
save it as a checkpoint.
And then once it's saved as a check point,

501
00:29:38.090 --> 00:29:42.690
we can then refer to it in our flow model server like that.

502
00:29:42.700 --> 00:29:45.500
That's the main file of tensorflow serving,
right?
That's,

503
00:29:45.501 --> 00:29:48.190
that's the model that we want to use.
So at the beginning,

504
00:29:48.191 --> 00:29:52.030
I talked about this custom m and t model,
but for the sake of this tutorial,

505
00:29:52.031 --> 00:29:53.140
to keep it short and simple,

506
00:29:53.290 --> 00:29:56.540
we're going to use this pretrained model and it's going to feed it and we're

507
00:29:56.541 --> 00:29:58.570
going to feed it in and be at this parameter model name.

508
00:29:58.870 --> 00:30:00.640
And then the path of where it's located.

509
00:30:00.940 --> 00:30:04.570
And then it's going to run the server with that model is going to do everything

510
00:30:04.571 --> 00:30:08.080
necessary,
is going to check,
the version is going to feed it into a loader.

511
00:30:08.140 --> 00:30:10.690
The manager is going to maintain its lifecycle and then we can,

512
00:30:10.750 --> 00:30:13.210
we can call it from our clients.

513
00:30:13.330 --> 00:30:18.220
So right now the server is local and then we can call it with our client.
Okay.

514
00:30:18.221 --> 00:30:22.450
So,
uh,
right.
So this is going to take a while.
Let me,

515
00:30:22.630 --> 00:30:25.660
let me fast forward.
So then once we have our server running,

516
00:30:25.840 --> 00:30:30.220
then we can call it via our client.
So this is what our clients,
it looks like,

517
00:30:30.580 --> 00:30:34.120
let me show you the client,
right?

518
00:30:36.610 --> 00:30:40.330
So the client looks like this very small,
very small,

519
00:30:40.630 --> 00:30:45.630
but all it does is it's sending a grpc request and it's going to receive a

520
00:30:45.671 --> 00:30:48.310
response,
right?
So it's going to send a request.

521
00:30:48.580 --> 00:30:50.830
And so this is why we had signatures,
right?

522
00:30:50.831 --> 00:30:52.960
This is why we named signatures as our parameter.

523
00:30:53.020 --> 00:30:56.230
And so once we have these signatures,
we can make the request.

524
00:30:56.410 --> 00:30:59.710
That's all it does is it's making a simple request that the client file,
let's,

525
00:30:59.800 --> 00:31:02.830
there's two files that are,
that we,
that we've talked about,

526
00:31:02.831 --> 00:31:04.000
right this client file.

527
00:31:04.240 --> 00:31:09.240
And then we had that custom file that on the server that's going to train that

528
00:31:09.640 --> 00:31:14.200
model for us and then create a servable with it so that our main TentaFlow

529
00:31:14.230 --> 00:31:18.100
serving file,
which has this huge c plus plus file can deal with,

530
00:31:18.310 --> 00:31:21.820
can handle its lifecycle that we don't have to touch unless we want to be very

531
00:31:21.821 --> 00:31:26.610
specific,
but not for the purposes of this tutorial.
Okay.
So,
okay,

532
00:31:26.620 --> 00:31:28.930
where were we?
So,
uh,
we,

533
00:31:29.410 --> 00:31:33.250
once we've built our server,
it's running locally,

534
00:31:33.460 --> 00:31:37.990
then we can use that client to then take this image,
which is a,

535
00:31:38.110 --> 00:31:41.620
what does this image?
It's a panda is a panda image.

536
00:31:41.860 --> 00:31:45.700
Cute Little Panda Kung Fu panda.
And then we're going to,

537
00:31:45.701 --> 00:31:48.010
we're going to pull it from the web and then we're going to send it to the

538
00:31:48.011 --> 00:31:52.030
client via this,
uh,
via this parameter right here,
image.

539
00:31:52.210 --> 00:31:54.250
And I'm using the inception client.
If it works,

540
00:31:54.490 --> 00:31:57.580
we're going to see a of classification output in terminal from the server.

541
00:31:57.700 --> 00:31:59.080
So this server is local.

542
00:31:59.200 --> 00:32:03.310
So if we run this right swimming,

543
00:32:03.311 --> 00:32:08.230
we haven't locally or we run low,
I had to file locally.
So if I run this,

544
00:32:08.500 --> 00:32:11.350
we get a response,
we get a response back from the server.

545
00:32:11.351 --> 00:32:13.000
And so the server's going to send a response,

546
00:32:13.210 --> 00:32:17.830
the string values or the likelihood for e or the or the classification values.

547
00:32:17.980 --> 00:32:19.970
And then we had the likelihoods for each,

548
00:32:20.020 --> 00:32:24.100
the likelihoods for each of these classification labels down here as well as

549
00:32:24.101 --> 00:32:28.510
floats.
But yeah,
see Cha,
Chinese panda up hand to a black,
white panda,
you know,

550
00:32:28.540 --> 00:32:32.620
Cardigan pretty bad.
But yeah,
the best results are up there at the top.

551
00:32:32.830 --> 00:32:36.670
And so that,
so we get those results back as,
as,
as,
um,

552
00:32:37.570 --> 00:32:41.000
as Jason.
And so then that's,
that's it,
right?

553
00:32:41.001 --> 00:32:42.980
So now we have the server running locally.

554
00:32:43.100 --> 00:32:46.010
We have our clients and now we want to put it on a server,
right?

555
00:32:46.011 --> 00:32:48.590
We don't want to have a whole local want to put this thing on the server.

556
00:32:48.770 --> 00:32:51.260
So to do that,
we're going to push it to Google cloud.

557
00:32:51.530 --> 00:32:56.530
And so Google has this great tutorial on how to do that and it's going to be

558
00:32:56.571 --> 00:32:58.940
using Coobernetti's.
And what is Kubernetes?

559
00:32:59.180 --> 00:33:01.610
Kubernetes is automatic container management.

560
00:33:01.610 --> 00:33:05.270
So this is what Google uses internally and it's basically production grade

561
00:33:05.510 --> 00:33:09.200
container management.
So you can have multiple containers,
multiple models,

562
00:33:09.380 --> 00:33:13.430
and basically there's a lot of infrastructure that goes around dealing with

563
00:33:13.431 --> 00:33:16.160
these huge models and when to use them and stuff.

564
00:33:16.370 --> 00:33:17.480
And basically it's like a little,

565
00:33:17.510 --> 00:33:21.140
it's a tool that helps tend to float serving serve models.

566
00:33:21.320 --> 00:33:25.310
And so it's running on the cloud and we can,
that's,

567
00:33:25.550 --> 00:33:27.800
that's how we're,
that's what we're going to use.

568
00:33:27.801 --> 00:33:31.640
So we can go right to this part too cause we did all the other parts.
Okay.

569
00:33:31.641 --> 00:33:33.230
So basically this is how it works,
right?

570
00:33:33.231 --> 00:33:37.010
So you log into Google cloud via terminal.
So assuming you've created account,

571
00:33:37.011 --> 00:33:41.000
you log in to be a terminal and then you create a container cluster.
Okay.

572
00:33:41.001 --> 00:33:44.720
And so this cluster is going to just contain all of your,
uh,

573
00:33:44.900 --> 00:33:47.330
your server code,
right?

574
00:33:47.330 --> 00:33:51.890
So that the whole tensorflow serving repo as well as if you have some custom

575
00:33:51.891 --> 00:33:56.150
model or inception that that goes in the container,
everything but the clients,

576
00:33:56.180 --> 00:33:58.820
right?
Tensorflow surveying and the custom model,

577
00:33:59.000 --> 00:34:02.810
both are both are contained in a single container.

578
00:34:03.140 --> 00:34:05.780
And then we say,
okay,

579
00:34:06.470 --> 00:34:09.470
since Google cloud is so tightly integrated with Coopernetti's,

580
00:34:09.680 --> 00:34:12.500
we can do this all in a series of commands so we can,

581
00:34:12.590 --> 00:34:15.710
so we can figure Google cloud to say here's a container that we want.

582
00:34:15.890 --> 00:34:19.550
Now that we've created it,
here's the one we want.
And this is the,

583
00:34:19.610 --> 00:34:23.330
that see this inception serving cluster is what we're going to put in that

584
00:34:23.331 --> 00:34:25.430
container.
And then once we have that,

585
00:34:25.460 --> 00:34:30.140
we can upload the docker image once we tag it with some value to that container

586
00:34:30.200 --> 00:34:32.900
in Google cloud.
And once we've done that,

587
00:34:34.670 --> 00:34:37.210
now we're going to deploy three replicates,

588
00:34:37.280 --> 00:34:41.900
three replicas of the inception inference server using Coobernetti's,
right?

589
00:34:41.900 --> 00:34:45.830
So the,
so the rectal cause are exposed externally by a Kubernetes service.

590
00:34:46.130 --> 00:34:48.830
And what Kubernetes does is it says,
well,

591
00:34:48.831 --> 00:34:50.870
I know you have several versions of this container.

592
00:34:51.020 --> 00:34:55.710
Let me pick which one I want to use and we can then create a,
a Coobernetti's,

593
00:34:55.960 --> 00:35:00.830
a configuration file and then deploy our container to it just using these two

594
00:35:00.831 --> 00:35:05.330
commands.
And finally,
when that's done,
we can query the model,
right?

595
00:35:05.331 --> 00:35:07.880
Just like that.
And it's going to do the same thing.

596
00:35:07.881 --> 00:35:12.560
Return a Jason file that we can then,
you know,
read and whatever else.

597
00:35:12.740 --> 00:35:17.420
Okay.
So that's the high level of how that works.
Uh,
the code is in the,

598
00:35:17.480 --> 00:35:21.020
is in the get hub repo.
It's in the description.
Definitely check it out.

599
00:35:21.350 --> 00:35:25.910
And uh,
God commented it extensively.

600
00:35:26.210 --> 00:35:30.200
What else?
So,
okay,
so to end this session,
let me answer some questions from,

601
00:35:30.201 --> 00:35:32.870
from the channel and then we're out of here.
Okay.

602
00:35:32.871 --> 00:35:35.420
So what kinds of questions do we have here?

603
00:35:39.660 --> 00:35:41.820
Eyes are two questions randomly.

604
00:35:44.940 --> 00:35:45.773
Question wine.

605
00:35:49.850 --> 00:35:50.683
<v 1>Okay.</v>

606
00:35:52.300 --> 00:35:55.300
<v 0>So here's a question from my differential neuro computer video.</v>

607
00:35:55.480 --> 00:35:57.610
How does it understand the order of problems?

608
00:35:57.611 --> 00:36:01.210
Like how can 100% to process text first,
then a graph problem?

609
00:36:01.300 --> 00:36:03.640
Why doesn't it just go to the graph neural network first?

610
00:36:03.790 --> 00:36:06.370
So it doesn't know the order.
We tell it the order.

611
00:36:06.490 --> 00:36:11.230
So we have to define what we have to train it on one data set and then we can

612
00:36:11.231 --> 00:36:12.490
train it on a different dataset.

613
00:36:12.640 --> 00:36:16.150
The differentiable neuro computer is definitely like the dopest model I've ever

614
00:36:16.151 --> 00:36:20.890
seen before because it's so generalized and it can do so many different things.

615
00:36:20.950 --> 00:36:24.380
So once we train it on one Dataset,
we didn't have to,
uh,

616
00:36:24.610 --> 00:36:29.350
synchronously train on the next data set and then just keep going like that.
Uh,

617
00:36:29.680 --> 00:36:33.250
but he doesn't know to say,
well,
let me first look at this graph and then let me,

618
00:36:33.640 --> 00:36:34.690
let me first look at this,
you know,

619
00:36:34.691 --> 00:36:38.350
subway graph and then let me look at the text.
You've got to train it in order.

620
00:36:38.530 --> 00:36:40.570
And so one more question.
Um,

621
00:36:45.280 --> 00:36:48.860
can you please explain how to save the models in tensorflow once the training is

622
00:36:48.861 --> 00:36:51.760
done?
So that can be utilized later whenever I want.

623
00:36:51.910 --> 00:36:54.490
Or is there any other video in which you explained that?

624
00:36:54.700 --> 00:36:59.590
So like we talked about right here is how we saved those models.

625
00:37:00.100 --> 00:37:01.180
Okay.
And so

626
00:37:02.760 --> 00:37:07.620
we save it just like this was defined and export path.

627
00:37:07.800 --> 00:37:10.650
And then we build it using this.
So basically,

628
00:37:10.680 --> 00:37:14.610
so a server bowl contains a checkpoint file,

629
00:37:14.730 --> 00:37:18.540
the PB file,
all the related Metadata,
things that come with the model.

630
00:37:18.750 --> 00:37:21.000
And the reason we even use this,
right?

631
00:37:21.001 --> 00:37:25.800
Why w w why do we even use this is because how else are you supposed to run a

632
00:37:25.801 --> 00:37:27.120
model in the cloud?
Right?

633
00:37:27.270 --> 00:37:31.500
That's why you need a tensorflow specific library because these computation

634
00:37:31.501 --> 00:37:34.470
graphs need some kind of environment to run in.

635
00:37:34.890 --> 00:37:39.090
And that's why it's very useful to have tensorflow serving as a file.

636
00:37:39.270 --> 00:37:42.510
If I were to,
if I were to build a model in production,
right?

637
00:37:42.511 --> 00:37:46.080
If I wanted to build an APP at scale that hundreds of thousands of people would

638
00:37:46.081 --> 00:37:48.710
use,
I would use tensorflow serving.
I did.

639
00:37:48.930 --> 00:37:53.930
There is no better tool out there for production in terms of reliability and in

640
00:37:53.941 --> 00:37:55.020
terms of scalability.

641
00:37:55.050 --> 00:37:58.290
So those are the two criteria that you should look for when dealing with

642
00:37:58.890 --> 00:38:01.830
production.
Great apps.
Okay.
So yeah,
that's it for this tutorial.

643
00:38:02.130 --> 00:38:04.650
If you like this video,
please subscribe for more like it.

644
00:38:04.860 --> 00:38:07.560
And for now I'm going to say Coobernetti's three times fast.

645
00:38:07.830 --> 00:38:09.060
So thanks for watching.

