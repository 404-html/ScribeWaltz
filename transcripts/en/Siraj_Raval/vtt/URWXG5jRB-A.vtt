WEBVTT

1
00:00:00.030 --> 00:00:03.600
Hello world,
it's Siraj.
And today we're going to talk about deep minds,

2
00:00:03.630 --> 00:00:06.420
starcraft two AI environment.

3
00:00:06.690 --> 00:00:11.690
So recently they're released this starcraft two environment that lets you train

4
00:00:12.330 --> 00:00:15.450
reinforcement learning models.
And that's what you're saying right now.

5
00:00:15.451 --> 00:00:16.710
It's a demo of it happening.

6
00:00:17.040 --> 00:00:22.040
But basically you can use starcraft two the game as a test bed to train and run

7
00:00:23.610 --> 00:00:27.930
your AI models on.
And these can be reinforcement learning models.

8
00:00:28.020 --> 00:00:30.660
That can be deep learning models.
That can be really anything.

9
00:00:30.661 --> 00:00:33.600
They can just be scripts that aren't,
that aren't even machine learning,

10
00:00:33.601 --> 00:00:37.680
just like hardcoded bots,
whatever.
But the point is that it's,

11
00:00:37.810 --> 00:00:42.810
it's meant to be a test bed for people to train and test their AI models on.

12
00:00:43.620 --> 00:00:47.430
So it's a really exciting time right now for deep reinforcement learning as a

13
00:00:47.431 --> 00:00:52.040
field because open AI beat Dota two,
uh,

14
00:00:52.140 --> 00:00:53.850
recently in the,
in the world champion,

15
00:00:53.851 --> 00:00:58.440
they'd beat the world champion at Dota two.
And then after that the mine,
uh,

16
00:00:58.570 --> 00:01:00.210
releases starcraft two AI environment.

17
00:01:00.390 --> 00:01:03.060
So there's a lot of exciting things happening right now in deep reinforcement

18
00:01:03.061 --> 00:01:03.540
learning.

19
00:01:03.540 --> 00:01:08.160
There's a lot of low hanging fruit in this section of machine machine learning

20
00:01:08.370 --> 00:01:12.480
as opposed to supervise learning where a lot of it has been solved more or less,

21
00:01:12.481 --> 00:01:14.340
right?
Gradient based optimization.

22
00:01:14.490 --> 00:01:17.670
You compute the gradient and then you update your weights and you know,

23
00:01:17.671 --> 00:01:21.030
your labels.
It's,
it's very,
you know,
it's,
it's been tried and done before,

24
00:01:21.031 --> 00:01:24.570
but for deep reinforcement learning,
there's a lot of unanswered questions.

25
00:01:24.571 --> 00:01:26.100
So it's hard.
It's a really exciting time.

26
00:01:26.101 --> 00:01:27.900
Right now what we're gonna do is we're going to,

27
00:01:27.930 --> 00:01:32.550
we're going to run a pre trained model and what I'm gonna do is I'm going to on

28
00:01:32.551 --> 00:01:37.500
my machine set up and install all the required dependencies and uh,

29
00:01:37.560 --> 00:01:38.191
the script,

30
00:01:38.191 --> 00:01:43.050
everything you need basically to go from zero to having starcraft two running on

31
00:01:43.051 --> 00:01:48.051
your computer with deep mines environment installed and modeled a pretrained

32
00:01:48.210 --> 00:01:51.270
model running.
Okay.
So what the,

33
00:01:51.271 --> 00:01:55.170
what the model is is it's called a deep Q learner.
I'll talk about what that is,

34
00:01:55.171 --> 00:01:58.470
but it's a deep Q learner and it's going to be running on the collect mineral

35
00:01:58.471 --> 00:02:00.690
shards mitty game of starcraft two,

36
00:02:00.930 --> 00:02:04.440
which means it's just the Bot that's collecting little trinkets called mineral

37
00:02:04.441 --> 00:02:08.070
shards.
And it'll do this autonomously without you needing to do anything.

38
00:02:08.250 --> 00:02:12.090
And so from there you could modify it or run your own algorithms,

39
00:02:12.091 --> 00:02:16.260
but once you have something set up,
it'll be a lot easier to,
to get into the,

40
00:02:16.920 --> 00:02:21.120
the,
uh,
the,
the bottom of things.
And what,
what am I doing with my hands?
Okay,

41
00:02:21.150 --> 00:02:24.420
so let's get to this for a second.
Okay.
So first things first.

42
00:02:24.450 --> 00:02:26.040
What does the history here,
so deep minds,

43
00:02:26.041 --> 00:02:29.880
first attempt at running games simulations came,
uh,

44
00:02:29.910 --> 00:02:32.820
for Atari Games,
right?
They,
that's why Google bought them.

45
00:02:32.940 --> 00:02:36.100
They created an algorithm called the deep Q learner,
which is it,

46
00:02:36.180 --> 00:02:37.740
which is the algorithm that we're running.

47
00:02:38.310 --> 00:02:41.430
And they use that algorithm to be any Atari game.

48
00:02:41.460 --> 00:02:45.000
And the way they did this is they combine two different ideas in machine

49
00:02:45.001 --> 00:02:47.550
learning.
They combined the idea of deep learning,

50
00:02:47.700 --> 00:02:49.680
which is all about learning features.

51
00:02:49.710 --> 00:02:52.200
You don't have to engineer what those features are like.

52
00:02:52.260 --> 00:02:55.920
I'm looking for a dog that has long ears and it has brown for no,

53
00:02:55.921 --> 00:03:00.880
it will learn what the necessary features are to map what it sees to some label

54
00:03:01.060 --> 00:03:06.060
and so they used a convolutional neural network for this to to create features

55
00:03:06.670 --> 00:03:09.880
learned dense representations from game screens,
right?

56
00:03:09.970 --> 00:03:12.160
So all I got where the pixels of the game,

57
00:03:12.161 --> 00:03:16.960
the pixels and then it learns dense representations from those pixels and then

58
00:03:16.961 --> 00:03:21.961
it con and then it converted what it saw into an output and that output was an

59
00:03:22.210 --> 00:03:24.250
up,
down left or right value,
right.

60
00:03:25.240 --> 00:03:28.900
Anything that you can use on a joystick for an Atari game.
And the,

61
00:03:28.910 --> 00:03:32.350
the way it did this is it didn't just take in the input from the what they saw

62
00:03:32.351 --> 00:03:35.920
in the game.
It also use what's called Q learning.

63
00:03:36.370 --> 00:03:40.030
It's cure learning is a type of reinforcement learning where we initialize

64
00:03:40.031 --> 00:03:43.350
what's known as a Q matrix and the Q matrix has a,

65
00:03:43.420 --> 00:03:48.420
has a collection of possible actions that an agent can take in a game and all of

66
00:03:48.731 --> 00:03:53.530
these actions are weighted.
Like this is an okay action,
this is a better action.

67
00:03:53.590 --> 00:03:55.390
This action could be the best action.

68
00:03:55.630 --> 00:04:00.630
And what it does is it picks an action from the Q matrix using some strategy

69
00:04:00.880 --> 00:04:05.260
that you decide.
It could be random,
it could be based on some pre weighted value,

70
00:04:05.261 --> 00:04:08.860
like an epsilon,
whatever.
But you pick an action from the Q matrix,

71
00:04:09.070 --> 00:04:10.780
you perform it in the game,

72
00:04:10.900 --> 00:04:14.650
you observe what's happening and then you see if you got a reward or not,
right?

73
00:04:14.770 --> 00:04:17.980
A plus one and minus one.
And then based on that reward,

74
00:04:18.100 --> 00:04:22.600
you'll update the Q matrix so that the actions are all weighted differently.

75
00:04:22.601 --> 00:04:23.020
Right?

76
00:04:23.020 --> 00:04:27.550
And so the idea is that eventually the Q matrix will have the best actions for

77
00:04:27.551 --> 00:04:30.430
you to perform at whatever time step you're in.
Right?

78
00:04:30.550 --> 00:04:34.300
The Q matrix acts as a weight similar to how in a neural network,

79
00:04:34.301 --> 00:04:38.050
the weights improve over time in reinforcement learning in cue learning,

80
00:04:38.290 --> 00:04:40.630
the Q matrix improves over time.

81
00:04:40.750 --> 00:04:44.920
So they combined both of those ideas together,
right?

82
00:04:44.921 --> 00:04:48.400
So deep cue learning and the way it does this is,

83
00:04:48.401 --> 00:04:51.910
and I've got a little bit of pseudo code here for how it works and this is the

84
00:04:51.911 --> 00:04:54.610
full pseudo code for how their algorithm works.

85
00:04:54.760 --> 00:04:59.470
Now keep in mind that that their algorithm wasn't just a simple,
you know,

86
00:04:59.590 --> 00:05:04.300
take a convolutional network and then run Q learning on top of it.
It's also got,

87
00:05:04.580 --> 00:05:09.520
uh,
two different features from neuroscience that uh,

88
00:05:09.550 --> 00:05:11.290
the first one is called replay memory.

89
00:05:11.500 --> 00:05:16.500
And replay memory is essentially a buffer in memory that's a temporary buffer

90
00:05:16.690 --> 00:05:19.750
that stores a state's actions and rewards.

91
00:05:20.440 --> 00:05:23.470
Basically your experience of what,
what,
what has happened in the game.

92
00:05:23.830 --> 00:05:25.350
And this basically this is to,

93
00:05:25.351 --> 00:05:29.890
this improved their AI when they use this concept of replay memory that acted as

94
00:05:29.891 --> 00:05:34.450
a temporary buffer that they could pull actions from as well as the Q matrix.

95
00:05:35.140 --> 00:05:39.670
So if we look at this a pseudocode,
we'll see that first initialize it,

96
00:05:39.671 --> 00:05:44.560
some replay memory matrix and initialize as a Q matrix randomly and it observes

97
00:05:44.561 --> 00:05:48.000
the initial state of the game.
Where are we in the game?
And that would be the,

98
00:05:48.001 --> 00:05:51.940
the collection of pixels that at first cs,
and then it runs the training loop.

99
00:05:52.060 --> 00:05:55.360
So in the training loop,
it will select an action from the Q matrix,

100
00:05:55.510 --> 00:06:00.510
either randomly by using some probability value epsilon or by bike selecting the

101
00:06:00.650 --> 00:06:05.650
optimal Q value from the cue that the optimal action from the Q matrix that it

102
00:06:05.961 --> 00:06:08.540
sees and then it will execute that action.

103
00:06:08.600 --> 00:06:12.650
In Open Ai's universe environment,
they call this the step right,

104
00:06:12.680 --> 00:06:14.060
the environment step function.

105
00:06:14.180 --> 00:06:18.530
So it will execute the action and then observed the reward that it receives and

106
00:06:18.531 --> 00:06:22.700
it will store the this,
the,
the state,
the reward,

107
00:06:22.790 --> 00:06:26.360
the old state and the action.
All of those that that,
those,

108
00:06:26.520 --> 00:06:28.880
those four values into replay memory.

109
00:06:29.210 --> 00:06:33.830
And then the next step is for it to a compute a loss function.

110
00:06:33.831 --> 00:06:35.720
And then we can see the loss function here.

111
00:06:35.840 --> 00:06:38.450
It's also called the bellmen equation.
Okay.

112
00:06:38.451 --> 00:06:41.780
This is also called a bellman equation.
Not to get you to confuse,

113
00:06:41.781 --> 00:06:43.220
but that's just what it's called as well.

114
00:06:43.430 --> 00:06:47.960
But basically it will randomly sample from the replay memory and then it will

115
00:06:47.961 --> 00:06:49.010
use a sample that it,

116
00:06:49.040 --> 00:06:54.040
that it retrieved to compute a loss function and then it will minimize the

117
00:06:54.291 --> 00:06:57.590
square of that loss function at every iteration.

118
00:06:58.400 --> 00:07:00.350
And as a loss function is minimized,

119
00:07:00.530 --> 00:07:04.100
the Q matrix is values are improved at every time step,

120
00:07:04.280 --> 00:07:08.480
which means that eventually every time the agent pulls an action from the Q

121
00:07:08.481 --> 00:07:09.140
matrix,

122
00:07:09.140 --> 00:07:12.800
it's going to be more and more optimal such that it's going to minimize the
loss,

123
00:07:12.920 --> 00:07:16.180
which will maximize the reward it receives.
I know that's,

124
00:07:16.210 --> 00:07:20.060
that's quite a mouthful to to,
you know,
say in 30 seconds be,

125
00:07:20.061 --> 00:07:24.470
but that's,
that's how the deep Q learner works at a high level.

126
00:07:24.620 --> 00:07:29.570
The next step for them was to try it out on go that the ancient Chinese game of

127
00:07:29.571 --> 00:07:34.400
go.
So a lot of AI experts,
experts said that it would take 10 years,
20 years,

128
00:07:34.401 --> 00:07:39.230
30 years for an AI to be able to beat the game of go because there were so many

129
00:07:39.231 --> 00:07:40.280
possibilities,
right?

130
00:07:40.281 --> 00:07:45.281
There's so many possibilities and the search space is far too vast for an AI to

131
00:07:46.581 --> 00:07:48.710
just brute force through all the options.

132
00:07:49.010 --> 00:07:52.820
There are so many different combinations of game states that it's just too hard

133
00:07:52.821 --> 00:07:54.850
for an AI to compute with our,

134
00:07:54.860 --> 00:07:58.100
with the limits of computing power that we have now,
that was their thinking,

135
00:07:58.250 --> 00:08:02.210
but they were wrong because they're always wrong when it comes to deep learning

136
00:08:02.211 --> 00:08:06.590
and all of these new technologies,
but basically Alphago was their attempt.

137
00:08:06.650 --> 00:08:11.330
They're successful attempt at beating the game of go and they used,

138
00:08:11.480 --> 00:08:14.540
they use two different neural networks here.
They actually use three.

139
00:08:14.570 --> 00:08:16.190
They use three different neural networks,

140
00:08:16.310 --> 00:08:17.720
these three different neural networks here.

141
00:08:17.840 --> 00:08:22.610
One was for the policy network and one was for the value network and both of

142
00:08:22.611 --> 00:08:26.900
these computer,
two different values.
One is the policy and that,

143
00:08:26.901 --> 00:08:28.160
and the next is the value.

144
00:08:28.280 --> 00:08:33.140
And it used both the policy and the value to help it help guide what is

145
00:08:33.141 --> 00:08:35.810
essentially a gigantic tree search.

146
00:08:36.020 --> 00:08:39.560
And the tree search is called a Monte Carlo tree search.

147
00:08:39.740 --> 00:08:42.440
And here's a brief description of how it works,

148
00:08:42.620 --> 00:08:47.620
but basically the Monte Carlo tree search or MCTs t simulates an a search tree

149
00:08:48.860 --> 00:08:53.860
and it's the Ai selects an action at each time step based on the action value

150
00:08:54.440 --> 00:08:55.273
and prior,

151
00:08:55.530 --> 00:09:00.060
which is the output of the policy network and some exploration parameter.

152
00:09:00.300 --> 00:09:05.250
So it uses the values of the policy network and the value network as guides to

153
00:09:05.251 --> 00:09:09.990
help it search the tree of possible moves that it can play at every time step.

154
00:09:10.110 --> 00:09:15.110
And they trained Alphago on tens of thousands of hours of expert gameplay and

155
00:09:15.811 --> 00:09:17.910
then they gave it,

156
00:09:18.600 --> 00:09:22.920
they gave Alpha go to the world champion Lisa Dole,
and it beat Lisa Dole.

157
00:09:23.550 --> 00:09:28.530
So beating the game of go was a much harder challenge than beating the 20

158
00:09:28.531 --> 00:09:32.400
different Atari Games given just the pixel values of the game screen.
Right.

159
00:09:32.700 --> 00:09:37.590
But now more recently,
they decided let's,
let's up the ante even more.
Right?

160
00:09:37.830 --> 00:09:40.740
And they decided,
let's not just do this alone.
Let's,

161
00:09:40.741 --> 00:09:45.630
let's open source this so that everybody gets to use this technology,
right?

162
00:09:45.780 --> 00:09:49.770
So starcraft,
starcraft is arguably one of,

163
00:09:49.800 --> 00:09:54.690
if not the greatest PC game of all times,
PC fanboys come at me.
But anyway,

164
00:09:54.990 --> 00:09:58.050
starcraft is one of the best PC games of all time,

165
00:09:58.290 --> 00:10:01.650
and it's got hundreds of thousands of players across the world.

166
00:10:01.830 --> 00:10:06.830
It's got people whose day job is to just play starcraft all day competitively,

167
00:10:06.961 --> 00:10:09.430
right?
In South Korea,
specifically,
uh,

168
00:10:09.510 --> 00:10:14.250
much closer to South Korea if you're out there.
But anyway,
so,
uh,
so,

169
00:10:14.251 --> 00:10:18.420
uh,
starcraft two huge awesome game.
If you've never played it before,

170
00:10:18.600 --> 00:10:21.510
then this is a great opportunity to download it.
It's free.

171
00:10:21.570 --> 00:10:23.640
I'll show you a little bit about that in a second.

172
00:10:24.060 --> 00:10:25.470
And if you have played it before,

173
00:10:25.530 --> 00:10:28.410
this is a great way to help improve your own strategy,
right?

174
00:10:28.530 --> 00:10:31.020
Because when you're building an AI for starcraft two,

175
00:10:31.140 --> 00:10:35.670
you're thinking about all the things that requires to be a good starcraft
player,

176
00:10:35.671 --> 00:10:39.120
right?
When you're thinking about when you should spend your wealth,

177
00:10:39.210 --> 00:10:42.000
how you should build your army,
what you should invest,

178
00:10:42.001 --> 00:10:45.090
your resources and time and energy and all of these things.

179
00:10:45.091 --> 00:10:48.030
You're going to try to want to replicate in an AI that you built.

180
00:10:49.050 --> 00:10:51.960
And so if you think about an AI for starcraft there,

181
00:10:51.961 --> 00:10:56.040
it has to be able to do a bunch of things that are quite difficult.
First of all,

182
00:10:56.130 --> 00:10:58.230
it's got to have an effective use of memory,
right?

183
00:10:58.380 --> 00:11:01.860
It's gotta be able to remember not just the things that have that have happened

184
00:11:02.010 --> 00:11:02.910
in the short term.

185
00:11:03.090 --> 00:11:06.270
It's got to be able to remember the things that happened back in the longterm,

186
00:11:06.300 --> 00:11:08.490
in the past and not just in the past.

187
00:11:08.640 --> 00:11:12.490
It's got to be able to plan over a long period of time.
Sometimes a,

188
00:11:12.510 --> 00:11:17.420
you want to make decisions that help maximize your current value,
right?

189
00:11:17.520 --> 00:11:21.540
You,
you want to kill an enemy because the enemy is next to your,
you know,

190
00:11:21.570 --> 00:11:22.740
vulnerable troops.

191
00:11:23.130 --> 00:11:27.900
But other times you want to make an action that is not as intuitive in the short

192
00:11:27.901 --> 00:11:30.360
term,
but in the longterm it is,
right?
Like,

193
00:11:30.361 --> 00:11:33.900
sometimes you want to spend a lot of money on some resource and so you're going

194
00:11:33.901 --> 00:11:36.040
to have a little money right now,
but in the longterm,

195
00:11:36.180 --> 00:11:40.020
that resource that you purchased is going to help you a lot more,
right?

196
00:11:40.021 --> 00:11:43.260
So it's not as,
it's not as,
you know,

197
00:11:43.290 --> 00:11:47.700
obviously intuitive as something like an Atari game where it's just like all you

198
00:11:47.701 --> 00:11:51.180
have to do is defeat,
you know,

199
00:11:51.210 --> 00:11:55.930
get from point a to point B or remove some block or till all the aliens on the

200
00:11:55.931 --> 00:11:57.280
screen.
It's not that simple,
right?

201
00:11:57.490 --> 00:12:01.750
And even as something as even some task as simple as expand your base to some

202
00:12:01.751 --> 00:12:06.190
location is actually pretty complicated.
You have to coordinate mouse clicks,

203
00:12:06.340 --> 00:12:08.200
your camera available resources.

204
00:12:08.350 --> 00:12:12.340
And what this does is it makes actions and planning hierarchical.

205
00:12:12.520 --> 00:12:17.380
And this is generally very hard for reinforcement learning algorithms to grasp.

206
00:12:17.530 --> 00:12:21.910
The concept of hierarchy is quite hard for reinforcement learning algorithms to

207
00:12:21.911 --> 00:12:25.570
grasp,
right?
Because you're performing an action and you're receiving a reward,

208
00:12:25.720 --> 00:12:27.730
right?
So there's the agent environment loop.

209
00:12:27.910 --> 00:12:31.930
It's not like a deep learning where we have all of these layers and there's all

210
00:12:31.931 --> 00:12:34.420
of this structure that's built over time.

211
00:12:34.600 --> 00:12:38.950
So deep cue learning was one good example of having a hierarchical structure,

212
00:12:39.100 --> 00:12:42.940
a hierarchical model in a reinforcement learning environment and I think it was

213
00:12:43.340 --> 00:12:44.350
one of the first,

214
00:12:44.500 --> 00:12:49.500
but we're going to see a lot more and a lot of the key discoveries that are

215
00:12:49.601 --> 00:12:50.830
going to come out of the field.

216
00:12:50.831 --> 00:12:55.270
The entire field of machine learning this year and next year are going to come

217
00:12:55.271 --> 00:13:00.271
from deep reinforcement learning when some really smart people combine the ideas

218
00:13:00.820 --> 00:13:01.960
that come from deep learning,

219
00:13:01.990 --> 00:13:06.990
mainly hierarchical learning and the ideas of reinforcement learning from a

220
00:13:07.421 --> 00:13:09.730
learning from an environment in real time.

221
00:13:10.270 --> 00:13:14.860
Andre Karpati had a recent talk at y Combinator,
I think it was called,

222
00:13:14.861 --> 00:13:17.140
why cons,
where he said that Agi,

223
00:13:17.141 --> 00:13:21.760
artificial general intelligence is going to result from having simulations right

224
00:13:21.910 --> 00:13:22.211
from,

225
00:13:22.211 --> 00:13:27.211
from creating an AI that can adapt in a simulation similar to how we adapt in

226
00:13:27.461 --> 00:13:29.800
the real world.
And this is a simulation,

227
00:13:30.130 --> 00:13:33.220
enter a twilight zone music.
But anyway,

228
00:13:34.300 --> 00:13:37.180
so reinforcement,
learning,
deep reinforcement,
learning,
super hot field.

229
00:13:37.300 --> 00:13:39.820
And this is your chance to get into it,
right?

230
00:13:39.821 --> 00:13:43.210
You don't have to work at deep mind.
You don't have to work at open Ai.

231
00:13:43.390 --> 00:13:44.810
You can just be some kid,
uh,

232
00:13:45.000 --> 00:13:48.610
who has a time and energy to work on this stuff.

233
00:13:48.730 --> 00:13:51.790
And if you have Internet access and you have the time to work on this,

234
00:13:51.920 --> 00:13:55.780
youtube can make an amazing algorithm and you post it on get hub,

235
00:13:55.781 --> 00:13:58.810
you posted on hacker news on the machine learning sub Reddit,

236
00:13:58.930 --> 00:14:00.160
you'll get great feedback.

237
00:14:00.250 --> 00:14:04.240
You can join some online research groups on a slack channel or on several of the

238
00:14:04.241 --> 00:14:06.670
forums online and you can just do great work.

239
00:14:06.790 --> 00:14:09.760
And all of this can be added to your portfolio,
your get hub,

240
00:14:09.910 --> 00:14:12.310
your resume for future prospects,

241
00:14:12.311 --> 00:14:15.490
whether that be studying at a university or working at one of these fields.

242
00:14:15.580 --> 00:14:18.280
But the point is,
in order to get anywhere you've got,

243
00:14:18.310 --> 00:14:22.600
you've got to do something right.
And,
and starcraft two is a great test bed.

244
00:14:22.601 --> 00:14:26.550
It's a great set of tools.
I've tested it out myself and,
and I think it's,
it's,

245
00:14:26.551 --> 00:14:30.010
it's,
it's a really great place to get started with deep reinforcement learning.

246
00:14:30.220 --> 00:14:34.810
Okay.
So,
uh,
okay now so onto the code,
right?
So,
uh,

247
00:14:35.180 --> 00:14:37.840
basically so it was a joint collaboration with blizzard.

248
00:14:37.841 --> 00:14:42.490
So blizzard already released an API that lets a user create scripted bots,

249
00:14:42.610 --> 00:14:46.150
machine learning base bots that are running from pickle files,
you know,

250
00:14:46.151 --> 00:14:50.740
pretrained models,
replay analysis and tool assisted human play.

251
00:14:50.960 --> 00:14:52.790
And so deep minds environment,

252
00:14:52.820 --> 00:14:56.990
it's repository is called Pi s c two Pi,
starcraft two.

253
00:14:57.140 --> 00:15:00.160
So it's all in python.
Thank the [inaudible] right.
It's in,

254
00:15:00.161 --> 00:15:02.930
it's in python but it has four components to it.

255
00:15:02.931 --> 00:15:06.680
The first is the API that it wraps from blizzard in python.

256
00:15:06.920 --> 00:15:10.640
The next is a Dataset of anonymized game replays.
Okay.

257
00:15:10.641 --> 00:15:13.910
So it's got a lot of these anonymize game replace that you can download from

258
00:15:13.911 --> 00:15:17.920
right here.
I'll go through that in a second.
And it's got a,

259
00:15:18.940 --> 00:15:21.050
a series of simple RL mini games.

260
00:15:21.110 --> 00:15:25.610
One of them is what we're going to do for this demo and to test out different

261
00:15:25.640 --> 00:15:28.730
environments,
right.
To test out the different algorithms.
I mean,

262
00:15:29.060 --> 00:15:33.320
so this was blizzards initial API and then deepmind wrapped it with their own

263
00:15:33.321 --> 00:15:35.480
python repository.
Right?

264
00:15:35.481 --> 00:15:38.990
So what we're going to do is we're going to just set up everything,
right?

265
00:15:38.990 --> 00:15:42.200
So I'm going to go through these installations that there are seven steps here.

266
00:15:42.201 --> 00:15:46.190
I'm going to go through all of them to,
to get started.
Okay.
So first of all,

267
00:15:46.191 --> 00:15:50.120
before you do anything,
you've got to download a starcraft two,

268
00:15:50.121 --> 00:15:54.110
the blizzard client,
it's free.
You just sign up on blizzard and you,

269
00:15:54.140 --> 00:15:56.030
you select the starter edition,
right?

270
00:15:56.031 --> 00:15:59.840
That's it just to like the starter edition and it's free and you can just

271
00:15:59.870 --> 00:16:03.200
download that and then you can play it just like that.
It'll take,
you know,

272
00:16:03.430 --> 00:16:04.430
it depends on your bandwidth,

273
00:16:04.431 --> 00:16:08.000
but it took me about an hour to download and set up and already I was running

274
00:16:08.001 --> 00:16:11.270
through the tutorial in starcraft two,
right?
Just an hour for me.

275
00:16:11.750 --> 00:16:13.220
So definitely download that.

276
00:16:13.250 --> 00:16:18.250
And once you've downloaded starcraft two then move on to these seven steps.

277
00:16:18.590 --> 00:16:21.400
So the first one is to install pie [inaudible],
right?

278
00:16:21.560 --> 00:16:26.560
And so luckily they have wrapped it into a nice little python library for us,

279
00:16:28.580 --> 00:16:31.340
so I can go ahead and install it using pip.

280
00:16:31.540 --> 00:16:36.380
So I'll say pseudo pip three install pie se two.

281
00:16:37.490 --> 00:16:41.230
Okay.
And it's downloading.

282
00:16:42.250 --> 00:16:46.260
Hey Scott,
it's,
it's building off of all those dependencies that right.
It's,
it's,

283
00:16:46.261 --> 00:16:50.380
it's built on top of the Blizzard Api that I just talked about and,

284
00:16:53.020 --> 00:16:56.710
and some other things.
Okay.
So now that was step one.

285
00:16:56.711 --> 00:17:01.180
We've installed pie se two and so the next step is to install the sample code.

286
00:17:01.270 --> 00:17:04.330
So the sample code,
you can just clone it directly from my get hub,

287
00:17:04.510 --> 00:17:07.030
just like get clone and it's going to download it

288
00:17:09.760 --> 00:17:10.630
just like that,
right?

289
00:17:10.631 --> 00:17:14.290
And the sample code contains the pre trained model and it contains all the

290
00:17:14.291 --> 00:17:18.490
python files you need to run this very simple reinforcement learning bot.

291
00:17:19.390 --> 00:17:22.060
And so once you've got that,
then that's step two.

292
00:17:22.080 --> 00:17:26.290
And then step three is to download the mini games from the starcraft two maps,

293
00:17:26.500 --> 00:17:29.680
right?
So we can click on this link and just like that,

294
00:17:30.250 --> 00:17:34.030
here are our maps,
our mini game maps.
Okay.

295
00:17:34.270 --> 00:17:39.150
And so we can move all of these into our folder for starcraft two and it's got

296
00:17:39.151 --> 00:17:42.160
to be in the maps folder right here we go in maps.

297
00:17:42.161 --> 00:17:44.260
So I'll just copy and paste it just like that.

298
00:17:44.410 --> 00:17:48.250
So now my starcraft two application that I've downloaded with the blizzard

299
00:17:48.251 --> 00:17:52.140
client has these maps.
And so once I have these maps,

300
00:17:52.320 --> 00:17:56.640
now I can install tensorflow and open AI baselines.
Oh,

301
00:17:57.660 --> 00:17:57.990
okay.

302
00:17:57.990 --> 00:18:02.670
So so right if you don't have a tensor flow you can install tensorflow with pip

303
00:18:02.671 --> 00:18:06.810
three or pip install,
you can install tensorflow,
I've already got tensor flow.

304
00:18:06.990 --> 00:18:08.340
And then once you've got tensorflow,

305
00:18:08.400 --> 00:18:12.600
tensorflow is to be able to train and run these machine learning models and then

306
00:18:12.601 --> 00:18:14.580
you have to install open AI baselines.

307
00:18:14.640 --> 00:18:19.290
So baselines is a collection of high quality reinforcement learning algorithms,

308
00:18:19.500 --> 00:18:20.040
right?

309
00:18:20.040 --> 00:18:23.700
The deep Q network is one of them that we're going to be using and it's got

310
00:18:23.701 --> 00:18:24.660
policy gradients,

311
00:18:24.661 --> 00:18:27.390
which she said another popular reinforcement learning algorithm.

312
00:18:27.540 --> 00:18:30.570
But basically it's your way of being able to implement these reinforcement

313
00:18:30.571 --> 00:18:33.750
learning algorithms without having to code them from up from scratch.

314
00:18:33.930 --> 00:18:35.880
And then you can modify existing ones.

315
00:18:35.970 --> 00:18:38.490
Tweak them to see if you can get better results.

316
00:18:38.491 --> 00:18:42.570
So it's a good way to test around with some tests,
some different,
uh,
affects.

317
00:18:42.720 --> 00:18:45.210
Okay.
So then I can download the baselines environment.

318
00:18:50.230 --> 00:18:54.820
And once I've downloaded baselines,
I've put my folder,

319
00:18:55.240 --> 00:18:59.740
I put my maps into my,
uh,
starcraft two folder,

320
00:18:59.950 --> 00:19:03.370
then I can go ahead and open the project with Intelijay.

321
00:19:03.520 --> 00:19:08.520
So the reason I say use Intelijay for this and not just open it with sublime or

322
00:19:09.400 --> 00:19:13.330
you know,
some regular text editor is because this,
oh,
hold on.

323
00:19:14.260 --> 00:19:18.490
Permission Error.
Of course we've got to do Sudo.
Of course.
There we go.

324
00:19:18.580 --> 00:19:22.540
So now the reason I say intelligent is because there's some logs that are really

325
00:19:22.541 --> 00:19:25.240
nice too to view a when it comes to,

326
00:19:25.770 --> 00:19:26.280
<v 1>okay,</v>

327
00:19:26.280 --> 00:19:29.580
<v 0>how your aging is running,
which is easy to do with Intelijay.</v>

328
00:19:29.581 --> 00:19:33.480
So if you've never used a,
uh,
uh,
an ide,

329
00:19:33.540 --> 00:19:37.860
the integrated development environment,
like intelligent,
this is a great,
uh,

330
00:19:39.690 --> 00:19:43.080
this is a great reason to do it,
right?
So go ahead and download intelligent,

331
00:19:43.081 --> 00:19:46.440
you can download from here.
There is a paid version,

332
00:19:46.441 --> 00:19:48.440
but there's also a free version of the community version.

333
00:19:48.450 --> 00:19:51.210
Get the community version so you don't have to get the paid version.

334
00:19:51.270 --> 00:19:54.450
And then if you want to train the model right from scratch,

335
00:19:54.690 --> 00:19:58.200
then you can run python three train mineral shards.
That Pie.
But we're,

336
00:19:58.201 --> 00:20:00.810
we're gonna do is we're gonna run a pretrained model.

337
00:20:00.900 --> 00:20:04.440
So though we don't have to train it from scratch,
we just want to,

338
00:20:04.620 --> 00:20:06.210
we just want to get something to work right.

339
00:20:06.211 --> 00:20:08.700
So let's go ahead and just open up that model.

340
00:20:37.750 --> 00:20:38.583
<v 1>Okay.</v>

341
00:20:41.870 --> 00:20:42.171
<v 0>Okay.</v>

342
00:20:42.171 --> 00:20:47.171
So Intelijay is opening right now and once it's opened I can go and open my

343
00:20:49.241 --> 00:20:49.991
project.

344
00:20:49.991 --> 00:20:54.580
So I'm going to import a project and where is my project?

345
00:20:54.581 --> 00:20:57.400
So my project is

346
00:21:00.700 --> 00:21:01.533
<v 1>okay</v>

347
00:21:02.310 --> 00:21:07.020
<v 0>in downloads pie,
s c two.
So go to downloads,</v>

348
00:21:08.080 --> 00:21:12.110
Pi s e two examples.
All right.

349
00:21:12.150 --> 00:21:15.120
And then I'll open it.

350
00:21:20.220 --> 00:21:21.053
<v 1>Okay.</v>

351
00:21:21.200 --> 00:21:25.010
<v 0>From existing sources finish,</v>

352
00:21:40.010 --> 00:21:44.690
you can install plugins to support python,
which I'll do right now.

353
00:21:45.170 --> 00:21:45.840
So clearly I'm,

354
00:21:45.840 --> 00:21:50.000
I'm doing all this steps as if I've never done any of this before.

355
00:21:50.600 --> 00:21:53.540
And then I've got to restart it to initialize the python plugin.

356
00:21:57.010 --> 00:21:57.843
<v 1>Okay,</v>

357
00:21:58.340 --> 00:21:59.173
<v 0>Kay.</v>

358
00:22:13.680 --> 00:22:16.380
All right.
So now I've got the,
the code in,

359
00:22:16.440 --> 00:22:21.440
import it into Intelijay and it's detected a python framework.

360
00:22:21.900 --> 00:22:25.590
And I'm going to say,
okay,
did this,
and then,

361
00:22:48.320 --> 00:22:53.240
and there we have it.
And now the client is running in the background.
It's,
it's,

362
00:22:53.420 --> 00:22:56.330
it's executed starcraft two.
It's the tech did that.

363
00:22:56.331 --> 00:23:01.331
My system has starcraft two installed and then it's run this script to run a

364
00:23:02.541 --> 00:23:07.340
pretrained model inside the starcraft two environment.
So it acts.

365
00:23:07.430 --> 00:23:11.840
So it's able to access the starcraft two game because the deep,

366
00:23:11.841 --> 00:23:16.760
because deep minds,
Pi se two repository under the hood,
it's using blizzards.

367
00:23:16.761 --> 00:23:20.990
Api is,
but it's a local API.
So it's not like it's connecting to more remotely.

368
00:23:20.991 --> 00:23:25.430
It's connecting to the game that's right on your desktop or your laptop in terms

369
00:23:25.431 --> 00:23:29.990
of the code,
the,
the model and the environment or bill from us.

370
00:23:29.991 --> 00:23:33.830
So in this,
in this code,
it's got the deep convolutional network right here.

371
00:23:33.960 --> 00:23:35.840
As you can see,
the parameters are here,

372
00:23:35.900 --> 00:23:39.640
the number of hidden layers that are here.
And then it's got the,
uh,

373
00:23:39.920 --> 00:23:43.940
and then it's got the,
it's wraps that open AI environment,
that step function,

374
00:23:43.941 --> 00:23:45.560
right?
So it's,
it's given,

375
00:23:45.770 --> 00:23:50.770
it's given these parameters and it's combined both the convolutional network and

376
00:23:51.020 --> 00:23:51.710
the,
uh,

377
00:23:51.710 --> 00:23:56.710
Q network together to then train this AI and it saves it as a pickle file.

378
00:23:58.370 --> 00:24:00.410
It saves the pretrained models a pickle file.

379
00:24:00.560 --> 00:24:03.620
And then once it's trained the pickle file,
we can,

380
00:24:03.800 --> 00:24:08.120
we can access that pickle file to run the pretrained model in the deepmind

381
00:24:08.180 --> 00:24:11.960
environment into starcraft two environment.
And if we look at this code,

382
00:24:11.961 --> 00:24:12.651
it's actually,
you know,

383
00:24:12.651 --> 00:24:16.700
it's quite a lot of code and I can make a different video to talk about how all

384
00:24:16.701 --> 00:24:17.600
the code works,

385
00:24:17.780 --> 00:24:21.320
but right now I just want it to help you install and configure this script so

386
00:24:21.321 --> 00:24:23.750
that you can run it yourself.
Don't be afraid to run it.

387
00:24:23.751 --> 00:24:27.470
It's actually pretty easy.
And all in all including downloads,
starcraft,

388
00:24:27.471 --> 00:24:30.500
two Dowling,
the code,
installing all your dependencies,

389
00:24:30.530 --> 00:24:34.280
it'll take you probably an hour and a half to go from zero to running your own,

390
00:24:34.340 --> 00:24:38.600
uh,
RL algorithms in this game.
Okay.
So I hope that,
I hope that helped.

391
00:24:38.750 --> 00:24:41.210
Please subscribe for more programming videos,
and for now,

392
00:24:41.270 --> 00:24:44.030
I'm going to play some starcraft too,
so thanks for watching.

