WEBVTT

1
00:00:00.260 --> 00:00:03.780
Hey,
are you okay?
I want to make music but I don't know how

2
00:00:09.920 --> 00:00:10.611
hello world,

3
00:00:10.611 --> 00:00:13.640
it's Saroj and this episode we're going to talk about the bleeding edge and

4
00:00:13.641 --> 00:00:15.290
machine generated music.

5
00:00:15.380 --> 00:00:19.280
Then write our own music generation script in tenter and just under 90 lines of

6
00:00:19.281 --> 00:00:23.560
python.
I'm a big fan of Hans Zimmer,
so we've mapped to become a no,

7
00:00:24.510 --> 00:00:27.590
but I don't have the time to put in 10,000 hours to be like Han's,

8
00:00:27.620 --> 00:00:30.740
but just imagine a program where I could say I want a song that conveys the

9
00:00:30.741 --> 00:00:33.860
feelings of hope and wonder while sailing the seven seas,

10
00:00:33.980 --> 00:00:36.290
but also the gas just feeling of beating too many beans.

11
00:00:36.350 --> 00:00:39.830
The program would convert that speech to text and for each word it would find

12
00:00:39.831 --> 00:00:40.880
the associated vector.

13
00:00:40.910 --> 00:00:44.630
It will then compare those vectors to vector is created by a model trained on a

14
00:00:44.631 --> 00:00:47.020
labeled music dataset for range of human emotions.

15
00:00:47.070 --> 00:00:50.270
The output of the machine would then be a set of chords that he thought best

16
00:00:50.271 --> 00:00:53.600
represented those emotions.
It would lower the barrier to entry.

17
00:00:53.660 --> 00:00:54.890
For me to compose music.

18
00:00:54.920 --> 00:00:58.790
Machine learning is a layer of intelligence is meant to extend our own.

19
00:00:58.850 --> 00:01:02.930
I'm still really bad at folding shirts.
A laundry folding robot would be so dope.

20
00:01:03.080 --> 00:01:05.870
Deepmind released a paper a few days ago called Wavenet,

21
00:01:05.900 --> 00:01:09.470
which is now the state of the art in both music generation and text to speech.

22
00:01:09.500 --> 00:01:12.600
Traditionally,
audio generation models are concatenate IV.

23
00:01:12.650 --> 00:01:15.500
That means in order to generate speech from some tech sample,

24
00:01:15.620 --> 00:01:20.150
it utilizes a huge database of speech fragments by picking a few and combining

25
00:01:20.151 --> 00:01:22.850
them to create a full sentence.
Same for music,
this works,

26
00:01:22.851 --> 00:01:26.180
but it's hard to incorporate things like emotion and natural sounds.

27
00:01:26.181 --> 00:01:29.300
When the output is created.
I putting a bunch of static fragments together.

28
00:01:29.330 --> 00:01:32.390
Ideally we could have audio generation that is parametric,

29
00:01:32.540 --> 00:01:36.020
where all the Info requires a generate audio is store in the parameters of the

30
00:01:36.021 --> 00:01:38.480
model that we give it.
That's what wavenet is capable of.

31
00:01:38.510 --> 00:01:41.810
Instead of generating an audio signal by passing its output through signal

32
00:01:41.811 --> 00:01:46.100
processing algorithms,
it directly models the raw way form of the audio signal.

33
00:01:46.160 --> 00:01:47.420
Researchers don't really do that,

34
00:01:47.421 --> 00:01:52.421
but wavenet did the model they use with a convolutional neural network where

35
00:01:52.490 --> 00:01:56.720
each layer had dilation factors that led to the interconnectedness grow

36
00:01:56.750 --> 00:01:58.010
exponentially,
but deeper.

37
00:01:58.011 --> 00:02:02.150
The data flow through the model and each generated sample at each step was fed

38
00:02:02.210 --> 00:02:04.220
back into the network to generate the next step.

39
00:02:04.280 --> 00:02:07.370
Let's take a look at the computation graph for the model.
The input data,

40
00:02:07.430 --> 00:02:09.560
a single note start as a raw audio wave.

41
00:02:09.650 --> 00:02:12.470
We've first format the way so that it's better suited for processing.

42
00:02:12.500 --> 00:02:15.950
Then we encode it to produce a tensor with a number of samples and a number of

43
00:02:15.951 --> 00:02:19.070
channels.
We feed that into the first layer of the convolutional network,

44
00:02:19.100 --> 00:02:22.430
which reduces the number of channels for easier processing and through a stack

45
00:02:22.431 --> 00:02:24.080
of layers.
With dilated convolution.

46
00:02:24.200 --> 00:02:27.650
We combine the outputs of all the layers and increased the dimensionality to the

47
00:02:27.651 --> 00:02:30.500
original number of channels.
If he did all tore lost function,

48
00:02:30.501 --> 00:02:33.770
which measures how well our training is going and finally the output is fed back

49
00:02:33.771 --> 00:02:36.200
into the network,
could generate the wave at the next time step.

50
00:02:36.260 --> 00:02:38.570
This process keeps repeating to generate more and more audio.

51
00:02:38.600 --> 00:02:39.890
This neural net was huge.

52
00:02:39.920 --> 00:02:43.970
It took 90 minutes on their GPU cluster to generate a single second of audio.

53
00:02:44.030 --> 00:02:46.910
Instead,
we're going to build a more simple music generation script.

54
00:02:46.940 --> 00:02:47.720
In tensorflow,

55
00:02:47.720 --> 00:02:51.020
we want to first import num Pi is there scientific computing library in python,

56
00:02:51.220 --> 00:02:54.470
pandas as our data analytics library and of course tensorflow our machine

57
00:02:54.471 --> 00:02:55.150
learning library.

58
00:02:55.150 --> 00:02:58.120
Can you QDM will help us show a progress bar during training and finally made

59
00:02:58.130 --> 00:02:59.860
I'm manipulation,
which is our helper library.

60
00:02:59.890 --> 00:03:03.190
Let's start off with the first step or hyper parameters or tuning knobs.

61
00:03:03.220 --> 00:03:03.970
For our models,

62
00:03:03.970 --> 00:03:07.300
we're going to use a type of neural network called a restricted Boltzmann

63
00:03:07.301 --> 00:03:11.200
machine as our generative model.
An RBM is a two layer neural net.

64
00:03:11.201 --> 00:03:14.680
The first layer is visible and the next layer is hidden nodes within layers are

65
00:03:14.681 --> 00:03:16.210
connected to note in the next layer,

66
00:03:16.240 --> 00:03:19.480
but no two notes of the same layer are linked.
That's the restriction part.

67
00:03:19.510 --> 00:03:21.880
Each node decides whether to transmit any data,

68
00:03:21.881 --> 00:03:25.000
it received the next layer or not.
By making a random decision.

69
00:03:25.060 --> 00:03:27.570
We want to first create a range of notes that our model will generate,

70
00:03:27.571 --> 00:03:30.640
so we'll write variables for our lowest note and our highest note as well as a

71
00:03:30.641 --> 00:03:32.470
note range,
which is a difference between the two.

72
00:03:32.530 --> 00:03:35.710
Then we'll write variables for the number of times the size of the visible

73
00:03:35.711 --> 00:03:38.890
layers,
the number of hidden layers,
number of training,
epochs,
batch size,

74
00:03:38.920 --> 00:03:40.120
and finally our learning rate,

75
00:03:40.150 --> 00:03:43.810
which is a flow constant since it always stays the same.
Next weekend,

76
00:03:43.811 --> 00:03:44.890
right out variables.

77
00:03:44.891 --> 00:03:48.010
We'll start out by creating a place holder which will serve as our gateway for

78
00:03:48.011 --> 00:03:49.450
inputting data into our model.

79
00:03:49.570 --> 00:03:52.480
Then a variable that will store the weight matrix or the connections between our

80
00:03:52.481 --> 00:03:54.970
two layers.
We also need to variables for biases,

81
00:03:54.971 --> 00:03:57.040
one for the hidden layer and one for the visible air.

82
00:03:57.070 --> 00:03:59.440
We didn't want to create a sample from our input in x.

83
00:03:59.500 --> 00:04:01.270
He's your aren't Gibbs sample helper method.

84
00:04:01.300 --> 00:04:04.450
Keep sampling is an algorithm that creates a sample from a multivariate

85
00:04:04.451 --> 00:04:05.620
probability distribution.

86
00:04:05.650 --> 00:04:09.700
It constructs a statistical model where each state depends on the previous
state.

87
00:04:09.790 --> 00:04:13.510
Then use this randomness to obtain a sample across the distribution it creates.

88
00:04:13.540 --> 00:04:15.670
We'll get a sample of the hidden notes as well,

89
00:04:15.671 --> 00:04:18.700
starting from our original input and a sample of the hidden nodes starting from

90
00:04:18.701 --> 00:04:22.270
our generated sample and we'll update our wheat values based on the differences

91
00:04:22.271 --> 00:04:24.970
between the samples that we drew and d original values.

92
00:04:25.030 --> 00:04:27.070
We will run these three updates,
steps for our weights,

93
00:04:27.130 --> 00:04:31.270
hidden biases and visible Pisces when we later run our session and we can store

94
00:04:31.271 --> 00:04:33.430
all of these in our update variable.
All right,

95
00:04:33.490 --> 00:04:35.560
it's time to start a session so we can run our graph.

96
00:04:35.590 --> 00:04:37.030
We'll initialize our variables.
First.

97
00:04:37.031 --> 00:04:39.790
We're going to reshape each song so they are vectorized in a good training

98
00:04:39.791 --> 00:04:43.150
format.
Then we're going to train our RBM on the examples one at a time.

99
00:04:43.180 --> 00:04:45.400
Once the model is fully trained,
we will make some music.

100
00:04:45.401 --> 00:04:48.880
We were our Gibbs chain with a visible notes are initialized to zero to generate

101
00:04:48.881 --> 00:04:52.210
a sample and reshape the vector to be a proper format for playback.
Finally,

102
00:04:52.211 --> 00:04:53.560
we'll print out the generated courts.

103
00:04:57.260 --> 00:04:57.710
Dope.

104
00:04:57.710 --> 00:05:01.970
The state of the art and music generation uses a CNN to generate raw way forms

105
00:05:01.971 --> 00:05:04.340
parametrically instead of concatenate valley.

106
00:05:04.520 --> 00:05:06.770
That algorithm requires lots of compute right now,

107
00:05:06.830 --> 00:05:10.850
so we can use an RBM to generate audio samples based on training data easily.

108
00:05:10.910 --> 00:05:14.510
And Gib sampling helps us create audio samples during training by randomly

109
00:05:14.511 --> 00:05:18.650
obtaining them from a generated probability distribution based on the input
data.

110
00:05:18.710 --> 00:05:21.890
And so the winter,
the last coding challenge is row Han Burma.

111
00:05:21.920 --> 00:05:25.750
He created an image classifier for astronomers to classify galaxies as either

112
00:05:25.770 --> 00:05:28.280
elliptical or spiral that asks the weak.

113
00:05:28.340 --> 00:05:32.180
And the runner up is Chichi Leon for creating a bird classifier for biologists.

114
00:05:32.240 --> 00:05:33.500
I bow to both of you.

115
00:05:33.650 --> 00:05:37.700
The challenge for this video is to use the RBM script to generate an audio

116
00:05:37.701 --> 00:05:41.060
sample.
Would they happy,
upbeat sound,
post your code in the comments.

117
00:05:41.180 --> 00:05:44.450
I'll judge them and announce the winner in my video one week from now.
For now,

118
00:05:44.480 --> 00:05:47.150
I'm going to get more silver hair dye.
So thanks for watching.

