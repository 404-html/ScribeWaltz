WEBVTT

1
00:00:00.120 --> 00:00:00.953
Hello world.

2
00:00:00.960 --> 00:00:05.960
It's arrived and welcome to the first video of my new reinforcement learning

3
00:00:05.971 --> 00:00:10.110
course titled Move 37 this is a 10 week course.

4
00:00:10.140 --> 00:00:14.100
All my videos will be released right here for free on my youtube channel,

5
00:00:14.310 --> 00:00:16.200
so hit subscribe to keep updated.

6
00:00:16.680 --> 00:00:21.680
The only prerequisites are knowing basic python syntax and understanding the

7
00:00:22.340 --> 00:00:24.240
backpropagation algorithm.

8
00:00:24.270 --> 00:00:28.140
Educational links to both topics will be in the video description.

9
00:00:28.500 --> 00:00:33.500
We humans have been able to solve some hard problems in our relatively short

10
00:00:33.721 --> 00:00:38.721
existence on earth using our biological intelligence to analyze data and come up

11
00:00:39.121 --> 00:00:44.100
with solutions.
But some problems like major diseases,
extreme poverty,

12
00:00:44.310 --> 00:00:49.260
longterm environmental sustainability,
have proven to be very hard to solve.

13
00:00:49.530 --> 00:00:53.970
Some of these major problems may take large groups of well trained humans

14
00:00:54.180 --> 00:00:58.890
decades to solve,
and we just don't have the luxury of waiting that long.

15
00:00:58.891 --> 00:01:03.891
We need those solutions now since its inception in the 50s artificial

16
00:01:04.081 --> 00:01:09.081
intelligence researchers have always been driven by a mixture of curiosity of

17
00:01:09.450 --> 00:01:14.070
how the universe really works and all truism to help augment our human

18
00:01:14.071 --> 00:01:16.710
intelligence with machine learning systems.

19
00:01:17.130 --> 00:01:20.790
And it turns out that AI isn't just a luxury,
it's a necessity.

20
00:01:21.060 --> 00:01:25.410
The amount of data generated on the Internet doubles every two years.

21
00:01:25.530 --> 00:01:27.120
It's an exponential growth.

22
00:01:27.330 --> 00:01:32.330
And the keys to so many mysteries like the ending of the soprano's lie hidden in

23
00:01:32.341 --> 00:01:36.180
that data data that no single human could parse through alone,

24
00:01:36.390 --> 00:01:38.940
but perhaps with the right algorithm could.

25
00:01:39.000 --> 00:01:42.510
So if you've just started to learn how machine learning works,

26
00:01:42.690 --> 00:01:47.340
you'll notice that the vast majority of the introductory algorithms and theories

27
00:01:47.341 --> 00:01:50.160
that you'll encounter in blog posts and courses,

28
00:01:50.430 --> 00:01:55.290
well mostly all be encompassed under the category of supervised learning in the

29
00:01:55.291 --> 00:01:59.010
machine learning pipeline.
The first step is to collect a dataset.

30
00:01:59.400 --> 00:02:04.400
They come in all sorts of file formats and generally look like an excel table of

31
00:02:04.441 --> 00:02:09.210
values.
Each data point has a different value for whatever feature it has.

32
00:02:09.540 --> 00:02:10.680
In supervised learning,

33
00:02:10.681 --> 00:02:14.610
we're trying to predict a value that already exists in our data.

34
00:02:14.820 --> 00:02:18.480
This is called the label.
Sometimes it's called the target variable,

35
00:02:18.510 --> 00:02:21.270
and it can also be called the dependent variable,

36
00:02:21.420 --> 00:02:25.590
whereas the rest of the features are considered independent variables.

37
00:02:25.800 --> 00:02:28.770
So given our features like the years of experience,

38
00:02:28.890 --> 00:02:32.580
the age and the educational background of an individual,

39
00:02:32.850 --> 00:02:37.850
it would be helpful to be able to predict their salary or given the age of a car

40
00:02:38.430 --> 00:02:39.600
and the driver's age,

41
00:02:39.601 --> 00:02:44.220
it would be helpful to be able to predict the risk of automotive accident.

42
00:02:44.760 --> 00:02:48.840
The great thing here is that the data we'd use to train our model,

43
00:02:48.870 --> 00:02:53.870
whichever one we pick already contains the desired response that is it contains

44
00:02:54.361 --> 00:02:57.900
a dependent variable.
It's like having training wheels on a bike.

45
00:02:58.230 --> 00:03:02.050
We could choose a linear regression model,
logistic regression,

46
00:03:02.110 --> 00:03:03.970
a neural network decision.

47
00:03:03.971 --> 00:03:08.971
Trees all have different ways of eventually learning the function that relates

48
00:03:09.110 --> 00:03:11.260
the input features to the label.

49
00:03:11.620 --> 00:03:16.620
Most data however doesn't have clean labels for us to use but we still want to

50
00:03:16.931 --> 00:03:18.790
derive some insights from it.

51
00:03:19.180 --> 00:03:22.150
That would be the field known as unsupervised learning.

52
00:03:22.450 --> 00:03:27.450
We could then cluster our data using techniques like k means and mixture models,

53
00:03:27.670 --> 00:03:32.470
allowing us to visualize groups of data points that are related that we wouldn't

54
00:03:32.471 --> 00:03:33.940
know of otherwise.

55
00:03:34.210 --> 00:03:39.210
We can also attempt to find a compressed representation of the data using say an

56
00:03:39.761 --> 00:03:40.780
auto encoder,

57
00:03:41.020 --> 00:03:45.550
then use that representation for a specific task or we could just try to find

58
00:03:45.551 --> 00:03:47.090
the anomaly in the data set,

59
00:03:47.110 --> 00:03:50.710
meaning identify what data point doesn't fit with the rest,

60
00:03:50.920 --> 00:03:52.840
like say a fraudulent transaction.

61
00:03:53.020 --> 00:03:58.020
Unsupervised learning algorithms are generally used to preprocess data during

62
00:03:58.241 --> 00:04:03.040
say the exploratory analysis phase or for supervised learning algorithms.

63
00:04:03.280 --> 00:04:07.840
Supervised and unsupervised learning algorithms are supremely useful.

64
00:04:07.930 --> 00:04:11.200
They are tools to recognize patterns in complex data,

65
00:04:11.530 --> 00:04:13.000
but consider this scenario.

66
00:04:13.150 --> 00:04:17.620
We're a new online product delivery startup and we've just deployed a fleet of

67
00:04:17.621 --> 00:04:22.540
vehicles and factories that help us move a product from point a to point B.

68
00:04:22.810 --> 00:04:27.160
So many things could go wrong before the product is successfully delivered.

69
00:04:27.370 --> 00:04:31.570
The trucks could break down,
bad weather,
could cause road closures,

70
00:04:31.720 --> 00:04:33.130
the food could go bad.

71
00:04:33.250 --> 00:04:37.570
What kind of learning technique should we use to predict the most optimal

72
00:04:37.690 --> 00:04:40.810
delivery route given all the other factors?

73
00:04:41.260 --> 00:04:43.900
This is a highly dynamic problem space.

74
00:04:44.230 --> 00:04:48.670
We need a learning system here that will be highly adaptive to changes and

75
00:04:48.671 --> 00:04:52.810
unfortunately we don't have a preexisting data set to learn from.

76
00:04:53.080 --> 00:04:58.030
We have to learn in real time what works and what doesn't work in a setting that

77
00:04:58.031 --> 00:05:01.510
introduces an entirely new dimension time.

78
00:05:01.990 --> 00:05:05.500
This is the task that reinforcement learning attempts to solve.

79
00:05:05.800 --> 00:05:09.490
It's somewhere in between supervised and unsupervised learning.

80
00:05:09.790 --> 00:05:14.790
While in supervised learning we have a target label for each training example

81
00:05:15.250 --> 00:05:19.060
and in an unsupervised learning we don't have a label at all.

82
00:05:19.210 --> 00:05:23.890
In reinforcement learning,
we have time delayed labels that are sparse,

83
00:05:23.950 --> 00:05:28.900
meaning we don't get that many and based on this signal of time delayed labels

84
00:05:28.930 --> 00:05:33.820
which we can call rewards,
we can learn how to behave in this environment.

85
00:05:34.180 --> 00:05:39.180
It's this powerful combination of pattern recognition networks and real time

86
00:05:39.281 --> 00:05:43.600
environment based learning framework's called deep reinforcement learning.

87
00:05:43.840 --> 00:05:48.460
That has resulted in some incredible recent AI success stories including the

88
00:05:48.461 --> 00:05:52.360
minds Alphago program and open AI fives go to victory.

89
00:05:52.750 --> 00:05:56.230
We'll discuss the details of those near the end of the course,

90
00:05:56.231 --> 00:06:01.231
but first we need to understand the algorithms and theory of pure reinforcement

91
00:06:01.701 --> 00:06:06.260
learning.
All of it starts with defining some kind of mathematical framework that

92
00:06:06.261 --> 00:06:11.261
encapsulates the idea of an AI interacting with an environment where time is a

93
00:06:11.301 --> 00:06:14.240
dimension and it's learning through trial and error.

94
00:06:14.420 --> 00:06:19.420
In 1906 a Russian mathematician named Andre Mark coff was interested in modeling

95
00:06:20.301 --> 00:06:24.800
systems that followed a chain of linked events.
Blockchain,

96
00:06:25.280 --> 00:06:26.113
no,

97
00:06:26.690 --> 00:06:31.310
so he defined what's now called a Markov chain to describe this process.

98
00:06:31.640 --> 00:06:36.320
A Markov chain has a set of states and a process that can move successively from

99
00:06:36.321 --> 00:06:37.730
one state to another.

100
00:06:38.060 --> 00:06:42.890
Each move is a single step and is based on a transition model t that defines how

101
00:06:42.891 --> 00:06:45.200
to move from one state to the next.

102
00:06:45.680 --> 00:06:49.730
This chain is based on a property he also invented called the Markov property.

103
00:06:50.120 --> 00:06:52.040
It states that given the present,

104
00:06:52.070 --> 00:06:55.310
the future is conditionally independent of the past,

105
00:06:55.700 --> 00:07:00.700
meaning the state in which the process is now is dependent only on the state it

106
00:07:01.011 --> 00:07:03.740
was at one time step ago.
For example,

107
00:07:03.741 --> 00:07:07.880
let's say we want to use a Markov chain to model the weather with just two

108
00:07:07.881 --> 00:07:12.380
states,
sunny and cloudy,
using what's called a transition matrix.

109
00:07:12.380 --> 00:07:17.380
We can represent the weather model in which a sunny day is 90% likely to be

110
00:07:17.721 --> 00:07:22.721
followed by another sunny day and a rainy day is 50% likely to be followed by

111
00:07:23.421 --> 00:07:24.560
another rainy day.

112
00:07:24.920 --> 00:07:28.910
Each states of the chain is a node in the chain graph and the transition

113
00:07:28.911 --> 00:07:32.270
probabilities are edges with the highest probabilities.

114
00:07:32.271 --> 00:07:33.770
Having the thickest edges.

115
00:07:34.250 --> 00:07:38.900
The most common framework for representing the reinforcement learning problem of

116
00:07:38.901 --> 00:07:43.901
an agent learning in an environment is called a markup decision process.

117
00:07:45.050 --> 00:07:47.540
This is an extension of mark off chains.

118
00:07:47.570 --> 00:07:50.300
The difference is the addition of actions,

119
00:07:50.301 --> 00:07:54.170
meaning allowing choice and rewards giving motivation.

120
00:07:54.560 --> 00:07:58.310
Every Mark Cobb decision process is defined by five components,

121
00:07:58.640 --> 00:08:03.020
a set of possible states on initial state,
a set of possible actions,

122
00:08:03.170 --> 00:08:05.390
a transition model and a reward function.

123
00:08:05.750 --> 00:08:09.320
The transition model returns the probability of reaching the next state if the

124
00:08:09.321 --> 00:08:13.190
action is done in a previous state,
but given s and a,

125
00:08:13.191 --> 00:08:17.600
the model is conditionally independent of all previous states and actions,

126
00:08:17.601 --> 00:08:21.050
which is the mark off property and the reward function.

127
00:08:21.080 --> 00:08:22.940
Our returns a real value.

128
00:08:22.941 --> 00:08:26.180
Every time the agent moves from one state to the other,

129
00:08:26.450 --> 00:08:28.850
and since we have a reward function,

130
00:08:29.090 --> 00:08:33.800
we can conclude that some states are more desirable than others because when the

131
00:08:33.801 --> 00:08:37.940
agent moves to these states,
it receives a high reward.

132
00:08:38.300 --> 00:08:39.890
The opposite is also true.

133
00:08:39.920 --> 00:08:44.150
There are states that should be avoided because when the agent moves there,

134
00:08:44.210 --> 00:08:46.250
it receives a negative reward.

135
00:08:46.880 --> 00:08:51.880
The problem then is that the agent has to maximize the reward by avoiding states

136
00:08:52.041 --> 00:08:56.850
which returned negative values and choosing the one which returns positive

137
00:08:56.851 --> 00:08:57.684
values.

138
00:08:57.810 --> 00:09:02.280
The solution is to find a policy which selects the action with the highest

139
00:09:02.281 --> 00:09:05.250
reward.
Agents can try different policies,

140
00:09:05.251 --> 00:09:08.760
but only one can be considered an optimal policy,

141
00:09:08.940 --> 00:09:13.140
which gives us the best utility.
So if we,
for example,
have a delivery drone,

142
00:09:13.141 --> 00:09:17.610
we want to get to our friend in the same room using the optimal path,

143
00:09:17.640 --> 00:09:20.760
we can use the mark Haub decision process to frame this problem.

144
00:09:21.120 --> 00:09:25.260
We can define our environment as a matrix with the starting state being in one

145
00:09:25.261 --> 00:09:26.094
corner.

146
00:09:26.160 --> 00:09:30.300
There could be obstacles in our environment like ceiling lights and Postgrad

147
00:09:30.301 --> 00:09:34.950
students that we'd want to avoid and our drone can go up down,

148
00:09:34.980 --> 00:09:36.060
left or right.

149
00:09:36.330 --> 00:09:40.650
We've got a set of states actions and rewards here and each environment will

150
00:09:40.651 --> 00:09:44.640
have its own characteristics that make it unique.
For example,

151
00:09:44.700 --> 00:09:47.640
this particular environment is fully observable.

152
00:09:47.641 --> 00:09:51.510
Since our drone always knows which state it is in.

153
00:09:51.810 --> 00:09:55.260
We could also say that there's no limit on time to deliver,

154
00:09:55.320 --> 00:09:57.690
so the problem has an infinite horizon.

155
00:09:58.020 --> 00:10:02.100
These factors will influence the choice of algorithm we use to find the best

156
00:10:02.101 --> 00:10:02.934
policy.

157
00:10:03.120 --> 00:10:07.320
The question then becomes how does an agent choose the best policy?

158
00:10:07.620 --> 00:10:09.420
That's a topic for the next video.

159
00:10:09.630 --> 00:10:14.070
Three things to remember from this video though in reinforcement learning and AI

160
00:10:14.071 --> 00:10:19.071
learns how to optimally interact in a real time environment using time delayed

161
00:10:19.141 --> 00:10:21.660
labels called rewards as a signal.

162
00:10:21.990 --> 00:10:26.760
The Mark Haub decision process is a mathematical framework for defining the

163
00:10:26.761 --> 00:10:31.761
reinforcement learning problem using states' actions and rewards and through

164
00:10:32.850 --> 00:10:35.130
interacting with the environment,

165
00:10:35.160 --> 00:10:40.160
an AI will learn a policy which will return an action for a given state with the

166
00:10:40.531 --> 00:10:43.650
highest reward.
I'm so glad you made it to the end.

167
00:10:43.651 --> 00:10:47.310
Hit subscribe and you will always win,
will be forward for now.

168
00:10:47.340 --> 00:10:50.280
I've got a mastermind environment,
so thanks for watching.

