WEBVTT

1
00:00:00.090 --> 00:00:03.900
Hello world.
It's Saroj in.
Have you ever tried unity before?

2
00:00:04.080 --> 00:00:08.130
If you've ever wanted to create a three d game or three d simulation,

3
00:00:08.370 --> 00:00:11.460
unity is the easiest way to do that.
Recently,

4
00:00:11.461 --> 00:00:16.380
unity release a tool to let developers train and test new AI algorithms in a

5
00:00:16.381 --> 00:00:18.630
three d world called M l agents.

6
00:00:18.870 --> 00:00:22.890
Let's explore how this toolkit works and use it to build a simple simulation

7
00:00:22.891 --> 00:00:27.330
that trains ais to balance balls with the rise and machine learning

8
00:00:27.331 --> 00:00:28.164
breakthroughs,

9
00:00:28.170 --> 00:00:33.170
virtual reality and processing power simulated environments are becoming more

10
00:00:33.271 --> 00:00:37.110
and more commonplace across a wide variety of fields.

11
00:00:37.170 --> 00:00:40.770
They allow scientists to test out their hypotheses in a safe,

12
00:00:40.890 --> 00:00:44.160
repeatable environment for all sorts of experiments,

13
00:00:44.430 --> 00:00:47.730
and if you want to test out how powerful your algorithm is,

14
00:00:47.910 --> 00:00:52.120
it's a lot funner to watch it perform in a three dimensional world than a two d

15
00:00:52.121 --> 00:00:57.121
one or trying to decipher logs in a console when it comes to building a three d

16
00:00:57.421 --> 00:01:00.090
environment for gaming or simulations,

17
00:01:00.300 --> 00:01:04.740
the two standards that have become commonplace are using unreal engine and unity

18
00:01:04.741 --> 00:01:08.280
as a toolkit.
Unreal engine is great for high end graphics,

19
00:01:08.281 --> 00:01:11.280
but if you're a beginner,
unity is really the way to go.

20
00:01:11.310 --> 00:01:16.310
It's allowed small budget indie developers to create really popular games

21
00:01:16.650 --> 00:01:21.600
through rapid prototyping games like Minecraft limbo and super meat.

22
00:01:21.601 --> 00:01:24.300
Boy,
we're all created using unity,

23
00:01:24.870 --> 00:01:27.480
but even large studios like CCP,

24
00:01:27.481 --> 00:01:32.130
the creators of Eve online use it for rapidly prototyping game concepts.

25
00:01:32.400 --> 00:01:36.990
Unity provides a game engine in a box that means a physics and a rendering

26
00:01:36.991 --> 00:01:39.780
engine with hooks for several scripting languages.

27
00:01:40.110 --> 00:01:44.400
It gives us this really cool visual editor for manipulating the game

28
00:01:44.401 --> 00:01:48.780
environment,
which lowers the barrier to entry to development substantially.

29
00:01:48.840 --> 00:01:53.100
If we started completely from scratch with c plus plus and open GL,

30
00:01:53.400 --> 00:01:57.360
it would take us days to get to the point where there's actually something

31
00:01:57.510 --> 00:02:01.740
rendered on screen,
but with unity,
this takes about 10 seconds,

32
00:02:01.950 --> 00:02:04.860
take that deadlocks to get started.
With unity,

33
00:02:04.861 --> 00:02:07.530
there's a free version and a pro version.

34
00:02:07.560 --> 00:02:12.050
We can just use the free version to as little dicky would say,
say that money.

35
00:02:12.720 --> 00:02:16.860
The big difference between the two is that the free version doesn't have some of

36
00:02:16.861 --> 00:02:20.970
the advanced rendering options that creates better looking fast running

37
00:02:20.971 --> 00:02:24.360
environments.
But for beginners we can just skip that.
For Right now,

38
00:02:24.420 --> 00:02:27.240
installing unity is pretty straight forward.

39
00:02:27.330 --> 00:02:31.740
We can download an executable right from the website and follow the installer

40
00:02:31.741 --> 00:02:35.840
instructions.
Unity supports both c sharp and Java script.

41
00:02:36.060 --> 00:02:40.530
Will you see sharp since that's what's most compatible with the ML agents tool?

42
00:02:40.830 --> 00:02:41.790
A class file,

43
00:02:41.791 --> 00:02:46.380
we'll lay out the definition of an object and we can instantiate an object from

44
00:02:46.381 --> 00:02:50.280
a class.
We can attach a new script to an object easily,

45
00:02:50.281 --> 00:02:55.281
so if we want an enemy logic object that handles AI for an enemy in our game,

46
00:02:55.800 --> 00:02:57.990
we'd write an enemy at logic class.

47
00:02:58.020 --> 00:03:02.800
Then attach that to every enemy entity.
When we run our game,

48
00:03:02.830 --> 00:03:06.880
each enemy will be equipped with a copy of the enemy logic object.

49
00:03:07.240 --> 00:03:11.710
Once we've installed unity,
we can clone the MLA agents,
get hub repository.

50
00:03:12.040 --> 00:03:16.630
Let's build a preexisting example environment,
train an agent in it,

51
00:03:16.810 --> 00:03:20.350
and then embed the trained model into the unity environment.

52
00:03:20.410 --> 00:03:23.950
First we'll launch the unity editor and log in if necessary.

53
00:03:24.220 --> 00:03:26.290
Inside the unity environment folder,

54
00:03:26.410 --> 00:03:29.080
we'll see the project panel at the bottom of the tool.

55
00:03:29.260 --> 00:03:33.670
We can navigate to the ML agent examples folder and select the 3d bowl

56
00:03:33.671 --> 00:03:36.670
environment.
Once we double click the [inaudible] icon,

57
00:03:36.820 --> 00:03:41.350
it'll load all the environment assets.
I guess we are now officially balling.

58
00:03:41.500 --> 00:03:42.400
Ah No.

59
00:03:42.790 --> 00:03:47.590
There are three kinds of objects within any kind of learning environment.

60
00:03:47.890 --> 00:03:52.780
The first is the agent which has a unique set of states and observations,

61
00:03:52.781 --> 00:03:57.520
takes unique actions within the environment and receives unique rewards for the

62
00:03:57.521 --> 00:03:59.860
actions it takes in this environment.

63
00:04:00.190 --> 00:04:04.840
This is classically called the agent environment loop.
In reinforcement learning,

64
00:04:05.290 --> 00:04:08.260
each agent has a brain.
Unlike Sophia,

65
00:04:08.680 --> 00:04:13.420
the brain defines a specific state and action space and it's responsible for

66
00:04:13.421 --> 00:04:17.320
deciding which actions each of its linked agents will take.

67
00:04:17.650 --> 00:04:21.520
The brain can be external,
meaning decisions are made using tensorflow,

68
00:04:21.910 --> 00:04:26.350
internal meaning and decisions are made using a trained model embedded into the

69
00:04:26.351 --> 00:04:27.100
project.

70
00:04:27.100 --> 00:04:32.100
The tensorflow sharp player meaning player input decisions or heuristic meaning

71
00:04:33.671 --> 00:04:36.370
decisions are made by hand coded behavior.

72
00:04:36.520 --> 00:04:39.220
The last kind of object is the academy.

73
00:04:39.430 --> 00:04:42.130
It contains all the brains within the environment.

74
00:04:42.310 --> 00:04:46.750
We can set multiple agents to a single brain or give them their own brains,

75
00:04:46.840 --> 00:04:51.310
decide actions to take in batch fashions and we can even utilize parallel

76
00:04:51.311 --> 00:04:55.090
computation.
There are lots of options here.
Adversarial play,

77
00:04:55.240 --> 00:04:58.330
cooperative play.
Let's go ahead and expand the ball.

78
00:04:58.331 --> 00:05:02.260
Three d game object and locate its child object ball,

79
00:05:02.261 --> 00:05:05.560
three d brain within the scene hierarchy in the editor.

80
00:05:05.860 --> 00:05:10.090
We'll ensure that type of brain for this object is set to external.

81
00:05:10.270 --> 00:05:13.990
Then it's time to build this thing.
Under file build settings.

82
00:05:14.140 --> 00:05:18.880
We'll choose development build to log debug messages,
then click build.

83
00:05:19.150 --> 00:05:23.830
This will save the environment binary to the Python sub directory of are cloned

84
00:05:23.831 --> 00:05:24.700
repository.

85
00:05:25.000 --> 00:05:29.080
Now we can train our agent brain using reinforcement learning.

86
00:05:29.470 --> 00:05:34.090
There's a premade Jupiter notebook in the repository we downloaded that we can

87
00:05:34.091 --> 00:05:35.890
use directly to train our agent.

88
00:05:35.920 --> 00:05:39.460
It's using the Python Api that unity provides for machine learning.

89
00:05:39.700 --> 00:05:44.080
We'll launch Jupiter than find the proximal policy optimization algorithm

90
00:05:44.081 --> 00:05:44.890
notebook.

91
00:05:44.890 --> 00:05:49.180
I've got a great video on how PPO works to see the link in the description.

92
00:05:49.690 --> 00:05:53.650
We'll set the environment to the name of our environment file from earlier.

93
00:05:53.830 --> 00:05:57.830
Then set the run path to a directory of our choice.
Lastly,

94
00:05:57.831 --> 00:06:01.100
we'll run all selves of the notebook except for the last one.

95
00:06:01.460 --> 00:06:06.260
We can use tensor board to observe the training process in more detail as it

96
00:06:06.261 --> 00:06:09.440
runs by running the tensor board command from command line.

97
00:06:09.590 --> 00:06:12.320
We can observe how rewards are being accumulated,

98
00:06:12.380 --> 00:06:14.150
how the loss functions decrease,

99
00:06:14.300 --> 00:06:17.510
and how the policy and value functions are changing over time.

100
00:06:17.870 --> 00:06:22.660
After the average award is about 75 we can stop the training process since the

101
00:06:22.661 --> 00:06:25.700
script periodically saves models that we can use.

102
00:06:26.060 --> 00:06:28.250
This is a trained tensorflow model.

103
00:06:28.490 --> 00:06:33.350
Our job now is to convert this model into a unity ready format.
To do this,

104
00:06:33.380 --> 00:06:34.790
we'll use tensorflow sharp.

105
00:06:35.120 --> 00:06:40.070
These are.net bindings to the tensorflow library.
I know,
I know it's Microsoft,

106
00:06:40.370 --> 00:06:42.170
but it's okay.
We need this right now.

107
00:06:42.500 --> 00:06:46.850
We need to enable it so first we'll download it and place it in the assets

108
00:06:46.851 --> 00:06:49.070
folder.
We'll target our platform,

109
00:06:49.100 --> 00:06:53.210
set the scripting runtime version two experimental and add the enabled

110
00:06:53.211 --> 00:06:56.150
tensorflow flag to the defined symbols.

111
00:06:56.420 --> 00:06:59.150
We can save the project and restart the editor.

112
00:06:59.390 --> 00:07:04.010
Then we'll export the trained tensorflow graph and move the export and bites

113
00:07:04.011 --> 00:07:06.350
file to the TF models directory.

114
00:07:06.650 --> 00:07:11.270
We can select the three d ball scene in the unity editor and select the ball 3d

115
00:07:11.300 --> 00:07:15.950
brain object from the scene hierarchy will change the type of brain to internal

116
00:07:16.070 --> 00:07:20.420
and drag the bites file from the project window of the editor to the graph model

117
00:07:20.421 --> 00:07:23.630
placeholder into d ball brain inspector window.

118
00:07:23.960 --> 00:07:26.450
Then set the graph placeholder size to one.

119
00:07:26.720 --> 00:07:30.650
Our placeholder will be called epsilon with the type of floating point and a

120
00:07:30.651 --> 00:07:34.640
range of values from zero to zero.
Once we pressed the play button,

121
00:07:34.820 --> 00:07:38.540
we can visualize the train model being used to control the behavior of the

122
00:07:38.541 --> 00:07:41.150
balance ball within the editor itself.

123
00:07:41.870 --> 00:07:45.950
The three points to remember from this video are that unity is the easiest way

124
00:07:45.951 --> 00:07:50.240
for developers and researchers to train their AI algorithms in a three d

125
00:07:50.241 --> 00:07:51.080
environment.

126
00:07:51.410 --> 00:07:56.410
The unity ml agents library lets us build AI using tensorflow and export them to

127
00:07:57.021 --> 00:08:01.280
a three d environment directly.
And it revolves around three key objects,

128
00:08:01.281 --> 00:08:03.440
an agent,
a brain,
and an academy.

129
00:08:03.710 --> 00:08:07.520
One or more agents can share a brain and they all live in the academy

130
00:08:07.521 --> 00:08:08.354
environment.

131
00:08:08.570 --> 00:08:12.960
Last week's coding challenge winner is Brendan Hunts necked using care.

132
00:08:13.130 --> 00:08:16.910
He implemented his own version of deep minds.
Alpha go zero,

133
00:08:17.060 --> 00:08:20.720
but for a different game called Othello.
I'm blown away.

134
00:08:20.750 --> 00:08:25.750
A plus 12 plus Brendan and the runner up is Shaad Mon Coochie car who used Karus

135
00:08:26.630 --> 00:08:30.350
to detect happiness levels via sentiment analysis.
Great work.

136
00:08:30.740 --> 00:08:35.240
This week's challenge is to build something with unity's ml agents.

137
00:08:35.420 --> 00:08:37.670
Post your get hub link in the comments section,

138
00:08:37.820 --> 00:08:41.780
and I'll personally review them and announce the top two entries.
Next Friday,

139
00:08:41.870 --> 00:08:43.790
please subscribe for more programming videos.

140
00:08:43.791 --> 00:08:47.120
And for now I've got to get some sun,
so thanks for watching.

