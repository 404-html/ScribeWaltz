WEBVTT

1
00:00:00.090 --> 00:00:04.800
That's for guests.
The fake go.
Um,
that one,
they're both real.
They're balls

2
00:00:10.890 --> 00:00:12.790
of a world.
It's a Raj.

3
00:00:12.810 --> 00:00:16.470
And this episode we're going to talk about generative adversarial networks.

4
00:00:16.500 --> 00:00:19.260
Deep learning has done a lot for us,
but it's most incredible.

5
00:00:19.261 --> 00:00:21.930
Successes have involved discriminative models.

6
00:00:22.020 --> 00:00:25.020
Discriminated models are great for classification tasks.

7
00:00:25.050 --> 00:00:27.090
They discriminate not in that way.

8
00:00:27.091 --> 00:00:30.780
Obviously models like neural nets support vector machines and hidden Markov

9
00:00:30.781 --> 00:00:34.770
chains where we map high dimensional rich sensory input to a class label.

10
00:00:34.900 --> 00:00:38.130
Discriminated model doesn't care about how data was generated.

11
00:00:38.160 --> 00:00:40.050
It just categorizes a given signal.

12
00:00:40.080 --> 00:00:43.860
They're the secret sauce behind tools like facial recognition and the language

13
00:00:43.861 --> 00:00:45.690
translation and sentiment analysis.

14
00:00:45.720 --> 00:00:48.630
But there's another type of model that hasn't been as successful.

15
00:00:48.660 --> 00:00:49.710
The generative model,

16
00:00:49.740 --> 00:00:53.580
generative models are harder to get right due to the difficulty of approximating

17
00:00:53.581 --> 00:00:56.110
some really hard probabilistic computations.

18
00:00:56.160 --> 00:01:00.450
A generative algorithm does care how data was generated in order to categorize a

19
00:01:00.451 --> 00:01:01.920
signal.
It asks the question,

20
00:01:01.950 --> 00:01:04.710
which category is most likely to generate the signal?

21
00:01:04.770 --> 00:01:08.370
Anytime you'd like to generate some novel data based on some input data,

22
00:01:08.430 --> 00:01:12.750
whether that be a chatbot's replies or a song or even a short story.

23
00:01:12.780 --> 00:01:14.340
Generative models are the way to go.

24
00:01:14.400 --> 00:01:17.550
So what is a generative adversarial network organic?
Well,

25
00:01:17.580 --> 00:01:21.190
Gans were introduced by a straight gene and Ian Goodfellow who didn't give a

26
00:01:21.240 --> 00:01:24.300
bite.
The basic idea is this,
you have two models,

27
00:01:24.450 --> 00:01:29.040
a generative model g that generates new data and a discriminative model d that

28
00:01:29.041 --> 00:01:31.890
estimates the probability that whatever sample data g shows,

29
00:01:31.891 --> 00:01:34.140
it came from training data rather than gee itself,

30
00:01:34.170 --> 00:01:36.630
they're essentially playing what's called a mini Max gain.

31
00:01:36.690 --> 00:01:41.520
Mini Max is a well known strategy of always minimizing the maximum possible
loss,

32
00:01:41.550 --> 00:01:44.550
which can result from a choice that one of two players can make.

33
00:01:44.580 --> 00:01:48.210
So Ge has fed a bunch of training data and attempts to generate new data based

34
00:01:48.211 --> 00:01:50.730
on that input.
Every time g generates a new sample.

35
00:01:50.760 --> 00:01:54.150
D We'll try to determine if the sample is from the models distribution where the

36
00:01:54.151 --> 00:01:58.500
training data distribution.
Think of Ge as a counterfeiter and d is the police.

37
00:01:58.530 --> 00:01:59.670
They're adversaries.
Gee,

38
00:01:59.671 --> 00:02:02.730
we'll keep trying to produce fake currency and use it without detection.

39
00:02:02.760 --> 00:02:04.920
While deed tries to detect if it's counterfeit or not,

40
00:02:04.950 --> 00:02:08.820
this keeps on happening and both bottles improve their methods until eventually

41
00:02:08.850 --> 00:02:11.250
the counterfeits are indistinguishable from the real thing.

42
00:02:11.310 --> 00:02:13.710
So what type of model do we use for Dng?
Well,

43
00:02:13.711 --> 00:02:16.530
many different types of models can be used.
In the original paper.

44
00:02:16.531 --> 00:02:20.700
The author use something called a multilayer perceptron for both to generate

45
00:02:20.701 --> 00:02:23.760
photorealistic images based on a training Dataset of images.

46
00:02:23.761 --> 00:02:27.210
But there's an even more interesting paper that came out that you scans that I

47
00:02:27.211 --> 00:02:27.960
want to talk about.

48
00:02:27.960 --> 00:02:32.160
It came out just last month and it's called generative adversarial text to image

49
00:02:32.161 --> 00:02:34.050
synthesis.
These guys created again,

50
00:02:34.080 --> 00:02:38.670
that can generate a photo realistic image based on a caption.
I am dead serious.

51
00:02:39.510 --> 00:02:42.360
So when it was given the caption,
a large blue octopus,

52
00:02:42.361 --> 00:02:44.670
Chi flies above the people having fun at the beach.

53
00:02:44.700 --> 00:02:48.300
It was actually able to generate an image of that show.
Just that,

54
00:02:48.330 --> 00:02:49.560
so how did it do this?
Well,

55
00:02:49.590 --> 00:02:53.100
they use a data set of images and their associated captions as the training
data.

56
00:02:53.130 --> 00:02:56.160
First they had to encode the data before they fed it into the models.

57
00:02:56.190 --> 00:02:57.001
They use an encoder,

58
00:02:57.001 --> 00:03:00.460
which was a hybrid neural network called a character level convolutional

59
00:03:00.461 --> 00:03:01.420
recurrent network.

60
00:03:01.450 --> 00:03:04.600
It takes both the image and the caption is input and creates a joint

61
00:03:04.601 --> 00:03:07.000
representation of both in multimodal space.

62
00:03:07.030 --> 00:03:09.700
It does this for both the generator and the discriminator.

63
00:03:09.730 --> 00:03:12.880
Both models are a d convolutional feedforward neural network.

64
00:03:12.910 --> 00:03:13.870
Once they had the encoding,

65
00:03:13.871 --> 00:03:17.380
they fed it into both models and each generated an image,
the generator,

66
00:03:17.381 --> 00:03:21.250
and that creates a synthetic image from the embedding an image that is close to

67
00:03:21.251 --> 00:03:24.670
the real image but not quite.
The discriminator net receives three inputs.

68
00:03:24.730 --> 00:03:26.170
The real image with the right text,

69
00:03:26.200 --> 00:03:29.230
the real image with the wrong text and a fake image with the right text.

70
00:03:29.260 --> 00:03:32.650
It encodes each of these and compares them to what the generator shows it.

71
00:03:32.680 --> 00:03:36.130
Looking for a similarity between its inputs and the generators outputs.

72
00:03:36.160 --> 00:03:36.871
Each time step,

73
00:03:36.871 --> 00:03:40.480
you will try to determine if the generated image was synthetic or real by doing

74
00:03:40.481 --> 00:03:42.970
this comparison.
After the determination has been made,

75
00:03:42.971 --> 00:03:46.690
the weights of both the discriminator and the generator are updated so they both

76
00:03:46.691 --> 00:03:49.120
get better over time.
By the time training is over,

77
00:03:49.150 --> 00:03:52.780
the generator will be pretty good at fooling the discriminator I producing photo

78
00:03:52.781 --> 00:03:53.740
realistic images.

79
00:03:53.770 --> 00:03:55.960
They looked like they must've come straight from the training data.

80
00:03:55.990 --> 00:03:57.520
So let's build our own game.

81
00:03:57.550 --> 00:04:01.150
We're going to build a simple python script that demonstrates how gangs work.

82
00:04:01.180 --> 00:04:05.110
Both models will be represented by learning Gaussian distribution curves that we

83
00:04:05.111 --> 00:04:08.200
plot using pipeline.
Once we have our dependencies in place,
well,

84
00:04:08.201 --> 00:04:10.300
one to define our hyper parameters.

85
00:04:10.330 --> 00:04:13.540
These are all different activation functions that will apply to our models.

86
00:04:13.541 --> 00:04:14.350
At some point.

87
00:04:14.350 --> 00:04:17.720
We'll also want to define how much we want to train our models as well as a

88
00:04:17.721 --> 00:04:19.990
number of hidden layers.
We want them to have,

89
00:04:20.110 --> 00:04:22.090
well then write for helper functions.

90
00:04:22.180 --> 00:04:25.810
The first two will help us visualize our golf Sian curves.
In our plot,

91
00:04:25.900 --> 00:04:29.920
GE builds a generative network and d bills the discriminative network.

92
00:04:29.950 --> 00:04:33.880
We'll initialize our networks and right applauding function to plot both curves.

93
00:04:33.940 --> 00:04:36.250
Finally,
we'll train our network using a for loop.

94
00:04:36.280 --> 00:04:40.240
We'll draw samples from a uniform distribution than trained both networks at

95
00:04:40.241 --> 00:04:41.260
different intervals.
Well,

96
00:04:41.261 --> 00:04:44.350
plot the graph as we train our model so we can visualize the result.

97
00:04:44.360 --> 00:04:45.830
Let's see what it looks like as a trains,

98
00:04:45.940 --> 00:04:49.360
as a trains to generate or curve will look more and more like the true data

99
00:04:49.361 --> 00:04:52.960
distribution curve as it tries to fool the discriminator network and there you

100
00:04:52.961 --> 00:04:57.340
have it.
You might be wondering what are the real world use cases for gans?
Well,

101
00:04:57.400 --> 00:05:01.420
gaming is one.
It's a daunting task to create a huge three d map manually,

102
00:05:01.450 --> 00:05:04.780
but with gans programmers can write a part of the map and generate the rest.

103
00:05:04.810 --> 00:05:06.370
Another is interior design.

104
00:05:06.430 --> 00:05:08.740
If a designer wants to visualize what a room will look like,

105
00:05:08.770 --> 00:05:10.840
they can just ask the machine for a photo of,
say,

106
00:05:10.841 --> 00:05:14.620
a living room with Beige Sofa is facing each other and for foreign is in the

107
00:05:14.621 --> 00:05:18.160
scene.
Pretty much anytime you need help visualizing anything,

108
00:05:18.190 --> 00:05:21.160
Gans can help fill in the gaps and augment your own imagination.

109
00:05:21.220 --> 00:05:23.110
I've got some great links for you in the description.

110
00:05:23.140 --> 00:05:25.180
Please subscribe for more and Mel Videos for now.

111
00:05:25.210 --> 00:05:27.820
I've got to go picks a deadlock,
so thanks for watching.

