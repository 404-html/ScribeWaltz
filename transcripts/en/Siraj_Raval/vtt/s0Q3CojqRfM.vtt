WEBVTT

1
00:00:00.030 --> 00:00:03.960
Hello world,
it's Saroj and what's the deal with factors?

2
00:00:04.200 --> 00:00:07.740
You're going to see this word a lot in machine learning and it's one of the most

3
00:00:07.741 --> 00:00:10.380
crucial concepts to understand.

4
00:00:10.650 --> 00:00:15.650
A huge part of machine learning is finding a way to properly represent some

5
00:00:15.841 --> 00:00:17.670
dataset programmatically.

6
00:00:18.120 --> 00:00:22.710
Let's say you're a manager at Tesla and you're given a Dataset of some

7
00:00:22.711 --> 00:00:26.460
measurements for each car that was produced in the past week.

8
00:00:26.760 --> 00:00:30.600
Each car on the list has three measurements or features,

9
00:00:30.750 --> 00:00:33.330
its length,
width and height,

10
00:00:33.540 --> 00:00:38.540
so a given car can then be represented as a point in three dimensional space

11
00:00:38.821 --> 00:00:43.710
where the values in each dimension correlates to one of the features we are

12
00:00:43.740 --> 00:00:48.720
measuring.
This same logic applies to data points that have 300 features.

13
00:00:48.870 --> 00:00:51.900
We can represent them in 300 dimensional space.

14
00:00:52.170 --> 00:00:57.170
While this is intuitively hard for us to understand as three dimensional beings,

15
00:00:57.900 --> 00:01:00.120
machines can do this very well,

16
00:01:05.100 --> 00:01:08.310
Kevin,
this data point x is considered a vector.

17
00:01:08.520 --> 00:01:11.040
A vector is a one dimensional array.

18
00:01:11.260 --> 00:01:14.730
Think of it as a list of values or a row in a table.

19
00:01:14.880 --> 00:01:19.230
A vector of n elements is an end dimensional vector with one dimension for each

20
00:01:19.231 --> 00:01:21.990
element.
So for a four dimensional data points,

21
00:01:22.200 --> 00:01:26.160
we can use a one by four array to hold its four feature values.

22
00:01:26.400 --> 00:01:30.750
And because it represents a set of features,
we call it a feature vector.

23
00:01:31.050 --> 00:01:33.750
More general than a vector is a matrix.

24
00:01:33.930 --> 00:01:38.930
A matrix is a rectangular array of numbers and a vector is a row or column of a

25
00:01:39.751 --> 00:01:42.360
Matrix.
So each row in a matrix,

26
00:01:42.420 --> 00:01:46.530
it could represent a different data points with each column being its respective

27
00:01:46.531 --> 00:01:51.300
features.
Less general than a vector is a scaler,
which is they single number.

28
00:01:51.660 --> 00:01:55.830
The most general term for all of these concepts is a tensor.

29
00:01:56.040 --> 00:01:58.710
A tensor is a multidimensional array.

30
00:01:58.860 --> 00:02:01.140
So a first order tensor is a vector.

31
00:02:01.320 --> 00:02:06.090
A second order tensor is a matrix and tensions of order three and higher are

32
00:02:06.091 --> 00:02:10.770
called higher order tensors.
So if a one D or it looks like a lined up,

33
00:02:10.920 --> 00:02:14.520
who are you?
I think they get it,
but I think they get it.

34
00:02:15.180 --> 00:02:19.650
You could represent a social graph that contains friends of Friends of friends

35
00:02:19.770 --> 00:02:24.660
as a higher order tensor.
This is why Google built a library called tensorflow.

36
00:02:24.840 --> 00:02:29.640
It allows you to create a computational graph where cancers created from

37
00:02:29.710 --> 00:02:34.710
datasets can flow through a series of mathematical operations that optimize for

38
00:02:35.881 --> 00:02:40.881
an objective and why they built an entirely new type of chip called a Tpu or

39
00:02:41.131 --> 00:02:42.480
tensor processing unit.

40
00:02:42.840 --> 00:02:46.470
As computational power and the amount of data we have increases,

41
00:02:46.610 --> 00:02:49.140
we are becoming more capable of processing.

42
00:02:49.260 --> 00:02:54.260
Multidimensional data vectors are typically represented in a multitude of ways

43
00:02:55.590 --> 00:02:58.890
and they're used in many different fields of science,

44
00:02:58.930 --> 00:03:03.930
especially physics since factors act as a bookkeeping tool to keep track of two

45
00:03:03.941 --> 00:03:05.290
pieces of information,

46
00:03:05.560 --> 00:03:09.280
typically a magnitude and a direction for physical quantity.

47
00:03:30.940 --> 00:03:34.090
For example,
in Einstein's general theory of relativity,

48
00:03:34.330 --> 00:03:39.040
the curvature of space time which gives rise to gravity is described by what's

49
00:03:39.041 --> 00:03:41.380
called a reman curvature tensor,

50
00:03:41.620 --> 00:03:44.620
which is a tensor of order for so Badass,

51
00:03:44.650 --> 00:03:48.490
so we can represent not only the fabric of reality this way,

52
00:03:48.700 --> 00:03:52.180
but the gradient of our optimization problem as well.

53
00:03:52.270 --> 00:03:56.110
During first order optimization,
the weights of our model,

54
00:03:56.170 --> 00:04:00.460
our updated incrementally.
After each pass over the training data set,

55
00:04:00.760 --> 00:04:04.030
given an error function like a sum of squared errors,

56
00:04:04.180 --> 00:04:07.660
we can compute the magnitude and direction of the way.

57
00:04:07.690 --> 00:04:11.770
Update by taking a step in the opposite direction of the error gradient.

58
00:04:11.980 --> 00:04:14.440
This all comes from linear Algebra.

59
00:04:14.830 --> 00:04:19.830
Algebra roughly means relationships and it explores the relationships between

60
00:04:20.200 --> 00:04:21.280
unknown numbers.

61
00:04:22.180 --> 00:04:25.720
Linear Algebra roughly means line like relationships.

62
00:04:25.750 --> 00:04:29.950
It's the way of organizing information about vector spaces that makes

63
00:04:29.951 --> 00:04:33.670
manipulating groups of numbers simultaneously easy.

64
00:04:34.090 --> 00:04:39.090
It defines these structures like vectors and matrices to hold these numbers and

65
00:04:39.341 --> 00:04:44.050
introduces new rules on how to add,
multiply,
subtract,

66
00:04:44.200 --> 00:04:47.020
and divide them.
So given two arrays,

67
00:04:47.140 --> 00:04:50.440
the Algebraic way to multiply them would be to do it like this,

68
00:04:50.740 --> 00:04:53.500
and the linear Algebraic way would look like this.

69
00:04:53.710 --> 00:04:58.300
We compute the dot product instead of multiplying each number like this.

70
00:04:58.450 --> 00:05:02.800
The Linear Algebraic approach is three times faster.
In this case,

71
00:05:02.860 --> 00:05:07.270
any type of data can be represented as a vector.
Images,
videos,

72
00:05:07.330 --> 00:05:11.170
stock indices,
text,
audio signals,
Douggie dancing,

73
00:05:11.530 --> 00:05:15.400
no matter the type of data.
It can be broken down in Qa to set of numbers.

74
00:05:17.350 --> 00:05:21.650
<v 2>The mom is not really accepting the data.
It keeps throwing errors.
Let me see.</v>

75
00:05:22.310 --> 00:05:24.200
Oh,
it looks like you've got to vectorize it.
What do you mean?

76
00:05:24.710 --> 00:05:27.830
The model you wrote?
Expected tensor over a certain size is its input.

77
00:05:27.831 --> 00:05:31.760
So we basically got to reshape the input data so it's in the right vector space

78
00:05:31.761 --> 00:05:32.571
and then once it is,

79
00:05:32.571 --> 00:05:35.990
we can compute things like the coastline distance between data points and the

80
00:05:35.991 --> 00:05:39.680
vector norm.
Is there a point in library to the day you got to love num?

81
00:05:39.681 --> 00:05:43.250
Pi vectorization is essentially just a matrix operation and I can do it in a

82
00:05:43.251 --> 00:05:44.084
single line.

83
00:05:44.670 --> 00:05:49.140
<v 0>Awesome.
Well you vectorize it dog.
I've got a bad propagate out for the day.
Cool.</v>

84
00:05:49.470 --> 00:05:53.250
We're to tend to base.
All right.
See Ya.
See Ya.

85
00:05:56.350 --> 00:06:00.350
A researcher named Macola off use the machine learning model called a neural

86
00:06:00.410 --> 00:06:03.500
network to create vectors for words.

87
00:06:03.620 --> 00:06:08.620
Word two VEC given some input corpus of texts like thousands of news articles.

88
00:06:09.470 --> 00:06:13.940
It would try to predict the next word in a sentence given the words around it.

89
00:06:14.180 --> 00:06:17.420
So a given word is encoded into a vector.

90
00:06:17.570 --> 00:06:21.560
The model then uses that vector to try and predict the next word.

91
00:06:21.830 --> 00:06:25.100
If it's prediction doesn't match the actual next word.

92
00:06:25.280 --> 00:06:27.710
The components of this vector are adjusted.

93
00:06:27.920 --> 00:06:32.920
Each words context in the corpus acts as the teacher sending air signals back to

94
00:06:33.501 --> 00:06:34.370
adjust the vector.

95
00:06:34.490 --> 00:06:38.570
The vectors of words that are judged similarly by their context are iteratively

96
00:06:38.571 --> 00:06:42.140
nudged closer together by adjusting the numbers in the vector.

97
00:06:42.320 --> 00:06:47.030
And so after training,
the model learns thousands of vectors for words.

98
00:06:47.180 --> 00:06:50.930
Give it a new word and it will find its associated word vector.

99
00:06:51.020 --> 00:06:55.190
Also called word embedding.
Vector is don't just represent data.

100
00:06:55.220 --> 00:06:57.050
They help represent our models.

101
00:06:57.051 --> 00:07:02.051
Too many types of machine learning models represent their learnings as vectors.

102
00:07:02.690 --> 00:07:05.630
All types of neural networks do this.
Given some data,

103
00:07:05.810 --> 00:07:09.410
it will learn dense representations of that data.

104
00:07:09.710 --> 00:07:14.540
These representations are essentially categories akin to if you had a data set

105
00:07:14.541 --> 00:07:16.520
of different colored I pictures,

106
00:07:16.760 --> 00:07:21.170
it will learn a general representation for all I colors.

107
00:07:21.200 --> 00:07:26.200
So given a new unlabeled I picture it would be able to recognize it as an I see

108
00:07:27.680 --> 00:07:31.180
factors good.
Once data is vectorized,

109
00:07:31.181 --> 00:07:32.990
we can do so many things with it.

110
00:07:33.170 --> 00:07:36.860
A trained word to VEC model turns words into vectors.

111
00:07:37.040 --> 00:07:41.630
Then we can perform mathematical operations on these vectors.

112
00:07:41.810 --> 00:07:46.810
We can see how closely related words are by computing the distance between their

113
00:07:47.031 --> 00:07:49.520
vectors.
The word Sweden for example,

114
00:07:49.521 --> 00:07:53.960
is closely related to other wealthy northern European countries because the

115
00:07:53.961 --> 00:07:58.100
distance between them is small.
When plotted on a graph.

116
00:07:58.160 --> 00:08:03.160
Word vectors that are similar tend to cluster together like types of animals

117
00:08:03.500 --> 00:08:08.500
associations can be built like Rome is to Italy as Beijing is to China and

118
00:08:09.681 --> 00:08:13.940
operations like performing hotel plus motel gives us Holiday Inn.

119
00:08:14.420 --> 00:08:18.650
Incredibly vectorizing words is able to capture their semantic meanings.

120
00:08:18.680 --> 00:08:19.520
Numerically.

121
00:08:19.820 --> 00:08:24.620
The way we're able to compute the distance between two vectors is by using the

122
00:08:24.621 --> 00:08:26.930
notion of a vector norm.

123
00:08:27.080 --> 00:08:32.080
A norm is any function g that maps factors to real numbers that satisfies the

124
00:08:32.451 --> 00:08:35.690
following conditions.
The lengths are always positive.

125
00:08:35.960 --> 00:08:38.060
The length of zero implies zero.

126
00:08:38.210 --> 00:08:43.210
Scalar multiplication extends lengths in a predictable way and distances add

127
00:08:44.061 --> 00:08:47.060
reasonably so in a basic vector space.

128
00:08:47.150 --> 00:08:51.950
The norm of a vector would be it's absolute value and the distance between two

129
00:08:51.951 --> 00:08:53.240
numbers like this.

130
00:08:53.300 --> 00:08:56.820
Usually the length of the vector is calculated using the Euclidean norm,

131
00:08:56.970 --> 00:09:01.020
which is defined like so,
but this isn't the only way to define length.

132
00:09:01.320 --> 00:09:04.410
The others you'll see the terms l one norm,

133
00:09:04.411 --> 00:09:07.140
an l two norm used a lot in machine learning.

134
00:09:07.470 --> 00:09:09.720
The l two norm is the Euclidean norm.

135
00:09:10.020 --> 00:09:13.140
The l one norm is also called the Manhattan distance.

136
00:09:13.380 --> 00:09:18.380
We can use either to normalize a vector to get its unit vector and use that to

137
00:09:18.691 --> 00:09:20.820
compute the distance computing.

138
00:09:20.821 --> 00:09:25.260
The distance between vectors is useful for showing users recommendations.

139
00:09:25.470 --> 00:09:29.460
Both of these terms are also used in the process of regularization.

140
00:09:29.760 --> 00:09:32.430
We train models to fit a set of training data,

141
00:09:32.580 --> 00:09:37.320
but sometimes the model gets so fit to the training data that it doesn't have

142
00:09:37.321 --> 00:09:38.820
good prediction performance.

143
00:09:39.000 --> 00:09:42.330
It can't generalize well to new data points.

144
00:09:42.360 --> 00:09:46.380
To prevent this overfitting,
we have to regularize our model.

145
00:09:46.560 --> 00:09:51.560
The common method to finding the best model is by defining a loss function that

146
00:09:51.571 --> 00:09:55.770
describes how well the model fits the data.
To sum things up,

147
00:09:55.920 --> 00:10:00.450
feature vectors are used to represent numeric or symbolic characteristics of

148
00:10:00.451 --> 00:10:05.420
data called features and a mathematical way they can be represented in

149
00:10:05.490 --> 00:10:10.320
multidimensional vector spaces where we can perform operations on them like

150
00:10:10.321 --> 00:10:13.890
computing their distance and adding them and we can do this by computing the

151
00:10:13.891 --> 00:10:17.250
vector norm,
which describes the size of a vector.

152
00:10:17.400 --> 00:10:19.560
Also useful for preventing overfitting.

153
00:10:19.860 --> 00:10:22.350
The wizard of the week award goes to Vishnu Kumar.

154
00:10:22.590 --> 00:10:27.090
He implemented both gradient descent and Newton's method to create a model,

155
00:10:27.091 --> 00:10:30.870
able to predict the amount of calories burned for cycling a certain distance.

156
00:10:31.110 --> 00:10:35.040
The plots are great and the code is architected very legibly.
Check it out.

157
00:10:35.100 --> 00:10:35.910
Amazing work,

158
00:10:35.910 --> 00:10:39.780
Vishnu and the runner up with the last minute entry is Hamas Schick.

159
00:10:39.870 --> 00:10:41.910
Loved how detailed your notebook was.

160
00:10:41.940 --> 00:10:46.710
This week's challenge is to implement both l one and l two regularization on a

161
00:10:46.711 --> 00:10:49.110
linear regression model.
Check the get hub,

162
00:10:49.111 --> 00:10:52.470
worry me in the description for details and winners will be announced in a week.

163
00:10:52.560 --> 00:10:55.500
Please subscribe for more programming videos,
and for now,

164
00:10:55.560 --> 00:10:58.610
I've got to not be normalized,
so thanks for watching.

