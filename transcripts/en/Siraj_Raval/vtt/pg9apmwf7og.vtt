WEBVTT

1
00:00:03.750 --> 00:00:08.370
Hello world.
It's the Raj.
How's everybody doing?
I'm in this new space.

2
00:00:08.371 --> 00:00:12.770
I'm at the upload VR space,
so I've got this whole code behind me things.

3
00:00:12.771 --> 00:00:15.000
So it's not all ghetto with me and Google hangouts.

4
00:00:15.150 --> 00:00:16.320
So I'm very excited about that.

5
00:00:16.620 --> 00:00:20.880
Let's see who is in the chat room and let me give my shout outs to people who

6
00:00:20.881 --> 00:00:25.770
showed up alive.
And then we're going to do our five minute Q and.
A.
Okay.

7
00:00:26.370 --> 00:00:28.380
So we've got Brandon,
we've got a,

8
00:00:28.410 --> 00:00:33.410
we've got three Harshaw Kapala Aditya [inaudible] a lot of cool people in the

9
00:00:35.281 --> 00:00:39.150
house right now.
Okay.
We are doing this legit.
Okay.

10
00:00:39.151 --> 00:00:44.040
I am done with the Google hangouts.
I am here to make this quality for you guys.

11
00:00:44.070 --> 00:00:48.240
Okay Mohan,
we've got a lot of cool people in here right now.
Okay.

12
00:00:49.420 --> 00:00:50.100
<v 0>Uh,</v>

13
00:00:50.100 --> 00:00:53.250
<v 1>so Ellery this is awesome.
I know,
it's very awesome.</v>

14
00:00:53.251 --> 00:00:56.370
So I was inspired in part by Dan Shiffman livestream,
you know,

15
00:00:56.371 --> 00:01:00.660
the way he does it.
And so we're going to have to code happened behind me.
Okay?

16
00:01:00.661 --> 00:01:04.050
So I'm here,
this is my computer.
We're going to have the code happen behind me.

17
00:01:04.051 --> 00:01:07.230
Okay?
I finally got into this.
Yes,
thank you.

18
00:01:07.231 --> 00:01:09.540
I'm glad you guys liked the new setup.
Okay,

19
00:01:09.541 --> 00:01:14.541
so Santiago and Veneet and Jay.

20
00:01:15.871 --> 00:01:19.050
And I'll say one more name.
Joyce.
Okay.
That was it for the names.
Okay,

21
00:01:19.051 --> 00:01:19.884
one more name.

22
00:01:19.960 --> 00:01:22.060
<v 0>Uh,
uh,</v>

23
00:01:22.420 --> 00:01:27.400
<v 1>who is it?
There's so many names.
So I'm going to choose done.
D.
A.
N.
That's a cool,</v>

24
00:01:27.520 --> 00:01:32.270
that's a cool a named Don.
And he said couldn't eat you bitches.
Great.
Okay.
Um,

25
00:01:33.040 --> 00:01:36.370
but women are not that.
Okay.
So now we're going to get started with the code.

26
00:01:36.430 --> 00:01:39.610
Okay,
let's do that.
Well actually let's do a five minute Q and.
A.

27
00:01:39.611 --> 00:01:40.211
What am I thinking about?

28
00:01:40.211 --> 00:01:42.550
So shoot me your questions and then we're going to do this.
Okay.

29
00:01:42.551 --> 00:01:46.570
We are going to generate some music.
We are going to generate some music.

30
00:01:46.571 --> 00:01:51.040
It's going to be dope.
We're using tensor flow to generate music.
Okay.

31
00:01:51.370 --> 00:01:55.720
Tensorflow,
music generation is no joke.
Okay.

32
00:01:55.721 --> 00:02:00.040
We are using a,
an encoder decoder model for this.
Okay.

33
00:02:00.041 --> 00:02:04.960
So what are you using for this?
I'm using a open broadcasts or service,

34
00:02:05.470 --> 00:02:10.410
uh,
obs to stream this.
Okay.
Okay.
I,

35
00:02:10.411 --> 00:02:13.690
I gave Schiffman a shout out.
Great Quality.
Thank you.
Okay.

36
00:02:14.770 --> 00:02:18.010
Questions about machine learning.
Guys come on.
Questions about deep learning.

37
00:02:18.011 --> 00:02:22.090
What's been burning your 10 serves in your mind.
Okay.

38
00:02:22.660 --> 00:02:25.440
What do we got here?
I know,
I know we've got,

39
00:02:25.510 --> 00:02:30.320
we're going to get some good questions.
[inaudible] are you going to use Gan?

40
00:02:30.340 --> 00:02:31.550
I'm not going to use again,

41
00:02:31.640 --> 00:02:36.620
but we can use again a for a music generation that is some very,
very new stuff.

42
00:02:36.980 --> 00:02:41.780
Okay.
I am definitely gonna do a license.
I have had so many requests for gans.

43
00:02:41.810 --> 00:02:43.610
In fact,
I'm not just going to be one game video.

44
00:02:43.611 --> 00:02:45.650
I'm probably gonna do like four or five.
Okay.

45
00:02:45.800 --> 00:02:49.250
Cause there is a lot of stuff to talk about.
Backpropagation with tensorflow.

46
00:02:49.280 --> 00:02:54.140
Raj backpropagation video specific backpropagation is coming within a week.
Okay.

47
00:02:54.230 --> 00:02:56.870
Or two weeks,
but it's coming the music.

48
00:02:59.470 --> 00:03:03.640
Okay.
What do
we,
what else can we use similarity detection on music sets?

49
00:03:03.730 --> 00:03:04.391
Absolutely.

50
00:03:04.391 --> 00:03:09.391
So remember if we take any dataset and we convert it into a vector,

51
00:03:10.300 --> 00:03:13.300
we can then find the similarity between these vectors because we were

52
00:03:13.301 --> 00:03:17.890
representing it mathematically.
Okay.
These are matrices.
Okay.

53
00:03:18.230 --> 00:03:22.180
Uh,
any sessions for Ghana.
Okay.
Lstm yes,

54
00:03:22.181 --> 00:03:26.500
we are using LSTM because we are trying to remember longterm dependencies.
Okay.

55
00:03:26.510 --> 00:03:29.500
Music is very long.
It can be multiple pieces.

56
00:03:29.590 --> 00:03:34.210
It could go through multiple phases.
Uh,
I will do Ganz genre for this music.

57
00:03:34.211 --> 00:03:38.980
It's going to be ragtime music.
Okay.
It's going to be piano,
ragtime music.

58
00:03:39.010 --> 00:03:39.281
Okay.

59
00:03:39.281 --> 00:03:43.660
I'm going to take two more questions and then we're going to get started with

60
00:03:43.661 --> 00:03:44.494
this code,

61
00:03:47.670 --> 00:03:51.930
which is the best server to use to run python code all the time for sending some

62
00:03:51.931 --> 00:03:55.690
automated,
uh,
something.
I,
it just went away.
Uh,

63
00:03:55.860 --> 00:04:00.330
the best python server,
the best server,
uh,
APP would be Heroku in my,

64
00:04:00.630 --> 00:04:04.380
in my opinion,
Heroku is,
uh,
the one,
uh,

65
00:04:04.400 --> 00:04:07.710
Heroku is pretty reliable.
Okay,
so two more questions.

66
00:04:07.711 --> 00:04:11.400
Can we work on multiple Dataset to arrive at one result?

67
00:04:11.590 --> 00:04:12.423
<v 0>Uh,</v>

68
00:04:13.330 --> 00:04:14.210
<v 1>yes,
you can.
Uh,</v>

69
00:04:14.211 --> 00:04:19.180
you would want to create one big dataset using all the different datasets.
Okay.

70
00:04:19.660 --> 00:04:21.460
And then,
so one more question.

71
00:04:23.450 --> 00:04:27.560
What technical skills should I know you should know?

72
00:04:28.500 --> 00:04:29.333
<v 0>Uh,</v>

73
00:04:31.710 --> 00:04:35.130
<v 1>so this specific one we're going to need to know a little bit of music theory,</v>

74
00:04:35.131 --> 00:04:38.730
but I'll talk about that.
Okay.
Uh,
and also,
uh,

75
00:04:39.210 --> 00:04:43.350
tensorflow basic syntax.
Syntax.
Okay.
So that's it for the questions.

76
00:04:43.500 --> 00:04:46.590
We have 307 people in this livestream.
Okay.

77
00:04:46.591 --> 00:04:48.840
So we are about to get started right now.

78
00:04:49.140 --> 00:04:53.520
Let's do this deep reinforcement learning,
not today.
So,
okay,

79
00:04:54.060 --> 00:04:55.020
what are we going to do here?

80
00:04:55.140 --> 00:05:00.140
So I was looking on get hub for a bunch of music generation code and I didn't.

81
00:05:00.720 --> 00:05:04.830
So I found there there are some,
uh,
that uses tensorflow specifically.

82
00:05:04.831 --> 00:05:09.690
So there are some,
but the problem is that a lot of them don't compile.

83
00:05:09.780 --> 00:05:14.370
A lot of them don't compile.
And why is that?
Because for music specifically,

84
00:05:15.330 --> 00:05:19.980
music doesn't have good libraries in python right now that we haven't,

85
00:05:20.160 --> 00:05:22.110
it's not that we don't have good music,

86
00:05:22.350 --> 00:05:26.310
it's not that the bridge between music and computer science hasn't been built.

87
00:05:26.400 --> 00:05:29.870
It has,
we've been working on this stuff for 40 plus years.
The,

88
00:05:29.910 --> 00:05:32.400
the bridge that hasn't been built is between,

89
00:05:32.550 --> 00:05:37.550
up is between python three tensorflow and python music libraries.

90
00:05:38.310 --> 00:05:42.060
There is Magenta,
but Magenta is okay.

91
00:05:42.061 --> 00:05:45.540
So first of all about Magenta,
I admire Magenta.
Let's,
let's look at Magenta,

92
00:05:45.541 --> 00:05:47.670
right?
Let's look at,
let's look at Magenta for a little bit.

93
00:05:47.820 --> 00:05:50.100
So Google has this effort too.

94
00:05:50.300 --> 00:05:51.133
<v 0>Uh,</v>

95
00:05:52.050 --> 00:05:55.580
<v 1>talk about music generation intenser flow and I really admire Magenta,</v>

96
00:05:55.980 --> 00:06:00.620
but what I think needs is better documentation.
I mean to me,

97
00:06:00.621 --> 00:06:02.870
I mean when I see this,
it's just like run this,
you know,

98
00:06:02.871 --> 00:06:05.270
it's essentially like a look at this ml generate,

99
00:06:05.450 --> 00:06:09.800
it's essentially like a bash script.
It's like a bash script.
Okay.

100
00:06:09.830 --> 00:06:14.030
And you,
you don't want to bash,
I mean if you're going to make a bash script,

101
00:06:14.031 --> 00:06:17.180
you might as well have a web app.
What we want is modular code,

102
00:06:17.181 --> 00:06:21.470
like some simple examples,
similar to how Karen has simple examples,

103
00:06:21.471 --> 00:06:25.250
maybe 10 to 15 lines of code.
So while I admire Magenta,

104
00:06:25.251 --> 00:06:28.100
I think we need better documentation for it.
Okay.

105
00:06:28.280 --> 00:06:32.390
So in the models folder for Magenta,
for the Melody Rnn,

106
00:06:34.190 --> 00:06:38.720
it has you generate a melody,
right?
So,
okay,
so this is the file melody.
RNN.

107
00:06:39.170 --> 00:06:42.710
So let's,
let's,
let's take a look at melody RNN for a second.

108
00:06:42.710 --> 00:06:45.920
That melody RNN module we have,
let's see.

109
00:06:47.090 --> 00:06:50.000
This code is showing up behind me.
Isn't that so cool guys?
This is amazing.

110
00:06:50.300 --> 00:06:54.110
Let's say there's a lot of stuff happening here.
So I mean,
look at this.

111
00:06:54.530 --> 00:06:57.620
We have Magenta Dot Prodo buff dot generator,

112
00:06:57.680 --> 00:07:01.280
magenta.music.one hot events sequence encoder decoder.

113
00:07:01.730 --> 00:07:05.930
There's a lot of stuff that is,
is encapsulated and abstracted away.

114
00:07:06.080 --> 00:07:09.470
Where is the actual RNN?
Where are the hyper parameters?

115
00:07:09.471 --> 00:07:13.970
Where are the layers?
Okay,
there is too much code for simple models.
Okay,

116
00:07:13.971 --> 00:07:15.890
I agree with Brandon.
There is too much code here.

117
00:07:16.310 --> 00:07:20.720
This is not a problem of Magenta alone.
This is a problem of all music,

118
00:07:20.721 --> 00:07:21.800
machine learning models.

119
00:07:21.890 --> 00:07:26.890
This stuff is not trivial because we are talking about music theory here.

120
00:07:27.170 --> 00:07:31.040
There are people who study music theory fulltime who don't fully understand how

121
00:07:31.041 --> 00:07:33.980
to compose music by there are human minds,
okay?

122
00:07:33.981 --> 00:07:37.430
And yet here we are doing it with machines,
but we can definitely do it.

123
00:07:37.430 --> 00:07:40.790
It's definitely possible.
Okay,
so that's enough about Magenta.

124
00:07:40.910 --> 00:07:45.470
Let's talk about the code that we're going to use.
So I found this code,
okay.
And

125
00:07:47.480 --> 00:07:52.070
there's so much code,
right?
There is so much code here.
So,
uh,

126
00:07:52.090 --> 00:07:55.190
so I found this code and this is what this guy tried to do.
Okay?

127
00:07:55.191 --> 00:07:57.260
So what he tried to do,
and we're going to write code,
we're okay,

128
00:07:57.261 --> 00:08:01.410
we're going to write code,
but before we write code,
I'm going to talk about,
uh,

129
00:08:02.030 --> 00:08:04.190
I'm going to talk about what he did here.

130
00:08:04.220 --> 00:08:08.780
So his first idea was to try a basic recurrent network.
And why did he try that?

131
00:08:08.781 --> 00:08:12.830
Because it's a sequence of notes,
right?
Sequence notes.
We use recurrent nets.

132
00:08:12.831 --> 00:08:16.400
It makes sense.
Predict the next sequence,
okay.
That,
that makes us,

133
00:08:16.460 --> 00:08:18.530
that makes sense.
Right?
So he tried that.
Okay.

134
00:08:18.531 --> 00:08:22.760
He's a basic recurrent nets and he was,
he was inspired by Andre,

135
00:08:22.780 --> 00:08:27.360
her poppy's blog posts,
the unreasonable recur.
That's,

136
00:08:27.740 --> 00:08:29.390
by the way,
Andre Carpathy,

137
00:08:29.391 --> 00:08:33.230
a n d r e j carpathy is amazing.

138
00:08:33.231 --> 00:08:37.310
And you should definitely look up anything he's written.
That guy's awesome.
Okay.

139
00:08:37.311 --> 00:08:41.870
He has,
he knows his shit.
So he tried a basic recurrent network and he was,

140
00:08:41.930 --> 00:08:43.790
we're not using Karratha using tensorflow.

141
00:08:44.030 --> 00:08:48.680
He used a basic recurrent network using Q LSTM cells and he trained it on

142
00:08:48.681 --> 00:08:53.390
ragtime music.
And let's listen to what he generated using just three songs.
Okay.

143
00:08:53.480 --> 00:08:56.880
So he just traded on three songs.
Let's listen to this.

144
00:09:09.160 --> 00:09:11.770
So it sounds great,
right?
It actually sounds pretty good.

145
00:09:11.980 --> 00:09:16.930
The problem is that it was over fit.
In fact,
this song that was generated,

146
00:09:16.931 --> 00:09:17.830
what's an exact,

147
00:09:17.831 --> 00:09:22.450
almost an exact replica of one of the songs that it trained on and that is

148
00:09:22.451 --> 00:09:25.460
overfitting,
right?
So that is a problem.
We don't want to overfit,

149
00:09:25.540 --> 00:09:27.910
we don't want to just repeat whatever we've learned.

150
00:09:28.120 --> 00:09:33.040
We want to create novel music.
So what he did was he said,
okay,

151
00:09:33.190 --> 00:09:36.580
instead of giving it three songs,
let me give it 400 songs.

152
00:09:36.850 --> 00:09:40.390
So then he gave it 400 songs.
Okay.

153
00:09:40.391 --> 00:09:43.540
And so when you gave it 400 songs,
this is what it generated.

154
00:10:02.750 --> 00:10:04.730
Okay?
So that one's not that bad.

155
00:10:04.731 --> 00:10:08.010
Actually using a recurrence using a recurrent network,
uh,

156
00:10:08.240 --> 00:10:11.750
with two LSTM cells on 400 songs.
It's not that bad.

157
00:10:11.900 --> 00:10:14.180
But there was a problem.
So

158
00:10:15.800 --> 00:10:19.940
the problem that he found was that when he,
even when he used 400 songs,

159
00:10:20.350 --> 00:10:23.870
uh,
the,
the networks still didn't get the,
uh,

160
00:10:24.080 --> 00:10:27.140
the relationship between notes.
So in music theory,

161
00:10:27.260 --> 00:10:32.120
so let's look at some music for a second.
In music,
in music,
we have scales,

162
00:10:32.121 --> 00:10:35.530
right?
It's music,
sheet music.
We have scales,
right?

163
00:10:35.540 --> 00:10:39.790
So notes are related to each other.
Each note is related to each other.

164
00:10:39.800 --> 00:10:44.750
So we start off with a note,
you know,
like look at this first note here.
Uh,
right.

165
00:10:44.751 --> 00:10:49.400
So this is a c and then the next note is a B.
So it's down one,
one half step.

166
00:10:49.520 --> 00:10:51.470
Okay.
Is this the short little lesson on music theory?

167
00:10:51.830 --> 00:10:53.870
These notes are in relation to each other.

168
00:10:53.871 --> 00:10:55.760
They're just ways of mapping out pitches.

169
00:10:55.761 --> 00:10:58.460
They're ways of mapping out sound waves.
Okay?

170
00:10:58.700 --> 00:11:03.050
And this is how we write out those differences in,
in sound waves,

171
00:11:03.051 --> 00:11:04.370
there's differences in pitches.

172
00:11:04.850 --> 00:11:08.390
What his model did was it couldn't recognize that there are,

173
00:11:08.640 --> 00:11:11.690
that one note is actually related to another note.
Okay.

174
00:11:11.691 --> 00:11:15.830
So this is actually one half step down.
One step up.
And He,
and this was,

175
00:11:16.310 --> 00:11:20.750
that means that if he inverted the nodes,
it would still predict the same thing.

176
00:11:20.960 --> 00:11:23.700
So,
you know,
if it was c then B it would,

177
00:11:23.750 --> 00:11:28.520
it would output something that was similar to as if we were to input B and c.

178
00:11:28.760 --> 00:11:30.230
So even if we inverted the notes,

179
00:11:30.231 --> 00:11:34.400
it would still still output the same prediction.
So to fix that,
he thought,
okay,

180
00:11:34.520 --> 00:11:38.240
how can I improve this model even more?
So then he said,
okay,

181
00:11:38.241 --> 00:11:40.940
so to improve this model even more,

182
00:11:42.230 --> 00:11:44.660
we're going to use a different architecture entirely.

183
00:11:44.960 --> 00:11:48.440
We're going to use an encoder decoder architecture.
Okay.

184
00:11:48.441 --> 00:11:52.820
So I haven't actually talked about this before and I will.
The next,
uh,
you know,

185
00:11:53.350 --> 00:11:57.160
uh,
two weekly videos are going to be talking about the encoder decoder

186
00:11:57.161 --> 00:12:00.630
architecture.
But to give a brief overview just for a second,
uh,

187
00:12:00.670 --> 00:12:04.450
and I'm sure most of you actually know what this is,
the encoder,

188
00:12:06.060 --> 00:12:09.720
the encoder decoder architecture it takes to recurrent networks.
Okay?

189
00:12:09.721 --> 00:12:13.710
So look at this green,
uh,
set of squares and this yellow set of squares.

190
00:12:13.830 --> 00:12:17.880
They are two different recurrent networks to different l s t m or current

191
00:12:17.881 --> 00:12:22.710
networks,
and each serves a different purpose.
One is the encoder.

192
00:12:22.740 --> 00:12:24.450
Okay?
The screen went is the encoder.

193
00:12:24.600 --> 00:12:29.430
It takes an input sequence and we feed it into this encoder.
Okay?
The second one,

194
00:12:29.431 --> 00:12:32.340
the yellow one is the decoder.
So you might think,
okay,

195
00:12:32.341 --> 00:12:37.140
so we just take that incoders output and we input that into the decoder.
No,

196
00:12:38.070 --> 00:12:43.070
we don't take the output because this is not what we do is we are trying trying

197
00:12:43.081 --> 00:12:44.760
to take the hidden state.
Okay?

198
00:12:44.761 --> 00:12:49.500
So if you look at this fourth square right here and it goes up and it says w we

199
00:12:49.501 --> 00:12:52.110
were trying to take that hidden state because we're not trying to take the

200
00:12:52.140 --> 00:12:55.560
output because it's not a classification.
We don't care about classifying,

201
00:12:55.770 --> 00:12:59.010
we just want that hidden state from the encoder.
We take the hidden state,

202
00:12:59.040 --> 00:13:03.270
which has a set of vectors that it learned feature vectors and we take that

203
00:13:03.330 --> 00:13:07.800
hidden state and we put it into our decoder.
Okay?
So we put it into our decoder.

204
00:13:08.820 --> 00:13:13.290
We feed that,
we feed the state itself into our decoder.
And then once it's,

205
00:13:13.440 --> 00:13:15.540
once we feed that input into our decoder,

206
00:13:15.660 --> 00:13:19.570
the decoder will then take that a feat that vector and then,
uh,

207
00:13:19.680 --> 00:13:24.270
decode it into,
uh,
uh,
a note.
Okay.
So the first,

208
00:13:24.460 --> 00:13:29.010
uh,
we take the first hidden state and we put it into the decoder and the decoder

209
00:13:29.011 --> 00:13:33.630
takes the hidden state and it outputs the next,
what would be the next,
uh,
uh,

210
00:13:33.660 --> 00:13:37.110
note in the sequence or the next a value in the sequence,
whatever it is.

211
00:13:37.350 --> 00:13:38.670
And so why do we do this?
Why,

212
00:13:38.671 --> 00:13:43.080
why have an encoder decoder architecture in general instead of having just one,

213
00:13:43.130 --> 00:13:45.150
uh,
uh,
recurrent network?
Well,

214
00:13:45.160 --> 00:13:48.870
this for the same reason that we have different networks for different tasks,

215
00:13:48.900 --> 00:13:53.460
like we have convolutional networks that do,
you know,
a certain task we have,
uh,

216
00:13:54.570 --> 00:13:58.260
that,
that look at that look for images.
We have recurrent nets for sequences.

217
00:13:58.261 --> 00:14:01.800
We have a feed forward nets for binary classification.

218
00:14:02.070 --> 00:14:05.640
And think about even our brain.
Our brain isn't just one neural,

219
00:14:06.420 --> 00:14:09.240
one type of neural network.
It's several types of neural networks.

220
00:14:09.420 --> 00:14:14.130
So having an encoder,
decoder architecture,
let's these networks specialize,

221
00:14:14.310 --> 00:14:19.310
and having them specialize increases the predictive capability of these networks

222
00:14:19.771 --> 00:14:23.430
because there are so specialized on a task.
Uh,

223
00:14:27.490 --> 00:14:32.170
okay.
So because they're so specialized,
uh,
they,

224
00:14:32.210 --> 00:14:35.890
they perform different tasks.
Well,
okay.
So that's,
that's the high level and it,

225
00:14:36.130 --> 00:14:38.050
and so he tried that and it worked even better.

226
00:14:38.051 --> 00:14:42.460
So that's the high level of what he did.
Okay.
And it was two recurrent networks.

227
00:14:42.461 --> 00:14:46.490
One was an encoder and one was a decoder.
And now we're going to,
uh,

228
00:14:46.510 --> 00:14:50.140
look at the code.
So there's actually a lot of code.
Oh,
so this,
so there's,

229
00:14:50.170 --> 00:14:55.070
there's a lot of code here and uh,
you know,
for different modules,
one for the,
uh,

230
00:14:55.490 --> 00:14:57.260
connector and for the keyboard.
But we'll,

231
00:14:57.261 --> 00:15:00.740
we're gonna do is we're going to just write out the meat of the code.
Okay.

232
00:15:00.741 --> 00:15:04.030
We're going to write out the model itself and then we'll talk a little bit about

233
00:15:04.100 --> 00:15:06.140
more about what else is happening here.
Okay.

234
00:15:06.141 --> 00:15:07.850
So let's start off by just writing out.

235
00:15:07.851 --> 00:15:11.330
The model itself will write out this in coder decoder model.

236
00:15:11.570 --> 00:15:14.030
So let's start off with importing our dependencies here.

237
00:15:14.210 --> 00:15:15.590
Let me make sure that this is visible.

238
00:15:15.990 --> 00:15:16.823
<v 0>Okay.</v>

239
00:15:18.410 --> 00:15:20.570
<v 1>And remember the code itself is in the,</v>

240
00:15:22.610 --> 00:15:26.630
the code itself is in the description.
So let's start off with our dependencies.

241
00:15:26.840 --> 00:15:27.673
Okay.

242
00:15:28.280 --> 00:15:32.680
We're going to start off with a few of his dependencies that he loaded here.
Uh,

243
00:15:32.770 --> 00:15:35.510
and I'll explain what each of them does,
so,

244
00:15:38.450 --> 00:15:38.610
<v 0>okay.</v>

245
00:15:38.610 --> 00:15:41.550
<v 1>Okay.
Um,
so,
okay,</v>

246
00:15:41.940 --> 00:15:45.620
so this module loader is going to do several things and we'll,

247
00:15:45.621 --> 00:15:49.440
we'll talk about the module loader when we get to it.
But this one,
this next,
uh,

248
00:15:49.460 --> 00:15:52.780
help our class is going to predict the next key,
the next key in the,

249
00:15:53.250 --> 00:15:58.050
in the sequence.
Okay.
So we have
hello from Hong Kong.

250
00:15:58.051 --> 00:15:59.130
Okay,
great.
Hi.

251
00:15:59.520 --> 00:16:03.420
And so we have our keyboard cell and we're going to use our keyboard cell.

252
00:16:03.540 --> 00:16:06.450
So what this does is it just predicts the next key and we'll talk about that

253
00:16:06.451 --> 00:16:08.920
more.
And we have one more help reclass uh,

254
00:16:08.940 --> 00:16:11.830
which is going to encapsulate the song data,
uh,

255
00:16:12.250 --> 00:16:16.770
and so that we can run things like,
so we can run,
get scale,

256
00:16:16.890 --> 00:16:21.120
we can run,
get relative.
No,
we can run a bunch of methods on our data.
Okay.

257
00:16:21.121 --> 00:16:23.460
So this is kind of a,
a data preprocessing.

258
00:16:23.461 --> 00:16:26.130
This is a data preprocessing step.

259
00:16:28.400 --> 00:16:32.240
<v 0>Okay,
thanks Jerry.
Okay.</v>

260
00:16:32.600 --> 00:16:34.860
<v 1>Oh,
so he is the original author of this code.
Okay.</v>

261
00:16:34.861 --> 00:16:39.660
So this is the best a code that I,
that I found.
Okay.

262
00:16:39.750 --> 00:16:44.610
So,
okay,
so let's,
let's start off with this.
So we're going to import deep music,

263
00:16:44.920 --> 00:16:49.920
uh,
using,
uh,
songs,
struct as music.
So let's,
let's write this out.
Okay.

264
00:16:49.921 --> 00:16:52.730
So these are our,
these are our only dependencies.
And there,
uh,

265
00:16:52.740 --> 00:16:54.360
there's not a paper.
There is a writeup.

266
00:16:54.570 --> 00:16:57.510
I'm going to link to that when we get to it.
Those are our dependencies.

267
00:16:57.660 --> 00:16:58.950
Let's write the code.

268
00:17:02.500 --> 00:17:07.260
Okay.
Do we have to pip these?
Uh,
yes,
we're going to have to pip them.
Uh,

269
00:17:08.140 --> 00:17:12.820
but I'm not the specific,
these are helper classes.
You don't have to pip these,

270
00:17:12.970 --> 00:17:17.260
but you do have to pip tensorflow and,
um,
the,

271
00:17:18.480 --> 00:17:22.480
uh,
concerts,
whatever the name is in the,

272
00:17:22.481 --> 00:17:25.350
read me of the code in the description.
Okay?
So let's do this.
Let's run,

273
00:17:25.351 --> 00:17:30.070
let's write out our code.
Where was I?
Okay.
Okay.
Here we go with this.

274
00:17:30.910 --> 00:17:35.200
Uh,
okay.
Oh,
there's a,
what else do we got?
Oh,
there's,

275
00:17:35.201 --> 00:17:36.790
there's actually two more dependencies.

276
00:17:36.910 --> 00:17:40.180
We have a num Pi cause we're going to generate random numbers,

277
00:17:40.181 --> 00:17:43.270
generate random numbers,
and there's of course tensorflow,

278
00:17:43.730 --> 00:17:47.290
tensorflow for flow.

279
00:17:49.260 --> 00:17:50.093
Okay?

280
00:17:50.220 --> 00:17:52.920
So the first thing we're going to do is we're going to write out the method for

281
00:17:52.921 --> 00:17:56.630
building the network.
Okay?
Uh,

282
00:17:56.640 --> 00:18:00.870
actually Dev Patel looks like me.
It's the other way around.
Okay?
So let's be,

283
00:18:01.410 --> 00:18:05.040
let's build our network and a,
let's build on that work.

284
00:18:05.041 --> 00:18:08.490
So the first thing we're gonna do is we're going to create our computation
graph.

285
00:18:08.790 --> 00:18:12.600
We're going to create the computation graph and it's going to encapsulate art

286
00:18:12.660 --> 00:18:17.040
tensorflow session and the graph initialization.
Okay?

287
00:18:17.490 --> 00:18:20.740
So we're going to initialize our graph.
And so anytime we use tensorflow,

288
00:18:20.790 --> 00:18:25.470
we create a session and a graph,
right?
These are just basic initialization steps.

289
00:18:25.471 --> 00:18:28.800
We do it every time.
And the great thing about these helper,

290
00:18:28.830 --> 00:18:32.340
these a helper libraries is this guy use the module loader to,

291
00:18:33.520 --> 00:18:34.353
uh,

292
00:18:34.440 --> 00:18:39.120
build this batch builders dot get module.

293
00:18:39.630 --> 00:18:41.370
Okay?
So what does this do?
This,

294
00:18:41.550 --> 00:18:46.550
this line right here basically just encapsulates that a graph creation and that,

295
00:18:46.620 --> 00:18:50.610
uh,
that uh,
session creation.
Okay,
so that's the first step.

296
00:18:50.880 --> 00:18:53.610
So we have our computation graph initialized.

297
00:18:53.880 --> 00:18:56.460
Now we want to start building our model.
Okay?

298
00:18:56.730 --> 00:19:01.730
So before we build our model or we need to create are placeholders,

299
00:19:01.920 --> 00:19:05.160
right?
These are our gateways for data to flow into the network.

300
00:19:05.370 --> 00:19:09.180
So we'll create our first place holder.
Okay.

301
00:19:09.360 --> 00:19:13.080
Our first place holder is going to be a for the inputs.

302
00:19:15.120 --> 00:19:18.300
Okay.
And these are going to be the notes themselves.

303
00:19:18.540 --> 00:19:23.020
This is for our notes,
okay?
This is far,
no two data.
We,

304
00:19:23.070 --> 00:19:25.800
we'll just call it no data,
okay?
So self dot inputs.

305
00:19:25.801 --> 00:19:30.360
And remember this is in Mitie format.
It's mitty format.
All right?

306
00:19:33.370 --> 00:19:37.420
So for self dot inputs,
these are the inputs to our graph,

307
00:19:37.870 --> 00:19:39.490
a TF dot placeholder.

308
00:19:42.720 --> 00:19:46.350
How can I get deep music?
I'm in math class.
It's okay,

309
00:19:46.380 --> 00:19:50.970
Bianca all look my,
my brain's not even working today.
It's all good.
Okay.

310
00:19:50.971 --> 00:19:55.740
Does it just take it in cerebrally take it in and it just subconsciously you're

311
00:19:55.741 --> 00:19:59.040
going to start to get this stuff.
This is,
this is not trivial stuff,

312
00:19:59.041 --> 00:20:02.700
but just follow along and we're gonna,
we're gonna do this.
Okay.
So,

313
00:20:03.920 --> 00:20:07.780
so now let's,
let's talk about this.
So we have our placeholder and a,

314
00:20:07.820 --> 00:20:10.090
the first thing we're going to do is we're going to say what,

315
00:20:10.091 --> 00:20:11.880
what type of data do we want to feed this?
Well,

316
00:20:11.881 --> 00:20:13.530
this is going to be an integer data,
right?

317
00:20:13.531 --> 00:20:17.710
This is numerical data because new mini me,
I can't believe I called it mid.

318
00:20:17.711 --> 00:20:20.430
I format on,
uh,

319
00:20:20.970 --> 00:20:25.920
the uh,
video on,
but thanks for correcting me and some guys directed me.

320
00:20:25.921 --> 00:20:29.040
So that was good.
I know one that I've never actually heard it said before,

321
00:20:29.041 --> 00:20:31.980
so it was like mid [inaudible] whatever.
So it's numerical data.

322
00:20:31.981 --> 00:20:36.150
So we'll use a float 32.
Okay.
We're using a float 32 for this.

323
00:20:36.420 --> 00:20:37.253
And um,

324
00:20:38.160 --> 00:20:41.730
the next thing we're going to do is we're going to say what,

325
00:20:41.731 --> 00:20:44.890
how much data do we want to use?
And so these are,
are,
uh,

326
00:20:45.030 --> 00:20:48.550
so these are placeholders for how much we want to give it to.
Like,

327
00:20:48.551 --> 00:20:51.220
what is the size,
like how many the batch,

328
00:20:51.250 --> 00:20:55.330
like how many batches are like if we have a basket of uh,
data,

329
00:20:55.420 --> 00:20:57.070
how much do we take out at a time?

330
00:20:57.071 --> 00:20:58.930
How much do we take out at a time and feed into our model?

331
00:20:58.931 --> 00:21:02.590
Those are our batches and we'll define this with our batch size.
Okay.

332
00:21:02.591 --> 00:21:06.240
Then which is going to be one of the arguments.
Uh,
but we,

333
00:21:06.290 --> 00:21:10.130
we are not going to talk about that right now.
Right.

334
00:21:12.250 --> 00:21:16.720
I'm glad you liked that Brandon.
Okay.
So,
okay,
where were we?
So that's our,

335
00:21:16.721 --> 00:21:19.960
that's how much data and then we have,
uh,
we've,

336
00:21:19.961 --> 00:21:21.130
we've got one more thing actually.

337
00:21:21.131 --> 00:21:24.360
We've got the input dimension and then we've got the name will,

338
00:21:24.370 --> 00:21:27.580
what are we going to call it?
We're going to call this input.

339
00:21:27.790 --> 00:21:31.870
We're going to call it input and clerk tensorflow as TF.
Thank you.

340
00:21:32.230 --> 00:21:35.920
Aditya exactly.
And the input and,
okay.

341
00:21:35.950 --> 00:21:40.540
So that's it for our placeholder input.
And so now that's,
so that's our no data.

342
00:21:40.541 --> 00:21:44.770
Now we're going to define our targets.
Okay.
Yes.
Okay.

343
00:21:44.800 --> 00:21:49.540
Now we're going to define our targets.
What are our targets?
They are the classes.

344
00:21:49.541 --> 00:21:54.490
Okay?
So these are,
um,
these are,
uh,

345
00:21:55.450 --> 00:21:57.340
whether or not a key was pressed.

346
00:21:57.370 --> 00:22:02.370
So this is a binary classification par 88 key binary classification problem.

347
00:22:04.720 --> 00:22:09.160
Okay?
So each of,
and this is not just,
this is not inherent in the mini data.

348
00:22:09.160 --> 00:22:13.270
This is just what we are feeding it.
We're saying there 88 keys on a keyboard,

349
00:22:13.271 --> 00:22:17.260
right?
We're looking at piano music and is that key pressed or not?

350
00:22:17.410 --> 00:22:21.490
If it's pressed,
it's a one.
If it's not pressed is a zero.
Okay?

351
00:22:21.640 --> 00:22:25.560
So that's what we're going to define for our target values.
So we'll say,
uh,

352
00:22:25.600 --> 00:22:28.330
with TF dot.
Oh,
it's not tipped on name.
Actually.

353
00:22:28.331 --> 00:22:33.331
It's tf.name scope and named scoping is a way for us to encapsulate some uh,

354
00:22:34.490 --> 00:22:39.400
uh,
TF object and give it its own name so we can reference it later on.

355
00:22:40.420 --> 00:22:44.890
Spell check on song struck.
Thank you.
Struct as music.

356
00:22:45.070 --> 00:22:47.710
Okay.
So with tf.name scope,

357
00:22:47.740 --> 00:22:50.990
we're going to say this is going to be our placeholder targets.

358
00:22:51.000 --> 00:22:54.710
I remember targets are for the,
uh,
the labels.
This,

359
00:22:54.760 --> 00:22:59.080
we're going to consider this a supervised classification problem using an

360
00:22:59.081 --> 00:23:02.080
encoder decoder architecture.
So,

361
00:23:03.910 --> 00:23:07.330
so let's go ahead and talk about the targets that we're going to use for this

362
00:23:07.331 --> 00:23:09.670
one.
So for this one,
we're going to say,
okay,

363
00:23:09.940 --> 00:23:14.370
we'll use a TFL placeholder like before,
uh,
and it's also gonna be an event.
Uh,

364
00:23:14.560 --> 00:23:14.751
it,
well,

365
00:23:14.751 --> 00:23:18.180
it's not going to be a flow is going to be an in 32 and why we're using an into

366
00:23:18.190 --> 00:23:22.630
32 instead of a float 32 because it is he going to be either a one or a zero.

367
00:23:22.631 --> 00:23:26.110
So it's a less memory to use an event than it is to use a float.

368
00:23:26.380 --> 00:23:29.170
Also useful for tensor board.
Okay.

369
00:23:31.660 --> 00:23:33.220
Okay.
So,
um,

370
00:23:33.880 --> 00:23:38.680
where were we TF in 32 so you're right,
so it's going to be either zero or one.

371
00:23:39.010 --> 00:23:43.420
And so what else do we got here?
We've got TF 32 we have to size.
So self dot.

372
00:23:43.630 --> 00:23:47.330
A batch size.
Okay.
And that's the size of our,

373
00:23:47.870 --> 00:23:50.840
and uh,
input at this actually goes over here.

374
00:23:51.730 --> 00:23:52.563
<v 0>Okay.</v>

375
00:23:54.190 --> 00:23:56.800
<v 1>Okay.
So what else we got here?
We've got the target.</v>

376
00:23:56.801 --> 00:24:00.700
That's the name of this thing,
right?
The name is the target.

377
00:24:02.440 --> 00:24:06.160
Oh my God,
there is so much naming happening here.
Yes sir.

378
00:24:06.161 --> 00:24:09.070
Re equals sign.
Thanks Nick.
Where's the equal sign?

379
00:24:09.520 --> 00:24:14.200
There's an equal sign somewhere.
Isn't there?
Uh,
whatever.
Okay.
Okay.

380
00:24:14.201 --> 00:24:18.490
So uh,
yeah.
All right,
great.
So that's that and uh,

381
00:24:20.290 --> 00:24:21.123
<v 0>okay.</v>

382
00:24:21.650 --> 00:24:26.640
<v 1>Okay.
So that's that.
And uh,
wait to a teeny,
are you serious?
They're not for bias.</v>

383
00:24:26.641 --> 00:24:29.540
Each in different languages,
different languages.

384
00:24:29.541 --> 00:24:34.220
It's different for for what it is a float values or are they take up more space?

385
00:24:34.221 --> 00:24:38.930
Generally self dot targets equals self dot targets.
Yes.
That is the one.

386
00:24:38.931 --> 00:24:43.220
Thank you nick.
That has an equal sign.
Okay.
So,
okay.
So guess what?

387
00:24:43.221 --> 00:24:46.610
We're going to use one more place holder because this is a recurrent network,

388
00:24:46.850 --> 00:24:48.530
we are not just feeding in our inputs.

389
00:24:48.590 --> 00:24:52.370
What else do we feed in to our recurrent network?

390
00:24:52.490 --> 00:24:55.490
We don't just feed in the input data when we're training,

391
00:24:55.491 --> 00:25:00.360
we also feed in the drum roll.
Please think about it.
Hidden state,

392
00:25:00.410 --> 00:25:01.640
our previous hidden state,

393
00:25:01.641 --> 00:25:05.120
we are feeding in both our input data and the previous hidden state.

394
00:25:05.150 --> 00:25:06.230
There are two things.

395
00:25:06.950 --> 00:25:10.580
So we're going to create one more place holder because this is a recurrent

396
00:25:10.730 --> 00:25:13.310
neural network and not a feed forward network.

397
00:25:13.490 --> 00:25:17.810
So we'll use the placeholder for uh,

398
00:25:17.870 --> 00:25:19.880
and they'll call it used preve.
Okay.

399
00:25:21.140 --> 00:25:24.620
You should definitely bet on AI when choosing a career because you will not,

400
00:25:24.680 --> 00:25:28.840
you will definitely get employed while,
uh,

401
00:25:28.940 --> 00:25:32.780
you will get employed.
So we'll say,
okay,
so we'll use the previous

402
00:25:34.460 --> 00:25:36.920
and uh,
I did import STF.

403
00:25:38.630 --> 00:25:39.463
<v 0>Okay.</v>

404
00:25:44.810 --> 00:25:48.500
<v 1>So I guess float and [inaudible] are both four bytes.
Uh,
so I was wrong,</v>

405
00:25:48.650 --> 00:25:52.550
but if you use a short,
it is two bytes.
Okay.
But don't quote me on that checkup.

406
00:25:52.880 --> 00:25:56.330
You know,
we,
we could argue about this later.
We'll see what the,
you know,

407
00:25:56.750 --> 00:26:00.020
post the link or something.
Anyway,
so,
so,
okay,
so the,

408
00:26:00.021 --> 00:26:04.120
for the placeholder for the placeholder value,
this is going to be um,

409
00:26:06.470 --> 00:26:10.880
a TF dot.
Boolean.
Should we use the previous value or not?
Yes or no.
Okay.

410
00:26:10.881 --> 00:26:14.510
And then um,
the name,
uh,

411
00:26:14.600 --> 00:26:18.710
we'll leave the size empty cause there's not really a size associated with the

412
00:26:18.711 --> 00:26:21.770
boolean.
And then the name is going to be use

413
00:26:23.840 --> 00:26:27.320
preve use previous,
right?
Yes.
Okay.

414
00:26:27.321 --> 00:26:31.130
So those are our three placeholders.
Hidden states.
Okay.

415
00:26:31.190 --> 00:26:35.270
Are hidden state or no data.
And then our target values.
These are our labels.

416
00:26:35.330 --> 00:26:38.840
Okay.
What else?
What else got the keys?

417
00:26:38.960 --> 00:26:42.890
We got people rapping in this chat.
So we have everything happening right now.

418
00:26:43.170 --> 00:26:46.380
It is chaos.
It was absolute chaos.
But that's what we like.

419
00:26:46.381 --> 00:26:51.210
We are agents of chaos in a good way.
Creative chaos,
constructive chaos.

420
00:26:51.211 --> 00:26:54.360
So the next step is to define our network.

421
00:26:54.870 --> 00:26:56.490
We are going to define our network.

422
00:26:56.491 --> 00:27:00.630
We have our place holders and we're ready to build the hell out of this model.

423
00:27:02.720 --> 00:27:04.550
I can barely program in front of another person.

424
00:27:04.551 --> 00:27:08.450
You're a brave man for livestreaming.
Saroj shotgun.
Thank you.
And you can,

425
00:27:08.451 --> 00:27:12.260
it just takes practice.
It just takes practice.
Like all other things.

426
00:27:12.560 --> 00:27:15.380
We have to train our minds to do these things.
Okay.

427
00:27:16.130 --> 00:27:21.080
Self dot loop processing module loader.
What the hell am I writing?

428
00:27:21.290 --> 00:27:23.990
What am I writing this?
Anyone know?
I don't even know what I'm writing right now.

429
00:27:24.350 --> 00:27:27.500
You know,
it's just,
it's crazy.
It's crazy.
No,
I do know what I'm writing.

430
00:27:27.770 --> 00:27:30.890
What this is,
is it's a part of the,
uh,

431
00:27:31.640 --> 00:27:34.580
module loader class.
And so what this is,

432
00:27:34.760 --> 00:27:38.090
is we are going to create a loop manually.
Okay?

433
00:27:38.091 --> 00:27:41.830
Because we are not using care Ross,
we are using raw tensorflow.

434
00:27:42.470 --> 00:27:44.330
Could you please explain what is going on?

435
00:27:46.340 --> 00:27:49.250
So that is what I am trying to do.

436
00:27:49.610 --> 00:27:52.430
So what I'm gonna do is I'm going to continue doing what I'm doing and maybe

437
00:27:52.431 --> 00:27:56.870
I'll even try to explain harder.
So I'll turn up my experimentation.
So,
okay.

438
00:27:57.890 --> 00:28:01.340
Okay.
So what we're doing is we're going to write a loop function and the loop

439
00:28:01.341 --> 00:28:05.510
function is used to connect one of the outputs of the network to the next input.

440
00:28:05.530 --> 00:28:06.500
It is the actual loop.

441
00:28:06.501 --> 00:28:11.501
So it is that code that takes that output and feeds it back in to the,

442
00:28:13.360 --> 00:28:13.720
<v 3>okay.</v>

443
00:28:13.720 --> 00:28:17.950
<v 1>Uh,
the,
uh,
input placeholder.
So where are we going to manually create this loop?</v>

444
00:28:17.980 --> 00:28:22.270
Okay.
We're going to manually create this loop.
This is too deep for me.

445
00:28:22.271 --> 00:28:26.440
Why is your hair black?
We have some amazing comments in this,
so,
okay.

446
00:28:26.650 --> 00:28:30.610
Very entertaining.
So,
so you get the code,
you get the entertainment.
This is,

447
00:28:30.611 --> 00:28:35.050
this is what it's all about.
These are why these live streams are awesome.
Okay?

448
00:28:35.170 --> 00:28:38.020
So let's see,
where are we going to feed this?

449
00:28:38.021 --> 00:28:42.430
And we're going to feed in the previous value and we're going to feed in the

450
00:28:42.490 --> 00:28:46.870
current value.
Okay?
The,
we're taking over the two values for our loop.
Okay?

451
00:28:46.900 --> 00:28:51.790
For our loop.
What is after self,
right?
Okay.
Ours,
uh,
we don't,
uh,

452
00:28:51.850 --> 00:28:55.300
so that's,
that's actually for the command line.
So we could define,
um,

453
00:28:56.640 --> 00:28:59.530
uh,
if we want it to be a loop or not,
but that's not the case here.

454
00:28:59.531 --> 00:29:01.880
So let's not even worry about that.
Uh,

455
00:29:01.900 --> 00:29:04.510
we're not doing it from the command line or the command line has,

456
00:29:04.840 --> 00:29:06.490
we already have default values for that.

457
00:29:06.520 --> 00:29:10.240
So now what we're going to do is for our loop,
we're going to say,
well,

458
00:29:10.270 --> 00:29:15.270
we'll take our next input and we're going to use this loop process processing

459
00:29:15.371 --> 00:29:18.370
method to taking the previous value.

460
00:29:18.371 --> 00:29:23.370
So it will take in the previous value to predict the next input.
Okay.
And

461
00:29:25.660 --> 00:29:29.290
Hayes,
Raj,
what age did you start coding?
When I was 14,

462
00:29:29.291 --> 00:29:33.910
I was this script kitty who would try to do whatever I could.

463
00:29:33.911 --> 00:29:38.710
And what was cool at the time was to deface like forums and not deface,
but like,

464
00:29:39.100 --> 00:29:40.960
you know,
deface.
It was so stupid.

465
00:29:41.080 --> 00:29:44.440
I was 14 also hacking halo two and stuff like that.
So that was,

466
00:29:44.441 --> 00:29:45.550
that was when I first started.

467
00:29:45.551 --> 00:29:50.551
But when I really started coding was like when I was I think a 20,

468
00:29:52.240 --> 00:29:56.350
so six years ago.
So,
you know,
we all,

469
00:29:56.530 --> 00:29:58.160
we all have to find our way.
Uh,

470
00:29:58.240 --> 00:30:00.670
but now I'm full white hat I'm brown had actually,
okay.

471
00:30:00.671 --> 00:30:05.530
So let's return some.
So,

472
00:30:05.590 --> 00:30:07.780
so we're going to use the TF conditional,

473
00:30:09.060 --> 00:30:09.893
<v 0>uh,</v>

474
00:30:11.020 --> 00:30:14.800
<v 1>object of tensorflow.
And what this is,
is it's a conditional statement.</v>

475
00:30:14.801 --> 00:30:18.640
So it's going to either return one or the other up of the parameters that we

476
00:30:18.641 --> 00:30:22.600
give it.
And I'm going to talk about what,
we're going to feed this in a second.

477
00:30:22.630 --> 00:30:23.463
So,

478
00:30:25.350 --> 00:30:26.183
<v 0>so</v>

479
00:30:27.430 --> 00:30:30.280
<v 1>I'm not going to get distracted.
Okay.
I'm not going to get distracted.
So</v>

480
00:30:31.960 --> 00:30:32.830
we're going to feed,
let me just,

481
00:30:32.890 --> 00:30:37.630
let me just type this up and I'm going to explain exactly what I'm typing up.

482
00:30:37.760 --> 00:30:42.310
If you have a land of value here and we have one more input and that is how,

483
00:30:42.370 --> 00:30:47.320
okay.
And then we have
one more.

484
00:30:47.620 --> 00:30:51.310
Okay.
So,
okay,
so on,

485
00:30:51.340 --> 00:30:55.300
so on training we're forcing the correct input and on testing we use the

486
00:30:55.301 --> 00:31:00.170
previous output as the next input.
So this is a conditional depending on,
uh,

487
00:31:02.280 --> 00:31:06.750
uh,
on training versus testing.
Okay.
Training versus testing.

488
00:31:07.140 --> 00:31:12.030
Okay.
Uh,
training versus testing.
So that's what,
that's what this is.

489
00:31:12.510 --> 00:31:15.720
Uh,
it's,
it's,
it's a differentiate between training versus testing,

490
00:31:15.721 --> 00:31:18.450
but we're just focused on training so we don't even care about it being

491
00:31:18.451 --> 00:31:18.931
conditional.

492
00:31:18.931 --> 00:31:23.490
Right now we're just focused on training lambda function.

493
00:31:23.890 --> 00:31:28.550
Lambda parameter.
Hi.
So many comments.

494
00:31:28.551 --> 00:31:32.600
I wish I could just respond to them all.
But time is of the essence,
isn't it?

495
00:31:32.630 --> 00:31:36.740
Backpropagation through time,
back propagate to update weights.
Okay.

496
00:31:36.741 --> 00:31:40.850
So now we're going to do the sequence of secret.
Now this is the cool part.

497
00:31:40.851 --> 00:31:44.630
This is the actual model part.
We have defined our placeholder values.

498
00:31:44.631 --> 00:31:47.330
We have defined our loop function.
Okay.

499
00:31:48.770 --> 00:31:49.080
<v 0>Okay.</v>

500
00:31:49.080 --> 00:31:54.080
<v 1>And and so now we're going to build our sequence to sequence model.</v>

501
00:31:54.091 --> 00:31:58.920
So we're going to build our sequence to sequence model.

502
00:31:59.160 --> 00:32:02.320
This is it,
this is it guys.
This is it for the sequence to sequence model.

503
00:32:02.321 --> 00:32:06.720
So tensorflow,
fortunately for us has a sequence,

504
00:32:06.721 --> 00:32:11.040
a sequence function built in.
Okay.
It has a sequence,

505
00:32:11.041 --> 00:32:15.210
a sequence function built in and we're going to talk about what it is doing,

506
00:32:15.300 --> 00:32:19.200
but let's write out what it's going to output first.
It's going to output,

507
00:32:19.740 --> 00:32:23.760
it's going to output our prediction value.
What is our next note?

508
00:32:23.970 --> 00:32:27.090
And we're going to minimize the loss here and I'll talk about what the

509
00:32:27.091 --> 00:32:28.400
minimalization,
okay.

510
00:32:29.620 --> 00:32:30.290
<v 0>Okay.</v>

511
00:32:30.290 --> 00:32:34.610
<v 1>Okay.
Exactly,
exactly.
Questions happened at the end of the live session.</v>

512
00:32:34.611 --> 00:32:37.310
Remember all your questions.
I'm going to answer them at the end of this session.

513
00:32:37.410 --> 00:32:41.240
We are focused right now,
so this is going to output our,

514
00:32:41.390 --> 00:32:46.130
our predicted note and our final state.
Okay.
And the final state is that final,

515
00:32:46.280 --> 00:32:50.150
a hidden layer for our decoder net network.
And so,

516
00:32:53.220 --> 00:32:56.880
okay,
so let's build this thing.

517
00:32:56.881 --> 00:32:59.640
So we've got our sequence to sequence model.
Okay.

518
00:33:00.540 --> 00:33:03.120
And we're going to build our decoder and get,
okay,

519
00:33:03.121 --> 00:33:06.420
so we're going to build our decoder and I'll talk about our encoder in a second.

520
00:33:06.421 --> 00:33:11.421
So the decoder is going to take a series of a perimeter inputs.

521
00:33:11.580 --> 00:33:16.380
So the input is going to be,
are the inputs that that came out of our encoder.

522
00:33:16.410 --> 00:33:20.580
Okay?
The,
the inputs that came out of our encoder that,
um,

523
00:33:23.610 --> 00:33:24.660
our encoder and

524
00:33:26.370 --> 00:33:29.880
the initial state is none because it's defined inside of the keyboard sell.

525
00:33:29.881 --> 00:33:33.810
Let me,
because defined in keyboards.

526
00:33:33.811 --> 00:33:36.300
So I'm going to explain this in a second.
Let me just talk this out.

527
00:33:36.840 --> 00:33:39.810
And a cell is going to be keyboard cell.

528
00:33:39.990 --> 00:33:41.790
It's going to be a little confusing as I type this.

529
00:33:41.791 --> 00:33:46.590
I've got one more line to type and then we're doing this.

530
00:33:47.040 --> 00:33:50.730
We're doing this live as bill O'Reilly said,

531
00:33:51.030 --> 00:33:55.950
we're doing it live loop are n n bloop.
RNN.

532
00:33:56.430 --> 00:33:59.010
That's what's up.
Okay.

533
00:33:59.340 --> 00:34:02.160
That's our sequence to sequence model right there.
Okay?

534
00:34:02.370 --> 00:34:05.220
We didn't define a loss function,
we didn't define our optimizer.

535
00:34:05.250 --> 00:34:08.460
But with this one line we said tensorflow,

536
00:34:08.550 --> 00:34:12.330
we want to use your built in sequences sequenced model that is to recurrent

537
00:34:12.331 --> 00:34:15.660
networks that we could just write out every layer manually.

538
00:34:15.810 --> 00:34:20.250
We could write out the weights,
we could write out the um,
uh,
each of the,

539
00:34:20.600 --> 00:34:23.250
you know,
computations happening at each layer.

540
00:34:23.400 --> 00:34:25.710
We could write out the activation functions,

541
00:34:25.800 --> 00:34:29.460
but tensorflow has itself encapsulated this for us.

542
00:34:29.640 --> 00:34:31.320
And so the reason we are giving,

543
00:34:31.350 --> 00:34:36.350
we're defining a decoder here is because our encoder was already defined and our

544
00:34:36.691 --> 00:34:40.320
encoder was defined in our helper class and also by this loop,

545
00:34:40.560 --> 00:34:42.300
a function that we just wrote.

546
00:34:42.450 --> 00:34:46.740
So the input to the decoder is going to be the output from the encoder.
Okay?

547
00:34:48.450 --> 00:34:53.250
What is even happening?
Okay?
Okay.

548
00:34:53.251 --> 00:34:57.480
So guys,
I also have a nother,
uh,

549
00:34:57.570 --> 00:34:59.340
code sample that I'm going to show you at the end of this.

550
00:34:59.341 --> 00:35:02.680
That's going to make even more sense than this.
Okay?
But it doesn't compile that,

551
00:35:02.700 --> 00:35:05.770
that's a problem because some dependency issue with the mid,

552
00:35:06.450 --> 00:35:09.920
I almost said it again,
the mini,
the mini,
uh,

553
00:35:10.950 --> 00:35:13.170
a python module.
Okay?

554
00:35:13.710 --> 00:35:17.040
I'm going to show you another piece of code out this piece of code for this.

555
00:35:17.460 --> 00:35:20.370
Okay.
Also,
I have videos on music generation.

556
00:35:20.371 --> 00:35:25.320
I have to and build an AI composer and a generate music intenser flow.
Okay.

557
00:35:25.321 --> 00:35:28.650
So,
okay.

558
00:35:31.630 --> 00:35:32.463
<v 0>Yeah,</v>

559
00:35:33.960 --> 00:35:37.020
<v 1>I don't understand a single line beginner.
That's okay.
That's okay.</v>

560
00:35:37.260 --> 00:35:40.620
Eventually it's all gonna make sense.
It's odd time for it.

561
00:35:41.650 --> 00:35:44.410
Now we're going to do the training steps,
so,
so,

562
00:35:45.110 --> 00:35:47.450
<v 0>okay,
hold on.</v>

563
00:35:51.290 --> 00:35:52.123
Okay.

564
00:35:56.220 --> 00:36:00.300
<v 1>Okay.
So now the training step.
So for a training step,
we're going to say,</v>

565
00:36:01.410 --> 00:36:02.243
<v 0>mmm,</v>

566
00:36:02.500 --> 00:36:04.450
<v 1>let's define our loss function training step.</v>

567
00:36:04.840 --> 00:36:08.530
So let's define our loss function.

568
00:36:09.100 --> 00:36:13.360
Let's define this thing.
So,
um,
for our loss function,

569
00:36:13.420 --> 00:36:17.670
we're going to say,
let's define our loss function.
Well,

570
00:36:17.671 --> 00:36:19.220
we could just manually write out,

571
00:36:19.280 --> 00:36:23.360
so first let's talk about what loss function we want to use here.
So,

572
00:36:23.390 --> 00:36:27.470
because notes can,
can occur at the same time,
right?
Let's,

573
00:36:27.471 --> 00:36:28.610
let's listen to this for a second.

574
00:36:34.020 --> 00:36:38.720
So you notice that some notes are pressed at the same time,
right?
So it's not a,

575
00:36:38.750 --> 00:36:41.390
it's not just a a classification problem.

576
00:36:41.391 --> 00:36:44.150
It's a multi-class class classification problem.

577
00:36:44.300 --> 00:36:48.110
We want to output multiple notes at if necessary.
Right?

578
00:36:48.350 --> 00:36:52.310
And so because we,
it's say multi-class classification problem,

579
00:36:52.520 --> 00:36:55.280
we're going to use it cross entropy as our loss function.

580
00:36:55.400 --> 00:36:59.210
So when we have multi-class classification appreciated Nikki,

581
00:36:59.510 --> 00:37:03.140
channel 13,
when we have a multi-class classification problem,

582
00:37:03.620 --> 00:37:08.090
we then use a cross entropy.
Okay.
If we don't then we were,

583
00:37:08.120 --> 00:37:11.790
then we're going to use
a sigmoid,
you know,

584
00:37:11.800 --> 00:37:16.640
a softmax function but because a B but because

585
00:37:19.150 --> 00:37:22.570
steel melts at 300 degrees,
what do you guys even talking about?
Okay,

586
00:37:22.930 --> 00:37:26.800
we're talking about music guys.
Okay.
So,
uh,
where were we?

587
00:37:27.820 --> 00:37:29.530
Because it's multi-class classification.

588
00:37:29.531 --> 00:37:33.490
We're going to use cross entropy as our loss function and cross entropy

589
00:37:33.520 --> 00:37:38.520
basically measures the difference between two probability distributions and the

590
00:37:39.250 --> 00:37:42.730
two different,
or it can even be multiple distributions.

591
00:37:42.731 --> 00:37:46.300
And these distributions are for each of our predicted values.

592
00:37:46.330 --> 00:37:50.620
Each of our predicted notes and the reason we're using cross entropy is because,

593
00:37:51.060 --> 00:37:52.670
uh,
I'm can have,

594
00:37:52.750 --> 00:37:57.750
we can have multiple notes and we want to predict the difference between a

595
00:37:57.850 --> 00:38:02.410
different notes.
Uh,
I appreciate it,
VJ.
Okay.

596
00:38:02.411 --> 00:38:07.360
Cross entropy.
Cross entropy.
And would it be easier if it was monophonic yes,

597
00:38:07.390 --> 00:38:10.900
yes,
it would be easier.
Absolutely.
Um,
but

598
00:38:12.650 --> 00:38:17.180
we don't like easy things do we?
So,
uh,
let's write out this function.

599
00:38:17.181 --> 00:38:20.270
So guess what tensor flows like.
Okay.
So Raj,

600
00:38:20.290 --> 00:38:23.690
I know you want to use sequence a sequence.
So what I did for you Raj,

601
00:38:23.960 --> 00:38:28.130
is I said I'm going to build in a sequence loss function into the sequence,

602
00:38:28.131 --> 00:38:32.870
the sequence module of TensorFlow's neural net class and

603
00:38:34.910 --> 00:38:36.880
hold to all your questions.
I'm going to answer it after this.

604
00:38:37.270 --> 00:38:41.650
So we have our outputs.
Okay?
So,
so it's saying like,

605
00:38:42.370 --> 00:38:45.970
I know you're using sequence a sequence and let's define a loss function for

606
00:38:45.971 --> 00:38:48.370
that built in sequence,
the sequence model you're using.

607
00:38:48.371 --> 00:38:51.500
So if you're going to use a module of tensorflow,
lite sequence sequences,

608
00:38:51.501 --> 00:38:55.710
sequence,
use it for everything,
use it for your loss,
use it for your,
uh,

609
00:38:56.530 --> 00:39:01.360
optimizer and all that stuff.
Difference in what,
okay?

610
00:39:03.750 --> 00:39:04.480
<v 0>Okay.</v>

611
00:39:04.480 --> 00:39:06.850
<v 1>Okay,
let me,
let me just check on what's going on here.</v>

612
00:39:06.851 --> 00:39:10.570
We have 366 people live now,
so we have even more people.

613
00:39:10.720 --> 00:39:14.680
So this is a good sign.
This is a good sign.
Hi everybody across the world.

614
00:39:14.860 --> 00:39:16.900
You are awesome.
Thanks for watching.
So,

615
00:39:17.720 --> 00:39:18.553
<v 0>uh,</v>

616
00:39:21.060 --> 00:39:22.800
<v 1>we have our outputs and we have our targets.</v>

617
00:39:22.830 --> 00:39:27.720
So what it's going to do is it's going to,
we have a sequence of notes,

618
00:39:27.780 --> 00:39:31.170
we have a sequence of notes and it's going to sample a sequence of notes,
right?

619
00:39:31.210 --> 00:39:33.300
So it's going to,
so we have a huge sequence of notes,
right?

620
00:39:33.480 --> 00:39:35.490
So we're going to give it like in batches,

621
00:39:35.491 --> 00:39:39.000
say the first sequence right here and it knows what the next notes are going to

622
00:39:39.001 --> 00:39:41.160
be.
It already knows what the next notes are going to be.

623
00:39:41.280 --> 00:39:44.340
So it's going to predict,
given a note what the next note is.

624
00:39:44.880 --> 00:39:48.900
And then the difference in value between the predicted no.

625
00:39:49.020 --> 00:39:52.050
And the actual note is what we want to minimize with our loss function.

626
00:39:52.410 --> 00:39:56.350
The loss function will minimize the difference between these two,
uh,

627
00:39:56.460 --> 00:39:58.890
notes and minimize.
By minimize,

628
00:39:58.891 --> 00:40:02.130
I mean we represent these notes as numerical values.

629
00:40:02.160 --> 00:40:05.730
So a c would be something like,
you know,
0.2,
three,
whatever.

630
00:40:05.940 --> 00:40:08.940
And the actual note is B,
which would be like 0.3,
seven.

631
00:40:09.150 --> 00:40:12.690
So we want to minimize that,
the difference between those predicted output.

632
00:40:12.691 --> 00:40:17.370
So the idea is that we have a huge sequence of notes and starting from the

633
00:40:17.371 --> 00:40:21.270
beginning in batches,
we feed it in batches of notes,
sequences,

634
00:40:21.450 --> 00:40:24.800
and we're predicting what's going to come afterwards and over time we're going

635
00:40:24.801 --> 00:40:29.070
to minimize this loss so that the predicted next note and the actual next note

636
00:40:29.200 --> 00:40:33.480
or going to be very similar then when we feed it a,
when we,

637
00:40:33.540 --> 00:40:35.820
when we tried to generate novel sequences,

638
00:40:35.821 --> 00:40:40.740
is going to generate novel notes in the style that it already,
uh,

639
00:40:40.920 --> 00:40:44.340
it already generated.
Okay.
So we have our outputs,

640
00:40:44.341 --> 00:40:48.330
we have our targets and now we're going to generate,
we're going to not generate,

641
00:40:48.690 --> 00:40:51.480
we're going to,
uh,

642
00:40:51.481 --> 00:40:56.340
define our loss function has cross entropy.
Okay.

643
00:40:56.550 --> 00:41:00.730
It's cross entropy.
And so it's saying,
so the sequence sequence sequence,

644
00:41:00.780 --> 00:41:05.780
loss of function is asking for a function to use with both of our,

645
00:41:07.260 --> 00:41:11.700
um,
with both of our sequences.
That's why it's part of our sequence,

646
00:41:11.701 --> 00:41:15.120
the sequence module.
Okay.
This is the same loss function.

647
00:41:18.930 --> 00:41:20.310
Wow.
Gregory,
I really appreciate that.

648
00:41:20.970 --> 00:41:23.820
It's the same loss function for both of them.

649
00:41:24.450 --> 00:41:29.190
I am going to use cross entropy for this with logics.

650
00:41:29.500 --> 00:41:33.500
Okay.
Such a long unnecessarily long name.

651
00:41:33.530 --> 00:41:37.910
I mean with lodge it's what even is with logics.
I I,
I mean I,

652
00:41:37.930 --> 00:41:40.460
I I looked this up before,
but uh,

653
00:41:42.110 --> 00:41:45.560
now you guys can see like how I searched for answers.
I just like Google it.

654
00:41:45.710 --> 00:41:47.570
I'm having,
I'm sure all of you guys do as well.

655
00:41:47.571 --> 00:41:52.460
But stack overflow is a great largest simply means that the function operates on

656
00:41:52.461 --> 00:41:54.950
the unscaled output of earlier layers.

657
00:41:55.730 --> 00:41:58.730
And that the relative scale to understand the units is linear.

658
00:41:59.750 --> 00:42:00.860
It means in particular the,

659
00:42:00.861 --> 00:42:04.640
some of the inputs may not equal one and the values are not probabilities.

660
00:42:05.990 --> 00:42:08.600
Oh,
okay.
So it's,
it means that the,
some of the inputs aren't,

661
00:42:08.601 --> 00:42:12.430
don't always have to equal one,
uh,
because it don't because it um,

662
00:42:14.830 --> 00:42:18.940
and the values because the,
because the values are not probabilities.
Okay.
So,
uh,

663
00:42:19.070 --> 00:42:23.860
that just means that our mitty notes can be like 1520.
Right?
So anyway,

664
00:42:23.861 --> 00:42:28.330
so cross entropy,
that's the,
the,
the name of the game.
Cross entropy.
Okay.

665
00:42:28.510 --> 00:42:30.790
So now we have two more.

666
00:42:32.960 --> 00:42:35.060
We have two more.

667
00:42:36.770 --> 00:42:37.290
<v 0>Okay.</v>

668
00:42:37.290 --> 00:42:42.270
<v 1>Uh,
functions here.
Time steps equals true.
We have,
what else do we got?</v>

669
00:42:42.271 --> 00:42:45.810
We have average across batch.

670
00:42:51.530 --> 00:42:55.340
Okay.
How long has this session can be?
It's going to be a 15 more minutes Max.

671
00:42:55.370 --> 00:42:59.240
Okay.
We are trying to cap it at an hour and we're almost done.
All right.

672
00:42:59.241 --> 00:43:00.650
We'll have a five minute Q and a at the end,

673
00:43:00.651 --> 00:43:02.870
but of course we're going to sample this and we're going to run the training.

674
00:43:03.200 --> 00:43:05.180
We're going to do it all.
It's going to be amazing.
Okay.

675
00:43:05.330 --> 00:43:09.440
I know I'm a little cut off.
It's all good.
This is our first stream.

676
00:43:10.010 --> 00:43:14.840
Okay.
We like this,
this high quality stuff.
Okay.
Uh,

677
00:43:14.900 --> 00:43:15.733
and

678
00:43:18.650 --> 00:43:23.150
so average across batch equals true.
Now these are just,

679
00:43:23.300 --> 00:43:26.420
um,
okay,

680
00:43:26.450 --> 00:43:30.560
so we want to take the average value across the batches.
Oh,
wait,

681
00:43:30.650 --> 00:43:33.290
average across time steps.
In fact,

682
00:43:33.320 --> 00:43:37.100
I honestly don't even think that we should even be,

683
00:43:37.460 --> 00:43:42.040
we should even need these two
parameters.
Uh,

684
00:43:42.080 --> 00:43:42.913
because

685
00:43:43.960 --> 00:43:44.590
<v 0>yeah,</v>

686
00:43:44.590 --> 00:43:45.810
<v 1>because uh,</v>

687
00:43:54.220 --> 00:43:57.610
we should just by default a want to use the average values.
Unless we're really,

688
00:43:57.611 --> 00:44:00.010
really fine tuning this thing,
we,
we don't need to do it.

689
00:44:00.011 --> 00:44:03.940
So that's our loss function sequence.
A sequence,
a cross entropy.

690
00:44:04.000 --> 00:44:05.590
Now we're going to minimize it and we're going to minimize,

691
00:44:05.591 --> 00:44:07.060
and this is our last line,

692
00:44:07.690 --> 00:44:11.230
I'm going to go over everything I just wrote right at the end.
So if you stayed,

693
00:44:11.260 --> 00:44:14.470
if you stuck through this long,
you are a hero or heroine.

694
00:44:14.920 --> 00:44:15.970
If you start through this long,

695
00:44:16.000 --> 00:44:20.890
you are amazing because we are now at the last line of this relatively complex

696
00:44:20.891 --> 00:44:23.470
code.
Actually,
if you are looking at this stream,

697
00:44:23.740 --> 00:44:26.020
you are looking at the bleeding edge of computing.

698
00:44:26.050 --> 00:44:29.320
If you are even able to understand 10% of what I just wrote,

699
00:44:29.500 --> 00:44:32.940
you are like one of the top people.
Okay?
You have that top potential.

700
00:44:32.941 --> 00:44:37.260
First of all,
you have the interest in this.
Second of all,
you have the uh,

701
00:44:37.500 --> 00:44:40.950
that has died.
That alone is amazing.
That is so valuable.
Okay?

702
00:44:42.180 --> 00:44:46.020
Okay.
Did I write that twice?
Yes,
I did.
I wrote twice.
Thank you nick.

703
00:44:49.170 --> 00:44:53.000
Okay.
So now our,
okay,
so our last step,
our last step,

704
00:44:53.020 --> 00:44:57.000
initialize the optimizer,
minimize the loss,

705
00:44:57.240 --> 00:44:59.970
minimize the loss.
Okay,
so we're going to minimize this loss.

706
00:44:59.971 --> 00:45:03.360
So to minimize this loss,
we're going to use Adam.

707
00:45:03.750 --> 00:45:06.930
Adam is our optimizer stepping.
I'll talk about why we're using Adam.

708
00:45:07.140 --> 00:45:10.500
But first let me write out a little bit of uh,

709
00:45:11.310 --> 00:45:16.110
since uh,
magic numbers.
Okay.
These are our hyper parameters and honestly,

710
00:45:16.410 --> 00:45:19.380
we shouldn't do it here.
We should have defined this earlier.

711
00:45:19.590 --> 00:45:22.740
This is bad practice.
I'm just saying it right,
officer up front.

712
00:45:22.741 --> 00:45:26.880
This is bad practice to write these out here,
but that's what we're going to do.

713
00:45:27.210 --> 00:45:29.430
Um,
just for these three lines,
okay.

714
00:45:29.670 --> 00:45:34.670
And I'll talk about what this is in a second.

715
00:45:36.060 --> 00:45:40.080
All right,
so these are our values.
Epsilon,
what else do we got?

716
00:45:40.081 --> 00:45:44.580
We have one e minus oh eight.
Okay.

717
00:45:45.450 --> 00:45:48.690
Boom.
That's it.
That's our,
that's our item.
That's our Adam optimizer.

718
00:45:48.691 --> 00:45:51.810
And so let's talk about Adam.
Okay?
But before we talk about Adam,

719
00:45:51.811 --> 00:45:54.330
let me write up the,
literally the last line of code.

720
00:45:54.750 --> 00:45:59.070
This is the last line of code,
lovey to rile.
This is the last line of code.

721
00:45:59.340 --> 00:46:02.640
So our last line of code is to just say opt dot.

722
00:46:02.670 --> 00:46:05.910
Minimize our loss.
That's it.

723
00:46:06.090 --> 00:46:09.540
We minimize this loss and this is dark actual training.
Okay.

724
00:46:09.660 --> 00:46:12.360
And what we're done with this,
we're going to later on in different modules,

725
00:46:12.361 --> 00:46:15.960
we'll save it,
we'll um,
uh,

726
00:46:15.990 --> 00:46:19.950
we can then predict values and this is our training.
Okay.

727
00:46:19.951 --> 00:46:22.050
So let's talk about Adam for a second.

728
00:46:22.051 --> 00:46:27.051
So Adam is one of our possible optimization functions and I definitely need to

729
00:46:27.421 --> 00:46:30.810
make a video that describes the difference between different optimizers.

730
00:46:31.140 --> 00:46:35.550
I definitely need to do that.
But,
uh,
that's not this.

731
00:46:35.551 --> 00:46:37.350
We're not going to talk about all the different optimizers.

732
00:46:37.830 --> 00:46:42.210
Just know that Adam is a great optimizer for,
uh,

733
00:46:42.480 --> 00:46:46.860
for specifically for sequence to sequence models.
So in general,

734
00:46:47.010 --> 00:46:50.250
what hyper parameter is,
should I use,
what optimizer should I use?

735
00:46:50.370 --> 00:46:53.370
What a loss function should I use?
These are,

736
00:46:53.490 --> 00:46:57.600
these are Meta questions and we are not at that phase yet where we can just

737
00:46:57.630 --> 00:46:58.920
learn to learn what we can,

738
00:46:59.040 --> 00:47:02.310
let our network learn what optimize optimization strategy to use,

739
00:47:02.760 --> 00:47:06.120
where we can let it.
Lauren,
what loss function to use,
what we can let it learn,

740
00:47:06.330 --> 00:47:10.050
the hyper parameters to use that is phase two.
That is where we as a

741
00:47:10.570 --> 00:47:11.403
<v 0>mmm.</v>

742
00:47:12.560 --> 00:47:15.410
<v 1>That we as a community are getting too.</v>

743
00:47:15.411 --> 00:47:18.050
We're going to start learning when you let it learn for itself a,

744
00:47:18.051 --> 00:47:22.670
right now we should define this ourselves.
So for Adam optimizer,
so.

745
00:47:24.410 --> 00:47:25.243
<v 0>MMM.</v>

746
00:47:26.950 --> 00:47:31.470
<v 1>Okay.
So it's actually,
this is a great link,</v>

747
00:47:31.471 --> 00:47:34.650
but it's not even,

748
00:47:34.651 --> 00:47:38.910
this is actually a little more complex.
I mean,

749
00:47:39.030 --> 00:47:42.670
how can I best explain this?
So,
so it's,
I mean it's,

750
00:47:42.671 --> 00:47:44.410
it's very similar to grading dissent,

751
00:47:44.411 --> 00:47:49.100
but there are several key concepts that are different that,
uh,

752
00:47:50.500 --> 00:47:52.540
it's okay if your life,
Fernando,
it's all,
it's all good.
Don't,

753
00:47:52.541 --> 00:47:57.400
don't worry about it.
Uh,
just know that Adam is,
is what's given us our best.

754
00:47:57.970 --> 00:48:02.140
I'm still figuring out how to explain optimization techniques without writing

755
00:48:02.141 --> 00:48:06.130
out a lot of math and codes.
Um,

756
00:48:06.760 --> 00:48:10.540
yeah,
I mean,
yeah,
so it's,

757
00:48:10.550 --> 00:48:13.450
it's very similar to grading dissent.
And I have,
uh,

758
00:48:13.580 --> 00:48:15.740
I've talked about grading and the sense several times,
but okay.

759
00:48:15.741 --> 00:48:18.920
So the point is that we are a,
so that's what we're using for that.
So let's,

760
00:48:18.921 --> 00:48:23.430
let's keep going here for a second.
We have a,
so that,
that is our model function.

761
00:48:23.431 --> 00:48:27.410
So what I've just done is I wrote out,
so in the,
in the actual code,

762
00:48:27.680 --> 00:48:30.280
these are all the different classes.
Okay.
They're there,

763
00:48:30.380 --> 00:48:34.790
there's all the different classes here.
So in the model class,
okay,

764
00:48:35.840 --> 00:48:40.370
he's got a class for the model and he's got several policies,
uh,

765
00:48:40.610 --> 00:48:44.060
for different things for like,
you know,
how do we want to choose our weights?

766
00:48:44.270 --> 00:48:48.230
How do we want a sample?
So the,
I,
I honestly think this guy went

767
00:48:49.100 --> 00:48:49.730
<v 0>okay,</v>

768
00:48:49.730 --> 00:48:53.080
<v 1>like overboard with this stuff because he,</v>

769
00:48:53.081 --> 00:48:57.160
he's going at it at such a granular level,
um,

770
00:48:58.270 --> 00:49:02.260
for,
for an example.
But for,
for,
for actual research,
I mean,
this is amazing.

771
00:49:03.420 --> 00:49:08.310
Uh,
so interestingly enough,
this model,
so let's,
let's talk about what happened.

772
00:49:08.311 --> 00:49:12.340
Let me,
let me,
let me,
let me,
uh,
compile this.
Okay,
let's,
let me,
let's,
let's,
uh,

773
00:49:18.280 --> 00:49:20.830
so we have five songs that were training on,
okay.

774
00:49:22.000 --> 00:49:23.290
I'm definitely going to do a video

775
00:49:25.420 --> 00:49:28.660
on TF optimizers.
That's a great idea.
Thank you guys.

776
00:49:28.661 --> 00:49:29.800
I will definitely do a video on that.

777
00:49:30.070 --> 00:49:32.980
I want to start releasing more videos that are focused on,

778
00:49:33.430 --> 00:49:37.240
so I'm gonna actually stop this because this is going to take a lot of compute

779
00:49:37.300 --> 00:49:41.380
to,
to,
to,
to calculate this.
And a lot of time this will take,

780
00:49:42.040 --> 00:49:42.760
uh,

781
00:49:42.760 --> 00:49:47.590
on my Mac book pro 2016 somewhere around one to three hours.

782
00:49:47.650 --> 00:49:51.790
Okay.
To train on up to a hundred songs.
Okay.
One,
two,

783
00:49:51.791 --> 00:49:56.590
three hours and then linearly increase that time depending on how many songs do

784
00:49:56.591 --> 00:50:00.250
you do in the future.
So interestingly,
Leah,
not for this,
um,

785
00:50:01.240 --> 00:50:05.470
for this code,
what this guy did is he said at the end

786
00:50:06.960 --> 00:50:09.870
I had big ambitions but I needed more gps.

787
00:50:09.930 --> 00:50:13.330
So I'm going to keep this model in mind until I have more GPU.

788
00:50:13.740 --> 00:50:17.780
So literally the blocker for this code was amount of GPU.
Yes.

789
00:50:18.370 --> 00:50:22.890
That's,
can you believe that we need better computation?
Okay.

790
00:50:22.891 --> 00:50:26.670
We need more affordable computation.
Do we need GP?

791
00:50:26.680 --> 00:50:29.480
You don't gps,
but if you want to speed up training,

792
00:50:29.630 --> 00:50:31.430
like you could do this on the CPU.
Okay.

793
00:50:31.580 --> 00:50:35.270
But if you want to speed up training by like 10 x gps are nice,

794
00:50:35.900 --> 00:50:40.430
I recommend AWS.
I have a great video on how to use AWS.

795
00:50:41.030 --> 00:50:43.580
And with a prebuilt Amazon machine image,

796
00:50:43.700 --> 00:50:46.940
it's got tentraflow builtin a bunch of great things.
Little you just,

797
00:50:47.210 --> 00:50:50.730
you just upload your model and go.
Um,
and the video on that is,
um,

798
00:50:52.210 --> 00:50:55.150
which one is that?
Tensorflow?
Uh,

799
00:50:57.770 --> 00:51:00.490
though the tension flow image classifier in five minutes.
That's the video.

800
00:51:01.840 --> 00:51:02.250
<v 0>Okay.</v>

801
00:51:02.250 --> 00:51:04.710
<v 1>It was blocked by the GPU.
So let's,
um,</v>

802
00:51:06.330 --> 00:51:11.040
see what the result was.
So I tried this earlier and,

803
00:51:11.041 --> 00:51:16.000
um,
where was this?

804
00:51:16.780 --> 00:51:17.950
Okay,
so let's,
let's play this.

805
00:51:21.110 --> 00:51:24.890
Do I have a mini player or any player?
Yes.
Garageband.

806
00:51:25.730 --> 00:51:29.780
Garageband sucks.
This is on five songs.

807
00:51:30.070 --> 00:51:31.670
It was trained on five songs.

808
00:51:39.490 --> 00:51:40.323
<v 4>Okay.</v>

809
00:51:41.210 --> 00:51:44.150
<v 1>A little dance.
Do a little dance.</v>

810
00:51:47.940 --> 00:51:52.590
Okay.
It sounds amazing because it's overfit on five songs.
That's,

811
00:51:52.591 --> 00:51:55.470
that's the reason.
Okay.
It's overfit that's why it sounds amazing.

812
00:51:56.700 --> 00:51:59.610
So it's interesting we have that curve,
right?
So it starts up really,

813
00:51:59.640 --> 00:52:04.620
so if we had a graph of like how,
how well the model fits to,
uh,

814
00:52:05.250 --> 00:52:09.490
how,
uh,
how much data we've given it,

815
00:52:09.520 --> 00:52:12.490
it's going to be like up here in terms of like quality and that's going to go

816
00:52:12.491 --> 00:52:13.324
down.

817
00:52:13.450 --> 00:52:16.240
And the more data we give it because it's not overfitting and then when we give

818
00:52:16.241 --> 00:52:17.590
it even more than it's going to go up again.

819
00:52:17.591 --> 00:52:21.220
So it's kind of like a you in terms of quality over,
uh,
you know,

820
00:52:21.221 --> 00:52:24.970
so y axis is quality and an x axis is,
uh,
you know,

821
00:52:24.971 --> 00:52:29.100
amount of songs we give it.
So that's,
that's what that is.
Wraps.
Raj,

822
00:52:29.110 --> 00:52:33.460
I will wrap over this.
Of course.
Can someone give me a,
uh,

823
00:52:34.480 --> 00:52:37.540
I remind you a lot of Daniel Shiffman you guys,
we are getting it wrong.

824
00:52:37.930 --> 00:52:41.510
These people remind you me know.
Hi.

825
00:52:41.511 --> 00:52:45.400
Remind Daniel Shiffman reminds you of me.
Okay,

826
00:52:45.600 --> 00:52:48.410
let's let me rap about this.
Big Data using python.

827
00:52:50.600 --> 00:52:51.040
<v 4>Yeah.</v>

828
00:52:51.040 --> 00:52:53.560
<v 1>Hello.
Big Data.
Well,
this is actually not a wrapping bead.</v>

829
00:52:53.561 --> 00:52:58.360
This is like a totally not arriving beat.
So I was so,
so,
uh,

830
00:52:58.450 --> 00:52:59.560
so that's it for this code.

831
00:52:59.710 --> 00:53:02.260
And now what we're going to do is we're going to answer some questions.

832
00:53:02.261 --> 00:53:05.830
So please ask questions about machine learning about this code.

833
00:53:05.960 --> 00:53:09.390
This is our last five minute Q and.
A.
And what I'm going to do is I'm going to,

834
00:53:10.430 --> 00:53:12.020
sure.
So please,
uh,
you know,

835
00:53:12.021 --> 00:53:14.930
say the questions and I'm going to wrap the answers to it.
Okay.

836
00:53:15.350 --> 00:53:18.410
Can we save a trained model on,

837
00:53:21.610 --> 00:53:23.500
can we save a train model on,

838
00:53:29.500 --> 00:53:31.410
can we take the train to shut the hell?

839
00:53:31.840 --> 00:53:34.720
Can we save the train models so we can load it and save the time for training

840
00:53:34.721 --> 00:53:37.720
again and again each time?
Let me answer that.

841
00:53:40.620 --> 00:53:45.570
This is beat sucks.
Balls hold on our or socks,
anything.

842
00:53:46.250 --> 00:53:50.700
Okay.
Yes,
it's the best.
Of course you can save it.
That's what I do.

843
00:53:50.701 --> 00:53:55.500
I'm not just being,
I'm telling you the truth.
Of course you can save it.

844
00:53:55.501 --> 00:53:59.460
The waits are so true.
You save it,
you vote it,
you do it again.

845
00:53:59.610 --> 00:54:03.630
You initialize the variables book.
I'm your friend.
I'm to tell you the truth.

846
00:54:03.660 --> 00:54:08.090
When you save those wastes,
tensorflow has a way for you to do it.
Okay.
Paul,

847
00:54:08.670 --> 00:54:12.720
what other questions do we got?
Can you give a quick summary?

848
00:54:13.080 --> 00:54:15.000
Let me wrap a summary.
Okay.

849
00:54:15.960 --> 00:54:18.660
First we imported our dependencies.
Yo,

850
00:54:18.870 --> 00:54:23.640
we went and started building our network.
So we had three names,
cooked scopes.

851
00:54:23.790 --> 00:54:28.410
Okay.
One for the input and one for the target,
one for the previous.

852
00:54:28.411 --> 00:54:30.740
That's meant I do it.
I'm lost kid.

853
00:54:30.980 --> 00:54:34.710
A largest function is what I use at the end.
Okay.

854
00:54:34.920 --> 00:54:37.720
I got a loop recurring net.
I went back,
uh,

855
00:54:37.721 --> 00:54:41.640
fed at my previous hidden state and that's went back to back sequence,

856
00:54:41.641 --> 00:54:45.750
a sequence models where I came back,
I gave it an output and the previous state,

857
00:54:46.240 --> 00:54:51.150
I wrap all that stuff every day I try to minimize the loss function.
Okay.

858
00:54:51.300 --> 00:54:53.850
Are you soft?
Max?
Cross entropy.
Okay.

859
00:54:54.030 --> 00:54:57.540
And now I'm going to minimize that with Adam.
Yay.
Okay.

860
00:54:57.541 --> 00:55:01.810
So that's what I did for that.
Let me end the wrap.
Okay.

861
00:55:02.440 --> 00:55:05.650
Thank you.
Okay,
so that's,
that's,
that was my summary of what we did.
And let me,

862
00:55:05.651 --> 00:55:07.840
let me summarize it one more time in plain English.

863
00:55:07.840 --> 00:55:12.840
So we imported our 10 are tensorflow machine learning library and three helper

864
00:55:14.140 --> 00:55:15.160
classes.
Okay.

865
00:55:15.370 --> 00:55:19.720
And so what we did was we built a network using three placeholders,

866
00:55:19.870 --> 00:55:22.360
one for the input data,
one for the,

867
00:55:23.080 --> 00:55:25.390
that's our notes and then one for our targets.

868
00:55:25.391 --> 00:55:29.350
And those are binary classifications.
Was this note pressed or not?

869
00:55:29.740 --> 00:55:33.580
And then for our hidden state,
because this is eight recurrent network,

870
00:55:34.330 --> 00:55:38.110
so we had three placeholders and then we built our sequence to sequence model.

871
00:55:38.111 --> 00:55:39.730
That is our encoder and r d quarter.

872
00:55:39.731 --> 00:55:42.580
What we didn't do is we didn't define our encoder here.

873
00:55:42.581 --> 00:55:46.840
That's in a one of our helper classes.
But we did define our decoder here.
Okay.

874
00:55:49.740 --> 00:55:53.260
So that's what we define using sequence a sequence.
So what we did was we fed,

875
00:55:53.261 --> 00:55:58.180
our encoder are and Nicole's notes set it created hidden,
uh,

876
00:55:58.181 --> 00:56:00.250
hit,
uh,
hidden state after training on that.

877
00:56:00.400 --> 00:56:03.400
And the hidden state got better and better,
right?

878
00:56:03.460 --> 00:56:05.790
And then we fed the hidden state to our coder.

879
00:56:05.950 --> 00:56:08.530
We didn't feed the output probability to our decoder.

880
00:56:08.590 --> 00:56:10.660
We don't do that in a sequence.
As he wins,

881
00:56:10.960 --> 00:56:13.840
we feed the internal hidden state to the decoder.

882
00:56:14.110 --> 00:56:18.890
And we use that decoder to take that hidden state,
uh,
to create our,

883
00:56:19.070 --> 00:56:21.010
um,
multi-class classification.

884
00:56:21.011 --> 00:56:23.470
Like the notes are going to be B and a or just right,

885
00:56:23.680 --> 00:56:27.070
depending on whether or not it's likely a cord or a,
you know,
something else.

886
00:56:27.760 --> 00:56:30.640
Okay.
So that's what we did.
And then at the end we said,
well,

887
00:56:30.641 --> 00:56:31.630
what does that lost function?

888
00:56:31.631 --> 00:56:33.730
Depending on the batches of sequence data we give it,

889
00:56:34.210 --> 00:56:37.810
what is that loss function that we are wanting to use to minimize the difference

890
00:56:37.811 --> 00:56:40.510
between the predicted note and the actual note?

891
00:56:40.750 --> 00:56:44.890
And that is a cross entropy and then we,
we minimize it using Adam.

892
00:56:45.520 --> 00:56:49.750
Okay.
And Adam will definitely be explained in a future video.

893
00:56:49.960 --> 00:56:53.290
So that's that.
I still don't understand what a hidden state is.

894
00:56:53.291 --> 00:56:55.810
So a hidden state is,
let me,
let me,
okay,
so neural network,

895
00:56:55.930 --> 00:56:59.020
let me just quickly explain this for a second.
All right.
So,

896
00:57:04.750 --> 00:57:06.190
so for our hidden state,

897
00:57:07.180 --> 00:57:10.930
so in a neural network we have a series of hidden layers and these layers are,

898
00:57:11.110 --> 00:57:15.100
we could think about them as operations.
Okay.
They are just matrices,
uh,

899
00:57:15.120 --> 00:57:16.570
and we have weights.

900
00:57:16.720 --> 00:57:21.280
And an operation like a summation or a multiplication or they can be any number

901
00:57:21.281 --> 00:57:23.420
of operations that we defined.
And so what,

902
00:57:23.490 --> 00:57:28.490
what's happening here is when we're feeding our data in at each of these states,

903
00:57:28.930 --> 00:57:31.840
okay.
At each of these,
uh,
we could also call it hidden layer.

904
00:57:31.930 --> 00:57:35.980
So state is equal to layer and each of these hidden layers or states where

905
00:57:35.981 --> 00:57:40.030
computing some operations,
some matrix operation,
it's all linear Algebra,

906
00:57:40.060 --> 00:57:44.200
it's all matrix math and we are taking the states and we're multiplying it by

907
00:57:44.201 --> 00:57:48.100
that input and that the state itself is going to be updated through an

908
00:57:48.101 --> 00:57:51.190
optimization technique like Adam or you know,
whatever else.

909
00:57:51.490 --> 00:57:53.440
And these weight values are the hidden state.

910
00:57:53.530 --> 00:57:58.270
So we feed those wait values back into the network in combination with our
input.

911
00:57:58.480 --> 00:58:02.290
Okay.
So in by combination we're using a,
a summation operation.

912
00:58:03.310 --> 00:58:03.940
Okay.

913
00:58:03.940 --> 00:58:07.630
So we're not just fitting in the input data like we would in a feed forward.

914
00:58:07.631 --> 00:58:10.510
Now like continue,
we're also feeding in the previous hidden state.
And this,

915
00:58:10.511 --> 00:58:12.970
this is because we're,
we're doing it over time,
right?

916
00:58:12.971 --> 00:58:17.320
This is back propagation through time.
How did you feed in the songs?

917
00:58:17.321 --> 00:58:18.230
Didn't get it.
Okay.
So I,

918
00:58:18.280 --> 00:58:22.570
what I didn't do is I didn't talk about the preprocessing of the actual songs.

919
00:58:22.600 --> 00:58:26.380
You're right.
That's a great question.
I only talked about building the model.

920
00:58:26.920 --> 00:58:30.610
And so for our songs we have,
you know,
two,
two more minutes.

921
00:58:30.880 --> 00:58:35.830
So for our songs we had our,
um,
music data class.
And

922
00:58:37.330 --> 00:58:39.940
so Minnie is a sequence of notes and there's,

923
00:58:39.970 --> 00:58:43.990
there's a number of preprocessing steps that happened here.
Uh,
there's,

924
00:58:44.020 --> 00:58:47.530
there's actually quite a bit of preprocessing.
Well,
so,
so let,
let me do this.

925
00:58:47.531 --> 00:58:49.360
Let me do this.
So there's actually one word.

926
00:58:49.390 --> 00:58:53.650
So I said I was going to give you guys one more,
uh,
um,
library.

927
00:58:53.830 --> 00:58:57.100
So let me paste this in the chat.

928
00:58:58.360 --> 00:59:02.950
Thanks Colin.
So go to this link right here.

929
00:59:03.300 --> 00:59:04.900
[inaudible] slash blues.

930
00:59:04.930 --> 00:59:08.340
So this is a great example of a,

931
00:59:08.420 --> 00:59:12.700
a very easy to understand music lie.
Um,

932
00:59:13.120 --> 00:59:17.080
I python notebook.
Okay.
It's literally just this,
it's one I python notebook,

933
00:59:17.110 --> 00:59:20.020
but the reason I didn't use it is because of dependency issues.

934
00:59:20.021 --> 00:59:22.100
So remember when it comes to python,

935
00:59:22.101 --> 00:59:24.890
we have a little bit of work to do for our music libraries.

936
00:59:25.100 --> 00:59:27.560
They're not good enough right now that,
you know,

937
00:59:27.561 --> 00:59:31.790
Python Mitie only works for Python two.
We need to update it for python three,

938
00:59:31.791 --> 00:59:35.000
which is what I use now.
Um,

939
00:59:36.380 --> 00:59:41.060
but what,
uh,
what she did,
I,
it's a,
she,
she uh,

940
00:59:42.520 --> 00:59:43.353
buck teeth,

941
00:59:43.510 --> 00:59:48.510
she took these midi notes and she converted them into a matrices of values,

942
00:59:49.991 --> 00:59:53.770
right?
They're already matrices,
right?
Uh,
and so we use,
and the,

943
00:59:53.771 --> 00:59:57.220
and the major cs were fed directly into the network.
They were vectorized notes.

944
00:59:57.430 --> 01:00:01.630
So that's kind of,
you know,
a one line of how data in general should be fed into,

945
01:00:02.500 --> 01:00:06.820
uh,
into a network.
Okay.
So check out that link.

946
01:00:07.330 --> 01:00:10.210
A Bhakti Priya slash blues and a,
yeah,

947
01:00:10.930 --> 01:00:15.190
two more questions and then we're done for the day.
Okay.
Java is all right.
No,

948
01:00:15.191 --> 01:00:18.070
it's not.
Guys,
can you guide someone who is starting out?

949
01:00:18.071 --> 01:00:21.730
What's the best way to go about learning?
Uh,
my video,

950
01:00:21.731 --> 01:00:23.290
start with machine learning for hackers,

951
01:00:23.530 --> 01:00:27.730
then move on to learn python for data science.
Then I'm huge now,

952
01:00:28.030 --> 01:00:32.020
so I'll just,
the way I like it for Q and a exactly.

953
01:00:32.950 --> 01:00:37.900
A,
she has a sheet.
Okay.
And then okay.
Actually to,
to,

954
01:00:37.901 --> 01:00:41.260
to not to do to two more questions.

955
01:00:43.630 --> 01:00:44.381
She's looking at you.

956
01:00:44.381 --> 01:00:49.120
She likes you how to use LSTs s sequence classifier.

957
01:00:51.150 --> 01:00:56.150
Cara Ross has a great example on the main read me on using a [inaudible] for a

958
01:00:57.660 --> 01:01:01.830
sequence classifier.
And in fact I have videos on this.
Uh,

959
01:01:02.010 --> 01:01:06.660
what was my last LSTM video
art generation,

960
01:01:06.661 --> 01:01:10.350
but that was more generative.
Uh,
Lstm frog sequenced classifier.
Yeah.

961
01:01:10.351 --> 01:01:13.060
Just to use a double stack Lsem and have your last uh,

962
01:01:13.350 --> 01:01:18.350
layer be a fully connected layer of followed by a softmax so you squash it.

963
01:01:19.500 --> 01:01:23.450
Okay.
And then,
all right,
one more question.

964
01:01:23.451 --> 01:01:25.730
We have one more question and then we're out of here.

965
01:01:26.510 --> 01:01:30.810
How to learn the mathematics of deep learning.
Gwar great question.

966
01:01:30.811 --> 01:01:34.850
So I answered this in my first video of this series,
but I'll answer it again.

967
01:01:35.390 --> 01:01:39.320
Linear Algebra,
calculus,
statistics.

968
01:01:39.560 --> 01:01:42.920
Do you need to know everything about a calculus and linear?
No.

969
01:01:43.340 --> 01:01:45.620
You just need to know the bare minimum.

970
01:01:45.621 --> 01:01:50.621
And I have linked to a cheat sheets for each of those in my first intro to deep

971
01:01:51.141 --> 01:01:53.000
learning video.
Check out those cheat sheets.

972
01:01:53.210 --> 01:01:56.510
As in do you need to know the inner the integrals for Calculus?
No,

973
01:01:56.750 --> 01:02:01.520
you just need to know the derivatives,
why we use it for doc propagation.
Okay.

974
01:02:02.600 --> 01:02:06.140
You just need to know the basics.
Am I sitting here studying calculus,

975
01:02:06.141 --> 01:02:09.650
linear Algebra?
No,
I'm focused on machine learning and deep learning.

976
01:02:10.490 --> 01:02:14.330
So I only need it.
I need,
I only need to learn what I need.
Okay.

977
01:02:14.480 --> 01:02:16.220
And that's a mentality all of us should have.

978
01:02:16.280 --> 01:02:20.700
Let's only learn what we need to learn,
okay?
And nothing more.
Okay.

979
01:02:20.701 --> 01:02:24.210
Because there is a lot of stuff to learn,
so we don't want to waste our time.

980
01:02:24.211 --> 01:02:28.500
Our time is very valuable.
Your time is very valuable.
Okay,

981
01:02:28.501 --> 01:02:33.120
so that's it on for this session session on Bayesean deep learning that is going

982
01:02:33.121 --> 01:02:34.170
to be at the end of this course.

983
01:02:34.171 --> 01:02:38.280
I cannot wait to talk about Basie and optimization Basie and learning one shot.

984
01:02:38.281 --> 01:02:41.580
Learning that in some advanced stuff.
But we will get there when we get there.

985
01:02:41.820 --> 01:02:44.430
I am so excited to have all you here a part of this journey.

986
01:02:44.431 --> 01:02:48.840
Thank you for showing up.
We're going to learn everything.
Okay.

987
01:02:48.841 --> 01:02:51.120
We are going to learn everything.
We're going to learn to learn.

988
01:02:51.510 --> 01:02:54.420
We're going to learn to learn,
to learn.
Okay.
So thanks for watching.

989
01:02:54.770 --> 01:02:58.080
I love you guys,
and for now I've got to go.

990
01:03:00.130 --> 01:03:00.963
<v 0>Yeah.</v>

991
01:03:01.420 --> 01:03:06.300
<v 1>Learn to learn
to learn.
So thanks for watching.
Yeah.</v>

