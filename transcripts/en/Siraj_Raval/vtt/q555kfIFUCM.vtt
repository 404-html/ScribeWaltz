WEBVTT

1
00:00:00.090 --> 00:00:00.691
Oh world.

2
00:00:00.691 --> 00:00:04.650
It's to Raj and today we're going to learn how backpropagation works in five

3
00:00:04.651 --> 00:00:05.130
minutes.

4
00:00:05.130 --> 00:00:10.130
Gradient descent is a popular optimization technique and can be used in many

5
00:00:10.141 --> 00:00:12.120
different types of machine learning models.

6
00:00:12.150 --> 00:00:16.890
It's used to optimize or improve the accuracy of our models.
Predictions.

7
00:00:17.160 --> 00:00:21.720
One implementation of it that is particularly popular is for neural networks.

8
00:00:21.930 --> 00:00:25.080
A neural network is eight learning model that can make predictions.

9
00:00:25.230 --> 00:00:26.640
We give it some input data.

10
00:00:26.670 --> 00:00:30.990
The x values represent the input and the y values represent the expected output.

11
00:00:30.991 --> 00:00:32.160
Labels are networks.

12
00:00:32.161 --> 00:00:35.700
Job is to learn this mapping so that given some arbitrary input,

13
00:00:35.880 --> 00:00:38.040
it can correctly predict its output label.

14
00:00:38.070 --> 00:00:41.130
So our three layer neural network will first have an input layer.

15
00:00:41.310 --> 00:00:44.130
Each neuron represents a different row from our input data.

16
00:00:44.160 --> 00:00:45.420
Then it has a hidden layer.

17
00:00:45.450 --> 00:00:49.080
Data will flow in one direction from our input layer to our output.

18
00:00:49.260 --> 00:00:53.070
And the way it does this is by having weights that connect each neuron in one

19
00:00:53.071 --> 00:00:55.050
layer to every neuron.
In the next layer,

20
00:00:55.110 --> 00:00:59.700
we can initialize these weights as matrices with random values to start off
with.

21
00:00:59.910 --> 00:01:04.020
Well multiply each row of the input by each column of our weight Matrix.

22
00:01:04.230 --> 00:01:08.400
The resulting values from this operation results in our hidden neuron values.

23
00:01:08.430 --> 00:01:12.360
We'll take each of those values and convert them to a probability value that is

24
00:01:12.361 --> 00:01:16.800
a value between zero and one by applying an activation function to it.

25
00:01:16.860 --> 00:01:19.260
The type will use in this example is a sigmoid,

26
00:01:19.290 --> 00:01:23.010
so each neuron receives a set of inputs,
performs a dot product,

27
00:01:23.040 --> 00:01:25.440
and then applies an activation function to it.

28
00:01:25.530 --> 00:01:29.400
We'll just repeat this same process again.
To calculate the output prediction,

29
00:01:29.430 --> 00:01:32.790
we compute the dot product of the hidden layer neurons and the next weight

30
00:01:32.791 --> 00:01:35.490
matrix between the hidden layer and the output layer.

31
00:01:35.520 --> 00:01:38.280
Then we once again apply our activation function to it.

32
00:01:38.370 --> 00:01:42.360
This resulting value is our prediction and this process that we just completed

33
00:01:42.390 --> 00:01:46.320
is called forward propagation.
If we compare this to our expected output,

34
00:01:46.350 --> 00:01:48.450
we'll see that our prediction is incorrect.

35
00:01:48.540 --> 00:01:52.470
We went to find the absolute best wait values that given any input,

36
00:01:52.530 --> 00:01:55.200
they would help calculate the correct output.
To do this well,

37
00:01:55.201 --> 00:01:59.010
first went to calculate an error value.
You want to minimize this error.

38
00:01:59.040 --> 00:02:02.700
If we were to create a simple graph of the error value versus some random weight

39
00:02:02.701 --> 00:02:03.480
from our network,

40
00:02:03.480 --> 00:02:07.770
it would look like this to smaller weight value in our hair is high but too big

41
00:02:07.771 --> 00:02:09.360
and our error becomes high.
Again,

42
00:02:09.420 --> 00:02:13.350
we want an optimal value for each weight in our network where our hair is

43
00:02:13.351 --> 00:02:15.600
smallest starting at some random weight value,

44
00:02:15.630 --> 00:02:19.200
we went to take a step in the direction towards the minimum error.

45
00:02:19.260 --> 00:02:21.930
This direction is the opposite to the gradient.

46
00:02:21.960 --> 00:02:24.570
If we take many steps descending down the gradient,

47
00:02:24.630 --> 00:02:27.270
eventually the weight will find the minimum of the error.

48
00:02:27.300 --> 00:02:31.050
We call this process gradient descent.
So how do we do this?
Well,

49
00:02:31.051 --> 00:02:32.190
we'll need to use calculus.

50
00:02:32.250 --> 00:02:34.980
Let's do a little refresher on three terms from calculus.

51
00:02:34.981 --> 00:02:38.670
We'll need to know the derivative is a term that means the slope of the tangent

52
00:02:38.671 --> 00:02:42.930
line to occur at a specific point in measures the rate of change of a function,

53
00:02:42.990 --> 00:02:47.070
a derivative of a function f of x gives you another function that returns the

54
00:02:47.071 --> 00:02:49.830
slope of f of x at a point x.
For example,

55
00:02:49.831 --> 00:02:52.410
the derivative of x squared is two x.

56
00:02:52.530 --> 00:02:57.530
So at x equals to the slope is for a partial derivative of a function of several

57
00:02:57.751 --> 00:03:02.740
variables is it's with respect to one of those variables with the others held

58
00:03:02.741 --> 00:03:07.270
constant and the chain rule is the process we can use to compute derivatives of

59
00:03:07.271 --> 00:03:08.710
composite functions.

60
00:03:09.040 --> 00:03:13.270
A composite function is a function of other functions that is we might have one

61
00:03:13.271 --> 00:03:16.420
function that is composed of multiple inner or nested functions.

62
00:03:16.480 --> 00:03:20.140
Say you have some function f of x and another function g of x using them,

63
00:03:20.141 --> 00:03:23.110
you form some composite function f of g of x.

64
00:03:23.200 --> 00:03:27.070
The chain rule states that the derivative of f of g of x is equal to the

65
00:03:27.071 --> 00:03:30.520
derivative of g of x times the derivative of f of x.

66
00:03:30.580 --> 00:03:33.910
A neural network is essentially a massive nested composite function.

67
00:03:33.940 --> 00:03:37.510
Each layer of a feed forward neural network can be represented as a single

68
00:03:37.511 --> 00:03:41.440
function who's inputs are a weight vector and the outputs of the previous layer.

69
00:03:41.470 --> 00:03:45.520
The purpose of backpropagation is to figure out the partial derivatives of our

70
00:03:45.521 --> 00:03:50.140
air function with respect to each individual weights in the network so we can

71
00:03:50.141 --> 00:03:51.670
use those in gradient descent.

72
00:03:51.730 --> 00:03:55.810
It gives us a way of computing the air for every layer and then relating those

73
00:03:55.811 --> 00:03:59.170
errors to the quantity of real interest,
a partial derivative.

74
00:03:59.200 --> 00:04:01.480
With respect to any weight in the network.

75
00:04:01.570 --> 00:04:04.510
We can use the chain rule to compute the partial derivatives.

76
00:04:04.630 --> 00:04:08.110
That is the gradient of the error with respect to each weight.

77
00:04:08.170 --> 00:04:12.460
Backpropagation at its core simply consist of repeatedly applying the chain rule

78
00:04:12.461 --> 00:04:14.920
through all the possible paths in our network.

79
00:04:14.950 --> 00:04:19.000
Our ultimate goal in training a neural network is to find the gradient of each

80
00:04:19.001 --> 00:04:20.740
weight with respect to the output.

81
00:04:20.830 --> 00:04:24.370
We do this so that we can update the weights incrementally using gradient

82
00:04:24.371 --> 00:04:24.790
descent.

83
00:04:24.790 --> 00:04:29.120
We reuse multiple values as we compute the updates for weights that appear

84
00:04:29.140 --> 00:04:32.950
earlier and earlier in the network.
After we have the error for the output layer,

85
00:04:33.070 --> 00:04:37.240
we calculate an error for each neuron in the hidden layers going backwards layer

86
00:04:37.241 --> 00:04:38.020
by layer.

87
00:04:38.020 --> 00:04:41.770
The air for a neuron in a hidden layer is the some of the products between the

88
00:04:41.771 --> 00:04:45.670
errors of the neurons in the next layer and the weights of the connections to

89
00:04:45.671 --> 00:04:49.660
those neurons multiplied by the derivative of the activation function.

90
00:04:49.780 --> 00:04:52.960
We will use those errors to calculate the variation of the weights.

91
00:04:52.961 --> 00:04:56.230
As a result of the current input pattern and ideal outputs.

92
00:04:56.320 --> 00:05:00.580
The variation or Delta of a weight is the product that the input neuron output

93
00:05:00.581 --> 00:05:03.700
value with the air of the output neuron for that connection.

94
00:05:03.790 --> 00:05:07.370
This process is repeated for all the input patterns and the deltas are

95
00:05:07.420 --> 00:05:09.820
accumulated at the end of a learning iteration.

96
00:05:09.821 --> 00:05:13.900
We changed the actual weights with the accumulated deltas for all the training

97
00:05:13.901 --> 00:05:16.450
patterns,
and we multiply it with a learning rate,

98
00:05:16.451 --> 00:05:20.140
which states how fast network converges to a result.
When we run our code,

99
00:05:20.141 --> 00:05:23.620
we can see this process in action as our prediction gradually increases

100
00:05:23.680 --> 00:05:25.090
inaccuracy.
Please subscribe,

101
00:05:25.091 --> 00:05:28.900
and for now I've got to go derive the meaning of life,
so thanks for watching.

