WEBVTT

1
00:00:04.770 --> 00:00:05.290
Oh

2
00:00:05.290 --> 00:00:07.480
<v 0>world.
It's to Raj.
In this episode,</v>

3
00:00:07.481 --> 00:00:11.500
we're going to build an image classifier using tensorflow in 30 lines of python,

4
00:00:11.560 --> 00:00:15.490
and I don't mean a classifier that can detect handwritten digits or iris
flowers.

5
00:00:15.550 --> 00:00:17.350
I'm talking literally anything you want,

6
00:00:17.380 --> 00:00:20.050
you'll be able to train this thing to classify chocolate if you want it.

7
00:00:20.180 --> 00:00:21.013
<v 1>Hmm.</v>

8
00:00:24.950 --> 00:00:26.420
<v 0>The possibilities are endless.</v>

9
00:00:26.540 --> 00:00:30.080
There's so many industries that can be disrupted by just this simple solution.

10
00:00:30.200 --> 00:00:33.560
A Japanese cucumber farmer built a machine to detect whether each of his

11
00:00:33.561 --> 00:00:36.230
cucumbers was one of nine different types.
Using this thing.

12
00:00:40.520 --> 00:00:42.590
Let's say we want to build a Saroj classifier.

13
00:00:42.680 --> 00:00:45.710
If we think about this problem in the traditional programming paradigm,

14
00:00:45.740 --> 00:00:47.540
we want to handcraft a bunch of features.

15
00:00:47.600 --> 00:00:51.470
Maybe we could do some edge detection to save the shape of my hair or use a

16
00:00:51.471 --> 00:00:53.780
color histogram to save the color of my teeth.

17
00:00:54.020 --> 00:00:57.500
The problem with that is there's so much variance in Serrano's.
My hair is alive.

18
00:00:57.530 --> 00:00:59.090
Seriously,
it's never the same.

19
00:00:59.120 --> 00:01:01.970
This is where a convolutional neural networks come into play.

20
00:01:02.000 --> 00:01:06.020
They're essentially a black box that constructs features that we would otherwise

21
00:01:06.021 --> 00:01:09.860
have to handcraft and these abstract features they create from training are so

22
00:01:09.861 --> 00:01:12.110
generalized that the account for variants,

23
00:01:12.320 --> 00:01:14.240
if we want it to train a CNN ourselves,

24
00:01:14.420 --> 00:01:18.470
we need a lot of computing power and a lot of time,
both of which we don't have.

25
00:01:18.500 --> 00:01:21.020
I don't even have time to do my dishes,
sorry,
roommates.

26
00:01:21.080 --> 00:01:24.470
That's why we'll want to use a pretrained CNN model called inception.

27
00:01:24.710 --> 00:01:28.550
Inception was trained by Google on a hundred k images with a thousand
categories.

28
00:01:28.640 --> 00:01:32.090
Our use case.
In this video,
we'll be classifying Darth Vader pictures,

29
00:01:32.120 --> 00:01:33.710
but in sexual wasn't trained on Bader,

30
00:01:33.711 --> 00:01:36.620
so we're going to perform a process called transfer learning.

31
00:01:36.650 --> 00:01:39.920
That means applying the learnings from a previous training session to a new

32
00:01:39.921 --> 00:01:42.170
training session.
If we look at the inception model,

33
00:01:42.200 --> 00:01:45.170
we can see that when we feed in an image as an input at each layer,

34
00:01:45.230 --> 00:01:48.860
it will perform a series of operations on the data until it output to label and

35
00:01:48.861 --> 00:01:52.310
classification percentage.
Each layer is a different set of abstractions.

36
00:01:52.340 --> 00:01:55.280
In the first layers.
It's basically taught itself edge detection,

37
00:01:55.340 --> 00:01:58.820
then shaped detection in the middle layers and to get increasingly more abstract

38
00:01:58.880 --> 00:02:01.130
up until the end.
If we look at the last few layers,

39
00:02:01.160 --> 00:02:04.580
these are the highest level detectors for whole objects for transfer learning.

40
00:02:04.670 --> 00:02:07.820
Well basically just want to retrain that last layer on features of Darth Vader

41
00:02:07.880 --> 00:02:11.360
so it can add a representation of him to its repository of knowledge.

42
00:02:11.420 --> 00:02:14.840
So this is going to be a seven step process and we're going to go through each

43
00:02:14.841 --> 00:02:17.720
step in order.
Sound good?
Step one is to install docker,

44
00:02:17.721 --> 00:02:20.900
which is a tool for creating a virtual container on your machine for running

45
00:02:20.901 --> 00:02:24.560
apps.
The benefit of docker is you don't have to install any dependencies on your

46
00:02:24.561 --> 00:02:28.370
machine.
So we'll eventually download a docker image that has all the necessary

47
00:02:28.371 --> 00:02:32.060
dependencies per tensor flow builtin.
Just download a docker toolbox,

48
00:02:32.090 --> 00:02:33.740
go through the installation process,

49
00:02:33.800 --> 00:02:37.220
and then you can launch your docker container anytime easily by double clicking

50
00:02:37.221 --> 00:02:40.310
the docker quickstar terminal.
Cool.
Now that we have docker open,

51
00:02:40.400 --> 00:02:44.240
that brings us to step to installing the tensorflow docker image by pacing in

52
00:02:44.241 --> 00:02:44.840
this line.

53
00:02:44.840 --> 00:02:48.050
It'll take a few minutes and once it's installed we'll move on to step three.

54
00:02:48.080 --> 00:02:51.200
Downloading our image data set to our local machine will stop docker with

55
00:02:51.201 --> 00:02:54.950
Control d and create a directory called Kia file slash star wars and our home

56
00:02:54.951 --> 00:02:55.371
directory.

57
00:02:55.371 --> 00:02:59.140
Locally we want to put a folder labeled Darth Vader that contains a couple

58
00:02:59.180 --> 00:03:00.670
hundred Bader pics.
In here.

59
00:03:00.700 --> 00:03:04.420
There's this dope chrome extension I found called [inaudible] batch download

60
00:03:04.421 --> 00:03:07.900
image that bulk downloads all the images from your Google image search results.

61
00:03:07.930 --> 00:03:09.400
Just go to Google image search,

62
00:03:09.430 --> 00:03:12.550
type in Darth Vader and started downloading all of those images.

63
00:03:12.580 --> 00:03:13.361
Once we've got them,

64
00:03:13.361 --> 00:03:16.510
we'll just drag that folder into our TF file slash star wars folder.

65
00:03:16.570 --> 00:03:17.950
That brings us to step four.

66
00:03:18.010 --> 00:03:20.180
Now that we have our images in our TF file directory,

67
00:03:20.210 --> 00:03:23.440
but wanted to link them to our docker container with this command boom all

68
00:03:23.441 --> 00:03:27.640
linked up.
Step five is to download the training script to via get to CD into the

69
00:03:27.641 --> 00:03:30.100
tensorflow directory,
then run get pull.

70
00:03:30.160 --> 00:03:33.640
This code will allow us to retrain the inception classifier with our newly

71
00:03:33.641 --> 00:03:37.240
linked Darth Vader image Dataset.
Step six is the actual retraining part.

72
00:03:37.270 --> 00:03:40.780
The bottleneck directory will be used to cash the outputs of the lower layers on

73
00:03:40.781 --> 00:03:43.570
disk so they don't have to repeatedly be recalculated.

74
00:03:43.600 --> 00:03:45.760
We'll run this example for 500 iterations.

75
00:03:45.820 --> 00:03:48.790
The next flag asked where to store our training model,
our apple craft,

76
00:03:48.791 --> 00:03:51.400
which we can later view intenser board our output labels,

77
00:03:51.401 --> 00:03:54.550
which will be that same as our training data folder name and the image directory

78
00:03:54.551 --> 00:03:56.170
where we stored our Bader images.

79
00:03:56.200 --> 00:03:59.800
Let's go ahead and run the script right from terminal will take about 30 minutes

80
00:03:59.801 --> 00:04:02.260
or so to train our classifier,
so do something productive.

81
00:04:06.380 --> 00:04:10.640
The should output a training accuracy somewhere between 85 and 99% when it's

82
00:04:10.641 --> 00:04:12.920
done,
and this brings us to our final step,

83
00:04:12.950 --> 00:04:16.130
we want to write a script that we'll use our new retrained classifier to

84
00:04:16.131 --> 00:04:19.550
detective.
A novel image contains Darth Vader.
We'll write this script ourselves.

85
00:04:19.580 --> 00:04:21.320
First thing's first,
we'll import tensorflow.

86
00:04:21.380 --> 00:04:24.290
Then we'll want to create a variable to store the user input image path.

87
00:04:24.291 --> 00:04:27.920
We'll create another variable to store the data from that image and one more to

88
00:04:27.921 --> 00:04:29.840
local.
Label up that image from the label file.

89
00:04:29.870 --> 00:04:33.350
Next want to grab our model from the saved retrained graph file,

90
00:04:33.380 --> 00:04:35.750
store it in the graph death variable and parts it.

91
00:04:35.780 --> 00:04:37.610
Now that we have our image and model ready,

92
00:04:37.640 --> 00:04:40.920
it's time to make our prediction by feeding the image data into our retrained

93
00:04:40.940 --> 00:04:43.310
model to get our prediction output.
In order to do this,

94
00:04:43.340 --> 00:04:44.690
we'll create a tensor flow session.

95
00:04:44.720 --> 00:04:48.020
This will give us an environment to perform operations on our tents or data in.

96
00:04:48.080 --> 00:04:51.410
The first thing we'll do in a session is get our softmax function tensor from

97
00:04:51.411 --> 00:04:52.640
the last layer of our model.

98
00:04:52.670 --> 00:04:55.790
The softmax function is used in the final layer to map input data into

99
00:04:55.791 --> 00:04:57.830
probabilities of an expected output.

100
00:04:57.890 --> 00:05:01.460
We will execute our soft Max tensor function on our input image data be in a

101
00:05:01.461 --> 00:05:02.480
session run function.

102
00:05:02.510 --> 00:05:05.750
It will output our predictions as an array well next one to sort our prediction

103
00:05:05.751 --> 00:05:08.900
labels in order of confidence.
And lastly,
for every prediction we have,

104
00:05:08.901 --> 00:05:12.020
we can get the predicted label and the score and printed out to terminal.

105
00:05:12.050 --> 00:05:14.900
Let's take the script and run it on one of our Vader pictures.

106
00:05:14.930 --> 00:05:16.350
The result is pretty good tensorflow.

107
00:05:16.380 --> 00:05:18.740
It makes it much easier to classify an image.

108
00:05:18.770 --> 00:05:20.720
And I've got a challenge for you guys on this episode.

109
00:05:20.750 --> 00:05:24.200
The challenge is to create a classifier that you think would be a useful tool

110
00:05:24.201 --> 00:05:25.310
for scientists to have.

111
00:05:25.370 --> 00:05:28.130
It can be any field of science you'd like upload your code to get hub.

112
00:05:28.190 --> 00:05:31.610
And in the read me write up a few sentences on how a scientist would use this

113
00:05:31.611 --> 00:05:35.090
poster repository in the comment section and I'll judge them based on utility

114
00:05:35.091 --> 00:05:38.030
and accuracy.
The winner gets a shout out for me to videos from now.

115
00:05:38.031 --> 00:05:38.870
So in two weeks,

116
00:05:38.900 --> 00:05:42.740
and I'll also send you a free signed copy of my book decentralized applications.

117
00:05:42.770 --> 00:05:46.310
For now,
I've got to go not by the iPhone seven so thanks for watching.

