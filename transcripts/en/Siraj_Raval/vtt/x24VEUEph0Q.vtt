WEBVTT

1
00:00:00.060 --> 00:00:03.120
My dear Megan Tang.
This is our love story,

2
00:00:06.180 --> 00:00:10.680
human or machine human.
Why I found some drops and soylent on it.
Oh,

3
00:00:10.690 --> 00:00:12.270
a world walking the third geology.

4
00:00:12.271 --> 00:00:15.600
In today's episode we're going to build an AI writer that is an app that can

5
00:00:15.601 --> 00:00:19.380
write a short story about an image just by looking at it.
Sorry,
Stephen King.

6
00:00:19.410 --> 00:00:20.220
You're out of the gate.

7
00:00:20.220 --> 00:00:23.910
Isn't it weird how just by stringing together an exact combination of words,

8
00:00:23.940 --> 00:00:26.880
we can produce something,
a profound beauty.
When we read these stories,

9
00:00:26.881 --> 00:00:29.280
our brains are somehow encoding them into thoughts.

10
00:00:29.310 --> 00:00:30.990
When we encode a sentence to a thought,

11
00:00:31.050 --> 00:00:33.570
the more semantically similar it is to an existing thought,

12
00:00:33.600 --> 00:00:35.310
the more we'll be able to relate to it.

13
00:00:35.370 --> 00:00:39.120
So how do we get an AI to write a story if it doesn't have any experience living

14
00:00:39.121 --> 00:00:40.530
life in the real world?
Well,

15
00:00:40.531 --> 00:00:43.740
we're going to build an AI writer in python using the deep learning library

16
00:00:43.741 --> 00:00:47.280
Lasagna,
and we've got a lot of code to go over.
So I'll explain as we go.

17
00:00:47.340 --> 00:00:49.320
Let's get python Anik.
At the highest level,

18
00:00:49.321 --> 00:00:53.040
we could code this APP in just three lines of code.
It's a little ridiculous.

19
00:00:53.070 --> 00:00:55.980
We import the generate class and call it a load off function,

20
00:00:56.010 --> 00:00:58.320
which we'll initialize all of our machine learning model.

21
00:00:58.350 --> 00:01:01.830
Then called a story function with the generated models and image location as the

22
00:01:01.831 --> 00:01:05.160
parameter.
That's it.
It'll output a story,
but let's dive a little deeper.

23
00:01:05.190 --> 00:01:07.650
A load off function is just one of the plate initialization,

24
00:01:07.651 --> 00:01:11.160
so let's take a closer look at the story function where the real magic happens.

25
00:01:11.161 --> 00:01:13.080
We'll start off by loading an image into memory.

26
00:01:13.110 --> 00:01:15.330
This will be the image that we want to tell a story about.

27
00:01:15.331 --> 00:01:18.570
We'll use a load image function to load it and have the parameters set to the

28
00:01:18.571 --> 00:01:20.580
location of the image on our machine.

29
00:01:20.610 --> 00:01:23.430
The load image function uses the scientific computing library,

30
00:01:23.431 --> 00:01:27.360
num Pi to get the byte representation of the image and then resize it so it's

31
00:01:27.361 --> 00:01:30.810
smaller,
but preserving it's aspect ratio.
Once we've loaded our image,

32
00:01:30.840 --> 00:01:34.470
it's time to input the image into a deep convolutional neural network to

33
00:01:34.471 --> 00:01:36.750
retrieve its features.
In a previous episode,

34
00:01:36.751 --> 00:01:40.140
we talked about how convolutional neural nets were great for image recognition

35
00:01:40.170 --> 00:01:43.980
since they roughly mimic the human visual cortex.
This.
CNN is pretrained.

36
00:01:43.981 --> 00:01:46.290
We initialize it into build cognitive function,

37
00:01:46.291 --> 00:01:48.720
which is called in the boiler plate load.
All method.

38
00:01:48.721 --> 00:01:50.460
Once we specified all the layers,

39
00:01:50.461 --> 00:01:54.060
we load up our pretrained sinaps weights fall called BGG 19th it's fall was

40
00:01:54.061 --> 00:01:56.130
trained on a huge Dataset of labeled images,

41
00:01:56.131 --> 00:01:59.070
so it will be able to recognize the objects in a novel image.

42
00:01:59.100 --> 00:02:02.580
Once we input our image into our CNN,
it'll return an array of features for us.

43
00:02:02.610 --> 00:02:04.860
He's features of the highest level features in our neural net.

44
00:02:04.890 --> 00:02:06.570
The layer right before the output layer,

45
00:02:06.600 --> 00:02:09.630
the most abstract representation of the image.
It's content.

46
00:02:09.660 --> 00:02:10.650
Once we have our features,

47
00:02:10.651 --> 00:02:14.190
we want to encode the image features into a multimodal neural language models.

48
00:02:14.191 --> 00:02:15.060
So what is this?
Well,

49
00:02:15.061 --> 00:02:19.320
it's based off a paper called unifying visual semantic embeddings.
In our code,

50
00:02:19.321 --> 00:02:23.010
we're using a pretrained models that will input a joint image sentence embedding

51
00:02:23.040 --> 00:02:24.720
into a multimodal vector space.

52
00:02:24.780 --> 00:02:28.110
It use an Lstm to encode the sentence and a CNN to encode the image.

53
00:02:28.140 --> 00:02:31.710
Then a decoder neural language model generates a novel description from the

54
00:02:31.711 --> 00:02:32.100
image.

55
00:02:32.100 --> 00:02:36.060
Since our model is pretrained and we embed our image into this multimodal space,

56
00:02:36.090 --> 00:02:38.850
our features are updated to include the weight of the joint space.

57
00:02:38.880 --> 00:02:41.160
Then we compute the nearest neighbors to do this.

58
00:02:41.161 --> 00:02:42.840
First we retrieved the array of scores.

59
00:02:42.870 --> 00:02:46.380
That is a list of all novel sentences generated from the novel image,

60
00:02:46.410 --> 00:02:48.360
which we then sort in order of closeness.

61
00:02:48.420 --> 00:02:50.220
Then we'll want to print out the nearest captions.

62
00:02:50.250 --> 00:02:52.110
Now that we have a set of caption sentences,

63
00:02:52.111 --> 00:02:55.500
one to compute a set of skip thought vectors for each sentence.

64
00:02:55.560 --> 00:02:58.890
Skipped up vectors are a vector representation of a sentence.

65
00:02:58.930 --> 00:03:02.500
This is another implementation of the encoder decoder model.

66
00:03:02.560 --> 00:03:05.800
The encoder and decoder are both recurrent neural networks.

67
00:03:05.801 --> 00:03:10.030
We take an input sentence and encode it into a skip fuck vector by inputting it

68
00:03:10.031 --> 00:03:11.630
into the encoding recurrent neural net.

69
00:03:11.631 --> 00:03:13.390
Since we are modeling a sequence of words,

70
00:03:13.391 --> 00:03:16.700
we use gated recurrent units or gr use at each neuron.

71
00:03:16.750 --> 00:03:19.760
Gr used consists of two gates on uptake gate and a reset gate.

72
00:03:19.810 --> 00:03:22.360
The gating units modulate the flow of data inside the unit,

73
00:03:22.361 --> 00:03:25.330
and unlike LSTM cells,
there are no separate memory cells.

74
00:03:25.360 --> 00:03:28.630
LSTM cells control the amount of memory content that is seen or used by other

75
00:03:28.631 --> 00:03:30.550
units in the network.
Gru cells don't.

76
00:03:30.551 --> 00:03:32.860
They expose it's full content without any control.

77
00:03:32.880 --> 00:03:36.280
So Jerry use have a less complex structure and are thus more computationally

78
00:03:36.281 --> 00:03:38.410
efficient.
We're starting to see these be used more and more.

79
00:03:38.411 --> 00:03:41.650
There are relatively new,
so when we feed that sentences into the RNN,

80
00:03:41.680 --> 00:03:42.690
it'll create an abstraction.

81
00:03:42.720 --> 00:03:46.720
The vector representation or skip thought vector sentences that share semantic

82
00:03:46.721 --> 00:03:47.920
and syntactic properties.

83
00:03:47.960 --> 00:03:50.860
We mapped to either the same or similar skip thought vectors.

84
00:03:50.890 --> 00:03:53.230
The function returns these vectors,
that's say num py or rape,

85
00:03:53.260 --> 00:03:55.480
which we can then modify via the style shift function.

86
00:03:55.510 --> 00:03:58.720
We'll take our thought vectors and modify them to match the style of stories

87
00:03:58.750 --> 00:04:00.610
using a pretrained recurrent neural network.

88
00:04:00.611 --> 00:04:03.040
The RNN was trained on a data set of romance novels,

89
00:04:03.041 --> 00:04:05.110
or each pathogen was mapped to a thought vector.

90
00:04:05.140 --> 00:04:08.710
So we're essentially computing a function that looks like this for a style
shift.

91
00:04:08.740 --> 00:04:12.700
F of x is a book passage thought vector x is an image.
Caption C is a caption.

92
00:04:12.701 --> 00:04:15.720
Stop actor and B is a bookstore vector.
We removed the caption,

93
00:04:15.730 --> 00:04:19.030
stop on the caption and replace it with the bookstore to create a book passage

94
00:04:19.031 --> 00:04:21.460
of vector.
Once we have our book passage style vector,

95
00:04:21.461 --> 00:04:24.600
we can generate the story by running the decoder function on its.

96
00:04:24.620 --> 00:04:27.370
The decoder is another recurrent neural network that given a vector

97
00:04:27.371 --> 00:04:31.090
representation of a sentence can predict the previous and the next sentence will

98
00:04:31.091 --> 00:04:34.720
run the Dakota on our passage vector and that will generate our story based on

99
00:04:34.721 --> 00:04:37.600
the image for us.
Let's take a look at what it says about this picture.

100
00:04:37.630 --> 00:04:38.920
Let's read the first few sentences.

101
00:04:38.950 --> 00:04:42.070
She was taking the man out of her mouth and she gave him a gentle shake of her

102
00:04:42.071 --> 00:04:45.580
head.
Oh my God,
I can't wait to see what happened in the past 24 hours.

103
00:04:45.581 --> 00:04:49.120
I had never met a woman before.
This thing is a pro for a small chunk of code.

104
00:04:49.121 --> 00:04:50.830
There's a lot of machine learning going on here.

105
00:04:50.860 --> 00:04:53.770
We use a convolutional neural net to compute image features.

106
00:04:53.800 --> 00:04:57.430
An LSTM recurrent neural net to encode our image into joint space and retrieved

107
00:04:57.431 --> 00:04:59.920
the sentence captions,
a Gru,
recurrent neural net,

108
00:04:59.921 --> 00:05:03.160
to calculate the skip thought vectors of those sentences.
And after sal,

109
00:05:03.161 --> 00:05:07.420
shifting an Rnn to decode our passage vector to a story that's for neural nets,

110
00:05:07.450 --> 00:05:10.870
you can run this on your local machine.
This unnecessary models are pretrained.

111
00:05:10.871 --> 00:05:14.310
For more info,
check out the links below.
And I just signed up for patriots.

112
00:05:14.311 --> 00:05:16.450
So if you guys find my videos useful,

113
00:05:16.480 --> 00:05:19.510
I'd really appreciate your support to help me to continue doing this full time.

114
00:05:19.550 --> 00:05:20.890
We subscribe,
subscribe for more ml videos,

115
00:05:20.891 --> 00:05:24.220
and for now I've got to go fix a null pointer exception.
So thanks for watching.

