Speaker 1:          00:00          Hello world, it's Saroj and let's make our own language translator using tensorflow. Today there are about 6,800 different languages spoken across the world and in an increasingly globalized world, nearly every culture has interactions with every other culture in some way. That means there are an incalculable number of translation requirements every second of every day across the world. Translating is no easy task. A language isn't just a collection of words and if rules of grammar and syntax, it's also a vast interconnected system of cultural references and connotations. And this reflects a centuries old problem of two cultures wanting to communicate but are blocked by the language barrier. Our translation systems are fast improving though. So whether it be an idea or story or request each new advanced means one less message will be lost in translation. During the Second World War, the British government was hard at work trying to decrypt the Morse code.

Speaker 1:          00:57          Did radio communications that Nazi Germany used to send messages securely known as enigma. They decided to hire a man named Alan Turing to help in their effort. And when the American government learned of their translation effort, they were inspired to try it themselves postwar specifically because they needed a way to keep up with Russian scientific publications. The first public demo of a machine translation system translated 250 words between Russian and English in 1954 it was dictionary based so it would attempt to match the sorts language to the target language word for word. The results were poor since it didn't capture syntactic structure. The second generation of systems used intro lingua. That means they changed a source language to a special intermediary language with specific rules and coded into it. Then from that generated the target language. This proved to be more efficient, but this approach was soon over shadowed by the rise of statistical translation in the early nineties primarily from engineers at IBM innovation at IBM.

Speaker 1:          02:02          A popular approach was to break the source texts down into segments and compare them to an aligned bilingual corpus using statistical evidence and probabilities to choose the most likely translation. Nowadays, the most used statistical translation system in the world is Google translate and with good reason. Google uses deep learning to translate from a given language to another with state of the art results. So how do they do this? Let's recreate their results in tensorflow at to find out the data set we'll be using to train our language translation model is a corpus of transcribed the Ted talks. It's got both the English virgin and the French version, and our goal will be to create a model that can translate from one to the other. After training, we'll be using TensorFlow's built in data utiles class to help us preprocess our dataset and we'll start by defining our vocab size, which is the number of words we want to train on from our dataset.

Speaker 1:          02:52          We'll send it to 40 k for each, which is a small portion of the data. Then we'll use the data utilized class to read the data from the data directory, giving it our desired vocab size and it will return the formatted and tokenized words in both languages. Well then initialize tensorflow placeholders for our encounter in Dakota inputs. Both will be integer tensors that represent discrete values. They will be embedded into a dense representation later. We'll feed our vocabulary words to the encoder and the Incode had representation that's learned to the decoder. Now we can build our model. Google published a paper more recently discussing a system they integrated into their translation service called neural machine translation. It's an encoder decoder model inspired by similar work from other papers on topics like tech summarization. So whereas before Google translate, we translate from language a to English to language B. With this new NMT architecture, it can translate directly from one language to the other.

Speaker 1:          03:48          It doesn't memorize phrase to phrase translations. Instead it encodes the semantics of the sentence. This encoding is generalized so it can even translate between a language pair like Japanese and Korean that it hasn't explicitly seen before. So I guess we can use an LSTM recurrent network to encode a sentence in language a. The RNN spits out a hidden state s which represents the vectorized contents of the sentence. We can then feed as to the decoder, which will generate the translated sentence in language. B Word by word sounds easy enough, right? Wrong. There's a drawback to this architecture. It has limited memory. That hidden state pass of the LSTM is where we're trying to cram the whole sentence. We want to translate. S is usually only a few hundred floating point numbers long. The more we try to force our sentence into this fixed dimentionality vector, the more lossy our neural net is forced to be.

Speaker 1:          04:39          We could increase the hidden side of the Lstm after all, they are supposed to remember longterm dependencies, but what happens is as we increase the hidden size h of the Lstm, the training time increases exponentially. So to solve this, we're going to bring attention into the mix. If I was translating a long sentence, I probably glanced back at the source sentence a couple times to make sure I was capturing all the details. I iteratively pay attention to the relevant parts of the source sentence. We can let neural nets do the same by letting them store and refer to previous outputs of the LSTM. This increases the storage of our model without changing the functionality of the LSTM. So the idea is once we have LSTM outputs from the encoder stored, we can query each output asking how relevant they are to the current computation happening in the decoder.

Speaker 1:          05:29          Each encoder output gets a relevancy score, which we can convert to a probability score by applying a softmax activation to it. Then we extract a context vector, which is a weighted summation to the Incode outputs. Depending on how relevant they are. Memory enough, pay attention. You have to buddy up spam. He sneaks past health, ammonia know as soupy center president cell memory. Enough pay attention. We'll build our model using tensorflow is built in embedding attention sequence to sequence function, giving it our encoder and decoder inputs as well as a few hyper parameters. We define like the number of layers. It builds a model that is just like the one we discussed. Tensorflow has several built in models like this that we can drop into our code easily. So normally this alone would be fine and we could run this and the results will be decent, but they added another improvement to their model that requires more code, a hundred gps and a week of training.

Speaker 1:          06:25          Seriously, that's what it took. We won't implement it all programmatically, but let's dive into it. Conceptually, if the outputs don't have sufficient context, then they won't be able to give a good answer. We need to include info about future words so that the encoder, how put is determined by words on the left and right. We humans would definitely use this kind of full context to determine the meaning of a word we see in a sentence. The way they did this is to use a bi-directional encoders, so it's two rns, one that goes forward over the sentence and the other goes backwards. So for each word it concatenates the vector outputs, which produces a vector with context from both sides and they added a lot of layers to their model. The encoder has one bi-directional RNN layer, then seven unidirectional corn. And layers. The Decoder has eight unit directional RNN layers.

Speaker 1:          07:15          The more layers, the longer the training time, so that's why we use a single by directional layer. If all the layers were by directional, the whole layer would have to finish before later. Dependencies could start computing, but by using unidirectional layers, the computations can be more parallel. We'll initialize our tensor flow session then our model inside of it. Let's see some results after training first, I'll give it this phrase looks good, and now another phrase, nope. While it's not perfect and we still have a ways to go, we're definitely getting closer to having a universal translation model, breaking it down and coder decoder architectures offer state of the art performance in machine translation. By soaring the previous outputs of LSTM cells, we can judge the relevancy of each to decide which to use via an attention mechanism and by using a bi-directional RNN. The context of both past and future words is used to create an accurate encoder output vector.

Speaker 1:          08:08          The coding challenge winner from last week is Ryan Lee. This was very impressive. He created a recipe summarizer by scraping 125,000 recipes from the web and documented it all beautifully with installation steps so you can reproduce the results yourself wizard of the week. And the runner up is Sarah Collins. Her Code Converts Scientific Papers to texts and prioritizes them by topic. This week's coding challenge is to create a simple translation system using an encoder decoder model. All the details are in the read me poster gambling in the comments, and I'll announce a winner next week. Please subscribe for more programming videos, check out this related video, and for now I've got to get a better GPU, so thanks for watching.