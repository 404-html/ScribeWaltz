Speaker 1:          00:00          Hello world, it's Saroj and we're going to dive into some reality bending research today. Well, write a program that generates an image from just the text description. There's a human condition called synesthesia where it stimulation of one cognitive pathway leads to experiences in a second cognitive pathway. So a person with synesthesia could be listening to Beethoven and it would trigger the taste of sweaty armpit in their mouth or see the color red when looking at the number nine the word literally means joined perception. Many great artists have this condition and no doubt it adds to their artistic ability. There's something really intriguing about this idea of being able to express a thought, an idea, a concept in two entirely different mediums. Could we replicate this ability programmatically? Mm, yes. Generative adversarial networks have demonstrated that it is possible to create fake but photo realistic looking images after training on an image data.

Speaker 1:          01:02          More recently, a paper called [inaudible] demonstrated that given any tech sentence, their model could then create a photorealistic image out of it. Think about the implications of that. This ability to synthesize one data type to another will eventually allow us to create all sorts of entertainment with just detailed descriptions, designers, engineers and scientists could all hashed out theoretical concepts with real time visual feedback on their thought processes, but at the same time bad actors could use this technology to manipulate people into believing something is real when in fact it's actually fake. We'll all need to get smarter to discern the difference between the two and indeed several big names are all now starting to work on ways of merging our brains with machines, which would help us do just that. Think direct thought to media synthesis. You and I are living in the most exciting time in human history right now.

Speaker 1:          02:01          So how does stack GAM work? It consists of two games. The first one is called stage one. It takes a sentence as input and outputs an image with primitive shapes and some basic colors. It's a low resolution image like what a windows phone would capture. Then the second Gan, stage two takes that low res image and original sentence as input and generates a much higher resolution version of that image by completing all the details. A popular task and machine learning is to learn what's called the density model of a distribution of some datasets. So for the density model p of x, it would accept some input and say, yes, that's x or no, that's not x x being the data we want to model. Gains have been used to learn density models. We want to be able to do two things with this model. The first is a sample from it that is imagined variations of the training data that we've never shown it before and the other is to condition on external data so that we can specify particular attributes as we sample.

Speaker 1:          02:58          We humans have no problem doing this. Imagine a pink elephant with wings. See how fast you did that? They used a set of images of birds and flowers with associated text labels as their training data. So the task was to build a conditional density model instead of answering the question, is this an hex? The question is instead, is this an ex given a why? This means we can specify particular attributes we want in the generated image, we could ask the generator to generate a bird with a yellow tail and ask Dee whether the bird has a yellow tail, so we'll be able to directly control the output of g. The first step is to encode the text description into an embedded and using a pretrained coder like word two vec. No. In our programmatic implementation we'll see that the only dependencies we'll need our tensorflow num Pi and our helper class.

Speaker 1:          03:49          If we move down to the main function, we can see a high level overview of what's happening. We first load up our skip thought model and store it in the model variable. Skip thought also called sent two VEC is the encoder part of an encoder decoder model that was pre trained on a large text corpus to reconstruct a piece of text from it's encoded vector. So we'll use it to convert our input text into an embedding. We can initialize our tensor flow session and then initialize our models inside of it. We'll need to give our model, the text embeddings the session and our predefined to batch size as its parameters. We want to extract latent variables from our techs and beddings and we could just do this directly, but instead we'll randomly sample latent variables from a gossipy and distribution with a mean and a diagonal covariance matrix are functions of the text and bedding. Why? Because the latent space condition on tax is usually high dimensional, like more than a hundred dimensions and we have a limited dataset waiting variables generated from it directly would make it hard for our generator to learn if we had much more data than it would make sense, but we don't. So this step helps us learn from a smaller Dataset. Minnie mouse.

Speaker 2:          05:12          So

Speaker 1:          05:12          for the generator a d convolutional network, the text and bedding is fed into a fully connected layer to generate the mean and the values for the diagonal in the covariance matrix so that we can use them for the independent Garcia and distribution. We'll use these values to compute a conditioning vector that we can then concatenate with a noise vector to generate an image through a series of upsampling blocks. The discriminator, a convolutional network, takes the text and bedding, has input and compresses it to less dimensions. Ultimately forming a three dimensional tensor. We also feed in the image through a series of downsampling blocks until it's a two D 10 cert. We then concatenate both tensors together and we use the resulting tensor as input to a convolutional layer to jointly learn features across the image and the text. Do you ask judge two kinds of inputs, real images with matching text and generated images with arbitrary text?

Speaker 1:          06:03          It's got two separate two sources of error, unrealistic images for any text and realistic images of the wrong class that mismatched the conditioning information. This could complicate the learning dynamics, so we separate the error sources. In addition to the real and fake inputs to d. During training, we add a third type of input, real images with mismatched texts, which d has to learn to score as fake. The discriminators goal is to maximize the probability that the image condition on the text is from the true data distribution and minimize the probability that the image is not from the true data distribution. The generators job is to create samples that full d, it minimizes the probability that the image is not from the true data distribution and we use the popular regularization term, the Kale divergence to help avoid overfitting. So eventually g from the stage one Gan we'll get really good at generating low res images.

Speaker 1:          06:55          They aren't very clear and could contain all sorts of shape distortions. Some of the details in the text input could be omitted in this first stage, so we introduce stage two again that generates photorealistic high res images. It conditions on these low res images and the texts and bedding. Again to correct any defects from stage one and extract any previously missed information in the text to generate more photo realistic detail for G. Similar to the previous stage, we'll use the text and bedding to generate a lower dimensional golf Sian conditioning vector, which we then convert to a three dimensional tensor. We also input the sample generated by the first scan through a series of downsampling blocks until it's a tutee tensor. Then we can catenate the image and text tensors into one tensor. We then feed that resulting tensor into several residual blocks to jointly encode the image and text features.

Speaker 1:          07:51          Lastly, it's fed through a series of upsampling blocks that will ultimately generate an image for d. It's a similar structure to stage ones the but with more downsampling blocks. Since the image size is a larger in this stage, once we've got some image texts payers, we can train our model in the sessions run function for the number of batches we defined. We will separately train our stage to Gan and its own session using the generated images as input. Once trained, we can use the stage to gans generator to generate some images given a text input. If we look in our saved folder after training, we can view the low res images that were generated. We can also view the High Rez generated images in their respective folder. As long as you have some images and some related texts, you can convert. Pretty much any text into a novel image using this model.

Speaker 1:          08:40          Let's do a recap. Dan's can be used to learn a conditional density model. One great application of this is converting texts to images and stack in is an architecture that can do just that first converting text to a low res image than to a high res image. Last week's coding challenge winner is Mike Mcdermott. I had some issues in my video generation code, which he made a great pull request for making the dependencies much easier to install so you can get started generating videos of your own. Great job, Mike. It was much needed wizard of the week. This week's coding challenge is to use at Gan to convert text to images using an entirely different datasets. Details are in the read me get humbling scoping. The comments and winners will be announced a week from today and the last episode of this course, please subscribe for more programming videos. And for now I've got to create the finale, so thanks for watching.