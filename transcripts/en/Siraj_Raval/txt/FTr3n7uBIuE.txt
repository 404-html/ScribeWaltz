Speaker 1:          00:00          Hello world, it's a Raj. And we're going to build a convolutional network using no libraries. I mean just num Pi, but no libraries, no tensor flow, no pie torch, none of it. We're going to look at the math behind it and we're going to build it with just num Pi for matrix math in python. Okay? And what it's going to be able to do, let me just start off with this demo to start off with. What it's going to be able to do is recognize any character that you type in, or not type in, but draw in with your mouse. So you could draw a six like that and then hit submit. It'll start working, and then it'll say it's a six. And then if you don't want you to six, you could say a letter, like a any number or letter, it's going to be able to detect slash predict.

Speaker 1:          00:41          So it's gonna be really cool because we basically were wrapping it into a Web App using the flask web framework. So it's gonna be, it's gonna be super awesome. Okay, so that's what we're going to do today. And this is our first neural network that we're building in this course from scratch. I mean, we made one in the weekly video, but this is the real hardcore convolutional network with all the layers, all the functions, everything. Okay, so let's start off with what it's inspired by. Well, it's inspired by Yan Lacoon, the genius. No, it's not. So young. Mccoon is a director of AI at Facebook. He's a total g. He is awesome because he was inspired by these original two guys right here who published a paper in I think 68 or early sixties or seventies but the paper was on the Mammalian visual cortex. And the idea they had was, and so here's a great image of it.

Speaker 1:          01:32          Let me make it a lot bigger. This has to be a lot bigger. So the idea they had was that mammals all see in a very similar way. And that way is hierarchical. So you have a collection of cells and these cells are neurons and these cells cluster and, and these clusters represent different features that are learned. Okay. So here in terms of neuroscience, they call these Clusters v One v two you don't have names for all these clusters in the brain. These clusters of neurons before it posterior, all this neuroscience terminology. But what we need to know is that at a high level, what's happening is every time you see something, a, a series of clusters or layers of neurons are being activated. Whenever you see something, whenever you detect something to pitch to be more accurate. If I detect a dog or a fee, you know, face or whatever, it's going to be a series of layers or clusters of neurons that fire, and each of these clusters are going to detect a set of features.

Speaker 1:          02:30          Okay? And these features are going to be more abstract. The, the higher up the hierarchy of clusters. You could think of it as a vertical hierarchy or even a horizontal hierarchy, what it doesn't matter. But the idea is that there is a hierarchy of features. And at the start, these features are very simple, are lines and edges. But then they get more abstracts and they become shapes and then they become a more complex shapes. And then eventually at the, at the highest level, at the highest cluster level exist the entire face or the entire dog or whatever it is. And this is how the Mammalian visual cortex works. And so what young mccuin said, and his team in [inaudible] 98 when they published probably the landmark paper of convolutional nets, which is kind of arguable I guess because it Crucell Skis, image net paper was pretty good and and I think 2012 but anyway, y'all the coons a g, I just wanted to say that he had the idea to be inspired by three things, three features of the human or the Mammalian visual Cortex, local connections.

Speaker 1:          03:31          And that means the clusters between neurons, how each neuron, each set of neurons in a cluster cluster are connected to each other and they represent some set of features. And then the idea of layering, how these, how there's a hierarchy of features that are learned and spatial invariants what does this mean? This word spacial invariants. It means that whenever you are, I detect something, whether it's, let's say we're going to fighting a shoe, right? If you see a shoe, you know it's a shoe, right? If it's a Yeezy, if it's a, you know, Adidas, whatever it is, you know, it's a shoe. It can be shaped this way or this way. It can be rotated or transform, no matter how it varies, we still can detect that it's a shoe. We know it's a shoe. So we are, it is, the way it's positioned is it's spatially in barrier.

Speaker 1:          04:17          We can still detect what it is. And so those three concepts were what inspired the birth of convolutional neural networks, programmatic neural networks, designed to mimic the Mammalian visual cortex. How cool is that? That's so cool. So how does this thing work? Let's look at how this works. So we have a set of layers, okay. And we'll talk about what these layers mean, right? What is layer a layer in each case is a series. It's, it's, it's, it's a series of operations that we're applying. Okay? So let's, let's talk about this, right? So we have some input image. So let's see. Let's see. This is the orange, that's the image, and you'll notice by the way, that this image, this is a convolutional network, by the way. This is what we're building, okay? You'll notice that this image right here or this image of the convolutional network isn't what you normally look at when you think of neural network, right?

Speaker 1:          05:09          You always see that image of the circles and everything's connected. So why is it different for convolutional networks? Because every layer and a convolutional network isn't connected to every, to every neuron in every layer isn't connected to every other neuron in the next layer. Why? Because that would be too computationally expensive. I'll go over that in a second, but the idea is that if you, if you see here, there is a part of the image that is connected. It's this little square of that orange and that is called the receptive field. Okay, I'm going to go over all of this. It's going to make more and more sense. You're going to be more confused. It's going to be, it's going to make more and more sense as I go further and further in depth here. So, so, so stay with me here. So we have a receptive field.

Speaker 1:          05:51          Okay. That is some part of the image that we are focused on. We are by focused, I mean that is the of the image that we apply a convolution operation to. Okay. And we take that receptive field and we slide it across the image. Okay? You're going to see exactly what I'm talking about in a second. I'm just going get over at a high level. We slide over the image. We are applying a dot product between our weight matrix, add a layer, and every part of that image iteratively. Okay? And so the, the reason that they look different, the convolutional networks look different is two reasons. Really. The first reason is that not every neuron in each layer is connected to every other neuron and the next layer. It's only a part of that because it would be a, to borrow from discrete math, a common historial explosion to connect every single pixel value in an image to every single pixel value in the next layer of features, right?

Speaker 1:          06:44          There'll be just a huge amount. So what we do instead is we take a part of that image and we irritably slide over it. Okay? So at a high level, you understand the sliding part, right? I think of it as a flashlight. Okay. Think of it. Think of the, uh, the filter at each layer that shines over the receptive field, that box as a flashlight, and you're shining over the image and you're, and you're applying dot products to all of these numbers. Okay? Just like that. Okay. I'm going to keep going into this. That was just the highest law. You're not supposed to understand it all yet. Okay. That was, that was very high level. We're still going deeper or go indeed, we're going deep. Okay. So check out this beautiful image right here. Isn't it beautiful? It's very beautiful. Also your beautiful for watching this.

Speaker 1:          07:22          So thank you for watching this. Okay. So I love my fans so much. Seriously, you guys are amazing. Seriously, you guys are the reason I do this every week. Okay, so I, by the way, I want to say one more thing to go on a tangent. The people who subscribe to my channel, no one thought they existed. We are programmers who are smart and we are also cool. No one thought these people existed but we exist. Okay. We are smart and we are cool. So you are amazing. Okay. Anyway, back to this. What this is is another way of looking at the network, right? We're just looking at different ways. We're looking at different ways so we can build a spatially in variant image in our head of what a convolutional network is like, right? No matter what the image is, we're going to learn to recognize what a convolutional network.

Speaker 1:          08:09          When we see one, I'm just trying to, you know, metta applying this logic to what we're learning. So what happens that each layer we are applying a series of dot products between the way major cs and the input matrix. Okay? And so what happens is, let's look at a third image. Okay. So this is the third image. What happens is we perform a series of operations, okay. At each layer. And so we could think of of differ, we could think of splitting up a convolutional network into two separate categories. The first category is feature learning, and that's what's happening at the, at the, at the head of the, the head to the middle, to almost a tail end of the network. And at the very tail end is classification. So there's two parts, there's the feature learning part, and then there's the classification part. And two for the feature learning part.

Speaker 1:          08:56          What happens our three operations over and over and over again. And we can call them convolutional blocks, let's just call it an convolutional blocks. I'm coining the term. So what happens is we first applied convolution, then we apply relu or any kind of activation. And then we applied pooling and we repeat that. That's us. That's a single block, three operations in a single convolutional block. Okay. So convolution, Relu, pooling, repeat convolution, relu pooling, repeat convolution. Relu pooling. Okay. And it's usually, you know, you have three blocks at least unless you're, you're building inception by Google, then you have 15, 15 of these. Uh, but you, you know, you have these convolutional blocks and at the very end, then you flatten that outputs into a smaller dimensional vector and then you apply a fully connected layer to it. So that means that you then connect all the neurons in one layer to the next one just because we want to then harness all of the learnings that we've learned so far.

Speaker 1:          09:54          That's why we fully connect at the end. And then we take those learnings and we squash it into a set of probability values with our last softmax function. And then we take the Max value of those probabilities. And each of these probabilities is a probability for us for specific class that it could be. And we take the Max value, let's say 72% as an we'll say, okay, well, 72% for banana and now we know it's a banana. Okay. So hopefully you get some of it, but it's very confusing still. I know we're about to go even deeper. Okay. So get ready for this. I haven't even started yet, so I haven't even started yet. Okay. So anyway, step one. So for step one, we are preparing a data set of images, right? So when you think of an image, you think of a matrix, hopefully a matrix of pixel values. If you don't think of it that way, think of the thing of it that way. Now you're thinking of an image as a matrix of pixel values, roads by columns, and each of these, um, each of these, uh,

Speaker 1:          10:53          points in the matrix represent a pixel right between zero and two 55, but it's actually better in terms of convolutional networks to think of an anemic as a three dimensional matrix. And you're like, what? No, what? It's too. No. So it's three dimension. So the first dimension is the length of the image. The second dimension is the width. And the third dimension is the depth. So wait, what is the depth? Because the depth represents the channels. And there are three channels for images, red, green, and blue. Unless you're talking about gray scale, then there's black, then there's, you know, black and white, but we're talking about color images. Okay. So there were three channels and you have these dimensions for each of the channels. So these values in each of these and each of these two d matrices for, and there are three of them, represent the,

Speaker 2:          11:38          the

Speaker 1:          11:40          amount of redness or the amount of greenness or the amount of blueness between zero and two 55 so in terms of convolutional nets, we think of images as three dimensional pixels. Okay. So I wanted to say that part. Okay. So that's, that's, that's what we think of our image as our input image and it, it has an associated label, right? We're talking about supervised learning, learning the mapping between the input data and the output label dog image, dog label learned the mapping given a new dog image. What is a label? Well, you just learned it, right? So, and we learn it through backpropagation back propagates the update weights. Remember the rhyme? You know what? It is a, I haven't wrapped yet in the series, but I will, don't worry. It's coming anyway, so every image is a matrix of pixel values. We know this, we know this are between zero and two 55 and we can use several training datasets.

Speaker 1:          12:31          There are two really popular ones there. See farther and there's cocoa and there's a bunch of other ones as well, but basically these are huge datasets and you can find similar versions of them and each of these images, their dogs, their cars or airplanes there, people, whatever, they all have labels for them. Ha handmade labels by humans, which is great for us. Okay, so that's, that's it. That's step one. Step one is to get your training data, which is your images, which are your images. Step two is to perform convolution. Now you might be asking what is convolution? Well, I'm here to tell you that convolution is an operation that is dope as f. Here's why it's dope because it's not just used in computer science and machine learning. It's using almost every field of engineering. Think of convolution as to paint buckets. You have one paint bucket, which is red, and the other one which is blue.

Speaker 1:          13:19          And what you do is just smear it all over yourself. No, you don't do that. Well you do is you take these two paint buckets and you combine them into one paint bucket and that new paint bucket is going to be a new color, whatever that combination of colors is, that's convolution. Convolution is taking two separate types of data, two matrices, and then applying. And then it's an operation that combines them. So you could think of convolution as synonymous to combination. Okay. And why do we apply? Why do we say that for convolution networks? Because what we're doing is we are combining the values for each of these layers with the input matrix. So thing of the input, uh, as that Matrix, right? And so, well, it's a three dimensional. It's a, it's, it's a, it's a, it's a three d tensor, right? But we're applying it to each of these dimensions, right?

Speaker 1:          14:05          So three of them. So just think of it as a matrix for right now. And so what we do is we take this, uh, fee. So at each layer and each layer there is a weight. So by the way, okay, so there's a lot of interchangeable terms and machine learning and it's easy to get confused here, but I want to set the record straight for a second. Wait is the same as feature matrix is the same as feature map is the same as a filter in this case and for convolutional networks, so cds or even colonel colonel is a different one. There's actually five interchangeable terms. I can see how it can be confusing, but if you get the basic idea of you have an input Matrix, which is your image, and then you'd have a set of matrices, which are your features that are learned, you know, edges, shapes, more attract shapes.

Speaker 1:          14:56          That's it. That's it. That's all it is. Matrix dot product matrices that are being multiplied by major cs all the way through that. That's all it is. Majors, he's going to be multiplied by matrix. These all the way through or just a chain of them. Okay. So what happens for convolution is we take a matrix and we multiply it by all the values in this matrix at a certain region, right? And so this is what I was talking about when I was saying we have as a receptive field because we don't just multiply it all at once. We multiplied by a little part of it. Okay? The receptive field and we slide it and we can define what that interval is. That sliding window. I know I'm talking a lot without coding, the coding is coming, believe me, the cutting is coming. But just check this out for a second.

Speaker 1:          15:33          We've got to learn this, uh, conceptually first. So we are multiplying the, the feature matrix by that input image just for every row in every column or just multiply, multiply, multiply them. And what happens is we have this new matrix that results the output and that output is considered the convulsed feature. Okay? And so what we do is we use that output as the input for them to the next layer. And we repeat the process over and over and over again. Obviously there's two more parts here. There's the activation, the Relu, and then there's the pooling, which I'll talk about as well. But that's the basic idea between convolution and that's why we call it convolution. Because we are combining or consolving the weight matrix or filter or colonel, whatever you want to call it, feature map by that input. We're combining it using the APP and using that output as the input for the next layer after activating it and, and pulling it.

Speaker 1:          16:25          Okay, so that's convolution and also, um, right. So we apply it to all of those dimensions for that, for that input matrix. Okay. And that gives us our activation map or feature map or filter, right? So many different interchangeable terms here. So anyway, so it's computed using the dot product. So you might be thinking, well, okay, I see how there is a dot product. I see how there's matrix multiplication, but how does that really tell us what features there are? I still, you're still not making the connection probably why understandably why this, these series of major operations help us detect features. Well, here's what happens. What happens is this, and here's the great thing about matrices and having several of them.

Speaker 1:          17:12          When we learn a filter or a way to whatever you want to call it, well this is, you know what, moving forward, let's just call it filter. Okay, let, I'm just saying, let's just call it filter. Moving forward for the rest of this video, when we learned a filter over time by training it on mouse mouth pictures, for example, a filter's gonna look like this at let's say at the first layer we, we learn a filter for detecting a curve that looks like this, right? This curve right here. And so what's what this filter is going to look like for the tech thing. This specific type of curve is, it's going to be a very sparse filter. That means there's a lot of zeroes except so there's all these years except for right here you see this 30 30 30 30 and notice that these values represent the shape.

Speaker 1:          17:50          They go in this direction of a shape. And so what happens is when we take this filter and perform the dot product, you know, we can involve it with whatever part of the mouse, if it's over a part of the mouse that matches that feature. Exactly. Then we, when we multiply all of those, uh, when we, when we performed the dot product between all those values and sum them up, that's the convolution operation right there. Okay. Just it's going to be a big number. Okay. And so then we know that we've detected a feature because we've, we multiplied it, sum it up, and there's a large number. And if there's not, if we multiply, if, let's say we had that receptive field over a different part of the mouse and that that curve doesn't exist, then it's going to be zero, right? Because if you look between these 30, 30, 30 values and that the equivalent, um, locations on this pixel representation of the [inaudible] image, these are zeros.

Speaker 1:          18:46          And so what happens when you multiply zero by 30, you get zero, right? So that's why it's important to make the rest of the, so the data that's irrelevant. We want it to be zero, right in the, in the feature maps are in the filters that we learn in the filters that we learn. We want the irrelevant parts to be zero and in the images. Okay. And, and in the input images. So I, so I can actually go even more into convolution, but it's not really necessary, but it's, it is super dope and it's super dope though. This is a great blog post by the way. I definitely encourage you to read this blog post. It's linked in the notebook, but the stewed Tim Tim, he goes into these, this idea of convolution and he talks about how it's applied to all these different engineering fields and he goes into, uh, the formula, the formula for the convolutional theorem is what he called w is what it's called.

Speaker 1:          19:41          Okay. And I'm just gonna go over this at a high level, but the convolution theorem is this general theorem for discrete, while there's a discrete version and a continuous version, right? The street is if there's one or zero black or white, you know, definite, uh, classes. That's something could be, whereas continuous is, is if it could be an infinite amount of values between zero and 1.5 0.2 5.7 infinity in that direction. But here's the, here's the formula for it. And so let me make it bigger just really quickly and then we'll get back to it because it's, it's really cool. But the convolution theorem states that we, and so in it, it's a general theorem that can be applied to any, any, any set of problems. But in terms of what's relevant to us is, is the convolutional theorem applied to a matrix operations. So what we can do is we can say what it, what it says is it's the input to times the kernel and it's the dot product.

Speaker 1:          20:40          It's a dot product between two different major seas. And we performed that for every value in all of those matrices. And we do that for all of the values that we have and we summed them up together. And that's what the sigma term represents. And we, and we actually express that right here, right? This operation right here, this multiplication and summation is the same thing, but it's a more complex way of looking at it or more mathematically accurate way. And also the fast 40 a transform is, is brought up by this and the fast fourier transform. Take some spatial data and it converts it into 48 space, which is like a wave form. And you see this a lot in your day to day life. Whenever you're looking at, uh, some sound, you know, you're, you're listening to some sound and you look at your MP three player and you see the waves.

Speaker 1:          21:23          That's a, that's a 48 transform happening. Uh, but I won't go into that. That's, that's for sound and audio. But anyway, it's a really cool a blog post. Definitely check it out. Okay, so back to this. So we talked about convolution, now we're going to talk about pooling, right? So what is pooling? So whenever we apply convolution to some image, what's going to happen at every layer is we're going to get a series of feature of, of, so each of the weights are going to consist of multiple images. And each of these images are going to be at every layer, there's going to be more and smaller images. So the first few layers are going to be these huge images, right? And then at the next few layers are going to be more of those, but they're gonna be smaller and it's just going to get you just like that.

Speaker 1:          22:06          Okay. And if then we squash it with some fully connected layer. So it gets some probability values with a soft Max. But anyway, what pooling does, is it re is it dense? Is it makes the Matrix, the major cities that we learn more dense. Here's what I mean. So if you, if you perform convolution between an input and a feature matrix or a weight matrix or filter, it's going to result in a matrix, right? But this matrix is going to be pretty big. It's going to be a pretty big matrix. What we can do is we can take the most important parts of that matrix and pass that on. And what that's gonna do is it's going to reduce the computational complexity of our model. Okay. So that's what pooling is all about. It's a pooling stuff. So there's different types of pooling. Max Pooling is the most used type of pooling by the way.

Speaker 1:          22:55          So basically multiply. So what happens is we, we strive, we have some, we defined some window size and then some strides size. So how, what are the intervals that we look at? And we say, okay, so for each of these windows, let's take the Max value. So for, so for uh, this one right here for six zero hate, the Max value would be eight. And so for one, three, 12, nine, it'd be 12, right? So we just take the biggest number. It's really simple actually. We just take the biggest number and we just do that for all of them. And so that, that's what pulling is all about. And so it's going to just give us that the most relevant parts of the image. And if you, if you think of these, these, these values and uh, in the, in the matrix as pixel intensities, by taking the maximum intense, the pixel with the most intensity or that the highest intensity, we're getting that feature that is the most relevant, if you see what I'm saying, it's a least opaque feature to use a term from image, um, math.

Speaker 1:          23:53          Anyway, so we, so we talked about pooling and we talked about uh, we talked about activation and so now, no, we talked about convolution and we talked about pooling and so now the third part is normalization or activation. So remember how I said how it would be, it's so important that we have these values that are not related to our image. Be Zero. We want it to be zero. So the result is zero. If the, if the feature is not detected well, the way we do that is using relu. And so relu stance were rectified. Linear unit. It's an activation function. It's an activation function. Okay. We use activation functions throughout new neural networks and we use them because it is that you can also call them nonlinearities because they increase, they make our model able to learn nonlinear functions, not just one of your functions but nonlinear function.

Speaker 1:          24:46          So any kind of function, right? That universal function approximation through. And we talked about that activation functions help make this happen. And so relu is a space is a special kind of activation function that turns all negative numbers into zero. So that's why it's going to make the math easier. It won't make the math break for a convolutional networks who apply Relu. So basically what we do is for every single pixel value in the, in the input to this activation function, we turn it, if it's a negative, we just say make a zero. It's super simple. It will be one line of code. You'll see exactly what I'm talking about. Okay. And so that's, that's, those are our blocks. So that's how our convolutional blocks work. However, there is another step that I didn't talk about that is a nice to have. And state of the art, convolutional networks always use it and that's called dropout.

Speaker 1:          25:33          So Geoffrey Hinton, the guy who invented, um, neural networks invented a feature in men at a technical dropout and would drop out. Is, is a good analogy is old people are not old people, but people who are stuck in their ways. Let me, let me, okay, so what dropout does is it turns neurons on and off randomly. What do I mean by that? That, I mean, the, the matrices for each weight value is converted to zero randomly at some layer of the network. And so what happens is by doing this, our network is forced to learn new representations for the data, new pathways that that data has to flow through. It can't always float through this neuron. And the reason we use it is to prevent over fitting, right? We want to prevent over fitting. We've borne to prevent being to fit to the data. Think of it as, you know, the older you get, the more set in your ways of thinking you're, you are right.

Speaker 1:          26:24          And so it's harder to think of new ways of, of, of thinking, right? Because you're so set in some ways. So a way to prevent that is to have a novel, crazy experience, whether it's skydiving or taking psychedelics or whatever it is. And what that does is it creates new pathways. So you're not, so you're kind of forced, your brain is forced, make new pathways, and this increases your generalization ability and you're not. So overfit, that's a very rough abstract analogy. But basically drop out is not as complex as that sounds dropped out can be done in three lines of code. So definitely check out this blog post as well that I've linked. But what it does is it just randomly picked some neurons in a layer to set to zero, right? So it's just, it's just three lines. Okay. And you can look at it in this notebook, right?

Speaker 1:          27:11          So that's, and then our last step is probability conversions. So we've got this huge set of values, right? All these little small images that are represented by this huge output matrix. And we want to take this huge set of values and make some sense out of it. We want to make probabilities out of it. And the way we do that is using a soft Max at the end. A softmax is a type of function, and it looks like this, this, this is a softmax function right here, but what we do is we plug these values into the softmax function and it's what you output a set of probability values, discreet probability values for each of the classes that we're trying to predict. Okay? And then what we'll do is given all those probability values, we'll pick the biggest one using Arg Max, the Arg Max function and num Pi.

Speaker 1:          27:54          And that's going to give us the most likely glass. Okay. Those are the seven steps of a fee. A full forward pass through a convolutional network looks like that. And so now you might be wondering, well, okay, so how do we train this thing? Well, using gradient descent, right? And when applied to neural networks, branded grading dissent is called backpropagation. Exactly. I hope you got that right. Anyway. Okay, so how do we learn these magic numbers, right? How do we learn what these weight value should be? What the features should be? Backpropagation is how we do it, right? And so we've talked quite a bit about backpropagation and gradient descent, but I'll do a little, I'll go over it again. Um, but the idea is that we have some error that we're computing, right? This is super, this is supervised learning. We have a huge, we have a human label, right?

Speaker 1:          28:45          For some data. So we put in a dog image or a bicycle image to look at the same it to relate to this image here we put in a bicycle image and the bike label, we pass it through the t, each layer. Dot. Product dot product l. Dot. Product activation function, pool dot product repeat, repeat, softmax or squash and into probability values. Pick the biggest one. And we have some prediction value. And what we do is we compare the prediction value to the out, the actual value and we get an error and we take our error and we compute the partial derivative of the error with respect to each weight value going backwards in the network. Okay? Like this. Okay. And so for regression, we use the mean squared error. If we're using linear regression regression, and for classification we use the softmax function. So remember how in the first neural network we built, and in their linear regression example, we used a, uh, we use mean squared error to compute the air.

Speaker 1:          29:38          And now we're using the softmax. So we'll take the oil, take the partial derivative of the error with respect to our weights, and then that's going to give us the gradient value that we then update each of those wait values recursively going backward in the network. And that's how it learns what those features are, what the ideal feature, the weight matrix value should be. But what about the other, uh, what about the other magic numbers? What about the number of neurons and the number of features and the size of those features? And the pooling window size and the window stride, while those that is an active area of research, there are best practices for values that you should use for those, for those hyper parameters, right. The tuning knobs of our network. And Andre carpathy has some great material on this and he's probably the leading source for convolutional networks right now in terms of um, written content and uh, yeah, I mean this is an active area of research, finding out what the ideal hyper parameters for our neural network should be and we're still learning what it should be, what, what, what, what, how we can get them rather than just guessing and checking, which is what we do right now, which is kind of like, you know, not as not as optimal.

Speaker 1:          30:45          Right? So anyway, last two things and then we're going to start with the code. When is a good time to use this? Well, we know what to classify images, we've talked about that, but you can also use them to generate images and that's for later on. That's a little more advanced. But to give you a little spoiler, a little teaser, in fact, this is in my intro to deep learning playlist, taking a convolutional network. You flip it and then you call it a d convolutional network, and then you can take some texts and create an image out of text. How crazy is that? Okay. There's also generative models where you have two networks fighting each other and you can generate new images. Whole bunch of really cool, crazy stuff you can do. But anyway, when should you use a convolutional network? Anytime you have spatial two D or three d data, what do I mean?

Speaker 1:          31:28          Well, obviously images are spatial. The word spacial implies that the space, the positioning of the data matters. So sound, you can apply to sound images or text where the, the spa, the position of the text matters, right? Because we have a flashlight, our filter, and we're involving over an image, right? But if you have some data like say customer data, where if you were to just flip the rows and columns, it doesn't matter what order they're in, they're still, you know, there's still features. So a good rule of thumb is if you swap out the rows and columns of your Dataset and uh, it's just as useful, like the space doesn't matter, then you don't want to use a CNN. It helps you do. Okay in a great, and last thing, the great example of using CNNs are for robot learning. You can use a CNN for object detection and then you can use a CNN for grasp learning and combine the two.

Speaker 1:          32:17          And then you could get a robot that cooks, which is really cool. I've got a great tensorflow example and a great adversarial network example. Okay, let's go into the code now. And so what I'm gonna do is I'm going to look at the class for the convolutional network in them pie as well as the prediction class. There's two classes here. Okay. So these are our three inputs. Pickle is for saving and loading our serialized model. What do I mean? Pickle is python's way of having a platform or language agnostic way of saving data. So you can load it up later. Tensorflow uses it, a bunch of other libraries uses it as well. None pies were matrix math. And when we've got our own little custom class for preprocessing of the data, because we don't care about that part, we care about the machine learning part.

Speaker 1:          33:01          Okay, so let's talk about arc light OCR or object optical character recognition class. In our initialized function, we're going to load the weights from the pickle file and in store and then store all the labels that we've loaded. We'll define how many rows and columns in an image load up our s convolutional network using the light CNN function with our saved weights. So assuming we've already trained our network, we load it with the saved weights from the pickle file and then we defined a number of pooling layers. Okay. So once we had that, then we can use this predict function. So given some new image will reshape the image. So was in the correct size to perform the dot product between that image and the first layer of our convolutional network. And we'll, we'll, we'll put it, go all feed it into our network and it's going to output a prediction probability for our class.

Speaker 1:          33:49          And we were returning. Okay. Super high level. We haven't even coded our CNN. That's, that's our first class. That's our prediction class. Now, now we're going to look at a convolutional network class. And what I'm going to do as I'm going to, I'm going to go over the code and I'm going to code some parts of it. So now we'll look at our convolutional network class, okay? So in our initialized function will initialize to list one to store the layers that we've learned, the weights of each layer, and then the size of the pooling area for Max pooling. Okay, we'll load up our weights, uh, from our pickle file just like this. And then we have our predict function. Now in our predict function, that's where the real magic is happening, right? Let's code what this looks like. So given some input x, we're going to feed it through all of these layers, right?

Speaker 1:          34:36          So what happens is we will say, okay, so the first layer is going to be a convolutional layer, okay? And we're going to define what all of these functions look likes, look like. But the first layer is going to be that convolutional layer. We'll feed in that first image and we'll say, okay, well this is the first layer. So it was a zero layer. We'll say border mode equals full. Uh, and I'll talk about that part later on. But that's it for that. And so what happens is x equals this layer. Okay? So that's our first layer. And then our next layer is going to be relu. So we'll say, okay, now let's apply an activation to the outputs of the previous layer. Okay? And then we'll set it equal to that. Okay. So we'll set the output from the previous leader equal to the input of this layer.

Speaker 1:          35:20          And then we keep going. We say, okay, so we've got another, uh, CNN, we have another convolutional layer and we do the same thing here. We say, okay, take the output from the previous layer. We'll define what the uh, name of this layer is, as well as the border mode, which I'll talk about the very end of this. If a border mode, which is valid. And then we say, okay, well we'll set the output of that equal to the input of this. And just keep repeating. Now it's time for us to apply a nother nonlinearity. So we'll just go ahead and apply our nonlinearity. Again, remember these are convolutional blocks. Oh, and we also want to pool. So also the, the order with which you can do this varies, right? You could do this in different ways and yeah, so I'm doing it a certain way right now.

Speaker 1:          36:07          You know, we could change it around, it would change our result, but the order, Matt, the ordering within the block can be, can be different. Okay. So, right, so we're gonna pool. It's, we're going to pick the most relevant features from, from that, uh, uh, from that output. And then we're going to perform dropout to prevent overfilling. And we're going to say there's going to be a 0.25% chance that a neuron is going to be deactivated, that we'll turn it off, set it to zero, and that's our dropout probability value. And then now we're getting into our, um, our, the second category of our network, not the feature learning part, but the classification part. And we'll say, okay, so let's flatten this layer. Let's reduce the dimensionality of all of that data. So it's something that we can then learn from. And we'll say, well, let's, let's set it equal to seven and then we'll say, I once again turn that output into our inputs here.

Speaker 1:          37:05          Okay? And so then we have another dense layer. We just, we just keep going with, or our first dense layer. And that means we are going to, it's a fully connected layer. So we're combining everything that we've learned because we're getting really close to squashing these values into a set of probability value. So we want to take all of our learnings and combine them, what they fully connected layer. And so we'll combine them with a fully connected layer and then, uh, we'll squash it now with our sigmoid or no, not our sigmoid, our softmax function. Okay? And then that's going to give us our output probability, and then we're going to say, well, which of the probabilities do we want? We want the Max one, right? We want the Max probability and we'll classify it just like that and return that value. Okay? That's the highest level. And so if you were using carrots or one of these high level libraries, this is all your code would look like. But we're going to do is we're going to look at these functions as well. Okay, so let's look at these functions.

Speaker 1:          38:07          So we'll start off with the convolutional layer function and have your notebook open with me as well. So you could go over this. The link is in the description. If you don't know now you know if you don't know now you know. So for our convolutional layer, given some input image, we're going to say, well we'll store a feature maps and the bias value in these two variables, features in bias will define how big our filter or patch is going to be. How many features do we want? How big is our image, how many channels RGB. So three. And then how many images do we have? So given those values, well the find a border mode. So a border mode. So is so when you apply it full to border mode. In this case, it means that the filter has to go outside the bounds of the input by filter size divided by two.

Speaker 1:          38:49          The area outside of the input is normally padded with Zeros and the border mode valid is when you get an output that is smaller than the input because the convolution is only computed where the input and the filter fully overlap. Okay. And they'll give us different, um, they'll give us different classification results, accuracy, results, and it's good to test both options. So what we'll do is we'll initialize our feature matrix for this layer as calm as can be, Zeros is going gonna be a bunch of Zeros. And then we'll say, okay, so for every image that we have for every feature in that image, let's initialize a Convult image as empty. And then for each channel, so doing this for each of the three channels, let's extract a feature from our feature map, define a channel specific part of our image, and then perform convolution on our image using that given feature filter.

Speaker 1:          39:37          So notice this convulsed two function. It's where the actual convolution operation is happening. This is more of a wrapper for that actual mathematical operation. So once we have that, we'll add a bias and a bias acts as our anchor for our network. It's kind of like the y intercept. It's kind of like a, a starting point for our model to exist. And then we'll add it to our list of Convult features for this, for this layer. Okay. And then we'll return that as the has our feature map. Ours are set of filter valleys, our weight matrices. And so let's look at this Convult Tootie, uh, function. So in our [inaudible] function, we'll define the tensor dimension of the image and the feature. We'll get a target dimension. And then these two lines perform this, this, uh, operation, this convolutional theorem that we defined right here. We're performing a dot product between the input and the colonel or feature for, for all of those, um, wait, now use, and then we're summing them all up and that's going to be our outputs.

Speaker 1:          40:36          And so the fast 40 ea function in num Pi does this very well. And so we can just use that as FFT two. But that's what it's a multiplication. And in summation operation. Okay. And so then we have our target value and then once we have our target value, we could say, okay, let's have a starting point and an ending point. And our target value is going to be within that range of what we want to return as the involved feature. Right? So we have some bounding box that we went to apply this to. Okay. So then, so we have that. So what else do we have? So we started off with our convolutional layer and then we had our relu. So what does real loose, really super simple relu Relu is just forgive. So for, for some matrix of Zeros, we'll go through every single pixel value in the input matrix.

Speaker 1:          41:24          And if it's a negative number, we just turned it into zero. That's it. That's Relu. Okay. And then so we have the, we had talked about really, we've talked about convolution, we have to talk about pooling. So what does Max pooling look like? So given our learn features and our images, let's initialize our more dense feature list as empty. And so here's what we do. We're going to, we're going to take the max values up, all of those parts of the input image, right? So we're going to say, we're going to say for each image and for each feature map begin by the row, the find a starting and ending point, okay. Which we defined with our pool size, hyper parameter. And so for each column, so we've got a set of rows and columns for each image. There's a notice, a lot of nesting happening here.

Speaker 1:          42:05          We're going to define started hand points for the columns as well. And then we're going to say define a patch given our defined starting and ending points, so some some bounding box. And then take the Max value from that patch using n and p. Dot. Maxx and that patch is what moves around, right? For all parts of that image. And then we returned that and we're going to store all of that in our pooled features. A Matrix right here. And we returned that as the output. And that's what we pass on in the convolutional network. Okay? So that's what Max pooling is. Okay. So we've talked about convolution, Relu, Max pooling and then drop out. So for dropouts,

Speaker 2:          42:44          yeah,

Speaker 1:          42:45          right. We have our probability value that we define as 0.25 and we just multiply it by the inputs. Okay. And that will, that's going to do is it's going to turn on or off some part of the matrix into. So by on and off, I mean zero, you'll make it either zero or not zero. So it'll have, so then our data, we'll have to learn to either be multiplied by it or find a different pathway. And that's for dropout. And then we talked about dropout and convolution, flattening, dense and softmax. So for flattening, it's just a, it's a tensor transformation. We just reduced the dimensionality of the input. Okay. And then for our

Speaker 1:          43:25          dense layer, our dentists are fully connected layer. Now this is the generic layer that you would see in a feed forward network input times. Wait, uh, and then you add a bias, right? Which is the dot product. Right here. This is, this is a dense layer. We take our input times our way out of bias. So that means we, we just performed the dot product between the full weight Matrix and the full way matrix instead of doing it at all the layers, because that would be way too tuition, computationally expensive for image data. We perform it at one fully, one fully connected or dense layer at the end. And that's a way for us to combine all of our learnings together so we can then promptly squash it with a, a softmax function. Okay. So then for our, uh, softmax layer, and then we have classify, so for our softmax layer, we will, uh, so this is the, this is the formula for Softmax programmatically speaking. Uh, but what it does is going to output a set of probability values and then we'll classify those values by taking the Arg Max, the largest probability. And that is our output. Okay. So that is our forward pass through the network. Okay. And so,

Speaker 2:          44:36          okay,

Speaker 1:          44:37          yes, that is our forward pass through the network.

Speaker 1:          44:47          So backpack, so backpropagation works pretty much the same way as I've talked about before. Several Times. Greatness and backpropagation works the same way. We take the partial derivative of our error with respect to our weights and the recursively, update our weights using that gradient value that we gradient equals partial derivative equals Delta interchangeable words. But here's a great simple example right here where we after the forward pass, we do the same thing in reverse order. So we calculate the gradient of those weights and then back and then multiply them by the previous layer. And then for our javascript portion we are taking the drawing from the user. Here's the main code for that paint window and a canvas and we are going to say capture the mouse's positions, capture all those points in that image with an event listener and they were going to say on paints.

Speaker 1:          45:34          So whenever they use actually starts moving that painting, whenever that mouse stops clicking and then the user hits the submit button, we'll save that snapshot or that image and then feed that into the network. And that's our flask APP. We'll define two routes, one for our home and then one for that image for the network. We can deploy it to the web. There's a Heroku APP. You could definitely check out the link link is in the description as well. Check out the notebook and yeah, that's it. Please subscribe for more programming videos and for now I've got to do a 48 transform, so thanks for watching.