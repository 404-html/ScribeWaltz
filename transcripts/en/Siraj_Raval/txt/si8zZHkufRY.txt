Speaker 1:          00:00          Hello world, it's Saroj and today we're going to use machine learning to help us understand our emotions. Our emotional intelligence distinguishes us from every other known living being on earth. These emotions can be simple like when you get so hype, all you can hear as Gasolina, Bye Daddy Yankee and we've invented language to help us express them to others, but sometimes words are not enough, like some emotions have no direct English translation. For example, in German vault iron, thumb kite is the feeling experience when you are alone in the woods and connecting with nature and Japanese. [inaudible] is the awareness of the impermanence of all things and the gentle sadness at their passing. Emotions are hard to express, let alone understand, but that's where AI can help us and AI could understand us better than we do analyzing our emotional data to help us make optimal decisions for goals that we specify, like a personal life coach slash therapist slash Denzel Washington, but how would it do this?

Speaker 1:          00:58          There are generally two main approaches to sentiment analysis. The first one is the lexicon based approach. We first want to split some given text into smaller tokens, be that words, phrases, or whole sentences. This process is called tokenization. Then we count the number of times each word shows up. This resulting tally is called the bag of words model. Next, we'd look up the subjectivity of each word from an existing lexicon, which is a database of emotional values for words prerecorded by researchers and what do we have those values? We can then compute the overall subjectivity of our text. The other approach uses machinery. If we have a corpus of say tweets that are labeled either positive or negative, we can train a classifier on it and then give it a new tweet. It will classify it as either positive or negative. So which approach is better?

Speaker 1:          01:51          Don't ask me. No. Yeah, totally. Ask me. Well, using the lexicon is easier, but the learning approach is more accurate. There are subtleties in language that lexicons are bad at like sarcasm. It seems to mean one thing, but it really means another. But deep neural nets can understand the subtleties because they don't analyze texts at face value. They create abstract representations of what they learned. These generalizations are called vectors and we can use them to classify data. Let's learn more about vectors by building a sentiment classifier for movie reviews, and I'll show you how to run it in the cloud. The only dependency will need is TF learn, and I'm using it since it's the easiest way to get started. Building deep neural networks will import a couple of helper functions that are built into it as well, and I'll explain those when we get to them. The first step in our process is to collect our dataset up. Learn has a bunch of preprocessed datasets we can use and we're going to use a Dataset of IMD being movie ratings,

Speaker 1:          02:53          load it using the load data function. This will download our Dataset from the wet well name the path where we want to it. The extension being pickle, which means it's a byte street. This makes it easier to convert to other python objects like lists or tuples. Later, we want 10,000 words from the database and we only want to use 10% of the data for our validation set. So we'll set the last argument to 0.1. Load data will return our movie reviews split into a training and testing set. We can then further split those sets into reviews and labels and set them equal to x and y values. Training data is the portion of our model learns from validation. Data is a part of the training process. While training data helps us fit our weights, validation data helps prevent overfitting by letting us tune or hyper parameters accordingly.

Speaker 1:          03:37          And testing data is what our model uses to test itself by comparing. It's predicted labels to actual labels, so tests yourself before you wreck yourself. Now that we have our data split into sets, let's do some preprocessing. We can't just feed text strings into a neural network directly. We have to vectorize our inputs. Neural Nets are how algorithms that essentially just apply a series of computations, two matrices, so converting them to numerical representations or vectors is necessary. The pad sequences function. We'll do that for our review text. It'll convert each of you into a matrix and pat hit padding is necessary to ensure consistency in our inputs. Dimentionality it. We'll pat each sequence with the zero at the end which we specify until it reaches the Max possible sequence length which will set to 100 we also want to convert our labels to vectors as well and we can easily do that using the two categorical function. These are binary vectors with two classes, one which is positive or zero

Speaker 2:          04:33          which isn't negative. Yo hold up factors got me feeling like feed forward. Neural nets got me feeling so upset. Only fixed size inputs like a small number sense so I dropped it and instead I used for current net which are made for sequences like typed up or written text that convert my text in to this end up feeding in. We train in word vectors that are embedded in Redmond more numerically. We can use generically a one to many mapping that upbringing to you lyrically on. We

Speaker 1:          05:00          can intuitively define each layer in our network as its own line of code. First will be our input layer. This is where we feed data into our network. The only parameter will specify is the inputs shape. The first element is the batch size, which will set to none and then the length, which is a hundred since we set our max sequence length to 100 our next layer is our embedding layer. The first parameter will be the output vector we received from the previous layer, and by the way, for every layer we write we'll be using the previous layers outputs as its inputs. This is how data flows to a neural network. At each layer it's transformed like a seven layer dip of computation will say mentioned to 10,000 since that's how many words we loaded from our dataset earlier and the aqua dimension two one 28 which is the number of dimensions of our resulting embeddings.

Speaker 1:          05:47          Next we'll feed those values to our LSTM layer. This layer allows our network to remember data from the beginning of the sequences, which will improve our prediction will set dropout 2.8 which is a technique that helps prevent over fitting by randomly turning on and off different pathways in our network. Our next layer is fully connected, which means that every neuron in the previous layer is connected to every neuron. In this layer. We have a set of learned feature vectors from previous layers and adding a fully connected layer is a computationally cheap way of learning non linear combinations of them. It's got two units and it's using these soft max function as its activation function. This will take in a vector of values and squash it into a vector of output probabilities between zero and one that sum to one. We'll use those values in our last layer, which is our regression layer.

Speaker 1:          06:35          This will apply a regression operation to the input. We're going to specify an optimizer method that will minimize a given loss function as well as the learning rate, which specifies how fast we want our network to train. The optimizer will use is Adam, which performs gradient descent and categorical cross entropy is our loss. It helps to find the difference between our predicted output and the expected output. After building our neural network, we can go ahead and initialize it using TF learns deep neural net function. Then we can call our model's fit function, which we'll launch the training process for our given training and validation data. We'll also set show metric to true so we can view the log of accuracy during training. So to demo this, we're going to run this in the cloud using AWS. What we'll do is use a prebuilt Amazon machine image.

Speaker 1:          07:20          This Ami can be used to launch an instance and it's got every dependency we need builtin including tensorflow, Kuta, lil Wayne's deposition video. If we click on the orange continue button and we can select the type of instance we want, I'll go for the smallest because I'm poor still, but ideally we'd use a larger instance with gps. Then we can accept the terms in one click. Next we'll go to our AWS console by clicking this button and after a while our instance will start running. We can copy and paste the public DNS into our browser, followed by which is the port we specified for access for the password. We'll use the instance id. Now we're in our infancy environment built with our Ami and we can play with a Jupiter notebook hosted on AWS will create a new notebook and paste our code in there.

Speaker 1:          08:05          Now we can run it and it'll start training just like that. So to break it down, there are two main approaches to sentiment analysis using a lexicon, a prerecorded sentiment or using state of the art, but more computationally expensive, deep learning to learn generalize vector representations from words feedforward nets except fixed size inputs like binary numbers, but recurrent neural nets help us learn from sequences of data like text and you can use AWS with a prebuilt. Am I to easily train your model in the cloud without dealing with dependency issues? The coding challenge winner from last week is Ludo born little architected his neural net so that stacking layers was as easy as a line of code per layer wizard of the week and the runner up is cgs. Soon he accurately modified code to reflect multilayer backpropagation. The coding challenge for this week is to use [inaudible] to train a neural network to recognize sentiment from a video game or view Dataset that I'll provide details are in the read me posts or get humbling in the comments and I'll announce the winner in one week. Please click that subscribe button. If you want to see more videos like this, check out this related video. And for now I've got to figure out what the FPI torches. So thanks for watching.