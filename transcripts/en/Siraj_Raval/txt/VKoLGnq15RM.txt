Speaker 1:          00:00          Hello world. It's it Raj. Geoffrey Hinton is one of the godfathers of deep learning. In the 80s he popularized the backpropagation algorithm, which is the reason deep learning works so well, and he believed in the idea of neural networks during the first major AI winter in the 80s while everyone else didn't think that they would work, and that's why he's awesome. But anyway, Hinton recently published a paper on this idea of a capsule network, which he's been hinting at for a long time. Hinting Hinton was hinting, yeah, he's been hinting at this for a long time in the machine learning subreddit and talks that he's been doing, but a few days ago he finally published the paper for it. So I was very excited to read it and talk about it. But anyway, but it offers state of the art performance on the m and ist a Dataset, which is the handwritten character dataset.

Speaker 1:          00:48          It's kind of the baseline. You probably know about it if you've done any kind of AI before, but it's those handwritten characters classifying them as the digits that they are, and it become delusional networks at this. And convolutional networks are the state of the art. So it's a really exciting time right now. And what this video is, is me talking about what the state of the art algorithm currently is, the convolutional network, all the developments that have happened in convolutional networks. Then we'll talk about capsules and how they work, and we'll end the video with me going through the tensorflow code of a capsule network, right? So here's an image of a convolutional neural network. And if we use a standard multilayer perceptron with all the layers fully connected to each other, it would quickly become computationally intractable because images are very high dimensional, right?

Speaker 1:          01:36          There's lots of pixels. And if we are continuously applying these operations to every single pixel in an image and every layer, it's going to take way too long. So the solution to this is was to use a convolutional network. And this was really popularized by young Macoun who is now director of AI at Facebook in the early nineties but the convolutional network looks like this, right? So first we have an input image, right? The input image has an associated label. So if it's a picture of a car like we see here, then it's going to have an associated label, which is car, right? So picture of a car and then car. And you do that for an entire Dataset. And what a convolutional network will do is it will learn the mapping between the input data and the output label. And so the idea is that eventually you'll give it a picture of a car after training and it will know, hey, that's a car because it's learned the mapping.

Speaker 1:          02:26          When we first feed this network a car, it's going to first be applied to the convolutional layer. And so in the convolutional layer, it's basically a series of Matrix multiplications followed by a summation operation. And I have a video on how convolutional networks worked, really in depth that I'm going to link to in the video description under more learning resources. But right now I'm going to go over it at a high level, right? So in a convolutional layer, it's kind of like a flashlight. It's which is being applied over every single pixel in the image. And it's looking for the most relevant parts of that image. And so it's a multiplication and a summation operation. But basically the convolution layer will output a feature map. The these, this feature map represents a bunch of features that it's learned from that image, right? So a set of features that are represented by matrices.

Speaker 1:          03:12          And so once we have these features that are learned by this filtering operation, then we're going to apply a non linearity to it, like a rectified linear unit, for example. And when we apply a nonlinearity to it, it's, it serves a lot of purposes. The first purpose is so that the network can learn both linear and nonlinear functions, right? Because neural networks are universal function approximators. But this for, for rectified linear unit in particular, uh, one of the reasons for using that as opposed to the other types of nonlinearities is because it allows to be is because it solves the vanishing gradient problem during backpropagation. So when we forward propagate through a network, we are performing a series of operations. We find the output a class probability, we compare that to the actual label, we compute an error value and we use the error to compute a gradient.

Speaker 1:          04:03          And a gradient tells us how to update our weights as we back propagate it through the network. And what the, what really does is it says it helps solve the vanishing gradient problem because sometimes as a gradient is back propagating, it gets smaller and smaller and smaller so that the weight update is, is smaller and smaller. And this is not good. We want, we want a big way update, right? We don't want that weight update to vanish. And so relu helps prevent that, that gradient from vanishing. And so at the end of this convolutional block is the pooling operation. So let's say you have a matrix, right? And inside of that Matrix of pixel values, you have a bunch of numbers, right? For Pixel intensity is between zero and two 55 and what what pooling does, like Max pooling for example, is it says let's create sections for all of these different pixel values and let's only take the maximum pixel value from each section and propagate that forward.

Speaker 1:          04:56          So it's a smaller section that's propagated forward and this and this increases and it speeds up the training time. And so if we look at comp nets, they're actually, you know, anyone can really implement a convolutional network these days, right? With libraries like carrots, which are very high level, anyone can implement, implement a very powerful convolutional network and just a few lines of code where each line of code corresponds to a layer in the network. So it's actually, it's, it's very easy these days to do that. And a lot of people can do that nowadays, right? So once you define your network, each layer with its own line of code, then you can compile it, you can define the optimizer and the, and the loss function. You'll train it with the fit function and then you can evaluate it on your testing data. So I've got this huge graphic here of the modern history of object recognition, which I'm not going to go into in detail here, but it's definitely something to check out after you watch this video because I've got the link to this on my slides in the video description, but it's a really detailed image of the history of convolutional networks across all the different images, all the different image net competitions, and all the improvements that have been made.

Speaker 1:          06:07          But I will go through some that I think are really, really significant. So one of the first improvements to CNNs w, uh, was the Alex net a network, which was in 2012. Right? So there were some key improvements here. So the first one was an introduced three Lw, which I talked about, which helps prevent the vanishing gradient problem. It also introduced the concept of dropout. So dropout is a technique where neurons are randomly turned on and off in each layer to prevent over fitting. So if your data is too homogenous, it's not going to be able to classify images that are similar but different because it's, it's to fit, it's overfit to your training data. So dropout is a regularization technique that prevents overfitting by randomly turning neurons on and off. And by doing this, the data is forced to find new pathways to go through, and because it's forced to find new pathways, the network is able to generalize better.

Speaker 1:          07:02          It also introduced the idea of data augmentation. So convolutional networks, as awesome as they are, they're really bad at detecting they're really bad at classifying images if they're in different rotations or if they're upside down. It's got to be in the kind of exact spot that it was trained on or very close to it. So what Alex Net did, or the w, so what the author of Alex and that did is he input, he fed in different rotations of images into Alex net than just the single rotation, and that made it able to generalize better to different rotations. Lastly, it was a deeper network. So they just added on more layers. And this improved the classification accuracy. After that there was VGG net, which was a major improvement. And the really, the only big difference there was them adding more layers. That was it really a after that there was Google in that, right?

Speaker 1:          07:55          So for Google and as it looks like this, but convolutions with different filters, sizes are pro were processed on the same input and then can concatenate it together. So in a single layer, rather than going through just one convolutional operation or set of operations, remember it's multiplications and then a summation operation. It's several of those together, right? So it's multiplying some multiplying, some multiplying some, and then it takes the outputs of all of those and then it can catenate those together. And that's what it propagates forward. And by doing this, it allowed it to learn better feature representations at each, at each layer. Right? So, and then there was resonant and so resonant was, the idea behind resonate was, well, if we just keep stacking layers, is the network get better every time? And the answer was no, it's good up to a certain point.

Speaker 1:          08:47          And then if you add more, there's a drop in performance. So what resonant said was, okay, we know that there is some optimal limit to the number of layers we should add. So after every two layers, let's add this element wise addition operation. So it just added this operation and this improved gradient propagation. This made backpropagation easier and it helped further improve the vanishing gradient problem. And after that there was dense net and dense net propose, uh, connecting entire blocks of layers to one another. So it was a more complex connection scheme. So, um, there are some patterns here, right? So there's some patterns here for all of these networks. The networks are designed to be deeper and deeper and there's also computational tricks that are being added onto these convolutional networks like relu or drop out or batch normalization that improved performance. And lastly, there is an increasing use of connections between the layers of the network.

Speaker 1:          09:42          But Hinton said, okay, there is a problem with convolutional networks. So remember that convolutional networks learn to classify images, right? So in the lower low it's Lael. So in the lowest layers of a convolutional network, it's going to learn the, the lowest level features of what it's seeing. So for dogs for example, in the lowest layer, it's going to learn the edges and the curvature of your ear and that the curvature of the dog's ear, maybe like a single tooth. And then as we, as we go up the hierarchy as in, as we, as we go to the next layer, and then the next layer, each of the features that it's learning are going to be more complex. So in the first layer of their edges and the next layer they're going to be shapes and the next layer they're going to be more complex shaped like an entire year.

Speaker 1:          10:28          And then finally in the last layer, they're going to be very, very complex shapes like the entire body of a dog, for example. Right. And this, this is very similar to how the human visual cortex works. We know for certain that there is some kind of hierarchy happening. Whenever we look at something that's, there's this hierarchy of neurons that are firing in order. When we tried to recognize something that we see that that's the high level of what we know. We don't know the exact intricate details of how the routing mechanism is between layers, but we do know that there is some kind of hierarchy happening. Tween each layer rights. So there is a problem with convolutional networks though. There are two reasons. First of all, sub sampling loses the precise spatial relationships between higher level parts such as a nose and a mouth, right?

Speaker 1:          11:16          So it's not enough to just be able to classify a nose and a mouth, right? It's like if you have a nose in the left corner of an image and then you have a mouth in the right corner of an image and then you have eyes at the bottom, you can't just say, oh, it has these three features. It must be a face. No, there's also a spatial correlation. The eyes have to be above the nose, which have to be above the mouth, right? So, but sub sampling or pooling loses this relationship. Uh, and also they can't extrapolate their understanding of geometric relationships to radically new viewpoints. Like I said before, convolutional networks are really bad at detecting an image if it's in any kind of different position, right? If it's rotated, if it's upside down, if it's to the left, to the right, it's gotta be in the kind of general, uh, position that it was of the images that are trained on.

Speaker 1:          12:05          And this is a problem, right? The idea is that instead of invariants, we should be, we should be striving for Echo variants, right? So the original goal of sub sampling or pooling, it's the same thing. Sub sampling or pooling is it tries to make the neural activities in variant to small changes in, in viewpoint, right? So, uh, what that means is no matter what, rotate, what position or rotation, some images in the neural network responds in the same way. The data flow is the same, but it's better to aim for ECQUA variants. That means that if we rotate an image, the neural network should also change. It should also adapt to how it's learning from this image or how it's processing. This image. So we need, so what we need is a network that's more robust to to changes in two changes in transformations of how images are positioned and we need, we also need networks that are more easily able to generalize, right?

Speaker 1:          13:01          That's, that's just the general thing. And all of Ai, we need algorithms that are more able to generalize to data that it's never seen before based on what it's trained on. And also there's one more thing. There's a 66 day old paper on convolutional networks that just came out, which is a pixel attack for fooling deep neural networks. Basically, the authors found that they could tweak just a few pixels in an image that otherwise looks exactly like a, a perfect classification, like a dog, that it would, that it would predict perfectly, but by just changing these few pixels, they found that the entire network's classification was, was really bad. And this is a problem, right? That's the, that's just not how it should be. That's, it doesn't make sense that it's that it's that separable twin attack. Right. If we're thinking about, if we're thinking about creating self driving cars, right?

Speaker 1:          13:49          These huge machines that are flying across the road and they're, they're using computer vision to detect things, they can't be susceptible to these kinds of a pixel attacks, right? They've got to be very robust. So Hinton introduced this idea of the capsule network, and here is an image of it, but the kind of the basic idea that Hinton had was the human brain much must achieve translational in variants in a much better way. It's got to do something other than pooling. And he posits that the brain has these modules that he calls capsules, which are really good at handling different types of visual, visual stimulus and in coding things in a certain way. So CNNs do routing by pooling, they route how data is transferred across each layer by this pooling operation, right? So if, if we input an image, we apply a convolution, a convolutional operation to it, and then a nonlinearity, and then we pull it based on what the output of that pooling layer is, the image is going to go in a certain for the next layer, right?

Speaker 1:          14:52          Uh, but it's going to hit certain units in the next layer based on pooling. But pooling is a very crude way to route data, right? There's gotta be a better way to route data. So the basic basic idea behind a capsule network is it's just a neural network where instead of just adding another layer, right? So usually we're adding different types of layers. It instead it nests a new layer inside a layer. So that's all it is. It's where instead of saying, okay, we've got this single layer neural network, let's add a different layer. No, inside of that layer, let's add another layer. So it's a nested layer inside an inside of a layer. And that nested layer is called a capsule, which is a group of neurons. And so a typical layer of neurons or units becomes a layer of capsules. So instead of making the ne the network deeper in the sense of height or I guess you could even say with, it's making a deeper in terms of nesting or inner structure and that's, that's all it is basically.

Speaker 1:          15:51          And this model is more robust to transformations in terms of rotation or how this image is, I mean it is chief state of the art for m and ist, which is a really, it's a big deal. We'll see how it scales later on to to you know, really, really huge datasets. But for M and ist, that's, that's, that's a pretty big deal. So the capsule network has two really key features and the first one is layer based squashing. And the second one is dynamic routing. So in a typical neural network, only the output of a single unit is squashed by a nonlinearity. Right? So we have a set of output neurons and the based on the output of each, we apply a nonlinearity to each of them. So instead of applying a nonlinearity to each individual neuron, we grouped these neurons into a capsule and apply nonlinearity to the entire set of neurons.

Speaker 1:          16:40          The vector of those neurons. And so when we apply nonlinearity, it's, it's to the entire layer, right? Instead of individual neurons and also implements a dynamic routing. So instead of, uh, so it's replaces the scalar output feature detectors of CNNS with vector output capsules, and it replaces Max pooling by routing by agreement. So each of these capsules in each layer, when they forward propagate data, it goes to the next most relevant capsules. So it's kind of like a hierarchical tree of nested layers inside of layers. And the cost of this new architecture is this, this is the routing algorithm, right? So basically the difference here, the difference, the key difference here between a regular convolutional network is the forward pass has an extra outer loop, right? So it takes far iterations over all the units instead of one to compute the output and the data flow looks a little, is a little more complicated because it's saying for every capsule that's nested inside of a layer, apply these operations, whether it's a soft max or a squashing function.

Speaker 1:          17:49          And what it does is it does make the gradient harder to calculate. And the model may suffer from vanishing gradient on larger datasets which could prevent it from scaling and becoming the next big thing. Right? I mean in the paper they only applied it to em and ist not hundreds of thousands of images. But I suspect that this network is going to scale really well because a, it's Hinton and you know, if Hinton says something's going to work, it's probably going to work. Okay, so let's go ahead and look at some code to see what this looks like. So what I found is this tensorflow and implementation of capsule net, which is still in progress. I mean this paper is relatively new, but I found this code that we can look at a right now. So this, this was built in tensorflow, right? So in this layer, notice how it's only got two imports, your num py in tensorflow.

Speaker 1:          18:36          So it's a really clean, it's a clean architecture here. So it's got this capsule, convolutional layer, right? So it's going to have a number of outputs, a kernel size, you know, these hyper parameters like striding. And so it's got this if l statement where it says, if we're not going to do the routing scheme, build it this way, but if we are, build it this way. So let's look at both. If we have our routing scheme, which we should have, it's going to say, and we'll start off with a list of all of our capsules and then we'll iterate through the number of units we specified and we'll say, okay, so for each, uh, let's go ahead and create a convolutional layer, like a standard tensorflow, convolutional layer, and store that inside of the capsules, this capsule variable. And then we'll take the capsule variable and appended to the capsule list, right?

Speaker 1:          19:21          And so eventually we're going to have all of these convolutional layers inside of this capsule slayer. So note, so this is how it's nested, right? So, and once we, and once we have all of these layers, we'll concatenate them together to proof. We'll concatenate them together and then we'll squash them with this novel nonlinearity function. And so, but if we do have this routing mechanism, which we wish the paper had, we'll say, okay, let's again, we'll create a list of capsules and then for however many outputs we want, let's create this capsule class added to the list and upended and then return a tensor with the shape, right? So if we look at this capsule, uh, implementation right here, this, this class, basically this is the routing mechanism right here, right? Just the one that we saw right here, this routing algorithm where it's saying, okay, let's go through, let's go through, uh, let's go through the number of iterations we want and apply these operations to them.

Speaker 1:          20:17          So notice that there is a nonlinearity here that the paper talked about, which is not Relu, it's, it's a novel nonlinearity, which looks like this. This is, this is kind of interesting. This, this nonlinearity that they used, that they found, uh, was good for applying to a group of neurons rather than a single drawn, right? So for Relu we apply it to a single neuron, but when we're applying a nonlinearity to a group of neurons or a capsule, they found that this a nonlinearity works. The works worked the best for them. And so that's what, that's what, uh, is, is happening here. So that's the capsule layer. So once we have the capsule layer, we can import it here into this capital network, and then we can go ahead and build our architecture, right? So we'll start off by saying, okay, so we'll start off by saying, okay, for each layer we have a primary capsular and we have a digit caps layer.

Speaker 1:          21:08          Let's go ahead and add our a capsule, a capital layer to them. So these are, these are both capsule layers, right? So for each of these layers, there is a nested convolutional network inside of each of those layers, which is a capsule. And then inside of those capsules, the nonlinearity is applied to it. The, the novel squashing function that we saw in the paper. And then at the very end, by the way, at the very end of this network, after it's, it's applied these operations to each layer with each capsule, it applies this decoder algorithm. So, um, and the paper, they found that at the end of the network they could just use a decoder to reconstruct a digit from the digit caps layer. So after it's applied this first set of a capsule, this first set of capsule operations, then it then it can reconstruct the input from that learned representation and then apply a reconstruction loss, uh, to improve that learned representation.

Speaker 1:          22:08          And that's what this is. This is the reconstruction loss that is used to, uh, learn that representation and improve that representation over time. So, uh, this and tensorflow implementation is a work in progress. Uh, I've got a link to it in the video description has lost my slides. Uh, but if you want you can train it. I've, I've, I've been training it as well, and you just need to, all you have to do to train it has just run python, train.py and it'll start training and you'll see the loss decrease over time. Please subscribe for more programming videos. And for now, I'm going to learn more about capsules. So thanks for watching.