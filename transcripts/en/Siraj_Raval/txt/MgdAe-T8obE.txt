Speaker 1:          00:00:03       Hello, we're all did SAROJ and welcome to this live stream of my new 100 k a youtube channel livestream. I'm very excited for this. Thank you guys for a hundred k subscribers. By the way. I am very happy and we are the fastest growing artificial intelligence community in the world. Okay, so today we are going to be talking about Disco Ganz and the name is pretty interesting, right? But it has nothing to do with disco balls. But Q, the disco mute. No, I'm just kidding. No, we're not going to do that. We're going to be building a model that is, that consists of six different neural networks now. So it sounds complicated and it is, but, but it's going to be really easy because I'm building a very bare bones version of it and it's going to be in tenser flow. Okay? And you're, you might be asking, why should we be doing this?

Speaker 1:          00:00:51       Saroj what is the point of this? I see purses and I see shoes. Uh, now here's, here's, here's the cool thing about this. Okay. Using Disco Gan, this is a new type of generative adversarial network. I mean there's paper literally just came out last month, like last month. And it's very exciting because I was just going over this paper and trying to make sure that I understood all the details before I did this stream. And now I've got a good, uh, I've, I've dropped it pretty well. So I'm excited to share with you what I've learned. So this paper came out not last month but two months ago, but it was called learning to discover cross domain relations with generative adversarial networks. Okay. And I'm going to answer questions in a second. Let me just explain this first. But the, the, the idea behind the paper was, so the word disco comes from discover, so, right.

Speaker 1:          00:01:37       It's like disco discover, cross the main relations. So the idea was we have two different image datasets. One of them is let's say purses and the other one is a set of, so let's say shoes. And what we can do using disco Gan is we can generate images from one dataset in the style of another. So here you see exactly what it can do. Like here's two sets of examples. The first set of examples is saying in a fully trained district, this Gauguin on these two datasets, purses and shoes, given some novel purse, you can say, generate this purse in the style, generate a shoe in the style of this purse. Right? So this, this is really interesting, right? So it's style transfer essentially, but it's not like the style transfer that we've seen before. So recall that we've, we've talked about style transfer before.

Speaker 1:          00:02:25       Uh, but it was more like, you know, Mona Lisa, uh, you know, starry nights style transfer. The style transfer that we did before was basic. Okay. Basic Aif. It was essentially like an, like an Instagram image filter where we just have, if we consider two images to be to, uh, uh, matrices of pixel values, and of course, you know, it's gotta be bad results, but you know what I'm talking about, right? We have a starry night picture and then we have some image and we just overlaid that style onto that other picture, like Van Gogh or whatever it is. And that's, that was a naive approach, right? We, why is it naive? Why is it naive? Because we're not, we're not focusing on some subject. We're not saying like it. The machine doesn't actually know like this exact Mona Lisa is all we want to transfer this style too.

Speaker 1:          00:03:17       Let's just transfer everything, right? So what this Gauguin does, it is, it is, it's like smart or style transfer, right? Because what it's generating isn't like a purse and it's kind of like molded into a shoe. No, it's an actual, it's an actual shoe. It looks like a shoe. It, it looks like it would be something that just wasn't generated. Right. It looks very realistic. Like a style in and of itself. And Gans are really good at this. They're really good at this. Like insanely good. This is a cycle again, video, right? Notice how we've, we've transferred the style of a zebra to a horse, but it's not like we're making everything black and white. We're just making the horse black and white, which is incredible. It's just incredible. This technology is incredible and it's just moving so fast. So I'm really excited to talk about this.

Speaker 1:          00:04:08       And what I found is that it's actually very intuitive to, to understand, okay. Uh, so we, let's, and here's the other thing. So we humans, we do this very well, right? Cross domain relations. If I have a, you know, if I've got a black suit, I know that what goes well with it, the cross domain, the domain of suits, what's the cross domain relation? What's the other domain that's related to this domain? I could say, you know, some dope like leather boots would go well with it or something like, um, we're just able to do it well or like Burger and fries, like this type of Burger. It goes well with this hyper fries, but can machines do it? And the answer is yes they can because we are able to make them do it. So we can frame it as a conditional image generation problem, right?

Speaker 1:          00:04:54       Given one image purse con generate a new new image condition on a different dataset. And that would be like shoes, right? So we've done, we've talked about conditional probability distribution before, right? Probability of x, given why rather than the probability of x alone. So that's essentially what it is. That's one way of for us to frame it. And in terms of use cases, games, design, science, engineering, anytime you want any kind of visual feedback of what you're thinking, just imagine, just imagine a a text or voice interface or you say, I want to take this three d model that I have is cad model and I want to apply all of these features to it and an a disco Gan that's been generally, that's been trained on all of these different datasets. We'll be able to say, okay, here it is. Just like that because it's already been trained, right?

Speaker 1:          00:05:46       So the, the actual generation process doesn't take forever. It's a training process that takes forever. So there's a lot of possibilities for pretty much any a science, engineering or art especially, uh, any field where there is some kind of creation or discovery involved. Perfect for it. And Games of course. Well, any kind of virtual world, there is so much possibility. Think, think beyond the purse and the shoe. Okay. That's just the example that we're using. Think big. Okay. Think big. Okay. And that, that is, that is where the money is. That's where the opportunity is. That's where, that's where it's, that's where it's, that's where it's at. That's what I would be doing if I was, um, doing, you know, a startup or whatever. I would be looking at ways of taking this technology and applying it to real world problems that people encounter. Okay. So that's the high level. So let me answer two questions and then we'll get started with the code. Okay. So the first, uh, question that is going to, I'm going to answer once I pull up my question window is,

Speaker 2:          00:06:55       uh, just logging in.

Speaker 1:          00:06:59       This is pretty cool. Uh, let me, let me make this bigger. The fun definitely needs to be bigger,

Speaker 2:          00:07:03       but we no longer support this version of your browser. That's what happens when

Speaker 1:          00:07:11       it happens. Okay. So we've got 324 people watching. Let me see who in the room. Why do you use particularly tensorflow? That's a great question. Why do you use tensorflow? So the reason I use tensorflow and up, and you're right because the, the, the authors of the paper did this in Pi Torch. Pi Torch is getting really cool. It's getting really, uh, used by a lot of cool people. Remember that Pi Torch in five minutes video that I just made, Yann Macun shared that on Facebook and it got like a Ha. I was like, oh my God, y'all look good dude. That guy is so cool. Anyway, okay. Um, Geez status. So yeah. So why don't I do an intention flow because this course is intensive low and I didn't want to just, you know, drop Pi Torch on you guys. All of a sudden we got, we've got to stick to tensorflow for now, but I promise you Pi Torch is coming. Tensorflow is great. Okay. It's not, it's not a bad, it's not a bad framework. It's just, you know, it's what we're used to and we want to focus on the algorithms right now instead of syntax. So that's what I'm going to keep using tensorflow. Let me answer one more question and then we'll get started with the code. Uh, the other question is,

Speaker 1:          00:08:19       the other question is how do you select, oh no, this is a great question. What is the best way in resource to learn algorithms? What is the best systematic way to learn machine learning? Best Resource. I know it's your videos and I'm already following them. So, okay, those are two separate questions. The first algorithms is the best way. So I have a great video on this. It's called how to succeed in any programming interview. I give you a great study guide to get your data structures and algorithms skills on point. So check that out. In terms of machine learning, guests, my videos, this playlist, I have, you know, several courses on it. You Udacity has several great machine learning courses. Uh, but yeah, that's what I would say. You Udacity, my courses, you know, just together great resources and if you want really in depth and you have the time and you've got the, you've got the motivation to not to, to, to learn without needing it to be entertained whatsoever.

Speaker 1:          00:09:09       Uh, which I need to be entertained. I just entertain myself with music and stuff. Uh, then probably the deep learning book, uh, by Ian Goodfellow and Yoshua Bengio. Okay. Uh, but more than that, you know, I, I'm beginning to, you know, it's, it's something that I believed I believed in my head for a while, but I've never actually said it. Um, because it's so, uh, radical, but textbooks are just no stop, stop reading textbooks. It's not about the textbooks we live in the age of, uh, you know, the Internet and Twitter and all this stuff. We don't need to, we don't have to have, we don't have, we don't need to read books. Book should read themselves to us. Right? Right. We need an easier way up. It's just data and algorithms, right. Learning is just a process of data and algorithms and we need to get it into our head and what's a great medium for that video?

Speaker 1:          00:09:58       Uh, and all sorts of, um, you know, very condensed, you know, stuff. Anyway, okay, let's get started with this. I'm going to talk about most of the code and I'm going to code the parts that really matter. Okay. So the first part is, okay, the first part of this is going to be us defining our dependencies. Let me make sure the text is big enough. Big like a boughs big, like a boss. Okay. Oh there's, there's uh, one more thing I want to talk about. So how should we structure this architecture? Let's do a thought exercise for a second, right? We want, we have to image data sets. Okay. We have to image data sets and we want to generate an image in the style of another. Okay, so conditional image generation. Well, one naive way we can think about this is an encoder decoder architecture, right?

Speaker 1:          00:10:43       So we have some image and then we encode it into a Lenten space, right? We have some latent space in coding and then we decode it again. But when we decode it, we keep that light and space and we condition it on the other image data set so that that image is the one that's decoded is exactly like a reconstruction of the original, but it's conditioned on the other dataset. That's a naive way of doing it because what that would do is like the original style transfer paper, it would, it would just make everything styled, but it wouldn't be that you wouldn't be that specific transfer that we're doing right now. So that's, that's the naive way. It's like a camera filter on Instagram. A better way is what if we had two encoder decoders write one for each image data set and to both conditioned on each other, that would be good and that it would make it backwards compatible.

Speaker 1:          00:11:26       But it's still naive, backwards compatible in that we could generate us chew in the style of a purse and a person that's stylish shoe, but the dope boy of doing this, and that's the one that we're going to do is doing to encoder decoders in an adversarial Tom context. Let me talk about what this looks like, but before we show, I show you the architecture. Let's define our dependencies and our datasets. Okay. So those are some questions that I wanted to answer. Uh, and I'll answer questions later. But let's start off with this. So we first bridge the gap between Python two and three with the future, which is totally necessary a lot of the time. Okay. Is that for things like the print function, whatever. And then we're going to use [inaudible] for saving files. And then num Pi for matrix math map plot live is our Handy Dandy visualization library for data.

Speaker 1:          00:12:12       We're using tensorflow for machine learning and uh, this is a custom class. These two are custom classes for generating data, using what's called a golf Sian mixture model, which is a really cool idea that I haven't talked about in this course and I'm excited to talk about that. Uh, and then we have a progress bar for, for training. And then, uh, we're also going to be using the TF slim library, which I haven't also used before, but don't worry, it's just, it's just tension flow. It just tentraflow neatly wrapped, neatly wrapped, similar to Carrie Ross. I'll talk about the differences. And then we have our, uh, this distributions class that's going to represent batches of statistical distributions. Uh, that's going to help us generate our data. And then lastly, uh, this graph replace a method which is going to let a function, which is going to let us, uh, to regenerate a network, uh, with replaced 10 stores.

Speaker 1:          00:13:05       And I'll talk about why that's that. That is okay. If, so, those are our dependencies and let's start defining our hyper parameters, right? So what are our hyper parameters will be four. We define our hyper parameters. We need to know how many networks we're going to use, right? We're gonna use a one, two, three, four, five, six, six different networks. This is what our architecture looks like right here and move out of the way just like that. So this is what our architecture looks like. Okay? It's kind of confusing if you, if it's your first glance. So like I've been looking at those for a while. It's actually not confusing. So here's how it works. We have four generators. Each of these boxes is a generator. This one, this one, this one, this one. And then we have two discriminators discriminator, one discriminator too. Okay.

Speaker 1:          00:13:48       And then what is going on here? Well, each of these generators, let's just look at these two on top of your right, like right here. Okay, this one is an encoder and this one is a decoder. We can just think of it that way. That's, that's how we're framing these generators. Okay? Okay. So it's an encoder and a decoder. And it's the same for this one. It's an encoder and a decoder. And then we have two discriminators. So these generators are both encoders and decoders. Okay? So what am I talking about here? So we'll give it an image. Let's say it's this, uh, this blonde woman, and then we will encode it and it, so what do I mean by encoding? So why don't we normally think about encoding. We're thinking about taking some image and converting it into a latent space. So rights and representation of that image.

Speaker 1:          00:14:30       And so that's exactly what we're doing here. Uh, but we are a conditioning it when we encode it on the, uh, separate, the other image data sets. So let's say we had the blonde woman, but then the other image data set was dark haired women. So this would be a mix of the two, right? What, what Incode is going to be a mix of the two. And then when we decode, we're taking that conditioned, uh, image that's generated and we're reconstructing the original. We're trying to best reconstruct the original. And what happens here is then we can measure the loss between the reconstructed image and the original image and we'll use that loss and feed that to the discriminator. So the same thing is happening for this encoder decoder para, but it's the opposite. So we, so for this one, it's image. Let's say it's like data set one, two Dataset to, for this one, it's from data set to data set one as in instead of generate a shoe from the purse, get generate a purse from a shoe.

Speaker 1:          00:15:21       So it's, so it's like the backwards version of it. It's like vice versa, you know, like the, the opposite. We do the same exact thing, but, but the opposite, what this does and the reason we have this is so that we can do it both ways. We can generate images in both, uh, crossed up in both domains. Just as well. And we feed both of those losses to our discriminators. Okay. So we have two separate discriminators for each of them. So these are both generative networks. So you might be thinking, wait a second generators, they're just, they're just going to be continuously upsampling. Right? Well, when we decode your, you're normally thinking about doing the opposite of what you just did before. So for an encoder decoder with, uh, with two convolutional networks, you would think that for encoding with convolutional nets, you're, you're downsampling. And then for the Di Carter, you're upsampling, but in this case, because they're both generators, we're essentially continuously, uh, upsampling. So that is weird. Generating an image, right? And then we're going to continue, we're going to generate again from that image. So it's just like one big huge generator pair, just continuously generating. Okay. Uh, and so that's, that's how that, that's how that is. And we can use different things. We can use convolutional nets for images and we can use, uh, all sorts of things like that. So

Speaker 1:          00:16:35       that is that. And now what I want to do is I want to talk about the data generation part. Okay? So let's get to the data generation part.

Speaker 1:          00:16:48       Okay. So for the data generation part, I'm going to talk about how we're going to do this. Okay. So the first step is, oh, the hyper parameters. So those are our models, right? So these are our hyper parameters for those models. The first, and I'm going to answer questions in five minutes. So the first set of hyper parameters, standard hyper parameters, and they apply to our, uh, uh, generators and or discriminators. These are general use cases, right? Because we're going to have some weight sharing happening between these models. Some of these, some of these models like the two first generators share weights. And what this does is it, it's just a lot. It's just improved across the main generation. So then we have discriminator, you know, uh, hyper parameters and then generate our hyper parameters. And then I have this inference network. So the reason we call it an inference network, it's just a generator. It's just the generator. But we're calling it an inference network just for like,

Speaker 2:          00:17:40       uh,

Speaker 1:          00:17:41       having that difference. So we don't get confused, right? So we can just call it generator again, but we want to call it something different just so we don't get super confused because there are a lot of terms, right? Encoder, decoder, generator, adversary discriminator. So there's like a lot of terms a, but it's two g, two generator pairs, like January janitor, January, January two. Discriminators. That's it. Okay. So these are our hyper parameters and these are the hyper parameters that were implemented in the original paper. Okay. So then, uh, the next step is first who save our results to our folder, our disco again folder whenever, whenever we're done. So we'll create a directory for that. Okay. So then how are we going to generate this data? Are we going to use images? No, we're going to use a more simple dataset because it's, uh, and I'll show you the results from the image generation and, and stuff. But for this, you know, just to, to get, uh, grokking of the architecture, we're going to do some really simple datasets and that's what we're going to generate it. Okay. Let me show you what it looks like when we generate it, but we're going to use something called a golf Sian mixture model to generate this data. Okay. So when, what we're going to do

Speaker 1:          00:18:44       is to, is we're going to generate data using a golf Sian mixture model. Okay. And these, there are two data sets, right? There are two datasets. There's the Red Dataset and there's the green dataset. And we want to generate that using gosh, in mixture model. So what is this, what is a golf swing mixture model? Well, recall that Gosselin is a, is the bell curve, right? It's a, it's, it's a distribution between two uh, numbers. It's a distribution in two and an interval pair. A mixture model uses several golf scions. Okay. He uses several golf seems to generate data and some of that data might be, uh, it uses those distributions, settled distributions to generate data. And why do we use Garcia mixture models? Uh, and so like this, this data right here, it was generated using a handoff Sian mixture model and we saved it from a previous training session. This is what our data is that will look like. And then what we want to do is we can consider these two different domains and we want to find the intersection. So whenever we're going to generate new images that are intersected, uh, between these two domains, so where would they be? They would end up being like right here. So here's what it would look like. Once we're done with this test Dataset, it will look like this. Let me just pull up this pdf.

Speaker 1:          00:19:54       You see these red values,

Speaker 1:          00:19:57       he's red values. That's what we're going to generate. And what this shows is it, it is a proof, is that proof that we can generate new images that are across domain, that is, they apply to both different both datasets. And then we literally just replace the datasets with something more interesting. And we can do all sorts of cool things, but let's focus on the architecture right now. Okay. So, yeah. So the way that's, that's how we generate this using gosh and Mr models. And you might be wondering, well is there another way for us to generate this without using, gosh, your mixture models. Sure. You could just use a, you know, a golf Sian alone. But uh, what this does is it's, it's better for clustering. It's better for finding relationships between the two data points. If we're using a mixture model, there's already an inherent relationship between the two data points. And what this does is it, it, it gives us some ground truth from which we can generate data from. Okay. So this is the formula for a gosh, you mixture model and here's what it looks like. We say the probability of x, that is some data set of elements. What, what we're going to generate depends on these three variables right here. The mean, okay. The uh, the mixing weights and then the, the wolves. This one Alpha, which is the, no, so the variance, the,

Speaker 3:          00:21:19       yeah,

Speaker 1:          00:21:22       I wrote these down. So this is going to be the variance, the mixing, wait, and then the, and then there's the sigma, which is the, the density function. That's what it is. Okay, so this, okay, so what we say is we say, and we're going to write this out programmatically, what we say is sigma notation for k were, wereK is a number of elements in our dataset. We first applied this alpha, this Alpha value, which is, which is a mixing wait, which we can decide beforehand. And then this end looking thing means the golf Sian distribution. So to golf, Sian, probably distribution of x, which is our data set of elements given our mean and our variants. Okay, what were the square of our variance? And so we need these three values and mean the barriers and the mixing weights to be able to generate a data points in this distribution. Okay? So that's what we're going to, we're going to do here. But let me first answer some questions here. Uh, okay.

Speaker 3:          00:22:13       Okay.

Speaker 1:          00:22:14       So how to determine the number of hidden layers required for a particular deep neural net question. Okay. So that's a great question. How to determine the number of layers in a deep net. The way we do that is by looking at what has been done before in papers and then reimplementing that because hyper parameter optimization is still a field of discovery. We need more algorithms to learn how to learn hyper parameters, right? One way to do this manually and since you just have the same model and each model has a different set of hyper parameters and then we run them on the same dataset and see which one works best, right? In parallel distributed training, right? And then see which one works best. And that's essentially what a lot of machine learning is. It's just like different people trying out different sense of hyper parameters. And there's not like one true way of doing it, right? It's use case dependent. And I think it definitely needs to be better, right? It needs to be more efficient. It needs to be something that we learn to learn. And that's actually, you know, um, my video is coming out on this, so it's literally in a few days, uh, which is really cool. And then two more questions.

Speaker 3:          00:23:29       Uh,

Speaker 1:          00:23:35       looks like I lost my connection here. Uh, so it's taking a well, but I see it, these two older questions. So can we use tensorflow for you? Can't. Absolutely. I mean, a lot of great researchers use tensorflow for research and it's, uh, it's been used a lot, but, but although Pi Torch has been used a lot more now for, for, for research by serious researchers. So. Cool. So that's it for the questions. Let me get back to the code. Okay. So for the GAA Sian mixture model, um, we're going to, we're still, we're still good, right guys? Where's the lighting hunger? Great. So let's go ahead and do this. So the first step is for us to generate our, uh, intervals, right? Defining our intervals that we're going to generate this gossipy mixture model from. So each of these sets of intervals is a, is we can consider them components, right?

Speaker 1:          00:24:23       We call them components, right? So from zero to zero, from two to two, from negative one to negative one from one tonight and one, right? So when we define that using the lambda or anonymous function, uh, given x. So we say we want to create a num, py, IRA from these intervals. Okay. And then we're going to store that in means. So that's our set of means. We convert that to a list and it's just a, and why do we convert it to a list? Because we can perform a lot of interesting operations with the list. The list is a great data type in, in python. And then we say, well, here's the, here's our standard deviation that we defined. We want it to be point, 0.1 and then our variances or we're going to say, so the, so the eye function of num Pi returns an identity matrix, which is a two d array with, with, uh, one's on the diagonal and an zero everywhere else.

Speaker 1:          00:25:10       And so why do we need that identity? Major CS are necessary to generate variances, variances. So we take the identity matrix and we multiply it by the standard deviation for every value in the means. So for all of those components, and that's going to give us our, our variances, right? Then we're going to, we're going to use our PR, we're going to generate our prior, which is this value right up here, which is our prior beliefs. So when it comes to probability theory, we have a prior, which is your belief before you come into a problem. So before you examine a problem, it's a Bayesian approach, right? Bayesians believe in priors, frequent his stone. It's a huge debate. It's an ongoing debate. So it's a holy war. Uh, but yes, I, I followed the Bayesean Basie and logic. Okay. So for priors we're going to say one divided by the means for all of the means is going to give us our prior value.

Speaker 1:          00:26:03       Okay. Uh, and so then we're going to use those values that we just computed to generate our, generate our golf Sian mixture model. Okay. And then once we have our golf Sian mixture model, we're going to sample from it. Okay. So we've defined what this model looks like, this, this architecture and this, this, this structure from which we can now generate data from. We've defined our parameters for it and we can generate data from it. And so this is our helper function to do that. And we'll get to our dataset. Okay. And then we will save the data set and I, and which I visualize and I showed you guys and then we're going to plot it showed you guys, and then we're going to store the samples and the labels. So this is interesting. So is we have samples, the labels, uh, but we're not going to use the samples, they're just there.

Speaker 1:          00:26:46       So we have just, and so we can think of this as x and y points, right? The samples or the x points and the, and the labels or the y points. Okay. Uh, and so we'll do that twice for two different datasets. We have one data set that we call x and we had the other that we call Z, right? Two different datasets. And so we all have both of the datasets. We just repeat the process twice and then we can sample from them. Right? So we have eventually we'll take both a non pyre raise for both sets of data and we'll have our x Dataset Nrz did. I said, okay, so that's our data and now we can start defining our networks, our model. Okay, so they, they looked at this problem, right? And so they were considering like what type of model should we use?

Speaker 1:          00:27:28       And we had this, we just had this interesting thought exercise earlier on in this live stream where I talked about, well should we use an encoder decoder? Should we use to in Cody cutters, should we use to encoder decoder is within our adversarial approach. They had the same thought process, right? And so this happens a lot in research where we are like thinking, well what if this would be the way I know it would if this would be the way, well let's just try out all the ways and see what works best. That's exactly what they did. So they, so they tried out one very simple model, another model, and then they came to this model that we're going to hit them it and they found that this far outperformed the other models and this is just a thing in machine learning and deep learning where it seems like the more complexity we add to a model, the better the results are.

Speaker 1:          00:28:12       Generally, obviously there are going to be anomalies, no pun intended when it comes to anomaly detection, but they're going to be an old lease. But generally the more complexity we add, the more we chain these models together, the more interesting the results are. Wavenet is a great example, a pixel RNN pixel, CNN and a path net. They're a bunch of examples, but that's what our brain is, right? We don't just have a convolutional network, we have a convolutional net for our eyes and we have a recurrent net in our hippocampus and we have feet forward nets happening probably in our somewhere else, but we have all sorts of networks that are happening, right? As we, when we combine them, we get an interesting result. And when the network observes itself and it goes through this loop of self observation and just leave the loop just keeps going faster and faster. That's when we get consciousness. But that's a whole different thing. Godel Escher, Bach, lots of philosophy there. Okay. So where were we? Consciousness. No. Yeah. Okay. This, this go again. Just go again. Okay. So here we are. Uh, okay. Where were we? So this is another image that they had in the paper where they said, okay, we have our, our datasets. And so let's talk about the failure cases, what we don't want. So if we look here at this and make this bigger, I know I answer questions. I know I haven't enhanced your questions in a while. Uh, so,

Speaker 3:          00:29:32       okay.

Speaker 1:          00:29:33       So sometimes sometimes the internet doesn't work, so that's okay. So where was, what we don't want is we don't want the images, the same image to generate a dif, different images. We want us a straight image to image mapping. What do I mean by that? Something that is predictable. So we know that, uh,

Speaker 1:          00:30:04       that there is no failure case where we give it to different types of images and it will generate the same image, even though these are two different images, right? We want variety, we want novel images to be generated no matter what kind of image we give it, right? We don't want the same mapping. So those are failure cases that it's, and it's a good practice to think about. Before we were building models, what are possible failure cases? What, what do we don't want to happen? Okay? And it's good to just as a thought exercise to map those out to, to, to put those down on paper. Okay? So that's what that is. And so now what we're gonna do is we're going to define these networks, okay? So let's define these networks. So remember, we have six networks, okay? And the way that I'm going to define these is we're going to use tensorflow slim. Now, this is just beautiful, right? So I've never actually used tensorflow slim in a, in a, in a video before, but it's, it's actually, you know, it's a, it's a very beautiful, uh, a library of tensorflow because, because, uh, there are, it's very compact and dense. So let me, let me answer two questions before start starting to build this.

Speaker 1:          00:31:16       Uh, let's see. One great question is

Speaker 1:          00:31:24       how can we make money with this? Okay. I mean, yeah, there's, there's a lot of potential here. What are some problems where people right now need to visualize something but they, they can't, they, it takes a lot of effort. The barrier to entry is very high. How can you reduce the barrier to entry for say, designing or visualization? Cad engineers are very well paid experts and there are very few of them and it takes a lot of time and energy and skill and practice to be able to generate images with cad. What if you use a generative adversarial network to let anyone generate images with a text query? Okay. With a simple description, boom, billion dollar idea, one more billion dollar idea. What can I think of on the spot here? When it comes to a special effects of know. Here's, here's a great one. When it comes to video effects, when it comes to after effects or Photoshop or an editor, an editing program, it's a bunch of effects, right? But none of them really use generative adversarial networks. Imagine the effects that you could create with this. It would blow whatever they have that's like, you know, some kind of linear combination under the hood, out of the water. You would just dazzle people. Okay? So that's where we are right now. Okay. So anyway,

Speaker 3:          00:32:38       yeah,

Speaker 1:          00:32:39       hairstyle transfers. Not happening today. I know, but uh, eventually it will happen. By the way, if you want to, here's, here's what it is. A Dye your hair with the silver stripe here, send me the picture on Twitter. I'll retweet it. Okay. Just do it. It's going to be dope. Uh, cool. So where are we? Everything about me is open source, guys, my hair, all about everything. It's all open source. Take, take what you need, take what you need. Okay, so the first thing is the generator. So we have two generators here. And remember we're calling one generator and we're calling one inference network just for the, just for a difference. We could call a generator too, but they're both generators. Okay? So in this case, we don't have images, right? We don't have images. So we're not going to use convolutional blocks. We're going to use fully connected layers because these are just numbers, right?

Speaker 1:          00:33:24       This is, these are just numbers who they are. This is, this is a set of feedforward networks. What they set a fully connected layers. Okay? So, uh, how many layers do we want? What we define that right here with number of layers. So the great thing about tensorflow slim is that we can define a set of layers by one line of code. What do I mean by that? We can say slim dot repeat. So given some data set z and then the parameters for what we want the network to look like, we can say, okay, so z equals h. Okay? So h is now, it's our data stored in h. So we'll say slimmed RRP, take the Dataset, take the data that we input, the input data, and then we want this many number of layers type of lyric, fully connected with this many hidden, uh, nodes. And then this is the activation function. We want to apply to it. Relu and it does that, that number of times. And then we had at the very end, one last fully connected layer and that's returned the output of that. And that is our prediction would, in this case, our generation, what we've generated. And it's going to be a set of numbers in this case, right? Because these are numbers that we're inputting. And so that's the great thing about tensorflow slim. And if that, if that doesn't make sense, then just like

Speaker 1:          00:34:39       I forgot, there's no internet. So, so that's what that is. Okay. So that's the first step. And uh, we're going to do this twice, right? So we're going to do that for our generator and we're going to do, that's for our inference network. We do it twice to set the fully connected networks and that's gonna eventually generate our data, right? So that's it for our generators. And then for our discriminators, our discriminators, we're going to use a fully connected layers as well. Okay. So both of these are going to use fully connected layers and

Speaker 1:          00:35:10       we're going to, for the reason we concatenate x, which is our hand put data with one is to approximate the log data density. And why do we want that? Well we are just out putting a, uh, we're out putting a binary value, right? Whether it's, whether it's real or fake, whether the image is real or fake. And in this case real or fake is if it is, uh, if the, if the generated image image is uh, from the true data distribution, which is the original image data set, or it's from the not true data distribution, which is something that's

Speaker 2:          00:35:42       uh,

Speaker 1:          00:35:44       a mix of the two. So we want to optimize for the mix of the two, right? So we'll minimize the opposite. That's what our discriminator does. It, it tries to judge if something is real. But real in this case is you can't tell that it is a mix of the two. It just looks like it's a, an image of its own and it comes from both distributions and you would think that it comes and he thinks that it comes from both distributions, but it's actually a novel distribution. I'm mixed distribution. So if it's a bad, if it's, if our, if our model is bad, it's going to be able to detect that this sloppy purse looking shoe thing is actually fake, right? It's, it's from a novel distribution. It's not as intertwined with the other tooth distribution sets. So say liberal fake, but if it's really well generated in that it is balanced in terms of the distributions that it's generated from that you can, you can't even tell then it's real. Okay. So that's what, that's what we mean by real in this case. Real versus fake. Okay. So then, uh, yeah, they're there for the discriminator. There are also a set of fully connected layers are both feet forward. And then we're going to output a TF dot squeeze for the generated data. And then we say the reason we're using squeeze is because it removes the dimensions of size one from the shape of a, of a tensor. Uh, cool. So that's it for our

Speaker 1:          00:37:03       generator. Okay. And our discriminator is plural. Okay. So then I could code these out, but let me answer some questions to see what the deal is here. Uh, who else? We got? Q and Q and. A. All right, so we have some people has it for tonight. So here's what the Q and a is. How can you use deep learning? How can you use deep learning for detecting a spoofed frank fingerprint? A spoofed fingerprint. So you would have to have a Dataset of,

Speaker 1:          00:37:34       oh, interesting. That's an interesting problem. So it depends on who's the, who's the spoofing that we are trying to predict, right? If it, if it's one person, it's very easy, right? Because you just have a bunch of samples of their real fingerprint, right? So it's a supervised learning problem, a supervised classification problem, right? So we have a supervised classification problem, left one person's fingerprint, and then you would just say, these are all the real ones. So you train it on the real ones where they're labeled railroad real. And then ideally you could, here's what I would do. I would take those real images and then I'd apply some kind of, uh, some distribution on those images so that it would create some buried version of them, some, you know, so maybe even use the generative adversarial network to generate fake but realistic looking images of that Dataset, but then label them fake. So then you, you've tried at home what's real and what's fake and given a fake one, you could just say that that's fake. Okay. Okay. So then two more questions.

Speaker 2:          00:38:29       Uh,

Speaker 1:          00:38:32       can I use a Gan for anime related purposes? Yes, you can. So literally, literally on my Internet's not working right now, but literally just search anime Gan like one word and you will find a, I swear to God or result on get hub for this. I was actually looking at this, but it generates animate characters using, using the generative adversarial network. And then one more question is, uh,

Speaker 1:          00:38:56       what approach should I follow to get a job in this? So that's a good question as well. That's a good question as well. So a good approach to follow if you want to get a job in this is whether you're in college or whether you are, you have a full time job. You need to find the time to learn this stuff. So that's step one. Find the time to learn this stuff. So dedicate some set amount of time every day to just study this, whether it be papers, whether it be code. Here's the, here's the best way, besides consuming content, my videos, besides consuming content, you need to try to reimplement papers. That's going to be really hard at first. But eventually, like anything, you're going to get better at it. Whenever you keep doing, you get better at whether it's kickboxing, whether it's machine learning, whatever you keep repeating, you get better at. That's how our brain works. So just keep looking at code, keep writing out code, keep trying to read them and papers and eventually before you know it, you won't even realize it. You're like, oh, now I'm an expert. It happens really fast, but you have to put in the time and the effort. That's all you need with the Internet. Okay.

Speaker 1:          00:40:01       I might freestyle at the end depending on how I feel. Okay, cool. So back to this. So I was going to code this, but honestly there is a lot like the Internet's not working so I'm just going to look at this and it's all good though. So here's how it is for our last functions and our networks, we define the functions for our networks, right? We've defined them there. They're done. Now we can actually start implementing our functions. So, so check this out. Okay. So check this out. First we're going to define placeholders for both of our data sets, right? We have x and we have a seat. Remember we generated both using a Gaussian mixture model. Those are, those are both of our data sets. We've got a data sets, let's feed them into our model. That's the next step. So now we'll define both far generators. And remember we defined them both respectively. One is the generative network and what is he? Inference. Network. Okay. He's our incoders, right? So these are incoders. They are, where are they? Where are they? They are right here. One the blue one right here, and this yellow orange one right here. That's what we've just defined. Okay. Those are our encoders in coder generators. Okay. And then

Speaker 1:          00:41:10       now talk about more use cases after this we've got both of our encoders and now we're going to define our discriminators, which were the two discriminators on the side here. Okay, so and did to generate these, we gave it the two respective data sets, right? Z and x and then our hype and then our hyper parameters that we defined forehand. Then we have our discriminators and we're going to use data network x and then data network Z, right? We have two different discriminators and we'll just, what we're going to feed it,

Speaker 1:          00:41:39       what we're going to do is we're going to feed it are the outputs of our generators, right? We wanna, we wanna feed it the output, right? What have we generated? I, let's see if it's real or fake. Let's discriminate if you're real or fake. So that's what we feed both of these. And the reason we use graph replace here on are our discriminators a is because it is going to help us calculate our loss function. Because we need two of these. We need, it's essentially a copy in the computation graph so that we can do this next step and you'll see why we create two discriminators and then two copies of them. So then we could then, uh, compute the soft plus function on all four of them. It's just too discriminators. But there copies of each. And if soft plus function is taking the law of probability of the exponential, the exponent of our features plus one, which is an activation function.

Speaker 1:          00:42:30       Okay, so we, and that's what the soft plus function I'll tension does. So we have four a sigmoid is that we've calculated. Okay. And then we'll take those values and we'll use those c. And this is why we will just add them together for the original and then the copy to Compute d code or loss. And then our encoder loss for both discriminators. So we have the laws for both discriminators and then we will combine them to have one big huge discriminator loss. And we say let's get the average of the encoder loss minus the average of the decoder loss. So we take the difference between the two. What is the difference? What is the difference here? Okay, between what is the original image and what is the generated image? Original data distribution generated data, data distribution, find a difference, minimize it. We went there to be no difference.

Speaker 1:          00:43:30       So it looks very similar statistically speaking, to hit to the discriminator. And that's gonna be our discriminator loss. Okay? And then we've defined our loss function, redefine our discriminators and then two of our generators. Now what do we have to do? We did you find our generator loss and then our two next generators, right? So that's what we're going to do now. So now we have two more generators and these same deal. We've got an inference that work and are generative network. And what do we feed it in the output of the previous generators, the incoders px and Qsi. You remember these were right up, where were they? Right up here. See Peak px and Qsi. We'd, if we feed those right into our next set or coders, and what does that look like? That looks like, that looks like this decoder, one decode or two. That's it. We define all six of them. See how simple that was. We define all six of them. Now where was I?

Speaker 1:          00:44:33       Where was I? Lots of TF warnings. Warning, warning. Okay, we've got a generator generators, we've got our discriminators and now we want to compute the generator loss we have are discriminated or loss. Let's compute the generator loss. So what do we do? We compute the sum of squared errors. All right, so we have generator one, the compute that computed the generated the ultimate generated product, right? That's what rex Z, the reconstruction of z and reconstruction of Xr. Those are the ultimate outputs of our network. The ultimate outputs, what that mixture of both image data sets and we want to compute the loss and that is our predicted output. The generated output minus the original Dataset. So the difference squared and the average. So the sum of squared errors for both datasets Casi and then cost x. And then we compute the advantage loss, which basically ties into discriminator loss into the generators loss.

Speaker 1:          00:45:33       But finding the average of those, all of the points in that vector, the average of all those. And we use that to compute the generator loss, which is one times the, uh, advantage loss plus one times the cost plus one times the cost of the other image. So two, two costs for two image data sets and that's gonna give us our final generator loss and I'll answer questions right after this. Uh, what after I get done with this block right here, so then, uh, we are, we are going to want to update our, our weights, right? We want to use our gradients to update our weights. So that's what we do. We say we'll use the get collection method to say within TensorFlow's computation graph, I want you to get these values that are, that have the trainable variable. And then we define what that variable is, inference generative discriminator.

Speaker 1:          00:46:27       And we define those up here. We define those appear seat, the scopes. That's how we call the scopes really easily. Very, very, very uh, good, uh, programming syntax. Syntactic sugar is the, uh, official term for it. Very, very cool term. Thank you. Francoise Chalet for continuously using that term. Okay, so, uh, creator of chaos. So the, so now let's optimize. We've got our, we've got, we've defined our networks, we define our loss functions. Let's optimize, we'll use Adam for grading dissent to optimize it, and then we can minimize both losses, the minimize the loss for the generator and then minimize the loss for the discriminator using the generators loss and the discriminators loss. And then what are the values that we want to update when we can compute our gradients? What do we want to back propagate drone network? Well, these are the, these are the variables that will, that will point our point tensor flow to the memory location of where we these networks are. Okay, so that's that part. And let me talk about the training part. Now we're, we're done with that part now where the last part is the training part, but let me answer some questions. Let's see. Okay. So

Speaker 1:          00:47:40       can again be used for audio generation? Yes. Gains can be used for audio generation. That is a novel field and not many people have attempted that before. Wave Net was, it was a great attempt at generating audio in the style of someone else. It didn't use a generative adversarial network though. So there's a lot of possibility here that there's so much you could do with audio generation generating speech of the style. Somebody else generating music and the song. That's a great one as well. Generative adversarial networks for music generation. How cool would that be? Han Zimmer. Han Zimmer music. Okay, two more question questions.

Speaker 1:          00:48:20       Saroj is wrong about textbooks. Here's the thing. People have different learning styles. People have different learning styles in the age of the Internet where our attention is split 300 x, right? We pay, we split our attention. I saw some statistic recently where it was like even 10 years ago, our spa, our attention was split across maybe two or three different domains across the entire day, but now it's split across 300 different domains. Twitter, Facebook, snapchat, life, girlfriend, boyfriend. This that. There's so many things, right? So a textbook requires a lot of attention and a lot of focus, but we live in an age of instant gratification. Let's be real. Right? So we want that instant gratification. We get that from everywhere else. We get that from, you know, mindless entertainment. We can get that from education to that's all I'm saying. But if a textbook works for you, it works for you. Right. But I'm just saying that if you are trying out the textbook for learning and it's just not clicking, there are other ways. It's not like the way to learn. Two more questions. Can we detect movement in video with Gan? Yes, you can. Detecting movement is a vague term, but detecting the direction that someone is moving. Sure you can. You can generate,

Speaker 3:          00:49:39       okay.

Speaker 1:          00:49:39       Different possibilities of given some set of frames, what is going to be the possible next frames than possible next range and then and then take those generated frames across all the generated frames. Like he moves this way. He was this way, it moves this way. Vectorize all those vector vector, vector vector a vector, and then compute the distance between those vectors, perhaps a co-sign, uh, the coastline distance or you know, one of, one of the distance functions. And then,

Speaker 1:          00:50:09       and then multiply those by some distance vector and you can see a set of possible distances and also get the most likely distance. That's never been done before. I just, I just made that, uh, can you suggest me an NLP books Raj? Uh, an LP. Stanford has a great course. Michael Collins is a great, my favorite NLP professor, he's a professor at Columbia who had now has an online course on Coursera and a, a textbook NLP textbook. Um, bishop, if you, okay, if you want a textbook then and uh, bishops, uh, pattern recognition book is great. Great section on NLP in there. One more question. This is fire. I know it's fire. What is the, can you show us how to do facial recognition next? Yeah, I will eventually. What happens when none of the Ganz generated images can be classified as a positive match within one of the initial data sets?

Speaker 1:          00:51:17       What, uh, what happens if none of the generated images from again can be classified as a positive match within one of the initial data sets? Oh yeah. So if you're Gan is bad, like it's not predicting, well then you tooth, I mean, there's several things that could be happening, but it's usually going to be two things. One, you got to change your loss function, try out a different loss function, right? And your loss function is just not working. And it's a very common thing with gans, although a Wasserstein gans definitely improve on this. Uh, so that's one possibility change a loss function, but the other is change your Dataset, add more data and more data. Okay. So that's it for that. And so now let's look at the training. And for training we're going to say, let's begin our session. And then inside of our session we're going to say stable from both data sets. So from our hex state of saffron, from Rosita Dataset, we use our shuffle function to sample some random values from them. And then we're going to say, okay, so for each of those values, for each of those values, so we're going to do, we train this six network model simultaneously. So it's happening all at once, right? It's not like we have some session and we have another session. We've announced no, it's one session and we train them simultaneously. So we say for, uh,

Speaker 1:          00:52:33       for each of those data sets, let's compute the, let's compute the, uh, let's minimize a loss for both our discriminator. So this is for a discriminator feeding hand, both, both sets of data x andZ , and then for our generator feeding in, again, both sets of data. And so you might be thinking, well, shouldn't this be generated or loss? These three components make up the generator lost the advantage of loss, the cost of x and the cost of Z. Recall up here. See, these are the three components. We could just say January generator loss as well, but, um, this is how they implement it in the paper when in Pi Torch, interestingly, I mean, but you could say to January or loss as well. Uh, this isn't it. There's no reason to do this specifically. Uh, so let's, uh, let me just compile this whole thing and, and, and see, see what's good. See what happens. Bop, Bop, Bop, compile, compile, compile, compile, and then we get to the end. Boom training. Just like that training, training, training. 2% now too. So he's going to take a while. So I'm gonna stop it before my computer gets crazy. Crazy. Like Charles Barkley. No Way. Charles. Gnarly. GNARLS Barkley. I was a great song. Where was I? Oh, okay. So open this. Let's look at our results. The results are going to be saved. We're not, we're not showing them in the tension flow graph. They're gonna be saved. And

Speaker 1:          00:54:19       so check out this, check out our loss function. It's like not doing that thing where it has to go down. It's like going, it's like going wildly crazy for both the generator handed discriminator. So that's just, that's generative adversarial networks for you. Although I'm going to talk about Wasserstein Gans and show how they improve on this, but like Tuck to sell, right? So we have our true samples and then we have our inferred samples. So the inferred samples right here showed that the combination of the two and so it just plots that it just creates those novel red points in from, from the two initial data distributions and

Speaker 1:          00:55:07       yeah, more or less simple simple idea there. But uh, that's what the paper was all about. And remember like when reading papers, it's really like, let me just, let me just like, I think this will provide value. This will provide values to you guys. So just kind of go for it. Like how I read papers really quickly. So the first thing I do is I definitely read the abstract and the introduction because those are super simple, right? It's just, it's just like, you know, motivations and why they're doing it. Look at the image. Usually like 90% of the time I'll be able to, you'll be able to Grok what the entire paper does just by reading the first page. And a good paper should be like that. You should be able to understand at a high level what is happening without having to look at the entire thing, right?

Speaker 1:          00:55:45       That's what the abstract is meant to do. Okay. And some people say you read the abstract at the end rather than at the beginning. Um, but, and so it depends. If you are trying to just get a quick idea, quick and dirty idea of what the paper is, then I read, read the abstract at the beginning. Uh, but some people say read it at the end, you know, all around. I would say that's a bad idea. I just, I, this is how I read papers and it's worked very well for me. So I'm going to speak on my own experience here. So, so that's the first step. And then so then they'll talk about their methodologies and bill talk about usually a diff, like many different methodologies and many ideas that they're talking about. But what you can do is you can go right to the meat of what it is.

Speaker 1:          00:56:28       And if you read a lot of papers, you'll get better at this because right here in the talking about notation and architecture, talking about a single Gann with reconstruction loss, this is irrelevant to the actual, um, the, the model that they built. But if you keep going, you'll find our proposed model. This is the actual model. So you can just, if you want to, you could skip the rest and then just read this. And which is what I did at first. It's the first pass. I'm like, oh, this is, this is what they built. And then we'll talk about their experiments and they'll have many, many different steps for the experiment. And it just kind of like go through it and you just want to get to the results. You want to know why did it, why they made the paper, what's the model they used and what are the results and once, so that's how I look at it.

Speaker 1:          00:57:11       And our first pass I was going. So I go all the way down here and I see the results and see can I see that while they got state of the art, you know, in, in these areas. And, and their conclusion is like how they would improve. Once you got that first pass and the first passion take you like maybe, I don't know, it takes me, it took me like 25 minutes, 30 minutes first pass. Then if you want to then go back and really like read through all the details and once you read through all the details, then look at the code. And there's an, a good paper always has associated code. If you, if you are writing papers, you need to be publishing code. Okay. You because you have to help other people out this stuff. So that's how I read the paper for this. Okay. So it's a very new stuff. It's very new paper and the next live stream is going to be,

Speaker 2:          00:57:56       uh,

Speaker 1:          00:57:58       the last live stream and it will be for, for this course and it's going to be a on not deep learning. It's a, it's a surprise. Okay, so let me answer some ending questions here before we're done.

Speaker 1:          00:58:12       I'm Hagers all love the show. We have an eight year old. That's awesome. That's awesome. Thanks for being here. Is it the code in the paper? How do I begin reading source code from a paper? So source code, he's usually not in the paper, but it's on get hub. So what I do is when I went to find source code is I will copy and paste the name of the paper like this into Google or get hub sometimes get up and it's going to show up. If they publish code. If it doesn't show up on Google's first page, they haven't published the code. And so that's, that's, that's not good. Okay. Two more questions.

Speaker 2:          00:58:44       Uh,

Speaker 1:          00:58:46       what's the source of papers you're mentioning? A great source for the papers is archive sanity. Check this out. Make that bigger is the word archive sanity. Let me paste that right here.

Speaker 3:          00:59:11       Where is it

Speaker 1:          00:59:14       checked out? This, there's the link right here. Archive Sanity and outcome. You can, you know what it does? It's a web app that looks at the best papers for you and it just picks and you can just, it just lists them chronologically and you can look at what the top high papers are. Great. Great tool. Okay, so, uh, one more question. Can we use RNN based architecture and an adversarial setting, but realistic text generation tasks mainly are recurrent net architecture and an adversarial setting protect generation tasks. Absolutely. Absolutely. So first of all, recurrent nets for text generation. It's been used before Shakespearian text, uh, all sorts of texts, well documented problem, well defined solutions. But you could do this in 10 lines of chaos. Andre [inaudible] is the king of this, uh, lots of 10 lines, snippets on the web to do this. Very easy to do using LSTM recurrent networks and an adversarial setting.

Speaker 1:          01:00:19       I haven't seen it, but you could, I would bet you could beat the state of Vr if you tried. I would bet you could because I haven't seen it before. And overall Gans are just blowing the state of yard on many, many different fields so that, that would be on my bet. Okay. So enough of that. Let me, uh, let me run me rap. So I'm going to freestyle, uh, so someone child to be, and let me answer one question in the meantime because it takes a while for that question to come in. Uh, one more question. Uh, and, and it's guys shout out topics for the freestyle by the way. Okay.

Speaker 3:          01:00:55       Uh,

Speaker 1:          01:00:58       can we train a net to configure a good gan? That's irrelevant. What was your uni? I went to Columbia, but look, don't think like, Oh shit. You know, he went to Columbia like he's a, he's a genius. I can't be like that guys. I am just like you. I am not some God. As much as I would like to say that I am, I'm really not. I'm just like you. I'm a human. I'm a, I'm a guy. You know, I'm not some, I'm not, I'm, I don't know. I don't have a phd or have, you know, I'm not like sitting there, you know, I'm just the guy who's really interested in this stuff and that's all it takes is an interest. Okay. Uh, and, and a desire to learn. Okay. So. All right, so we've got some. Okay. All right. 10 Sir. All right, so I'm going to, I'm going to freestyle about tents. Hit it.

Speaker 3:          01:01:56       Can we go jazzy? Tensors it's going to go down in a second where I want to hear that. I want to hear the beat drop. It's all for Tensor, Huh? It's going to come in a second. Here we go. 10 Sir. Here we hear we, we do. We, I love 10 stores. Try to give back and give you some a bit lectures.

Speaker 1:          01:02:20       Every day I come out, it's like I'm a section leader of a person. Every time I give it, you don't even know if I'm leading. I see these different variables every single day. I see you like one, two, three, four. Okay. I think I can define a tense or out of that my way or use it back to her class analysis. Okay. There are different ways of putting this out there. Anyway, I'm going to talk about demo one, two, and three. All right. Hey, drop the beat. Drop it back. Like if you take a seat, not while I'm rapping, man. I'm not your enemy. I see Jupiter notebooks. Can't you see that's the only way to do this. You got a beat on top whenever you will lease it. Whenever you show it to the world. Do you want to make sure it is creasing, creasing, like paper?

Speaker 1:          01:03:00       You fold it, you show it to professors like you're the man. That's it for this. That's it for the, that's it really. The beat. The beat didn't drop, but I just roll with it. You know the beat in life will drop when you ask it to. Okay. So that's it for the live stream. Thank you everybody for watching. And uh, for now I've got to go create the finale. And don't worry, I have so much more to come. This, this is nothing. I'm just getting started. Okay. So I am just getting started. Don't be worried like, oh no. The course is, no, no, no, no, no. I'm just getting started. Okay, so for now I've got to write the finale, so thanks for watching.