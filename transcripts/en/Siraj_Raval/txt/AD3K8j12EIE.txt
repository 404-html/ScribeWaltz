Speaker 1:          00:00          Hello world it Saroj and neuro ordinary differential equations. I know that sounds complicated. Don't go anywhere, but it's a very important to idea. And guess what the biggest Ai Conference of the year is called neuro trips. It just happened last month in December of 2018 over 4,854 research papers were submitted to nerves. That's a lot of research papers. And this paper that I'm about to explain to you in detail right now, won best paper. It was one of four that won the best paper out of over 4,854 research papers, people from Nvidia, researchers from Google, researchers from Facebook, all of these big tech companies submitted their best work. And this paper was one of four. That one the best. And I'm gonna explain to you how it works in this video. Now, why should you even care? This is a demo of the, of the, of the paper behind me, by the way.

Speaker 1:          00:51          So you can see, there's a lot for me to explain here, but why should you care? Let's start with that. This idea can be applied to time series data. What is time series data? Any kind of any type of Dataset that depends on time. So what would be an example? Finance data, right? How do stock prices change over time? Health care data? How do patients biometric signals change over time? Anything that changes over time, that is, can be considered time series. Data time is an important element, right? This technique can predict, uh, can make predictions on time series data better. Then recurrent neural networks, which is kind of the defacto go to type of network to make predictions with time series data, including including all variants of recurrent neural networks, including residual networks. There's, there's so much, there's so much theory. They're dense networks, highway networks.

Speaker 1:          01:44          Um, there's a lot, but we have something to focus on in this video. It's called neuro ordinary differential equations. Okay, so let's get started with this. What even happened here, right? So of all these papers, this is the one that got the best paper award. So what is the basic idea before we talk about calculus? That's really what I'm hyped to talk about today. Calculus because this is using integral calculus. Instead of differentiating, we're going to integrate. If you don't know calculus, don't worry. I'm going to try to put that prerequisite knowledge into this video as well, because I want this video to be for everybody, not just for researchers, not just for AI enthusiasts, but for everybody. Even if you've not coded before. So let's see how, how, how, well, I can do this. Okay, so here, here we go. So here's the basic idea.

Speaker 1:          02:31          Neural networks are function approximators. Neural networks are a series of matrix operations. So there's, there are a series of operations, right? Like add, subtract, multiply, divide that we apply to matrices. Matrices are groups of numbers, right? So we have an input matrices, we apply these operations to it, and then we get an output. And this output is the prediction. And so what we, the way we think about these neural networks, the way this is a picture by the way, the way we think about these networks is we group them into what are called discreet layers. So layers are in the network. They're just a block of operations. And so we can actually make a rap song out of this, right? So input times weight, add a bias, activate repeat input times weight, add a bias, activate repeat. And so we just keep doing that over and over and over again.

Speaker 1:          03:19          Input data times the weight Matrix, add a bias, and then apply that to an activation function, which could be several. And then we just repeat that over and over again. And then we get the output, right? So what this image shows is a neural network. Okay? And it shows the Matrix operations that are occurring. These lines represent multiplication, operations apply to all of these numbers such that we get the next layer. So this idea of layers is very persistent in neural networks. Neural networks have layers, right? So that's, that's the basic idea of neural networks. So what this paper does is it says, instead of having these be discreet layers, that means, you know, we have one layer and then we have a second layer rights individually independent grouped layers blocks, right? Just separate, discreet. Instead of doing that, let's consider a neural network as a continuous function.

Speaker 1:          04:09          That is a function that is both infant test Mully big and infant testimony. Small, right? This is a continuous function, right? There's no grouping, there's no blocks. It's just one function forever, within forever without rights in one direction. That's a little inside joke for people who follow my channel regularly subscribed, by the way, cause you're, I promise you, you're not gonna be getting this kind of lecture anywhere else. Subscribed. Okay, so here we go. That's what this paper does. It, it groups neural networks into this category of continuous functions. So instead of having single layers, we just have the entire network be one continuous block of computation. And what this means is no more specifying the number of layers beforehand, rate. You know, normally you have to decide how many layers do I want my neural network to have, right? And once you do that, then you can build the network.

Speaker 1:          04:58          But with this idea, instead of specifying the number of layers beforehand, we specified the desired accuracy and it will learn how to train itself within that margin of error. Now, this is still in the early stages. By the way, the team, it's at University of Toronto, um, at the vector institutes, they are still working on this and it's not perfect. It is a research paper. It wasn't meant for production, but it is a very promising direction for 2019 for AI. And I think that all AI enthusiasts should read this paper or at least watching the video. So direct them to this video because this is going to be easier than reading the paper, which was a, I had to, I had to review quite a lot of integral calculus, but that was a lot of fun. Okay, so why does this matter? Time series data is, is why that matters.

Speaker 1:          05:44          Okay, so why does this matter? Because it means faster testing time than recurrent networks. So just faster neural networks. If we do it this way, instead of considering our network as discrete blocks of discreet layers, we consider it as a continuous function. And once we do that, we can have faster testing time. So we're, we're making a trade off between precision and speed. It also means that we get more accurate as results for time series predictions. I he continuous time models, what can be considered time series data. Tell me finance, right? How does, how do stock prices change over time? Health Care, how does, how does the patient's biometric data change over time? How does what change over time? All of that is time series data. And the thing about neural networks is that with these discrete layers, they expect the intervals for these, for these time series data sets to be fixed, right?

Speaker 1:          06:39          So these are predictable, fixed intervals. Every day we're taking a recording or every hour or every second. But that's not how the real world operates, right? So one example would be healthcare patients and hospitals aren't always regularly recording a patient's biometric data every hour or every day. It's irregular. And so neural networks are bad at predicting. Uh, the are are bad at predicting an output for time series data that is irregular. But this is a promising direction. This ode network, which is ordinary differential equation network that these guys have invented. And one more point, why else is this matter? So two points I've already talked about. Faster testing time and more accurate results for time series predictions. I he continuous time models. And the third point is this opens up a whole new realm of mathematics for optimizing neural networks, right? If we frame it as what's called a differential equation, which I'm gonna explain, don't worry if you don't get that, which I'm gonna explain, if we frame it this way as a continuous differential equation, then we could use, instead of, you know, gradient descent, which were used to only, we can use an entirely new class of optimizations solvers, which are called differential equations solvers.

Speaker 1:          07:50          And there's a hundred plus years of theory behind these that we haven't really applied to neural networks yet in a mainstream way. And so there's a lot of promise there. Um, and Oh, lastly, we can compute gradients with a constant memory costs. So every time you're adding layers to your network, you are linearly increasing the amount of memory that it's using, right? But with this idea, you have a constant memory cost. So in this video, the concept that we're going to cover our basic neural network theory, rec, residual neural network theory, ordinary differential equations, then ode networks, Oilers method, the ad joint method. Um, and then we'll have a conclusion. So that's a lot. So sit down and enjoy. This is going to be amazing. Here we go. So neural networks are popular types of machine learning models, right? Neural networks are built with linear Algebra, meaning they are matrices with Matrix operations and they are optimized using calculus.

Speaker 1:          08:41          Calculus is the study of how things change over time. It's using physics a lot. We'll get into this in a second. So neural networks consists of a series of layers which are just matrix operations and each layer introduces a little bit of error that compounds through the network as we propagate forward. So that's one diagram. Now here is a more mathematically accurate diagram of what's happening in a neural network. We have our input data. This is a lot. So we have our input data, right? This would be an image, video, text, whatever it is. And then what we do is we take that input data and we multiply it by ww is the weight matrix. And so we take the input data, we multiplied by our weight, we add a bias, be see this be, and then we activate. Okay, so f is the activation function.

Speaker 1:          09:24          So, so look right here where my, where my mouse is circling around, we have our input data, which is p times the weight, add the bias and then take that, the result of that, the some of that and applies activation function to activate. And that is the output of the first layer. We take that output and we feed it into the next layer and we are going to do it again. And we do it again. So we could think of a neural network as one giant composite function of functions within functions. So the output is gonna be a function of a function, of a function, of a function for as many layers as there are. And so in terms of code, here is our activation function. It's a sigmoid function. And nonlinearity I have a million videos on how neural networks work. So, I'm not going to go into that in detail right now, but just basic idea.

Speaker 1:          10:08          We have our input data, we have our labels, we want to find the mapping between both. And so for our training loop we do, we perform forward propagation input times, wait, alibis activates, and then we compute the partial derivative with of the error with respect to our weights and then propagate that backwards. That's backpropagation Google or Youtube Saroj backpropagation in five minutes to see how that works. Okay, now let's get into this. So the idea is that to reduce compounded error, let's just stack more layers, right? It's more layers, the more layers of deeper than network, he deep learning. And so, um, is this your machine learning system? Yes. You just pour the data into this big pile of linear Algebra than collect the answers on the other side. What if the answers are wrong? Just sir the pile until they start looking writes that in essence is how deep learning research goes.

Speaker 1:          11:01          Let's be real everybody. So there's a lot of theory behind how many layers to at your network. But the basic idea is that the number of layers deeply affects the output of the network. Too few could cause under fitting. So meaning your line won't fit the data too many and it could cause overfitting meaning your line will overfit the data. Um, but so you want to get adjust, right? So the line perfectly fits your dataset. So neural networks, all of this stuff, it's glorified, um, line fitting to data in a way. So there's so many different rules of thumb for how many layers you should be choosing. You know, here are three bullet points. Here are three examples. And so when we use a higher level library like care os to build a deeper network, right? So we're going to, let's say we'll build an LSTM network and care os right here. And so for each line, each line represents a single layer. If we want it to make this network deeper, then we can just say model dot had LSTM and then 32 or however many dimensions we want it to have. And so we could just keep on stacking them like this stack, stack, stack, stack, stack, stack, stack, stack, stack, stack on to level up. You see what I'm saying? We keep on stacking them, uh, if we want it to be deeper and deeper.

Speaker 2:          12:19          Okay.

Speaker 1:          12:20          And so there's, there's a, there's a threshold here where adding more isn't gonna do anything. So solution to this was proposed by Microsoft, by the way, Microsoft, you guys are killing it. Like let's go. I'm, I'm proud of you guys for just being super open source and you know, onyx and all this stuff. So keep going. So in December, 2015, Microsoft proposes idea of, of what are called residual networks. Okay. And so they won the image net classification competition using these residual networks and residual networks had the best accuracy in the competition. So if we look at it, be Google, they beat everybody. Um, this is, these were the results. And so this is Microsoft right here. And so the smaller bar is better because it means a lower error. Okay. So resonates, beat everything else. And they had the best accuracy. And what they were able to do is they were able to train networks that we're up to a thousand layers deep while avoiding vanish and gradients, meaning avoiding that lower accuracy that occurs with deeper networks.

Speaker 1:          13:16          And six months later their publication already had more than 200 references. That's a big deal. So people really liked this idea. And so the, the, the way residual networks work very simple, really very simple. Like it's just too simple. You know how we, we just keep feeding the output of the previous layer, the next layer. So the only change we're making for residual networks is we're t, we're not just feeding in the output of the previous layer, we're feeding in the output of the previous layer and the input of that layer. So we add that, so f of x plus x, and that's what we feed into the next layer. And then we do that again and again and again. And so what this does is it creates these skip connections or we're not just fitting in the output of the previous layer. We're also putting feeding in the input of the previous layer to the next layer. So network and decide how deep it wants to be. So that's, so this would, this is an example, right? So resonates.

Speaker 2:          14:10          Okay.

Speaker 1:          14:11          Versus you know, other types of networks.

Speaker 1:          14:16          So instead of hoping each stack of layers directly fits the underlying mapping, we can explicitly let these layers fit a residual mapping. So we cast it from of x two F of x Plus X. Okay? So it's, it's very simple like I said, so, uh, we add the output of the activation function, which is f. So x is the input. So x would be input to the network. F of x is going to be the activation function applied to x, and then we add x again. And that's going to be the output for that. That's going to be the output and also input for the next layer. Okay? So f is the function of the cake layer and it's activation. So this is the formula for a residual network right here. Okay? This is what's going to act as the input to the next layer. Not just the output from the last layer, but the input from the last layer.

Speaker 1:          15:05          We add both. That's it. That's it. Now, here's the cool stuff. So what if we recast this equation by just adding in some constant, let's say the constant is one, right? There's already a constant one. It just, we can't see it, but let's just name it some letter. Let's name an age, we can name it. Anything. Well, we wait, we could name it V, we can emit cue. Let's call it h and let's say h equals one. So we know that h equals one, and we'll say eight. So we'll add it to our equation. So our original equation is x of, so x equals x plus F of x. So we'll just add h in there. So it's x equals x plus h times f of x, where H is one. So it doesn't really do anything. It's just it's just there and it's still true. Now, why did we do this?

Speaker 1:          15:47          Because if we cast it in this way, this equation right here, x equals x plus h times f of x is the formula for what's called Oilers method for solving ordinary differential equations when h equals one. So you might be thinking, what are you talking about? What is Oilers method? What are differential equations, and what do they have anything to do with neural networks? So hold that thought. Let's go get some code first. Okay. So this code right here is a normal layer. Okay? This is a normal layer in a convolutional neural network meant men for image classification rights. So we have a normalization, um, operation activation, convolution, normalization, activation, convolution, right? So let's modify that to be residual. What did we change? We captured x, the input, and we set it to Rez because we want to use that in a second. And so remember x is the, is the input to this layer.

Speaker 1:          16:45          And we're also, when we, when we give it to the next layer, here's the other difference. We're giving it the output of the activation function and the input. And that's what we feed to the next layer, right? So this is a different, these two lines right here. Okay. So back to the question. I just wanted to show you that what is the significance between residual networks and ordinary differential equations? Okay, so a differential equation is an equation that tells us the slope without specifying the original function, who's derivative we are taking. Let's just go into this. Okay, we've got some calculus. So if we have a function right, with any kind of function sine cosine, x cubed plus seven x, any function, if we graph it, if we graph that function, it could do this. I can do this. You can do this. If we wants to find the slope of the tangent line to any points on that function, that slope is the derivative, okay?

Speaker 1:          17:42          The derivative of the function f of x evaluated at x equals a at any points, gives the slope of the curve at that point. Why does this matter? Because if we derive a function at any point, it gives us a slope and that slope tells us the direction that that function is moving at that specific point in time. Now, that's the derivative now. So, right. So any function we can compute a derivative of, right? So x squared, the derivative is two x because what do we do? We take that, we take the power to, we move into the coefficient and we subtract one, uh, from that power and that gives us a derivative. It's that simple. So x cubed, three to the x, and then we do three minus one is two, so three x square, so x to the fourth. The derivative is don't look for execute.

Speaker 1:          18:28          I'm not even gonna look now x to the fifth. Five x to the fourth. Okay, six to the x to the sixth, six x to the fifth. Okay. That's the derivative. And we use a driven a lot in machine learning, right? The derivative helps us compute the gradients that tells us how to update our network, but we don't use a lot or are integrals. Now the integral is the, is the opposite of the derivative. So assume that we have the derivative and we want to find the original function. How do we do that? How do we go from the derivative back to the original function? What we just do, the reverse, we take that coefficient, we put it in the, in the power, right? The exponent, and that gives us, and that's called integration. So taking a function, finding the derivative is called differentiation, but taking the derivative and finding the original function is called integration, where we're using the integral. And so this is a symbol for the integral right here.

Speaker 1:          19:22          So differential equations, okay. Are there are two types. There's ordinary differential equations and there are partial differential equations. So ordinary differential equations involve one or more ordinary derivatives of unknown functions where it's partial differential equations evolve, one or more partial derivatives of unknown function. So the idea is that in a an ordinary differential equation, we are taking the derivative of one independent variable. Whereas in partial differential equations, we're taking the partial derivative. So there are multiple independent variables, right? We are differentiating with respect to one of those variable because they're all independent. But in an ordinary differential equation, there's one independent variable. So we want to solve the differential equation. So what a differential equation does is it sets the derivative equal to some, some value. It could be, you know, the Dui dx equals f of x plus three, you know? Right? So it gives us that.

Speaker 1:          20:19          So we set the derivative to some value, right? So let's say, here we go, here's, here's a perfect example. If we have some function, right, some function in the world, we want to find the derivative. When we say that derivative plus some random number, like three or four or five equals one, that's a differential equation because we can use a differential equation solver method like Oilers method, which we'll get to or some other method to then find the original function. So the whole point of all of this, of partial, ordinary, all of this is to find the original function to approximate the function. And in general, that's what we're doing in machine learning. And in general, that's what we're doing in science and technology and math and everything. And we are trying to find the function and the function tells us everything. The function tells us everything. Math tells us everything. Not to get too poetic here.

Speaker 3:          21:07          So

Speaker 1:          21:08          we use this a lot in physics, right? So if we have ideas, if we have concepts like displacement and velocity and acceleration, if we take the displacement and we differentiate, we get the velocity. If we differentiate that we get the acceleration. If we take the acceleration and we integrate that, we get the velocity. If we integrate that we get to the displacement. So you see what I'm saying? It's very useful in physics and, and, and these different fields to both differentiate and integrate. So I'm going to bring this back into machine learning. So one more example, two x has several, uh, integral as possible integrals and it has one derivative. All of these expressions.

Speaker 1:          21:45          So here's an example of a differential equation, right? So we have some, a derivative right here, right? Dyd X equals x plus y over five, okay. And so what we do is we say, let's take that derivative and compute the original function using this ordinary different differential equation solver. And that will give us our solution. So this result in graph, let me just compile that. This result in graph is our original function until all we knew was a differential equation right here. And using an ordinary differential equation solver, we were able to find that original function. Now let's apply this to ordinary differential equation networks. Now consider it a very simple ode. We'll call them odysseys from now on. Okay. Then that's what it stands for. Consider a very simple ode from physics. So let's say we want to model the position of a marble. Let's say we have a marble, right when we just want to roll it on the floor, we're going to roll this marble on the floor. And what we want to do is we want to predict where it's a position is eventually going to be using, uh, it's philocity and using time. So these are the three variables that we are considering here. Time, velocity, and position. Okay? And so we can model the relationship between these variables using this function. So we can calculate it's philocity by differentiating, it's positioned. So the derivative of the position is it's philosophy and we can represent that using this equation right here. So

Speaker 1:          23:12          Oilers method solves this problem by following the physical intuition. So the a position at a time very close to the president depends on my current velocity and position. So let's say we're traveling at five meters per second. Okay? We're traveling at five meters per second and we traveled for one second, right? So if we're traveling for five meters per second and we traveled for one second, how far have we traveled? Five Times one, one second, five. We've traveled five meters. And so we can model this using this formula right here. Okay. So, uh, the position is going to be equal to where we are x of t plus age, the velocity times the derivative of the position. And that's going to give us, so it's going to be five times the number of seconds we've been traveling plus wherever we were previously. Okay? And, but since we know that x prime of t equals f of x, we can rewrite this initial equation as this.

Speaker 1:          24:05          Okay? So x of t plus h is going to be equal to the current position plus the velocity times, uh, the number of seconds that we've traveled. Okay? So we are computing how far were traveling by taking the, wherever we're starting from, wherever we're starting from. And then taking that starting point, adding how far we've traveled by multiplying the velocity by the time that we've traveled. And so what this looks like then, if we squint at this, it looks at, we can, this is the formula for Oilers method yet again. And which also looks like the formula for residual layer. So we are connecting physics to neural networks by framing the residual network equation as a differential equation, just like we did in physics. So differential equations can represent neural networks. Neural networks can be represented as differential equations. So what does this mean? It means that we can create new models for optimizing these networks using different numerical approaches to solving, um, ordinary differential equations, which there are over a hundred years of theory behind.

Speaker 1:          25:13          Over a hundred years of theory are behind ods. And we haven't applied any of that to neural networks in general. I mean, there's some papers, I mean, there's, you know, it's not in a mainstream way. We haven't seen anything in production like this. And what this means is it is the possibility of creating arbitrarily deep neural networks and training of these deep networks can be improved by considering the stoke called stability of the underlying ode and as numerical discretization. So two more points to create these arbitrarily deep networks with the finite memory footprint. We designed these neuro networks based on Odsp and so great and to sense the the optimization technique we all know and love in neural networks, the most popular optimization technique can be viewed at it's applying Oilers method for solving ods to gradient flow, which can be, which is right here. It can be represented right here.

Speaker 1:          26:04          So you might be thinking, so what's the difference here for not applying it to gradient flow because there are not discreet layers isn't instead of continuous function, what? What then does an Ode net look like? So an ode is a function that describes a change of some system over through time. Thank you calculus, right? We're using the derivative to find the original function. And in this setting, time is a continuous variable. Now imagine neural networks as this continuous system. So what happens then? If we do that is that time really it becomes something more like the depth of the network and know that there are usually a discrete number of layers, but in this case there's a continuous number of layers. So we can either think of this as having waste that are a function of the depth or as having shared weights across layers, but adding the depth as an extra input to f.

Speaker 1:          26:52          So anywhere you can put a Rez residual network, you can also put an ode network and each Odey he block can be used to replace a whole stack of Rez blocks. In fact, they had an example in using the M and ist handwritten character Dataset as an image classification example, and they were placed six Rez blocks with one single ode block. So again, traditional deep nets input input to a layer. It gives us the hidden stay hidden, say go to the next layer, gives the next, hey is that go next to that gives, given that one resonates, we do the same, but we add the original input. Okay. That's, that's how that works. So,

Speaker 2:          27:28          okay,

Speaker 1:          27:28          if we model our networks like this, we can use ordinary differential equations solvers, like Oilers method at solve the trajectory of a system by taking small steps in the direction of the system dynamics and adding them up, which allows for better training methods. Okay. So

Speaker 1:          27:47          let's cover Oilers method. Just one more time. So the Blue Line represents the function, right? And so what we want to do is we want to approximate this function. So what we do is we say, okay, let's take this first point on the line and find the derivative of it. Okay, it's going to give us a tangent line and we take a small step in that direction. And then we say, let's do it again. What is the derivative of the function? Okay? And let's give it another tangent line and another one and another one. We just keep doing that over and over and over again. And so what we can do is we can roughly approximate what that function is by continuously deriving, um, the points on that line. And that's Oilers method. And so it's good. It's gonna give us a rough approximation. So Oilers method isn't that great. It's a very primitive solver. But the point is to show that we can frame these networks as differential equations.

Speaker 1:          28:35          And so, um, what happens is in a residual network, uh, there's, there's a discreet sequence of finite transformations, whereas in an ode network there is a vector field which continuously transforms this state. And so this represents a difference between them there, their vector spaces, right? And so there have been over 120 years of developments for efficient and accurate Ode solvers. Let's apply those to neural networks and see what happens. Now that's the next step here. So how, so how then do we solve this thing? If we, if we frame a neural network as an ode, how do we solve it using an Ode solver? Do we use Oilers method? We could, but we could use a better one and said, let's use what's called the ad joint method, which dates back to 1962. Okay. It's like an instantaneous analog of the chain roll. Okay. So the thing about the ad joint method is that there is not a lot of content, if any, about it on the Internet that is accessible. So I'm going to do the best that I can to explain this and I myself need to continuously learn about this. And even the authors of this paper are still figuring out

Speaker 1:          29:40          how this is going to work. They believe and a lot of people believe in, I think that this is going to be as big as gans were, um, generative adversarial networks. Okay. So the [inaudible] method. So the basic idea is that we are computing three integrals. We're competing three integrals, okay? So we're computing the ad joined, which captures, had the loss function changes with respect to the hidden state. That's one we're computing, um, the hidden state backwards in time together. And then we're a third integral. Tell us how the laws changes with the parameters theta, which are the weights. So there are three integrals and we combine these three integrals into a single call to an ode solver. Give it to that. It's going to concatenate the original states, the ad joint states. And the other partial derivatives into a single vector and that will then solve our equation.

Speaker 1:          30:29          So here is what the algorithm looks like. We compute a gradient with respect to the time or tea, whatever ti is going to be. We define an initial augmented safe. Then we defined the dynamics of that, augment that, say concatenate the time to read it as together solve the reverse time ode and then return the gradients and an update our weights. Okay, so that's quite a lot. But in the code, let's look at some code. This ode solver was applied to the Ford propagation of the network. So they have a code that's applied to time series data. They have code that's applied to em and ist. They have several code examples. And so I've, I've actually even, you know, I've downloaded the code and we can run it right here, super easy. Just need to insult high torch and what it's going to do it, it's going to approximate this function using a continuous neural network, neural network and Odie network.

Speaker 1:          31:22          And it's using an ode solver to Ford propagate the, the data. It's still using gradient descent. But it's using this [inaudible] method to Ford propagate the data and it's approximating with this line looks like. And so we can see how it PR approximating both the trajectory. I'm a different version of that graph and then the learned vector field and it's training over time. Okay. But um, so I just want to show that as an example. Okay. So you can run this code and I've got the code, you know, it's like right here. Okay. But I took out all of these like unnecessary bits just so for learning purposes and I've written it here. So let's just go through this. Here's a normal residual block, right? We already looked at an example of this before, but here's a normal residual block, you know, activation function, downsampling convolutional network pass, right? That's a normal residual block. Now here is an ode function. So what we do is we are again defining these, um, neural net layers, convolution normalize, activate, and then we are again normal, uh, forward propagation looks very similar to a regular neuro layer. Now in the Ode Block we encapsulate that function. So we've call it OT, he flunked. And then in the forward propagation method, here is our solver right here, the ad joint method, which they created in Pi Torch by the way, check out their library

Speaker 1:          32:46          and we input that Ode function to this solver for forward propagation. And that's really the key difference right there. And so in the main method we add these pooling layers and so we say, okay, here are layers. It's going to be a single ode block that's given this ode function. Let's have a fully connected layer at the end and build our model. Our model is a sequential model consisting of these down sampled layers are feature layers, which is our single continuous ode block. And then our fully connected layer that the end to output that prediction.

Speaker 1:          33:18          And then we go on as we normally would, we declare our gradient descent optimizer, which is going to be stochastic gradient descent. We initialize our training data, we apply our training data to the model, we get our prediction, we compute our error. What's the difference between the prediction and the actual and we back propagate a using the optimizer and that's it, right? So that's the difference here. So let's summarize what just happened here. And all networks are a popular type of DNL model. They're built with linear Algebra matrices and optimize using calculus, graded descent. You know, all these different solvers. They're consistent. They consist of a series of discreet layers, which are just matrix operations. And each layer introduces a little bit of air to get around this error. Microsoft introduced residual networks, which is that skip connection idea f of x plus x equals the input to the next layer. And so what the researchers found was like this equation for a residual network resembles an ordinary differential equation. So why don't we use ordinary differential equations solvers, like wheeler's method or the Agilent method to then solve a neural network and instead have continuous layers. And when they did that, it actually worked and it had a better prediction result than a recurrent network did for approximating

Speaker 1:          34:33          this function right here. And what happens then is we can apply it to irregular time series data in the future, um, to have better predictions. We can have faster testing time. Um, and we can apply a whole new class of mathematical operations that are dedicated to ods, to neural networks. And that just opens up a whole new branch of AI possibility, which is amazing. And, um, we can compute the gradient with a constant memory costs instead of constantly second layers. We have one continuous ode, Blah. Okay. So I hope that made sense. I hope that excited you. I've got links to everything that I've talked about here and helpful links in the video description. Subscribe if you haven't yet. Um, thank you for watching and for now, I've got to go make some more videos and play my course and music video. So thanks for watching.