Speaker 1:          00:00          Hey therapists, I can't see you anymore. I've updated my privacy policy. Hello world. It's Saroj and if you're interested in a career in data science, an easy way for you to have a huge competitive advantage over your peers is to learn about tools that preserve a user's privacy. I'll demonstrate an example of what I mean by training a model on sensitive patient data to predict diabetes while preserving patient and on nymity. But Let me first explain why privacy matters to you personally. Online services both big and small have become accustomed to collecting and sharing user data with little to no restriction like it's some kind of wild, wild west situation. But all of that is fast changing. Just this year, the European Union enacted the general data protection regulation or Gdpr Gdpr as a set of laws aimed at protecting the data of EU citizens. Any online service that has users based in the EU has to comply.

Speaker 1:          01:03          It has provisions like if a service has more than 250 employees or deals with sensitive personal data, they require a privacy officer and a service requires affirmative consent. Before saving any tracking cookies or personal data. Failure to comply with GDPR could result in fines up to 20 million euros and some companies are already facing massive penalties. Facebook, it's not just the EU, though 107 countries have now put in place legislation to secure the protection of personal data for consumers. This is a good thing. The reason you have value is your data. It's how you think. It's what you eat. It's who you know. It's how often you listen to desk by seat though everything about you is data. Data protection laws aren't going anywhere and we'll see more of them in the coming years. A crucial skill to learn then for aspiring data scientists is how to preserve user privacy, wild training, machine learning models on sensitive user data.

Speaker 1:          02:08          We can see some real world applications of this already. Apple started rolling out privacy techniques in Ios 10 they're able to anonymously collect user data like the words they type that aren't in the keyboard dictionary and train models on that to improve user experiences. Another example is Google. They collect anonymized data in chrome to do studies on malware and in maps to help predict traffic jams. Numerate allows data scientists from around the world to train their models on encrypted financial data that keeps the clients data private. There's a huge growing space for startups that respect user privacy. I'm going to cover three important privacy preserving techniques that when put together form a powerful pipeline that allows us to train un-encrypted model on encrypted data from multiple users. Federated Learning, secure multiparty computation and differential privacy. I know it sounds super technical, but this is a technical channel.

Speaker 1:          03:12          Subscribe if you're new. By the way, let's start with federated learning. Before training a machine learning model, a data scientist must first collect user data. Users generate this data using devices like their phone or their laptop by recording events in the real world. The way this data is usually aggregated is by following a simple server client model that has come to define most of the worldwide web data is sent from multiple user devices to a central server that contains the model and the model then trains on this data. We can consider this centralized learning since the learning or training process occurs on a central server. Federated Learning, however, flippity flips this pipeline. Instead of bringing training data to the model via a central server, we bring the model to the training data on whichever machine it's located. This allows the people who create the data to maintain ownership without sending it off to a machine that they don't control.

Speaker 1:          04:15          Let's see. An example of how this works using two python libraries. The first is called [inaudible], which is a set of tools that enable encrypted privacy, preserving deep learning. Definitely give it a star on get hub. Trask is a privacy beast and a friend. The second is called Pi Torch, which is an open source library for training machine learning networks. I guess something Nice did come out of Facebook. After all. After importing both libraries, we can create a simple toy datasets in just two lines. That data and the labels for that data we're using the float tents or attribute of Pi sift here which creates a tensor. A tensor by the way is an n dimensional matrix where n is a number we define and it's the format we feed data to neural networks in. That's why tensorflow flow is called tensorflow. Tensors flowed through the computation graph as that a data scientist creates.

Speaker 1:          05:08          We can also create a simple linear neural network model in a single line. Then when we train, this model will approximate the ideal weight values for the network through the process known as gradient descent. I have at least 4 trillion videos on how that works. Link will be in the video description. That's centralized learning now to perform federated learning. We're going to Redo this example, but this time we have two separate worker nodes who don't know each other, Jack and Jill that we want to send our model to train on. Now we're going to create some training data and send it to both of these nodes. We'll assume they created it themselves. The way we can access this training data is by using pointers. Pi Sift represents pointers to tensors that live on remote machines easily. Once we have these pointers, we'll store them in a list and start training our model.

Speaker 1:          06:01          It will iterate through each worker's respective dataset and train the model on them. So federated learning allows us to train our models while the training data is distributed across multiple nodes. The problem though is that we can easily learn about either jack or Jill's data by reverse engineering the learned weights of our model. I knew Joe Pushed Jack Down that hill. The way to fix this is to use a technique called secure multiparty computation or SMPC, which provides the ability to compute values of interest from multiple encrypted data sources without any party having to reveal their private data. If we have a value, we'd like to encrypt it split into multiple shares. These shares are kind of like a private key. There'll be distributed amongst two or more owners, and in order to decrypt the variable, all the owners have to agree to allow decryption. So in essence, everyone has their own private key.

Speaker 1:          06:58          It takes the consensus of every party with a share for us to be able to decrypt the data. And by far the coolest part about secure multiparty computation is that it has the ability to perform computation. While the variables are encrypted, meaning we can perform, add or subtract operations on encrypted data. This can be done locally or remotely as we can perform operations on encrypted tensors that live across multiple parties. The worker nodes we've instantiated in our case, that's right arithmetic on encrypted values. We can perform computations on values that we can't even see as NBC. Techniques protect model weights while allowing multiple worker nodes to take part in the training process with their own dataset. You know what that means, right? It means that since neural networks are a series of math operations, we can use SMPC to encrypt a neural network model and have it trained on encrypted data.

Speaker 1:          07:57          After we create our worker nodes, we can initialize our datasets, then encrypt both the model and the data in the training loop, the encrypted model, we'll be able to learn from encrypted data distributed across multiple machines. While SMPC encrypts computation, differential privacy ensures that went a model. Learn something. It only learns what it's supposed to learn without accidentally memorizing private information contained in a Dataset. Think of differential privacy like a promise between a data scientist and a data owner that says that the data owner will not be adversely affected in any way by allowing their data to be used in the training data set, no matter what other data might be available about that person. So for example, SMPC allows us to train a deep neural network while both the model and the data are encrypted. Eventually we'll want to use that model somewhere and either decrypt its future predictions or perhaps even decrypt the model entirely.

Speaker 1:          08:57          Differential privacy ensures that a model's weights and predictions don't accidentally disclose information about people from the training data. If, for example, a model is trained on diagnostic measurements, we want it to be able to predict whether a certain diagnostic readout has diabetes without accidentally learning whether a specific patient named Stanley Hudson has diabetes. Some popular examples of differential privacy techniques include randomizing user ids, slightly preturbing numerical values, and injecting noise into datasets. Using these techniques. Together we can create a training pipeline that both keeps users data private and keeps a models of valuable intelligence privates. There are three things to remember from this video. Privacy is what lets you keep your value as a functioning member of society. We can use a combination of federated learning, secure multiparty computation, and differential privacy to create intelligent apps that keep both data and models private. And [inaudible] is a library that makes it really easy to implement all of this. How do you feel about the whole privacy debate? Let me know in the comment section and please subscribe for more programming videos. For now, I've got to encrypt my wallet, so thanks for watching.