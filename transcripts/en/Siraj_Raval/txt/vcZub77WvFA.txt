Speaker 1:          00:00          Yes.

Speaker 2:          00:02          Let's see who we've got in the room today. All right, I'm me. Go up here and then see who we've got. What does that sound is heard some music.

Speaker 1:          00:28          Okay.

Speaker 2:          00:29          Oh, what is that? What is that?

Speaker 1:          00:35          Okay.

Speaker 2:          00:36          Hi. Hold on. I'm

Speaker 1:          00:39          yeah,

Speaker 2:          00:39          hearing something. What does that sound? What does that sound?

Speaker 2:          00:59          Hi everybody. I'm about to get in this room in a second. I'm going to talk to all of you guys. I'm pulling up this video. So here we go. Oh, I'm hearing a, an ad. So it's my noise cancellation. I everybody, oh world. It's to Raj. And today we're going to create a, a two layer neural network. Let me just get this ad out of the way. All right. There. Okay, here we go. Okay, so we, I'm in the room and I'm ready to go. Hi everybody. Okay, that was crazy. Okay, so I am live and there are 83 people watching right now and I'll everybody. Hi Andre. Hi David. Hypercar. Hi Yasue. Today is Wednesday, m and ISC. No, it's not him. And I see, okay, so here was what we were going to do. We are going [inaudible] two layer neural network from scratch that can predict the XR value.

Speaker 2:          01:52          Okay. So what does [inaudible] mean? That means we're just going to give it an array of numbers. Okay. So that means it's going to be ones and Zeros. I'm not going to use Siano, I'm not going to use tensor flow. I'm not gonna use anything but num Pi and, and the time library. We are building this thing from scratch. Just a two layer neural network. Uh, no psychic, nothing like, because people have been asking me to build a neural net from scratch for a while, so I'm going to do it. Okay. And so disclaimer, okay. So like I've made a video on building a neural network before. I know you did that in kindergarten. That's awesome. Uh, but Parker Ali Baba, US Mitchell, I'm goes, keep shouting out. I've built a neural network before in a video called building neuron up brick. Build a neural net in four minutes and it's my most viewed video.

Speaker 2:          02:37          Uh, but that was like a pretty easy, you know, a neural net. Uh, this is going to, this is the first time I'm going to say that this is going to be a little bit intensive. It's got to be a little bit of a, math is going to be a quiet, quite a bit of math in here. So this is not going to be a dead simple. I'm just going to say that for the first time because you guys want to know the neuro that from scratch. So like let's just go, you know what I'm saying? So, okay, here we go. We're going to do this. Uh, and it's, it's gonna. It's gonna predict the x value. Okay, so it's going to, we're going to get ones and Zeros and then we're going to generate some random data and we're going to, we're going to compare them.

Speaker 2:          03:12          So it's like x or means and computer science. It's like a exclusive or which means it's, it's only returns true if they're both different, like or ones or if it's a one and zero, oh, it's a one. If it's a zero and a one, oh, it's a one. If it's a zero and a zero, it's a zero. If it's a one and a one, it's a one. It's a zero. Okay, so exclusive more. All right, so that's what we're going to do a, and I'm going to start off by doing a five minute Q away like always, and then we're going to get right into the code. Okay? So ask away. Five minutes starts. Now. Where do I work? I don't work for anybody. Uh, although I do have a video coming out for open Ai, which I'm super psyched about. Can't say anything about it yet anyway.

Speaker 2:          03:51          Uh, how do I learn math from machine learning? Uh, linear Algebra specifically and statistics. Uh, you'd asked if he has some great courses on both of those things. What do you think of microdosing Lsd? I think it's a great idea, but you have to have a friend with you. What are your thoughts on the Udacity nanodegree in machine learning? Great courts. What Linux distro do you recommend who been to, how long have you been coding? Six years, but only seriously like three years. How can we help you? Other than with money? Please promote my videos. Seriously. Like get people to subscribe to my videos. Like I feel like I put all my time and energy to these videos and I don't have nearly as many subscribers as I want. So anything you guys can do to get people to subscribe would be super helpful. When would you use a drone that over a random forest?

Speaker 2:          04:37          Oh Man. Just go by this simple rule of thumb. If you, a deep neural net will outperform any other machine learning method. Pretty much 90% of the time at least a. So if you have a lot of data, use a neural net. If you don't have a lot of data though, if you only have like a hundred samples, then I would look into a random forest big and bill.com does that for you. Okay. What's your goal with youtube? I want to be a public figure. I want to beat the face of machine learning, the face of artificial intelligence, the face of computer science. Uh, and I want to use youtube as a platform to get this stuff out there and get everybody building AI and building intelligence software. All right. How can, can you tell us how long will this live stream at approximately be? This is going to be about 30 to 40 minutes. MMM.

Speaker 2:          05:33          What is the coolest Ai ml project you've come up with? Uh, I think building an AI composer, it's my second machine learning for hackers video build an AI composer was like my favorite project. Cause you generate music with machine learning. How cool is that? Why didn't you try for Masters and Phd? Well, I mean my parents definitely wanted me to have a masters in Phd. Uh, and they still kind of do and they always will. Uh, but I'm just going to keep doing this because I have the time to do this. I can study on my own and I have good advisors more or less. So I'm kind of doing a phd, but it's like not an official, it's like a, it's like a new thing and I don't, I just don't want to have to get with the bureaucracy of academia. Not to say all research labs are bureaucratic, but most of them are. Um,

Speaker 2:          06:23          please try to make these a little early for us here in India. I can do it 30 minutes earlier and much love to everybody in India. More d three videos please. Okay. I, I can, I can, uh, yeah. All right, good to know. Bose QC 35 headphones. Exactly. It's, uh, where did you go to school? I went to [inaudible] high school in Houston, Texas. That's where I grew up. And then I went to Columbia University in New York City. Uh, and yeah, for Undergrad. Do you believe academia is the best path for learning machine learning and AI? Uh, man, that's, that's a great question. I think any, anything that can give you the time

Speaker 2:          07:03          and the time and, and the time to study, but she running on the Internet is the way to go. It's so if that means academia, if that means you've got a scholarship and they're paying for your housing and they give you a little stipend so you can eat, that's the best way. But if you can make money otherwise and then you could just like live like, you know, do some contract work, then you can, you know, then that that's what I did. I just made some money doing contract work for mobile development and I used that money to just live and study machine learning on the Internet. I can't do classes in real life. I can't pay attention to professors who just go on and on and on and they just, Ah, you know what I'm saying? So I just want it to x button, three x button on life and the Internet is the way people ask me this question all the time.

Speaker 2:          07:42          Like, how did you learn machine learning? What is the way to do this? And I just tell them every time the internet, the Internet makes everything obsolete. Colleges were invented before the Internet and that was the way to do it. And they were a centralized. But now with the Internet, anybody can learn anything online. So I watched videos very fast and I look at other sources like reddit and Twitter feeds. I follow people like policies for new Boston and the whole injuries, Andreessen Horowitz team. So I have a lot of data inputs. I'm going to have one more question and I'm going to get started. Uh,

Speaker 2:          08:14          what's the best entry point for people that are relatively new to machine learning and AI? My video, start from the very beginning and download the code for each video and run it locally and get it to work and study it. I promise you, you will get to be a pro in no time. Okay, so that's it for the questions. Let's get started. We're going to build this two layer neural network and it's going to predict the XR outputs. All right, I'm going to help. So here I go. I'm going to share my screen and we're going to get started. Here we go, screen share desktop start. All right, I'm gonna move this out of the way and boom, let's see. You guys can see all this. All righty, great, cool, cool, cool, cool. All right, so let's get started. This is a big enough thing. Yes. Okay, so the two layer neural network.

Speaker 2:          09:18          Okay. So here we go. Let's get started. So the first thing when they do is important, our dependencies. The first dependency we want to, we want to import is a non pie. Would you start scientific computing, library num Pi is going to last you all of our matrix multiplication, which is very important in your own networks. Matrix multiplication is very important in neural networks. And I'm gonna explain why. So that's our first library. And our next one is gonna be time and time is going to help us, uh, time, time, how long our training is going to be. Okay. So let's, so that's it. That's all we're going to, uh, that's the only libraries were going to import. Okay. So let's just get started. Let's go ahead and start defining our variables. Okay. So the first one we wouldn't want to, we wanted to find is our number of hidden neurons.

Speaker 2:          10:00          And we're just going to say 10. Why? Because we're going to have 10 uh, values like ones and zeros going array of ones and Zeros. Then we're going to input into our network and we want to compare it to 10 other ones and Zeros, and then he'll put the extra value. That's, that's, that's what the output is going to look like. Just ones and Zeros. 10 of them. Okay. And so now we want artists show the number of inputs. How many? 10 there it's 10 uh, neurons and it's 10 inputs. And then how many outputs do we want to have? Well, we want to hop 10. Glad to have you here. Good vibes. All right, so here we go. So we want the number of outputs to be 10. Okay. And then one more thing. We want sample data. How much sample data do we want to have?

Speaker 2:          10:44          Well, let's just have some really big number and we're just going to pick 10 of them. So what's a big number that we can pick? And we're just gonna pick 10 of them. Let's do, uh, this is a neural network for classification. Yes, we're going to pick 300. All right. 300 sample data that we're going to generate. Okay. So that's it for our variables. Okay. So now we're going to define our hyper parameters. What are hyper parameter is going to be? Well, the first one is going to be the learning Rick. And the learning rate basically defines how factory, what our network to move. And he's a tuning knobs. We can make it go faster or we can make it go slower depending on what we want. Okay? So that's our learning, right? And there's one more tuning knob. There's one more hyper parameter that we want and this model, and it's called momentum. Okay? These are two different variables and we're going to use them both, uh, too

Speaker 2:          11:29          to lower the loss function, which is going to be called Cross entropy as we train our neural net. Okay. Those are the only two hyper parameters that we're going to use. All right? So those are our two hyper parameters and that's it. So now we're going to generate a random sample. Oh, sorry. Now we're going to see are now we're going to seed our random number generators. What does this mean? Well, we're going to, we're about to generate some random data, right? And, uh, we want to make sure that every time we run this code, it generates the same random numbers every time. Why? Because we want to test the code, right? And so that's what seeding does. Seating, make sure that we generate the same random numbers every time we brought our code. Okay. So that's it. So let's see this shit. Okay. So there we go.

Speaker 2:          12:26          You know what? I'd be a little more descriptive. Non-Deterministic. Uh, see that's the technical term for this in computer science, right? And I did a video on, um, I mentioned non-determinism and my paper sent the video. Okay. So, um, so we have that, can someone say how many people are in the room? It's, it's kind of a, it's, it's blocked. Okay. So we've got the sigmoid. So, so now we've seen it our numbers and now we're going to define our activation function. The, uh, traditional active, thank you. Good vibes. The traditional activation function that we're, that we use is the sigmoid function. So let's write that out as a function, right? And so sick and white is the function that's running every a neuron in our network. So what does sigmoid look like? So let me just write up this formula and my video should be watching order.

Speaker 2:          13:16          Absolutely. Uh, honestly, if you were to, if you were to watch all of my videos from like the very, very start, like the, what is bitcoin video you would learn? So I literally put everything I've learned into these videos. Like literally everything I've ever, I am into those videos. Okay. So, so what does a sigmoid function? The sigmoid function turns numbers until probabilities. Okay. They turn numbers into probabilities. What does that mean? Well, in our neural network, when we have our input data, it goes through the network and each of the weights is they set a probabilities. Like which way should it go? Point by 0.7% this way or 0.6% this way or 0.3% this way. Output. Okay. And so probabilities and they, these probabilities are updated when we train our network. So, and so it gets better and better over time. So that's what the sigmoid function does every time it hits, every time.

Speaker 2:          14:07          Our input data kits, um, one of those neurons are one of those layers. Uh, it's going to, uh, turn that number into a probability. That's all it does. Okay, so, and so traditionally we would just use one activation function, we call the sigma the activation function, but this time we're going to use two activation functions. Why? Because, uh, uh, it's four x source specifically. Uh, the, the, the tangent, the tangent function, the tangent function is, is, is helpful, uh, because uh, it, it just makes the loss even better than if we were to use just a plain old sigmoid. Uh, and I'm not going to go do like super simple, so I could just do the white, but I want to be accurate, right? I'm going to make the most accurate and neural network that I can from scratch. Okay? So I'm going to use two activation functions.

Speaker 2:          15:01          One is going to be for our, uh, first layer and the other is going to be for a second leg. Okay? So let me just write that down. And so this, whatever it's returning is the actual function itself. It's the programmatic function of the map function of the, of the other tangent prime function. Okay. So those are two activation functions. Um, right. Okay, so that's it. Those are our two activation functions. So now let's write our training function. Okay. So I'm gonna, I'm gonna, I'm gonna hit enter. I'm going to like move up. So just if you take a screenshot, okay. Three, two, one. Here we go. Okay. So I'm going to go down to [inaudible] and now I'm going to write my craning functions like that train. And so the training function is going to write a awesome, it doesn't make a different, pick the one or 1.0 you're, you're absolutely correct.

Speaker 2:          15:50          So the training function is going to take five creditors. I'm going to write them and them and explain what these parameters are. Okay? Okay. Bv B. [inaudible] okay. Okay. So what are we doing here? So what does the x, the x is the input data. The V is our transpose, which is going to help us perform matrix, help a multiplication. The, the uh, the uh, sorry, let me, I missed something. Let me start over. The X is our input data. The t is our transpose, which is going to help us perform a matrix multiplication. The V and Wrr, uh, layers to our network are two layers and BV and BW. Our biases, our biases are going to help us make a more accurate prediction. Okay. And they're going to be one bias for each of the layers and are in our network. Okay? So that's, those are our inputs.

Speaker 2:          16:49          So let me just write that, uh, input data transpose a layer one, layer two, and then by, okay, so let's do this. Do this. So the first one is going to be, uh, forward, uh, forward propagation. So we're going to do matrix multiplication makers multiply, uh, plus our biases. Okay? So let's do this. Okay. So we're going to have some, let me just write this and I'm gonna explain it right after, right this, okay? So NP dot.com and do matrix multiplication. Do the dot product. And then let me write one more above up and then do the same. But this time you start tangent function a. Okay? So let me explain this. Okay, here we go. I am going to randomly shuffled the data. Uh, and so this is going to be a simple perceptron yes, top line percent. Exactly. You can call it a perceptron.

Speaker 2:          17:53          You can call it a neural, a feed forward neural network. Uh, it's a clever rebranding that they call it all deep learning. But you know, this stuff has been around since the 50s. We just now have this amazing data and compute and that's why it's awesome. Um, okay, so here's what, here's what we're doing. This is forward propagation. Okay. We're taking the dot product of the input data x and we're putting it into our first layer fee. That doc product is, is computing that matrix multiplication and it's adding the bias in to get that a, that a delta value. Then we're going to perform that first activation function. Uh, I, that's the first time I've been called the Beyonce up neural networks. Thank you. It was pretty good actually. Uh, okay. So then we're going, we're going to do the T, we're going to perform our activation function on that data.

Speaker 2:          18:40          Okay. So that's our first um, operation. The next one is going to be, um, now we're going to have to do our sigmoid function. Okay. So, so we already applied our first activation function analysis by our next one, which is going to be the, uh, we're going to take the value that we just had that we just createdZ , that delta. And then we're going to add that the next layer, right? The second layer w from, we're going to perform the dot product. So we've computed the first part and then we're going to take that results and we're going to compute that next layer using the result was in the dot product. Okay? So, oh man. Okay. So, um, it's now we're going to add our next biases, right? The bias for that. We just need to do this twice, right? But we did it once and now we're going to do it then this next time.

Speaker 2:          19:22          So, uh, and we're going to calculate that sigmoid, you think that value that we just had and we're going to call it and why we're going to call it y. Okay? So that drug forward propagation, so that goes full. And so what we, so when we have a feed forward neural network, it's not recurring, but we do have something called backwards propagation. So, so it goes forward and backward. It doesn't go loot, it just goes forward and backwards. So what does that mean? We update, we update our weights one way and then we update them backwards and we just keep doing that while we're training. Okay. So let's do backward propagation backward. So say those, say backwards. Okay. Um, so let me just write this and then I'm going to explain it. It's going to be two lines. Okay. Let me get some of that coffee. And you know what I'm saying? That stuff helps. All right. Dot. Products. Um, eew. Okay. Okay. Um, so yeah, these are our two deltas that we're getting

Speaker 2:          20:33          from, for our backward propagation, and that's what we use. Our transpose, our transports are transposed, is basically our matrix of, of weights flipped. And we flip it because we're going backwards, right? We would make the matrix itself backwards, uh, values, and then we, and then we use that to calculate our backward propagation. Ultimately we want this he value. Okay, and what are we going to use this, this, this value for what? We're going to use it to predict our loss. And we're going to compare our predicted loss function from our actual loss function. And we're going to minimize our loss doing that. Okay? So that's the goal. We want to minimize our loss. That's how we train. We want to minimize our loss. Hi. Ton of guilt is from Greece. That's an awesome name. Okay. I Love Greens. Okay, here. Here we go. So let's predict our lungs predict our luxe.

Speaker 2:          21:22          Okay. So, so to predict our loss, we're going to calculate the APP we're going to take, I mean, let me, let me just, um, I'm just trying to sell and cause someone shout out how many people we'd have in the room right now. Evie, we want to predict our loss. Okay? So what we're going to do is predict our laws. And these are two deltas and we're using both of these that we're using the Z value, okay? That we predicted from up here at this forum. Thank you. From this forward step. And then we're going to use the, the x value, which is our input. Okay. MMM.

Speaker 2:          22:00          And so there's that. And so now we're going to calculate our loss. Okay, thanks guys. So now we're going to calculate our loss. So this is going to be quite a long life. Let me just write this out. And you know, in tensorflow, in a bunk of libraries, a lot function is just a line of code. You know, like you've just a loss parameter, but we're doing this from scratch. So we're going to actually use the math here. So let's go ahead and say, um, and, and I have something to say about this as well. Let me about using math and machine learning and if you should learn it or not, and what the poll. Okay, so let me just do one minus y. Okay. Let me make sure that this is correct. Loss is the negative of the mean of the transpose. We take our log function of y and we add one minus of transposed. Okay? So this is, this is, this is called Cross entropy. This is a, this is a,

Speaker 3:          23:01          mmm.

Speaker 2:          23:04          Mikael, I would, I would do a meetup for sure. Yeah, I should, I should, I should do that. I actually, I had a talk yesterday. One of my fans ask you to have a talk at SF state. And I did that. Um, but there weren't that many people. I mean there were like 15, but like, dude, there's like 168 people here, you know what I'm saying? Live is awesome. Uh, but yeah, if there were enough people, I would totally do a meetup. Yeah. Um, I'll look into organizing that or if you would look into that, that would be super helpful as well. Okay, so here we go. So this is cross entropy. Okay. So why Cross entropy? Well, there's a, there's a bunch of different loss functions. There's the mean squared error. Uh, there's a bunch of different loss assumptions, but we're using cross entropy because we're doing classification generally.

Speaker 2:          23:44          If doing any kind of classification tasks, you want to use the cross entropy function. Why? It just tends to give us a better results. Okay. And then one more thing about using math and machine learning. This is a long function and it's kind of difficult to understand. Do you need to know the math to use machine learning to get good at machine learning? And my answer is no, but it helps. No you don't. But it helps because if you ask them of the best javascript programmers in the world, right? If you ask some of the best front end designers in the world, they make the websites like Airbnb, like stripe, like you know the best designers in the world. How like if you ask them to make a rectangle, they'll just do like, you know, um, rex, you know, make rex or some function. But if you ask them like the details, the mathematical details of the pixel values and you know, all the math that goes into making a rectangle, they won't know.

Speaker 2:          24:35          All of this is a series of abstractions. It's just a series and series of abstractions. Eventually we won't even need to know. We won't even need to know what cross entropy or, um, any of these functions are. Eventually we won't even need to know programming. We'll just tell him what we wanted to do. Right? So it's just, it's just a series of abstractions. We're all building on top of each other. Um, and right. So I, I don't think it's necessary to know the math to do this, but it helps. Okay. That's, that's it. Exactly. Don't reinvent the wheel. That's it. Um,

Speaker 2:          25:07          okay, so here we go. So that's a cross entropy rights. We calculated all that. So let's, let's do our last line of business going to return and return our loss function. Okay. So we're going to return a lot of function and we're going to return our Yelp to values and our, um, our errors, right? They are errors in our deltas. Okay. So, okay, so that's it. That's it. Okay. So that different training step, I'm going to screenshot that. I'm going to go down a little bit more. Three, two, one. Oh, okay. Up, up, up, up, up, up, up. Up. All right, so here we go. But now let's write a prediction. Do we have one more fun to drive and then we can get, go ahead and get started with our actual, the, the meat of our code. So we have one more fun to drive and it's going to be the prediction function.

Speaker 2:          25:50          This is going to predict our value. Okay, so let me write down the variables. Bv and then B, w. B. W. Okay. So, uh, I Cooley smiley. Wow. We got some, uh, non ml people in your, okay, here we go. Uh, everybody say hi to Cooley's smiling. She was a youtuber that I met in la. Okay, so here we go. So we're going to do a prediction step. Okay? So what does that mean? That means we are going to perform matrix multiplication to, uh, predict our value, our end result. And to do that, we're going to use these, these, uh, uh, variables that we've already calculated. And p. Dot. Dot. Um, oh my God, I need to just say this. I'm a Ds said when I'm on a date, I don't need a neural network. Predict my loss. That is gold. That is gold. That is gold. Okay. So, um, let's, let me write this. Just going to be two lines. Be Two lines. All right? So let me write that out. And then I'm going to, I'm going to explain it like what the heck is happening there? MMM.

Speaker 2:          27:11          And then we're going to return it and we're going to start sigmoid function too. Um, uh, so it's got to be very then 0.5 adds type. Okay, so here we go. Big Void. So A and B are both of the final value that we're calculating using those variables that we've already used. Internet. Okay. And we're using the, and we're going to return whatever we returned is going to be our prediction. And what does our prediction prediction going to be? It's going to be a number, it's going to be a one or a zero. Okay. And we're saying only if the value is greater than 0.5. Do we return, uh, uh, sorry. If our value is greater than 0.5 it's going to be a one else is going to be a zero. And 0.5 is just a prediction. It's a probability.

Speaker 2:          28:07          All right. Jay Haas. All right, good point. Good point. Okay, so here we go. Um, oh, thank you ass cut. All right, so those are our functions. Let's go. Let's go ahead and get started. So we're going to set up our initial parameters. So we're going to create our layers. So let me create a lakes. Now it's time to create our legs. Okay, so let me go ahead and do this. Um, okay, so here we go. So the equals our first layer, it's going to be random. That more malt scale equals one side's the four number pin.

Speaker 2:          28:48          So that our first layer, and now let's make our next, I remember this is a two layer neural network. Okay. Two layers. And we're going to create them both the same way. Okay? And I'm going to explain why we're using these variables has our size right after I finished talking this out. Okay. There we go. Two layers, the NW and we're going to, so the size is basically those net number of the number of employers, which just number of input values, which is 10 and the number of hidden layers, which is 10 as well. Okay? So, so 10 and then 10 hours. Okay. Every time. Whereas a stripe and your hair gets their seat. I need to make it more great. Okay. It's there for sure though. Definitely check that out. Okay, here we go. So here we go. Here we go. Um, we're going to say, we're going to say we're going to create our biases. Okay. We, we've used our biases in the parameters, but we haven't actually initialize our biases. So let's use our biases. Okay. So we're going to say our first bikes is going to, uh, use our hidden variable as a parameter. And then we're going to use our next bias, which is going to be, um,

Speaker 2:          30:18          our next value is going to feed the number of Alpha lenders. Okay? Okay. So those are our biases. Okay. And now we're going to generate our data. We've created our layers, we can have our biases, and now we're going to create a variable called parameters, which are what you input as a parameter. It, it's going to be an array. It's going to be an array of all of our values that we've just created, our two layers and our biases. It's just gonna be easier to input it into our training function whenever we call it. Um, my hair is totally open source. If you guys want to have a silver stripe in your hair, go for it. But just like, you know, you can credit me if you want to. I don't, I don't care. Everything about me is open towards take my ideas, take my everything.

Speaker 2:          30:58          Just it's, I'm open source, you know what I'm saying? So parameters and, oh man, it's really hard to not reply to comments cause you guys are so awesome. I don't want to not talk to you. Right? Um, so, all right, so we've created our parameters. Let's generate our data. Okay. X equals NP got random. Dot. Binomial. Um, all right, so we're going to generate our data. And so this is why this is where, uh, that's that, that sample of 300 variables, this is where it's gonna come into play. This is why we initialize it so that we can generate data. We're going to generate 300 samples. Um, and what, I'm going to use a learning rate and a second. And, and so that, so we generated our data and so now we're going to calculate our trans folks, right? Guess what? Guess what guys? Guess what it is time. It is time to train this ish. Okay. It is what I call training times. It is training time. This is going to be awesome. This is our last step is a four loop. We're going to train. Okay.

Speaker 2:          32:12          All right. And I'm going to get better at the video quality and I'm going to get better and everything. So just everyone relaxed. So here we go. So training time, we're going to train this for a hundred peacocks. Okay? So for epoch with everything pot in the range of a hundred. So we're gonna do this for 100 bucks. Okay? Everybody say hi to each other. Everybody's cool. Okay. So we're going to start off with by initializing are our NP error array, uh, which we're going to up. We're going to add data to in a second. And then we're going to say our update variable is going to be the length of our parameters that we just initialize up there. And we're going to also initialize our time because we're going to time how long we want to run this neural net. Okay. We're going to time it. And so that's where that second variable, that second dependency time comes into play. Just for this. Okay, let's do this. Okay. For each data point we want to each data point, we're going to, uh, update our weeks of our network. Okay. Um, we're definitely still fighting. Um, we're definitely stuff like, so

Speaker 2:          33:26          the notion that work is to find the extra value of ones and Zeros.

Speaker 2:          33:34          Okay. So for each data point we want to calculate, we want to update our law, our weight, our waste. Okay. So we're going to say for every value in the range of our input data. Okay. So, and that's, and how do we say how big our input data is? We use the shape function of X. Okay. So we're going to say we're going to take our loss function. We're going to talk to like our last and our gradients by doing what? By using the training function that we just calculated right now. We just, uh, sorry, initialize and declared and then create. And that for our printers comes into play. Just for simplicity's sake, we're just using grant instead of saying Xb, BP, BW, just say prince. Great thing about python. Okay? So that, that's us calculating our last ingredients. Now we want to update our loss. How do we do that? Well, we're going to have two more little loops here. Okay. So we're going to say four J in range up the length of the parameters.

Speaker 2:          34:36          Um, prams, J minus equals update. Okay. Okay. And so I'm going to do one more, one more, and then that's going to be hit. That's, that's literally yet I'm going to explain what I'm doing and why I'm, why I'm doing it. Okay. So, um, uh, so okay. So this is where our learning rate comes into a place and comes into play. This is where our momentum comes into play. Both of those hyper parameters that we initialize, this is where we actually use them. So learning rates, times, gradient, um, plus momentum time, the update value that we just calculate, those two four loops are going to help us calculate our loss. That's going to help us calculate our loss. Okay. And then we want to our error with the loss. That is it. That's it. What am I doing here? Okay, cool. Say, okay, so that's it. And now we're going to print that. That value is 35 minutes in. Okay. How many people would we got to hear someone shout it out, shout it out. It's now let's print out our results. Okay, we're going to print them out and so I'm going to print it out using present, be using and so

Speaker 2:          36:06          sorry, present d and then, okay, cool. Um, lots is going to be percent 0.8. We're going to print out, our lots are going to clean up our time. Like how long has this thing been going on? Um,

Speaker 2:          36:24          and then we're going to say percent. Um, and then we're going to take eat pock. We're going to print out our epoch, we're going to print out our loss. We just going to be the mean value of our care. And then are at the time we're just gonna be timed out, clock and present minus the starting point. So someone check this and make sure that I have, don't have any errors in my syntax on this long epoch loss time point. Right. Point Four. Okay. F S I think that's good. I think that's good. Oh, and so then, uh, uh, that looks a Typo at line 83. [inaudible] 83. What does my typo? My typo is a lens is link. Thank you. Okay.

Speaker 2:          37:20          So, okay, that's it for a training function. Let's go ahead and try to predict something. Okay. So as say, tried to predict something, how do we do that? Well, if we say, okay, let's say x is going to be our random, uh, now you find a binomial value, a one between 1.5, and how many of them we want 10 of them. Okay. So we're going to try to predict something. We're going to try to predict the XR prediction. Okay, go. Okay, well we got to print those, that x value. Like, what are we going to predict? And then finally, our prediction, which we create, we use our predictive method for, let's go ahead and do the demo. This baby. Okay. And our framers, boom. That's it. Let's see what is going on here. Let me, let me run this.

Speaker 2:          38:11          All right. Bye. Can anyone see this? I can see it. It's okay. So let's go ahead and run this. Okay. So I've got number of samples is not defined. Number of samples is not defined. Okay. So online 67 number of samples is not defined. Oh, did I not define that? What I did? Oh, you know what? It's a, it is a, Oh yes, it is training. It is. Okay. Okay. Okay. Okay. Okay. Okay. Yes, yes. Okay. So what just happened, what just happened is I only had a single error and I fixed it anyway. So this train for a hundred steps and you guys are so nice. You guys are awesome. Uh, so

Speaker 2:          39:08          I know, I know. I've got to calm down. I'm just like really excited that this worked. So what happened? That first line right here. Let me, let me make this bigger, make it a lot bigger and make it a lot bigger. That first line is our input data, right? And this next line is our x or values. It's those are our core values. Okay? And we basically said, here's our input data, here's some randomly generated ones and Zeros and then compute the x shore and then get better and better over time. And as you can see, our loss function is getting better over time. See if it's your last 0.14, one, three and then we up here it's like 36 and so it gets better. It's timing it, it's timing the epoch. When the code works, it's alive, it's smart, it works. That's our extra value. Okay, so I'm going to end this with a five minute Q and a and I'm going to post the code on get hub and guys please share my videos. Like I'm trying so hard to hit 50 k subscribers by January 1st so that's the best thing you can do for me is hit subscribe, Jake, your friends and hit subscribe. I'm going to make so many videos. I'm like just getting started. Okay, so five minute Q and a and then we're out of here.

Speaker 2:          40:14          Thank you. Moisten in the box. Thank you. See you at our Sean. Um, what am I trying to predict? Trump. I'm trying to break the XR value.

Speaker 2:          40:22          All right. Can I use a neural network to predict black Friday sales? Yes, absolutely. Um, just look at a history of past sales and trying to create a future sale. You want to put that into a spreadsheet either manually or, I'm sure you could find that somewhere. Um, thank you. Johannes. Uh, I have not blocked you a cross entropy in NLP. Exactly. What's my favorite color? A Sapphire blue. What's your name on gate hub? Ll sorts. Now health. I actually rapped about my name on get hub. If you looked, I have not blocked you. I had rapped about my name on get hub. Just look at my channel trailer. It's a, do you, do you watch the flash? I don't, but I should. What would be your favorite company to work for? You know, man, I don't know if I could work for a company anymore.

Speaker 2:          41:11          Maybe. I mean the best one would be open AI, open AI because it's awesome that that would be the one. Um, could you use a neural net went to classify and multidimensional time series a? Yeah, no, for sure. Absolutely a time series of sequential data. Right. So you'd want to use, uh, a recurrent neural network. Are CNN is good for prediction? Yeah, I mean for image, uh, actually seasons had been used for text prediction as well, like what's going to be the next and then they'd been using NLP. Well, you mind the macro pro 2016 with the touch bar? Yes, I have it. I have it. I have it right here. Well, guess what? It's got USBC ports and my display doesn't use USBC. Uh, there's a little touch bar, so I had to use my old Mac book. So that sucks for apple. Right?

Speaker 2:          41:55          I did say deep mine, but like, I mean the two, I mean, they're both good. They're both good, but I like open AI is openness and democratizing thing. Value. How old am I? I'm 25 years old. Are you on, are you an AI or a Syrah? Geology. I am myself. I am Saroj. I'm also Raj [inaudible]. Raj, two more questions and we're out of here. How did you get into cs? I couch surfed in London. I met a guy named Alex McCall. She was the dude who was like, hello may I'm coding. And I was like, Yo, this text editor you've got here, it's so pretty and colorful. I want to do stuff like that. So yeah, I'm really envious of you in San Francisco. You can make a lot of money and do great things. And from then on I was like, I got to be a computer scientist. I was eight, I was 19 years old. I was an economics major at the time because all I cared about was money. And then I was like, wait a second, there's more to life than just money. All right. So one more question. Um,

Speaker 3:          42:55          okay.

Speaker 2:          42:55          When are you

Speaker 3:          42:58          okay,

Speaker 2:          42:59          uh, oh, here's a good one. Can we use neural networks for Agi? That's a great question.

Speaker 3:          43:06          Okay.

Speaker 2:          43:06          I don't, who knows? I mean, yes, I mean just the idea of a neural network. Our brain is, we know it's a neural network, but like grading, does our brain use backpropagation does or does it? Rain news gradient descent. It's our brain using recurrent networks. Is it using convolutional? It's like all these different types of networks we don't know, but we're getting really close and the state of the art is being broken at an increasingly fast rate. Um, every day, uh, deep learning right now is more physics was in the 19 hundreds. The Einstein's and Newton's and Marie curies and famous, uh, uh, scientists of computer sciences are, are, are, are alive right now. Oh, there, either they're in middle school or elementary school or they're in the field. So it's a very, very exciting time for your watching and awesome. So that's it for this tutorial.

Speaker 2:          43:55          Thanks everybody for watching. I'm going to have with my video come out very soon on Friday and then a very, very, very, very special video that's going to come out on Monday and it's going to be awesome. It's going to be awesome. Okay. It's going to be the video. I've been most excited to make ever. All right, so thanks guys for calling out the number of people here. Thanks everybody for watching this video. Thanks Brian. Christina Aditya, Andre, Mikael, a cigar. Sean, Ian Elfish, Danny top 1% multi ship. [inaudible] you guys are awesome. Thank you for watching. Please share my videos. I love you guys. And for now, I've got to write so many video scripts. Uh, so thanks for watching.