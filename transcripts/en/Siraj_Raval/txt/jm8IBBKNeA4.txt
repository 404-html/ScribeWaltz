Speaker 1:          00:00          All right.

Speaker 2:          00:03          All right. We're about to go live. I'm going to start streaming and there we go. Okay, here I come. I'm going to about to go live. All right. Hello world. It's a Raj, and in this live stream, we're going to be solving this Kaggle competition. This is a $100,000 salt identification challenge, right on Kaggle is, you can see right here, there's a month to go still. So there's still time to, to compete in this challenge. And there are 2100 teams across the world that are competing for this 100 k prize money and of those teams, I am one of them. And what I'm going to do is I'm going to try to win that prize money in this livestream because I'm crazy. We'll see how far I can go, right? And I'm going to do it with you. What I want you to do, both my live stream people in the house right now and my recorded viewers, we're going to later watch this video, is I want you to fight, fire up a Google colab notebooks.

Speaker 2:          01:00          I just go to colab.research.google.com and I want you to fire up a notebook. Why am I saying this? Because you don't have to deal with the dependencies. That's why we don't want to have to deal with that to locally. And we can use colab to do that. Okay. 100 K. Exactly. So that's what we're going to do in this live stream. Let's, let's take a look at this data. Okay. And you might be wondering what model are you going to use for this? Right? Well, I'm not going to tell you because that's my secret. No, of course I am. It's called a unit. It's a unit. It's a type of convolutional network. We'll talk about that later, but we've got to do some exploratory data analysis first. Okay. Hi everybody. Okay, so that's what we're going to do. And let me just say 10 names and then we're gonna get started. Vodoun Satya at Sun Deep Sigh. Steve, Omar and [inaudible] and [inaudible]. All right, so and smart sniper. All right, so I'll do a Q and a later on, but let me just get this, get this thing started. Okay. So

Speaker 2:          01:58          they want to, so they want to find, they are oil and gas companies. They want to find where salt deposits are in the earth. And you might be wondering why, because it turns out that where there is a lot of salt, there's also, there tends to be a lot of oil and gas. And why do they want oil and gas? Because they can sell that, right? And so these companies make money by drilling under the earth, finding oil and gas and selling that. And so right now a lot of this is done by humans, right? Um, geophysicists, they're trying to predict where salt is going to be. They have to use human vision to analyze these seismic images and see and try to discern whether or not salt is present. And if it is present, then it's a good idea to drill there. But the problem is if a human wrongly classifies some piece of land as being salt rich, then they have to waste company resources by drilling in that land, right? So they don't want to do that. They want to be very efficient. So we want to use machine vision to figure out which of these thousands of images have salt. And once we find that, then we can dedicate all of our time and resources and human capital towards that region of the earth to find the oil and gas. How do we do this? Well, well, it turns out if we look at the data,

Speaker 1:          03:16          okay,

Speaker 2:          03:16          the data that they've given us, what they've given us is a set of images inside of this test and train, test and train file. There's a train dot CSV. There's a test.zip and this contains images. Okay. So,

Speaker 1:          03:29          okay,

Speaker 2:          03:30          let me just join this competition real quick. Are, there we go. So, um,

Speaker 2:          03:36          right. I've accepted the rules. There we go. There we go. So, uh, it's got an ID and it's got a mask. And we're, what we're going to do is we're going to visualize this inside of Colab and then we're going to, uh, perform some exploratory data analysis and then build our model. Okay? So that's a bit of our background. Let's just get right on into this. Okay. So I want you to in Colab with me certain installing for dependencies that we're going to need that are not to pre-installed here. Okay? So the first one is going to be, uh, image io because we want to deal with images, right? This is an image data set. I'm also going to show you how to import a Kaggle Dataset into colab. Okay? So we're going to do that as well. So we're going to, we're going to install image io, which is going to help us with image processing.

Speaker 2:          04:20          We're going to also install Pi torch to build our deep learning model. Are you net model? Why am I using Pi Torch? All the cool kids are using Pi torch. I am one of them. And so are you probably, so let's just, let's use some pie torch. Okay. Also, tensorflow is cool. I know Google's watching. I know Kaggle is out there. I love you guys as well. Look, I am so much less, um, how do I say this? Religious about a specific framework. I, I, you know, sometimes you got to use Pi towards, sometimes you got to use a little bit of tensor flow, whatever works, whatever. If there is an existing implementation of whatever you want to build and it's already on get hub, I don't care if it's in Kuda just use it, right? Whatever it is, they're build off of it and then switch it to whatever. All right, so we're going to use Pi torch.

Speaker 2:          05:09          What are you guys even saying that seriously disrupt my core with AI at Google. Amsterdam, you guys are like on something. Anyway, so Pi Torch Kaggle as well. Of course. That's how we're going to import the datasets. And then lastly, um, Pi widgets, which is going to be like a little graphical thing for, to see how the model is, you know, training, it's a little visual thing for us. So it's called Pi widgets. So what I'm gonna do is I'm just going to go ahead and install all of these with this. Uh, that's it just for, and now he's going to do some dependency installs for us in the browser. Using a GPU is a GPU runtime environment. Boom. Image io successfully installed. Sucks. Oh, torch is going to take a while for sure. In the meantime, while this is loading, let me answer some questions here.

Speaker 2:          05:53          Hi. Can you say some use cases for training on KL loss function. Oh Wow. Interesting. Yes, for sure. So neural style transfer is one example that, so for those of you who don't know, k l stands for, I'm going to mispronounce this cold back. Leibler divergence. Okay. The coal back Leibler divergence. Um, it's also used in some adversarial model. So generative adversarial networks. It's used sometimes there. Um, but neural style transfer, whenever you want to take the style of an image and put it on another one, whenever you want to do an adversarial model, that could be a good use lost function actually for adversarial models. That's one of the hottest areas of research right now is figuring out what a good loss function for an adversarial network would be. Um, those are two. Do you watch anime? No, I used to watch dragon ball Z, but now I just make content and run the school of Ai.

Speaker 2:          06:47          That's, that's what I do. Hurray for Pi Torch. How do you easily decide the ml algorithm to choose that is the, that is the value that you bring as an AI researcher, as an, as a data scientist, as a machine learning person, figuring out what model to use. Right? That's, that's what you bring. So how it's not, it's not a simple process. You just have to get a feel for what all these models are good for and when to use them. It's an intuition that you have to build. Um, is Google colab faster than spider? I have to say. I haven't used spider before. Can you explain about active learning? Active learning is learning in real time. It's adaptive. Um, I would say reinforcement learning is, can be mostly categorized into that field of particularly relating to optimizing some network of, say, Internet of things devices or a supply chain or some kind of real time pipeline of delivery or whatever.

Speaker 2:          07:46          How do we get better at Bda? Look at get hub repositories, look at these Jupiter notebooks, watch my live streams to hit subscribe if you haven't yet. That's how you get better. Please make augmented reality models in videos, um, augmented reality models of machine learning. Interesting. When, uh, there is a good augmented reality device that's accessible to consumers. I will do that. Not Magic leap. Apple will do it. Let's just be real. Apple will be the one to do that in 20, I think it's 20, 22 or 2020. When apple releases their ar device, that is the moment that ar becomes mainstream. Give me motivation please. Okay. I, the fact that I'm even here, it should be enough motivation for you. Right? I could totally mess this up right now, but that's it. M and M or MGK. Um, m and M. Okay. Okay. Great.

Speaker 2:          08:39          So this installed. Okay. We don't even need Pi Pi widgets. Actually that, that's just a nice to have. Let's keep going here. So now we're going to import our dependencies. So we're going to first import ols. That's going to be for our file input output because we're going to have to deal with files. We're going to import num Pi to do some matrix math. We do that every time we do machine learning. Of course, we're going to import image Ieo, like we just installed a map plot lines so that we can do some exploratory data analysis panda. So we can do some data, preprocessing a porch, which we already installed.

Speaker 2:          09:15          And then, um, we need to just in case we need it, like a backup dataset. Let's important the torches a torches like backup data thing. So that's that. That's, that's US importing our dependencies. Okay, so now I'm going to show you how to import data from Kaggle into Google colab. All right, so that's, that's what I'm going to do here. So let me just compile this import. Great. It worked. And so here's what we have to do. We're going to say. So from Google, colab import files, and we need to import a specific file here. Let me show you what I mean. So we want to connect Kaggle took Google colab. So we're going to run this and so this is going to give us an interface to upload a file to Google Colab. So what we're going to do is we're going to go into our capital account, uh, go under my account. This is like a throwaway, like super old accounts, so I don't even care. And then go to go to the API and create a new API token. Okay. What is going to do is it's going to download a file called Kaggle. Dot. Jason. Okay, I'm going to rename this to Kaggle. Dot. Jason, cause I already have one. Kaggle. Dot. Jason. Good. And then we're going to upload that file to Google Colab. So I'm going to say, um, choose files on the desktop, Kaggle. Dot. Jason. And uh, I did that.

Speaker 2:          10:40          Okay.

Speaker 1:          10:42          Okay.

Speaker 2:          10:42          Right. Kaggle dot. Jason.

Speaker 2:          10:46          There we go. Okay. There we go. Upload it. Awesome. It's there now. So now that that's there, let's check if it's there. Okay. So what we can do is we can run a command line, like a command here. So we can say, ah, let's do an ls and let's ensure that the file that we just uploaded is indeed there. It's, it's right there. Perfect. So now that the file is there, we can see we're connected to the cow goal Api. And the reason I did this is because we want to import that data set directly from Kaggle into colab. And this is an important skill, by the way. You're going to need this. So pay attention here. Um, so what I'm going to do now is yes, all this script is going to be uploaded right after the live stream. Guys, there has to be some kind of pomp and circumstance here. I can't just give you everything from the start, right? You got it. It's got to be a journey so that, that's how it goes. So now let's do some configuration. So the Kaggle Api client actually prefers that we store, um, that we store it somewhere. What's the tilty till the mark? Like, um,

Speaker 3:          11:55          this thing, right?

Speaker 2:          11:59          So, uh, we need to store it in the Kaggle directory. So it's, that's where it's going to expect it to be. And then we can, once we do that, we're going to copy that file to that directory that we just created. Okay. That's what we're doing right now.

Speaker 2:          12:16          We just created this directory that's Dot Kaggle directory. And once it's there we have to see h moderate. Can anybody tell me what ch Maude does? A little bit of Unix, a quizzing for you. Hello world. It's her garage. So somebody came here just for that. So I thought I would. So ch mod 600 and this is for permissions, not 6,600 and once we have that, we're going to say, just give it the permissions that it, that it needs because we're going to access this file in a second. All right, so that's it for our file configuration. All right, good. And now check this out. Check this out. We can download our data set directly from Kaggle. Good. Exactly. Yes. Eunice unix permissions. Yeah, exactly. Copy that. I don't care. You know what I mean? So, uh, we're going to download it exactly from, uh, from there. So what we can do, you go here, see here it is. We can just copy this and paste it here. Okay. And now we can download this data set directly into

Speaker 3:          13:26          colab.

Speaker 2:          13:27          Yes.

Speaker 3:          13:29          Good, good.

Speaker 2:          13:35          Yup. That's, that's how it should be. That is how it should be. Now I'm going to unzip it. So I'm going to say, well what do we have here?

Speaker 3:          13:47          Okay.

Speaker 2:          13:48          Okay. It's all here. All of those files are now here with an ls. Remember exclamation point before ls. Now once we have that, we are going to want to unzip this train.zip file, right? We, cause we want to see these images, we can't just have it be a zip file. So now it's going to unzip all of those images and there are quite a lot, I think there's 1800 of these images. When will the school of Ai Workshops start? That is a great question. We have a lot to do and um,

Speaker 2:          14:21          look for midterms, midterms. So five weeks in midterms or when I'm going to feel like we'll all be ready to do, uh, a little, you know, global hackathon kind of deal. No promises, but we're going to do something big for sure guys. There's way too much that's coming. I don't want to say it. I don't want to, I don't want to say what's happened, what's going to happen. Like there's too much. You guys aren't even going to believe it, so I don't even want to get too hyped right now. So there's a lot of stuff coming like in general that it's going to be, so anyway, so we downloaded all those images and a, where is it? Okay. That's a lot, right? That's a lot of images. Oh my God, hold on. That's a lot. Am I really going to scroll through all this?

Speaker 1:          15:06          Okay.

Speaker 2:          15:07          Wow.

Speaker 1:          15:08          Okay. Hm Hm.

Speaker 2:          15:17          Okay. There we go. So those are all of our images.

Speaker 1:          15:23          Okay.

Speaker 2:          15:23          Hello Germany. Okay. All right. So now we're going to create a class to represent our data set. So we downloaded our Dataset, we connected it from Kaggle, and now we are going to create a class for it. Okay. So our class is going to be

Speaker 1:          15:40          okay.

Speaker 2:          15:41          Just see where we are right now. Cool. Cool. Cool. Everything's going well. Our class is going to be

Speaker 1:          15:48          okay.

Speaker 2:          15:48          Um, tgs salt data set, that's what I'll call it. And the input will be the Dataset that we have. All right, that's it. So this is just the basic like preprocessing class, you know, simple stuff. But we're going to initialize it with the location of the Dataset and the list of files. By that I mean the images. Now, um, let me set that root path to remember this is the constructor function. So this just some basic initialization steps here. And once we do that, now we're going to create to get her methods the first get her method. This is really just for us to be able to access what's inside of this, this inside of the data. We're just gonna return the length of the law, the number of images that we have. So, you know, we just needed a function that will tell us how many images we have. That's, that's it. And then we need another function that's going to get one of those images so we can actually process it and, and do things with it. So by the index. So we'll give it a value, an index value. And we'll say somewhere at this index we want to, um,

Speaker 2:          16:58          we want to, uh, retrieve whatever you've got. So the file id is going to be, uh, we're going to give it the index. That's going to be the place in the file list array. That's gonna be our file Id. And then we're going to give it the image folder. And this is where we use the [inaudible] command that I talked about earlier, the operating system or not command the library. And we're going to join the root path with the images.

Speaker 1:          17:24          Yeah.

Speaker 2:          17:25          Now it's not enough to just have the folders. We're going to have the path as well, the image path. Okay. So we're going to do this not just for the image, but for the, um, the mask. So let me talk about the mask as well. We have a mask.

Speaker 2:          17:44          [inaudible] image folder, file id, and plus. Dot. P and g are right. That's our image folder in our image path. Okay, so file ID, image folder, image path. And that's good. That's good for that. Now, so this is, let me just comment this. So this is going to be for heart image folder plus path. Now we actually need another one for the uh, the label folder plus pass. So what is our label here? So let's, let's, let's, I'm going to visualize this and then it'll be much more interesting, but right now I want to just say the, the label is going to be,

Speaker 3:          18:34          yeah,

Speaker 2:          18:35          uh, I'm just going to copy this actually. The label's going to be the, the mass image of the salt and I'll show you exactly what I mean by that, but it's going to call them, it's gonna be called mask folder. It's gonna be called Mass Path. And we're going to use the same operations, except this is going to be the root path. There's going to be called masks. And then this is going to be called the mask folder. Okay. Mass folder plus file id plus P and. G. That makes sense. Okay, now we have both of those. Now we can read it, read it, and store it in memory. So remember, these are not formatted for proper data preprocessing. That's what we have to do. So we're going to convert these into byte arrays using the image io library. We're going to read whatever that, um, that data directly, and we're going to convert it into a byte array right here and p. Dot. You int eight?

Speaker 3:          19:28          Uh Huh.

Speaker 2:          19:29          D type, I think that works. I feel like this should be highlighted. D type equals you int eight, right?

Speaker 3:          19:41          No.

Speaker 2:          19:44          Yeah, that's fine. So that's that. And now we want one more and that's gonna be for the mask and that's going to be an umpire or re, uh, we're gonna read that one as well. Now that's gonna be the mask path mass path. So now that we have both of those, now we can return them both. Right? So this is, this is just that basic class. We want it to make data preprocessing step and now we can finally return both the image and the mask. Why byed array? Because that's, that is what our neural network will, it will expire. Expect it's going to need a vectorized format. Uh, right. It's going to need a vectorized format. Now the number of dimensions that your input data should be will depend on your neural network for the unit in particular. We're going to do it this way and I'll tell you why when we build the unit. Okay, we've got some people typing in Russian in the chat and all sorts of languages, which is amazing. It's amazing to see that. Uh, we have a very global community here. Um, and yeah, I just feel very honored to have everybody here, both my live and my recorded viewers. Thank you for being here. Okay, so now that's it for that, for that a class.

Speaker 3:          20:51          Okay.

Speaker 2:          20:52          Wow. It did not. It actually worked. Okay, let's keep going. I'm getting too good at this. So now we, we created that. Now we're going to initialize it. We're going to actually use that class that we just initialized.

Speaker 2:          21:06          So we'll read or use panders to read that CSV file first, right? The train though CSV, which contains the ids for the images as well as the mask. And I'm about to visualize it so we can finally see what I'm talking about here. Um, there's also one more data point that is really interesting and that's the depth data and that's in its own CSV, the file. Now what we can do is we can think of that def file as a feature that we're going to input into our model, right? So just we'll think of it as a feature.

Speaker 3:          21:35          Okay.

Speaker 2:          21:35          And, uh, if we think of it as a feature, then the idea of using a neural network where the network is just, we don't have to do any feature engineering, right? We can just input all the features we need into our model and it's going to just figure out what the relevant features are, right? That's the idea. So do we really know if it's going to be, do we really know if it's going to be a relevant teacher? Who knows? We're going to have to visualize it. All right, so now that I, okay.

Speaker 3:          22:10          Dot. Values. Oh right list. Good depth. What's the deal here?

Speaker 2:          22:27          File be depth dot CSV.

Speaker 3:          22:32          Really?

Speaker 2:          22:35          Oh, depths with an S. Okay. Now let's visualize this.

Speaker 3:          22:44          Okay,

Speaker 2:          22:46          so now that we've imported it, we can, we can visualize this. So we're going to say a plot, a two by two array. This is going to be an image using the image and the mass. So we're going to plot them side by side so we can see what the differences and we can also, um,

Speaker 3:          23:03          okay,

Speaker 2:          23:04          we can just analyze it. All right, so, um, we're going to create a plot

Speaker 3:          23:10          here

Speaker 2:          23:14          and image show the image. Okay, so actually no, no, no, no, no. We're going to say subplots we have, we're going to create a subplot. And so for each of these axes we're going to plot a different image. So we have to plot our original image. Now next to it, we're going to plot our label. What is our label? Our label is going to be our mask. You might be thinking, why is a mask considered a label? Well, the reason is because we want to segment the salt and we want to say that it is different.

Speaker 3:          23:53          Okay.

Speaker 2:          23:55          From the input image. Okay. So set title, and this is just for, to, for make to make it pretty a little bit image and then set the title, the mask, the mask, the movie, the mask. All right. That should work. Good. Now we can finally print it out. All right. Just two lines. And then we can finally see these images for let's just do five images, let's say, and we're going to print out both the images and the mask from our Dataset and which ones? Well, let's just randomly pick.

Speaker 3:          24:37          Yeah.

Speaker 2:          24:37          One between zero and you know, however big the data set is. So the length of the Dataset

Speaker 3:          24:47          and um, yeah.

Speaker 2:          24:53          And then we'll finally plot it using our function that we created earlier. Image mask. What's the deal? Aix. A R R is not defined.

Speaker 3:          25:14          Oh, okay.

Speaker 2:          25:23          [inaudible] is not defined.

Speaker 3:          25:27          Oh, oh, right. That's a, that's a dot method. Gotcha. How about now?

Speaker 2:          25:38          Yes. Good, good. Very good. Very good. Check this out. So, so supervised learning, it's super easy. We're going to get into reinforcement learning next week. Don't even worry. I have so much reinforcement learning content coming for you. You won't even be prepared. That's how much I have coming for you. But right now let's focus on supervised learning, right? So Soult identification, what they've done here is they created a mask and what the mask does is it segments the salt in the seismic image. So on the left you're seeing the raw seismic image of the, of the earth, right? The layer of the earth that it's, that it's captured right next to it is the mask and the mask is our label. Because what the mask does is the white area is uh, where the salt is or no, the black area is where the salt is. So it segments out the salt for us. So we can see that. Okay, in the first image that's all white. So there's no salt. But in the second image, yes there is some salt there in the third image, not so much. Fourth, fifth. So that's how we are.

Speaker 3:          26:46          Okay?

Speaker 2:          26:47          That's the label that we're trying to learn. There is a mapping. So consider the input a matrix of numbers, a pixel values, right? Cause it is, it's a matrix of pixel values and we want to be able to map, learn that mapping between that matrix of pixel values and the label matrix pixel value. So just think of it as the lines connecting all of them. And there's this, there's a, there's one single function that we can learn and this is the perfect optimal function that exists somewhere out there in time and space on this hyper plane of curvature of optimization, right? The optimization landscape. It's like a bunch of hills and valleys and we want to find the ideal parameters for our function, right? That function is going to be this beautiful black box where we input the image and it's going to output the mask, which is going to tell us whether or not.

Speaker 2:          27:35          So is there, the only problem is we have to learn this function. We've got to learn those weights. And the search space for those weights is massive. They can be anything. So what we're going to do is we're going to use a very good model, a very, I don't want to say great, but a very good model to learn this function. Okay. So great. So now we saw that. Okay. Uh, let's just, just for fun, let's just also plot the distribution of depths so we can see, because there's this other interesting statistic that we could use here called

Speaker 2:          28:12          hello, hello Poona and Hello Paris. We even have beans in the house, guys, deans, our deans are here to help you write the deans of school of Ai. And there are about 800 of them were not accepting applications for new deans. We still have to build the [inaudible] infrastructure that we've, I'm working on right now. But after a while, we will start accepting new applications for Dean's. Give us maybe like two or three months, but our deans are here to help you and they're 800 of them. So definitely, you know, ask them questions. They're here to help. Um, yeah, why so salty? I was waiting for someone to say that. Good job. Truly, truly a funny person. All right. So, um, ah, so now we're just going to plot the distribution of depth depth just because, um, it'd be interesting to see and then we might use that as a feature depending on what it looks like because we want to see if there's a correlation here. We'll just call it depth distribution.

Speaker 3:          29:12          Okay.

Speaker 2:          29:13          It could be interesting. Okay, there we go. So what this is showing us is a distribution of depths for all of those images. So the depth is a number, it's a scalar. It's a single number between z between zero and 800 as we can see here. And what it's doing is it's measuring, this graph shows us that, hey, most of the, um, there it's most of the depth values, the, the, the range of depth values is between four and 600. It's about 500 ish. Now what would be interesting is to see the correlation between these depth values and uh, the, the, the, the occurrence of salt in the image. So maybe we could even, um,

Speaker 1:          30:00          okay,

Speaker 2:          30:00          maybe we could even see what that looks like. Right. So, um, there is one thing I want to mention though is that,

Speaker 2:          30:10          is that the, okay, so the image is actually if we look in the data file, we'll notice the image is actually in this run length in coding mask. It's an r l e mask, which is just a bunch of numbers. So run linkedin coding by the way is, I mean there, there are many different types of encoding schemes out there, right? Um, but this is one of them. And it's a compression scheme and run like the encoding is basically Tldr. It's a, it's, it's a lossless data compression technique that says, you know, remember, remember images or matrices right there, matrix. He's a pixel values. And in a single row the same value could be repeated multiple times, right? So let's say that value for simplicity sake is just the letter B or it's a number two 55 and if it's repeated eight times in a row, well we can just say eight B or eight, two 55 as we see right here, represents all those values. And so if we just store it like that, that's less space. And it, and then we can like we can decode it. I was looking for the word, we can decode it later. So, um,

Speaker 1:          31:14          okay.

Speaker 2:          31:14          Well we need to do is remember how we said we have to do some data preprocessing to input it into our model. This is another data preprocessing step we have to do. We have to convert that rung linkedin coding scheme, um, into uh, an actual vector that we can input into our, uh, unit, which I'm going to talk about before we built. By the way we have to go over how units work.

Speaker 1:          31:38          Okay.

Speaker 2:          31:38          So we have our width, we have our height.

Speaker 1:          31:42          Okay.

Speaker 2:          31:42          We have our number of rows and columns, right? Just the width and height of our image. Okay. So I'm going to do a try, um, block here because I want to, because uh, there could be an exception and we'll, we'll catch the exception there. But what we first want to do is get all of those numbers and then those pair, so by numbers, remember has had eight B. So eight is the number in that list. So we want to get those numbers. So get all of those numbers for the RL e string and the string is what is specifying? What's that R l e file.

Speaker 1:          32:20          Okay.

Speaker 2:          32:22          You guys are all over the place in the chat. By the way, you guys need to not be talking about VGG. Okay, we're, we're talking about units. Okay. Vgg is not as good for this. It is objectively worse. That's I'm saying that right now a VGG is objectively worse for this problem and a unit. Why? Just pay attention and you'll see. You'll see why. Okay. Um, all right, I love you guys by the way. So let's get back to this. All right, so that's our, those are our only numbers, but that's not enough. We also need our pair of values. Remember those pairs? There's just, there are a bunch of these parents, so we're going to retreat both of those and it's going to be a num, py or ray. We can reference it using those, the numbers variable that we just created and we want to reshape it so that it fits into our network just like that. Now that we have that, we can create our image. We're using this, um, image variable and we're initializing it as empty, but it's dimensions are the rose by the columns, uh, that we, that we've already defined.

Speaker 2:          33:27          And uh, you int eight. Okay, now, now we've defined this empty image and now we can fill that image with the values that we've created here. So we'll say for index length in our l e pairs, get the pixel value and then store whatever we have defined right there. Okay? So that's in our for loop. And now we can do some reshaping and we can say, um, reshape that image by the rows and columns and then, and then, um, send it to its transpose because, um, that's going to flip the black and whites. Um, so it's just easier to read because the screen is already white in the background and that's the end of our tri loop. And so we could say, let's, let's catch any exceptions. Um, if there was an empty image, that's the exception, then we'll just say, okay, it's just an empty image and we'll return that. Okay. Okay. That's that. I think this needs one more. Yup. And at the very, very end well returned the image. All right. That's it for that. That is it.

Speaker 2:          35:06          I'm your role model. Thank you very much. I like I, I accept your responsibility. Okay. I accept that role. I accept the role of being your role model. I will not let you down. I will continue to teach AI and continue to inspire and educate for free. That's the real deal here for free. Watch me develop, deliver some quality education for free in a way nobody has done before. Watch. Just watch. Okay. But I guess I'm doing it right now, but it's, this is the beginning. There's a typo. How many hours do you study? Let me just take a one question. How many hours do you study? I know this is a valuable question, so I'm going to take it because I analyze all my data and I read, you know what audience retention looks like, what kind of content the audience likes. I'm always analyzing data, by the way. And I know this is a very viable question. I study probably, um, less than I did before because as you build knowledge, you need to study less because you're just billing. It's a dependency graph of knowledge. So I probably study like a five to 10 hours a week, 10 hours Max. Most of it is output. It's not even studying anymore. Um, most of it is output. Although with reinforcement learning I am studying more. So then probably nine to 10 hours. Uh, that's it.

Speaker 1:          36:25          Okay,

Speaker 2:          36:26          cool. Let me answer one more question. What is the Kaggle challenge? Cheers from Italy. Um, good artsy. I love Italy but we won't get into that. Um,

Speaker 1:          36:39          okay.

Speaker 2:          36:40          Thank you very much. Please do a video about how to get started in machine learning for beginners on Kaggle. That's a great idea. And I'll add it to my cue. All right, line 13 rows. Oh, thank you very much. See, thank you for that. Good. That was it. No, like I said, we want to measure, um, we want to see the proportion of like how much depth is it felt effecting the salt. And so what we're going to do is we're going to write that out right here. Okay, so salt.

Speaker 1:          37:22          Okay.

Speaker 2:          37:24          Okay. I'm going to focus you guys. You're very interesting. The questions are very interesting here, but I've got to focus here. So the salt proportion is what we're looking at right now.

Speaker 2:          37:41          Now we want to measure how salty and images, what we want is a single metric to say how salty is an image? How great would that be if it was like it's 20% salty? Well, and then we can have some low, you know, some threshold value where this minimum amount of salt requires, uh, you know, further eyes to look at this. So we want to develop that metric. So what we can say is, let's see all the unique values that we have in our image, right? Those unique numbers. And we're going to count how many unique numbers there are. That's a very personal question, my friend. All right? Um, return counts one, two, or one. Dot. Okay. And that's how the total number of values that we could have. That's it,

Speaker 3:          38:35          right? Yes. All right.

Speaker 2:          38:41          No, now that I've defined that file, I'm going to create a training mask. Do you guys saying line eight? I'm fine. Train mask. Mask. Okay. So I'm going to cry. I'm going to merge the depth. I'm going to take that of value. I'm going to merge it into the uh, other, uh, data frame object that I have. That's what I'm doing right here. Cause I wanna I wanna use that perhaps. Probably. Yes, I'll use it. Okay. So that's what I'm doing here. I'm going to merge that one more line. The salt proportion?

Speaker 3:          39:30          Yeah.

Speaker 2:          39:31          Train mask. Mask. Yeah, I know about the title. I don't care. It was fine. Uh, because we're already, we were beyond that. Okay. We are beyond

Speaker 3:          39:45          proportion pecs. Good. All right. I think that's trained math sol proportion. Okay.

Speaker 2:          39:57          I guess that, and so now we can literally merge it and why saying let's merge using the merge function, the depth and then we'll store it in this column right here and then we'll see what we have. Now let's see what we have. Good. Okay. So we have our salt proportion, that's its own column and a, we have our mask values, we have our r l e values, we have our id, we have, and we have our depth right here under z. So we have all of the values we need and deed. What deep learning is going to do is going to figure out the features that are important. Okay. So there's one more exploratory data analysis step that I want to do here. Um, and that is comparing the depth to the salt a ratio and seeing how they relate because that would be really useful to see that wouldn't it be? It would be very useful. I know. So proportion merged and then z and now we want to see the, what am I going to call this, the proportion of salt versus depth.

Speaker 3:          41:20          Hmm. Hmm. Interesting. Cool. All right.

Speaker 2:          41:32          Uh, let's see how correlated that is. NP P. Dot. Let's see. Um, one more thing before we get into, let's just get into units. So yeah, depth is one thing. We have it in our, we have an entire feature set. It's one of the features. Let's keep going here. So this is a computer vision problem, white, we're, we're trying to learn the mapping between the input and the output data. And so there are a bunch of models out there that we could use. There's resonate, there's inception, there's Alex Net, there's Vgg and this is a map of the accuracy to the amount of operations that requires. This is a map. So it was noticed that VGG requires the, a lot of operations, but of the most accurate is if we go all the way up. That's inception before. But there's one model that's not on this list. So let's see.

Speaker 2:          42:20          This is the accuracy by the type of model and ynet wins out all of them. Ynet but a of all of those models, we're going to use zero. We're going to use a unit. And you might be asking why are you using a unit? Because think about our specific problem. We are not just trying to do a classification between different images. We're trying to do a classic, we're trying to do a segmentation. So we're trying to do classification, molten multi-class classification inside of a single image right here is salt here is not salt. And it's been shown that unit is uh, a standard architecture for computer vision when we need not to only segment the whole image by its class, but also to segment areas of an image by class, right? So produce a mask that will separate images into several classes. Um, but the, the, so there are downsides as there always are. There are many layers, so it takes a significant amount of time to train. Okay. But you know, there's always an up and down side.

Speaker 2:          43:25          Could we use auto encoders? We could, we could use, I mean the unit is essentially an auto encoder, right? Let me talk about how that works. So, um, but in contrast to a particular, in, in contrast to a regular auto encoder, it predicts a pixel, why segmentation map of the input image rather than classifying the input image as a whole. Okay. So it's saying like, here is the every pixel in the original image and it's asking a question to which class does this individual pixel belong? Okay. And that kind of flexibility allows it to predict different parts of the seismic image. Salt, not salt. What it's doing is it's passing the feature map from each level of the contracting path over the, the analogous level to the expanding path. So let me talk about how this works. Okay. So here's an image of a unit.

Speaker 2:          44:14          Okay. So it's an, it's an encoder decoder architecture and what it specifically, and here's another image actually. It's okay. Um, all right. There's a lot to time clearly. I'm excited right now to explain how this works. So let's start about, let's, let's start with the first layer, the encoding layer. What did the first layer contains? Is a bunch of encoding blocks like convolutional blocks. And what it's doing is it's downsampling. Okay? So that first part is we're applying convolutions followed by a Max pool downsampling APP, a APP technique to encode the input image and to feature representations at multiple different levels. So here is an example, okay. Of what that looks like. We're saying do a convolutional, so a convolution is like a flashlight. It's if we imagine image as a matrix of pixel values, a convolutional operation says let's for every pixel value in that image, let's do a matrix multiplication by our, our, our, uh, feature map that exists, that that's a part of that convolutional filter and let's multiply it and we'll get a result.

Speaker 2:          45:21          And that result is a bunch of feature maps and we continually do, we continually input those feature maps. And there are more and more and more as we go down the list of blocks until we get to the output. But it's continuously creating more and more feature maps by applying a convolutional flashlight. It's like a, it's like a flashlight because it's, it's, it's going over every value in every row for every column until it does it for all of the values in that convolutional image [inaudible] for that image. And once we'd done the convolutional operation, then we're going apply at pooling operation. And what a pooling operation does is it saves us time, right? It saves the algorithm time because it only looks at the Max value for Max pooling inside of a specific region. So it's saying what's the most relevant part of the image?

Speaker 2:          46:06          And let's, let's, let's propagate that forward and forget the rest. And by downsampling what downsampling is, as is a fancy word for taking a high resolution image and making it a low resolution image. It's compressing that image. Now the, there's a second part, the decoder of the network that consists of upsampling and concatenation. Now I know a lot of people think, why don't want to call this de convolutions, I want to call this upsampling. And so what upsampling is, is it's taking a low resolution image and it's creating a high resolution image. And so it's, it's doing the opposite. And so we can see what that looks like right here. It's still convolutions. It's still, um, an activation function. Relu uh, look at my video activation functions Saroj for more information on how, which activation function you should use. And then once it's done that it's going to output that.

Speaker 2:          47:00          Now here's the really interesting part. So if we look back at the image, what makes a unit really interesting are these lines right here? Can anyone tell me what these lines mean? And they're also here. What do these lines mean in this a unit? What is that right? The answer, by the way, it's a skip connection. What it's saying is right. So the check itself, the data is propagating right from every convolutional block, right? But while it's doing that, it's sending what it computed across the network to the, to the other end of the network, to the decoder layer. So it's passing info, not just forward. It's passing it to the side to the decoder. And that's a skip connection. And the reason it does that is that this improves accuracy apparently, but it's a skip connection and we can see exactly what that looks like. So rather than continuously just writing out a bunch of the same lines because it just convolution Max pooling convolution, I've art, I've actually prewritten this, this, um,

Speaker 2:          48:02          yeah, skip connections. Yeah, that's what I said. But yes, exactly. I'm actually prewritten in out here. So let me show you exactly what I mean. So, right, we have our input that we input right here. We have our feature map and then we input that directly into the first layer. Right? We apply our activation function to it and then we continually, uh, say convolution pooling, send it to the next layer, convolution pooling sends to the next time we do that for four convolutional blocks. We join all of those. We can catenate all those values. Okay. And to the [inaudible] features is going to be our, um, our lowest level representation of the data. And that's what we input into the D Kotler. And so this is the, let me just write that down. This is the decoder layer. Okay. And so once we have our decor layer, we do the exact opposite in that we're doing a UN upsampling up a operation here.

Speaker 2:          48:55          Now here is where that skip connection is happening. Notice this contact Nate. Skip connections. Million exclamation points though. So what are those? Look, I'm being silly, but uh, it's connecting. See you. Six to see four. Where's [inaudible]? Well, [inaudible] is up here. Okay. It's, it's doing the same for you. Seven to c, three c three is up here. It's doing the same thing for you. Seven to eight to see, to what we're see too, it's up here. So that is the stip connection operation that you're seeing programmatically happening right here. Okay. So convolutions pooling, activation function, repeat convolutions, pooling activation function, repeat in code, decode, skip connections, that unit, good for multi-class calcification inside of an image, Aka Image Segmentation, Aka our problem is what we're doing. Why four layers? Quite great question. So Andrea carpathy and his blog posts, um, uh, the unreasonable effectiveness of recurrent networks. First stated that, um, there is a diminishing return to the amount of layers that you can add to a network.

Speaker 2:          50:10          So the more layers you add doesn't necessarily mean that it's going to be better. And there's a kind of sweet spot between three, four, five, six layers, one sections like 30 layers. But for computationals Sake, uh, that's, that's what it is. So when we compile that, it's just going to show us, thank you Cara Ross, what that looks like. The architecture. Okay. And once we have that, and then, you know, there's more just like data preprocessing, but we have those images as by your res. And we do one more preprocessing step where we just combine them into a giant vector, one by one by one by one. It's just an array of all those values. And once we have that we're going to use probably the most used part of psychic learn I've ever used, which is this training testing split, um, module which lets us split the training and testing data. And once we have that,

Speaker 2:          51:05          we have two callbacks, uh, one is early stopping. These are just a flags for our network. Thank you. Free Code camp. I'm glad to see you guys here. I did a, I did a medium blog post for them. They're, they're very cool. Great brand check 'em out model checkpoints were going to save a model as this and then we fit the model. Okay. So what I did was I actually started training and then I stopped it right here at the second epoch because the first epoch took 277 seconds, which is six 12, 1824, four minutes. And I had to do that for 50 epochs. So 50 epochs times four minutes is 200 minutes, which is six, 12, 18, three hours and 20 minutes. That's some real time that for you, Matrix operations happening right here. Skip connections, right? And uh, that is too long for a live stream and we're reaching an hour as well. But that is the model. Okay. Now, what else could we use here? Well, it turns out now Kaggle has this great idea of having a discussion right on the,

Speaker 3:          52:08          uh,

Speaker 2:          52:10          website. It's also got people doing the units. It's got people trying out different things. Let's see what, what's like the most up voted? Uh,

Speaker 3:          52:20          right

Speaker 2:          52:22          search by most votes.

Speaker 3:          52:25          Okay.

Speaker 2:          52:25          This person wrote up a whole intro to seismic geo physics. Thank you very much. Look at this quality stuff. People, these people are sharing data. They're sharing results. Even though it's $100,000 prize. It just want to help out. They just want to help. That's, that's the spirit. That's the spirit right there. Right? Okay. So what I'm gonna do is going to end this with a, with a Q and a and a rap as well. So let's just, um, let's do a little rap. Okay. Just say it. Just say a topic. Did you train on Gpu? Yes, I train on GPU. You can do that by the same runtime and then change runtime type GPU. Just like that. Right. Obviously I'm not gonna train a two hour model on a live stream. Right. We don't have two hours, but that's, that's the idea. And yes, I'll do a video on, on what implementing neural networks do. That's like all my videos. Okay. Um, instrumental beat.

Speaker 3:          53:25          I

Speaker 2:          53:26          would love to present in Holland. No, Netherlands. I love you. Seriously. How I'll come back India as well. I'm coming back guys. Everywhere. Africa, I'm coming. Um, South America as well. I got to do a world trip before the year ends. Um, I'm saying too much. Where was I?

Speaker 4:          53:45          Yeah, exactly. Yeah. I don't know what this is. Matt.

Speaker 2:          54:01          Recurrence.

Speaker 2:          54:09          That's why the youth Colab, it's so bad. I'm just kidding, man. It got me so mad. I try to use it. Put all my dependencies. I tried to put different things, man. Can't you see I got a demo over here. I got convolutional blocks. I got 48 lines, man. Gotta wear some socks. I'm not wearing anything right now. I'm just trying to wrap. I'm trying to do it for you. You're the crowd. I see that you've got this contact and nation and upsampling, but I'm going to do something else. It's called downsampling. When I'm done with this, I'm going to close my laptop and go, because that's how I do math. This is how I flow. Let me end it on that. Good. Yeah. No. Before I go too high up right now, uh, this has been a great live stream. Thank you everybody for joining and for now, I've got to go, um, work on school of AI stuff, so thanks for watching.

Speaker 1:          55:00          All right.