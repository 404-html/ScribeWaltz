Speaker 1:          00:00          How does a computer, yeah, make art. The great artists of our time have all had their own distinct style. The Vinci could evoke wonder and layer hidden messages. In his words, Goya was able to create an unmatched sense of dread. They'll leave blurred the line between reality and dreams. The loan artists given the right medium is able to create beauty. When the film camera was first invented, it wasn't thought of as an artistic tool, just one able to capture reality, but when artists got their hands on it, it was the birth of a new era. In fact, every time we've created a technology, artists have found a way to use it as a creative tool, whether it was meant to track our hands or just to take hulks. Recently, advances in machine learning have allowed us to generate amazing art pieces with a few lines of code.

Speaker 1:          00:43          What if you could prototype in a piece a hundred times faster? Having your medium actually collaborate with you? If we can extend these biologically learned patterns with machine learning patterns, it'll become much more clear that it's not that machines are artistic competitors. It's that we've upgraded our own creativity. Hello world, it's Saroj and let's write a python script to transform any image into the style of an artist that we choose. One of the first attempts at computational artistry with by a British artist named Harold Cohen in 1973 inspired by his wife, the prominent Japanese poet Hiromi Ito. He created a program called Aaron, which created abstract drawings, Coen hand coded based structures into it that we're used to arrive at a four it was a tree like structure and using heuristics. The program was able to take encoded rules and generate new combinations of what it knew incredibly. The paintings that generated ended up being displayed at museums across the world from London's Tate modern to San Francisco. Soma. Fast forward to 2015 when Google released deep dream. The Internet went crazy.

Speaker 1:          01:50          All Glory to hip, no toad. They trained a convolutional net to classify images, then use an optimization technique to enhance patterns in the input image rather than its own weights based on what he had learned. Soon afterwards, three German researchers use a CNN to transfer the style of a given painting to any image. They later created a website called [inaudible] that lets anyone do this easily. Since that initial paper, the Ai community has started to think about the possibilities for artistry. You think machine learning, even Kristen Stewart from twilight publish a paper on artistic style transfer as part of her new movie come swim. Yeah. Also I'm aware wolf, so let's understand how this style transfer process works. By writing our own script and carrots with a tensorflow backend, we're going to use a base image, which is this extremely attractive photo of me and a style reference image.

Speaker 1:          02:41          Our script, we'll take the style of this image and apply it to the base image, so we're going to feed these images into a neural net by first converting them into the de facto data format for all neural nets. Tensors. The variable function from Ross is backend. Tensorflow is equivalent to TF dot variable. The perimeter will be the image converted an array. Then we'll do the same for the style image and we'll create a combination image where we can later store our end result. We'll use a placeholder to initialize it with a given with an height. Let's see if we successfully loaded our images. Yup. This checks out. Now we'll want to combine these three images into a single tensor that we can feed into our model. We'll use the concatenate function to do this in just one line. The next step will be to download our pretrained model called BGG 16 that chaos has wrapped for us beautifully setting our input to our newly created tenser and the weights to the image net weights we'll set include top to false since we don't want to include the fully connected layer at the top of the network.

Speaker 1:          03:37          Dgg 16 is a 16 layer convolutional net created by the visual geometry group at Oxford that won the image net competition in 2014 the idea here is that a convolutional net pretrained for image classification on thousands of different images already knows how to encode the information contained in an image. It's Lauren filters at each layer that can detect certain generalized features. We're going to use these filters to help us perform style transfer and the reason we don't need the convolutional blog at the top is because it's fully connected. Layers and softmax function help classify images by squashing the dimensionality feature map and how putting a probability we're not going to classify. Just transform our input. We'll frame this style transfer task as an optimization problem where we have some loss function that measures an error value that we want to minimize. Our loss function in this case can be decomposed into two parts.

Speaker 1:          04:31          Content loss and style loss. We'll initialize a total loss is zero and add each of them to it. First, the content loss, we can think of an image as having both a style component and a content one. We know that the features that a CNN learns are arranged in order of progressively more abstract compositions. Since the higher level features are more extract detecting things like faces and the meaning of the universe actually not that abstract, we can associate them with content. They detect the objects that make up an image. When we run our output image and our reference image through the network respectively, we'll get a set of feature representations for both from a hidden layer that we choose and we'll measure the Euclidean distance between them to calculate our law named after the ancient Greek mathematician Euclid Alexandria. The idea of distance is very useful in machine learning.

Speaker 1:          05:15          We can use it to find rankings, recommendations, similarities. It's a mathematical way of comparing data. The second loss will calculate his style loss. This is still a function of our networks hidden layer outputs, but it's slightly more complex. Let's do this. We still pass both images through the net to observe their activations, but instead of comparing the raw activations directly, like for content, we'll add an extra step to measure the correlation of the activations. For both of our images, we'll take what's called the grand matrix of the activations hadn't given layer in the network. This will measure which features tend to activate together. This is calculated by taking the inner product of all the activations added given layer, which are a bunch of vectors. One for each feature. So this resulting matrix contains the correlations between every pair of feature maps at a given layer. It represents the tendency of features to co occur in different parts of the image. Once we have this, we can define their style loss as the Euclidean distance between the gram matrices for the reference image and output image and will compute the total style loss as a weighted sum of the style loss at each layer we choose. It turns out that for style, using just a single layer like we did for content loss doesn't get great results, but when using several layers

Speaker 2:          06:27          results improve. But it's good content and style from Susan.

Speaker 1:          06:43          Now that we have our losses, we need to define gradients of the output image with respect to the loss and then use those gradients to iteratively improve our output image to minimize a loss. So we'll calculate the derivative of our loss with respect to the activations in a given layer to get our gradients and use them to update our output image, not our weight, like we usually would. Gradients give us a direction on how to update our output image such that the difference between the base image and the style image becomes smaller. We can call our help classes combination lost function, giving it the model and the output image as perimeters, so we'll combine the loss functions into one. Then get the gradient of the output image with regard to the loss using the gradients function of chaos, which translates to TF dot gradients under the hood.

Speaker 1:          07:29          This gives the symbolic radiant of one tensor with respect to one or more other tensors. Next we'll run our optimization algorithm called El bfgs over the pixels of our output image to minimize this loss, which is very similar to this, the Cassick gradient descent, but quicker to converge. We'll feed our minimizer function, the gradients we calculated and it will output the result image. Let's see what this looks like. Dope. I'm going to submit this to Gq. There are mobile apps that do this as well. Prisma lets you pick filters on your mobile device and our tea. Stowe even lets you apply filters to video. We're still in the early stages of using machine learning to create art. So there's a lot of opportunity in this space. So to break it down, convolutional neural nets allow us to transfer the style of any given image on to another.

Speaker 1:          08:14          To do this will compute loss functions for style and content. Using outputs from hidden layers we choose and we can minimize our loss using an optimization scheme similar to the cast gradient descent called El bfgs and of the coding challenge from last week, is it tied? Of course key. Can you several features to model Google stock data and artfully used rms prop as his optimization technique wizard of the week and the runner up is Andreas wine. Zach loved your plot. The coding challenge for this video is to apply style transfer that combines a base image with two different style images. Poster gave a blink in the comments and I'll announce the winner. Next video, please subscribe for more videos like this, check out this related video. And for now I've got to minimize my losses. So thanks for watching.