Speaker 1:          00:00          I don't have to worry about managing my own servers. There is no spoon. Hello world. It's a rod and serverless computing seems to be on every developer's mind these days. In this video I'll explain how using Google cloud you can easily build, train and deploy an APP that can predict the purchasing power of a user. We can then give them a personalized price for whatever our product is. Think about all the hassles that a developer has to deal with when it comes to managing the servers that run their applications. Provisioning, deployment, patching, monitoring, all of these little details about the server and operating systems don't necessarily have to do with the features of the APP, but are still necessary components to be thinking about. Serverless computing implies a very high level of abstraction when interacting with the cloud. Focus on the code, not the infrastructure. It lets developers reduce the entire coding pipeline.

Speaker 1:          01:07          Google cloud has been pushing this concept for a while and they recently released an internal research tool to the public called colab. If you go to colab.research.google.com you can create a Jupiter notebook immediately. It runs on their cloud infrastructure and you get to use an Nvidia Kad Gpu for up to 12 hours at a time foe free. You can install any dependencies using pip and train and test machine learning models with a zero setup. It sounds too good to be true, but it's true. That's the easiest way to get started with Google cloud, but let's talk about their other services. They've got quite a lot. If we look on the main page and it can be confusing trying to understand what each service does and what makes it unique from the others. We can categorize cloud services in general, like a pyramid. At the bottom of the pyramid is infrastructure as a service.

Speaker 1:          02:05          It's the lowest level. A cloud provider can offer CPU operating systems. You as the customer are in charge of putting the pieces together to run your business. You get complete control of all the components from load balancing to hardware options. If we move up the pyramid, we arrive at containers as a service. Containers help modularize apps so different containers can hold different services. You can have a container host, the front end of your web APP and another host the backend. You don't have to deal with operating system level dependencies and get prepackaged libraries ready to use for you. If we move up the chain of abstraction, we arrive at platform as a service. This is where the cloud vendor, we'll handle all the low level infrastructure details and you can just upload your app and have it run remotely. At the very top of the pyramid is software as a service.

Speaker 1:          03:05          These are tailored for end users applications like email and bitcoin faucets schemes. So let's talk about what Google cloud offers you in this regard. Who will compute engine? Is there infrastructure as a service offering? You can create virtual machines, allocate CPU and memory as you like. It's like building your own computer and handling all the details of running it minus the dope liquid cooling system. Of course you can select micro instances, which is 0.1 cores and one gigs of Ram to massive CPU clusters or even tps with over 300 gigs of ram custom size virtual machines. It's all possible. Google is currently the only provider offering TPU. A tensor processing unit or TPU is very different architecturally from a Gpu. A GPU is a processor in its own right. It's optimized for vectorize numerical code. A TPU is a coprocessor. It can't execute code and its own right all code execution takes place on the CPU, which feeds a stream of micro operations to the TPU.

Speaker 1:          04:16          Tpes are cheaper and use a lot less power and can thus finish giant prediction jobs cheaper than a GPU. A TPU is aren't really great for training networks right now. They're more focused on executing predictions with them after training and there's no reason a TPU couldn't run something other than a tensorflow model. It's just that nobody has written the compilers to do so yet. They're not as general purpose as GPS. They're designed specifically for machine learning tasks. Google's Coopernetti's engine is their container offering. It allows developers to easily run their docker containers in a fully managed Coobernetti's environment. It comprises of a group of Google compute engine instances which run Coobernetti's. A master node manages a cluster of docker containers and runs a Kubernetes Api server to interact with the cluster and perform tasks like servicing API requests and scheduling containers. Docker is like a virtual machine but doesn't create a whole virtual operating system.

Speaker 1:          05:21          It's a way for developers to package up an APP with only the necessary parts it needs. Coopernetties was open sourced by Google over a decade ago and builds on their experience running production workloads at scale. It's a container centric management environment. All the computing networking and storage infrastructure is handled by the service. It's not an all inclusive platform though all of the default solutions it provides for things like deployment and scaling are optional and configurable. It's a good mix of options and ease of use. The real platform as a service is Google APP engine. You bring the code, they handle the rest. It automatically handles scaling to meet load and demand for users, so don't worry about sudden spikes in traffic. You pay exactly for the resource that your APP requires at any given point. It uses Netties under the hood to handle all of this so you don't have to.

Speaker 1:          06:22          This coupled with their cloud functions make for a powerful combination. Cloud functions allow you to trigger events via function calls, so say a user signs up. A cloud function can be triggered to alert the developers and luckily for us AI wizards, Google has a version of APP engine dedicated specifically to machine learning apps called machine learning engine. It's already being used in enterprise settings to solve problems like ensuring food safety and identifying clouds in satellite images. It allows developers to train machine learning models using multiple frameworks including tensorflow, carrots and sidekick learn and once trained. It also lets you serve that model to users. You can use multiple frameworks to build multiple models and have them served at the right time, update them as necessary. This is all a part of the tensorflow serving library and while normally you'd have to think about a lot of those details.

Speaker 1:          07:23          ML engine takes care of a lot of that for you. And my favorite part is the ability to automatically tune the hyper parameters for your model, which can save you lots of time trying to guess and check what the optimal values should be, so let's try it out for our demo. We're going to try and predict the income of a given person. I created a model based on a census income data set available from the UCI public repo of data sets. The data set has many features for each individual including their education, marital state hours worked, et cetera. Since we've already created a Google cloud project, we can start by creating a cloud storage bucket using the web Ui that we can easily access in the browser. We have our downloaded data and we can upload it to the cloud storage bucket with just a few commands, both the training and testing data.

Speaker 1:          08:21          With that, we can run a distributed training job in the cloud including any hyper parameter tuning as we see fit. It's using a wide and deep neural network. The idea here is that linear models are easy to use and understand. They can memorize the relationship between individual features pretty easily. Feature engineering results in a lot of derived features. So linear models can be considered why learning, but linear models aren't good at generalizing across different features because they can't see this relationship unless we feed in a set of higher order derived features that capture this, but this is labor intensive. Neural Nets or deep models are better at this, but there are also prone to overgeneralization and don't do the best job memorizing specific feature combinations. So we can use a jointly trained model that combines both wide and deep learning to produce a more accurate output.

Speaker 1:          09:21          We'll give it a name and an output path, then submit the training command to have it start running inside of the Gui. We can see the progress up our job visually. Once trained, we can deploy it to production. I referencing the train model and using the versions create command. Now we can test our deployed model by retrieving a response for a novel data point. Looks like this person likely has an income less than 50,000 USD per year, so we can offer them a more affordable price for whatever our product is. I really hope you've found this video useful. Please subscribe for more programming videos, and for now I've got to go deeper, so thanks for watching.