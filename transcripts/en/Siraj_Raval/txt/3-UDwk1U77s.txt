Speaker 1:          00:00          Hello world, it's Saroj. And what happens when we encounter data with no labels? Can we still learn from it? We're going to use a really cool model called a variational autoencoder to generate unique images after training on a collection of images with no labels. Most of the recent successes in deep learning have been due to our ability to train neural networks on huge sets of cleanly labeled data. So if you have a set of inputs and the respective target labels, you can try and learn the probability of a particular label for a particular input. Intuitively this makes sense. We're just learning a mapping of values. But the hottest area of research right now is learning from the raw unlabeled data that the world is absolutely brimming with unsupervised learning. One type of model that can do this is called an auto encoder. Auto encoders sequentially deconstruct input data into hidden representations.

Speaker 1:          00:53          Then use those same representations to sequentially reconstruct outputs that resembled their originals. Auto encoding is considered a data compression algorithm, but their data specific so it can only compress data as similar to what it's been trained on. They're also relatively lossy. So the decompressed outputs will be a bit degraded compared to the original inputs. Usually a well known compression now wear them like jpeg does better. So what are they useful for? Absolutely nothing. Just kidding. One major use case is data denoising where we train an auto encoder to reconstruct the input from a corrupted version of it, so that given some similar data that's corrupted, it can de noise it. Another is to generate similar but unique data, which is what we'll do. There are a bunch of different auto encoder types and each has its own unique implementation details. But let's dive into one I find particularly interesting. The variational autoencoder whore VAE. While deep learning is a great way to approximate complex functions, Basie and inference offers a unique framework to reason about uncertainty. It's an approach to statistics in which all forms of uncertainty or express in terms of probability at any time. There is the evidence for and against something which leads to a probability, a chance that it is true. When you learn something new, you have to fold this new evidence into what you already know to create a new probability. Basie in theory, describes this process mathematically.

Speaker 1:          02:23          The idea for the Vae came when these two ideas merged, so looking at it through a Bayesean Lens, we treat the inputs, hidden representations and reconstructed outputs of Aba. He as probabilistic random variables within a directed graphical model, so it contains a specific probability model of some data acts and latent or hidden variables. Z. We can write out the joint probability of the model. This way, given just a character produced by the model, we don't know which setting of the latent variables generated the characters. We've built variation into our model. It's inherently stochastic. Looking at it through a deep learning lens. Aba, he consist of an encoder decoder and lost function, so given some input x, let's say we've gotten 28 by 28 handwritten digit image, which is seven 84 dimensions where each pixel is one dimension. It will encode it into a latent or hidden representation space, which is way less than seven 84 we can then sample from a golf Sian probability density to get noisy values of the representations.

Speaker 1:          03:24          Let's start writing this out programmatically. After importing the libraries and finding our hyper parameters, we'll initialize our encoder network. It's job is to map inputs to our latent distribution parameters. We'll take the input data and send it through a dense fully connected layer with our choice of nonlinearity to squash the dimentionality, which will be removed. Then we convert the input data into two parameters. In a latent space, we've predefined the size of using dense, fully connected layers. Again, Z mean and z log sigma. The decoder takes Z as its input and help puts the parameters to the probability distribution of the data. Let's say each pixel is either one or zero. Since it's black and white, we can use a newly distribution since it defines a success as a binary value to represent a single pixel, so the decoder, it gets the Leighton representation of a digit as input and outputs seven 84 Bernoulli parameters.

Speaker 1:          04:18          One for each of the pixels, we're going to use these two variables. We have here to randomly sample news similar points from the late normal distribution by defining a sampling function. Epsilon refers to a random normal tensor. Once we have z we can feed it to our decoder. The decoder will map these latent space points back to the original input data. We'll initialize it with two fully connected layers and their own respected activation functions and because the data is extracted from a small dimensionality to a larger one, some of it is lost in the reconstruction process, but how much we need a measure of this and that will be our last function.

Speaker 2:          04:59          I want to generate some data printed dick typically and so mom model is the castic probability

Speaker 1:          05:11          for a vie. The loss function looks like this. The first term measures the reconstruction loss. If the decoder output is bad at reconstructing the data, well that would consequently incur a large cost here. The next term is the regular advisor. Regularizing in this case means keeping the representations of each digit as diverse as possible. If a two was written by two different people, we could end up with very different representations. That's a bad thing. We only want one since they both wrote the same number. We penalize bad behavior this way and it makes sure similar representations are close together. So our total loss function is defined as the sum of our reconstruction term and the KL divergence regularization term. All right. This is the dopest part. The way we normally want to train this model is to use grading dissent to optimize a loss with respect to the parameters of both the encoder and the decoder.

Speaker 1:          06:08          But wait, how are we going to take derivatives with respect to the parameters of ace to castic or randomly determined to variable? We've built randomness into the model itself. Gradient descent usually expects that a given input always returns the same output for a fixed set of parameters. So the only source of randomness would be the inputs. What do we do? Cowboy, we don't reprint it her ass. We want to re parameterize samples so that the randomness is independent of the parameters, so we define a function that depends on the parameters deterministically and we inject randomness into the model by introducing a random variable. Instead of the encoder generating a vector of real values, it will generate a vector of means and a vector of standard deviations. Then we can take derivatives of the functions involving z with respect to the perimeters of its distribution. We'll define our models optimizer as rms prop which performs gradient descent and it's lost function as the one we defined.

Speaker 1:          07:13          Then we'll train it by importing our digital images and feeding them into our model. For a given number of epochs and batch size. For our demo, we can plot out the neighborhoods of the different classes on a two d plane. Each colored cluster represents a digit representation and close clusters are digits that are structurally similar. We can also generate digits by scanning the latent plane, sampling late points at regular intervals and generating the corresponding digit for each of these points. So the three key takeaways here are that variational auto encoders allow us to generate data by performing unsupervised learning. They are a product of two ideas, Basie and inference and deep learning and we can back propagate through our network but performing a re parameterizing step that makes randomness independent of the parameters we derive our gradients from. Mike Mcdermott is last week's coding challenge winner. He cleverly applied the multi armed bandit problem to the stock market each bought he created used a different investment strategy based on reinforcement learning to profit and his eye python notebook is a great example of what well documented code looks like wizard of the week and the runner up is sgs joure.

Speaker 1:          08:22          He successfully took state into account which made it the contextual bandit problem. Dope submission skies. I vow to both of you. The coding challenge for this video is to use a vie to generate something other than digital images. Details are in the read me get humbling SCO in the comments and winters are going to be announced next week. Please subscribe if you want to see more programming videos. Took up this related video and for now I've got to de noise my hair, so thanks for watching.