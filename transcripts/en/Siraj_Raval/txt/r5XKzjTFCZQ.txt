Speaker 1:          00:00:07       Hello, we're old. It's Haraj. And welcome to this live session. Today we're going to be talking about the differentiable neural computer. Just, I mean, just take a second to just soak in how awesome that name is. I mean, this is, this is a really, really cool model. It, it came out of deep mind a few months ago and it is the successor to the neural Turing machine. So recall from my last weekly video, we talked about metal learning or learning to learn the edge of deep learning and, and future directions that we should move. So this is like, this is a relatively complex model. Uh, and it's definitely the coolest model that I've ever seen and I've, I've really had a lot of, it was, it was really fun studying this, this thing because it gave me so many ideas of directions we should move and things we can do with this.

Speaker 1:          00:00:56       But let's talk about the problem here. And the problem is how do we create more general purpose learning machines? And that was the problem with which they designed this, this, uh, this model. Okay. So I'm going to show you guys what the demo is for this and I'm going to then talk about what other things we could do with this. But before we, before we do that, let me answer some questions. All right, so to start off with, go ahead and ask some questions while I explain a little bit more about this. Okay. And then I'll look back at questions through slack. Okay. So neural networks are great, right? We've, we do a bunch of amazing things with that with them. But the problem with neural networks are, are that they are made to focus on a single task or whatever you train it on, right?

Speaker 1:          00:01:48       You can take a neural network and you can train it to, you know, learn to recognize cat, you know, cats in images, but then you can't take that same neural network and then ask it questions about the London subway or the best path to take, uh, two from point a to point B. You just can't do that. But what if you could, and how would you design a system like that? So that is what the DNC is, is it is, it is a neural network with an external memory store. So it's, so it's got two parts. You've got the controller, which is a normal neural network. You could use a feed forward net, you could use recurrent net, you could use any kind of network you want. And in this thing, this example, we're gonna be using a feed forward neural network. And then you've got the memory bank and the memory bank is an external matrix and the controller interacts with the memory bank by, by performing a series of read and write operations.

Speaker 1:          00:02:42       So we call these heads, we end right heads. So it takes an input it up, it, it propagates it through the network. And simultaneously it's reading and writing to the matrix to learn about, to both, right, what it's learned and then read from the past time steps, what, what it can use to then make the output prediction. And then it outputs the prediction, right? So that's the basic idea. And what we're going to do in this example is we're going to, let me see if I have the output somewhere. The output is to basically map. So here's the output. I know you guys like seeing the output, which is a definitely a good thing. And so here's what it looks like when I ran this. We have two sets of numbers, right? Zero is just one hot and coded vectors. So zero one zero, zero, zero that's the input.

Speaker 1:          00:03:34       And then one zero zero zero one zero, zero and then we want to learn the mapping between the two. So then given some input, we'll know the output, right? So it's just binary mapping, which is a very simple use case, which is what we need for this model because the model itself is where the learning should occur. But let me show you what they used it for, which what deepmind use it for. Okay, so this is what it looks like. So you have your inputs and your outputs and it learns the mapping between the two, right? [inaudible] right. We, that would in my build a neural net in four minutes video, same exact idea except a way more amazing model. So what they did, and let's talk about what they did first. What deepmind did is they said, okay, let's apply this to the London Underground. Okay. And right.

Speaker 1:          00:04:18       So the questions are coming in. So let me answer two questions and then let me go back to what, what I was just talking about. Question number one, does it learn and predict the hyper parameters? No. Uh, that that is hyper parameter optimization and it can be added onto this. And the second question is, uh, can we do image recognition with this? Yes. Oh, and then one more question. What's the difference between this and the neural train machine? I'll get right to that in a second. Okay. So back to this, what they did was they applied it to the London Underground. Okay? So what, what do I mean by this? Basically the first thing they did was they generated all these like random graphs, right? This is a graph problem. It's a graph problem, right? Subway systems are all graphs. They have nodes. And they're all connected.

Speaker 1:          00:05:01       And what they did was they gave it a set of graphs and they had these handcrafted, uh, inputs, right? So they would say, so this graph resembled some generated subway and then it had a set of labels. So it was a supervised learning problem, right? So they had this generated graph or subway and then it's associated labels. And the labels would be that the different paths that you could take from point a to point B. So you could go through Oxford circus and taught them hands, whatever, and in central whatever. So that's what they did. And then one, once they kept training it on these randomly generated graphs, then they gave it the actual London Underground, a graph with its associated pairs. And it learned to then if you, if you asked if you, if you then said, you know, two points like point a and point B, it would tell you the optimal path to get there because it had been training on that.

Speaker 1:          00:05:56       But here's where it gets even. So Norman, so you could, you could do that. Normally you, you wouldn't need an external memory store to do that. You could do that with a recurrent network and LSTM network. But what was really cool then was they added something else onto this. They added a question answering system. So not only did they train it on the London Underground's paths, but they also added a natural language to it. So they train at first on randomly generated graphs, then they train it on a text database where the, where it was a question answer database, um, and it learned to associate a questions with what they're associated answers and then an associated both. So then you could then ask it, hey, we, you know, in natural language, like a query, what's the best place to, what's the best way to get from point a to point B in natural language?

Speaker 1:          00:06:45       And then because it had this external memory store that had the previous learnings from the generator graph, it could then apply those to the natural language questions. So you see these two entirely different data types, these two entirely different datasets that this thing was able to train on. So it w it learned to optimize for one dataset and then it learns to optimize for the next dataset and it could associate between the two, which is the cool part. And you could then extrapolate this kind of thinking to anything really. You could train it on some set of images and their associated labels. And then something entirely unrelated like you know, uh, also a question to answer dataset. So you could learn natural language and then also maybe an audio data set so it could learn the labels for audio. So then you could ask it things like, hey, what kind of sound does this cap make?

Speaker 1:          00:07:36       So it would see the cat picture and then it would associate a sound with it and it's got the language. So it's like this general purpose idea. Now it's not perfect, it's not Agi, but it's a step in that direction, which is very cool. And they called it a computer, right? Why do they call it a computer? Will recall that computers, computers are, they have two parts. They have a processor and then they have memory, right? You have your CPU and then you've got ram random access memory. And so what happens little, you know, colonel level talk for a second. What, what's happening at the kernel level. Every time you're doing anything on the computer is you the, the Ram preloads a bunch of instructions. And then each instruction is fed to the CPU a one step at a time. And what the CPU does is it takes in an instruction, decodes it, executes it, and then repeats the process.

Speaker 1:          00:08:28       And this process is called the instruction cycle. And it is the hallmark of how computing works, right? And then there's the GPU, but that's a different story. We're talking about the CPU. Now, von Neumann architecture and computer science was his very famous, right? And a lot of computing is based off of that idea. But what this is, and now this is not deep mind talking. This is Saroj talking. Uh, or this is, this is my, this is, this is my w what, what should happen? We can use this as a framework for building hardware as well, right? So it's, it's a computer, but it's, it's, it's all software. There's no hardware associated with it. But if we, if we switch our thinking from serially, uh, decoding these instructions and instead learning from instructions at the kernel level, at the hardware level, then we can get some really interesting results. Now there are people working on this, uh, but I think it's really cool to think about what the next, the successor to both von Neumann architecture is and also the successor to silicon and what new mediums we could use for computing could be. So it's a lot of very exciting possibility with just this, uh, architecture, the software architecture. Okay. And let me show you guys one more thing that they did. So to keep going with this idea of associating to different data types. So they first fed this thing,

Speaker 1:          00:09:51       they first fed it, some associations like Joe is a mother, a freer, you know, Bob is a mother or a Ba. Ba is a husband and Bert. So a bunch of different associations. So natural language, texts, associations. Okay. And then once it had those associations, 49 inputs later, then you could say things like, who is fray as maternal great uncle. And because it's, it's a graph problem. It took this natural language and it constructed a graph out of it, then it's just a graph problem, right? You can just traverse the graph to find who Frey as maternal great uncle is, even though we didn't explicitly tell it, who would that uncle was. Okay. So there, it can do multiple things, right? It's not just language, it's also a graph construction. And the fact that it's using an external, uh, David structure for memory. It's such a simple concept isn't it? I mean, if you think about it, it's one of those intuitive things like Duh, like of course there should be an external memory store, but just no one tried it before. I mean, you know, we did have the dynamic memory network out of Facebook. We didn't have the neural Turing machine by deep mine. But this is, this is a really cool idea. That's what I'm trying to say. This is a really cool idea and

Speaker 2:          00:10:58       okay.

Speaker 1:          00:10:59       Yeah. So

Speaker 1:          00:11:02       that's what they did. And we've got an also neural networks. Neural networks have memory, right? They have memory, but the memory is so interpolated with the memory are the weights. It's interpolated with the processing you. But if we detach the memory into a separate component, that's when the results start to get magical. And that's what, that's what this is. Okay. Oh, and here's the coolest part. So the whole system is differentiable. Okay. So the whole thing is differentiable. What do I mean by that? That means, you know, when we differentiate or, uh, back propagate our net, our neural networks, we for propagate. And then we take the difference between the output and the prediction. That's our error or loss. And then we use that loss to compute the partial derivative with respect to each of the weights backwards. And then we continually do that. And that's how we update our network.

Speaker 1:          00:11:48       But, and that's how we differentiate. But this whole thing is differentiable. So it's not just the controller, the network, but it's also the, it's also the memory memory store. So this thing is differentiable too. So will you compute the partial derivatives with respect to all of these rows in memory? Okay. So there's actually a lot of parts here. And what we're gonna do is we're going to go through each part, step by step, and I'm going to talk about how each part works. Okay. Uh, so get ready for this. This is going to be up. This is gonna be amazing. Okay. You're going to have your mind blown. So let's go, let's go ahead and get started with this. We have a lot to go over. Uh, so it's gonna be a lot of fun. The first thing that they did here. So let me answer one of the questions.

Speaker 1:          00:12:29       One of the questions was how is this different from its predecessor, the neural Turing machine? So there are several ways that in here I in text how it's different. Here's how it is different in text, but there are several ways, several ways that is different. But basically it can all be summed up as there are more memory access methods. Okay. And so it's different ways of interacting with memory. So a more complex, uh, a more optimized way of interacting with memory, then the neural Turing machine and, and it added this Tim temporal. I loved the, I love the terminology here. I love how deep mind uses like a neuroscience terminology because I mean they have actual neuroscientists working on their team but they added this temporal link Matrix. So you see these arrows pointing to different arose in this memory bank or memory matrix. The reason that's there is so that they so that they can so that the network can know when it's reading or writing the order with which things were written.

Speaker 1:          00:13:28       Re read or written to memory, right. It's the order and the order helps because you know, whether it's who is frail as great uncles, paternal, whatever, you want to know the order sometimes. And so this adds an ordering to the ReadWrite heads. Okay. So right. Okay. So one more thing before we start looking at the code. Okay. I don't want to talk about attention. So right. So we have our controller, we have our read and write heads and then we have our memory bank. Okay. So the question then is how do we know where in this matrix to write to, how do we know where in this matrix to read too? And the degree to which we should do those things. And that's where attention comes into play. We call it attention because it's a way for us to frame how, uh, how precedents or how, how important or how waiting is played into how we read and write.

Speaker 1:          00:14:25       What do I mean by that? How do we know where to store, store to store stuff basically. So they added three attention mechanisms. The first one, it's called content lookup. So thinking about content address systems, right? When you have a content address system, the, the key, uh, the key each tells you what that content, that content value is. So it's the same thing with this. We have a reader right head, okay. And that's good. That's going to contain some, you know, content address. And then we have a similar content address and memory. And so what we do is then we find the similarity, the coastline similarity between all the content addresses to see what's the most similar. And when we could use that, that value that's the most similar to then update our network, right? So, so content look up via a similarity measure is the first.

Speaker 1:          00:15:14       And we'll go through each of these uh, attention mechanism. The second one, uh, the second, uh, attention mechanism is a temporal linking, right? So how do we know the order that things were re read and written to memory and how do we then update our network based on that? And the third one is allocating memory for writing. So this is a dynamic allocation for writing part. Instead of having some static amount of memory dedicated to writing, we are dynamically allocating it. So we're going to erase it and then, and then rewrite, rewrite over it dynamically. You'll understand more when we get to that part. So there are three attention mechanisms here. Okay. And I love how they compared the attention mechanisms to the hippocampal, uh, ca three and one synapse synapse regions of the brain, which is super cool. And they also did this actually recall from the, from the deep cue, from deep cue, the, the network that could beat all the, of all those Atari Games that use something else from the hippocampal region. It was called, what was it called, experience replay, which uh, came directly from neuroscience. So deep minds papers read a lot like new mantas except they actually pub. They actually have great results. So, oh yeah. Anyway, so here we go. With this, let's get to the code and also let me answer

Speaker 1:          00:16:36       two more questions before we get to the code. Question one, can we use this DNC for real problems like data association? Absolutely. Absolutely you can. And they did, right? With the, with the family members in the family trees, you can abstract that or you can extrapolate that problem to something entirely different. Like finding associations between

Speaker 1:          00:16:59       people, obviously people, but ideas and images and different types of data, different data types, numbers, stuff like that. Yes, you can. And then one more question, can you explain the memory and head operations in great details. Still confused about it? Yes. Let me, let me do that. As I go down because that's, that's what this, that's what the code is, right? The details of that. But high level, we have an external memory store, right? We have a neural network controller and then we have an external memory store, which is a matrix, right? It's a matrix that we defined and you know, x by y Matrix and we ever our neural network and for our neural network we feed it in an input. Okay. And it's forward propagating. Right? And then it's reading and writing to memory. So it's saying, so the degree to which we're reading and writing is dependent on how we structure it, which we're going to go into.

Speaker 1:          00:17:59       But it's reading and writing to memory too. It's reading to see w just like it would read its own weights just like it's, it would read its own right weights. It's reading from the memory store to make predictions to compute that those, that series of matrix operations that it would to then output that that prediction. Right. And it's writing for memory to then just like it would write to its way. It's right. Just like how you multiply each weight by each value as you propagate it forward. You're also multiplying it by this memory, this external memory, a matrix, and then we differentiate, which is going to be awesome when we get to it. Okay, so let's get started with the code from scratch. We don't have time to just write it all out because there's a lot of code and there's a lot of theory here.

Speaker 1:          00:18:41       So we're going to focus on the theory. We're going to focus on the theory and then uh, we'll, we'll compile it and run it and it's going to be awesome. Okay, so, oh, there's another question. Is it better to have loops in neural networks thing to go without it? Is it better to have loops? It's better if that's your use case. What we're using in this in this example is a feed forward network. It's a feed forward network, uh, but you could use a recurrent net. In fact, in the paper they use a recurrent nets. And when you would, when would you want to use guys helping answer this question? When would you want to use a recurrent network? When would you want to have loops, right? When would you want to have the state fed back into the input at the next time step, when you have a sequence, okay?

Speaker 1:          00:19:24       When you have any kind of sequence, any kind of sequential data, then you would want a recurrent network, okay? But the most simple type of network is a feed forward net. And that's what we're doing here because we want to really, you know, break it down to its bare essentials so we can understand the general architecture. And once we understand the general architecture, then we could use this for crazy new use cases that no one has ever done before. We are at the bleeding edge right now. So let's, let's get started. Okay. So we're going to define the DNC, the differentiable neural computer as its own class. Okay? And like all classes, we want to initialize it in the, in its function. So let's go step by step through what we're going to define here. So the first thing we're going to define, uh, is the input data and the output data, or the sizes of both.

Speaker 1:          00:20:09       Now this is what it's gonna look like. Recall that it is a set of pair of binary pairs, one zero zero one zero zero and these pairs are going to be randomly initialized. It doesn't matter, but there's gonna be a mapping both for the input and output data, just a sets because they're both just a series of ones and zeroes, right? And we want to learn the mapping between the two so that given some novel inputs with ones and Zeros, we can then predict what the probable output set of ones and Zeros would be, right? Because we already have a set of them. It's a supervised learning problem. And then we want to then predict what that label would be. So we have our inputs size and our output size that we're going to define here. Okay. And notice that these are coming from the parameters up here, right?

Speaker 1:          00:20:52       We're going to define these when we initialized our DNC later on. But we're defining the class right now. So that's the first part. The next part is for us to define our read and write vector size. So notice how it's called numb words. And words size. But there are no words here, right? This is kind of like, like they had words in the deep mind code. So this is kind of like leftover from that. But there are no words here. But what we can think of these two variables as are the sizes of our read and write vectors. Because when we initialize them, we'll be using these variables as, as parameters to define the size of our read and write vectors. And a couple other variables that will, that will, uh, talk about. But they're basically these constant values that we are going to use to, to initialize a bunch of variables later on.

Speaker 1:          00:21:44       But their constant, these values, these two values are constant, right? The number of words and the word size. In fact, we're going to use it to initialize the size of our memory matrix. Our memory matrix is going to be the number of words by the word size. That's the size of our memory matrix. Okay. So then, uh, we want to define our heads, right? So how many heads do we want? And so that is, so basically a head is an operation. How many times, how many times do we want to be reading and writing to memory while we're training our network? And we're just going to say one, we're going to have a single head for every time step. It's going to be reading and writing to memory. Just one head. Okay, so just to keep it simple, but we could have multiple heads. Okay, so then we're going to define our interface size.

Speaker 1:          00:22:29       So what is this? So, oh, so let me go back up here. So I left out one part because I wanted to get to it now. So we have our inputs, right? We have our input data and then we feed it to our controller. It reads and writes and then it and then outputs a prediction. But a DNC doesn't just output a prediction. It also outputs a, an interface vector. And what the interface vector does is it defines how we're going to interact with the memory bank at the next time step. So it's outputting, putting things, it's out putting our prediction and what's called an interface vector. And we use this interface vector to then feed it back into the network. So then that the next time step, we know how to interact with the memory bank. So that's what we're doing here. We're defining that the size of that interface vector. And yes, there, there are like three places in the code where there are magic numbers. And let me make sure that you guys can see this. So there are three places in the code where there are magic numbers, but,

Speaker 1:          00:23:36       but that's uh, that's just how it is because I mean, we could, we could change these in our results would be better. Uh, you know, I tried out, I tried out several, several numbers here, but these, these, these produced the best convergence and that's just all of deep learning, right? For all hyper parameters. But we're defining them by these, remember these set a vector is this numb words and word size that we're going to consistently used throughout this, this code. Okay. So we define our input data. We define these two variables that will help us initialize our memory matrix, saw a length and width and then are the number of heads we want to read and write with the interface size, which is the size of that output, that associated vector with the output. And then are then we're going to define our input size, which is the size of the input, right?

Speaker 1:          00:24:27       Which is going to be uh, after we flatten it, what does that size of that input going to be? So we're going to define that here, using those, those same two, uh, parameters that we talked about before. And then the output side, what do we want the output size to be? Well, it's going to be the size of the output that we defined earlier plus the interface size, cause it's one big vector that we could split and then use later on. Okay. And then we're going to define a, a distribution on both outputs, both the,

Speaker 1:          00:25:01       both the prediction and the interface. So we have distributions around both. And finally we create our memory matrix. All right. Our memory matrix. This thing up here, right over here, it's just the matrix. We can define it in one line of code. It's just a matrix. It's not some, you know, pseudo magical thing. We'll define it as a set of Zeros using our number of words by our word size, but there are no words. It's just the size versus the length versus width. Okay? And Times W. Okay, we have some more variables here. So we've defined our matrix, we defined a sides of our heads, our input size or output size or you know, input output heads and memory matrix size. And now remember we don't just have an external memory matrix. We have a third matrix, right? We have our neural network, which we can consider one huge matrix. We have our memory matrix, but we also have this third matrix over here, which is this temporal linkage matrix, right? This is how we sequentially, this is how whenever we're reading and writing to memory, we decide what order we should read and write to the ordering matters, right? Ordering definitely matters. Whether it's a graph, traversal problem or natural language problem, the order matters because

Speaker 1:          00:26:21       we're going to continually feed it data. Okay. So right. So, okay, so we have our usage vector and the usage vector is going to record which locations have been used so far. So it's kind of like it's deciding where in the memory bank have we are read and written to before. Then we'll store that there and we'll use that usage vector to then define our, our temporal link Matrix. Okay. Later on. But right now we just initialize it with Zeros and then we have our precedence. Wait, we're just going to represent the degree to which the last location was written to and the previous time step. Once I get through up to this output, wait right here, then I'll answer questions. Okay. So then, right, so that's our Tempura link Matrix essentially. So we have what we've defined our major components and now we've got to define our, uh, read and write head variables.

Speaker 1:          00:27:22       Oh, right. Head weights variables. I mean, let me update that because we only have one head, right? But we have weights for that head right? We have a set of read weights and a set of right weights and these weights are just matrices. They're small matrices, but they define the degree to which we're reading and the degree to which we're writing. It's what do I mean by the degree to which, well recall that reading and writing is just, they're just matrix operations. They're just multiplication and we can define how much we're multiplying. We can tune that similar to how we use a learning rate when whenever we're updating our weights, it's like that. These weights, these read and write weights define how much we're multiplying the memory matrix by, and remember the entire thing is differentiable. So everything is updated, right? You might be, because you might be wondering how do we know what the read wage should be or what the right way it should be, or even what the usage weight or the link matrix should be.

Speaker 1:          00:28:20       We're differentiating everything based on the output that the loss between the output and the, uh, the predicted output in the actual outputs. And we're using that to differentiate the entire thing. All the components, which is amazing. If you think about it, it's an end to end system end to end differentiable. So we have a read weights are right weights and then our read vectors, which are going to use the right read weights to then apply that. That's what we actually do. The Matrix multiplication with we take our weights times are vectors and then that's how we get our, uh,

Speaker 2:          00:28:54       okay.

Speaker 1:          00:28:55       That's how we get our output for the, for the Matrix. Okay. So then we've got our

Speaker 1:          00:29:00       uh, placeholders, right? These are our tension flow placeholders. We're going to feed in our input and output pair, right? Those ones and Zeros, both of them are gateways and we just feed them both in learning the mapping and then predicted output. Okay. So then we define our networks. So because this is a super simple use case, we have one read head. Okay. We have just binary input output pairs. Let's just define a feed forward to layer feed forward network, right? It's got set of weights and set of biases. So weight bias, weight bias. Nope, that's it. Okay. Uh, let me make sure that we can see everything here.

Speaker 2:          00:29:35       Okay.

Speaker 1:          00:29:35       Okay. This is a little longer line. So let me, let me go over here. Float 32 we've named our, you know, standard deviation of 0.1. We defined the size by using the input size. Okay, so this part's going to be cut off here as well, but just recognize that it's, it's similar. Okay. So let me make it bigger again.

Speaker 1:          00:29:56       Okay. So where was I? So, okay, so we divide our network and let me, let me talk about these and then I'll answer questions. So then we have our output weights. So we have weights for our output, right? So all of these components have weights. Are, are our output, our output values have weights, spoke the interface factor and the output. And why do they have weights so that we can then differentiate. We don't, we don't, uh, we take the partial derivative with respect to not just our controllers weights, but by the weights up our outputs, by the weights of our heads, by the weights of our Matrix, uh, and by the weights of our Tempura link Matrix. So we take the partial derivative with respect to everything. So even the weights, even the outputs have weights. Okay. Both the and we and we initialize them randomly using this TF truncated normal function. Okay. And then we also have a read vectors output. Wait. Okay. So now let me answer some questions because now we can get to the, the fun part. So the questions are, let me just see who's, who's who. Okay. We've got 408 people here.

Speaker 1:          00:31:13       All right. People are doing good. Okay. Is it better to have loops in neural networks then too? No, no, no. I already answered that. Can you tell me what is the difference between fine tuning and transfer learning? Transfer learning.

Speaker 2:          00:31:27       Okay.

Speaker 1:          00:31:27       Okay. So fine tuning is a, is a kind of vague term. You could think of. Transfer learning as fine tuning. In fact you could think of all machine learning is fine tuning. We are, we are iteratively improving our model. But transfer learning is when you train a network on some task. Okay. And then you used that pretrained model to then learn from my different, try a different tasks. So you're transferring the learnings from one task to another. Two more questions. What if the Matrix is larger than the amount of Ram you have. So that

Speaker 1:          00:32:01       would probably not happen unless you have a really, uh, both, uh, really bad computer and the model is huge, like gigantic. Uh, but if that happened, then you would have a, an overflow or ramp up your, uh, an overflow for, of Ram and your, and your computer. Your system would notify you of that with a, with a pop up unless it was like, I dunno Debbie and or something would and in which case you're just, you're screwed. Okay. So then do you guys use radial basis functions as kernels in neural networks? So I haven't seen those used a lot. Uh, those are rarely used. I see it. I can recall. Radial basis function is being used in

Speaker 3:          00:32:44       uh,

Speaker 1:          00:32:46       mark off chains. So like Markov models. I see. Those used with those is mid is mid I data a sequence of data that can be fed into a looping neural network. I'm trying to create a mid I generator. Yes. I've got like three videos on that. Generate music intenser flow, how to build an AI composer and how to generate music and tensorflow live. Check out all of the videos. Okay, so back to this back. Back in black. Okay, so we define all of our, all we did was we just defined all of our vectors that we're going to be using, right? All of our, all of our components, we defined our components of this thing but the country. Okay? So that would be the controller, the, the weights for the output and the weights for the interface vector. The, the read, the, the one head that we're using in the waits for it, the memory bank, which is a memory matrix that the tempura link Matrix and it's associated weights.

Speaker 3:          00:33:40       That's it. Okay.

Speaker 1:          00:33:43       That's what we just defined. And now we're going to actually,

Speaker 3:          00:33:47       uh,

Speaker 1:          00:33:49       go right into the step function. So, so notice how I've got two functions up here. Each of these is for a different attention mechanism. So we have three attention mechanisms for the network for the controller to decide how we're going to deal with this memory bank, how we're going to update this memory bank. And read from it. Uh, but let's go straight into the step function and then we'll talk about the details of these helper functions. So we'll start at a high level and then go, uh, increasingly a more low level.

Speaker 3:          00:34:20       So

Speaker 1:          00:34:22       here's what happens. The step function happens at every time step, right? So whenever we build our session at the end, we're going to run this step function continuously. So at every time step, the controller receives an input vector from the Dataset and emits an output vector, but it also receives a set of read vectors from the memory matrix at a previous time step via the read heads. Okay? Then it emits an interface vector that defines its interaction with the memory at the current time step. So now I'm going to add something else on. This is notice I'm adding things on, uh, iteratively. So remember how I said, Oh, you have one input and you have one output. And then I was like, actually, you have one input and then you have two outputs. One output is the predicted output and the other output is an interface vector that defines how you interact with the memory bank. And the next time set, well we've actually not just got two outputs. We've got two inputs. So one of the inputs is the data itself, but the other input is going to be

Speaker 3:          00:35:21       the

Speaker 1:          00:35:24       read vector from the previous time step. So think about this for a second. This is not a recurrent network. We're using a feed forward network but because we're not feeding in the state of the controller from a previous time step, but what we are feeding from a previous time step is the read vector. So it's a feed forward network but so the controller is feed forward but as a whole there is recurrence happening. So you could think of the differential neural computer as a whole, as a recurrent network but not in the traditional sense or feeding in the previous state of the network to the previous time step. But in the sense that we are feeding in the read vector from the previous times that from the memory matrix, from the external memory store back into the input. So in that way it's recurrent. Okay.

Speaker 1:          00:36:09       So, so that's what's happening at every time step. It receives an input vector from the dataset and missing an output vector and an interface vector. And then at the next time seven and puts the next, the, the next input is going to be from the Dataset and the read vector. And then we just repeat that over and over again. So let's, let's do this. Let's, let's programmatically go through what this looks like. So the first step is first is for us to reshape our input so that it's, it's, it's fit for the size of our network, right? So we've got our input data and remember, uh, we also have the read vectors, right? Both of those things. We've got our input and they're read vectors from the previous time step. And then we take that input and then we forward propagate through the network. We forward propagate through the network and in remember it's a two layer network, right?

Speaker 1:          00:37:00       So we do a matrix multiplication by the weights and biases Tan h as our activation function matrix multiplication 10 age. And this l two activation is going to be our output. Okay. Um, well, sorry. No, no, no. Sorry. This is going to be our output and we're going to use that last activation to compute it by then doing matrix multiplication. But this is our output vector. And then this is going to be our interface vector. So remember there are two outputs, the both a normal output and the interface vector. Okay. So now we have both of our outputs. We forward propagated and it's time to then use these two vectors to then learn and do more, more things with. Okay. So then we've got this one line. So remember I said how there's magic numbers in three parts of this code? We talked about that one. This is the other part. So the partition, what the F is this? So the partition is a,

Speaker 1:          00:37:57       it's what we're going to use to define our interaction with the memory matrix. Okay. So we take this partition and it's a, it's a 10, it's a, it's a, it's think of it as a list or an array with 10 parts to it. And it's just one big, uh, one big matrix that we're then going to that were there aren't going to convert into a, a set of keys and a set of strings and a set of vectors. Okay. So let me go ahead and, uh, talk about how we're going to split these up into a set of keys and vectors. All right. But first, let me answer some questions before we get into this. So in terms of questions we've got, can we use neural nets to make something like a PCB layout designer because currently available ones are crap. Ah, yes. How would you do that? You would want to use a generative model? Uh, probably a generative adversarial network. Train it on.

Speaker 1:          00:39:09       Now, I'm not, I'm not familiar with the components of how a PCB works, but I assume that you can think of it as a graph problem as well because you have different components and data is flowing through the hardware in a certain way or you know, it's some kind of mapping. But think of it as a graph, traversal problem and you're feeding it all these graphs and it's supervised, right? And you have the correct paths for all of these examples. And then given these set of examples, you can then generate a new path which would then be a new PCB layout design. That's just one way. But yes, you could for sure. Okay, so back to this. So we want to convert our interface vector into a set of readings factors and we use the partition to do that. Okay. So what do I mean by the partition?

Speaker 1:          00:39:54       Let me, let me back up here for a second. Let me back up. Let's back up. Let's back up. We have our controller and it outputs to things it outputs are output vector, which is our prediction. And the outputs are interface vector. Okay. And our interface vector is meant for our network to then decide how to interact with the memory bank at the next time step. But how do we know? So how do we know exactly how to interact with the memory bank? Well, we're gonna use this partition and what the partition does is it defines sizes for 10 different. Think of these as placeholders. What these 10 placeholders are going to be, or they're going to become these 10 variables right here. They're going to become these 10 variables. So before we initialize these variables, we define the sizes of them. And what these variables are going to do is they're going to be, they're going to come out of the interface vector.

Speaker 1:          00:40:46       So think of the interface sector as one big matrix or one big vector, same thing. And then we use the partition to then split that vector and two sizes that we've predefined beforehand. And each of those is going to be a a V and an important component that we're going to then use to update our memory bank. So we have our interface vector, we define a partition to split it up into a set of sizes that we want and then we dynamically partition it using tfs dynamic partition function into the read key into three read variables, three right variables and then three gate variables which I haven't talked about, which we will. And then I read a set of read modes. So there, there are quite a few parts here. Okay. So, so once we have these variables then we can define what the Sh, what the shape and size of them are.

Speaker 1:          00:41:39       So we've initialize them as parts of the interface vector because we're going to then update our memory bank using these. So we have our set of read vectors and our set of right vectors. So the keys defined where we want to, we will, we'll use the keys to find a similarity between whatever we have in our, our memory bank too so that we could then read from it whatever is the most similar or maybe it's whatever is the least similar that's that's up for it to learn with its own weights. Right. It depends on the use case and that's up for it to learn. A lot of these things like you might be asking, well why, uh, why does it choose one place or the other? That's all dependent on the data, right? It learns how to interact with it. We're just defining the, the blueprint here.

Speaker 1:          00:42:25       We're defining a blueprint and then it's going to learn how to interact with the memory matrix, but we can definitely improve the, uh, the blueprint. So, so the key is going to be like, you know, it's like for a dictionary, it's, it's, it's the content address and then the string is going to be, it's going to help us initialize our read weights. Okay. This is str is going to help. This value is going to help us initialize our read waits for a right factors. It's the same thing we have, our key string is going to help us, uh, initialize our weights. And then we have two vectors here. There are two components to writing. First we erase and then we write. So it's like an override. So if before we write to something, we wouldn't erase what's already there. And then right over that empty space. Okay. And yeah, and so then we initialize these using the sigmoid function and then the soft plus, which is another, uh,

Speaker 2:          00:43:22       yeah,

Speaker 1:          00:43:23       which is another probability function. Okay. So then we have a read vectors in our right vectors that we, we've created from our interface factor, which was one of our two outputs. And now we're going to define our gates. So what are these? So recall from LSTM networks from gru networks, we had gates. Now these gates are just ski scalar values. They're just single numbers. But we call them gates because they define, they define the degree to which we, we perform a certain operation. So remember LSTM networks had, forget it had each LSTM cell has a forget gate. Okay. And it had two other dates and then a gru unit has a update gate. It's got to reset gate and these gates to find that the, the degree to which we are performing these operations, whether it be reset or update and it's differential. Those were differentiable too.

Speaker 1:          00:44:15       So recall we, whenever we differentiated our LSTM network, we updated those gate values and so then it read and erased and in Erie or wrote as it as necessary as it as it would best converge. So it was up to it to learn what these gate values should be. Everything is learned, the right vectors, the, the gates, the keys, all of it is learned through differentiation through backpropagation. Okay. We're just the beautiful architecture that everything is, is, is learned. We don't define anything sadequee except for those, those two lines of magic numbers that I, that I showed you guys. Okay. So then we have three gates. The first one is the free gate, which defines the degree to which locations that read heads will be freed. Okay. Then the allocate, which is the fraction of writing that as being allocated to in a new location and then the right than the right gate.

Speaker 1:          00:45:11       The amount of information to be written to memory. So how much do we want to write to memory? So we, so we have one gate for reading, one gate for writing and then one gate for dynamic memory allocation. So at every time step we, we are deciding how much memory do want to allocate to some, to some head value, whether it be a read or a right. It's not some static dynamic. You know, a static allocation is dynamic. It changes at every time step. Okay. So then we have one more uh, variable from our partition that we talked about at the very end. What was that last one? It was the read modes. So what are these read modes? The read heads can use gates called read modes to switch between content, look up using a read key and reading out locations either forwards or backwards in the order they were written. So the mode is, it tells us how we should read. Okay. So it's like, it's an addition to the read weights that helps define should we read in a forward direction or backward direction. And it again, this thing learns how to do that. We just defined that, hey, there should be a direction with word reading, create some differentiable value that we can then learn to update via training. Okay. And then so we've defined that and now we're going to,

Speaker 1:          00:46:36       when I said we're going to dynamically allocate memory, this is what I mean, the usage Vectra is, is, is what, what is, what's going to help us dynamically allocate memory. So we have a retention vector, which is this, which is the helper vector for the usage factor. But the retention vector is used to calculate the usage vector and it's asking us what's available to write to, okay, what is available to write too. So then, uh, we can then write to it. So let me go back for a second. Let me, let me do a little high level refresher. We had our input, we Ford propagated it through the network and then we computed, our output vector is both the predicted output and the interface vector. And then we can, and then we partition that interface vector using this partition, a variable and the dynamic partition function and to a set of parameters with which we can then interact with our memory bank at the next time step.

Speaker 1:          00:47:33       We then, uh, initialized and flattened and resized all the reading right vectors and the gates, which the regionwide white vectors. And then the gates, which controls the degree to which we're reading and writing as well as the read modes, which is the direction that we're reading. And now we're going to actually perform the read and writing both the reading and the writing. Okay, so for the, so we're going to start off with performing the writing and then we'll do the reading. Okay, so now let me, let me write that out. So we're going to rights first. So now we're going to actually do the writing and reading. We define all of our variables. Now it's time to actually do the writing. So for the writing will compute our usage vector, which we'll use to dynamically allocate memory. And then we're going to define our set of right to waits for the right head.

Speaker 1:          00:48:18       So retrieved the writing allocation waiting. Okay. And then we just, we decided where to write to and then we can define our right weights using both of those variables, both the allocate, allocate weights and the alloc allocate gate. And that's going to give us our right weights, how much space to allocate for them and where to write too, like how much space to dedicate to writing and where to write too. And then then we can update our right to our memory matrix using that. So first we recall we first erase using this erase vector, then we right. And both of these operations occur through matrix multiplication. First we erase, then we right. And that's it for writing. Okay, now we're going to read. So we writ rewrote first and now we're going to read. So for reading and guys if you sit through this, I'm, I'm, I'm, I'm going to wrap at the end.

Speaker 1:          00:49:15       So, so, so get ready for this and we're almost done. So I'm going to answer questions. Okay. I know there's a lot of parts to this, but it's, it's, it's an amazing architecture and it's definitely worth looking into. It's, it's an amazing architecture. Okay. So we've written to our memory matrix and now we're going to read from it. So as well as writing, the controller can read from multiple locations and memory memory can be searched based on the content of each location using content lookup or the associated tempura links can be followed backwards and forwards to recall information written in sequence or reverse, which is our third attention mechanism. Now I haven't, I haven't actually talked about our, uh, attention mechanisms yet, but let me, this is going to be our first, which is, uh, which is recalling memory using our tempura links, which is that, remember that Matrix of, of arrows right up here. That's going to be our, that's essentially an attention mechanism. Like how do we, uh, we didn't write to memory based on what's happened before. Okay. So we'll define our link matrix, which is that those, those sets of arrows, that third matrix, that the third big matrix I talked about using, uh, though the weight factor that we defined here at using our right are right with vape weights. Okay. And so then

Speaker 1:          00:50:36       we'll use the link matrix and our right weights to define our precedence weights and our precedents weight is going to help us with our, let's see what else.

Speaker 2:          00:50:58       Okay,

Speaker 1:          00:50:59       so then are precedents, wait, so we don't actually,

Speaker 1:          00:51:14       okay, so we, so the precedents waste, which was used to create the link Matrix and then we updated it right here. So we just updated it like we've, we've used it to define our link Matrix and now we can just update it. Okay. And so let me make sure you guys can see this. Okay. So now we're going to define our three modes. Remember our read modes forward, backward and content. Look up by using matrix multiplication on, on our read weights. Okay. Uh, for, for three modes and these are all differentiable, all the modes are differentiable. And

Speaker 1:          00:51:49       for lookup we're going to diff, we're going to initialize it using content, look up. And then we're going to, now we can initialize our read weights using those three modes. And then we'll create our read vector using the redx weights and the memory matrix. And then we multiply it together to get a read or read vector and then we can return it as the sum of our outputs and what we just, uh, calculated that product value that which is what we've now read from memory and we feed it back into the next time step. Right? Okay. And so what this, what this function right here is this run function is, is it's essentially just, uh, creating the outputs for, for each of the inputs using unstack a sequence of outputs. So this, this just generates are, are, are associated output sequence. So a small little helper function.

Speaker 1:          00:52:44       Uh, but okay. So also, let me go back and now let me define the other two, uh, attention mechanism. So we talked about one of the attention mechanisms, which was the, uh, Tempura linkage, right? How do we define the order with which we are updating our memory bank? But there are two more attention mechanisms here. The first one is content lookup, right? So remember it's, it's kind of like recommendation systems. It's kind of like, you know, uh, word vectors where we are finding the distance between two vectors. We're doing that with our controller and our memory bank. So whenever we're reading from, uh, our memory bank where we have a vector here, right, the, the read wait vector. And then we have our vectors, which are all the rows in the memory bank. And we want to find a similarity between all of them and then find the one that is either the most similar or the least similar and it's up to it to learn what like level of, of similarity it needs to then read from it learns where to read from using this content.

Speaker 1:          00:53:40       Look up attention mechanism. So we find the l two norm of using the key. So let me, let me read this out. A key vector emitted by the controller is compared to the content of each location in memory according to a similarity measure. The similarity score is determined, a weighting that can be used by the heads for associative recall or by the right head to modify an existing vector in memory. So we have a key, uh, Riki and we then compute the l two norm, which is a way of normalizing of vector. It's a standard formula for normalizing a vector. It's a square root of the sum of the absolute values squared. So if we take all of the components of a vector, square them, and then square root it, and that's going to be r a l two norm. And we'll do that for both the memory matrix and the key.

Speaker 1:          00:54:23       And then we'll perform a matrix multiplication to compute these similarity. And then we'll squash it with a soft Max and return that. And that's gonna give us a probability value. That is a percentage of how similar the key is to to the memory matrix. So that's, that's one, that's our second attention mechanism. And then our third attention mechanism is the actual dynamic allocation of memory as we, as we train this model. So how do we, so basically what this does is it retrieves the writing allocation weighting based on the usage free list. So which is the usage of vector and the usage of each location is represented as a number between zero and one and a weighting that picks out unused locations. It's delivered to the right head, it's independent of the size. And the contents of the men memory. So that means that you can use a on, you can use a DNC, this DNC on a task where using one size of memory and later you can upgrade to a larger memory.

Speaker 1:          00:55:16       So it's independent of sizes. See the only parameter for it, his self that we're not telling it well you should be, you should allocate memory of this size of exercise. It decides that for itself and it can be arbitrarily big or small. Theoretically it could be infinity. Okay. So we are then taking the sorted usage vector. We're going to sort the usage vector, uh, uh, sending lea, and then we're going to compute the cumulative product of all of those components. And then for, and then you and then initialize our allocation weights using those values. And then for each of our use of vectors, we're going to flatten it and then add it to the weight matrix and return the allocation waiting for each row in memory. This is essentially a matrix of allocation values that we can then use to decide how best we want to allocate memory.

Speaker 1:          00:56:03       So it's a matrix that we were returning from this. Okay, so that's, those are our three attention mechanisms. I know it's a lot to take in, but those are our three attention mechanisms. And now let's get to our main function and we'll talk about how we can actually use this. We, we've defined everything, we didn't find our attention mechanisms, our parameters are hyper parameters are components of our differential neural computer and now it's time for us to actually train this, this thing. So we'll start off by defining these parameters and these parameters are going to be used, are going to be used to initialize our data randomly, right? This random dot ran into is going to initialize our input data and our output data, which are a set of binary numbers ones and Zeros. Okay? So now we'll go ahead and initialize or tensor flow session initialize a differentiable and your old computer and then run it.

Speaker 1:          00:56:52       And the run function is then going to compute the output. It's gonna, it's gonna run this step function seat that we defined this huge step function. It's going to at every time step for a number of iterations we want and it's going to output the, you know, whatever that output's going to be like one zero, zero, zero like that. That's going to be the output. And then once we have our predicted output and we have our expected output, we then want to compute the loss between those. And we'll do that with a dis sigmoid cross entropy function. And that's gonna give us our loss then. And this is use case dependent, we don't always have to do this, but we're doing it right now. We're going to regularize each layer of our controller. And what regularization does is it can improve convergence sometimes. Okay. And we'll use the l two loss to do that, which is similar to the l two norm.

Speaker 1:          00:57:45       And then once we have that we'll add the regular risers in and then we'll optimize it using Adam, which is grading dissent. And so when we do gradient descent, it's being applied to the controller, the memory bank, their head, the three attention mechanisms and the team portal linkage matrix. Everything is, is, is differentiable. And once we've done that, we can initialize all of our variables and then initialize our input and output data using the, The v the variables we defined right up here that I talked about right up here. And then for each iteration we're going to feed in the pair, each input output pairings, your feed dect and then run our session using our loss optimizer and output and then feeding in our input and outputs. And that's going to give us our results. And let me run this again, which is the code, but in it's, it's in a, I literally just took this from a, a python file and then paste it into a Jupiter notebook. So let me do it from, from command line, which is a python DNC to pot. Nope, both on TNC dot pie. Okay. Bunch of warnings because we've got a lot of, uh, you know, things, a lot of deprecations happening. Okay.

Speaker 1:          00:59:04       Okay. So I just want to see what everyone's saying here. Okay, great.

Speaker 1:          00:59:16       I know I should write a book. I did write a book, it's called decentralized applications. I'll probably write a book later, but I've got to do a lot of other things first. So, uh, other questions. Can we run DNC on calming home PC? Yes, you can. If you have a, any kind of computer, you can run this thing because look at this. I mean this, this is just binary scalar values. I mean, you don't need a Gpu for this. You just need to CPU and any kind of numerical data you can run easily on a CPU, on a home PC. But if you want to start applying this to graph traversal problems, like deep minded, like solving the, the London Underground, like the shortest path, uh, then you need a Gpu. But you could still do this on a home PC. You don't need a cluster for this one. Would you need a cluster? If you are applying this to natural language, then you need a cluster. If you're applying this, if you want to create a question answering system, then you want to, you do this in the cloud. Okay. Using, uh, using, I have a video on that coming out in two days. So I'll, I'll tell you guys, use Floyd hub use Floyd hub. Okay. But I have, I have a video coming out on that. Okay. So on cloud options. Okay. So we've got two minutes and I'm going to spend the rest of this session answering questions for the last two minutes. But, uh,

Speaker 1:          01:00:34       I also want to play this in the background because it's, it's cool. Okay. This is what you can do, right? Associations, associations between unrelated datasets, whether the beat language, natural language or graph traversal or image recognition or speech recognition or speech generation or audio generation or graphics generation or you know, all sorts of different domains can be learned if we have an external memory memory store that acts as the blueprint to then create these learning systems, these neural networks. So we separate the processing from the memory and we have results that make our hearts sing. So for the last set of questions we have here, uh,

Speaker 1:          01:01:27       do you know if they invited people for the Ai Nanodegree I don't know. Uh, and live stream next week. So this is the last livestream for awhile. I've been doing live streams for the past 17 weeks and it's been super fun. Uh, this is a Ted talk and, uh, instead I'm going to be doing something similar. It's, it's going to be a prerecorded version of like me, you know, typing out stuff. It's not going to be live at least for a while. Uh, and yeah, you're still going to get amazing content like this. It's just not always going to be live. Uh, I will do lives again in the future. Don't worry. I know. It's not like I'm stopping my lives and, uh, yeah. So, but this is, this is this differentiable neural computer is the future. This is this idea of separating processing and memory is the future. Okay. Let me, let me wrap as well to, to the sa out. So just throw on a beat and um, I'll wrap about something.

Speaker 3:          01:02:24       Okay.

Speaker 1:          01:02:26       Thank you guys for staying through the session. You guys are heroes. I'm very proud of you for staying through this. This is nontrivial and the fact that you stayed through this, it makes me very proud of you and very proud to be a part of this community that we are all in together. So whenever you guys are ready, go ahead and, uh,

Speaker 3:          01:02:42       play that beat.

Speaker 1:          01:02:43       How much data does this need? It depends. Uh, it depends, uh, how much data it needs on what your use cases. Oh, I definitely need you guys. Okay. I'm going to keep having livestreams. Don't worry about it. Uh, just, just not the, you know, current time being. Okay. I love the DNC. I try to do it like I want to be me. I don't care about all these models. You see, I only want to use something that takes three different data sets and put them together. You see, look, I take a one die. You didn't converge. The beat stops, but it doesn't matter, man. I'm like, I splurge. I burst out into the world like on the king. Don't stop and look at on machine learning mat. It's like being the worst service ever created by Microsoft. Oh, I just, this Microsoft man, don't look before I get shot. Okay, so that's the wrap. All right. So thank you guys for showing up for now. I've got to go create some, some more amazing content for you guys. Thank you for showing up. I love you guys and cool. Thanks for watching. Huh?