Speaker 1:          00:00:00       Are we ready for Tpu time? All right, let's go. Let's go start streaming TPU time. We're going live. We're going live. We're live. We're live. We're live. Okay. All right everybody. Hello world. It's a Raj and welcome to my live stream on the TPU, the tensor processing unit in this live stream. I'm going to talk about the technical details of what a TPU is. I'm also going to do a short demo of, uh, it's going to be a benchmark comparing the CPU to the TPU. And then I'm going to do another benchmark comparing the GPU to Tpu. And at the very end we're going to do some natural language processing. This is a default, um, example that was provided by Google and we'll go over that at the very end. Okay, so this is a live stream. Let's start off with a two minute Q and.

Speaker 1:          00:00:47       A. This is the structure for this lecture and uh, that's, that's how we're going to go. Hello everybody. We've got deans in the house. We have a, we have a very vibrant community that we're building every single day is another day. Uh, that, that's, that's a part that's a part of the school of Ai. Okay. So two questions and we'll get right into this. And I want to preface this by saying this will be a technical lecture. Okay. We're talking about systolic arrays. We're talking about hardware. It's going to be awesome. I'm very excited for this lecture. Okay, so the first question here is, um, a TPU has, who has used Google's colab. Uh, we all have. And uh, the second question is, is it like the USB thing that Intel made? Yes. In that it is called an [inaudible], an application specific integrated circuit. Okay,

Speaker 1:          00:01:43       now that's it. And Hi Chris, Free Mind Wafa and college guide and Mel's and Pooja. Hi everybody. Okay. Now straight into this, let's, let's get into this. Why did Google make this thing right? I mean, there's gps out there, there's TV, there's CPU is out there and video is doing a great job shadow to Nvidia. By the way, this is no, this is nothing against Nvidia. I love Nvidia. They are awesome. But this TPU thing is pretty cool. So what what's happened at Google is that there has been a giant demand for deep learning over the years. Let me just make this small.

Speaker 2:          00:02:14       Okay.

Speaker 1:          00:02:16       That's a good question, but okay. So,

Speaker 2:          00:02:19       okay.

Speaker 1:          00:02:20       The demand for neural networks and deep learning has grown over the years at Google and Tpu development actually started in 2013, which is five years ago. This is a long time ago, um, in, in, in the machine learning world and it's been used in production since 2015. Okay. So deep learning has been super important at Google. All of their products, the ones that we know and use everyday search translate photos, Gmail, um, uh, the Google assistant, all of these products use deep learning. Okay. So what deep learning is, is we have all of these machine learning models, right? We have random for us, we have uh, just a support vector machines and one of them is a neural network. And we give what we give a neural network, a lot of data and a lot of computing power. We call that deep learning. And that has outperformed almost every other machine learning model almost every time.

Speaker 1:          00:03:11       Like 99% of the time. And so they use it. It's so crucial to all of their products, right? So that's the key here. Deep learning is absolutely crucial to Google. Okay. So why did they make their own chips? Right? We can do deep learning on a CPU. We can do deep learning on a Gpu. Well, there's been so much progress in machine learning over the years, right? And just the past few years, every single benchmark, whether it comes to image classification, whether it comes to um, video generation, whether it comes to, you know, so many different use cases, deep learning has outperformed everything else. And we seen these bench, we've seen new state of the art results in each of these subfields of, of, of machine learning. And so why deep learning? Well, if we, if we were to plot out the amount of data and the performance of a model, uh, the red line here is our traditional learning algorithms.

Speaker 1:          00:04:03       They kind of plateau after a while, right? The performance can only get so good based on how much data you give it, but neural networks, and that's what the acronym and n stands for neuro networks. If you give them a lot of data, they will start outperforming everything else. As you see here, the small, the medium and the large. So it's a game changer. Okay, so and, and what our neural networks, well, I have so many videos on what neural networks are, but briefly, neural networks are models created with linear Algebra. So Linear Algebra is the name of the gay. Okay. Linear Algebra is awesome. Neural networks are just a chain of matrix operations applied to input. Input data is actually a wrap for this input times weight, add a bias, activate repeat input times weight add a bias activate. So that just keeps going and going and going at the basic level, right?

Speaker 1:          00:04:54       So, and we can think of that input data, image, video, whatever it is as a matrix of numbers. And we give it to this black box neural network that's doing this opera series of operations and we get a result, right? So if it's an image, an image is actually a UN Un un, a matrix of pixel values between zero and two 55. And we feed it to this very simple, um, uh, neural network here, which is just the equation y equals mx plus B, where m is actually the, uh, wait, not so it's w so y equals w x plus B, where B is a constant, it's a bias value. And so we are just taking that input image a preprocessor preprocessing it such that it becomes a vector, multiplying it by buyer weight, adding a bias, and then adding an activation function, which is actually not listed here.

Speaker 1:          00:05:43       And the output is our prediction. Now that, like I said, they're just a series of matrix operations, but it turns out, so when it comes to hardware, hardware is tailored current CPS and gps, they're tailored for specific types of operations, right? Multiply. There's a specific, there's a specific literal piece of hardware dedicated to multiplication and another one to division and another one. So all of these math operations, well, it turns out for neural networks in particular, what we're doing is we're com, we're continually performing the same set of operations over and over and over again. So when it comes down to is a giant Matrix, c matrix full of numbers, all of them are being multiplied in parallel. So it's, so it all boils down to multiply a bunch of numbers together and add the results. So we can think of this as a single operation, which at which we can call multiply, accumulate or Mac.

Speaker 1:          00:06:41       And so if there was a hardware level construct dedicated to a multiply accumulate operation, then we could start going, then we could start computing even faster. Right? So let's, let's, we're slowly building on, the idea is here that got Google to create a TPU. And so Moore's law and vetted by Gordon Moore, it's a founder of, of, uh, of IBM in the 50s. He said that the computing power would double every two years. Um, and that so far has held true, but the, so not IBM, Intel and so was, was it actually Intel more? Was it in or IBM, I've got to check my facts here. I always mess up these. These, uh, more was intel. There we go. Oh, uh, anyway, great. Until at 50. I got the 50 part. Right. Anyway, back to this. This is live by the way. I'm so many things can go wrong here because it's live, but that's, that's how we do.

Speaker 1:          00:07:34       So Moore's law is plateauing and so, uh, we've got to find a way around this. Well, Google definitely has to cause all of their products and profits depend on it, right? So how do we get past the limits of Moore's law? The limits of what hardware can do for us, for deep learning. So now we're going to get a little more technical here. Okay, so a CPU is a scaler machine. So scalar is a single numbers. These are linear Algebra terms. Like, like I said, this whole talk is going to consist of only linear Algebra. There's no calculus, there's nothing else. By the way, neural networks are models built with linear Algebra and then optimize with calculus. We'll just talk about calculus. Like for 1% at the very end to talk about a certain type of optimizer specific to the TPU called a cross Shard optimizer.

Speaker 1:          00:08:22       We'll talk about that at the end. So a CPU is a scalar machine. It accepts single values, so it processes instructions one step at a time. So they can perform matrix operations, but they're not in parallel. They're sequential and Moore's law is coming to an end. So that's a CPU. Now, a CPU has multiple cores and each of these cores contains, um, arithmetic logic units. It's got a control structure. It's got cash memory, um, it's got, um, a dram. It's got memory, it's got memory with it. Uh, what a GPU is, is, is it's the, it's similar to a CPU except it has multiple cores, like hundreds of cores. So while a CPU has four cores, a GPU has hundreds, of course. Now, why does this matter to us? So gps were designed for three d game rendering, which involves a lot of parallel processing, right?

Speaker 1:          00:09:18       Shading, ray tracing a pixel, values that are all these things that are happening in Wa at once in these three Ed dames. Uh, they require parallelism. So what gps do is they are optimized for parallel operations on numbers. Okay. So to get a little more technical, here are a list of attributes for both as CPU and a Gpu. And we'll just talk about to a CPS or for low compute density, whereas gps are for high compute density by density. We're talking about groups of numbers being operated on simultaneously, right? So matrices, vectors, whereas CPS are sequential. Okay. So to get more technical we can think of, if we, if we think of a CPU as a scalar machine, we can then think of a GPU as a vector machine. We're a vector is a one dimensional array of numbers that can be operated on at the same time.

Speaker 1:          00:10:16       And so GPU is our general purpose chips, right? They don't just perform a vector operations, they can really do any kind of operation, although they are optimized for vector operations. So how do we improve upon a GPU that does well with the vector operations? Well, let's just, let's just look at what we're doing with the GPU here very quickly, how it works. Uh, and then we'll go into 10 TPU specific code. So we're going to just perform this simple matrix operation here. So check this out. I just want you to look at this matrix operation. What this is, it's, it's a major operation that says, multiply this scalar number two by this matrix. Okay? And so what linear Algebra. Okay, so just quick primer here. So Algebra is the, is a, is a mathematical, uh, it's a branch of math that it's dealing with scalar numbers, right?

Speaker 1:          00:11:10       Single numbers. But when it comes to operating on groups of numbers at the same time matrices, we need a different set of rules, right? So do we multiply the two by the 10, then add that to 10 by the four? Or do we do or do we, you two times, right? So there's not a very, if we just look at this without looking at the rules of linear Algebra will be kind of confused. But when we see this diagram, we see, oh, okay. Linear Algebra tells us that when we multiply a scalar by a matrix, we're just taking that scalar and multiplying it by each of those values. And that's the output value. So that's one of the rules of linear Algebra, of which there are many. So let's do this in a mx net, which is a very short, just want to make sure everything's all good.

Speaker 1:          00:12:01       Good. Okay. Everybody's in here so that, okay, so let's do a very short demo in mx net. It's just a few lines of code and I just want to show, and this is basically Kudo by the way. I'm so excited for this. Okay, so this is basically Kuda. Let me just, um, install mx net and wallets installing. I'll explain what is happening here. Um, import as Amex. Let's make sure that we are doing this on the GPU first. So the runtime is going to be the GPU. Good. It's the GPU. Now let's just, let's install this. So what this is [inaudible] is this open source library. Um, let me just say I like tensor flow better, but I'm just doing this because it's a very simple three line of code, um, example that we can use to demonstrate what a matrix operation looks like on the Gpu.

Speaker 1:          00:12:46       And so what mx net is doing in, in this case is it is a very, we're going to create a single object called it called an n d array, right? An end dimensional array. And so that is the, that is the, uh, construct the object in mx net that we're gonna, that's gonna represent a matrix that we're going to use to perform this operation in three lines of code. Okay? So while this is loading, in fact, let's just, uh, let's do this. So what we're gonna do is we're going to say a, the result is going to equal mx dot nd on end dimensional array dot ones. Now, one's means that this is going to be a two by three matrix. So this is going to be a two by three matrix of just ones. All those values are ones, it's a two by three matrix.

Speaker 1:          00:13:32       And all those values are once we're going to do this on the Gpu, uh, and then we're going to say B is equal to a, so that entire matrix times the scale or number two. And let's just add one and then we'll return that back as a num Pi Array. So this is Amex net, but it's basically Kuda. It's a very thin wrapper over Kudo, which is the, which is Nvidia's GPU specific programming language. Kuta is awesome. It's all about optimizing a, it's super hard. Um, but I really like Kuda. Um, I just want to say that anyway, so we have, we have installed, um, still going. So it's going to take a while and then we can do this. So like I said, we're taking a two by three matrix of once saying on the GPU, multiply it by the scalar number two and return the result as a, as a matrix, as a num Pi, a matrix. And here it is. So that is a very simple GPU operation that we just performed. Okay. So I just wanted to show that. And now let's do a little Q and a before we get into the tensor. The TPU part. Um,

Speaker 1:          00:14:40       everything's a okay. Thank you. Um, any questions?

Speaker 3:          00:14:51       Okay.

Speaker 1:          00:14:52       I don't understand a thing in our ELL math. Is it okay? Um, yes, it's okay. I mean, everybody starts out not knowing a thing about whatever topic they're learning. So of course that's okay. We look, we're trying to grow the amateur research community here and we are just at the very beginning. We are at the very beginning of this revolution of growing the amateur research community. We at school of Ai, I deeply believe that anybody can do research it, they don't necessarily have to be a part of academia. They don't necessarily have to be a part of the industry. We believe in growing the independent amateur research community, and we think that's with the democratization of tools like Colab, like TPU is like, you know, all these technologies, it's gonna become easier and easier for everyday developers to make valuable contributions to the field. So it doesn't matter. It's all about growth. I'm growing, I'm learning this thing could just totally fail. Right? The TPU in Colab is actually, it's kind of like Beta ish. It was like it was messing up for me last night. So who knows if this is even going to work right. I this could totally fail. So we're all learning and growing at the same time. One more question. Can you, does TPU support only tensorflow? Yes. It only supports tensorflow. Great.

Speaker 1:          00:16:09       That's it. Now the TPU itself. So Google actually has built three generations of these. TPU is now. Okay. So the first one was in 2015. The second one was in 2017 and the third one, it just came out this year. And also a TPU pod is what they call a cluster of these tps are combined together. It's called a TPU pod. Okay.

Speaker 2:          00:16:31       Okay.

Speaker 1:          00:16:32       So it took them 15 months to make Tpu v one. And that is actually very fast because what the TPU is, is it's an application specific integrated circuit or a sick. Now, if you're familiar with Bitcoin, a six are used to mine. Bitcoin not GPU is generally why? Because a six are custom made chips designed specifically, uh, in that case to mine bitcoin. Now can we apply the same logic to deep learning? Of course. What if we create an created an application specific integrated chip on a sick, not a general purpose chip specifically for the Matrix operations that neural networks require, what would we call that? Well, we would call it a TPU, a tensor processing units. Now the downside of this are that tps are inflexible. A six are inflexible. Once you build that chip using say the, let's say for the Matrix accumulate operation, the Mac, right. Specifically for neural networks. Now it's not going to be able to do individual like different different operations. It's only going to be able to do that hard coded operation. The Mac that we hard coded into it, right? So it's a specific chips specifically I said specific like three times out specifically for deep learning.

Speaker 3:          00:17:48       Yeah.

Speaker 1:          00:17:49       And there had been 10

Speaker 3:          00:17:50       and so there, here's a great image of it. I should have been showing this. The multiply add accumulate. Um, step looks like this, right? So we have our multiplier where most point a times B we add our bias and then we accumulate the results and then we have our output. So that's kind of a diagram of what the Mac, uh, structure looks like.

Speaker 1:          00:18:11       And so Google decided they needed, they needed to make a matrix machine, right? GPS are vector machines, they can also be applied to maitre season. Cpu is our scalar machines. But Matrix, these are two dimensional sets of numbers. And actually it's it, they need more than just a matrix machine. They need a tensor machine because what is it tense or can anybody answer the question? What is a tensor you watching? Okay, so if you said that a tensor is an end dimensional array, you are correct. A tensor can be one dimensions, two dimensions, three dimensions, four dimensions, five, six hundreds of dimensions. Because each feature in a data set can be considered a dimension of data. Now we just can't visualize it with our biological brains, but, uh, that's, that's a different topic.

Speaker 3:          00:18:57       Okay.

Speaker 1:          00:18:57       So they wanted a design as chips, specifically for Matrix operations. And what we're saying here is the, the TPU chip,

Speaker 1:          00:19:05       right? So it's got, it's got, um, four independent chips. And what this is, it's a diagram of a single chip. So each chip has to compute cores called tensor cores just like this. And inside of each core are two different units. One is for operating with scalers and vectors and the other is called the mx whew. That's specifically dedicated to the multiply accumulate operation that we talked about or matrix units. Msu, it's also got eight gigabytes of on chip memory, the HBM with each tensor core and they can perform 16,000 operations in each cycle. And these cycles are happening in parallel. We're going to get into this. They even decided to use a different type of floating point representation called beef float 16 rather than the standard I Tripoli half precision representation. Um, and so what this allows them to do is execute user computations independently. Um, and it's got a high bandwidth interconnect that allows these chips to communicate with each other directly. Um, and we're talking about the TPU pods that I mentioned earlier. So what's, what does pipeline looks like is we have a neural network model that we build with what's called a Tpu estimator. Estimators are the application level construct in tensorflow associated with models. So they built one specifically for the TPU. Why? Because when we build one with the TPU estimator it, we'll then compile a down using Xla, which is this custom proprietary software that Google has built that allows it to be run. Um,

Speaker 3:          00:20:38       yeah,

Speaker 1:          00:20:39       in the form of a systolic array. Now you might be thinking, well, what's a systolic or right. We're going to get right into that in a second. Okay. So the excellent is it is an experimental compiler for the back end of tensorflow and it turns your tensor flow graph into linear Algebra and it has backends of its own to run on CPU and GPU and tps. And it's proprietary. But you know, we'll see how that goes. So the systolic array, we're just going down the rabbit hole here guys. We are going so down the rabbit hole before we start coding. Let me see if there are any, uh, questions. Um,

Speaker 3:          00:21:16       cool.

Speaker 1:          00:21:18       Okay, so the systolic array, what this is, it's, it's a, it's a kind of hardware algorithm. The systolic array is a type of hardware algorithm that describes a pattern of cells on a chip that computes matrix multiplication. How did these cells on this chip perform? The Matrix? Small operations that are neural network requires. So systolic, the word systolic is derived from the, the, I think there's Latin for wave, it's, it's Latin for wave and it describes how data is moving across the chip. It's moving across the chip in waves, like the beating of a human heart, which is kind of a cool and kind of scary but cool. Mostly. Uh, but um, so here's an example here, right? So we have these two matrices, we multiply them together and here's our output and this output is going to be computed using a systolic array algorithm.

Speaker 1:          00:22:08       Okay. So for this two by two input, the each term in the output is going to be the sum of two products, right? So a and e we're going to multiply those two together and it's going to be a sum of a and e Plus B. And g. So if you, if you stare at this image for a while, it'll start to make sense the ordering with which these operations are happening. So let's say that a, B, c, d represent our activations and e, f, g, H, I, g h are our weights. So for our array we're going to first load up the weights like so. So e f g h are going to be loaded up and our activations are going to go into an input queue, which for our example, we'll go to the left of each row. So we need to apply our activations to the result of our, our weight operations.

Speaker 1:          00:22:55       So we're going to take our input data, multiply it by our weights, uh, and then apply our activations to it. And we want to do this in parallel. So what the systolic array algorithm does is it's, it allows multiple cycles of computations to happen in parallel. So what's going to happen is each cell will execute the following steps in parallel four steps. The first step is to multiply our weight and activation and come. It's going to be coming in from the left. And if there's no cell on the left, take one from the input queue, add that product to the partial sum coming in from above. If there are no sell above, uh, the partial sum is going to be zero. Pass the activation to the cell to the right. If there's no cell to the right, throw away the activation, pass the partial sum to the cell on the bottom. If there's no cell on the bottom, collect the partial sum as the output you might be thinking. And those are the four steps of a cycle and many cycles are happening in parallel. You might be thinking, what did you just talk about? That's a very, that's a very valid question.

Speaker 3:          00:23:54       Yeah.

Speaker 1:          00:23:54       So I also have a python example specifically for you right here that demos exactly what a systolic array it looks like. But we have a lot to cover here, so we're not going to go over this entire code, but definitely check it out. Well documented code, um, not written by me, but very well documented code. Check it out. So by these rules, what, what tends to happen is it looks like this, the wave of data is moving to the right and down simultaneously. So it's like, it's a way that's coming down like this. So let, we actually have a, uh, a Gif that demos this. This is Google's Gif, right? So it's like, watch this. So it's starting at the top left and the wave of operations is moving. It's, it's spreading in the right and downwards, directions like that. Okay.

Speaker 3:          00:24:43       So,

Speaker 1:          00:24:44       and so what happens is normally a, to compute a matrix operation, it's going to take a sequential solution. So on a CPU would take n cubed time where this, uh, where this takes n cubed cycles, where n is the number of cycles. So this is the, this is the, uh, this is the performance where as on a Tpu, it's going to take three and minus two, which is a very peculiar, um, performance, uh, metric, but that that's what it is. It's three and minus two as opposed to n cubed, which is a huge performance, right? Especially at scale. So like I said, this is what it looks like. It's going to the right and it's going down. That's the flow of data on the TPU hardware chip.

Speaker 3:          00:25:26       Okay.

Speaker 1:          00:25:26       So it's just Mac operations multiply, accumulate, multiply, accumulate, multiply, accumulate at the same time in parallel, massively parallel. And so the high density of the systolic arrays, let's Google pack in 16,000 of them into a single Amex. Whew, that's a lot. That's a lot. Um, and what this translates to for us is a lot more speed and efficiency when we're training our neural network, both for inference, um, and for serving inference and for training.

Speaker 1:          00:25:58       So that's it. The rest of the chip is important and worth going over. But the core advantage of the TPU is this Msu, uh, unit. It's this, it's a systolic array matrix multiplication unit designed specifically for neural networks. So what are use cases? So we've seen some benchmarks here and there. Um, but I don't think that we need to question whether or not the TPU is a faster chip than a GPU for neural networks. Right? Because Google use, Google uses it for Google search, right? Just think about how massive Google searches. There's no question that a TPU is faster than a GPU, specifically for neural networks. That being said, do not underestimate Jensen Wong and video because I promise you they are working on something just as good, if not better. So it's, it's all, it's all, it's all love. It's all family. We're all in this together. So, um, that's it. Anyway. Anyway, so CPU, yes.

Speaker 3:          00:27:00       Yeah.

Speaker 1:          00:27:01       CPES are, what are the, what are the use cases for each? We have CPS, we have gps, we have tps. Cpu are good for quick prototyping, very simple models. Think linear regression, right? Think just very small amounts of data. Performance as a matter. You just want to do something really quickly. See pubes are great for that small thing. Thanks. Small, right? Small effect of batch sizes. I, it's not going to take a long time to train, um,

Speaker 1:          00:27:26       like that. Just like that. So, but GPU are good for models that are not written written in tensorflow, right? Because tensions, because you can only run tensorflow code. Think about the, the Xla compiler I talked about a 40 purpose. Um, so, so there's that. And so medium to large models, great for GPS. But TPS, tps are great for tensorflow made models that are dominated by Matrix computations, neural networks, and we're talking giant models that takes, that takes weeks or months to train. What would be one great example, machine translation. Okay. I think I remember reading a machine translation paper and I'll take questions after this. So start asking questions. I remember reading a machine translation paper that talked about, I think it was a bilateral bilateral recurrent network and it took them a week to train. Machine translation takes so much data to train. It took Google a week to train Google on their computer. That's insane. So that is a perfect use case for a TPU right now. Let's take some questions before we get into our benchmarks.

Speaker 3:          00:28:46       Okay.

Speaker 1:          00:28:47       Okay. Uh, deepmind's Alphago has brought down from 176 gps to 40 purpose. Yeah. So Alphago used tps for sure. Uh, nick systolic arrays sound like a very fundamental data structure. They are a very fundamental algorithm for sure. Um,

Speaker 3:          00:29:05       okay,

Speaker 1:          00:29:05       one more question. What else can I keep you do other than specific jobs or is it that a TPU is that limited and I only know basic Algebra? What can I do with this math in our l? Great question. So um,

Speaker 3:          00:29:23       yeah.

Speaker 1:          00:29:24       First of all, a TPU art made for neural networks and what else can you do besides their own networks? You can, you can try to do other things and there's some research there. Actually you can make some valuable contributions by trying new types of models out, but they are designed for no one that works

Speaker 4:          00:29:41       for a second question and that's it for the questions for now. When do we use linear Algebra? In reinforcement learning? Look, you and me or neural networks, we are neural networks in our head. Okay. We are neural networks, but we are neural networks in the context of a mark called decision process. This reality is a reinforcement learning process. We can frame reality mathematically. We are agents in an environment. We are in a specific state and we're trying to optimize for the neck. What is going to be the best next state to be in? What is the best next state to be in? Right? And the, the way we get there is by performing the next best action that's going to optimize for a reward value. So we, neural networks are inside of a mark Haub decision process. So reality is deep reinforcement learning. So what you'll find is that it's not about, should I do, should I study RL or should I study, supervise, or should or should I study unsupervised?

Speaker 4:          00:30:39       No, it's all apart. It's, they're all pieces of the same puzzle. So it all, it all matters. Anyway, let's get into our benchmark here. So this for this benchmark, we're going to do a simple ad operation. Now look no promises. This is going to work. He said this is kind of Beta. Um, and it didn't, you know, it worked and then it didn't work last night. So we'll see if this works. But we're going to perform a simple tensorflow based ad operation on the CPU and then compare that to the TPU. We can all use the TPU because it's inside of Google colab. Okay, so let's, let's start with this. So our first step is going to be to import our dependencies. We have [inaudible], which is our, which is four stands for operating system, which lets us do some, you know, input output and you know, file file retrieval and things like that.

Speaker 3:          00:31:27       Yeah.

Speaker 4:          00:31:28       Tentraflow time num Pi. Great. And we'll install these dependencies.

Speaker 4:          00:31:36       Okay. Now let's do a CPU benchmark here. Let's, let's write out our operations. So like I said, this is going to be a very simple ad operation. We'll call it ad ad op. Okay. Add Up. So ad op is going to take to value to scalar values x and y and it's going to return the, it's going to return what happens when we add them together just like that. And so x and y are actually going to be tensorflow, placeholders, what are tensorflow place holders. They are gateways into our computation graph that we can input data into. Now I wrote this by the way, because this is so new that there was not, there was not like a benchmarking script. So I just wrote this. Okay. So, um,

Speaker 3:          00:32:18       okay.

Speaker 4:          00:32:18       Um, so why is also going to be a 10th of a place holder? And we're going to input data into each of these in a second.

Speaker 3:          00:32:27       So

Speaker 4:          00:32:28       there are a hundred possible values that are going to be here. Okay. There are a hundred possible values. Now will initialize our tensor flow session or computation graph with DTF dot session. Remember, none of this is TPU yet. We're about to convert it into TPU code. So inside of our TensorFlow's session,

Speaker 4:          00:32:50       inside of our tensor flow session, we're going to start a timer to, to clocks and performance here. Now what are we going to do? We're going to compute a results by saying session.run. So inside of the computation graph, run this ad operation on both x and y. And what are we going to feed in as our data into these placeholders? Well, using the feed Dick, a parameter will say, well for x we're going to input are a value between zero and 100 using the a range, a function of num Pi and for why we're going to do the same thing of value between a one and 100.

Speaker 2:          00:33:35       Okay,

Speaker 4:          00:33:38       hold on. Let me make sure that that is going to be back there. Good. All right, so there's that feed Deckland, make sure it's all good here. Okay. Now once we compute that opera, so basically we're taking two arrays between one and 100. So it's just one through a hundred each of those values and we're just adding them all up. So the result is going to be one plus one is two is going to be the first value. The next value is going to be a two plus two, four for two, four, six. That's gonna be the result and it's gonna be a hundred of those in an array. And uh, we'll end the timer here. That's, that's all we want to do and will compute the elapsed time. The end mine is a start and we'll print that out. How much time has elapsed here? And we'll print out the result as well so we can see what we computed. Okay. So let's see if this works. Of course not.

Speaker 2:          00:34:28       Okay.

Speaker 4:          00:34:29       Oh, see I already knew. Okay. X,

Speaker 2:          00:34:33       right. Okay.

Speaker 4:          00:34:39       And this one goes like that feed dicked equals.

Speaker 2:          00:34:53       Okay.

Speaker 4:          00:34:55       Like I said, zero two, four, six, eight that is running on a CPU. And so this took 0.01 seconds. Okay. Now we're going to come. We're going to convert this to Tpu code. You're ready. You're ready for this. So that is our TPU. So let's remember this benchmark. Your 0.01. Now let's compare it to the TPU. Okay.

Speaker 2:          00:35:14       Okay.

Speaker 4:          00:35:15       So how do we do this? So in Colab, what we can do is say, change the runtime to the, Oh my God, there it is. TPU. Awesome. Save.

Speaker 4:          00:35:30       Connecting. Please connect. You gotta connect for me. Do not do not up Google. Google is watching this. By the way. They're here. They were actually helping me last night on DM, on Twitter. They're like, are you sure about this? Maybe check this out. [inaudible] so thank you Google, because they saw that I was doing this live stream. Oh, it's initializing. Oh my God, this is going to work. It's connected. All right, so TPU Tom guys. So TPU address equals GRPC. Um, so in Colab we can actually, it's the TPU is, has a name, so we can call it using the [inaudible] environment variable and we'll say Colab, a TPU address. Okay. Address, that's the name. And once we have that will say, well, what is the TPU address? Let's, let's see what it is. TPU address is tip you address. Now inside of our tent or floats session, we will initialize a session using the TPU address or the address of where it is. And now we can list all of the devices, which are the cores, the CPU cores, um, in that list of devices. And I'll print them out at the very end. Uh, print TPU devices, devices. Now let's just see if this works

Speaker 4:          00:37:12       really, but I did define Wes. Did I not? Did I not really. Okay, fine. Fine. All right. In Porto is, did I really not? Okay, let's just import everything again. Tentraflow as TF. Okay. It's busy. It's busy, it's got that cheap, it's got that CPU, it's got that TPU. So there we go. We are connected to our TPU and now we're going to do this. So we'll do the same operation we did before. And I'm going to take questions after this. So just start asking questions right now guys. Now I'm just going to do the same thing I did before where I have this ad operation. Okay. But, um, the different, and I'll do the same thing as up here.

Speaker 3:          00:38:09       Okay?

Speaker 4:          00:38:10       By the way. So when I said I wrote the code, I wrote the, I wrote the, this, this, um, TPU versus CPU benchmark. It was based off of, uh, Google's original code, but there was no, like TPU versus CPU version. There was only a TPU version. So I converted it between the book, between both so we can see the difference. Um, okay, so where were we? So the TPU operations here.

Speaker 4:          00:38:36       So what we're going to first do, and here's, here's a, here's an important bit, or we're, we're going to rewrite that ad operation for the CPU specifically with this rewrites function. Now, like I said before, um, tps are running on the systolic array, a hardware architecture. So we have a multiply and accumulate operation that is specific to the TPU. And in order to be able to compile down using the XL, Hey, a compile time environment, we need to convert it into a unex l a friendly, ah, format of matrix operations. And that's what rewrite does. So once we initialize our tensorflow sessions, we'll do this in a try. Finally loop.

Speaker 2:          00:39:18       Okay,

Speaker 4:          00:39:19       what's say session.run and uh, well the first thing we're going to have to do is initialize the system. Okay, that's the first thing we have to do. And then we'd say, well, let's start the timer and we'll print out the results of what happens when we say run a session using those TPU operations. We're going to feed in the same deal. So it's gonna be the num py. Dot. K range between Oh oh and 101 in 100 and for why it's going to be end pay n p. Dot. A range between one in 100 and right? That's it. That, that I think is it. So then once we have that now, now we can end the timer by saying and time. We'll count the amount of time that has elapsed since we started. So if elapsed equals and minus start and we'll print the elapsed time. Now that's it for that. And then in our finally loop and make sure finally is here. Finally, we will, uh, shut down the system. So this is necessary. We have to, when we're using the TPU, we have to shut down. We used to run this command after we are done with our computations and then we can close the ship, the session. Okay. Let's, let's hope this works. It's not gonna work, but let's, let's hope it works. Of course. So, okay.

Speaker 4:          00:41:07       Why would that be invalid syntax in valid syntax on line 14.

Speaker 2:          00:41:13       Oh,

Speaker 4:          00:41:15       right. Okay. So print session.run TPU operations x, y one more I think. Right? Okay. I really hope this works. I really hope this works. This is what life is all about. You, I'll make a fool of myself or okay, here we go. Time is not defined. Yes it is. Okay, fine. So, um, I think it's like time is not defined. So import time to port time NP, isn't it? Okay. So, so I guess import tensorflow as well. Might as well come on. Yes. Really. It has no attributes such shutdown system. Oh, because of this. Okay. Yes. Oh, TF. Dot. Contribute, contribute dot TPU dot shutdown system

Speaker 4:          00:42:59       are start asking questions. I'll take some questions in a second. And I think this is finally going to work. Yes. Good. Okay. So 0.016 on the TPU versus where were we before 0.011. So actually this, the, a GPU was faster, so this is actually, oh no. Yeah, yeah. The GPU was faster. So this was actually a GPU versus TPU comparison. The GP was faster for this extremely small operation, this ad operation. So my point here is just due to, I've done two things. One to demonstrate that you can use the TPU and to that when it comes to small models like what we've just done here, it's not like Tpu is like the be all end. All right. It's got a specific use case. So now let me take some questions. Um, Ken tps process, neural networks made with care. Yes. We'll do that at the very end. One more question. Um,

Speaker 3:          00:44:08       okay.

Speaker 4:          00:44:09       What is the next big thing after Tpu is, um, it's uh, it's quantum computing, quantum computing, so I'll have more to say on that later. So that's it. Quantum quantum and at least the unleashed the quantum anyway. Um, it's, it's all about having a wide variety of different types of types of chips and they all do different things. Chips are

Speaker 1:          00:44:31       good, good for specific tasks. You know, tps are good for neural networks. CPS are good for sequential tasks. GPS are good for rendering and just altogether we can use them for Agi, artificial general intelligence, but not not to get, you know, anyway, checkout Jerome linear by the way. Great. A philosopher. So, uh, that was it for Q and a and now there, there are some key, let's see what time we have. There are some key TPU functions in tensorflow that I'd like to talk about right now. Okay. So the first one that you should know, there are three. The first one is the TPU estimator. Okay. So estimator is our application level constructs in tentraflow. Right? So they apply to any, let me just minimize this.

Speaker 1:          00:45:17       They apply to any kind of model, but we can use a TPU specific estimators using this command right here or function right here at tps matter for TPU specific logic. Now Standard estimators are good for CPS and gps, but TPU estimators are made for Tpu. And so this is what it looks like when we build that. Now notice that one of these parameters is this config parameter. So using this TPU dot Ron Config, that's the other TPU specific object that we have to create. And so we can do that using this tpu.run config function. And we, and we give it the evaluation, uh, the model directory, the session configuration and the TPU configuration.

Speaker 1:          00:45:59       And the third is the cross Shard optimizer. So like I said, that 1%, we're talking about calculus. It's right now. So neural networks are models built with linear Algebra and they're optimized with calculus using gradient descent. For example, the most popular optimization strategy in neural networks that I won't go into just search grades, the sensorial and you'll find like 50 videos on it. But, um, gradient descent works on tps. All sorts of optimizers work, work for neural networks, work on tps, uh, as long as they are made for neural networks, specifically Adam. Um, there's a bunch of optimizers there, variations of grading dissent. Um, but

Speaker 3:          00:46:44       uh,

Speaker 1:          00:46:45       the TPU uses a specific technique called all reduce, um, that I need to actually learn more about. Um, and I'm not going to pretend to know all the details about all reduced cause it's um, it's a very specific technique. Um, and we can't just fit literally everything into this video. And um, I'll talk about tps more later. But what we know is that the cross Shard optimizer is made to take an existing optimizer and optimize it for, for the all reduced algorithm to aggregate the gradients. The gradients are the direction. This, these skip these, these values that are vector values that are used to update our weights in whatever direction we're optimizing for. And it broadcast the results of each of these gradients to each shard and each TPU core. So what

Speaker 1:          00:47:30       this looks like for us as developers is we will build our optimizer just like normally, and then we will wrap it using this TPU dot cross Shard optimizer. That's the difference. And then we just do the rest. And so that's really the, the main differences here when we're building TPU specific models. Now of course there are a few other little changes. So evaluation metrics and tenser board, you have to use specific TPU code for each of these things. And the tensorflow documentation is great for that, but those are the three big ones. Now, one more benchmark here. Okay. So, um, image classification with CFR. Okay, so see far as an image classification data set and the, the problem setting here is given an image, classify what it is, a dog, a cat, a car, whatever it is. It's very standard, a task and machine learning. Now the standard neural network architecture to solve this is a convolutional network. But for the sake of this demo, we're just going to use a linear, a multilayer perceptron. Okay.

Speaker 3:          00:48:36       Okay.

Speaker 1:          00:48:37       And we'll, we'll compile it on both the GPU and the Tpu to see the difference in performance and in the code and, and, and how that changes.

Speaker 3:          00:48:45       Okay.

Speaker 1:          00:48:46       So if we look here, we're sent were we can see the dependencies and so I already have it written out and I'm going to modify it the live so and hopefully it works. So what we see here is us importing tentraflow, we're importing tensor flow and Cara Ross and so we have our basic um, multilayer perceptron MLP model that we built right here. Okay?

Speaker 3:          00:49:08       Okay.

Speaker 1:          00:49:09       Input Times weight, add a bias, activate input times weight out of buys, activate. That just keeps repeating. Okay? And we have some special tricks here. I'm fully connected. Layer a batch normalization and activation function and drop out. Okay? So what we do is once we built our model, we'll say, okay,

Speaker 3:          00:49:26       okay,

Speaker 1:          00:49:27       let's get our CFR code using this low data function and let's build our model. Let's use those two functions we just created and then compile it using Adam, which is a great descent technique. And then we'll fit that model on the training and testing data. Now when we do this on the Gpu, let's run this code and then we'll modify it for the TPU. So we'll save that. Hopefully this works. You never know. You never know. You never know. Never know. Okay. So let's see what happens here. When we train this thing, it's going to download this data and let me answer some questions while it's downloading the c four data. When is the wrap the rapids? The wrap is at the very end. The wrap is a reward for those people. It's a, it's a longterm reward for those agents who are optimizing for that reward to get into reinforcement learning, uh, at the very end, we're almost there. Okay.

Speaker 3:          00:50:25       Okay.

Speaker 1:          00:50:25       Well, the free tier of Google colab faster than running

Speaker 4:          00:50:28       it locally. Um, that's a good question. So, uh, Google cloud offers a tps as well, so we can use the Google cloud console to run our models in the cloud. We don't, it doesn't have to be with Colab just for teaching purposes. We're using colab right now, but um, it doesn't have to be end. Yes. Okay. So happy birthday. Harsh. Okay. That's it for the questions. While this is downloading. So this is actually going to take a while. So while this takes a while, I'm actually going to turn this into TPU coat.

Speaker 3:          00:51:01       Okay.

Speaker 4:          00:51:02       Okay. Let me turn this into Tpu code now.

Speaker 3:          00:51:05       Yeah.

Speaker 4:          00:51:05       So to convert this into TPU code, what do we do here? So we're still going to have our model just like before. Uh, the difference now is we're going to say let's, let's do that for first is we're going to apply that cross Shar optimizer that we talked about before. So we'll say optimizer equals

Speaker 3:          00:51:23       okay.

Speaker 4:          00:51:24       T F. Dot. Train Dot Adam optimizer just like we had, you know, right here. And the learning rate is like, like it's right here. One. Okay. Now what's the difference? We're going to create our TPU, optimize her. Now how do we do this? Remember, like I said before, we have to use that cross Shard optimizer and wrap our existing optimizer around that. And that is what we use to train our model, the TPU optimizer. Okay. Now there's that. There's that bit. But before we train our model, we have one more thing to do. Um, for Colab, which is to say that we're going to connect in Colab to the TPU that we have.

Speaker 3:          00:52:21       MMM.

Speaker 4:          00:52:24       Inside of this virtual environment

Speaker 3:          00:52:33       and weed,

Speaker 4:          00:52:37       we want to use a cluster. Okay. We want to use a cluster of TPU is not just a single TPU. So we'll use the cluster resolver. So what the co cluster resolver is, is it's letting us access apod, right? So TPU pods are clusters.

Speaker 3:          00:52:56       Okay.

Speaker 4:          00:52:57       TPU GRPC Url, the one we just defined. And then we'll say, well, what is our strategy? So it turns out that Care Ross has a specific type of distribution strategy, uh, for that pod call. And we can call that using TPU distribution strategy and we'll use the cluster resolver as the parameter to that.

Speaker 3:          00:53:23       Okay.

Speaker 4:          00:53:23       And now that we've done, we can say, let's take our,

Speaker 2:          00:53:29       uh,

Speaker 4:          00:53:30       care Ross model and convert that to a TPU model using the chaos to TPO TPU app, aptly named function. And uh, and we'll use the strategy of the one we defined

Speaker 2:          00:53:45       strategy. And that's it. Okay.

Speaker 4:          00:53:51       Um, so this is actually going to take a while to download. So what we'll do actually is pause this. Okay. Cause it's going to take, this is live and we just want to see if this compiles and it's not going to compile because it's the runtime is Gpu. But if we set the runtime to Tpu and save that by to connect, we just want to see if this compiles this TPU code. Um, can we fix this and then I'll answer some questions. Okay, so invalid syntax where strategy equals strategy. Oh of course.

Speaker 2:          00:54:40       Winning. Okay,

Speaker 4:          00:54:42       good. So that, so there this TPU that that was us converting it into Tpu Code that this is in care Os. Okay. So we're not going to sit here and wait for it to download the data and train and you know, et Cetera. But that's how that goes. Now Google has this default train Shakespeare in five minutes example

Speaker 4:          00:55:04       that we'll find right here. And what this does is it uses care os to train a recurrent network to generate speech in the style of Shakespeare. So the input data or Shakespearian text like these poems loves had led me. No doms lacquer doesn't even make sense. But let's just go through this very quickly. We're downloading the data from Project Gutenberg. We build that data in Coupeville, Colab, we have a specific address for the TPU. Okay. We're doing some data transformation. None of it is TPU specific. It doesn't seem, none of this is TPU specific. We build our model. This is a long short term memory network. A type of recurrent network made specifically for longterm sequences like giant, uh, Corpus core Pi corpuses of taxed. And I know, you know, lst just search Lstm Saroj for a video on that. And uh, yeah, nothing TPU specific here yet. When we train the model, here we go like we did before TPU. Dot Care Ross to TPU model. So it's resolving to the Mac, uh, multiply, accumulate architecture of the systolic array architecture for the TPU. We're fitting it, they train it, they make predictions with,

Speaker 3:          00:56:24       okay.

Speaker 1:          00:56:24       None of it as TPU specific. And here, here's the hearing the results. Okay. So it's actually very simple to use a TPU now and this is a first, so thank you Google for releasing this. Let me answer some questions and then, uh, that's it for this live stream and we'll wrap as well.

Speaker 3:          00:56:40       Okay.

Speaker 1:          00:56:41       This is a part of move 37? Yes. Um, can we get a link to your colab file? It's in the video description. It's an eye python notebook. You can then upload it to Google colab because it's a backwards compatible. And last question, we'll Google make more money through AI than advertising in the future. Advertising is done by Ai. Let me just end this with a very important thing that needs to be said. Ai is everywhere. Ai is manipulating all of us. Ai is normally with advertising, with billboards. It's a, it's a, it's a one way. Speaking of that with billboards, it's a one way. Um,

Speaker 3:          00:57:26       yeah,

Speaker 1:          00:57:26       with billboards and with ads that we see in real life, it's a one way relationship that we see the billboard and we decide if we want to buy it or not. But with the Internet, with Ai, when we see an advertisement online, it's going to measure what we do in re in relation to that advertisement and then subtly show us something different to manipulate our behavior such that we will be more likely to click on that ad the next time. So we're all subtly being manipulated by these AI algorithms because that's how the, um, that's how the business model works for these social media giants.

Speaker 1:          00:58:06       And that's just how it works right now. But we've got to get to a world where we are, our AI algorithms are not optimizing for our attention for the sake of this third party advertiser, but instead for our time well spent AI algorithms that benefit us. And that's a, that's a new world that we're heading towards and it's a beautiful world and we all at school of AI envisioned that world and we will get to that world. I promise you I'm, but one step at a time. And, and thank you for being here and thank you for being a part of this community. Um, so there's a lot of exciting things that are coming out of Ai. You know, like any power, like, like fire, fire can burn us or it can use, it can be used to give us warmth and to sustain us to, in the same way AI can be used to manipulate us and, and distract us and control us and, and, and polarize us. But it can also be used to bring us together to solve some of the hardest problems in the world and make the world a better place. And we at school of AI or determined to make that the latter reality, our reality. So thank you for being here. Uh, it's, it's been, it's been a pleasure. And let me just end it with a wrap because I sent him because I promised her rap. Um, so going from a little serious talk to a yeah,

Speaker 1:          00:59:22       more fun talk. Um, looks like my youth, my youtube APP is a, I'll just go, I'll just go acapella. I like. Okay. Um,

Speaker 1:          00:59:37       I actually, I want to, I want to close this out at 59 minutes and I just want to leave you with that thought. Actually, let's, let's not be silly at the end this time. Let's be a little serious and say, you know, this is something that needs to happen. Um, that's it. That's it for this live stream. Thank you guys for showing up for now. I've got to go do a bunch of a school of AI work. I have some great content coming out for you this weekend on how I'm so productive. So you better watch out for that. It's coming out on Sunday or Monday, and another video as well that I'm going to keep close to the chest. So thank you guys. I love you.