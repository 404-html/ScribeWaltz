Speaker 1:          00:00          Hello world and Siraj and the technology that excites me more than anything else is called a brain computer interface. I'm going to talk about it philosophically, theoretically, and of course technically via a python demo that lets you control a robot with your mind. Nikola Tesla was one of the greatest scientists to ever live. He invented the alternating current, the radio, the electric motor, and so much more, all of which make possible our modern way of lives. In 1926 he was quoted as saying, when wireless is perfectly applied, the whole earth will be converted into a huge brain, which in fact it is all things being particles have a real and rhythmic whole. We shall be able to communicate with one another instantly. Tesla perfectly predicted the modern smartphone. The smartphone has only been around for two decades, but it's already hard to imagine life without it.

Speaker 1:          00:54          Evolution has managed to squeeze 100 billion neurons into our biological neocortex, which if unfolded, it's just the size of a small Napkin, but like Tesla predicted. There is also a digital brain, a digital neo cortex. We're building the wires that connect the Internet together, act like a highway network for what's essentially a giant nervous system. Smartphones act as the bridge between our biological neocortex and our digital one. Researchers down that when people were asked to stop using their smartphones for a long period of time, they had a spike in heart rate and blood pressure as well as feelings of anxiety. There aren't actual physiological changes that occur when we're separated from our phones and that makes sense. Part of our identity who we are, is already in the cloud in the form of our social media accounts and message history. We're already cyborgs are smartphones are like a third arm.

Speaker 1:          01:49          And since our biological intelligence wasn't designed to be able to parse and understand billions of data points at once, we've created artificial intelligence is to help us do just that. Almost every software app we use today uses AI in some way to learn from the massive datasets being generated and deliver the insights we need instantly. So Ai isn't just a feature, it's a decisive smartphones act as a bridge between human and machine intelligence. But this global brain we're building for ourselves has some problems. Since AI algorithms run everything and they're constantly being improved by teams of researchers. If they were to achieve consciousness and decide to go on strike, our entire civilization would be crippled. We couldn't get money from our bank. We could communicate transportation and manufacturing would grind to a halt and we couldn't do anything about it. These intelligent agents would have a clear advantage over us.

Speaker 1:          02:46          They'd have access to way more computing power and data. Then our biological brains do. For the first time in human history, an intelligence explosion is occurring and if we're to continue to be at the forefront, we've got a truly upgrade. Our own intelligence, according our smartphones every year, can only take us so far. It's too limiting a medium in terms of the input and output bandwidth as far as output, converting our thoughts into data via thumb. Typing on a virtual keyboard is too slow and cumbersome around a few tens to a hundred bits per second. In terms of input. Our eyes are much higher bandwidth using our eyes to process data from say, newsfeeds is on the order of a few megabits per second. The only way of getting past these limitations would be directly capturing signals from the brain itself. We need a device that would act as a brain computer interface that would maximize the bandwidth of the interaction between our biological and digital brain, making it much more fluid and natural.

Speaker 1:          03:47          It would give us read and write access to our own brain. Basically open sourcing it and if we could do that, the possibilities are endless. Depression, anxiety. Think of all of these as tuning knobs. Neural technology will let us become masters of our own minds, letting us achieve any mood or emotion or mental state we'd like. We'd be able to connect with people in ways we never thought possible. Sharing memories directly, elevating the intimacy of love and friendship. A thousand fold. We truly be able to understand and empathize with any human. Imagine an APP store for the mind. We could download packaged virtual experiences and memories directly into our brain. Downloading new skills like foreign languages, even entire graduate school curriculums into our head, and he would take seconds smartphones with a first step in this direction, but that medium has also acted as a barrier between us and fully experiencing our reality.

Speaker 1:          04:44          So how do we create this brain computer interface? Let's look at the timeline of progress in this field. In 1924 the German neuroscientist Han's Berger was the first to discover the brain's electrical activity. He placed needle electrodes under the scalp of patients who had lost some of their skull bones in surgery. His device was called the electroencephalogram or EEG. A few decades later, Darpa initiated a private program to further explore brain communications using EEG technology for military use. But it wasn't until [inaudible] 98 that an actual implant was placed inside the human brain. It was for a stroke victim who couldn't speak. So a team of researchers decoded the high quality signals they received such that he could slowly spell out words by picking out letters from an onscreen keyboard. In Oh five a monkey's brain was successfully used to control a robotic arm showing that it was possible to convert these electrical signals into one way interpretable commands by a machine three years ago.

Speaker 1:          05:48          The first direct brain to brain communication, a simple yes or no was achieved by transmitting EEG signals over the Internet between two people. By focusing on the Yes light, an EEG device generated a sense signal to the inquirer, the the internet to activate a magnetic coil positioned behind the inquirer's head, which stimulated the visual cortex and caused the inquirer to see a flash of light. This demonstrated that we could both read and write to the brain a two way communication and just two years ago a paralysis patient became the first receive a sensory enhanced robotic hand, which he controlled using just his brain. It also allowed him to feel when the hand was being touched, the hand was surgically wired directly to his brain, providing him with two way electrical feedback. Despite the signals coming from a robotic hand, the patient said that the sensation of having his fingers touched was almost natural.

Speaker 1:          06:44          Fast forward to present day. Brian Johnson, who sold his startup to paypal for $800 million is now building a company called colonel after pledging 100 million of his own money to the operation, which aims to build the neural tools that can read and write the underlying functions of the brain. Facebook recently announced that it's yearly conference that they were developing a device that uses optical imaging to scan your brain hundred times per second to let you translate your thoughts directly into text. And of course the most notable attempt at this is by none other than Elon Musk himself, who recently founded neuro link as a way to develop ultra high bandwidth brain machine interfaces to connect humans and computers. All of these companies are pretty secretive about the work they're doing. You can get hints and glimpses by looking at their hiring pages as in the requirements for the type of people they're looking for.

Speaker 1:          07:42          They're usually people with experience in either neuroscience or machine learning, brain surgery, building of hardware, or even all of the above. There are two types of BCIS, the invasive kind, which required surgery and the noninvasive kind. The noninvasive kind include measuring eye movements and pupil size oscillations via camera using magnetic resonance imaging, Aka an Mri or using an EEG device. Pupil measurement research is still in its early stages and an MRI is an incredibly bulky and expensive device that uses a magnetic field and radio waves to produce detailed images of the brain currently limited to medical facilities. However, for EEG devices, there are some commercially available ones out there like the muse, neuro, sky and emotive. These are consumer EEG so that you can wear yourself and stream your own brains data to your computer, allowing you to control all sorts of things with your mind is actually a lot of fun.

Speaker 1:          08:42          You can lift a rock in a three d simulation, control our robots movement, control mouse movements and type all with your minds. For example, the open BCI device is an open source EEG with up to 16 channels that you can read from via a python library for easy access. Once you've connected the board to your computer, you can read from it by specifying which pins you'd like to read data from. And once you have electrodes attached to each one and a devices on your head, you can enable it to string the data to your machine. You'll first be able to view that data in terms of numerical readings from terminal, but with some javascript you can see it as a much prettier graph, but are there any noninvasive mediums to write to the brain? Well, just two months ago, a paper in cell reported a new noninvasive brain stimulation method from MIT called temporal interference stimulation.

Speaker 1:          09:34          The technique is based around applying to electrical fields to the subjects head. Each field is applied using to scalp electrodes and the interaction between these fields create brain stimulation. Invasive BCIS are an easier way to maximize our input output bandwidth, but surgical implantation is difficult. The most obvious way is to use a closed loop implant similar to what's already been done to some disabled patients. This involves cutting open the skull and Scalp and placing a very small chip with many electrodes cramped into it. Onto the brain. This is obviously not preferred since it's really risky. One idea originally from Scifi that Elon has expressed interest in is creating a neural lace. This is very thin silicon coated in a polymer with cross connections made purely of polymer. It's a programmable mesh of stretchy, flexible silicon nanowires. The meshes then put into a liquid making it curl up.

Speaker 1:          10:33          A syringe with the diameter as small as 100 micro meters can draw it up and inject the mesh into specific regions of living tissue. Once in the brain, it uncurled to about 80% of its original configuration sits on top of neurons and starts moderating for humans. This could be implanted via an injection somewhere on the body traveled via the bloodstream and ultimately interface with neurons directly after crossing the blood brain barrier. Another idea mentioned by one of neuro links, cofounders DJ set is creating neural dust. These are tiny 100 nanometers silicon sensors. The width of a hair that could live throughout the cortex and could be injected into the bloodstream, similar to a neural lace on the outside. A super small device could communicate with the dust sensors via ultrasound inspired by RFID technology. The same that's used for hotel key cards to unlock physical doors.

Speaker 1:          11:31          While these ideas are awesome, we've got some problems solve first, like how does the brain work? How can we record lots of neurons all at once? Can we build such small sensors and can they last in the body for a long period of time? And what does the neuro security stack that protects your brain from being hacked look like producing a reliable minimally invasive neural device that can both correctly read your mind and also stimulate neurons to write to your mind is hard Aaf, but as long as there are many smart people working on this, the future looks bright. New York shitty. The winner of last week's coding challenge is Tim Kula Teralba Gpu programming is for the bravest of programmers and 10 coolest sped up Mandelbrot set visualization successfully with Kuda. Check it out and the runner up is each on why we use Kuda to parallelize gradient descent. The KUDA coat didn't result in any big speedups, but a plus on the effort. Please subscribe for more programming videos. And for now, I've got to try to interview Elan, so thanks for watching.