Speaker 1:          00:00          How does that make you feel? I'm not sure. It's hard to say. Yeah. Well there's an episode of Syrah geology coming in. I just don't have time for this right now. So hello world. It's Saroj. Have you ever had a hard time trying to understand what someone is trying to say to you? People aren't confusing. I don't even understand my own emotions half the time, much less what someone else is feeling or thinking. When we read a piece of text or listen to someone speak. Our brains are performing tone analysis. We're trying to understand the meaning behind the words. That is the sentiment and the emotions and the style. It's not an easy task at all, but this is something machines can do pretty well. Now it's all a part of a subfield of machine learning called natural language processing. Standard approaches to NLP a couple of years ago in both extracting a set of features from some labeled piece of text that feature, and we're usually end grams and grams have nothing to do with Graham crackers even though they are extremely delicious and grams are just sequences of words.

Speaker 1:          00:58          So a Unigram is one word by grandma's to and Trigram is three. Once the end grims were extracted, the next step was to train a linear model on some pre labeled data so they could classify similar texts. The process of feature extraction saw a hugely forward when Google open source, something called word to Vec, word to Vec is a toolkit that helps encode words into vectors. These vectors are representations of words called word embeddings that are learned by training on a given corpus. The tool kit consists of two distinct models, skip gram and continuous bag of words. The skip grant model predicts the neighboring words given the current word and a given window. In contrast, the bag of words model predicts the current word given the neighboring words and given window. These models both help predict and code words. Once we had these vectors, we can use them to do all sorts of text classification, including tone analysis, deep learning.

Speaker 1:          01:51          One of the first papers to show that deep learning could improve text analysis, who's called convolutional neural networks for sentence classification. Although CNNS were intended for computer vision. These guys applied it to NLP. They first train the CNN on a set of word vectors that Google extracted from 100 billion word corpus using their own word to Vec toolkit. After training, the CNN had built representations of all sorts of word categories. Since the vectors weren't labeled in any way, the training was considered unsupervised. After they had a train on a set of Google vectors, they then train it using labeled data so that he could perform sentiment analysis. A later paper called Tex categorization using LSTM for region embeddings improved on it. They did the same basic experiment, but the key difference is that they didn't just use a CNN to create embeddings. They also use a long short term memory network.

Speaker 1:          02:40          On LSTM is a type of recurrent neural network that can remember dependencies from way back in the sequence of data and they found that they had the best classification result. When they combine the embeddings from both the LSTM and the CNN, just like two peas in a algorithm, that was terrible. They also found that embedding regions that sets of words. What's more effective than embedding single words. This idea of abstractions of a hierarchy of knowledge in a given document helped inspire a fresh paper released just two months ago called hierarchical attention networks for document classification. These guys said, let's create a new neural architecture to model a document. It starts by encoding words and applying an attention mechanism to extract the most important words. Then it encodes a sentences using the weights. It learned applies an attention mechanisms to that to, to extract the most important sentences.

Speaker 1:          03:31          It uses those weights to build document level vectors. So it's creating vectors for each layer of attraction within a document and building them off of each other. The encoder for each of these levels is called a gru neural net. So after they initialize the model using vectors, they got using word to beck a trended on labeled data and doing this with their novel neural architecture pretty much outperformed all previous attempts. So how do we implement this stuff ourselves? Well as professor [inaudible] says, deep learning requires a rocket, the model and rocket fuel that data, but sometimes you just want to get shit done and training is to time expensive. That's why we're going to use IBM's Watson Api to perform tone analysis on an example set. First we'll need to sign up for their cloud service called blue mix. Once we registered, we'll click on Watson, then tone analyzer or create a new service using the standard plan which lets us try it out for free so no need to enter a credit card, which is perfect because I'm poor.

Speaker 1:          04:23          Then we'll click on service credentials and record our username and password since we'll need them to authenticate from our web APP. Then we can generate a new note express web app using NPM and the express generator module in our APP will import our Watson developer cloud npm module, which acts as a thin javascript wrapper around their API. We have two routes here, both of our index page. We'll start with the get route and we make a get request to the index page. We want to display some html that we can send programmatically. Here. We'll send an input form that asks for some corporates. Then in our post trout will retrieve the input that the user submits via the post request. Then initialize the tone analyzer variable, filling our credentials and call the tone function using the user input variable as the parameter. This will return to the analysis of the text as Jason, which we can view in terminal. Let's try it out. As you can see it race the Corbett's on three levels, emotions, language, style, and social tendencies. You'll get back a percentage for each of three language styles and five social tendencies. It does this at both the document level and sentence level. Tone analysis is a tool you can use to better your writing and be more clear about the message you're trying to convey. Dope links for you down below. Please subscribe for more ml videos. I've got to go fix an indentation error, so thanks for watching.