Speaker 1:          00:00          Hi, I'd like to buy this phone. We only accept doge coin. What? Hello world. It's so Raj. And let's build a simple bitcoin trading bot using machine learning. Crypto currencies are generally considered high risk investments. So many traditional finance and Economic Gurus from the famous investor Warren Buffet's to the Nobel Prize winning economist Joseph Stiglitz have said that Bitcoin is a scam with no substantial worth that will soon experience a crash as bad as the 17th century tulip mania that overtook the Netherlands. If we look at the chart of Bitcoin's price over the years, we can see that there have been several mini crashes throughout its history. By definition, a bubble is a price largely above the real value of an asset and it bursts when people realize that. So the question then becomes what is the fair value of Bitcoin? We know that Bitcoin doesn't require the bank credit mechanism for money creation.

Speaker 1:          01:05          It uses mathematical algorithms to mint. It's tokens. It can also be used where normal money would be impractical, like in micro transfers between machines, Aka the Internet of things or trading in multiplayer games. It's uncensorable and it allows for anonymous transactions. Bitcoin's current supplies around 17 million bitcoins, while the current cash in circulation in the U S is one point $5 trillion, meaning we could currently replace 15% of all us cash with bitcoin. Clearly it has real value unlike ripple, but that doesn't mean it's price will continually rise forever. Who knows what its real value is? It could be 15,000 or $15 per coin. It's fair to say though that at this point, bitcoin must have developed price curve inefficiencies that can be exploited in a trading system. We could try momentum investing, not momentum from gradient descent wizards. This is an investment strategy that aims to capitalize on the continuous continuance of existing trends in the market.

Speaker 1:          02:13          The idea is that once a trend is established, it's more likely to continue in that direction than to move against that trend. The problem, however, is that crypto is far too volatiles, so conventional model based strategies won't work well. We need a fast trading trend agnostic strategy. That means it holds positions for only a few minutes and is not exposed to the bubble risk. We need to exploit short term price patterns using an algorithm of our choice, but before we pick an algorithm, let's find ourselves a nice big coin Dataset. Luckily for us, Kaggle has a fund data set of minute by minute historical prices of Bitcoin. It includes seven factors that can use including prices and volume, and before we feed this data into our model, we need to normalize it. That means adjusting the values of all of these features that are measured on different scales to one common scale.

Speaker 1:          03:14          This will later boost our model's performance since the feature values will relate to each other more so it will be easier to find the relationship patterns that exist between them. One easy way to do this is to take a sliding window of a size. We decide and slide it across the data to make sure that there are no zero or n a n values. They're normally, this would be a pain since we've got multiple dimensions here, but thanks to the python pandas library, we can represent each window as a pandas data frame. Then perform the normalization operation across the whole data frame, meaning all of its columns for the sake of cleanliness. Let's give functions their own self contained class as a data loader. That way we can reuse it later to extract, transform and load our datasets. Now that we've done this, we simply need a model that accepts data of shape m where M is a number of dimensions in our data, so what can we choose here?

Speaker 1:          04:18          If we look through the academic literature on the topic of predicting market trends that hasn't been burned by the big banks will see that all sorts of models have been used for time series forecasting, which is what this problem is considered. Random forests support vector machines, neural networks, and seeing as how neural networks tend to outperform most other machine learning models. Most of the time when given lots of data and compute, okay, deep learning, we might as well try that model. Keep in mind that a neural network is a composite function. Every layer in the network is a function and the network as a whole then becomes a function of functions for as many layers as there are. The output of the previous layer is what feeds into the next layer as input until a final output is computed. If we train a feed forward neural network on this data, it will be the case that at every time step it will use a new data point as its input, but we are under the hypothesis that each data point is closely related to the data point that came before it, that there is some relationship between successive prices in the graph.

Speaker 1:          05:35          So we need a way for our network to be able to understand this. These data points are not created in isolation. They are a part of a larger sequence so we can make a change to our model instead of feeding it every new data point at every time step so that the hidden layer is continuously updated. We'll feed it in the current data point as well as the previous learned in state. Keep in mind that the hidden state is just a matrix through the learning process, it's going to be modified so that wind dot product operations are continuously applied to it to produce an output. During inference, the resulting output will be much more accurate. This recurrence allows the network to not just learn from the data but to learn from what its previously learned allowing per sequence learning. But recurrent networks have a big problem.

Speaker 1:          06:32          They can't remember really long sequences. So Bollywood movies are out of the question. This is called the vanishing gradient problem. During the optimization process, the gradient is recursively computed and apply to every layer of the network from the last one all the way back to the first. And as this gradient is computed layer by layer, it gets smaller and smaller and because the gradient update gets smaller, the network isn't able to store the memory of the changes. It's learning over a long period of time. That memory slowly diminishes. So we need to use a special and popular subset of recurrent nets called the long short term memory networks. The idea is simple rather than each of the nodes of the network just having a single activation function. Each node is a memory cell in that it can store other information so it maintains its own cell state while rns are fed their previous hidden state and the current input at every time.

Speaker 1:          07:34          Step an lst m does the same except it also takes in its old self state and outputs. It's new self states. What's happening inside of this memory cell is really cool. It's digital biology. There are three steps here. The cell needs it for debt, the irrelevant parts of the previous state. It needs to selectively update itself based on the new input. It's just seed and it needs to decide what parts of the cell state to output as a new hidden state. This is all achieved by using a forget to gate an input gates and an output gate. A gate is a way to optionally let data through. Each is composed of a single sigmoid neural net layer and a multiplication operation. That's it. So firstly a function of the previous hidden state and the new input passes through the forget gate, which lets us know what is irrelevant and can be removed from our cell state.

Speaker 1:          08:33          It will output values close to one. For parts of the cell state, we want EIP and zero for the ones we want to get rid of. For example, if the price starts spiking super high for days because of some major event, the price before that could be irrelevant. We can forget that next, a function of the inputs passes through the input gate and it's added to the state to update it. There could be something new we've noticed and we want to add it to the cell state. That's why this is there. And lastly, the output gate decides what values from the cell state are going to be added to the hidden state output. So this will learn the parts of the sequence in the distant past that are worth remembering and those that are not. This ability to preserve information in the cell state for long stretches of time is what makes LSTM networks so special.

Speaker 1:          09:25          They solve the problem of bandaging gradients. Since the forget gate is essentially the weights and activation function for the cell state and because the LSTM can learn to set that, forget gay to one for important things into cell state data can pass through unchanged. Notice that our bitcoin Dataset is super big since it's minute by minute. We can't just load 1 million data windows all have once into care os the train. That would require a godlike amount of Ram that only Jeff Bezos hats. A better solution would be to use the Karrass fit generator function to iterate over date, out of unknown length, passing on the next piece every time it's called. We can train our model on one small chunk of data at a time and thanks to Ken Ross building an Lstm model requires just a few lines of code on your 15 to be precise. When we start training our model, it will take a few hours, but afterwards we can try forecasting the bitcoin price.

Speaker 1:          10:25          We can do this either on a point by point basis, meaning we add one to the tee variable, then ship the window of true data, predicting the next point along, or we do the same but multiple steps ahead. We can see that one step ahead is doing a reasonable job, but the predictions seem kind of volatile without doing more tests. It's hard to decide why this is the case and whether better parameters might fix this. Could add in sentiment data from Twitter to see how the crowd affects the price here. News sentiment is another option and we could merge the data then normalize it. So it's all numerical. Using Penn does a cutting edge technique right now is to use Basie and methods alongside LSTM networks to compensate for the fact that the factors that influence price variations, yeah, themselves change over time. Details on all of that in the video description

Speaker 2:          11:19          before you crash the stock market, hit the subscribe button and I'll keep making you better, faster, and smarter. For now, I've got to invest in some furniture, so thanks for watching.