Speaker 1:          00:00:01       Hello world. It's the Raj. Good to see everybody. Let's do this. We're going to, we're going to talk about recurrent nets today. So we're all my homies at, we're all my homies. That is my question. I have so many questions and not enough answers. So Raj, how far are we from solving intelligence? Anywhere from five to 10 years is my best guess. Let me mute this. I don't want to hear myself. I mean I do, but not do the audio. Okay. So we're, you know, five to 10 years. I would say. I would, I would hope, I mean the things are moving so fast. Yo, Yo Yo yo, Yo, yo Yo. Okay, so welcome to Bollywood. Okay. Hi Bonnie. Rosio Henry, she vom you on Aria. Tension flow versus by torch are, yeah, that's a great question. I, I am starting to like Pi Torch and, and in fact, uh, the machine, when the community is starting to say good things about Pi torch, that debate is happening right now, which is just crazy because tensorflow has so much momentum, but it's a good sign of showing just how fast things change in this field.

Speaker 1:          00:01:00       Nothing is concrete. Okay. Uh, how should I start? Mistake. So yeah, so five minute Q and a, right? So, hi pocket man and uh, Vishwas how should I start machine learning? Watch all of my videos. Really. There's this guy who just commented on one of my videos are genetic algorithms video and he was like, he was like you and I think his name was Fernando, but he was like, you started me on the path for data science. And now after watching all your videos, now I understand every line. And before I didn't know anything, so really watch my videos, all of them, I started to notice me. Voke hi, how's it going? Mohamad hello from Russia. Moscow. Uh, uh, is that over? That is like Rooskie the Hobo, this Levine's cubits. Ricardo, sir. Hamster. Okay. So we've got a lot of people throw those questions at me.

Speaker 1:          00:01:43       There are those questions. I mean, because today we are talking about recurrent neural networks and we're going to float memory aspect. That's what we're focused on because it is really important to talk about the memory aspect. Notice me Senpai I'm here. You're on a Tesla stock. Hello? From Paris, Poland. Man, I've gone to Stein. Wow. We have s Hungary. Oh my God, this is awesome. Okay guys, let's talk question. Can, can I start deep learning with no experience with machine learning? Uh, yes, absolutely. But you do need it. Some experience with math. So I would suggest introductory courses in statistics, calculus and linear Algebra. But what I'm trying to do is teach those things on the side as well. So, you know, just use those as reference material. Finland, Sheila, Brazil, Canada, uh, Ben, Ben, Ben, Benito. Um, Canada. Maple Syrup. How's it going? Justin Trudeau.

Speaker 1:          00:02:36       Okay. I'm the only data's guys and I've got, okay. Rns are better than life. No, but they're close. What's the difference between a CNN and RNN CNN? Did you see it? Okay. CNS apply standards or a whole different architecture? You you have, you're pulling involved, you've convolution, convolution, you have strides. They're, they're, they're focused on images and actually recurrent that's can be used images but there but we use them most of the time for sequential data. I really, everything is sequences in life. All, all data is sequential. Columbia. Okay. What's Pi? Torch by torch is a new library. It's similar to tensorflow but different and I'm going to definitely, definitely make a pie torch in five minutes video soon and I know it's going to get a lot of views but not yet because we are focused on this torch. Don't worry. We're going to talk about Pi Torch later and I'm sure it's going to get more and more popular over time because all the cool kids are talking about it.

Speaker 1:          00:03:25       You need a statistic about your subscriber nationality. We are, we have over 200 countries represented in this community. 200 countries. It is fast becoming the largest machine learning community in the world. So it's very exciting to be here right now. What do you think about profit by Facebook? So I saw that library and uh, I mean, uh, I have to look into it more. I didn't really read the details. Aws or Google cloud ml. Great question. So AWS recently went down, it went down because, uh, for whatever reason, and then all these apps we use just went down because of them because everything is based off of AWS. Uh, but that's just a, that's just a problem of centralized systems in general. And it shows why we need decentralized systems. That's a whole different topic. I wrote a book on that I could talk on.

Speaker 1:          00:04:09       I linked about that, but we're not going to, but it's comparing those two. I would say AWS because it's got more people using it and it's, uh, it's just, it's, I don't know. I'm, I'm used AWS and the customer support is also pretty great. They were subtracting school math. Uh, honestly it was English. English was my favorite subject. I love language. I love writing. I consider myself a technical writer and an engineer and a data scientist. But a English was my favorite subject in school, followed by close second math close second was map. Absolutely. I remember being super excited by math and having a hard time understanding it and in middle school, but I was just so excited about it. Uh, that you know, that that excitement is really what pushes you through that. The obstacles that you'll face. One more minute and then we're gonna get started with this code.

Speaker 1:          00:04:54       It's an eye I python notebook open AI and tensorflow don't compete there. They're not competing. They're complementary food. Why no twitch? Why? Um, because I just like to have it on Youtube because I dunno. We'll see. I mean, if I could, if someone thinks that did speedy use twitch, I will, but I'm not sure what, how do I build up CD for Google internship, have projects on get help. I'm actually going to release a video on job interviews this weekend or next weekend, so stay tuned for that. One more question and then we're going to start it. Uh, which one do I want to answer here? Can I can, how to use, okay. This is a good one. Parliament said how to know when to use LSTM and when not to LSTM generally improve our prediction. Every time. I mean, all of the greatest advances coming out of sequential learning involves LSTs right now in some way. And Andre Karpov these blog posts is so epic, the unreasonable effectiveness of neural networks. And most of that uses, uh,

Speaker 1:          00:05:56       LSTM use. So we don't have to watch live programming at five fts. Uh, okay. So that, that's a good reason. Okay. So I'll think about that for next time. I'm continuously trying to improve my livestream. So, okay, so you've got 333 people here right now. So let's get started with this code. We don't want to keep people waiting. Okay? So let's get started with this. Let's talk about what we're going to do. Let me just say what we're going to do first. We're going to build a recurrent nets. It takes him two sequences, okay? So one sequence is a binary sequence, right? At ones and Zeros, and be your arch nemesis. Neeraj high. And the other sequence is the same sequence, which shifted, okay? And we'll talk, we'll talk about how, what that shift means. Okay? But the idea is that we going to map these two sequences together.

Speaker 1:          00:06:37       We're going to learn the mapping between these two sequences using a recurrent neural network. So let's get started with this. Okay, let's start broadcasting guys, we have so much sequential sub to learn right now. We have, we're going to dive into recurrent nets. It's going to be awesome. Screenshare okay. Screen share. Here's our screen that we're going to share it. Now I have the notebook in the get hub link in the description for this video because you guys have been asking for that. So you know, I thought it would honestly, I thought it would be funner. I thought it would be fun or if, if you guys don't see the code, but you know what, you'll see it as a code it, but you know what, I'll just give it to anyway, so that's, that's that. All right, so here we go. With this here.

Speaker 1:          00:07:19       Here's the, here's the, here's a notebook and I'm going to code parts of it and parts of it are now going to be coated. So it's going to be a mix of both. Let's make it a little bigger. Okay. Let make it a little bigger. Okay. Or, or a lot bigger, one or the other. Okay. So that's what we're going to do with that. Let's get the comments over here and we're going to start building this. So everybody pull up a notebook. Okay. It's in the link and we're going to talk about what this happening, uh, as we go, getting started. Okay, so here we go. What we're going to do is we're going to use a recurrent nets. Oh. And also of course, I need to have my bass up here. What am I doing? So we're gonna do is we're going to build a recurrent neural network and we're going to build it with tensorflow.

Speaker 1:          00:07:58       We're going to build a recurrent neural network with tensorflow. Okay. And we're not going to use any other libraries to build their network. I mean, we're going to be some libraries, but they're not going to be to build the network. They're just going to be there because you know there to plot the data. Okay? Okay. So let's, let's get started with this. So what is a recurrent neural network? It is a type of neural network that is used for sequential data. We're going to learn about sequential data right now, and this is a great paper that I have linked right here that talks about recurring nets. Definitely check out that paper after this live stream, but not right now. It's probably the most comprehensive paper on neural now on recurrent nets that I found. And remember when you read a paper, remember to read the abstract first.

Speaker 1:          00:08:40       Okay? Don't expect to understand everything from a paper, just read the abstract and then keep on going. Okay? And if you understand you have tracks. That is, that is, that is a, that is a uh, achievement in and of itself. So we're going to start off by importing our dependencies, right? We're going to import I pythons display function because there's going to be displaying images. These images are going to help us a look at what we're doing. Num Pi is going to help us generate some binary input data. Tensorflow is obviously for machine learning. And then map plot line will help us plot this data. So I'm going to start off by showing this image, okay? Now what this is, is it as an image that shows a recurrent nets that is learning over time. So we have it's learning over time and these three squares or the states of the, the hidden state that is that, that hidden layer in the recurrent nets over time.

Speaker 1:          00:09:29       So it's this, it's so it's, so it changes over time, right? Why? Because we feed it, not just the input data, we feed the input data and the previous hidden state. So when we perform an optimization technique like backpropagation, it's essentially back propagation through time mode. We're just getting started. So you didn't miss anything. It's back propagation through time. So it's like back to the future, you know, it's like time travel and we'll talk about, not literally time travel, but in a way, because we remember what happened beforehand. We're going into the past. Okay. So, so that's what, that's what this is. So let's get started with this. Okay. That's, that's a definition of it. Recurring that now what we can do is we can start coding this thing. So we're going to start off by coding up our hyper parameters. Okay. So for a hyper parameters, we're going to define a number of talks to a number of reports, which is 100.

Speaker 1:          00:10:18       Remember so and so. So whenever we are training a network where you define epochs and inside of these deposits we have batches. So we have, so for like 10 epochs we'll have like five batches. So it's like 50 total iterations, right? 10 Times five because it is a, it is a nested loop, right? So for every epoch, how many batches do we want to claim against? So we're going to find that number of people I can obviously the more he talks, the longer the training, the better our prediction right to it's a trade off between computation time and accuracy, like all things with when it comes to machine learning, truncated back pop length. So, so the next uh, variable, the concrete it back properly. I'm going to talk about in detail when we get to it, but right now let's, let's skip that. Okay. Um, before our state size, it's going to be four.

Speaker 1:          00:11:03       So what is our state side? The state ties in. That is the number of neurons in our, a hidden layer in our hidden layer. Okay. So that's what it is because we have a three layer recurrent net that we're building and that hidden state if what we're going to up update over time and we're going to talk about how that works, then we've have an expert is going to be number of classes. So by number of classes it's into our data is binary. It's going to be two classes, but this isn't classification. It's a sequence to sequence mapping. The number of classes is one and zero. So it's two. Then we're going to talk about the echo step. The echo step is going to be

Speaker 2:          00:11:40       uh,

Speaker 1:          00:11:41       three. And so the echo stuff, I'm going to talk about that one in a second too. So remember these are the two variables that I'll talk about when we get to them. The truncated backpack lane and then uh, the echo step. And so the next one is going to be the batch size. So the backsides, remember inside of each epoch we have a number of batches. And so we're going to find it as fun. And then lastly, the number of that, just the number of batches is going to be the total series length. And we're going to take the quotient right. So in Python, this, this, um,

Speaker 1:          00:12:08       a character means potion by the backside and the quotion of that by the truncated back properly. And this is going to make sense when I talk about complicated backdrop length in a second, but let's go ahead and just type that in first. Okay. So that's kind to defined a number of batches we have. Okay, so let's start, let's go ahead and get started with collecting our dataset. What we're gonna do is we're going to generate this data and this is, this is a, this is a net. This is a good step to talk about. Like generating the data is a good segue to talk about because a lot of times we just want to test out our nerves, our neural net, but we don't want to test it out on, you know, real world data so we can just generate it ourselves using them five. And there's a lot of strategies to generate data, but what we're gonna do is we're going to generate a time series data specifically. We're going to generate 50 k samples of time series data, and it's going to be all binaries. Okay?

Speaker 2:          00:12:58       Okay?

Speaker 1:          00:12:59       So we're going to use the [inaudible] function to do this,

Speaker 2:          00:13:03       okay?

Speaker 1:          00:13:03       Okay. And we're gonna make sure that compiles, right? Okay? So the num py array function is going to say, well, we're going to generate a random distribution of data using the random by choice function. And we're going to say we want to, uh, classes, the length of the series that we defined earlier, and then the probability that each of those classes gets picked, it's 50, 50, right? So it's a 50, 50 chance that each number in the series as it's generated, it's going to get picked. That's what that is. And then, so that's going to be stored in x. And then what we're going to do is we're gonna generate our echo. So let's talk about what this echo means. So we're going to do, we're going to shift his three steps to the left. We're going to ship this time series, three steps to the left.

Speaker 1:          00:13:43       And what do I mean by that? By shifting it three steps to the left, we're going to pad it with Zeros, we're going to pat that data would zeros because it's going to be the same essentially the same exact series, but shifted to the left by three steps. And we're going to learn that mapping, right? We don't need any testing data because it's going to be the same shift, right? So it's two sequences and I'm going to show it with this image. This image is going to work. Okay, so, so the role function, the role function is going to say, uh, so we have x and then echo steps. So, so given like two numbers, let's say like two and 10 it's going to start to from before 10 and then end before it. So it's going to have to sue the outputs of this line right here is going to be, so if it's like two 10 right? Access to an echo stuff is 10 it'll be like eight, nine, one, two, three, four, five, six, seven. Okay. And then the reason we do that is because now we're going to pad the beginning with Zeros will say why and zero and then echoed step. And then finally we'll set all of those to zero. Okay. And then, then we're going to do lag by three.

Speaker 1:          00:14:47       Okay, so then what we're going to do, what's going to reshape it, right? So we have two more lines. If this Gary Beta, before we get started with our actual machine learning steps. So we're going to reshape this data so that it's formatted properly to feed into our network. We want to feed this into our network and you have to format it properly and numb wise reshape function does this, he takes a matrix and it reshapes it into a size that we give it. Now the batch size that we're going to reshape it as. Okay, so that's for why that's what x and now, now we're going to do it for y. Okay. This is to have an x and y that are correlated with time. Exactly. Okay.

Speaker 3:          00:15:25       Okay.

Speaker 1:          00:15:26       Okay. So then per batch size we have that and then we have negative one. It's great. So that's our x and Y, and then we're going return those values. So let's, let's look at, let's visualize what this looks like by running this function. Once there's function that we just generated, generate data and it's going to be stored in the state of variable. And then we'll go ahead and protect data. But let's see what this does. Let's see what this ducks,

Speaker 3:          00:15:47       okay,

Speaker 1:          00:15:47       yeah. I'm going to show the data. Here we go.

Speaker 1:          00:15:50       NP is not defined because I didn't compile a NP beforehand, right? To pile this. Yes. Let me compile that. Boom, compile. Okay. And then compile the hyper parameters and then compile this function. Okay, so this is our data, right? This is just a bunch of ones and Zeros. It's sequential data, right? And it's formatted to be a two d matrix of points, and now we can visualize it. It's all ones and Zeros, right? It's all ones and Zeros and to the x. The x values and the y values are both ones and Zeros. The x values are the initial ones and Zeros, the binary data, the sequential, the sequence of ones and Zeros. And the why data is that same exact sequence but shifted over three steps and there's first resets are then replaced with Zeros. And we want to learn the mapping between these two points. The point of this whole exercise is to learn about how memory is incorporated into a Nerc or recurrent net. And that's going to be the really cool part and that's what we're going to get to you. Okay. So, so that's the data that we've generated. Okay. So this is an image that shows what this data looks like. Okay. Basically it is. So this is an image of what the data looks like and

Speaker 1:          00:17:00       it's a reshape data matrix and the Arrow is going to show that. So showing the time steps, right? So for every new row it's a, it's a, it's, it's a new time step. We can consider it a new time step when we, when we feed it in, it's going to be at a different time step. Okay. So, so let's go ahead and get started with our actual machine learning with tensorflow. So step two, step one was to, uh, step one was to generate the data. And now step two is going to be to build this model. It's going to be to build this model. So we, so the the, so we didn't, so, so the point of reshaping the array, the tensor array, whichever you want to call it, will, so that we can feed it into our placeholder, which at that gateway that data flows in into tensorflow. Okay. So, so let's get started with our tensorflow portion. Right?

Speaker 1:          00:17:46       Okay. So, so for building our model, the first thing we want to is generate a, or to initialize both of are placeholders. We're going to have place holders for our x values and for our why doggies. Okay. So for our placeholder, we're going to say, let's use our float. So what is a data type or our first place holder? It's going to be a float 32 because these are 32 bit integers and then we want the shape of it, right? So it's gonna be a two d matrix of five, uh, rose and then 15 no, sorry, five columns. And in 15 rows, right? Just like, just like we saw up there, that's exactly what we saw. We want to fit it to be that. Okay. And it's going to be truncated back, proper lens, truncated backdrop. And remember, I'm going to come here to talk about this in a second. It's truncated back off my butt. So that's the batch y that Jack's okay. So we're going to do that for both place holders and in fact I can just copy this and paste it again because it's the same thing. Okay.

Speaker 1:          00:18:43       Are All those showing up now? You're not missing anything. Just look at the notebook so you can catch up. It's in the, it's in the link. Okay. So, so that's that. Most of our, uh, most of our placeholders, right? So remember in tensorflow, these placeholders are gateways. This is what we feed our data into. These are primitive types. These are primitive. It's intense. Your phones please. Holders, variables, constants. These are the primitives with, with, with, with tensorflow works and these gateways we feed the input data and the labeled data. So we can essentially, we can consider this, this echo to be a label, right? In a way it's, it's a mapping between two sequences, the sequence, the sequence mapping. Okay? And we're not using LSTM because we have plenty of time to talk about LSTM in future episodes. We're going to talk about art and video and music generation, and what's the secret to winning with chat bots and all that stuff's going to be using LSTM and we'll talk about that in detail.

Speaker 1:          00:19:33       But what we're doing right now is we're just going to generate these two placeholders. Okay? So we have these two placeholders and the next thing we're going to do is we're going to initialize our state, right? So this is, so this is the great part of this tutorial. This is what I'm really excited about. What we're going to do is we're going to show what really is happening and that states, right? It's not some magic, it's not just right. We're going to show how what is happening in the memory portion of Becket and state. So we're going to initialize a state because we feed in the state every time as well, right? That's the, that's the great thing about recurrent nets, right? We're not just feeding an input data every, every, uh, every iteration of training. We're also feeding it in the previous hidden state that was learnt.

Speaker 1:          00:20:15       So this is the gateway for that hidden states. So in recurrent net we have a placeholder, not just for the input data and its labels, but also the, the state that it learned beforehand. Right. So it's got a previous state and occurrence. Okay. So that's what our placeholder is for. What does the type, it's going to be just like the other two, it's going to be 32 bits and then we're going to, the shape of it is going to be the bad side and then the state size. Right. And we could find a street size has a number of neurons that happened at beforehand. Okay. So batch size and then state size. So we've defined these. Great.

Speaker 2:          00:20:52       Okay.

Speaker 1:          00:20:52       Okay. And then now of course we haven't talked to her. So let's see what we got here. D type. So you know, it's, it's good to see this error. It's happening in real time. Right? So for d type it's placeholder shake five, four, float, float, 32. What do we got here? What TF dot. Float 32 batch size, state size without placeholder values. And this one is, oh, so you know what this is that this is actually meant 32 because I think all right.

Speaker 2:          00:21:25       Oh,

Speaker 1:          00:21:27       I great. Okay, so that's, that's the course dates. Okay. So now what we're going to do is we're going to build our weights and biases for this network, right? We've got our place holders, we defined the gateway. It's the data is ready to be fed into the network. Now let's define those weights and biases and talk about what is happening. Uh,

Speaker 2:          00:21:48       okay.

Speaker 1:          00:21:49       Yes. So we're going to talk about what happens to the state. It's going to be dope. It's going to be sewed up because you never really see, see this pat, it's part time. So I'm really excited about that. But let's go ahead and, and, and, and, and define our weights. So the, so remember because it's a three layer network, we have two sets of weights, right? Those are the weights between the intraday, right? The input, the input from the input layer to them in state. And then from the hidden state to the Apple Apple Layer, right? So there were two weights. Let's define those two sets of weights. So the first one is going to be a w. Dot. We'll call it w and we're going to use the key, I've got variable for this. And why do you use, do you have that variable? Because it's that tensorflow primitive, right? And variables can be updated over time, right? Unlike tensorflow, constants, variables can be updated over time. And that's why we're using a variable. We're going to randomly initialized it. It's value, okay? And we're gonna use a state size plus one. And

Speaker 1:          00:22:43       okay, so this is, so this is the distribution of value. This can be random at random distribution of values. That's going to update. Now how, why do we initialize our weights? It's random, right? Why don't we just set it to zero? It just, it tends to, it we generally in the papers that get the best results, it's kind of become commonplace to initialize weights is as randomly, right? But there could be better, you know, ways of doing that in the future. But right now randomly initializing weights. It seems to work best so well now define the data type for this. We're just going to be a CSF flow 32 again. Okay, so that's it for our data type is going to be ETF 32

Speaker 2:          00:23:24       okay,

Speaker 1:          00:23:26       so that's, that's randomly in neutralizing our weights and then we're going to initialize our biases and so for our body sees why do we even have a bias? So biocese what they do is they improve convergence and we think about biases in terms of some typing and talking and teaching at the same time. This is dope. I'm really getting better at this. Guys like doing this every week. It's, it's really, it's just public speaking. Basically. I'm just giving a speech every week. What happens is a biocese, we're improving convergence. That's what it does. So we can think of biases in terms of when your regression, right, took me in linear regression biases. You can think of it as the y equals mx plus B equation, right? It's very simple for you about it and some plate linear model. Harder to think about any law. Nonlinear mom in a, in a nonlinear model, a nonlinear model. Why is he's still offering that constant anchor points such that it improves convergence. But it's not that, it's not that, uh, it's, it's, it's, it's not that it's a, it's a, it's a w a y intercept per se. It's just a, it's just, it's just not an anchor point. So that we, the value that we predict aren't too far off from, from what they should be. And, and we can talk about more about by biases in a second.

Speaker 3:          00:24:37       Okay.

Speaker 1:          00:24:38       All right. The delight can be quite long. So, so that's it for our, um, for our initial set of weights. So weights and biases one weeks and by teeth one and then wait and buy teeth too. Why is this one? And then weights and biases too, right? So weights to biases too. So, and then in second sense. Okay. And so the different between both of these in second sense and so demurrage between both of these is going to be the state size, not plus one, and then the number of classes. And then finally the Beta type. It's going to be set to flood 32 and then the number of classes, right? Because it's going to be one or two. Okay. Because the output and the reason he's going to be numb classes for this one is because we were going to output, uh, that set of classes. Right? Um, okay. Okay. No, no, I can slow down. Don't worry about it. I'll slow down. Okay. So that's that. Okay. We defined this. Okay. So now this is what, so we define our ways. We define our biases. It's a lot to remember. It's a lot to remember, but at the same time, just, just taking all this in of regally co continuously is good. Like there's a lot of things that we're learning even subconsciously that we don't even know. Okay? So it's good to even think, take these things in week, even if you don't fully understand it, you know, I the to start, you will slowly start to get it. Okay? So,

Speaker 1:          00:26:04       so we did, we defined those biases. Now what we're gonna do is we're going to look at this image, right? And so what we're, so we can think of this as, we're not so as batches, right? So we have a full matrix of binary values, and we're going to look at this in batches. We're gonna look at this in batches. So not all of it, and we're not going to feed it in all at once. And so that's what this dotted square over these values is. It's kind of like looking at the batches. Okay? So let's keep going. So,

Speaker 3:          00:26:36       okay,

Speaker 1:          00:26:37       now what we're going to do is we're going to unpack these columns, uh, and so, so we can feed it into our rank. Okay? So unpacking is for what we're going to, we're going to unpack these columns into, uh, a set of lists, and each of these lists are going to be fed it, right? So remember that matrix of ones and Zeros. We just want a one array, right? So at a time we just feed in one of those arrays out. It's one of the sequences at a time. Unpack a matrix into, uh, a one dimensional array. I mean, right? So, uh, so took stuff from inputs here and we're going to do that for both of our, both of our values. We're going to do it for arc, we're going to do it for our you put series, and then we're going to do it for output series. And we've defined those placeholders for each, right? And so it's gonna be on a single access. And what we can do, the end goal here is to map, uh, two sequences together, right? We have one team within a node. So you can, can we want to map them together? Like what is the mapping? Like what does that, what is it correlation, what is the relationship between these two sequences? And neural nets are universal. Approximators. We can find a mapping between Lauren, a function that maps between two different values or sequences. Okay. So we can find placeholders,

Speaker 1:          00:27:56       uh, for both of these. Uh, and uh,

Speaker 2:          00:28:01       okay.

Speaker 1:          00:28:01       Right? So batch y placeholder. Okay, great. And then you try it. Let me make sure I mean that right? Okay, great. So then backtracks not defined because I need to say a batch x is placeholder and here we go. So that was it for that. And then they said that access equals wine access equals one is the value for the unpacking of everything. Unpacking the values. What are you talking about here? Access as book one knots in negative one to one. And here we go. Let's see you crazy. You crazy. Python interpreter. Are you crazy?

Speaker 2:          00:28:41       Okay, we're talking about here.

Speaker 1:          00:28:47       Okay. So, um,

Speaker 2:          00:28:49       okay,

Speaker 1:          00:28:50       interesting. So what's happening here is the, does how you shaped,

Speaker 2:          00:28:58       let's see,

Speaker 1:          00:29:02       should I debug this guys or should I keep going? This is your call. I think I can continue to debug this fire. I also don't want to waste people's time. Okay. So what we're going to do now is we're going to think about, it's, so what is our input here? Debug. Excellent. He bought, keep going. Debug. People are okay. So deeper. Awesome. No debugging, man. People are split. I just got to make a call here. Keep going. You bug. It's helpful. This debug. Okay, so a 50, let's go. Let's, let's see. Buckets. So, so what did we get it? We gave it batch x and then batch y placeholder. Oh, in the, oh, so, um, so let's see. So these placeholder values could, could be updated, right? So,

Speaker 3:          00:29:51       okay.

Speaker 1:          00:29:52       Okay. So what are these placeholder values? So, um, for our placeholder values, it's update these

Speaker 2:          00:30:00       and then

Speaker 1:          00:30:03       let's see, do that again

Speaker 2:          00:30:06       again, again.

Speaker 1:          00:30:10       Great. Okay. Okay. So it was as place holders. What happened was for the placeholders, I um,

Speaker 3:          00:30:16       yeah,

Speaker 1:          00:30:17       I, I need you to define a type. Okay. So let's keep going. Do you blog and send nudes? Oh my God, we've got some. Okay, so here we go. So

Speaker 2:          00:30:27       yeah,

Speaker 1:          00:30:27       current back, split into cloud. So this is what it looks like, right? So we've split, uh, are two dimensional, up our matrix into feasible one dimensional arrays. That's what we've, that's what we've done. Okay. So

Speaker 1:          00:30:41       that's all you're done. So now what we're going to do is we're going to, let's see, what does the next day, now we're going to, okay, so this is the cool part. This is the part that I've been waiting for. This is the part, okay. This, this is the part right here, the forward pass. This is what we learned about memory and how memory is incorporated into a recurrence net. That will, we're about to do. Okay. So pay attention to this because this shit is awesome. So for our four passes, we're going to start off by defining art placeholders, right? So are place holders that we're going to update over time. Now, we could do this inside and of our computation graph, but we're going to do it beforehand. Now, I mean, we could just have a gigantic, uh, computation, right? But we're going to do this beforehand. So we'll define our current state that is the state of the Corinthian layer as the initial state that we defined beforehand. And it's about recurrent nets. So what we're going to do is we're going to track this series of states over time, and we're going to store it in this array. So it's not magic guys. It's not magic how we remember this, uh, hidden state. We store it in an in memory array. So let's go ahead and do this forward paths, right? So remember, whenever we're tray,

Speaker 1:          00:31:53       this is why we're current nets are all about, so what this is, is it a Ford Pass through the network? This is bright. We have a forward pass and then a backward pass. We, oh, we update our weight, right? So what we're going to have is our Ford Kaspersky. So for all of the inputs that we have that we defined beforehand, we're going to take the Caribbean quits. Okay? And we'll, we're, we're gonna, we're gonna, we're gonna take this input and we're gonna feed it through our network, right? And we define our weights and biases. We can just give it to that. So we're going to take the current influx and we're going to, first of all, we're going to reshape it into eight stock, into a shape that are recurring that accepts. So it's going to be, we're going to, we're going to reshape that currently, but into the back sides. Okay?

Speaker 3:          00:32:33       Okay.

Speaker 1:          00:32:33       Could you, the giving back side.

Speaker 2:          00:32:36       And

Speaker 1:          00:32:37       so this is the shape one. Okay? So this is the shape that we're going to give it to him. And then, uh, what we're going to do guys, we're going to be so many gaming videos later on. We just need to get through this course first. Okay? And

Speaker 1:          00:32:50       then what we're going to do is we're gonna mix both to include state and the, uh, implicated, right? So we're going to say input state and input and state concatenated with this variable gets it is it's going to be the concatenation, which is a computer science term for combining two values together. Great words. Use it in real life as well. We're going to combine both the current inputs or does that, is that current, uh, inputs, uh, which is the data and the current state. And so how do we combine these two values to combine these two values?

Speaker 1:          00:33:24       We're going to use the concat function, which basically combines two 10 shirts together. And so, and then what we do and then what we do okay, is we're going to perform, are, uh, four task, major qualifications to calculate our next stage. So what do we do? Well, first of all, we're going to perform to do this. We're going to perform a, let's talk about this. Let's talk before. So a nonlinearity, which is the Tan h function. But before we squash our data with our nonlinearities, we're going to perform our matrix multiplication. That is what is happening at every layer of, of a network, right? It is a series of Ma Matrix operations that are happening, whether it be multiplying those weights by the input data, whether it be pooling those values, whether it be convoluting them in a convolutional nets. It's all matrix multiplication. It's all linear Algebra, okay?

Speaker 1:          00:34:16       So we're going to perform matrix multiplication and tensorflow has this does, it's very easy for this with this matrix multiplied status and we're going to concatenate, uh, we're going to take that input and state value that's concatenated and we're going to perform that matrix multiplication with the weight, right? The initial weight. So that's, this is what's happening in the forward cast. At the first part of our recurring that this is what's happening in that first part. And we're going to add the bias, okay? And that's what's gonna give us our next stage. Now, what are we going to do with this state? Right? Well, we're going to take, we're going to take that matrix, multiply value, and we're going to squash it with a nonlinearity, which is what we always do, usually in a, in a, in a, not just a recurring that, but any kind of neural networks.

Speaker 1:          00:34:58       We have some matrix multiplication, then followed by a nonlinear. So this could be a Tan h function. It could be a sigmoid function, it could be, um, uh, you know, what else is it? Relu there's a lot of different options. In this case, we're gonna use a 10 age function, okay. Which is generally we see 10 eights use a lot in recurrent nets specifically. Okay? So, yeah, we've got a band, these spammers, so we're going to shoot. So sigmoids are great for generating output probabilities. So whether it's a convolutional net or Trenton, it generally we see sigmoid at the end, at the last convolutional block or the last, uh, Neulasta layer of a network to just output that, that value that's between zero and one that describes our output probability. And sigmoid takes a while to converge. Tan H is better for these? Uh, it's, it's faster.

Speaker 1:          00:35:46       Okay. Okay. So then, okay, so here's the cool part. So we define it speaks series, empty list at the beginning. Right now we're going to append it with the state that we calculate it. So we're going to append it with a next date value. We're going to keep doing this, right? So eventually this, this list is going to be filled with a sequence of, of learn hidden states. This is the memory aspect of work for neck. We are remembering every step through times we're going to, we're going to remember every hidden states values and it's not steady. It just remembers it and throws it into space. It stores them in a list, it stores them in memory and in memory data structure. And then once we've done that, we've set the current state to the next state. So as you recall from data structures and algorithms, this is very similar to um, you know, uh, a stack or a tree where we set the current to the next, right? It's, it's, it's kind of similar to that. It's very similar logic of, of continuously updating.

Speaker 2:          00:36:46       Okay.

Speaker 1:          00:36:48       So, so that's what that is. So that time forward pass guys, that's our forward pass. And what what is going to do is it's going to give us all of those hidden state and it's going to save them in memory. It's going to save them in memory. Okay. So and watched there be an error. Okay. Batches. Size is not defined online. Eight. Unfortunately I don't have one line numbers in, in the Pi Python notebooks, but uh, let's see. Uh, batches, size, batches, size, batches, size, what do you got your badge. So I'm going to do a five minute Q and a after this as well. Okay. So stick to this and we'll get the primary curious. So batches size is going to be, where do we have batch size?

Speaker 2:          00:37:34       Oh,

Speaker 1:          00:37:36       current input. Oh, right. Back side. Not that to size. Dog. State series is not defined states. Where are we? So that's where that append, it's called state series. Duh. Okay, great. Okay, so we defined that. Look at Chris' comment. Batches. Typo. Thank you. It should be the half size. It should be. I mean, I mean,

Speaker 1:          00:37:58       thank you. Great. So we've done that. And here's an image of what is happening, right? So this is a way of thinking about it, right? So we have, we're concatenating the current inputs with the current state die and the concatenation occurs to a matrix multiplication operation. And then we squash it with the nonlinearity. But before we squash it, we multiply it by that week though. Okay. So it's, it's a matrix multiplication and then we uh, squash it with the nonlinearity and we get that next state and we're storing all of those states through time. Okay. This is the state saved for all hidden layers or only for some, it is saved for all hidden layers. It is safe for all in layers now. Okay. Okay.

Speaker 1:          00:38:46       But it's, but, but not in this case. Not In this case. We could, but that would be very computationally expensive. Right to store. If our data was huge for them, it was huge. Storing all those hidden states would be very computationally expensive. So, and now I'm going to explain why we use that, that value, which was truncated that proper length. So the trunk he departments, we're only going to save portions of that in state, that sequence at a time and we're going to discard the old oldest values over time. Right? So it's similar to, so it's similar to the vanishing, remember has advantage in gradient problem, the error becomes less. Uh,

Speaker 3:          00:39:28       okay.

Speaker 1:          00:39:29       The error value is less, a bit is sent back over time. So

Speaker 3:          00:39:35       okay,

Speaker 1:          00:39:36       what we're going to do is we're going, we're not, we're not going to save all those states because these layers are not going to be enrolled at the beginning of time. We're going to truncate at a limited number of time steps. So it's only going a, we're going to do this. It didn't, it didn't happen yet. It will. They will continually do this. Okay.

Speaker 1:          00:39:56       Much love Ai Aficionados. Okay, so now we're going to do the last step and we're going to minimize the loss. Okay? So let's go ahead and we have our hidden states would calculate it. We've done our forward pass and now we're going to do are backward pass. Okay? So this is the learning step. This is the learning suddenly role. I always roll up my sleeves for the, for the fun parts. So, so let's calculate our loss first. So to calculate our loss, but when you'd say, look what we're going to define a variable called logics series. And inside of logic series we're going to say we're going to perform matrix multiplication between our state value and the next way down, right? So we did it for, so we've done board past, we've done half a forward ass and now has to do that second part of the four paths, right?

Speaker 1:          00:40:41       Actually we're not that, we're not fully done with four yet. We've done that first part right now. I'm going to do that last part until it gets the output and then we'll do our a backward pass. Okay. We are yet, we are absolutely doing backpropagation for this and I'll talk about that when we get to it. So this is our next step right there. Our second set of weights and art second set of, uh, so this is our second set of biases. So for every state we have in that states series of risks that we, that we calculated, we're going to Tuck it, calculate the legit serious. So when gets his sword for logistic transformed. Okay. And so what we're gonna do with this, these are the values that we are going to squash at the end. These are the outputs. This is the output values that we're going to squash at the very end, but they softmax and that's going to give us our predictions. Okay. So all of our logic, we're going to squash with an ending nonlinearities which is going to be a function. Okay. This is our ending nominee area and tensorflow has this great bill can softmax function for lodge? It's been logics series. Okay. So this is going to be our list of all of our predictions. Okay. Over time.

Speaker 2:          00:41:50       Okay.

Speaker 1:          00:41:53       We, yes, we always use backpropagation for training. Backpropagation is the way to train neural networks, except I've only seen one paper that didn't do it, that deepmind release, which was called synthetic gradients and they didn't really use it a more afterwards, which was crazy because as the time I was really excited. I was like, oh shit, back propagation is obsolete now. But like in their, uh, in their next papers, they didn't really use that again. So we'll see. So yeah, that propagation, we always use backpropagation for now until something better comes along. But okay. So let's now let's measure our losses. We have our predictions across time and now, now we're going to measure our losses. So for our losses,

Speaker 2:          00:42:34       okay,

Speaker 1:          00:42:35       we're going to use the sports softmax cross entropy with logics function. Now talk about long methods names, right? Anybody's used objective seat. You are right at home right now. We have a very long variable name here that I'd like to explain and we'll let you feed it. What it's going to do is going to calculate our losses using okay. And

Speaker 2:          00:42:57       okay.

Speaker 1:          00:42:58       Okay. So what do we got here for all of our largest labels for,

Speaker 2:          00:43:03       okay.

Speaker 1:          00:43:03       All of logic and is it in the zip?

Speaker 2:          00:43:09       Okay.

Speaker 1:          00:43:09       Roger [inaudible] series labels back complicate to update weights back, complicate the update. So I was actually asked to speak at this galvanize a conference and it was like the head as a speaker and this and they had all these like lead data, lead data scientist at Airbnb. We data scientists at square and they asked me to rap. So it's like, and it's a Roger Apps. So I was like, Oh, let me think about it. So we'll see how that goes. Maybe I'm busy right now. Okay. So, so here's what this line does.

Speaker 2:          00:43:36       He's what crystalline does. Okay.

Speaker 1:          00:43:39       So this is going to return a tensor. So actually what this does, it's going to read. So it's going to do exactly what we did before. And this line right here, it's going to compute the Softmax, uh, into, so we're actually doing this twice. It's going to compute the softmax, it's going to compute those probabilities for the logic and the labels. So for a sequence and the, for both of our sequences, right? And then it's been performed cross entropy on them. And so cross entropy measures the difference between two probability distributions. Okay? Is a method of measuring the difference between two probability distributions and that's going to, that's going to be our loss. That difference is what we want to minimize. We want to minimize the difference between our output probabilities and our sequence, our label sequences, right? I'm going to that Echo, right? We want to minimize it different. We're going to minimize it to an optimization function. Now whenever you think about neural networks, everything, it's about optimization. It's about measuring an error value and optimizing it. Everything is an optimization. Everything is optimization. And in fact in whites you start thinking about everything in terms of losses and optimizations, whether it's a relationship with somebody, like how do you, how do you minimize your losses? How do you, um, whatever you're doing, a sequence of steps in life. You know, you want to argue an optimizer, I'm getting a little off track. So back to this,

Speaker 3:          00:44:55       okay,

Speaker 1:          00:44:56       now we're going to minimize these losses. So this is going to calculate a list of losses. And what we're going to do is we're going to use the reduced mean function, what you were using the produce mean function too.

Speaker 3:          00:45:12       Okay.

Speaker 1:          00:45:13       Compute the average of those losses, okay? And you're one value, okay? That's our total loss that we've measured. Okay? After one, full forward propagation, this total loss is the, is the, is the output loss that we want to minimize, okay?

Speaker 3:          00:45:31       Okay.

Speaker 1:          00:45:32       Okay.

Speaker 3:          00:45:34       Okay.

Speaker 1:          00:45:34       The total loss. Now what we're going to do is we're going to do our optimization set. No trends or cloak has a bunch of built in, has a bunch of built in optimization steps. And what we're going to do is worth you add or Grad, no added Grad is a, is a really cool, it's really cool. Um,

Speaker 3:          00:45:55       yeah,

Speaker 1:          00:45:56       let me just type this up. Total blocks. Okay. So that's, that's the input that we get. So Adam Grant is a really cool, uh,

Speaker 1:          00:46:06       optimization technique. Okay? So it's very similar to use the castic gradient descent. So what happens is there's some features whenever, so here's why we use Adam gripes. Sometimes there are features that are very important, but for s for whatever reason or neural, that doesn't weigh them properly because it, because it uses the same learning rates across. So 0.3 by the way, is our learning right there. We're feeding this added right optimize. So for some reason it doesn't, the learning rate is the same across all of our features. So for some reason it doesn't weigh. Those are very important features properly because it's sparse. Sometimes sparse features can still be a very important so that, so that is one of the cruxes of neural networks. If feature is sparse, that means there's not a lot of data in a feature, but it's still important, then it won't recognize it properly.

Speaker 1:          00:46:51       So it added ride does is it uses different learning rates for those sparse out teachers. And what this does is it gives them more weight. It gives them more importance in training. Okay. And because we have binary data here, we're going to use out of Brad. Okay. And I have a great paper on that, which is in the, uh, in the comments of the link. Great paper notes on added Grad. It's actually very, it's, it's, it's not, it's not very simple. It's simpler than most optimization papers to read. So I cling to it. Definitely check that out after work. The Outer Brad is, uh, basically in a nutshell. It's a,

Speaker 3:          00:47:26       okay.

Speaker 1:          00:47:27       It gives voice to the little guy. It gives voice to the little guy because the learning rate is adaptable. Okay.

Speaker 2:          00:47:33       Okay.

Speaker 1:          00:47:33       Waitstaff receive high gradients, will have their effective learning rate reduced or weights that were too small or infrequent updates. We'll have their effective learning rate increased. Okay. So, but it not like a magic wand. Right? It doesn't always work, but it, right. So, so you, oh, we generally want to try out different optimizations, sex and having one line, right? We can just implement our optimization in one line. It makes it really easy to try out different optimization steps. Right? It's all about experimentation and trying out different options and seeing what works best. Okay. So that's it for our loss and then minimizing it. Okay. State series is not defined here because, okay. Because state series not states. Great. Okay. So then, so that's the, so that's that. Now this is the visualization stuff. So I'm actually not going to code out this visualization stuff because it's all, it's all, you know, it's, it's not machine learning. Okay? So I'll skip this part, but basically this is going to visualize what we see. Okay. And what is there to do? It's going to plot the training and the training output and the current prediction. It's going to plot out all three of those things. Okay?

Speaker 1:          00:48:43       So that's a visualization step. Now we're waiting to get to the training, the, the, the computation graph. Now we've defined the four tasks. We defined the backpropagation through that and you know, the loss function and in the back of obligation step, which is the, uh, the uh, Outer Brad, right? So that is the, that is the optimization step two back property. Now we're going to train the network, I'm going to use those values and we've calculated through the forward and backward pass to do this training step. Okay?

Speaker 1:          00:49:11       So in contraflow we always to find a section, right? 10 sections encapsulate our graph. I Ho thumb. So entrance. So sessions and capsulate our brand and we'll say session not run and what wouldn't. And so this is a really stupid step. I don't know why we have to do this. I mean, I don't see why it should just know that we all the variables, we've initial idea for a part of this graph. What else would we use them for it? So you know, not, not. So remember not all code, all code was written by people. Nothing is perfect. Nothing was sent by the gods, but so, right? So it just shows that everything is not perfect. Okay? So,

Speaker 2:          00:49:48       okay,

Speaker 1:          00:49:50       so we're going to run this session, right? Using the variables that we've initialized beforehand. Okay? So now we're going to say, let's define our plot. It's going to be an interactive mode. And then we're going to say, I'm listening to Sheila as a figure,

Speaker 1:          00:50:05       initialize that bigger value. And then we're going to say, show the graph, show the graph, and then we're going to say the loss decrease. We, we're going to map a lost over time so we can print them out, right? We want to show all the lost. I used a decreasing over time. So we'll initialize it as an empty list. Now we're going to go ahead and do art training, iteration set. So right, so we defined those detox, right? So the number of the talks that we had beforehand, and for each of those reports we're going to generate data to write. We define that generate, generate data function beforehand. And it's when the output, the sequences, right this secret, no one is going to be that initial binary sequence. And the other one is going to be that copy but echoes, right? So it shifted over. Okay. And then we're going to initialize our MP and it didn't state, so our current state is going, we knows it's going to be initialized as

Speaker 3:          00:50:58       okay.

Speaker 1:          00:50:58       Uh, an empty matrix. I'm going to use a num Pi zero function to initialize it and the state science. Okay, that's our hidden state. Now we're going to printout. That's okay. So now we're going to say for each that, so can each of those detox, we have a set of batches, right? And so he's, that Jews are going to be inside of a range for all of those batches. And so I think we defined five batches. So for five batches in the number of detox,

Speaker 3:          00:51:31       okay,

Speaker 1:          00:51:31       we're going to start refining the starting and ending point for the badges. Okay? So starting and ending point for those batches is going to use that truncated backstop length because we don't want to remember all of the hidden state, right? We only want to remember in states, um, you know the, the most recent couldn't say because why? Because it would be [inaudible]. I'm going to definitely have a Q and a right after. This is not an eight, not five minutes. Okay. Five more minutes before the two and a guys. Okay. Stick with figures. So for a truncated backup links.

Speaker 3:          00:52:02       Okay,

Speaker 1:          00:52:03       we're going to say, um, okay, so that's a starting point and an ending point. And then I want you to find the starting point in the ending point. And I would've got better. Dah, Dah, Dah. Okay. That's why, why and then starts and, okay. So these are our badges for our y. Okay. So I'm reading the comments as well. Okay. So, okay, so, um, so those are a bachelors. We defined our batches and now we're going to run the computation ground. So, uh, so if our total loss but the finer total loss and then our training staff and then our current stake. So remember we define these things beforehand. So right now what we're doing is we're going to just run what we had beforehand inside of the computation graph, leaving the session.run function. All right, so then the fine art, total loss or training scrap or current state. And it's going to get these values for each state, right? So for each iteration of our training predictions series, boom, boom, boom, boom, boom. And then repeat dexa feedbacks to the feedback is how we feed in those place holders that we defined beforehand. It does help once we define as place holders, we have to

Speaker 1:          00:53:58       thank you. So the fee days is how we define those placeholders and how we'd be in this place. Holders. Okay. So then in our batch x, okay. And then batch x. That's why I told her. Okay, we've got three more minutes guys before we get to the Q and a three more minutes. Okay. Stick with me here and it states, okay, so we are feeding in the whole all three batches, right? So all three placeholder values for the state, the input data and the output sequence. Right? Who did the echo sequence? This is what we do in recruiting that we don't just feed in the input data. We also did the, um,

Speaker 2:          00:54:51       so then,

Speaker 1:          00:54:53       okay, so we've got five more minutes here. So let me just, oh well turn in front of lots lists. We're going to, we're going to Upenn, the losses, every new blocks that we calculate, right? So this is that in lots of stuff we find beforehand and for steaks

Speaker 2:          00:55:10       and then,

Speaker 1:          00:55:11       right. So then for this part I can just, you know, he says it's not printing out values here.

Speaker 2:          00:55:19       Okay.

Speaker 1:          00:55:21       And then we'll just close the box.

Speaker 2:          00:55:26       Okay. Okay.

Speaker 1:          00:55:30       So let's go ahead and peat bog this. So for batch x in number of batches per batch, Idx in number of batches. Okay. So let's see what we got for batch IDX. Uh, there's no friend people time. Great. What else we got here? Unexpected him that and be debt in feed. Did you have an unexpected index or sorry. Okay. And it states, so for current states, so for in its state for currency online, 31 and it states equals current state. So that actually what this is his current underscore states. Correct. Underscored states. And then it states, oh, it's, it's underscore current states and it states, oh, and then we need a comma here.

Speaker 2:          00:56:23       Okay,

Speaker 1:          00:56:24       great.

Speaker 2:          00:56:27       Oh wait,

Speaker 1:          00:56:28       no name start is not defined in stark is not defined. We talking about what he's talking about on an IDX. I didn't find star, right. Start plus truncated folk batch ids. All right, so for nids start x, great.

Speaker 2:          00:56:57       Yeah.

Speaker 1:          00:56:58       What does next year we've got a lot of debugging. Oh my God. I'm like, okay. So

Speaker 2:          00:57:03       hold on baby. Sorry. Ids, we've got, hold on. Who did I hear

Speaker 1:          00:57:16       must be a value for placeholder and shoot five 15 you mo for. Okay, we've got three minutes left here. So,

Speaker 2:          00:57:24       so hold on,

Speaker 1:          00:57:27       let's visualize this. We've got to talk about the visual laser visualization. We've got to fit in the Q and a

Speaker 2:          00:57:33       last, just go for it. Yeah.

Speaker 1:          00:57:39       In plot is not defined plot I'm talking about applies not to find PLTL Ios.

Speaker 2:          00:57:45       Okay.

Speaker 1:          00:57:46       Ooh, lots is not defined. It is define, oh, plot is lexicon. So from slots.

Speaker 2:          00:57:56       Okay.

Speaker 1:          00:57:57       Oh yeah. Cause it's the visualizer. I didn't, I didn't compile that visualizer. So compile this baby up here and then over here and then do this. Okay guys, it's training. It's, it is training now. Okay. Placeholder. Okay. So it's training. Yes. Okay, good. Good. Now, uh, so actually I want to stop this training because we don't have, we're not, this is going to take awhile, so we're going to stop the training. But what happens is, okay, so actually, I mean this graph is kind of confusing here. So,

Speaker 1:          00:58:30       so, uh, it's, it's going to learn the, the mapping between both of those sequences. And so in this, in this, this is what's what it's going to output, right? Salsa training. But this is what the output is going to look like. Okay. So in this first graph on the top left over here, you see that the last function is minimizing over time. Okay? So that's, that's what that, that loss function is doing. And now it's these other graphs. You see that the blue represents the uh, input sequence, right? Cause then it's kind of a weird graph, a way of not, it's kind of a weird graph to look at, but essentially what's happening here is the blue represents at input sequence. The red represents the, uh, what is it? The red represents the echos, which is that I'll put sequence and the Greens are valued. Everyone at the echo and net is generating

Speaker 2:          00:59:13       [inaudible].

Speaker 1:          00:59:14       Okay. Okay. So that's what that that's doing. So essentially it is remembering the mapping, just ones and Zeros. It's remembering the mapping and buddy. And a bit is going to know that mapping so well and we don't need testing data because, uh, it's, it's going to be the same rights, an echo of the same as the input. So you don't really need testing. They're the point of this and is too worried about the memory and how memory was incorporated. Right. We did it by hand. Okay. Why are there spikes in the loss function?

Speaker 1:          00:59:42       Great question. There are spikes in the loss function. Why those spikes in the last one? No, it was because, uh, what was it? It was because we're starting, so we're starting on a new epoch and w because we're generating new data. So because the Matrix is reshaped, the first element in all of the rose is going to be adjacent to the last element in the previous row. So the first few elements and all the rows are going to have dependencies that are not included in the states. So the net is going to perform badly on the first batch. So it's going to go up and down until it gets to that ending point. Okay. Because of epochs essentially. Right? So let's, uh, stop screen sharing and then we'll do questions.

Speaker 3:          01:00:18       Okay.

Speaker 1:          01:00:18       Okay. Hi Guys. Okay, let's, uh, let's, uh, let's talk a bit. Let's ending five, many Qa. Hey, hey. Hey. What do you got here? Darshawn and Luciana. Okay.

Speaker 3:          01:00:32       So

Speaker 1:          01:00:37       why do you use the last state from the previous run? I think in this state for the current runs, because we are back propagating through time. It's hidden state string. A forward pass. Okay. Hi. Suraj for time series five years. Python in tensorflow. That's a good question. I mean who knows. I mean who knows. It could, you know, he couldn't run on binary silicon transistors. We've already surpassed human, um, human levels on a lot of state of the art problems already. So it could very well run on that. Okay, thanks Fernando. Have you heard about api.ai? I did a video for them a long time ago. Yeah. Now they are. They got bought out by Google, but I know people who work there. Uh, thank you alpha.

Speaker 1:          01:01:24       Yes, Kudos to the community. The chat is becoming more and more informative each session because, because the reason the chat is becoming better is because we are growing at 4,000 engineers a week. We have growing very fast. We are growing faster and faster. And part of it is you guys are my videos, the quality. But the other part is you've got, this field is growing so fast globally. We are all on a rocket ship. Okay. So this community is growing fast and if we define the culture of this community early, this culture of informing each other of helping each other. Okay. It's gonna do, it's going to propagate forward in all directions. Okay. We are growing so fast. We're going so fast.

Speaker 3:          01:02:06       Okay.

Speaker 1:          01:02:07       Thanks Paolo. Okay, so I'll take three more questions before we end this session. Session on cue learning Sangram. Great idea. I'm going to have one after this course is well, not know. Actually during this course I'll have a learning session during this course. Two more questions. Uh, I am behind the Korean. Okay. It's overnight. Thanks. She how, how, how impactful do you think an ml specific chip that can let people train networks and much faster without needing a centralized system? Like you mentioned earlier, how impactful guys? Uh, so training in general, there's a huge, huge space for opportunity around distributed training services. So if you can, you know, we all have a lot of computing power that just lies. It'll our laptops, our desktops. If someone can leverage this and create a distributed computing network where people get paid for

Speaker 1:          01:03:03       lending their computing power to tensorflow or whatever, it's going to make them a lot of money. It's going to make people a lot of money. And this, this is an example of one of those jobs of the future as automation Golfs, all Labor based jobs, we're going to need new jobs. And one of these jobs he signs out putting data is to let people use your computing power. As the, as a necessity of computing power grows over time, this is going to be a more and more important thing and it shows why we need that because AWS went down and a lot of services that we use went down with it. It shows the bad part of centralized systems. Two more questions actually. Hey Sir, I see and you explained how to do tension felt in distributed mode. Uh, yeah. Uh, yeah. Later on. Later on. Not right now. Can I narrow that change it's size or structure like hidden layers. Hold on. That question went away. Two more questions. Any plans on making videos related to Alexa play round? No. One more question.

Speaker 1:          01:04:04       How'd we get the optimized hyper parameter? How about RNN for learning hyper parameters too. So guys right now, so if you saw that interview video that really quick questions I did with dominate attention on the text flow team. He had the same idea and I had the same idea in a lot of people. We just all kind of have the same ideas that learning ideal, uh, hyper parameters and learning model architectures and learning, which monitor a model to use. Learning about learning is what's hot right now. We're all trying to figure out how to best do that. So that is what is everybody's kind of focused on right now learning to learn. So it's a great question. We don't know the answer. We have some strategies, you know, like rich search and random search, but there's a lot of space for improvement. A lot of discoveries can be made. Okay. So that's it for this session. Okay. And I'm going to go, but before I go, I should probably end this with a wrap because nobody asks for it. But I just want to do it. Somebody say a topic and they were going to go 32nd route because I wanted to do it.

Speaker 1:          01:05:01       Someone say a whatever subject I see first, I'm just going to go for it. Okay. Uh, rap. How do we use a model? We just built a use it. I mean you could use it with the one line function. It's, I'm learning a lot. Thank you. I thank you guys for being here. Okay. Can you get some info? Intuition. Backpropagation

Speaker 3:          01:05:20       yeah.

Speaker 1:          01:05:20       Well, one's the nose map to oes and one's inputs. Go in, add weights, get someone's past that shit to my sigmoid function. Get that era. What's real and prediction and that's why I use gritty yet descent. It gives direction and it doesn't pretend. Update weights and repeat 10,000 times. Outputs are lit. I'll be doing just fine online as I do it. Now. I do it like in the future because I'm a loser. Not really. I'm a winner. That's it on from the whole place. San Francis silly. Yo, stop rapping. I can't because I got a float. Don't be homophobic. Not. That was from anyway. Okay, so please don't. Okay. All right guys, that's it for this life session. Thanks for tuning in. We're going to keep going with this. Every single week. We are on a warpath to solve intelligence, and we will keep going every week and we will feel the learn. We're gonna learn the shit out of everything. That's going to be awesome. So stay tuned. We've got videos coming out. We're going to increase video output is gonna be awesome. Love you guys. And for now, I've got to minimize my losses, so thanks for watching.