Speaker 1:          00:00          Here we go. I'm about to start starting the stream. All right. Okay. Okay. Okay. Okay. Okay, here we go. Here we go. Here we go. Um, all right. Hello world. It's a Raj and welcome to my live stream. Um, in this livestream I'm going to build a machine learning Api, uh, and it's called a neural style transfer Api. And why? Talk about it if I can just show you. Okay. So here is the base image. Okay. It's this best laptop for machine learning image. And when I feed it into style transfer, it's going to become this. Okay? So it's going to do that. So what I want to do is just do another demo and just show you really quickly how this works. Hi everybody. It's good to see you. I have a little chat window here of everybody here. Hello. We're all blessings from Norway.

Speaker 1:          00:51          Wow. The Vikings, the vikings are alive and well. Oh Man, I'm so happy to see you guys. Uh, I wanted, I kind of like last minute decided to do a livestream because, uh, you know, I set up a live stream studio and uh, it's time. So I really want to do these weekly, right? So, so that, that's the goal. Okay, so down to business. So what we've got here is we've got this little app file and inside of this app fall, I have my, my thumbnail videos that I use for youtube. And so what I want to do is I'm going to, um, take that image and I'm going to send it to my API. So what I did was I created an API and so I can send that image there. So here's the, here's the console. Let me open up the console. We got people from all over the world here in Malaysia, India, Casa Blanca as a movie, Saudi Arabia, Canada, Nantucket, Nan Tuck it. That's what's up right there. I think that's like a, a jam as well. Nantucket jam sounds like it would be though, right? Like a peach jam. All right, so let me send this uh, image test. I remember I called it test three or jpeg. I'm sending it to, uh, the, hold on, it's not called test three is called test two. Dot. Jpeg.

Speaker 1:          02:06          Uh, who test two. Dot. Jpeg and okay, hold on. Gimme a second. Gimme a second here. Uh Oh no. Yes, I see. It's because it's not a Jpeg, right? It's gotta be, it's gotta be be a jpeg right now. It's an, it's a PNG de image files. We got Utah, Utah. What's up Utah? Okay, so test three now. Now let's do this again. Let's do this again. You Ready? Ready? So remember that, how to learn mathematics fast image I just showed you guys now. Okay, here we go. Here we go. I thought the data machine learning is happening in France, in France on a trained neural style transfer model is happening as we speak right now. And when it is done, we will see that output. Okay, so where is that output? Where is that output? So you saw that, right? That did not exist before and that is some machine learning right there.

Speaker 1:          03:09          I took my plain little image and I turned it into this style transferred image. Okay, so that's, that's it's tutorial for today's video. Okay. So I'm going to show you guys how to do that in the cloud. Okay. So you can build this code yourself. I don't care where you are, I don't care who you are. You will build this code and you will in four command line, uh, for commands on command line. You will send this to the cloud and create an Api just like that, that you can run in France on. You could curl the image, you could send a post request, you can build a service. That's how this goes. Okay, so we're going to start off with this. Okay. So also also before I started doing this, I'm just going to say 10 names just to, you know, done. Y'All fit tra polar bear at Lewis, RB Schick, Ali, David and Ben.

Speaker 1:          03:56          Thank you guys for being here. So let's get started. Okay, so for people who are watching this recorded, we wanna, I wanna I want to satisfy both the live and my recorded audience. So we have three steps to do this. The steps, step one is to build the model, of course, right? We've got to build the model and once we build the model, then we can train the model on Floyd hubs. I'm going to talk about a service called Floyd hub. It's not the best service. Okay? I just want to say that it's not the best, but it is the best for beginners just to get something quick and dirty. Up and running. Floyd hub is great. All right. If you want to start scaling really, really scaling to, you know, hundreds of thousands of users, then you want to move to AWS, then you want to move to Google cloud or even a zero.

Speaker 1:          04:35          And I'll have videos on each of those individual platforms coming out soon, either next week or the week after that. Okay? So the third step is to serve the model via an API. So if you're not familiar with neural style transfer, here's how it goes, right? You have an image, right? It could be me, it could be you. And then you have a source image and that's the style that you want to transfer to your base image. And when you do that, boom you get, you get this filter. And this paper came out I think four years ago, four years ago now, and it was called neural style transfer. It was by gat teas. That was the name of the researcher. And there had been a lot of variations to that, to that original paper. But really that original paper was the, the landmark. It's, it's the one that started it all.

Speaker 1:          05:19          Okay. We've got someone saying, make South America great again. So we got some really hilarious people in this chat room. Okay. So you guys are so hilarious by the way. Uh, I forgot how funny you guys are. Okay. So how are we going to do this? Well, it turns out that we, all we need to do is use one neural network. Okay? It's, it's a convolutional neural network. We're going to build this thing in python in tensorflow, and that's all we really need. So what happens when we have a convolutional neural network is what it is, is it's a series of matrix operations. It's a series of matrix operations. We have our input data that is an image. And an image is just a matrix, right? It's just numbers, pixel values between zero and two 55 that say what color every pixel should be. And we take that input, um, matrix, and we continuously apply operations to it.

Speaker 1:          06:05          Multiplication, right? The dot product. And in Linear Algebra terms, we're just multiplying a matrix by the Matrix, by the matrices and the weights of the network over and over and over and over again. And we might add some other little tweaks to that, but it's just a series of operations. And you might be thinking, well, that's too hard. I don't understand that. Well, let me stop you there. It's not too hard. First of all, stop telling yourself that it's all about your belief that it can. It's like athletes, right? So if, if you believe that you can become, you know, like wolverine level jacked status, you will do that. If you believe that you can learn something, then you will learn it. You got to believe, right? So again, I am not a phd, but do I feel very comfortable with this stuff enough to go live in front of a live audience on the Internet with my name on the line? Yes, I do. So, and why is that? Because I believed that I could learn it even though I'm not a phd. And guess what? I did learn it. So, so take that as an example. Okay. So,

Speaker 1:          07:02          uh, right, so convolutional networks, they're just a series of matrix operations apply to input data until we get an output. And the output is a prediction, right? And so, right, this is used for image classification, but what the researchers have neural style transfer thought were they, they said, you know what, let's use those intermediary layers and and use those to trans to transfer the style of one image onto another. Let's not care about the outputs of the network, right? So just cut out that, uh, the, the last part of the network and just care about what's, what's happening in the middle right here, right? So these colored images are actually just matrix values. These are matrix values and they have numbers and through optimization, through the learning process, those neuro, those numbers become better and better at classifying the image, right? Through a training process, which we don't actually care about right now. What we care about are these learned layers because we can use those learned layers and apply what they've learned to our image, right? So that's what we're going to focus on. We're going to use these layers to then create our style transfer image. And so how do, um, how does that work?

Speaker 1:          08:16          So here's how it works. So there are three parts of this workflow, right? So we have a content extractor, we have a style extractor, and then we have, um, the combination of both, like, so we put them together for now I'm going to talk about math. Okay. So get ready for this. I'm going to talk about math in a second. So, uh, this is the fun part, right? So, uh, so the first part is the content extractor, right? So we have our image and what we want. So, so there are different ways that we can think about this problem. And so what the researchers did was they said, okay, we have an image. And that image has some content in it. That means it has a human, you know, objects inside of the image. It's got a sunset, it's got a mountain, whatever it's got in terms of, um, objects though.

Speaker 1:          08:58          That's the content. And we want a way to preserve that content and, and, and model that content and represent that content mathematically. So then we can, we can later, um, utilize, that's right. So what they did was they used a very famous convolutional network called BGG 19. This was a convolutional network that was trained on hundreds of thousands of images of all sorts of categories from boats to planes to dogs, whatever. Okay. So they took this pretrained network. Okay. And this is what it looks like. A convolutional network. We have a convolution, which is a matrix operation, a pooling, a pooling operation, which is another operation. You know, I have a great, I don't, I don't want to repeat what I've repeated before. If you want to know how a convolutional network in general works, search convolutional network Saroj on youtube first link. It's an amazing explanation if I do say so myself.

Speaker 1:          09:54          Uh, but so that's how it works. But what, what we care about are these, are these layers. Okay. So this is actually just one layer, right? So, but what a convolutional network is, it's multiple layers. So we care about this, this layer, you know a few of these layers right here and this is pulling. So, so if we were to visualize, and by the way guys, the code is going to be in the get hub, uh, in the video description that get helped code by the way. So if so, follow along with me here. So if we were, if we were to visualize what these layers have learned, it's going to look like this, right? So it's going to be images. I mean, it's going to be edges and then it's going to get increasingly more abstract over time. So edges will become eyes and eyes will become faces and faces will become humans as the layer the network progresses. So what they did was they represented content as this formula. Get ready now? Now you got me excited because now I'm talking about math by the way,

Speaker 2:          10:53          okay?

Speaker 1:          10:55          Okay, so

Speaker 2:          10:58          [inaudible]

Speaker 1:          11:01          you guys are so funny. Oh my God, you guys said the most random stuff. Okay, so content, right? So content, how did they represent content? Like what is actually in the image in terms of objects. So what they did was they passed both images through the network, right? So you have your base image, what you want to stylize, and then you have the, the, the style image or the image, the art, let's just call it artistic image. And so they pass both through the network. And so what they did was they found and they took a particular layer, let's call it layer to layer two of of five, but it can be any layer really. Um, and it took layer two or five and that's, that's um, that's a matrix of values, right? Is it to learn matrix of values.

Speaker 2:          11:43          Okay.

Speaker 1:          11:45          And once we do that, uh, we can then that's gonna that's gonna give us, um, a scalar value, okay. That's going to give us a scalar, a single value. And what we can do is we can find the difference between those values, right? So we pass in one image and it's going to learn a filter. It's going to learn a filter for that. We'll pass it on. Another image is going to learn a filter for that. We can basically subtract those two filters together and we can get the, and then we can square that. And we can do that for all of those, um, images. We'll sum them all up and then, uh,

Speaker 2:          12:17          okay,

Speaker 1:          12:18          we're finding their difference and then we're squaring it. Okay? So that's how that works. And that's what the sigma notation meets for all of those images. Okay. So, um, we're also multiplying each of the represent representations by the value Alpha. That's right. That's right. The value Alpha. Um, but we have those two features. That's what I'm trying to say here. We have to learn to matrices. We're finding the difference between them and that is our content loss. So, right. So last functions, I've got such an amazing loss function video coming out this weekend. I cannot wait to show you guys this. But last functions are all about measuring how bad our network is that making a prediction, right? So what we want is we want a loss function. That's not going to measure how bad we are at classifying dogs. We want a loss function that measures how bad we are at creating a stylized image. And so if you think about it, that's a very abstract, uh, at first thought like how good a filter is. But by and representing it mathematically this way, we have found a way to

Speaker 1:          13:23          thrives 20, 20, I mean guys, one thing at a time, right? I got to take over this, uh, youtube game first and then we'll talk about that. I'm not, I'm not saying anything anyway. You guys are so funny. Oh my God, I need to like close this chat window because, um, okay. Anyway, no, I like it though. I can do both. So, so that's content, right? We have, we have the content and now we're, the next thing we're gonna do is we're going to strike the style. So, so content is just, think of it as a single value, right? So all of this comes down to a single value that represents, um, how,

Speaker 2:          13:56          okay,

Speaker 1:          13:57          how, um,

Speaker 2:          13:59          yeah,

Speaker 1:          13:59          how much content has been transferred to our base image. It's like how different our base images from our content image. And what we want to do is we want to minimize that difference. And when we minimize that difference, our base image will become more and more and more like that style image in terms of content. But content is not enough. There's also style, right? So for style,

Speaker 2:          14:21          okay,

Speaker 1:          14:22          for style, we're going to do the same thing except,

Speaker 2:          14:26          okay.

Speaker 1:          14:26          We don't want, we don't care about the content. We care about the, uh, the texture of the image, right? The colors, the what, what is where, right? So if we think about like a filter in general, there's not really images that are being overlaid. They're just like these textures and colors, right? So there's this kind of gradient, not, not in the sense of the math word. It can be very careful about words here. But, um, how do they extract the, well, what one thing to think would be, well, let's just directly do the same thing, right? So input the style image and find a difference. Right? Between them. But what the instead what they did was they said let's compute what's called a gram matrix. And what a gram matrix is. It is, it is the, it is the transpose of the Matrix, right? So a grant matrix is the multiple, if you take a matrix and you multiply it by the transposed, so you flip it of itself, that's the grand matrix.

Speaker 1:          15:20          And so they took those grand matrices for both the base image and the style image, the artistic image, and they found the difference between them. So that's what this, so this calculates the gram matrix then, uh, once we do that, we can find the difference between them and we multiply by this out by this value Beta. So there are two like threshold, not threshold, but static values. Here we have Alpha, which is for the content. We have Beta, which is for the style, and we can tweak those to see how stylized our end result will be. So that gives us our loss for, for saying how stylized we want our image to be. So what we want to do is we want to combine both of those. And the way that we combine both of those is by creating this final equation here. So we just, we just add them up.

Speaker 1:          16:06          That's it. We add the content loss and we add the style loss and that's our total loss variation loss, you can call it. So our loss function consist of other loss functions and the reason we added it is because we just want a single, we want a single end to end optimization approach where we can just optimize the entire thing. We, we compute the error using this loss function and then we use the air to compute the partial derivative with respect to the weights of our network. Okay. And if you want to understand how backpropagation works, again search backpropagation Saroj on youtube and like a million links will pop up there as well. Literally anything AI related, just search the name of what you're thinking and then the word Saroj and I promise you it's going to show up on youtube 98% of the time.

Speaker 1:          16:55          Okay. So all right. So, and the way they optimize it was using El bfgs and this, this is a second order optimization scheme and I've got a great video on that as well. Just search second order optimization by Saroj on Youtube. And we talk about how instead of just competing the derivative, we compute the derivative of the derivative and in some cases it's called Newton's method and another in another case it's called El bfgs. Um, but it is a gradient based optimization ski. Okay, so we're going to build this now we want to make this into an API. So let's get to the code, shall we? So let's, let's just build a simple model here. Okay. So what we first want to do is we want to import Vgg, right? So VGG is that convolutional network. We also want to implement, we will also want to import tensorflow. We've got people doing the hundred days of ml code challenge, by the way. Amazing stuff. Hi. I am so proud of the people of the wizards who are, who are doing that. It's just a, it's just amazing mind blowing. So VGG is our network. Tensorflow is our tensorflow, is my city. No, no. Tensor flow is our machine learning library. And then we're going to import, transform.

Speaker 1:          18:14          We're going to import, transform to then transform our images as we, as we do want to modify them a yes, I know coding train. I love Dan. We actually collaborated with Dan and I'll probably will collaborate with Dan again. He's a great guy and I promoted him and he's promoted me. And you know, in a way we're kind of all like the Kardashians of machine learning shift. Mid me three blue one brown, uh, this evening. Uh, Carrie, Carrie Huang is coming over. Um, he doesn't know that I'm live streaming right now, but he's coming in a few hours. So I mean, we all, we're all like, you know, cross promoting each other. And you know, what I really want to do is I want to make a curriculum that involves all of them. And you know, we all have our, you know, we all have our distribution channels or youtube channels, but we're all pushing content towards the same curriculum.

Speaker 1:          18:57          Can you imagine the anime cross over level? You know, if I'm like sitting here building convolutional network, Daniel Shiffman comes in and he is like adding in his like p five dot. JS javascript stuff for like front end stuff. You know, Carrie comes in, he's hilarious. Uh, Andrew Trust comes in and he like explains things so beautifully. Jabrill's comes in of course, and he liked, makes a game out of it. Drills is all is awesome as well. We collabed as well. Uh, how cool would that be? We guys, we need to make this happen. I mean, it's gonna happen. So, um, we'll see. We'll see. So one step at a time, basically the Kardashians as, okay. So, uh, let's, let's get to this before we run out of time here. So, uh, from utils import get image. Okay. So now what we really want is a couple of those layers. So we're going to specify what those layers are going to be. So let's just call these layers by their names, which they were named by the VGG authors. Uh, and once we do that Relu to renew three relu that Oda really, really relieved by the way his stance were rectified linear unit, which is a type of activation function that diminishes the vanishing gradient problem we look for. Okay. So we've got four layers here that we're going to begin to think about when it comes to style. And then when it comes to content, we just have one layer. We're going to call relu four two. Okay. And so now that we have that, let's,

Speaker 1:          20:31          let's now say, let's optimize. So in the optimize function here, we want to perform optimization. Okay? So what I'm gonna do is going to say inside of our TF graphs, so we want, we want to initialize our graph and we wants to take VGG an existing network and extract those features from it, right? So, so how do we do that? How do we actually extract features from, from Vgg in? So I'm going to show you how we do that. Um, as default. If that device is going to be our device, right? How's device? Yes, Youtube is my main job. It is my main job. That's the way I like it. Okay. Uh, okay. So with that, we're going to compute this style image first. Um, and that's going to be TF dot placeholder. So we want a place holder for that style image, right? So the initial, let me make this bigger. Let me make this bigger, bigger, bigger, bigger is always better. Not Always, not always.

Speaker 1:          22:00          Okay. Style image is going to be, we're going to use a place holder. So what the placeholder is in the, in, in the Tantra flow computation graph is it is the gateway into the network, right? So this is where data flows in. And so that's why we're creating a place holder here because we want a way to input the style image, right? So we're gonna do that for the style image and we want to do that for the, uh, the base image. Okay? So our placeholder is going to be of tight float 32. It's a very simple image. We have a predefined shape that we want to give it. My sound and image are not in sync. Okay? Well that we got to fix that. Really? Okay. Hold on. Let me go

Speaker 1:          22:57          check that out. Really, my sound and image are not in sync. Seriously. Oh my God. See, this is what happens when a life stream alone. I'm like trying to set up this, uh, this whole thing. Okay. Uh, okay. So what I want to do actually is just, okay, so next time I'm going to be better about this, but let's find that since it's syncing for you. Right. Great. Let's look at this code. So let me just show you this code guys. Okay. So we have our style image. Okay. So, and then we have our base image here. We have our network vgg.net we feed both in as parameters that gives us our network. Okay. So once we have that, we take those layers and we compute the gram matrices of each of those layers. Okay. We compute the gram matrices of each of those using them and then using the matrix multiply featured, right? Remember the gram matrix is the transpose of a matrix multiplied by itself, which is exactly what this line is. This is the grand matrix. We are feeding all of those. Um,

Speaker 2:          24:03          okay.

Speaker 1:          24:03          We are feeding and I'll wrap at the end. By the way, do not leave and I will wrap. If you leave, you don't get to hear the rap, I'll probably make a fool of myself. But that's just how it goes, right? Whatever it takes, right? Ai Solve Ai or die trying. So Graham Matrix, we're storing all of those inside of this array style features, okay? So that that was us computing the style, a loss in the form of Graham matress cs for all of those layers that we defined beforehand, right? So that was that. And so once we do that, then then and only then we can say, okay, inside of our TF graph, let's compute the content preacher. So we already did this for style and now we're doing the same thing for content. So we already defined a content layer, right? And remember there was no Graham Matrix involved in the content. So you don't see that here we are directly pulling the learn to value for the content just like this. Whereas for the gram matrix, we had to compute that, right? Using TF or NP numb Pi's Matrix multiply function. And so once we do that, then we can combine both of them. So this right here is just, um, this is like a,

Speaker 1:          25:18          this is like, uh, an exception check, right? If it's, if it's slow, then, uh, set the image back to what it was before because it's taking too long so we can ignore that part. So, um, now for our content loss, we already found, uh, the, the value from that specific layer. And this is us computing it by multiplying it by the weight. And so if you look at this, you're thinking, wait a second. Uh,

Speaker 2:          25:45          yeah.

Speaker 1:          25:46          What you're thinking like, wait a second, where is that? Where is that? [inaudible] right? So here it is. This is, this is exactly what we're doing. See? So this equation is what we are looking at right here.

Speaker 2:          26:05          Okay.

Speaker 1:          26:05          Where is it? Where is it? It is, it is the difference between both of those, uh, features divided by the size of the content and multiplied by the weight. And that gives us the content loss. Okay. So now that we have all of those style layers,

Speaker 2:          26:27          yeah.

Speaker 1:          26:28          Now that we have those layers, we can, uh, compute the style was, so we had the style layer values and now we're competing the style lost values until it comes out to this. So what is happening here? Let's talk about, we have our, the size, the height by the width, by the number of filters. So it's a three dimensional tensor size. We have our features. So what we're doing is we're reshaping what we just computed. Uh, no, not this one. This is for each of the layers. So these are the features for the layers. We have our grand major season, that's our style. Grand matrices. We append to the law says the difference between those grand matrices and that gives us our style losses. And lastly, we use that. We use that value multiplied by the way, and that is our style loss. Now the word variation here just means, uh, this total loss. So I said the total variation loss. And so in order to compute the total variation loss, we add both the style loss and the content lost together. And that's going to give us our total loss. Okay. So this right here is this preprocessing step. That's not, it's not really necessary, but they went ahead and did it anyway. Um, it's just kind of a,

Speaker 1:          27:45          it's a nice to have. It's, it's a Gotcha. That's the thing about a lot of this research code. I mean, this is research code. Like it's Kinda, it's Kinda messy. It's, you know, it's, it's not good. One of the most impactful things you can do, by the way, if you're interested in Ai, which I love you if you are, which you are. So I love you. If you're watching this, uh, one of the most impactful things you can do is creating clean, well documented, readable code. All right? For the ml community as, as a whole, right? This is, this is what beginners need better code. And, um, carrots would be a great wrapper for this and there is a karass wrapper for this. Um, but, um, right? And so this is the training loop, train, train, train, et cetera. Okay. So now let's get to the part that I've been talking about the API part, right? So how did I do this? So if you look at what I've done here, if you look at what I've done here, I have made an API using Floyd hubs. So let's, let's talk about Floyd huff for a second. Okay. So Floyd hub is not the best.

Speaker 2:          28:52          Okay?

Speaker 1:          28:53          Oh my God, it's Lloyd. All right. Fully hobbies, not necessarily the best GPU, um, provider on the web. But I'll tell you what it is, is it's really easy for beginners to get started. Okay? So I've got a couple of jobs here that I've been running. Um, and so when it comes down to it for Floyd hub, you literally can say, here is my, here is my, uh, repository. So I've got a repository here and right? So let me open this. Here's my repository. Let me, let me look at this. And Sublime. Sublime, okay. Evaluate,

Speaker 2:          29:33          okay,

Speaker 1:          29:34          you've got to have this. So in this Floyd requirements that txt, you just add all of your dependencies, just like list them, and it's going to install all of dependencies in your virtual environment, in the cloud for you. So this is gonna be tentraflow can we care? Os It'd be whatever you want and it will install that for you and your virtual environment. So what you can do is once you, once you find your repository, you can run the command Floyd in it, right? So Floyd and knit fast style transfer. And what that does is it initializes a new project in your current directory and then you can serve, well, first of all, you can even train that model. So this is a pre trained model that we can serve directly from our

Speaker 2:          30:11          hold on,

Speaker 1:          30:15          old on. We can run that directly from our command line like that. And so what this is now doing is it's taking our code, it's,

Speaker 2:          30:26          yeah,

Speaker 1:          30:26          putting it on the web. And so, oh, I've reached your Max number of jobs. I've been running a lot of jobs on this thing. And so once we do that, then we can simply, I got some people, okay. So then we can simply just run a simple API call using this line of code just like this, and then it's going to return back, whatever that, whatever that output images like I just like I showed you before. Okay. Um, that is super simple. And if we go to the, the code that I have for you in the get hub description, if we go to that, check it out. Amazing instructions for you. Here's how to train it.

Speaker 2:          31:16          Yeah.

Speaker 1:          31:16          Or here's how to train on Floyd hub one line of code and you're training your model on this, a cloud service. Here's how to evaluate it, right? You trained it, okay, here's, here's, here's how you rent in France. Okay. You can do the same thing for video. And really this applies to any kind of model. And how do you even do this? Well, you got to make a Floyd hub account, right? Let me do this with you. Okay, let's do this together. Let's make a [inaudible] hub account sign up for free. Um,

Speaker 2:          31:44          okay.

Speaker 1:          31:46          It's raw. It's raw edge, okay. No credit cards need to care. And no, they didn't sponsor this. They did not sponsor this video. I just personally like it. Okay. Just like that. And so I dunno how it's already, oh, it's already got projects for me, so we can just look at those if we wanted to. And so inside of this work, it's even got the instructions, like how do we do well, we just initialize it here. And so once we initialize it locally, we can add some code here and then we can run it on a Gpu. So Floyd, Ron Gpu, and then it's gonna run on a GPU in the cloud. So I want to say, and so once we do that, we can serve it as well from, from that API. We can, we can create an API around this. So you might be thinking, well, w w there must be machine learning APIs out there. So clarify as one example, right? So clarify does have, um,

Speaker 1:          32:46          does have some models that are trained for you pre trained in the cloud and they're pretty good, but they're doing way too much, right? So someone can clearly, uh, pick one of these and do them really well. He's the, one of the newer models, I'm talking about generative adversarial networks. I'm talking about, um, I'm talking about variational auto encoders. Um, you know, one of the more advanced models that you know, people don't know about so much and you know, clarify as a great example of a, of a, of a group of people who create a really sustainable business around just Ai. So think about it, just having one single API call as a service, you can do that. You can make so much money, you can make so much money doing that. And um, you know, just just yesterday I had a farmer in Belgium who, who emailed me a picture of his crop and he was asking if I can help him with classifying. That's right. So there's just so much potential out there to create really impactful businesses for the world. Uh, clarifies one example. I just want to show you that really quickly. Okay. So, uh, that's it for this live stream. Let me answer some questions at the, at the end and I'm going to wrap. So guys, go ahead and ask them questions and then once you do that, uh, I will.

Speaker 2:          34:01          Okay.

Speaker 1:          34:02          I will, I will end this live stream after our answer your questions. Okay. So while I look for an instrumental to play rap instrumental,

Speaker 2:          34:18          any questions guys?

Speaker 1:          34:21          What are the questions here? How to become confident with large code bases? Great question. So the best thing to do is to contact the author of the code base, right? So they are the architect and they know what parts are best for beginners. So don't just try to dive in yourself, contact the author, um, look at the documentation. Ideally, ideally it's got documentation, but if not, contact the author and uh, they will help you. They will guide you. Don't just try to do it yourself. I mean you can, but uh, it would take longer. So you want to go the fastest way to do this. Um, what bit are you thinking of making next? Um, I want videos on GPU, s specifically GPU providers. Should I go for AI or ml? Which one before? Guys, AI is the biggest bubble machine. One again, is inside and deepening its inside, but she wanted, gives a subset of Ai. Do you think Java's triple over Tech Python soon in ml? I don't think it's going to overtake it, but I think it will catch up for sure. It is catching up. Is machine learning very impro if very important. Of course it is my niece. Thank you. And two more questions. Two more questions.

Speaker 1:          35:30          Is Floyd costly when you said train and test? Um, yes. So, so that's why I'm saying it's only for beginners, right? Because once you want to start scaling, Dan's going to get expensive. And, and I think when it comes to scaling, you want them the cheapest solution that is also the most dependable. And for that I would choose either AWS cloud or Azure. But to start off with, you know, there's, there's a very small learning curve for Floyd hub. That's why you should start with that. Um,

Speaker 2:          36:02          okay.

Speaker 1:          36:03          How does start with deep learning? Uh, watch my intro to deep learning, sir. Playlist on Youtube. Juris law. Um, it's because that's my signature. And a William, last question. What ceiling would you recommend for tensorflow? I would recommend python. Okay. So now I'm going to rap about some topics where someone just say a topic, one word go

Speaker 3:          36:24          Friday everybody. It's Friday. How it kind of a hard beat. Here we go. All right, so I'm about to wrap NASA a freestyle. Someone's going to say a topic right now. Here we go. Where's the, where's the, where's the topic guys? Where's the topics? I think these guys are a Gnn that's not really like a topic. Awesome. Awesome.

Speaker 2:          37:00          Okay.

Speaker 1:          37:01          Hey Yo, are we some Isaac Asimov? Don't try to look at me. Don't try to Piss me off. I try to read Psi by him. My books in mom movies in my looks try to dress that way. I got hair is so wavy. Okay. I level up. I leveled down NLP man. I'm rapping like a cloud. You guys are laughing in the chat but it's okay cause I got big coin any way. It's all good. Hey, hey. Hey. All right. Oh Wow. Actually I sleep with her. Alright guys, so thanks for tuning in to this live stream. I've got more livestreams coming up for you. I love you guys. Thank you for watching. I've got so much AI machine learning content coming up. So if you haven't subscribed, hit the subscribe button and, um, yeah. Thanks guys

Speaker 2:          37:46          for watching. Okay.