Speaker 1:          00:00          Hello world. It's a Raj and Kenon AI really create another Ai. Of course it can. Neuro evolution is a technique that involves optimizing neural networks using evolutionary algorithms. We'll cover some example demos involving neuro evolutionary algorithms learning in a simulated environment. In this video, mastering the art of deep learning takes lots of motivation and countless hours of productive work, but at the same time the need for deep learning experts keeps increasing more and more. Because of this, there is this gap in the supply and demand equilibrium which is unlikely to go away anytime soon. To solve this, companies are releasing systems that automate the creation of AI models. One example of this is auto ml by Google. This is a cloud offering that lets developers, including those with no ml expertise build custom image recognition models. This will likely be expanded to include other basic and l building blocks as well like speech translation, video and natural language recognition.

Speaker 1:          01:11          This entire pipeline from importing data to tagging it to architecting the model to training is all done through a drag and drop interface. Drop it like a boss. It's doing this using a parent child network architecture. A parent neural network will propose a child model architecture randomly, which is then trained and evaluated for quality on some task. That feedback is then used to inform the parent how to improve its next generated child architecture. This process is repeated thousands of times every time generating new architectures, testing them, and giving that feedback to the parent to learn from the gradient update acts as feedback and eventually the parent learns to assign high probability to areas of architecture space that achieved the best accuracy for a given dataset. Another more recent example is by Uber on neuro evolution. Uber Ai Research released a set of five papers focused on this technique.

Speaker 1:          02:17          The researchers state that genetic algorithms are an effective method to train deep neural networks for reinforcement learning problems and that they outperform traditional reinforcement learning approaches. In some domains, evolutionary methods have a long history, but they are on the rise. This continues the trend of resurrecting decade old ideas, applying them to real world problems using modern hardware and getting strong results. This was the case for convolutional networks, recurrent networks and reinforcement learning in general. In fact, the whole history of deep learning is full of reinvention and resurrection. Backpropagation, for example, was reinvented several times. Machine learning is an optimization problem, whether the task at hand is classification, regression or reinforcement learning. The objective is almost always to find a function that maps input data to output data. A data scientist will try and infer the parameters and hyper parameters of their model from the training data and verify with the test data.

Speaker 1:          03:29          Whether the approximated function performs well on an unseen Dataset. The core problem is finding the right hyper parameter settings that result in the lowest loss or the highest reward. The traditional way to do this is using gradient descent, which is called back propagation. In the context of neural networks, let's say our network has two different parameters. This is just so we can visualize the optimizations surface. Usually there's a giant number of parameters, so it's impossible to visualize the graph of all the values of both parameters and all the possible error values. Looks like a very rough terrain with hills and valleys like East Texas. We want to find the point at the bottom of the steepest valley. The minimum, the gradient we compute and every iteration tells us which direction to search to find that minimum. Think of it like rolling a ball around this surface till we find that minimum.

Speaker 1:          04:26          That's currently the most popular way to optimize a network. Neuro evolution, genetic algorithms, evolutionary strategies. These techniques, however, all revolve around the concept of genetic evolution. If we're doing genetic optimization in the context of deep neural networks, we'd start with an initial population of models. Usually a model is randomly initialized and several offspring are derived based on this initial model. In the case of deep neural networks, we initialize a model, add small random vectors, sampled from a simple golf and distribution to the perimeters. This results in a cloud of models, which all resides somewhere on the optimization surface. This is an important distinction from gradient descent. We Start and continue to work with the population of models instead of a single model. So starting from this original population, the genetic optimization cycle begins first. A fitness evaluation is performed. This corresponds with checking where the models are in the optimization surface and determining which of the models perform best, meaning which are the most fit by some measure.

Speaker 1:          05:39          So muddles will perform better than others, and that's because of the way their parameters have been initialized. Next, a selection is performed based on the fitness evaluation. In evolution strategies, the offspring is reduced to a single model weighted by the fitness evaluation for deep neural networks. The fitness is defined as the loss or the reward. So we're basically moving around on the optimization surface and using the offspring to point in the right direction. This is another big difference from gradient descent. Instead of computing the gradients, we're sending out multiple antennas and moving in the direction that looks best. It's kind of like a structured random search. The end result of this selection phase is that we have a single model. The next step is reproduction Tki. The same process as in the initial phase is repeated based on the newly selected prime model. A new set of ops spring is derived.

Speaker 1:          06:37          The process then continues with these offspring. Typically in genetic optimization mutation is also performed in order to improve the variety of offspring. All right. Now let's see what the code looks like for using neuro evolution to create the optimal neural network capable of being the simple game of flappy birds. To define the basic architecture of the game, we've got two classes, pipe and bird. These classes defined the behavior for the pipes and the bird element of the game. These classes along with the game function are enough to run the game. The game function initializes both of these classes and runs one round of the game. The arguments to this function, genome and config are used for running the neuroevolution algorithm. Implementing the running of the game in one function allows it to be called at any time at the convenience of the NDA t algorithm.

Speaker 1:          07:34          Neat. The function also computes the fitness of each genome during the game run. The evaluate genomes function relates directly to the running of the neat algorithm and is responsible for evaluating all genomes of the population for a particular generation. It calls the game function for all genomes present in the current population and assigns the fitness value returned by the game function to the corresponding genome. We are using the neat implementation of the neat python package, which requires a config file that contains a list of algorithm parameters and their values. Let's just maintain the default values for all the parameters except for the number of inputs and the fitness threshold value which will set to 600 once the game is trained. We can see that the number of generations taken by the algorithm to meet the fitness threshold very linearly with the difficulty of the game, which was dictated by the length of the gap between the pipes.

Speaker 1:          08:36          The optimal neural networks were not only different in terms of their weight values, but also in terms of their architecture. All right. Some things to keep in mind from this video. Neuro evolution is a technique that involves optimizing neural networks using evolutionary algorithms as the demand for better AI increase. So will the demand for AI that can design AI most efficiently and we can use neuro evolution as an alternative optimization technique to great and descent. Last week's coding challenge. Winner is dare baldy. First of all, I love the documentation he wrote, makes it really easy for anyone to get started. He successfully face swapped me with an actor named Kal Penn and has ambitions to apply this technology to music signals. Next for a first get hub repo, you totally rocked it. Awesome job. And the runner up is power deep. Sing stellar effort on trying out different architectures for bass swapping. This week's challenge apply neuro evolution to help an AI move more efficiently in a twoD or Three d environment poster, get hubs, links in the youtube comment section and I'll announce the top two entries next week. Yo, did you like this video? If so, hit the subscribe button and all your dreams will come true. For now, I've got to evolve, so thanks for watching.