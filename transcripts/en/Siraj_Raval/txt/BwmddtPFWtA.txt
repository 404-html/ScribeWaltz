Speaker 1:          00:00          Hello world, it's Saroj and today we're going to generate words. So given some book or movie script or any kind of text corpus, you can, it's plug and play so you can give it any kind of text corpus. It will learn how to generate words in the style of that Corpus of text. And in this case we're going to give it a book. The book is called metamorphosis by Franz Kafka, which was a really crazy weird writer from the 20th century. Anyway, his cool dude. Anyway, we're going to generate words in the style of that book. And this can be applied to any texts, any type of text. It doesn't just have to be words, it can be code, it can be h, you know, html, whatever it is. But that's what we're going to do. No libraries, just num Pi. So I'm going to go through the derivation, the forward propagation, calculating the loss, all the math.

Speaker 1:          00:49          So get ready for some math. Put on your linear Algebra and calculus hats. Okay, so this is kind of what it looks like. This first image here, uh, and I'm going to actually code it as well. So it's, I'm not, I'm not just going to glaze over. I'm going to code it so we can see the outputs as I go. Uh, but the very end part will be, uh, I'm going to code the important parts. Let me just say that. Okay. So, okay, so check this out. Given some techs corpus, it will predict the next character. So what you're seeing here is it actually pretty predicting the next word, but we're going to do a character level recurrent networks. So that means it's going to generate character by characters. Okay? So character by character, by character, not word by word by word, okay. So, uh, it's going to be trained for a thousand iterations and the more you train it, the better it's going to get.

Speaker 1:          01:37          So if you leave this thing running overnight on your laptop, uh, then by the time you wake up, it'll be really good. However, I wouldn't recommend training it on your laptop. As my song says, I train my models in the cloud now because my laptop takes longer. Right? So what is a recurrent network, right? What is this? What is this thing? We've talked about feedforward networks and I've got two images here of feedforward networks. The first image is the most popular image, right? It's that uh, really funky looking neuronal architecture. But it's, you know, it can be kind of confusing if you think about it because it's not like these neurons are classes and these classes have links to all of the other neurons. Like it's some kind of linked, you know, massive crazy linked list kind of thing. No, it's or tree like thing.

Speaker 1:          02:24          It's not really like that. What's really happening are a series of matrix operations. So these neurons are the output of a series of Matrix. These neurons are actually just numbers that we then activate with an activation function. So a better way of looking at it would be as a computation graph, a more mathematically sound way of looking at it. So if you have some input and you know the input, it could be anything. What you would do is you would multiply the input by the weight Matrix, add a bias value, and then activate the result of that and that would be your output that you then feed into the next layer, a layer that the, what you see as these neurons, a layer is actually just the result of a dot product operation followed by adding a bias value. If you want to add a bias, which you should in practice, you should add a bias.

Speaker 1:          03:17          I built neural networks without biases before for examples, but you really should add of bias and I'll talk about why in a second, but you should add a bias value and then you activate the output of that. And by activates, I mean you take the output of that doc product plus bias operation, the output of that and you feed it into a an activation function, a nonlinearity, whether that's a sigmoid or tan age or rectified linear unit. And the reason we do that is so that our network can learn both linear and nonlinear function because neural networks are universal function approximators. If we didn't apply an activation function to it, it would only be able to learn linear functions. We went to learn nonlinear and linear functions, and that's what we apply an activation function to it. Now a great way to remember this whole thing is to just rap about it.

Speaker 1:          04:05          So input Tom's weight, add a bias, activate, repeat. Here we go. Inputs singing with me. Tom's weight, add a bias, activate, repeat. And you just do that for every layer. You just repeat that process. Okay, so feedforward networks, they're great for learning an input output pattern. What is there? What is the rule here between the set of inputs and the set of outputs, right? And in the end of feed forward network, and in fact all neural networks, it's just a one. It's just one big composite function. What do I mean by that? I mean that you can think of a neural network as a giant function, and inside of that network are smaller functions, nested functions. A composite function is a function that consists of other functions. What do I mean by nested functions? Remember this computation graph that we just looked at right here.

Speaker 1:          04:57          These are all functions. Each layer is a function, right? And put times weights at a bias activate. That is a function that you feed the output of two as the input to the next function. So what a neural network is. So the, the, the most nested function right in the middle is this most nested function is the first layer value whose output we then feed to the next layer, which would be the next function, whose output we feed to the next layer. So the largest function, the, the most, the, the, the function on the, on the outside here is then the output layer, because we're feeding it, the inputs, outputs the, the outputs of what? Of all that chain, that chain of computation that already occurred, right? So that's what that is. Um, so it's a composite function and we would use feed forward nuts anytime.

Speaker 1:          05:44          We have two variables that are related. Temperature, location, hide in way car speed and brand. These are all mappings. But what if don't have done, I'm just adding my own sound effects. Don't dunt dumb. What if the ordering of the data mattered? Right? What have you had stock prices, right? It's a very controversial topic, but I just, you know, the stock price thing gets a lot of views even though I, you know, I don't wanna I don't wanna I don't know. I don't really personally care about finance data, but I know some of you guys do and you know how I'll probably talk about it more in the future. Anyway, tangent back to this. What if the time matters, right? So stock prices are a great example of when time matters. You can't just predict a mapping between time and the price, what happened before, what the stock prices before are, what matter to the current stock price.

Speaker 1:          06:38          That's actually debatable in the, in the context of stock prices, but it applies to all time series data. So video, right? If you want to generate the next frame in a video, it matters what frames came before. You can't just learn a mapping between a frame and the time that that frame shows up because then what happens is given some new time, you can't just generate a frame based on nothing else, right? It depends on the frames that came before it. You see what I'm saying? The sequence matters. The sequence matters here. The alphabet or lyrics of a song. You can't just generate a lyric or an alphabet depending on the index that it's at. You've got to know what came before it. Try to recite the alphabet backwards and so to get into neuroscience for a second, try to recite the alphabet backwards. It's hard, right?

Speaker 1:          07:28          Z, y, x w key cue you. Okay? See, I can't even do it right now. I'm not going to edit that out. So or any song, try to recite a song backwards. You can because you learned it in a sequence. It's a kind of conditional memory. What you remembered depends on what you've, what you've stored previously, right? It's conditional memory in that way and that is what recurrent networks help us do. They help us compute conditional memory. They help us compute. The next value in a sequence of values. So that's what we're current networks or are good at it. That's what they're made for. And it's not like this is some new technology, recurrent networks were invented. They were invented in the eighties neural networks were invented in the 50s but why is this super hot right now? Why are you watching this video? Because with the invention of bigger data and bigger computing power, when you take these recurrent networks and give them those two things, they blow almost every other machine learning model out of the water in terms of accuracy.

Speaker 1:          08:23          So it's just incredible. But anyway, this is a picture of a three layer and make it, this is a picture of a three layer recurrent network, right? So you've got your first layer, which is your input, your hidden state, your output layer. And so that would just be a feed forward network. But the difference here is that we've got this other layer right here and that is so what that what that other layer is, it's not actually another layer. The difference is that we've added a third weight matrix. So we've got our first way matrix our second way, but we've got a third weight matrix. And that's really what makes it different than a feed forward network is that we're adding a a third weight matrix. And what the third weight matrix is doing is it's connecting the current hidden states. So the hidden stay at the current time, step two, the hidden state and the previous time step.

Speaker 1:          09:13          So it's a recurrent weight matrix and you'll see programmatically and mathematically what I'm talking about. But that's really the key bit here for recurrent networks. That's what makes it unique from a feedforward networks. And so what this does is whenever we feed in some value, you know, cause we are training this network, right? We continuously feed it new data points, data point after data point from our training set. But for feedforward networks, we're only feeding in the input. We're not feeding in the the previous hidden states. We're only feeding an input after input after input. And the hidden state is being updated at every time step. But because we want to remember a sequence of data, we're going to not just feed in the, the, the current data point, wherever we are. We're also going to feed it in the previous hidden state. And that is, and by that I mean the values that are computed from the previous time.

Speaker 1:          10:02          Step four, that hidden states, right? That set of numbers, that Matrix. And so you might be thinking, wait a second, why don't we just feed in the input and in the previous input as well from the previous time step, why are we feeding in the input and the previous hidden state? Because input recurrence only remembers what just happened that previous input. But if you feed it in the previous hidden state, then what's happening is it can remember that sequence. It's not just about what came right before. It can remember everything that came before it. Because you can think of that hidden state as a kind of like, uh, like thinking about like clay that's being molded by every new input. It's been molded by every new input. And by feeding that clay that's being molded back into the network, it's, it's being, uh, it, it's, it's learning neural memory.

Speaker 1:          10:52          So it's, it's a form of neural memory, conditional memory, and it can remember sequential data. So, right. So here's another example just to give a few more example before I go into the code here. But we have, so this, this is a very popular type of a image for recurrent networks. So what's happening is it's we're feeding in the current input, calculating a hidden state and in computing an output. And then for the next time step we're giving it that new data points as well. And so the Blue Arrow is what's different here compared to a feed forward network we're feeding in the previous hidden state as was the input to compute the current hidden states to compute our output, our y value. And we're using a loss function to improve our network every time. And so if you think of what that recurrence looks like, it looks like this.

Speaker 1:          11:40          So do you remember that feed forward network we just looked at? The difference here is that we are feeding in the output of the hidden state back into the input. The output of this wait times bias, uh, activate operation in a layer. Okay. So of the formula for recurrent network looks like this, which basically says at the current hidden state, h t is a function of the previous hidden state and the current input. Okay. And the Feta value right here are the parameters of the function. So the network learns to use hot as a lossy summary of the task. Relevant aspects of the past sequence of inputs up to t, the loss function that we're going to use here is going to be the negative log likelihood. Okay. This is a very popular loss function for recurrent networks like plain old recurrent networks. Not using anything fancy like long short term memory cells or by directional, um, capabilities.

Speaker 1:          12:38          But the negative log likelihood usually gives us the best output or the best accuracy for plain old or current networks, which is why we're going to use it. And we'll talk about what that consists of in a second. But our steps for this are going to be the first initialized our weights randomly like we always do and then give it. Then we're going to give the model a char pair. So what is a char pair? The char pair is going to be the input chars so that's some seeds, some, some letter from the train techs that we wanted to give us input as well as a, as the target char. And the target chart is going to be our label. So our label is actually the next char. So if we take the first two chars from some input texts from some input Corpus, let's say the word is the, the input chart would be tea, then the target charter would be h.

Speaker 1:          13:22          So given t we want to predict h. So you see how that that target char acts as our, as our label that we're trying to predict. And so once we, once we have that age, we can compute the most likely next character. And then compare from our forward pass, we're going to calculate the probability for every possible next char given that t according to the state of the model using the parameters. And then we're going to measure our error as a distance between the previous probability value and the target char. So that that's, that's what axes are, are, are, are labeled the next chart in the sequence. And we just keep doing that. So it's a dynamic error, right? And so we, once we have that air value, we'll use it to help us calculate, to help us calculate our gradients for each of our parameters to see the impact they have on the loss.

Speaker 1:          14:11          And that is back propagation through time. And we call it through through time because we are using that, that hidden state to hidden state matrix, that recurrent matrix value. But otherwise it's just the same. It's just backpropagation it's called through time because we are applying that to um, hidden say to hidden state matrix to it. Okay. So then once we have our gradient values, we're going to update all the perimeters in the direction via the grit in the right direction to minimize the loss. That's grading dissent, br gradients. And we just keep repeating that process. So everything is the same here. Grading dissent as a feed forward network, Caribbean descent, um, calculating an error value, a Ford pass. But the difference is that we are connecting the current hidden state to the previous hidden state and that changes how, um, the network learns. So what are some use cases I talked about?

Speaker 1:          15:05          Time series, prediction specifically weather forecasting. Yes. Stock prices, traffic volume, sequential data generation as well. Music, video, audio, any kind of sequential data. What is the next, no, the next audio wave form. The next frame in the video. Okay. And then for other examples, I've got great one, a great one here for binary audition that was originally invented by Trask, who's a great technical writer. Definitely check that out. And so once we understand the intuition behind recurrent networks, then we can move onto LSTM networks and by directional networks and recursive networks, those are more advanced. Uh, networks. And they do, they solve some problems with recurrent networks. Before you get there, you've got to understand recurrent networks. Okay? So this code contains four parts. The first part is for us to load the training data, then we'll define our network, then we'll define our loss function.

Speaker 1:          15:58          And the loss function is going to contain both the forward pass and the backward pass. So the real meat of the code is going to happen in the loss function. And what it's gonna do is it's going to return the gradient values that we can then use to update our weights, uh, later on during training. But the Ma, the meat of the code is going to happen in the loss function. And once we've, we've come, we've defined that will write a function to then make predictions, which in this case would be to generate words and we'll train the network as well. Okay. So our first step is going to be a load up our training data. So to load up our training data, um, the F to load up our training data, I'm going to say, okay, so let's define what that, what that data is by the way.

Speaker 1:          16:41          So if we open this file, we'll look at it. Kafka, Kafka, Tst, it's one morning when Gregor Samsa woke up from trouble dreams. The right. So this is just a book. It's a big book, a big txt file. That's what the input is going to be. Okay. So we'll, we'll open it up using uh, the native functions here of Python and it's going to be recursive cause we want to, we want all of it. We'll just read that simple plain txt file and then we're going to say, okay, let's get that a list of data points and or chars. In this case we'll store it in chars and we'll define how big our data is as well as our vocab vocab size. And we can say that the, it's going to be the length of the data that that big text file as well as the length of the charges.

Speaker 1:          17:31          How many charges do we have? And we'll print it out for ourselves just so we know how many chairs there are. And once we've, we've done that, we can go ahead and print it out and it's going to tell us how many unique chars there are, which matters to us because we want to make a vector of the size of the number of charges that there are. So let me go ahead and print that out and it's going to tell us exactly what the deal is. And so we've got a character that's how many characters it's got. It has. Okay. So the data has 137 k characters in 80. One of them are unique. Okay, good. Good to know. Good to know. Our next step is to calculate the vocab size. Okay. So we're going to calculate the vocab size because we want to be able to feed back jurors into our network.

Speaker 1:          18:21          We can't just feed in Ra, uh, string. You know, chars we've got to convert the chars two vectors because a vector is an array of, of float values in this case, or a vector is an array, a at least of numbers in the context of machine learning. And so, uh, so we'll calculate the vocab size to help us do this. So we're going to create two dictionaries, and both of these dictionaries are going to convert the, um, both of these dictionaries are going to convert the characters, two integers, and then the integers to characters while respectively, one will convert from character to Integer, which, which is the one that I've just written. And then the next one is going to say, let's convert the integers to characters. And once we've done that, we can go ahead and say, well, let's print all of the values that it's, it's storing because these are our dictionaries that we're gonna use in a second to convert our values into vectors. So let's go ahead and print that. And what's the deal here? Oh, and numerate, right? And knew my great, great, great. Right? So here are our vectors right there. It's a dictionary or here, or here are our dictionaries, one for characters to integers and one for integers to characters. Okay. So once we have that now,

Speaker 1:          19:48          so we'd done that already. And so then we're going to say, let's create a vector for character. Hey, so this is what vectorization looks like for, for us. So let's say we want to create a vector for the character a. So we'll say we'll initialize the vectors empty. So it's just a vector of zeroes of the size of the vocab. Okay. And so of the size of our vocab, and then we'll say, okay, so, so convert the not now we're going to do the conversion. We'll say a char to manager, so a to the integer. Uh, so that's, so that's gonna be our input is going to give us an integer value and we're gonna set it to one. And so what happens is when we print out this vector, it's going to be a vector of size. Let's see if I got that right. So there's gonna be a vector of size.

Speaker 1:          20:43          Hold on. Oh, right. Important on Pie. Duh. I forgot. Importing num Pi. Yeah. Okay. Right. So it's a vector of size. How many a unique characters were there? There were 81 unique characters, so it's in vector of size 81 and all of those values, all this elements in the vector are going to be zero except for the one that is the mapping between a and its respective integer in that dictionary. So that's how we mapped it. That's why we created this two dictionaries. So this is what we would feed in as a, so we will feed it into of these, cause remember I said that we have a char pair, so we'll feed an ae and whatever the next character is as our input, which would be our input and our label value and the label is our other character, our next character. Okay. So then for our model parameters we're going to define our network.

Speaker 1:          21:31          Remember it's a three layer network. We have our input layer are hidden layer and our output layer. And so all these layers are fully connected. So that means every value is going to be connected to every other value between layers. Okay. So the way we'll define that is to, um, well first let's define our hyper parameters. I've, I got to, we got to define our tuning knobs for the network. So we want to say that our numbers and have a hundred a hidden neurons for it's for, it's a hundred a hundred neurons for its hidden layer and then we're going to say that we want, um, they're, they're going to be 25 characters that are generated at every time step. That's our sequence length. And then our learning rates is going to be this very small number, uh, because if it's too slow, then it's never going to converge.

Speaker 1:          22:20          But if it's too high, then it will overshoot and it's just never going to converge. The learning rates, by the way, is a, is how quickly I network abandoned old beliefs for new ones. So if you're training your neural network on cat dog images, the lower the learning rate, the less likely it will be too when given a new dog picture. If you're just been trying to get on cat pictures, if you give it a new dog picture, the less likely it will be to consider that as a part of the training data. If you're trying to be able to recognize both it, the lower the learning rate, the more likely it will consider that dog picture just an anomaly and just kind of discard that internally. So it's a kind of way of to tune how, um, quickly a network gow abandoned old beliefs for new ones.

Speaker 1:          23:00          That's another way of putting it. Um, anyway, so that's for our hyper parameters. Now we'll define our model parameters, right? We've defined our model parameters and now we can define our uh, networks, weight value. So the first set of weights are going to be from our inputs. So xow x, h, so x is our input. These, this is what the, the terminology is, right? So the weights from our input tor hidden states, right? So that's going to be initialized randomly using the non Pi's random random brand and the function. And it will be a value between the, the hidden size that we've defined. And the vocab size because those are the two values, uh, that we're dealing with here. And we'll multiply it by 0.01, because we want, we just want to scale it, uh, for a character level recurrent network because it's a character level current network.

Speaker 1:          23:51          So input to hidden states. And so then we will repeat that process. But this time for are not from our input to hidden buck for our hidden state to our next hidden state. And so that's our recurrent weight matrix right there. That's a recurrent way matrix. And so lastly, we'll have our third weight matrix, which is our, um, what's, what's our third way matrix. Our third weight matrix is our, uh, hidden states to our outputs value or how put, and so that's going to be vocab size too, between, between the vocab size and the hidden size. And then we will also, since we have to bias biases will say the bias for a hidden state will be initialized as a set of zeroes of size, of the hidden size. Cause it's for our hidden state. And then we will, uh, so that's our hidden bias. And one more bias. And that is for our, our output by it. That is our output bias. Also a collection of zeroes. Uh, the difference here is that is of the vocab size. Okay. And so, yeah. Great.

Speaker 2:          25:05          Okay.

Speaker 1:          25:06          Oh, let's see what we got here. Hidden size is not defined in size is right here. Compile come about. What's the deal? Hidden size is not defined. Yes it is. Yes it is invalid syntax. Great.

Speaker 1:          25:36          So the function is going to take this as its input, a list of input chars a list of target chars and the previous hidden state. And then this function is going to output a loss, a gradient for each parameter between layers. And then the last Hayden state. So what does a forward pass, so the forward pass and a recurrent network looks like this. This function describes the forward pass or this function describes how the hidden state is calculated. Right? So so, so how is the forward pass calculated? So the forward pass is, remember it's just a series of matrix operations. So this is, this is basically our forward pass right here. What years looking at right here. So this first equation right here is, uh, what is, let me make this smaller. So you can see. So the way we compute this math operation right here, this is the forward pass, is the dot product between the input to hidden state weight matrix and the input data. That's this term right here, plus the dot product between the hidden state. Um,

Speaker 2:          26:38          okay,

Speaker 1:          26:39          the hidden state, the hidden state Matrix and uh, the state. And then we add the hidden bias and that's going to give us the hidden state value at the current time step, right? So that's what that represents. And then we take that value and we feed it, we compute a dot product with the next way matrix. And that is a hidden state to the output. And then we add that out, that output by US value. And that's going to give us the unnormal and normalize log probabilities for the next chars, which we then squash and to probability values using the, this, this function p, which is actually right here, p right here. But I'll talk about that in a second. Okay, so that's our forward pass. And then for our backward pass, the backward pass is going to be, before we talked about the backward pass, let's talk about the loss for a second.

Speaker 1:          27:28          So the loss is a negative log likelihood. So it's the negative log value of p and P is this function here. So, uh, which is represented programmatically by this, uh, right here, right? So it's the, uh, it's e to the x where x is the output value from the, that it received divided by the sum of all of the, uh, e two the probability values, okay? And that's going to give us p a p value, okay. And so we take that p value and then we take the negative log of that p value and that is our loss scaler, that loss scalar value. And so once we have that loss, we're going to, we're going to perform backpropagation using that loss. And so the way we compute backpropagation to go over this is by using the chain rule. So the chain rule is from calculus.

Speaker 1:          28:19          What we want to do is compute gradients for each of the layers. Okay? So for each of the weight major sees, okay, given an error value, we're going to compute the partial derivative of the error with respect to each way recursively. So the reason we're using the chain rule is that so, so because we have three weight matrices, we have the inputs to hidden, hidden to outputs and hidden to hidden. We want to compute a grading values for all three of those. So that's what this looks like. We want to compute great and values for all three of those weight matrices. And the way we're going to do that is to computer loss using the negative log likelihood and use that loss to compute the partial derivative with respect to each of these wait major cs. And once we have those though, that's our gradient value.

Speaker 1:          29:02          That's the change. That's the delta. We can then update all three wait major cities at once and we just keep doing that over and over again. So our first gradients of our loss is going to be computed using the, using this function. So compute p minus one, and that's going to give us our first grade dance and we're going to use the chain rule to, to backward pass that gradients into each, uh, into each weight matrix. So let me talk about what I mean by this. So the chain rule. So remember, remember how I said neural networks are giant composite functions, right? It's a giant composite function. And what the chain rule lets us do is it less? That's compute the derivative of a giant of a function as a song, as the product of derivatives of it's nested functions. So the chain rule in the case of f of x, right here, if f of x is a composite function that that consists of g of h of x, then the chain rule would be to say, well let's compute the derivative of g of h of x times a derivative of h of x said Nesad function.

Speaker 1:          30:04          So you multiply it by the derivative of the inside function and that's we'll give you the derivative of that bigger function, okay? And You keep doing that for as many nested functions as you have. Here's another example. If I want to derive the function three x plus one to the fifth, then I would say, well this is actually a function. And the function is, um, the outer function, g of x is three x plus one to the fifth. So, oh, so we're using the power rule. We take the exponent value, move it to the Coefficient and subtract one from the exponent. So then it would be five times three of x plus one to the fourth times the derivative of the nested function, which is three of three x plus one. And that's the chain rule. And so, and if we multiply those two derivatives together, that will give us a derivative of the larger function f of x.

Speaker 1:          30:55          So that same logic applies to neural networks because neural networks are a composite functions. So we are, we're cursively moving this derivative, this partial derivative value. By moving, I mean multiplying dot product or computing the dot product between the partial derivative, calculate it at the last layer, and we're multiplying it by every layer recursively going backward. This will make more sense as we look at this programmatically. But that's what's happening here. And uh, yeah, that's what's happening here. So let's, let's, let's, let's code this out. By the way, the bias, the reason we add a bias is that allows you to move the thing of it as like this, you know, in the y equals mx plus B equation, it allows you to move the line up and down to better fit the data. Uh, without be the line will always go through the origin zero, zero, and you might, you might get a poor fit.

Speaker 1:          31:42          So a bias is kind of like an anchor value. Any way to define our loss function, our loss function is going to be. So we're going to give it our inputs and our targets as its parameters, as well as the, uh, hidden state from the previous time step. Okay? So then let's define our perimeters that we're going to store these values in. So I'm going to define four parameters. Okay? These are lists that we're going to store values at every time. Step in, okay? As we compute them. So these are empty dictionaries. So ActiveX so excess is going to, we'll store the one hot encoded input characters for each of the two of the 25 time steps. So concern this will store the input characters. Hs is going to soar, the hidden state outputs. Okay. Why has we'll store the target values and ps is going to take the why's and convert them to normalize probabilities for chars.

Speaker 1:          32:38          Okay, so then let's go ahead and say uh, h of s h sorry h s the value of h s is going to be, the reason we're copying that looks to check this out. We're going to initialize this with the previous hidden states. The h s currently with the previous hidden states and using the equal sign would just create a reference but we want to create a whole separate array. So that's why we, we don't, we don't, we don't want hs with the element negative one to automatically change if change, if age previous has changed. So we'll create an entirely new copy of it. And so then we'll initialize our loss has zero and then and that, okay, so we'll initialize our loss as zero. So this is our loss scalar value. And then we'll go ahead and do the forward pass. So the forward pass is going to look like this.

Speaker 1:          33:27          Okay, so we've already looked at it mathematically and now we can look at it programmatically. So we'll say, okay, so for each value in the range of inputs, so for the length of inputs, let's compute a forward pass. So the forward pass is going to be um, we were going to start off with a that one of k representation. We placed a zero vector as the teeth and put and then inside that teeth input we use the manager in inputs list to set the correct value there. Okay, so that's then that second line. And then once we have that, we're going to compute the hidden state. Now remember I showed you the equation before. We just repeat that equation here. And then we compute our output just like I showed before. And then our probabilities, the probabilities for the next chars once we have our probabilities will compute are softmax cross entropy loss, which is the negative log likelihood.

Speaker 1:          34:18          It's also called the cross entropy. You'll actually see that in tenser flow, the cross entropy as a predefined function. But we're computing it by hand here. And so once we have the forward pass, now we can compute the backward pass. We're going to compute the gradient, value's going backwards. So initialize empty vectors for these gradient values, right? So the gradients. So these are the gradients are the derivatives, the derivatives are our gradients is the same thing here. So we're computing are derivatives with respect to our weight values from x to age, from age to age and then from age to y and we'll initialize them as Zeros. And then as also we also want to derive, we also want to compute partial derivatives or gradients for these um, uh, bias values for our hidden state and our outputs. And then, uh, as well as for our next, which means the next time step that the hidden state in the next time step derivatives for all of them.

Speaker 1:          35:10          When we do backpropagation, we're going to, we're going to collect our output probabilities and then the derive our first gradient value. Now our first grade and value, it looks like this. Let me go back up here. This, this, this is how we compute our first great and value with respect to our, uh, with our Spec to our loss, right? That's a first gradient value. So, uh, we're going to compute the output gradients. So which is the output times the Hin states transpose. And we can think of this one. So check this out right here. So this is our first partial derivative with r four r are hidden state to why to our output layer, that matrix. And you can, and so what we do is we compute the dot product between that output and the transpose of the hidden state. The reason we use a transpose is we can think of this intuitively as moving the error backward through the network, giving us some sort of measure of the error at the output of that layer.

Speaker 1:          36:05          So when we compute the dot product between the transpose of some layers matrix with the derivative of the next layer that is moving the air backwards, it's kind of, it's, it's backpropagation because the error value, it's constantly changing. With respect to every layer that it moves through. And by multiplying it, by the transpose of a layer, the dot product from the partial derivative with the previous layer at times where we currently are, it's going to output a gradient value that, that, that derivative, right? And we'll use that derivative, uh, later on to update other values as well. So we're are, we're also going to compute the derivative of the output bias and then we're going to back propagate into h. So notice how we are continuously performing dot product operations here for every single layer we have, we're also back propagating through the 10 h nonlinearity, right?

Speaker 1:          37:01          So we are competing the derivative value and this is programmatically what the, what the derivative of Tan h looks like. And we're using the, uh, computer derivatives from the previous layer that we were at at the end of the network, the tail end, as we move through to the beginning as we're using them as values to compute the dot product of the whole point of computing the dot product. With these, uh, with respect to each of these layers is that we are computing new, um, gradient values that we can then use to update our network later on. So then we use that, a raw value to update our hidden value. And then we lastly, we could, we compute the derivative of the input to the hidden layer as well as the derivative of the hidden layer to the hidden layer. And once we have that, we can return all of those derivatives are gradient values are our change values, we can return all of that.

Speaker 1:          37:54          Now there's also this step right here to mitigate exploding gradients, which we're not going to go into right now because it's not really necessary. However, I will say this, that, um, whenever you have really, really long sequences of input data like book, like the Bible, just a huge book, then what happens is as the gradient is moving, by moving, I mean you're competing the dot product of it for every layer with the current way matrix, wherever you're at using the partial derivative, the, the value will get smaller and smaller. There's, it's a problem with the recurrent networks that's called the vanishing gradient problem. Okay. And so, uh, it gets smaller and smaller and there's a way to prevent that. One way is to clip, uh, those, those values by defining some, some interval that they can, that can, they can reach or another way to use LSTM networks, which we're not going to talk about.

Speaker 1:          38:44          But anyway, uh, yeah, so that's our forward and backward pass. We computed that inside of the loss function and we computer our loss as well right here using softmax cross entropy. So for as many characters as we want to generate, we will do this. So we'll say the Ford pass is just like we did before. It's the same exact thing. It's just repeating the code over and over again. Input Times, wait, activate, repeat, get the probability values, pick the one with the highest probability, create a vector for that word, customize it for the predicted char, and then add it to the list. And we just keep repeating that for as many. Uh, n defines how many characters we want to generate so we can generate as many characters as we want on a train to network and we'll print those out. Okay, so then for the training part, we really competed, we've completed that, that meat of that code, right?

Speaker 1:          39:34          But now for the training part, we're going to feed the network some portion of the file and then for the loss function we're going to do a forward pass to calculate all those parameters for the model for a given input for a given input output pair, the current char, the next char, and then we're going to do a backward pass to calculate all those gradient values. And then we're going to update the model using a technique. It's a, it's a type of grading dissent technique called Adda Grad, which is just it just a case of learning rate, but it's, it's, it's great in a sense. You'll see what I'm talking about. It's not complicated, but it's called add grad. So we're going to create two arrays of Charles from the data file to target. One is going to be shifted from the input one. So we basically just shifted by one as you notice here.

Speaker 1:          40:13          So now we have our inputs in our targets, right? And these numbers are actually character values in the dictionary, but they represent, uh, they help us, they help us create directors, uh, where the indices here represent the one out of all the Zeros and the zero vector. And that's what we feed into our model. So at a grad is our gradient descent technique. And the difference here as composed as compared to regular grading dissent is that we decay the learning rate over time. And what this does is it helps our, uh, network, uh, learn more efficiently. This is the, this is the equation for Adot Grad where a step size means the same thing as a learning rate, but basically the learning rate gets smaller, smaller during training because we introduced this memory variable that grows over time to calculate the step size. And the reason it grows while the step size decreases is because it's inside of the denominator of this function right here. This is the programmatic representation of the mathematical equation that you're looking at right here. So here's the programmatic implementation of that. We calculate this memory value, which is we are gradients, right of our parameters. And then we update our, and then we update our weight Matrix, um, condition on the learning rate, which decays over time via this function right here.

Speaker 1:          41:30          So finally, so this is really, this is, so we've, we've, we've done all the math and now it's just implementing it. So, um, so we have our, uh, weight matrices here. We have our memory variables for a Grad and then, um, we will say for a thousand iterations will actually a thousand times a hundred iterations. We want to feed the loss function, we want to feed the loss function, the input vectors to see how this part works. We're gonna feed the loss function, our input vectors, and then we're going to compute a Ford pass using that loss function. And it's going to compute the loss as well. It's going to return the loss function or the loss scaler. It's going to return the derivatives or gradients with respect to all of those weighed values that we want to update. And then we're going to, um, performed the parameter update using ad or Grad, right? So we'll feed all the derivative values to uh, our ad, a Grad. Um,

Speaker 1:          42:30          this is Adam [inaudible]. So whose whole fee, all those derivative values to our ad Grad function right here. Okay. And it's going to update our parameters and basically the learning rate just decays over time. That's what the, that's why Ma'am is calculated to decay the learning rate over time and um, which just helps with convergence and a, there's different grading dissent techniques, Adam on different ones like that, but, um, uh, momentum. But yeah, I had a grad is one of them. And so once we do that, we can look at our sample function here, our sample function and our sample function is going to, right here, we're going to keep, we're going to generate 200 word up, 200 character sentences at a time for, uh, for a thousand times a hundred iterations. So a lot of iteration, 100,000 iterations. Okay. So let's go ahead and run this and see what happens.

Speaker 1:          43:18          Okay. See, the first iteration is really bad. Look at that. It's just like weird characters. Okay. But now it's got a more human readable characters. Okay. It's getting better now. It's like he ate left. Notice how the loss is decreasing very rapidly here as well. Okay. And so yeah, it's getting better over time. Okay. So that's it for our network. And let me stop this and you can feed it anything really, you can feed it. Any text file it's going to work with any text file. Okay. So we've computed the Ford Pass, the backward pass that the backward pass is just the chain rule. Okay. I've got links to help you out in the description, but it's just the chain roll. We're just continuously computed computing derivatives or grading values, partial derivatives or gradients. Same thing. We call them partial because there were, with respect to each of the weights in the network going backward and we are moving this error by moving, we're considering the dot product of each layer.

Speaker 1:          44:12          This matrix by the derivative of the previous layer just continually. And that's the chain rule. And if we do this, we can generate words, we can generate any type of word. We want to have given some, some text corpus. You can generate Wikipedia articles, you can generate fake news, you can generate anything really code. And yeah, so also for deep learning, you might be asking what for deep learning, which of these layers do we add deep? Which where do we add deeper layers to? Do we add a more layers between the input and the hidden state between the hidden state and the output or between the hidden stay in the hidden say Matrix, which direction do I had? Deeper and deeper layers? Well, the answer is that it depends. This is one thing that's being worked on, but the, but the idea is that you'll get different results for whatever, um, whatever set of matrices that you, um, add deeper layers to their different papers on this. But yes, adding deeper layers is going to give you better results. And that's deep learning. Recurrent nets applied to deep learning. But this is a simple three layer feed forward network that works really well. And I would very much encourage you to check out the get help link in the description and the learning resources to learn more about this. So yeah, please subscribe for more programming videos. And for now, I've got to do a four year transform, so thanks for watching.