Speaker 1:          00:00          Hello world. It's a Raj. In this video, we're going to compare the most popular deep learning frameworks out there right now to see what works best. The deep learning space is exploding with frameworks. Right now it's like every single week some major tech company decides to open source their own deep learning library and that's not including the dozens of deep learning frameworks being released every single week on get hub by cowboy developers. How many layers you get. Let's start off with psychic learn. Psychic was made to provide an easy to use interface for developers to use off the shelf. General purpose machine learning algorithms for both supervised and unsupervised learning. Psyche provides functions that let you apply classic machine learning algorithms like support vector machines, logistic regression's and k nearest neighbor very easily, but the one type of machine learning algorithm it doesn't let you implement is a neural network.

Speaker 1:          00:48          It doesn't provide GPU support either, which is what helps neural network scale. Since like two months ago, pretty much every single general purpose algorithm that cycle or an implemented has since been implemented in tensorflow psychic. You just got learned. There's also cafe, which was basically the first mainstream production grade deep learning library started in 2013 but cafe isn't very flexible. Think of a neural network as a computational graph. In Cafe, each node is considered a layer, so if you want new layer types you have to define the full forward backward ingredient updates. These layers are building blocks that are unnecessarily big. There's an endless list of them that you can pick from in tensorflow. Each note is considered a tensor operation like matrix at or Matrix multiply or convolution, and a layer can be defined as a composition of those operations. So tensorflow is building blocks are smaller, which allows for more modularity.

Speaker 1:          01:40          Cafe also requires a lot of unnecessary verbosity. If you want to support both the CPU and the GPU, you need to implement extra functions for each and you'd have to define your model using a plain text editor. That is just ghetto model should be defined programmatically because it's better for modularity between different components. Also, cafes, main architect now works on the tensorflow team. We're all out of cafe, but speaking of modularity, let's talk about care. Ross terrace has been the goto source to get started with deep learning for awhile because it provides a very high level API to build deep learning models. Kara sits on top of the other deep learning libraries like piano in tensorflow. It uses an object oriented design, so everything is considered an object be that layers, models, optimizers, and all the parameters of the model can be accessed as object properties like model dot layers three dot output.

Speaker 1:          02:30          We'll give you the output tensor for the third layer in the model and model dealt layers three dealt weights is a list of symbolic weight tensors. This is a cleaner interface as opposed to the functional approach of making layers functions that create weights when being called great documentation. It's all Gucci. Yes, I'm bringing that back, but because it's so general purpose, it lacks on the side of performance. Chaos has been known to have performance issues when used with a tensorflow backend since it's not really optimized for it, but it does work pretty well with the Theono backends. Two frameworks that are neck and neck right now in the race to be the best library for both research and industry are tensorflow and piano. Piano currently outperforms tensorflow on a single GPU, but tensorflow outperforms the Yanno for parallel execution across multiple gps. Deanna has got more documentation because it's been around for awhile and it's got native windows support, which tensorflow flow doesn't yet dammit windows.

Speaker 1:          03:22          In terms of syntax, let's just take a look at some code to see some differences. We're going to compare two scripts in tensorflow and theone. They both do the same thing, initialized symfony data and then learn the line of best fit for that data so it can predict future data points. Let's look at the first step in both tensorflow Angiano or generating the data pretty much the same way using num py or raise, so there's not really a difference there. Let's look at the model initialization parts. This is the basic y equals mx plus B slope formula, intention flow. It doesn't require any special treatment of the x and y variables. They're just, they're natively, but in piano we have to specifically say that the variables are symbolic inputs to the function. The tension flow syntax of defining the B w variables is cleaner. Then we implement our gradient descent function, which is what helps us learn.

Speaker 1:          04:04          We're trying to minimize the mean squared error over time, which is what makes our model more accurate. As we train the syntax for defining what we're minimizing is pretty similar. Then when we look at the actual optimizer which helps us do that, we'll notice a difference in syntax. Again, tensorflow just gives you access to a bunch of optimizers right out of the box. Things like gradient descent or Adam Piano makes you do this from scratch and we have our training function which is again more verbose. See the trend here. Deanna so far is making us implement more code than tensorflow, so it seems to give us more fine grained control but at the cost of readability. Finally we'll get to the actual training part itself. They look pretty identical but tension flows methodology of encapsulating the computational graph peels conceptually cleaner than pianos. Tensorflow is just growing so fast that it seems inevitable that whatever feature it lacks right now because of how new it is, it will gain very rapidly.

Speaker 1:          04:50          I mean just look at the amount of activity happening in the tensorflow repo versus the theatro repo on get hub right now and while Kara serves as an easy use wrapper around different libraries, it's not optimized for tensorflow. A better alternative if you want to learn and get started easily with deep learning is TF learn, which is basically care os but optimized for tensorflow. So to sum things up, the best library to use for research is tensorflow, the world class researchers that both open AI and deep mind are now using it for production. The Best Library to use is still tends to flow since it scales better across multiple Gpu then its closest competitor piano. Lastly for learning, the best library use is TF learn, which is a high level wrapper around tensorflow. That lets you get started really easily. Also, shout out to Raho Deo for being able to generate an upbeat mid. I file a bad ass of the week. Please subscribe for more programming videos. For now. I've got to go worship tensorflow some more. So thanks for watching.