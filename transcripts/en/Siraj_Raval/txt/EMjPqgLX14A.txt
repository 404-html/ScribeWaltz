Speaker 1:          00:00          Hello world. It's a Raj and just a few days ago, Facebook's AI research team released a demo called dense posts. They were able to map out every single pixel of a human body in a given video and not just for a single human, but for many, many humans all at once, even more impressive. They were able to do this in videos with lots of distractions and general chaos on a single GPU. Splost. Nope, just deep learning. This is pretty cool and has tons of applications and we're going to learn how they did it in this video. Creating d animated characters is a lot of work. The tools to do so are expensive and require teams of people working for months on end. Big Production Studios with the available budgets are able to track humans and convert them to animations, but for people without big budgets, this technology will democratize that ability.

Speaker 1:          00:55          Remember my video on the deep fakes algorithm? Well, this can be used for a full body version of that. If we can track a human, we can swap out their entire body with a different one. Lending to some surreal scenarios like Geoff Hinton charging into more door instead of Aragorn augmented reality applications could use this to label people that you see in real time with all sorts of statistics and it would give away for machines to be able to better read our body language for everything from sentiment analysis to full body tracking to virtual reality gaming. This is the new state of the art in human pose estimation. And to understand how it works, we're going to have to try and work through the thought process the authors had both in terms of building on previous discoveries and trying different methods. So let's say we have a video of a human dancing.

Speaker 1:          01:48          This is a two dimensional grid of pixels, but when you or I look at it, we're able to tell that there are indeed three d objects that are being represented by this two d grid. We want our computer to have this ability as well and in a way that we can visualize it. Meaning we'll want to transform this tutti human into a three d model. Once we have that three d model, we can do whatever we want with it, put it into any sort of scene, swap it with another three d model, change its features, lots of possibilities, but all of these possibilities depend on us being able to construct a three model of this human in real time that updates its movements as the human moves. When want to construct a correspondence, this is a computer vision term that is a measure of how well pixels in one image correspond to pixels in another image.

Speaker 1:          02:40          In our case it would be a two D to three d image and we don't want any holes in our image. We want all the pixels to be as close together as possible, so we'll call it a dense correspondence and we'll just focus on doing this for humans. For now to create a dense correspondence will need to perform some object detection, object segmentation and pose estimation. That's a multilevel problems, so we want to start with a method that's as simple as possible. We got so many different avenues to go about this, but ideally we could use some sort of labeled data set because having a label just makes training machine learning algorithms much easier since it just involves learning a function that represents the mapping between the input data and the labels. In this case, what would the ideal labeled datasets look like? There are tons of labeled image data sets out there these days.

Speaker 1:          03:35          Pascal Cfr, image net. These are basically big collections of images of a variety of random objects each with their own particular label. These datasets have been a catalyst for all of the recent progress in computer vision algorithms, but we don't want to just label a human. If we see one in a video, we want to be able to create a three d model of one. There's not really a Dataset of human images with the label being the three d model, so we'd have to create one ourselves and it would involve humans manually creating annotations that relates three d images to surface spaced representations of the human body, and that's exactly what they did. They basically asked the annotators to annotate regions corresponding to defined body parts like the head and the legs. All of these annotations were labeled with their corresponding three d body part, which acted as the label.

Speaker 1:          04:33          They did this for 50 k humans, which summed up to be a total of 5 million manually annotated correspondences. Once we have our Dataset, we're going to have to decide on what model to built. We know that deep learning is the state of the art when it comes to classification and that convolutional networks are the state of the art in image classification, but it's not just classification we're trying to do, but regression as well. What's the next move the human is going to make? The next question would be, has anyone done something similar with a convolutional network? Of course. Luckily there was a recent architecture called dense Raj that did this for all sorts of objects and got decent results, so we could start with that architecture. In the first step, the network, we'll classify a pixel as belonging to either the background or one of several region parts that gives a rough estimate of surface coordinates.

Speaker 1:          05:32          This is essentially a labeling task that can be trained using gradient descent. In the second step, a regression model will indicate the exact coordinates of the pixel within the region part. A more formal way of saying this is that in the first stage they will assign position l two the body parts c that has the highest likelihood as calculated by the classification branch and in the second stage it will use the regressor to place the point l in the continuous coordinate pair UV. That is a parameterization of part c c can take 25 values. One would be the background, meaning that P is a 25 way classification unit and we can train 24 regression functions are each of which provides two d coordinates within its respective parts. Both the classification and regression tasks are trained by minimizing a respective loss function, but the regression loss is only taken into account for a part if the pixel is within the specific part.

Speaker 1:          06:30          So our network can work, but it requires a lot of tasks for a single network like parts segmentation and pixel localization. They use a technique called region of interest. Pooling to create different regions and fed the results, seeing features into regions specific branches. This decompose the complexity of the task into controllable modules, all of which which could be trained jointly in an end to end approach. So it's a fully convolutional network on top of region of interest pooling that is entirely devoted to two tasks, generating a classification and regression head that provide part assignment and park coordinate predictions to improve the model. They used a technique called cascading meaning using a collection of models using all the information collected from the output of one model as additional information for the next classifier in the cascade. The output of the region of interest align module feeds into the dense network as well as networks for the masking and key points tasks.

Speaker 1:          07:35          The first stage predictions from all tasks are then combined and fed into a second stage refinement units of each branch when experimenting on novel videos. There architecture worked well across the board for not just one human but multiple humans. Unfortunately they haven't released the initial source code just yet, but we can recreate what they did using the details in the paper. Also, the Dataset they created is open source. So definitely check that out. Three things to remember from this video. Dense pose is a new deep learning model that can estimate three d models of multiple humans from just a video on a single GPU. They use a collection of convolutional networks to do this performing classification for assignment and regression for coordinate to predictions. And they open sourced the data set they created mapping humans to their corresponding three d models. Please subscribe for more programming videos. And for now, I've got to make a good bipolar, so thanks for watching.