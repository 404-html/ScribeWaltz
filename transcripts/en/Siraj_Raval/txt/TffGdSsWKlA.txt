Speaker 1:          00:00          Hello world, it's Saroj and we're going to try to predict earthquakes in this video using data science. Now this is an ongoing challenge on the popular data science competition website, Kaggle. It's called La n l earthquake prediction. And the idea is to use an existing datasets of seismic activity to try to predict when an earthquake will happen and their people. This is an ongoing challenge. There are people who are, as we speak, submitting their best work to try to predict an earthquake using this provided dataset. And in this video we have three learning objectives. The first learning objective is to get some idea as to how to think like a data scientist. That's the first one. The second one is to understand the cat boost algorithm, which stands for categorical gradient boosting. It's very popular. Don't worry if you don't understand that. We're going to go into that.

Speaker 1:          00:50          And the third objective is to understand the support vector machine algorithm, specifically support vector regression, not classification, which is usually used for. So those are our three main learning objectives. There's going to be a lot of math, there's going to be code. And we're going to do that all in this video in a seven step process. As you can see here, we're going to go from zero installing, solving all of those dependencies to exploring the Dataset and then implementing the math and the code, et Cetera. So it's super exciting. And if you haven't subscribed, okay, so let's get into this. So they're, our first step is to try to understand the background of this problem. So predicting earthquakes has long been thought to be near impossible, but if we were able to predict earthquakes than we could save countless lives and properties, these are very destructive forces of nature.

Speaker 1:          01:39          So there are many ways of predicting the magnitude which equates to how destructive and earthquake is. So we can see here our first equation for magnitude that I'm circling with my mouse pointer log based, 10 of a over a zero, a zero is the base quake. The standard calibration earthquake for testing purposes and a is the intensity of the actual earthquake. So when we divide those two values and take log base 10 of them, we get the magnitude and there are different ways of writing this equation. As we can see here, we can denote it as I, we can set, I equal to, I base zero times 10 to the m, et Cetera. And so that doesn't necessarily have to do with this problem. It's just really interesting to note as background knowledge. So when we take that magnitude and we try to see what the places with the worst earthquakes are, we'll find that San Francisco is number two. Chilay is number one. And this is a bad thing, right? So we wanted, this is a picture of an earthquake and we want to try to predict when this is going to happen. So the idea is that given seismic signals, we're asked to predict the time until the onset of earthquakes. Now these earthquakes or actually laboratory earthquakes. So in the lab, two plates were put under pressure and this resulted in sheer stress. And we can see what this looks like by playing this little video right here, right at three 24 he's demonstrating this process

Speaker 2:          03:08          wish on the end of the sample and cause it to share like that. And so that sharing would be motion on the undefault surfing.

Speaker 1:          03:16          So it's just like that, just like in the lab. And so we can see the training data is just a single sequence of signal and it seems to come from one experiment. However, the testing data consists of several different sequences called segments that may correspond to different experiments. And we can see a preview of that right on the website if we go to data. And so here on this Kaggle page we can see a sample of what it looks like and all of these segmentations have a segment id and then there's a time to failure. Okay, so there, there are two features there, right? So two columns there.

Speaker 3:          03:49          Okay.

Speaker 1:          03:50          And so we want to try to figure out the relationship between those two columns. So before we begin, definitely check out these two helpful resources. So the first one is called Kaggle, colonels. And what these are are basically Jupiter notebooks and it's a way for the community on Kaggle to share what they've done with this challenge and they're super cool. Definitely check them out. They definitely gave me a lot of ideas as to how to approach this problem and they will for you as well. How to think about this problem. It's a great initiative. And the second one is the website papers with code that the community is really digging, including me right now. And if you have an idea for a different type of model or whatever model you want to use, this is the website to search for, right? So let's say that I want to use, I don't know, genetic algorithms.

Speaker 3:          04:36          Okay.

Speaker 1:          04:36          For supervised learning. Then I can just type in genetic supervised

Speaker 1:          04:43          and see what shows up. Okay. So two papers showed up and we can see the paper and the code, which is awesome. This website is awesome. I endorse this website, I approved this message. Okay. So, um, so those are the two that definitely check those out. All right, so, so there we go with that. And now we are on to step one. Here we go for about to install an import, our dependencies. So of course we need to install Kaggle because that's how we're going to actually retrieve this data set. And by the way, if you're watching this, I want you to open up colab.research.google.com and just do this with me because you can, this is all in the cloud. You don't have to worry about configurations or dependencies. It's all happening in the cloud. Okay? So I mean locally you don't have to worry about, so let's install that and now we're going to install the num Pi, but the latest version to perform some math operations specifically for statistical features, which we'll talk about. And lastly, we're going to install the cat, boost the library, which will allow us to use cap boost. Great. We installed that. So now that we've installed those dependencies, we can start importing all of the ones that we're going to use. So of course we're going to import pandas because that's for data preprocessing. We're also going to import num Pi, which is going to be four matrix math.

Speaker 1:          06:01          Now for machine learning cat boost, we're going to use just like we installed specifically it's regressor which cause, because this is going to be a regression model, which I'll talk about and pool which I'll also talk about and uh, I guess for scaling we will want to use psych. It learns preprocessing, uh, what was it called? Standard scaler. Standard scaler.

Speaker 4:          06:28          Okay.

Speaker 1:          06:29          Uh, what else? For hyper parameter optimization, we'll want to use grid search. That means finding those ideal hyper parameters for arc, a cap boost algorithm also for arch support vector machines. Speaking of a psychic learn, how does that second learn? Has Everything seriously? It's got it all. It's got it all. And for Colonel Ridge. Okay, so I got to talk about that. We're not going to talk about that right now, but we will at the, at the end. But Colonel Ridge is kernel methods are a way of improving support vector machine predictions and making sure that we can create a classifier line or regression line in a feature space that we can visualize or in a lower dimensional featured space. So coronal ridge.

Speaker 4:          07:22          Okay.

Speaker 1:          07:22          And then of course data visualization. We're going to import for this specific use case map plot line. Great. Okay. So those are our dependencies.

Speaker 1:          07:38          That worked. Awesome. So now we're on to step two, importing our dataset from Kaggle, right? So we want to collect that data set. So to do that we're going to use Google colabs own file access feature, which is awesome, which lets us import files directly into coke colab. Now once we've done that, we can create an object called uploaded that will receive the result of whatever we upload and then we can perform this command line operation that will move that Kaggle dot. Jason Haas, I'll talk about what I'm, what I'm, what I'm doing in a second. Uh, doc, Kaggle and hand. C H mod 600. Okay. So what we're going to do now is run this

Speaker 1:          08:36          and we're going to upload our Kaggle Dot Jason File. So what this is, is a way for Colab to know what your authentication details are. It's the first thing you do is you go to my account and then you click on create new API token and it will download that API token has a Jason file. So then we'll go back and we'll upload that Kaggle dot Jason File. All right, so once we've uploaded our Kaggle dot Jason File, we can access the Kaggle Api directly using Kaggle competitions list, and it's going to show all of the competitions that exist there. Now that, that, that was just for, you know, demo purposes. Now we can actually download the earthquake data.

Speaker 1:          09:34          It's a great, now it's downloading that data directly from Kaggle into my colab run time environment. And once I have that, so I'm going to see where it is on my system. Looks like. Okay, there we go. We got it. We got to unzip this, uh, training file. So we'll use the unzipped command, uh, to, to, to unzip it, trained.csv.zip. And then we'll see what it looks like. And once it's done on Zipping, we can see it right here, totally unzipped. And we can now go on to step three, which is performing exploratory data analysis. So our first step is going to be to extract that training data into a pan, does data frame so we can look at it, visualize it, and then eventually perform some statistical analysis on it.

Speaker 4:          10:17          Okay.

Speaker 1:          10:17          So for Eda, we've got our handy dandy, a pandas function here, specifically read CSV, one of the most used functions and data science. And we know the name of our file as we can see it right above us. It's called train dot CSV. And how many data points do we want to import a, this is a pretty big file. So let's just say 6 million because why not? And then we're going to say, well, let's name our data types. Our first data type is going to be called acoustic data.

Speaker 4:          10:47          Okay.

Speaker 1:          10:48          And that data type is going to be a an integer 16 bit integer and our next data type is going to be called time to failure.

Speaker 4:          10:59          Yeah,

Speaker 1:          10:59          I mean we can name these anything we want, but we're just going to call it that and since this is a very floaty number, we're going to call it floats 64 there's a lot of decimal places here and once we have that we can try and print out the first 10 entries. Let's say 10 and there we go. That's our data set right there. The first 10 entries and these values are are all going to change and now our next step is to visualize this data. Now specifically we want to visualize the first 1% of samples in this data. Now I've already written out this part in particular right here because data visualization is not a super hard, right? We can do this in Tablo. Drag and drop interfaces exist on a zero and all of these platforms. So I just wrote this out here and when we plot this out, when we compile this,

Speaker 4:          11:49          okay,

Speaker 1:          11:49          when we compile this, we'll see that right before the time to failure, right before this blue line, this time to failure, that is when the biggest uh, acoustic a quake quake happens. So we know that there is a, a point before the actual earthquake where there's a spike in acoustic activity, seismic seismographic activity. Now, if we were to visualize not just that, but all of the data, not just the first 1%, we'll see that actually

Speaker 4:          12:24          yeah,

Speaker 1:          12:24          at right before the earthquake, which is that this blue line is vertical Blue Line right before this vertical Blue Line, the earthquake, every time there is a, there is a spike in acoustic activity. So there that, there is our pattern right there. And, and eda exploratory data analysis helped us visualize this pattern. So we know that the way it goes is spike in activity, earthquake and this just keeps happening. Other questions we could ask is, is this interval constant to any other factors affect this? Right? We're, we're framing our question by using exploratory exploratory data analysis bit by bit.

Speaker 4:          13:03          Okay,

Speaker 1:          13:03          so we'd done that part and now let's move on to feature engineering. But before we do that, I also want to give out, give out a shout out to Anton law loss. So his colonel was really interesting. What he did was he, he had the intuition to say that the data looks like sound waves. It's also waiting as we can see in this visual analysis here. So he converted it into an audio file. He didn't always did. Then he animated the wave and compared it to a dubstep song, which is interesting. So the, his idea was that, you know, perhaps just like a dubstep song, we can hear the buildup of an earthquake before the drop. So that's a really cool colonel that I would definitely check out. It's right here, it's called audio analysis with animation.

Speaker 4:          13:46          Okay.

Speaker 1:          13:47          He's got a bunch of samples here. We can even play it to see what it sounds like and then we can play to see what it sounds like with the dubstep.

Speaker 4:          14:09          Okay,

Speaker 1:          14:10          so interesting. Anyway, data is amazing. So now onto step four, feature engineering. So remember we only had those two columns, but there are a lot of features we could have that could help increase the accuracy of our regression model, which we're going to build in a second. So which features do we want to add? Now there are a lot of tried and true features, statistical features that have improved models in the past. So what we're gonna do is we're going to add about nine, maybe 10 statistical features to our data set. So it's going to be a lot more features. And hopefully the idea is that this is going to improve the capabilities of our model. It's going to have a better prediction value. So let's do all of that in a single function called generate features given our dataset.

Speaker 4:          14:57          Okay,

Speaker 1:          14:57          so that's what we call a generate features. And we're going to say ass train is going to start off as a list and we're going to continually append that list with a series of features. And I'll explain what each of these features are. So of course the mean is the average value of our data set, the standard deviation tells us how spread our Dataset is from the mean.

Speaker 4:          15:19          Okay.

Speaker 1:          15:20          The minimum value is just the minimum value in our dataset. Kurtosis I'm going to explain in a second that's an, that might be a newer one for you if you haven't been in this space for a long, long time. Uh, what else? Skew. That's a, that's a fun one.

Speaker 5:          15:45          What else? Um,

Speaker 1:          15:54          maybe tile values a value or two and we'll split it.

Speaker 5:          15:58          It's just like that Quan tile and that's it. This is add those.

Speaker 1:          16:14          And we will now return what this looks like

Speaker 1:          16:22          and there we go. So let me explain what each of these features are and then we're going to talk about how we can add them to our data set. So of course the mean is sigma notation here. We take all those values and divide by the number of them. Uh, the median is the middle number, right? If when we ordered them sequentially, what is the middle number? The Standard Deviation is tells us how spread out our data is. So here's a, here's a visual representation of the standard deviation. So here it says 99.7% of the data for this example case is within our, within three standard deviations of the mean. So if this data set looked much more spread, this, this distribution of data, then it could be up to 15 standard deviations or 20 or you know, a bigger number. So it's how spread our distribution is.

Speaker 1:          17:12          That's the standard deviation. And the way we find that is twoF is just take the mean and subtract each data points, square it, add them all up together, and then divide by the number of data points minus one, take the square root of the whole thing. That's our standard deviation. Kurtosis then uses the standard deviation to compute itself. We use that the STD to compute a kurtosis where and so the way this works is we take the mean and we say take each data point minus the mean over the number of data points and do that for as many data points as there are sigma notation and divide by the standard deviation. The fourth power. Now the Kurtosis tells us it's a measure of whether the, whether the data is heavy tailed or light tailed relative to a normal distribution. As you can see in this picture, positive Kurtosis is higher negative is lower normal distribution.

Speaker 1:          18:04          And the reason this is important is because it tells us how many outliers we can expect in our data. Data sets with higher Kurtosis tend to have heavy tails or at outliers, right? So, and the opposite is true as well. And then there come skew. So XQ tells us the asymmetry of our distribution of data. So this can be positive, negative or undefined. And lastly, kwan tiles are basically ways of of dividing our, the, the, the range of probability distributions in our data. And we can have several Quan tiles. It's just a way of segmenting that data out and there's a lot of statistical features out there. We could have added more and it, it would, you know, it would affect our prediction in different ways. There's no way to fully know that. I mean there's model evaluation techniques. We'd have to try it out.

Speaker 1:          18:53          That's, that's part of the fun, right? So, so there's that. Now in this line, we're going to add all those features to this dataset. Once we have added those features to our Dataset, we can visualize the completed datasets with the new features that we added using the describe function. So there we go. We've got our new features that have been added to our dataset. Boom. All right, so now onto step five, cat boost. So capp boost, let's start with gradient boosting because cap boost is a type of gradient boosting. So grading, boosting, it's a technique that can be used for both regression and for classification problems, both types. And it produces a prediction model in the form of an ensemble. That means many a collection of weak prediction models. Typically they use decision trees. Now I want to note that gradient boosting is not specific to decision trees.

Speaker 1:          19:47          We just tend to use decision trees because they're easy to implement and they tend to give good results. But this say generalizable technique that can be applied to any type of model. Here we go for regression. The idea, and let's talk about it in the context of decision trees for explanations sick. So with gradient boosting, we start off with a single tree and we are training it on some distribution of the data and it's going to build a decision tree out of that data, right to to be able to make a prediction about that data. Once we have that decision tree, it's not going to be perfect. They're going to be data points, parts of the distribution that are off. And that's the hidden state. It could be better. So we take that hidden state and we use it to create yet another decision tree.

Speaker 1:          20:29          And so the idea is that we're focusing on those data points that are not as, um, encompassed in the regression line or the regression model of the decision tree. And we're going to focus on those data points and more in the next decision tree. And that's going to create a hidden state, a representation of the data. And we're going to do that again. So we iteratively do that. And so the equation then becomes h of x, where x is all of the data is equal to the first hidden state of x plus the second hidden state of x, which are the hidden states of each of these trees. So in general, boosting works by iteratively learning week classifiers and adding them to a final strong classifier. And after a week learner is added, the data's reweighted so that wrongly classified examples gain weight and correctly classified samples lose weight. So in this way, the weak learners are trained to concentrate more on the misclassified examples as we can see in this image right here,

Speaker 1:          21:27          right? So, uh, great and boosting. It's a type of grading dissent to algorithm. And we can see this pseudo code explains it very well, right? So we have some data sample distribution d and so for m equals one two M or m is a number of base models. We want to use, that's up for us to decide. We can train our base model from the training sample distribution, compute the error, and then adjust the distribution of the next model to make the mistake of the model more evidence than we output the constructive based model and use that as the input to the next iteration for em, uh, models, right? So however many models we decide that we want to use, and that's the basic idea behind a gradient boosting, iteratively improving a bunch of weak learned weekly learned models until we get a final strong model that is better than the sum of its parts. So now onto cat boost, which is a version of gradient boosting. There's actually several types of gradient boosting techniques. Xg boost is one, you know, add a boost there a lot.

Speaker 3:          22:27          Okay,

Speaker 1:          22:28          so Yan index is the Russian Google and they use gradient boosting a lot and they use it to power a lot of their services. So the idea is that for, for cat boost, cat booze stands for categorical gradient boosting. So what they do is

Speaker 1:          22:45          they say, let's handle categorical features automatically. So without any explicit preprocessing to convert categories into numbers, cat boots will convert categorical values into numbers using various statistics and combinations of the existing categorical features and combine both the categorical and numerical features. And so there are some advantages. There's uh, you know, usually tends to perform better than the other types of gray and boosting. As you can see here, it has the lowest log loss values of the gradient boosting algorithms that we see here. So it's, it's really popular, it's getting more popular. And the Eli five of this, as you can see here is we model the data with simple models and then we find the errors. These errors signified data points that are difficult to fit by the simple model. And for later models iteratively, we focus on those hard to fit data points to get them right. And in the end we combine all the predictors together by giving some weights to each predictor.

Speaker 3:          23:42          Okay,

Speaker 1:          23:42          so let's go ahead and implement this now here our handy, uh, psych it learn library. Thank you. Psychic learn where we'll use this pooling keyword to collect the training data, both the input and the output data. And then we'll initialize our regression model using the regress regressor keyword. How many iterations? Let's say 10,000 our loss function is going to be as a Kaggle competition suggest mean absolute error and the type of boosting we're going to do, which there are many types is going to be ordered. And now we go back and we will fit the model to the training data and the testing data.

Speaker 4:          24:43          Okay.

Speaker 1:          24:45          And printout the best score. And now it's training. And then when we see our score, we'll see that our mean squared error is 1.8599999999999999 which keeps us in maybe top 300 to top 500 it could be better. But my point is that this is our first model, our first attempt. So obviously it's not going to be the best and we'll just iteratively improve. Right? So that's, that's the idea, right? So

Speaker 4:          25:11          yeah,

Speaker 1:          25:12          then we could, you know, submit this for more. So let's get on. Let's go on to step six, which is implementing arch support vector machine and the radial basis function colonel. So in simple regression, we're trying to minimize an error, right? So regression models try to find the relationship between variables, right? One or more variables. And in a support vector machine for regression, we're trying to fit the air within a certain threshold. That's the difference. So

Speaker 1:          25:38          the idea is that the blue line can be considered what's called the hyper plane. The red lines are the boundary lines. So the Blue Line is the line of best fit, where the red lines are boundary lines that we're going to decide by some value, they're off from the hyper plane and we're only going to consider data points within this margin. So only those data points, there could be data points all over the place. And so simple regression will take into account all of those data points. But support vector regression will only take into account the data points inside of that margin, right? And those margins are defined by the support directors, which are the data points that are closest to the line of best fit. So

Speaker 4:          26:18          okay,

Speaker 1:          26:18          that's our objective with the support vector regression and the boundary lines. We draw our either plus or minus e distance from the hyper hyperplane where he has some value that we defined. So we can define the equation for our boundary line as such, it's going to be uh, where you know y equals mx plus B is the line. So, uh, we'll, we'll just add or subtract eight from that equation and going to give us our line of best fit. So we're only so notice how there are some data points outside of that, but we're not considering, those were only considering and fitting our data to the, to those data points inside of those boundary lines. And that's a support vector regression.

Speaker 3:          27:00          Okay.

Speaker 1:          27:01          So a lot of times we have what's called a non linear decision boundary. That means that a straight line cannot fit the data, which means our support vector machines not going to work well and the weight. And so on the left, as you can see here, you know, so we ideally we'd like to have the data points separated such that if we were to classify or to regress the data, we would be able to linearly separate it or linearly create the line of best fit. You know, how do you make a line of best fit for that data over here, it's hard. Uh, it's impossible actually, uh, in, in, in that space. So what the solution to that is to use what's called the kernel trick and the kernel trick, the, a very simple way of thinking about kernels are they are a similarity function given two objects, the colonel will output some similarity scores and we can use those scores to reframe the problem such that a linear regression line or classification line can either separate or find the line of best fit for the data. And there are a lot of different types of kernels, right?

Speaker 3:          27:59          Okay.

Speaker 1:          27:59          So, right inputs based a feature space, there are many different types of kernels out. Therefore scms linear polynomial Gaussian sigmoid. Uh, but in this, in this, he's Galcion were radio basis as we can see here, but they're basically the dot product of two vectors. We can think of it that way. And so this is the equation for the support vector machine, right? Where we have a weight value that is learned through some optimization technique, light gradient descent. And then we have the mx plus B, you know, the, the line of best fit format as well. And these are supported vectors. So that's the basic idea behind a support vector regression. And that is not as used. It's not used as much or understood as much as a support vector machine specifically for classification. So I thought I wanted, I wanted to shine some light there on that, right? It's usually used for classification where a linear decision surface called a hyperplane is used to separate these classes.

Speaker 3:          28:55          Okay.

Speaker 1:          28:56          So what we're going to do is we're going to uh, build the support vector machine using psychic learn and then we're going to pick the ideal of hyper parameters using a hyper parameter optimization technique called grid search, which you're going to pick a bunch of values for. Each pair of values is going to evaluate the validation error function, then pick the pair that gives the minimum value of the validation error function. It's basically and try out the pears until it finds the best pair for, for our model. And so what we can see here is that I've scaled those values. I'm using my support vector, regression, str, and these are my, you know, the, the possible values that I've defined for my grid search, high parameter optimization technique. So it's going to try out the pairs of gamma and see to try to find those, those ideal hyper parameters for our model so that it's, that's going to fit the data and grid search CV does that, the colonel is going to be right the radial basis function as we saw before.

Speaker 1:          29:51          And then we'll make a prediction and then we'll output the best CV score, which, which is right here. Now obviously we could do better. There are different types of models we could use. We only use two. But remember our learning objectives here, we're learning how to think like a data scientist at least a little bit. Uh, learning about the cat boost algorithm and learning about the support vector machine algorithm, specifically support vector regression. And we did that. So future ideas, uh, we could try recurrent networks, we could try genetic algorithms, we could try ordinary differential equations. And think about this as a time series where there's just irregular data, which Oh, nets are great for. So there's a lot we can do and it's super exciting. I hope you found this video useful. I'm going to have a bunch of helpful links in the video description. What's a data science model you want to learn more about? Let me know in the comments section and please subscribe for more programming videos. For now, I'm going to find a lasso. So thanks for watching.