Speaker 1:          00:00          Hello world. It's a Raj and Yolo. You only look once. That is the demo. For today's video we're going to build an object detection algorithm called Yolo. You only look once for real time video so you can feed this thing on Avi Mov any kind of MP four file and it's going to be able to in real time like this demo that you're seeing behind me that I'm running off of my laptop in real time, be able to detect what objects are where. Now this is one of the most popular and one of the most new techniques out there for object detection. So in this video I'm going to go over a little bit of the history of object detection techniques just over the past decade or so so we can really see why Yolo is so impressive. Then I'm going to go over the architecture of Yolo and then at the end we're going to go through the code and we're going to use it on both a pre trained model and we're going to learn how to build our own Yolo model as well.

Speaker 1:          00:57          Okay. So I hope you're excited for this cause I am. Right. So let's get right on into it. So let's start off with some object detection history starting from 2001 so in 2001 the first really good facial detection algorithm came out and it was created by two guys, Paul Viola and Michael Jones. And so it was aptly named the Viola Jones Algorithm. And this was the first time that facial detection really worked for a Webcam. And so, I mean object detection has been around since the sixties this is, this is nothing new, right? But this was the first time that it really worked better than anything else. And they're out rhythm did what all the other algorithms in general did before them. They hand coded in features and fed them into a classifier, namely a support vector machine. So it was trained on a Dataset of bases. Right? And they would, they would hand code the location of eyes of the mouth of the nose, and then their relations to each other.

Speaker 1:          01:54          Right. So as great as it was, as at detecting faces, it was really bad at detecting faces in any other kind of configuration than what the hand coded features said. Right? So wanted, once they collected these features by hand, by manually for each of these images, these features were vectors, right? And they would feed all these vectors together for a single image into a support vector machine. A classifier, right? I support vector machine can act as a linear classifier, a binary classifier. Yes, no, and that was it. Real face or not a real face. And this worked. This worked really well at the time. And here you can see some pseudocode of how it worked, but basically for each of the images they would down sample the image to create an image, which is, which in this case is actually a feature, right? And then they would iteratively add all those features up together.

Speaker 1:          02:44          They would concatenate them and then accumulate a set, a series of filter outputs and feed all of those outputs to a support vector machine. If you want to know how support vector machines work, check out my math of intelligence playlist. Okay. So then in 2005 a much more efficient technique came out, still using features, hand coded features, but it was called hog or histograms of oriented gradients and it was invented by it, a veep Delo and build tricks. So the basic idea of this algorithm was, was for for every image. So for faces, right? Namely they applied it to faces, but they also applied it to pedestrian detection on roads, which is really useful, right? For self driving cars, for traffic lights for things like that. But the idea was, okay, you've got an image of a face, right? You've got an image of a face.

Speaker 1:          03:30          And the goal is how dark is the current pixel compared to all the surrounding pixels. So for every single pixel in an image, they would detect how dark it was compared to the surrounding pixels. And then they drew an Arrow showing in which direction the image got darker. Okay. And so they repeated that process for every single pixel in the image. So every pixel was then replaced by an arrow. And this Arrow was called a gradient, not a gradient in the sense of backpropagation for neural networks, a gradient, just, they're just using that word for this, right? So the gradient show the flow from light to dark across the entire image. So this is what picture would eventually look like. It would go from a series of pixels, a matrix of pixels into a matrix of gradients or arrows, which point which direction things are getting darker.

Speaker 1:          04:19          And so then once they had that, they broke up the image into small squares, 16 by 16 pixels each. And then in each square they counted up how many gradients point in each major direction. And they were placed that square in the image with the Arrow directions that were the strongest. So in that, in that region, what direction was most prevalent? Right? And they would replace the square and the image with arrows in that direction. And the end result of this is that the original image was converted into a simple representation that looked like this. That was basically it captured the basics. The essence of a face, right? And so and so they can use this image to then given a new face, they can use some sort of similarity metric like you know, you know the Euclidean distance or any of the various distance metrics that we could use to detect how similar it was to whatever image it was given.

Speaker 1:          05:09          And based on some threshold value, they could say, yes, this is a face or no, this is not a face. So recall that this kind of basic feature map right, that they've generated looks very similar to what convolutional nets learned themselves. But in this case they had to hand code what this feature map looks like. So then in 2012 the era of deep learning began, right? This is when every so often in in computer science, an idea comes along that just makes you not only rethink your existing ideas but completely discard your existing ideas. And that's what deep learning was, right? So in 2012 for the image net competition, this is a yearly competition to train for, for researchers to try to, you know, try out their algorithms and see who can outperform the other Kurczewski and his team with Hinton use a convolutional network that outperformed everybody else for image net.

Speaker 1:          06:03          Right? So for this was in 2012 but the thing is that convoluted is convolutional nets had been around since the 90s why did they work this time? If you can answer, you get bonus points. There are two reasons. It's because of a, lots of gps and lots of data that's, that's a difference. They gave it lots of computing power and lots of data. Now the thing about humans, we're really good at classifying images, right? We're really good at this, but we do more than just classify images. So notice that whenever there's a classification task, yes or no, is it there or not? That's it, right? That's classification. Whenever there is a classification task, the images that we feed these models just have the, the the class in question as a center of the image. Like, if it's a dog, you'll just see a dog, maybe a white background, but it's just a dog.

Speaker 1:          06:54          But the thing is when we look around at the world, right, it's not just, we're not just classifying things, we are detecting things. That means that we are not just identifying what an object is, but we are classifying. But we're detecting the boundaries around that. Object to the differences of that object with the other objects in the images and the relations between all the objects. So the question is can convolutional nets as great as they are, a classification also help at detection. That is not just detecting whether or not an image is in a picture, right as a single image, but, but, but detecting if that image exists, what its relation is to other images. Can we build a bounding box around that image? Is the image partially occluded? But we can still see it? Yes. The answer is yes. Right. And this was proven by a team that was very recently.

Speaker 1:          07:47          This was proven. But one very basic thing we can do is we can say, okay, let's take an existing convolutional net and repurpose it, repurpose it as an object, a texture detector. How are we going to do that? Now let me tell you how we're going to do that. We can take any existing classifier, VGG net inception. These are huge convolutional nets that were trained on huge data sets, right by Google, et cetera. And what we can do is we can take our image in question. Let's say it's, let's say it's this image here and we want to detect her car and Vg Nett and obviously the tech a car because it was trained on car images amongst many other classes. What we can do is we can slide the classifier over every single, um, over a bunch of squares of that image, right? So we can, we can classify every single region as we define maybe a 12 by 12 pixel region for every single one.

Speaker 1:          08:39          And so as we're sliding it across the image, we're just continuously classifying every single box, right? Of this image. We're going to get a bunch of different classifications. We were going to get a bunch of different classifications and then we can only keep the ones at the classifier is the most certain about, and we can use that to then draw a bounding box around the image. But this is a very brute force approach, right? We don't, we're not just going to, this is a very computationally expensive approach. We don't want to just do that, that that's, that's done this. That's not a good way. There's gotta be a more efficient way of doing things. Right? So a better approach was invented just two years ago called CNN. Okay. So the idea behind our CNN was, before they feed it, they fed it to a convolutional network.

Speaker 1:          09:26          They would use a process called selective search to create a set of bounding boxes. Or in the paper they called them region proposals in an image. So at a high level, the selective search process process looks at an image through a series of windows of different sizes, right? These are just randomly size, randomly placed windows. And for each size it tries to group together adjacent pixels by texture, color, or intensity to identify objects. So given some input and image, the first step is to generate a set of region proposals for bounding boxes for however many we want by some threshold, right? And then once we have that, we run those images in the bounding boxes through a pretrained, Alex snap or whatever kind of CNN. We want to compute the features for that bounding box. And then finally a support vector machine to classify to see what the object in the image in the box, what the object, the image in the box is of.

Speaker 1:          10:23          Then we run the box through a linear regression model to output tighter coordinates for the box once the object has been classified. And this proved to be an effective approach for object detection. The most effective by repurposing a convolutional network for object detection, by using this selective search algorithm to create bounding boxes beforehand and then feeding all of those boxes to a CNN to compute a list of features and then, and then computing class values from them. So for RCA are for our CNN, there had been lots of improvements like lots of fast. Our CNN baster, our CNN mask, our CNN and I've got papers and links to them all here in the description. Uh, actually mask our CNN just came out this year. But yellow I think is the best approach and not just because of the name. Yolo is the best approach, um, because it outperformed all of them.

Speaker 1:          11:18          It outperformed our CNN and all of its variance that I know of. Maybe some guy just yesterday invented some new version of our CNN, I don't know. But as far as I know, Yolo is uh, the state of the art in object detection and Yolo, it takes a completely different approach, which is very exciting. All right, so check this out. So Yolo, Yolo does is it's not a traditional classifier that's repurposed to be an object detector like our CNN. What it does is it actually just looks at the image once hence its name. You only look once, but in a clever way. Let's say we've got this image right and we want to is tact all the classes that are in the image. We've got a dog, we've got a bike, we've got a car in the background. How are we going to do this?

Speaker 1:          12:01          So what Yolo says is let's divide up the image into a grid, a 13 by 13 cells. And so each of these cells is responsible for predicting five bounding boxes. So some part of a bounding box is going to intersect into that little square that the, that that is that little square, right? So it's, it's responsible for detecting up to five of them. Like it could be part of this one part of this one part of this one or this one, right, but five and so what a bounding box does, cause it describes the rectangle that encloses some object. Yolo is also going to output a confidence score that tells us how certain it is that the predicted downing box actually encloses some object. So it's not just PR trying to predict whether or not a set of bounding boxes exist. It's also each of these squares is predicting what the class is for each of the bounding boxes that it's predicted the existence of.

Speaker 1:          12:54          And so the score doesn't say anything about what kind of object is in the box. Just if the shape of the box is any good. So the predicting bounding boxes may look something like the following, the higher the confidence score, the fatter the box that is drawn. So notice for our Doggie, for our bike, we've got some really thick bounding boxes, meaning that it, it can tell that there is something significant there. So each of the bounding boxes, the s for each bounding box to sell also predicts a class, right? It's predicting two things, whether or not a bounding box exists. And then the class of that bounding box, and this works just like a classifier, it gives a probability distribution over all the possible trick classes that the network has been trained on. It's a convolutional network. So Yolo was trained on the Pascal voc data set, which has 20 different classes like bicycles and boats and cars and dogs, and the confidence score for the bounding box.

Speaker 1:          13:48          And the class prediction are combined into one final score that tells the probability that this bounding box contains a specific type of object. For example, the big fat yellow box here on the left is 85% sure that it contains the object dog. So based on some kind of threshold value, like 80% plus, we can just leave that bounding box there that we decide. So since there are 13 by 13 grid cells and each cell is predicting five bounding boxes, we end up with 845 downing box boxes in total. That's a lot of bounding boxes, but it turns out that most of these boxes are going to have very low confidence scores. So we only keep the boxes whose final score is 30% or more. Like I said, whatever threshold value we want. And once we do that, the end result is going to be this.

Speaker 1:          14:38          We're going to have only three bounding boxes left, and these three bounding boxes had scores that were higher than our threshold value, 30% and as you can see, that's exactly what we wanted to detect. But the, this, the key here, the secret of why Yolo is so good is because it's not just that from the 845 total bounding boxes, we only kept three of them, but it's that even though there were 845 separate predictions, they were all made at the same time, right? Yolo, you only look once the convolutional network is only looking once we are feeding all of these boxes to the neural network all at once, right? So it is a convolutional network, right? And the architecture is very, very simple. This is what it looks like, right? You've got input, convolution, pooling, convolution, pooling, convolution, pooling, right over and over and over again.

Speaker 1:          15:30          And there are actually several versions of Yolo out there, or there's Yolo. And then there's Yolo [inaudible] also called yellow, 9,000 better, faster, stronger. I know these names are just going crazy. I love it though. [inaudible] memes and signs together when total win, right? So, uh, this is what it looks like. Uh, and so by the way, we're going to be looking at yellow to just skip Yolo one. Cause Yolo too is obviously better. Uh, so, uh, we, what we're to do is once we feed our bounding boxes into our convolutional network, we're going to end up with 125 channels for every single Brits Self. And these 125 numbers contain the data for the bounding boxes and the class predictions. And the reason it's 125 is because each grid cell predicts five bounding boxes and a bounding boxes described by 25 data elements. And these are the elements, the x fly, that's the locations, the coordinates, the width and height for the bounding box was rectangle, the confidence score and the probability distribution over the classes using it.

Speaker 1:          16:31          It's very simple. You give it an input image, right? It goes through the convolutional network in a single pass and it comes out the other end as a 13 by 13 by 125, uh, sized tensor describing the bounding boxes for the grid cells. And all you need to do then is compute the final scores for the bounding boxes and throw away the ones that are scoring less than 30%. And so yola two is better because it's faster, it understands more generalized object representations and you can find a paper for it here. Okay. So what I want to do now is go through some code. So let's go through some code. This is, this is probably the best implementation of a Yolo too that I found on get hub so far. So, uh, originally the, the authors of Yolo wrote it in this, uh, wrote it in dark nets.

Speaker 1:          17:22          So dark net is a, is an open source neural network library. And See, I know why didn't they just use tensorflow, but they wrote it in dark net and um, yeah, it's written foresee and Kuta. It's fast. It's great if you, if you want to work with see work with, see, but basically the best implementation I've found is called dark flow. So what this guy did is he translated dark net to a tensorflow virgin and we can use dark flow to do several things. And I'm going, I'm going to do it with you right now. Okay, let's get started to hear right. So we can choose one of the following three ways to get started with dark flow. I've downloaded it here so you can just clone it, calling it locally, and then you can set it up. So the first step is to run setup.py. No module named Scythe on. Okay, so then I'll install Saigon. Okay, so that's the first step we're going to, we're going to set up cy thon and once it's done that, then we can install all of our dependencies. And the reason we're using Psiphon is because right darknet is written in c and we need some kind of rapper to introduce these c libraries to our python environment. So now I can go ahead and install Paul our dependencies. That'll take some time. Okay, that was it. That was our dependencies. And so now let's go ahead and download some weights.

Speaker 1:          18:50          Okay, so downloaded this, this pretrained weights and I'm going to open them. Tonya will awaits right here. Okay, now I want to load these weights up.

Speaker 1:          19:17          Okay. So let's look at this code for a bit. Um, all right, so for the code we can see that it is quite a big project, but if we look under net, we're going to see some of the versions of the different yellow implementations. So we can just go to, uh, let's see where we can go to. We can go to Yolo v two. Okay. So if we go to build.py, we can see the tensorflow implementation of dark net. So remember, because this was written in c, uh, we are not using tensorflow native functions to build this convolutional network. So when we go into the in function, we'll see that the s there are some, you know, boiler plate flags for how to train this network. Uh, and there are different ways of building this network. We can build it from a pretrained PB file and you know, that's how we save files and tensorflow as dot PB.

Speaker 1:          20:04          That's one of the ways. And when we build the Ford Pass, like this is kind of where the convolutional network is built. We have some placeholder that the input data can go through that is namely the image or the video, whatever it is. And then iteratively they're taking all the ex, the, the author took the existing dark net layers and they're adding them to tensor flow just like right here. Okay. So, and then we have a bunch of flags and then we have a way to save the file once we've trained it. So, but the basic idea is that we can use a pretrained version of this model that was trained in Si by downloading weights and then uh, using the train.py file to train it on our video or on our images. And then lastly, we can then we can then predict what the bounding boxes are going to be by using predictive, the predict our pi a class.

Speaker 1:          20:54          Right. Okay. So there are several things we can do. We can train our model using our own data set. We can use a PR, we can use pretrained weights, we can train it on images, we can train on video. But for this demo, why don't we just use a pre trend model that they already have, right? The author has weights that we can download and then we can train it on one of our own videos, right? So if I just paste this in, we can train it on one of our own videos and then we can see the results for ourselves, right? Just like that. And so now this is detecting all the objects in the video, in the video using yellow. It's awesome. And uh, yeah, if you want to do this yourself, it's pretty simple. I've got the code, the description, the slides that I was talking about, everything is in the video description and the get hub link. So check them out. And I hope that you found this useful. Please subscribe for more programming videos. And for now I've got to think of some Yolo maims. So thanks for watching.