Speaker 1:          00:00          Hello world. It's Saroj and let's train an AI to create a set of topics for any news article that we give it. We can frame the machine learning process a lot of different ways. Learning from data that has labels, learning from data that doesn't have labels, learning how to act on continuous data, but a more application focused way of framing. The learning process is as either a discriminative or generative discriminative models. Tell us what some data is. They discriminate, differentiates, classify, call it what you will. They ask the question, is this a picture of bitcoin or bitcoin cash? Seriously, what is going on there? What language is this text? Who is singing this song? What show is this? What type of object is this? Generative models are even more exciting. They generate new data, new images, new videos, new music, new texts, which reminds me of a video by another youtuber named Carey.

Speaker 1:          00:57          For now, just know that there are two neural nets, a generator and a discriminator that are each co-evolving evolving to outsmart the other. Definitely subscribe to him, put more formally. Most of the advances in machine learning have been in discriminative models where we try to estimate a function called the posterior probability. That is the probability of y given x where x is an input sample and why as an output. So we can imagine x to be an image and why to be the kind of object that is in the image like a Tesla. It tells us how much the model believes that there is a Tesla given an input image compared to all possibilities. It knows about generative models. Instead estimate a function called the joint probability, the probability of y and x. That is the probability that x is an image and there is a Tesla in it.

Speaker 1:          01:48          At the same time, the reason we estimate the joint probability for generative models is because using it, we can generate images of Teslas by sampling car types. Why and new images acts from the probability of y index. There are so many awesome generative models out there that we could go over. Auto encoders, try to reconstruct the input data, then we can use it's learned dense representation to generate new types of similar data. Generative adversarial networks consist of two neural networks, one trying to fool the other one by presenting fake but realistic looking data as the discriminator improves. So does the generator until the generated data is indistinguishable from the real thing.

Speaker 1:          02:45          But let's start with a relatively basic one called latent Dirichlet allocation or Lda Derris. Schley is a type of distribution like the Gaussian, it's specified by a vector parameter containing variables corresponding to each topic which we write as such. These are direct laid distributions for three different topics. The bottom triangle has a side for each topic and the closer a point on the triangle is to aside the higher the probability of a topic. The curb is the probability density function over the mixture of topics, so the edges of the triangle all have a zero probability. Just like the probability Facebook has a super intelligent AI sensationalist say yes, the word latent is there because a variable we have to infer rather than directly observe is called a latent variable. Since were directly observing the words and not the topics, we refer to the topics as the latent variables as well as a distributions themselves.

Speaker 1:          03:43          We are allocating Leighton Dera Schley distributions over text data. LDA is a way to automatically discover topics from a set of documents and was created by Andrew [inaudible] who popularize it in his Coursera course on machine learning. One of the things I learned both when I was um, running the Google team and now building a team at Baidu topic modeling isn't efficient way to analyze large volumes of text data. There are many use cases for it. Search engine rankings, analyzing how research topics evolve in a conference, finding out the interests of different users using their tweets. Suppose we have a collection of sentences. We could use Lda to automatically discover topics that the sentence is contained. If we want to topics, then the first topic would be a list of words, each with percent values that indicate how relevant they are given a topic in that category. The same goes for the second topic. Then it assigns percent values to each sentence to define how relevant each topic is to that sentence.

Speaker 2:          04:51          I think so. I'm not sure what it was. I conditioned this result on a couple of different data sets. So you applied for it. Yup. I love it. Well, we shouldn't submit yet until we can replicate and we got two weeks left. I'm going to write the paper.

Speaker 1:          05:08          We can think of. This whole thing has a three step process. We tell the algorithm how many topics we think there are. The algorithm will assign every word to a temporary topic. Then the algorithm checks and updates the topic assignments looping through each word in every document. Now it's look at it programmatically. We can import our documents using the pandas library to load our data points into memory as a data frame object. We're going to perform some cleaning steps here straight out of Compton. I mean the natural language processing playbook. Since our words matter, three steps, tokenizing, stopping and stemming tokenizing segments. A document into individual tokens that we define the size of. Since we're applying Lda at the word level, we'll tokenize or document into words. Once we've converted our documents, a sack of tokens, we'll want to remove stop words. These are the meaningless words that won't contribute to our topics like for an or and art history majors.

Speaker 1:          06:08          The very definition of a stop word is flexible. We can define that list ourselves or we can use a predefined list. It also depends on the topic or modeling. If we're modeling a collection of music reviews and been terms like the who will be hard to model since the will usually be removed, even though it's a part of the band name. Next we'll perform stemming. This combines similar tokens in our case words into a single token like a a an a could be reduced to a. Now we want to create a document term matrix because we need to understand the frequency with which each term occurs within each document. The rows are docs in the collection and the columns are the terms will traverse through our documents, assigning unique integer ids to each unique token while also counting words. All of this will be stored in a dictionary.

Speaker 1:          07:02          Now we're ready to generate an LDA model using this document term matrix. We'll iterate through every document we have in our dataset and randomly assign each word in the current document to one of our topics. We'll go through each word in the documents and for each topic compute to probabilities. Then reassigned a new topic. By using these probabilities, the probability that we place a word in the correct topic will increase every time since we will place more and more words into the correct topic, we can examine the results by printing out the number of topics we chose as well as a number of words per topic and these topics seem very news related and the words seem to be very relevant to the topic. So I think we did a pretty good job to summarize, generative models learn a joint probability distribution between input data and the labels.

Speaker 1:          07:55          While this discriminative models learn the conditional probability distribution between them. Topic modeling is all about finding the hidden thematic structure in a collection of documents and latent Dirichlet allocation is a generative topic modeling technique that can be implemented pretty easily. Hammad shake is the wizard of the week. He created a linear regression model using a Bayesian approach to find the relationship between movie sales and movie ratings. This is very likely the best made Jupiter notebook on the topic. On all of get hubs. So take a look at it for sure. Very well done. And the runner up is Noah Dell, who used Basie and optimization to find the learning rate for a neural network that correlates bitcoin search frequency and is price also. Definitely check that one out. Great work. No. This week's challenge is to perform Lda on a text data set of your choice bonus points for real world use cases. Poster. Get help, link in the comments and winners will be announced next week. Please subscribe for more programming videos. And for now I've got to find a better topic. So thanks for watching.