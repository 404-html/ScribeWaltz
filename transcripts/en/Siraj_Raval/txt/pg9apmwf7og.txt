Speaker 1:          00:00:03       Hello world. It's the Raj. How's everybody doing? I'm in this new space. I'm at the upload VR space, so I've got this whole code behind me things. So it's not all ghetto with me and Google hangouts. So I'm very excited about that. Let's see who is in the chat room and let me give my shout outs to people who showed up alive. And then we're going to do our five minute Q and. A. Okay. So we've got Brandon, we've got a, we've got three Harshaw Kapala Aditya [inaudible] a lot of cool people in the house right now. Okay. We are doing this legit. Okay. I am done with the Google hangouts. I am here to make this quality for you guys. Okay Mohan, we've got a lot of cool people in here right now. Okay.

Speaker 2:          00:00:49       Uh,

Speaker 1:          00:00:50       so Ellery this is awesome. I know, it's very awesome. So I was inspired in part by Dan Shiffman livestream, you know, the way he does it. And so we're going to have to code happened behind me. Okay? So I'm here, this is my computer. We're going to have the code happen behind me. Okay? I finally got into this. Yes, thank you. I'm glad you guys liked the new setup. Okay, so Santiago and Veneet and Jay. And I'll say one more name. Joyce. Okay. That was it for the names. Okay, one more name.

Speaker 2:          00:01:19       Uh, uh,

Speaker 1:          00:01:22       who is it? There's so many names. So I'm going to choose done. D. A. N. That's a cool, that's a cool a named Don. And he said couldn't eat you bitches. Great. Okay. Um, but women are not that. Okay. So now we're going to get started with the code. Okay, let's do that. Well actually let's do a five minute Q and. A. What am I thinking about? So shoot me your questions and then we're going to do this. Okay. We are going to generate some music. We are going to generate some music. It's going to be dope. We're using tensor flow to generate music. Okay. Tensorflow, music generation is no joke. Okay. We are using a, an encoder decoder model for this. Okay. So what are you using for this? I'm using a open broadcasts or service, uh, obs to stream this. Okay. Okay. I, I gave Schiffman a shout out. Great Quality. Thank you. Okay. Questions about machine learning. Guys come on. Questions about deep learning. What's been burning your 10 serves in your mind. Okay. What do we got here? I know, I know we've got, we're going to get some good questions. [inaudible] are you going to use Gan? I'm not going to use again, but we can use again a for a music generation that is some very, very new stuff. Okay. I am definitely gonna do a license. I have had so many requests for gans. In fact, I'm not just going to be one game video. I'm probably gonna do like four or five. Okay. Cause there is a lot of stuff to talk about. Backpropagation with tensorflow. Raj backpropagation video specific backpropagation is coming within a week. Okay. Or two weeks, but it's coming the music. Okay. What do

Speaker 1:          00:03:00       we, what else can we use similarity detection on music sets? Absolutely. So remember if we take any dataset and we convert it into a vector, we can then find the similarity between these vectors because we were representing it mathematically. Okay. These are matrices. Okay. Uh, any sessions for Ghana. Okay. Lstm yes, we are using LSTM because we are trying to remember longterm dependencies. Okay. Music is very long. It can be multiple pieces. It could go through multiple phases. Uh, I will do Ganz genre for this music. It's going to be ragtime music. Okay. It's going to be piano, ragtime music. Okay. I'm going to take two more questions and then we're going to get started with this code, which is the best server to use to run python code all the time for sending some automated, uh, something. I, it just went away. Uh, the best python server, the best server, uh, APP would be Heroku in my, in my opinion, Heroku is, uh, the one, uh, Heroku is pretty reliable. Okay, so two more questions. Can we work on multiple Dataset to arrive at one result?

Speaker 2:          00:04:11       Uh,

Speaker 1:          00:04:13       yes, you can. Uh, you would want to create one big dataset using all the different datasets. Okay. And then, so one more question. What technical skills should I know you should know?

Speaker 2:          00:04:28       Uh,

Speaker 1:          00:04:31       so this specific one we're going to need to know a little bit of music theory, but I'll talk about that. Okay. Uh, and also, uh, tensorflow basic syntax. Syntax. Okay. So that's it for the questions. We have 307 people in this livestream. Okay. So we are about to get started right now. Let's do this deep reinforcement learning, not today. So, okay, what are we going to do here? So I was looking on get hub for a bunch of music generation code and I didn't. So I found there there are some, uh, that uses tensorflow specifically. So there are some, but the problem is that a lot of them don't compile. A lot of them don't compile. And why is that? Because for music specifically, music doesn't have good libraries in python right now that we haven't, it's not that we don't have good music, it's not that the bridge between music and computer science hasn't been built. It has, we've been working on this stuff for 40 plus years. The, the bridge that hasn't been built is between, up is between python three tensorflow and python music libraries. There is Magenta, but Magenta is okay. So first of all about Magenta, I admire Magenta. Let's, let's look at Magenta, right? Let's look at, let's look at Magenta for a little bit. So Google has this effort too.

Speaker 2:          00:05:50       Uh,

Speaker 1:          00:05:52       talk about music generation intenser flow and I really admire Magenta, but what I think needs is better documentation. I mean to me, I mean when I see this, it's just like run this, you know, it's essentially like a look at this ml generate, it's essentially like a bash script. It's like a bash script. Okay. And you, you don't want to bash, I mean if you're going to make a bash script, you might as well have a web app. What we want is modular code, like some simple examples, similar to how Karen has simple examples, maybe 10 to 15 lines of code. So while I admire Magenta, I think we need better documentation for it. Okay. So in the models folder for Magenta, for the Melody Rnn, it has you generate a melody, right? So, okay, so this is the file melody. RNN. So let's, let's, let's take a look at melody RNN for a second.

Speaker 1:          00:06:42       That melody RNN module we have, let's see. This code is showing up behind me. Isn't that so cool guys? This is amazing. Let's say there's a lot of stuff happening here. So I mean, look at this. We have Magenta Dot Prodo buff dot generator, magenta.music.one hot events sequence encoder decoder. There's a lot of stuff that is, is encapsulated and abstracted away. Where is the actual RNN? Where are the hyper parameters? Where are the layers? Okay, there is too much code for simple models. Okay, I agree with Brandon. There is too much code here. This is not a problem of Magenta alone. This is a problem of all music, machine learning models. This stuff is not trivial because we are talking about music theory here. There are people who study music theory fulltime who don't fully understand how to compose music by there are human minds, okay? And yet here we are doing it with machines, but we can definitely do it.

Speaker 1:          00:07:37       It's definitely possible. Okay, so that's enough about Magenta. Let's talk about the code that we're going to use. So I found this code, okay. And there's so much code, right? There is so much code here. So, uh, so I found this code and this is what this guy tried to do. Okay? So what he tried to do, and we're going to write code, we're okay, we're going to write code, but before we write code, I'm going to talk about, uh, I'm going to talk about what he did here. So his first idea was to try a basic recurrent network. And why did he try that? Because it's a sequence of notes, right? Sequence notes. We use recurrent nets. It makes sense. Predict the next sequence, okay. That, that makes us, that makes sense. Right? So he tried that. Okay. He's a basic recurrent nets and he was, he was inspired by Andre, her poppy's blog posts, the unreasonable recur. That's, by the way, Andre Carpathy, a n d r e j carpathy is amazing. And you should definitely look up anything he's written. That guy's awesome. Okay. He has, he knows his shit. So he tried a basic recurrent network and he was, we're not using Karratha using tensorflow. He used a basic recurrent network using Q LSTM cells and he trained it on ragtime music. And let's listen to what he generated using just three songs. Okay. So he just traded on three songs. Let's listen to this.

Speaker 1:          00:09:09       So it sounds great, right? It actually sounds pretty good. The problem is that it was over fit. In fact, this song that was generated, what's an exact, almost an exact replica of one of the songs that it trained on and that is overfitting, right? So that is a problem. We don't want to overfit, we don't want to just repeat whatever we've learned. We want to create novel music. So what he did was he said, okay, instead of giving it three songs, let me give it 400 songs. So then he gave it 400 songs. Okay. And so when you gave it 400 songs, this is what it generated.

Speaker 1:          00:10:02       Okay? So that one's not that bad. Actually using a recurrence using a recurrent network, uh, with two LSTM cells on 400 songs. It's not that bad. But there was a problem. So the problem that he found was that when he, even when he used 400 songs, uh, the, the networks still didn't get the, uh, the relationship between notes. So in music theory, so let's look at some music for a second. In music, in music, we have scales, right? It's music, sheet music. We have scales, right? So notes are related to each other. Each note is related to each other. So we start off with a note, you know, like look at this first note here. Uh, right. So this is a c and then the next note is a B. So it's down one, one half step. Okay. Is this the short little lesson on music theory?

Speaker 1:          00:10:51       These notes are in relation to each other. They're just ways of mapping out pitches. They're ways of mapping out sound waves. Okay? And this is how we write out those differences in, in sound waves, there's differences in pitches. What his model did was it couldn't recognize that there are, that one note is actually related to another note. Okay. So this is actually one half step down. One step up. And He, and this was, that means that if he inverted the nodes, it would still predict the same thing. So, you know, if it was c then B it would, it would output something that was similar to as if we were to input B and c. So even if we inverted the notes, it would still still output the same prediction. So to fix that, he thought, okay, how can I improve this model even more? So then he said, okay, so to improve this model even more, we're going to use a different architecture entirely. We're going to use an encoder decoder architecture. Okay. So I haven't actually talked about this before and I will. The next, uh, you know, uh, two weekly videos are going to be talking about the encoder decoder architecture. But to give a brief overview just for a second, uh, and I'm sure most of you actually know what this is, the encoder,

Speaker 1:          00:12:06       the encoder decoder architecture it takes to recurrent networks. Okay? So look at this green, uh, set of squares and this yellow set of squares. They are two different recurrent networks to different l s t m or current networks, and each serves a different purpose. One is the encoder. Okay? The screen went is the encoder. It takes an input sequence and we feed it into this encoder. Okay? The second one, the yellow one is the decoder. So you might think, okay, so we just take that incoders output and we input that into the decoder. No, we don't take the output because this is not what we do is we are trying trying to take the hidden state. Okay? So if you look at this fourth square right here and it goes up and it says w we were trying to take that hidden state because we're not trying to take the output because it's not a classification.

Speaker 1:          00:12:54       We don't care about classifying, we just want that hidden state from the encoder. We take the hidden state, which has a set of vectors that it learned feature vectors and we take that hidden state and we put it into our decoder. Okay? So we put it into our decoder. We feed that, we feed the state itself into our decoder. And then once it's, once we feed that input into our decoder, the decoder will then take that a feat that vector and then, uh, decode it into, uh, uh, a note. Okay. So the first, uh, we take the first hidden state and we put it into the decoder and the decoder takes the hidden state and it outputs the next, what would be the next, uh, uh, note in the sequence or the next a value in the sequence, whatever it is. And so why do we do this?

Speaker 1:          00:13:38       Why, why have an encoder decoder architecture in general instead of having just one, uh, uh, recurrent network? Well, this for the same reason that we have different networks for different tasks, like we have convolutional networks that do, you know, a certain task we have, uh, that, that look at that look for images. We have recurrent nets for sequences. We have a feed forward nets for binary classification. And think about even our brain. Our brain isn't just one neural, one type of neural network. It's several types of neural networks. So having an encoder, decoder architecture, let's these networks specialize, and having them specialize increases the predictive capability of these networks because there are so specialized on a task. Uh,

Speaker 1:          00:14:27       okay. So because they're so specialized, uh, they, they perform different tasks. Well, okay. So that's, that's the high level and it, and so he tried that and it worked even better. So that's the high level of what he did. Okay. And it was two recurrent networks. One was an encoder and one was a decoder. And now we're going to, uh, look at the code. So there's actually a lot of code. Oh, so this, so there's, there's a lot of code here and uh, you know, for different modules, one for the, uh, connector and for the keyboard. But we'll, we're gonna do is we're going to just write out the meat of the code. Okay. We're going to write out the model itself and then we'll talk a little bit about more about what else is happening here. Okay. So let's start off by just writing out. The model itself will write out this in coder decoder model. So let's start off with importing our dependencies here. Let me make sure that this is visible.

Speaker 2:          00:15:15       Okay.

Speaker 1:          00:15:18       And remember the code itself is in the, the code itself is in the description. So let's start off with our dependencies. Okay. We're going to start off with a few of his dependencies that he loaded here. Uh, and I'll explain what each of them does, so,

Speaker 2:          00:15:38       okay.

Speaker 1:          00:15:38       Okay. Um, so, okay, so this module loader is going to do several things and we'll, we'll talk about the module loader when we get to it. But this one, this next, uh, help our class is going to predict the next key, the next key in the, in the sequence. Okay. So we have hello from Hong Kong. Okay, great. Hi. And so we have our keyboard cell and we're going to use our keyboard cell. So what this does is it just predicts the next key and we'll talk about that more. And we have one more help reclass uh, which is going to encapsulate the song data, uh, and so that we can run things like, so we can run, get scale, we can run, get relative. No, we can run a bunch of methods on our data. Okay. So this is kind of a, a data preprocessing. This is a data preprocessing step.

Speaker 2:          00:16:28       Okay, thanks Jerry. Okay.

Speaker 1:          00:16:32       Oh, so he is the original author of this code. Okay. So this is the best a code that I, that I found. Okay. So, okay, so let's, let's start off with this. So we're going to import deep music, uh, using, uh, songs, struct as music. So let's, let's write this out. Okay. So these are our, these are our only dependencies. And there, uh, there's not a paper. There is a writeup. I'm going to link to that when we get to it. Those are our dependencies. Let's write the code.

Speaker 1:          00:17:02       Okay. Do we have to pip these? Uh, yes, we're going to have to pip them. Uh, but I'm not the specific, these are helper classes. You don't have to pip these, but you do have to pip tensorflow and, um, the, uh, concerts, whatever the name is in the, read me of the code in the description. Okay? So let's do this. Let's run, let's write out our code. Where was I? Okay. Okay. Here we go with this. Uh, okay. Oh, there's a, what else do we got? Oh, there's, there's actually two more dependencies. We have a num Pi cause we're going to generate random numbers, generate random numbers, and there's of course tensorflow, tensorflow for flow. Okay? So the first thing we're going to do is we're going to write out the method for building the network. Okay? Uh, actually Dev Patel looks like me. It's the other way around.

Speaker 1:          00:17:59       Okay? So let's be, let's build our network and a, let's build on that work. So the first thing we're gonna do is we're going to create our computation graph. We're going to create the computation graph and it's going to encapsulate art tensorflow session and the graph initialization. Okay? So we're going to initialize our graph. And so anytime we use tensorflow, we create a session and a graph, right? These are just basic initialization steps. We do it every time. And the great thing about these helper, these a helper libraries is this guy use the module loader to, uh, build this batch builders dot get module. Okay? So what does this do? This, this line right here basically just encapsulates that a graph creation and that, uh, that uh, session creation. Okay, so that's the first step. So we have our computation graph initialized. Now we want to start building our model. Okay? So before we build our model or we need to create are placeholders, right? These are our gateways for data to flow into the network. So we'll create our first place holder. Okay. Our first place holder is going to be a for the inputs.

Speaker 1:          00:19:15       Okay. And these are going to be the notes themselves. This is for our notes, okay? This is far, no two data. We, we'll just call it no data, okay? So self dot inputs. And remember this is in Mitie format. It's mitty format. All right? So for self dot inputs, these are the inputs to our graph, a TF dot placeholder.

Speaker 1:          00:19:42       How can I get deep music? I'm in math class. It's okay, Bianca all look my, my brain's not even working today. It's all good. Okay. Does it just take it in cerebrally take it in and it just subconsciously you're going to start to get this stuff. This is, this is not trivial stuff, but just follow along and we're gonna, we're gonna do this. Okay. So, so now let's, let's talk about this. So we have our placeholder and a, the first thing we're going to do is we're going to say what, what type of data do we want to feed this? Well, this is going to be an integer data, right? This is numerical data because new mini me, I can't believe I called it mid. I format on, uh, the uh, video on, but thanks for correcting me and some guys directed me. So that was good.

Speaker 1:          00:20:26       I know one that I've never actually heard it said before, so it was like mid [inaudible] whatever. So it's numerical data. So we'll use a float 32. Okay. We're using a float 32 for this. And um, the next thing we're going to do is we're going to say what, how much data do we want to use? And so these are, are, uh, so these are placeholders for how much we want to give it to. Like, what is the size, like how many the batch, like how many batches are like if we have a basket of uh, data, how much do we take out at a time? How much do we take out at a time and feed into our model? Those are our batches and we'll define this with our batch size. Okay. Then which is going to be one of the arguments. Uh, but we, we are not going to talk about that right now. Right.

Speaker 1:          00:21:12       I'm glad you liked that Brandon. Okay. So, okay, where were we? So that's our, that's how much data and then we have, uh, we've, we've got one more thing actually. We've got the input dimension and then we've got the name will, what are we going to call it? We're going to call this input. We're going to call it input and clerk tensorflow as TF. Thank you. Aditya exactly. And the input and, okay. So that's it for our placeholder input. And so now that's, so that's our no data. Now we're going to define our targets. Okay. Yes. Okay. Now we're going to define our targets. What are our targets? They are the classes. Okay? So these are, um, these are, uh, whether or not a key was pressed. So this is a binary classification par 88 key binary classification problem. Okay? So each of, and this is not just, this is not inherent in the mini data.

Speaker 1:          00:22:09       This is just what we are feeding it. We're saying there 88 keys on a keyboard, right? We're looking at piano music and is that key pressed or not? If it's pressed, it's a one. If it's not pressed is a zero. Okay? So that's what we're going to define for our target values. So we'll say, uh, with TF dot. Oh, it's not tipped on name. Actually. It's tf.name scope and named scoping is a way for us to encapsulate some uh, uh, TF object and give it its own name so we can reference it later on. Spell check on song struck. Thank you. Struct as music. Okay. So with tf.name scope, we're going to say this is going to be our placeholder targets. I remember targets are for the, uh, the labels. This, we're going to consider this a supervised classification problem using an encoder decoder architecture.

Speaker 1:          00:23:01       So, so let's go ahead and talk about the targets that we're going to use for this one. So for this one, we're going to say, okay, we'll use a TFL placeholder like before, uh, and it's also gonna be an event. Uh, it, well, it's not going to be a flow is going to be an in 32 and why we're using an into 32 instead of a float 32 because it is he going to be either a one or a zero. So it's a less memory to use an event than it is to use a float. Also useful for tensor board. Okay. Okay. So, um, where were we TF in 32 so you're right, so it's going to be either zero or one. And so what else do we got here? We've got TF 32 we have to size. So self dot. A batch size. Okay. And that's the size of our, and uh, input at this actually goes over here.

Speaker 2:          00:23:51       Okay.

Speaker 1:          00:23:54       Okay. So what else we got here? We've got the target. That's the name of this thing, right? The name is the target. Oh my God, there is so much naming happening here. Yes sir. Re equals sign. Thanks Nick. Where's the equal sign? There's an equal sign somewhere. Isn't there? Uh, whatever. Okay. Okay. So uh, yeah. All right, great. So that's that and uh,

Speaker 2:          00:24:20       okay.

Speaker 1:          00:24:21       Okay. So that's that. And uh, wait to a teeny, are you serious? They're not for bias. Each in different languages, different languages. It's different for for what it is a float values or are they take up more space? Generally self dot targets equals self dot targets. Yes. That is the one. Thank you nick. That has an equal sign. Okay. So, okay. So guess what? We're going to use one more place holder because this is a recurrent network, we are not just feeding in our inputs. What else do we feed in to our recurrent network? We don't just feed in the input data when we're training, we also feed in the drum roll. Please think about it. Hidden state, our previous hidden state, we are feeding in both our input data and the previous hidden state. There are two things. So we're going to create one more place holder because this is a recurrent neural network and not a feed forward network. So we'll use the placeholder for uh, and they'll call it used preve. Okay. You should definitely bet on AI when choosing a career because you will not, you will definitely get employed while, uh, you will get employed. So we'll say, okay, so we'll use the previous and uh, I did import STF.

Speaker 2:          00:25:38       Okay.

Speaker 1:          00:25:44       So I guess float and [inaudible] are both four bytes. Uh, so I was wrong, but if you use a short, it is two bytes. Okay. But don't quote me on that checkup. You know, we, we could argue about this later. We'll see what the, you know, post the link or something. Anyway, so, so, okay, so the, for the placeholder for the placeholder value, this is going to be um, a TF dot. Boolean. Should we use the previous value or not? Yes or no. Okay. And then um, the name, uh, we'll leave the size empty cause there's not really a size associated with the boolean. And then the name is going to be use preve use previous, right? Yes. Okay. So those are our three placeholders. Hidden states. Okay. Are hidden state or no data. And then our target values. These are our labels. Okay. What else? What else got the keys? We got people rapping in this chat. So we have everything happening right now. It is chaos. It was absolute chaos. But that's what we like. We are agents of chaos in a good way. Creative chaos, constructive chaos. So the next step is to define our network. We are going to define our network. We have our place holders and we're ready to build the hell out of this model.

Speaker 1:          00:27:02       I can barely program in front of another person. You're a brave man for livestreaming. Saroj shotgun. Thank you. And you can, it just takes practice. It just takes practice. Like all other things. We have to train our minds to do these things. Okay. Self dot loop processing module loader. What the hell am I writing? What am I writing this? Anyone know? I don't even know what I'm writing right now. You know, it's just, it's crazy. It's crazy. No, I do know what I'm writing. What this is, is it's a part of the, uh, module loader class. And so what this is, is we are going to create a loop manually. Okay? Because we are not using care Ross, we are using raw tensorflow. Could you please explain what is going on? So that is what I am trying to do. So what I'm gonna do is I'm going to continue doing what I'm doing and maybe I'll even try to explain harder. So I'll turn up my experimentation. So, okay. Okay. So what we're doing is we're going to write a loop function and the loop function is used to connect one of the outputs of the network to the next input. It is the actual loop. So it is that code that takes that output and feeds it back in to the,

Speaker 3:          00:28:13       okay.

Speaker 1:          00:28:13       Uh, the, uh, input placeholder. So where are we going to manually create this loop? Okay. We're going to manually create this loop. This is too deep for me. Why is your hair black? We have some amazing comments in this, so, okay. Very entertaining. So, so you get the code, you get the entertainment. This is, this is what it's all about. These are why these live streams are awesome. Okay? So let's see, where are we going to feed this? And we're going to feed in the previous value and we're going to feed in the current value. Okay? The, we're taking over the two values for our loop. Okay? For our loop. What is after self, right? Okay. Ours, uh, we don't, uh, so that's, that's actually for the command line. So we could define, um, uh, if we want it to be a loop or not, but that's not the case here. So let's not even worry about that. Uh, we're not doing it from the command line or the command line has, we already have default values for that. So now what we're going to do is for our loop, we're going to say, well, we'll take our next input and we're going to use this loop process processing method to taking the previous value. So it will take in the previous value to predict the next input. Okay. And

Speaker 1:          00:29:25       Hayes, Raj, what age did you start coding? When I was 14, I was this script kitty who would try to do whatever I could. And what was cool at the time was to deface like forums and not deface, but like, you know, deface. It was so stupid. I was 14 also hacking halo two and stuff like that. So that was, that was when I first started. But when I really started coding was like when I was I think a 20, so six years ago. So, you know, we all, we all have to find our way. Uh, but now I'm full white hat I'm brown had actually, okay. So let's return some. So, so we're going to use the TF conditional,

Speaker 2:          00:30:09       uh,

Speaker 1:          00:30:11       object of tensorflow. And what this is, is it's a conditional statement. So it's going to either return one or the other up of the parameters that we give it. And I'm going to talk about what, we're going to feed this in a second. So,

Speaker 2:          00:30:25       so

Speaker 1:          00:30:27       I'm not going to get distracted. Okay. I'm not going to get distracted. So we're going to feed, let me just, let me just type this up and I'm going to explain exactly what I'm typing up. If you have a land of value here and we have one more input and that is how, okay. And then we have one more. Okay. So, okay, so on, so on training we're forcing the correct input and on testing we use the previous output as the next input. So this is a conditional depending on, uh, uh, on training versus testing. Okay. Training versus testing. Okay. Uh, training versus testing. So that's what, that's what this is. Uh, it's, it's, it's a differentiate between training versus testing, but we're just focused on training so we don't even care about it being conditional. Right now we're just focused on training lambda function. Lambda parameter. Hi. So many comments. I wish I could just respond to them all. But time is of the essence, isn't it? Backpropagation through time, back propagate to update weights. Okay. So now we're going to do the sequence of secret. Now this is the cool part. This is the actual model part. We have defined our placeholder values. We have defined our loop function. Okay.

Speaker 2:          00:31:48       Okay.

Speaker 1:          00:31:49       And and so now we're going to build our sequence to sequence model. So we're going to build our sequence to sequence model. This is it, this is it guys. This is it for the sequence to sequence model. So tensorflow, fortunately for us has a sequence, a sequence function built in. Okay. It has a sequence, a sequence function built in and we're going to talk about what it is doing, but let's write out what it's going to output first. It's going to output, it's going to output our prediction value. What is our next note? And we're going to minimize the loss here and I'll talk about what the minimalization, okay.

Speaker 2:          00:32:29       Okay.

Speaker 1:          00:32:30       Okay. Exactly, exactly. Questions happened at the end of the live session. Remember all your questions. I'm going to answer them at the end of this session. We are focused right now, so this is going to output our, our predicted note and our final state. Okay. And the final state is that final, a hidden layer for our decoder net network. And so,

Speaker 1:          00:32:53       okay, so let's build this thing. So we've got our sequence to sequence model. Okay. And we're going to build our decoder and get, okay, so we're going to build our decoder and I'll talk about our encoder in a second. So the decoder is going to take a series of a perimeter inputs. So the input is going to be, are the inputs that that came out of our encoder. Okay? The, the inputs that came out of our encoder that, um, our encoder and the initial state is none because it's defined inside of the keyboard sell. Let me, because defined in keyboards. So I'm going to explain this in a second. Let me just talk this out. And a cell is going to be keyboard cell. It's going to be a little confusing as I type this. I've got one more line to type and then we're doing this.

Speaker 1:          00:33:47       We're doing this live as bill O'Reilly said, we're doing it live loop are n n bloop. RNN. That's what's up. Okay. That's our sequence to sequence model right there. Okay? We didn't define a loss function, we didn't define our optimizer. But with this one line we said tensorflow, we want to use your built in sequences sequenced model that is to recurrent networks that we could just write out every layer manually. We could write out the weights, we could write out the um, uh, each of the, you know, computations happening at each layer. We could write out the activation functions, but tensorflow has itself encapsulated this for us. And so the reason we are giving, we're defining a decoder here is because our encoder was already defined and our encoder was defined in our helper class and also by this loop, a function that we just wrote.

Speaker 1:          00:34:42       So the input to the decoder is going to be the output from the encoder. Okay? What is even happening? Okay? Okay. So guys, I also have a nother, uh, code sample that I'm going to show you at the end of this. That's going to make even more sense than this. Okay? But it doesn't compile that, that's a problem because some dependency issue with the mid, I almost said it again, the mini, the mini, uh, a python module. Okay? I'm going to show you another piece of code out this piece of code for this. Okay. Also, I have videos on music generation. I have to and build an AI composer and a generate music intenser flow. Okay. So, okay.

Speaker 2:          00:35:31       Yeah,

Speaker 1:          00:35:33       I don't understand a single line beginner. That's okay. That's okay. Eventually it's all gonna make sense. It's odd time for it. Now we're going to do the training steps, so, so,

Speaker 2:          00:35:45       okay, hold on. Okay.

Speaker 1:          00:35:56       Okay. So now the training step. So for a training step, we're going to say,

Speaker 2:          00:36:01       mmm,

Speaker 1:          00:36:02       let's define our loss function training step. So let's define our loss function. Let's define this thing. So, um, for our loss function, we're going to say,

Speaker 1:          00:36:16       let's define our loss function. Well, we could just manually write out, so first let's talk about what loss function we want to use here. So, because notes can, can occur at the same time, right? Let's, let's listen to this for a second. So you notice that some notes are pressed at the same time, right? So it's not a, it's not just a a classification problem. It's a multi-class class classification problem. We want to output multiple notes at if necessary. Right? And so because we, it's say multi-class classification problem, we're going to use it cross entropy as our loss function. So when we have multi-class classification appreciated Nikki, channel 13, when we have a multi-class classification problem, we then use a cross entropy. Okay. If we don't then we were, then we're going to use a sigmoid, you know, a softmax function but because a B but because

Speaker 1:          00:37:19       steel melts at 300 degrees, what do you guys even talking about? Okay, we're talking about music guys. Okay. So, uh, where were we? Because it's multi-class classification. We're going to use cross entropy as our loss function and cross entropy basically measures the difference between two probability distributions and the two different, or it can even be multiple distributions. And these distributions are for each of our predicted values. Each of our predicted notes and the reason we're using cross entropy is because, uh, I'm can have, we can have multiple notes and we want to predict the difference between a different notes. Uh, I appreciate it, VJ. Okay. Cross entropy. Cross entropy. And would it be easier if it was monophonic yes, yes, it would be easier. Absolutely. Um, but

Speaker 1:          00:38:12       we don't like easy things do we? So, uh, let's write out this function. So guess what tensor flows like. Okay. So Raj, I know you want to use sequence a sequence. So what I did for you Raj, is I said I'm going to build in a sequence loss function into the sequence, the sequence module of TensorFlow's neural net class and hold to all your questions. I'm going to answer it after this. So we have our outputs. Okay? So, so it's saying like, I know you're using sequence a sequence and let's define a loss function for that built in sequence, the sequence model you're using. So if you're going to use a module of tensorflow, lite sequence sequences, sequence, use it for everything, use it for your loss, use it for your, uh, optimizer and all that stuff. Difference in what, okay?

Speaker 2:          00:39:03       Okay.

Speaker 1:          00:39:04       Okay, let me, let me just check on what's going on here. We have 366 people live now, so we have even more people. So this is a good sign. This is a good sign. Hi everybody across the world. You are awesome. Thanks for watching. So,

Speaker 2:          00:39:17       uh,

Speaker 1:          00:39:21       we have our outputs and we have our targets. So what it's going to do is it's going to, we have a sequence of notes, we have a sequence of notes and it's going to sample a sequence of notes, right? So it's going to, so we have a huge sequence of notes, right? So we're going to give it like in batches, say the first sequence right here and it knows what the next notes are going to be. It already knows what the next notes are going to be. So it's going to predict, given a note what the next note is. And then the difference in value between the predicted no. And the actual note is what we want to minimize with our loss function. The loss function will minimize the difference between these two, uh, notes and minimize. By minimize, I mean we represent these notes as numerical values.

Speaker 1:          00:40:02       So a c would be something like, you know, 0.2, three, whatever. And the actual note is B, which would be like 0.3, seven. So we want to minimize that, the difference between those predicted output. So the idea is that we have a huge sequence of notes and starting from the beginning in batches, we feed it in batches of notes, sequences, and we're predicting what's going to come afterwards and over time we're going to minimize this loss so that the predicted next note and the actual next note or going to be very similar then when we feed it a, when we, when we tried to generate novel sequences, is going to generate novel notes in the style that it already, uh, it already generated. Okay. So we have our outputs, we have our targets and now we're going to generate, we're going to not generate, we're going to, uh, define our loss function has cross entropy. Okay. It's cross entropy. And so it's saying, so the sequence sequence sequence, loss of function is asking for a function to use with both of our, um, with both of our sequences. That's why it's part of our sequence, the sequence module. Okay. This is the same loss function.

Speaker 1:          00:41:18       Wow. Gregory, I really appreciate that. It's the same loss function for both of them. I am going to use cross entropy for this with logics. Okay. Such a long unnecessarily long name. I mean with lodge it's what even is with logics. I I, I mean I, I I looked this up before, but uh, now you guys can see like how I searched for answers. I just like Google it. I'm having, I'm sure all of you guys do as well. But stack overflow is a great largest simply means that the function operates on the unscaled output of earlier layers. And that the relative scale to understand the units is linear. It means in particular the, some of the inputs may not equal one and the values are not probabilities. Oh, okay. So it's, it means that the, some of the inputs aren't, don't always have to equal one, uh, because it don't because it um, and the values because the, because the values are not probabilities. Okay. So, uh, that just means that our mitty notes can be like 1520. Right? So anyway, so cross entropy, that's the, the, the name of the game. Cross entropy. Okay. So now we have two more.

Speaker 1:          00:42:32       We have two more.

Speaker 2:          00:42:36       Okay.

Speaker 1:          00:42:37       Uh, functions here. Time steps equals true. We have, what else do we got? We have average across batch. Okay. How long has this session can be? It's going to be a 15 more minutes Max. Okay. We are trying to cap it at an hour and we're almost done. All right. We'll have a five minute Q and a at the end, but of course we're going to sample this and we're going to run the training. We're going to do it all. It's going to be amazing. Okay. I know I'm a little cut off. It's all good. This is our first stream. Okay. We like this, this high quality stuff. Okay. Uh, and so average across batch equals true. Now these are just, um, okay, so we want to take the average value across the batches. Oh, wait, average across time steps. In fact, I honestly don't even think that we should even be, we should even need these two parameters. Uh, because

Speaker 2:          00:43:43       yeah,

Speaker 1:          00:43:44       because uh,

Speaker 1:          00:43:54       we should just by default a want to use the average values. Unless we're really, really fine tuning this thing, we, we don't need to do it. So that's our loss function sequence. A sequence, a cross entropy. Now we're going to minimize it and we're going to minimize, and this is our last line, I'm going to go over everything I just wrote right at the end. So if you stayed, if you stuck through this long, you are a hero or heroine. If you start through this long, you are amazing because we are now at the last line of this relatively complex code. Actually, if you are looking at this stream, you are looking at the bleeding edge of computing. If you are even able to understand 10% of what I just wrote, you are like one of the top people. Okay? You have that top potential. First of all, you have the interest in this. Second of all, you have the uh, that has died. That alone is amazing. That is so valuable. Okay? Okay. Did I write that twice? Yes, I did. I wrote twice. Thank you nick.

Speaker 1:          00:44:49       Okay. So now our, okay, so our last step, our last step, initialize the optimizer, minimize the loss, minimize the loss. Okay, so we're going to minimize this loss. So to minimize this loss, we're going to use Adam. Adam is our optimizer stepping. I'll talk about why we're using Adam. But first let me write out a little bit of uh, since uh, magic numbers. Okay. These are our hyper parameters and honestly, we shouldn't do it here. We should have defined this earlier. This is bad practice. I'm just saying it right, officer up front. This is bad practice to write these out here, but that's what we're going to do. Um, just for these three lines, okay. And I'll talk about what this is in a second. All right, so these are our values. Epsilon, what else do we got? We have one e minus oh eight. Okay.

Speaker 1:          00:45:45       Boom. That's it. That's our, that's our item. That's our Adam optimizer. And so let's talk about Adam. Okay? But before we talk about Adam, let me write up the, literally the last line of code. This is the last line of code, lovey to rile. This is the last line of code. So our last line of code is to just say opt dot. Minimize our loss. That's it. We minimize this loss and this is dark actual training. Okay. And what we're done with this, we're going to later on in different modules, we'll save it, we'll um, uh, we can then predict values and this is our training. Okay. So let's talk about Adam for a second. So Adam is one of our possible optimization functions and I definitely need to make a video that describes the difference between different optimizers. I definitely need to do that.

Speaker 1:          00:46:32       But, uh, that's not this. We're not going to talk about all the different optimizers. Just know that Adam is a great optimizer for, uh, for specifically for sequence to sequence models. So in general, what hyper parameter is, should I use, what optimizer should I use? What a loss function should I use? These are, these are Meta questions and we are not at that phase yet where we can just learn to learn what we can, let our network learn what optimize optimization strategy to use, where we can let it. Lauren, what loss function to use, what we can let it learn, the hyper parameters to use that is phase two. That is where we as a

Speaker 2:          00:47:10       mmm.

Speaker 1:          00:47:12       That we as a community are getting too. We're going to start learning when you let it learn for itself a, right now we should define this ourselves. So for Adam optimizer, so.

Speaker 2:          00:47:24       MMM.

Speaker 1:          00:47:26       Okay. So it's actually, this is a great link, but it's not even, this is actually a little more complex. I mean, how can I best explain this? So, so it's, I mean it's, it's very similar to grading dissent, but there are several key concepts that are different that, uh, it's okay if your life, Fernando, it's all, it's all good. Don't, don't worry about it. Uh, just know that Adam is, is what's given us our best. I'm still figuring out how to explain optimization techniques without writing out a lot of math and codes. Um, yeah, I mean,

Speaker 1:          00:48:10       yeah, so it's, it's very similar to grading dissent. And I have, uh, I've talked about grading and the sense several times, but okay. So the point is that we are a, so that's what we're using for that. So let's, let's keep going here for a second. We have a, so that, that is our model function. So what I've just done is I wrote out, so in the, in the actual code, these are all the different classes. Okay. They're there, there's all the different classes here. So in the model class, okay, he's got a class for the model and he's got several policies, uh, for different things for like, you know, how do we want to choose our weights? How do we want a sample? So the, I, I honestly think this guy went

Speaker 2:          00:48:49       okay,

Speaker 1:          00:48:49       like overboard with this stuff because he, he's going at it at such a granular level, um, for, for an example. But for, for, for actual research, I mean, this is amazing. Uh, so interestingly enough, this model, so let's, let's talk about what happened. Let me, let me, let me, let me, uh, compile this. Okay, let's, let me, let's, let's, uh, so we have five songs that were training on, okay. I'm definitely going to do a video

Speaker 1:          00:49:25       on TF optimizers. That's a great idea. Thank you guys. I will definitely do a video on that. I want to start releasing more videos that are focused on, so I'm gonna actually stop this because this is going to take a lot of compute to, to, to, to calculate this. And a lot of time this will take, uh, on my Mac book pro 2016 somewhere around one to three hours. Okay. To train on up to a hundred songs. Okay. One, two, three hours and then linearly increase that time depending on how many songs do you do in the future. So interestingly, Leah, not for this, um, for this code, what this guy did is he said at the end

Speaker 1:          00:50:06       I had big ambitions but I needed more gps. So I'm going to keep this model in mind until I have more GPU. So literally the blocker for this code was amount of GPU. Yes. That's, can you believe that we need better computation? Okay. We need more affordable computation. Do we need GP? You don't gps, but if you want to speed up training, like you could do this on the CPU. Okay. But if you want to speed up training by like 10 x gps are nice, I recommend AWS. I have a great video on how to use AWS. And with a prebuilt Amazon machine image, it's got tentraflow builtin a bunch of great things. Little you just, you just upload your model and go. Um, and the video on that is, um, which one is that? Tensorflow? Uh, though the tension flow image classifier in five minutes. That's the video.

Speaker 2:          00:51:01       Okay.

Speaker 1:          00:51:02       It was blocked by the GPU. So let's, um, see what the result was. So I tried this earlier and, um, where was this? Okay, so let's, let's play this. Do I have a mini player or any player? Yes. Garageband. Garageband sucks. This is on five songs. It was trained on five songs.

Speaker 4:          00:51:39       Okay.

Speaker 1:          00:51:41       A little dance. Do a little dance. Okay. It sounds amazing because it's overfit on five songs. That's, that's the reason. Okay. It's overfit that's why it sounds amazing. So it's interesting we have that curve, right? So it starts up really, so if we had a graph of like how, how well the model fits to, uh, how, uh,

Speaker 1:          00:52:07       how much data we've given it, it's going to be like up here in terms of like quality and that's going to go down. And the more data we give it because it's not overfitting and then when we give it even more than it's going to go up again. So it's kind of like a you in terms of quality over, uh, you know, so y axis is quality and an x axis is, uh, you know, amount of songs we give it. So that's, that's what that is. Wraps. Raj, I will wrap over this. Of course. Can someone give me a, uh, I remind you a lot of Daniel Shiffman you guys, we are getting it wrong. These people remind you me know. Hi. Remind Daniel Shiffman reminds you of me. Okay, let's let me rap about this. Big Data using python.

Speaker 4:          00:52:50       Yeah.

Speaker 1:          00:52:51       Hello. Big Data. Well, this is actually not a wrapping bead. This is like a totally not arriving beat. So I was so, so, uh, so that's it for this code. And now what we're going to do is we're going to answer some questions. So please ask questions about machine learning about this code. This is our last five minute Q and. A. And what I'm going to do is I'm going to, sure. So please, uh, you know, say the questions and I'm going to wrap the answers to it. Okay. Can we save a trained model on, can we save a train model on,

Speaker 1:          00:53:29       can we take the train to shut the hell? Can we save the train models so we can load it and save the time for training again and again each time? Let me answer that. This is beat sucks. Balls hold on our or socks, anything. Okay. Yes, it's the best. Of course you can save it. That's what I do. I'm not just being, I'm telling you the truth. Of course you can save it. The waits are so true. You save it, you vote it, you do it again. You initialize the variables book. I'm your friend. I'm to tell you the truth. When you save those wastes, tensorflow has a way for you to do it. Okay. Paul, what other questions do we got? Can you give a quick summary? Let me wrap a summary. Okay. First we imported our dependencies. Yo, we went and started building our network.

Speaker 1:          00:54:21       So we had three names, cooked scopes. Okay. One for the input and one for the target, one for the previous. That's meant I do it. I'm lost kid. A largest function is what I use at the end. Okay. I got a loop recurring net. I went back, uh, fed at my previous hidden state and that's went back to back sequence, a sequence models where I came back, I gave it an output and the previous state, I wrap all that stuff every day I try to minimize the loss function. Okay. Are you soft? Max? Cross entropy. Okay. And now I'm going to minimize that with Adam. Yay. Okay. So that's what I did for that. Let me end the wrap. Okay. Thank you. Okay, so that's, that's, that was my summary of what we did. And let me, let me summarize it one more time in plain English.

Speaker 1:          00:55:07       So we imported our 10 are tensorflow machine learning library and three helper classes. Okay. And so what we did was we built a network using three placeholders, one for the input data, one for the, that's our notes and then one for our targets. And those are binary classifications. Was this note pressed or not? And then for our hidden state, because this is eight recurrent network, so we had three placeholders and then we built our sequence to sequence model. That is our encoder and r d quarter. What we didn't do is we didn't define our encoder here. That's in a one of our helper classes. But we did define our decoder here. Okay.

Speaker 1:          00:55:49       So that's what we define using sequence a sequence. So what we did was we fed, our encoder are and Nicole's notes set it created hidden, uh, hit, uh, hidden state after training on that. And the hidden state got better and better, right? And then we fed the hidden state to our coder. We didn't feed the output probability to our decoder. We don't do that in a sequence. As he wins, we feed the internal hidden state to the decoder. And we use that decoder to take that hidden state, uh, to create our, um, multi-class classification. Like the notes are going to be B and a or just right, depending on whether or not it's likely a cord or a, you know, something else. Okay. So that's what we did. And then at the end we said, well, what does that lost function? Depending on the batches of sequence data we give it, what is that loss function that we are wanting to use to minimize the difference between the predicted note and the actual note? And that is a cross entropy and then we, we minimize it using Adam. Okay. And Adam will definitely be explained in a future video. So that's that. I still don't understand what a hidden state is. So a hidden state is, let me, let me, okay, so neural network, let me just quickly explain this for a second. All right. So,

Speaker 1:          00:57:04       so for our hidden state, so in a neural network we have a series of hidden layers and these layers are, we could think about them as operations. Okay. They are just matrices, uh, and we have weights. And an operation like a summation or a multiplication or they can be any number of operations that we defined. And so what, what's happening here is when we're feeding our data in at each of these states, okay. At each of these, uh, we could also call it hidden layer. So state is equal to layer and each of these hidden layers or states where computing some operations, some matrix operation, it's all linear Algebra, it's all matrix math and we are taking the states and we're multiplying it by that input and that the state itself is going to be updated through an optimization technique like Adam or you know, whatever else.

Speaker 1:          00:57:51       And these weight values are the hidden state. So we feed those wait values back into the network in combination with our input. Okay. So in by combination we're using a, a summation operation. Okay. So we're not just fitting in the input data like we would in a feed forward. Now like continue, we're also feeding in the previous hidden state. And this, this is because we're, we're doing it over time, right? This is back propagation through time. How did you feed in the songs? Didn't get it. Okay. So I, what I didn't do is I didn't talk about the preprocessing of the actual songs. You're right. That's a great question. I only talked about building the model. And so for our songs we have, you know, two, two more minutes. So for our songs we had our, um, music data class. And so Minnie is a sequence of notes and there's, there's a number of preprocessing steps that happened here.

Speaker 1:          00:58:43       Uh, there's, there's actually quite a bit of preprocessing. Well, so, so let, let me do this. Let me do this. So there's actually one word. So I said I was going to give you guys one more, uh, um, library. So let me paste this in the chat. Thanks Colin. So go to this link right here. [inaudible] slash blues. So this is a great example of a, a very easy to understand music lie. Um, I python notebook. Okay. It's literally just this, it's one I python notebook, but the reason I didn't use it is because of dependency issues. So remember when it comes to python, we have a little bit of work to do for our music libraries. They're not good enough right now that, you know, Python Mitie only works for Python two. We need to update it for python three, which is what I use now. Um, but what, uh, what she did, I, it's a, she, she uh,

Speaker 1:          00:59:42       buck teeth, she took these midi notes and she converted them into a matrices of values, right? They're already matrices, right? Uh, and so we use, and the, and the major cs were fed directly into the network. They were vectorized notes. So that's kind of, you know, a one line of how data in general should be fed into, uh, into a network. Okay. So check out that link. A Bhakti Priya slash blues and a, yeah, two more questions and then we're done for the day. Okay. Java is all right. No, it's not. Guys, can you guide someone who is starting out? What's the best way to go about learning? Uh, my video, start with machine learning for hackers, then move on to learn python for data science. Then I'm huge now, so I'll just, the way I like it for Q and a exactly. A, she has a sheet. Okay. And then okay. Actually to, to, to not to do to two more questions. She's looking at you. She likes you how to use LSTs s sequence classifier. Cara Ross has a great example on the main read me on using a [inaudible] for a sequence classifier. And in fact I have videos on this. Uh, what was my last LSTM video art generation, but that was more generative. Uh, Lstm frog sequenced classifier. Yeah. Just to use a double stack Lsem and have your last uh, layer be a fully connected layer of followed by a softmax so you squash it. Okay. And then,

Speaker 1:          01:01:22       all right, one more question. We have one more question and then we're out of here. How to learn the mathematics of deep learning. Gwar great question. So I answered this in my first video of this series, but I'll answer it again. Linear Algebra, calculus, statistics. Do you need to know everything about a calculus and linear? No. You just need to know the bare minimum. And I have linked to a cheat sheets for each of those in my first intro to deep learning video. Check out those cheat sheets. As in do you need to know the inner the integrals for Calculus? No, you just need to know the derivatives, why we use it for doc propagation. Okay. You just need to know the basics. Am I sitting here studying calculus, linear Algebra? No, I'm focused on machine learning and deep learning. So I only need it. I need, I only need to learn what I need.

Speaker 1:          01:02:14       Okay. And that's a mentality all of us should have. Let's only learn what we need to learn, okay? And nothing more. Okay. Because there is a lot of stuff to learn, so we don't want to waste our time. Our time is very valuable. Your time is very valuable. Okay, so that's it on for this session session on Bayesean deep learning that is going to be at the end of this course. I cannot wait to talk about Basie and optimization Basie and learning one shot. Learning that in some advanced stuff. But we will get there when we get there. I am so excited to have all you here a part of this journey. Thank you for showing up. We're going to learn everything. Okay. We are going to learn everything. We're going to learn to learn. We're going to learn to learn, to learn. Okay. So thanks for watching. I love you guys, and for now I've got to go.

Speaker 2:          01:03:00       Yeah.

Speaker 1:          01:03:01       Learn to learn to learn. So thanks for watching. Yeah.