Speaker 1:          00:00          Hello world, it's a Raj. And today we're going to learn how to deploy a karass model to production. And the APP that we're going to build, this specific app is going to be able to detect any handwritten character that you draw in the browser. It's just, it's just going to be able to tell what that is. So let me write out a two. This is a debt. This is the demo right here. I hit predict. So to let me clear that. Okay, how about a three, Scott? A three. Okay, let's try something else. How about a nine? That's a seven because that was badly drawn. And then one more. How about a zero, a zero or a six? Zero. Cool. Okay, so that's what we're going to do and we've got four steps here. Okay. So the first step is the trainer model in care off.

Speaker 1:          00:46          So we're going to train our model, simple and t, Henry and [inaudible]. It's all over the Internet. That's not really the value add here. The value add, it's going to be us writing our flask backend. So we're going to glaze over this chaos model, this m and ist, and then we're going to get to the real meat of it, which is writing out a flask web app to serve it. So our first step is going to be a trainer model and Cara Ross. And then our next step is going to be a save it. Then our third step is going to be to write our flask backend, which is a python web app framework. It's a very, it's a, it's a very small, thin, very thin weight, a micro framework for, for serving data in the browser. It using a python backend for requests, you know, get posts set, um, to serve on.

Speaker 1:          01:24          So we're going to use it to serve our saved care os model. Okay. And then once we're done, it's going to be running locally just like it is right now. Like it is on local host on port eight, eight, eight, eight. And then we're going to deploy this code to Google cloud and that means anyone can use it. So I already made a video on how to deploy our tensorflow model to production, right. Using tensorflow serving. And that's where like, you know, high grade production quality, you know, millions of people are using it. It's what Google uses. That's when you want to use tensorflow survey. So you could use carrots with tensorflow serving as well. In fact, Francoise show, let Schwab Chalet, I kind of work on my French and Dutch now that I'm here in Europe. We might get interrupted by an angry Dutch lady, but it's all good.

Speaker 1:          02:05          The show must go on. She's next door. Okay. Uh, where was I? Yeah, so we have to, we could use tensorflow serving if we wanted to. Um, but you know, save it with carrots and then use tensorflow serving cause you, it's, you would build a model of care ROSC save it as a tensorflow file and serve it with tensorflow serving. But the easiest way, the absolute bare bones, easiest way to take a train model, deploy it to a web app and then share it with your friends is using this. Okay. This is the easiest way that I've found that I've been trying out a bunch of different ways. So if you have a model and you just want to show people, right, this is the way to do that. If you have a model and you want to not, not just show people as a demo because it's easy to access in the browser, but you want to actually build a business around it, tension, float serving as the way that also applies to this.

Speaker 1:          02:50          Right? And we'll talk about how it applies. What I mean by applies as we would still deploy our code to Google cloud and we would still, uh, train a model with chaos, right? The only difference is for the back end, we wouldn't use flask. We would use tensorflow survey. Okay. So this is for that, you know, easily getting it into the browser basically. That's, that's what I'm trying to say. Okay. So where was that? Let's, let's, let's get, sorry. Right. So first I'm gonna explain what we're, what's going on here, like what this is, and then we'll get into the, to the code part. Okay. So, okay, so what is our stack look like or, so we're going to use carrots with a tensorflow backend. So carrots where the attentional backend to train a very simple eight layer convolutional network to recognize handwritten character digits, right?

Speaker 1:          03:34          So zero through nine are these images and then they all have a label, you know, zero one, two, three, four. And our job, our, our CNNS job is to learn the mapping between the two, right? Pattern recognition, very tried and true model. Everyone knows about it, but here's the, here's the cool part. We're going to use flask as our backend to serve these pretrained models. So we're going to train it locally, it's gonna and then we're going to save the model as a weight file and then we're going to serve it with flask. You could also use node, right? You could use any number of jazz web frameworks. There's like amber, all these jams from books out there, but I would not want to write a back at back backend code in javascript. People do it all the time. I don't want to do it because I just prefer python, right?

Speaker 1:          04:16          We, we, we, we as machine learning engineers, we write python anyway, so let's just use a python framework. You could also use Django, but again, this is the simplest way to do it. Why? Because flask is just you. You literally, I mean, look at this. This is an example right here. Pip install flask, import it, initialize it, right or out. Boom. And that's all you need to run. And then this, it's going to type out, hello? Where is going to show helloworld in the, in the browser with Django, you know there's a, there's a kind of skeleton to it, you know the APP, you know the html files has to go in a certain directory. The js files had to go in a certain directory. It's similar to um, what's that ruby framework rails. It's similar to rails in that way. But you know, philosophy is just super simple, right?

Speaker 1:          04:58          I'm talking to people who like are experimenting with their models and they just want to like show people, right. You know, 10, 2000, 400 people, but not like millions of people. Right. Tensorflow serving as the way to do that. Okay, so what else is out there? Right? Why, why use flask and just native javascript? Aren't there other frameworks? The answer is yes, there are. This is just the simplest way cause I've been trying out a bunch of them, but I want to talk about three just so you get an idea of what the space looks like. Okay. So the first is uh, [inaudible] dot js. Now this is like the defacto, like most stars for using tariffs in the browser. It's gotten GPU support. It can only, it can only, it only supports Ford pass inference. That means you don't train your models in the browser. You have to have them trained.

Speaker 1:          05:43          Once they're trained, then it's going to use GPU acceleration to make that prediction right. To go through the layers. Once you've got your inputs to just make, make inference faster and it's using web loss and Ndra these two libraries that are basically like the javascript versions of Num Pi and uh, not Kuda but like, um, yeah, whatever, whatever, whatever. Kutas whatever's on top of Kuta right when it comes to when it comes to running things in python. So that's what it does. You use Karasin tensorflow. It's got a bunch of interactive demos and uh, it's pretty cool. It's pretty cool. Uh, but again, it's a lot of work to get to get to get working. I was trying it out, but check out these demos. These demos are pretty cool. Like this bi-directional Lstm Demo. So it's basically like doing sentiment analysis on, on texts. So like I really, really love my life is going to be, you know, green. Whereas they say, whereas the FSA hatred, you know, sex, sin, drugs, oh, it's not that bad. 50, 50%. Okay. I hate people. Hatred. Urgent kill me.

Speaker 1:          06:59          Needs more training, need more training. Okay. Just like people do sometimes. Okay. So that's one way you could do it, right? So if you want Gpu acceleration once this, so I'm saying try out this way, do this way. You can literally get this running in 30 seconds using the code in the browser. Literally you just clone the Repo, install it with pip install, require requirements at txt recursively and then run the APP. It's that simple. It's literally that simple and you'll have it in your browser. Once you get that, once you get this working and you've got to feel for how this, this structure works, then move on to using [inaudible] such js. Okay. Later on if you want Gpu acceleration and then tensorflow serving. Another way to do this is web DNA. So actually, so Francoise, the, the creator of care os recently shared this library as the fastest way to do it in the browser web DNS.

Speaker 1:          07:50          Uh, so he liked cl so they clocked it against Carol sijs and it Blue Care Os. Dot. Js Out of the water. However, how long, so you might be saying like, why don't you just use this one then why Karen called Jazz? When I tried running this, I got a specific error and I didn't see it in the issues, but it is a real error and the error was uh, it was like XC run cannot find metal dependency. And I was like, what do you mean metal? So I was like looking up like metal OSX and I found out that metal is an, is an apple specific dependency for running code on the GPU. Right. Don't you love how apple always like, like wraps all of their libraries with these like neat, pristine interfaces and it just makes you want to just dive into and be like, yes, I'll do anything with metal.

Speaker 1:          08:32          Yes, this looks so awesome. But the fact is that uh, not everybody has OSX and this requires, this has an OSX specific dependency, web, Dnn. So if you have Mac, definitely, definitely try it out. But not everybody has Mac. This, the one I'm using is, is operating system agnostic, which is what we want. Okay. So now that we have that, let's what else? So, so that's one. And so the, the third one is called Neo cortex. So neocortex is also very cool. It's got some great examples in the browser that we can, that we can try out. So in initializes and then does the same thing or like predicting classifying these images. But the problem with neo cortex says, look, look at this and this is a good thing to do. Look at when the last commit was, it was a year ago. We want libraries that are actively maintained. Why? Because if you don't, then it leads to what's called Code Rot and code rot is when code just deprecates. Right? And we don't want deprecated code, but it is pretty cool. Okay, so those are three that I wanted to talk about. Okay. So let's go ahead and actually do this. Okay. So, um, where do we get started here to get started? Let me x out of these. X, x, x, x, not the, okay. Gone. Gone, gone. Okay. So now

Speaker 1:          09:56          let's get back to what we were doing here.

Speaker 2:          10:01          Sorry. So

Speaker 1:          10:01          the first thing we're going to do is, is train our model, right? So I'm just going to glaze over this code and like talk about what's happening. I'm not going to actually type it out because we've seen him and I see many times, but I'm just going to refresh just so we have some kind of base to move forward from. So we're going to build a model that's going to be able to classify handwritten character digits. Okay. So we're going to MPR import future just for python compatibility between two and three carats and all of its dependencies. And once we've got care os we're going to say, okay, we're going to do this with mini batch grading dissent with batch sizes of one 28 images per batch. There's 10 different classes and then 12 epochs, right? So there, there are certain number of batches within each box.

Speaker 1:          10:41          So we defined a number of epochs and then run a certain number of batches for every epoch during training. And then we're going to use this beautiful method that's super simple. If only all data sets you use something like this, right? Low data, it's going to download it, split it into training and testing sets for us. Just like that. Okay? Once we have that, then we're going to say, well, assuming what, what format it's in, we're going to arrange the data in a certain way. So that's why this, this channels first a flag is there. Either it's going to be, you know, it's going to show that a certain layer of convolutional matrix cs or it's going to do it the other way. It's going to reverse it. And so then more reshaping and to float 32, and then we're going to, uh, convert our class vectors to be binary class major CS.

Speaker 1:          11:24          Uh, is it this or not? And then we're going to build our model. So it's going to be a sequential model and chaos with eight different layers, right? So we're going to start off with a convolutional layer that's going to act as a filter for that image. And then we're gonna say, okay, now that we have this layer, we'll add another convolutional layer. So it's going to be, it's going to take that big image and continuously split it into smaller subsets of images. And they were going to use pooling to decide what, which, what part of the image is most relevant. That that is what part of the image has had something drawn on it. Write the character itself. That's, that's the relevant part. Then dropout is a technique to improve convergence by randomly turning neurons on and off. We're going to flatten it because it has way too many dimensions and we want a very small, uh, output prediction, right?

Speaker 1:          12:10          A label. We don't want like this huge array as our output. Once we have that, then we're going to say, okay, we're going to use a dense layer because we want all that relevant data drop out. Again just for convergence and say usually drop out happens twice, sometimes three times, but I don't see it happen more than that. Unless you're using like, yeah, generally it's two to three times a. Then we're going to use a last dense layer to squash it. Using a softmax function is going to output a list of probabilities of what class is going to be. It's a multi-class classification problem and then once we're done with that, we're going to use our optimizer at a Delta, which if you saw my evolution of grading dissent video, which I hope you did, you now have a better understanding of why we use these optimizers, right?

Speaker 1:          12:52          It's an adaptive learning rate method that is is is rivaled only by Adam and Adam Grad. Okay, and so then we're going to use this categorical Google Cross entropy as our loss function because we have multiple classes, not binary cross entropy. Is this, that is this, you know, black or white? No, it's categorical cause we've got 10 classes. Then we're going to train that stuff. We're going to train the model, okay with this fit method and then we're going to test how well it did. And then at the very end, this is now, this is the very important part. That's that's US building our model. Okay, we're done building a model. Now it's time to save it to Jason and the weights file. So we're going to save two different entities here. One is a Jason file and the Jason File is there because we want to save the structure of the model itself.

Speaker 1:          13:38          This is going to save the architecture of the model. The other part are the weights and the weights are the learnings, right? Once we've trained it, all of those learned values from the matrices of what it means to be a zero or a one or a two, any of those images, all that's going to go in our weights file. Okay? And so we're going to save that and we're going to say both of them and we're going to load both of them later using flask. Okay. So that's it for our training part and then we would just train this model, right? Which I could do, but I mean I, I trained it before, uh, right. So just like that, right? It's going to do everything, but I already have a trained here at the h five file and the Jason Fall, h five by the way, is the format for Karrass models.

Speaker 1:          14:25          Okay. So we'll have both of those and once we have those, we're going to, let's see, let's see. Now we're going to write our flask APP. So we did, we trained our model, we did step one, and then we saved it. That was step two. And now we're at step three, writing our flask backend to say to serve our saved model. That is the h five file as well as a Jason File. And then we'll look at the dependency classes that make the help make that happen. Okay, so going to first import flask, right? This is our web framework. Our microservices framework and a few of its a related dependencies, right? Because flask alone isn't enough. We've also got to have these dependencies. We've got flask, we've got render templates. Why do we have render templates? Because uh, generating html from python alone is kind of messy.

Speaker 1:          15:15          So what this does is it lets us define an html fought html file, a standalone html file, and then we can just call that, it's called index dot html. This is going to help us render that using python and then requests for just handling all these guest posts. Set requests, right. Okay. So that's for flask. And then we're going to import, hold on. It's render template, not templates template to play. Okay? Uh, that's not a language. Okay. So then we're going to import PSI PI and [inaudible] gonna just have a couple of functions because what we're gonna do is we're going to take that image that they user draws and we're going to reshape it using these methods, right? So this is our scientific computing library and we're going to reshape whatever they use or draws. So it's in the right image format. And then feed that directly into our pretrained model.

Speaker 1:          16:02          Right? So we've got him saved in, read an image, resize. Those are the three that we're going to need. Okay. So we've got that. And then of course num Pi Duh cause that's our matrix math operations, uh, library. We love that. And then we're going to have of course import care os dot models to import our model that we've trained. And lastly, Ari. Ari is regular expressions, a great way to handle a huge sets of string data without having to sort through all of it. Okay. Uh, and so that's all we've got some more. So import sis is going to help us do some system level operations. Like this is how, this is not what we're actually going to load the file itself. Okay. We've got that. And then we're going to import unless for some operating system data. And then we're going to, now we're going to use this to say, okay, we're going to say this is where our model is.

Speaker 1:          16:52          Let's define where our model is. All right and, and o to do this. See our imports are already coming in handy here. So we're going to say, okay, so that's where our model is saved in this model and the model folder. Okay. And then from load import that, okay. So that's gonna, that's gonna that tells our app where the model is going to be saved. And then we're going to say we're going to import this load classes, which is going to help us load the model and then we're going to initialize it. Okay. So that's it for dependencies. And now we can initialize flask app are initialized our Flask App. So we're gonna say, okay, so the APP is flask given end name, which is going to be, we'll, we'll define later. Okay. So that, that's it for our flask APP. And then we're going to import two or so, not important declared to global variables that we've defined elsewhere.

Speaker 1:          17:46          And you'll see what these are. But basically the is the model object for how we encapsulate the model file. And the graph is the computation graph, right? Uh, which is like a session from inside. We run the model, it's a kin to a session. In this case, and then we're going to initialize these variables. So we're gonna say, okay, so model and graph initialized both of them from the load, from then, that's where the load, uh, dependency comes from, from load in port. That's where that come from, comes from. Okay. So then once we have that, let's write out that main function, right? That's right at that main function. So we're going to say, okay, if name equals Maine,

Speaker 2:          18:31          okay,

Speaker 1:          18:34          what port should we run this APP in? Right? Hold on. If, well, we're going to decide that the port is going to be,

Speaker 1:          18:48          well, let's get it from the environment. It's going to be port 5,000 because nothing else is running on that port. Oh, let's, it is, which it's not. It's not. And once we have that, we can run the APP locally using app.run. So we'll say, oh, that's, that's where we're going to run it. That's like in the browser. That's the address. So it's gonna be like 0.04 at port 55,000 all right. And so we can define that as well. Super simple stuff. You know, if you do machine learning for a while, everything else seems super trivial. Just boring in a way. No, it's not boring. Just, I mean just you know, boiler plate stuff. But the, the what we're doing overall is cool. Okay. So then we'll say, okay, so run this APP. We've, we've defined it and let's, I've already did that. We defined it and now that's it.

Speaker 1:          19:36          That's it. That's it for the, that's it for the name pot for the main main file. So now we've got to write some helper functions here, right? So we've got two helper functions that we want to make here. The first one is called index, and then the other one is called predict. So we'll say, okay, so this is how we're routing our, uh, files. So this is, so this is the APP dot fuck app.py file. This is how all of this is how we tell our APP. What happens whenever a user goes to a certain address, right? So if you go to, you know, slash looney tunes, then load up the looney tunes function. If you go to slash hello world. And a lot of the hello world function. So if they, if the user goes to this, just, you know, slash that means that while they're on the main page, so let's give, let's serve them the index.

Speaker 1:          20:21          So we'll say at this route, let's define an index function, which we'll write this functionality and all it's going to do it all it's gonna do is it's going to render the template, which is index dot html, which we'll talk about afterwards. That's one of our dependencies here, right? This is the high level code. So that's it. It's going to render that html template, index dot html that's going to serve html. That's okay. So then once we've got that, then we can say, okay, well what about if the user goes to this predict, uh, route? Well, the user isn't going to specifically go to predict when the user hits a submit. You know, when they draw the number to hit submit on click, that's when predictors called, well, now we've got to write code for that predicts when to show what happens when the user clicks that. So we're going to use two methods here. We're going to use both, get an post and we'll see. Okay? So

Speaker 2:          21:29          okay,

Speaker 1:          21:30          for project we're going to say, well, let's get that image data. So from the request, where are we good from? The requests were going to get that data and hold on.

Speaker 1:          21:47          Okay? And that's going to get the raw, that's going to get the raw data format from the image, right? It's just the raw serialized data. And now we have to reshape it so we can fit it into our model, right? The user drew drew something, right? We were going to get that image or we're going to reshape it. So it's fit to be fed into our model. And then we're going to take that image VA into our model and it's going to output the prediction. Okay? So we'll say, okay, so we've got that image data and now we're going to encode it using this helper function. Oh, we have one scene. We have one more helper function that I forgot about. So we've got image data and we're going to say, uh, that's going to convert the image. That's going to basically convert the image into a more suitable format. Uh, so once we have that, we're going to read it into memory. So right now it's still, we're reading it directly from the saved image. Now we're going to read it into memory using this mode l and then we're going, and then we're going to invert the image. So we're going to say, okay, and P. Dot. In verb. And what this does is, is it's a bit wise inversion so that all the black becomes white and all the white becomes black. And it makes it easier for us to classify,

Speaker 1:          22:59          okay, once we have that, then we're going to make it the right size will say x equals image, resize ex, uh, when we want it to be 28 by 28 pixels, right? Because that's what our model expects. That's, that's the image size that we trained it on. So we're going to resize it like that, and then we're going to reshape it. So let's say x equals X. Dot. Reshaped, and then we're going to end. So now we're going to make it into a four d tensor of one 20 of one 28 21 of that size. So it's say four d tenser and that, that is what we feed into our model of 40 tensor. Okay. So image that reshape. So now with our computation graphs, so this is where that graph global variable comes in, say graph dot. As default, the prediction. So we'll say modeled up, predict x, given that newly reshaped image and then convert the response to a strength. So we've, we're going to take whatever comes out as a response and we're gonna use this array string method to convert it to a string. And we're going to say NPDR Arg Max to get the Max value of whatever came out of that using only a single access to eye single dimensional response, which is just one string. And then we're going to return it. Return response

Speaker 2:          24:22          just like that. Okay.

Speaker 1:          24:24          And yeah, that's it. Oh, we've got, and then that one, we had that one more method. What was it? It was a convert image. And so this is that last, uh, two line helper function. So this is, this is going to help us convert our image into a raw representation or it's going to decode it from base 64 and to raw data base 64 is like the default encoding whenever we write it out. That's just like how it, how native the native javascript decided to encode it as. And we have to Dakota from base 64 and to just raw, um, binary data so we could then, a lot of image conversion happens here. So we'll say, okay, and so this is what we use our regular expressions. These were to say, well, get all the parts of the image that are encoded in base 64 and

Speaker 2:          25:13          convert them into this string so that

Speaker 1:          25:24          put them in the string so that we can then they code them. So then we could say, we'll take whatever the, so it's going to be saved as output dot P and g, the image, uh, that the user drew. So we're going to take that image and then

Speaker 2:          25:37          say decode it from base 64.

Speaker 1:          25:47          Cool. Okay. So that's it for, that's it for this file, for the APP to five APP dot. Py File. And then once we have that, now we can look at our dependencies here. Like how did we, how did we do this? Right? And so we called some classes here and what I want to do is talk is talk about what classes we called. Okay. Let me just make sure all of this is correct. Um, basically four.

Speaker 2:          26:11          Yup. Yup, Yup, Yup, Yup. Okay,

Speaker 1:          26:21          cool. All right. So, so we have that. And so now I want to,

Speaker 2:          26:27          okay,

Speaker 1:          26:28          look at the, uh, where was I? Okay. App Dot. Py. Train a pie. And then,

Speaker 1:          26:40          okay, so then the low dot pilots. Look, it's, so let's look at some of these file. So low dot Powell, remember we load the Pyre, remember what we called in it at the beginning? Remember when we were like, where was it modeled? Graph in it. This is what's happening in load up high under model. And this star, these are saved, uh, models by the way. H five and Jason, what we did was we loaded up our Jason. Okay. And our h five file and we compile them so that we can then get the default graph. Okay. And so we said no, we compile them and then we returned that default graph from tensorflow cause we needed both of them. Okay. So we had both of them. And so, uh, when we hit compile, it's not actually training, it's just evaluating based on what it already is.

Speaker 1:          27:26          Like it's already trained and we're already, we're, we're, we're going to make sure that the loss has a certain, that's what we have these prints statements. We're going to make sure that the loss is a certain way. The accuracy is at a certain level and that uh, yeah. And then we can print those out. So that's what load.py does. And then we've got this index dot js file. Now this is, this is what's happening in the background of the, of index dot html. This is like pure javascript and there's no j query. Basically what it's doing here is it's, it's listening to your mouse movement to track, to actually make the drawing happen. It's all in javascript. It's actually very short. It's 83 lines of code, but in Javascript it's using an event listener to the tech mouse movements to then draw pixels on the screen.

Speaker 1:          28:09          Okay? That's what this first part is doing. And then right here what it's saying is now that here's the, here's the really important part, and then wait for the plane, okay? And then the clear buttons is just going to clear the screen and we can select the colors and the line with. So here's the last dependency that I want to talk about is the index dot html file. So for index dot html, we're drawing a bunch of dom elements, right? Like you know, the button and the, uh, the, the like little browser thing at the top, this, the panel at the top. And so then we've got this. Now here's the most important part. When we click my button right on click, this is what, this is the function that we want to run, okay? We're going to say, get the element by ID. So we're pulling an element directly from the dom and storing that in the canvas object.

Speaker 1:          29:02          And that is our image that the user drew. And then we're going to say, well, converted to it, uh, uh, a suitable string format. So then we can then make a request to it and then use Ajax as a way to make a post to the predict route using the image as a parameter. So it's going to make a prediction using that image. And then it's when it, when it's successful with this callback, it's going to return that response and that response was here, right? This is the output prediction. Once it's, uh, made the prediction and we've reshaped that image, it's going to return it here and then it's going to output. It's just like that right to html. Okay. So, um, that's how that works. And so now, where are we now? So we've done step three. We've written our flask backend to serve our sales model.

Speaker 1:          29:50          It's that easy. There's no other magic happening under the hood. It's just raw flask, raw java script and chaos with the tension for the backend. And you can make a web app just like that. And there's a bunch of other helper library. There's a bunch of other libraries and and talking about them as well, and they give you things like Gpu acceleration, um, and more infrastructure. Tensorflow serving, I'm talking about tensorflow serving specifically gives you more infrastructure to run different versions of models to deal with life cycles management, things like that. Okay, so now we're in the last plot part deploying our code to Google cloud. Okay, so this is actually super, super simple. It's two, it's two commands, g cloud APP deploy, and then g cloud APP brows. This deploy APP, this deploy command is going to take your entire repository when you're in that main, when you're in that main directory and just deploy all of it to Google cloud, assuming you've authenticated, right?

Speaker 1:          30:46          Like you've already typed in your username and password, they will deploy that to its own container and Google cloud. Okay? And once it's done, it's going to build a container. So it's going to build a container image, it's going to dockerize it and then deploy that to app engine. Hold on. It's not going to dockerize it. We would have to dockerize it ourselves if we wanted to, but it's going to deploy as a container to app engine and app engine is Google cloud's a service. We're running apps, it's like Heroku, but like Google cloud's version. And then once it's there, if we want to view it, we just type in Google g cloud app brows and then it's going to be at our project I id, which is going to output.hubspot.com and just like that, you can now serve this model to your friends and show them what you've built.

Speaker 1:          31:27          Okay. So the more info is here, right? Right on Google cloud, I put a link to it, but basically you need to download the Sdk for Google cloud to be able to use this. It's not just a simple pip install. Okay. And once you've got that, then you could run it super easily. Right. Uh, also this guy, I found this really great tutorial as well. Uh, it's, it's more detailed then what I'm doing. It's actually a little complicated like, but it's a, it's a pretty good tutorial as well. Linked to that. They're basically, yeah, we can just deploy it to Google cloud. Cool. So I'm going to end this by answering two questions and then we're out of here. So question one is, um, how can I count objects in an image? Great question. So for something simple like that, you can just use open CV, right? You can just use open CV. You don't actually have to use a pre trained model for that. You could, but I think that would be overkill. Uh, yeah, use open CV and then the method is called, um, yeah, we'll, is it like multi object detection? Yeah. Multitrack or that's the class, the multitrack or class. And in one more question,

Speaker 2:          32:42          uh, could you

Speaker 1:          32:46          please make a video about semantic nets using tensorflow? I've got several videos on semantic next with tensorflow. Um, check out my live, my earlier live videos from the u Udacity deep learning nanodegree several semantic nets like a game of Thrones. Just search like game of Thrones. Saroj that one's going to be good for you. Cool. Please subscribe if you liked this video. And for now I've got to go deploy my hair to production. So thanks for watching.