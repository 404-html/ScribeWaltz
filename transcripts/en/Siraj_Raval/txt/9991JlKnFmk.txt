Speaker 1:          00:00          Hello world, it's to Raj and we're going to detect who the intruder is in our security system. So say we've got a website and we're monitoring all of this network data and there's one guy, one bad dude, one bad own Bray who's trying to break into our security system. We want to find who this guy is. And the way we're going to do this is by implementing a machine learning model called k means clustering. And I've never implemented this before, so I'm really excited and it falls right in line with the rest of the stuff I've been talking about in this series. So let's get started. First, I've got this little image to show you what the algorithm looks like. This, it's a five step process and this gift kind of shows what that looks like. We start off with data that has no labels, it's just a cluster of unlabeled data.

Speaker 1:          00:48          And we don't really know anything about the data other than the features. We don't know what classes the data belongs to. That's what we're trying to learn. And what the algorithm will do is it will iteratively take this data, this unlabeled data, and it will create clusters from this data. And we don't know what, where these clusters are going to be. The algorithm is going to help learn where those clusters should be. Okay. So, and I'll talk about what these steps are. Well let me define what this term is. K means clustering. This is one of the most popular techniques in machine learning. You see it all the time in Kaggle contests and the machine learning sub reddit everywhere. It's a very popular algorithm and it's very easy, uh, more or less, I mean more than other things that I've been talking about. So that's a good thing.

Speaker 1:          01:35          But let's talk about what we've learned so far. What we've learned is that machine learning is all about optimizing for an objective, right? We are trying to optimize for an objective that's, that's the goal of machine learning. And we've learned about first order and second order optimization, what's first order gradient descent and its variance, right? Where we are trying to, if we were to graph the error of a function versus it's weight values, we want to find the minimum of the function so that we can find the ideal weight value so that our error is minimized. And to do that with grading dissent, we compute the first derivatives, right? The partial derivatives with respect to our weights using our error. But for second order optimization, we do the same thing except we compute the second derivative that is the derivative of the derivative. And there's pros and cons to both.

Speaker 1:          02:23          And we talked about when you would use one over the other and the previous videos. But what happens, here's the, here's the big question. What happens if you don't have the label? How are you supposed to compute the air? Right? It's, it's usually the predicted label minus or sorry, the actual label minus the predicted label. That's the air use it to compute the partial derivatives with respect to each weight value. But if you don't have the label, how are you supposed to compute the air? And that's where unsupervised learning comes into play. Specifically. K means clustering. So I've got this diagram here to show the differences here. So there are two outcomes that we could possibly want, right? Either a discrete outcome that is some contained outcome like red or blue or blue or black or white or up or down or not guilty or guilty right there. These containerized outcomes, binary outcomes, not just binary cause it could be more than zero and one, it could be multi-class, but outcomes that are containerized into

Speaker 2:          03:26          okay,

Speaker 1:          03:26          specific labels. Whereas continuous outcomes are like time series data where it could be a value between two and three or between two and 2.5 or between two and 2.25 and it could just go infinity in that direction of that, of that numerical interval. Right? So with supervised learning, we learn how to do predict a continuous outcome using linear regression. That was our first video. And then the next thing we learned was how to predict a discrete outcome. And we use logistic regression for that, and that's when we have labels. But if we don't have labels, then we use clustering to predict a discrete outcome. That's what we're gonna do. We're gonna predict a discrete outcome. And by defining these classes for people, it's these discrete classes. And then we're going to find the anomaly that is the intruder. And so then for a continuous outcome, you, you'll want to perform dimentionality reduction.

Speaker 1:          04:23          And that's next week. So we're not, we're not there yet, but we'll talk. We talked about this one and this one and now we're going to talk about clustering. So let's keep going here. So on supervised versus unsupervised learning, what are the pros and cons? Well, for supervised learning it's more accurate. I mean, you've got the labels, of course, it's more accurate, right? It's like, it's like having training wheels on your bike, but you have to have a human who labels this data or it's just labeled itself somehow. But unsupervised learning is more convenient cause you, cause most data is unlabeled, right? You don't just have this neatly labeled data like, oh this is this or it. No, data is messy, the world is messy, life is messy. So that's what we want, ideally to run our algorithms unsupervised. But the problem is that these algorithms are usually less accurate.

Speaker 1:          05:11          Right? But it requires minimum human effort cause no one's got a label. These, these, uh, data points by hand. Okay. So let's talk about how this algorithm works mathematically and then we'll get right into the code. Okay? And so what I'm gonna do is I'm going to glaze over most of the code and I'll write a few of the most important bits. Okay? So how does this work? So we've got a set of points, right? So a set of points x. And so what I'm gonna do is I'm going to take this and I'm gonna make two copies of it so we can see this Gif while I talk about the algorithm. So let me do this, hold on just like that and put that here and then put this here. Okay. So,

Speaker 2:          05:53          okay,

Speaker 1:          05:54          so the way this algorithm works

Speaker 1:          05:58          is we've got a set of points, right? I set of points x and then, and then we define a value. Kay and Kay is the number of clusters that we want, right? K Means Algorithm. That's where k comes from. So we've got a set of points x, all of our data points, and we've got k that's our input. And so we're going to say two, let's just say two for now and we'll talk about how to realize what the best k value is. But let's just say two right now. And so then once we have that, we're going to place a set of centroids at random locations. How many centroidsK centroids. So if we chose to for k, then the centroids are just data points that are just randomly plotted on the graph. Okay. These are called centroids because eventually they're going to be the center of each cluster that we learn.

Speaker 1:          06:45          So the center, so the centroid points are there, k of them, and we just plot them randomly. Okay. So we definedK , we have our set of data points and then our set of centroids k of them, and we just plot them randomly. Okay, now what? Now here's the steps we do, and we repeat them until convergence, which is what we predefined beforehand with threshold value. So what we do is we say for each point in the data set, so if, let's say for we have 40 data points, so for each point, let's say for one of them we're going to find the nearest centroid. How do we do that? Well, we compute the distance between that data point and and each of the central points. So there's two, in our case there's going to compute the distance, the Euclidean distance between that point and both of the centroid and isn't to find the one that's closest to it.

Speaker 1:          07:32          And that's where this Arg Min function comes in. What is the minimum value in this set of values? So we're going to find the shortest distance and that's going to be our cluster. So we're going to assign that data point to the, to the cluster, Jay or Jay is for the centroid that is closest to that data points. Okay? So then what happens is we've got a set of clusters now and we do this for every single data point. So every single data point will belong to a cluster and that cluster will be defined as a centroid point. That is closest to that data point. Okay, so that's the initial cluster that's going to be defined. Then for each of those clusters, Jay, we're going to take all of those data points in that cluster. We're going to add them all up and then divide by the number of them.

Speaker 1:          08:16          And what is this called? It's called the mean, right? Or the average. So now you're getting to see where this name comes from. Right? K means, right. It all makes sense. I was like, God, I love that. Okay, so k means is the name of the algorithm and so we find the mean point or the mean value for all the points in that Dataset and that mean is going to be become our next centroid point. Okay. So we've defined centroids, we've, we've grouped our data points into each of these centroids and then we will then find the mean of all those values. And that will be, those values will be our new centroid. So in our case there will be two, right? So there'll be two new centroids that we then plot and then we just keep repeating the process. So we go back to for each point acts and for those new centroids find the distance for all the closest data points and it's going to be a new cluster.

Speaker 1:          09:07          And so that's what you're seeing here. It's going to be a new cluster. And then we just kept keep repeating that process until when, until none of the cluster assignments change and then we're good. So right. So that's kind of how that works. And so, uh, right. So we can also terminate the program when it reaches an iteration budgets and we'll say after, you know, x number of iterations, just stop running. Right. So then one of one great question that I hope you're asking is how do we know what value forK we should use? Well, it's very simple if we know what classes we want to classify or how many classes would you say that that's the value for k. So if we know that we want to classify people as either a, from a certain region or from a certain place, then we'll just say like of these three places, you know, Spain, Mexico, and I don't know, Argentina, random the countries, well I guess those are on my mind.

Speaker 1:          10:06          Then we know that k should be three because we have three countries that we're targeting. But if we don't know how many classes we want, these are just unknown unknowns. Then we will have to decide what the best case value is. And that can be a guess and check method, but it's actually a smarter way to do that. And it's called the elbow method. So the elbow method is a very popular method. Well you have to do, you have to rub your elbow and I'm just kidding. So what happens is you've got this graph here, okay. And it looks like an elbow, right? It's got this elbow point right here. So what you do, and I've got this part in Javascript to show you what this algorithm looks like. I know Javascript, right? So here's what it looks like. We perform. K means once. So we, so we define a set of key values between let's say between one and 10 okay?

Speaker 1:          10:54          So that's a good starting point. K could be either one cluster or it could be 10 clusters. Let's try k means for all 10 of those. And so what we do is we perform k means for all of those key values. And then so that's what this is. So for, for as many k values as we have between zero and 10 let's perform k means to find all those clusters. And then for each of the clusters, let's find the mean value. And what is the mean value? It is the error value. So what is the distance between the centroid and all of its points? And that's going to be the mean or the error. And what we want to do is we want to compute the sum of the squared error values. And so that's what this line is right here, the sum of the squared hairs.

Speaker 1:          11:35          And so we say math dot pal, which means square the data point for each data point minus the mean. So for every data point in ours, in our Dataset, we're going to take a data point minus the mean value and square it. And so what that's gonna do is if we were to graph that for all of those number of cluster values for all those k values, we'll see that the sum of the squared errors for each of those iterations of k means makes this elbow like graph. And what we want to do is we want to pick the k value that is right at the elbow, right at that tipping point right here. So it would be six in the case of this graph. And that is our optimal k value. Okay. Because after that there's very diminishing returns. As you can see, we want to find the minimal error value.

Speaker 1:          12:19          And we've found that for this k value of six, the error is uh, at, it's not at its smallest, but at the point where it's everything after that is just diminishing returns. And we could say 10 or 12 or 14 but then for computational efficiency sake, we could just say six. So we don't have to run that many iterations. So we'll just say six. Okay, so that's the elbow method. All right, so, uh, three more points and then we're gonna get started with the code. So how is it, this is computer between the centroids in the data points, the Euclidean distance, right? We've talked about the Euclidean distance for linear regression, right? Euclidean distance. It's a very simple distance formula. You just take each of the data points and then you say, uh, x one minus x two plus c squared plus y one minus y two squared.

Speaker 1:          13:08          And then if you have z values, Z one minus z two squared and the square root of all of those. And that's the Euclidean distance. And what that does is it takes two data points and it gives us a scalar value. And that is the distance between all those data points. So you can apply the Euclidean distance to a multidimensional dataset where you have, you know, any number of dimensions. You just subtract the values for each of those dimensions, for each of those data points, uh, the square, and then you find the square root. Anyway, I'll show you the equation when we get to it. But yeah, the Euclidean distance and to answer any questions about why the Euclidean distance as opposed to other distance metrics. The answer is because k means technically minimizes the within cluster variance. And I know we haven't talked about this term yet and we will, but I'm just trying to be very detailed here.

Speaker 1:          13:56          And if you look at a definition of variance, which I'll define more in detail later, it is a, it is identical to the sum of the squared Euclidean distances from the center. So in Euclidean space and because it is identical to the sum of Euclidean distances, we use the Euclidean distance as our distance metric as opposed to something else like the Manhattan distance. Okay. So lastly or two more actually, when should you use this? If your data is numeric, and that means if your features are numeric, right, you have numbers for your features. If you have a categorical feature like uh, the shoe color, uh, or true or false, a boolean value, you can't really map that in Euclidean space, right? These are, these are categorical features. So, but if your data is numeric, k means is your, is your, is your algorithm. It's also the simplest algorithm you'll see.

Speaker 1:          14:48          It's actually a very simple algorithm. And when we talked about the pseudo code, it's a relatively simple algorithm. And the advantage it has over other techniques is that it is fast. That's the real key value. It's simple and it's fast. You know, a quick and dirty clustering algorithm. It's great for that. Okay. And it really shines when you have multivariate data. So that is more than one dimension. Okay. So lastly, two other examples of God here. One for fraud detection and then one for m and ist without labels. I know what that is possible and eyes he without labels. Yes, it's possible. Anything is possible. So anything is really possible here. So for credit card fraud detection and for finding these labels for these, uh, for these MSI is t images where let's just say we don't know the labels. Usually we know the labels.

Speaker 1:          15:36          Well, let's just say we don't know the labels and if we don't know the labels and it's going to cluster the images into what their, what their respective clusters should be. And those are clusters for ones and twos and threes in terms of the image, the images. Okay, so check those other two examples out. And now let's get into the code. Okay, so I'm moving here. I'm moving. Okay, so the first thing we want to do is we're want to import num Pi for matrix math map plot line to map plot live to plot out our graph. And then the animation module of plotline cause we're going to graph, we're going to animate some graphs in a second. Okay. So then let's take a look at our data set. Okay. So where is our datasets here? Our dataset is as what our data set looks like.

Speaker 1:          16:21          Okay. So we've got two dimensions. We've got two features in our dataset and the feature on the left is how many packets are sent per second. And the feature on the right is what's the size of a packet. Okay. So what we're trying to do is we're trying to detect the anomaly and that is a d. Dot. Sir. If you know what ddosing is, it's basically flooding a server with packet requests until a server goes down. So we're trying to see how many packets are sent per second from this user and then what is the size of that packet. Okay? And then that's that. Those are our data points and we have a few of these data points. Okay. So that's it. Just two features for our data and we'll talk about, like I said, dimentionality reduction later on. If we have a million features, how do we reduce it to two or three so that we can visualize it.

Speaker 1:          17:04          So that's our dataset that we want to load. Okay. So then here's what we've got here. This is the Euclidean distance. Now this is the formula that I was talking about. So given two data points, p and Q, take each of the values. So if it has, you know, Q, it could be x, it could be y, z take, he did the values, subtract them to get the difference, square it. And then add them all together and then square root, that whole equation. And so that's what the sigma notation to notes. It's a sum of a set of values. We're starting at equals one for end values. Find the difference difference between all the data, all the feature values in a data points squared, and then find the square root of all of that for the, some of the sum of the squared errors. Okay.

Speaker 1:          17:49          Uh, technically the sum of the squared errors. Yeah, and that's Euclidean distance. So that's a Euclidean distance. And so now let's look at the actual algorithm itself. So what I've done here is I've defined it's hyper, it's hyper parameters, and then I'm going to code out the algorithm itself. Okay? So four k means we've got a k value that is a number of clusters. And since we were just going to say too, okay, we have two clusters that we want and then we have an epsilon value. And the epsilon value is, it's a zero. It's a threshold. It's the minimum air to be used in the stop condition. Okay. When we want to stop training, okay. When their air is zero. And then we have our distance. So what, what type of distance? We want to compute the EUCLIDEAN distance. Okay, so let's keep going.

Speaker 1:          18:39          Let me make it a little bigger. So what we're going to first do is store this, the past centroids in this history of centroids list. Now this is not really a part of the algorithm. This is just for us. So we can then graph how the centroids move over time later on so that we can visualize it. Okay? So then we're going to say, okay, the distance metric is going to be Euclidean. So we define it right here as this method, that variable. And then we set the data set. So we load up that data set from txt. We've got those two dimensions, right? The amount of packets that are sent, and then the size of each packet. Okay? So we've got our data set, we've defined the distance metric, and now we're going to say, okay, let's get the number of instances and the number of features from our Dataset by taking this shape attribute of our dataset.

Speaker 1:          19:26          So our rows and our columns or rows are going to be the number of data points we have. And there are columns are going to be the number of features we have. So we want to store those two values. Then we're going to define K centroids, right? So like I said, we randomly plot the centroids on the graph and where they're going to be, how many centroids they're going to be. K centroids special k the cereal. Okay. So, um, okay, so we're going to say our Dataset is going to be, we're going to say, okay, so from our Dataset, how many clusters do we want to find right? Chosen randomly. So we're going to say size Kay. So we're gonna find a random number between zero and a number of instances that we have that that is a number of data points minus one of size.

Speaker 1:          20:12          K. Okay. And so we're going to store that the, the centroid values in this prototype's variable, right? So we've got our centroids and then, uh, we want to set these to our list of past centroids as well. So we're going to take those centroids that we just defined randomly and set them to our history centroids list. So then we can just keep a copy of it over time and we'll keep adding our centroids that are calculated to this history centroids list so we can graph it later, you know, for our own visualization. Okay. And then we have our prototypes old list. It's going to be initialized as a bunch of Zeros. It's empty, it's an empty vector or tensor. And so that's going to keep track of centroids every iteration. Right? So we've got our prototypes, which is our current and our prototypes old, which is are the set of centroids that we had before.

Speaker 1:          21:04          And those two are what we're going to use for the actual algorithm, the histories or the histories. Centroids is just for us to see how it changes over time. Okay. And then we have one more list and that is the belongs to list to store the clusters over time. The clusters themselves, all the data points contained in a cluster. Okay. And then we have our distance method, which is going to take the current prototypes. And then the prototypes old and the distance between them. And it's going to store that in the norm. Uh, variable. Okay. And then the number of iterations, which we'll start up as zero. Okay, so now let's go ahead and write out this algorithm, shall we? So we're going to say, okay, so while the norm, the distance is greater than epsilon, where epsilon is zero, in our case we're going to say, well, the number of iterations we're going to add one, cause this is our first iteration.

Speaker 1:          21:54          We have begun training our model. So we'll say, okay, well the iteration is going to be one. And so let's compute with enormous. So given our prototypes where we are currently, let's compute the distance between those prototypes and the prototypes that we had before. And that's going to be our norm. Then we're going to say, okay, so for each instance in the Dataset, so let's go through every single instance. So we'll say, okay, well what is the index of an intense and what is the actual value of that instance? Those are the two variables that we're going to use to enumerate or iterate over the data sets. We'll say, let's define a distance vector of size. Kay. So now it's time for us to define what this distance vector looks like. So we'll say, okay, so the distance vector is going to first of all be initialize as a set of zero values of size k.

Speaker 1:          22:46          Okay. So it's uh, it's going to be empty at first and we're going to add value to it over time. Okay. So we'll say, okay, so for each centroid value, we have our random centroid values, right? We already computed the, there's values randomly at the beginning, right? So we're going to say, okay, so eat for each centroid value. Let me just make this a comment for each century value. So for the, for the prototype, and there's a stored in our prototype, so we'll say for each index and then the actual value for the prototype, let's go ahead and do this for call of those prototypes. So for each instance in our Dataset, and then for each centroid, so we have two nested for loops, right? And then for each centroid and numerate prototypes, we're going to say we want to compute the distance between what do we have to do.

Speaker 1:          23:34          We have to compute the distance between each data point and its closest and each centroid. So for every data point we went to compute the distance between it and every other centroid and we want to find the minimum distance centroid that closest centroid to it so we can put it into that specific cluster. So it was like compute the distance between x and x is the data point and centroid. So we'll say distance vector is going to be index prototypes. So for the centroid value, so here's what we're going to do. We're going to compute the distance using distance method between the prototype, which is the centroid and the instance value. So for all of those x values, for all those data points, we're going to compute the distance and story and a distance vector list, and then it's going to be indexed by the index prototype.

Speaker 1:          24:28          Okay? Uh, and then we're going to do that for all of them. So that's why we have a nested for loops. So that's going to store all of those distances in that distance vector method. And then when we want to do is we want to find the smallest distance and assign that distance to a cluster. So we're going to say, okay, what is the what, what cluster does each disk does each a data point belonged to? And we'll say, well, we'll define it in his belongs to array and define it by its index instance. And so now we'll do that arguments that where we'll find the minimum value for between all those distance vectors that we calculated. Okay. Between all of those distance values and we use the Euclidean distance that we define previously in a, in its own function. And so as we were going to say, okay, so we're going to store them all in there and then we're going to say, okay, so then we have a list of temporary prototypes. Those are temporary centroids because we're going to update our centers in a second and we want to save where we are right now so we could store it in our history list. So say we'll create, um, a little tenser right here and it's going to be up sized k by the number of features, right? Because that's the number of clusters we have. And we're going to say,

Speaker 3:          25:48          okay,

Speaker 1:          25:49          for each cluster, hold on for each cluster and our k of them, let's say. Okay, so for, for each cluster and there are k of them,

Speaker 1:          26:04          let's go ahead and say, let's get all the points assigned to a cluster. Let's get all of those points assigned to a cluster, right? So we're going to say, okay, I'm just going to paste it in for it cause it's a little faster. We don't have a little bit left. Okay. So for each cluster in their k of them, we're going to get all the points assigned to a cluster. Okay? And that's what this, what this uh, line does right here. We're going to get all those points assigned to a cluster and store them in instances close. Okay? So those instances, close contains all the data points within a specific cluster. And we're doing this for each of the clusters, so, right. And for each cluster k of them, we're going to find the mean of those data points. And so now you get the k means for k clusters, compute the mean of the data points in that cluster using n p.

Speaker 1:          26:48          Dot. Mean. So that is the average. We add them all up and then divide by the number of them for all of those instances and store that mean in prototype. And then we're gonna that's gonna be our new centroid. So we'll add our new centroid to our temporary prototypes list and we did that so we can then take that temporary prototype and assign it to our main prototype list. Okay. So then were the temporary prototype variable acts as a buffer as well, computing new centroids to store them and then we can add it to our main prototypes variable. And then we have our history centroids list, which will also append to just for us to calculate this for us to visualize how this centroids move over time later on. And you'll see exactly what I mean by that. At the end of this, we were turned our calculated centroids and that is at the end of our algorithm and we have reached convergence and then our history of centroids all those centuries that we've calculated over time and then belongs to which is the c, the all the clusters or the where we cluster all those data points and then the index is going to be the cluster that it belongs to that k cluster.

Speaker 1:          27:52          And so that's how we define all those data points in a single cluster. Okay, so that's it for the algorithm. And then we're going to graph what this looks like. So in our plotting algorithm we're going to say, okay, well we're going to have two colors, red and green for our central, we'll define them here. We'll split our graph up, bites, acce access and it's actual plot. And then for each point in our Dataset, and we've got several points in our data set, we're going to say, okay, we want to graph all these points by their cluster color, which we've, which we're going to define. So all the, all the data points in one cluster are going to be one color and all the data points and another cluster are going to be another color. Okay? So then we'll get all those points by looking in.

Speaker 1:          28:34          D belongs to list, okay? By its index, right? So for the index, for each point in our Dataset, get the instances, those are all the data points that belonged to a specific cluster and then assign each data point in that cluster a color and plot it its own respective color, okay? And so that's what we do here. And then we're going to log the history of centroids. Remember how he said how we want to see what the history of centroids that our calculators are so we can check, see how it changes. That's what we're doing here. So in the history points list, we'll say for each centroid ever calculated, print them all out and then plot them in their own graph. Okay? So that's what's happening in the method. And then we're going to actually execute it, right? We've written methods for our K-means Algorithm.

Speaker 1:          29:19          We've written a method to plot out our results and now we can execute these methods. So we're going to load our Dataset, train the model on that data where k equals two. So we want to clusters and then it's going to give us back our computed centroids the history of all the centers that were computed over time and in our list of data points defined by the cluster that each belongs to. So we could take those three values and then use them as parameters for our plotting function. And that's going to plot our results. And so when we run that, we're going to get these two clusters, right? So if, if we didn't run our k-means algorithm and we just plotted our data, it would look just like this, except there would be no color. Cause we don't know what the clusters are. But what happened was the algorithm learned that the, that the right clusters were red and green for the, for these, for these respective clusters.

Speaker 1:          30:15          And the blue points are the central central points. And so those are the centroids. Okay. And so over time what happens is, look, here's, here's what I mean by over time, over time, what happens is it learns the ideal center points for the center for the centroids. Okay? It starts, it learns these ideal center points over time. The first, if they're randomly blue and then it gets better and better iteratively, it finds better and better centerpoint's find the most optimal center point for the cluster that we're going to learn. And then it plots it. Okay? So that's it. Please subscribe for more programming videos. And for now, I've got to find my center. So thanks for watching.