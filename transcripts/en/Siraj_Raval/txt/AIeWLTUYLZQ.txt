Speaker 1:          00:00          Hello world, it's Saroj and today we're going to implement a popular reinforcement learning technique called policy gradients to help us choose which slot machine to play that will give us the highest winnings. We've talked a good bit about supervised learning. It's where we build an algorithm based on both input and output data, which is great for classification tasks. Unsupervised learning is when we don't have a history of outputs. We have to build an algorithm based on input data alone. Reinforcement learning is similar in that is also building an algorithm based on input data alone, but we framed the problem in a different way. The algorithm presents a state, it depends on the input data and it's either rewarded or punished via an action that it takes and this continues over time. It learns from the reward or punishment and continually updates itself over the longterm to maximize the reward.

Speaker 1:          00:56          Reinforcement learning doesn't get as much love these days from the Ai community as the other two do because some of the most interesting problems right now, like those in speech recognition and LP and computer vision or areas where it's hard to define the notion of a longterm reward, don't hate three and force. Currently the problems that are best solved by reinforcement learning are either ploy problems like getting an NPC to get through a level without dying or really complex problems like self driving cars or folding laundry or understanding my ex-girlfriend. Things that you could do in a simulation, human level tasks. So a lot of the work in reinforcement learning is theoretical instead of application-based, which is just as important. Don't get me wrong, however, there are some real world use cases today Finnair the flight booking company, for example, uses RL to decide what action to take for what customers to increase their lifetime value.

Speaker 1:          01:53          And deep mind reduced Google's cooling Centerville by 40% by using RL to decide the most efficient energy routing strategy for the lowest cost. So let's take a look at our problem, which comes from probability theory. The problem is that a Gambler, which we'll call our agent, has to decide which slot machine to play, how many times to play each machine and in which order to play them. Every time that a machine is played, it outputs a reward value, which is randomly generated from a probability distribution specific to that machine. The goal of the Gambler is to maximize the sum total of rewards earned through a sequence of Liverpool's. He iteratively plays one lever per round and observes the associated reward. To do this, this is called the multi-armed bandit problem. The band at being the name of an old style slot machine with an arm or several on the side that you pull down.

Speaker 1:          02:46          The agent has to make a choice between using machines that are known to produce good results, exploitation and trying out new machines that have unknown results but could give better results than the others. Exploration. Exploitation is optimizing decisions based on existing knowledge and exploration is a company to acquire new knowledge. It's a tradeoff that all reinforcement learning agents make when optimizing for a reward value, and it's particularly relevant in this problem. So let's start initializing some values. After importing tensorflow and num py, the only two libraries will meet to use. We can define our bandits. We'll be using a four armed bandit. That is one slot machine with four levers and we can refer to each arm has abandoned. So we'll define our bandits as a list and each of these values will help decide if a reward is given when pulled the lower the bandit number, the more likely we'll get a positive reward.

Speaker 1:          03:39          The higher the bandit number, the more likely we'll get a negative reward. We want our agent to choose the bandit that will give that reward and we'll initialize a variable for storing the total number well, next to find a pull bandit function which given a banded, you will first generate a random number from a normal distribution with a mean of zero. Then compare the parameter value to the generated number. Depending on the result, they don't either return a positive or negative reward. In practice, this model is used anytime you have a project with a fixed budget, it can be used to help best allocate resources to maximize success. Since it's specifically designed to deal with the uncertainty about the difficulty and payoff of each possibility. I wish I had this thing in college. Our agent needs to learn which kind of reward it gets for each possible action so that it can choose the optimal ones.

Speaker 1:          04:27          It's learning a policy, so we're going to use a popular method called policy gradients. To solve this, we'll use a simple neural net that learns a policy for picking the best actions and adjusting its weights through gradient descent, using realtime feedback from the environment. Since we're only using a single bandit or agent ignores the state of the environment just like the u s government, there's only ever a single unchanging state. If we were introducing multiple bandits, then our agent would need to take state into account when deciding an action. We would lord a value function instead, but let's keep it simple with a single state.

Speaker 2:          05:03          When you force the policy actions or qualities.

Speaker 1:          05:09          Our policy gradient network consists of a set of weights and each weight corresponds to each of the possible arms to pull and represents how beneficial our agent thinks it is. To pull each arm. We'll initialize the weights to one, which means our agent will be optimistic about each arms potential reward. When we update our network, we'll use what's called an epsilon greedy policy. This is a way of selecting random actions with uniform distributions from a set of available actions. Using this policy, either we can select random actions with epsilon probability or we can select an action with one minus epsilon probability that gives maximum reward in a given state. Well, define epsilon is 0.1. It's the chance of taking a random action. Basically, most of the time our agent will choose the action that corresponds to the largest expected value, but sometimes with the it will choose randomly so this way it can try out different arms to continue learning about them.

Speaker 1:          06:03          Our agent is the neural network. It's feed forward and only has one set of weights. We'll initialize them has a tensor where each is a set of one for the number of bandits. Then we'll use the Arg Max function to choose the weights with the highest value and store that as our chosen action. We now need to establish what this training process looks like since we want to feed the reward and chosen action into the network to compute the loss and then use that to update the network. We'll initialize tensorflow, placeholders for both the reward and the action values. Next, we'll define the responsible way it corresponds to the units in the output layer, which corresponded to the chosen action. When updating the policy, we want to update the likelihood of the actions we actually took as opposed to all possible actions. So this will be a slice of our weights and we can define the size of it as well.

Speaker 1:          06:52          So this is what our policy loss equation looks like. This is what we want to minimize this character is the policy which we take the log of and a is the advantage. This is a critical part of our l. It's a measure of how much better an action was. Then some baseline. There are different ways of deciding what that baseline is and it can get pretty interesting, but right now we'll just set it to zero so we can just think of it as just a reward we receive for each action. This loss function lets us increase the weight for actions that give a positive reward value and decrease them for actions that give a negative reward value. When we define our loss function programmatically, we can see that it corresponds to the equation where reward hold. There is the advantage. We can then optimize it with gradient descent and given learning rates.

Speaker 1:          07:40          When we minimize our loss, it will return a gradient update. So for our training step we'll initialize ATF graph. Then for a given number of episodes, we'll either try a random action or choose one from our network. Exploitation versus exploration will receive a reward from our action of picking one of their benefits. Then we'll update the network using our gradient wait values relevant to the action and all wait values. We can use a feed dick to feed in both the action and the reward will want to see which reward and which band that we are on during each iteration. So we'll print them out. When we compile this, we can see that bandit fours values increase way faster than the other ones. It decides what is best and then those all in audit, we can extend this coke later on so that both state and action affect reward, which would be considered the contextual bandit problem.

Speaker 1:          08:28          So let's go over what we've learned. Reinforcement learning is usually applied to toy problems or really complex problems. Policy gradient methods are a type of RL technique that optimizes a policy with respect to the expected long term return using gradient descent. And we can apply this strategy to the popular multi-arm banded problem, which asks how to best allocate resources to maximize success. The coding challenge winner for this video is Mike Mcdermott. Mike improve the bleeding edge memory network model from my last video to create a Q and a chat Bot by adding a bidirectional LSTM and time distributed dense layer to it. This is seriously amazing stuff. He could publish as a results to a journal wizard of the week, and the runner up is Michelle Botcher who also had publishable results and you can run his code right from the command line. You guys blew my mind and I bow to you. The coding challenge for this week is to use policy gradients to solve the contextual bandit problem. So state is taken into account, details are in the read me did humbling scope in the comments and winners are going to be announced next week. Please subscribe. And for now I've got to maximize my arm size. So thanks for watching.