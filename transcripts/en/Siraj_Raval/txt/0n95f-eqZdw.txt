Speaker 1:          00:00          Are the real surreal. No, I'm the real survivor. No, I'm the real Siraj. Hello world. It's Saroj and top research lab open AI has recently caused quite a controversy in the Ai Community. Their new AI model dubbed Gpt to is truly remarkable. It's able to generate texts that legit looks like a human, wrote it after being given some prompt. The controversial part though is that in their blog they stated that they didn't want to release the fully trained version of it because they were concerned about potential malicious applications of this technology. Within a few days of that blog post, almost every single well known AI researcher had something to say about it. On Twitter, the director of research at Nvidia got into a flame war with their policy director. The creator of Karastan announced it as unnecessarily hyping up research and many articles have now been written about it that make it seem like the terminator is about to bust out of our computer screens and kill us all.

Speaker 1:          01:04          Just use a picture of Jigglypuff instead. She is the most dangerous fighter in super smash brothers. The big question here is, should open AI have kept their model closed or should they have open sourced it? I'll answer that at the end of this video, but first I'll start by explaining its potential applications, both good and bad, how it works in technical detail and how we can prevent misuse. Open Ai has done some phenomenal work for the Ai Community. Their algorithm dubbed open AI five beat some of the world's best dota players. Recently they developed an algorithm that learned how to achieve a really high score in the notoriously hard game Montezuma's revenge from a single human demonstration. They trained a robotic hand to manipulate physical objects with unprecedented dexterity. I wonder what else that hand could do. Interestingly, a lot of the techniques they used had existed for many years, but what they showed was that with enough scale, these techniques will start to surpass all previous metrics.

Speaker 1:          02:14          Reaching a new state of the art. Their newest model gpt to is no different. It's a collection of existing ideas repackaged at scale and by scale I mean trained on a lot of data using a lot of computing power as is generally the case when it comes to deep learning research. The authors used a neuro architecture called the transformer configured it. What a progressively larger number of parameters. Then trained it to predict the next word in a sequence of text using a large dataset consisting of 40 gigabytes of text scraped from the web. When given the task of writing a response to the prompt, recycling is good for the world. No, you could not be more wrong. The algorithm generated a detailed coherent argument as to why recycling is bad for the environment. One of the open AI employees printed that response out and taped it above the companies recycling bin to remind people of the power of AI when given a prompt for what seems like a believable news story, it's able to generate the rest.

Speaker 1:          03:20          I've never seen such coherent texts generated by an AI before. It's more convincing than 21 savage, his American accent, like all technology. Humans can use it to empower other humans or exploit other humans in terms of empowerment. This could help end writer's block giving aspiring authors the ability to ideate much faster, give it a possible scenario and it'll generate what could happen, accelerating the ability to write. It could also allow for endless entertainment options in gaming, allowing for narratives, storylines and dialogues that are completely unique. Every time you play through a game, grand theft, auto and gpt to is able to not only write prompts, it's also able to translate texts from one language to another. Summarize long articles and answer trivia questions. That means we can use this as a coherent chat bot to help companies scale their customer support. We could give it any book or article or scientific paper we'd like and have it summarize the key points for us, gpt to will save us time, improve our ability to write and help create more fulfilling experiences in entertainment and gaming when it comes to exploitation.

Speaker 1:          04:34          However, this could be used to generate misleading news articles. It can also be used to impersonate others online and in general. This brings us even closer to completely automating the production of abusive or faked content to post on social media at scale. We can be sure it will be used in all the ways I've listed. Both good and bad propaganda is as old as language itself and propagandists have used every available technology to scale the distribution of their message. What's different now is that no longer is this kind of power limited to entities like governments with enough money to buy the necessary people and machines capable of spreading their message. Now anyone anywhere can do it at scale with a model like gpt to before we discuss ways of preventing misuse. Let's discuss how it actually works. Starting with, of course, as always, the data gpt to was trained on a huge corpus of links that were scraped from the Internet.

Speaker 1:          05:37          The researchers collected their training data by using reddit as a filter. They collected the most up voted links from reddit. About 8 million of them then scraped their text, creating a compact training Dataset of about 40 gigabytes in size. They called it web text and extracted the source text directly from html. Then they needed to decide how to encode the text so that I was in a proper format to perform machine learning on. They chose a technique called bite pair and coding or Bpe, which is used to encode the input sequence. BPE was originally proposed as a data compression algorithm back in the 90s when Green Day made good music then adopted to solve the open vocabulary issue in machine translation. Since it's easy to run into rare and unknown words when translating to a new language, since rare words can often be decomposed into multiple sub words, BPE finds the best word segmentation by iteratively and greedily merging frequent pairs of characters.

Speaker 1:          06:40          It starts by treating individual characters as tokens. Then iteratively mergers. The most common token pairs end times after tokenization, these tokens need to be arranged into matrices to be ready to fed into a model as inputs. Num Py, the Python Matrix. Math library helps do this for the model itself. They used what's called a transformer. Deep minds Alpha Star, which be a top starcraft two player also used a version of a transformer. The transformer architecture was created by the Google brain team and it's turning out to be one of the largest contributions to AI replacing recurrent networks as the goto model for sequence learning, gpt to stands for generative pre-training transformer to, it's an unsupervised language model built on the ideas of open Ai's first version compared to the original transformer architecture proposed by Google, the transformer decoder model used by open AI discards the encoder part, so there's only a single input sentence.

Speaker 1:          07:43          Instead of two separate source and target sentences. The model applies multiple transformer blocks over the embeddings of input sequences. Each of these blocks contains what's called a multi-headed self attention layer and a feed forward layer. Each of these layers perform a different series of matrix operations on the input data. The final output produces a distribution over target tokens. After a softmax normalization, which is known to output probabilities. There aren't explicit labels here. It's merely asked to predict the next word in the sequence given the previous words. If anything, the label is actually just whatever word it's asked to predict label section. For the loss function. It uses what's called the negative log likelihood, but without backward computation in the context window of size k located before the target word, the loss looks like this. It's not task specific either. It uses the pretrained language model directly, so say in a labeled Dataset, each input has end tokens and one label.

Speaker 1:          08:48          Why gpt first processes the inputs sequence through the pretrained transformer decoder and the last layers output for the last token is the hidden state. Then with only one new trainable weight matrix, it can predict a distribution over potential. Next words, every time it tries to predict the next word, it compute an error using the difference between the actual word and the predicted word and uses that to update its weights. Using Goodall gradient descent linked to how that works in the video description and that's it. It had zero prior understanding of language or how it works. Kind of like Kanye during interviews. The training data isn't even task specific, but after training on it over the course of a week, using several cloud TPU is that likely costs several tens of thousands of dollars. It was able to generate realistic texts, summarize articles, translate languages and answer questions, so how do we prevent misuse of this kind of technology?

Speaker 1:          09:47          A group of Cambridge researchers created a role playing game that anyone can access in the browser right now called get bad news.com you play the role of an anonymous person who slowly rises to power using an army of automated bots spreading fake news. It takes 15 minutes to play and lets you see what it's like to create and spread fake news at scale. After I played it, I definitely felt like I had a much more critical eye when it came to reading texts that I saw online. Educating ourselves on the potential for this technology is one part of the solution. Similar to how when people learned about the potential for Photoshop, they became less likely to be fooled when staying Photoshop pictures. The other part of the solution is ironically AI. Yes. Using AI to fight AI, I found a tool called big box that uses AI to classify whether a piece of text is real or fake after having been trained on a labeled Dataset.

Speaker 1:          10:45          It's basic, but a really cool demo you can download and run it pretty fast. Using docker, I gave it the generated gpt to text and it classified it as fake, which was pretty cool. Sometimes you have to fight fire with fire. Technology will solve many of our problems, but it will also give us new problems to deal with. Now back to the big question, was it ethical for open AI to have kept it's fully trained model private? Plenty of people right now have the ability to easily recreate gpt to from scratch. The instructions are all there. They released a paper blog post and the code for a smaller version of it. I'm certain that the team at open AI was aware of this before they released it all. What they did was brave. They've taken the first step of asking the question, how difficult should we make this technology to use and help start an important ongoing dialogue in the community?

Speaker 1:          11:39          If a model is trained on private data, like patient records, that model could easily be reversed, engineered to reveal patient identities. And even if a model is trained on fully public data, sometimes aggregated data in the of ml models can become more sensitive than the data as a whole. For example, an authoritarian regime could reverse engineer a model train on Twitter profiles to find the location of potential dissidents. Thus, it's important to use privacy preserving tools in general while developing AI models linked to how that works. We'll be in the video description in the future. Open Ai and other research labs, if they feel their model could be misused, should commit to open sourcing a model by a certain date. That way it gives the rest of the community time to come up with counter measures for the potential negative uses. And as for the sensationalist journalists, please stop using terminator images for AI new stories. These are mathematical tools that can be used to empower people or exploit people, not conscious super intelligent beings. They are matrix operations being computed on silicon chips powered by electricity does sum it all up in a single sentence. Keep calm and learn AI. And what do you think about an AI as a decision? Let me know in the comments section and please subscribe for more programming videos. For now. I've got to recreate gpt too, so thanks for watching.