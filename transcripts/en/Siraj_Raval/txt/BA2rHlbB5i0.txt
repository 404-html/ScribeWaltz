Speaker 1:          00:00          Hello world. It's Saroj and it's time to upgrade the entire Internet. I'm going to explain how the interplanetary file system or IPFS works. We'll also use it to build a simple video streaming web app onwards towards the permanent wet. The web is the backbone of our civilization. So much of our economy, our mission critical systems, our most basic communication channels depend on it to function. The Orchestra of apps that give us such amazing capabilities today are only possible because we built the necessary infrastructure to support them. What we call the web is just a collection of protocols. These are sets of rules, ideas on how to best transmit data that have won over the hearts of developers and then made standard by browser adoption and tool dependency. Anyone can write a protocol and the existing ones were written by people aiming to solve small problems for themselves and in the process ended up being adopted by everybody else because they worked really well. There's one protocol in particular, however that is in need of a serious upgrade http developed by Tim Berners Lee.

Speaker 2:          01:21          We

Speaker 1:          01:22          use http to access most of the web. It delivers virtually all file types and is considered a client server protocol. A client you on your laptop with the browser for example, can send the request message to a server, say amazon.com and the server will then return a response message like a home page. It's worked well enough, but we're starting to see some real problems with http. The first problem is bandwidth. More and more of the world is coming online fast and that means more people are demanding more data. Websites crash all the time because too many people are trying to access them at the same time. Imagine this scenario. Let's say I'm sitting in a classroom with some other students were studying sex robots. I want to share a useful picture with them so I upload it to Facebook. Then give them all the link so they can fetch the image.

Speaker 1:          02:12          Assuming there are say eight links and the total bandwidth required from my upload is eight megabytes. Then when 30 students request that image, now that's 240 megabytes of bandwidth usage. If I uploaded a video and that took 200 megs of bandwidth, when all the other people watch it, it'll take 48 gigs of bandwidth. This is a horribly inefficient routing scheme since they are so physically close to me and it's getting to the point where providing this amount of bandwidth is no longer profitable for companies. If we look at how much bandwidth has improved over the past few years, it's not very significant. Compare that to how fast our storage space is decreasing in cost and it's clear that we are fast saturating these pipes so the web will start to feel, which leads to the second problem. Latency. Unfortunately we can't get around the speed of light. It's a constant.

Speaker 1:          03:05          Could we just do an ICO? No. What? So the only way to make data transfer faster is to move the data closer to you, which is why some cloud services offer storage locations by region. Still if I need a file and it just so happens to be on a device one meter away from me having my device make a request through the backbone of the web to some nearby city isn't nearly as fast as just pulling it from that device directly. The third problem is resiliency in terms of connectivity. If your ISP has an outage and your towns connection to the global Internet goes down, you're out of luck. There's nothing you can do about that in terms of data permanence, if Wikipedia was hacked and all its data deleted, we'd lose that knowledge forever. It'd be akin to book burning Dwin. Only a few copies of certain books were available, but it doesn't even have to be that drastic an event.

Speaker 1:          03:56          Does this error look familiar to you? You've likely seen a four oh four before, which signifies that some content has been deleted or moved. Http links get broken all the time. It's like mini book burning all the time. If data isn't manually copied by someone, that data is gone forever. The last problem is centralization because data has become so centralized. Abuse services own most of our data and we have no say in how that data is used. This is unimaginable power. The services can manipulate large swaths of the populist with a few clicks and access to them can be easily blocked by governments and other oppressive forces like what happened during the Arab spring or during the pro democracy protests in Hong Kong in 2014 we have got to upgrade the internet. It's too important not to. IPFS was designed to help fix these problems and help us move towards the permanent web, uh, web where links never die and no entity controls your data.

Speaker 1:          04:58          While the http web is Ip addressed, the IPFS web is content addressed. When requesting data from a content address, you'll receive it faster since it'll route from whoever owns a copy of that content address closest to you. This helps make bandwidth usage much more efficient. When you upload a file to Ipfs, you get back it's immutable Hash. Each file in the network is identified by its content. Using this hash. The Hash is cryptographically guaranteed to represent only that file. Anyone can provide storage for IPFS data. The storage providers are incentivized with a crypto token and data is replicated and chunked across all of them by the network to maintain data permanence. To achieve this IPFS combines several great ideas from computer science in the past few years. First to store data, it uses a distributed hash table similar to one propose at MIT called cord. A DHT is a dictionary like interface to data that is stored on nodes that are distributed across the network.

Speaker 1:          06:06          The node that gets to store a particular key is found by hashing that key, so in effect, the Hash table buckets are now independent nodes. In a network. Data is exchanged between these nodes using a mechanism called bits swamp. Inspired by the popular bittorrent torrent protocol, the de facto way to transfer large datasets across the web. A user can discover nearby peers to share data with. Then when requesting a file, they can download many small bits of it for many different peers all at once. This format compensates for bottleneck points, which makes it much faster than downloading a large file from a single source to give structure to the DHT and let users find the data they need when they need it. IPFS uses a data structure called a Merkle Dag inspired by the gift protocol get uses a Merkle Dag to enable distributed version control for source code and IPFS uses it to give structure to the entire network.

Speaker 1:          07:02          It's simple and flexible. We can conceptualize it as a series of nodes connected together in the form of a directed a cyclic graph. It can look like a singly linked list or a tree, so whenever data is added to the Dht, the system generates a public private key pair and the user gets both. All right. Let's get our hands dirty with IPFS. We'll first install the IPFS client on our machines. There's a tar ball available for Mac and Linux and an exe for installation for windows. Once I downloaded it, I can untarnished and move the IPFS binary to somewhere in my executable so I can just run it directly from terminal like so it's installed. We can now initialize IPFS from terminal and it will create our node, generate an identity for us and allow us to read files on the network. If I read an address, I can see the text I just retrieved in terminal to go online.

Speaker 1:          07:53          We'll run the demon in another terminal. We can now discover our peers in the network by their peer identities. We can read files directly from these peers and there's even a fancy web console for us to be able to see where all the peers are in the world. We can use this Ui to manage our instance. Let's say we want to upload a video that we have locally. We can use the IPFS add command to add that video to the network and use the resulting hash we get as his identity. Then in terminal, we can view it directly or we can watch it via a local gateway. In the browser. Let's say we have a simple static web app that lets users upload video and view it. It's just a simple html webpage. With some assets we can add the whole directory containing our web app to IPFS directly.

Speaker 1:          08:40          The very last hash next to the folder name is the one we want. It's the highest level hash of the Merkle Dag we've just created. We could create a simple DNS text record and that view our site locally or@theonlinehdptoipfsgatewayatipfs.io it's called interplanetary because if and when we do become an interplanetary species, it's much more efficient to use a web that finds the data you need that's closest to you, not light years away. The three closing points are the http web is a few decades old now and has become slow impermanent and centralized. IPFS is a newer protocol that aims to create a permanent web or links our content not IP addressed and it uses a distributed hash tables to store data, a Merkle, Dak to give it structure and a bit torrent like data exchange mechanism to maximize bandwidth. Last week, coding challenge winner is two showers. Tony Open mind needed to be able to compute operations on a tensor of arbitrary type, so to shard demonstrated how to do this programmatically. Such an epic PR, great job, and the runner up it's row hot, so walked. Who implemented some much needed and awesome styling updates to open minds documentation. This week's coding challenge is to create your own simple web app using IPFS post who get help. Link in the comments because I'm announcing the top two entries next Friday. I really hope you liked this video and if you did hit the subscribe button and all your dreams will come true. For now, I've got to make a crypto rap, so thanks for watching.