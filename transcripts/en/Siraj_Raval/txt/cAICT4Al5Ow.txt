Speaker 1:          00:00          How do we classify things? We consider people to be experts in a field if they've mastered classification, doctors can classify between a good blood sample in a bad way. Photographers can classify if their latest shot was beautiful or not. Musicians can classify what sounds good and what doesn't. In a piece of music, the ability to classify well, it takes many hours of training. We get it wrong over and over again until eventually we get it right. But with a quality Dataset, deep learning can classify just as well, if not better than we can. We'll use it as a tool to improve our craft, whatever it is. And if a job has been not in this, it'll do it for us. When we reached the point where we aren't forced to do something, we don't want to just to survive will flourish like never before. And that's the world we're aiming for.

Speaker 2:          00:45          Hello world, it's Saroj and today we're going to build an image classifier from scratch to classify cats and dogs. Finally we get to work with images I'm filling, hyping up to do to block arena. So how does image classification work? Well, there were a bunch of different attempts in the eighties and early nineties and all of them tried to a similar approach. Think about the features that make up an image and hand code detectors for each of them, but there is so much variety out there. No two apples look exactly the same, so the results were always terrible. This was considered a task only we humans can do, but in 98 or researcher named young lacount introduced a model called a convolutional neural network capable of classifying characters with a 99% accuracy which broke every record. Macoun CNN learned features by itself in 2012 it was used by another researcher named Alex for Shefsky at the yearly image net competition, which is basically the annual Olympics of computer vision and it was able to classify thousands of images with a new record accuracy at the time of 85% since then, CNN had been adopted by Google to identify photos and search Facebook for automatic tagging.

Speaker 2:          01:55          Basically, they are very hot right now, but where did the idea for CNN has come from you? These cabinets all be inspired by a cortex. Every single thing we see and just the board text. Some cells fire up the lines and stuff aside. Neurons in a column organize alive and this helps us class of why what we see. Cats, dogs, pigs, man, even Broccoli comp. Next, learn dope ass doses from features. Cross it by me as the king of tee shirts. Well first one to download our image Dataset from cackle with 1,024 pictures of dogs and cats. Each and its own folder. We'll be using the care os deep learning library for this demo, which is a high level wrapper that runs on top of tensorflow. It makes building models really intuitive since we can define each layer as its own line of code. First thing's first, we'll initialize variables for our training and validation data.

Speaker 2:          02:44          Then we're ready to build our model. We'll initialize a type of model using the sequential function, which will allow us to build a linear stack of layers so we treat each layer as an object that feeds data to the next one. It's like a Conga line kind of know the would be a graph model which would allow for multiple separate inputs and outputs, but we're using a more simple example. Next, we'll add our first layer, the convolutional layer. The first layer of a CNN is always the convolutional layer. The input is going to be a 32 by 32 by three herea pixel values. The three refers to RGB values. Each of the numbers in disarray is given a value from zero to two 55 which describes the pixel intensity at that point. The idea is that given this as an input, our CNN will describe the probability of it being of a certain class.

Speaker 2:          03:31          We can imagine the convolutional layer as a flashlight shining over the top left of the image. The flashlights slides across all the areas of the input image. The flashlight is our filter and the region it shines over is the receptive field. Our filter is also an array of numbers. These numbers are weights had a particular layer. We can think of that filter as a feature identifier as their filters slides or involves around the input. It is multiplying its values with the pixel values in the image. These are called element wise multiplications. The multiplications from each region are then summed up and after we've covered all parts of the image or left the feature map, this will help us find not buried treasure, but a prediction which is even better. Since our weights are randomly initialized, our filter won't start off being able to detect any specific feature, but during training, our CNN will learn values for its filters.

Speaker 2:          04:24          So this first one will learn to detect a low level feature like curves. So if we placed this filter on a part of the image with a curve, the resulting value from the multiplication and summation is a big number. But if we place it on a different part of the image without a curb, the resulting value is zero. This is how filters detect features. Well, next past, this feature map through an activation layer called relu or rectified linear unit. Relu is probably the name of some alien, but it's also a nonlinear operation that replaces all the negative pixel values in the feature map. With zero, we could use other functions, but relu tends to perform better in most situations. This layer increases the nonlinear properties of our model, which means our neural net will be able to learn more complex functions than just linear regression. After that, we'll initialize our max pooling layer.

Speaker 2:          05:14          Pooling reduces the dimensionality of each feature map, but retains the most important information. This reduces the computational complexity of our network. There are different types, but in our case we'll use MACs, which takes the largest element from the rectified feature map. Within a window we define and we'll slide this window over each region of our feature map, taking the Max values, so a classic CNN architecture. It looks like this. Three convolutional blocks followed by a fully connected layer. We've initialize the first three layers. We can basically just repeat this process twice more. The output feature map is fed into the next convolutional layer and the filter. In this layer we'll learn to detect more abstract features like pause and dose. One technique will use to prevent over fitting that point when our model isn't able to predict labels for novel data is called dropout. A dropout layer drops out a random set of activations in that layer by setting them to zero as data flows through it.

Speaker 2:          06:10          To prepare our data for the dropout or first flatten the feature map into one dimension. Then we'll want to initialize a fully connected layer with the dense function and apply relu to it. After dropout, we'll initialize one more fully connected layer. This will output an end dimensional vector where n is the number of classes we have, so it would be too, and by applying a sigmoid to it, it will convert the data to probabilities for each class. So how does our network learn? Well, one to minimize a loss function which measures the difference between the target output and the expected output. To do this, we'll take the derivative of the loss with respect to the weights in each layer. Starting from the last to compute the direction we want our network to update will propagate our loss backwards for each layer. Then we'll update our wake values for each filter so they can change in the direction of the gradient.

Speaker 2:          06:59          That will minimize our loss. We can configure the learning process by using the compile method, what we'll define our loss as binary cross entropy, which is the preferred loss function for binary classification problems. Then our optimizer rms prop which will perform gradient descent and a list of metrics which we'll set to accuracy since this is a classification problem. Lastly, we'll write out our fit function to train the model, giving it parameters for the training and validation data as well as a number of epochs to run for each and let's save our weights so we can use our trained model later. Overall accuracy comes to be about 70% similar to my attention span and if we feed our model a new picture of a dog or cat, it will predict it's labeled relatively accurately. We could definitely improve our prediction though by either using more pictures or by augmenting an existing pretrained network with our own network which is considered transfer learning.

Speaker 2:          07:52          So to break it down, convolutional neural network are inspired by the human visual cortex and offer state of the art and image classification CNNs Lauren filters each convolutional layer that act as increasingly abstract feature detectors. And with Karastan tensorflow you can build your pretty easily. But winter of the coding challenge from the last video is Charles David Blot. He use tensor flow to build a deep net capable of predicting whether or not someone would get a match or not. After training on a Dataset and had a pretty three data visualization of his results wizard of the week and the runner up is la man got clean, organized and documented code. The coding challenge for this video is to create an image classifier for two types of animals. Instructions are in the read me poster, get humbling in the comments and I'll announce a winner next Friday. Please subscribe. If you want to see more videos like this, check out this related video. And for now I've got to upload my mind, so thanks for watching.