Speaker 1:          00:00          Yo, what's your research paper called? Hello world and a paper titled Everybody Dance now out of UC Berkeley just dropped and it's causing a lot of hype in the Ai Community right now, given a source video of a person dancing, ideally a professional for the best results. They were able to transfer that dance routine to another person called the target. After collecting just a few minutes of training data from the target performing standard moves, the implications of this technology are profound. It's like auto tune for dancing. No longer do we necessarily need to be able to dance well in order to create a video of ourselves stancing and notice that in their demo. Although the output video is clearly a bit pixelated and blurry to being somewhat into the uncanny valley, it was still able to generate even the reflection of the dancer in the glass behind him. Very impressive. In the short term, we'll likely see this technology be used for casual consumption in the form of a gift generating app or a snapchat filter.

Speaker 1:          01:13          People will use this as a fun pastime, but as it gets better, and it certainly will, we'll start to see movie studios, celebrities and advertising agencies using it to help enhance their productions in a low cost way. Why hire an entire background dance crew when you can just generate them? A lead actor in a movie or a commercial won't need intensive training to record any dance sequence and because it can lower costs so drastically expect to see better quality content coming from the immature video production community as well. In a way, it democratizes dance. The dark side of this, just like the deep phase technology that I explained a few months ago, is that it's moving us towards a world where any video evidence can be manipulated, people can be framed for crimes they didn't commit. Anyone can use AI to synthesize voices, faces, photos, and now video.

Speaker 1:          02:11          So we'll need to create new methods of identity verification. Since video, whether from a smartphone or a police body cam is still the key make or break element of many defense cases. One solution is to use AI to fight AI. In fact, giphy cat, a popular video hosting platform is already using AI to classify videos is real or fake. Another solution is drum roll please. A blockchain, a newer startup called factum has a solution that's already been tested by the Department of Homeland Security. Since the Bitcoin blockchain is an immutable data structure that no one can modify. Factum is using it to timestamp video data from, say a security camera at specific intervals because it digitally signs and hashes data instantly. You're right. As the pixels are pulled off the camera, they can confidently claim that a video was really taken by the camera that digitally signed the data.

Speaker 1:          03:10          In general, we can all use the blockchain to digitally signed and confirm the authenticity of a video file that relates to us. The more people that add their digital signature to a video, the more valid it can be considered. This kind of scheme, we'll need to factor in the qualifications of the people who sign a video file, but it's promising. So back to our researchers, this is not the first attempt in this field at creating new video content. I manipulating existing video footage. In 97 researchers created videos of a subject saying a phrase they didn't originally order I finding frames where the mouth position matched the desired speech. In 2003 a group used optical flow as a descriptor to match different subjects performing similar actions. More recently it was demonstrated that deep learning techniques can help with motion retargeting without supervised data using generative adversarial nets.

Speaker 1:          04:11          This work was very likely inspired by that one. Our researchers split up their training pipeline into three steps, pose estimation, global pose, normalization. Then mapping from normalized posts, stick figures to the target subject. Let's go over each first pose estimation. Once they found a suitable source video of a professional dancer, they needed to encode the body positions of the source subject and what better way to do that? Then by using the pretrained pose detector called open pose, we can accurately estimate all of the subjects multiple joint coordinates. This is a convolutional neural network, a specific series of matrix operations that was optimized for pose estimation by using the common strategy of gradient descent. They just downloaded the pretrained weights file. In fact, anyone can try out pose estimation in the browser today and I actually have a detailed video on how pose estimation works already.

Speaker 1:          05:10          Link is going to be in the video description. They took those coordinates and drew a representation of the resulting pose stick figure by plotting the key points and drawing lines between the connected joints and because of video is just a series of images in the form of video frames. They did this for every frame in the video creating a rich dataset of dance poses. They then normalized each input image to help account for the differences between the source and target body shapes and locations within the frame. I analyzing joint positions for each subject and using a linear mapping between the closest and farthest angle positions. In both videos. They use this Dataset as input to a generative adversarial network for training. The is composed of two neural networks called the generator and the discriminator. The generator applies a series of transforms to the input image to produce the output image.

Speaker 1:          06:08          The discriminators job is to then perform a binary classification, trying to discern if the output image is real or fake. That is if it's actually the target subject dancing or a fake version. The structure of the generator is called an in coder decoder. It takes the input image and tries to compress it into a much smaller representation using a series of incoders. These are operational blocks consisting of a convolution and an activation function. The idea is that by compressing it like this, we can hopefully have a higher level representation of the data. After the final Incode layer. The decode layers do the exact opposite using operational blocks consisting of a deconvolution and an activation function and reverses the action of the encoder layers. A performance improvement here is to directly connect the encoder layers to the decoder layers using skip connections. These skip connections give the network the option of bypassing certain encoding and decoding parts if it doesn't have a use for it.

Speaker 1:          07:14          The specific type of Incode or decoder architecture is called a you net. Meanwhile, the discriminator has the job of using two images, one from the target video and one from the generated output and deciding if the second image was produced by the generator or not. It's a convolutional network that structure looks similar to the encoder section of the generator works a little differently. The output is an image where each pixel value represents how believable the corresponding section of the unknown image is. To train this network, there are two steps. Training the discriminator D and training the generator g to train d first g creates an output image, the looks at the input target pair and the input output pair and produces a probability about how realistic they look. The weights of d are then adjusted based on the classification error of the input output pair and the input target pair.

Speaker 1:          08:14          Geez. Weights are then adjusted based on the output of d as well as the difference between the output and the target image. Both the Angie will improve over time during this process. This training procedure worked well enough, but they found that the generated dance video frames, we're still kind of choppy and didn't look very realistic, so they added in two techniques can portal smoothing and a facial GAM. Instead of generating individual frames, they modified their generator to predict two consecutive frames where the first output is conditioned on its corresponding pose, stick figure and the generated frame at the previous time step. The second output is conditioned on its corresponding posts stick figure and the first output. So the discriminator was actually tasked with determining both the differences in realism and the tempur coherence between the fake sequence and the real sequence and to increase the realism of the face.

Speaker 1:          09:18          They use an additional Ghana specifically for generating faces. Their model was able to create a reasonably long video of a target person dancing given body movements, but the results still suffer from some shakiness solutions to this could be to use more training data. Of course, another would be to try out a different normalization technique for both the source and target videos. Since the shakiness is likely a result of the underlying differences between how the source and target subjects move, given their unique body structures or to try out a different Gan loss function. Since Gan research is very active right now, one only needs to glance@archivesanity.com to see an example of what's being done there. Overall, this research is very exciting and opens up a lot of new possibilities for both researchers and entrepreneurs. Is this video real? Who knows? Hit subscribe. Any way to stay updated on my life's work. For now, I've got to synthesize a new GPU, so thanks for watching.