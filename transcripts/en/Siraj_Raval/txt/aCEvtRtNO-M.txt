Speaker 1:          00:00          Hello world. It's a Raj and let's see if we can teach an AI car how to drive to the top of a mountain using a super popular strategy called Q learning. Since Video Games are considered virtual worlds, we'll want to use reinforcement learning to help our AI learn how to best interact with its environment. That means using time delayed labels, Aka rewards to help it learn how best to act in the environment through trial and error to complete its objective. If an AI has a model of all the necessary elements of its environment, meaning the states rewards, transition model, et cetera. Basically everything that makes up a mark Haub decision process, it can easily then use a planning algorithm to compute a solution to whatever its objective is. So easy. A liberal arts major could do it. This is considered model based learning. An Ai will interact in the environment and from the history of its interactions.

Speaker 1:          01:01          The AI will try to approximate the environments models afterwards. Given the models it's learned. The AI can use the value iteration or policy iteration algorithms to find an optimal policy, but our agent doesn't necessarily have to try and learn an explicit model for its environment. It can also derive an optimal policy directly from its interactions with the environment without building a model, and this is called model free learning model. Free learning involves predicting the value function of a certain policy without having a concrete model of the environment. The simplest way to do this is using the Manet Carlo technique, meaning using repeated random sampling to obtain numerical results. But this only works on episodic tasks where you have a certain set of actions. Then the episode ends with some total reward money. Carlo learning states that the return of estate is just the mean average of the total reward from when a state appeared onwards.

Speaker 1:          02:06          So in order to reduce the variance, we can use a different method of prediction. Temporal difference learning updates the values of each state based on a prediction of the final return. For example, let's say Monte Carlo learning takes a hundred actions and then updates them all based on the final return. Td learning would take an action and then update the value of the previous action based on the value of the current action. Td Learning has the advantage of updating values on more recent trends in order to capture more of the effect of a certain state. Td has a lower variance than Monte Carlo as each update depends on less factors. However, Monte Carlo has no bias as values are updated directly towards the final return. While TD has some bias, as values are updated towards a prediction, both TD and Monte Carlo or actually opposite ends of a spectrum between full look ahead and one step look ahead, any number of steps can be taken before passing the return back to an action.

Speaker 1:          03:15          So a question arises. How many steps is optimal to look ahead? Well, it varies greatly depending on the environment and it's often a hardcoded hyper parameter. Td learning can be used to learn both the value function and the Q function. What's that? You said, does Q stand for quick time? No, thankfully. If we want our AI to always choose an action that maximizes the discounted future reward, we'd want to use some form of TD learning. We could define a function cue that represents the maximum discounted future award when we perform an action a in state s and continue optimally from that point on. The way to think about this function is that it's the best possible score at the end of the game. After performing an action a in a state s it's called the Q function because it represents the quality of a certain action in a given state.

Speaker 1:          04:10          Using it, we can estimate the score at the end of the game knowing just the current state and action and not knowing actions and rewards coming after that. So suppose we're in a state and deciding whether we should take action a or B, we want to select the action that results in the highest score at the end of the game. Once we have the magical Q function, the answer becomes really simple. We can just pick the action with the highest Q value. So we need to learn this Q function and we can call this process cue learning. We can express the Q value of state s and action a in terms of the Q value of the next state s. This is called the bellman equation. If you think about it, it makes sense. Maximum future award for the state inaction is the immediate reward plus maximum future reward for the next state.

Speaker 1:          05:04          The main idea in cue learning is that we can iteratively approximate the Q function using the bellman equation. In the simplest case, the Q function is implemented as a table with states as rows and actions as columns. So to start the Q table is initialized randomly. Then the agent starts to interact with the environment and upon each interaction the agent will observe the reward of its action and the state transition. The agent computes the observed Q value and then updates its own estimate of cute. So we've got a basic iteration going and it works for simple policies. However, there's a problem, it's not exploring like Hernand Cortez. Shout out to AP history. Let's say we're at a state with two doors, one door, it gives a reward of plus one every time and the other door gives zero reward, but has a 10% chance of giving a 100 reward on the first run.

Speaker 1:          06:05          The second door doesn't get lucky and returns a reward of zero. Since it's acting greedily, the Ai will only ever take the first doors since the considers it the better option. So how do we fix this random exploration? At some probability, the program, we'll take a random action instead of taking the optimal action. This allows it to eventually figure out that there is some extremely high reward hidden behind the second door if he tries it out. However, we want the AI to eventually converge at some optimal policy so we can lower the probability of taking a random action over time as the agent becomes more confident with its estimate of Q values. So using Q learning and open Ai's gym environment, we can teach a mountain car how to successfully climb a mountain. In this environment. There is a car on a one dimensional track between two mountains.

Speaker 1:          07:01          The goal of the car is to climb the mountain on its right. However, the engine is not strong enough to climb mountains without having to go back to gain some momentum by climbing the mountain on the left here, the agent is the car and possible actions are drive left, do nothing or drive right at every time step, the agent receives a penalty of minus one which means that the goal of the agent is the climb the right mountain as fast as possible to minimize the sum of negative one penalties it receives. The observation is two continuous variables representing the velocity and the position of the car. Since the observation variables are continuous for our algorithm, we desk or ties the observed values in order to use Q learning. Initially the car will take forever to climb if we select a random action, but after learning, it learns how to climb the mountain within less than a hundred times steps.

Speaker 1:          07:57          You're still with me, right? Of course you are. You beautiful wizard. Look, there are just three key points to remember here. Model. Free learning is when an AI can directly derive an optimal policy from its interactions with the environment without needing to create a model beforehand. Cue learning is a model free learning technique that can be used to find the optimal action selection policy using a Q function and the exploration versus exploitation dilemma is exemplified by the question of whether an AI should trust the learnt values of Q enough to select actions based on it or try other actions hoping that might give it a better reward. The winner of last week's coding challenge is odd. Nones a heat. I'd non created an AI in swift that navigates around a grid to find the goal without knowing it beforehand. Then it constructs an optimal path to the goal using model free reinforcement learning, superb job and shout out to Alberto Garces for creating a really awesome value iteration algorithm for getting a taxi from point a to point B efficiently. This week's challenge is to create a simple cue learning algorithm in python for an AI in any type of game world. Get humbling scope in the comments section, and I will personally review and announce the top two entries next Friday. Please subscribe for more programming videos, and now I've got approximately my value function, so thanks for watching.