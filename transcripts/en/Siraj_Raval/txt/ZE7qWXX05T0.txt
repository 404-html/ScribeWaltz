Speaker 1:          00:00          Hey, are you okay? I want to make music but I don't know how

Speaker 1:          00:09          hello world, it's Saroj and this episode we're going to talk about the bleeding edge and machine generated music. Then write our own music generation script in tenter and just under 90 lines of python. I'm a big fan of Hans Zimmer, so we've mapped to become a no, but I don't have the time to put in 10,000 hours to be like Han's, but just imagine a program where I could say I want a song that conveys the feelings of hope and wonder while sailing the seven seas, but also the gas just feeling of beating too many beans. The program would convert that speech to text and for each word it would find the associated vector. It will then compare those vectors to vector is created by a model trained on a labeled music dataset for range of human emotions. The output of the machine would then be a set of chords that he thought best represented those emotions.

Speaker 1:          00:51          It would lower the barrier to entry. For me to compose music. Machine learning is a layer of intelligence is meant to extend our own. I'm still really bad at folding shirts. A laundry folding robot would be so dope. Deepmind released a paper a few days ago called Wavenet, which is now the state of the art in both music generation and text to speech. Traditionally, audio generation models are concatenate IV. That means in order to generate speech from some tech sample, it utilizes a huge database of speech fragments by picking a few and combining them to create a full sentence. Same for music, this works, but it's hard to incorporate things like emotion and natural sounds. When the output is created. I putting a bunch of static fragments together. Ideally we could have audio generation that is parametric, where all the Info requires a generate audio is store in the parameters of the model that we give it.

Speaker 1:          01:37          That's what wavenet is capable of. Instead of generating an audio signal by passing its output through signal processing algorithms, it directly models the raw way form of the audio signal. Researchers don't really do that, but wavenet did the model they use with a convolutional neural network where each layer had dilation factors that led to the interconnectedness grow exponentially, but deeper. The data flow through the model and each generated sample at each step was fed back into the network to generate the next step. Let's take a look at the computation graph for the model. The input data, a single note start as a raw audio wave. We've first format the way so that it's better suited for processing. Then we encode it to produce a tensor with a number of samples and a number of channels. We feed that into the first layer of the convolutional network, which reduces the number of channels for easier processing and through a stack of layers.

Speaker 1:          02:22          With dilated convolution. We combine the outputs of all the layers and increased the dimensionality to the original number of channels. If he did all tore lost function, which measures how well our training is going and finally the output is fed back into the network, could generate the wave at the next time step. This process keeps repeating to generate more and more audio. This neural net was huge. It took 90 minutes on their GPU cluster to generate a single second of audio. Instead, we're going to build a more simple music generation script. In tensorflow, we want to first import num Pi is there scientific computing library in python, pandas as our data analytics library and of course tensorflow our machine learning library. Can you QDM will help us show a progress bar during training and finally made I'm manipulation, which is our helper library. Let's start off with the first step or hyper parameters or tuning knobs.

Speaker 1:          03:03          For our models, we're going to use a type of neural network called a restricted Boltzmann machine as our generative model. An RBM is a two layer neural net. The first layer is visible and the next layer is hidden nodes within layers are connected to note in the next layer, but no two notes of the same layer are linked. That's the restriction part. Each node decides whether to transmit any data, it received the next layer or not. By making a random decision. We want to first create a range of notes that our model will generate, so we'll write variables for our lowest note and our highest note as well as a note range, which is a difference between the two. Then we'll write variables for the number of times the size of the visible layers, the number of hidden layers, number of training, epochs, batch size, and finally our learning rate, which is a flow constant since it always stays the same.

Speaker 1:          03:43          Next weekend, right out variables. We'll start out by creating a place holder which will serve as our gateway for inputting data into our model. Then a variable that will store the weight matrix or the connections between our two layers. We also need to variables for biases, one for the hidden layer and one for the visible air. We didn't want to create a sample from our input in x. He's your aren't Gibbs sample helper method. Keep sampling is an algorithm that creates a sample from a multivariate probability distribution. It constructs a statistical model where each state depends on the previous state. Then use this randomness to obtain a sample across the distribution it creates. We'll get a sample of the hidden notes as well, starting from our original input and a sample of the hidden nodes starting from our generated sample and we'll update our wheat values based on the differences between the samples that we drew and d original values.

Speaker 1:          04:25          We will run these three updates, steps for our weights, hidden biases and visible Pisces when we later run our session and we can store all of these in our update variable. All right, it's time to start a session so we can run our graph. We'll initialize our variables. First. We're going to reshape each song so they are vectorized in a good training format. Then we're going to train our RBM on the examples one at a time. Once the model is fully trained, we will make some music. We were our Gibbs chain with a visible notes are initialized to zero to generate a sample and reshape the vector to be a proper format for playback. Finally, we'll print out the generated courts.

Speaker 1:          04:57          Dope. The state of the art and music generation uses a CNN to generate raw way forms parametrically instead of concatenate valley. That algorithm requires lots of compute right now, so we can use an RBM to generate audio samples based on training data easily. And Gib sampling helps us create audio samples during training by randomly obtaining them from a generated probability distribution based on the input data. And so the winter, the last coding challenge is row Han Burma. He created an image classifier for astronomers to classify galaxies as either elliptical or spiral that asks the weak. And the runner up is Chichi Leon for creating a bird classifier for biologists. I bow to both of you. The challenge for this video is to use the RBM script to generate an audio sample. Would they happy, upbeat sound, post your code in the comments. I'll judge them and announce the winner in my video one week from now. For now, I'm going to get more silver hair dye. So thanks for watching.