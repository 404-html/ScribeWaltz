Speaker 1:          00:00          Yes, I beat it that that impress you. If I built an AI, beat this for me, would that impress you? Hello world walking this Raj Ologie and this episode we're going to build an AI to beat a bunch of Atari Games. Games have had a long history of being a testbed for AI ever since the days of Pong. Traditionally game programmers have taken a reductionist approach to building AI. They've reduced the simulated world to a model and had the AI act on prior knowledge of that model. And it worked out for the most part. I guess not really, but what if we want to build an AI that can be used in several different types of game worlds? All the world models are different, so we couldn't just beat it in one world model. Instead of modeling the world, we need to model the mind. We want to create an AI that can become a pro at any game we throw at it.

Speaker 1:          00:38          So in thinking about this problem, we have to ask ourselves, what is the dopest way of doing this? Well, the London based startup deep mine already did this in 2015 deep minds goal is to create AGI. That's one algorithm that can solve any problem with human level thinking or greater. They reached an important milestone by creating an algorithm that was able to master 49 different Atari Games with no game specific tuning whatsoever. The algorithm is called the deep Q learner and it was recently made open source on get hub. It only takes two inputs, the raw pixels of the game and the game score. That's it based on just that it has to complete its objective, maximize a score. Let's dive into how this works since we'll want to recreate the results. First, it uses a deep convolutional neural network to interpret the pixels. This is the type of neural network inspired by how our visual cortex operates and expects images as inputs, images, or high dimensional data.

Speaker 1:          01:20          So we need to reduce number of connections. Each neuron has to avoid overfitting. Overfitting by the way, is when your model is too complex, there are too many parameters and so it's overly tuned to the data you've given it and won't generalize well for any new Dataset. So unlike a regular neural network, I CNNs layers are stacked in three dimensions and it's makes it easy to connect each neuron only to neurons and it's local region instead of every single other neurons. Each layer acts as a detection filter for the presence of specific features in an image. And the layers get increasingly abstract with feature representation. So the first layer, it could be a simple feature like edges, and then the next layer would use those edges to the text and shapes. And the next one would you use those shapes to detect something even more complex like Kanye, these hierarchical layers of distraction or what neural nets do really well, like really well.

Speaker 1:          02:00          So once it's interpreted the pixels and needs to act on that knowledge in some way. In a previous episode we talked about supervised and unsupervised learning, but wait, it's called reinforcement learning, reinforcement learning. It's all about trial and error. It's about teaching an AI to select actions to maximize future rewards. It's similar to how you would train a dog. If the dog fetches the ball, you give it a treat. If it doesn't, then you withhold the tree. So while the game is running at each time step, the Ai Execution Action based on what it observes and may or may not receive a reward. If it does receive a reward, we'll adjust our weights so that the AI will be likely to do a similar action in the future. Cue learning is a type of reinforcement learning that learns the optimal action selection of behavior for the AI without having a prior model of the environment.

Speaker 1:          02:41          So based on the current game state, like an enemy spaceship thing and shooting this since the AI will eventually know to take the action of shooting it, this mapping of state to action is its policy and it gets better and better with training. Deep Q also uses something I'll experience replay, which means the AI learns from the Dataset of its past policies as well. So we're going to build our gay bought in just 10 lines of python using a combination of tensorflow and Jim tensorflow is Google's ml library, which we'll use to create our CNN and Jim is open AI ml library, which we'll use to create our reinforcement learning algorithm and set up our environment. Oh, if you haven't heard, open AI is a nonprofit AI research lab focused on creating Agi and an open source way. You've got 1 billion bucks. Buy It from people like Elon Musk.

Speaker 1:          03:20          Good luck. Get the best. Let's start off by importing our dependencies. Environment is our helper class that will help initialize our game environment. In our case, this will be space invaders, but we can easily switch that out to a whole host of different environments. Jim is very modular opening. I wants it to be a gym for AI agents and training and get better. You can submit your algorithms. They're cipher and evaluation. They'll score it against a set of service. I'd metrics I approved. We'll also want to import our deep Q network helper class to observe the game in our training class to initialize a reinforcement learning. Once we've imported our dependencies, we can go ahead and initialize our environment well, set the parameters to space invaders and then initialize our agent using our DQ and helper class with the environment and environment type at the parameters.

Speaker 1:          03:58          Once we have that, we can start training by running the trainer class with the agent has the perimeter first. This will populate our initial replay memory with 50,000 plays so we have a little experience and train with. Then it will initialize our CNN to start reading and pixels and our key learning algorithm to start updating our agents decisions based on the pixels it receives. This is an implementation of the classic agent environment loop. Each time set, the agent chooses an action and the environment returns and an observation and a reward. The observation is raw pixel data which we can feed into our CNN and the reward is a number we can use to help improve our next actions. Jim Neatly returns these parameters to use via the step function which we brought in the environment helper class, so we started training. We can start the game with the play function of our agent. Object. We can go ahead and run this and terminal and the space invaders windows should pop up and we'll start seeing the AI started tending to play the game. It'll be hilariously bad at first, but we'll slowly get better with time. The Ai will get more difficult to defeat the longer you train it, and ideally you can apply to any game you create, but more info. Check out the links down below and please subscribe for more machine learning videos. For now, I've got to go fix a runtime error, so thanks for watching.