Speaker 1:          00:00          Hello world, it's Saroj and activation functions. Which one should you use for your neural network? There's so many out there. Artificial neural nets are roughly based on our brains neural net, not in the way that they're made up of a biochemical soup, but in the way that multiple nodes or neurons are interconnected and signals can pass through these notes. It's this hierarchical structure that gives us such amazing results. Our universe itself can be considered to have a hierarchical structure. Elementary particles form Adam's, which form molecules, which form cells, then organisms, planets, solar systems, and entire galaxies. To make sense of this hierarchical complexity, evolution has settled on a specific kind of structure for our brains that can represent all these layers of abstraction in an ordered way. And so when we model this rule of hierarchical layers on computers, we get very similar results. Even though the substrate is different, it's silicon.

Speaker 1:          00:58          So it's incredibly exciting to study the mathematical relations of deep neural networks and ways of improving them. Because by doing so, we're not just coming closer to better results for specific tasks or coming closer to discovering fundamental laws embedded in our universe. The basic idea behind how a neural network learns is that we first have some input data that we've got to vectorize. Then we feed it into the network, which means we basically perform a series of matrix operations on this input data layer by layer, and the simple case for each layer, we just multiply the input by the weights at a bias, apply an activation function to the result and pass the output onto the next layer to do the same thing. And we keep repeating that process until we reached the last layer. The final output value is our prediction. We find a difference between it and the expected output, which is the label.

Speaker 1:          01:50          Then use that error value to compute the partial derivative with respect to the weights in each layer going backwards recursively we then update the weights with these values and repeat the process until the error is as small as possible. And that's leaped dearness deep learning who the, so you might be asking, why do we apply these activation functions to our data? Couldn't we just multiply the input by the weight values, add a bias and propagate that result forward? Well, they do something really important. They introduce nonlinear properties to our network, so we can call them nonlinearities. But why is this a good thing? Let's take a step back. And linear function is a polynomial of just one degree, like y equals two x or y equals x. So if we were to map these functions on a graph, they would always form a straight line.

Speaker 1:          02:39          If we added more dimensions, they would form a plane or a hyperplane, but their shape would always be perfectly straight with no curves of any kind. That's what we call them linear. But every other equation is nonlinear. Polynomials have higher degrees like y equals two x squared. Trig functions like sign or co-sign. Nonlinear functions when produce a line that always has some kind of curvature. Linear equations are easy to solve, but they are limited in their complexity. We want to be able to represent any kind of function with our neural network and neural networks are considered universal function approximators. That means that they can compute any function at all. Almost any process you can imagine can be thought of as a function computation, trying to name the song that you're hearing, translating Spanish to English, punching an evil clone of yourself, so we need a way to compute not just linear functions, but nonlinear ones as well.

Speaker 1:          03:36          If we didn't use a nonlinear activation function, then no matter how many layers are neural network has, it would still behave just like a single layer network because summing these layers will give us just another linear function. This is not strong enough to model many kinds of data though, but by using a nonlinear activation, the mapping of the input to the output is nonlinear and we want it to be differentiable. That means we're able to calculate the derivative of it. An example of a differential function is x squared. Since we can differentiate it to two x, which is it's derivative. We needed to be this way so we can perform the backpropagation optimization strategy where we find a nonlinear Eric radiant to learn complex behavior. The whole idea behind activation functions is to roughly model the way neurons communicate in the brain with each other.

Speaker 1:          04:25          Each one is activated through its action potential. If it reaches a certain threshold, we know to activate a neuron or not, the activation function simulates this spike train of the brain's action. Potential input Tom's weight, add a bias, activate input, Tom's wait, ad buys activate, so we could think of a lot of possible activation functions, but how do we know which one to use? This choice is dependent on a couple factors, not including if it just sounds cool. Let's talk about the three most popular ones. Sigmoid 10 h and relative sigmoid has the mathematical form of f of x equals one over one plus e to the negative x. It takes some number and squashes it into a range between zero and one. It was one of the first to be used because it could be interpreted as the firing rate of a neuron where zero means no firing and one means a fully saturated firing.

Speaker 1:          05:22          It's pretty easy to understand, but it has two problems that have made it fall out of popularity recently. The first is that it causes our gradients advantage. When a neuron's activation saturates close to either zero or one, the gradient at these regions is very close to zero during backpropagation, this local gradient will be multiplied by the gradient of this gates output for the whole objective. So if the local gradient is really small, it will make the gradient slowly vanish and close to no signal will flow through to the neuron to its weights and recursively to its data. The second problem is that its output isn't zero centered. It starts from zero and ends up one. That means the value after the function will be positive and that makes the gradient of the weights become either all positive or all negative. This makes the gradient updates go too far in different directions, which makes optimization harder.

Speaker 1:          06:13          I can control these gradients. So how do we improve on it? Well, there's another activation function called the hyperbolic tangent function or 10 h. It squashes the real number into a range between negative one and one instead of zero and one. So it's output is zero centered, which makes optimization easier. So in practice it's always preferred to the sigmoid, but just like the sigmoid, it also suffers from the vanishing gradient problem enter Relo or the rectified linear unit. This activation function has become really popular in the last few years. It's just an ax zero x, which means that the value is zero when x is less than zero and linear with a slope of one when x is greater than zero. It was noted that it had a six x improvement in convergence over 10 h and the landmark image net classification paper by Kurczewski. A lot of times in computer science we find that the simplest, most elegant solution is the best, and this applies to revenue as well.

Speaker 1:          07:11          It doesn't involve expensive operations like Tan h or sigmoid, so it learns faster and it avoids the vanishing gradient problem. Almost all deep networks use relevant nowadays, but it's only used for the hidden layers. The output layer should use a softmax function for classification does it gives probabilities for different classes and a linear function for regression since a signal goes through unchanged. One problem that Relo has sometimes though is that some units can be fragile during training and die, meaning a big gradient flowing through a relo neuron could cause a weight update that makes it never activate on any data point again, so then gradients flowing through it will always be zero from that point on. So a variant was introduced called the leaky relative to fix this problem instead of the function being zero when x is less than zero and instead has a small negative slope.

Speaker 1:          08:01          There's also another popular variant called Max Salary, which is a generalized form of both Relo and leaky Relo, but it doubles the number of parameters for each neuron. So there's a trade off. The original question was what type of activation function should you use in a neural network? And the answer is rare, rarely revenue, revenue, revenue. But if a lot of your neurons die, then try a variant like the leaky relu or Maxell sigmoid just shouldn't be used anymore, nor should tan h. And although relish should be applied to the hidden layers, the output layers should use a soft max work classification or a linear function for regression. There are other activation functions out there and there's still a lot of room for improvement in this area. They are a crucial part of neural networks. So any new discovery here, we'll have huge impacts in the field moving forward. If you liked this video, hit the subscribe button for more like it. Check out this related video and for now I've got to take a leak. Irrelevant. So thanks for watching.