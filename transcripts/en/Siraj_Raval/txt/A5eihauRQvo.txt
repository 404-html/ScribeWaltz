Speaker 1:          00:00          Hello world, it's Saroj and let's build a game bought that learns how to get from point a to point B using a special type of reinforcement learning called Q learning. Reinforcement learning means that learning by interacting with an environment through positive feedback or reinforcement, it's similar to how you give a dog a treat, but only if it rolls over and it's evolved over the past few decades. In the late 1950s and American mathematician named Richard Bellman was trying to solve what he called the optimal control problem. This describes the problem of designing an agent to minimize some behavior of a system over time. Eventually, he and his colleagues finally discovered a possible solution to it, which was later called the bellman equation. It describes the value of a problem at a certain point in time in terms of the payoff made by previous decisions and it also describes the value of the remaining decision problems that result from the initial decisions by involving a systems various states. In this way, it broke the problem down into simpler sub problems. The bellman equation is now used in many, many fields. It helps with minimizing flight time for airplanes, maximizing profits for hedge funds, minimizing the beef that Soulja boy seems to have with everyone. No, no, no, no, no, no, no.

Speaker 1:          01:11          So bellman squad was making waves throughout the math community, but meanwhile, a psychologist named Edward Thorndyke was trying to understand how learning works. By studying the animal kingdom. He came up with what he called the law of effect, which states responses that produce a satisfying effect in a particular situation become more likely to occur again in that situation and responses that produce a discomforting effect become less likely to occur again in that situation. Thanks. Captain obvious now is actually pretty important. Discovery. One of his experiments was putting a cat in a wooden box and observing it while it tried a bunch of different ways of getting out until it finally hit the lever that opened the box. When you put the cat back in the box, it immediately knew that hit the lever to get out. And it was able to do that because of the process of trial and error, which is what thorn deck was essentially describing in the law of effect.

Speaker 1:          01:57          A couple decades later, a British computer scientists named Chris Watkins stock that perhaps these two ideas could be combined to create a new type of learning algorithm. The idea of designing an agent that minimize it, some behavior of a system over time, like the bellman equation and does so through the process of trial and error, similar to the law of effect. And so he invented a novel reinforcement learning technique. He called Q learning. So what is this? Let's say we had five rooms in a building connected by doors and we'll just think of everything outside of the building as one big room. All of spacetime is room five. We can think of the system as a graph. Each room is a node and each door is a link like room one has doors to both room five and three so they're connected. Our goal is to put an agent in any room and for it to learn how to get to room five through trial and error.

Speaker 1:          02:43          So the gold room, his room five does that room five as a goal, we can associate a reward value with each door, which is the link between notes. So doors that lead immediately to the goal room get an instant reward of a hundred doors, not directly connected to our goal room, gets zero reward in cue learning. The goal is to reach the state with the highest through a set of actions. So if each room is a state, each action is represented by an Arrow. And the mapping of state to action is the agent's policy. It uses the reward that you as a signal to improve its policy over time. And it stores what he has learned through experience and what's called the Q matrix. The rows represent the possible states and the columns are possible actions leading to the next state. It updates the Q matrix over time as it learns the best actions to maximize the reward.

Speaker 1:          03:23          Seems pretty useful, right? Cuellar he's gotta be used everywhere in video games, right? Consumer video game box needs to be good but not so good that a human couldn't beat them. The bots that used Q learning to master games like chess and checkers and most recently Atari Games become insanely good at whatever they play. Academic AI learns while consumer game AI generally just make educated guesses, it doesn't really learn and its actions are all scripted but as different as they are. The two fields are converging. As we discover more about machine learning. For example, in Forza Motor sports, you can create a drive Atar for yourself. It's an AI that learns how you drive by observing you and can then imitate your driving style. Having adaptive behavior like this will make games more interesting and there's a lot of potential for more of it. So let's write out a 10 line high level python script that uses cue learning to train a Bot to get from point a to point B.

Speaker 1:          04:13          This game is a five by five grid. Our agent is the Yellow Square and the goal is to find its way to the green square or the Red Square to end the game. Each cell represents a state the agent can be in and there are four actions up, down, left and right. Moving a step will give us a reward of negative 0.04 the red cell gives us negative one and the green cell gives us positive one so we ideally want to get to the green cell every time the game world is already built for us. So we'll just start off by importing that at the top. Then in our main function, we can go ahead and create a wild statement set to true because we want our agent to run indefinitely. Next we'll initialize our bots position from a worldclass and set it to the agent variable. So now we want our Bot to pick the right action to take in the game world.

Speaker 1:          04:52          And the question is how do we decide that I got a grid of squares measured five by five and I'm going to get to green in one piece alive. I'm going to make a cute matrix. Initial life. She'd been every single reward of mine or cod. Then I'm a pick an action straight how to queue. Then I'm a do it. Yeah, you just fresh and new update you with reward on beat and once I got that I'll go ahead and repeat. Yeah. We'll use our box position as a parameter for the Max Q function, which we'll choose an action from our Q matrix as well as the potential reward and then we can perform that action by inputting the action as a parameter to the do action method, which will return our bot the action. We took the reward received any updated bots position after taking the action.

Speaker 1:          05:32          Now we're ready to update our Q matrix, so we'll use the updated bots position as the perimeter will print out both parameters to terminal so we can observe the results. We'll run this script by typing in python learner.py into terminal and it'll pop up as a Gui. The Bot will immediately start trying out possible paths to get to the green one, and we can observe this score in terminal improving over time. This bot in particular gets really good really fast, like in 10 seconds. It's found the ideal path and it's just going to keep doing it. So to break it down, reinforcement learning is the process of learning by interacting with an environment through positive feedback. Cure learning is a type of reinforcement learning that minimizes the behavior of a system over time, through trial and error. And it does this by updating its policy, which is a mapping of state to action based on a reward. The coding challenge for this video is to modify this code so that the game world is bigger and has more obstacles. Let's make it harder for our cue learning Bot to find the optimal strategy. Details are in the read me poster. Get humbling in the comments and I'll announce the winner. Next video. For now, I've got to optimize my life. So thanks for watching.