Speaker 1:          00:00          Hello world, it's Raj. And today I'm going to show you how to build your own self driving car in a simulated environment. And it's actually much simpler than what we've been doing. Like so recall we've been talking about generative adversarial networks and variational auto encoders and all sorts of stochastic models. This comparatively is actually surprisingly simple, which is crazy if you think about it, a self driving car, it's simple. No, but it actually is. It's nine lines of chaos. I mean the model itself is and then the helper code. But we're going to talk about how you can build this and I'm going to show you step by step the entire process. Okay? So here's an example of what it looks like, uh, when it's, when it's fully trained and it's running in the simulated environment. So I'm super excited. I hope you are. And at the end I'm going to answer some questions just straight up from youtube comments.

Speaker 1:          00:46          So I'm excited for this. Let's, let's get started. So the first thing we want to do, I'm going to stop this. A demo. This is what it's eventually going to look like. And now the car's going to crash into the water because I turned off the autonomous feature. Oh, let me just turn off the simulator. Okay. Simulator is gone. Okay. So let's first go ahead and talk about what this is and what the whole deal is behind all of this. A little bit of theory and then we'll get back into the code. Okay. So where are we? Where are we? There we are. Okay. So how does simulate a self driving car is the name of this tutorial. And so you Udacity recently open source their simulators. So they built a simulator specifically for self driving cars. So cool. Right? It's just for self driving cars.

Speaker 1:          01:30          Uh, and it was, it was built for their self driving car and inner degree students and they recently open sourced it. So, uh, it was built with unity, which is if you haven't used unity before, I would definitely recommend at least checking it out. If I had more time, I would definitely be diving into unity and building all sorts of cool three d models that I could play with. And you know, maybe even generating three models with generative models. Uh, but yeah, it's, it's, it's a great tool. It's very much using in virtual reality and more and more in machine learning for, for modeling and things like that. But definitely check it out. But yeah, they built this thing in unity and uh, they added a bunch of prebuilt scripts. They added a bunch of scripts and the scripts a control things like gravity and momentum and acceleration and all sorts of things that you would think would be in a self driving car simulation.

Speaker 1:          02:21          And you can modify these things as well as they're just values that you could just change in code. Uh, but you can find that the actual simulator itself right here on get hub, and I'll also have a link to it in the description and in the get hub read meat. But it's a binary file. You don't have to compile it from source. That is like an exe, you know, you just download it. It's like a dot hap file and you can just download it for Linux, Mac and windows and then boom, you're good to go. And it's got two modes. It's got a training mode and an autonomous mode. And I'll talk about what each of those modes is, but the, the high level is you just download it and it just works. No dependencies to install or anything. So it's super, super simple. Okay.

Speaker 1:          02:59          So that's what they built. And so let's talk about the three step process and how this works. Okay. There's a data generation part, then there's a training part and then there's a testing part. So the first part is the data generation park. And so what happens is, uh, we are going to first look at what Nvidia did. So to generate data. What nvidia did was they took their, they took a car and they did the same exact thing that we're going to do. We're going to replicate what Nvidia and you, Udacity did. It's the same thing. They built a nine layer convolutional network and the attached three cameras to the head of a car. Okay? So the attached three cameras to the head of a car and they had a human driver drive the car while the cameras were attached and all the car had to do with collect data and buy data.

Speaker 1:          03:46          I mean the steering, uh, let me turn off the sound here. The steering commands that the, that the human driver was inputting as well as the camera feed. So the three sets of camera images coming from, from all three cameras. Okay. At the same time. And that was the data generation park. Okay. And so there were, there was images from the center left and right cameras with the Associated Steering Angle, speed, throttle and brake. So there were four of physics variables as well as the static images that were coming in. And it saves it all to a CSV, which is what we're going to do. And ideally, ideally you have a joystick for this thing, but I don't have a joystick. Ain't nobody got time for that. And just because it's like, it's easier, it's, it's smoother with the transitions and stuff using a keyboard that wasd keys.

Speaker 1:          04:32          It's kind of erratic, but whatever. I mean it works. It works. What you thought was based off of a keyboard training. So whatever it works. Uh, so yeah, so that's where the data generation part and then there's the training part. Okay. So what the I, the idea is that we have a human driver. Okay. So the human driver is just driving around and we're reporting all that data and then we're going to get a machine to clone that behavior. So we call this process behavioral cloning. Okay. Behavioral cloning. And to do this, we're going to build a nine layer convolutional network, nine layers. And in chaos, that's just literally nine lines of code. And it was based off of Nvidia's end to end learning for self driving car paper. And Yeah. So they trained a model for 72 hours on the whole book in a whole bunch of different driving conditions, sleep, rain, hail, all, all sorts of things.

Speaker 1:          05:20          Wet Rain, you know, everything. So here's the hardware design. So the hardware design was, it's obviously a steering wheel, but the steering wheel is attached to a controller. What was that? What was the, uh, so many acronyms here. Could the controller area network bus. And so it's feeding in those four variables that I talked about. Right. And you're also have your three cameras and the three cameras are all feeding in continuous streams, like just frame by frame of all the images that the car is seeing and it's fed into this drive px, a computer, which is essentially like a motherboard with us, a cluster of genes onboard, gps attached to it, and then a, and then a solid state storage would just store the results. That's it. Okay. So that's the, that's the high level idea behind the hardware behind it. And then the software behind it.

Speaker 1:          06:03          Oh, and also a interesting little tidbit, I just report it from the paper. Instead of inputting the steering command are directly, they inputted one over r and they found that this may turn smoother and it made the, it made the, uh, autonomous angles at the car moved independent of the geometry of the car. So the car was facing left, it didn't matter or the conversation, right. It's all about what the cameras is, what the camera's c and then using that as the signal to then move the steering ankle. So it's, it's independent of the direction of the car is facing and it sounds kind of counterintuitive. Like wait, you would want that but you actually wouldn't end. We'll learn more about why that is as we get into this code. Okay. So the software designed for this code and it's so crazy cause going from generative adversarial networks to this.

Speaker 1:          06:55          It's like that's it. That's all it takes for self driving car. Like what? But that's, that is actually how it is. Check this out. So here's, here's, here's what happens. We have, uh, we have two inputs, right? We have our steering angle and then we have our sets of cameras, right? Let's, so three sets of images. Those are our inputs. And so we can think of the problem as a supervised learning problem, right? We have our input data, which is, which are the images and then our output labels, which are the, if we could just abstract all of the steering angles and one value, you know, like one binary value that would be our label. Like based on what you see, how should you move the car? That's it, right? So we have a CSV just listed and all of those things, right? And at the same time we have uh, what the car is trying to do.

Speaker 1:          07:46          So during the training process, assuming we've generated data 72, let's say 72 hours of human generated driving data, we can then, uh, compare the, both of those results together so we can compute the error or lost between the two. Right? So if we have, let's just say we have the camera angles and the steering wheel from the autonomous car and then the camera angles and the steering wheel variable from the human generated data. And then we just vectorize both of those values into one value. And then we find a difference between those values and that difference is our error. And then we use backpropagation to then update our weights based on that error value. Okay. So it's taking that, that weight value or that back, that air value and in back propagating it in our weights. Okay. So images are images are fed into the CNN and then it computes a proposed steering committee.

Speaker 1:          08:39          Okay. And that is the predicted steering command. And we want to find a difference between the predicted steering command and what the actual steering command would be from the training data. Okay. Uh, and then the proposed command is compared to the desired command, right? Label the target label versus the actual outputs. And then the weights of the CNN are adjusted to bring the CNN output closer to the desired output. And the way that adjustment is accomplished via backpropagation. So eventually you want, it's trained, then all you're going to need is, is the center camera, just one camera, you feed it to the, to the CNN, and then it outputs a prediction. And the prediction is the angle of the steering wheel, just one output. Right? So that's it for training and then for testing, right? So once, once we've, once we've trained this autonomous model for testing, we could think of it in, in terms of a simulator as a server client architecture, right?

Speaker 1:          09:30          The server is going to be the simulator itself. That is our, you know, the APP that we downloaded that, that little three d game and the client is going to be our python program that what we write and what it's going to do is it's going to just be a feedback loop. It's going to be a feedback loop so we can consider it as just, so this is how we can consider it. It's just the client is piping in steering angles and throttles to the server and the server is piping back, uh, images from the car and steering angles so that it can train it, right? It's just that it's just a, it's just a feedback loop. So that's, that's the basic idea behind, you know, how this works. High level and video, have the paper on it and then you, Udacity, you know, was inspired by it and now I'm bringing it to you guys.

Speaker 1:          10:13          So it's just the chain of a chain of whatever. Okay, so let's build this thing, right enough talk, let's go this thing. So that's the first step is to install our dependencies. And you can do this really easily with just one line of code. There's an environments dot yml file and you can use Anaconda to do this. Okay? So if we look in the environments file, we've got like 20 dependencies here, but you don't have to, you know, as long as you run this one line of code, it's going to install all of these dependencies. I make this bigger, all of these dependencies super easily. Okay? So yeah, you've got open CV, which we all know is a pain to install. So luckily for us, Anaconda is going to make it super easy to install. Okay? So that's the first step. Now, I would do this myself, but I've already installed the dependency here. Uh, but what I can do, right? So you just paste that in and once you paste that in, then you're going to type in, I can show you the command. You say source, activate. What was the command?

Speaker 1:          11:17          I'll put it in the, I'll put it in the get how breathing, but you then you activate your conda environment and it's gonna be called behavioral cloning. Source activate, behavioral cloning. And then that activates the environment. So you install your dependencies in this container, this Anaconda container, and then you activate it. So then all those dependencies are then active in that Shell, in that session, in terminal. Okay. And so once we have that, then we can generate our data. Okay. Where we've got our dependencies. Now, step two is to generate our data. So this is, it's a five step process. We're going to install our dependencies, which we assume I've done. Step two is to generate the data. So we're going to drive and we're going to replicate, replicate what the Nvidia did with the human drivers. Step three is to write the training script, step forwards to train it.

Speaker 1:          12:00          Then have our agents learn or clone the behavior of us, the human. And step five is to write the testing scripts and then let it run. Okay? So let's go ahead and generate our data. So to generate our data, we're going to have to actually drive this baby, which is gonna be a lot of fun. Now you just double click on the APP, you know, I downloaded it, you can double click on it and then you know, it's got, it's got a couple of settings I sent it to. Fantastic because why not? Of course the choice, uh, but uh, you know, fastest, you know, depending on your processor, you, you would want to change this. So I'm going to go ahead and run it at 800 by 600 and it's gonna, it's gonna pop up just like that. And I'm going to go into training mode.

Speaker 1:          12:39          Okay. So for training, for training mode, I'll just click on training mode. And the controls are going to be w a s. D. Okay. So just like this, see, this is me driving right now. Hi, I'm driving the car. Okay. And now I'm going to stop the car and I'm driving it again. Okay. So this is human driving. So what we're going to do is we're going to generate data so that we can train our, our car on that data. Okay? So to do this, we just have to type in, just type our right to record are. And so then it's going to ask, well, where do you want to save this data too? And I'm going to say let's save it to the desktop. Okay, let's save it to the desktop and start recording. And now what it's doing is it is reporting three sets of images from three virtual cameras of the car.

Speaker 1:          13:24          Okay? Assume that the cameras are on top, but what it's doing is it's capturing frames from this game from three different angles. And it's also recording the, my, the four variables I talked about speed, throttle, and the, the other two of the steering angle. There's one more that I'll look at, but basically what you want to do as you train this thing is you want to complete either one between one and five laps. Okay? Ideally five. But, and look, I'm off track, but let me just stop recording. And so then once you're done, you know, driving between one and five laps, then hit our again and it's going to capture that data. So it's going to replay what you've just done and it's going to go up to a hundred percent and when it's done, it's going to save it all to a CSV file that we're going to look at and see what's inside of there so we could observe it.

Speaker 1:          14:12          So we've got 52% and in the meantime, let me go to the CSV file and show you guys that. What else? What else can I show you while this captures? Well, it's actually fun to just watch this. Let me see what else I can talk about the training and testing script. Uh, yeah, we're going to be using carrots 95% here we go. Done. Okay, let's check it out. What if, what did we save here? So if I go to open and then I go to my desktop there, my driving log, just just like we thought it would be. Okay. So let's take a look at what this is. Let's take a look at what this is.

Speaker 1:          14:47          What we've got here are three images, right? And it's the three columns are images, they're pointing to images. And these are the images that it's pointing to right here, this image folder. Okay? They're pointing their absolute, uh, directory values. It shows us exactly where they are. See, these are the images that they're pointing to, just like that. And then these are frames that were captured from the game. And we've got three sets of images for the center for the left. And these are labeled with the direction and the right. So we've got three sets of images and then we've got four values here. And these four values are those values that I talked about up here, which are the, which ones are they? The steering angle, the speed, the throttle, and the brake. All four values. And these are what I inputted and see how at the start I wasn't moving.

Speaker 1:          15:36          So these are, these are zero values, but then as I start moving, they're going to change. Okay. So that's it. That's it for our data. So what you want to do is write training between one run, one full lap and five laps. Okay, so somewhere in between there. So yes, that's, that's it for our uh, data generation part. Where were we in our process? We were at step two. So assume that I've generated data. Okay. Just like that. It would take a while, but I generated data. So the next step is for us to write our training script. So let's write this training script. Okay. So where were we for our training script? I'm going to say, okay, so we've got two files here. We've got a training script, which is modeled up high, and then we've got a testing script called Dr Puy. Well, let's write our training scripts, right?

Speaker 1:          16:23          We've generated our data and we've saved it to the desktop. And now we want to build a model that will see a convolutional network and we'll read from this data and then it will output output steering commands. So we've got pandas. Pandas is going to be our data analysis to toolkits. None Pie's going to help us do matrix math. We're going to import psychic, can't learn just so we can split our training and testing data into sets into sets. And then we're going to use care os as our machine learning model sequential because it's going to be a linear stack of layers. Very simple model. And then Adam for gradient descent model checkpoints. So we could save our model. And then we've got a mid layer types that we're going to use. Then our helper class utils it's going to define our input shape and generate training images and then our, our cars for command line arguments.

Speaker 1:          17:10          And lastly, r o s module for reading files. So what I'm gonna do is I'm going to write the code for building the model itself. Uh, but all of all the rest of it is prewritten, right? So the data loading parts are prewritten. We're going to load that CSV file that we just created, right? That driving log and make it bigger, make it bigger. So that driving log, we're going to, we're going to generate that data or we're going to read from that data using pandas and then we're going to say, okay, so now we have that in a data frame variable, very easily possible variable. Thank you pandas. Then we're going to say for the center left and right, uh, columns get those values and that this is going to be our input data that's going to be our x, right? And then are a output data is going to be our steering commands.

Speaker 1:          17:52          So we can, we can can catenate that into one variable, right? So then we have our input data and our input and our output labels and we want to find the mapping between the two and it says a supervised learning problem. And once we find the mapping between the two, then given novel input data, which is novel a camera images of what a car sees, we can then output the predicted label, which will be the Syrian command c very intuitive. Right. Okay. So we've got our input data and our output labels and then we can split the data into training and testing data. So it's 80% training, 20% testing, and we can define the size of that in our command line argument and test size. Okay, we're just going to be a point to oh, this is going to be this value. And so yeah, so we're going to have a training, our training data and our testing data and our validation data and we returned that.

Speaker 1:          18:39          Okay, so, and then we've got the code for building our model, uh, and then training the model and then the main function, right? So let's forget about these command line arguments. So let's just go straight to the main function and let's say that for our main function after printing, you know, whatever our parameters are, we're going to load our data using that funk, that function that we just talked about. And then we're going to take that data and use it to train our model and look first we'll build our model. And then once we built our model and loaded our data, we'll train our model, right? Using those two parameters as well as any command line arguments. So this committee, this helper function just as for converting a string to a boolean value for our command line argument. But this training model function is really interesting.

Speaker 1:          19:23          Uh, let's first build a model and then we'll talk about the, uh, the training model function. Okay. So for this first part, we're going to build this model and it's going to be a sequential model, right? It's going to be sequential model, very simple model. And we can just pull this straight from the paper it's, or even the paper write the paper told us the parameters with which we could build our network. So we'll use those parameters, we'll use those parameters to build our network. So the first step is for us to say, okay, we want our first layer to be an image normalization layer. Okay, we can totally do that. Our image normalization layer using our lambda function and the land of function is going to help us do so really easily. And we'll say, okay, it's going to be, let me type this out and then I'll talk about it.

Speaker 1:          20:08          Input, shape. Okay, so what's the deal here? So x exited. Yeah. So this is going to avoid saturation and make our grants work better. Now notice that these are magic number's right here. Uh, but we, but the authors found that, you know, after training and testing out different values, these are the ones that work best for normalizing the images, right? When we input it to avoid saturation and make the gradients worked at or what do, what do we mean exactly by that is, you know, the images can be shadowy, they can, you know, depending on the lighting, it can be certain things can be obstructed or the hue or that, you know, all of these color correction values could be off and they could give us results that are bad. So we went to avoid that, right? So that's that. When we, when we say image normalization, what we really mean is we want to format or reshape these image tensor values and two values that will give us good predictions in the end.

Speaker 1:          21:06          Right? So that's it for our first model. And then we've got a bunch of convolutional layers that we can essentially copied it. Copy and paste ones who've written out the first one because they're very similar. And so the first one is going to have a filter size of 24. Okay, I got you. And it's got, it's going to be a five by five convolution. Okay. I can do that as well. And then it's going to have an activation function, which is going to be Elu, which means exponential when your unit. So can, we've got Elu here. Okay. Which is more suited for this task then rectified linear units. And we're going to sub sample with two by two because that's the length of our strides. And then we'll add our closing parentheses. Okay. So that's it. Um, and so why do we use the loo?

Speaker 1:          21:51          Is it's because it takes care of the vanishing gradient problem. That's why. So okay. So we've got five of these layers. Let me just paste that one, two, three, four. How many of these five I think that's fine. That's fine. Okay. So then we want to make sure, okay, so what's the difference here? The filter size here is 36 this is how you can just read from a paper and just write out the model directly, right? It's not that hard. They have the parameters and with a simple library like carrots, you can, you can build a model pretty easily. So we've got 36 we've got 48 64 64 okay. And so then we can remove the sub sampling for this, for these last two layers, since the census, stride size is one by one respectively for each of the, for each of them. So they'll just end in the activation and then we're going to add a dropout layer.

Speaker 1:          22:42          Okay? So it's asking for 50% dropout. We can do that. So we'll say, okay, drop out. We'll add it based on what the user says it's going to be. And we're going to, it's going to, we're going to input 50% later and then we're going to say, okay, so now that it's dropped out, we're going to flatten the the data. And so why do we thought flattened it? Because we're going to start feeding in a series of fully connected layers and y, y, y series of fully connected layers because the convolutional layers are meant to handle feature engineering. So that means that the image processing part, so the filters, right? So we, we feed in a set of images and what the convolutional layers we'll do is each of them respectively is going to create a set of filters. And these filters are going to be, you know, increasingly abstract, right?

Speaker 1:          23:31          So they're going to start off with low level features and they're going to get increasingly more extract. So they're going to be able to detect images. But what we want to output is not an image, but we want to, what it is that we want to output is a value and that value is single value. And that as the steering command, the right, it's a direction. How do you want to move this, this wheel. So to do that, to get a single value from these high dimensional image tensors, we have to, we have to squash that data. And the way to do that is by applying a series of fully connected layers. Okay. And so what's gonna Happen is, uh, each of these fully connected layers is going to progressively get smaller and smaller in terms of the number of neurons they have. And you'll see what I mean here.

Speaker 1:          24:09          So it says the first one should have a hundred neurons. So we'll say, okay, so this is a dense, which means fully connected layer and it's activation function, like all the, all the other layers is going to be he Lou. And so then what we can do is we can just copy and paste this. I'll say one, two, three. So there's going to be four of them in total. Each of them is going to be smaller. So hundred neurons, 15 [inaudible], 10 neurons, and then just a single neuron. And this one won't have an activation function because it's the last layer and that's going to output are driving a value our steering down and say, okay, so it's a summary. And so of all of those summer, all those values to get the summary and then return the model. Okay. And so then we'll say,

Speaker 1:          24:57          how much do we have here that they're, there we go and then return the model. Okay, cool. So right, so for our fully connected layers, we start off with a big layer and then we want to get progressively smaller. And so the way to do that is by using a model summary or by is by using a series of fully connected layers that get smaller and smaller. And what that's going to do is going to squash our data. So it's going to basically be like a triangle in terms of the number of values in that, in that, in those matrices that are being propagated forward in our network. So it's gonna be like, you know, uh, 10 uh, a matrix, the matrix with 10 in indices and then like a matrix matrix with five indices and a matrix with two indices and eventually one single scalar value.

Speaker 1:          25:36          And that is our output and that's what this is going to do. Right. And the wine in the paper, they noted this, but I, I didn't actually document this. They noted that it's not sure where the image part and the ND, uh, steering part begin and end because it's all kind of connected, right? In terms of what the, what the neural network looks like. But that's part of the black box magic of neural networks. We don't know exactly where one feature starts in one feature ends in terms of where it lies in bs in this abstraction hierarchy. But we know that all as a, as a whole, as a sum total, there is some, there is some connectedness, right? So anyway, so that's it for our model. And then once we have this model, we're going to train the model. And so let me talk about this last training function. So for the training function, we're going to say, okay, take the model and then the, the training data and the validation data. So modeling data and train it, right? So to do this, we're going to run the model checkpoint function. And what this does is it saves our model at ace in a checkpoint that we bear, that we,

Speaker 2:          26:42          Yep.

Speaker 1:          26:42          That we say that we want and it's gonna be model dot h five and then we're going to say, well, what is the loss function that we want to monitor? We're, which we're going to call the validation loss. Not yet, but we're going to call it that. And we only want to save the best model. Okay? And then we're going to say the mode is going to be auto and automotive means the direction is automatically inferred from the name of the monitor quantity. Okay? So that's it for our checkpoint. And then we want to compile our models. So what are we going to do here? We're going to say, okay, well what is our loss function are going to be? Well, there are several laws, assumptions that we can use and we're going to use a really simple one called mean squared error. What does that mean?

Speaker 1:          27:18          So our model is going to output a predicted a steering angle, and then we have an actual steering angle from the human driver given those camera angles. And then we want to find the difference between those angles and do that for all of the uh, data points that we have. Okay. And then some of the differences. And then, uh, hold on. We want to find the difference. Square, the difference, Adam, all the differences and then divide by the number of them. And that's going to be the mean squared error. The sum of the squared errors. Okay. Or the mean of the squared errors. Right. Uh, and so then we're going to use the Adam optimizer, which is gradient descent. Uh, yeah. And so that's how we compile our model and then we're going to generate some data. So what we're doing here is we're running the fit generator.

Speaker 1:          28:06          Okay. And so what did fit generator does is it lets you do realtime data augmentation on images on the CPU in parallel to training our model on the GPU. What do we mean by that? What we are generating batches of data. Okay. While we are generating those batches of data from, from our training data, like the, the, the buckets or the containers from which we train our model in from the, from the huge set at the same time we're actually, we're, we're training our model, right? So what this fit generator function does is it does both simultaneously. Okay. So it's super valuable and then cool. So then we can train this model, right? We just, we'll, we'll run python model.py and it'll start raining on our driving. He'll start training on our driving logs and we want to then, uh, once it's training, once it's done training, we're going to write our testing scripts.

Speaker 1:          28:55          So training can take a while. It depends on whether you're using it on the CPU or the GPU, if you're running it on the cloud or on your local machine. But training can take a while and we can definitely speed it up by running it on the GPU. I read this thing on my Mac book and it took about eight hours to train. So yeah, it all depends on what kind of specification you have. Right? So once we've, so assume that we've trained it, it could, it's going to take hours and hours to train. So we're not going to train it right now, but as soon we've trained it, then we can write our testing script. Okay, so let's go to our testing code. So let's check this out. So let's, let's check out our testing code. So for our testing code, remember it is a server client model.

Speaker 1:          29:31          That's what we're using. We're using a server client model and that means that the uh, the, that the simulator is the server and then our client is then script that we're writing. So we want to think of it like a thing of it, like a server client models. We're going to use flasks to do this, the web app framework. So let me talk about the dependencies and then we'll talk about the code. So the first thing I want to do is to find our command line arguments. Base 64 is going to help us to code camera images. They time for, uh, for timestamps. Uh, what else? We got high level file operations. Socket io is going to help us work work with this as a real time server in terms of piping commands through event handlers. We're going to use a vent lid for concurrent networking.

Speaker 1:          30:12          Uh, and then we have uh, more web framework dependencies and then image manipulation with pillow flats because our web framework and then bites, I have to deal with input output and of course care os it's just a load our model and our help class. Okay. So we'll start off by initializing our server. Okay. It's going to be a socket io server and it's going to use flask to do this. And we'll initialize our model and imagery as empty because we're going to fill those as we later on. And then we're going to set a Max and mins feed for our autonomous cars. So it can't go faster than 25 miles an hour and it can't go less than 10 miles an hour. Uh, and then it's what we're going to define a speed limit as well. What are you going to be at Max? Speed.

Speaker 1:          30:51          Okay. So then let me skip this function because we're going to write it now. Let me talk about that. The other functions. So let's go into this main function. So these are the command line arguments that defined, you know, what, where is the model and where are the images, uh, that we want to wear. The images from the runner going to be saved. So we'll load the model using our load function and then, and we'll tell it via command line where that model is and it's going to create an image folder and then it can either record the run or not depending on how we, what we say. And then it's going to launch this flask middleware, which is going to let our client communicate with the server and then we're going to deploy it as a w s Gi server, right? And the, that acronym, what was it again?

Speaker 1:          31:34          It was w Sgi. Let me brush up on my web, my web stuff here. That'd be West Gi. It's not crud create, read, update a web server gateway interface, right? We have 101 acronyms. Okay. So what these functions are, are, these are event handlers and this is why we're using a socket io as a server because behind under the hood for the simulator, it's got the, it's got the architecture necessary to send commands that we need. We just need to create event handlers to then accept those commands, right? So we don't have to code the server part, we just have to close the client part. So this is really the easiest way to get started with building a self driving car. This simulator that you'd ask to be released is the easiest way. Obviously there's grand theft auto, which is awesome and stuff. Uh, but you can only run that out if you're on windows.

Speaker 1:          32:22          And then also, you know, there, it's got a whole bunch of extra features that we don't necessarily need. Um, but yeah, we could also do it in grant that thought of it. Right now this is the easiest, if you're looking for the easiest way to get started with building self driving cars, this is the way. Okay. So, so we have event handlers for connects, which is going to just send a, uh, the sid the session Id, uh, and then we've got a send control. We're just going to send the actual commands that are model emits, but then we have the telemetry function. And so that is going to be the, um, that's going to be the big thick, the meat of our code. And that's what we're gonna write out here. Okay. So we're going to write up this, this part right here. So we're gonna say, okay, so what this does is it's going to make the prediction and then send it.

Speaker 1:          33:04          It's going to make the prediction of what the steering angle is going to be and then we're going to send it to the server. Okay? So let's go ahead and write this down. So if data, so we're going to feed this in our data and then we're going to create an if statement says if data. So once we got our data as our, as our focus, we're going to say, let's get the current angle of the car. So we'll say the steering angle. We want the steering angle of the car. Okay, so that's the first variable that we want and it's going to, we're going to retrieve it from our data data frame. Okay. And then we want the throttle. Okay? So remember we want these variable, we want these values so that we can manipulate them and turn them into one single scalar value that will tell our car where to go.

Speaker 1:          33:45          And we want the speed and the speed is going to be a float. It's going to be a float value. And just like the rest it's going to, we're going to pull it from this data frame using the Scott Key, which is going to be speed, human readable Qi. And then we're going to get the current image from the center of the camera for the center camera of the car using pillow, right? So say image of open. So and then we'll use this word bites. Io comes into play. This is what we're using it because we want to convert it from a 64 this image into what we can read directly into our model. So we'll say base 64 d code data and then image. Okay. And so then we'll say, okay, so once we have those things, we'll go ahead and and do some tensor processing on this image. So we need to feed this image into our network, right? So to do that, to do that, we're going to say, image, let's see what else do we need here, image. And then

Speaker 1:          34:58          so we're going to convert the, the image to an array and then we're going to apply the preprocessing step to it. And then the model is going to expect a 40 array. So that's what, that's what this image is going to be. And so once we have that, then we can say, then we can say, uh, predict the steering angle for the car. Give them that image. So we're going to say, okay, so let's predict the steering angle. Using our model was a model that predicts just one line, given the image and the batch size, whichever, which is whatever it's going to be. Let's just say one for this simple example. And then once we have that, we're going to say, okay, so then given the speed limit, that global value, they'll be initialized. It's greater than if the speed of the car is greater than what we've said as our speed.

Speaker 1:          35:37          Then change it. Say, okay, well the new speed limit is going to be the men speed. So then that means slow down, right? And so else if it's not, then we'll, then we can say the speed limit is going to be the Max speed, right? So we don't want it to go faster than the speed limit is what we're saying here. And once we've done that, then we can say, okay, the throttles going to be 1.0 minus the steering angle. So this is a magic number territory. This is where it guessing and checking comes in, what will say minus speed over speed limit turns to George squared. Okay. And then we can send the control using that helper function that we defined before using the steering angle and throttle. Cool. That's it for this code. Right? So

Speaker 1:          36:26          that's it. We profit, we preprocess the image and then we fed it into our model and then it outputs the the steering angle and then we can use that to then send that control directly to the server. We have the sand control function. Okay. And it's going to admit that as a, as a, as a packet to the server and the server will read that in. So the actual event, the event handling logic is, is written by the, is under the hood for the simulator and we can just send it via our client. That's it. Okay. So then once we had that, then we can compile it and it's going to run the script just like I did right at the beginning. All right, so to end this, I'm going to answer three questions randomly from the youtube comments and then we're out of here.

Speaker 1:          37:03          Okay. So the first question is, let's see what, what kind of questions we got here, how to work with multiple datasets. This is a great question and it's worthy of a video, but essentially there are several ways of thinking about this. One way is you can just combine all your data sets into one big dataset using pandas. And that's what I would do. That that's probably the best way. That's the cleanest way. But another way you could do it is thinking about it serially. So you'll want to train your model on one dataset and then train your model on the next Dataset. But I would, I would just combine all your data sets into one, two more questions that I'm going to read from the comments. Um, what else do we got here?

Speaker 1:          37:40          How can I discover a foreign language like Russian? Do you tell me? So this is for the language translation of video. So discovering a language is, um, quite a challenging task, but think of it has associations. So as long as you have some kind of labels and the labels could be a different language, but maybe you're trying to discover an ancient language or discover the rules of some language you don't know, then as long as you have some kind of associations, you can treat it as a supervised learning problem. Okay. And, uh, I've got two language translation videos and links to those in the description of that video. And one more question. Um, once you have this neural net training can be optimized to run locally or does it have to be run as is every time? Yeah. You can re optimize a network. You can use the pre trained model or you can use your card. You can retrain it. Again, it all depends. You can retrain it on your data or you can use it or you can use it pretrained models at someone else's trend. That's it for this session. Please subscribe for more programming videos. And for now, I've got to go clean my kitchen, so thanks for watching.