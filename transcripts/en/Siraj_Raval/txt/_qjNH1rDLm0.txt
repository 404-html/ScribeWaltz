Speaker 1:          00:00          Hello world, it's arrived and let me show you how to learn deep learning in six weeks. What's next? Rocket Science in one week and this video will explain the six week curriculum I've created to help you learn the art of deep learning. The only prerequisite is knowing basic python and by the end of this curriculum you'll have a broad understanding of some of the key technologies that make up deep learning. You might be thinking why deep learning without machine learning machine learning is a broad set of algorithms use to derive insights from data sets. Deep learning is a subset of all of those algorithms, specifically all the various types of neural networks and when applied to massive datasets and given massive computing power, these neural networks will outperform all other models. Most of the time. Deep learning is the hottest field in AI right now and it's responsible for everything from Google's latest duplex assistant to Tesla's self driving cars to robot companions.

Speaker 1:          01:12          You can get a job as a deep learning engineer by browsing through listings like angel list, Hacker News and indeed.com and when it comes to getting hired by these employers, usually what they're looking for is experience building, training and deploying deep learning models for real life use cases. I uploading one project every week to your get hub profile. You'll have built a substantial portfolio to show perspective employers and some of them can even be deployed to Heroku to be usable as a web app. So where do we begin? Deep learning is built on mathematical principles. It's all math really specifically. It's linear Algebra, probability theory, Calculus and statistics. These are sub fields of math that cover an enormous breadth of topics each and it's a little daunting to try to tackle them all. Ideally we could learn from a source that teaches us the subjects, but not all of them.

Speaker 1:          02:13          Just the relevant parts that apply to deep learning. The deep learning book by Google brain researcher, Ian Goodfellow does a perfect job of that. It's free and openly available on deep learning. book.org for the first week of this curriculum read part one of this book. It does an incredible job of diving into specific subtopics in each subject that are used consistently in deep learning terms like a derivative and doc product are explained in an easy to understand way with ample explanation behind the math notation that's used. I've also included a cheat sheet for math notation to help you understand what the symbols mean. If you don't, don't worry. If you don't get how all of these concepts are used in a neural network right away. Just get yourself familiar with the ideas presented in the book first. No, that a matrix is a group of numbers and matrices can be modified in different ways.

Speaker 1:          03:14          No, that a distribution models the probabilities of different possible outcomes occurring in an experiment. Once you've read part one, it's time to build your first neural network. Watch my build a neural network in four minutes video on youtube to get started. Then read Andrew Track's amazing blog post called a neural network in 11 lines of python. Fire up your text editor and start coding your neural network in Python. You don't have to memorize it to, you could even type it out line by line as you watch my video or stare at the finish code, but the simple act of typing it out will be beneficial to your memory. Your network is simply a series of operations that are applied to some input data until it results in some output data. Nothing really special about that, but the real magic of these things as a result of an optimization technique called back propagation, it's how a neural network learns to improve its output over time.

Speaker 1:          04:16          It's a technique from calculus. I have a great video on this called backpropagation in five minutes. Check that one out. By the end of this week, you should have a basic idea of how a simple feed forward neural network works backpropagation included. Once you've mastered that, everything else becomes easier. Every other neural network is just some variation of this and each variation excels at a specific use case. Speaking of variations, let's move on to week two convolutional networks. There's a lot of different types of data out there. Numbers, text, video, audio in different types of networks can be used to learn from each of them. While feedforward networks are great for learning the mapping between numerical input and output data, convolutional networks are wellmade for learning from image data sets. If we think of an image as a group of numbers each describing it's pixel intensity value on an RGB scale, then we can consider it a matrix and this matrix can be input into a neural network operated on and result in an output which is a class probability.

Speaker 1:          05:32          Confidence were invented by young macaroon's team over two decades ago and are still responsible for some of the state of the art advances in computer vision technology including driverless vehicles. A great resource to learn about them is the convolutional neural networks course on Coursera taught by professor Andrew Ng of Stanford University. It dives into both the architecture and application specific details in an easy to understand way like all neural networks, confidence have lots of variations, deep convolutional networks, skip connection networks that can also be used as building blocks for more complicated models like variational auto encoders, so it's important to really understand the details of how these work. A good supplement to the course is Andre Karpov. These lecture notes on CNN, a part of his cs two 31 and convolutional neural networks for visual recognition course. He has some amazingly detailed technical documentation covering how they worked.

Speaker 1:          06:37          I also have some detailed videos on CNN, a linked to them as well. Moving on to week three we're current networks. While feedforward nets are great for numerical data and confidence are great for images were current networks are great for sequential data. Any kind of data where time matters, audio, video, since videos are a sequence of image frames, stock price data, recurrent networks are the perfect network to use here. Why you might ask normally neural networks while training only use the next data point as an input, but recurrent networks take both the next data point and the learned state value from the previous time step as input to this recurrence allows it to remember data sequentially course on. This is again on Coursera by professor Ang called sequence models. This will cover the variations of recurrent networks including long short term memory and gated recurrent unit neural networks.

Speaker 1:          07:38          Additionally, I have some great videos on this link to those will be in the provided syllabus at the end of this week. Make sure to write out a simple or current network using Andrew tracks, Lstm RNN python blog post as a guide. Now that we have the basic types of neural networks out of the way, we can dive into some of the tooling before it will be all about getting yourself familiar with some of the tools in this space. And because up to this point, you've only built your neural networks using num Pi. You'll appreciate the benefits of using libraries like tensorflow and care office rather than typing out grading updates by hand. You can benefit from TensorFlow's automatic differentiation. For example, of all the deep learning libraries, I'd found tensorflow to be the best tool to use since it offers a complete pipeline for AI development including building testing, training and serving models in production.

Speaker 1:          08:35          A great course on this is cs 20 by Stanford called tensorflow for deep learning research. You can find all of those videos on Youtube. I also have an awesome playlist on tensorflow on my channel that I'll link too. If you can understand tensorflow, it's computation graph, the basic operators, how models are saved and how to serve them in you will be unstoppable. US is just a wrapper on top of tensorflow that makes it even easier to write models. Read the documentation page examples to quickly get an understanding of how it works. You also want to read this blog post that compares some of the Best Gpu cloud providers so you get a sense of their pros and cons and can decide on which one is the best one for you to use. At the end of this week, you should write out a simple image classification demo using tensorflow as practice.

Speaker 1:          09:26          Once we've got that down in week five we can peer into one of the newest models in deep learning, the generative adversarial network. This allows us to generate all sorts of data and it's currently very popular. We can learn about these models from youtube. I've got some great videos on gans. You can also find some interesting lectures on gains that I've compiled in the syllabus by researchers across the field and explore their possibilities across a wide range of applications and two projects for this week. Build again from scratch and build again. Using tensorflow for the last week, we can focus on the most bleeding edge of all techniques, deep reinforcement learning. This is what's responsible for some of the latest breakthroughs in the field, including Alphago, the Atari DPQ learner, and more. Berkeley recently released a free course called cs two 94 deep reinforcement learning. You can find all the videos on youtube or reddit community of students and a bunch of help or materials on their website.

Speaker 1:          10:31          This one is nontrivial and it could even take you two weeks to finish this bit. Reinforcement learning is different from supervised and unsupervised learning. While these either try to learn a mapping or cluster similar data points together, our El tries to learn by trial and error and some kind of environment where time is a dementia, not just a static data set. It's very likely that the brain learns using some combination of our l and other types of learning, which could be why deep RL, a combination of learning styles has gotten such incredible results. For a final project, create a deep cute learning algorithm using tensorflow. You can have it playing Atari Games using the open AI gym environment. Deep learning is the dark art of our time, extremely powerful and mysteriously good at everything we throw at it, followed the curriculum I've created. Find a study buddy to help you stick to it and post your experiences in the slot channel. Good luck. Welcome to the end of the video. If your model is not converging, hit the subscribe button and it will for now. I'll get to learn deep learning again, so thanks for watching.