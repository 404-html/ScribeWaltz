Speaker 1:          00:00          What type of secret agent are you? A reinforcement learning agent. Hello world. It's a Raj and sensor networks. In this video we'll learn how to use reinforcement learning to find the most efficient data routing strategy for a network of connected wireless devices. Every time packets of data flow from one computer to another, like from your laptop to a web address like google.com and back to your laptop. When you say you a webpage, a number of intermediate devices like routers are involved. Each time that data passes through a router, it processes it then sends it along to the next device and the network. In a multi hop situation, which is quite common on the Internet, several routers are involved in getting the requests where you want them to go. That process of handing off data between devices takes time. More and more of that happening, meaning more and more hops adds up to more and more time potentially slowing down our experience as the hop count increases.

Speaker 1:          01:12          There are lots of different factors that determine the speed in which we can use certain websites or web based services like how much our roommates loves pirate Bay and hop count isn't the most important, but it definitely plays a big role. So we'll try to route data between devices with the fewest number of hops using reinforcement learning. L aims to solve the problem of learning in an environment through trial and error where time is a dimension and the most common way to frame this problem is to use the mathematical framework known as a Mark Haub decision process. Once we formally define our problem, Mark Cobian style in terms of states actions and rewards, we'll need to formally define our solution. A simple way of thinking about our ideal solution is a series of actions that will need to be learned by the agent in order to complete its goal.

Speaker 1:          02:12          For example, a network of wireless routers that helps a company transmit secure information needs to learn several tasks. It needs to learn how to best route data so that it reaches the right server as fast as possible. How to efficiently allocate energy usage amongst its nodes, how to react to changes in its topology, et cetera. The correct actions they will need to take in our example, it's routing data a specific way through its nodes will depend on the current situation. If the network traffic is really high, for example, it will need to perform a different set of actions to route data. Then if it was low, the reward is always decided in the context of the state that it was decided in along with the state that comes next. As long as the agent learns an appropriate action response to any environment state that it can observe, we'd have a solution to our problem.

Speaker 1:          03:12          This is where the idea of a policy comes into play. The most basic type of policy is a mapping from the set of environment states to the set of possible actions. We can think of a policy like a simple input, output, function input, any environment state, and it will output the associated action that the agent will take. If our agent wants to be able to stay updated on its strategy, it needs to specify this mapping. We can call this kind of policy deterministic. Since the action it will take entirely depends on its input, which is the state. In contrast, a stochastic policy, we'll let the agent choose actions randomly. We can define a stochastic policy as a mapping that accepts an environment, state s and action a then returns the probability that the agent takes action a while. In state s the most common way to denote the policy by the way, is by using the Greek letter.

Speaker 1:          04:10          Pi and Greek is dope. So in our sensor network problem, let's assume we just want to be able to transfer a file from one router to another router using the fewest hops necessary. And we can think of our network as a grid. For the sake of simplicity. A deterministic policy would specify something like whenever a network transfer requires, say more than five hops, reset the process. Or if the network can achieve a transfer in two hops or less, record that in an activity log for later analysis. A stochastic policy would say something like whenever the network requires more than five hops, reset the process with a 50% probability or continue routing with a 40% probability and otherwise continue operating as per normal. Whenever it's able to achieve a transfer in two hops are less. Record that in an activity log with a 90% probability otherwise continue operating.

Speaker 1:          05:10          Normally we can actually express a deterministic policy using the same mathematical notation that we would generally reserve for us to cast tech policy if we like as well. Overall. Specifying a policy is an important step in the reinforcement learning process, but figuring out what the optimal policies should be is just as important. How do we do that? Use duct tape to find out, let's return to our sensor network environment and start off with a really bad policy so we can see what it takes to improve. In our network of nodes, we want our agents to be able to send a certain file from one node to another by routing it through other nodes. Each visit is considered a hop to make the problem more interesting. Let's also say that there are certain nodes that should be avoided as they contain virtual environments that will likely corrupt our file for our starting policy.

Speaker 1:          06:09          We'll just choose to have our agent visit every state meaning device. There is, let's calculate how much of a reward it will get if it does this. If it starts at the bottom left corner of this environment and collects all the necessary rewards along the way, we add them up and the some will turn out to be a single number. This is actually a formal term in reinforcement learning. R L agents learn to maximize what's called the cumulative future reward. The word used to describe cumulative future reward is the return and it's denoted with uppercase r. The reward could also be discounted. A discount factor describes the preference of the agent for the current reward over future rewards, but let's assume we're not dealing with that right now. If we follow this policy by starting at a different state in the environment for as many as there are, we'll have computed an important group of values.

Speaker 1:          07:08          We can think of this great of numbers as a function of the environment state for each state, it has a corresponding number. We can refer to this function as the state value function for each state. The state value function yields the expected return. If the agent started in that state and then followed the policy for all the time steps. The state value function for policy pie is the function of the environment state. For every state s it tells us the expected return. If the agent starts in state ass and then uses the policy to choose its actions for all time steps, the state value function will always correspond to a particular policy. So if we change the policy, we changed the state value function in a Markov decision process. We can express the value of any state as the some of the immediate reward plus the value of the state that follows.

Speaker 1:          08:07          This kind of expression is popularly known as the bellman equation named after the mathematician and black tie efficient auto Richard Bellman. This equation is also used in fields as diverse as control theory and economics, but it's absolutely crucial in reinforcement learning. In fact, there are four different kinds of bellman equations, but we'll just focus on this one right now. It's used to calculate the value of a given state by considering its options and we can use it to estimate the best action to take to find the optimal policy. There is another type of value function we should discuss though the action value function. While the state values are a function of the environment state, the action values are a function of the environment, state and the agent's action for each state es en action a the action value function yield the expected return. If the agent starts in state s then chooses action a and then uses the policy to choose its actions for all time steps in the state value function.

Speaker 1:          09:15          We kept track of the value of each state using one number in the action value function. We'll use four values for each state, each corresponding to a different action up, down, left or right for our agent at the agent wants to move up. It will then follow the policy until it reaches the terminal state. We then record the reward. We then do the same process for left. It'll follow the policy. The cumulative reward is our action value as it continually does this for every action at every state we get our completed action value function. We need to define the action value function before talking about how the agent can search for an optimal policy. The main idea is that the agent interacts with the environment and from this interaction it estimates the optimal action value function. The agent will then use that action value function to compute the optimal policy.

Speaker 1:          10:12          Once we have the optimal action value function, we can construct the optimal policy for each state. We need to pick the action that yields the highest expected return. If we follow the maximum action values for each state will quickly find the optimal policy, but that leaves the question, how do we find the optimal action value function? It's the intermediary step that's allowed us to find the optimal policy. And it's also the topic for the next video. Three things to remember from this video, though, there are two types of policies. Deterministic, where the action taken entirely depends on the state and stochastic, which allows for randomness. To learn an optimal policy. We need to learn an optimal value function of which there are two kinds, state action and action value. And we can compute the value function using the bellmen equation, which expresses the value of any state as the some of the immediate reward plus the value of this state that follows. Please subscribe for more programming videos. And for now, I've got to compute the right action. So thanks for watching.