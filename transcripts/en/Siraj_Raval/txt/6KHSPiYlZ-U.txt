Speaker 1:          00:00          Five days left to enroll in the decentralized applications course. Lincoln's description, hello world. It's a Raj or as my computer generated version would say hello world. It's Siraj. I don't know how well you could hear that, but that was the generated version of my voice. He says a lot of things.

Speaker 2:          00:16          What is this madness? This is crazy.

Speaker 1:          00:20          I like to travel. He says a lot, but basically he has a deep learning generated version of my boys. The Demo for today is going to be us generating our own voices. What I just played was this web app called liar bird behind the hood. They're using what's called a an encoder decoder architecture to generate voices after having trained on your voices. So definitely check that out. It's called liar bird. Got Ai, but we're going to talk about the technology that it uses and it uses something that is very similar to what Baidu released recently by new recently released a paper called neuro voice cloning and it's really rare for any of these big Chinese companies like 10 cent or Alibaba or Baidu to release something open source. So I'm very grateful. We should all be very grateful and we should find ways to incentivize these companies to share more of their findings because China is really, really, really going in on AI and we want to see more open source work, uh, from that region of the world.

Speaker 1:          01:18          So, uh, what we're gonna do is we're going to talk about deep voice, one deep voice to the points three and then the latest version of the boys, which is by dues, um, text to speech system called neural voice cloning. Basically you can say something and then translate it into somebody else's voice. Like Kate Winslet for example. That's why I had the picture of Kate Winslet, one of the authors of this, this get hub library that we're going to look at later. He translated his voice into Kate Winslet's voice. So you could say something, enter it into the system, and then Kate Winslet will say the exact same thing. That

Speaker 1:          01:51          right. So, and we're going to look at the code as well. It's tentraflow code. We're going to figure out how this architecture works, how it's evolved over the past two years at Baidu has been working on this and hopefully you'll get an idea of how you yourself can do this as well. The link to the get hub repo is going to be in a video description. Okay, so let's, let's go into this. So deep voice one, so in February of last year by do released the voice by do by the way, is like China's version of Google. It's like the biggest Chinese company as focusing on deep learning right now. But they released a system called deep voice, and what it was was a production grade system. That means that means that it was a system that was made for use in production environments for people to actually use that apply deep learning to text, to speech synthesis.

Speaker 1:          02:32          That means you type a sentence and then a computer then reads, it reads that sentence and it says what that sentence is, right? So the traditional tts, Texas speech pipeline consists of hand engineering, all these features of like what a boy sounds like, what a single word sounds like, et cetera. But deep learning does away with feature engineering, as you probably know, if not, it does away with it and said it learns all the features that it needs. And so the system can be trained and retrained and just a few hours. And here's the, here's the really interesting bit by new clamp, a 400 x speedup over wavenet. So wave net was deep minds really kick ass paper of the year last year, uh, on tts systems. And that's a 400 x speedup on the world's leading AI institution, which has really impressive. It's true. It's very impressive. So yeah, China, we gotta, we gotta, we gotta get in on China Sea, see what's going down in China.

Speaker 1:          03:28          You know what I'm saying? So, uh, so let's talk about this architecture. So what they use was an encoder decoder architecture who was inspired by famous paper called sequence to sequence learning that came out of Google. But the model looks like this. Here's what the pipeline looks like. So you say something, right? You say something, it could be like, hello world, it's Raj. And then it, it has a text version of that, right? So there's a text version and what it does is the model will burst. It'll take that text, right? All of those words, and it will convert those words into what are called phonemes. Phonemes are the way of describing what a word sounds like. So here's, here's what I mean by that. So think about the words though. And then rough. They both end in this, O u g H, right? But they sound different.

Speaker 1:          04:13          So how do we represent that for a computer to read, right? So the way we represent that, that is by using what are called phonemes, and there are dictionaries of phonemes online. So a, so check this out the word, the phrase white room in the phony version of that is w a y one. So [inaudible] means a certain way of saying Hii. Cc the sound here. Ahi. When we say it in the context of white, it makes a certain sound like hi, right white. But if we say Hii in another context, like hi, it's different, it's hi, instead of I rights, there's an h in front of it. So we could differentiate that using phonemes. So one would be like a one and then another would be [inaudible], right? So it's different than just saying like, here's what those letters are that consists of the word, here's what it sounds like.

Speaker 1:          05:03          So the first part of the model would convert the text into the phonemes, right? Once it had the phonemes and would use another model to convert those phonemes into a prediction of what the duration was of those phonemes and in the frequency of those phonemes as well. So how long do we say those phonemes in the context of, uh, of a sentence, right? How long does that phony molasses, right. It could be small, it can be long and there's, you know, it could, it could be a millisecond difference, but there's still a duration for how long it is. Frequency means intonation. Like how high, how low? Think about languages like Mandarin, right? Intonation matters. Like, you know, there's like four tones in Mandarin, like, you know, like that. So, uh, tone matters, duration matters. And then lastly, once it had all three of those things, right, it had the phoneme count, it had the duration, it had the frequency, it combined all of those and fed it to a decoder.

Speaker 1:          05:56          And what the decoder would do would be, it would reconstruct the original audio, but in the context of the original voice, right? So you have some text, it predicts all of these three features, and then it's going to construct the voice. So what would the spoken version of the text, the, and that's what it did. It wasn't converting anything into a different voice, it was just reading the text. But this was a really cool model. And the second step, like I said, was predicted durations and frequencies. And by the way, the way they did this, the way they predicted the durations and frequencies frequencies by, is by using that model in the middle that we talked about up here. But that model in the middle is calledF segmentation model because it's segmented the audio clips of each, uh, phony, right? So you've got the audio of the text, you at the phoneme that we're feeding into the segmentation model, and it will map one to the other.

Speaker 1:          06:51          Once we've got those, we'll combine the phonemes of durations and the frequencies to output a sound wave that represents the text. So they, they'd model this after deep mines. Wavenet so they were inspired by deep minds wavenet architecture, which looks like this. So the deepmind wavenet architecture, we can make an entire video. In fact, I, I have one on this. Uh, it's called generating music using tensorflow. So check out that one. So wavelets architecture is really interesting because it's able to say, how am I supposed to pronounce this in the context of what I'm about to say? So it uses what's called an intention mechanism to look at the input data and say, okay, so let's say we're trying to have a system that generates a speech of hello world. It's a garage. It's going to look at the word Saroj in the future. To see how it should pronounce hello world hits.

Speaker 1:          07:39          Because you know how phrases like they depend on like the, there's, there's a kind of flow, like an intonation flow when it comes to phrases and saying something is always dependent on what else you're saying, if that makes sense. So in a sentence, you're saying a couple of words and the intonation of later words depends on the intonation of previous words. It's kind of complicated, but it's basically looking at the future to try to predict the past. And that's the attention mechanism in this case. But basically it was an encoder decoder architecture that's, it had two different models and then in a few months later they released what's called deep voice to basically, it was a same model, but they just scaled it up, meeting they, they train it on even more speakers. So they were the scale ups from 20 hours of speech and a single voice to hundreds of hours of speech with hundreds of voices.

Speaker 1:          08:26          And each voice corresponds to a single vector. So about 50 numbers, which summarize how to generate sounds that imitate the target speaker. Right. So if we have some targets, speaker like me, we train this model on my voice, it's going to generate a single vector and that means a matrix with 50 different numbers in there. That's it. And that represents my voice. And from that the model can generate my voice like what we just did, which is really incredible. So then deep voice three was a couple months later, so nearing like closer to closer to now, which they said let's, let's do away with that model. Let's create a new model. And it trained an order of magnitude faster and it allowed them to scale to over 800 hours of training data. Even more voices. 2,400 voices.

Speaker 3:          09:12          Yes.

Speaker 1:          09:13          So it looked like this. So it was still a sequence to sequence model, right? It had an encoder and it had a decoder. But the difference was that it uses convolutional blocks. So recall that convolutional networks are used for image processing. However, if you look in papers recently, convolutional blocks are being used more and more in a sequence learning in general, right in recurrent networks and in coder decoder architectures because they'd done so well. Our CNN for example, was used by Facebook research recently to map someone's body in a, in a video and then turn it into a three d model in real time. So convolutional networks are being used more and more in sequence learning like sequences. In that case would the brand of a video

Speaker 1:          09:54          and what they use was a low dimensional speaker embedding. So they represented every single speaker with a different embedding and once they, once they had that embedding, they could retrain the model into somebody else's voice, which is kind of what led up to neuro boys cloning, which is their most recent paper, which came out just a few weeks ago. So neural flow voice cloning is really impressive. It's really impressive because it was an example of few shot generative modeling of speech. That means that using only a few samples of my voice, like a few sentences, it will be able to then generate speech in the style of meat. So it's like style transfer, right? Taking van Gogh's paintings and applying it to a novel paintings, but doing it for speech. And not only doing that but doing it with just a few examples, which is incredible. It's incredible. Right? So let's check this out.

Speaker 1:          10:43          So, so what they did was they said, let's segment this problem into two different categories. There's speech speaker adoption and then their speaker encoding. So Speaker adaption is saying, let's say I want to say something in the style of Kate Winslet. Speaker adoption is the problem of saying I'm, I'm, I'm going to say something to this model. It's going to listen and then it's going to output something in the style of Kate Winslet. But in order to do that, it's got to have an embedding a representation vector, essentially a vector of what I sound like. Remember that Matrix with 50 numbers in it that represents the Rod. That is an example of speaker adoption. How do we train a model to adapt to not only my voice, but to Kate Winslet scores? And then there's a problem of speaker and coding. How do we encode my voice?

Speaker 1:          11:28          How do we represent my voice into that matrix? So segmenting the problem into those two different problems helped them build this model. So in the case of speaker adoption, what that, what it meant for them was fine tuning a multi-speaker generative model with a few Connie examples and they train the whole thing using backpropagation. So it was using backpropagation for both the encoder and a decoder. And so what they found was when they mapped out all the speaker embeddings that they learned from all the different speakers that they trained on, they found that, uh, there were clusters around speakers who had the sim, how, who had similar regional dialects. There were also clusters around speakers who had similar genders, males and females, or clustered together. So males from a specific region all cluster together in terms of where they weren't in this latent space. Right?

Speaker 1:          12:16          This embedding space, the learn space, which is really cool if you think about it. There's a lot we can learn from that in terms of how language works and how different people relate to each other in terms of their language and where they're from. So there's that and it was called neuro voice synthesis. So I thought, okay, cool. Let's see, let's see what we can do here. Obviously they haven't really started code, but they did release the paper. I took a look at the paper. It's a really good paper, check it out. Uh, links are going to be in the description, but liar bird was a way for us to, you know, really do this with a web app. Uh, there's also this really great repository that I found. Um, it's uh, it's all, it's only four months old, so it's relatively new. And their architecture is very similar to neural voice cloning, which is why I picked it for this video.

Speaker 1:          13:00          But basically they had two networks, right? Just like the neuro voice clinic, like an encoder decoder type architecture. So the first network is an encoder and the second one is a Dakota. We can also think of it as the first network being a classifier. And the second network being a synthesizer or a generator. Let's just look at the code here. So I've downloaded the code and we're going to look at it right now. So under models our pie, we can see some of the codes. So let's, let's make this bigger and, and see what, see what's going on here. Okay, so they've got two networks here. They've got network one and network too, and they're trained sequentially. So we've, we're first going to train network one and then we're going to try network to in this code. So network one is going to act as a classifier.

Speaker 1:          13:41          What they did was they trained it on what's called a timid dataset. These are 630 speakers with the label being the, uh, phonemes for each speaker, right? So if a speaker says, you know, hello world, it's Raj. The label for that input data is going to be h e y l two, right? The phonemes for the, for that, for those words. And when it comes to input data with labels, it's a form of supervised learning. So what the first network did was essentially it was just learning the mapping between the input data and the phoneme, right? So then if you give it some novel speaker like who, who it's never seen before, it'll be able to predict what the phonemes are over time. So remember in neural networks are just a series of matrix operations, right? And these are just numbers and operations that are being applied to and using some optimization scheme like backpropagation or whatever genetic algorithms, it will slowly update the wait values over time.

Speaker 1:          14:35          So that that whenever we have some input data, it's going to hit the right numbers in those matrices to then output the, the most likely prediction, right? Over time it's going to be trained by minimizing the error, like how far off it is from the actual label. So when you give it some novel speaker, it's never seen before, it's able to predict what the phonemes would be just because, uh, there's a likelihood that it's going to be that based on what I've seen before. So if we look at this for the first network, it first loads up that vocabulary right? The phoneme to Idx, to a phoneme IDX to phoneme. And basically that those are the labels and the phonemes, like the words and the phone names for all those speakers. And then it does some, it does a form of data preprocessing by using a very small neural network just to make sure that the data is normalized.

Speaker 1:          15:20          So they called that a pre net. Then once it's, once it's got that a normalized data, it feeds it to the network. So they call it here in this code C, B H, g. But if we look in the, uh, modules CBHD, uh, it's actually just a convolutional network, right? So it's just, uh, it's just layers of convolutions over and over and over again. Accomplish one normalized convolution to normalize pollution, three normalized. And then if we go back to the models, we'll see that, okay, it's, it's, it's got the input data, it's got the labels and it has the, it has the output, right? So what is it predicted label? And then lastly, it does a final linear projection by using a fully connected layer. This is usually, this is like a very common thing in neural networks to have your last layer of your network actually just be fully connected or a dense layer in the case of tensorflow, a soft Max and then output a class probability and then the predictions are going to be formatted using two and 32 and then we return all of those, right?

Speaker 1:          16:19          So that's the first step to train this first network on the input data and the phonemes. And once we've got that fully trained network, then we can train the next network. By the way, the loss function they're using for that person network is cross entropy loss, which means we're just going to do them essentially the mean squared error. That's just a different way of saying mean squared error taking the mean of the, of the uh, actual, the prediction minus the actual label squaring it. And then returning that for the second network. It's going to say, oh, we've got the first network train knowledge generates some new, some new audio, right? So how is it going to do this? So when it comes to the new audio that what they used was they used some speaker Dataset. So for Kate Winslet, they used two hours of audio books read by Kate Winslet.

Speaker 1:          17:04          Okay, so, so what I mean is they said, let's first input what Caitlin's s says, that wave form into the first model, right? So it's a fully trained model. They fed the, what Kate Winslet said, not what the timid data sets speaker said, what Kate Winslet said to that fully trained model. Now what's it going to output after having been trained? It's going to output the phonemes. Exactly, yes. Once it's got the phonemes for what Kate wins, we'll say, then it's going to feed those phonemes to this second network, right? Which is the speech synthesizer network. Again, it's going to do some data processing using this pre network again, and then it's going to feed it to the same kind of convolutional network, right? It's, it's a similar convolutional network, but it's in the reverse, right? So remember neural networks are just a series of matrix operations, right?

Speaker 1:          17:54          You have some input data, you do some majors multiplication, you do an activation function and then you repeat and everything's really a glorified version of that. And this just matrix multiplication over and over and over and over again. And when it comes to deep learning, research, deep learning research is this playing around with what types of operations we're using, what sequence we're doing those operations in. And in the end, some of the best models are just researchers playing around with what should come first or should we try this multiplication here or should we divide this and then use the mean? There's this great xkcd where the guy's just like, how'd your deep learning model works so well? And the other guys just like, I don't know, I just jumbled up all this linear Algebra together and it made us great output. And it's basically the appointing research in a nutshell.

Speaker 1:          18:37          When it comes to the second network, it was able to generate audio from the phonemes. So the way it did that was it use a different type of loss function. And the different types of the different types of loss function was called a reconstruction loss. So we could see it right here. So it use a reconstruction loss. It had the phonemes and using those phonemes it applied the series of majors operations in this convolutional network that at first it knew nothing, right? So it's just that Dah, Dah, Dah, Dah, Dah. Here's the output, a jumble of numbers. And so that jumble of numbers, they compared it to the original what Kate Winslet set, right? What did it, what it Kaywin's that originally say that's the actual outputs. So then it computer or reconstruction laws, which is how well could it reconstruct what Kate Winslet said from the phonemes that were generated from the first model.

Speaker 1:          19:24          You see what I'm saying? It fed what Kate ones. I said to the first model output phonemes phonemes to the second model output, what the, the reconstructed version of what Caitlin's and say, which is actually just a bunch of numbers at first and compared that output to what Kate ones, it's voice originally was that the reconstruction error, they'll the loss between those and it used that difference. And it minimized that difference over time using backpropagation. Okay. So over time then the second model was able to better reconstruct the Kaywin's. It's voice over time. Now that's just Kate Winslet. If we kept beating at different speakers, it will learn multiple speakers, right? So it could reconstruct all of those different speakers over time. But they just fed Kate Winslet. So eventually right after training both models in that way. So the first was a classifier trained using cross entropy loss.

Speaker 1:          20:14          The second was a generator and the loss was instead of reconstruction loss, the first was trained on a million speakers. The second was trade on just Kate Winslet. So then if I say something, it will go through the first, create a phoneme, list that peanut to the second, and then you'll reach, you'll reconstruct it as Kate Winslet after training. So that's, that's how they did that. And they did it using tensorflow. The model works, um, the guy even has a, a demo of it as well, work with, and where can we see them? It's on soundcloud, you called it.

Speaker 4:          20:43          What's the culture of the revolution thus far? Had exhausted the young cub it's seems,

Speaker 1:          20:53          yeah, you can check out the demo there as well, but yeah, pretty cool stuff. All the lengths that up, the stuff I've talked about are going to be in the video description and I hope you found this video useful.