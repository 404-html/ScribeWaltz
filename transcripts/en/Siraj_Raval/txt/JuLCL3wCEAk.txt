Speaker 1:          00:00          Hello world, it's Saroj and have you ever bought company stocks before? We're going to train three different machine learning models on stock data. Then compare their results to see which one has the best predictive ability for future prices. Think about the process of creating an investment strategy. Once you decided that you do want to make an investment, you need to figure out which companies out there are most likely to give you big returns. Generally, you'd start by doing some research on the company's history and team. You'd read past news articles on how the company has fared over the years. You'd look at how stock prices have changed over time. Maybe observe what others are saying about the future of the company on Twitter, and finally you'd collect all of this data that you've gathered in your head to make a prediction about a future price. If that's not the perfect use case for machine learning, learning from past data points to predict future ones.

Speaker 1:          00:53          I don't know what is in a very recent paper from Auburn on this topic. A small group concluded that using data from several sources like Google trends, Wikipedia and Google correlate resulted in a model capable of assisting in investment decisions and had a relatively high accuracy for movement prediction, so we know academics are researching this and we know big banks are definitely doing this after all, quantitative analysts have it in their job description to apply methods to risk management problems, but is anyone else? Well, the Quantopian community is pretty cool. It's essentially a crowdsource hedge fund that encourages freelance data scientists to develop models for stock trading. They've got a bunch of educational resources on their website and data sets available for training as well. The idea is that once you sign up, you can write an algorithm in python using their ide. Then you have two options.

Speaker 1:          01:44          The first option is submitting it to one of their algorithm contests. If you can make better predictions than the competitors, you win a cash prize. The other is submitting it for an evaluation to see if you can get your model licensed. If it's licensed, then you'll receive a small allocation of funds that your model can use to trade stocks and you'll get 10% of the net profit it gains. It's a crowdsourced hedge fund and they built a pretty active community that discusses different algorithmic strategies. Quantopian isn't the only one of these DIY hedge funds to pop up. There's also numerous [inaudible] which allows algorithm contributors to remain anonymous and prove themselves through weekly tournaments. It pays in Bitcoin as well. When it comes to the type of model we could use, we have a huge assortment to try from reading results from papers is a good start to see what's been tried before.

Speaker 1:          02:34          Random forests, linear regressions, deep neural networks, and it's not just about using pattern recognition algorithms. There's also a space for reinforcement learning where a model can learn from its past predictive ability giving itself a reward only for good behavior, essentially self improving over time. So between all the individual different types of models, we could use the different combinations of them, all the different data points. We could feed it both numerical, textual, video, audio based, and the different time intervals. We could trade in. There are truly endless possibilities for us. The fact that it's such a multivariate problem is the very reason that investment advisors have been around since the beginning. We trust their insight that they've collected enough data to make valuable predictions but better than any human is of course a well tuned machine learning algorithms, so let's use the sentiment from news headlines and historical price charts together to predict future prices in python.

Speaker 1:          03:33          We'll use to machine learning libraries for our problem. The first is called NLTK or natural language toolkit, which will help us perform a wide range of text processing options like classification, tagging and tokenization. The second is called psychic learn, which we'll use for the actual machine learning. The data set we're using is the adjusted closing price gathered from the past 10 years for Microsoft stock. We'll use eight of those years for training and two of them for testing as well as a Dataset of New York Times article headlines about Microsoft for sentiment analysis. We can use the pandas data processing tool to combine both data sets into one data frame that we can then manipulate where the features are, the date, the closing price, and the article headline for that day. If there is one, well next one to perform sentiment analysis on these headlines, so we'll use the sentiment intensity analyzer from nltk to do this.

Speaker 1:          04:28          This will output sentiment scores for four classes of sentiment, negative, neutral, positive and compound, which is the aggregated score. The way it does this is by using a lexicon of predefined word sentiments. There are actually several popular sentiment lexicons out there. They are annotated manually by humans and pattern recognition algorithms. Use them to summarize the polarities of entire documents. We'll add rows for each of the sentiment classes. Then fill those rows after computing the polarity of each headline. Now that we've got our features, we can try out some different models to predict our objective variable, the price for a given day. Well, first try out a random forest model. This is a collection of decision trees, a model where an input query is entered at the top and as it traverses down the tree, the data gets bucketed into smaller and smaller sets at your passing through a series of thresholds for some number of trees, tea, we sample end cases at random with a replacement to create a subset of the data at each node.

Speaker 1:          05:27          M predictor variables are selected at random from all the predictor variables. The one that gives the best split according to the objective function is used to do a binary split on that node. And then at the next node we choose another m variables at random from all predictor variables and do the same. The second model we'll use is linear regression, which will draw the line of best fit between our variables. And the third model we'll use is a multilayer perceptron, also called a neural network. This is a series of matrix operations apply to input data capable of learning both linear and nonlinear functions due to activation functions that are applied at each layer. Until we receive an output. Prediction will input our data into all three initialized models and observe the results in graphs for each of them. They'll random forest model doesn't look too nice.

Speaker 1:          06:17          It's pretty off the linear regression model. It looks a little bit better, but it's still pretty bad, and the MLP classifier looks to be the best of all of them. So let's go with that one. To recap what we've learned, stock prices can be predicted by combining several data points from across the web. We can use the sentiment of the public company history and past prices as key data points in this process. And by using the Nlt K and psyche learning library, anyone can do this if they've tuned their algorithm properly. This week's coding challenge is to create a stock price prediction algorithm using at least four different data sources. You can find details in the github read me and winners will be announced in one week. Please subscribe for more programming videos. And for now, I've got to invest in myself, so thanks for watching.