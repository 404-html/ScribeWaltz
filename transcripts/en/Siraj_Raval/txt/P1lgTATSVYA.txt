Speaker 1:          00:00          Streaming in three, two, one go event is starting, stream has begun. The world is here and ready for reinforcement learning. I think we're live. Great. Hello world. It's the Raj and welcome to my live stream. And in this livestream I'm going to attempt this Kaggle challenge called the two sigma financial modeling challenge. It's $100,000 worth of prize money. And I'm going to sit, I'm going to create an algorithm that's going to hopefully get into the top 50 leaderboards. We'll, we'll see how well it does, but I want to just start off by saying that the point of this video is first of all to talk about some time forecasting techniques. Now, for some reason in all of the videos that I've made, I haven't talked about time series forecasting, um, outside of the context of deep neural networks. But I will in this video and the other part of this video is for me to show that reinforcement learning can be used in the real world, um, in an applicable setting.

Speaker 1:          01:03          And, um, it, this is part of move 37 so, so, so, so that's why I'm doing this. And, and, uh, how this, uh, video is going to be structured is it's going to be an intro Q and. A. So I'm going to answer two questions. So go ahead and start asking them now. I'm going to go over a time series lecture brief Q and a exploratory data analysis or Eda for our datasets, brief Q and a and then reinforcement learning. And the point of this is to predict, um, a value. Um, obviously all machine learning is about predicting a value, but we're trying to predict a specific target value and we'll talk about that when we get to it. But let me just start off by answering two questions and then we'll get into the code. Okay. Uh, the first question is, hi everybody, thank you for being here.

Speaker 1:          01:46          The first question is, can you suggest some gesture recognition algorithms? Sure. So, uh, right now pose estimation is the, the, the, the state of the art. So go to use tensorflow. Dot. JS probably the easiest to use implementation for pose estimation in the browser. Anybody can do it. So that's the one to use. Okay. So that's the first one. The next question out of two is what does it require to create a Bot like a human? Great advanced question. Now I'm going to minimize this and hello everybody. So the, that question, um, about like a human that is Agi, artificial general intelligence, you know, the Turing test, which hasn't really been passed yet. Um, but that would be an open domain chat bot that would, it would be trained on data that is not a closed loop but just, you know, the entire Internet and this hasn't been solved, but uh, the, the, the, the best way forward with that would be, I would say using deep reinforcement learning in the context of, of text data and, and the web and, and having an open domain where there's this, there's a cycle where you're using reinforcement learning where you're using a reward signal, the train a deep neural network where it's searching the internet itself, it's, it's querying the internet and it's building off of these queries.

Speaker 1:          03:09          It's using natural language processing, hence the deep neural network to create abstractions from the text data. And then based on those abstractions, it's learning to maximize a reward, which would be to say, you know, you could frame it so that a human would say, you know, yes, no binary, you know, that this is a good response or not. Uh, yeah. So, so that's kind of a research direction that I'm thinking of of, of what hasn't been done. But what would be cool for human level chatbots. Okay. So that's it for the Q and. A. Let's start talking about time series analysis because in this dataset they're asking us to predict a, a target variable based on the past. Okay. So, um, so let's just talk about time series analysis in general, right? Where we have two variables, right? So let's start off with univariate, a single variable time series analysis.

Speaker 1:          03:55          So we have some price data. Let's say this is for bitcoin. Okay. This is a bitcoin price over a period of days. Now, if we want to forecast the price for the next day, how do we do that? Right? This is a time series where the variables depend on the time, right? The what the values are, what their target variables are, are, are, are completely dependent on the time step, right? So, so that would, that's what makes it different from a regular data set. A time series data set depends on the time. So what we would do here is the naive approach. Let's just start off with a naive approach where we say what this next data point is going to be in this graph is going to be the target variable. That's it. So we're just going to say that the predictive variable, and here's the equation is going to be the the variable from the previous time step.

Speaker 1:          04:44          That's it, right? That's the equation right there. And we would call this the naive approach, right? And so what happens is when we have a data sets like this where just imagine the entire thing is one dataset and we have split it into training and testing data where we say, okay, this is the entire training dataset. Now based on this last data point, predict the next point. Well, it's going to say, well based on this one, let me just do that again because that's our variable. That's our equation, our forecast model. And so it'll do that again and again and again. And what happens is it's just a straight line. So this is a very bad approach and this is the naive approach, but let's see how we can improve on this. So how would we improve? Well, check out this graph so it's got volatility, it's going up, it's going down.

Speaker 1:          05:29          But notice that there is an average line. You can imagine that there is this average line, the line of best fit, you could call where it is the average between the ups and the downs. And we could draw it mentally through this model. And so if we do that, if we do that, then we can make the assumption that the next price is, is going to be the average of all the prices that came before it, right? So if we have that, that sequence of values of all of those why values the y values are right here on this, on this axis, and the x values are here the days, then we could use this equation. Now don't be afraid about the fact that we are using a little bit of math here. What this says is the target variable y which we can call y hat.

Speaker 1:          06:18          The one we want to predict is going to be equal to the sum. That's a sigma notation. This eat, this Greek he looking letter, the sum of all of those variables that came before it divided by the total number of them x. So from I to x, where x is the number of variables, add them all up. That's what sigma notation means. And then divide by the number of them and that's the average and that's our prediction. So if we do that, then this is what our line is going to look like. Okay, so it's saying based on this last data point right here, what's going to be the next one? Well, it's not going to be up here. It's going to be down here because we're taking into account all of those data points from the very, very beginning to the very end. But notice that this is not ideal either, right?

Speaker 1:          07:02          We need something that's going to be better than that. So how do we improve on that? Well, we would use a different technique called the moving average. So what the moving average does is it says, well, the points at the very beginning and the points near the end, these are completely different directions. So let's only consider the points, um, immediately before our forecasts are our target variable that we want to predict. So we'll have a window, okay. And we'll just average those and we'll leave out what we'll leave out the beginning. And so what that equation looks like is this where y hat, the predictor variable is going to be the sum of all of those, that all of those values that came before our, our target variable up to a certain threshold which we define as p, you know, say the first, the previous five or the previous six variables divided by the total number of them.

Speaker 1:          07:55          And that's the average. And so if we do that, now notice there's a little bit, um, it's, it's getting better, our prediction, right? It looks like this. Now how can we approve on this? Um, notice I'm going through a lot of techniques very fast, so slow me down if you, if you feel like it's too fast. Um, well, one way we can improve that is by using a technique called simple exponential smoothing. What that means is, you know, let's, let's take into account all of those variables because clearly all of them matter, but let's weight them differently. Okay, let's weight them differently where we say the variables that came immediately proceeding are our, for our predictor, our target variable. We'll weigh them more than the variables that came at the very beginning because these matter more. So how do we do that mathematically? Right? And here's how we take this constant value, which we're gonna call Alpha and we do the same thing where we're at, where we're adding them all up, but we're multiplying it by this, this, this, this, a constant value and squared cubed to the fourth, to the fifth.

Speaker 1:          09:04          Notice this trend here of exponential, exponential increasing. And so this is called simple, exponential smoothing, okay? And what this means is that these variables are going to be weighted differently. Now here's a question for you. I'm very excited and I'll be very impressed if someone can answer this question. What does this formula look like that we already know about from reinforcement learning literature, such rich literature? What does this formula look like? If anybody can answer that, I'm going to be very impressed. Let me keep going though. Okay, ready? Okay. It looks very similar to the discount factor from reinforcement learning. So in the reinforcement learning context, we have an agent. It acts in an environment, right? It's making an action. And it receives an observation of this, of the next state. And in order to maximize reward, how do we maximize reward? Well, here's how we calculate reward at every time step we can predict what the reward will be for being in a specific state up to our end state, the terminal state, and we'll add up all those rewards multiplied by a constant factor called the discount factor. And we are waiting those rewards in order of um, the rewards that came pre immediately, previously. We're weighing them more. We're saying that there are more important than the reward that came at the very, very beginning. And that's the discount factor.

Speaker 1:          10:33          Yeah. Wow. Actually people got that. I'm, I'm very, I'm very impressed. Good job guys. Very good. So let's keep improving here. So, so the, by the way, the reason I wanted to say that is because is to just give you some intuition behind reinforcement learning. It's, it's, it's a framework for viewing the world really. And, and how intelligent agents interact in the world. It's not the actual mathematics of intelligence of pattern recognition, but it's more about framing these pattern recognition networks. In the context of a dynamic world that adapts to that intelligent agent. More on that at the end. So Holt was a mathematician in the, in 1964 I think was a year who invented a linear trend model where he said, you know what, this idea of single exponential smoothing it works, it's fine. However, let's improve on that because it doesn't take into account the idea of a trend.

Speaker 1:          11:31          Now a trend is a general direction that we see that a graph is moving in and the way to mathematically define a trend as Holt a suggested in his linear trend model would be to create a forecast equation that consists of two other equations. So we have a level equation and then we have a trend and we use both of those equations to compute the final forecast equation. So it's l plus h B where l is the level of equation and B is a trend location. We have to constant factors, we have alpha and we have beta. They're both different and we can tune them accordingly. And the level equation is the same idea of exponential smoothing, but applied to, um, both the level, the average value in the series and the trend. And if we do that, then notice our graphs forecast is getting much better.

Speaker 1:          12:28          Okay. Now there's one more technique I want to talk about. And this is an improvement that Holt made to that a linear trend model. We'll start coding in a second, but, uh, it's called the seasonal, it's called his winter seasonal method. So there's another, there's another concept in forecasting called seasonality, where in a, in a set of data there's gonna be, there's gonna be seasons, right? So, um, in any kind of time series data, there's going to be some kind of seasonal, not any, but most of them real world. There's going to be some seasonality where there's going to be some kind of predictable up and some predictable down, let's say, you know, retail, um, for retail stores there's going to be more people buying toys in December because of Christmas and you know, a lot of western countries or you know, wherever, uh, or there's going to be, you know, some kind of trend in the, in the seasonal direction for stock markets as well.

Speaker 1:          13:22          You know, based on this, this is what's happening, here's how the market is going to go. So in order to mathematically defined seasonality, we have now three equations. So we're adding onto what we had before. Again, we're using our level, we're using our trend, and now we add a third equation, which is the seasonal, um, equation where the level of equation shows the weighted average between the seasonally adjusted observation and the non seasonal forecast for time. T t the trend equation is the same as Holt's linear method. And the seasonal equation shows the weighted average between the current seasonal index and the seasonal index of the same season last year. So they're all, all each of these equations is interdependent on each other. Okay. And so what happens when we do that is now we are getting somewhere. It's checkout this, this graph, it's, it's a much better graph, right?

Speaker 1:          14:14          So, so, so that, that's the idea of seasonality. Now that's for the case of univariate time series. Now if we have multi-variate time series, that's multiple input data for whatever the multiple predictor variables for whatever our target variable is going to be, which is the case for our two sigma financial modeling contest, then we're going to use a model that is very similar to a whole to winter seasonal method that's taking into account. They'll the level, the trend, the seasonality, right? To make the forecast. But it's also finding linear interdependencies between these predictor variables. So it's the same idea of a, of a, of multiple equations that are relating to each other in a way that we, once they relate to each other, we can create a graph and said some popular models for that are a Rhema or remax, um, et cetera. Um, and I'll make it, I'll make a dedicated video on those because you really need to make a dedicated video on those in, or we could create it.

Speaker 1:          15:16          Like we could treat this as a supervise problem is many people have done where we use the power of LSTM networks to then treat it as a supervise problem where we say the predictor variable, let's say pollution, uh, that's the, or sorry, the target, the, all these words, the target variable is going to be the result of the predictor variables, the temperature, the, um, the human waste use amount, et cetera. So there's a mapping between those two. And the reason we use LSTM networks, long, short term memory neural networks is because they take into account longterm sequence data and they can, they store memory in a way that is beneficial to sequential data, which is the case of time series data. And so that's why we've seen a lot of LSTM networks you being used in time series data. Okay, so let's get, let's get into some EDA. And like I said, I'm going to answer some questions now. All right. So what are some questions? Okay. Uh, what's out of focus?

Speaker 1:          16:21          I know it is. Okay. Uh, when did you start programming? I started programming, you know, that's, that's, that's, that's a hard question because, you know, I've, I've been, I used to like, I guess the, the, I guess the earliest time that I started programming was, oh, it's almost embarrassing to say, but like mark modifying halo two when I was I think 13 or 14. Uh, so that was, that was, that was a while ago, but I wasn't even programming. I was more like downloading scripts and just like hacking it and stuff. So, so it's been a while, but really seriously programming, um, probably a couple of years, couple of years. Okay. Who is paying you? Nobody's paying me. I mean, Youtube ads are paying me a Patriot fan. You guys are paying me, um, to do this. Nobody's paying me, nobody's paying me cargo and nobody's paying me. I would have to say that if somebody was paying it, which is the best book for our El Sutton and Bartow, have the Bible of our l, which is called an introduction to reinforcement learning. Find it on the Internet. It's, it's all, it's, it's all available for free. And, uh, last question is, um, I'm going to get a haircut for sure. Will you attend? I will do time series data. Definitely depends on other factors. It depends on a lot of factors. If there's multiple variables. Okay.

Speaker 1:          17:39          Okay. Now, uh, to the Dataset, let's go ahead and do this. So, so first of all, now the Dataset is in the video description. So, so check the video description. We're going to do this in Google together. Okay. So we're ready for our exploratory data analysis step.

Speaker 2:          17:54          Okay.

Speaker 1:          17:54          Okay. So what I did was, by the way, with Google Colab, with these two lines, you can mount whatever data set you want into Google Colab and then Colette directly. So what I did was I downloaded it. It's an h five file, uploaded it to my Google drive and then called it with this, uh, these two lines of code. Very simple. Thank you. Google colab or making it much easier to do. All right, so, so let's get into this code.

Speaker 2:          18:17          Okay.

Speaker 1:          18:18          Our first step is going to be to list out our Dataset, right? We, we, we have this data set where I have this data set in my Google drive and I have a link for you in the video description and I'll just want to see if it's there. Okay, good. It's there. That was it. Okay. So once I've seen that it's there, now I'm going to convert it into a pandas data frame. But before that I've got to import this dependency or install this dependency called tables. That's going to let me do that. And now we can import pandas are handy dandy data, preprocessing python library to then to then say, let's import this Dataset at train dot h five. Okay. Recursively and we're going to import it as trains. That's what we're going to call it. And then we're going to say our data frame is going to be trained dot yet, and then we'll, we'll call it by its name, train, and that's it. And hopefully good. So now we have it as a data frame and now we can see how big is our data set. How big is this thing? We got to check it out. This thing is massive. It is over 1.7 million data points, which is big. So let's examine this dataset just to see the head, just the, the, the first few variables.

Speaker 1:          19:37          Okay. So here's our data set. Okay. So we have an id, we have a timestamp, which is going to, you know, be a different time. And so these are all of our predictor variables. Now what do these mean, right? W W what, what do these mean? Right? And um, so there's like more than 40 to 44 variables and then we have why? So why is our predictor variable? So in the, in the, in this competition, two sigma, what they did was they said this is a book, these are a bunch of financial instruments. So financial instruments are like derivatives, bonds, mortgages, you know, stocks, assets, all of these different types of financial instruments. But they anonymize them. So we're calling them just technical 41 technical 42 and then we have our predictor variable. Now what does this predictor variable they didn't reveal to us, but we can think of it as a price, right?

Speaker 1:          20:25          Let's just think of it as a price in a, in a trend. And this price for this asset is dependent on all of these other anonymized financial instruments. And so based on all of these financial instruments, can we predict the price for whatever this is? Let's just say it's a stock for this case. Okay? So let's keep, let's keep going here. So our next step is to say, well, how many are, we'll call them labels too. So why he's going to be labels, how many labels and how many values do we have? So what we're gonna do is we're gonna list them both by creating two matrices and saying the labels are going to be upended by the number of columns that we have. The values are going to be upended by the number of non empty variables we have. And then we'll print out all of those columns and all of those values starting from the very beginning. All right. Oh right. The F, let's see if that works. Good. Okay. So these are all of our variables, all of our values, all of our labels in values. Okay. So just like that. So now we want to see how much we have to do some data cleaning. How much missing data do we have? So now we can use a map plot line to see just how much missing data we have. Cause we probably have a lot. So Plt, that's our map hot wide. And then we'll say we'll use this inline.

Speaker 2:          21:59          Okay.

Speaker 1:          21:59          A call to say that we want to be able to show a map pop live graph inside of the browser. Okay. So we're going to create a map, plop live graph. And we're going to say that it's going to contain a finger size. That's going to be between 12 and 50 so we'll, we'll, we'll keep it, we'll keep it small, relatively small. And we're going to start from those labels, which I named and I n D and connected to those labels. We have all of our values and I'm going to color them. I'm going to label them why. So in my graph it's going to say why. And now we can say,

Speaker 2:          22:47          okay,

Speaker 1:          22:47          let's say set the why ticks. So these are going to be the intervals between these variables to um, let's say it's going to be half of the width that I defined four, which is 0.9, because those values we saw before, um, they seem to be, they seem to be, um, in that range. So now we'll say, why ticks?

Speaker 2:          23:18          Okay,

Speaker 1:          23:19          and let me just do that again. Why ticks tick labels. So, um, that's the labels. And then we have our other line, which is our horizontal line. Oh, I'm going to name it the other line horizontal and we're going to have that, that's for, that's for y. And then count of missing values. That's, we're looking for the count of missing values x label. And one more, which is our title for our graph number of missing values in each column. Okay. That's it. And show the plot. Okay. Let's see. Of course, invalid syntax to, to, to, to X. Dot X. Dot. Set. Why ticks I n d plus with just like that? Uh Huh. I N D NP is not defined right? Is it really not defined? I didn't important empire up there. Okay, fine. Has an NP. Okay. Six size, right. So sometimes you just got to deal with these errors.

Speaker 1:          24:43          Nice. Okay. So it looks like we've got quite a lot of missing values and our data. And so, you know, if we, we could, we could just clean them all out, but this is a good step to wow. So fundamental 61 has a lot of missing values, so there's a lot of missing values in this data. Okay. So, so that's, that's what we wanted to do was just to, just to see that. And so let's just show one more pretty graph. It's, it's, it's a rainbow graph and we can use a, the other plotting library calls seaborne to, to do this. It's just one more very simple graph. And so we'll say, and I'll take questions. Um, right after number eight here. So six, how many people do we have in here? Okay. Two and 33. Okay, cool. So that's it for this. So now we can see at each time step we want to see at each time step what the data looks like. So we'll say how much of each a predictor variable do we have at each time step. Okay. So now, um,

Speaker 2:          25:58          okay,

Speaker 1:          25:58          we can see that. Cool. Okay. So that's the count for each versus the timestamps. So there's more and more. It's, it's a, it's going up. Okay. So it's going up. The trend of the data is going up. So that's just one thing to know. It's, it's a linear trend upwards as, as, as, as, as time goes on. Okay. And so lastly, we'll just one line of code and we're done with this, uh, Eda Park.

Speaker 2:          26:22          Okay.

Speaker 1:          26:23          Um, how many unique assets do we have in total? And that's Prince, the length of df.id. Dot. Unique 1,424. Okay, so, so let's answer some questions and I'll talk about reinforcement learning. Okay. Um, okay, cool. So can we use our n n with Lstm to predict the scenarios? Yes, you can, like I mentioned before and deep reinforcement learning is the, is the cutting edge for that too. Um, why not use Phillip asks, why not use pandas data frame methods to call the columns instead of using loops? Philip, that's a totally valid question and we could have done that. And lastly, uh, one more question. What is your opinion of no, that's no. How deep do you need? Do you need to know math for reinforcement learning? How deep do you need to know math for reinforcement learning? Um,

Speaker 2:          27:28          okay,

Speaker 1:          27:29          that's a great question. I compared to supervised and unsupervised learning, it is more necessary to know the math behind it because that ecosystem is not as developed as a supervised learning, um, ecosystem. And, um, while you, we can use open Ai's, Jim, could you a simple, you know, random policy for an agent inside of a game if you want to do anything more complex. Um, deep Q. If you really want to understand these algorithms, then yes, you're going to need to know how, what the idea behind policy functions are and the idea behind value functions both for a state and an action. You're going to need to know how the bellman equation works. And that's really what it comes down to. Understand the bellman equation and everything else will follow. And then that that's, and there, there's four of them actually, and I will continue to talk about them, but let's continue going here.

Speaker 1:          28:18          So, um, that's it for my QA now to our l, right? So that's our eda now for our ELL. So, um, how do we use reinforcement learning in time series data? So in reinforcement learning, there is an agent that is acting on the outside world. It is observing the effects of the environment and it's learning how to improve it's behavior. That's why we see it being used so often in games, right? So, but in contrast, a time series forecast is, is, is a setting where there is a passive observer. So the agent is passively observing the the Dataset and it's not really interacting with the environment because the environment is not reacting to the agent. It is, it is a one way. Um, it is a one way action, right? Whereas in a game world, for example, I Karch poll, right? Where the, where the pole is trying to balance itself, right?

Speaker 1:          29:12          If the agent's action in a given state is to move to the left than the environment, the, the, the platform that it's balancing on, we'll then move. It's reacting, right? So in a real world, how do we use this? Well, what is a system that adapts to changes that an agent, an AI mix? Well, the stock market could be one where a state, uh, will change because the state is the account balance. If you have an account balance when you make, when an agent makes an action, like buy, sell, hold, the balance will change. Or if we want to get more Meta, then the, the entire stock market will change. So if an agent makes a trade, then the market will change, right? So we can, that is a reactive environment. What's another reactive environment? Electricity grids, sensor networks, interconnected routing grids of, of, of, of, of data, of, of, of connections, right?

Speaker 1:          30:05          So any kind of system that adapts on adaptive system that, that, that reacts to an agent interacting with that environment is a use case for reinforcement learning. So a static data set is not necessarily a reinforcement learning scenario. So how do we solve this though? Because there are a bunch of companies out there that have these systems like Google for example, they used reinforcement learning to improve the, they, they use it to improve the quality of their, um, power usage and their giant data center. And they reduced our cooling bit bill, but I think it was 40% and even more after that. So there are companies out there, electricity companies, power utility companies as public works, companies that have these systems that need to be optimized, but they don't, and they have these real time data sets, rights meters that are happening in real time.

Speaker 1:          30:58          What they need then is a reinforcement learning solution. But right now, and here's an here's a startup idea. I want to get to you guys. Um, this stream is going up and down. Like there were 200 people here and now there's 600 people here. This is crazy by the way. So, um, so where was I? So, um, this is a call to action for startups. Okay. Because I see a real need here, here. Here's a pain point where there are companies that need a reinforcement learning solution to help optimize their profits for their systems and their data scientists out there that want to use reinforcement learning to then solve these systems. So what there needs to be is an intermediary that is, that offers a simulation as a service. And so what these simulation as a service companies do or startups will do is they'll go, they'll approach, and here's how I would do it.

Speaker 1:          31:47          I would approach one of these companies and say, you know, I, you know, I understand reinforcement learning. I understand that, you know, we can offer you a 30% reduction in your costs if you give us access to your real time Api. And we'll create a simulated environment based on that. And then we will give it to say Kaggle two to then allow their data scientists to create our El Algorithms. And so there is an intermediary step here. Now, now Kaggle can do this themselves and they have thought about this and who knows what's gonna happen there. But this is an idea that, that, that's time has come and more and more people are getting interested in reinforcement learning and there needs to be more simulated real world, not game world environments out there. So that's my suggestion. And so hopefully you understand the difference here between time series forecasting and reinforcement learning.

Speaker 1:          32:36          Um, from, from, from what I've said so far, why there's a need for it and how we can apply reinforcement learning to time series. If there is some reactive component to the dataset itself, it can just be a static data set. It has to be a real time API. Okay. So, so there is a possibility that we're going to see more of that in the future. Now, um, what I did find though, what I did find was a library. So the closest thing on Kaggle to, um, to this idea of reinforcement learning was created by this guy and it's called the Kaggle Gym. So what he did was he framed, he framed the reinforcement learning problem. He framed the, not the reinforcement learning. He framed the, the two sigma problem of predicting the, the target variable as a reinforcement learning problem as a mark decision process. And what I think this was the pioneering step in saying, let's create a simulation of a dataset and then solve the datasets, um, and then solve the Dataset in the context of a simulated setting.

Speaker 1:          33:48          Right? And so he created this library called Kaggle Gym, um, which, which takes that library. And what I've done is I've pasted in this library here and we're going to talk about it and then we're going to use it. So we're going to use that capital gym library to solve this problem. Okay. So, um, so little refresher here. So in reinforcement learning, we have a mark Haub decision process where we have an agent. It performs a set of actions in a given state to, to maximize reward. And the action that it takes given a state is considered the policy. So policy suggests it's a function that says, that says, given this state and given this action, oh no, given this state, what's the best action to take? Okay? That's how policy works. And so, um, there's two other, uh, functions here that are part of a mark of decision process.

Speaker 1:          34:38          The transition probability that says, what is the next likely states to go in? If you take an action in this given states and a reward function that's going to help you Max, help the agent maximize what we're awarded receives for taking a given action. Now this can be learned over time and that would be considered cue learning. That would be considered a model free method. These two functions could be learned over time, or they can be given to us beforehand, in which case this would be a complete Markov decision process. But in the real world, we will never almost never have a complete mark off decision process. We will almost always have a partially observable mark Haub decision process. What that means is that we won't have these transition probabilities, we won't have this reward function will have to learn them or we could just avoid those functions and learn what's called the Q function directly. Let me talk about that at the end. Okay. I just wanted to introduce the idea of a mark of decision process before we get into this code. So in this Kaggle gym environment that frons a sloth who births a suggested we have an r score. So what, so what Kaggle suggested was that,

Speaker 2:          35:53          okay,

Speaker 1:          35:55          what Kaggle suggested, let me just go back. Was that the data, we evaluate the scores using this equation right here. Okay, let me make this bigger. This is called the or score. So the, our score is one minus the, the um, the difference between the target and the

Speaker 2:          36:18          okay.

Speaker 1:          36:18          And the predictive variable squared, the sum of all of them divided by, uh, the predictive variable minus, um,

Speaker 1:          36:28          what was you again, this constant value you, it's not called you, it's called forgetting the name of it, but the, this constant value and one minus that and that's r squared. And then we can derive our from our squared by saying or equals sine of r squared times the square root of the absolute value of r squared. And that's going to give us our, and that's going to be are we can consider that a loss function because it's going to give us one scalar value. It's gonna give us a scalar value, which we can use to measure how good our, um, our predictive variable is, our predicted target is. And then based on that or score, we can see what the leaderboard says and then you know, we can see what everybody's, our scoring is here. So the highest one was 0.02. So we'll see what we can get using this Taggle gym library that was created before. So let me answer any other questions.

Speaker 1:          37:19          Mu Yes. Thank you very much. Moo, moo. Alpha Theta. I was in Mu Alpha Theta in high school. How can I forget? Move. Okay. How does start the basics? Move 37 is my course. It's all on Youtube for free. Check it out. Um, right me. You all right. Great guys. Thank you. Okay, so, um, so what is this, what, let me start off with this. The or score. This function is just the programmatic version of the equation that I just showed and uh, that's it. So that's what we're going to compute it. So let me, let me go through this. So inside of this gym, this Kaggle gym environment, we have an observation. And so what the observation is, is it is our, uh, training. It is our, it is our predictive variable. What we want to predict and our, it is our, it is the variable that we are predicting.

Speaker 1:          38:08          So the predictive variable and the target variable, what is already there, because we already have those targets or labels, we can call them labels, right? So labels, man, I'm sweating today. Yes. Okay. We can call them labels. So inside of our environment. So inside of this environment and in our environment, what is it? Our environment is art static data set. We'll split it up into training and testing data. Okay. And then here's the step. So this is basically recreating that open AI gym environment or, or an agent takes it, take us, it takes a step. The perimeter is the action that it takes and then it works. He's an observation and a reward. So in this very naive implementation, how it's computed, how the or score is computed is just by saying that the predictive variable is only going to be the variable from the previous time step.

Speaker 1:          39:01          We could do that actually. I mean we could, we could choose our own policy based on this, but but what inside of this alone, all it's saying is this is really the, the the key right here. Like this, this part right here, the reward for taking a step in this environment is going to be the, our score of our predictive variable and our target. Okay. That's our reward. And we returned that as well as an observation, which is going to be the values of both as we saw before. And, um, a boolean this has done or not. And then info, which is a logging, um, variable, right? So, so based on that, we can create a policy. So, so let's, let's write one using this, this variable. And the reason I pasted it all is because this, this could be its own python file, right? Taggle Jim. Dot Pie. Okay. So let's, let's test this out. So we'll say let's create our own agent environment loop. We'll define our own policy and then based on that, we'll, um, we'll keep, we'll, we'll try to improve it. Okay. So inside of this test function will say, go ahead and create the environment using make, which is the function that I just defined. Get the initial observation, which is going to be our variables that we defined before. We'll print them out so we can see them, you know, just for logging purposes, you know, what is the observation of both the target and of the, the um, the training data or the, the, the predicted, the predicted value. And then based on both of those,

Speaker 1:          40:37          we'll create our training loop. Okay. So this is the agent environment loop. Okay. Based on that. So what we'll say is wild, true. Here's the loop begins. But target value is going to be the initial observation.

Speaker 2:          40:56          And then, um,

Speaker 1:          40:58          we'll choose some starting point to just start from, um, like what is the predictor variable that we want to start from and we'll just say, um, 6.06.

Speaker 2:          41:13          And then, um,

Speaker 1:          41:16          observation was, so what are we going to get return when we take a step? I'll continue to explain this guys, let me just write this out. I'm, I'm not done explaining this is going to, so it's going to return. So this is the class that we just talked about. It's going to return all three of these things based on the action, which is the target we take. And if we're done break, we're done with the loop else. Now what do we do with the rewards, right? So we can choose any policy and here is where we actually show what that policy is going to be. And so what I'm going to do and as you're seeing right now is I'm going to print out three variables and then I'm done. So I'm going to print out the info. I'm going to print out the, the amount of rewards I'm going to print out the first few rewards, zero through 15 okay. So that's that invalid syntax for make. Oh right. Environment

Speaker 3:          42:15          equals make.

Speaker 1:          42:19          Okay. And then I'll test it out. All I do is just run test and that's going to give us

Speaker 3:          42:27          what environment's not defined. Did it really? No it is, it is. Check this out. Right? Right. And then, Yup.

Speaker 1:          42:49          Oh okay. I'll surveys shins. Not defined line nine ops or

Speaker 3:          42:56          vacation

Speaker 1:          43:04          rewards is not defined. Rewards out of pen. Oh, rewards.

Speaker 2:          43:15          Yeah.

Speaker 3:          43:17          Okay.

Speaker 1:          43:20          Okay. Let me answer some questions now cause we're definitely gonna have some questions here. Okay. Oh, break.

Speaker 3:          43:33          How did this not catch it? Okay. Gotcha. Okay. Let me answer some questions here. Thank you.

Speaker 1:          43:41          MMM. Let's see what we get here. Okay. So our public score is going to be 0.017 so compare it to,

Speaker 1:          43:59          so we're like number 43 and guess what this, okay, so, so, so guess what, so here's our policy, right? Here's our policy right here. All we're saying this is the naive method, but in the context of a Mark Cobb decision process, this is, that's it. The, the, the basic idea here is that we framed this as a mark Haub decision process where an agent is taking action in an environment, uh, to move from one state to the next state. And we're trying to maximize reward. And the policy to choose that action is going to be the predict. The, the variable that want to predict is going to be the variable from the last time step. And then to compute the how good it is, we're just going to find the difference between the predicted and the actual variable. So it's going to be the variable in t minus one and.

Speaker 1:          44:45          T. That's it. We could have done this in one line of code. However, in the context of a mark of decision process which we have here, we can then add to it by creating another policy by creating a better policy that's going to improve on this. It's like what would be an example to learning? Okay, so Q learning where an agent is taking an action, given a state in order to maximize a reward. And we are computing this Q table, which is um, a bunch of, it's a, it's a giant matrix of possible actions that we can take in any given state. And then we're going to optimally choose what those actions will be by iteratively updating the Q table using what's called the bellman equation. And with a bellman equation does is it relates one state to another. And if we can relate any one state in an environment to another state, then we can compute those variables that are different between them, right?

Speaker 1:          45:42          Like the state, the state, the state value function, and the action value function. And using those, we can compute an optimal policy. So that's how we could improve on this. However, like I said before, we need an environment that's going to be reactive. And this is also just a show that you don't necessarily have to have the greatest, you know, cutting edge algorithm in the world to place well, um, on to, on a, on a challenge or to do well in general, in machine learning. Sometimes linear regression can work better than a deep neural network if your data set is small or, or, or, um, you know, for, for a variety of reasons. So my point is that, um, so we placed using this very simple methodology. Obviously it's a very naive method, but I wanted to really sneak in a lecture on reinforcement learning on cue, learning on the difference between time series forecasting and reinforcement learning into this problem of this capital challenge.

Speaker 1:          46:40          And, and this was the, the most, this specific challenge was the most RL friendly challenge that's available on Kaggle right now. And like I said, this is a great example. This is a great opportunity for aspiring data scientists out there to create a service that creates simulated environments that can be offered to real world companies. And, you know, just to create a business out of that. Um, so I see a real need for that and that that could be a use case for this. I'll answer two more questions. Um, Qa or have, I actually have a great cue learning video coming out this weekend. I'm going to have some great links for you in the video description, the data sets in the video description and uh, let me do a wrap. So just say, uh, just say, uh, just say I'm a topic, I'll do a freestyle rap on the topic before I end this live stream before it ended. Okay. So it'll beat

Speaker 1:          47:40          my favorite pie company is school of Ai and we're actually a nonprofit organization and it is the adventure of a lifetime. And it is, it is a story that's going to be told, um, decades from now. And it's not even about me, it's about the deans. It's about the people running this. Um, yeah. Really it's, it's, it's a family on the students, the wizards. We're all a family. The people watching this. We are all a family. If you were here at the end of this live stream, you are a dedicated data data scientists who cares about, or AI researcher who cares about the future of AI and using it to solve real world problems. And that's our mission. Those are our values. Okay, so um, open Ai. Okay. No, no, I want to do different one chat Bot.

Speaker 2:          48:26          Okay.

Speaker 4:          48:28          Yeah.

Speaker 1:          48:29          Okay. Here we go. This is, there's always a little tag at the beginning.

Speaker 1:          48:40          I try to use a chat bot. I try to make map plot lied plotted out with the graph. But you can't because it's text data, man. You gotta use math. I don't know what you're using, man. You're out of class. You've got to take a chat Bot and visualize it in a way that people can't realize it. It's okay. Let me show you instead of not clot lab, let's do something else. Like I call it map plot. Jai. It's a new library. I just invented it. It's made for chat bots. The visualize in the browser. It's like a laptop. It runs on any browser in the cloud, Gpu, CPU, CPU. I don't care. That's it. That's it for you. All right. That's it. All right. That's it for the rap. Our thank you guys for showing up. I hope I made this joyful for you.

Speaker 1:          49:23          Time series forecasting or l Mark Haub decision processes and the accessibility of Kaggle as a way to earn a passive income. And, and, and in a way to hone your skills as a data scientist. These are all the things that I hope you've learned in this livestream. I love you guys. Um, we're about to hit 500,000 subscribers, so I can't wait until all your friends, we want to grow this community as fast as possible. Uh, so thank you guys. I love you and thanks for watching. For now. I've got to go work on school of AI stuff. So yeah, thanks for watching.

Speaker 2:          49:53          Okay.