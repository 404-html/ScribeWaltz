Speaker 1:          00:00:07       Oh world. It's the Raj and welcome to this live stream. Uh, today we're going to be generating m and I s t digit, uh, images with a variational auto encoder. Now I know what you might be thinking. M and ice tea, not again, it's please know m and ist. You might be thinking I'm just screw this. You might be trying to get out of the live stream, but I'm telling you stop. Okay? Because this is not about Emma and ist. This is about something much cooler than him. And ice tea. This is about generative models, specifically my current favorite type of generative model, the variational auto encoder. Okay? This is going to be dope. We can even generate Pokemon with this. Okay. And we're going to look into that. So sit tight because that's going to be dope. Who want, who doesn't want to make Pokemon?

Speaker 1:          00:00:53       I do. Okay, so, hi everybody. Thanks for coming in. I'm going to start off with a two minute Q and a and then I'm going to go right into the code. Okay? I've got this ipython notebook, Jupiter notebook and it's got the images in there, but I'm going to be coding all of it live and a, yeah, so it's gonna be awesome. So let's get down to business to defeat determinism. Okay. So do we have any questions because I love answering them. All right, so any questions about AI, machine learning, deep learning, all of it is all good. You know, so just just throw them at me and I will definitely be the person to, to answer them. Okay. So Hi everybody from Columbia and India and Pokemon and Hydro Bot and all over the place. We have over 150 countries represented in this community and we almost have 100,000 subscribers making us the largest AI community in the world.

Speaker 1:          00:01:53       So it's a huge deal right now we are making history. You guys are a part of history. So Matt Camp will asks, can you cover or maybe mentioned clustering? So that's a great question. In this course, this is a part of a course. Remember guys, a series of, of data points of knowledge that we are, we are building off of over time. And so right now we are finally on unsupervised learning. So what is one technique to learn from, from data that without labels clustering. So, uh, with for clustering, we are going to talk about clustering at the end. Yes. We'll, we'll talk about clustering. Not Like k means clustering, uh, which is the most popular type what we're talking about, clustering in an embedded space. Okay. So that's clustering in terms of the latent space that's we're going to go over. So one more question and then we'll get it.

Speaker 1:          00:02:43       We're going to get started with this. Okay. So Akshay Bhatia asks thoughts on cafe too. Okay. So here are my thoughts on cafe one through five. No, just no is my thoughts on it. Don't use it. Use tensorflow. Use Pi torch. Don't use cafe. Why? Because cafe makes you define models, uh, through what are those? Like Jason Files? No, you want to, you want to define your models programmatically. Why? For modularity? Because you want to call them through different parts of your code. You don't want to just define some static file that you then have to go into and um, there's not, there's not a lot of reusability with cafe. Um, internally within cafe there's a lot of reusability, but you can't really use it with other frameworks as easily because it's not programmatic. Okay. So that's it for the questions. Actually, one more. This is, it's actually a very good question.

Speaker 1:          00:03:32       Could auto encoding be usefully applied to generate genomic sequences? Yes, absolutely. You could. Absolutely. Because remember an auto encoder, and we're going to talk about this reconstructs the input data. So that's exactly, so you could reconstruct genomic sequences, but you could also generate novel genomic sequences. You could have you seen the movie Jurassic Park. We could make that happen. I'm dead serious. We could generate DNA sequences of velociraptors and t Rex's by reading datasets of, of current mammals and then extrapolating or what's the reverse of extrapolating back into the past and trying to find those base pairs from, from which everything else sprung up from. So there's a lot of cool stuff coming. Okay, so we are just at the beginning of this. Let's get started. Okay. Okay. Ganzer coming next. So let's get started with this. Uh, we've got 333 people live, so this is gonna be amazing.

Speaker 1:          00:04:31       Let me, let me make this bigger. So, generative models model a distribution of data over high dimensional space. What does that mean? So this image is a little weird because it starts with Czi. So Czi is the latent space. So what do I mean by that? Czi is after we've encoded some data, we have then represented it in some, right? We have a representation of some high dimensional data. And what do I mean by high dimensional? Well, even an Mni is t digit image is 784 dimensions. So the, these dimensions are pixels, right? 784 because it's a 28 by 28 image, right? So 784 dimensions. We've taken the 784 dimensional data, Pii data piece, data point, and we've compressed it into a lower dimensional space. That could be like 10 or 12 or 20 dimensions. It's a representation. But what happens when we in compressed data always there is some loss of that data.

Speaker 1:          00:05:27       It's lossy. So, so, uh, we take that lossy small dimensional representation and we generate and we reconstruct the original input with this yellow part, right? So this is the, so it's already been encoded into z and then we decode it and then we create a generated distribution of this data. Okay? So what do I mean by that? So let me, let me say two things. The first thing is that latent is the same as embedding. It means the same as hidden. Okay? These are just, when it comes to cicastic models, when it comes to generative models, only now do we start using the word latent. Because I dunno, Bays just like the word Layton, but it means the same thing. Don't be confused. Latent embedding hidden. It means the same thing. Okay? So then distribution, what do I mean by distribution? Right? You can think of a tutee distribution.

Speaker 1:          00:06:15       A tutee distribution is that is that bell curve, right? It's a golf Sian curve. So that's a two d distribution. But how do you, what? What even is a distribution in general? A distribution is a realm of possibility. Okay? It's a realm of possibility. What are all the possible values that some piece of data can be contained in? What is the wall that it can be contained in, and that is what a distribution is. A high dimensional distribution represents the realm of possibility of all the values from some dataset. Okay, so we have some generated distribution versus the true data distribution, which is what the actual there that we're training on. What is a real distribution of that data versus what we generate and we're going to use a loss function that I'm going to define that's going to minimize the difference between those two until they are indistinguishable from each other.

Speaker 1:          00:07:03       And this word indistinguishable is used in variational auto encoders and it's used in generative adversarial networks, which I know you guys are super excited to go into. I am too, and we're going to go into those, but we've got to, we've got to get our variational stuff down first. Okay. The name of the game is stochasticity. Okay. Non-Deterministic randomness. Okay. There's so much cool stuff we can do once we start getting into this. Okay. So auto encoders are a type of generative model. Okay. And so a regular auto encoder, it looks like this. It's boring. It's plain. It's simple. But I mean, I mean you can use it for a lot of cool things, but just comparatively compared to variational auto encoders, they're not as cool. So it's a five step process, right? We start off by initially by initializing our weights randomly, we always randomly initialized weights and then we do a full forward pass through the batch, through the encoding and then decoding.

Speaker 1:          00:07:56       So we can just think of both neural nets and auto encoder is there are two neural networks, right? An encoder and a decoder. But when we do a forward pass for all intensive purposes, we just think of it as one big ass neural network, right? We start at the input and we get to the output. But for forming a chain of computations, right? It's just matrix multiplication at every layer until we get an output, right? And then we construct a loss function via MSE, which means mean squared error of the original data to the reconstructed data. What does the mean squared error. It's a very generic loss function used a lot. It's used a lot. And it just means take the output versus the expected output been the difference between the two, those two. So subtract it, uh, and then add up the difference between all of your input output Paris divided by the number of them, and then those differences, you square them divided by the number of them and you get the mean of the squared error.

Speaker 1:          00:08:49       Okay? So it's a very simple formula and then we calculate the great answer using backpropagation. Okay, that's auto encode. Those are auto encoders. But right now what we're going to do is, um, you guys are hilarious. Okay, I love you guys. Okay, so those are auto encoders, right? So now we're gonna, we're gonna, we're gonna, we're gonna turn it up a little bit. Okay. Now we're going into variational auto encoders. So what are the problems with auto encoders there? They have so many problems. 99 of them. No, that's not, that's JC. So problems are they will overfit unless you have a large training dataset. So this actually applies to all neural networks, all neural networks. Oh, we'll overfed until you have, um, all generic deterministic neural networks that we're used to, that we've been talking about. They will all overfit unless we have a lot of data.

Speaker 1:          00:09:37       So how do we prevent this? How do we prevent over fitting? There's, or there's another problem, the gradients diminish quickly. So wait, updates get progressively smaller. The farther we go from the output, the vanishing gradient problem. And now if you can remember and um, here's a pop quiz for you. How do we solve the vanishing gradient problem for recurrent networks that give you five seconds, five, four, three, two, one. LSTM cells, right? LSTM sells gru cells. Why? Because they capture gradients over time because we were back propagating through time and we use these cells to, to not have that great advantage. But there's another way that we can solve this. The solution to both of these problems is to add a variational component to regularize training. What do I mean by regularize to improve, to improve training. So the variational component is we add some kind of random distribution inside of that.

Speaker 1:          00:10:32       And we're going to, we're going to go through this mathematically. So I'll talk about, so I'll, I'll go into detail about, about what I'm talking about, but we start off with an input x and then the inputs going to be some image or whatever. And then we say, what is the probability of xe given x? So what is the probability of this hidden state value given x? And it's going to give us this hidden state. And then we take the hidden state and we said, what is it? Probability of the output x given this hidden state? And that's gonna give us our output. And this little three d dimensional bad ass looking curve thing is what we're going to, we're going to coat it, we're going to coat it. Okay. So, so there were introduced three years ago. They're built on top of standard function approximators neural networks.

Speaker 1:          00:11:11       By the way, neural networks are considered universal function. Approximators I haven't actually said that in this course, by the way, but they are. What do I mean by that? Given any set of inputs and outputs, you have some function, right? You have some, you have some relation to mapping between this data, right? Like the most, the simplest one that we could think about is y equals mx plus B, right? The F of x equals y equals mx plus B. There's a relationship between y and x and the function is mx plus B. But neural networks can find a function, a mapping between any sets of input, output pairs, and uh, so there are amazing, okay, so let's go ahead and get started with this. Not LSD, l s t m although anyway, let's, let's just get into the code guys. Uh, where were we? So let's just get started with this.

Speaker 1:          00:11:57       Oh, and I've got some bad ass examples here. I didn't, I didn't actually show those examples, but we can generate Pokemon. So this dude, remember, by the way, we, we got to get away from this m and ice t stuff, guys. I mean if you look at the nips papers from 2016 the most used data set by far was m and ice tea. But what would be better as a standard like base format? Pokemon would be better. Not The new stuff. I'm talking about the original hundred 50 Pokemon that everybody knows and loves. Okay, use those. Why? Because when we are using generative models, we can generate new Pokemon that are a fusion of the ones that already exist. So this guy tried it out over here and now, and I've linked to this and the and the notebook. So check it out. So these were the original and this is where the reconstructions, so the reconstructions were clearly terrible.

Speaker 1:          00:12:43       He did. It was a very bad job. But the point is that he tried. So that was awesome. So check out that repo a and a for at least for the Dataset. And then one more example, we can generate faces as well. Now this is the second most used example for both generative adversarial networks and for variational auto encoder is generating faces. So this fray faces manifold is the, it's like the Dataset. And for generating faces, you can do that too. Okay, so those are two examples. And Dell, let's just get started with this code. All right, so the first step is for us to import our dependencies because we have only three that we're going to use. Hard mode is not on because uh, we're not hard mode in this case would actually be a lot of code. And then what I want, what do you mean by hard mode?

Speaker 1:          00:13:25       I'm talking about using just non pie. We're going to use tensorflow. Okay. And that's the name of this livestream tensorflow, right. So right, so, so, so that's what the name of the livestream. So once we've imported our dependencies, we're going to import our Datas Dataset. Okay. So tensorflow, luckily for us has M and ice tea beautifully packaged for us and we can just import it. We could just import that data set. There's no if and or buts there is just data when it comes to this stuff, it's easy. We just say input data, we've already called it and then we can just read it with this read Dataset function. The Reed datasets on Chin. Given the location of our data, which is going to be in temp data. And then we say one hot equals true, which means we want to one hot encoding. What does that mean? It's a way of encoding data such that it differential differentiates it without ranking them. So then what happens is they get an error. So that's, that's the next step. So then what I've got to do is I've got to make sure it's plural.

Speaker 1:          00:14:33       There we go, boom. So we've got our dataset because we don't care about the data. Really. We care about this, this model, what does it look like? So let's go ahead and define some parameters here. Uh, well the number of Pixels, right? [inaudible] these are a set of handwritten character digits there, 28 by 28 pixels for the image. I'm going to say 20 by 28 does a number of, and we're going to use this as a parameter when we define our tensors, right? So let's go ahead and say, let's define our placeholder. And what is this? This is an unsupervised learning problem. We don't have labels for this data or we do, but we don't, we don't care about them. In this case, we're going to learn to Jen to reconstruct our input images given our, uh, just our input images. Just, just given that alone.

Speaker 1:          00:15:13       So we're going to say tentraflow doff float. And so this is our placeholder, right? This is what we're feeding in our, our initial values. Okay. So x equals TF. Dot. Placeholder. Get the float. Good. We got the good stuff. Now let's go ahead and define some variable or some functions here. So the first one is going to be the weights. Very so, by the way, this is for our input input, the images, this our gateway, right? Remember placeholders are gateways and a, yeah, so wait variables. Okay, let's get start with our but wait variable. So the weight variables are going to represent the strength of connections between our units here. So given some shape, which is a tenser and some name, we're going to create wait variable. So the reason we're defining these functions here is because they're uneasy way for us to reuse what is going to be used for as many layers as we have.

Speaker 1:          00:16:09       This is good practice to encapsulate your logic that you think is going to be reusable into functions. Okay? So that's just oop, object oriented programming. And so then we've got our, our good stuff. So truncated normal. It outputs a random value that's either going to be bounded, truncated means bounded. So it's gonna be bounded between some set interval. Okay? And so the standard deviation is gonna be 0.1. The deviation is how far off from the mean does it, does it, does it move? Okay? So that's going to be our, our weight values, that's it. Those are our weight values. And then we could just return that as a variable because tentraflow works with variables. So we'll add initial to our variable and then we're going to give it a name that we're going to name it outside of the function. And then it goes into the function like, like it does.

Speaker 1:          00:17:01       Okay. So then we've got our biases. So bias. These are actually not that well understood. There's, there's not a lot of great explanations around why we use biases. It's just kind of like, you know, hand wavy, Oh and then we have our bias and then that's, and then the cows come home. That's actually a bad analogy for this. But what happens is we need biases because they are used to increase the flexibility of the model to fit to the data they prevent over fitting there like an anchor but like an anchor point. So you know, y equals mx plus B that the equation B is the bias because it's, it's it, it moves, it translates a graph right across a plane and um, they allow us to fit data even when the, an input value is zero. If an input value is zero, then if there wasn't a bias, it would just train, it would just output zero.

Speaker 1:          00:17:49       But since there's a bias, there's some constant value there. We have something, we have some value that we could continue training. So prevents overfitting and it increased the CR. It increases the flexibility of our model to fit to the data. Specifically, it allows the network to fit the data when all the input features are equal to zero, which is what I just said. Okay. So let's, I'm going to answer questions once I've finished this, this, um, this is good stuff. So we've got our truncated normal and the trunk had a normal, it's also going to be a, um, it's going to be generated randomly and then we're gonna return it as a variable. Variable is a modifiable tenser and uh, yeah, these are primitives and tensorflow guys, isn't Pi Torch so dope though. I'm definitely gonna make a video and Pi Torch soon. Not, not that tensorflow is an also awesome, but, uh, there's some good stuff coming out of Pi Torch recently.

Speaker 1:          00:18:41       Seriously. Uh, so we've got FC layer. So what does this, so we've got our weights, we got our biases, and now we're going to create a fully connected layer. A fully connected layer is one where all the neurons in one on one on the input are connected to every neuron in the output. Okay. So we, that means we want all of the data. And what do I mean by like you might be thinking like, well don't we always want all the data like when else? No, because sometimes we only want part of the data. Will it be a good example of this convolution? It's convolutional networks. If we have an image of say a banana, we don't care where in the image the banana is. We just care that there is a banana. So what do we do? We don't use a fully connected layer.

Speaker 1:          00:19:18       We use a convolutional layer that acts as a flashlight and it slowly moves through the image until it finds that banana. And it only uses that part. Why? Because it's computationally efficient. Okay. So given our fully connected layer, what we, we don't want that in this case we want a fully connected layer because we are not labeling data, we are generating data. So we want all of that image. Okay? So cause we don't know what it is that we want. We're not classifying anything we're generating. We don't, we don't care if it's a banana or whatever it is. Um, okay. So, so the fully connected layer is going to do a matrix multiplication between the input the weights and then add the bias. It's like y equals mx plus B. It's same thing. So those are our three layers that we have. Okay. So, um, yeah, those are our layers.

Speaker 1:          00:20:12       And now let's start writing out our encoder. So before we start writing at an encoder answering questions, let me answer some questions. Now. What do we got? We got before we have even more people here. I will, yes, I did a video intenser board. I'll, yeah, yeah, yeah, yeah, of course. Great. So, uh, [inaudible] says, please do a comparison between tensor flow and Amex net in your style someday. I'll consider it. I did. Tai says difference between RBM and an auto encoder. Please. So rbms that's a restricted Boltzmann machine and those are used, I mean to me the coolest use case of rbms are in generating music. There are more similar to mark off chains then to uh, auto encoders. They auto encoders reconstruct their, they're meant for, they're meant for image compression. Whereas rbms are are from the start meant for generation. Two more questions and then we'll get started.

Speaker 1:          00:21:10       Pershant asks, is it possible to make multilayer auto encoders learn to completely repeat in completely repeat. I'm just going to paraphrase is probably what you're saying is to to to reconstruct the input. Yes they are. Two more question actually two more questions. Does drop out reduced overfitting yes. It that's the, that's the point of dropout. That's the, that is the point of dropout. When somebody is overfit their minds are too closed. So how do you prevent close mindedness? You add new experiences, you, you do, you have some kind of new experience and then so drop out, drop out is akin to that. It is opening and closing connections randomly so that new Sonata synaptic connections can be made. All right. And then the last question is, does it make any sense to use to fcs to run in parallel and they combine atlast give output, does it make any sense to use to fully connected layers to run in parallel and then they combine?

Speaker 1:          00:22:11       So yeah. Yeah, no, for sure. I mean you could run them in parallel. Distributed tensorflow is a thing. We haven't actually talked in detail about that, but uh, yeah, it's definitely a cool thing. Yeah, for sure. So yeah. Anyway, back to the encoder. So we've defined our functions. Let's go ahead and start putting them to use by defining our encoder, right? This is what our encoder, it looks like we've got our input and codes it into light and space and then we output it. All right. It's simple stuff. We've been talking about this. Let's go ahead and get started with their encoder. The encoder time guys for our latent dimensionalities. So it's going to be 20 so what do I mean by latent dimentionality? So we have this latent space, this, this, this representation space. We want a certain number of dimensions, right? And what, what are those dimensions is going to be, let's say 20 now, should they ideally be 21 at 21 why not 19 we're just going to try 20 a lot of machine learning is guessing and checking these hyper parameters.

Speaker 1:          00:23:02       So we have a latent dimensionality of 20 for this light and space and then a hidden dimensionality of 500 what do I mean by this? This means 500 neurons and the input layer. So let's go ahead and say the final layer one. This is going to be a two layer feed forward neural network. So let's just find our two layers. We have our first set of weights and now we're going to use those variables that we just defined. We just define them and the number of pixels and the hidden dimensionally, this is going to be the size of our tensor. We want to make sure that that then that's why we define these things because we want them to be in a size that fits our tensor and then we're going to name it. Okay, so that's our weight. That's for our weights. And then for our right, so each layer has it's own set of weights and its own set of biases.

Speaker 1:          00:23:50       And we have our hidden dimensionality and then our B encoder. Okay? So right there we go. Boom. So that's for layer one. And then, oh, we have one more step here. Okay. So this is that, this is the fun part. I'm just gonna write Tan h here and then explain what I'm talking about, right? So we've got some input data, we put it through the weights and the biases and well, we've needs, we need to compute something, right? We need to compute something. So to compute something, we're going to use a Tan h function. So what is Tan h? So this is our fully connected layer, given the, uh, input data, and then those weights that we just defined in those biases that we just defined, we're going to compute some output value. Then we're going to feed the output value to this activation function.

Speaker 1:          00:24:38       That is Tan h. So teenagers are activation function, okay? So weights, uh, encoder biases and ex FC layer. It's all good. Okay, so activate. So the teenage function is considered the hyperbolic or the hyperbolic tangent function, which is similar to sigmoid except the range. So sigmoids range is between zero and one, whereas 10 ages range is between negative one and one. So it's a, it's a, it's a larger range. And when do we use it versus sigmoid? So there's actually a lot of theory behind activation functions and when we use it, but in this case, we're just going to say, uh, it avoids the vanishing gradient problem compared to the sigmoid, not in all cases. But when it comes to generative models, Tan h avoids the granted vanishing gradient problem. Why? There's a whole bunch of reasons behind that. We don't have time to go into all that right now, but let's just say it avoids the advantaging radiant problem. And this is a problem in all neural networks. Okay? No, no one is safe. No network is safe. Okay? So whatever we can do to prevent that, we're going to do, all right, so this, uh, this, this next layer where we can actually just copy and paste this cause it's, you know, it's the same thing more or less.

Speaker 1:          00:25:52       Am You m U and then m you, what do I mean by m you mean? Okay. M U is stands for mean and then we're going to rename it and then I'm going to answer and no, it'll actually, I just answered some questions so I'm just going to keep going here and you buy a Cs and then we'll call it what I've just called it. Do what I do. I do what I do. I do what I do. Oh Man. Okay. So X. Right? It's all good, right? It's all good. You guys got to, yeah, it's got to keep me in check. You guys got to keep me in check. You guys got to keep me in check. I love reading your comments. You guys are hilarious. Okay. Uh, see what I love about you guys is that you help each other out. That's, it is a culture that we are trying to create here.

Speaker 1:          00:26:46       Okay. So those are two layers. And so now is the, is the dope part. Okay. So, so this is what makes me really excited. So we've got our two layers and then generally, I mean normally that's it, right? This is we wipe our hands clean, drink some coffee, do whatever you gotta do, go home, play Gamecube, knocking cube. But in this case or we're going to do is we are going to, um, normally we would, I'll put a vector of real values, right? And that's it. And then we use backpropagation like we always do, we compute the partial derivative with respect to the weights given our error value. And then it's all good, right? It's all Gucci. But in this case, we have a stick, we have randomness built into the model. So we can't just brag, propagate. Normally we've got to, we've got to perform a trick, a re parameterization step.

Speaker 1:          00:27:40       And I'm going to talk about what I mean by that. So what we're instead of incentive, how putting a vector of, um, instead of how putting a vector of real values to this hint state, we're going to output a vector of means and the vector of standard deviations and then we're going to have a random component. Okay? And so when we back propagate, we're only going to back propagate in relation to this mean value and the standard deviation value. We don't care about this random value over here. Why? Because we can't, we can't take a partial derivative of a value that is random, that is not predetermined. The mean and the standard deviation are predetermined values. So of course we can back propagate but a random value. We can't do that. So what we do is we separate the random value. We put it off to the side and say, all right, you just sit there. It's all good. You just, you just live your life. And then we have this mean and we had the standard deviation and that's what we back propagate with. Okay? So I'm going to show you what I mean by this. So, so what we do, his here is, I'm actually going to,

Speaker 2:          00:28:37       uh,

Speaker 1:          00:28:39       wait a second. Uh, let's see. So for layer two, layer two, oh yeah, so there's no Tan h function here. So, so for layer two, there's just at the FC layer, okay, because this is the mean value this. So this is the mean value. So we're not going to perform that activation function. We just, if we did that, it wouldn't be the mean, right? So we've got that layer and then we've got our log STD

Speaker 1:          00:29:06       layer. Okay. So then, okay, so we just do this twice. So this is going to be for our standard deviation. So we'll just call it STD. And then what we're going to do is we're gonna say w log STD. So they also call it log sigma and a lot of uh, re repository. They call it logs sigma. Uh, but what we say is we say we say log STD and wd legacy be locks, STM, we name it properly. Cause remember we got to make sure that the names are all good. Be Log, STD, all the names, all the good stuff. Always, always low prices. Wait, that's something else. Okay. Log, STD.

Speaker 1:          00:29:49       And then h and coder. Let me make sure that it's all good because you got to do this right. You can just compile some bs. So FC layer. Uh Huh. Yup. Okay. Latent [inaudible]. Yup. Bias. Wwe. Yup. Yup, Yup, Yup. Oh No. And then latent, you got to make sure that it's all good over here. Okay. Okay. Okay. Okay, I get ya. Yeah. Oh See, see I missed some stuff here. Okay. So we've got our encoder and now we're going to do to write out our Dakota who's ready to ride out and decoder. Oh actually before we were at card or we got around it, our, our, our step here. Right. Okay. So here's the step. Here's the step. So this is, I'm going to write this out in all caps by the way, because it's that important. Get ready for this if you haven't been paying attention to, if you haven't been awake, wake up.

Speaker 1:          00:30:44       Cause this time for the randomness, the randomness. See how many times or would that ass, that's how important this is. So what we're gonna do is we're gonna say noise equals TF. Dot. Random normal for. So we're going to create a random normal distribution. And then we're going to say, well, what are, what are the intervals that we want to generate? This random normal distribution. This is our randomness. This noise, this variable right here is a randomness from which we're going to, we're going to take this randomness and we're going to, let me just show you what I'm, what we're going to do with it. So we've got our, so we have our mean and we have our standard deviation values. And we're going to say,Z is going to be our ultimate output. So this is the z is our ultimate output. So the z is the ultimate output of our Incode coder that we then feed to our decoder.

Speaker 1:          00:31:33       So what we care about in the end is z. That's, that's the output. So given some, so given, so our values, so we have our mean. So we take our mean and we add it to the multiplication to the, to the product. I blink for a second of our noise and our standard deviation. And we're going to perform this exponential step by taking half of the standard deviation. And that's going to be our, our step ultimate muscle ultimate outputs of our encoder. Okay? Okay? So when we back propagate, we care about these two values. Okay? The mean and the standard deviation, we don't care about the noise, but the noise helps us generate z by, because we multiply it by the standard deviation plus the meat. So it's adding randomness to this output without effecting backpropagation, which is perfect. How beautiful is that solution? It's a beautiful solution. It, so this lets us back propagate and um,

Speaker 1:          00:32:44       and the less information that we pass, uh, using that one variable, the more efficiently we can encode the original image. Okay? So the higher we raise the standard deviation on our golf galcion until it reaches one. Okay? So this constraint forces the encoder to be very efficient. So because we are looking for efficiency here, okay? So that's it for our encoder. And, uh, let me make sure that it, okay, so let's see what we got here. Wait, variable. It's not defined because I call it weight variable. Wait, did I even see I even variables. That's what it is. Wait variables.

Speaker 3:          00:33:24       Okay.

Speaker 1:          00:33:26       There we go. All right, so,

Speaker 3:          00:33:32       okay,

Speaker 1:          00:33:34       so we've got some errors. So we've got weight variables is not defined. Oh, because I renamed it up here. That's why.

Speaker 3:          00:33:43       Yeah, great. Yup. Yup.

Speaker 1:          00:33:47       FC layers not defined. I definitely define FC layer. I don't know what you're talking about. Oh, because it's lowercase, right? Guys, this is what's, this is the thing that happens when things happen, right? Things happen when things happen. Okay. We're getting closer, we're getting closer. Watch the debug the, it's all about the debugging in the end. Mean what are you talking about here? What'd you talking about, Willis? What do we got? We've got dimensions must be equal, but our seven 84 and 500 from Matt Mall. Oh, okay. Well, okay, let me, let me answer some questions as well while we talk about, and let me see what people are saying over here. Programming woes. Yo, we've got even more people. This is, this is a record right here. Uh, can we use an autoencoder to detect anomalies in time series data? That's a good question. I, I wouldn't, I wouldn't use that. I would use, um, how would use k means, which I haven't talked about, but I will a spoiler, but I will start talking about, um, K-means and a more generic machine learning algorithms. Uh, but I wouldn't, uh, what's this second layer you just wrote? Same weight dimensions. There's the other layer. Yeah, same weight dimensions. Uh, except we are not applying an yeah, exactly. The second layer is the same dimensions. Uh, accept.

Speaker 1:          00:35:21       Okay. Let me just, I gotta I gotta pace this one in because we got to keep going in. What is this? This is something else. Uh, there's so much, these are all my notes. There are so many notes here, isn't it? Either. So, so in terms of documentation, I mean, this is the most documented code I think I've ever written. So that's going to be a treat for you guys. Okay.

Speaker 2:          00:35:48       So where were we?

Speaker 1:          00:35:52       I was answering a question here. I wanted to make sure there are so many different things happening here that want to make sure that what this guy asked is, so the second layer is going to have a fully connected, so it has a fully connected layer at the end of it, whereas the first layer does not. The first layer applies and they both have fully connected layers, but the first layer applies an activation function to it, whereas this next layer doesn't. Because if we applied an activation, then we wouldn't get a mean value. And we need a mean value. We need to me and we need a standard deviation value as a way of representing the computations that happen in the encoder.

Speaker 2:          00:36:29       Okay. So,

Speaker 1:          00:36:35       oh my God. Oh my God. Okay. So we've got to say,

Speaker 2:          00:36:41       nope.

Speaker 1:          00:36:43       Eventually we're going to compile this. Eventually

Speaker 2:          00:36:48       it worked. I'll, Hallelujah.

Speaker 1:          00:36:50       Okay, so Dakota time guys. Two more questions and then decode or time.

Speaker 1:          00:36:56       Okay. Ricardo Muliro ask, would it be possible to develop this model with simpler TF code like TF, learn dot input data, TF, Aflac? Yes, absolutely. No. Uh, um, yeah. Ricardo, I'm just, I'm doing it intention file because it's more intensive and this is a longer session. But if you look at my weekly videos care os it's all, you can do this in 10 lines. Yes, you can do this when an API with lump one line, but we're trying to learn, uh, you know, a lot of stuff here too. Okay. What's the difference between a usual denoising auto encoder and a variational autoencoder uh, denoising there's a lot of differences, but, um, denoising auto encoders don't have to cast the city built in so they don't have this, uh, they don't have this re parameterization step that we just performed. One more question. Is it possible to use symbolic language to show or try to figure out how the model works? Symbolic language, symbolic language could mean a lot of things, but yeah, um, symbolic language to show them. So one way of thinking about this are, is in terms of computation graphs. So if we, if we think about what the computation graph it looks like, uh, so Pi Torch is actually a great, as is a, is a, is a great resource for that because what you rights versus what you see on the graph, they look very similar. It's, it's, it's not different. So Pi Torch, actually one more question because I'm just kind of

Speaker 1:          00:38:21       what is latent space? Lane space is the representation of what we've just learned. A lower dimensional representation of what we've just learned. Latent equals hidden equals embedding that means the same thing. Okay. So Dakota time. So for the decoder, we've got that embedding and now we're going to reconstruct the embedding and I'm going to,

Speaker 1:          00:39:01       okay, so let me, let me open something up. I'm going to, I'm going to open up one that already has, I'm just estimating time here. So what I'm gonna do is I'm going to say I'm going to open up my exact same notebook except it's got all the code already there. And uh, which is what I'm reading off of by the way. And I'm just, because we need to explain with the Dakota look. So for the decoder we have two layers. So it's, it's, it's also a two layer co. It's a two layer feed forward network, just like the encoder. And if you, so what do we, so let's go buy this line by line. Okay, so line one is we defined our weights for layer one. Okay. And then we used the latent dimentionality, which is 20 and the hidden dimentionality, which is 500 which is the number of neurons to define what the size of this weight variable, which is a matrix, what does that look like?

Speaker 1:          00:39:49       Then we say this is our bias variable. And so our bias variable is going to be much smaller. It's only good to be the sides for hidden dimensionality, which is 500 and then what we say is we say, we say, okay, so given our weights, given our biases, let's then define our hidden layer. Let's define the actual layer itself. So we say this is going to be a fully connected layer. So layer one is fully connected. They are both fully connected layers, both layer one and layer two are both fully connected layers and this is how we define we say are fully connected layer gets art z, which is the output of our encoder. This is where z comes in. What we just calculated, what the re parameterization step is right here given an Rz given our set of weights, given our center biocese compute the fully connected layer, it's what you output a set of.

Speaker 1:          00:40:39       Um, it's going to output that a set of values of doctor that we then squash with our dimensionality, with our activation function, which is going to be Tan h are hyperbolic tangent. And then that's going to be the, the hidden state values were and that, that sort of an h hidden decoder. Okay. So that's layer one. And then for layer two we say, okay, we've got those values and we wanted the feed them to our next hidden layer. So again, we, we, and we're going to call this reconstruction. Okay. And this is important because we're going to use reconstruction later when we generate a novel data points. So we say w reconstruct the be reconstruct and he's our weights and biases for layer two. And then, and then we say for our fully connected layer, we'll use both of those terms to then, uh, I'll put a vector of values that we didn't squash with a sigmoid function. And so why do we use a sigmoid function versus a Tan h function for this last, uh, step? Because a sigmoid function is going to output a set of values between zero and one. These are Bernoulli parameters. What, what do I mean by Bernoulli parameters? Because

Speaker 1:          00:41:53       in the end we just care about if something is real or fake, right? That's a binary output. It's a binary classification and a Bernoulli distribution only outputs values between zero or values that are either zero or one there binary out, but that's what we call it. Bernoulli. So Bernoulli's just a word for this, but to be technical about it, it's considered 784 boat Bernoulli parameters, which means 784 values between zero and one. Okay. And that's going to be the reconstructed image, uh, right. And so that's what sigmoid helps us do. It turns into those eyes to eyes between zero and one. And so our reconstruction, this is, this is our reconstructed image right here. This, this, this set of value stored in here. Okay. So that is our reconstructed distribution. Okay. So,

Speaker 3:          00:42:40       okay.

Speaker 1:          00:42:41       Uh, let me, let me answer some questions. Uh, and then we're going to get started with our loss function, which is going to be amazing. Okay, so questions we've got our or a, how do you, why are we calculating me and using FC layer? Uh, right. So why are we doing that? Because the, the FC layer is going to give us a mean if we don't apply an activation function to it.

Speaker 3:          00:43:23       Okay.

Speaker 1:          00:43:24       So Dick, he, Omar asked, how do you handle time series data labeled in? So for time series data, you would want to use a recurrent network, uh, and you would just, uh, the, the labels would be the targets and the expected output and then you could do something like mean squared error to do that. I've got a great video on that. See how to use tensorflow for time series. Could you also, so math camp, so asks how do we cluster the latent space? I assume we take z and then what?

Speaker 3:          00:43:54       MMM,

Speaker 1:          00:43:56       that's a great question. So the latent space, we can actually visualize that with map plot live, which we, we will do. And it's, it's actually, it's going to be clustered already. It's going to self cluster, it's going to self cluster. And we could just read that visually as humans and then say and then see like, oh, all these data points are related over here. So for m and ist clusters are going to happen. And this is a great lead in question two. What I'm going to show you for and ist clusters are going to happen around, uh, numbers. So, so all the ones, all the representations of the one, all the images like of the number one are going to be represented together here. And then all of the twos are gonna be here and in threes, right? All these images, we're going to be clustered together in a similar space and light and space. Okay. So, uh, that's a great leading question to us defining our loss function. And you'll see why. So now we've got our generated or generated image and we want to compare the generated image to what we, to the actual image, right? So how do we, we want to compare that, right? Cause if we go back all the way up here, what is it we want to do? We want to use a loss function to minimize the two distributions

Speaker 1:          00:45:05       between the generated data and the true data. And uh, so what we do

Speaker 3:          00:45:11       okay,

Speaker 1:          00:45:12       is we say,

Speaker 1:          00:45:14       this is what our last function looks like. So get, so get ready for this. So math time, this looks kind of scary, doesn't it? Because it's not in plain English. Math is an entirely different beautiful language that we have to get used to. We just have to get used to it. It just wasn't taught well in school. We're going to Redo all of that. Okay. We're going to Redo all of that. Okay. Math teachers were so boring. Most of mine were and they didn't really didn't show the beauty that math is. Math is so beautiful. Okay. So let me, let me show you guys what I'm talking about here. So what we have here are three equations, right? These three lines of all of these symbols that represent constant terms. These are laws, these are rules that we've found to always output a the same thing, right? Given this given some set and certainly all of the same thing given send some set of inputs. We can always, I'll put a set of outputs. So, so here's what this looks like. So what we have here, uh, or a set of inputs and outputs.

Speaker 3:          00:46:17       Yeah.

Speaker 1:          00:46:18       So what we want to calculate is the variational lower bound, which is what this first equation is. It is the variational lower bound and it consists of these two other equations. So if we take these two other equations and combine them, that's what this, that's what this topic equation. So this topic, why Asian is called the variational lower bound, and how do we calculate it? Or we take the log likelihood, the second value minus the KL divergence, which is this second dice. So see, see how this, first of all, he says log p x a probability of x, given Z, what is the law of probability of x given z? Minus. So we say it, we say it up here, right? Minus the Kale divergence term. And I'll talk about what each of these mean. Okay. So

Speaker 3:          00:47:03       yeah,

Speaker 1:          00:47:04       the, the very, so the variational lower bound is what we want to minimize. That is our, that is our final loss function. And that's what, that's what we call the, the, the uh, combination, which is really the difference of these two terms. So let's talk about them in order. So the first one is called the log likelihood. So what do I mean by the log likelihood? So

Speaker 1:          00:47:26       for the log likelihood, it tells us how effectively the decoder has learned to reconstruct an input image x given its latent representation z. So how effective is it? How well did we compare these? How well does this new distribution right up here compared to the old one? We want them to be very similar. And these are high dimension this, these are high dimensional distributions, right? But we can just think of them for our sake as just that bell curve. We want them to fit. We went to those two bell curves to look the same to, to be merged so that if you were to plot one on top of the other, you can tell a difference. It would just be an outline. So that's what we want and that's what this term gives us. Okay? That's what the log likelihood gives us. Okay. So, and we can write this out programmatically, right?

Speaker 1:          00:48:13       Still log likelihood is going to be the log probability of x given our given, that's the Kale divergence term. So the, the, uh, let's see, let me, let me go down here. We've got it. I got to make this phone bigger. So let's start off with each of these terms. Okay. So we've got the sigma sigma means that we have some set, some set of values. So we're going to continuously perform these operations as a, as a, as a set. So we're going to say x times the log of y. And so we'll ride this out programmatically. It's all the same. It's just a chain of operations that's going to give us this log likelihood. So we say x times a log of y plus one minus x times log of one minus Y, and then we can write to sell. Okay. And I'll show you that.

Speaker 1:          00:48:57       And so this next term is the KL divergence term. So the cake for the Kale divergence term, if the encoder outputs of those representations of Zee are different than those from the standard normal distribution, it's going to receive a penalty. So it's a regular realization term, that's what we call it. Because if we didn't include this that the encoder would learn to cheat and give each data point or representation in a different region of Euclidean space. Euclidean space is the same as latent space. In this case, it would just the same thing. So if we didn't use this regularization term than if I wrote a two and that's an image and I fed it to the model and they encoded it and it would plot it in representation space, there'll be over here. And then if someone else wrote it, let's say Jeff Dean wrote a two.

Speaker 1:          00:49:42       Okay. And then encoded it. And then we plotted that if we didn't use irregular riser, then the both representations will be far off. It would just be like over here and over here. But we don't want that. We want them to cluster. And so that's what this does it, it makes sure that the distance between two values that should be very close together is small. That's what the KL, divergence or the, the, the, the full name of this as the, was it coal, black Leibler divergence, something like that. And so the Kale lover, that's what it does. It minimize the distance between these two points so that there are cluster together, which is what we want. So both of these equations do to respective things for us. The first term, the first equation, the log likelihood is going to make sure that our reconstructed image is similar as possible to our input, to our input image. And the second term, the Kale divergence. Make sure that the, uh, that the values that we encode our are close together in hidden space in Layton space. Okay. So that's what both of them do. And if we take both of these terms and we subtract them, and these are right. So the value on the left is what the entire equation equals. So we'll just use that over here. If we just subtract, both of them will get the variational lower bound, which is what the ultimate loss function is. Okay? So,

Speaker 3:          00:51:02       okay.

Speaker 1:          00:51:03       Um, and what this is going to do is it's going to let us use to cast the gradient descent, which they standard backpropagation technique. It means backpropagation with respect to the variational parameters that those are the mean and the standard deviation. Okay? Uh, and so this is what it looks like programmatically, right? And what we did here is we just wrote out each of these, uh, values and operations. So if we were to look at that, if we were to look at the, um, what can we look at here? Uh,

Speaker 1:          00:51:40       let's see. So for log likelihood, we'll say x times TF. Dot. Law of reconstruction plus one e minus nine. And what does it x times log likelihood y minus C, it's the same thing, right? It's, it's the same exact thing. Plus one minus x times TF log, but one mice, sir. You see how it's the same thing? Like we're just literally number by number operation by operation. We were just riding it out programmatically. Right? Anyone can do this. It's, it's, it's, it's not hard just to get used to these symbols. They're just symbols. Okay. They just represent something. And for some people actually it's easier to look at the code. And, and for, for me, for example, it's actually easier for me, like for me to think about mathematical terms if I look at it programmatically because I'm programming anyway. I'm not, I'm not writing out equations anyway.

Speaker 1:          00:52:24       Okay. So, but these are, these are standard terms and you'll see them a lot in papers. Also. On a side note about math, when you see just chains of equations and papers, it's easy to get afraid because we haven't learned to represent these equations in more condensed terms. So usually what you're going to see is you're going to see a condensed version of an equation and then you're gonna see it. You're going to see the long form. So you're gonna see all of it. And so like the sigma term right here means the, the stigma term. So this could actually be re this equation right here could actually be written out by lots and lots of equations, but we use a sigma term to, to represent it. Okay. So those are our terms there. And so, right, so we've written out for the Kale term, the log likelihood and then we use our variational lower bound, which is a log likelihood minus the Kale term.

Speaker 1:          00:53:15       And then we use the mean value of that. And then we use our optimizer. So we're going to use a negative value here because of a, it's a little quirk with tensor flow, whereas if you were to use a positive value, it wouldn't minimize it, it would maximize it. So we're going to use a negative value to offset, uh, this, this core contents for flow. And I've, and I've recorded that in the get hub and the read me not in the review, but in the code that you have. Okay. So yeah, so that's our, that's our term. And we're using a, what was it we're using at a Delta, which is a form of stochastic gradient descent. And uh, yeah. Okay. So let's keep going with training. So now we initialize our variables. You've got her save her function here. And then let me answer one question. By the way, what are people saying here?

Speaker 2:          00:53:59       Uh,

Speaker 1:          00:54:02       Bernoulli better. Newly. Okay. Oh my God. This is one guy who's like, I'm Italian and I have to say it. Stop using hands. It was the funniest comment. Um, okay. So, okay. Two questions.

Speaker 3:          00:54:19       Okay.

Speaker 1:          00:54:21       And has a lot of good questions. Could you consider doing NLP for text mining using deep learning and work Tabak and topic model to automatically classify unstructured text?

Speaker 2:          00:54:33       Uh,

Speaker 1:          00:54:34       you would use NLP for text mining. That's, that's a good thought. Using deep learning. That's it. Even better thought and word to Vec. Yeah, I actually did this in a video. Uh, what was it? Word vectors using game of Thrones. So search Saroj word vectors. Game of Thrones. Okay, that's it. All right. Um,

Speaker 1:          00:54:59       right. So, okay, so it's a for training. We're going to say, okay, let's, we're going to import time to clock our training time. And we're saying, okay, we want 100,000 iterations and we say we're recording [inaudible] interval is going to be a thousand because we want to print out things every thousand intervals. And so we have these three arrays just for just for us, for our, uh, logging purposes. They don't do anything like for the, for the actual computation there just for us to log this log of our values over time. And so then we say, okay, and same for iteration array is just for us for logging. And so then we say, okay, for all of those iterations and we're going to feed data into our model in batches. Like we always do batches of 200 to 200 images in a batch or we say, okay, so the first batch went to you.

Speaker 1:          00:55:42       Um, and were you use a round functions to make sure it's binary feed into x batch. And then this is how we feed it into our model. We say section dot. Ron Given our off optimizer feed Dick. And we always fit it in, in, in terms of, in, in a dictionary x batch. And there's no target labels, right? There's no, this is unsupervised learning. There are no labels here. Okay. So then, and this is where, and then every interval, uh, every thousand iterations, we're going to print out the values that we are in for, for, for all of these terms here. Okay. And so then when we train it,

Speaker 2:          00:56:17       mmm.

Speaker 1:          00:56:21       So you see it's training right now. It's gonna take awhile, but let's wait for her to train. Let me answer a question while the trains. Uh,

Speaker 3:          00:56:31       okay.

Speaker 1:          00:56:32       So another question before we get started with the good stuff is a

Speaker 1:          00:56:42       make a video on basic pandas dataset. All tutorials are already processed. I don't know any book recommendations for ml. Yeah. So the deep learning book by Bengio actually, you know, just to really condense what I'm asking you to look at. So yeah, it's training right here, blah, blah, blah. Stop Training. That's going to take some of my Mac book would probably take like six hours to train this. Uh, just read that math chapter that first met. Like what does the math needed for deep learning? Like that's a great chapter. It's a great chapter. And uh, I've got some great stuff coming out on that, but I'm not going to spoil you guys too much. So let's assume we've trained it and then we plotted it. And so this plot is what, why we is why we collected those values in arrays so that we could plot the values of these three loss terms over time. And you could see that they are converging. They are, they are minimizing like that. Okay. Uh,

Speaker 1:          00:57:39       but uh, right and so actually, so these last terms should be going the opposite way. Like it should be going like that. Not like that. So if we were to flip this, they should, it should look like that because I lost minimizing this is actually increasing. And I think this is because we set the, uh, remember how we set the, the variation variational lower bound, too negative. I think that's why I, but it should be going down. Okay. So then for our results, here's how we plot out her images. So we say, okay, so let's take some images from our testing set and then we reshape it to 28 by 28 pixels. And then we say, okay, for a subplot, let's plot it. Okay. And so that's what it, so that's what's, what's on the left here. Looks like. Okay. So what's on the left here?

Speaker 1:          00:58:22       These are our test images and the ones on our right, our, our reconstructed images to say, okay, reconstruction dot evil feed dick x. So we, we take our, um, test image and we feed it directly to that reconstruction, uh, layer at the very end, right up here, this last layer of the decoder, we feed it in directly using the evil function of tensor flow. And we say, okay, so feed that directly to the x reconstruction, reshape it to it's 20 by 28, whatever it is, it's going to be output of whatever, it's not a values, and then plot it. And we could see how we've now reconstructed these values. Okay. Um, and so haven't wrapped in a while. So I'm going to rap about something. So let's see. Uh, I've got 390 people. So let's see, for a wrap, I'm just going to rap about variational audit. Actually, if I were to wrap about variational audit encoders, then you would think that it's off that I've planned something. So what I'm going to do as I'm to ask one of you guys to take say a topic and I'm going to rap about it because the lesson is over. So the wrap is here. So someone says, shout out a topic that you want me to rap it out in machine learning or deep learning.

Speaker 1:          00:59:34       Well, I answered some questions. So with this technique, is it possible to generate content, textual basing and pulverized pulverize news about this theme and the Internet?

Speaker 3:          00:59:45       Yeah.

Speaker 1:          00:59:45       Good job for using that term. Pulverized. Yes. You can create fake news. Absolutely. I mean this is, this has been done. Yeah. Don't rap, rap and Hindi. What a rap about Mexican cartels. That's not relevant. I am a super newb. C'Mon guys. Russian girls guys. That's not deep learning. I mean it could be, it will be in the future, but

Speaker 3:          01:00:08       uh,

Speaker 1:          01:00:10       I am waiting for a term guys. Come on.

Speaker 3:          01:00:13       Okay,

Speaker 1:          01:00:14       cycle again. There it goes. So

Speaker 1:          01:00:17       go for it. Let's, I've got my DJ over here. There we go. Psycho. Gans everybody, it's time for the cycle. Are you ready for the cycle again? Cause we're gonna go. We're gonna talk about cycle. Gans Psycho. Gans it's like I do it when I ride my bicycle. Man. I do it every day. I was talking about gangs, but right now it's about to be via is. I'm like a clown. Wait, listen, I'm trying to listen to this beat. I'm trying to wrap about generative adversarial. Neat. It's so cool when I do it. I've got networks that generate stuff like I'm a fluid. You could generate new fluids, you could generate everything. You could generate anything that your mind thinks it's all the same. You take some data. You weren't a distribution man, man. It's like Mu Alpha Theta in maths in high school. Remember that club that you were in everyday, man, it's like a flub. Yo. Anything that I said rhymes because it's like online every time. Okay, so that was it. Uh, right. So thanks guys for showing up for this and uh,

Speaker 3:          01:01:25       okay.

Speaker 1:          01:01:25       No, are we've got, uh, what do we've got? We've got a hundred. Got 375 people here. Thanks for watching guys and make sure to look at this stuff. The code is in the read me, the code is, uh, the code is in the getting the get hub repo and the description. I want you guys to start generating things. We don't have enough people generating cool stuff. We only have people generating m and ice tea digits and faces. Let's start generating some cool stuff. Let's start generating DNA sequences. Let's start generating, uh, like new fashion styles and just let's start generating entire three d worlds and maps and just alternate realities and just things that you would never even imagine. Entire movies. We're going to generate all of that stuff. Okay. So thanks guys for showing up. Come and do a hit in Moscow. I would love to come to Moscow when I do go to Moscow. Hopefully I don't die if I go there. No, I'm just kidding. I love Moscow. I don't, I can't say I love it because I haven't been there, but I love Russian dot. [inaudible] pizza. Okay. Uh, yes, I know Russian Donnie's nine. Krosky you didn't expect that, did you? No one expects that I actually know a lot of languages. Okay. So seven in fact, uh, not including programming. Bye guys. I love you guys. Think for now, I've got to go, uh, work on generative adversarial networks, so thanks for watching.