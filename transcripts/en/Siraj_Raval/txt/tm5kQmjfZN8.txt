Speaker 1:          00:00          Oh world. It's Raj and more robots. We're going to learn a bit about how military robots work and then teach a simulated robot how to walk using a reinforcement learning technique called proximal policy optimization. Military warfare is as old as mankind and the better the technology, the more likely the chance of winning is so it makes sense that militaries want the most advanced weaponry they can get their hands on by whatever means necessary. Ideally, militaries can minimize deaths of their own soldiers while maximizing their effectiveness. That means removing the human from the loop as much as possible, which is where the field of military robotics comes in. The first real advances there came from none other than electrical wizard. Nikola Tesla. In 1898 Tesla demoed a controlled motor boat to a government representative. It laid the foundation for the first unmanned vehicles and weapons as world war one began, Germany used radio controlled motor boats to ram into enemy ships containing more than 300 pounds of explosives during both world wars.

Speaker 1:          01:08          Inspired by Teslas. Initial invention using radio waves as a communication signal was a revolutionary idea. The Germans use it to deploy the first remotely piloted aerial drone called the Fritz. They dropped the device at a high altitude from a bummer. Then a bombardier would steer the Fritz via radio link using the joystick and it wasn't just the Germans getting it on the radio controlled fund. Americans dropped more than 450 radio, remote controlled grind bombs during World War Two remote controlled vehicles on land, air, and sea have improved in the decades since World War II as more than 40 countries have developed a technological breakthroughs in that regard. There are three important trends happening right now that are causing rapid advancement in military technology, namely the plummeting costs and soaring performance of computer hardware. The rise of cloud networked robots, and of course advances in machine learning put together these trends mean it's getting easier to create more powerful robots.

Speaker 1:          02:12          Soldiers no longer need to aim at targets if they can just use a convolutional network to detect enemies. Calm Nets are neural networks that can run the features of any type of object it's trained on be that a certain type of vehicle or a type of soldier. Using a pretrained network. Soldiers can dawn a set of goggles that show a computed bounding box around their targets or even feed the computed coordinate to a weapon and how it's positioned, adjusted accordingly and fire automatically effectively using auto aim. Reinforcement learning has been an effective tool to get robots to learn how to navigate virtual environments. Some soldiers go through extensive simulated training before being deployed to the real world and an l agent could prove useful as an unpredictable and adaptive enemy during combat simulation for a range of soldiers from fighter pilots to ground soldiers, these agents could use techniques like cue learning or policy gradients or actor critic to learn how best to avoid enemy fire and even create effective strategies to confuse the enemy before attacking.

Speaker 1:          03:22          Ai Can also be used to change the medium with which soldiers control if their weapons, they can issue orders via voice command instead of having to use a keyboard in tense situations. Thanks to developments in natural language processing. Speech recognition is getting really good and recurrent networks help make this possible. If we have some labeled audio Dataset, we can train a recurrent network by slicing the audio up into small chunks, feeding those chunks into the RNN sequentially and generating a prediction for the next chunk, computing an error back propagating the error gradients and repeating that process. Lots of times since recurrent networks are made for sequential data, audio is a perfect use case. After training, they can recognize human speech with incredible accuracy and this is essential for mission critical applications. Governments have mountains of data from surveillance systems and satellites of all kinds, finding disturbances using unsupervised learning techniques like anomaly detection via auto encoders, war.

Speaker 1:          04:28          It doesn't just have to happen in real life. It can happen online as well. Today, the process of finding and countering bugs, hacks and all sorts of cyber infection vectors is still effectively artisinal. Professional security experts search millions of lines of code to find and fix vulnerabilities that could be taken advantage of by users with ulterior motives. All of this could be automated using deep learning to find the features of vulnerable code and learning to patch bugs by itself. So we'll based intelligence can only do so much, but by integrating learning algorithms, machines can become far more capable. When a human is in the loop, they are able to make the final call the highest level decision. That's the one that the human makes, but as AI gets better, it's able to make more decisions for itself. So how many decisions did we delegate to AI?

Speaker 1:          05:21          How much is too much? Take the idea of a swarm. For example, any aircraft manned or unmanned can be brought down by a single missile, but a swarm can take multiple hits and keep going. It's a collection of drones that can coordinate together as a single entity. One could take out enemies systems while the other creates a distraction. While the other picks up important cargo. Evolutionary techniques like particles form optimization can make this happen. The U S Department of Defense recently released a document called a human systems roadmap review that reveals a plan to use AI to create autonomous weapons using social media analytics to make decisions on lethal force with minimal human involvement. It shows that while having a human in the loop is necessary for the near term, there are far term vision is self aware of systems. They defined this as systems that have perception, reasoning and intelligence allowing for entities to have existence, intent, relationships and understanding in the battlespace relative to a mission.

Speaker 1:          06:30          That means drones that coordinate amongst themselves make decisions for themselves, prioritize by themselves, and of course decide who to kill and when to kill them by themselves. That was kind of scary and reminiscent of the infamous skynet system from the movie terminator. In the movie, the military system becomes self aware and decides to kill all humans and no one can shut it down. We know of one system that can't be shut down and that's bitcoin. Because the blockchain miners must verify transactions using the proof of work algorithm. It would require more computing power than the 500 fastest supercomputers in the world combined to shut it down and no one has that much computing power on AI agent that lives on this kind of blockchain couldn't be shut down. And if it's objective was to say destroy humanity and it has access to weapons of mass destruction, that's how skynet happens IRL.

Speaker 1:          07:30          So how do we stop this? We've got to spread awareness and educate ourselves on how AI works. The more people that understand the power of AI, the less likely it is that governments will abuse this power and the more likely it will be used for beneficial purposes. More research into creating explainable AI systems that give us a detailed list of why an AI made a certain decision will help unravel the black box of deep learning and help fuel a debate for policymakers as they regulate this technology. So to give an example of how a robot can learn to walk, we can use a reinforcement learning technique called proximal policy optimization. This is to randomly initialized neural networks and a teacher that rewards forward progress. The policy gradient technique takes steps in the direction that improves a policy. A similar technique is called trust region policy optimization or trp.

Speaker 1:          08:25          Oh, the idea is to take steps in the direction that improves the policy while simultaneously not straying too far from the old policy. That's the difference. Making too big a change from the previous policy in high dimensional environments can lead to a dramatic decrease in performance. A little forward lean can help running speed, but too much causes a crash. A naive solution is to take minuscule policy steps, but the question then becomes how small a step Turp Oh, takes a principled approach to controlling the rate of policy change. The algorithm places a constraint on the average KL divergence between the new and old policy after each update. So proximal policy optimization is an implementation of trp. Oh, that adds the Kale divergence term to training a loss function. With this loss function in place, we can train the policy with gradient descent like a typical neural network.

Speaker 1:          09:23          In our PPO algorithm, we capture sequences of states' actions and rewards from our environment and add it to our data. Batch line 10 adds value estimates to each visit. It's state from the rollouts with predicted state values in hand. We calculate the advantages and add these to the Dataset. In line 11 the advantage of a state action is how much better or worse an action performs. Then the expectation of present policy from the same state. We update the policy in line 13 finally, we update our value function to reflect our latest data. In line 14 we use the present date of batch and the previous data batch to smooth changes in the value function. For our policy update function. We store the old policy and compute the KL divergence as we make policy gradient updates. And after only 25,000 training episodes are humanoid, we'll start learning how to walk.

Speaker 1:          10:20          It's pretty hilarious to watch the progress in the meantime. All right, so three ending points here. Militaries can use AI to create autonomous weapons systems and that means less and less of a need for humans in the loop. Uh, skynet's likes scenario can occur if the public isn't made aware of AI dangerous and governments go unchecked. And proximal policy optimization uses two neural nets and the teacher to forward progress. To train an AI to complete an objective. This week's coding challenge is to use the PPO technique on a game of your choice. Details are in the read me posted. Get up links in the comments section, and winners will be announced next week. Please subscribe for more programming videos. And for now, I've got to go prevent skynet, so thanks for watching.