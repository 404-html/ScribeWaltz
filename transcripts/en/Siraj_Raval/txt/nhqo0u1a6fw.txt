Speaker 1:          00:00          Hello world, it's Saroj and let's talk about how gradient descent has evolved over the years. Tensorflow gives us quite a few options for picking a gradient, descent based optimization strategy. This is what causes our neural network to actually learn from data, but it's not immediately clear how we should pick one. Once we understand the underlying details of the most popular ones, it will become much more clear which one we should use. When you study machine learning for a while, you start making connections between these mathematical techniques and the way we learn. At this point, I view pretty much everything through the lens of data and computation and it's a beautiful feeling. For example, sometimes if our Dataset is too homogenous after training, our model will be to fit to this data. It'll be overfit, meaning it won't be able to generalize well, so if given some different data points, it will be able to make an accurate prediction.

Speaker 1:          00:57          We have to keep our training data diverse and in the same way, if we keep our brains training data diverse by traveling and seeking out novel experiences, we'll be able to generalize better and generalization is the hallmark of intelligence, so that's why I like anchovies. If you were to ask what the most important machine learning technique is, the answer is without a doubt gradient descent. It is the foundation of how we train intelligent systems and it's based off a very simple idea. Time travel instead of immediately guessing the best solution to a given objective. We guess an initial solution and iteratively step in a direction closer to a better solution. The algorithm just repeats that process until it arrives at a solution that's good enough. Since there is no way we can know the best solution from the start, this educated guess and check method is supremely useful.

Speaker 2:          01:52          Gradient descent, find the ideal minimum, control our variance, update our parameters and lead us to convergence.

Speaker 1:          02:02          A simple direct example of gradient descent would be if we want it to calculate the minimum value of x squared. That is where the wide value is. The smallest. We could just randomly guess x values from all over the place, but if we used gradient descent, we arrive at a solution way more efficiently. The derivative of x squared is two x and we used that derivative to calculate the gradient at a given point, so we could first guess three and take a baby step in the direction opposite of the gradient and x equals three which is negative six then the next guest might be at 2.3 then 1.4 then 0.7 until we finally reached zero, which is the local minimum. Both we and the machine can see that this is the optimal solution since we can follow the trail of optimization visually and mathematically as it leads us to convergence.

Speaker 1:          02:49          Another way of thinking about the idea of gray and dissent is examining how a professional athlete improves that a gate. As long as there is some objective function that can measured like the number of wins in a season and some data that contributes or detracts from that function say passes, number of three pointers, steroids, that player can iteratively take baby steps in his routines after analyzing the data to improve, so to decide which of the gradient descent optimization techniques we should use in our model. Let's learn about the various discoveries around grading the sense over the years. Traditional gradient descent computes the gradients of the loss function with regards to the perimeters for the entire training datasets for a given number of epochs. Since we need to calculate the gradient of the whole Dataset for just a single update, this has relatively slow and even in tractable for data sets that don't fit in memory.

Speaker 1:          03:44          So to get around this intractability we can use to castic gradient descent. This is where we perform a parameter update for each training example and label. So we just add a loop over our training data points and calculate the gradient with regard to each and every one. These more frequent updates with high variance because the objective function to fluctuate more intensely. This is a good thing in that it helps it jump to new and possibly better local minimum. Whereas standard grading dissent, we'll only converge to the minimum of the basin that the parameters are placed in, but it also complicates convergence to the exact minimum since it could keep overshooting. So an improvement would be to use minibatch gradient descent as it takes the best of both worlds. By performing an update for every subset of training. Examples that we can decide the size of training in many batches is usually the method of choice for training neural networks.

Speaker 1:          04:38          And we usually use the terms, the Cassick rating dissent. Even when many batches are used, the oscillations in plain old SGD make it hard to reach convergence though. So a technique called momentum was invented that lets it navigate along the relevant directions and softens the oscillations in the irrelevant directions. All it does is it adds a fraction of the direction or update vector of the previous step to the current step, which amplifies the speed and the correct direction. So it's just like momentum from classical physics. Thanks Aristotle. When a ball is pushed down a hill, it accumulates momentum, meaning it gets faster and faster. In the same way, our momentum term increases four dimensions, who's creating Ann's point in the same direction and reduces updates for dimensions? Who's gradients changed direction? This means faster convergence and reduced oscillations, but a researcher named Yuri Nesteroff saw a problem with momentum.

Speaker 1:          05:35          Once we get close to our goal point, the momentum is usually pretty high and it doesn't know that it should slow down, which could cause it to miss the minimum entirely. He solved this problem in a paper he released in 1983 and we now call this strategy the nests rob accelerated gradient in the momentum method. We compute the gradient, then make a jump in that direction amplified by the previous momentum. In this method we do the same thing but in a different order. We first make a jump based on our previous momentum, calculate the gradient, then make a correction which results in an update. This more anticipatory update prevents us from going too fast and were more responsive to changes. So this idea of more dynamic learning of adapting our updates to the slope of our air function is a good one. Perhaps we could apply it to our learning rate as well.

Speaker 1:          06:24          Good ideas are good. That's what Adec Grad does. It stands for adaptive gradient and allows the learning rate to adapt based on the parameters. So it makes big updates for infrequent parameters and small updates. For frequent ones. It uses a different learning rate for every parameter at a given time step based on the past gradients that were computed for that parameter. This means that we don't have to manually tune the learning rate. It's main weakness though, is that the learning rate is always decreasing since the accumulation of squared gradients and the denominator grows because each added term is always positive. At some point the learning rate could get so small that the model is just stops learning entirely at a delta was invented to solve this and add a grad. We're constantly adding a square root to the some causing the learning rate to decrease. So instead of summing all the past square roots, we restrict the window of accumulated pass gradients to a fixed size.

Speaker 1:          07:21          We defined the sum of great as a decaying average of all past squared gradients instead of just stored previous squared grades. So the running average at a time step depends only on the previous average and the current gradient. So now we're getting somewhere, we're calculating individual learning rates for each parameter, calculating momentum values, and we're preventing a vanishing learning rate. What could we possibly do to improve here? Sprinkle in some adaptive momentum. Since we're calculating learning rates for each parameter, why not also store momentum changes for each of them separately? That's what Adam does. It stands for adaptive moment estimation. We calculate the first moment, the mean and the second moment the uncentered variants of the gradients respectively. Then we use those values to update the parameters just like in at a delta. If we were to visualize these optimization algorithms during the learning process, we'll see that the adaptive learning rate methods quickly find the right direction and converge super fast while momentum and the nest rob accelerated gray and go in the wrong direction.

Speaker 1:          08:26          So which optimizers should you use and your neural network? It seems that Adam is the best overall choice since it usually outperforms the rest followed very closely by the other adaptive learning rate methods out of Grad and add a delta momentum. Plain STD and nesteroff strategy are cool, but when data is sparse, which it usually is in real world data sets, they don't perform well. I know we've talked about a lot of different optimization strategies and it's a lot to take in all at once. So what better way to apply this knowledge then by implementing one by yourself? The coding challenge for this week is to implement Adam from scratch in python. Details are in the read me get hub leads, go into comments and winners will be announced in one week. If you subscribe, all of your dreams will come true. Check out this related video and for now I'm going to go, so thanks for lunch.