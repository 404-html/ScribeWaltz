Speaker 1:          00:01          Deep learning frameworks, assemble hello world, it's Raj and there are so many deep learning frameworks out there. How are you supposed to know which one to use? I'm going to compare 10 of the most popular deep learning frameworks in this video across a wide variety of metrics from ease of installation to performance to popularity on get hub. After reviewing the merits and drawbacks of each of them will be able to come to some kind of reasonable conclusion at the end. So let's start with the obvious one. Tensor flow. Out of all the deep learning frameworks, tensorflow is without a doubt the most popular in terms of developer activity on get hub who will created it to help power. Almost all of it's massively scaled services like Gmail and translate, then open sourced it for the rest of us. Nowadays, recognizable brands like Uber, Airbnb and Dropbox have all decided to leverage this framework for their own services.

Speaker 1:          01:03          Currently it's best supported client language is python, but there are also experimental interfaces available in c plus Plus Java and go and because it's so popular it has bindings for other languages like c sharp and Giulia created by the open source community. Having such a massive developer community has resulted in tensorflow having rich detailed documentation not only from its official website but from various third party sources from around the web. This documentation covers its various features like tensor board cancer board lets developers monitor the model training process, the various visualizations and it's a crucial part of it's sweet. Another crucial part is tentraflow serving which allows developers to easily serve their models at scale in a production environment and includes distributed training. Tensorflow lite even enables on device inference with low latency for mobile phones,

Speaker 1:          02:03          but despite all of this tensorflow is pretty low level. You have to specify a lot of magic numbers like the number of layers in your network, the dimensions of your input data, and this requires a lot of boiler plate coding on the developer's part, which can be tedious and difficult. By default. Tensorflow lets developers create static computation graphs at compile time. We must define it, then run it, meaning all the conditions and iterations in the graph structure have to be defined before it's run. If we want to make any changes in the neural network structure, we have to rebuild it from scratch. It was designed this way for efficiency, but a lot of the newer neural architectures dynamically change, so this default define and run mode of tensorflow is counterintuitive and can make the bugging difficult. They did add a define Byron option called eager execution later on, but it's not native.

Speaker 1:          02:59          Expect it to be even better in TF 2.0 which is about to release most of the time tensorflow is compared to the Pi Torch Library, a native define Byron framework. Pi Torch was created by Facebook to help power it services and it's now used by brands like Twitter and salesforce. Unlike tensorflow though your income high torches default defined, Byron mode is more like traditional programming while training a Pi torch model for each iteration in an epoch. A computational graph is created after each iteration. The graph is freed, meaning more available memory because it defines the graph in a forward pass versus a defined then run framework like tentraflow. Backpropagation is defined by how the code is run and every single iteration can be different. Pi Torch records the values as they happen in our code to build the dynamic graph as the code is run high torch. Also nails debugging.

Speaker 1:          03:59          We can use common debugging tools like PDB or Pi charm and the modeling process is simple and transparent. High Torch has declarative data, parallelism features a lot of pretrained models and has modular parts that are relatively easy to combine and just like tensor flow. It allows for distributed training. On the flip side, however, a Pi torch lacks model serving in the well thought out way that tensorflow does and lacks interfaces for monitoring and visualization like cancer board, but you can connect Pi Torch to tensor board via some third party libraries like cancer board x. If we look at various papers from neuro hips, the biggest AI summit of the year, it's clear that researchers tend to prefer Pi Torch to tensorflow. That's because it's best for prototyping or small scale projects. When it comes to larger across platform deployments. Tensorflow seems to be the better option, but I should also note that the popular cafe two framework introduced by Facebook in 2017 is built for mobile and large scale deployments in production environments and was recently merged into Pi torch.

Speaker 1:          05:09          This gives pie torch production grade scalability. Curiously, deep mind. Perhaps the most prominent AI research lab in the world doesn't use Pi torch. They use their own framework called Sonnet, which is built on top of tensorflow. Deep minds. Developers spent a lot of time having to acquaint themselves with the underlying tension flow graphs in order to correctly architect their applications. But with Sonnet, the creation of neural net components was made easy because at first constructs python objects, which represent some part of a neural network. Then separately connects these objects into the computation graph. These modules simplify the training process and can be combined to implement higher level networks. Developers can also easily extend Sonnet by implementing their own modules. This makes switching between models easier. Well, let's put the research versus production pipeline debate aside for a second. What if you're just a beginner and just want to learn how all this stuff works?

Speaker 1:          06:09          The minimalist python based library called Karrass can be run on top of tensorflow or Microsoft's CNTK care. Ross has support for a huge range of neural network types and makes prototyping dead simple and the code is very readable. That's the reason I use it as a teaching tool. So often in my videos it's really easy on the eyes building. A massively complicated deep learning model can be done in just a few lines of code. It has built in support for training on multiple GPS and can be turned into flow estimators and train on clusters of gps on Google cloud. But the downside of it being so high level is that it's not as customizable. It's also constrained to the libraries. It's built on like tensorflow, NCN, TK, so less functionality than a lower level library like tensorflow, but easier to learn. Carrots is the best learning tool for beginners.

Speaker 1:          07:02          All right, let's move on to mx net. Jeff basis. I mean Amazon's deep learning framework. Mx Net has been adopted by AWS. Parts of apalled are rumored to be using it and it offers APIs and a huge variety of languages natively. Even Pearl, where mx net excels is in its ability to scale linearly more so than tensorflow. The CTO of Amazon published benchmarks for mx nets training throughputs using the inception v3 image analysis algorithms and claim that the speedup obtained by running it across multiple gps was very linear across 128 GPUs. Mx Net performed a hundred times faster than a single GPU. Mx Net has a high performance imperative API, which is pretty awesome. It's got to simplicity of carrots and it's dynamic like Pi Torch, which makes debugging a lot easier. Unlike Pi Torch, however, mx net supports hybridization as part of its glue on interface. The hybrid block class seamlessly combines declarative programming like tensorflow and imperative programming like Pi Torch to offer the benefit of both users can quickly develop an debug models, would imperative programming and switch to efficient declarative execution by simply calling hybrid blocked dot hybridize.

Speaker 1:          08:15          We'll notice mx net advantage and symbolic Api APIs when training on many gps in some specific cases glue on his three x faster than Pi Torch. But take this with a grain of salt as benchmarks depending on so many factors and it's integration with AWS is unbeatable because Duh, it's Amazon's own pipeline. Let's not forget Microsoft though CNTK or Microsoft cognitive toolkit is a DL framework that supports python c plus plus c sharp and Java. It's got support for CNNS and rns and it's using Skype as Xbox and Cortana. It's targeted toward letting developers easily build models for products in speech and image problems and it offers support for Apache spark. It's the easiest of all the frameworks who integrate into a zero Microsoft's cloud offering. And one thing in particular I like about CNC K is that it handles passing sequences of varied length better than the other frameworks.

Speaker 1:          09:11          In TF, you have to do padding masking and sometimes even write your own softmax function that ignores mask elements. In Pi Torch, the scenario is less painful with functions like pack petted sequences, but you still have to pat at the beginning. Masking in general makes your model vulnerable to errors. In CNC k you just have to pass the sequence without any padding or requiring a mask later on and everything is taken care of. It handles sequences a variable length internally. Some of the criticisms of CNTK include it's strict license as they have not adopted conventional open source licenses like GPL, ASF or MIT. The community seems to consist of mostly windows developers who would like to include machine learning models in either desktop or mobile applications. Also, shout out to chainer a framework created by a Japanese startup. It's similar to Pi Torch and that it has a native imperative Api, but it's difficult to debug.

Speaker 1:          10:08          The community is relatively small, but it's supported by giants like IBM, Intel and in my fantasies make a Godzilla. It can be run on multiple gps with little effort and the main use case we've seen thus far is in speech recognition, machine translation and sentiment analysis. If you're core programming languages, Java definitely take a look at deep learning for j. It's written mainly for Java and Scala and supports a huge variety of neural networks. It was made for enterprise scale and works with Apache Hadoop and spark on distributed CPU and Gpu is also, their documentation is stellar. Java isn't very popular among machine learning project, so it's hard to integrate it with other ml libraries. Perhaps the main utility here is that android apps are usually written in Java. Thus this would be a good choice if you'd like to build a full stack Java pipeline, which includes android devices and speaking of mobile chat out to core ml.

Speaker 1:          11:06          It's not a framework that's made to build models necessarily, but it does help you bring existing models built in other frameworks to apple devices, and last but not least, let's talk about onyx. A Pokemon with a pretty high HP, but also the open neural network exchange format. It was developed in partnership between Microsoft and Facebook. They both decided there was a need for interoperability in the AI tools community. Since developers often find themselves locked into one framework or ecosystem, onyx enables more of these tools to work together by allowing them to share models. The idea is that you can train a model with one tool stack and deploy it using another for inference and prediction. To ensure this kind of interoperability, we must explore our model into the onyx format, which is a serialized representation of the model in a proto buff file. Overall, choosing the perfect framework for a DL project can be hard.

Speaker 1:          12:03          You have to take into account many factors like the type of architecture you'll be developing with which programming language you're going to use, the number of tools you need, et cetera. Here are my conclusions. If you're a beginner to programming in general, use care us as it's still the easiest library to learn from. If you'd like to build a production, great application and deploy it to Google cloud, use tensorflow. If you'd like to do research, use high torch, but also checkout sonnet if you prefer deploying to AWS, use mx net if you want to deploy to Azure Ucn, t k if you're a Java developer. The planning forJ is your best bet, I don't think has got anything unique compared to the other frameworks. And once you've already started building a model, use onyx to use tools from other framework ecosystems with it. Oh, and anything ios related you can leverage core ml for what's your favorite framework. Let me know in the comment section and please subscribe for more programming videos. For now, I've got to define and run, so thanks for watching.