Speaker 1:          00:00          I start this stream. When I start this stream, we're going to do a Kaggle challenge when I started this dream. Okay, here we go. The stream is beginning. This stream is beginning. All right guys. Okay, here I come. I'm coming. I'm coming guys. I'm coming. Okay. Hello world. It's the Raj and welcome to my live stream. In this livestream, I'm going to complete a Kaggle challenge. The Kaggle challenge that I'm going to complete is called the taxi. The taxi trip duration challenge is, this is a, this is a Kaggle competition with $30,000 worth of prize money. So it's a, it's a big deal. Uh, and so in this video I'm going to show you how to solve this challenge using a technique called xg boost, which I haven't actually talked about before, surprisingly. Well. I talked about it once in my math of intelligence series, but like that was a while ago.

Speaker 1:          00:56          So what I'm going to do four, because this video is for my live viewers and from my recorded viewers later on. So I'm trying to make sure all of the time is used very well. So, um, I'm going to use some amazing tools guys. I've, I've, I'm so excited to show you some tools. So first of all, Colab, by the way, Google's Colab, I haven't talked about it. We're going to talk about that. We're going to use some amazing libraries. Uh, it's gonna be a lot of fun. Okay, so I'm just going to say 10 names, uh, and then we're going to get started. Uh, Alex, I love you. Alina Palestinian. Shivonne punk edge, Audrey Sabeeth, three Palestinian Jonas and lacks lifters. All right, here we go. So hello from Hong Kong by the way. All right, we've got people from all over the world here. I'm so excited to do this.

Speaker 1:          01:37          Okay, so where do we begin? Let's take a look at this. So Kaggle is a website that allows you to compete with other data scientists and you can collect prize money. It's a great business model that they built over time. So companies have data sets and they want data scientist who, you know, work on these data sets and to complete challenges. So they give these datasets to Kaggle, Kaggle supplies the data scientist, you and us and all of us. And uh, they supply some money and then they earned that money through. It's like a reward for solving the, you know, it's a win win situation. You provide them a prediction, they'll provide you with some money. So it's, it's, it's a, it's a lot of, it's a lot of cool things. Okay. So Taggle is awesome. And, and what I want to do is to look at this challenge.

Speaker 1:          02:21          $30,000 in prize money. That's a lot of money. What they want to do here is, let's see what they want to do. What they want to do is predict the time for a ride in New York City. Okay. Seems, seems a simple enough. Let's look at the data. What did, what kind of data do we have? Your, okay, so we have a training, that CSV file. We have a test dot CSV file. Okay. So we've got two different CSV files and we have a sample submission. CSP, Paul, we have some data fields. So these are our features. When we download this Dataset, we're going to see all of these, a CSV file, all of these features in, in this CSV file. And the great thing about Kaggle is a, we can just download it directly just like this. Okay? So, but I'm going to do something even better than that.

Speaker 1:          03:08          So we're going to show you how to, how to, how are we going to use this without even needing to download it? Okay? So, um, what we want to do is predict this feature right here, trip duration. That is our label. That is our y value. These are going to be our inputs, right? Because we know that we want to predict the trip duration. So dependent on all of these other factors, these dependent variables or that these independent variables, we're going to predict the dependent variable, the trip duration, okay? So how are we going to do that? So normally I would start up a Jupiter notebook and I would start coding and start installing dependencies locally. But today, today I'm going to do this in the browser. So if you're watching this, what I want you to do is open up a browser window, another one, and type in colab.research.google.com this is going to blow your mind guys.

Speaker 1:          04:00          So this is an internal tool that Google has been using for a while and they open sourced it and what it allows you to do is build a Jupiter notebooks in the cloud. So this, this is a fully configured python environment. You can install any dependencies you need and you get 12 hours of GPU training time on a test on a Kad, Gpu and Nvidia, k a GPU for free, no strings attached for free. That is amazing. That is amazing. Okay, so let me show you what I mean. So I literally, you just go here. Let me, let me do it again. Colab dot research I google.com and I'm going to open a new notebook. Okay. It's loading up. It's, it's loading up that that that Kuda runtime. It's got the GPU in the background and I'm going to print out some python. Hello world.

Speaker 1:          04:48          It's Saroj. Okay, let's see what happens here. Great Pythons working. So what I'm gonna do is I'm going to solve this challenge from inside of this colab. I'm not going to, uh, do this locally. I'm going to do this, uh, in, in the, in the browser, right? All of my dependencies. I'm going to install those in the browser. I'm going to do everything in the browser for this Kaggle challenge. Okay? So let's, let's go ahead and get started. Um, and uh, well first let me talk about the tools that I'm going to be using. So I'm going to be using, first of all, the data set we have here are features, right? That's one part. The other part is the compute. The GPU. We have that already in the cloud. Uh, the other part is the python environment. We have that in the cloud. And the last part is our algorithm.

Speaker 1:          05:33          And so we're going to use pandas for data preprocessing. We know we need to preprocess the CSV file. We're going to use xg boost for learning, which is a python library in and of itself. And we're going to use map, plot live to visualize that data. Okay? So that's how this is gonna go. Um, now, so what are the steps here? We're going to split it into training and test data. We're going to do some, uh, exploratory data analysis. I know you guys want to see some Eda. So we're gonna say we're gonna see what do these features look like, how long is the trip, how much overlap between the training and testing data is there. And then we're going to use xg boost on the model and then we're going to train it. Are we to use xg boost on the data? We're going to train it, we're going to save it. Okay. So before, so I got to explain how xg boost works, but let's just start coding and then I'll explain as we go. Okay.

Speaker 2:          06:25          Okay.

Speaker 1:          06:25          I cannot marry you. I am, I am happily single. So that's, that's how that goes. Thank you though for the offer. So, um, we need to import this data set, right? So how are we going to import this Dataset? Well, ideally we could just, you know, we, we could import it directly from Kaggle. So that's one way. Just using the Cowboy Api. Another way is for us to download it's is it's for us to uh, download it, then upload it to Google drive and import it from there. So that's what I did. So I'm going to show you how to do that. So we're going to have to install one dependency and we can use pip from right within this. Um, this is going to take about 20 minutes more minutes. So hold your horses. This is important. Don't be asking how long this is going to take.

Speaker 1:          07:08          God, this is some important stuff here. Go watch a prank video if you don't want to see it as, no, I'm just kidding. Stay, I'm going to make this fun. Don't leave, do not leave. Okay. Okay. So Pi drive is our dependency for us to be able to import data directly from Google drive. Okay. So, so just remember that part. I'm going to talk about what I mean. So I have a few like a sub sub modules from Pi Drive that I'm going to import like Google off cause we're going to need to authenticate with Google. I'm going to import the drive of course, because I need to be able to access Google drive as well. I need to be able to access colab.

Speaker 2:          07:47          Okay.

Speaker 1:          07:47          Yes, Dan's are coming guys. I know you guys. I Love Gans you love, we all love gans, but there's also an audience that is, that hasn't had their appetite satisfied when it comes to a Kaggle challenges and just like machine learning, not deep learning machine learning. You know, I think of course the pointing is amazing. It's a lot of fun. Uh, but,

Speaker 1:          08:12          but uh, there is a time and place for machine learning. Sometimes you just want to get a quick and dirty solution out there and you don't want to have to wait for, uh, you know, processing times that a neural network would take and you don't have a lot of data. Uh, so I think a good, you know, exploratory step is to use some machine learning, specifically xg boost or another ensemble method. There's bagging, there's boosting. We're going to talk about all of this in a second. Okay. So that is our Google drive dependency, um, list. Okay. So now our data dependencies, data dependencies. Okay. So our data dependencies. And I'm going to answer some questions after I've imported all the dependencies that I need. So first of all, pan does. Of course, this is going to allow us to preprocess our dataset. And of course num Pi is always a go to for any kind of matrix math that we might have to do in the end.

Speaker 1:          09:00          We also want to be able to visualize this data set. So we're going to import map, plot line, uh, and I want to be able to also set this parameter format, plot live such that, um, the figure is going to be a certain size. It's going to fit in my screen. So I'm going to call, I'm going to say what is the width and height? Do I want it to be, I would say 16 by 10 for this, uh, for this screen, uh, another data preprocessing library is seaborne. That's gonna help us visualize a w. And so psychic learn is a great library. But what I partake in particular, what I really liked psych it learn for is uh, the, the um,

Speaker 1:          09:46          the training testing split a functionality my gans are showing. What are you talking about? My Gans are showing, yes, exactly. A psyche learned up model selection, import, train, test, split. I mean this, this is just like one of those super useful, um, functions right here, t train tests, cause you're always going to need to be doing that. Right. And then of course xg boost because we're going to be using xg boost. And then, uh, what else do I need? What else do I need? What else do I need? I think I need to get, um,

Speaker 2:          10:20          okay.

Speaker 1:          10:22          Make sure that Matt Paul live is going to be inline. Uh, I need to make sure that it fits inside of my screen.

Speaker 2:          10:30          Okay.

Speaker 1:          10:30          Oh yeah, I can make a much bigger fun. How about that? That's much better, isn't it? Thank you for mentioning that. Okay, so, um, dot. Unicode, uh, minus, I make sure that that's false. So that fits in to my, uh, browser. Okay. Um,

Speaker 2:          10:54          okay.

Speaker 1:          10:55          Yeah, I think I zoom it's, it's zoomed in guys. All right. Okay. Okay. Okay. Okay. Okay. Okay. Um, so now what I'm gonna do is I'm going to authenticate. So I've imported my dependencies. Let me go ahead and compile that. Matt [inaudible] live is not fun, right? It's Matt plot line. That's what it was.

Speaker 1:          11:31          Okay. So now I've done that and um, now that I've imported that, I'm going to authenticate. So now it's time to authentic case. I'm going to say authenticate me, my friend, authenticate user, and uh, create a new authentication object using Google off. Uh, this is just the authentication flow. I've got my credential. So as long as you're, the great thing about, you know, using colab is as long as you're logged into Google, which I always am, it's going to be very easy to just authenticate because it's already got your, uh, off details. And lastly, I'm gonna pull my drive from Google drive jeep off. Okay? So let me authenticate then to Kate.

Speaker 2:          12:17          Okay?

Speaker 1:          12:17          All right. Do not be sleeping right now. Okay? I don't care where you are. Do not be sleeping. All right? So I've authenticated and now I want to access my data. So where am I going to find this data? Okay, I think what I'm going to do is

Speaker 2:          12:46          yeah.

Speaker 1:          12:47          Say

Speaker 3:          12:52          MMM,

Speaker 4:          12:54          open notebook. I have this one. What does this one, Dah, Dah, Dah, Dah, Dah, Dah, Dah. No, this is nothing. Open. Another notebook. What else would I have here? What's in here?

Speaker 1:          13:10          Nothing of course. Okay. So I need to import some data that I have. So what I'm going to do is I'm going to go to, um,

Speaker 2:          13:19          okay,

Speaker 1:          13:19          my drive account, drive.google.com. Make sure it's all good in there. Their tickets. Your recent, uh, what do I got here? Okay, so test and train dot CSV. So what I'm going to do is I'm going to get the shareable link for train dot CSV.

Speaker 2:          13:36          Okay.

Speaker 1:          13:36          Okay. So here's my train dot CSV and here's the ID for it. So train the, here's the ids. These are both CSV files that I import it. And then here's test,

Speaker 2:          13:50          okay?

Speaker 1:          13:51          Uh, where's test test dot CSV right there. Get sharable link.

Speaker 4:          13:55          Okay,

Speaker 1:          13:56          so here's test dot CSV. Now I have both of these ids for both of my datasets. Now how did I do it? So import a CSV, Google colab from Google drive. So now you're watching me look at stack overflow to remember how to do this. Okay,

Speaker 4:          14:14          so Dah, Dah, Dah, Dah. How did we do this?

Speaker 1:          14:17          We have to say,

Speaker 4:          14:21          MMM,

Speaker 1:          14:27          how did we do this train and test

Speaker 4:          14:31          [inaudible] I did all that. Uh, right. Oh, Gotcha.

Speaker 1:          14:39          Right, right. Okay. Thank you. Google drive or a person. And then once I do that, then I can say, you know, import pandas and read it. Okay? So what I'll say

Speaker 2:          14:52          is a trained, downloaded, and now I have got my training data and here's the ID of it. Okay, now here is my testing data and here is the idea of it. Okay? Hopefully this works. You know, I think it's going to work. Now I'm going to, I already imported pandas. So now I can say here's my training data and now print, uh,

Speaker 1:          15:18          DF dot train

Speaker 2:          15:22          dot head.

Speaker 1:          15:24          Uh, let me just see if that works. I hope that works. I think that's gonna work.

Speaker 1:          15:37          It's importing it from Google drive. It's converting it into a panda data frame. And then, um, once it's a pandas data frame, then I can say, okay, now take the F. Dot. Train and print the head and like print a random row from it. Yes. Okay, good. Good, good, good. Now, um, the F. Dot. Test Equals Penn does dot test CSV. And that is test dot CSV and make sure I've got both. I've got to book the training data. I've got both the testing data. So let's look at this data sets. Okay. So here is our data set. We've loaded it into pandas.

Speaker 2:          16:16          MMM.

Speaker 1:          16:18          Oh, we see that it's got to pick up longitude to pick up latitude, a dropoff longitude. These are map coordinates. Okay. So how can we plot this out? Right? This is, this is map data latitudes and longitudes, or x, y coordinates on a tutee a graph. So we could even graph a lot of this out, right? So let's, let's check this out. Okay.

Speaker 2:          16:38          MMM.

Speaker 1:          16:40          All right. So I've done that. Now I want to do some data preprocessing. So let's do some data preprocessing. Now that I've, uh,

Speaker 2:          16:47          yeah,

Speaker 1:          16:48          now that I've done this. Okay. So first of all, what w what is the, let's see, what can we do here? Um, so we're trying to predict the duration of a trip, right? So what if we visualized, uh, what the average duration was? How about let's, let's try that. Let's, let's try that. Let's try that. So, oh, is it not? Test Dot CSV. Oh, thank you. Yup. There we go.

Speaker 2:          17:16          Okay.

Speaker 1:          17:17          That's my wizards to letting me know what the deal is. Okay, so let's visualize how long was the trip? Let's, that's our first part. How long is the average trip? So what can we do to predict what the average trip? So we have our training data and what we can do is we can say, let's see that trip duration, uh, feature, let's use that trip duration feature and well, use numb pies, log function to con to regularize that feature. So it's going to be easier to visualize, um, as a logarithm. Um, let's, let's add one just to make sure that it's not a overshooting the graph. Um, and now we're going to create this, uh, Matt, uh, this, this plot. So it's going to be, first of all, I want a histogram of the log trip duration. Okay. So the log trip duration and I want to see what those values are. So I'm going to say, well, what's my ex label? My Ex label is going to be, um, what's, what are, what are we measuring? So the log of the trip duration, which we've, which we've already, um, computed. Okay. That didn't, the log of the trip duration. I think that's good. I think that works. Yeah. Blogger, the trip duration. Then we're going to have the why label B, the number of training records, like the, not the number of data points. So we want to see the log of the trip duration and let's see if that shows, it's definitely gonna have an error in a second. Okay. What is the syntax here? The syntax error,

Speaker 4:          19:01          uh,

Speaker 1:          19:02          is, where is it? Line two. Oh, Gotcha. Okay. Let's see what's, what's what the deal is here. Dot. Values. Oh, this,

Speaker 4:          19:11          right, right. Okay. Let's try it again. Okay. Now my history ground I've got right. Okay.

Speaker 1:          19:23          Now train is not defined.

Speaker 4:          19:29          Okay.

Speaker 1:          19:35          Right. Oh, D F train. DF train. That's right. It was not, not train D F train.

Speaker 2:          19:44          Okay.

Speaker 1:          19:45          What do mean train is not the foe, right? The F train.

Speaker 4:          19:52          Yeah.

Speaker 1:          19:54          What do you mean it's not just like, oh, I keep using it. Right. So every time I use it, yes. Okay. Map, plot, live, check this out, check this out. So it seems like there are all kinds, there's, there is this average, right? Like there is this average trip duration that they all kind of, um, there's this, there's a distribution, there's a distribution of which it seems like the, the median is about 6.4. Um, and then it's all around there. So generally the data all seems to be, uh, in the, in the same, um,

Speaker 2:          20:32          yeah.

Speaker 1:          20:32          And the same like a vector of values, like set of values. Okay. So,

Speaker 4:          20:39          okay.

Speaker 1:          20:39          Okay. Makes Sense. Makes Sense. Um, what else can we do here? Oh, you know, we'll be cool is if we could just visualize all of as like, uh, how much overlap there is between the training and the testing data, right? So we've got training data, we've got testing data and we don't want the testing data to be too similar to the training data, right? Because then there, if our data, if our model is, is over fit, we're not going to be able to know how to, how to prevent that, right? So we want to make sure that there's not a lot of overlap. So let's sample, uh, let's say for like 10,000 data points, we're going to, um,

Speaker 1:          21:18          oh, let's say I'm going to [inaudible] so, so let me tell you what I'm about to do is plot out the training and the testing data in terms of their latitudes and longitudes. And let's just see what this looks like. Right? So remember we don't have a map of New York City. We, all we have are these data points. And so if there is enough data and you know, if there is enough data, if we don't, if we were to plot all out, if we were to plot out all of these little latitude and longitude dots, theoretically we could make a map of New York City just from those, from that data. So let's, let's try to do that. So, mmm, let's have our law are a variable for our longitude and then we're going to set a border value. Okay. Um, so I'm just going to like pick some negative 74, negative 74. I don't, you know, um, we're going to have a latitude, uh, which is going to be a little less than that. 40, 40. Yeah. Maybe it could be negative 75, negative 75. I don't know. We're, we'll, we'll, we'll fix this later. Um, and then we're going to have a plot. So we have our figure and we have our access. So using the sub plots function of map, plot line, we can, um, create this. Okay.

Speaker 1:          22:40          Of course you will. Yeah. It will look like a map. Of course. Uh, number of columns is too, uh, don't want to share this. Yes. Share the x value, share the y values and um, right. So, okay. Okay. I want to take a second to just answer any questions. Um, so ask any questions you'd like and the, then the chat. I'm going to answer those. Meanwhile while I code this. So I'm going to be like looking, I questions scatter plot. You think the true def dot train pick up a longitude, dark values up to n for as many as there are a defined and previously. And um, now the f train pickup, longitude, pickup latitude doc values. Again, so up to,

Speaker 1:          24:00          okay. So one question, one great question is why did I use the log? So logarithms are a great way for us to measure the relationship, uh, between, um, different points in a dataset. So logarithms follow a curve and sometimes, uh, data sets can be very, very, very large. So on large timescales we won't be able to see patterns that exist between them because they're so sparse, spread a po for so far spread apart. So when we use the logarithm, it kind of condenses that data and we can see it in a, we could see it in a better way. We can see patterns that we wouldn't exist if we didn't use the log. And there's a lot of reasons for that. But, um, are you gonna use tensorflow? No, I'm going to use xg boost, which has its own library. I'm not always gonna use tensorflow guys. We want to try different libraries. Right? There's other things out there. Pi Torch is another one. Uh, tensorflow is amazing though. Okay. So, um, what kind of one more question. What kind of,

Speaker 2:          25:05          okay,

Speaker 1:          25:06          what kind of RN end model will be suitable for c plus plus Code Generation? Uh, first of all, wow, that's quite an undertaking. C plus plus cogeneration. Uh,

Speaker 1:          25:21          you would need a lot of code. Get hubs. Api is great for you. Just pull a bunch of c plus plus code. Now how would we do this? You would, um, be predicting the next, uh, so character level RNN generation. Andre Carpathians has a great blog post on this. Um, you know, I think this has been tried before like recurrent nets for code generation and the results are not good. What I would do is actually try something, you know, out there like a convolutional network for text generation. Not many people have tried that. Convolutional nets have been used for text classification, but text generation, not so much now specifically I would use a type of convolutional network that no one has ever tried. So probably like,

Speaker 2:          26:07          okay,

Speaker 1:          26:07          oh, you know what, I use a DC Gan deep convolutional generative adversarial network search a DC Gan DC gangs have been mostly used for images like Pokemon and things like that. But I don't think anybody's tried to use that for text generation. So I would say try that. Okay. My Internet speed is amazing. That's okay. Blog is magic. Got It. So I'm going to,

Speaker 2:          26:31          yeah,

Speaker 1:          26:31          it's explained that better next time. Okay. All right. Where was I values

Speaker 2:          26:38          and

Speaker 1:          26:39          what color is going to be? The color is going to be blue. So let's have this be blue. The first access is going to be blue.

Speaker 4:          26:47          Uh,

Speaker 1:          26:51          one, uh, there's going to be one dimension. The label's going to be the training data and the learning rates is going to be 0.1. Now that's for that. And I'm going to do the same. Oh, we got a question here. What made you switch to a nonprofit model for school of Ai? Great question. I

Speaker 4:          27:12          okay.

Speaker 1:          27:12          Did not start this to be charging people

Speaker 4:          27:17          mmm.

Speaker 1:          27:18          For access to AI. That just goes against why I even started this and I, you know, there's this idea in silicon valley, you could think about silicon valley is a state of mind of, of, of scaling for the sake of scaling is good. Like that is a good value to have. So when I, when I was thinking about ways to scale, I thought, well, I'll just charge, you know, students and then I'll use that money to hire people and just scale. But I am just fine. I'm doing just fine. I don't need to do that. You know, I tried it. Uh, you know, it was an experiment. You know, I, I mean, I made some money doing that, but there are better ways to make money and um, yeah, so, and yeah, you, that's why, that's why, okay. That's all I'm going to say, but I'm just, I'm just getting started really, like this is not, this is just a, this is, this was nothing. Okay. Acts as one pickup latitude. Color. Oh, this, this color. I want it to be green though. And this is going to be the latitude, right? Pick up a latitude.

Speaker 2:          28:22          Yeah.

Speaker 1:          28:23          Okay. And that's the value.

Speaker 4:          28:26          And then

Speaker 1:          28:29          now let's have a legend. No, let's not have a legend. Let's just show this plot. Now let's just show this plot. What do we got here? Of course it's gonna be an error. What's our, oh my God. Wow. Oh my God.

Speaker 2:          28:44          Okay.

Speaker 1:          28:44          Oh my God, guys, this is the school of Ai. By the way. What you are experiencing is the school of Ai Right now. Um, it's a state of mind. We are the school of Ai and soon we're going to have meetups across the world. I'm going to have point people in different cities. The plan is just getting started. By the way, this is just, this is just play time. You know what I mean? Like this. This is just like 0.01% of where we're going with this. So just just think of it as an idea, a state of mind. It's a way of life really. The school have a school of AI as a way of life, of optimism, of uh, of swagger, of confidence, of, of a belief in the power of Ai to do good for the world. Checkout this. Um,

Speaker 2:          29:32          yeah,

Speaker 1:          29:32          the next course coming out in a few weeks. Guys, listen, I've got this on block. Just let me finish this Kaggle thing and don't be asking me about school of AI now, but thank you for asking about it. I love you guys. Okay, so check this out. We just made, we literally made Manhattan look at this map. So on the left is our, our training data on the right is our testing data. And if we look at it, unless like, you know, general way, they look pretty similar. They look pretty similar, but that is a map of Manhattan based only on the trip duration of all of these rides. Okay. So that's amazing by the way. So let's just keep going here. What else do I want to do in terms of, uh, data analysis? Uh, let's just say that's it. Um, what else are we going to do here? What are we going to do here? Okay, so now I'm going to build this model so I could do more, but I really just want to get to building this, this model. So now come training the model part. Okay. So remember how I said x train? Um, let's just call it x train x test. Why train? Why test. Remember I said psychic learn has this amazing function. Train, test, split. Here we go. We're using it right now. How easy was that? We're just going to say, um,

Speaker 1:          30:54          I need a list of feature name. So feature names. Dot Values. Um, why? Uh, it's, I'm gonna explain what these variables are in a second.

Speaker 2:          31:10          Okay.

Speaker 1:          31:12          Test size is 0.2. Random state is 19. I don't know. Okay. Let me talk about what I, what I'm, what I'm doing here by the way. Okay. So, um,

Speaker 1:          31:27          so we need our feature names. So our feature names, the names of all of our features, we're going to pull from our training columns. So these are, these are all of our features. Now we're also going to put a y value. The y value is going to be the trip duration. So we're going to use the log for this. Um, and so I'm going to say trip duration. And this is a value right here. And that's going to be yes, the F train. Thank you. Do you have train? We've got people on it. That's what I like to see. You guys are amazing. Uh, trip duration doc values.

Speaker 2:          32:09          Okay.

Speaker 1:          32:10          Remember how I did that up there? I added one. I'm doing that again. Okay. So this is our input data and here are our labels. Okay? So that, that's why I imported both of those. And so now I've done that.

Speaker 2:          32:25          Okay.

Speaker 1:          32:25          I've got my uh, training day, I've got my testing data and now I'm going to import xg boost. So xg boost, which I've already imported. Um, and now,

Speaker 1:          32:40          uh, what do we do? We say actually boost up parameters are going to, is we're going to, we're going to give this xg boost. It said, okay, before we use xg boost, I got to explain what this is. So let's get back to the explanation for a second. Where was I? Okay, first of all, well also be asking questions and I'm gonna be looking at this for questions by the way. Okay, let's talk ensemble for a second, right? So when it comes to machine learning, uh, ensembles, there's this idea of ensembles where you have, you have multiple models that are trained on this data set, and then you can either combine those clouds, you always combine those predictions in a certain way. The way you combine them is different. Sometimes it's called, it's, it's using bagging. Sometimes it's using boosting, and I'll talk about each of these, but right?

Speaker 1:          33:29          So it's just a collection of predictors, right? These can be trees. Um, so if it was trees, you could call it a random forest. It could be, uh, it could be a collection of linear regression models. It can be a collection of logistic regression models. Um, but we can, we can classify ensembling techniques into bagging and boosting. So what bagging is, is it's, it's, it's an ensembling technique where we built many independent predictor models, right? So we have, let's say a tree, a decision tree. We have another decision tree, we have another decision tree, we have another decision tree and they're all trained on different parts of the training data, right? So we have, you know, say data points one through 100 than data points 101 through 200 and data points 200, one to 300 for each of those trees, they'll all, they'll all be training and then using some technique, we're going to combine those predictions together. So these are independent models that we then combine together using, um, an equation to it. It depends like the equation for combining the predictions together depends on the model that we're using. Uh,

Speaker 2:          34:34          yeah.

Speaker 1:          34:36          So there's that. And then both overfitting which is better. So actually for overfitting, uh, bagging is better that this image answers your question it, but if we want to reduce the bias and the bias and variance more, we'll use gray and boosting. So let me go back here. So, um, where was I? So that's baggings independent models that are combined to make a final prediction. Boosting is another type of ensembling technique where we're not making these predictions independently but sequentially, right? So the models are actually being optimized together. So one method of doing that is called gradient boosting. Okay? So what that means is in terms of gradient boosting, so check this out, hold on.

Speaker 1:          35:24          Okay, so we have some loss function. Let's say, let's say mean squared error that we want to minimize. I'm going to, okay. By the way, if you are here, don't you dare leave during the math part. Don't you dare leave. Second of all, I'm going to wrap at the end and you can enjoy it. You can laugh, whatever, but don't you dare leave. Okay. So here we go. So, um, we need a loss function, right, to, to train any model. And so if we look at this loss function here, we'll see that this is, this is, this is actually a very simple loss function. It looks complicated, but it's not they, so what this let's just, let's just segment this. Let's segment this out. Okay. So see the sigma would that he looking thing that's called sigma, but just let's just think about that part alone. What that is, is it's the sum of our, uh, errors. Okay. So this is some of our errors. And so our air values are, you know, the difference between our prediction and our and our actual label, and we can find the difference there for all of them. So sigma means sum them all up, all of your errors up together. So that, that's, that's, that's, that's this part right here. Then once we have that,

Speaker 1:          36:29          then once we have that, uh, we will multiply it by this Alpha value, uh, and our learning rates. And so these can change, you know, these are the tuning knobs for our model, right? This can be 0.1, 0.2, 0.15, six, seven, eight, whatever it is. And there are different techniques to learn what the optimal hyper parameters for a model should be. Right? So there's grid search. There is um, you know, random search. There are different hyper parameter optimization techniques. So learning how to learn, uh, Bayesean optimization currently is kind of at the forefront of hyper parameter optimization theory. A great to research avenue to go down if you want to. Okay. So where was I? So this is us trying to find the mean squared error. And what we do is we use, so we're, we use that error to compute a gradient, right? So the gradient tells us how off our model is.

Speaker 1:          37:20          So, so in terms of the weights, the coefficients for our model, whatever the model is, need to be updated over time. So we'll use the error to compute the gradient and the gradient tells us how to update our weight values. And so when it comes to gradient boosting, we're not just using the gradient to update a single tree cause we have multiple trees, decision trees. This shit isn't trees look like this. Check that out. We're using the grading to update all of them. Right. So, so we train one, we use the grading to update that and the next one and the next one and the next one. So that's trained, that is used to update the next one. So it's, it's, it's like they're all linked together by this grading that's updating everything. And so that's gradient boosting. And so xg boost is a version of gray and boosting where it is designed for a set of decision tree specifically. And xg boost is also like the top a Kaggle model a lot of the time because it's easy. It's quick. Uh, and um, although I am convinced at whoever won those competitions using xg boost, if they just use neural networks, they would have a, they would have done better. Um, but they didn't have the resources or whatever. But yeah, I still believe in deep learning over anything else. But I think this is, um,

Speaker 1:          38:36          I think this is a good, uh, my hair is different. It's, it's silver. It's silver sea. This is the original hair that I'm going after. I just didn't like, you know, die for a while. So it got kinda blonde. But silver is like my original hair, you know what I'm saying? So, uh, where was I? So that's xg boost. It's a bunch of decision trees that are all optimized together using gradient descent. The gradient is updating everything, all of the trees sequentially.

Speaker 2:          39:03          Okay.

Speaker 1:          39:04          And not independently. Okay. So now,

Speaker 2:          39:08          uh,

Speaker 1:          39:09          okay, if you have to go to sleep then definitely go to sleep. Thank you for tuning in. Okay, so now that was that. What can we do now? Now I'm going to say, you know what, I'm going to use the default parameters for xg boost and uh, let's just say default prams, cause there's a lot of different parameters we could talk about sub sampling lambda and thread a whole bunch of different parameters. I'm going to be using the default one DF to train a team.

Speaker 2:          39:45          Okay. Uh,

Speaker 1:          39:53          okay, what I'm going to do,

Speaker 2:          39:58          okay.

Speaker 1:          40:00          So I'm going to run this and then while this is training, so training is unfortunately going to take a while and I can't just train during a live stream. Uh, so what I'm gonna do is I'm going to show you a model that has been trained already. Okay? Okay. So over time, so this is a time, uh, this, this data set changes over time. So what we can do is we can say on the, on the rights my rights, you're seeing the actual pickup density at these times and on the left you're seeing that predicted pickup density. Okay? So this is, this is using a trained model. It's predicting where rides are going in real time, over time. And this is what happens when you have a trained model, right? So once you train the model, you can visualize it using Matt put live, et Cetera, et Cetera, et cetera. Okay? So that's it for this livestream. I just wanted to go over that. Um, the code is going to be in the get hub description. Uh, so definitely check that out. I have more coming out on that. And lastly, I'm going to rap. So let me just wrap instrumental.

Speaker 1:          41:01          Is this going to play here? No, it's not. Okay. Let me find my phone. Okay, hold on. All Right, rob. Instrumental. Okay. It's time to wrap and then also answer some questions. Grid search is time consuming? Yes it is. That's why I prefer Basie and optimization. I don't want to, okay, how can I make my own environment like chaos and tensorflow? Listen, you don't want to do it yourself. You want to use Anaconda. You want to use um, virtual end. You don't want to have to do things that want to use docker. You don't have to mess with dependencies, right? We have tools now to automatically install the dependencies you need. Okay, so now I need somebody to say a topic cause I'm going to freestyle rap. Okay. All right. Some say someone say someone say a hot topic. Here we go. Here we go. Someone said are the language.

Speaker 1:          42:07          Hey, okay, here we go. Here we go. I don't like to use our eye drop bars. I do it. Then back and back and back and back. Our Bra. I'm like Elian Mar. See a logo is a place I want to go and drive a yacht in the sea, in the bleed blue man. It's all good, man. I'm like new every day. I released subsidy. Yo, you try to stop me. It doesn't come back. I'm like, are thin yo hall. He was the guy who was rapping in the Apollo theater man at night. Okay. XG boost. Is it? That's it. That's it. That's it. That's it. That's it. We got a 32nd wrap in guys. We did this Kaggle competition. We, we visualize our data set. We talked a little bit about xg boost. We did all of that in 40 minutes. You guys have learned so much and I am so happy for you. Thank you for s, for staying, uh, during this live stream. I'm so excited to have you guys here. I'm going to try and do this every single week for you guys on Friday. Please subscribe if you haven't yet. I'll tell your friends and subscribe. That's what it's all about. I'm trying to grow this community, the school of AI together. We can do that. So thank you guys for coming.

Speaker 2:          43:14          Okay.

Speaker 1:          43:14          For now, I've got to take a get a plane, so thanks for watching.