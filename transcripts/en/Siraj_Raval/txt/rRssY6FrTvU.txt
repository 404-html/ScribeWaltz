Speaker 1:          00:00          Hello world, it's Saroj and automated trading. Bots are a popular way to earn a passive income and with so many tools available on the internet, anyone can build their own. In a few simple steps. In this video I'll explain how to use a reinforcement learning strategy called cue learning to simultaneously predict prices for three different stocks in a portfolio using several data points. If we look at finance industry reports, we'll find that stock on and future trading are all heavily automated and the algorithms behind both trade execution and optimization have yielded billions of dollars for a few companies that invested early on in machine learning in the 90s and early two thousands that means that they've been working on this for up to two decades now, which is way before the deep learning hype started in 2012 in fact, AI in finance is perhaps the most mature application of AI in any industry setting and while lots of these companies are very well funded so they can afford the hardware necessary to run machine learning algorithms, the cost of hardware is following and the accessibility of these algorithms is increasing as open source culture.

Speaker 1:          01:23          It goes mainstream. We can see this shift of activity move into the realm of startups by browsing the landing pages of companies like new yoedicke.ai that offered traders and AI system for back testing and daily forecasting with automated trading. Another example is Alpaca dot. Ai got to love that name, which creates a deep learning engine that learns what your winning trades are and then generates a unique trading algorithm for you. Based on that data alone, you can even tune your algorithm with sliders without using code. There are so many data points we could use that effect prices at different scales. News and rumors affect prices, micro and macro economic cycles, new laws, but let's start simple. Our starting data set will be three CSV files. Each contains stock prices for the past 17 years for IBM, Microsoft and Qualcomm prospectively. The data set was created using the Alpha vantage API that lets us access both real time and historical stock data.

Speaker 1:          02:33          We'll want to build an algorithm that can learn from this dataset. Let's think about how to frame this problem. We have both historical data and access to real time data via the API that can constantly pipe in updated prices, which we can add to our CSV files. This is a real time environment where time is a dimension. We know then to use reinforcement as it takes time into consideration using reinforcement learning. We can frame our problem as a mark Haub decision process, which consists of states' actions and rewards are agent will be able to execute an action in this environment. And luckily for us, the action space in this case is pretty simple. For any given stock, it can only perform three actions. Buy, sell or hold buy. We'll buy as much stock as possible based on current stock prices and the amount of cash we have whilst sell.

Speaker 1:          03:33          We'll sell all shares of a stock and add the generated cash to our cash balance. If we're buying multiple stocks, we would then equally distribute the cash we have for each. Hold on the other hand does nothing. So the number of actions at any given time step is three to the n where n is the number of stocks in our portfolio. Now at every time step, our agent will be in a state and from that state will make an action for our state. In this case, let's consider it a combination of the current stock price, our account balance, and the number of stocks we own. So, for example, if we own 50 shares of IBM stock, a hundred shares of Microsoft and 20 shares of Qualcomm and have $1,500 in our balance, we can represent our state as an array which contains the amounts and prices of each stock as well as the total account balances we have.

Speaker 1:          04:33          So it's this state category of the MDP that will contain any and all data that our algorithm could learn from. And we can add more to it later. But let's start simply, if we simply plot out the amount of capital we have four, three times steps, we'll see that the first two ideas for our reward won't help the agent learn from the difference in value between each time step. So we'll go with the third and this exchange environment we've defined, there are actually multiple agents, meaning there are other traders with their own respective account balances and open limit orders that are affecting prices. Unfortunately, we don't have access to their data. So we're dealing with a partially observable Markov decision process. And because it's partially observable and we don't know what a full state looks like, we also don't know what either the reward function or transition probability looks like.

Speaker 1:          05:31          Dynamic programming is a technique that we use. If we knew these two terms beforehand to compute the optimal policy, but since we don't know them beforehand, we could instead use another model based reinforcement learning technique to learn these two functions. Once we do that, we'd be able to compute the optimal policy because we'd eventually learn what the effect is going to be of taking a particular action in a particular state. But what if we didn't need to explicitly learn these two functions? What if we could just learn the mapping from states to actions directly? We could compute a policy without needing to construct a full model of our environment model free reinforcement learning. That'd be much more computationally efficient. So let's start with that. Of all the model free techniques, cue learning is the most popular in cue learning. We define a function to noted as Q of Sna.

Speaker 1:          06:36          This represents the maximum discounted future reward when we perform an action in state s and continue optimally from that point on. It's also called the action value functions. Since it measures the value of an action, we can think of this function as the highest possible account balance we can have at the end of a training episode. After performing action a in state s it represents the quality of a certain action in a given state. In our case, our possible actions are either buy, sell, or hold. Once we'd have this queue function, it will rate all three. Then we can just pick the action that has the highest Q value. So how do we compute this Q value? We can express the Q value of state s and action a in terms of the Q value of the next state. This is called the bellman equation.

Speaker 1:          07:30          It says that the maximum future reward for this state and action is the immediate reward plus the maximum future reward for the next state. The great thing about the bellman equation is that it lets us represent the value of a given state in terms of the value of the next date. It mathematically defines the relationship between states, which makes it possible for us to approximate the Q function. In the most simple case, the Q function is implemented as a table with states as rows and actions as columns. We'll initialize Q randomly then observed the initial state s which is going to be an array. Then we iteratively compute the following four steps. We select an execute an action, then observed the reward and new state. Using these three values. We use the bellman equation to update our Q function, then set the current state to the next state.

Speaker 1:          08:30          We just keep repeating that process for as long as we want to set an episode for based on just the data we have given our model, we can view the performance in this plot. The red line is our initial investment and the Green Line is the average after 2000 runs. It looks like we made about 4,000 simulated dollars in a thousand days. Notice though that the portfolio values are very volatile, which indicates the instability of our agent. So while that was a decent profit, the variants is too high for us to ignore that to longterm more data would definitely help. We could add in binary company news for each stock, meaning positive or negative tweets. We could also add in some company performance metadata like market share, growth rate, market capitalization, profit margins, et cetera. So our state array would be much bigger, encompassing many more values. That would mean that our state action space would be much larger, so approximating our Q function would take exponentially longer.

Speaker 1:          09:42          However, there exists an algorithm that can approximate any function. Can you guess what it is? Yes. Neural networks. If we use a neural network to approximate the Q function, we could then consider our method deep cue learning, and of course there are more advanced are El techniques out there by policy gradients, actor, critic, and inverse reinforcement learning. All of which I'll talk about in future videos. Three things to remember from this video though, a partially observable Mark Cobb decision process is one where we don't know what the true state looks like, but we can observe a part of it. A cute table is one where the states are rows and actions are columns. It helps us find the best action to take for each state. And Cue learning is the process of learning what this cute table is directly without needing to learn either the transition, probability or reward function. Thanks for watching my video. Please subscribe for more programming videos and for now I've got to find the cute table, so thanks for watching.