Speaker 1:          00:00          Okay.

Speaker 2:          00:01          Hello? Can anyone hear me? Who can hear me? I am alive. I know I'm late, but can you hear me is the question I am talking, I am talking. I'm not sure if anybody, oh my God. Holy Shit. That was loud.

Speaker 1:          00:26          Yup.

Speaker 2:          00:28          Yeah, you guys hear me? Okay, here we go. Hi everybody. Hello world. It's to Raj and we're today we're going to build a neural network. Okay. It's going to be a special type of neural network. Okay, awesome. It's a special type of neural network. Hi. That is called an LSTM neural network. It's like probably the most advanced neural network I could think of. But to do it in an easy way. So this is going to be a challenge, but it's gonna be Super Fun. We're going to generate, um, city names using this in python. We're going to generate city names, just random city names. So we're going to train it on a list of every us city in every single city in the u s and then it's gonna generate new city names from scratch. Yes, to long, short term memory much. Mathias, a neural network is, uh, an algorithm that replicates neurons in the brain and you can train it to learn things. It's a learning algorithm. Okay. So we're just going to get right into, well, I want to do a five minutes Q and a and then we can just get started. Okay. Hitesh the different types of neural networks. Okay. So, so there's the regular type of neural network, which is just feed forward and a feed forward. Neural network data just flows one way from the beginning, from the input layer all the way to the output layer. That's a feed forward and all network. And those things are good for,

Speaker 1:          01:56          uh,

Speaker 2:          01:59          predicting the next number in a sequence of numbers. I'm going to build this using 10 TF learn. Yes. And tensorflow, it's a, it's a library on top of tensorflow. You Guys gonna, you're gonna, you're gonna love this. A recurrent neural network is a type of neural network where the data flows not just through it, but then back again. So it, so it's recurrent, it goes back and forth through that. And that's good for memory. So anytime you need to remember maybe a sequence of music or a sequence of a text or poetry if you want to generate some things, some new type of texts. That's what recurrent networks are good for. Um, I can build a neural network from scratch. It's possible using num Pi, the scientific computing library. Um, and then one more type of neural network. And the one we're going to do in this video is called an LSTM neural network, which is a type of recurrent network.

Speaker 2:          02:48          It's called long short term memory. So it can remember things from way back in the sequence of data. So it's like even remembers everything, which is what we're going to need for the city because there's so many different cities for this, uh, city generations. Let me answer a few more questions. I've got about three more minutes. I want to get right into it this time. How did you learn to build neural networks? By just taking time and going through the machine learning stuff? We're going to get up and just downloading a bunch of neural networks from get hub repositories, just playing around with them, seeing how they work for comparing them, comparing different library increment implementations of it, and then building my own from scratch using them plot. Where can you get tensorflow? Google tensor. Just Google tensorflow is firstly a non is a pain. Um, build it from scratch. I'm gonna, I'm gonna. Um, build an LSTM from close to scratch is c sharp, good for neural networks. Um, there's the Microsoft CNTK library at which I don't really like that much cause I don't like Microsoft. I mean, no, you know, you know. Anyway, I'm not gonna apologize. All right. All right, we're going to get to the tutorial. Um,

Speaker 2:          03:59          yes. LSTS are definitely more powerful than hidden Markov models. I mean, hidden Markov models. We're kind of the precursor to deep learning and this whole craze going on with deep learning right now, deep learning kind of makes hidden Markov models not as not as relevant anymore. Um, how to design and build a Dataset. You just have to, um, how do you design and build a data set? You can just create an excel spreadsheet and then for the x column, you know, like put like number of times that I've been laid and then yes, no, yes, no. For everyday or something. There you go. There's your data's that CSV and python can read CSE files very easily. Right now I'm in San Francisco. Um, the goal of this neural network is to generate a city names from scratch, like new city. Next, we're going to train it on city names. It's going to generate new city names. Okay. So I'm just going to get right into it. Here we go. Let's see, screen chair. I would recommend my videos to get started. Start from machine learning for hackers. Number one, screen share. So I'm going to go to the desktop. Okay. And Holy Shit, that's a lot of shit. All right, so I'm gonna move this and it's going to show me, and we're gonna make this bigger. So I'm going to be in the corner and you guys, your text, I'm going to put right up here so I can look at it, your chats while I code this thing. Okay. Um,

Speaker 2:          05:30          uh, okay. Apart from tensorflow and python, yes, there are dependencies I'm going to be showing you guys wished dependencies we're going to use. Uh, let me make sure that everything is working well here. Okay. Let me make sure everything's working well. Make sure my text editors big, nice and big that are right. That's what we want. Oh Shit. Where did that go? All right. There we go. All right, let me make sure everything's nice and big. All of Knights and big texts. Is that, um, is that good in big for you guys? Is that readable right here? This, this level? Um,

Speaker 3:          06:09          okay.

Speaker 2:          06:09          This is good. Right? All right. We're just going to get started and if anybody, suddenly I say we're going to, this is going to be, uh, probably 30 minutes. I'll make it bigger. Okay. Blah, blah, blah, blah. Okay. How big? That's big, right? All right. Here we go. A bit bigger. Okay. That's just big as we're going to go guys. Okay, let's get started. Oh Shit. Yeah. Okay. Here we go. Uh, uh, uh, okay. So first things first, we're going to import our dependencies. Okay. Let me bring up my notes for a second. So I make sure that this is the right shit. Okay. So Da da Da. And we're going to build this in python and it's going to be, you know, so many fucking windows up there. Uh Oh, sex for Ubuntu for deep learning. I would say goodbye. I know I'm on Osx, but like, okay, here we go.

Speaker 2:          07:05          So we're going to start off by importing the first module. This is going to be called a future. So the future module is toward, uh, it's the link between python two and three. It's a way for us to use python two or three without there being any kind of version errors between what we're overusing. So I'm gonna say from future import, uh, absolute important. I'm going to have a recording of this session for sure. Division and the print function. Those are, that's, and let me turn on python cause right now it's in plain text. So I saw on boom, there we go.

Speaker 2:          07:43          There you go. So hold on. That shit. It's big. So those are our, those are our, uh, dependencies. Okay. So we have one, we have another one. Okay. So the next one is uh oh s so we're going to important oh west. That allows us to have operating system didn't any functionality. We're going to use it to read in file paths, which is where our data set's going to be. We're going to read in our dataset using this, which we're going to download it in the script. Okay. So that's, oh asked. Um, what else we need to import six and we're going to import something from six called a moves. And so let me just write this at the top. Like fucking Ellis. Sorry for the language. I don't know. I don't care. No, no, no apologies ever. Oh cm. To generate city names. Okay.

Speaker 2:          08:26          That's what this is. This whole thing. Okay. So from six important moves. Okay. And so what that's going to do a six to another library for like commonalities between different python versions, uh, and moves is, uh, we're going to use that for the URL live module, which is how we're going to pull data from the Internet. Okay. In a second. So what else? We're going to import, um, SSL, which is how we're going to long short term memory network, a long, short, long, short term memory to generate city January the city, let's just say generate. So if he's okay, so importance of cell that's going to let us connect to the Internet. Um, and then of course we're not in port CF learn. Um, we have learned is going through VR machine learning library after we're going to do use TF learn, uh, we're going to work with some operations for our data. So we're going to say from TF learned 30 minutes. Exactly. Thank you. Um, from field Florida data utiles work in full screen, um, working full screen. Uh, no. Okay, here we go. Stuck. So we're going to import everything from the data utilities. Okay. Um, so okay. That's, those are all of our dependencies. But TF learn, uh, that's, those are all of our dependencies. That's it. Now we're going to start coding. Okay? So the first thing we want to do is get our data assaults. Step one, retrieve the data, okay? That's, this is the data.

Speaker 2:          10:06          Okay? They're going to, we're going to get our data. Okay. So, so first things first, we're going to create the path for our dataset and we're going to call it US cities. Dot Text. Okay. That center, we're going to call our dataset. So if we don't have that file already, okay? If we don't have it, we want to tell our machine that. So that's where the ois module was going to come in. We're going to say, if not [inaudible] Stop Path, uh, his file and then the path as the argument. But TF learning is a, is a wrapper on top of tensorflow that makes it a little easier to use. So it's great for learning.

Speaker 2:          10:43          Okay. So it's not really an add on. It's just like if I were to write an entire neural network, maybe like a hundred lines of neural network in tensorflow and TF learn, I could access that neural network and maybe two lines of code. And it just says like access neural networks. So it's like an, it's a layer of abstraction on top of that complexity of tensorflow. So the trade off is you don't get that, you know, fine grain control over everything. But the good thing is it's easier to understand. It's more readable. Kara us is similar to a TF learning and that is also an, it's a layer of abstraction over tensorflow, Angiano and uh, and a few other libraries. Okay. So retrieving the data. So if, uh, so if we don't have that in her file path, we want to create it, right? So we're going to say, con, we're going to create a context capsular context that says,

Speaker 1:          11:37          MMM,

Speaker 2:          11:39          uh, create on verified. Bridget. I'm too, I'm too soon into this. Can make it from scratch right now, but next livestream, I will. Okay. But just follow along. We're gonna, you're gonna, you're gonna like this. Create an unverified context. What does that do? That just initialize a bunch of key and trucks managers. Cause we're going to get data from a server in a second. So that's just some initialization stuff. Okay, let me, um, let me make this a little smaller. Okay. That's all. That's a little small. Okay. So now we're going to say,

Speaker 1:          12:11          uh,

Speaker 2:          12:13          now we're going to retrieve the actual, we're going to recruit the actual data. So we're going to say we're gonna use a URL live module of moves to retrieve our data. And uh, let's see, URL light module. We're going to retrieve our data and we're going to say request URL, retrieve, make computers from scratch. Nice. Okay. One day I actually have before it's, it's not a big deal. Anybody can do it. Okay. So I will upload it. I'm recording it. So six packages, good for um, being a kind of

Speaker 2:          12:55          gateway between python two and three. So you can write python to code and it works on Python three and vice versa. Okay. And so here is what we're going to retreat. So this is the link to the data set that we're going to retreat. I have this paper, I, let me, I have this written down somewhere, so I'm going to paste this in. It's is the data, is that okay? It's a huge link. It's a huge link. Okay. Boom. This whole thing. That's our u s cities data set. Okay. So I'm just going to say get dataset. Oh, just like that. Make it a little smaller. Okay, so that's our dataset. All right, we've done that and we've retrieved it. And uh, I want to save it to my path and with my context equal to the context I just created. Okay. So boom. That's it. That's my dataset. So now, okay, it's time to do some machine learning on this stuff. Okay. So,

Speaker 1:          13:53          mmm,

Speaker 2:          13:55          it's going to be on get hub. It's going to be on get help. Don't worry man. It is such a trip to read comments and code and do it all live at the same time. This shit is crazy. I love it though. I love it. I love you guys. You guys are awesome. Okay, so max length equals path hypo. Who spotted that half pipe or Javier Path? Typo. Oh, thank you. You. Thank you that ass. Okay. So Max length. So this is the max link. We want cities to be at city named Max length. We don't want them to the cities that we generate to be, um, line break is a good idea. Thank you. Thank you for that. Uh, so we don't want our city names to be longer than 20 characters. So we're going to set this max length.

Speaker 2:          14:38          All right? I checked the type of, okay, so Mattson, Max length. Uh, and so now we're going to vectorize our textbook. Okay. So we're going to take vectorize that text file. What does that mean? Well, we have a bunch of import typos. Hi. Thanks for coming. Okay, so that's rising a text file. We have a Dataset we want to vectorize it would be when we have a data set of words, we want to vectorize it. So what does that, what does that mean? I'm going to get that type of, wait a second. What does that mean? It means take all these words, all these thousands of words, and find those commonalities between those words and make those abstractions of those commonalities. So if I have a set of words like, uh, a raccoon swirl, pidgin and dog, the vector that would be created from those eventually would be animal. It would, it wouldn't be the specific word animal, but it would be a representation. You like a numerical representation that that is all of those things.

Speaker 2:          15:38          Okay? So it, so that's what vectors are. They help us extract huge series, his series of data into something that we can manipulate later and we can use these abstractions to generate new data. So we could use that animal abstraction to that to then generate a new animal like a wolf. Okay. That's the great thing about abstractions. Thank you, Emerson. Okay, so let's, let's make our vectors. So our vector is going to spit back, um, are, uh, three things. We're going to call it x, Y, and a chart dictionary. So it's going to, let me just write, let me just write this out. Okay. So it's going to be called, um, text file to semi redundant sequences. Okay. That's the math. That's the function that TF learn provides. Uh, and it's, it's going to create vectors for us. And it's going to take arguments, the arguments is going to take is the path. So where we are, I know that where we are, uh, uh, where's the data set that we're making vectors from? So that's the path. And then the sequence, the Max length, which we defined earlier, uh, the Mac, I won't say I will post the repo at the end of the video. So where we are posting, uh, what is the Max length? We already defined it, right? And then finally, redundant step,

Speaker 2:          17:02          uh, equals three. Okay? So that means, uh, how many times do we want to do this? So we're just going to say three times. Okay. And I'll make a line break here. Okay. So that's to vectorize and so that, so what, what x, y and uh, it's going to be probably a 20, 20 more minutes. Uh, we'll, we'll see 20 more minutes. Okay. So, uh, what is this doing? This is, uh, it is creating a directors from our words and it returns the inputs, the targets and the dictionary, the inputs, that's the words, the targets, those are vectors and the dictionary of those things. Okay.

Speaker 2:          17:44          I am not on Adderall, but I will admit I am on toffee. Yo, you need coffee. Do this, this, this stuff that I do full time. You know what I mean? I will admit it. I loved, I love coffee. Couldn't do without coffee. Okay. Anyway, but I have never tried out a row. I've never tried Adderall. I have friends who have, I've never tried it. Okay. All right. Thanks Andy. Touch, uh, just 20 minutes. Oh, you are more, I mean I could go more. Okay. The neural network is going to generate city names. We have a data set of cities and it's going to generate new city names. Okay. This shit. Okay. Curator, man. Shrooms. I've done shrooms before. In fact, uh, the reason I started the youtube channel is because I had a very, uh, good true trip. Oh my God. He just admitted that he just admitted that because he doesn't give a fuck what anyone thinks.

Speaker 2:          18:38          Okay. Here we go. Here we go. Create an Lstm. Oh yes. You test a redundant steps equals three means where we're just going to, uh, go through three sequences to, to make that happen. Okay. Okay, here we go. Here we go. Here we go. So now we're going to create our LCM create LSTM uh, really Jorge. Holy Shit. I love it. I love it. A Java script. Maybe someday. Yeah. I mean eventually, hopefully Jorge, that you quit your job to do something that you liked cause it's your, it's not like you're, you just became crazy anyway. Okay. Uh, create LSTM let's do this. So we're going to create an Ltm and we're going to call it G. Okay? We're going to call it g, uh, on. You Dad said, yeah, I am working with you Udacity on the self driving car course. All my God, I just admitted that, but it's going to be awesome.

Speaker 2:          19:29          Yeah. And you know that self driving car nanodegree I'm working on that. I can't wait. I'm going to show you guys next week. Okay, next weekend I'm going down to mountain view to meet with the team and stuff. That's gonna be crazy. They just saw my self driving car in five minute video and then they were like, oh, you should work with us. And I'm like, oh my God. So that's going through and it's you, the guy who like, you know, made Google x and stuff. Anyway, that's going to be exciting. Okay, here we go. So TF, learn dot throw. We're going to create our LSTM and your career all, yeah, we're going to create layers. Okay. We're going to just layer by layer. That's what TF learned is. Wait for it. You can define entire layers in one line of code. So it's like layer, layer, layer, layer, layer, layer.

Speaker 2:          20:02          Okay. Um, I don't care what the government thinks. Come at me. If the government is watching and they think they're going to do something, cause I did some drugs. Come at me. Okay. Come at me. You can't, you can't stop me. Okay, here we go. So TF, learn dot input data. Okay, we're going to say the shape I, so this is our equal layer. We're getting, this is our inkjet layer. Okay? This is our input. Like this is where we're going to put the data in. So we're going to say the shape equals none. Max Link. I'm going to just come in and I'm going to, I'm going to talk about what I'm, what I'm doing in a second. So Char, Idx. Okay. Tara IDX. Okay, so what is this doing? It's the saying, make our input layer and is the size of our input layer. It's going to be up to 20 characters. Um, and I wrote input wrong, didn't I? Where input data it. Okay, so that's our inco layer and now we're going to create our next layer and our next layer is an LSTM lik, okay, so GF learn dot LSTM she five 12. That's going to be the size of our matrix.

Speaker 2:          21:13          Thank you. Ace. Ace. I will, I will get better at.

Speaker 1:          21:17          MMM.

Speaker 2:          21:18          This stuff to make it more understandable. Okay, so return sequence equals true. Okay, so what is this? This is the LSTM layer. How was TM layer is basically you've got, you've got 512 nodes in this layer. That's that 512 number a layer. Okay. You have 512 neurons or nodes in this. Like it's a law that's, that's deep learning. That's the deep part. Okay. You got 512 of them.

Speaker 1:          21:47          MMM.

Speaker 2:          21:48          Thank you Omar. I got this. Okay. 512 lakes. And uh, the LSTM is basically long short term memory inside of each of those neurons. You've got a little gateways, basically an LSTM was created recently. And basically anything that we put it out, we throw any kind of data that we throw an LSTM app, usually LSTM is or what works best to learn. We're not sure why they're the most, they're the most complicated, complicated types of neural networks out there, but they're also, uh, they also provides the best, most performance results. Okay. So we've got four more layers to create. Okay. Wifi 12th. I mean, it's just, it's one of those things where it's just kind of become standard. Like, you know, one guy did it and it worked and he got great results. So let's just all do five 12th. I mean, I can do five 13 to your right.

Speaker 1:          22:39          MMM.

Speaker 2:          22:40          Peter, you're right. So it's Peter. Peter is right. So it's not, it's not just what's in the layer. It's, it's not just how many nodes are in the, it's how many layers. But I guess in terms of wide and deep learning, it is both. Lstm means long short term memory network. Lsms are great for chatbox. Why are both variable names g? Because we're going to keep, we're going to keep using G. We're going to keep using g as we build our lips. Okay. Okay. Uh, we're going to keep using g cause we're building our LSTM. Okay. So now we've, we built that, uh, and now we're going to create dropdown. Okay. So TF, learn dot dropout. Okay. And we're going to say, gee, again, we're going to keep using g. Okay. So what does that mean? What is dropped out? Okay, so how does it relate to recurrent neural networks? And LSTM is a type of recurrent neural networks. So it is a subset of a recurrent neural network. I'm not a part of the Alphago team yet,

Speaker 2:          23:36          but, okay. So, um, it's a type of regard. So what is dropout? When we are training our neural network, when we are training our neural network, okay. Data is flowing. The neural network, right? Data is data is flowing through our neural networks. And when we train it, sometimes we have something called overfitting, whether it w w we have something called overfitting and over fitting is when I'm wearing it only on a CPU, not a GPU overfitting. It's when the data, when the Lord model is fit too well to only the Dataset that we trained and we can't generalize it to new things. So to prevent over fitting, there's a technique called dropouts dropouts and would drop out does is it randomly turns off some notes, it randomly turns off notes, wild training. So it doesn't go through the same grooves, you know, like these grooves that we create.

Speaker 2:          24:27          And then every time, you know, neuro pathways are kind of like, when you think a certain way and you get older, you have these grooves, right? And so it's hard to break out of those groups. That's kind of a rough analogy, but like drop out basically turns off notes. So it has to find new groups that time. And what that does is it makes it easier to generalize. Okay. So that's, that's as good as my explanation is going to get pregnant. So that's dropped out. We're gonna create another LSTM layer. Okay. I started frying my CPQ. I was going to pry my CPU. Exactly. Uh, okay. So, um, KF learn dot dropped out. So that was our dropout layer. We're going to create one more LSTM layer. I'm going to send you five 12 again, and then we're going to add more dropped out because we want to drop out at every layer. Okay. So another dropout g five 12.

Speaker 1:          25:21          Yeah.

Speaker 2:          25:21          Okay. Uh, and then, oh no, sorry. Drop out. 0.5 by the way, 0.5. Will we create dropout? It's just a coefficient. It's just a measure of how random do we want to get. We could say ploy for, because they 0.6. Let Art, I'm going to get to that in a second. Okay. I'll be shaking. I'll spend drop out one more time. So drop out. It's a way to randomly turn off nodes in your neural network as it trains. And what this does is it prevents overfit. It prevents over fit. And overfitting means you're, you're trained model won't work for new datasets. It's only gonna work for the data set that you trained to make predictions, but you want to make predictions of data that it hasn't seen before. And that's why we use dropout. And, and uh, Jeff Hinton, the guy who was like the godfather of neural nets made it and everyone's been batshit crazy about it since. Okay. Why did I write the last two lines twice? Because we have multiple layers to this. Lstm it's not just one light. Okay. Uh, uh, m zero four, eight one. Uh, that's, that's good intuition. But I, it's, I don't think it's that, I think it's like a,

Speaker 1:          26:36          it's a,

Speaker 2:          26:38          it's a measure of, of, it's, it's, it's, it's just a, it's just a coefficient and I think like later on it's like, it's changed in a different way. Anyway, I got to keep going here. Okay. So, uh, so that's our drop out layer and then what else is left? What else was like, we have our last layer, we're going to create our last layer are fully connected layer. Our last layer. Okay. So g equals TF, TF, learn dot [inaudible]

Speaker 1:          27:03          mmm.

Speaker 2:          27:05          Do a fully connected, okay. Gee length, the length of the fully connected layer. It's going to be the size of our dictionary. And then we're going to create our activation function, which is called soft mass. Softmax is a type of, it's a type of logistic regression. It's a type of logistic regression that is good for classification, which is what we're doing. We're classifying big sequences of text and then we're going to generate new sequences. Okay. I'm going to draw the shit. You know what? I'm going to show you guys a visualization that second. Let, let me just,

Speaker 1:          27:40          uh,

Speaker 2:          27:42          uh, did you, uh, I'm going to push it all to get hub and you're going to be able to watch this later. So just follow along or now be interactive. I'm going to watch what you're saying and so, okay, so here we go. So that's it. We built our neural network. That's all. Okay, we're done now we're done with that. Now we're going to generate our actual sequences. Okay. So generates cities. Okay. We're at that part, right? That generate cities. Generates cities. Okay. So here we go. We're going to generate six. We're going to call this generated file. M um, right. We're good. Other January fall Amagansett Tsr in dot sequence generate CFR has a sequence generator for us. It's made to generate the sequence, the sports [inaudible]. Okay. Well, okay, good to know. Um, you tried is not frozen.

Speaker 1:          28:31          Uh,

Speaker 2:          28:34          is it a regression problem or in classification problem? This is a, this is a regression problem that uses classification early on. The end result isn't classification is using classification early on to jet to classify two is classifying words. His doctor's first. Okay, so secret generated. Why is g um, wait, what is important? Uh, I'm going to apply at normal hours to tech summarization. Uh, why is g interpreted as five separate layers when it's one variable being overwritten? Uh, uh, because it's, uh,

Speaker 2:          29:15          no, well, it, we're adding layers to what already exists. Is there a list of commands NTF learn? Yes, it's on the get hub. Okay. So, okay. So g dictionary equal to tar IDX and then sequence. Okay. And then, uh, sequence, uh, Max length or size. Nice. Nice. Nice line. No, not over it and had it too. It's exactly, um, quipped gradiance equals 5.0 I'm going to explain this in a second. Let me just write this out. Uh, not a lot of space in the brain for a lot. There we go. Okay. I can to generate new sequences. I am, uh, Ahmed. Yes. How big is your source data? It's about 20 megabytes. It's just a list of every us city. Um, and uh, let's see. Um, okay, so, so what this is doing, it's going to say generally those sequences put them in our dictionary.

Speaker 2:          30:19          The Max length of 20 a clip, the gradients at 5.0 that's just a standard number. We're just going to keep their, it doesn't matter that much. And then the checkpoint path model us cities. So this is going to save checkpoints as be trained, which means like if something happens to my computer, like you know, I download some porn and it destroys everything. It's going to have those checkpoints save. So the model, next time I started training, it's going to go from that build it from scratch. And Rico, that would be so long, but I will, I will do that later. Okay. So here's our last part, the training part. Okay. That's it. We have five more lines guys. We have five more lines. Okay. Stick with me here. So for I in range 40. Okay. So now as a training partner, we're going to say courtesy.

Speaker 2:          31:00          I'm gonna explain what a seed is and it's like the seed random sequence from text file. Um, half Max. Okay. I see. What does a seed do at seed? Helps us start from the same point every time when we generate new things. So when we were about to generate a bunch of city names, right? And so if we start from the same point, that's what a seed does. And let's just start from the same point so that we have a little bit more. Um, we have a little bit more names are more similar than they would than they would otherwise be. Okay. Let me just read this. March. Could you please do a new line of TF and fully connected? Yes, the call. Thank you. Okay. Okay. So that's our seat. And now we're going to fit our model are generated cities model. This is the, this is, this is the learning part.

Speaker 2:          31:52          Okay. So, so I'm going to take this three inputs. I'm going to say x, our input data, why is what we've created with our vectors or validation set is a culture 0.1 batch size of 128. So now in defining how much we're going to train, I'm just going to say 128 for batch size. Um, how many POCs do I want? And he pockets like an episode. How many episodes run ID is, uh, what are we going to call this thing? Or Can I call it [inaudible]? Okay, this is, that's the train. The fit is the train. Okay. And so now, so now we're going to print, uh, he testing. We're going to say the casting because we're going to create this terminal and we're going to say, um, print, uh, else. We're going to say print test, print. I'm gonna print the generated. We're going to put the generator to generate 30 temperature equals 1.2, um, sequence scene. And then this we created. Um, okay. So that's it. And maybe we can do this a few times. So then with different directors, what is temperatures? Meaning? I'll pretend I have a second. Okay. So this time it's with a temperature of 1.0

Speaker 2:          33:11          I 40. Um, you could do more than 40. I just don't have like a huge GPU, so I'm just like 40 is good enough for right now. I don't feel like training it for longer.

Speaker 2:          33:24          Okay. So then that's it. Okay. That's it. Let's see. Um, okay. The end up fit line. Okay. Okay. And a fill line. Um, so endorphin basically is, uh, taking our, what we've generated from sequence it from our sequence generator at that model. Okay. And model. And it is, it is, it's taking the training data and it's adding into our LSTM. So that's, that's the actual training bit. Do you make enough money out of Youtube? No, the answer is no, but I won't be bought out unless you're deep mind, so it's all good. Um, how did you choose the batch size? It is in relation to the dimensions or know of samples of data. Uh, it's not related. I just randomly chose one 28. Shouldn't it be G. Dot. Fit. What's m? So that's a good question. So Emma's Emma's the, the, um, model. But, um, so there's, there's a little bit of magic happening here. As you can see, like those two things aren't connected, but TF learn knows that they're connected. Um, and yeah, so let's just, let's just train this. Um, okay. Let's see what happens when we let me for now. Command line. Uh, okay. Where are we? Where are we at? I thought on, let me make this bigger. Okay. Thank you. Big As hell.

Speaker 3:          34:51          Okay.

Speaker 2:          34:56          Oh my God, I have scenarios. Okay, here we go. Okay. Oh my God. Okay.

Speaker 3:          35:10          Hm.

Speaker 2:          35:13          Um, uh, okay. So let me answer some more questions. Uh,

Speaker 2:          35:21          how does I do and I, if you don't have to follow, you don't have the library. How do you do a, you have to download the file and then what is the loss function? So loss function helps us, um, the loss function will help us. It's something we need to minimize. We train the loss function is something we need to minimize as we train. And it's basically a way for us to measure how good are our learning algorithm is doing. Okay. So I'm about to compile this thing. Um, but I'm running out of time so I'm going to see what is this era. Let's see. Um, this is going to be a lot of shit because the problem is that this tax was so big that I didn't, I should have been compiling and checking for errors as I was going. Um, but it's okay. It's okay. I'm just double checking is, does anyone have a link? I'm going to have a link to this video when it is done. Um, and right now I am going to print out what's happening here. Let me go ahead and, let's see. No, I don't want to purchase that shit. Okay. So, all right.

Speaker 2:          36:35          Okay. Training. Where is that happening? Line 33 don't need that.

Speaker 3:          36:43          Okay.

Speaker 2:          36:48          No module named Gfr. I fucking kids. Okay. I know. I don't, I don't register it. Uh, yes, I'm Indian and American both, uh, born Duis. Parents from India. Uh, no module Natgeo fine. Okay. Give me a second here. Um,

Speaker 3:          37:12          hold on.

Speaker 2:          37:18          Okay, so now it's, now it's uh, it's working. Okay, so it's training on that data. What's happening? It's training on that data and you can see that there's a bunch of like a text here that's printing out like vectorizing texts, texts, total length, distinct chars total sequences that we didn't cope when we never printed this stuff. We never did that. Okay. That's because TF learn, uh, is doing this itself. This is a lot of magic happening with CF warmth like these, these functions like sequence generator, right? Sequenced generator. And then random sequence from text file is a very high level functions. Okay. Bigger font.

Speaker 3:          37:57          Okay.

Speaker 2:          37:59          Hi. So this is going to take pro. So let me [inaudible] this is probably going to take like, uh, probably, uh, 20 minutes to fully trained with, we don't have time for, so I went ahead and I'm going to pause that and it's, so here's, here's an example. I compile this beforehand. Okay. So you can see you like what the temperature is that we row what's happening. These are new cities that are, that have been generated. They don't exist. They've been generated from the cities that we already have, like more wood and you know, CNN. So rocks. Okay. So stuff like that. And if you want to see the Dataset that we use for this, let me just take that thing and just going to copy that Dataset.

Speaker 2:          38:40          Yes. I'm going to ask me anything. Yeah, for sure. Give me a second. Let me just take this data set and then printed and I'm going to put it into, um, my browsers. Do you guys and see what it looks like. I'm going to paste it in and I'm going to put it right here. Okay, so take a look at this. This is the Dataset. It's huge. It's every single us city. Okay, so train on these words. Okay. So that's about 20 megabytes. All right, so now that's, that's that. That's what we have time for today. I want to keep doing these live streams. I'm going to do a five minute ama before ending. So let me just stop screen sharing and go to full screen for a second. Okay. I'm going to put this down here, so see what's going on. All right. And I'm going to answer a five minute questions.

Speaker 2:          39:27          Okay. Any questions? Okay. Okay. Here we go. Five minutes. Dope. Um, I um, I want to do a good one too with windows. I'm scared of losing data. Use parallels then parallels is great for that parallels download parallels. Thank you. What exactly is an epoch? Epoch is like an episode of training. So you have like 20 epochs. That means like 20 episodes of training. How many hidden nodes there were five, 12 in this one. Draw the neural network created yet. Um, meet you. Then second, did I, did I study a university? Yeah, I went to Columbia University. I studied computer science there. Whereas you start learning machine learning, my channel started under, she only for hackers one go through every single video up til now. How should I start learning? Just that, my video.

Speaker 1:          40:13          MMM.

Speaker 2:          40:16          Okay. So for movie lens data for recommendation engine, I would recommend using tensorflow a deep, you need a deep neural network. There's a type of recommendation system called a collaborative filtering system. If you go to git hub and you type in TF, Dash, r e c. O. M. M, there's a great library for that. I'm an image recognition application. See my video called Korean image classifier and intentional flow in five minutes. I did that with Tia and you can, you can specify the CPU or Gpu using one of the perimeters. There's a method for that. If you search CPU or Gpu, tensorflow and pupil, it's going to be the second link on stack overflow. I take any books you can recommend for a python newbie. It's called learn python the hard way it's called learn python the hard way. It's the best python book you could read. Okay. Uh, worth area to write your thesis on tensorflow. I would say, uh, what's hot right now? Probably. Hmm. A one shot learning with tensorflow. One shot learning and I have a video on that and see search my channel one shot learning.

Speaker 1:          41:21          MMM.

Speaker 2:          41:24          Let's see. Implement data compression and noise reduction. You can do that using an auto encoder. I have a video on that called build an auto encoder in five minutes. Can you do video classification model? Carlos, that is a great idea and I don't think I've done that yet. So that, that'd be cool. How cool would it be to generate video? Like random videos? So you train it on a movie and then they generates new seats. I that didn't exist. So like real life scene, but then it's how cool. That would be amazing. Right? Do I do youtube full time? Yes, I'm a fulltime utuber. Okay. Um, you're really interested in theoretical cs. Um, I'm not sure if I get him mathematics or computer science. Go for computer science that way. You wanted the applied mathematics for computer science. Hi Omar. Love you. Thank you. Uh, how would you do it with cans? Yes. You're Schwann. Yes, I'm India. How do you do? UN? Okay. Um,

Speaker 2:          42:28          anyone can be a data scientist. That's when new series is about. And because you guys are here live, I'm going to tell you what my next video is about. I'm still writing descriptive, but it's coming out in two days. It's called a, it's using Twitter for sentiment analysis. So we're going to be mining Twitter data and we're going to do, do sentiment analysis that I know my hair is less crazy today. Right. That's cool. That's crazy. Okay. Why are Indians good at cs? I, you know what it is? I think it's just like your parents just growing up or just like study, study, study, education, education and, and make money. And that's all like computer science. Really. Um, uh, you're amazing George. Sure. I learned python if I already know job when people as plus Mr Benitez. Absolutely. Yeah. Thank you John. Uh, you Udacity do the machine learning nanodegree not that data.

Speaker 2:          43:22          Now panel this one. Exactly. It's easy to learn if, you know, can we make a peer to peer in neural network? Yo, I never even thought about that. The answer to life. That's a, what would that even look like? That's that's, that's a good question. Um, anyway, America, people are lazy. Well, we're, we're not lazy. I mean, we're going to Mars in 10 years. We're going to have a million person calling in 40 years and it's going to be the whole world. So we're all going to do it. Okay. We'll build a new, we'll go to the new country and on Mars. It's gonna be crazy. Anyway, I'm going off topic. What are the spikes behind you? I'm in a sound room. Okay. I'm going to sound rude and this is good for a blocking sound. I'm working out of a coworking space in San Francisco.

Speaker 2:          44:05          Thanks Karen. I love Toronto. Okay, so that's all for today. I'm going to go, I'm going to stop the broadcast. Um, uh, and so one more question. Did you participate in Kaggle? I have participant in Kaggle. Kaggle is a great source to become better at data science. Look at past projects, looking at what they did and try to try to replicate the results. Me and Ilan. Yeah, I'm actually, I might be meeting you on this week because Greg Brockman the CTO at open AI said he wants me to come in to talk and so like you aren't comes in once a week. I know that. So like there's a chance that I'd be like, Yo, Yo Greg, you want to intro meeting Eli? There's other, you know what I mean? So we'll see. Anyway, thank you so much, guys. Okay. All right. Uh, I'll do another one soon. I don't know when, but thank you so much for watching. For now. I've got to go drink some coffee, so thanks for watching.