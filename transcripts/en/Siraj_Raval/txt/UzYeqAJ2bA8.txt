Speaker 1:          00:00          Hello world it Saroj and Alphago zero is deep minds popular AI that beat the world champion at the game of go in a match that was watched around the world. In this video, I'll explain how it works using code and animations. If you're new here, don't forget to hit the subscribe button to stay up to date on my latest AI content. Deep mind shocked the world in 2016 when the initial version of Alphago beat 17 time world champion Lisa Dole. Just a year later, Alphago Zero unimproved virgin that unlike its predecessor, was trained without any data from real human gangs. It learned only by playing against itself beat the world champion. In regard to Alphago, Ian Goodfellow, our researcher at Google brain noted that we're starting to see AI move beyond what humans can tell the computer to do and the AI can actually figure out how to do something better than the best human.

Speaker 1:          01:08          The idea that AI can think in ways we can't was exemplified when Alpha go played move 37 against Lisa Dole. It was a seemingly bad move to the human master, but it turned out to be crucial to its success. It had learned a way of playing that surpassed the 2000 years worth of strategies humans had come up with, but that should not seem scary. The dot product operations became self aware. If we can combine this alien way of thinking with our own to solve problems, there's nothing we can't do. The possibilities really are endless. Whether it involves discovery or creativity, we just need to make sure everybody understands how this power works. A traditional game of go starts off with an empty board and let's be real, probably some tea. Both players have an unlimited supply of pieces called stones. One taking the Blackstones and the other takes the white stones.

Speaker 1:          02:13          The goal of the game is to use your stones to form territories by surrounding vacant areas of the board. It's also possible to capture your opponents stones by completely surrounding them. Both players take turns placing one of their stones on an empty point at each turn with black starting off. These stones are placed on the intersections of the lines and one's played. Stones cannot be moved, but they can be captured when the game ends. Both players count one point for each. They can point inside of their own territory and one point for each stone they've captured the player with the larger total territory plus captured stones is declared the winner with so many possible positions to play here. There are endless tactics to learn to try and master this millennia old game. So when thinking about developing an algorithm for Bo, we need to start listing the properties of this game in a fundamental mathematical way.

Speaker 1:          03:20          We know that go is a discreet deterministic game with perfect information. Meaning there are individually separate and distinct moves and positions. Every move has a set outcome. There are two players competing against one another and both players are able to see everything. The whole position of the board. It turns out that tic TAC toe is another game that has those same properties. Life isn't though, unless you're Neil. If you were to develop an AI to beat tic tac toe, then we could write a script that would pick a move that results in the lowest score for the opponent. We know that there are three possible values for a given game state. Either it's a loss, a win or a draw, so our script would try out every possible game states in real time forming a tree of states. Then perform a depth first search to find the optimal move to play.

Speaker 1:          04:21          When we run our script, we'll find that it will always either win or draw with any human it plays against, but could we apply this same tree search to go well? It turns out that there are more possible go positions than there are atoms in the universe. So since go is such a complex game with so many possible moves to play using a brut force search to find the best one simply isn't possible. Using current computing hardware, we need a strategy to initially explore several possible moves on the board. Then focus this exploration over time as certain moves are found to be more likely to lead to wins than others. When our depth first search method estimated the value of a given state in the search, both players must play optimally choosing the move that gives them the best value and this required computationally expensive recursion, but instead of making the players choose optimal moves, what if we computed the value of a state by making the players choose random moves from there on and seeing who wins?

Speaker 1:          05:37          This is the basic idea behind the Monte Carlo tree search, the Goto Algorithm for writing bots to play discreet deterministic games with perfect information. It uses random exploration to estimate the value of a game state. A single random game can be called a layout and if we play thousand layouts from a given position, x and player a wins more than half the time, it's likely that the position x is better for player a than for player B. We can create a Monte Carlo value function that estimates the value of a state using a given number of random play out. The code will be similar to the depth first search code, but the difference is that instead of iterating through all move possibilities and picking the best one, we randomly choose moves. But random doesn't sound too intelligent, does it? What if we could have our algorithm choose a move using some knowledge of what moves are worth playing, but not by trying them all out, which would be very expensive computationally, but instead by computing a heuristic approximation of the true value of each move and choosing moves within play outs.

Speaker 1:          06:57          Based on this heuristic, the UCT or upper confidence tree score can help with this. It's made for balancing both game exploration and exploitation in a sensible way. I computing a heuristic value of a move. The the is computed using the aggregated score after playing a move in all simulations thus far. The number of plays of that move, the number of games states and an adjustable constant representing the amount of exploration to allow this equation helps balance exploitation by playing known moves with high values and exploration. Trying how moves that have a low visit count and updating the algorithm with this new found knowledge of how valuable they are. Notice that UCT involves learning over time, whereas both DFS and plain MCTs or static at first UCT is exploring lots of game states, but as it collects more data, the random play outs become less random and more heavy, meaning it explores move paths that have already proven to be good choices, ignoring the rest.

Speaker 1:          08:15          This creates a self reinforcing cycle, becoming more skewed towards good moves than bad moves. But check this out. Sometimes two game states can have pieces in different positions, but the essential components of the situation or the same meetings strategically, the difference in peace locations might not matter to the UCT score. These are two distinct game states, but to humans, we could intuitively know to that similar strategies would apply to both. It's essentially the same game state. Rather than having AI memorize a surestep value for every game state it finds and storing it in a hash table. What if it could learn heuristic values from images of the game board? Alphago zero sprinkled in a convolutional neural network to do just that. It used a massive 20 layer residual network for this. This is a convolutional network that avoids the vanishing gradient problem common in neural networks, which makes them unable to learn longterm sequences by creating a shortcut at each layer, which allows the gradient to skip over intermediate layers and reach the bottom during backpropagation without being diminished.

Speaker 1:          09:40          While neural nets usually output a single fixed output, their network had two heads. One took the output of the first 20 layers and produced probabilities of the go agent making certain moves and the other took the output of the first 20 layers and output a probability of the current player winning. So the training process of Alphago zero for playing against itself in Games of digital go started off with initializing the residual network. It used 1600 Monte Carlo tree search simulations per move that it played and sampled about 2000 positions from the most recent 500,000 games. Along with whether that game was won or lost for every move. It recorded the results of the MCTs evaluation of those positions, meaning how good the different moves in these positions were. Ace on looking ahead and whether the current player won or lost the game. The neural network was trained using both the move evaluations produced by the MCTs search and whether the current player won or lost at every 1000 iterations of this process.

Speaker 1:          10:58          The algorithm evaluated the current residual network against the previous best version. If it won the majority of the Games, it would begin using it to generate self play games instead of the previous version and that's really it. It just use simulations to generate training data for its neural nets. Then learned from them in a supervised way. In total, it took Google 40 days to train Alphago zero to become the best go player in the world using 64 GPU and 19 CPS, which costs millions of dollars. While the hardware is expensive right now, those costs are dropping. And the idea of Alphago, an AI that's able to master such a complex game better than any human can by playing against itself with no human knowledge input is dope. Aif Hey, you made it to the end. You beautiful ways, or you hit subscribe and connect with me on Twitter, Instagram, and Facebook for more developer education. For now, I've got to search a tree, so thanks for watching.