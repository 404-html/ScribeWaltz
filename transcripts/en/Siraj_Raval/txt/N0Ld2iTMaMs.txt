Speaker 1:          00:00          If deep mind hired me, I just reveal their secrets. So hello world, it's so Raj and deep mind just dropped a very impressive paper called neural seen representations and rendering. Their AI is capable of rendering an entire three d environment from just one or a few input images and to make it even more impressive it learn how to do this without any labels. It just learned from data eight obtained itself from exploring different three d environments. I'll explain how it works in this video since there are a lot of applications for this technology. For example, companies like Google and GM are spending billions of dollars on research and development for self driving cars. If a self driving car incorporated this technology into it stack, it would give it yet another signal of what to expect on the road. After training on millions of hours of Dash Cam footage, it could create a reliable three d map of what's likely to come up on its path.

Speaker 1:          01:02          Adding to is predictive decision making capability and thus further reducing the risk of an accident. Also, creating a quality game is no simple task. Often it requires many hours of practice with a whole host of tools to begin the process of creating an intricately detailed three d world. This technology could allow anyone to generate a three d game world from a simple two d drawing or even a real life photo, letting them iterate on their ideas much faster. And these three worlds could be used not just for gaming but for virtual and augmented reality. The Ar versions could let engineer's design different versions of a component much faster acting as a design tool. So the researchers had a straightforward goal, create an AI that can understand any given scene. If you or I were placed into any given scene, be that a Savanna in Africa or Savannah, Georgia, we would immediately perceive and interpret everything around us.

Speaker 1:          02:04          We would observe where the nearest Wifi was, whether it was day or night based on where the sun was positioned, what type of animals surrounded us. Like we're Fiki where the nearest village might be based off of footprints. Our brain with Lauren to form representations of the environment that would support not only classifications of what we see, but also motor control, memory planning, imagination and rapid skill acquisition all related to the environment. All of this, without any teacher telling us these things, we would do it ourselves. David Marr, one of the originators of the field of computational neuroscience suggested in his influential book vision that there's likely a generative process in the brain that gives us this incredible ability, one that doesn't require supervision. In contrast, the current most popular computer vision technique is to use deep convolutional neural networks on big labeled image data sets to learn how to interpret images.

Speaker 1:          03:10          This label centric or supervised approach requires an intensive process of humans manually labeling images. Then having a neural network learn the mapping between the input data and the output label. Only then can it classify what it sees in an image, but even then, this approach doesn't give the AI the ability to really discern what it's seeing and in what ways different objects in a scene relate to each other. So the researchers decided to create an AI that internally represented the environment. It was placed in more like we would minus our constant existential crises. The idea then is to give this AI some input data, ideally two d images of a scene that it's placed in. Essentially what gets cs along with its position in that scene and use that data without any kind of label as the training Dataset. Luckily they had an open source game engine ready called deep mind lab that they made that can be used as this training dataset.

Speaker 1:          04:11          It's a collection of three d environments that can be used to train an AI agent in including a simple square room that you can easily very floor and wall textures of in a more complex, randomly generated maze, but it seems like a simple seven by seven square room would be a good starting point for training. This AI, they call their AI the generative query network or Gq n it consists of two parts, a representation network and a generation network. The representation network takes as input what the agent observes inside of the Three d environment, essentially a two d image frame. The representation network then outputs a representation of that input. The idea is that this representation, we'll capture the most important elements of the scene, like the position of objects, colors, and the room layout in a compressed way. It will learn to detect those features.

Speaker 1:          05:09          They aren't hand coded. Then the generation network will be asked to predict Aka imagine a scene given both a previously unobserved viewpoint and the seed representation created by the first network. The generator essentially learns how to fill in the details given the highly compressed abstract representation created by the first network in furry likely relationships between objects and regularities in the environment. I'd like in the relationship between these two networks to the relationship between a crime scene witness and a sketch artist. The witness remembers fragments of a criminal, their height, their hair color, their choice of Linux distro, and the sketch. Artists must discern the full picture of the criminal case on a few details, inferring the likely other traits based on what they're given by the witness. Put more formally. First, the algorithm collects a set of different viewpoints from the training scene. Each viewpoint is an image each fed sequentially into the representation network, which is a convolutional neural network that's known for image classification tasks.

Speaker 1:          06:21          An image is a matrix of numbers and through a series of matrix operations, a convolutional network will continually modify that input matrix. The result is the representation. It will create as many representations as there are viewpoints. Then they performed a summation operation on the representations to create a single representation or are is then fed to the generation network. For the generation network. They used a recurrent neural network. Since they are capable of processing sequences of data during training, recurrent networks aren't just continuously fed the next data point in a dataset. They are also fed bill learned state from the previous time step, which is what gives them recurrent knowledge of the past. They learn from what they've learned before allowing for a contextual understanding that incorporates time into their predictions since they wanted an agent that could predict the next frame in a sequence of three d environment frames.

Speaker 1:          07:25          They needed to use a sequence model and the generator network used what's called a latent variable to mathematically very the outputs a bit. The generator then generated a likely image for a given viewpoint and that generated image was compared to the actual viewpoints on error. Value was computed by computing the difference in these two images mathematically. Then they updated both networks using that error to be just a bit more accurate. The next training iteration via the popular backpropagation technique, updating the weight values of each. This optimization strategy meant both the representation network and the generation network were improved over time. At the same time as the agent navigated whatever environment it was in. Making this an end to end approach. The first trained it on a few simple seven by seven square maps with a few objects in them over time. It rapidly learn to predict what an entire map look like, so they gave it a more complex maze instead and over time it learned how to represent that as well.

Speaker 1:          08:36          At first it was a bit uncertain of some parts of the map, but with more observations and buy more. I made only five total. It's uncertainty disappeared almost entirely. Eventually they wanted to use it to control a robotic arm to grab a colored object in a simulated environment because Yolo, no, not the algorithm. Deep reinforcement learning is a combination of deep learning, Aka learning, a mapping and reinforcement learning, Aka learning from trial and error in an environment. It's been behind some of the big AI successes of the past few years like Alphago and Ataris DQ learner. The idea is that the AI agent learns of policy for playing a game. My learning directly from pixels from the game frames, no hints as to what the objective of the game is, what the controls meet. The problem with this approach is that it requires a very long training time to converge to good results.

Speaker 1:          09:35          So they conducted an experiment where they first trained the GQ N to learn how to represent observations of the environment. Then they use it's learned representations, has input to a policy out rhythm that learned how to control the arm. The representation encapsulate what the AI saw, the arms joint angles, the position and the color of the object, the colors of the walls in a much more compressed way than just using the raw input pixels. And because of this they saw that it was substantially more data efficient, requiring only a quarter of the training time that a raw pixel virgin would require. Very impressive indeed. Gq N is exciting because a major limiting factor on what it can do is computing power. If given enough computing power, who knows what kind of amazingly detailed environments it could generate. And this is exciting for anybody, designers, artists, engineers, scientists who could use a tool to help them visualize and create things.

Speaker 1:          10:39          Three things to remember from this video. Deep minds generative query network learns how to perceive and interpret an environment without labels. It consists of a representation network, which encodes image frames and a generation network that generates them based on those representations. And it did surprisingly well, requiring only a fourth of the training time for a deep reinforcement learning task that a raw pixel focused algorithm would require. Ai Is never boring. If you want to stay up to date on the field, answer, survived the AI, apocalypse it the subscribe button for now. I've got to keep reading papers, so thanks for watching.