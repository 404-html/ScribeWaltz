Speaker 1:          00:01          Okay.

Speaker 2:          00:06          Sure.

Speaker 1:          00:07          Okay.

Speaker 2:          00:20          Hello everybody.

Speaker 2:          00:25          Let's see. Let me move, make this work. Oh, we're all bits or Raj? I guys, good to see you. Let me mute myself. Okay, there we go. All right, we've got people coming in the room. Hi everybody. Hi. Pressed in high stray Leandro. Ivan, uh, Nico Vosshall. Nyshawn Sebastian. All right, got to kill it with that fresh tensorflow. Uh, Shuba deep. It is in an ice tea this time. Um, but I will get fresher stuff later. All right, so, okay. Um, in this livestream we are going to, uh, we are going to create a classifier in tensorflow for m and ist. Okay. What does that mean? We're going to create a classifier in tensorflow, no TF learn, just tensorflow, straight up tensorflow. And we're going to do this for a handwritten character digits like images. So we're going to give it an image of like the number six and our, our, our model is going to be to look at that and say, oh, this is a six. So it's going to be able to read, uh, the image, the number that is in the image, and we're going to do this in straight up tensor flow. All right. Um, so that's what we're gonna do and we're going to start off with five minutes of Q and a and then I'll get right into the code. All right, so five minutes starts now.

Speaker 2:          02:07          Um, that's cool. Map, right? Yeah, I, uh, I love that map because it's really colorful. Um, okay. Um,

Speaker 2:          02:19          Openai universe. That is a question. Did Google contact you? They did. Actually. I'm meeting them next week. Um, we'll see what happens. I mean, I'm sure there's something we can do together. Uh, this is my apartment. Yup. How, how to study chemistry, uh, watch chemistry videos on Youtube. Well, that's not my, I'm not into, why are you so awesome? I just believe in myself. How does this differ from the example in the docs? Uh, I mean, this is kind of like a custom version. How many layers will your model have? It's going to have four layers. It's going to be a convolutional neural network with four layers. Uh, what is the dopest way to build AI to detect fake articles on the Internet? That's actually a hard problem that we are working on right now. Detecting if something is fake or not. Like, uh, a validity test is actually like, um,

Speaker 2:          03:13          I mean, what does that even look like? Like if you had a supervisor, if you had a set of articles that were labeled as real and a set that labeled is false, I mean, as fake, you could, you could do it that way. But even then, like if a fake article was good enough, like it would just pass. So we needed, we needed a vet. We need a very, very, very strong, robust model for that. And like, you know, Google auto reply level amount of data that it's trained on. Will you use LSTM? Uh, yes. I will. Um, Mac book or windows PC Mac book. We'll scholar be by python. No. Um, did you lift your job? Why? I, we used to work at Twilio. Um, uh, but I want to do this full time. Why don't you make a video about NLP in particular, the entity type recognition problems? Uh, I have several videos and an LPA. I'm going to make more though. What is your laptop specs? I just got the new Mac book. I got lost in the neural network. Um, okay, so let's, let's go ahead and, uh, do this. You guys can see me, right? No, no, no. I'm still here. Just someone safe that they can see me. I just had to, I just had a problem for a second. Um,

Speaker 2:          05:47          all right. Let's see. I'm still broadcasting. Okay. I'm still broadcasting. Audio is back. No, no, they, they can't see me. Okay. Um, came off right. There we go. All right. Okay. There we go. We're plus code. Uh, and so now you guys can see me. Okay, here we go. Okay, so we've got people watching and we're going to screen share. Okay, so screen sharing time. Um, sure. Okay,

Speaker 3:          06:23          boom. Um,

Speaker 2:          06:27          all right, so here we go. And uh, all right, you guys can see everything dropped out for a bit. I'm going to make this bigger. Okay, so now we've got 161 people watching and this we're, okay. So here we go. You guys can see. All right. Beautiful. Right? All right, so let's do this. So we're going to import our first library, which is the future library. Why future is going to let us, uh, so future is going to let us, there's lag, right? Okay. Is there still lag? Um, yeah, I'm gonna, I'm gonna get a, I'm going to, I'm going to get a better connection next time I'm lacking. Okay. Hold on. Uh,

Speaker 3:          07:12          hold on. Damn it.

Speaker 1:          07:24          Too much.

Speaker 2:          07:33          Audio is good. Okay. Bad video quality.

Speaker 3:          07:38          Um, hold on.

Speaker 2:          07:42          Okay, so, um, it's still lagging. Okay, so the video is lagging. Um,

Speaker 1:          07:49          okay.

Speaker 2:          07:51          I'm not sure and too much lag. Okay. So there's too much lag and video lags a lot. Okay. So just try to restart it. Okay. Um, let's see. Let's see. Let's see, let's see. Restart. I can restart. Restart the one. Actually let, I see there's a little, I'm not sure. Okay. About, um, one 23. Okay, so one 23. Oh, so everybody, it's lagging, right? That's what's happening right now. It's lagging. That's what's happening right now. The stream is lagging. All right. So I'm just going to keep going, um, because I can't help but the lag, so I'm just going to keep on going and, uh, it's still lagging. Okay. Um, sound is okay, but the video is lagging. Uh, it's breaking.

Speaker 2:          09:14          I'm just going to keep going. I don't give up. Okay. So, so for future we're going to import the print print function. Um, all right, great. We're going to import the print function and we're going to import the cancer floor. Those are two library that we're going to inquiry and future's going so that we can have the print function from python two and we'll get, we could do it for a python three. Okay. So, so there's that. And uh, so now we're going to, uh, so, so we had that. Now we're going to import our dataset. Okay. Um, it's fine. All right. No lag. Close your browser tabs. All right, good. Good call. Actually when the clothes that boom, boom, boom.

Speaker 2:          09:57          So, uh, so, okay. So we're going to import our data set, right? So the first thing we're going to do is we're going to import the exam. Our example data set of BM and ice tea. And I'm going to talk about what format we're going to, we're going to import this ass. Okay, so we're going to import, this is our helper class input data input data is going to do is it's going to pull our data from the web, are Emma and Ist art are handwritten character digits are images. It's going to pull that from the web and it's going to store it in our folder and it's going to format, it's going to format it, so that's good so that we can use it in our code. Okay, so good. Finally, there's no leg. All right. And I'm sorry about the lag guys. Um, I'm going to not have this ever happen again.

Speaker 2:          10:36          All right, so I'm going to work it out of a place where there is no law. So we're going to create a variable called name an ist using our input data help, uh, help her class. And we're going to, we're going to use the read data datasets method. We're going to read it from wherever it's safe. So it's going to save it to the temp folder are our data folder in our temp folder. And we're going to, and we're going to say one hot people's true. What is one hot equals true means? That means that let's talk, let's talk about this for a second. What is one hot encoding? Okay, so this shows up a lot. One hot encoding, one hot encoding is a way of, of of formatting data of representing data. So that it, so that it, uh, so that it's more machine readable.

Speaker 2:          11:21          So what would be an example? So if I had an array like car car, if I included this, normally I could just say like, I could represent this as one zero two one. Okay, I could, I couldn't say that, but um, what is this? This looks like tooth is the greatest value. It looks like tooth is greater than one because the value that I'm representing it as is, is greater. But this is wrong. Like the tooth isn't actually greater. I just need a way to differentiate between different, right? So a better way of representing these would be, uh, as binary digits so that I could represent health is one zero zero zero and then I could represent car as zero, one zero zero and then I could represent tooth as, you know what I'm saying? So there are different ways of representing, uh, of, of representing data and it's using this kind of binary format.

Speaker 2:          12:16          So it's a way that lets us encode data. So it's more machine readable and it's good for regression in classification, but no, uh, it, it doesn't say that one is greater than the other. So that's what we call it. That's what we say. One hot okay. To and coding it. But there's not like, it's, it's, it's not greater than the other. Okay. So that's one hub doctors. Okay. So that's us getting our dataset import data. All right, so next step is to, uh, get the love you too, Michael. Next step is to get the hyper parameters. All right, so hyper parameters. These are our tuning knobs and I'm going to talk about each of them. So, okay. So, um, the first one I'm going to import, he's called the learning rate. Let me explain what I'm doing with here.

Speaker 3:          12:59          MMM.

Speaker 2:          13:02          So why am I sending it to 0.001? Okay. So as data flows through our neural network, we were going to apply an activation function, uh, at each layer. So what this function is going to do is it's going to transform our data in some way and we're later going to use, we're later going to use, um, that transformed data to update the weighted connections between the layers until what the learning rate does is it's, it's what we apply to that weight updating process. So the greater the learning rate, the faster our network trains, but the lower the learning rate, the more accurate our neural network trains. Okay. So it's, it's that trade off between time, like speed and accuracy, speed and accuracy. Um, and one hot is used in ward representations as well, and that's what kind of, what all hyper parameters are. They're, they're, they're kind of trade off between speed and accuracy. So that our learning rate and now, um,

Speaker 2:          13:57          now we're going to have our training iteration. So how, how, how much do we want to train? I, so we're going to say 200,000 iterations. Okay. So generally the more iterations of the better it's going to, the more iterations that the better our model is going to train. Okay? So that's that for our training iterations. Now we want our high Heba. Hey guys, say hi Eva. She's my friend Irl. So for batch size, we want to say backside is going to be 128. What does that mean? That means we have 128 samples. That's the sides of our batch. The batch is what we train. Okay. Um, all right. So, um, there's that and we're going to send our display that to 10. What does that mean? Well, how often do we want to display what's happening while we're training? Let's say every, every 10 iterations.

Speaker 2:          14:44          So it's, so that's what we're going to say. What to say every 10 iterations. Okay. So there's that. And now we want to create our network for amateurs are network parameters. All right, so for network parameters, uh, we want to say how many, uh, uh, what is the size of our image? Let's say seven 84. What does that mean? I keep doing this minus sign, let's say seven 84. Um, so that means our image shape is going to be a 28 by 28 image, right at 28 pixel by 28 pixel. That's our, that's our, that's our, uh, besides of our image. So, so there's that. And then there are number of classes, um, is going to be 10. What does that mean? Well, we have 10 digits, right? The digits zero through nine or 10 digits. Now we're going to, our last network parameter is called dropout. Oh, I cannot wait to explain dropout. I love dropout. Okay. Okay, here we go.

Speaker 2:          15:41          Dropped out. Okay. Is this awesome thing? So dropout is a technique created by Hinton. Uh, Landon, I'm going to do it. I'm doing, I'm going to do another Q one Q and a at the end of this session. All right, so let drop out is, is it's this awesome method invented by Geoffrey Hinton and his team that prevents overfitting by randomly turning off some neurons during training. So data is, is forced to find new paths between the layers to allow for a more generalized model. Okay. So what does that mean? So, um, I think the best analogy I could think of is like old people. Okay. So generally they say like the older you get, the more set in your ways you're are you are, and there's a kind of scientific truth to this because there are these neural pathways in our brain, kind of like grooves that we kind of carbon to our brain that we kind of get used to thinking about.

Speaker 2:          16:29          Right? And so that's kind of what it is with dropout. If you train your neural network has certain way and you don't use dropout, then those pathways are going to continually be the same way. So it's going to be to train on the data, yet it's going to be to fit on the data you trained it on. It's not going to be able to generalize, well, it's not going to be able to predict new data sets. But what you could do with dropout is while you're training, what dropout does is it randomly turns off neurons while while the data is flowing through, right? While the data is flowing through, it's randomly turning off neurons randomly. And so that the data is forced to find new pathways. And what this does is it creates a more generalized model. So the model is then able to train and test on all sorts of data. Okay. And drop out is a very, very successful technique. Okay. So that's what dropout is. I, there's a little bit long, but that's what we're going to use for dropout. And it's a probability. It's the probability value, okay. And like everything else, it's a trade off and it's all about tonight's about trying and testing what works best. Okay. So, um,

Speaker 2:          17:30          so, so that's that. Okay. So now we're going to create our, uh, our, uh, placeholders, how we're going to, uh, how we're going to, uh, get the data in there. So what we're going to say is everybody say hi to Brian. He's also my friend Irl. Um, so we're going to take our placeholder variable are tensorflow placeholder variable, and we're going to say nine. And then the number of inputs. What does this mean? This is our gateway. This is our gateway drop out is not like shrimps kind of. It is actually. It actually, it is a, that's a, that's a great point. So we're going to, we're going to create, I'm going to, let me just type this out and then I'm gonna explain what is happening. What is happening today. Okay, so nine and then the number of classes. What is this? What we've created two gateways.

Speaker 2:          18:16          We've created two gateway for our data. One gateway is for the images are for the images. The other gateway are for the labels. So we're going to have an image, like a number, like an image of the number zero and then the actual labels zero. Um, and so both of those are going to be fed in at the same time. So, our, so our convolutional network is seeing our labels and it's seeing our images at the same time. Okay, well we need one more thing. It's called keep probability, which is going. And so TensorFlow's placeholder op, uh, uh, uh, tensor flows, a placeholder. A object is what is what represents those gateways. It's a gateway into our computation graph. So that's how the data flows into our computation graph. Can we have our placeholders? Okay. So we need one more place holder and that's kind of bleed a float 32.

Speaker 2:          19:01          So it's a 32 bit float and that's, that's what, that's what are our dropout. I told her to drop out is going to flow into our network. So we have three gateways, actually. One is for our, our, our image. One is for our label, and then one is for our dropout. Okay. So there's that. Um, and now what we want to do is create our convolutional layers. So let's create our convolution layers and I'm going to explain what's happening here. And so what we're gonna do is I'm going to create a, a, a function for this. So it's gonna be a Tutee convolutional layer. It's going to take a set of or parameters. Okay. What are these parameters that I'm, there have been 40 here. So what we're going to do is we're going to have these four parameters and uh, we're going to define our convolutional layer here.

Speaker 2:          19:46          So I'm going to type this out. And tensorflow has a built in convolutional, uh, function, right? And, uh, so we're going to say what are our men are very our perimeters for this. Well, we already imported them, uh, in the main method that we just called here and we have something called strides. And I'm going to explain it in a second. Book strides takes in these strides that I've defined previously. Uh, one, one, one, one, and then padding, it's going to equal same. And then, right? And so then x is, so let me explain this. Okay. So look, so this is, so this is why I, this is why I have um, a into a, its own function. I've quit cause there are two, there's actually three things I want to do here. I want to do three. Here's ago. Here we go and move to the claims. So what's happening here? Okay, so here's what's happening. Here's what's happening. Okay?

Speaker 1:          20:39          Yeah.

Speaker 2:          20:41          Okay. So a convolution is so, so when we're feeding these these images into our network, a is basically taking a part of that, taking that image and transforming it in some way. So that's what convolutional layers do. They take the image and they process it and they transform it in some way. So each layer is going to have some, um,

Speaker 2:          21:00          some representation of that image and it's going to get hierarchically more abstract. The higher you go in your network. So the first layer is going to be a two D layer, right? And we're going to add our bias to it. And what is a bias too? It's like all other hyper parameters. A bias is going to, it's going, it's a tuning knob. It's going to, you know, I can make it go one way or the other, but generally what biases does that makes our model more accurate. Okay. And, uh, so, so there's that. And uh, so, so what is called convolution is a fancy word for transform. Okay. For, for image transforms and we're going to return the relu function. Relu is an activation function rectified. Linear unit is the full name of it, but basically it is an activation function and we're going to, and we're going to use it in our convolutional layer. Okay? So that's what that, that's what's happening here. And then strides are, uh, so then strides are the, uh, uh, it's a list of integers and it's going to be a t a t a tensor, right? So, so strides are our tensors and tensor is just means that those just mean data. Tensors equal data. All right. Um, so that's, that's one convolutional layer. Okay. That I've created that as a function. Um,

Speaker 2:          22:12          okay. Thank you eva. Okay, so that's our convolutional layer. And so now, uh, I want to create another function called Max pooling layer. All right. So then that's pooling layer is what is pooling, well, let me talk about what pooling is. We've, we've created a convolutional layer, but let's talk about pooling. Cause this, this happens all the time in convolutional nets. It's like Paccar has a great, that's a great explanation. Convolution is like putting filters on an image. It's a, it's an increasingly abstract set of, of, of filters. Okay. So what pooling is, is a pooling layer takes a small rectangular blocks okay. From the convolutional layer and its sub samples them to produce a single output from that block. So it's taking samples of an image. It's taking little samples, pools. Okay. Like little pools from an image. Okay. And it's, so they're, they're going to be Max pooling because we want to take the average or the maximum of the, of the learned linear combination of the neurons in the block.

Speaker 2:          23:09          Okay. Um, so that's what, that's what Max pulling is. So we're going to, we're just going to do one thing. If we're going to say we're going to return them to the Max pooling functions. So tensorflow has a built in Max pooling function and we're going to, that's what we're going to return. Okay. So it's going to be the size, um, one k k one, two that, uh, and so that is that that is a Ford be tenser that the 40 tensor. Okay. So there are four variables there. And then same thing about our strikes, that's going to be another 40 tensor a one K K one. And padding is going to equal saying, I'm going too fast. I will slow down, I will slow down. Um,

Speaker 1:          23:56          all right,

Speaker 2:          23:59          so that is our, that is our pooling layer. Um, and so now let's quit our model. Okay. We've created our, our, our, our uh, definitions here. Now let's create our model. Okay, so create models. And um, so we're going to say convolutional nets, x weights, biases. So let me, let me type out these parameters and what is, what's going to happen here or model. We're going to take our x, which is our inputs, our weights, which are the, or the connections or synapses between our, our layers are biases that were, that's going to affect each of all layer or layers in some way. And then our dropout. Okay. So first before we do anything, we want to reshape our input data. So it, so it is, uh, we want to reshape our input data. So is formatted for our computation graph that we're about to create. Okay. Um, a screen shot went off. Video is down for you. Hold on, hold on, hold on. What just happened? Hold on. Here we go. Um, for camera off turn. Camera on.

Speaker 1:          25:09          All right.

Speaker 2:          25:10          Back videos back. Great video is back man. It is. We are on an adventure today. Okay. Video is back, right guys. Okay. So I'm going to say, uh, I want to reshape my input and, okay, great. And uh, this is the one time I'm going to have video issues guys. I'm just recording out of my place today. I usually regret in my studio, but it's close today, but I'm never going to let this happen again. All right. I thank you all for showing up because life stream, I'm going to, I do this for you guys. I'm going to keep on doing it. All right. So, um, so, um,

Speaker 2:          25:43          we're, where are we? Right? So we're going to, we're going to reshape our input data, right? And what is the shape that we want it to be? Well, we can define that as our parameter here. We can refine what our shape is going to be. And we want our shape to be a, uh, it's going to be 28. What, what was her conviction was 28 by eight pixels. And then, uh, our width and height, we have here a set to one, right? So that's how we're going to reshape our input. Now we can call our compliment leasing a layer. Uh, so, so now we're going to create our convolutional layer, convolutional layers. And uh, what is it? So we defined our convolutional function up there, right? So now we're going to actually use it and guess what? We haven't actually defined our weights yet. I'm going to define our weights in a second. Right now we're defining this function without defining our weight. Um, ah, so now we can, we can set our biases and our biases are going to be what are our bias is going to be. We haven't defined those either. So we're gonna, we're just gonna use these as placeholder, uh, first. Um, and

Speaker 2:          26:50          we're going to, uh, let's see. So that's a convolutional layer. That's our biases and weights. So now we have our Max pooling layer. All right, so, so we've created our convolutional layer and then we're going to add to that convolution and that's pooling. Um,

Speaker 1:          27:05          okay.

Speaker 2:          27:05          So then our Max pooling layer is going to take that convolution that we just had. So it was going to pick that Max full Tutti function that we just created. It's going to add another set of weights and

Speaker 3:          27:18          um, uh,

Speaker 2:          27:21          and then we're going to have our biases as it's going to be BC to boom, boom, boom. All right, so that's our convolutional layer and our max pooling layer. Okay. And, uh, so, so what happened here? So we have our accomplish on there and the next moment. So now that's our first layer. That's our first layer. Um, and now we're going to do our next layer. Okay. So our next layer is going to be convolutional two D, right? So that was our, one of our layers. And now we're going to,

Speaker 3:          27:51          uh,

Speaker 2:          27:54          now we're going to create our next layer and it's going to take our previous layer that we just created as an input. And we're going to say weights are going to be WC two. And then, so again, we're going to define our weights in a second. We haven't really defined all of them, uh, but we're going to define them in a second. And that's going to be our list of weights. Uh, and one more thing. We need to Max pool this layer too, right? We want to Max pool each of our layers. Um,

Speaker 3:          28:23          mmm.

Speaker 2:          28:25          All right, so let me, let me do that. Uh, Max pulled Judy convolutional the k equals two, and then,

Speaker 2:          28:37          okay, so, so that's what we got. So that's our Max pooling layers. All right, so now, uh, we, we have our layers and now we want to create a fully connected layer. Okay. So we've got our, both of our convolutional layers and now we can create a fully connected with why do we create a fully connected layer? A fully connected layer is to stay generic layer. That means like all of the layers are connected to every, so every neuron in this slit, in the fully connected layer, it's connected to every neuron in the previous layer. So the previous layers are two convolutional layers. So the convolution layers are taking, are creating a calm Lucian's or transformations or filters of the image, right? And then a fully connected layer isn't, it's just representing that the image, the image data, it's just a representation of image data without it transforming it in a convolutional way. So the first thing we want to do, because we want to reshape it, right? We're going to reshape the data for our

Speaker 3:          29:24          mmm.

Speaker 2:          29:27          Uh, for our convolutional layer. And then we want to get the shape, uh, and it's going to be as a list. It's going to be as a list of inputs.

Speaker 3:          29:40          MMM.

Speaker 2:          29:44          And so, okay, so that is our first connected layer.

Speaker 3:          29:53          And so now, um, I am,

Speaker 2:          30:00          I am doing the next one. Okay. So, so the next one is going to be, we're going to add, uh, so, so this is where the actual matrix multiplication happens. Okay. We're gonna, we're gonna take, this is what the actress actual matrix multiplication happens. Um, this is where the right. So for cars said, this is where the actual classification happens. This is where the, this is where the Matrix multiply happened. So all that data that we've been transforming, this is work. This is where we kind of combine it together. Okay? So what we're going to do is we're going to perform matrix multiplication using tensorflow. So technical has a built in matrix multiplication function. And for our weights, we want to use the same waste that we had. Um,

Speaker 2:          30:38          uh, and then we've got to, we're going to use our biases as well. Okay? So we're gonna use our weight. I'm going to use our biases, right? So there's that and now, now, okay, now that we've done that, uh, we're going to apply dropdown. Okay? We've, we've, we've created our convolution layers are fully connected layers and now we can apply dropout. Oh, you know what? I guess what, I forgot something I forgot to. Uh, at our activation function, what does our activation function? It is Relu and we're going to apply it to that, that, uh, at layer. Okay. So now we're going to apply, dropped out. So we're going to take, so guess what? Tensorflow has a built in dropout function and we define dropout earlier. Um, okay. Line 45 check the parameters. Um, let's see. Convolutional one weights and biases be too. Yeah, that works. Right. Um, so there's that. And so now we're going to say for our output, um, mine 45 what's, what's going on line 45, line 45 we have max pooling.

Speaker 3:          31:42          MMM.

Speaker 2:          31:44          Max pooling a convolutional one. Max Pool Two d. What is happening over here? I'm trying to figure this out. I thought line 46, uh,

Speaker 2:          32:03          Max pool two D or, anyway, I'm 40 is missing brackets. Oh, there it is. Okay, great. Thank you. Okay, cool. Okay, so anyway, so we applied, dropped out and now we're going to have our output. And so what does our output and be output is going to predict our class. So output is going to predict our class. Okay. So we're going to say, so now that we've, we're at the bay for Matrix multiplication on everything that we've calculated beforehand. Everything that we calculated beforehand, cause we're going to say our weights, that's going to be out. And then our biases. Um, we're going to be out and now we're going to return out. I'm 47 accomplishment too. Aww, thank you. That's, thank you Pippa. I appreciate that bias is out and then we return out. Okay. And that good and whatever we're turning here. That is our class classification. Okay. That is our class calcification. All right. Um, and hold on. Hold on. Matrix multiplication to Dah, Dah, Dah. Awesome. 47, so 47, college on TV at what's, what's happening on 47. Okay. A 47 is accomplished on Tuesday. Yes. Right. Um, okay, so now we were turned out okay.

Speaker 2:          33:42          Convolution on what it is you have to reshape dates. All right. So anyway, I'm just going to keep going here. So now, uh, now let's create our weights free weights. Um,

Speaker 2:          34:05          yeah, let me make my editor a little wider. Um, I'm not gonna be able to see the comments fully. Okay. Anyway, so let's create our weights. Weights are going to be, so we're going to create these weights as a dictionary, right? It's time for us to create our weights. And so our weights are going to be, uh, I was the for tensor flow variables. Okay. So are wasting gonna be a list of four tensorflow variables and uh, we're going to say, do you have variable, do you have that random I will. Yeah, I can use sense of boy, why, why don't I use tens of warp it is, okay. I'll, I'll use sensor board. Oh, when I'm done. Uh, right. So let me, I'm gonna okay. So it's going to be kind of hard for me to, to write this out and explain it the same time. So what I'm going to do is I'm going to just say, I'm going to write this out and ban. I'm going to explain what these weights are. So these are a list of that list of values and um, but what does this do? We're going to say our variable is a TFF random normal. Uh, it's what's going to be the same kind of thing, right? We're just going to have four tensor flow variables. And each of these, uh, variables, uh, is okay. Each of these variables is going to be a different value.

Speaker 1:          35:25          Okay?

Speaker 2:          35:26          Uh, okay. And so it's going to be 64. So, so there's that. And then I one got variable. So here I go. I'm about to finish writing these up. Miss, I'm on my third of a four. And do you have top random normal seven times seven times 64, uh, one 24. What am I doing here? So that's not an actual, uh, in 24.

Speaker 2:          36:13          One more variable guys. No, no, no. One more variable. Uh, don't, don't worry about it. One more variable and we're good to go. Alright, so CF got variable, um, gift up random normal, right? It's the same thing. All of our weights we want. And then we're going to say, and I'm going to explain what's happening here in a second. Okay? Okay. So here's what's happening. Here's what's happening. Here's what's happening. We have our first set of weights. Um, uh, it's a five by, so here's our first set of white. This is a five by five convolution with one input and 32 outputs. So it's a five by five that's at width. And Height and then of of of our input. And then if there's going to be one input that's going be an image. And um,

Speaker 3:          36:55          okay,

Speaker 2:          36:56          32 outputs, that's the bit that, that's a number of bits. Another one, five by five 32 inputs, 64 outputs. What are 32 meets? They're 30 32 different connections going 32 different ways and 64 right? So it's taking one image and it's splitting it and splitting it like increasingly through our network. These are synaptic connections. Okay. And so then it's going to be our fully connected layer. That's the way for a fully connected layer seven by seven by 64 inputs, 1,024 outputs. Okay.

Speaker 3:          37:23          MMM.

Speaker 2:          37:24          Lastly, it's going to take 10 24 inputs at the last layer. And then this is where we actually predict our class. The number of classes is going to be 10 so those are our weights. Okay. Now let's go ahead and construct our model. Okay. So let's construct our model construct model and let me know if it's likely or not. Like, if it's, let me know if it's not lagging because, okay. So I would appreciate that. So let's construct a model. Well, guess what? We already defined our, uh, Anthony Johnson. That was a thug life moment right there. Thanks for saying that. Uh, uh, I should go a little slower day. Yeah. Uh, so,

Speaker 3:          38:03          uh,

Speaker 2:          38:07          the weights are massive, aren't they? They weights or Matt. So now we're going to construct our model. What are we going to stroke constructed with? We're going to construct it with, uh, our input data x, our weights. So we'll, we just define our biases and our, uh, key prom, which is our, uh, dropout. Okay. So that's what we are constructing our model now. Now let's define our optimizer and our loss. And what can we do that we can start training? So what is our loss? We could call it loss or what you'd call it, costs, let's call it costs. We're going to use cancer close reduced me function, which is, which is kind of synonymous with reducing the loss. Okay. Um, and what is the type of loss function that we want to use? Well, Kaiser flows. Neural network class has the lost function that is good for image classification called soft Max. Cross entropy with logic. Okay. And let me explain what the asterisk is. Okay. This is a big fancy word right here. A big fancy mathematical words. So what does this doing? This is measuring the probability error and a classification task. Okay. It's measuring, it's measuring the probability error in a class of classification task. Um, and it's when the classes are mutually exclusive. So that means that an image that is at zero and the enemies that are one our exclusive. Okay,

Speaker 2:          39:25          so that's our, that's our, that's our loss or costs. Let me just call it cost for simplistic. And now let's define our optimizer. What is our optimizer and why do we even need an optimizer? Well, whenever we are creating a neural network, we want to create a, an optimizer and we're gonna use an optimizer call, Adam. And what Adam does is it reduces the loss over time. Grow gradient descent, process a gradient descent process. Okay, I'm using the latest version of tensorflow and what are, what is our learning rate? Well, it's going to take our learning rate is a parameter which we already define. Okay. And what is it going to do? Well, it's going to, and guess what? There's a, there's a function for this. Minimize our cost. That's what optimizer does. It minimizes our costs. And the more we minimize our costs, the more accurate our model gets. And, and why are we using the atom optimizer? Adam has become really popular recently. There are other types of optimizers we can use like add or Grad. Okay. Um, but for this case, we win. He just said, I'm going to use tensorflow to visualize these weights at the end. Yes. So now that we've created our optimizer enough cost, now we can evaluate our model. So what does this look like for a model evaluation? We want to say, well, what is a correct prediction? What does that look like? We want to make sure are correct prediction. We want to see,

Speaker 1:          40:47          okay.

Speaker 2:          40:47          Okay. So I'm going a little too fast. Me, calm down. Everybody take a deep breath with me, right? Uh, deep laggy breath with me. Right? Okay. We're calm everybody. It's all good. It's all good. Okay. So now what are we doing? We are evaluating a model and we're using to plus equal function, um, to man. We are going to use tensorflow as equal function.

Speaker 3:          41:19          You got the difference between the predicted value and um, uh, and uh, what uh,

Speaker 2:          41:35          the test data is. So we have test data and then we have our predicted value and want to want to see the difference between that. Okay. So that's what we're doing here. And security. What is our action, Chrissy? This, he's going to be a difference between our correct park, correct prediction and we'll whatever we bit, yeah, so we're going s so you have to ask a correct prediction and uh, and that value is going to be a float float 32. Okay. So now we can initialize our variables. So let's initialize our tentacled barracks. I guess what we always want to do this and tensorflow, we always want to, uh, we always want to, um, no video. Hold on, hold on. Oh, there's no video. Hold on. Let me fix that.

Speaker 3:          42:23          Boom. Boom.

Speaker 2:          42:27          There we go. Video is back. Okay. So we're, we're getting through this guys. We're getting through this. We're, we're almost there. Now. We're ready to train our graph and we're, or we're good to go. So we're going to initialize the variables and the variables are going to be, so how do, what are we doing here? Every time we initialize our tension flow glass graph, we want to initialize our variables, okay? Every time we wanted, we're going to do that. So now we, we've initialize our variables. We can launch the graph, the graph base, our neural network, but we full, we call it the computation graph, okay? So with, so how do we do that while we create a tensor flow session? And a graph is encapsulated by a session. Okay? So we're going to say Rhonda session, and we're going to initialize a session. Uh, we're going to initialize a session.

Speaker 3:          43:12          Um, uh,

Speaker 2:          43:15          okay? And we're going to say equals one. I said we're going to keep training until we reached maximum iterations. All right? So let's, let's go ahead and do that. So, so while, um, the step, so now in this while loop, we're going to say, we're going to pick the batch size, I'm going to say is less than the training iterations. Um, let's see, what is, what's happening here. We're gonna, we're gonna run our optimization and we're going to feed it, uh, the values and from our batch and our, uh, both of our batches and bar. What's the last thing? Our dropout or dropout. Okay. So raw and our dropout. All right. Um, so, okay, so there's that. And then we want to print out what's happening here on your printout, the iteration step and, uh, iteration step. Okay. So hold on. So now it's just gonna be a bunk of print statements and man, it is really laggy this time. And I'm like, okay. So anyway, uh, so, okay, so we have that. We've our model, so, all right, so let's see what this looks like. So Python, hold on, hold on. Maximum. This is what's happening here. I Don. Bye.

Speaker 2:          45:00          Right? So this is what it looks like. Um, I'm going to have to close that quote in the end. Okay. So here's what I'm not going to train right now. So that's going to add more lag because it's computation, but what it's doing is going to train on our, in our labeled data set and it's going to test on the, on the label on the, on the dailies that we, we uh, have his testing data. And let me, um, let me show you guys what I mean by, uh, hold on. I am an ist browser visualization. I mean I have a, I have a little, yeah, here's what it looks like. Um, hold on. It all right. Nothing's working. All right. Um, hold on. Okay guys, I um, we have, there's a lot of lag this time and so what I am doing right now is I'm going to close out the stream. I'm going to post the code on get hub and I'm going to do a last, hold on, let me get that camera, turn the camera off, turn camera on, make sure it's there. Hold on, hold on, hold on, come on camera.

Speaker 2:          46:24          All right, there we go. So, okay, I'm going to do a five minute Q and a now. Okay. And then we're going to close out the session and the next time there's going to definitely be no lag. All right, so go ahead and ask a five minute questions. Q and A. I'm going to get the, I'm going to get those. I'm going to get the code and get help later. Um, uh, let's see. Uh, my parents are from India. I was born in Houston, Texas. We need a video on Lstm for audio. When that's happening on Friday. Uh, rob, I have video on Lstm for audio happening. What graphics card do you use to train models? An nvidia titan x would be ideal. Um,

Speaker 2:          47:07          what color scheme are you using? And sublime text. It's the default one. What school do you stuck? What school you study? I studied at Columbia. Heard about Amazon go. I think it's a great idea. Well, which will be a great force for CV, computer vision. Um, the self driving car course on your destiny. Can you help me write a neural network to predict if a girl is into me? How would that work? Uh, you'd have to have a label Dataset of girls that are into you and then some of set of features of why they are into you or maybe maybe unrelated. That's actually a great quote. Maybe that could be an unsupervised problem, girls that are into you versus not. Well, you'd have to have a set of girls that you know are into for sure. And then a set of features. So you could learn those features or you could just hard code those feature that would be better.

Speaker 2:          47:44          Um, maybe the hair type, what they look like. Um, what are the important features, what type of girl is into maybe their personality. And then based on that you could kind of predict what other girls would be Ntu. Um, if you ha if you were able to extract those features, Facebook data would actually work for this. Um, our region, San Francisco one day. Great. How should it started as a newbie and AI? Watch my videos. Uh, what company do you want to work for? Uh, no company, but I want to, I love open Ai. I love them. And I and I and maybe Google Age, sex, height. I'm 25. I'm male. I'm six feet video on string classification. I have one, I have several actually, but I'm going to, I'm going to do more, um, speak something for school, uh, and start downloading code for all of my videos and start writing them locally on your machine whenever you have spare time and try to get your professors to learn to do more machine learning stuff in all of your computer science class.

Speaker 2:          48:44          But she wanted, it can be applied to almost every field of computer science. No, not almost every field of computer science, c plus plus or python, python and um, no video. Um, how long do you sit on the PC and do research? I'm on my computer all day. Care Ross is a great thing, is a great library for starters, but I would prefer TF learn for starters. Okay. Um, all right. So what type of music you like? Mostly hip hop, uh, Tupac, Kanye West, uh, a lot of electro and a, yeah. What's your favorite project in ml? I like, um, man, uh, I like the synthetic gradients paper that, uh, deepmind came out with that was really like mind blowing to me. Okay. Uh, sex or python. Oh man, that's a hard one. Uh, sex is so like, it just happens then it's gone. So I would say python if I had to pick.

Speaker 2:          49:36          I know that. So anyway, I love terminal. I love seeding into directories and making directories. I just love being in a unix shell. It's so awesome. And, uh, anyway, what is my motivation to continue learning? Machine learning? Machine learning is the solution to all pro. It is going to help us solve everything. It is the most important thing in the entire world. Okay. Um, we have to solve intelligence guys. Uh, uh, okay. Where should I start? Learning neural networks. Intensive flow. Start with my videos and then also tensorflow hasn't great tutorials on their website. Um, do I have a girlfriend? No, I am newly single man. I get this question a lot. Uh, but I'm not actually looking for a girlfriend, so I'm, my girlfriend is python. All right, so open AI is how I found you. All right. Anyway, thank you. Please do a video on a live video on a current nights.

Speaker 2:          50:21          Okay, that's it. Okay, cool. I will, I love you guys. And next time there is not going to be la as much lag. I promise you I'm going to, I want to do these live sessions every week, okay. Uh, and going to, all right, and thank you guys so much for showing up resellers song. I will freestyle. Okay, here I go. Here we go. What is the topic? Coffee. I love coffee. I do it back every day. I'm not me. I'm somebody yells, my mind is so free. I seen machine learning because I want to be something more than me. I want to go and flow and so high in the sky that my mind is so fried from doing this live. Okay. So that's my, that's my freestyle. I love you guys. I've got to go for now. I've got to not have a laggy connection, so thanks for watching. Have you guys.