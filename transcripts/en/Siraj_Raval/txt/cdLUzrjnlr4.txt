Speaker 1:          00:04          Oh, a world, it's Suraj and today we're going to code up a three layer recurrent neural network that can predict the sum of two binary numbers in under 80 lines of python. But first I want to show you guys what a genius I am. I'm going to recite the alphabet backwards. Ready? Okay.

Speaker 1:          00:24          Okay. Z Y for dammit. The reason it's so hard to repeat the alphabet backwards or sing songs backwards is because we learned these concepts as a sequence and when data is represented as a sequence positional memory matters. So let's say we wanted to train a neural net on a video of a top spinning on a table. Each data point would be a frame of the video. So if we want it to predict where the top would be in the next frame, it would be super helpful to know where the top was in the last frame. Sequential data like this is why we built recurrent neural networks. Plain old neural nets expect fixed size inputs and half fixed size outputs to the state of the hidden layer is based only on input data. As it all flows in one direction or feet forward in a recurrent neck. We incorporate the concept of memory in order to do this at each time step we input a combination of the input data and the hidden layer at the previous time. Step recursively. So if we try to predict the next word in a song that contained phrases like I like eggs and I like toast, brilliant songwriting, I know and our network tried to predict the next word. How would it know what follows? I like it needs to know what part of the song is in and hidden recurrence helps it do that.

Speaker 1:          01:34          We're just fusing copy to help us copy things and num Pi to do math. We're going to start out by writing a sigmoid function. This function will help us map any value. We feed it into a value between zero and one. It helps convert numbers to probabilities will also write a function that generates the derivative of the sigmoid. The derivative is the slope of a sigmoid at a given point. The derivative will help us to calculate our error during training. Later on. Next we're going to create a lookup table that maps integers to their binary representations. We'll initialize it as empty and set a max length of the binary numbers we'll be adding. Then will compute the largest number of possible to represent with the binary link we chose eight. Our lookup table is then filled with binary numbers so that we can easily return the binary value of any integer we input.

Speaker 1:          02:17          Now it's time to initialize our input variables. First we'll set our learning rate, then our input dimensions. Since we will be adding two numbers together, we'll be feeding into bits strings, one character at a time, so we need to have two inputs to the network. One for each of the numbers being added, hidden, dim defines the size of our hidden layer that we'll be storing our carry bit. And lastly, we define our output size. Since we're only predicting one, some will say to one well initialize three matrices of synapses. The first synapse matrix connects our input layer to our hidden layer, so it has two rows and 16 columns. The next thing APP's Matrix connects are hidden layer to our output layer, so it has 16 rows and one column. The last snaps connects are hidden layer and the previous time step to the hidden layer in the current time step, it connects the hidden layer in the current time.

Speaker 1:          03:01          Step two, the hidden layer in the next time step. Well, just to keep on using it, so 16 rows and 16 columns. Next, we need three variables to store the synapse updates for each of the matrices. Once we have enough sinaps updates will update the matrix seats. All right, let's start training it to kill humans. I mean, uh, predict binary songs. Yeah, we're going to iterate over 10,000 training samples. We'll start by generating some random addition problem. We'll initialize our first variable to use our lookup table to find the binary form of it and story in a, then we'll do the same thing for B. Well, some both integers and get the binary and coding for the some see what this store that neural networks predictions and an empty binary array, which we'll call d will reset the air measure to zero every time. This will help us see how each new bit of data contributes to our predictions.

Speaker 1:          03:44          We then initialize two lists. They will keep track of the layer two derivatives and layer one values at each time step and since time step zero has no previous hidden layer, we initialize one that's off. Okay. Now we're ready to iterate through the binary representation or generate an input first. This is a list of two numbers, one for May and one from B. Y is the output and is the value of the correct answer, either one or zero. Now it's time for the real magic step of neural networks. We construct our hidden layer by propagating our input to the hidden layer. Then we propagate from the previous hidden layer to the current hidden layer. We some both of these two vectors and passed them through the sigmoid function. After both the previous hidden layer and input have been propagated through their various matrices. We some the information well then construct our output layer.

Speaker 1:          04:26          It propagates the hidden layer to the output. In order to make a prediction, we can't forget to compute how much the prediction missed. We'll store the derivative analyst holding the derivative at each time step. Then we'll calculate the sum of the absolute errors to track propagation. We'll end up with a some of the error at each binary position. We'll then need to round the output to a binary value in storage in the designated slot. We copy the layer one value into an array so that at the next time that we can apply the hidden layer at the current one. Okay, so we've done the forward propagation for all time steps. We've computed derivatives at the upper layers and stored them in a list. Now it's time to back propagate. Starting with the last time stepped and propagating to the first. We indexed the input data just like before, and select the current hidden layer from the list and the previous hidden layer.

Speaker 1:          05:09          Then the current output layer, it's time to compute the current hidden layer error. Given the error at the hidden layer from the future and the error at the current output layer. Yay. All of our derivatives have been back propagated at this current time step, so we can construct our synapse updates. We're not actually updating them yet. Those ones back prop is done. Then we update our synapses using the learning rate. We initialized then MTV update variables. Let's see some results. The predicted value is pretty off the mark at first, but with each iteration gets better. If you want to learn more about recurrent neural nets, check out the description for links and hit that subscribe button. For more programming videos. For now, I've got to go synchronize them. Go routines, so thanks for watching.