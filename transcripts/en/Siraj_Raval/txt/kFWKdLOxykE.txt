Speaker 1:          00:00          Oh world, it's Raj. And today we're going to learn how to use a pretrained tensorflow model on an android device. So this applies to any device that runs on android. We would train the model on our desktop or on our laptop or on the server, and then we would use that pretrained model on our mobile device. So there's no training that would happen on the device. The training would happen on our bigger machine, either a server or our laptop, but then the inference, the actual making of making predictions that would happen on our android device. So that's what we're going to do today. And uh, we're doing it for a handwritten character image classifier. So the user would, here's, here's what it looks like. So you draw an image and then you hit detect and it's going to output the result. And the result is the probability of what that class is.

Speaker 1:          00:51          It's a, it's a multi-class classification problem. So it's a very, very simple APP. And the point of this is to learn how to use tensorflow with android. Very simple use case. Right? And there are a couple other examples that I'm going to give, but in this example, uh, it's just as simple as it gets. So it's a great way to get started. You can use this repository that I'm going to link to that I'm going to be looking at here as a base for any other type of classification project that you want to make. Cool. So that's what we're doing. We're going to write a handwritten character image classifier. The user draws it with their hand and then he hits the tech and it's going to classify it. And doing this on a server or on your laptop is good because training is a very computationally expensive process and it's a very energy intensive process, right?

Speaker 1:          01:39          We can't just plug our phone into our, into the wall all the time, but we can do that for our laptop or desktop. So that's why we do it on those machines. But in France we can easily do that on our phone. So what I have here a is this little architecture image of what this looks like from a, from a high level, we've got to, um, we've got two repositories here. We have the android Sdk, and then we had the android n d k. Right. So what's the difference here? Well, the STK is a standard developing kit and the MDK is the, uh, native development kits. What do I mean by that? Well, the SDK has written in Java and the Ndk is written in c plus. Plus we use the Sdk as an interface to code everything we want in Java at a high level.

Speaker 1:          02:25          So that's all of the things that are native to android activities and fragments and image listeners. So I'm going to actually go into quite a bit of detail on how android works. I'm not sure how much, you know, I've never really talked about this stuff. So rather than being a fly by, I'm just going to go as detailed as possible and hopefully, you know, you don't already know all this stuff. Okay. So I'm just going to assume you don't know much about android. So, uh, right. So android has a very specific way that things work. This is very specific way that objects interact with each other, right? You have these activities and then you have these fragments, image listeners and all of these. And then on click and create all of these event listeners for how uh, you would interact with an android APP. It's, it's specific to a touchscreen like interface.

Speaker 1:          03:10          And so that's what the Sdk is good for it. You can do all of that stuff in Java. But what the NDK is good for, it's for looking at or is for working with c plus plus files and what is tensorflow written in the core of tensorflow. It's written in c plus plus. So that's why we use it so we can look so we don't have to deal with c plus plus. That's why we had the SDK. But under the hood, the SDK talks to the NDK, which is written in c plus plus. So what happens is you see these two arrows. So this Arrow Arrow one takes the image a bit map image@theuserdrawsandsendsittothistensorflowjni.cc class, which is like the main tensorflow. See a wrapper for android written in c plus. Plus. It takes that image and it, it, it turns it into a tensor. So it resizes it into a tensor, gives it to the model of the pretrained model, which is a pro buff file, which we'll talk about more, but as a pretrained convolutional network.

Speaker 1:          04:06          And it's going to output the prediction, which is a tensor to back to that t plus c plus plus file. And then that CPI plus file is going to return a list of probability values in the form of an rea straight to our SDK. That is our Java class. And we'll talk about all of these classes in order from the highest level to the lowest level. Uh, but that's, that's essentially what it's doing here. Okay. So that's the example that I'm going to use. But there are several examples of how tensorflow can work, can work with android. So this one is probably the most well documented one that I've seen, but it's called android tensorflow, flow machine learning example. Basically it is a general purpose classifier. So if you have a device, you can plug your device into your computer with android studio. And then in real time is just going to classify everything that you look at this wallet, you know, 29% wallet.

Speaker 1:          05:00          That's a pretty bad classification plan, present wallet. But Hey, uh, 20% ballpoint pen. And the reason it's so well documented is because it comes with an associated blog post, which has got all of these pointers to work with Bazell and uh, you know how to set up your workspace so that it's pointing to the right version of the STK and the NDK. Uh, so yeah, all that stuff. But the reason I'm not doing that is because it is more complicated than what we're going to do a, but that's a good next step to go to after you do this. Okay. So that's the image classifier example. There's another classifier which is a little more detailed. So it does the same exact thing as the first, but once it's classified the image, it then queries both Wikipedia and a Wolfram Alpha for some more data about whatever object it is.

Speaker 1:          05:47          So it will classify an image like let's say it's a lemon, and then use the lemon as a parameter when it makes an http request to the server and then it returns a short paragraph description of what a lemon is. So you can see how this can be pretty useful. For now we're going to need the apps that are actually useful to day to day life. Uh, right. So there's that. And there's one more that I wanted to talk about. They're all really classifiers cause you know you have the camera on the device. So why not just classify everything? I mean there's so many use cases for classification that haven't even been attempted before. It's such a, it's such a broad application, right? So this is guest sketch and there's actually a web app version of this too. But you would draw something and then it would classify whatever it is.

Speaker 1:          06:26          So obviously you would need a model that's trained on a lot of different images. It's not just m and ist, but like a lot of different images. So that's when you do something like this. Inception would be a great model to use for this pretrained inception. I love how they named it inception even though it has nothing to do with the movie, but uh, it works like a dream, so, okay. That was anyway, where were we? Okay, so we're talking about installation. So that's what we're first going to talk about. So again, remember I'm assuming you don't, you've never used android before. Uh, so if you have used android before then just skip ahead. Like I'm going to estimate four minutes, but okay. So the first episode, download android studio. So android studio has everything you need. So you know, there are several things that you could use with android.

Speaker 1:          07:08          You can use eclipse, the ide, we could use android studio or you could just use this with terminal and a text editor. But I'm saying that you should use android studio because whereas before it was super buggy, like when I was using it a lot, a year and a half ago. Now I just downloaded a, and I've been looking through a lot. It's actually really, really good now. Like it takes care of a lot of dependency issues for you inside of the ide, which is amazing. Uh, so you can download it from here. It's got, it's, it's available for all the operating systems. And so these three steps are the next three steps. And actually if you're using android studio, you can do it all inside of android studio. You don't actually have to do this from command line. This is if you're using eclipse or terminal and a raw text editor like sublime or something.

Speaker 1:          07:57          But you would download the STK and then you download the NDK. Okay. As well as the build tools. And once you have those, you would point those to the right. You would point your workspace to those, to the location of both. So your app knows where to, uh, where to find these repositories. Okay. So that's what we've got there. And so in my grade will file, I've got this, you know, the target Sdk version so that I know where that that's going to be. Okay. And so let's see, what else do we got here to properties, local properties. Cradle is the build system for android by the way, UC use Maven, but now it uses great old. So yeah, that's, those are the dependencies that we would use. And then once you've downloaded these dependencies, then you want to train the model, right? So remember we train the model on a desktop or on a server and we do that in python just like you would even if you weren't using android, you would first train the model there.

Speaker 1:          08:59          So let's go over that. What the python file looks like at a high level and then we'll keep going with the android app and the code, the Associated Code. So for the model in Python, let's see what we got here. This is what the model looks like. So this is the care os version. There are two versions. We've got the care os version and the tensorflow of virgin. But I'm going to go through the kiosk version really quickly. Okay. So we're going to import tariffs and Python. Okay. So once we have those, we're going to build a convolutional network. A convolutional net is used to detect images, right? Humble show nets are used for any kind of image classification or even generation. We can use convolutional nets for image generation. We just reverse the way things work. Like we would use gradient ascent or we would slice off the last half and then add some kind of ad is to cass tech note in there.

Speaker 1:          09:49          So it generates, um, some novel data. We would add a random variable in a see variational auto encoders for how that works. But you can generate new data types. And also obviously generative adversarial networks, which I've made videos on both of those topics. See my you Udacity, deep learning and a degree course all on youtube. Okay. So a couple of methods here. The first one is to load the data. And this is just, you know, standard reshaping. We have to remember we have to vectorize our input data. What do I mean by vectorize? I mean, we used to take our image raw image and then convert it into a tensor format so that we can feed it into our model. And so in this case, it is a four dimensional tensor, right? So the length with, uh, the size and the, the, the depth, which is, which is going to be one, right?

Speaker 1:          10:34          Because it's only a single layer image. So we reshape it using numb Pi's, native reshaping functions. It's a float 32. So these are float 32. These are 32 bits, a pixel values, who an image and out of 255 possible, uh, and then we got out of 255 possible RGB values. And so once, then once we have that, uh, vectorize, we'll split it into a training and a testing set. And that's it for load data. And then for a model, great thing about [inaudible] is it's only a few lines and we can build a convolutional network just like that. It's going to be a sequential model with three convolutional blocks. So we do perform convolution, which is essentially like taking a flashlight over a filter of images and looking for what sticks out. So in this case it would be the Mni is t the the actual digits, right?

Speaker 1:          11:21          Cause they're black on white background. So it would detect, hold that. And then we perform pooling. So pooling is an operation where we look at an image like an array. So let's say you know, we've got like four different, we've got a square, uh, where you know, each of the values in the array represent is a different number. And then we, for Max pooling we would take the maximum value from that square. So it's, let's say we got like four, nine, 12, 15 we would take that maximum value, which is going to be a, so the Max value and that's you know, a post you opposed to average pooling or one of those, this is just a, a more often used pulling strategy. Okay. So I actually, I already explained it so I don't need to go that, go to that. Okay. So we do polling and then we do it again, again, perform convolution. And what this is doing is every time it's creating more images and smaller versions of that image. Right? And

Speaker 2:          12:17          Yeah,

Speaker 1:          12:17          we keep doing that until we're at the very end. And now we've got this very high dimensional four by four by 256 dimension tensor. But we need, and how put probability, right? We need an, we need a prediction die. You're already set up prediction values for the probable classes multi-class classification. So what will we do? Well, we have to flatten that tensor into a, a one dimensional array, which is the list of probabilities. So we'll, we'll hit or flatten it and then we'll add to fully connected layers to which are the dense layers. And uh, the last, uh, activation will be a softmax function. We're just going to output a single, uh, value for each of the class outputs. Okay. So that's it for the model and then, yeah.

Speaker 2:          13:06          Okay.

Speaker 1:          13:07          Okay.

Speaker 1:          13:09          Thanks Internet. Okay. So that's it for the building, the model part. And then for training with care, Ross, remember it's just two lines. We compile the model and by compile we defined two main things. We define the loss function that we're going to minimize because it's a multi-class classification problem. We're using categorical cross entropy and because it's an end, what are we going to use to actually minimize that loss? Or we're going to, we're gonna use an optimizer, a form of gradient descent, which I have a video on called evolution of gradient descent. And we're going to use add a Delta, which is an adaptive learning rates gradient descent strategy. Okay. And so once we have that, then we're going to actually train the model, which is, which we do with one line of fit function. So in tenser flow, this would be the akin to defining a session if finding the grass and then iterating through defining the for loop where we iterate through all the batches.

Speaker 1:          13:58          Here we just defined the number of batches as a parameter to the number of epochs who do it all in one line and then the training data as the input. Okay. And so that's the first key part is training. And so the other key part is exporting the model. So once we train it, we have to export it. So we have to export it into a Prodo buff file, write a PD file, PB file is a saved, uh wait file that are a model lives on that. That is our train are trained model, right? So once we've tried on model model and we save it as a pro but file so we can then use it later. And what is the protocol file? Well it is a serialized version of our model, right? With all of the Lord waits. And so we do that in this method, right?

Speaker 1:          14:43          And this is the way to do it for android. Okay. So well actually this, this can apply to any type of use case but it is a general purpose saving function, right? So definitely saved this function for later use cause you're going to use this a lot whether you want to use your model on a server with tentraflow serving, whether you want to use it on Android, ios. I know Ios has the, you know, the new, um, what was it called? Cornell, which is really cool and I'll talk about that as well. But android is more used. So I'm going to talk about android first. So, right, so we write the graph. Okay, we write it and when we, the right graph function is going to help us define what we're going to call it and then we do the actual saving and then we freeze it.

Speaker 1:          15:27          And what, what do I mean by freeze? Well, freezing means we're going to save our weights wherever they are. So our weights are at a certain, you know, they're all a certain, they all have certain numerical values, right? So we're going to freeze them at those values. That means it is, it's essentially like saying a declared these weights as constant, right? So you can't change it that they are immutable variables. And then so once the weights are saved, we're going to open it again and then resave it, uh, using this optimized for inference function, which saves it as float 32 values, which are, which are the type of values we need for android devices. So that's why we save it, open it, and then re save it. And then we'll convert it into flow 32 and then receive it. So we do it twice. Okay.

Speaker 1:          16:10          And we on the second time we save it. We, we have it serialized two string values. Okay. So that's it for that. And then for our main function, that's where we call all of these functions that we've just defined. We make sure that there is no output folder and if there is a, if there's not, then we create one, we load up our data, save it in these variables, build the model, train it on the data that we just load it up and then export that train model and then export that train model using the tension flows, native saver function and the model has parameters to the uh, ouch directory. And then we can use that same model in Android, in an, in our android happened. Okay. So that's it for that part. And so we've got that part and now we're going to look at this bar.

Speaker 1:          16:58          Okay. So let's assume we've trained it. We trained our model, it's, it's saved. Now what do we do? Well, we've downloaded our SDK and our Ndk we need one more thing. We need something tensorflow specific, right? The SDK and the NDK or just a part of android. They have nothing to do with tensorflow. Remember up here, this, this, this part right here with a tensorflow underscore Jay and I that cc, that is what Google made. Google made this themselves so that we can use tensorflow functions on Android, on device. So that's what we need to download. And the way that we download this is to, is to download an AA. Our file that has an android Arcov are archived file. So what is it? This is a way of archiving that dependencies, all of those that we need into a convenience, a little rapper, like one file that we have to look at, but under the hood, inside of this file or all of the uh, c plus plus files.

Speaker 1:          17:53          But we just don't have to look at them. If it wasn't wrapped like this, then we would have to look at all the c plus plus files. And we would use a tool like Baisil to build all of them from source. But the great thing about Aa harm is, is that it is compatible with the great old build system that is built, that is made for android. So we can just download it and then import it into android studio and it's going to know all of those files and it's good to put them in the right directories for us. So, so there's a lot of great magic happening inside of Android Studio, which is actually very useful, right? It saves time and energy. And I've linked to a tutorial on how to import an AAR file, which is very similar to a jar file. So it made it and we would use a lot more, we would use jars a lot more.

Speaker 1:          18:39          But in a grateful, we use AAR files a lot more. So yeah, check out this tutorial on how to do that. It's very simple. Okay. So once we've done that, we've got our tensorflow specific dependencies, we've got our SDK or Ndk, we've downloaded this repository. Once we'd done that, we could just literally compile it and run it on the simulator or the device if you have one. One of the reasons I chose this specific repository to demo is because it doesn't require a device, right? Not everyone has a device. You can do this in the simulator because all of these classifier examples that I talked about, like this one and this one,

Speaker 1:          19:20          and this one require you to have a camera and you can't use a camera on a simulator, right? You need the actual device. It will be cool if you could use a Webcam and, but you can't. So this is the most accessible a repository that I could find. Okay, so let's go over the steps in this tutorial. So what are we going to do here? We've talked about the python file and how that works. Now let's go into the android specific codes. So we're going to go into android studio and go from a high level to a low level. So we'll go, we'll, we'll, we'll go down two paths. The first path we'll go down is the actual visualization path. So we'll start at the main activity and then we'll iteratively go down the chain of dependencies, draw, model, draw a render, and then drop you, which is the lowest level of what's happening. And that's the draw specific code. And then we'll go over the machine learning specific code. So that is attentive low classification class and in the classifier class, which it calls as the lowest level, the classification class. Okay. So once we've done that, then we'll be able to compile and run our model. So let's, let's start, let's start off here, right? So here's the model link to it. It's in the description. Check it out if you haven't yet. Uh, okay. So let's see here. We have

Speaker 1:          20:36          this main activity. So, and by the way, I have commented every single line in this code. So there is no excuse to not, uh, learn how this code works. Even if you don't listen to this video, check out the link. Every single line is commented very well. Okay? Even the dependencies. So what we're going to do is in the main activity we're going to, uh, we're going to define the main activity. So that sounds counterintuitive. We're going to define the only view that the user, uh, deals with, right? Which is this one in the XML file, right? This is it right here. So the user will, in this box right here, we'll draw with their finger, the number, and then he'll hit, they'll hit the test button. And then in this text you see the classes here we've got, we've got three classes we've got to draw you.

Speaker 1:          21:34          That's where the user draws the number. And then we've got this button, this clear button, which will, which will clear the canvas if we want. We've got this detect button, which is going to perform inference. So it's going to take that image as input, send it to our tensorflow model, through the tensor flow tension flows through the android Sdk, which can access the tensor flow c Plus c plus plus file on device. And it's going to return the outputs and it's going to return that output and it's going to output it inside of this text to you. Okay? So that's, that's like what the visualization looks like. It's a one view app. Very simple. And so, and here are trained models, by the way, write this. We've got to train models and not just to train models. We have a set of labels, right? Zero through nine so that we know what are the possible class labels cause this is a multi-class classification problem.

Speaker 1:          22:29          And so we've got to train models, one for Ken Ross and one for tension flow and we'll be using the care os one just because, uh, why not? But the tensor flow one would probably be, will be more accurate, but it takes longer to train. Okay. So let's go into this main activity and look at what the, what is happening here, right? So we're going to import a bunch of, um, drawing classes. By the way, the activity is how the user interacts with the android architecture. With the android APP. Almost all activities interact with the user. So the activity class takes care of creating a window for you, which you can place or Ui, which in which you can place your Ui with the set content view. When we initialize it in the, uh, on create function, right? Right here. Okay. So you know, if the more complex your happies the more activities that you would have, we've got one activity and that activity is drawing the number and hitting the teks or hitting clear and repeating that. That's the only activity. And in this activity, all of those actions are encapsulated. So we've got that. And then, uh, we've got a bunch of other dependencies. Each of them is going to be used for classification as well as drawing elements, right? So it's split basically the dependencies either draw what we're looking for or let me make this bigger.

Speaker 1:          23:51          Can I make it better? Yes, yes I can. You can do anything that you dream

Speaker 1:          23:58          unless it's crazy. Well, even then you could, uh, with the right amount of computation and training. Okay. Okay. So let's see. Okay, so now let's look at the main activity. So for the main activity, we've got our UI elements, right? Those buttons that I just showed you as well as the views, right? We've got it set up views and got the UI elements and we've got our coordinates. So let's talk about each of these in an order. Okay, so the pixel width is the width of our image 28 by 28 pixels. The button. All right, we've got two buttons. The class button is the detect button. The clear button is a clear button. The text view is what app, which shows the output. And the m classifiers is an array list, which is an array with some, you know, it's, it's basically built on top of the array class.

Speaker 1:          24:44          Yeah, it's a, it's a list of so you can perform, you know, getters and setters on an array. Uh, and so it's a list of classifiers ever we're going to use. So the reason that they list is because we're going to use to classifiers and this and this repository, not just one. We're going to use two, one for tennis as low and one for care Os. So we can compare the results, right? So that's why it's an array list. And then we've got our views, right? So we've got our draw view, which is the big view and we've got our draw model, which is the smaller version where the user actually draws. And then we've got our set of points and these are coordinance that the user is, is tapping that the user taps. Okay. And then we've got the private set of coordinates, which is like a copy of those.

Speaker 1:          25:25          But internally that we can then modify inside of this class, but they're not exposed to publicly. So other classes can't interact with them. And this is just good programming practice too, to have your doc to have your variables that are not going to interact with other classes, set them to privates because you know, otherwise things can happen that you don't expect where you're calling a class but it's not. And then it's going to modify some variables. But because you haven't said it's private, it's going to, yeah, because you haven't set that some variables to private. It's going to modify those variables and you don't want that to happen, so you have to set them to private. Okay? So we've got that and now and our on create method, which is like the basic building block for an activity class, it's going to run only once for the entire lifecycle of the activity, right?

Speaker 1:          26:12          And we've got a bunch of these activities, specific lifecycle methods, but the first one is on create and it just runs once. Okay. So we've got that. And so now let's go ahead and create an instance for this, for this main activity. And once we have that, we're going to say, okay, so let's get that drawing view. So we, we define this in Xmls, we're going to call it by its ID that we define an XML store in drawing. You do the same for the model. And then we're going to set the model. So we're going to take that view and set its model to that draw models, super missile, encapsulating it inside of that view. And then we're going to create an entouch listener to activate whenever the user taps it. And then, uh, we're also gonna do the same for the clear button as well as the detect button.

Speaker 1:          26:53          Okay. And then we're going to do it for the view. So this is, this is just us wiring our XML to our class, our program out of class so we can then manipulate these values. And then we've got a couple of functions here. We've got our own resumed function, which is, you know, assuming that the user has gone, gone, gone out of the APP, it's going to call this and call these methods, which basically it's a way of saving our state and making sure that our APP doesn't crash. And these are just, uh, basically like, um, they're like exceptions. They're like android wide exception rules. So it's like saving where we are. And so we've got that. And now let's load our model. So this is going to create a model object in memory using the same tensorflow proto buff model file, which contains all of the learn weights.

Speaker 1:          27:39          So it's going to take these weights and it's what you saved them into this m classifiers array list array. So we can then use them. So it takes those saved weights in local storage on android device and saves them into memories. So it's an in memory array list that we can then, um, perform inference with. And so we have a try catch function for that because if they're not there, then we have to say, hey, there's an error initializing these classifiers. Okay. So then we've got this, uh, on click methods. So this is whenever the user clicks on anything. So we have a bunch of, if then statements to define where if the user clicks what happens. So if the user clicks the clear button, then we have to clear the model, reset it and invalidate any new entries the user does. And then we set the text to empty like nothing is there anymore.

Speaker 1:          28:23          Now if the user clicks the, uh, classified button, then we're going to take the array of pixels, right? So we get the pixel data of everything that a user has drawn up to that point. And we store it in a pixel array and then we say, okay, we'll initialize a string is empty because we're later going to fill it with the help of prediction. And then we're going to say, okay, so for each of the classifiers, for both tensorflow and chaos, recognize. And so I'll, I'll, I'll show you what this recognized function does, but recognize or classify what those set of pixels bar, you know, a zero, a one, a two or three. What, what is this? What, what did they use or draw and then take that label. And if it's empty, then I'll an NEC, um, a question mark. If it's not empty, then output the label and set that into the text.

Speaker 1:          29:07          Phew. Okay. So that's the highest level of what is happening. And so now we've got this entouch method, which when the user moves their finger, draw a line accordingly in that direction. Okay? So, so we take the action and we store it as an integer. And the actions all have predefined in, uh, like in memory or not in memory, but, uh, that, that are a part of android. So check this out. So if the action that the user has, so basically if the user has touched the screen, which we defined with motion events, action down, which is a zero. See these are all predefined and Androids Sdk. So basically if the user has touched the screen, again drawing the line, but if a user has moved, if the user has started moving, then uh, start then start actually drawing the line in a different direction. So touching it just initializes the line drawing and moving. It actually begins drawing the line more than just a dot. And then if a finger is lifted, action up, then stop drawing.

Speaker 1:          30:09          Okay, so then, uh, these are, these are the implementations of these functions that we just find the, the price touchdown as well as the press. Such move. So for the press touchdown, we get the coordinates of wherever the user has touched, calculate those positions in memory, and then start drawing the line using where we were before and where we are now. And then in this main drawing function, this is when the user is actually moving their finger. Take both of those sets of coordinates, where we were before and where we are now. And then draw a line between those two points and keep doing that every time. Okay. So yeah, and then for press such up, we just take draw models and line function, which is going to help us stop drawing the line. Okay. So that's the main activity. And so notice how we had the straw model function that is continuously used all over the place. So let's look at what's happening there, right? What is happening in the straw model function? Well, in the draw model function we are,

Speaker 1:          31:20          okay, so we are drawing the model. That's what we're doing in the draw model function. So it's a collection of getter and setter functions that we can use later on to draw a character model. So we'll initialize it with a line element, which is a, it's just two values, the x and y coordinates. And then we'll have an internal representation as well to manipulate those values. So we'll manipulate them and then we'll return those manipulated values back to the public x y values. So then we can then draw them on XML in the XML file. So for a single line, so we have a private line class align consists of elements. Okay. So we have a model, a model consists of lines and align consists of elements. Okay. So what is uh, so let me talk about each of these. An element is the line from point a to point B, right?

Speaker 1:          32:07          So there are a bunch of elements in a single line. Align is everything that you draw from when you start to when you finish. So aligned to be an entire number. Or it could be like, you know, you draw the first, Oh that's aligned and then you draw the rest of the, you know, if you're trying to draw a six [inaudible] oh and then you are the, you know, the curve. Both of those are two lines and inside each of those lines or set of elements right? From point a to point B. So that's, and then all of that is a model. Okay. The draw model. Okay. So model line element. That's the of of what this looks like. Object hierarchy. Okay. So we've got that. And so now let's talk about these other functions. So this is our current line are the width and height of, of that line, height of that line.

Speaker 1:          32:56          And then we have the list of lines, right? So inside of a model there are lines and inside of the lines there aren't elements. So we're defining a raise for each of these objects. And so we've got to set a getter and setter functions for each of them. Okay? So start line and line, add line elements, get the sides of it, get the line, and then clear. Okay, so that's it for our draw model. And then we've got draw a renderer. So we're, so we're going down even lower level. So we're going down the rabbit hole of lines. Okay? So for our draw a renderer class, we're going to say let's draw all these lines to a canvas. So we've got our render model. And so this is just straight up drawing functions. So this is the straight up drawing function. So all of those functions that we define in this main activity as well as Deidre model, they are high level functions.

Speaker 1:          33:48          They're not actually drawing them to XML. This is how we're programmatically deciding what drawing looks like. This is how we decided with drawing looks like. But then in this draw a render function, we take all of that programmatic logic and we apply it to XML. So we're going to directly manipulate XML elements in this class or so we're essentially drawing lines on canvas. So we'll first set this set anti alias, um, function to true because we want to minimize distortion artifacts. It's just good practice to do. And then we get the sides of the line to draw. And then given that side, we create a for loop. So get the whole line from the model object, set its color to black and then get the first of many lines that make up the overall line. So we get the element size and we say that's a single element.

Speaker 1:          34:35          And if the element size is less than one, that means that the user hasn't drawn anything. So just skip it. But if it, if these, if the user has then store that in an element object, get the coordinates of, of the, of that element object, the x and y coordinates. And so for each coordinate in the lines of for all of the elements in the line, we have a set of elements in an array, right for a single line, each of those elements and the coordinates and then draw them on the canvas using where we were before, where we are now. And then this paints a object which is going to create, make those, make that drawing black. And then we, and then we make the new law that we make the new coordinates, the old coordinates in this last x and laughs wife function.

Speaker 1:          35:15          And so that's draw renderer and now we'll go to the lowest level for drawing, which is draw a view. So for draw view, I mean this is almost like unnecessary to go over because it's so low level I, this is just one of those classes that you should just import and like not even worry about because there's not really any machine learning specific knowledge here. But I'll go over anyway. Just had a high level. Uh, we want to first a reset the use, so it's empty to empty the drying. So we'll hit this reset function, which is going to take that bit map image of whatever the user drew, drew, and then empty, empty out all the pixel values so they're not black. So they're white again. And then, uh, this is the set up function. It's a private function where we define the views size, the size of the models, bitmap, and then scale it.

Speaker 1:          35:56          So it's properly scaled for whatever android device that you're on. Again, this would be automatic, like ios would do this automatically, uh, but, and when the user begins drawing, initialize a model, render class and draw it on the canvas. So this is where we, this is where we initialize that draw a render a class that we just talked about. This is where we take it and we, we apply it. And so then we draw, it's whatever, whatever it drew, whatever it says to draw, we then draw it. And then we kept, so the rest of these are basically calculating the position of the finger on the screen. So very, very detailed. X, y coordinates of where your finger is on the screen. And then we draw the canvas using the bitmap, which is the image that we just drew. We release it, which is just memory, a memory management. And so we released it from memory using this recycling function because android uses garbage collection, which is a type of memory management technique. So apple uses retain cycles and android uses garbage collection.

Speaker 2:          36:59          Okay.

Speaker 1:          37:00          Two different methods. Garbage collection is happening. I'm refreshing my mobile knowledge here. Garbage collection is happening at runtime as um, automatic reference counting on high west is happening at compiled time. That's difference. Uh, so which one's better?

Speaker 1:          37:22          There are pros and cons of both. That's out of the scope of this. This is about machine learning, but if you guys want more, let me know of this stuff because I've done mobile development quite a bit before. Okay. So, uh, so then we get the pixel data and then we, for the potential floats input. So this is how we define the pixel. You know, every single pixel, we have a for loop, we're t, we're going to every single pixel and we're storing it in that pixel array. So for all of those other functions, we can use that pixel array for both draw, model, draw renderer, uh, and even this, this, this function drop drove you. Okay? So that's for all of the uh, view logic. And now let's talk about the machine learning specific logic. So it's are at the highest level, which is our tensorflow classification class, and then our classifier, and then our classification class. Okay? So for tens of classification we'll say,

Speaker 1:          38:16          how does this work? Okay, so why don't you have android native native android from a bunch of standard hint android development kit Sdk, Sdk, functionality is being, is being important here. And then we've got this interface. So remember that c plus plus file that I talked about right here, this is what we import it. Okay? So this is coming directly from Google. This is what they wrote as the bridge between tensorflow c plus plus an android. We import it right here. Maybe by Google. Okay. And so what we're going to do is we're going to implement this class. So this class that we're going to create is an implementation of classifier. Wait, what is classifier classifier is the public interface for how our APP interacts with the classifier. Okay? So that so publicly we give it a string. So whatever we name it as well as the recognize function, that is the one public function that we are making visible to all the rest of the APP. The recognized function, which just going to take the pixel array, has input of the image and then return the classification. And like the output, this is a zero 26% chance, zero. You know, for what's a hundred minus 26 74% chance, not zero or at 74% percent chance one or whatever it is. So

Speaker 1:          39:37          yeah, so it's an implementation of that. So that's the interface for this class. Okay. So what we're gonna do is we're going to say, okay, so this is old. This is the threshold that we defined. So it only returns if it's at least this confidence, it's gotta be at least 10% confidence of what it's looking at. And if it's not, don't return anything, right? It's gotta be at least 10% confident of whatever output probability that it's predicting. And so we have a set of variables here that we're going to talk about when we implement them. So let's just go through each of these functions. So the first function is the read labels function. So given a saved, drawn model, let's read all the classification labels that are stored and write them to our in memory labels list. So remember we have this, um, classification. Where is it?

Speaker 2:          40:25          Okay

Speaker 1:          40:26          labeled file right here. You know, so this is what we read it into memory. So into memory, we say. So for all those labels, initialize them as an array list, read them, add them to our Ra and return those labels. That's what that does. So this is the, uh, this is the second most important function. This, this recognizes the first most important, but as the second most important. And so what this does is given a model, okay, so given a model and it's labeled file and as Metadata, let's fill out a class of our object with all of the necessary made it Metadata, including the output prediction, right? So these are all the attributes of the model that we can basically get to inset later on. So we've got a name that we defined, we've got the input by name for the input. So okay, so we've got a name for the model.

Speaker 1:          41:14          We have a name for input data, we have her name for help, a prediction. We have art labels that we just read right from, from storage into memory. And then we have the models path that we, and that's where the, that's where all the raw assets, asset files, car, as well as where the model is. And so that's what he's inner inference comes into play, right? That comes from Google. Look, this, all this stuff. This is how we take data and feed it to the model. This is the gateway. This class is 10 or tensorflow inference. Interface class is the gateway between Androids, Java functionality and tension flows, c plus plus functionality maybe by Google. And so this is what we define what that is and we'll send it to the TF helper attribute. And so how big is the input? Well, we'll define that.

Speaker 1:          42:01          We'll pre allocate a buffer for the pro bowl. Those outputs that come out, right, it's not going to just one output. This is a multi-class classification problem. We have a set of outputs, so stored in the hall and an array list. Okay, so then, so that's the second most important class. And then remember this public class, we're just exposing the name, the string that we define. And this is the first most important, not class function. This is the first most important function class function, do op, Dutch, English. I'm just switching between languages and my mind is like, okay, so then for our recognized class, which is the most important class, here's how it works. Using the interface, we give them the input name, the raw pixels from the drawing and the input size. So we feed that into our tensorflow flow interface, right? That class.

Speaker 1:          42:48          And then we check the probability values. So we say, okay, if we want to keep those probability, if we want to keep those probability values in memory, which is why it's a bullying, then feed it that those probabilities as an array of probabilities, which we do, which, so we're going to send it to true and then we get the possible outputs. So when we run it, we s we set these output named as the perimeter, so it's going to fill those applique values. Um, so then we get the possible outputs using the names which are here, and this is where we pre allocate the buffer.

Speaker 1:          43:28          So that, so then we get those possible outputs, uh, via the fetch method. So we first run using those output names and then we fetch it. Okay. So we take the actual output and the output names, which is the list that we fill with the output, whatever the output prediction is, we fill it into that output names a list. And then we find the best classification from those outputs. So we say, let's initialize our classification object, and then we say find the best classification for each of the output predictions. If it's above the threshold that we defined for accuracy, right it out to the view in that text view. So we say, okay, for the length of each output, print out the output, print out the labels just for us for debugging. And then if the outputs is greater than the threshold, hit the prediction value of the output is greater than the, than the threshold. Okay. And it's greater than, uh, the, uh, yeah, the output. And it's greater than the Alpha prediction that we defined inside of that classification object. Then update the object with the new output and it's label. And then we can write to memory. Well, first of all, return it and then we'll write right to memory when we call it. Okay. So

Speaker 1:          44:39          yeah, so we talked about our classifier and our classification class. Okay. So one more thing, right? So the classification class itself, we've got two variables. COMP is the output prediction and the label is the input level. And so we define the threshold here. You know, w 1.0, which is 10% the minimum threshold. And then the label, which is no, and the only public facing function is this update function. Oh. And the get label and the get comp. So we to get her functions and then an update function, which is how we update it, which we called right here. Right.

Speaker 1:          45:15          And that's it. So yeah, if you have an android device, you can easily deploy it to that. Right. Just compile it and then pick one. Oh, so I'm going to talk about one more thing. When would you use a model on your device versus on the server? Uh, you would use it on the server if you want to constantly deploy new versions of a model. So you'd use tensorflow serving, right? The con to that, right. Because the con to that is then you would need your mobile device to constantly make HTP requests. So it always has to be connected to the Internet.

Speaker 1:          45:48          What you could do is you can have it both ways. So the best of both worlds would be to have an inference model on the device, which is static, right? You can't just update it once you deploy a new version of the APP. So the user would have to update the APP, but you could have a version on the APP and you can have a version on the server that is constantly updated that you can set some frequency to check and then update the model locally using the server. So you should have the best of both worlds. Okay. So let me answer two questions to end this and then we're done. We're done. So the two questions are, can you do a video that's like Siri, like how to code the Siri Product. So I could think about apple is they don't really open source anything, right?

Speaker 1:          46:26          So we don't actually know what their machine learning, like how it works. I'm going to bet though that it's okay. Like it's not that good comparatively because in this day and age, if you want respect, if you want credibility, the name of the game is to publish and publish often. And apple doesn't do that. Uh, but how does Siri work? I'm going to guess that it's going to use a set of predefined, um, responses on a server making http requests. It's barely using any machine learning. Maybe it's using machine learning just to, um, well it isn't using machine learning to detect your speech, but I don't think it's using machine learning to generate responses. If it is, I'd be very surprised. Uh, yeah, so that's the first question. And then one more question is how to train feedback and get their respective ratings predicted rating of new feedback.

Speaker 1:          47:13          So that's a supervised classification problem, right? You've got some texts and if you've got a label and then you went to one of the mapping between the two. And so how do you learn the mapping between two, like a label and an input? Uh, supervised classification, right? This just general classification. The way you do this for text is using an LSTM network, which is a type of recurring net made for more or for text data. Cool. Please subscribe for more programming videos. And for now, I've got to create a new series, so thanks for watching.