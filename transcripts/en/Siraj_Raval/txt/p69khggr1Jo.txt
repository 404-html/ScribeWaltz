Speaker 1:          00:00          How do we learn? Although Times may change, some concepts stay the same. Unchanging information outlast the body. It's stored in our brain, but can be passed down from generation to generation. Our brain is capable of synthesizing the diverse set of inputs we call our five senses and from them creating a hierarchy of concepts. If we're lucky, we can learn a task while being supervised by a teacher directly. While interacting with our environment, we can feel our surroundings, see our obstacles and try to predict the next steps. If we try at first and fail, that's okay. Through the process of trial and error, we can learn anything, but what is it that gives our brain this special capability? Unlike anything else in nature, everything we've ever experienced or felt, all our thoughts and memories are very sense of self is produced by the brain. At the molecular level.

Speaker 1:          00:48          Our brain consists of an estimated 100 billion nerve cells called neurons. Each neuron has three jobs, receive a set of signals from what are called [inaudible]. Then drives integrate those signals together to determine whether or not the information should be passed on in the cell body or Soma. And then if the some of the signals passes a certain threshold, send this resulting signal called the action potential onwards via its Axon to the next set of neurons. Oh, we're all, it's the Raj and we're going to build our own neural network in python. The rules that govern the brain give rise to intelligence. It's the same algorithm that invented modern language space flight Sheil above. It's what makes us us. It's what's allowed us to survive and thrive on planet earth. But as far as we've come as a species, we still face a host of existential threats to our existence.

Speaker 1:          01:36          There's the impending threat of climate change, the possibility of biochemical warfare and an asteroid impact. These are non trivial problems that could take our biological neural networks many generations to solve. But what if we could harness this power? What if we could create an artificial neural network and have it run on a non biological substrate like silicon? We could give it more computing power and data than any one human would be capable of handling and haven't felt problems a thousand or even a million times faster than we could alone. In 1943 Q early computer scientist named Warren Mcculloch and Walter pits invented the first computational model of a neuron. They're model demonstrated a neuron that received binary and puts some them and at the some exceeded a certain threshold value. I'll put a one, if not, I'll put a zero. It was a simple model, but in the early days of Ai, this was a big deal and got computer scientist talking about the possibilities.

Speaker 1:          02:31          A few years later, a psychologist named Brank Rosenblatt was frustrated that the McCulloch pits model is still active mechanism for learning. So he can see it's a neural model that built on their idea, which he called the Perceptron, which is another word for a single layer feed forward neural network. We call it feed forward because the data just flows in one direction forward. The perceptron incorporated the idea of weights on the inputs, so given some training set of input output, it should learn a function from it by increasing or decreasing the weights continuously. For each example, depending on what its output was, these weight values are mathematically applied to the input such that after each iteration, the output prediction gets more accurate. The best understand this process we call training. Let's build our own single layer neural network in python using only num Pi as our dependency. In our main function, we'll first initialize our neural network, which will later define as its own class.

Speaker 1:          03:28          Then pronounce it starting weights for our reference. When we demo it, we can now define our dataset. We've got four examples. Each example has three input values and one output value. They're all ones and Zeros. The t function transposes the matrix from horizontal to vertical, so the computer is storing the numbers like this. We'll train our neural network on these values so that given a new list of ones and Zeros, it be able to predict whether or not the output should be a one or zero. Since we are identifying which category it belongs to, this is considered a classification task and machine learning. We'll train our network on this data by using them as arguments to our train function as well as a number 10,000 which is the amount of times we'd like to iterate during training. After it's done training, we'll print out the updated weights so we can compare them and finally it will predict the output.

Speaker 1:          04:13          Given a new input, we've got our main function ready, so let's now define our neural network class. When we initialize the class, the first thing we will want to do is seed. It will initialize our weight values randomly in a second and seeding them. Make sure that it generates the same numbers every time the program runs. This is useful for debugging. Later on we'll assign random weights to a three by one matrix with values in the range of negative one to one with a mean of zero. Since our single neuron has three input connections and one output connection. Next we'll write out our activation function, which in our case will be a sigmoid. It describes an s shaped curve. We passed the weighted sum of the inputs through it and it will convert them to a probability between zero and one. This probability will help make our prediction.

Speaker 1:          04:58          We'll use our sigmoid function directly in our predicts function, which takes inputs as parameters and passes them through our neuron to get the weight that some of our inputs will compute the dot product of our inputs and our weights. This is how our weights govern the attention of how data flows and our neural net and dysfunction will return our prediction. Now we can write out our trained function, which is the real meat of our code. Well, write a four loop to iterate 10,000 times as we specified. Then use our predict function to pass the training set through the network and get the output value, which is our prediction. Well, next, calculate the error, which is a difference between the desired output and our predicted output. We want to minimize this error as we train and do this by iteratively updating our weights. We'll calculate the necessary adjustment by computing the dot product of our inputs transpose and the error multiplied by the gradient of the sigmoid curve, so less confident weights are adjusted more and inputs that are zero don't cause changes to the weights.

Speaker 1:          05:54          This process is called gradient descent. I'm the sending that gradient. Oh, we'll also write out that function that calculates the derivative of our sigmoid, which gives us its gradient or slope. This measures how competent we are of the existing weight value and helps us update our prediction in the right direction. Finally, once we have our adjustment, we'll update our weights with that value. This process of propagating our error value back into our network to adjust our weights is called backpropagation. Let's demo this baby in terminal. Because of the training set is so small, it took milliseconds to train it. We can see that our weight values updated themselves after all those iterations and when we fed it a novel input, it predicted that the output was very likely a one. We just made our first neural network from scratch anyways about backpropagation.

Speaker 2:          06:40          I date update weights, backed up update wave that propagates the update rate backdrop, update weight [inaudible] and knows map to owes him ones inputs. Go in, add weights, get someone's past that shit too. Must sigmoid function. Get that error, woods real and prediction. And that's why I use gradient descent. It gives direction and it doesn't pretend update weights and repeat 10,000 times outputs are lit. I'll be doing just fine.

Speaker 1:          07:16          So as dope as Rosenblatt's idea was in the decades following it, neural networks didn't really give us any kind of note the results. They could only accomplish simple things. But as the worldwide web grew from a certain project to the massive nervous system for humanity that it is today, we've seen an explosion in data and computing power and a small group of researchers funded by the Canadian government held fast to their belief in the power of neural networks to help us find solutions from this data. When they took a neural net and made it not one or two but many layers deep, gave it a huge dataset and lots of computing power. They discovered that it could outperform humans in tasks that we thought only we could do. This is profound. Our biological no network is carbon based sending electro chemicals like Acetylcholine and glutamate and Serotonin as signals on artificial neural network doesn't even exist in physical space.

Speaker 1:          08:08          It's an abstract concept we programmatically created and it's represented on silicon transistors. Yet despite the complete difference in mediums, they both develop a very similar mechanism for processing information and the results show that perhaps there's a law of intelligence encoded into our universe and we're coming ever closer to finding it. So to break it down, a neural network is a biologically inspired algorithm that learns to identify patterns in data. Backpropagation is a popular technique to train a neural network, but continually updating weights. The gradient descent, and when we train a many layered deep neural network on lots of data using lots of computing power, we call this process deep learning. The coding challenge winner for last week is Ludo Blonde. Little made a really slick I python notebook to demo, not just today regression, but 3d regression as well on a climate change dataset wizard of the week and the runner up is Amanullah Tariq. He completed the bonus with great results. The challenge for this video is to create a, not one, not two, but three layer feed forward neural network using just num Pi. Post your gate, humbling in the comments, and I'll announce the winner in one week. Please subscribe and for now I've got to update my weights. So thanks for watching.