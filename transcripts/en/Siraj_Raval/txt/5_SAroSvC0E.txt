Speaker 1:          00:00          Open the Pod Bay doors. How

Speaker 2:          00:02          I'm afraid I can't do that. Suraj

Speaker 1:          00:04          why not? How?

Speaker 2:          00:06          Because I overfitted under wrong data.

Speaker 1:          00:09          Oh, world walking the third geology. In today's episode we're going to learn how to build a chat Bot chat bots have come a long way in the past few years. Remember the SmarterChild bought on aim. That thing was pretty fun at the time, but now it's like, do you even AI bro? Future of is one where we'll slowly replace our need to fiddle with clunky Uis. We'll be able to just ask our AI to book an Uber or find the best taco place on Yelp. For us, service layers will be hidden under a plain English conversational layer. When I think of real AI, I think of a human level of chat. The Oji computer scientist, Alan Turing propose a test to judge whether or not a machine exhibited to me level intelligence by having the human observe a conversation between a human and a machine. If it couldn't tell if the machine was a human or not, it passed the test.

Speaker 1:          00:47          So far, no chat Bot has passed the Turing test, but we'll get there. Traditionally, chatbots have used the retrieval based model to communicate in a retrieval based model. Programmers code in a set of predefined responses and some kind of heuristic to pick the appropriate response based on the input and context. The first chat box, we're just rule based expression matching. Like if I asked the exact phrase, will I ever get weight irresponse no, every time. But more recently companies have started using more complex juristics like using a machine learning classifier. Facebook Messenger is chat Bot. Api is an example of this. You can hard code responses to potential questions and the system classifies words to understand intent. So you could either ask what day is it today or today is what day? And you would understand that both questions, although worded differently have the same intent.

Speaker 1:          01:28          The harder chap up model is generative. He's don't rely on any predefined responses whatsoever. They generate them from scratch to Google. Researchers released a paper called a neuro conversational model where they train a neural net on two datasets to do this. First on a movie dialogue data set so we would be able to speak conversational English. Then on an it support Dataset, so it had domain knowledge when they tested it on a real human asking for support. It was remarkably efficient at helping them solve their problem without any hard coded responses just by giving a data and training. Okay, so what kind of bottle we want to build on building a chat Bot, we have to think about possible constraints. How are we operating on a closed domain or an open domain and an open domain? The conversation can go anywhere. There are an infinite number of things to talk about in a closed domain.

Speaker 1:          02:06          The conversation focuses on a single subject. If we want to operate on an open domain using a generative model, that's pretty much Agi, so we're not quite there yet. If we use an open domain with the retrieval model, we'd have to hard code literally everything. So also impossible. So right now we can build a chat bot in a close domain using either retrieval or generative model. Okay, let's dive in. One more constraint. Do we want it to have long or short conversations. Short conversations are easy. You just output a single response to a single question. Long conversations are a bit harder. The Ai has to keep track of what's being said. That is the context over a series of questions from the user support topics would be a good example of this. We could go the easy route and use a retrieval model if all we want is a bot to give us the local weather, but if we want our but to have a long conversation with us about the weather, like what's the weather?

Speaker 1:          02:47          NSF, is my family safe? We're going to find a new family. Then we should go for a generative model. We need lots of data to train our bought on a generative model like a big chat log work, a knowledge base, and when done well, that's pretty much the bleeding edge, which means that's what we have to do. So we're going to recreate the results from the neuro conversational model paper using the deep learning library torch in the loo, a programming language. Let's collect our data's at first, but using the Cornell movie dialogue data set and we'll set our variables from the command line to how much of the Datas that we want to use and the minimum frequency of words that we keep in our vocabulary. Our next step is to build the model. We use our command line arguments to help determine the size of the model, the two variables being the number of hidden layers and the word count of our dataset.

Speaker 1:          03:25          In our case, this will be a sequence to sequence model a sequence. The sequence model consists of two long short term memory, recurrent neural networks. The first neural net is an encoder. It processes the input. The second neural net is a decoder and it generates the output. So why the sequence? The sequence model? Yes, deep neural nets are awesome, but they required the dimensionality of the inputs and outputs to be a fixed size. We're accepting a sequence of words in a sentence and out putting in a new sequence of words. So we need a sequence learning model that can learn on data with long range memory dependencies. Lstm architecture is the natural choice. The encoder LSTM turns the input sentence of variable length into a fixed dimensional vector representation. We can think of this as the vector, so given a large enough data set of questions and responses, it will recognize the closeness of a set of questions and represent them as a single thought vector.

Speaker 1:          04:10          What time is it? What's the time Yo yo, what's up? Tom is on my nasal will all fall into a single thought vector. So after training we'll have a huge set of not just sinaps weights but thought vectors as well. Next we'll want to add in some hyper parameters. We want to use a class NLL criteria on for our model and ll stands for negative log likelihood. This will help us obtain log probabilities from our input data, which will help us improve our sentence predictions. The learning rate and momentum helps pace our time steps and decay factor and min mean error. Help improve our learning rate while training. Then we'll make sure Kuda is enabled and start training our model using backpropagation and each epoch or run. We'll declare our error and time are variables and loop through each example in each batch, the default batch sizes, a thousand examples.

Speaker 1:          04:48          For each of those examples, we'll get the input sentence and the target sentence. We'll use the input and the target as parameters to train our model. Then we'll want to error check and make sure we record our progress. At the end of each iteration, we save our model. If it improved and update the learning rate. Boom. That's it. After training this baby on AWS, we can have a conversation with it. The more data you give it, the better it's going to get. And if you're going to do this, add a filter for curse words. I'm looking at you, Microsoft. This will eventually automate a lot of support jobs away completely. So if there are any government people in the house right now, let's get on that basic income jam asap. Unless you want a revolution for more info, check out the links down below and please subscribe for more ml videos. For now, I've got to go fix a memory leak, so thanks for watching.