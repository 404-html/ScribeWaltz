Speaker 1:          00:00          Too much.

Speaker 2:          00:07          Hello world, it's a Raj and welcome to this live stream. Uh, in this live stream I'm going to be talking about a node dot js and we're going to be using machine learning for that. Uh, so the code is going to show up behind me when I move it. So it's going to show up. Just setting up some lives, a lot of things right now it looks like the code is not behind. There we go. Okay, cool. Cool, cool, cool. Cool. All right, so, okay, I'm so excited to be here. Thank you guys for coming. Uh, so we have a lot to talk about. Today. We are going to build a node dot js app from scratch and this app is going to be able to translate to in real time. So check this out. Here's the translation demo. Okay, let me make this a little bigger.

Speaker 2:          00:51          So this is in real time. This is what's happening. Okay. I can say, how are you? And it's going to translate it in real time into French. So English to French translation in the browser. Immediately I can say hello world. It's Saroj. And then, uh, it's going to say, you know, whatever it needs to say. Okay? So that's the demo that we are going to build today. Uh, this person is saying my left ear is highly educated. Well, guess what? Your right ear is going to be as well because there's no audio in the left ear. Okay? We're, we're gonna fix that. We're going to fix that for you. Okay? So

Speaker 1:          01:28          yeah,

Speaker 2:          01:28          his day on, we're going to fix that for you. In the meantime, let's get right into this. Okay? So, ah, what I want to do is just say five names and then I'm going to start a coding this. Okay. So, uh, I'm unprepared, uh, silly Tian and Martin and Depok and uh,

Speaker 1:          01:45          okay.

Speaker 2:          01:45          Russia. Okay. And then the question, I only see one question. Have you ever loved anybody? Yes. It's called my community. It's called you guys. So that's why I do this thing. Okay. So let's make everything big. And now we're going to look at this. So I'm going to talk a little bit about what no job, no dot js is. And then we're going to talk about how machine translation works using sequence to sequence models. And then we're going to code this thing out in Java script. Okay?

Speaker 1:          02:11          Okay.

Speaker 2:          02:11          All right. So let's, let's get right into this. So No. Dot js. So javascript was initially invented just for, just for in the browser scripting. It wasn't made to create stand alone applications, right? So you wouldn't build a model that allows for requests to be coming in, uh, in javascript. And javascript wasn't made initially to handle a bunch of concurrent requests, you know, like for posts, requests for, for, for data, et Cetera. But the developers of node dot. Js said, you know what, let's create a javascript runtime environment. That's an entire environment that you can use javascript to create stand alone web apps. And since then it's kind of exploded in popularity. Look, no. Dot. JS is not a new technology. It's a, it's, it's, it's a relatively, um, you know, in, in, in programming terms, it's an older technology. It's about five years or more older, but it's a very, um, it's a very good technology and we can use it.

Speaker 2:          03:08          We can combine it with machine learning for some amazing results. So if we look at the analogy of Java, so most of us are familiar with Java because computer science one oh one class is a teach Java generally. And if you're not, that's okay. Uh, but basically what I'm trying to show is the analogy of what a node runs on and it runs on what's called The v eight engine. And Va engine was created for c plus plus specifically, but it, but yeah, you can use it with javascript now. And what it does is you type in javascript in the browser and it compiles it down to very fast machine code that's readable by your machine. Okay. So no longer is it? No longer is javascript just constrained to the browser. You can use it for all sorts of things. Okay.

Speaker 2:          03:56          Nice audio for you. Fix the audio. All right, well it's fine now. Okay. So, uh, the other thing about node dot js is so on. If we look at the official node dot js website, let's look at the official node dot. Js website or write node. Dot. JS is a javascript runtime engine built on chrome's via javascript engine. And what it does is it creates an event. And here's the key. Here's the key right here. It creates an event driven nonblocking I io model that makes it lightweight, inefficient. Let's break this down. I o means input output. So this can be anything from reading files, riding local files, making an http request, right? Anything that you were inputting into a model and then you're getting an output that's an input output model. But that, but the problem with input output models is the idea of blocking, right?

Speaker 2:          04:44          So blocking happens when you make a request and there's another request. But wait, this request is not finished yet and now it has to wait for the other request, right? So it's blocked. Request number two is blocked from, from modifying, uh, the, the, the APP because requests, one is in process. So what javascripts node dot js does is it has this thing called the event loop. Okay. And this is what the event, Luke looks like. The event loop consists of three different modules. You have your call stack, you have the node Api Apis and you have your callback queue and great two years for the audio. Awesome. And also attention mechanisms. I know you guys want attention mechanisms. I need to make a dedicated video about attention that's coming. You can't just be like, oh this is a tension but and then that's it. No, attention is a is a highly interesting topic.

Speaker 2:          05:37          It's an advanced topic and we will get into that. I know you guys want it. I want it to you know how much I like attention mechanisms, but basically the idea of attention mechanisms is normally in a neural network you have a bunch of things to choose from, right? The network has a bunch of options on what to make a prediction width, different submodules of data. And what attention does is it says here is the most important part of that data to pay attention to and here's what to make a prediction with. And it, what it comes down to is probabilities rights. What's the percent likelihood that this will result in an output that is most aligned with the label that I have. And there's different methods for this. There's a whole field of study dedicated to attention mechanisms. You could dedicate a two, three years to this.

Speaker 2:          06:22          Okay, so how does machine translation work? And that was, that's a no dot js. Well, before I get some machine translation, let me just finish explaining no. Dot js three modules here, the call stack note APIs and the callback queue. Okay. This is, this is very, you know, trivial stuff. And so here's an example, right? So what happens is for these three functions, well two functions, what happens is they are first, when you execute this code, all of those functions go onto the call stack. The call sec then calls the node Api Apis, right? Set Timeout as a part of the node API. And then both are both are place in the, into the callback queue. The callback queue is waiting for the call stack to be empty and once it's empty, it's going to send one by one, uh, different functions to that call stack. And then once and any new functions first, enter the call stack, call the node Api Apis, then go to the callback queue and then they're executed and the call stack again.

Speaker 2:          07:15          So it's kind of this loop, right? And so what happens is because there's this process that's happening, there's no blocking. And so what that means is you can make concurrent requests, not just one, but a thousand requests at the same time. And because there's this loop functionality, you don't have to wait for one request to finish before starting the other one. Now onto machine translation here is our input data and our output data. Okay? So what this is, let's look at our data set. By the way. Uh, our data set is an input output dataset. Uh, example LSTM sequence to sequence a French data. This is a very popular dataset and there's a great tutorial for this on the care Ross blog as well. Uh, minds is going to be more interactive, but let's see where the input data is. Here it is. Okay. So that's really what I want to.

Speaker 2:          08:07          And plus ours is going to be in Javascript, not in python. So this is our input data. Okay. And they, this, this website has a bunch of great data sets for every language. So a bunch of phrases like hello, how are you with the associated, uh, other language that has translated to, I'm also going to be taking questions. So in five minutes from now I'm going to be taking questions, so be sure to ask those questions before I start coding. Okay. So what we're going to do is we're going to translate English to French. Okay. So we downloaded an English French pair. So the input looks like this left column right here. The output looks like this right column right here. And what we want to do is learn that napping. Exactly. We want to learn the mapping between the two. And so the first step for us is to preprocess this data. So what's it's going to look like is this, it's going to be a set of word vectors and here's what the first word vector will look like. Go becomes g o period. That's it. And this is going to be an array where every element in the array is a different character. That's a part of that phrase. And the output will be the same.

Speaker 2:          09:17          Great. Okay, so now, uh, for the encoder decoder architecture, here's how it works. We have to recurrent networks. Okay. We have a one recurrent network, that's the, we call the end coder. We have another recurrent neural network neural network called the decoder. So there's two neural networks here. One acts as what's called the encoder. And the other, as a decoder, the encoder is responsible for outputting a fixed length in coding of the input English sentence. So recurrent networks are a type of neural network that are fed, not just the new, a new data point in the next time step, right? So feedforward networks are fed new data points every time step, right? So here's one data point. Here's the next day to point. Here's the next data point. And every time step that that hidden weight matrix that that is the neural networks brain, essentially, it's all of its learnings over time.

Speaker 2:          10:07          It's a collection of numbers, right? Ones and Zeros that gets better and better through the optimization process for normal feed forward neural networks. You're just feeding in input data and that, uh, that, that, that weight matrix for the network is kind of like Plato. It's been cool. It's been molded over time to be more and more optimized, right? So it's be, it's becoming better and better. So when you feed it a new output, it's going to give you the perfect, when you painted a new input, it's going to give you the perfect output. But recurrent networks are not just vetting new input data. So it's not just new data points. It's also the, it's also the previous, um, hidden state. So it's the previous version of that Plato Weight Matrix. So it's being fed both of those. And the reason for that is because we're current networks are made for learning sequences.

Speaker 2:          10:53          Now it does very well with learning sequences, right? So predicting the next character, the next number, the next image, the next frame in a sequence of images, video, it does very well at predicting sequences. The problem is that there is a, there is a problem with learning long term memory, right? So if forgets what happens in the past, now I have some great videos on LSTM networks. This problem is called the vanishing gradient problem and we can talk about it forever, but basically Tldr, just search LSTM Saroj for videos on that. But, uh, the idea is that the gradient vanishes slowly as we back propagate into previous layers. And so there's a version of the recurrent network. Um,

Speaker 1:          11:37          okay.

Speaker 2:          11:39          The version of the recurrent network is called a n l s t m network. Okay. So a long short term memory network. And here's what it looks like. And let's talk about this for a second because it's very interesting in, in addition to having a hidden state, this LSTM recurrent network has a cell state. And here's what a cell looks like. Now, remember these words, gates cells. These are just, these are just ways for us to understand what this actually is. And what this actually is, is a series of operations that we are applying to input data. And so we have what's called an input gate. Uh, forget gate and I'm an output gate and a cell state. We have four different things to be thinking about here when we're thinking about an LSTM cell. And the whole idea behind this is to trap that gradient over the long term. And this kind of, this, this scheme has resulted in us being able to trap that gradient in an earlier, uh, layers of the network such that it can remember longterm sequences. So if you look at these gating variables here, these three equations, if we look at these three equations,

Speaker 1:          12:47          yeah.

Speaker 2:          12:48          All right. And I'm going to answer questions right after this. Let me just finish this. If we look at these three equations, we have an equation for the forget gate, the input gate and the output Kate and its input times weight add a bias and then the parentheses activate. Remember, input times weight, add a bias, activate. These gates are just single layer neural networks, right? They are perceptrons where we're taking the input, multiplying it by its own respective weight Matrix, adding a bias value and taking that output. And what we do with all three of these gates is we compute the cell state. So look at this equation right here, okay? What this equation is, is a, it is, uh, when we are taking all of those gates and we are, we're saying we're taking, going to take the, um,

Speaker 1:          13:33          okay,

Speaker 2:          13:33          the first equation, multiply it by this cell state from the previous time step plus the implicate times his cell state. And that gives us our cell state. And then we take our output times tan h the activation of the cell state. And that gives us our hidden state. So these two variables right here, and this is our self safe from the last time step. These two variables right here are what we care about. And that's what the LSTM computes. So when it comes to training, here's how it works, okay? So here is how it works.

Speaker 2:          14:04          Let me just say how it works and then we're, we're, we're good with this. Okay? So what's happening is we take, okay, so during the training phase, we take our input sentence. This is an English sentence, okay? We take that English sentence and we feed it into the encoder. So this is a vectorized version of the English sentence. We feed it to the first encoder, the encoder, that's an LSTM network. Now, what we care about is not the output. We care about the learn hidden state from the encoder and what that hidden state is, we can call it a lot of things. We could call it a a Plato weight Matrix. We can call it a, uh, we can call it a

Speaker 1:          14:41          okay,

Speaker 2:          14:42          a hidden state. We can call it, uh, uh, a thought vector. We can call it a learning. We can call it, uh, you know, it's what it's learned. It's a dense representation. It's a compressed version of everything that it's learned over time, a generalized version. And we use that learned hidden state from the, from the fruit, from each iteration of the training loop to initialize the decoder. So that initial lot, so that hidden state that's learned from the encoder is used to initialize the decoder. What the decoder is fed is the French sentence. So the English sentence, check this out. You got to listen to this. The English sentence is given to the encoder, the encoder output on a hidden state, it will have an output. But we don't care about that. We only care about the hidden state. Then it's learned. We feed the hidden state to the decoder now too, to initialize it and we feed the decoder the French sentence.

Speaker 2:          15:34          So what happened? What's going to happen is the decoder is going to translate the French sentence using the learned hidden, say from the encoder character by character. It's going to predict every next character given that input. Now, uh, it's gonna output something that looks very bad at first, but we have the label, which is the actual French phrase. We'll compare it via some lost function. We'll use that to back, propagate the gradient across both networks. And we repeat until eventually the output is going to be very, very similar to the input, which is our translated phrase. That's for training. And then lastly, for inference, we're going to encode the input sequence, retrieved that state, use it for the decoder, and then we don't give it a French phrase. We just immediately, uh, output. We predict every next character cause we don't have a label. Okay.

Speaker 2:          16:24          So now for questions, five questions and I'll get to the code we have. We have a lot of great code for use, so don't even worry about that. Okay, so question one, uh, how was it great for Linear Algebra? Linear Algebra is the study of Algebra apply to groups of numbers, right? So normally you're multiplying single numbers, four times five, five times five, and you're off, you're applying operations to single numbers. But when you have groups of numbers, matrices, which are what neural networks use that are run on Gpu, that can do massive parallel computation, you need a new type of math to be able to apply operations to groups of numbers at the same time. And that's why linear Algebra, that's what it does. And that's why it's important for neural networks. Question two, when is the school of the ice starting? It's starting right now. A question three. Oh and the next, the first course is going to start in a few weeks. So, uh, question three.

Speaker 1:          17:21          Hm.

Speaker 2:          17:22          What are the benefits of RNN with Lstm over our end with Gru, gru is actually, um, used more often now, but I think lst, m's are easier to understands. Um, lastly.

Speaker 1:          17:39          Okay.

Speaker 2:          17:40          Is He app similar to Google translator? Yes. Well, I mean, Google translate uses, um, an encoder decoder architecture. Uh, it's very different than this one now because it's iterated over time, but it's essentially the same idea of encoder and decoder architecture. One more question. Can you explain how data is formatted and how it will be fed in? Yes.

Speaker 1:          18:01          Okay.

Speaker 2:          18:01          Data is going to be formatted as an input vector, right? Where every character is its own element in that vector or a re, uh, and then we're going to feed that in. Okay. So now let's, let's get into the code here. Uh, what do we got here? What time are we at? Okay, cool. Cool, cool, cool, cool, cool, cool. Uh, all right, so

Speaker 2:          18:26          cool. Um, so okay, where do we begin here? So, uh, if we go to node dot js, what we can do is we can go to no job, no jazz stock, or we can go to downloads. And we could say what type of operating system do we have? I have a Mac, so I'll click on that and that's going to install node, the node dot js using a visual manager. Okay. It's a visual package manager. Once we have that you, if you just joined, you haven't missed the code part yet, so that's the good part.

Speaker 2:          18:53          The code is going to be in the video description by the way, so definitely click on that. So inside of node dot js, we can download this. We can say it. Let's, let's, let's look in the docs here. Well, how do we get started with this, right? No. Dot js looks hard. Well, guides are usually the best place to look in any kind of technical documentation inside of the getting started guide. Here's how we do it. We create a simple sublime file or a simple text file. We paste this in and then we hit APP, uh, and we name an APP dot js and then we in the command line type in node, APP dot js and at local host 3000 we'll see the message. Hello world. That's how you install node dot. Js. So what I'm gonna do is I'm going to get right into the meat of this code that we're going to use a, it's going to be beautiful and we got to start somewhere. So I'm going to write out this encoder decoder architecture for ourselves, uh, for us, and then we're going to start coding it. Okay. So beautiful. So, um,

Speaker 2:          19:53          once we've installed tensorflow dot js, so npm installed tentraflow dot js uh, we can import it to, right? So there's several ways to be importing tensorflow. Dot. Js uh, but what I'm gonna do is I'm going to import it using NPM, which is the node package manager. Now, uh, there's also another package manager you should know about yarn, which is just as good, if not better than NPM, that Facebook created because they were having problems with npm. Okay. And they tried everything really, but they were still having problems. And at the scale of Facebook, of course you're going to have problems.

Speaker 1:          20:29          MMM.

Speaker 2:          20:31          Right? So now we're going to load the pretrained models. So this a class file will contain the pretrained model for us that we can use later on. Uh, what else are we going to import? We also have some basic dom elements that I've written out in a separate file that we're going to look at, but that's really, it tends to dot. JS is like our main file here. And then we're going to talk about the other things. Okay. So now let's load our pretrained models as well. So we're going to have some pretrained models. Uh, we're still gonna build models, but I just want to load them so we have both options, right? So we could, we could load our pretrained models or, uh, we can just, okay, cool. I just wanted to see what people are saying. We can, uh, download pretrained models or we can have it from the web or downloaded from the web. So I'm going to download it from the web. I think it's, where's the,

Speaker 1:          21:29          okay,

Speaker 2:          21:30          what were these,

Speaker 1:          21:32          uh,

Speaker 2:          21:35          API APIs called? What was this website called? Oh, here we go.

Speaker 1:          21:41          Okay.

Speaker 2:          21:41          That's what it was called. So these are the, this is the, so Google has this pretrained model on the web for us as a Jason File. So these models are saved as Jason files, but it's not just the model. There's also metadata, right? So metadata includes things like versioning, information, timestamps, um, things like that. Everything that's around the model, but it's not necessarily the model. Okay? So we're going to initialize this as a constant file. These are hosted model URLs, so we can load pretrained models will also build the model. So don't even worry about it. Okay, so we've got that. Great. Now we have our train model. We have our hosted URLs. Now we can, um,

Speaker 2:          22:26          create our translator class. So not now let's build a translator. Okay, so here's how we do this. We define a class called translator. Inside of the translator class. We're going to initialize it and we're going to use the ASYNC function because we don't want to wait for the dom elements to load. When we initialize our class file. Now we're going to initialize it using the URLs that we are provided. We will build our own models for sure. Now I'm going to initialize that URLs file as it as our own variable. I'm going to say, well here's our model and we're going to wait for it to load using the await function. And I haven't going to have a function for low load, the hosted pretrained model using the URLs dot models. Okay. And we have that. Okay, so that is our model right there.

Speaker 2:          23:19          What it does, the await function by the way, it just expresses it. That expression just, it causes the async function to uh, the, to pause until this function that, that it's a weighting has, has loaded. So that's why we did that. Now we're going to do the same thing using the away to method for uh, the Metadata, right? We cause we have metadata as well. It's not just the model itself. We have the metadata and this model actually consists of two parts. We have our, remember we have our encoder, so we're going to call it prepare in coder model and it has rd coder. And in these functions that I'm defining right here, we're going to segment out this model and then prepare both of them. And I'll return this at the very end. That's our initialization function for our translator class. Okay, so, um, let's start off with the, the metadata because right, it's not just the model that we're loading. It's the metadata as well. We have two different things to be loading here. Let me also see. Hi everybody. Okay. I just want to see what everybody's saying. Cool, cool. We have people joining it and we have people leaving. Don't leave. Stay, stay. Okay. This is important stuff. This is more important than web development in general. Look, machine learning is the future of all code. Okay? Everything's going to be machine learning and if you want to stay on top of things, you need to understand how this works. Starting with metadata. Okay, so back to metadata. Back to metadata.

Speaker 1:          25:02          Where were we?

Speaker 2:          25:04          So we can retrieve this metadata from our helper class. Cause we have a helper class here and we're going to use the loader for member. I imported that loader at the very beginning load hosted metta data using the URLs. Metadata. Okay, so that's the loader. That's our translation. Meta data. Now we can say, well, let's take our decoder.

Speaker 1:          25:42          Okay.

Speaker 2:          25:43          And so one of the, so now we're going to retrieve something from the metadata. So we're, remember we were treating what already exists, so we can say what we know that one of the variables of the Metadata, and if we look into tensorflow, docs, this has these, it has these features, but one of the variables of the input data or of the metadata is the Max sequence length for both the encoder and the decoder. Okay, let me zoom out a bit so people can see what we're talking about here or there we go. Now we have the same thing for our encoder. So we have a Max decoder length. We have a max in Kotor length. And again, we can use that same model to retrieve that. And it's going to look very similar except it's going to be called the encoder. Yes.

Speaker 1:          26:42          Yeah,

Speaker 2:          26:43          you're right. Cool.

Speaker 1:          26:44          Uh, Max encoder sequence length, right. Okay. Yes. So now we have our Dakota or we ever encoder sequenced length. And now, uh, there's, there's actually, um, cool

Speaker 2:          27:02          cons. Yes. So cons needs to be, let's see. Translation, metadata. Oh, rivalry. Thanks. Um, right. There we go. Thank you. Right? Yes, thank you guys. This is how we do it. Crowd sourced debugging. So now back to this. So we have our, we have a token index as well. Now the reason we're going to use this token index is so we have a way of knowing where in the phrase we are, when we are predicting the next character in the sequence, we need a way to predict where we are. And so the metadata has that and it's called the, it's called the input to token index. Now we have that not just for the input, but we have this for the target index. Now what the target index is, is it's the final label that we are trying to predict, right? So that label is the output French phrase. Okay? So that's our index.

Speaker 2:          28:06          No, that's it for metadata. Now I want to prepare the encoder model. Prepare encoder model. Okay? And it's going to take the model as its input. So it's going to segment out the encoder model using the, the, the, the pretrained weights. And we can, we can do is we can, we can train it on our own data, right? So it's already been trained on uh, you know, language pairs, but we can just train it ourselves. We can just add training to it. Right? And also this is the future of machine learning where you don't necessarily have to train your own model from scratch every time because someone else has done that in the same way that you don't have to write code, right? You don't have to write all the bare bones code for everything every time someone has already done that. So you import a library. So the paradigm that we're going to start seeing with machine learning is one where we are not just importing libraries, we are importing pretrained models and not just single models, but multiple models trained on multiple different types of data. And we can combine all of these models together and then train it on our own data and we won't need as much data because of it. It's a much more efficient process.

Speaker 2:          29:17          Now enough of that. And let's get back to this. Okay. So, uh, back to the encoder model. So what does this look like? So how many encoder Toki encoder token count, how many encoder tokens do we have? Right? So what these tokens are, are the number of characters that we are inputting into the encoder. And what we can do is we get,

Speaker 1:          29:43          yeah,

Speaker 2:          29:44          for the length of this, and it's going to be of size of shape too because there are three different, um, zero one, two. So those are the elements in the array. There are three different, uh, words in a phrase and we can say that's well, that's going to be the number of tokens. Now we have to decide, well how many inputs is going to be in this? And we can use the equal sign. We could say, well modeled on input zero. That's going to be the, that's going to be input to the model. Now remember, we are using the hidden states as the, as the, uh, initial app as the medium with which to initialize a decoder. So,

Speaker 2:          30:23          so we need to retrieve both of those. So we have our hidden state, state H, which we're going to retrieve from the models. And we're going to say, well, here's a specific layer where it's out where it's at, but it's not enough to just, uh, it's not enough to just say, we, we need the hidden state. We also need to have that cell state. So remember, recall I just talked about LSTM networks. And what LSTM networks do is they don't just have a, they don't just offer a hidden states. They offer cell states. That's what's learned. And what does that prevent that prevents the vantage and gradient problem. And what does that do? That lets us learn longterm sequences. Okay? So that's our hidden state. That's our cell state. And what we can do is we can say, well, let's create a new list and put those values into this single list that we've just defined. And we'll take that list and we'll use it to build the model. Okay, I see it. Translation, Metadata,

Speaker 1:          31:27          uh,

Speaker 2:          31:30          input token index. It is right. Translation metadata.

Speaker 1:          31:42          Okay.

Speaker 2:          31:44          Yes, I will use a semi colon consistently. We got some, we got some Java script. Um, people in here, which is good. It's not just all python people. We want diversity. We want variety guys. Okay. Javascript, python, look, python is what is obviously where it's at for machine learning. And this is kind of controversial to say, but if I were to pick a number two language for machine learning, I'm going to pick javascript, not are not scholar, not go, not, not, uh, Matlab for sure. Um, but it's javascript. Um, and there's a lot of reasons for that, but there's a lot of developer activity around javascript libraries for machine learning and have a great video on that coming out this weekend, by the way. So, um, now we can build the model. So now we're actually using a tentraflow dot. Js function, TF dot model to build this thing. And, um, we can say, well, here are our inputs and here our outputs are outputs are going to be the states, right? Because we want those states to be fed into

Speaker 1:          32:54          the

Speaker 2:          32:55          decoder. So that's what we're out putting.

Speaker 1:          32:58          Okay?

Speaker 2:          32:59          Okay. So that's us preparing our encoder and now let's prepare the decoder. Prepare. Okay. Prepare decoder model using the same. Remember we are segmenting out this model. Oh, I see, I see.

Speaker 1:          33:21          You're right. Yes, that's what you meant.

Speaker 2:          33:25          Okay. I just want to fix some syntax here and now we're back. Thank you. So prepared. Decoder model looks like this. So again, we are going to do the same kind of basic idea where we are deciding what we are. We are trying to see what the number of a decoder tokens are going to be. So they nom decoder tokens and it's gonna be modeled on input one that shape. And then we're going to say, well, okay, so let's define our hidden state. So we have, our constants are hidden state, uh, and that hidden state is going to be just that. Again, we have our hidden state. Uh, now what we need to do is define a latent dimension. Now the reason we are defining this lightened dimension using the hidden state is it's going to help us define the input. Now let me talk about what exactly I mean by that. So it's going to be one less than the hidden states size or length. Because how many live people do we have by the way? Constantly in dementia. Cool. Cool. All right. So, um, so we're gonna use this latent dimension to help us define the inputs directly into both hidden states. So let me show you what I mean. So I have a decoder state input. H I have a decoder, state

Speaker 2:          35:01          input C, which is the cell states. And then I have a decoder state inputs variable. Okay. So these are our three variables

Speaker 1:          35:12          equal sign up 54. Thank you. Okay, thank you.

Speaker 2:          35:19          All right. Um, now, now, now we're going to actually use some tensorflow. Dot. JS input functions. One is going to be t f. Dot. Input shape using the latent dimension. Here is why we use the lightened dimension. We are using the shape of the hidden state minus one to define the decoder states hidden state. Okay. So it's going to be minus one because we are predicting the next character in a sequence, right? It's not the, it's it's going to be the, if, if the input sequences t we're predicting t plus one. So the next one. So the input shape has to start one before [inaudible] 67 thank you. Latent dimension is the shape we have our name, which is going to be decoder state in cooked h k and now let's do the same thing. Let's make this a little smaller so we can all see this. Let's do the same thing for the cell state and we'll do the same thing. Now these are our inputs. Now that we've defined both of them, now we can say, well, here's our input age and here's our inputs. See, now these are our state inputs, right? So these are our hidden states and these are our, these are our, this is our hidden states and ourselves state. Okay, now that we have both of them, now let's retrieve the l s t m model. Just like that.

Speaker 2:          36:58          Okay, so I just want to take a little break to say, Quan Luis Clore said, how do you convince the public we really live in the machine learning era? Well, it's too hard for a country like Bolivia and suit America believe to believe in that. I don't care where you live. You can live in Antarctica as long as you have an internet connection and some kind of computing devices, it can be a hundred dollar convert to your local currency Chromebook as long as you can access. Let me show you this. Co lab.research.google.com. Okay? You go to this in your browser, okay? This could even be Internet explorer, like the worst browser ever, but as long as you have a browser, you create a notebook. You say hello world in that notebook, you hit this.

Speaker 2:          37:51          It's compiling it on an Nvidia k, 80 GPU. You have 12 hours of training time on this thing for free. It compiled it on that. Any printed it out? You have everything. You have your algorithms. Okay? That's all available. Um, machine learning, sub reddit. It's available on the tutorials online. You have the data. It's all available using awesome. Get held public lists. Um, Reddit r slash. Datasets. You have the education, me, other youtubers. You have the compute here. So I don't want to hear, I live in South America, Bolivia, whatever. I can't do machine learning. I don't care where you are here, you can do this. So don't ever, uh, give me an excuse like that again. All right, so that's, that's enough of that. Um, I, he will be suing me. No, no, no. Actually Microsoft asked me to make an a zero video and I said no. So let's see them. Sue me again. All right, so here we go. Back to this. Where was I? I got a little off track. A little passionate about that one. Thank you. Back to the,

Speaker 1:          38:52          what about variable name?

Speaker 2:          38:54          Right, right, right, right. Thank you. Okay, so online 70.

Speaker 1:          39:00          Okay.

Speaker 2:          39:01          We have our TF dot input. Um, what, what went on here?

Speaker 1:          39:11          Yeah. Interesting. Looks fine to me. T F. Dot. Input Age.

Speaker 2:          39:22          Okay, so back to the retrieving the LSTM models. So we can say, well, here's our decoder and we already have, um, our decoder, uh, we're going to take that last layer of the decoder decoder. Lstm

Speaker 1:          39:40          good capital l.

Speaker 2:          39:43          Oh, I see, I see, I see. Capital l there is no Capitol Hill. Okay. So back to this, we're going to define our initial input by saying all those layers going through their Dakota LSTM um oh, capital items instead of Oh, I see, I see, I see, I see. So again, thank you capitol. I, here we go. Ah, Gotcha. Got It.

Speaker 1:          40:18          Cool.

Speaker 2:          40:21          All right, cool. So now inputting book Capitol Capitol. Great, please zoom guys. How much zooming do you want here? Okay, here we go. The current state of LCM inputs. Zero. So this is our initial input. Okay. This is our initial input and now we can initialize the model using the first input and the hidden states input, which we've already defined here. So we can say apply outputs equals decoder. Lstm how much time has passed so far? Do you coach? Thanks. No, I mean like how long has this streaming

Speaker 1:          41:00          decoder inputs state state. Cool, thanks.

Speaker 2:          41:12          All right, that's our outputs. All right. I have about 10 more lines of code here. Ah, so stick around. I have 10 more lines of code and I'm going to answer some questions at the very end. Okay. I'm going to answer some questions. So don't go anywhere back to this decoder states inputs. Right? So now I have those outputs.

Speaker 1:          41:35          MMM.

Speaker 2:          41:37          So what we want to do is we want to get that last output. It hasn't been activated yet. We haven't applied a dense fully connected layer to it. So we have both of our hidden states, by the way, we have both of them already, both the hidden states and the cell states. And what we can do is we can just say, let's take the first element of this apply outputs.

Speaker 1:          42:01          Okay.

Speaker 2:          42:02          Array or list and store that in the decoders hidden states. And we'll do the same thing for the cell state. And that's just going to be the next, the next one. And then we have our hidden, so these hidden states, we're going to get their own list. Now that we finally retrieved these hidden states, we can, we can give them their own list.

Speaker 1:          42:26          And so

Speaker 2:          42:28          these alone aren't going to do anything. We have to apply a fully connected layer to them. What this is going to do is it's going to output a probability value. Why do we want a probability value? That's what the dense layer is, because a probability value will allow us a probability value will allow us to make a prediction on what character is the most probable next character. Right? So if it's going to say you know, 70%, um, you know, h and it's very likely going to be h as the next character. And that's what we apply a dense layer to to it. So to get that final outputs, we're gonna use that dense layer

Speaker 1:          43:14          two. MMM.

Speaker 2:          43:24          To define that output. We have our decoder model or so that's our, that's our final output. We applied the dense layer to the outputs. Now we have a prediction. Okay. So the decoder model

Speaker 1:          43:40          is going to use,

Speaker 2:          43:44          it's its own TF model.

Speaker 1:          43:50          Okay.

Speaker 2:          43:51          And it's going to use as input to the decoders inputs concatenated with the states, the state inputs,

Speaker 1:          44:02          outputs. Thank you.

Speaker 2:          44:07          And we have our outputs, which are the decoder outputs. Okay. That's it for this little portion of code gets, okay, let's review what I've written here, by the way. Okay, let's review this.

Speaker 1:          44:25          So

Speaker 2:          44:33          my 97 right there we go. Making sure I got all my, I still haven't registered sublime text at this point. It's just become a meme. Like I will never register a sublime text because I just, I dunno why sublime better like sponsor me or something. I don't care now. I don't worry about it. I love you sublime. I don't know why. I just don't want to, I like being a rebel back to this.

Speaker 1:          44:58          MMM.

Speaker 2:          44:59          I, my friend Hawes Balaban Ian who is helping me live stream. That's you sitting next to me. So He's helping me with this live stream stuff. Okay. He's got to be our channel back to this. Uh, okay. So let's just, let's just go. Anybody can go to TF js examples and here's your, are all of these examples, by the way, you have your card poll. All right. This balances a car pull in the browser and it's got a live demo. Most of them have live demos in the browser that you can see. If you just click see the example live, I want you to for sure go to this, uh, get hub repository. If you haven't yet looked at, think training in the browser.

Speaker 1:          45:38          Okay?

Speaker 2:          45:39          Okay. And so are, so what I'm gonna do, I'm going to download this right now. I mean, download it. Let me click open. Let me go to it.

Speaker 1:          45:48          Okay.

Speaker 2:          45:49          Terminal. Let me go into it. Make this bigger. And now let's run this ourselves. So CD translation, and then I'm going to answer some questions right after I run this demo. Okay? So if we go here, let's, let's look at this. What does it say? Translation. Okay. It says to do two things. Yarn and yarn. Watch. So yarn, I can do that. So what yarn is, is it's a version of NPM that Facebook created. Remember,

Speaker 1:          46:18          uh, that that can,

Speaker 2:          46:20          that bypasses a lot of the issues with NPM at scale. And so once we've run yarn, now we can run yarn watch. And what yarn watch is going to do is in the browser at our local host, it's going to run that pretrained model. Okay? At local host, one, two, three, four. Now it's going to show up here, okay? That's the whole point is that it needs to show up. Now we're going to load that pretrained model and we're going to go ahead and say you are cool. Do [inaudible] that's my French, right? They're homeless. Do flow, mush, homeless dude format. Okay. Question Time. So just ask questions. I'm reading all of this in real time. Let's answer some questions here.

Speaker 1:          47:13          Yeah.

Speaker 2:          47:13          Uh, linked to everything is going to be in the video description. Definitely check out colab that research.google.com check out the get hub repository for tenths below dot. Js. I love you. The school of AI first course will start in a few weeks. I have big things planned for everybody. It's a very exciting time right now. And uh, yarn does not come with Hadoop. However, you can use yarn with Hadoop. Why use a different activation function h in one place? That's a great question. So, uh, check out my video. Which activation function should I use? Saroj just Google that. Any advantage of no dot js over Python r um, yes. One example is the ability to easily use distributed computing, right? So client side machine learning involves using the gps of whoever the host is. And if we use client side machine learning with Javascript, you can train it on their gps in real time. Now, you could do this with python using Django and flask, but it's much easier with javascript.

Speaker 1:          48:17          Okay,

Speaker 2:          48:19          three more questions. Uh, how can I make a related word to a specific topic? Ai. Okay. Word two, Vec Ghoul that word or no? Youtube that word. Two VEC Suraj. But you turn words into vectors and then you can find a similarity between these vectors. It's a numerical operation and we're, Yvonne says, I'm the cutest. I'm sure and passionate says deep learning with Django. It's not a question. Last question. Okay. The last question is the shawl. The shawl asks, can you suggest some good books for linear Algebra with regard to machine learning? No, I'm not gonna. I'm not gonna suggest good books. Why? Because you can very easily learn linear Algebra from three blue, one brown. Okay? If you search linear Algebra,

Speaker 2:          49:15          my buddy grant here, promoting them right now, essence of linear Algebra, definitely check out this series. Amazing playlist. This is enough. Watch all of this. Then watch math of intelligence by me. If I do say so myself on amazing playlist. Okay. This is when I was in Europe. I was just like going through that. I was just so hype. You know what I'm saying? Like I was totally in the zone. I was combining art and music and like everything together with machine learning, it was sick. Now I have to outdo that with my first school of Ai Ai course and I will. But that, that's, that's that question. Okay. So anyway.

Speaker 1:          49:58          Okay,

Speaker 2:          49:59          I love you too. Look, you guys don't need to qualify. I'm not gay. Okay. Even if you are gay, that's fine. Who Cares? Okay. People are gay. That's humans. But I love you too. It's okay for us to be able to say I love you as men, as women, whatever. Okay. We are beyond gender roles by the way. We are you guys, listen, we are moving beyond being a human. We're moving beyond these bodies, period. So forget about gender roles. We are merging into this super intelligent civilization using Ai. So don't even trip about like, I'm not gay. I love you by the way. And lastly, um, will it be free? Of course it's going to be free. Yes, it's going to be free. And I have so much content coming out for you this weekend. It's going to blow your mind. We are living in such an exciting time right now.

Speaker 2:          50:51          Next week we're going to have our first school of Ai meetups starting. I'm going to contact all the people who I've selected to be deans. It's going to be amazing. You guys are amazing. This is the most important community in the world. Nobody as the community globally is doing as much in terms of impact. And you know why? Because nobody's using AI as much as we are. We are the most AI aware, active, hungry, ambitious community in the world. And it's all thanks to you guys. All right, so that's it for this. I'm not going to rap because it's kind of laggy right now, but I will wrap, I promise you in the next livestream. Okay? So lots of reps are coming. I love you guys and thanks for watching.