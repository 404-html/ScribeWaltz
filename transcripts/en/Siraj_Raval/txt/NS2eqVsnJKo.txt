Speaker 1:          00:00          Hello world, it's Saroj. And can a computer make better usic than you? Maybe it's close to doing. So in this video, we're going to talk about AI to generate music and I've got a great demo for you. It's using deep learning to generate music. And what we can do is we can change the genre of music by tuning these sliders. Let's, let's take a little demo sample.

Speaker 2:          00:40          All right. All right.

Speaker 1:          00:42          Anyway. Okay. There we go. So that's the, that's the demo for this video. It's a simple web app. It's using a deep learning model in the backend at to, to generate these sounds that was pretrained on a Dataset of monophonic piano notes. And we'll talk about all of that, right? So we, we have to get to where we need to be. That means we have to start off with, um, hidden Markov models, then moved to recurrent networks and finally get to a generative adversarial networks, which is the demo that I'm going to talk about at the end of this video code included, of course, because we love code. If you don't code, you should code. All right? So anyway, so let's start with the history. Just one little, one little snippet of pre, uh, pre software. So before there was any software, uh, we were, music is older than language and before there was any software, the first automatic music came from nature.

Speaker 1:          01:32          So when chimes, for example, or the, or an ancient Greek wind powered Ionian, harp sores, Japanese water instruments, et Cetera, et Cetera, et cetera. But in the 17 hundreds, the first automatic music became algorithmic. So there was a German game that generated short piano compositions from fragments, which choices made by dice. Just want to give you a little fun little, you know, pre software before we get into the good stuff, just so you know that. Okay, so that was then, now we're getting into the 19 hundreds and then onto, onto now the Markov chain. Very, very cool model. It's used in weather prediction. It's used and it's been used in speech recognition. It's been used in a lot of fields. It's be, it's been using all of the sciences for sure. Biology, physics, Markov chains have been used extensively. But anyway, um, it's, it's a very simple, basic idea.

Speaker 1:          02:23          And I have this little slide right here to, to show, uh, a little bit about Markov chains that I'm taking from Clemson. But basically the motivation is in this case, it's always about predicting the next data in a sequence. That data could be a word, it could be a musical note, it could be some speech, but it's all about predicting the next data in a sequence. And what it is, is it's a, it's, it's a basically a graph that shows probabilities of going from one point to another and by points, those points could be data points, like strings for example, right? So if you have the word, what if I say the word, what? There's a certain that the next word will be is there's a certain probability to, the next word will be the, and based on those probabilities, this, the computer can traverse that graph to maximize for the probabilities so that the most likely word is said or generated.

Speaker 1:          03:14          And that's the same thing goes for music. And these had been used, like I said, and whether, for example, right, so if we have three states, sunny, rainy, and cloudy, and depending on the inputs, it could be any of those things. What a hidden Markov model does is it models all of those state transition probabilities from sunny to, to rainy to whatever it is, right? So let's say, you know, if it's rainy, then there's a 60% chance it's going to be, it's going to be cloudy, right? And it's going to be 60% less than it's going to be sunny, right? So there is, there was some relationship between all of these states that can be modeled as probability values. And that's what, that's what it's for, right? And so what it comes down to is just basic multiplication, right? Because when a hidden Markov model does, is it doesn't just, it doesn't generate completely novel data.

Speaker 1:          04:00          It generates data that is a subsection of the existing data, right? So the best way to explain what I mean by that is for us to look at a bit of a bit of code, right? So let's look at some code. For this hidden Markov Model. It's right here. This 25 lines snippet of Python code is a hidden Markov model. Okay? So in this case, we have two words. Okay. These are strings. Um, what we were going to do is we're going to create a table out of all of these words are, and then what we're gonna do is we're going to say, let's generate some output by learning those, those probability values between those words. So it's a learning that the likelihood that one word will come after another, and it's basically just picking a random word from the next word. And we can print that out, right? So this is a very, very simple model and it's, it's, it's just basic multiplication is what it boils down to. Multiplication from some existing set of data. Um, and then learning the probabilities of these words. Just one by one by one, right? So, um, one example of an artist that used a Markov chain to generate music was in 1958. His name was Ian is Xenakis and use it in his 1958 composition and a logic which we can listen to. Thanks to the power of the Internet, right?

Speaker 1:          05:22          Yeah. So that sounds terrible. But anyway, uh, that was what he did. So that's, that's what it was. It was, it was, it was modeling the probabilities of a, of a note occurring after a certain sequence of notes. That's what it did. And so that basic idea of modeling the probability of a note occurring next is kind of like the constant of using AI to generate music, right? Because when we have some piece of music, it's a sequence of notes, right? All music is sequential and there was some likelihood that a certain note to will come next after a certain set of notes have already occurred. And we can model that with a hidden Markov model. But I hidden Markov model can only produce sub sequences that also exist in the original data. But what if we want to extrapolate beyond those exact sub sequences and create really novel, um, data that's generated.

Speaker 1:          06:10          And that's where recurrent networks come into play. Recurrent neural networks. In 1989 the first attempt to generate music with the recurrent net was limited by their short term coherence. So someone tried to uh, generate music, uh, in his style, a Bach using recurrent networks in 1989 and uh, it was limited because it can only grasp short term sequences, right? So if if Bach hat of a huge piece with, you know, four different sections, it would stop, it would, it would forget about the beginning part in the beginning was very soft and the second part was very strong and the third part was like mellow and right. What a recurrent network does normally without any kind of variation to it is it will. So normally a neural network will learn the hidden state based on every new data point that it's fed. Right? Every new data point.

Speaker 1:          07:03          That's it. And it's learning, it's, it's, it's like, think of it like clay, it's like molding, it's internal representation to become robust against all variations of that input data, whether it's images of stop signs, whether it's a musical notes, it's learning some abstraction of all of that. So then using that obstruction, we can generate new notes. But what we're current networks will do is in every time step, it's not just learning based on the new data point, it's learning based on the previously learned abstraction. So at every time step that obstruction is fed, not just a new data point but an older version of itself. So there is a recurrence there. And what this does is it lets it lets it lets us learn from sequences. The problem is that a recurrent network can't learn longterm sequences because of what's called the vanishing gradient problem. I'm not going to talk about the vanishing gradient, but just search vanishing gradient Saroj on, on Youtube. But basically we want to wait to trap that memory that's being back propagated across time to get a little technical. And the solution to that was, you know, the yard, the solution was to use what's called a long short term memory network, an LSTM network. And let's listen to a bit of this. A recurrent network.

Speaker 1:          08:25          Okay.

Speaker 1:          08:30          Yeah. So that was better that we have to admit that was better than before. Not necessarily Hans Zimmer status, but it was better. Um, so, so Doug [inaudible] was one of the first in 2002. Now we're like 16 years ago, so we're getting closer to switch from using a standard recurrent network to what's called a long short term memory network and this improve the architecture. It improved what the, what the model could generate. Now Doug works in for the Magenta team at Google brain. Magento has been developing a code for generating music using machine learning and they are really on the bleeding edge of this stuff. Magenta has made so many cool different little products that we can, we can view in the, in the browser. But the thing is, even though we're current networks have been around for awhile, it wasn't very much common knowledge until Andre carpathy posted the unreasonable effectiveness of recurrent networks in May, 2015 which was more than a decade later, which is incredible if you think about it, the power of good documentation can move the masses to try some code. What Andrea used his recurring network on was generating Shakespearian text so it would train on Shakespeare text and then it would be able to generate text. In this style of what it had just trained on Shakespeare.

Speaker 3:          09:43          Uh,

Speaker 1:          09:43          so what I have here is a very simple LSTM network. Okay. So for just for us to get an idea of what it looks like, right. So

Speaker 1:          09:54          we are defining our nonlinearities, which are our activation functions, which makes sure our network can learn both non linear and linear functions. We then define a training datasets. We have our hyper parameters listed up here. We initialize the weights of our three layer neural network and then inside of our training loop we start training this thing. We performed the simple addition problem. We then update the deltas to show that hey, there's a certain change in the weight values and now that we've co computed a certain change in the weight values we can, we can update those weights by performing an optimization technique called back propagation, also known as gradient descent. See my video backpropagation in five minutes. What I need is basically just like a tree to just connect all of my videos across the cause. All of these topics. By the way, if you don't understand, just search to Raj and then the name of that topic. And I promise you I have a video on that. So,

Speaker 3:          10:45          okay. And

Speaker 1:          10:46          we're updating those weights were making sure that the gradient is not vanishing, that it's trapped properly inside of each of those synapses and it's being back propagated properly as well. Um, but yeah, so moving forward, so September, 2016, two years ago now, we're getting really, really close, deep mind published their seminal paper on what's what they called wavenet. Okay. So wave net was, uh, an an architecture that generated state of the art human sounding speech. Okay. So no model up to that point had made speech that sounded so human like, and people who were very surprised because no one had done that. And here's the real key to it. Here's the real kicker to all of this. Wave Net was not a recurrent network. It was a convolutional network. What? Right. So convolutional networks are used for learning from images and we're current networks are used for, for learning from sequences.

Speaker 1:          11:36          But what deepmind did is they use a convolutional network to learn to process images by treating time like a spatial dimension, which is very interesting stuff. Anyway, wave net was was it required massive amounts of Gpu like seriously and then later on someone made a fast version of Wavenet, which you can find on get hub, just search fast wave net. But then fast forward a year later, Magenta built on top of wavenet to create end synth, which we can try from the browser. It's basically a sound maker that lets us try out different instruments and combined them together and make all new, all new sounds out of it. So up to this point, there are some, there are some, there are certain important questions in this space that we have to answer rights. How do we decide on a proper representation of music? What music data should we use?

Speaker 1:          12:26          Whose music counts box or an Edm artist? You know, do we want to learn from the entire documented history of music with the big goal of producing something similar or something novel? Or should we try to construct entire compositions or, or to improvise with us? Do we want this to replace us? Do we want it to augment us? Right. That's, that's a major question for us to ask. Well, when it comes to startups in this space, and this is a part of the AI for business series, this is why I'm teaching you this stuff because we, there's definitely a space to create businesses that, that help artists improved their sounds using AI to help consumers be able to become artists in a way that they couldn't before. By giving them the tools to make orchestral sounds, for example, that were, that would require a hundred human people and you know, in the past, and you could just do it with a web app.

Speaker 1:          13:21          Now juke deck is one example of a startup that lets anybody create music. You could select the mood style temple in length and it uses AI to do this in the background. And there's a, there's a subscription model you can pay. You can get your first five songs a month free and then you pay seven USD attrac for 150 bucks. Creators can even buy the exclusive copyright for those songs. And these are songs that are generated by AI, which means their bottom line is zero, which means that they are making some money. This is a great business model genius. And, and there's, there's a, there's huge room in this space for more of that. Eva is another example, right? So what they do is they do music composition. It was founded last year in London and it's taught to compose classical music, which they already have clients, film, film directors, advertising agencies, game studios.

Speaker 1:          14:13          Ampere is another example. I talked about this with Taryn southern and uh, you know, a couple months now ago. Uh, but that's what she used. That was a tool that she used to, to help generate music. There was started by, it needs to be a film composer who wanted to make music that was more of a collaboration between humans and machines. It does basically the same thing. IBM's Watson. So this is not a startup. This is a big company, but even there are going into this space and they're using their cognitive technology. By the way, Ibm guys, let's stop using the word cognitive. Let's start using the word, you know, deep learning or AI or machine learning. There's nothing cognitive about what you're doing. Anyway. Don't hate me. Ibm. I still love you. Okay, so you know. Anyway, just let's stop. Let's stop trying to make it seem like, you know, the human brain is like inside of some server.

Speaker 1:          15:03          That's not where we are right now. Anyway, where were we? So architecture. Okay, so that's the history. Up until now, a couple of years ago, Ian Goodfellow released a paper called generative adversarial networks that allows for novel way to generate data. And I think that, and this is now, now we're going into, now we're going from objective to subjective, right? So I'm, I'm telling you my opinion now. My opinion is that the way forward is to use a generative adversarial network to generate sounds that would give us better results than a recurrent network, an LSTM network, a hidden Markov model, a convolutional neural networks. Even better than wavenet because gans are truly a new technology that they have not properly been harnessed. And video harness them pretty well for this paper on generating faces like in real time was very cool. But again, these are notoriously hard to train. There's a lot that can be improved of the space is moving so fast.

Speaker 1:          16:01          Um, and what I have here, by the way, is this good humbling, you should definitely check it out. It's called music generation with deep learning, huge, huge collection of resources for you to use. If you want to learn more about this space, papers from all across the board, from all sorts of countries, blog posts, code, uh, conferences and workshops related to music generation with AI applications that you can play with in the browser. Super useful stuff. Definitely check it out, but I want to talk about it again right now. Okay. So again, has two parts. It's got a generator and it's got a discriminator. And both of these, most of these are neural networks. Okay? So one's job is to generate data. The other job is to look at what the generator generated and discriminate it and say this is real or this is fake.

Speaker 1:          16:44          It's binary, zero or one. That's what it does. And so with the power and ease of care os we can create a generator in one line. That's right. One Line of code. We could create a neural network, define oldest perimeters and call it a generator. Okay. So that's our generator. What it will do. So in this example, it's generating an image in the style of it, this handwritten digits. But we can generate music sounds right. So what the generator will do is it will take in that input image. Okay. It will learn some latent representation. That's a collection of numbers and this in this image, you can see that and then it will, it will vary it a little bit. So it generates something entirely new. And this is a series of operations, right? And this, and it's going to start off totally random, right?

Speaker 1:          17:31          Like how would it know how to vary this input data? It doesn't know. It's dumb. It's just like, oh, let me just multiply by X. And then the vibe, okay, the, the real learning comes from this. When it generates that new sample, the discriminator, we'll say that's real or it's fake. At first, the discriminator, we'll be able to tell immediately that this is real. And this is fake because the discriminator has been trained on the training data as well. So they will know, oh, that's not a real sequence of notes that's fake. That because I know that that sequence of notes is a part of the training data. Okay. So talking about music, when that discriminator makes that a classification and says it's fake, then the generator will need to update itself. So using backpropagation, so both networks are back propagated so that the optimization technique will slowly shift those wait values.

Speaker 1:          18:22          And so the operations will slowly shift as well. So the next time that some data is going through the generator, it's going to be more likely to generate something that is harder to discern whether it's real or fake. And so the whole point of this process, I'm not like, I'm not like, I'm like not even reading the notes because I know gans so well now, but the whole point of this process is so the generator becomes so good at generating something that the discriminator cannot tell if it's real or if it's fake. And you can think of it as like a cat and mouse game. You can think of it as like a police and a counterfeiter. Eventually the counterfeiter gets so good that the police cannot tell if the money is real or fake, and that's what we're trying to do. The point of generative adversarial network is not the discriminator.

Speaker 1:          19:04          The point is the generator. That's what we want. When the whole thing is trained and to end, we remove the discriminator. We take that generator and we just start generating. In our case, it would be pieces of music and musical notes. Once we have those pieces of music, that is our song that we can then play. Now it's easier to generate music that is monophonic. Right? So the, these are a single notes, piano notes. We get whatever Harb notes, but most music comes in the form of chords, right? Collections of notes, polyphonic and that's a little bit harder to train, but I think again, these are the way forward to to, to make that happen. Okay, so that's how that goes. I had this sample here called Gan music and basically if you want an idea of what it looks like programmatically, here's what it is, right?

Speaker 1:          19:51          So these are our parameters for both D, which is the discriminator and g, which is the generator. And these are just wait values and bias, values, input times weight, add a bias activate. That's how every neural network works. Just remember that. So now we have our generator and our discriminate. We define them right here. Here's our generator, here's a discriminator. We were using tensor flow to create both of them. We can plot them, see the difference. We can create a sample and say, okay, here's the real and here's the fake. We have two loss functions, one for the real, one, one with for the fake, and then we solved them both using gradient descent. This is our optimization strategy. And at the end we can print it out and we can plot is very simple. This was like hundred 36 lines of code with tensorflow to create again and you just drag and drop your music dataset and boom, you're good.

Speaker 1:          20:39          Um, so yeah, this, um, this demo, it's on get hub, check it out as well. Check out my links. I have some great links for you in the video description. There's a lot of potential to use AI to uh, create services for both artists and consumers regarding music. And really if you want to extrapolate to all sorts of entertainment, everything I've talked about, your can be applied to text to, you know, essays to songs, to poetry, to images, to video even, right? All of this, all of this is sequential data and because all of this is sequential data, we can, we can, we can generate it, we can learn from it. Then we can generate new sequences and this can help all sorts of people. There's a huge business use case for this and now is the time because all these tools are just now starting to be democratized. We now have access to a GPU is via cloud providers. We now have access to algorithms. We now have access to data sets. We now have access to education, right? So all this is being democratized. It's your responsibility to do something about it. Um, that Santa my Spiel, and I hope you found this video useful. Please subscribe for more programming videos. And for now, I've got to listen to some music, AI music. So thanks for watching.