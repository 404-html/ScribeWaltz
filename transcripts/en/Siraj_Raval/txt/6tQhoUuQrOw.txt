Speaker 1:          00:00          Hello world, it's a Raj. And our task today is going to be two. Tried to predict if a team is gonna win a game or not. Now this is for football or as Americans call it soccer, which is one of the most, which is the most popular game globally when it comes to sports. And of all the domestic teams out there, the English premier league is the most popular of all of them. So we're going to predict the outcome for an English premier league team using a Dataset of past games. And this Dataset I'll show it to you right now, has a bunch of different statistics. This is what the data set looks like right here. You've got a home team and you've got an away team right here. So it could be arsenal, it can be Chelsea, Brighton, Manchester City. So you've got a home team and you've got an away team.

Speaker 1:          00:44          And then you've got a bunch of statistics. So these are all acronyms. Uh, but I have definitions for all of these acronyms that we can look at right over here, right? So we have acronyms for the, uh, full time home team goals, the home team, the away team, the shots, the target, the coroner's, the amount of yellow cars, the amount of red cards. So there's a lot of different statistics here, right? There's so many things that go into what makes a team win or lose, right? And so we're going to take all of these features and then we're going to use them to try to predict the target or the label. And the label in our case is going to be the FTR, which is the, uh, full time result. So the FTR is right here, right? H a H. D, right? So it could be either the home team, h, the away team a or a draw d.

Speaker 1:          01:36          So it's a multi-class classification problem. This is not a binary classification problem. It's not just the home team wins or loses, it's multi-class cause there are three possible labels, home team away, team or draw. So that's what we're going to try to predict given all of those features in the Dataset. Before I show you the steps, let me just demo this really quickly. So I can just say x test and then just take the first uh, row from this. And the labels are gone. This is just for the all the features given no label and we could see that it says home, right? So it's able to predict, given all of those other features, whether or not a team is going to win, lose or, or tie the game. Okay, so backup to this. We're going to try to predict a winning football team and our steps are going to be a, it's a four step process.

Speaker 1:          02:21          So our steps are going to be to first clean our data set and made sure that we only use features that we need. What do I mean by that? When it comes to predicting who's going to win a team? There's an entire industry around this right there. Our pregame analyses by commentators are postgame analyses by commentators entire channels like ESPN or dedicated to trying to predict who's going to win a match. And in fact, even during the game, there are commentators trying to predict who's going to win like during halftime, who's going to win the full game. So this is a, this is something that's been going on for forever, right? Since gladiator gladiator Roman days or whatever it's been going on, right? People are trying to predict who's going to win a match, but we're going to do something that people don't do often and that is using statistical analysis or otherwise known as machine learning, mathematical optimization to try to predict who's going to win.

Speaker 1:          03:08          If you think about it, this is like one of the most perfect machine learning problems out there, trying to predict who's going to win. Think of all the features out there and those features don't necessarily have to do with the game. They could be the sentiment of the audience, the sentiment of the crowd of news articles. How are people talking about a team? What hashtags related to the team or trending on Twitter? Are they home? Are they away? What's the weather like that day? What are the forecast predictions? So there's so many different data points that could go into potentially up from across the web telling us whether or not a team is going to win or lose. But since I've never talked about this topic before, I'm just going to start off from a very basic level and based on your feedback and how you feel about this topic, I can talk about it more and do more advanced things later.

Speaker 1:          03:51          Okay, so we're going to clean our Dataset, then we're going to split it into a training and a testing set. And what I mean by that is we're going to use psychic learn to do that. I have still, I have yet to find a better library for splitting training and testing data. Then psych it, learn it is still like the best out there. Even if I'm using tensorflow or Pi Torch to build my model, I'll still use psych. It learn to split my training and testing data. It's just a one liner, super simple. And then once we split it, we're going to train it on three different classifiers. So remember this is a classification problem, a multi-class classification problem. And so we're going to use either at logistic regression, a support vector machine or, and I've talked about both of those in my math of intelligence series, links to those in the description.

Speaker 1:          04:33          Uh, but I'll also talk about them a little bit in this video just as a refresher. And the third one is a model that I haven't talked about before and that's called xg boost. Well, you could think of it as a technique model, same thing. So we're gonna use those three as our classifiers. We're going to train all three of them on the Dataset and then we're going to pick the classifier that has the best result. And that is going to be the classifier that we use to predict the, the winning team. And we're also going to optimize its parameter. It's hyper parameters using grid search, right? So we're using an ensemble of machine learning methods, which cycle learn makes it very easy to do. Once we pick the right one, then we'll optimize that model and then that will take that optimized model and use that to predict the winning team.

Speaker 1:          05:14          And so the history of this is, like I said, it's been going on for a long time and sports betting has just been increasing in popularity for many years, right? If you look at the past five years, it's growing at double digit rates and there's a lot of reasons for this. Number one is just the accessibility of the Internet's rights. More people have Internet access and and betting on the Internet it's easier than in person. Uh, another reason is just that machine learning is becoming democratized and so everybody's being able to build these predictive models, uh, to try to predict these scores. So this is, this is definitely a field that's increasing in popularity and, and this is not something that's happening in the fringe of society. This is a very mainstream task. Kaggle, the data science community hosts this yearly competition, March madness or machine learning mania, whatever you want to call it, to try to predict the scores for the NCAA.

Speaker 1:          06:03          That is basketball and you have an entire community around this and people are trying out different models and discussing them. So definitely check that link out as well. So this is something that's happening. And I also found several papers talking about this. So it's not just something that people who want to make money do. This is something that legitimate researchers at academic institutions look into and try to try to predict. Right? So from this paper I, I'm quoting verbatim, it is possible to predict a winner of English county 2020 cricket games and almost two thirds of instances, right? And then for this other paper right here, something that becomes clear from the results is that Twitter contains enough information to be useful for predicting outcomes for the Premier Li. That's for Twitter, right? Right here. So they use Twitter sentiment to try to predict just Twitter alone to try to predict who's going to win.

Speaker 1:          06:53          So there's a lot of different angles we can look at here, right? We could use sentiment analysis, we could use the past score history, we could use a whole bunch of different things. We're going to use a score history, but you could try to simulate the game and a simulation and then, you know, try to see from that. But you know that there's a lot of different possibilities here. And check this out in 2014 being which is owned by Microsoft correctly predicted the outcomes of for all the 15 games in the knockout round for the 2014 World Cup, every single game, 15 of them will 100% accuracy. So you can be sure that beings model is really good. However, they are not going to share it with us because it's, it's of like, you know, a financial analyst at JP Morgan or chase. If they know how to predict these stock prices, they're not going to tell us why would they share their profits with us.

Speaker 1:          07:43          So what we've got to do is we've got to figure it out for ourselves to try to reverse engineer the techniques so that we can benefit from it. Okay, so that was a little primer on the background. So back to the Dataset. So this data set that I got is from football data.co. Dot Uk. You can find it right here if you go to slash data dot PHP. And then what I did was I selected the England football results. And Luckily for us, they've got data sets for every season back like two decades. So it's perfect. And if you want one, you can just click on premiere league and boom, it downloads just like that. And I showed you the data set. So one thing right off the bat that we can notice is that if we were to just graph, and I've already done this beforehand for us and it's in mark down right here, we'll see that the home team has the majority stake of this graph.

Speaker 1:          08:29          So that means right off the bat, without doing any machine learning, we already know that if you are a home team, you have an advantage to win. Probabilistically speaking, if you're the home team, you're more likely to win than if you're not just from v from that alone. And we can reason about this a couple ways. We could say, well, if you're the home team, then you know football's a team sport and the cheering crowd helps you and you don't need to travel for, you're less fatigued. Uh, you know, you're familiar with the pitch and the weather conditions, all these things. You had a hot dog from the standard and it tastes really good. Just kidding. Uh, baseball, food or any kind of sports or like stadium food is never good. You know what I'm saying? I've got two great repositories for us. I'm about to start the code here, but I've got one for another EPL prediction, great.

Speaker 1:          09:12          I python notebook or Jupiter Notebook and I've got one for that. That Kaggle competition that I just talked about for NCAA prediction. Definitely check them both out. And this guy, a dish pundit has really great tutorials and software on his get hub. So just check out all of his, uh, repositories cause he has some really great example code. So what we're gonna do is I'm just going to code out a good part of this just from the start and then we're going to just go over the rest. Okay. So don't save. All right, move, move, move, move, move. Okay, so first things first. So our dependencies in this case are going to be to import pandas for data preprocessing. We want to import pandas because that's like the most popular data processing library. And we also talked about xg boost, right? That is one of the other machine learning models that we want to use a which is going to form a prediction model based on an ensemble of decision trees, which I've talked about as well as decision trees.

Speaker 1:          10:12          Uh, and so another thing we're gonna do is we're going to import a logistic regression, right? That's model to have three. There are three different models that we're going to train a, our data set on. One of them is xg boost, the other is logistic regression, which is used whenever the response variable is categorical, right? Either yes or no or you know, some kind of a non continuous discrete value, you know, black, white, red, green, you know, things like that. So which is perfect for us home, you know, Ho a win, lose or draw. So we have a logistic regression and then we have one more, which is going to be the support vector machine, right? It support vector machines. I'll talk about that as well. And then finally we're going to want to import this display, this display library, because we are going to display our results.

Speaker 1:          11:02          Okay. So that's it for our dependencies. And now we can read our data set. So now we're going to go ahead and look at pandas and paint is gonna is gonna. Let us read from our CSV file that we downloaded that I've called final dataset dot CSV. And then once we have that, we're going to preview that data. So I'm going to say, okay, just go ahead and display the data that I've just pulled into memory as a pandas data frame object. I'll look at its head that is just the first few columns of that Dataset. And once I have that, oh, can I can go ahead and print it. And now we can see this, this data set, what it looks like. And so notice there's a whole bunch of acronyms here. Lots of datasets have acronyms like this and they can be confusing.

Speaker 1:          11:42          But like I said, I've got this, uh, legend of what each acronym means. Uh, the home team gold difference, the difference in points of difference in last year's prediction for the past three games, the winds for the past three games, for the home team, the number of wins for the past three games for the away team. So, you know, I've kind of aggregated this data and I've just made it into something a little more consumable. And so still remember that we still have one single target that we're trying to predict. And that is FTR, right? The fulltime result for the full time game. Who is the team that won the home team, the away team or was it a draw? And so that's our target that we're trying to predict. So before we get into building this model, let's first explore this data set. So if we were to explore this Dataset, we could say, okay, so w first of all, let's just kind of think about what is the win rates for the home team.

Speaker 1:          12:35          So what is the win rates for the home team? So how often does the home team win? Aside from anything else? This is kind of what we just talked about right? How do we do this programmatically? What we say, okay, get the total number of matches and that's going to be that first index in the data frame object and then calculate the number of features from it. So we want the number of features and we'll subtract one because one of them is going to be the label. That's not going to be our feature, right? The FTR. So we'll subtract one from that. And then we're going to calculate the the matches one by the home team, which is going to be the length of the data. All right, Tom Dot ft FTR. Okay. As for that for the home team. So that's number of matches that are, that were won by the home team. And finally we'll calculate the win rate, the win rate for the home team as well.

Speaker 1:          13:33          And then once we have that, finally we can print out the results and it's going to tell us exactly how many times the a home team has won as a percentage of all the wins. So I can go ahead and print that. I've got this print statement right here and then we can go ahead and see the result. Okay, so already this is the, this is the graph that I showed at the beginning. 46% about 46% of wins are from the team that is home just right off the bat. Just something for us to know, right? We're, we're exploring the data, we're trying to think about what are the features that matter the most, right? Feature selection. That's the process that we're going to now we're going through. So I remember when it comes to deep learning, we don't have to really think about what are the ideal features.

Speaker 1:          14:16          Deep learning learns those features for us. However, that's like a next step. We were just going to try to build some more basic models first and then you know, whether or not, you know, based on feedback of how you guys liked this topic, I might do a deep learning video on sports analytics later, but right now we're just going to build these three simple models and thinking about feature selection, uh, is a really important skill to have as a data scientist. So if right, which deep learning, you don't have to do that. But again, you've got to have a lot of GPS and crucially youth to have a lot of data, right? You have to have a lot of data to be able to do that. Now in this case, we don't have that much data. We in this dataset set downloaded in like, you know, two seconds of course, uh, it, it was only 500.

Speaker 1:          14:58          It's only about 500 data points, right? We want huge amounts of data, at least a hundred thousand. Now if we had at least a hundred thousand data points, then this would be something to use deep learning for it, right? If we're trying to aggregate a bunch of different results, sentiment from Twitter, uh, past team scores, a different, you know, talking points from other people, then we would use something like deep learning. But in this case, we want to try to visualize a distribution of this data. So what we'll do is we'll say, okay, so from pandas does this great tool that lets us compute what's called the scatter matrix. And the scatter matrix basically shows how much one variable affects the other. So we're going to scatter matrix for a set of our features to try to predict, to try to see just visually what is the correlation between these different features and see just for ourselves this, this, this will help us pick the relevant features that we want to use.

Speaker 1:          15:52          Right? So we have the home team gold difference, we have the away team gold difference, we have the home team points, the away team points that difference in points and then the difference in last year's prediction. Okay. And so once we visualize this, some of them have a positive correlation, the line is going up, some of them have a negative correlation. So that means like in terms of, so that means if the goals increase for the home team, then maybe the points decrease for the for the away team, right? And so that we can look at the positive versus negative correlations. That's an indicator of how features are related together. Right? This isn't have some direct relation to what we're about to do, but it just good practice to think about ways of visualizing our data, seeing the relationship between, between different features and then tried to predict what those best features are for our model.

Speaker 1:          16:36          Okay. So then once we've explored our data, we're going to prepare it. So remember we have one single target variable, one single objective or label as we like to call it. And that is the FTR, the fulltime result. So what we wanna do is say, given all of those other features, try to predict the FTR. Okay. And make us some money. Yeah, no, I'm just kidding. I mean, yes, actually you probably want to make some money. We're trying to predict the full time results, right? And so we're going to split it into the FTR and then everything else, then we'll standardize it, which means it's all going to be on the same scale. We, that means we want all of our data to be an integer format and we want it all to be on the same scale. So it's not like we have like one feature is in the hundreds of thousands.

Speaker 1:          17:17          And then the other feature is in the, you know, between one and 10. If we're, if the, if they're going to be small values, we want them all to be small values. And what this does is it improves our prediction capability of our model. So once we've standardized our data, then we're going to add these three features, which is the, the last three wins for both sides. And we looked at that before, right? H M one, two, three and then a, an an a m one, two and three. So if we look back at the data, some of the data was categorical. Like if we look at this Dataset, you know, you have the referee, we have HTR, we don't want any of that, right? We want all of our data to be, uh, either a number. We want it to be some continuous variable, no discreet numbers.

Speaker 1:          17:56          So we're going to preprocess those features by saying, create a new data frame. Find those feature columns that are categorical by saying, if it's, if the data type is equal to equal, equal to object instead of an integer and then convert it into an Integer, right? So that way we remove all the categorical features. We only have one categorical variable and that is are labeled the FTR. We don't want our features to be categorical. Those are going to be continuous variables. And so once we have that, we've preprocessed our data, we've explored it, we've added the features that we thought were most relevant and we could see them all here, right? No more categorical features, they're all numbers. And so once we have that, now we can train and we can split our model into a training and a testing dataset. If within very easy one liner would psych it learn, right?

Speaker 1:          18:43          And this is going to split our, with the train train test split function, it's going to split that CSV. It's going to split that data frame object into a training and a testing set and it already knows what the label is going to be and it's going to put them all in a one dimensional array. All of those labels, the FTR scores for each of the associated inputs. And we have 12 features, right? We have 12 features for a single input. And so for the next step, now we're going to actually build this model. So I'm going to come back to these helper functions that are going to help us train the model. But let's right now just build this model, right? So I'll go down here. So let's just write this out right now. Okay, so I'm going to say, okay, so we know that the first model that we want to try out, or at least one of the models that we want to try out is logistic regression.

Speaker 1:          19:29          I'll give it some random state as a seed that, you know, this could be any number of things, right? Well, I'm just going to say, you know, 40 I could say 42 it doesn't matter, but just some seed number. And we could try out different seeds to see how the results vary. But I'm just going to, you know, put some magic numbers down right now to, to, to get some results out. And so the next class if I were going to build is a support vector machine. So the order of classifiers as I initialize them, it doesn't really matter. So, so that's irrelevant. But the fact that I am a initializing them is important because it means that these are the three important ones that we are using. And so my third classifier is going to be xg boost. Now I'm going to talk about what all of these are in a second, but let me just write them out here.

Speaker 1:          20:12          We have an xg boost classifier and then we have a seed. It's going to be maybe too, let me print that. Boom, boom for a, B, and C. Okay. Right? And so if we train this, we'll see that clearly. Uh, the xg boost library did the best. So we already know that xg boost is the best model for this data. And notice that the xg boost model had an accuracy score. And an f one score of about 74% that is a 74% accuracy on the testing data sets, which is a really good, it's really good. It's better than just guessing, right? It's way better than just trying to guess what team is going to win. That's about a 75% accuracy is pretty good. So let's, let's go back and see what these models are, by the way. So these models, so logistic regression. Now remember I have a video on logistic regression and I have a video on support vector machines.

Speaker 1:          21:09          Just search both of those on youtube. And then the word Saroj, it'll be the first link that shows up. But for logistic regression, it's used to four to predict the probability of an occurrence of an event by fitting data to logistic curve. So a logistic curve consists of this equation right here, that probability, right? So if you have two classes, if it's a binary classification problem, whether or not someone is dead or alive, the x axis would be the concentration of the toxin. Whether you're, you know, you're trying to predict if someone's going to live or die based on this toxin and the y axis is going to be the probability of each of those classes. And we use a logistic regression curve to noted by this equation where you just plug in the x value and it will output a probability to show that.

Speaker 1:          21:52          Now in the multi-class case as is our problem as is our problem, we're going to use a multinomial logistic regression which the library does for us, but that's what the logistic regression does. Its use extensively across a wide range of fields and uh, it's a very, very popular model. That's going to be our first, and these are all classification model by the way. Remember, once we frame our problem, then we can pick what model we want to use. We know that this is a classification problem, therefore we wouldn't use a model that is well suited for classification. And then the next question is based on our data, which of the models is best to use? And we don't always know that right off the bat, even very experienced data scientists don't always know that. So they have to try out several models to see which one works best.

Speaker 1:          22:38          And then for support vector machines. So when a support vector machine does is it will find, so let's say we have two classes and we plot them in two dimensional space, just like in this image right here. What it will do is it will try to find the points that are closest to each other to find the smallest margin between both classes. And once it finds these points, these support vectors, it will build a hyperplane right in the middle. So that the, so the distance between that line and both of those points is the smallest. And the reason it does that is so then once we give it a new Dataset, a new data points, whatever side of the line it falls on, that's the class that's going to be, that's how we classify it. So in the simple case for two classes, it's just a line and then it just falls and the data will fall on one of two sides.

Speaker 1:          23:22          But in the more complex case, you'll have all three classes and then it will draw a line that's kind of curved between them. So it just like segment the graph into the three different segments. But the idea is still the same. Finding those closest points and finding the line that minimizes the margin. Okay, so that's for support vector machines. And the last one is xg boost. So we talked about random forests. It's kind of, it's, it's very similar to a random forest. The xg boost algorithm is one of the most popular algorithms on Kaggle when it comes to winners. A lot of xg boost is happening, but basically the classification and regression tree, the decision tree that's used for both classification and regression is a good model. It's not a great model, but it's a good model. It's a very simple model, right? You give it a bunch of features and it's going to slowly build.

Speaker 1:          24:08          There's a variety of ways of doing this, but it's going to build a tree where each a branch or level in the tree equates to one question, right? So if you're trying to predict whether or not it's going to rain, it will be like is the, is the, is the sky cloudy? Yes. No. Yes. Okay. Did it rain yesterday? Yes. No. Yes. Okay. Then there's a 75% chance it's going to rain. So that's a decision tree, right? So what xg boost does, it's a gradient boosting technique. What it will do is it will create a bunch of weak learners. Those are decision trees that are okay, like their, their predictive capability isn't that good. And it will combine the results of all of them. So it's an ensemble method. It will take all these trees and then find a result by using the prediction capability of all of those trees.

Speaker 1:          24:51          So that's for xg boost. And we have this tree right here where we have input, age, gender, occupation, a bunch of different features. And we're trying to answer the question, does this person like computer games, different trees will specialize in answering different parts of the question. Like does this person uses the computer daily? What's their age, what's their gender? And then we'll combine the results from all of them. So the function of this kid is the result of these two trees, uh, predictions combined. So that's what each of those are. And then if we go up here, back to these a helper methods that I was going to talk about. And by the way, the f one score is just the measure of a model's accuracy. It's a very standard score that just measures how accurate a model is. So back to these three helper methods, uh, what we did here was we gave the a train predict function, our classifier as well as the training and testing datasets.

Speaker 1:          25:42          So if we go back up here, we'll see that in the train predict method, we indicated the classified that we're going to use. Then we trained it given the, uh, training and testing data. We predicted the results and then we predicted the labels. And so this train predict method, use the train classifier, my method to start a clock, fit the model and then a print the results. And then for the labels it started a clock, made a prediction and then stop the clock. So that was it. They just predicted the labels and then it fit the classifier. Just that. And so once we did that, we realized that xg boost gave us the best result, right? So xg boost is the model that we want to use a 74% accuracy. So, but that's not enough right? Now that we know that xg boost is the best model, now we can say, okay, let's optimize this model and what their different ways that we can optimize this model.

Speaker 1:          26:31          Right? But in our case, we're going to optimize it by optimizing the hyper parameters. So this is hyper parameter optimization. There are a bunch of different hyper parameters that go into xg boost. So we were kind of shielded from that because we use the psychic learn library. But we can use psychic learn ironically enough to optimize hyper parameters that we don't even normally see. So if we'd come down here, we can import grid search, which is a Buddhist basically brute forcing that. That's what grid searches. We're brute forcing all the possible combinations of all of the hyper parameters. We'll create an initial set of hyper parameters here. We'll initialize the xg boost classifier, will make an effort and scoring function and then perform grid search on that classifier with the scoring function. Given the initial parameters that we just defined up here, and then it's going to find the ideal parameters for that model and notice that the f one score and the accuracy score increased, right?

Speaker 1:          27:26          So after we optimize the hyper parameters, the F1 scoring, the accuracy score increased, which means that our model is now way more optimized anyway. Disclaimer, you know, you could make money using this, you could lose money using this. Who knows, right? This is a, this is a, this is an educated guests. This is a statistical guests based on past data sets, but we can definitely improve this model, right? We could bring in more data, more relevant features. We could bring in sentiment analysis, we could add other features. But there's also one more thing that I want to say. So it's able to predict whether or not the home team will win, right? But based on a data point from this CSV file, however, we don't always know what all of these things are. How are we supposed to predict whether or not there's going to be, you know, x number of fouls?

Speaker 1:          28:13          How are we going to, how are we supposed to know whether or not there's gonna be x number of offsides? We don't necessarily know these things beforehand, these features beforehand. So the trick is to pick features that are completely predictable. What do I mean by that? That means that what are features that are going to help predict who's gonna win? But those features themselves are predictable. Like how many players are going to be on the team? Well, you know, for a fact they're going to be five players on the team. Who are the players going to be? What is the lineup? When is the game happening? Where is the court? Right? So things that you know for sure and so it's all of your features are known, they're all predictable, then your result will not require you to guess what those features are, which is the case in this very basic example.

Speaker 1:          28:56          Another, another way to improve the model is to just use way, way, way more quality data. We could also just predict each of these features themselves, like we could try to predict how many goals are going to occur in a game. We could try to predict all these things and then just have probabilistic values for all of these features and then try to predict the home team, right? So there's a lot of machine learning that could be happening, but ideally we know for sure what all of these features are going to be and we can use them to then predict the winning team. So if you're interested in this topic, definitely check out all the links in the description and let me know what you guys thought of this topic in the comments. Please subscribe for more programming videos. And for now, I've got to play some soccer, I mean football. So thanks for watching.