Speaker 1:          00:06          Oh,

Speaker 2:          00:08          this incredible cool world walking the Sira geology. In today's episode, we're going to talk about building an AI artists. Art is the embodiment of the human experience. It's a synthesis of all of our emotions and experiences in life. You might be asking yourself, is it really possible to build something that can do this? The answer is yes. It's incredible. We can train a neural net to learn an artist's style and tell it to modify a picture into a painting. In that style. It all started when the Google research team released a blog post called inception ism. They trained a deep convolutional neural network on a huge data set of images so they can start to detect everyday things like dogs and buildings. Once it was able to do that, they give it a novel image and asked to detect an object they had learned in the image.

Speaker 2:          00:45          If it's not something in the image, I look even slightly similar to what they had already learned. Say a cloud that kind of looked like a dog. It would then modify. The image will look more like a dog. This resulted in some pretty crazy pictures. Then another group wrote a similar paper called Neuro Algorithm for artistic style, but they repainted in imaging the style of another using famous paintings as the base image, so when they trained their neural net on starry night and gave it a novel image, it would modify. The image will look more like starry night by are defying all of its features. We've gone over convolutional neural nets in a previous episode at a high level. We're going to really deep dive into the code for this one and try to understand exactly how this process works. We're going to go through the necessary code to recreate the results from the neural style paper in python using deep learning library care Os.

Speaker 2:          01:21          Let's get started. We'll start off by defining our arguments. When we run our script, we defined the base image, the style image, the output image or creating variables for them. Then reference a precomputed weights file called Dgg 16 these are just a bunch of precomputed synapse weights trained to recognize everyday images, which will later add to our neural net as a starting point. We'll also want to initialize bullying's that define whether or not to rescale our image or maintain their aspect ratio based on user input. Then we initialize our variables for style and content. Wait, so what do we mean by style and content weight? Well, in the neural style paper they found that when they train a neural net to recognize the painting style of his artists, the learn features in the initial layers were style base. Like you know how when you train a neural net to recognize a dog, initial areas detect edges and the next one's in tech shapes.

Speaker 2:          02:00          Then more complex shapes and a whole dog. Well those same layers of abstraction apply to paintings, but what they found with the initial layers, the edges and curvature and other low level features equated with style. They also found the highest few layers were more based on content. So when the starting night photo, the higher levels would be that dope son thing and perhaps a collection of stars. The lower levels would be the curvature of the night sky and the color scheme and this way soon it's helped us separate content from style and mirror the capabilities of our biological vision. We then need to define how much we want to weigh one or the other because we can optimize for one of them or both depending on how much we weight each. We'll get a different output. Will Sarah image dimensions and then create a tense or representation of our base image, style, image and output image, and we'll combine all three into a single input.

Speaker 2:          02:38          Tenser and tenser. It's a multidimensional array. An example would be like colors, textures. Each of those would be raised as well, so colors would be its own array containing the seven main colors. Then each of those colors would be an array of sub colors. We convert our image with single tenser because it's an easily possible data structure for our neural network. Tensors help reduce the high dimensionality of our images and that in turn reduces the complexity. Now we're going to want to build our model, which is going to be a convolutional neural network. We'll add in our input tensor as the first layer. Then we'll start to finding the other layers. We're going to add in 31 layers to our neural net. There are three types. The convolution two layer, it means it has a set of learnable filters which have a small receptive field.

Speaker 2:          03:14          The receptive field is a subset of filters that are used to connect the neurons to a local region in the next layer instead of every single other neuro zero padding layer helps us control the size of the output volume by padding zeros across the border. Then there's the average pooling layer pulling as a concept in is where we take the input image and split it into a sec or pool of rectangles. Pulling helps us avoid overfitting and reduce the amount of parameters can computation by only using a subset of the image as representation rather than all of it. The idea is that once a feature has been found, the exact location isn't as important as it's rough relative location to other features. We'll take the average value from the pool. The activation function is called [inaudible] or rectified linear unit. The relo function is faster than sigmoid without a huge difference in generalization accuracy, so we'll use that to the numbers.

Speaker 2:          03:52          Here are the number of output filters and the length and width of the input image well named each of our layers as well. For reference. Now that we have our model, we'll add into precomputed weights we called earlier. Then we're going to define our loss function for the style content and Total Variation, Aka the uniformity of the image. Last functions help us calculate the difference between the expected output and the actual output. We're going to take these loss functions and combined them into a single scalar or number. Then we'll get the gradients of the generated image, usually the loss which helps us map the color scheme. Our last step is to train our model by minimizing the loss using backpropagation. The backpropagation algorithm will use in this case is called limited memory bfgs LBF. GS helps minimize our loss function and it's space efficient and that it only stores a few key vectors.

Speaker 2:          04:28          Instead of all of that minimizing the loss function needs, modifying the output image iteratively so it looks more and more similar to this artistic style we want Osi Pi gives us a technique as a built in function and after training is over, if we can rescale it and save the output image, that's about it. This is what happened when I tried out the algorithms. You could also apply this algorithm frame by frame to video content with more data and computing power. It's only going to get better. I hope that in the next few years we'll be able to generate not just paintings, but other forms of art like sculptures and interior design and facial hairstyles. For more information, check out the links down below and please subscribe for more ml videos. For now, I've got to go fix a compile time error, so thanks for watching.