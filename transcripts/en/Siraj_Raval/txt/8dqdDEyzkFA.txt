Speaker 1:          00:00          Guys, I think I found it classify tree of life hello world, it's Saroj and the most exciting class of machine learning techniques is called unsupervised learning. Teaching machines to learn for themselves without having to be explicitly told if everything they do is right or wrong is the key to true artificial intelligence and perhaps the most important research goal of 2019 I mean, how else are we going to get to fully automated luxury gay space communism? In this episode, I'm going to give you a broad overview of this area as well as teach you two of the most popular unsupervised learning techniques. Principal Component Analysis and k means clustering in order to save someone's life. A patient at a hospital has been suffering from several epilepsy related seizures. Luckily we have a Dataset of their neural activity recorded by electrodes that were inserted into their brain. The lead surgeon asked us to use unsupervised learning techniques on this neural data to find out what part of their brain is causing the seizures so they can perform surgery on it. Well, we save the patient's life. We'll find out at the end of this video and subscribe. If you want to keep learning about AI technology for free, we can divide machine learning into two types, supervised and unsupervised. There's also reinforcement learning, but that only applies in a real time environment, not a static data spreadsheet. There's also quantum machine. Can you please keep it simple for once?

Speaker 1:          01:38          Supervised learning is synonymous with pattern matching. It's done using the ground truth, meaning we have prior knowledge of what the output values for our input data. It should be, you know that hot dog, not hot dog classifier trope from the popular show silicon valley that's supervised learning. My life is literally that show, so I don't watch it. The goal is to approximate the relationship between input and output data. Most machine learning across every industry is done this way. It's easy, it's straightforward, and it tends to perform very well if given enough examples, but clean, perfectly labeled datasets aren't always easy to find. In fact, 80% of the world's data is unstructured. The goal of unsupervised learning is to automatically find structure in a Dataset. This can itself be the goal. Discovering hidden patterns in data or a means to an end to learn what the most relevant features are.

Speaker 1:          02:35          We can further subdivide unsupervised learning and to different types of techniques. Clustering finds data points similar to each other and groups them together. If we had any kind of population data, whether we were a government organization or a startup with a product like diet, water, yes, that's real. Basically anyone trying to reach a certain set of people. We want to segment that population into smaller clusters with similar demographics and purchasing habits so that we could target them most effectively. Spending our marketing budget. Anomaly detection finds the outliers in a collection of data points banks uses to find fraudulent transactions. Association finds correlated features between data points. Then lets us infer other features of a given data point airbnb uses to recommend other listings you'd probably like and dimentionality reduction reduces the number of features in a dataset which makes it easier to visualize and interpret.

Speaker 1:          03:34          Young Lacoon director of AI research at Facebook puts it best with his quote. If intelligence was a cake, unsupervised learning would be the cake. Supervised learning would be the icing on the cake and reinforcement learning would be the cherry on the cake. We now know how to make the icing and the cherry, but we don't know how to make the cake. Talk about strange but weirdly effective metaphors. Here's the year young, so let's take a look at our data to decide what to do with it. This is a 30 minute long recording of neural data from an epilepsy patient. A set of electrodes were inserted into the brain of this patient to record the activity of neurons in real time. It picked up electrical spikes of neurons and we can see several features here that relate to the recording devices, measurements like the channel number frequency and the number of samples.

Speaker 1:          04:23          Let's first visualize this data using digital alchemy like a python. We went to extract spikes from the signal and to do that we'll find data points in the signal that are above some predefined threshold and align them at their peak amplitude. We can do this with just 100 random spikes and see that there are at least two types of waveforms in the data. One group of spikes would they sharp high amplitude peak and a second group with a broader initial peak and these spikes were likely generated by more than one neuron. If we can find a way to group these wave forms into different clusters, it will help us figure out which spike corresponds to which neurons, which will help surgeons decide where to perform surgery. But in order to cluster the wave forms, we're going to need to decide which features to input to our algorithm.

Speaker 1:          05:13          One possible feature, it could be, for example, the peak amplitude of this spike or the width of the wave form, but not all features are equally informative and useful. We need to select the features that represent the spike wave shapes the best and get rid of the rest. For our prediction to be accurate. The way we're going to do that is to use a type of unsupervised learning called dimensionality reduction of which there are several techniques like brute force, no, we're going to use a popular one called principle component analysis or Pca. PCA finds the principle components of a dataset. Principal components are the underlying structure in the data. They are the direction where there is the most variance, meaning where the data is most spread out. It's useful to measure data in terms of principal components rather than on a normal x, y axis.

Speaker 1:          06:05          Imagine that we had orange of data points, which we'll denote as tri force symbols as an ode to the princess to find the direction with the most variants we can find. The straight line with a data is most spread out when projected onto it. A vertical straight line with the points projected onto it will look kind of like this, not very spread, so there's a small variance, likely no principal component here, a horizontal line, however with lines projected onto it looks way more spread out a high barriers. There's no straight line we can draw that has a larger barriers. Any horizontal one. Thus the horizontal line is the principal component. In this example, to find the principle components, we use linear Algebra, one of the mathematical pillars of machine learning, two concepts. Here I can vectors which have a direction and eigen values, which are numbers that tell us how much variance there is in the data in that direction.

Speaker 1:          07:01          These two concepts come in pairs like Yin and Yang and the eigen vector with the highest eigen value is the principal component and a three dimensional data set. There are three variables. Imagine all the data points lie on a piece of paper size plane. In this three graph, when we find the three eigen vectors and values to, we'll have large eigen values and one of the eigenvectors, we'll have an eigen value of zero if we rearrange our axes to be along the eigenvectors rather than the original variables. Discarding the third one, we essentially get rid of the useless direction and are able to represent it in two dimensions. We can do this in a single line. Thanks to psychic learn. We just need to specify how many components we want mine, myself with 50 features. Mother m l comes to me predicting just the best ones. Let it be.

Speaker 1:          07:57          Once we've reduced the dimensionality of our data, we're ready to perform. Clustering. The second type of unsupervised learning. A popular clustering technique is called k means. First we choose a number of k random data points from our sample. These represent the cluster centers and their number equals the number of clusters. Then we calculate the distance between all the random cluster centers and any other data point. We then assign each data point to the cluster center closest to it. Since we started with random data points, it won't give us a great result, so we repeat the process and instead of using random data points as cluster centers, we calculate the actual cluster centers based on the previous random assignment. This just keeps repeating and with every iteration the data points that switched clusters go down and we arrive at a global optimum. We're now in the gang, a newer version of the Gucci Gang.

Speaker 1:          08:57          A question arises though, how do we choose the number of clusters we could try running k means multiple times with different cluster numbers. When we plot the results, we can analyze it to see if we chose to many clusters, too few or just the right amount based on our domain knowledge. We can expect to find more than two or three separable clusters from a single electrode recording in our plot seems to confirm this notion. Another way to decide this is to use the elbow method. The way that this works is to run k-means several times and increase the number of clusters every run and during every run we calculate the average distance of each data point to it's cluster center. The number of clusters increases and the average intercluster distance decreases when we reach six clusters in the average distance to the cluster center does not change anymore and this is called the elbow point.

Speaker 1:          09:52          It gives us a recommendation of how many clusters we should use. By clustering the data, we're able to sort the neuron spiked into distinct regions which correlate to different parts of the brain. This is going to be supreme helpful for our client at the hospital and we just use data science to save a patient's life. Before we popped champagne, there are three things to remember from this video. Unsupervised learning helps find previously unknown patterns in a Dataset without needing a label. Principal component analysis is a dimensionality reduction technique that helps find the most relevant features in a dataset. And k means clustering is the most popular clustering technique. Grouping similar data points together for further analysis. And what is your next data science project? Let me know in the comment section and please subscribe for more programming videos. For now, I've got to find myself, so thanks for watching.