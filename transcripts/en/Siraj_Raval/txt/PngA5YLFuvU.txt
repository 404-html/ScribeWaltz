Speaker 1:          00:00          Hello world, it's Saroj and drones are a lot of fun. We're going to train a drone in a simulated environment to be able to fly to any location. We give it autonomously avoiding numerous obstacles. It's never seen before. Along the way, we can think of drones as aerial data capture devices. Each one can potentially hold a huge amount of sensors, although right now most of them just use a camera and if we connect a drone to a data storage and analytics pipeline, we can solve specific problems using the data it's captured. For example, a startup called sky specs is using them to identify anomalies for wind farms. When a wind farm has motors that are running hotter than they should, they're drones are used to help monitor the humidity and weather conditions to more quickly identify when a problem will arise. Also, Cape is playing a vital role for mining.

Speaker 1:          00:58          Companies are providing volume metric mappings that help them measure their stockpiles with their high resolution imagery fed into an image processing pipeline. The accuracy of asset measurement can increase by up to 20 fold. It's these type of measurements that help companies transition from a discrete to continuous forecasting cycle. Forecast don't have to be made in intervals. When you have a drone, it can be an ongoing process. So before we start talking about the theory behind how we're going to train our drone using machine learning, let's take a look at this simulated environment called the Ross development studio. Ross or the robot operating system is a framework for writing robots software. It's a collection of tools that make the process of architecting our robot's behavior much easier. It's goal is to enable code reusability in robotics, research and development. Robots can have all sorts of hardware components from arms two legs, two can openers to seven different types of sensors, all performing different tasks at the same time.

Speaker 1:          02:08          And while all of these hardware modules might be performing different tasks, they all still have to talk to each other, which can get complicated to architect. What Ross does is it lets us abstract each of these components as individual nodes and once we have our drivers installed we can use, it's built in communications layer to create a cohesive integrated system with concurrency support enabled. The Ross runtime graph is a peer to peer network of processes that are potentially distributed across machines coupled together using the Ross communication infrastructure. The Ross Development Studio is an in browser environment, meaning we don't have to install anything on our computers to use it. We just had to visit the link in our browser. Once there we can create a new project and name it something unique, then we can open the project by clicking on the open project button. Once we have the environment open, we can go to the tools menu and open a Linux shell.

Speaker 1:          03:13          Inside the shell will go to the source directory. This is the place where Ross code has to be put into the rds in order to build tests the bug and execute it against robots simulations. We can pull our code directly from get hub right here if we wanted to to train our drone and then watch the parrot drone simulation under the simulations menu. Finally, we can launch our package so it will start training the parrot drone right from the shell using the Ross run command. It's going to start moving and wild ways, but that makes sense. What our robot is doing is learning using an algorithm called deep the terministic policy gradients. Sounds complicated. I know. So let's take a step back. Traditionally, the class of algorithms considered reinforcement learning only really worked in tiny disc retired grid worlds, and that stopped them from gaining credibility as being viable real world tools for context.

Speaker 1:          04:13          Discrete data can take only integer values, whereas as continuous data can take on any value for games, that difference boils down to either a few set of possible actions or many. When deep mind released their deep reinforcement learning algorithm called deep to the Ai community, saw that deep learning methods could be used to solve high dimensional problems, powerful stuff. Perhaps it could also be used to deal with continuous action spaces. Learning and continuous action spaces has been a huge obstacle for AI developers and the most interesting real world problems in robotics control fall into this category if we disc retired our continuous action space to finally we'll end up with what's called the curse of dimensionality, which basically states that the more dimensions you work with, the less affective statistical techniques become. But on the flip side, a naive discretization of our action space ignores valuable information about the geometry of our action domain.

Speaker 1:          05:19          So deep mind created an algorithm specifically to solve this continuous action space problem called deep deterministic policy gradients. It uses a mixture of two techniques, policy gradients, and actor critic. Let's take a look at how both work. First policy is a mapping of states to actions. It's how an agent decides how to interact with an environment and an optimal policy can be learned. Policy Gradient Algorithms are a set of techniques that optimize a policy end to end by computing noisy estimates of the gradient of the expected reward of the policy. Then updating the policy in the gradient direction. The gradient is a signal that we derive using calculus that tells our learning agent how best to update its knowledge of the environment. The algorithm will observe many training examples of high rewards from good actions, negative rewards for bad actions, and it'll slowly increased the probability of the good actions over time.

Speaker 1:          06:25          But in practice, getting one reward signal at the end of a long episode of environment interaction makes it difficult to a certain exactly which action was the good one. This is popularly known as the credit assignment problem. One way to solve the credit assignment problem is to use an actor critic algorithm. Actor critic algorithms are used to represent the policy function independently of the value function. The policy function structure is known as the actor and the value function structure is called the critic. The actor produces an action given the current environment state and the critic produces an error signal given the state and results into a reward. If the critic is estimating the quality or Q function that assigns values to possible actions, it will also need the output of the actor. The output of the critic drives learning in both actor and critic. We can use neural networks to represent both of them.

Speaker 1:          07:27          So the deep deterministic policy gradients algorithm combines both of these techniques. It's a policy graded algorithm that uses a stochastic behavior policy for exploration, but estimates a deterministic target policy which is easier to learn for context in deterministic models. The output of the model is fully determined by the parameter values and the initial conditions. The castic models possessed some inherent randomness. DDP PG uses a form of policy iteration in which it evaluates the policy then follows the policy gradient to improve performance. It's also an actor critic algorithm in that it uses to neural networks. They compute action predictions for the current state and generate an error signal at each time step. The input of the actor network is the current state and the output is a scalar value representing an action chosen from a continuous action space. So the critic output is the estimated Q value of the current state and the action given by the actor.

Speaker 1:          08:32          A specific theorem called the deterministic policy. Gradient theorem is used as the update rule for the weights of the actor network. The critic network is updated from the gradients obtained from the error signal. There were two tricks on top of that that the team at deep mind used to improve convergence. The first is using an experience buffer using a replay buffer to store the experiences of an agent during training. Then randomly sampling from it while learning has proven to be beneficial and the second is using a set of target networks are using a set of target and networks to generate the targets for our error computation. We regularize our learning algorithm and increased stability. We can create a custom environment class that uses Ross function calls to define our robot and its environment. We can start by registering our environment into the pool of available environments of Open Ai's gym library.

Speaker 1:          09:32          Then we can get the configuration parameters directly from the Ross parameter server and connect it to the simulation called Gazebo. The main function is the step function. It's called during each training loop. This function receives as a parameter the actions selected by the learning algorithm. This parameter is just the number of the action selected, not the actual action. Our Algorithm won't know which actions we have for this task. It only knows the number of actions available and picks one of them based on its current learning statics. We'll convert that number into the actual action for the robot ourselves. After that, then we send that action to the robot through a series of steps as follows, unpause this simulator, send the command, wait for the execution of said command, observe the state of the environment and again, pause the simulator. Then we processed the current states, calculate the reward, return the current state, return the reward and return a boolean value indicating whether we're done or not, meaning we completed our objective.

Speaker 1:          10:41          That's it. When we test our robot out in the simulated environment, we'll see that at first it's very sloppy in its movements, but eventually it gets really good at navigating our environments. No matter how many obstacles it faces, we can then take this code and run it on a real drone as some developers have and eventually apply it to a real world problem. Three things to remember from this video. The robot operating system allows for code reusability amongst roboticists and is currently the best free framework out there to architect a robotic system. We can train a drone to navigate an environment using the deep deterministic policy gradients, algorithm or D. DPG and DDG is a mixture of policy gradients and actor critic able to solve tasks in continuous action spaces. Please subscribe for more programming videos. And for now, I've got a fly, so thanks for watching.