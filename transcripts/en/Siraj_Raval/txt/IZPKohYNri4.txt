Speaker 1:          00:00          Hey, I can dream up solutions. Whoa. Hello world. It's arrived and there's a really cool paper that recently came out called world models. The AI researchers created an algorithm for a simulated car that let it learn how to drive on a race track by itself and uniquely this AI teaches itself how to drive by practicing and its own dreams. By that I mean a hallucinated version of the world that it creates itself. Pretty wild stuff. This paper was well done because it combined several deep RL techniques to achieve incredible results. It's the first AI to solve the open AI car racing environment and the researchers created some great documentation as well. They found a lot of inspiration from the way humans perceive the world. We develop a mental model of the world based on what we are able to perceive. We have this internal model of the world that we build in our head from everything we see.

Speaker 1:          00:59          It contains a few selected concepts and the relationships between them to represent a larger, more complex system. Our brain is learning to represent both space and time for a given event and we use that representation to predict the future like what we will likely perceive or feel. For example, we might perform a fast reflex when endanger pro baseball players are able to hit hundred mile an hour fastballs because they are instinctively able to predict where the ball will go. It's all subconscious including steroids. They don't need to consciously roll out a bunch of possible future scenarios to form a plan. Reinforcement learning is the branch of AI concerned with trial and error. How does an agent learn how to maximize rewards in an environment scenario? Our l agents can benefit from having a robust representation of the past and present states of the world, which helps create a good it predicted model of the future.

Speaker 1:          02:00          The researchers created a model inspired by this biological analog. They looked at the problem of playing a racing game and divided it into three different tasks, visual representation, modeling and control. At each time step, the agent receives an observation from the environment. In the case of the racing game, it was game frames. That vision model was responsible for incoding the observation into a lower dimensional representation, a more compact version. Then they took this representation and fed it into the memory model, which used the learned representation to learn how the world tends to behave and make predictions on the next state of the world, essentially learns a model of the world. Lastly, the controller model was spent the representation from both the nm and selects good actions. Based on that, let's look into how each of these components work. We can consider a single game frame as a high inputs.

Speaker 1:          03:00          It's a two d image frame that's part of a video sequence and they used a model called a variational auto encoder or VA to learn a representation from this VA. Ease consist of an encoder and a decoder. The encoder, it takes the input and creates a representation. The decoder takes that representation and tries to reconstruct the input. What makes it variational and not just a regular auto encoder is that a bit of a randomness is applied to the representation when decoding, so the decoded output is always some slight variation of the inputs, not exactly the inputs. That's why the VA, he is considered a generative model. It can generate new data from what it's trained on. Just don't train it on NSFW images. That encoded representation was fed to the memory model directly. It's a recurrent neural network useful for predicting sequences. If it's given a sequence of image representations as input, it will try and predict the next one, which is essentially a prediction of the future state of the world that the agencies, recurrent nets, produce a feedback loop while training learning not just from the data but from what it learned in the previous time step.

Speaker 1:          04:15          This optimization technique is called back propagation through time. The controller model is responsible for determining the next action to take in order to maximize the expected reward of the agent during a rollout of the environment. The controller is a simple single layer feed forward neural network that takes the representations from both the nm as input. The flow of the data is such that the raw observation is first processed by the, the input into c is the output of the concatenate it with EMS. Hidden state see then outputs an action vector for motor control and then takes the current output from the and the action as an input to update its own hidden states to be used in the next time step. Open Ai released an environment called gym that makes it really easy to train AI agents in simulated game worlds. The idea is that the library, it takes care of integrating different game worlds and basic environments related configurations and you as a developer can focus on creating an algorithm to solve that game.

Speaker 1:          05:23          The code for this demo is relatively simple. We start by retrieving the observation, the image frame from the environment. We also initialize our recurrent neural network model. We then create a wild statement for our training loop. We'll use the bae to encode the observation. We'll then use that output Czi as an input to our controller as well as the hidden state of the recurrent network. This will give us an action to take. We'll use the step function Jim provides to execute that action and get back a new observation, a reward. If we got one and a boolean value that represents whether or not the game is over. We can use this reward to update our cumulative for ward and use the learn parameters, a, Z and h as input to our recurrent net to create a new hidden state. We repeat this process until the game is over.

Speaker 1:          06:16          The results were incredible. It learned how to drive really well and since their world model is able to model the future they use it's predictive abilities to generate a dreamlike world. They asked it to produce the probability distribution of the VA ease hidden state, which ended up being predicted game frames. When these game frames are generated by em, it ends up creating a dreamy game environment which they could put the trained controller into and watch the car execute driving actions inside of the train the agent inside its own dream and transferred that policy to the actual environment. This approach of training and AI inside of a simulated latent space, dreamworld has practical applications. Game engines require using heavy compute resources for rendering game states onto image frames. We could avoid wasting cycles, training an agent in the actual environment and instead train the agent inside it's simulated environment.

Speaker 1:          07:19          While the human brain can hold decades or even centuries of memories, neural networks trained with backpropagation have more of a limited capacity. Future versions of this work could use higher capacity models or use an external memory module three points to remember from this video. The world models paper demonstrated that an AI could learn by training and its own simulated environment. There are model use, a vision, memory and controller module to learn from its environment and its vision. Model of variational auto encoder is generative able to produce variations of the input data. Hi, one, a master, any programming language, we'll hit subscribe and it'll happen for now. I've got to travel to Europe again, so thanks for watching.