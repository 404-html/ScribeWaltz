Speaker 1:          00:00          Hello world. It's a Raj and today we're going to talk about pose estimation using tensorflow. Dot. J S while you're seeing behind me is a demo of realtime pose estimation using tensorflow dot js. What it's doing is it's estimating what all my body part poses are. If I go back a little bit, it's going to show like all these little things that, ah, okay, so you see a little bit of that. I'm going to go back out of that. So that's the demo for this. Today's video, and we're going to do this using tensorflow. Dot. J Yes, this is really, really simple to do in the browser. Anybody can do it. You don't need to install any dependencies. You don't need to make any kind of configurations. You don't need to download anything. That's the great thing about javascript is that it's so simple to use and because tensorflow has now been ported to the browser, pretty much anybody can make incredible machine learning applications that anybody can access because pretty much any computing device has javascript enabled for j for Browser access, right?

Speaker 1:          00:57          So that's what we're going to do today. I'm going to talk a little bit about tensorflow. Dot. JS and then we're gonna go into the code, how pose estimation works architecturally, programmatically, et Cetera, et cetera. Okay. So right. And they'll use to require lots of time and money to get started. You had to build your own, you know, deep learning rig by your own Gpu, you know, install this stuff, configurations, Kuda drivers this, that, it was a huge headache. And since then the whole field has, has kind of went into this period of ease of use. Everything's been democratized, everything's getting easier for anybody, which is awesome. And I think that tensorflow. Dot. JS is a good example of that being magnified in the, into the, into the eye of the public. But really there are three points that I particularly find very interesting about tentraflow.

Speaker 1:          01:48          Dot. JS and in general machine learning happening in the browser. W why? I think it's a good thing. So the first reason is privacy, right? So privacy is a big issue. If you're a health care company, if you are a, um, you know, if you have any kind of critical data that you don't want people to know about, privacy is crucial. So ideally you could train your model at the edge instead of on the cloud. That means on your device and set up on someone else's device. And if you can do that, you can keep the data local. You don't have to send it anywhere, which is awesome. The second reason is wider distribution. Like I said, pretty much any computing device, phone, laptop, tablet has javascript enabled, whether it's through a browser, whether it's through, you know, whatever, they have javascript enabled, so you'll reach the widest audience.

Speaker 1:          02:33          The third point is distributed computing. So if we can leverage clients, side data for many users to train a model in real time, I think this is particularly exciting. If you think about it, you have a web app and you deploy it to a bunch of people, you can train it on their data in real time and make your model as a whole better, which is, which is awesome, right? So Uber and Waymo and all these companies, they're doing that for their self driving cars. They're learning in real time. And then that, that those learnings are being transferred to a global model. And now we can do this for anything really. Not just for self driving cars but for simple web apps, classification, you know, inference for all sorts of things. Game playing, um, predictive analytics for, for, for marketing companies and finance, the stock prices, things like that.

Speaker 1:          03:21          New Mariah is doing that in some way. But anyway, there's a, there's a lot of potential there. And I have a video on tensorflow dot. Js so just search it tensorflow. Dot. Js um, Saroj on youtube and you'll find it. But I just want to go over it like one simple part of it. So we have the, um, just this, this image right here. So web GL is, is how graphics are computed in the browser right now. It's a, it's a low level library for that. And so on top of what GL is the browser and what you see on the right is soon because he's talking about key purpose, right? So ideally you could train this thing on Tpu is, but we don't really have that yet in our machines. You know, we have gps and CPS, but you can train intentional jazz on CPS, Angie [inaudible], it doesn't matter.

Speaker 1:          04:02          Uh, but right, so on top of the browser you have the ops Api and the ops API is just like tensorflow it, right? You've got variables, constants. It's basically long code, you know, it's, it's detailed code of how you define your computation graph, what it looks like, what the variables are, what the parameters are, what the learning rate is with all those hyper parameters are. You can do it in detail or you can use the layers API, which is basically care os one line per layer, Dah, Dah, Dah, Dah, Dah, Dah. You can do it in nine lines what you would otherwise do in like a hundred lines, which is what the layers API is. So that's, that's kind of high level of what it looks like. If you want to do something simple, easy layer is API. If you want to get detailed and make sure your model is state of the art.

Speaker 1:          04:40          Usually ops API and the pipeline is actually very simple. So what I've seen on get hub in terms of developer activity is a lot of developers are using pretrained models because it's so easy to use, right? So transfer learning I think is going to be a huge use case. It already is a huge use case for tensorflow. Dot. Js. You can use a pretrained model, Alex Net, mobile net, you know VGG nets, Google in that there's a million pretrained models that were trained on different sets of images, on Games, on human interactions, etc. Take one of those pretrained models and you can use it in the browser easily with like a few lines of code. So the pipeline is a three step process. The first part is to load a model, right? So in order to do that, we need two components. We need the model file and we need the weights manifest, right?

Speaker 1:          05:28          So where to find the weights and what the architecture of your model is, right? How many layers you have, what are the hyper parameters and the weights, what does that, what does it learn? And once you have those two things, boom, you have a, you have a pretrained network that you can run in the browser. Programmatically speaking, this is very simple, right? So we can load up a frozen model from what wherever we have, we can create constants for where our weights are, where our model is, what we want to name our input node, what would you, what do we want to name our output now and then we can load our frozen model just like this using the model URL. Remember there's two, those two components. That's all we need. The rest of that, the code you see here is just like for naming purposes, but this is this last line including these two parameters are all you need.

Speaker 1:          06:13          Is it then that single line, that's it. To load up a model that's already been trained and this could have been trained in tensorflow, python, it can be trained in tensorflow, C, c plus plus, but you can import it into a central node dot js very easily. The second step is preprocessing. So a neural network has a specific input definition. You can't just input any kind of data, it's got to be in the right format, right? You can't input a, a, an 18 dimensional vector into a three dimensional, a neural network, right? So if it's weight values are assuming a certain dimension size, you can just say, oh, it's going to be this dimension size and just force it in. It doesn't work like that. So you have to do some preprocessing in order to do that. So to do that, first we'll convert pixels to tensorflow.

Speaker 1:          06:55          Dot. JS is input tensor. So we have some source and the source is that, uh, the, the, the source of, of, of what, what we're training on. And then what we can do is we can say, okay, let's take the input and then from pixels, the sources that the data that we're training on, we'll convert it into pixels just like that, that, so that's our first step. So now it's in these, this pixel aided format that's appropriate potential flow that Jay, yes. Then we'll crop the inputs. So if we want to use part of the image, so we'll say, where's the mobile net. Dot. Crop image input. And that's that pixilated versions and we cropped it. And lastly, we'll set the batch input dimensions to zero. Since we only want to infer one single image. Now it could be more if we wanted it to infer more images, but for now we could set it to zero just like that.

Speaker 1:          07:40          And now we have this uh, image and we can return it just like that and we'll return it and as a float value and we can add any other kinds of preprocessing that we want. The last step, step three is inference or prediction, whatever you want to call it. And that's just two lines of code. Take that preprocessed source and then make a prediction based on that. Right? So, so, so just like that, we take that process input, we feed it into the model and boom, it outputs a prediction and just like that. And the result is going to be a dictionary containing probabilities of every class that it predicts. If it's a classification model and the, the class prediction is the one with the highest probability, right? That's the highest likelihood. So there are some great resources to learn more about tensorflow. Dot. Js uh, obviously the tensorflow.

Speaker 1:          08:25          Dot. Js Docs are incredible. I've got a video on it. This is a video. And also Daniel Shiffman, who's a Youtuber, definitely check out his series on tensorflow. Dot. Js um, I have a link to it right here. Check it out. So that's it for a little bit of a primer on tensorflow dot. Js. Now I want to talk about pose estimation. The problem we talked about at the very beginning, so if you ever use a Microsoft connect, right? Or if you've ever been in a demo like that, you just show up in front of it and all of a sudden he had maps. Your skeleton, right? There's this like skeleton, a layer on top of your real body and wherever you move it's going to move. So what it's done is it's estimated your pose, the pose of your human body or your arms are, and not just where your arms and your hands are and your feet are, but where they are in relation to each other.

Speaker 1:          09:11          And that's what that line is. That's connecting those dots. So if we think about the pose estimation problem, it's not, it's not a generic classification problem. We're not trying to do to detect that, hey, this is an arm or hey this is a foot, but we're trying to do is build this post based on those things. So if we think about it intuitively, this is an image and we are trying to process an image using learning technology. We want to learn about what oppose looks like. And once we learned what a human pose books like, we can make an inference or prediction on a novel post like myself and instantly it will know, right? So how do we do that? Well, if we think about it, right? We were using our intuition about neural networks. Images, okay? Immediately convolutional networks springs to mind, right? So we'll use a convolutional network to do this.

Speaker 1:          09:59          The next question is, well, how do we do that? And let me get to that in a second. But I also want to say that this idea of post estimation has many use cases, right? So if we think about augmented reality, right? If you're in the game world, it should know where your hands and feet and you know, even head are in order to make objects in the environment move appropriately for animation as well. Right? So you know, Gollum, you know all these like Lord of the rings films where they have all the balls on the, on the person and they're addressing this green or black suits and they're crawling and they're doing things. And then later on you see the animated version of that, right? So normally this is a kind of thing that's going to democratize animation. Big Hollywood studios that have huge budgets have been the, the only people able to do this.

Speaker 1:          10:42          But with this, anybody can make these PPOs estimations of people very easily. Okay. So there's that. And lastly, fitness, right, fitness tracking to see, you know, if you're doing the right form, if you're, if it's, you know, like whatever, whatever it is, it can do that as well. So, um, right. So back to my explanation of how it works, we should use a convolutional network that much we know. Now, the second part is the thing about, about the way it's actually going to learn about oppose before inference. How is it going to learn? What opposed it looks like? So if we think about it that what's the easiest way? The easiest way is to think about this as a supervised learning problem. So if we Google pose estimation supervised, we've got a handful of papers that do this and some of the more popular papers are using supervised learning, but there's also, if we Google search unsupervised, there are unsupervised methods as well to do this.

Speaker 1:          11:32          So this would involve clustering of some sort, but for the supervised part, if we think about it very intuitively, what it's going to do, and now this looks very similar to if you recall my Yolo object detection video, just search Yolo Suraj first link on Youtube. If you look at this picture, it looks very similar to Yolo because it's grouping all of these body parts into different classes. And then once and then once it's found the one that is most encompassing of a given body part, we can then we can then say that this is the right ankle or whatever. So at first it seems like a classification problem because we have to identify that this is indeed a foot. We don't have to tell the user that, but we have to identify that and then identify other body parts. So if we think about it as a supervised learning problem, we could give it labeled images of body parts just like you know, a hand or an arm and different variations like all those variations and what it would do separately or at the same model would it would first learn the mapping between arm and arm image, the mapping between hand and hand image, the hit mapping between, you see what I'm saying?

Speaker 1:          12:34          So it would learn the mapping between all of those body parts and then once it's learn all those mappings, then once it sees a person it's going to know Duh, Duh, Duh. Okay. That's everything that I've learned. And it can label them with a point marker. And once it's got that point marker, then we can connect the dots, literally connect the dots. And that's just, you know, that's just, um, that's not even machine learning, just connecting the dots. That's just um, you know, d three or some kind of visualization tool. But then we have the posts. So that's a supervised way of doing it. Right? So, but then the unsupervised way would be different. Now that's not as popular, but that is a way to do that. So I've got this great video on that called, it's using the Will Ferrell image will Ferrell image. It's actually, it's yellow again, yellow object detection.

Speaker 1:          13:20          Basically it's looking for direction. So it considers all of these pixels as gradients and it's looking for directions that these gradients of light are moving. So first converts the image of black and whites and it sees where the, the, the direction, all these great things are moving. And if the direction for say a nose, he's like this, this, this, this, this, it's likely a note or it's likely some, some body part that sticks out because it has a point like an elbow or a nose or a shoulder, um, or uh, you know, fingertips et cetera or ahead as like it just stands out from in terms of lighting, it's using the clustering of lighting to identify body parts and that's what unsupervised learning is all about. Clustering, right? That's not as robust as as um, mainstream, I guess you could say in the ml community as a supervised methods which are easier, but it's still a way, it's a very useful thing to think about as well.

Speaker 1:          14:10          So pose net is the model that we are training with, right? Suppose net is the model we're training with. It was trained in pure tensorflow. We can port it to tensorflow. Dot. Js and we can think about the problem of pose estimation as to problems. Actually there's a problem of single-person pose estimation and then there's multiple pose estimation, right? So if you have one person in an image versus several people in an image, wildly different, you know, contexts. So single pose estimation is actually faster because it's only one person, but multiple person is slower. But then you can detect, you know, a hundred or whatever people. And interestingly, multiple pose estimation, it doesn't matter how many people there are, the complexity is in, it's not like n squared or n cubed or something. The more people, it just gets linearly more complex, which is very interesting.

Speaker 1:          15:01          But anyway, the way this works is we first let's talk about, you know, PostNet. So first an input RGB image is fed into this convolutional network post net, and then either a single pose or multipolar decoding algorithm is used to detect poses and then the output is oppose. Now this pose is going to contain a list of key points and an instance level confidence score for each detected person, right? So here's the key point for this person's elbow and this person's arm, et Cetera, et cetera. And then we can map those on using some visualization tool, right? So these are just, um, coordinates on an, on an image plane, right? So this guy, he's got 17 pose key points. That's it. And we can map those on and what is going to do is it's gonna give us all those coordinates. It's our job to then map those onto the image so we can view them same for this call as well. So, um, this single port pose estimation model, it requires a couple of inputs, right? So a couple of parameters, um, how much to scale the image, whether or not to flip it horizontally. Uh, what's the, what is that image, what's the pose? And then the outputs are going to be the Po's itself, the key points and then the model, you know, whatever, you know, logging data that you add in as well.

Speaker 1:          16:20          But we can, we can, we can tune the level of the level of quality we want by tuning the stride perimeter, right? So we can tune how accurate it is for one person by tuning the stride parameter from eight 16 up to 32 and then back down again. And there's a trade off here. So one is faster in terms of processing and want a slower one has. If it's faster, it has lower accuracy, but if it's slower it has higher accuracy.

Speaker 2:          16:50          Yeah.

Speaker 1:          16:50          When PostNet processes that an image, it is in fact returning a heat map along with offset vectors that can be decoded to find high competence areas in the image that correspond to those key points. And this is what I was talking about when I referenced the Yolo Object Detection Video, right? So it's looking for like notice his nose, there's, there's, there's a gradient of light in a direction that's moving towards the center of his notes and that can be considered a heat map of key points.

Speaker 2:          17:21          Okay.

Speaker 1:          17:22          Out. So when it comes to, so that's kind of the general way that mold, that single person pose estimation works. Multiple person pose estimation is similar but it's slower. It has very similar inputs. The image scaling factor, whether or not to flip it horizontally, the stride m and then there's the additional one maximum pose detections. Right? So how many people the, the high level threshold do you want to detect in terms of their poses? And the output is going to be all of their poses. Now when it comes to even multiple posts detection, there's, there's, there's, there's more to that algorithm. But notice how we can coat it just like this. This 10 line of code snippet shows us how to do multiple pose estimation very easily. All we literally say is called that function with those inputs and it's going to give us all of those poses as an output. All of those coordinates. Okay? So what I want to do now is code a little bit to show you just how simple it is. Okay? So we can do this in html very easily. I'm going to do that. In fact right now, um, this, tune this to html,

Speaker 2:          18:25          okay,

Speaker 1:          18:25          boom. Okay. So using html, we can clean it. We can create any simple html file in our, you know, text editor and then look at it in our browser easily. But you might be thinking, wait a second, how are we supposed to load a tentacle dot. JS If we don't have to install any scripts? Well, we can call it directly.

Speaker 2:          18:45          Okay.

Speaker 1:          18:45          By saying the sorts of this script is going to be online. So that, so it's unpacking that data from the, from the interwebs. And it's saying that we want that data.

Speaker 2:          19:01          Okay.

Speaker 1:          19:01          Okay. All right. So that's US loading tensorflow dot. Js. Now we'll load pose net. So for loading pose net we'll say, well we have our library that we want to use. Let's,

Speaker 2:          19:16          okay,

Speaker 1:          19:17          he used the model that we want to use and that's called Posen it. So it's got a source equals equals better data script, right? Just like that done.

Speaker 2:          19:33          Yeah.

Speaker 1:          19:34          And

Speaker 2:          19:39          Yeah. Cool.

Speaker 1:          19:45          Right? So there's, there's that, there's that. I'll end the head head, her head tag just like that. And that's it for our header. Now, now we can code this thing. So let's say like, what do we want to run this on? And I'll say, well, it's going to be this cat image that I have locally. So I want it to detect the pose of the cat, right? So this can be a human, it can be whatever it is, but I'll just say cat.

Speaker 2:          20:09          Okay.

Speaker 1:          20:09          And that's going to be it for the body. And then now that I've got that, I can go ahead and place our, my code inside of this script tag. So now, now comes at tentraflow dot. Js I'm important to the library. I've imported the model and now I can actually go this thing. So

Speaker 2:          20:33          okay,

Speaker 1:          20:33          I'm going to say, well here are my inputs. The scale factor is people who'll be 0.5. The output stride will be 16. The whether or not to flip it horizontally. Well, let's not do this time. And the image itself is going to be an elements that I pull from the dom using get element by ID. It's going to be cold cat. And then I'll load in posing it just like that. Postnet dot load. And then I'm going to say let's return the net, um, estimate a single pose using that image element, scaling factor, flip horizontal, and then output stride. And then it's going to print out in the console that post. Okay, so let's see what this looks like. Um, I'm going to say open TF dot. Html in the browser. Inspect in the console. Here are my key points right here, right? [inaudible] all my left I left year. This, this I did with not without installing anything, this is happening locally. I just have a single image. It made those predictions. Here's where everything is. Um, check out the links in the video description. I've got links for you that's going to show you how to do this in the browser code, code wise and other learning resources.

Speaker 3:          21:54          Hey, you made it to the end of the video. If you want to be an AI, God hit the subscribe button for now. I've got to make a pose, so thanks for watching.