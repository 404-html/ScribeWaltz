Speaker 1:          00:05          Hello world. It's Saroj and in this episode we're going to talk about tents or board tents. Board is a data visualization tool that comes packaged with tensorflow. Tensorflow programs can range from super simple like an addition problem, two super complex using thousands of computations and they all have two basic components, operations and tensors. The idea is that you create a model that consists of a set of operations, feed data or tensors into the model, and the tensions will flow between operations until you get an output tensor, which is your result. Tenser board was created as a way to help you understand the flow of tensors in your model so that you can debug and optimize it, but we're going to visualize a classifier that can recognize handwritten digits. In tenser board we use two examples of the classifier. What is a simple version in 60 lines of python and the other is a more complex version path.

Speaker 1:          00:54          Don, I love you. Don't worry if you don't understand every line of code. We just want to get a general sense of what's happening under the hood so that we can later understand the tensor board visualizations. Here we go. We'll start by importing tensor flow and our input data class, which pulls our training data from the web. Then we'll define two helper functions and it weights were returning a variable that contains randomly generated wait values along a normal distribution. Our second helper function, we'll create our model a three layer feedforward neural network. We're going to create three layers and to find them via the name scope function. The named scope function creates high level namespaces for operations in the tensor flow graph that we will later visualize and each layer we'll run a dropout function and an activation function and dropout is a function that forces our neural network to learn several different representations of patterns.

Speaker 1:          01:36          To that generalization improves the activation function, creates probabilities out of numbers. So now that we have our two helper functions, we can create variables to store our training and testing images as well as our training and testing labels. Next we can create input and output placeholders for our data. Then we'll initialize our weights between each of the three layers using the function we defined earlier. We'll define histogram summaries for each of our weights so we can visualize the distribution of weights later intenser board. The next step is to add dropout placeholders to are hidden and input layers. We can then create our model using the variables we've created. Now that we have our model, we'll create our cost function. The cost function is a measure of how good a neural net is with respect to its given training sample and the expected output. We want it to decrease over time.

Speaker 1:          02:14          We'll also measure the accuracy of the network. We want that to increase over time. Well later. Visualize both. Next, we'll create a session to run our graph computation. In our session we'll save our model using the summary Writer. Lastly, we'll initialize our variables for training and train our model. That's it. I know that was really fast. Unlike series comprehension, you heard me apple. Let's visualize that code intense or board and I'll explain it. More. Summary operations or how it tends to board acquires data from our tensor flow runs. They are functions like TF dot matrix multiply. We created a number of them in our code here. In the events tab we can view our scalar summaries, accuracy and cost. The x axis shows a time timestamps and the y axis shows the accuracy measure or the percentage of correct predictions. Over time we can blow up our graph for a closer look or view a wider range of data points by expanding the y axis.

Speaker 1:          02:55          We can also drag a rectangle on our graph to zoom in on a certain region if we like and double click to zoom out. If we mouse over the chart, it will produce crosshairs with data values. We can change how smooth our line is by adjusting the slider. Istep option shows time steps. The relative option shows the time relative to the first data point. That means the number of minutes or hours since the training run was started and the wall option shows the actual time to runs happen. We can create more tags as well in the sidebar. It can either be an entirely new tag or tag that will group a bunch of existing summaries together into a new category. Let's switch to a more complex version of our handwritten character classifier example. With even more scalar summaries, we can type in, it's over 9,000 and since no summary contains those terms, it becomes a new category.

Speaker 1:          03:33          But if we look under Max and mean, they both contain summaries for biases. So if we type in biases, it'll create a new category that contains both biases from these two existing categories. We can also download our graphs in CSV or Jason Format. For this more complex example, we have lines for both training and testing data. That way we can compare runs. Let's move on to images. We created an image summary. This lets us view the images in both the training and testing folder. Okay, let's switch back to our simple classifier and move on to the graphs tab which lets us inspect our TF model. We can see each of the three layers we created via the name scope function. Let's double click on one of our layer namespaces to expand it. The first thing that pops out is the dropout function which we declared inside of the scope as well as our relu and Matrix multiply functions.

Speaker 1:          04:10          Notice the arrows from both notes pointing to the cost operation. The direction of the arrows chose the direction that the tensors are flowing are two placeholder operation notes x and p keep input hidden which is used for the dropout function service entry points for our tensors. They move through each of the three layers through the weights into the cost function every time eventually to the accuracy function and finally to the output placeholder that gives us a prediction to reduce clutter. Tensor flow automatically shows nodes with many connections to other nodes in their own area to view and detail. If we wanted to we can add them back to the main graph by clicking the add to main graph button and the detail car Yo um, bringing it back. Let's take a look at the sidebar. We can choose which one we want to display trainer test and show the graph at each time step.

Speaker 1:          04:47          The color option lets us see at each time step what structures are being used, which device each operation is running on compute time along a scale and memory usage. We can also manually upload a safe tensor flow graph right from the Ui. If we'd like. Let's move on to distributions. We created histogram summaries for all three of our weights between layers and we can view the distribution of each weight here. Let's switch to our complex classifier. The light part shows all the weights across time and the shaded parts shows those weights that are actually activated during training. These curves represent percentiles like the Max and mean and median. The histogram plot allows us to plot any variables we like from our graph. It's showing how the values of our weights change with training isn't it beautiful? Tenser board lets you visualize your data so you can debug and optimize it. You can create summary operations in your tensorflow program, which takes tensors as inputs and produces outputs. Tenser board reads these summaries and displays them visually. The challenge for this video is to create a tensorflow program that visualizes audio data in some way and winter. It gets a shout out for me in two episodes, more info and links in the description. Please subscribe for more videos, and for now I've got to go get some sunlight, so thanks for watching.