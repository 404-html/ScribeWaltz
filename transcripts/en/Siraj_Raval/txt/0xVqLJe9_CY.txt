Speaker 1:          00:00          Hello world, it's Saroj and today we're going to learn how to prepare a dataset before feeding it into a machine learning model or example code will predict if someone is meditating or not by training on a Dataset of brain scans. Data is raw information. It's a representation of both human and machine observations of the world. Everything can be represented as data, all science, art, literature, all of it can be represented as ones and Zeros on a computer. When we enter a virtual world, we are literally surrounded by data. Since it is the fundamental building block of everything we see and when we observe something physical in real life, it becomes data in our brain. Unless our universe is a simulation, then everything you see is already data. If you don't work at a tech giant, how are you supposed to get that data? That brings us to step one in preparing data, deciding the right kind of data to use that data set you use entirely depends on the problem you're trying to solve.

Speaker 1:          00:58          If I want to build a chat bot that comes up with new innovative product ideas, I'm not going to use a Dataset of Tim Cook Dialogue. Data is a means to an end and the good news is there is a public dataset for almost any topic. You can think about a couple of sites I like to use to find cool datasets are Kaggle since I love the format of their website and how they explain each of their datasets in detail. Also, the datasets subreddit is great for requesting datasets you want and there is this awesome list of public datasets in the read me of this get hub repo that I'll link to in the description. Google's advanced search feature is also super helpful. Usually combining a few keywords with the word data or database is enough to find what we need and to make it easier. We can specify the type of file we want like CSP and the type of domain like edu or Gov.

Speaker 1:          01:46          Usually a website has an API that makes it easier to get the data you need, but if it doesn't, you can use a library like beautiful soup to take a raw html webpage and just scrape the data directly. Diy data died. No. Once we decided the type of data we want, our second step is to process it. We're going to write a function to extract data from a brain scan data set. Then we can feed that data into a single layer. Neural network created intenser flow, the network. We'll create a separator line between two classes so that given a new person's brain scan data, it can predict if they are meditating or not. Let's take a look at this data. This is a list of neurological metrics collected via an EEG device for a set of human volunteers. There are two possible classes, either meditating represented by a one or not meditating represented by a zero, and there are three features for this data.

Speaker 1:          02:39          A measure of mental focus, a measure of calmness and the volunteers age want to format our data properly. Data could come in the form of a text file or a relational database or like, well, we have a CSV and there's a library to convert pretty much any file type to another. So make sure you have your data, format it to a file type that you most feel comfortable with. Once it's in the right format, we'll want to clean the data. Sometimes we have instances in our data that are incomplete. We can iterate through each row and delete those instances by checking if the value is empty or not. We should also decide what features to use. Deciding what features are important is one of the key parts of data science. If we don't use the right features, our model will make bad predictions. We only want to use features that are relevant to our problem.

Speaker 1:          03:26          Their gender has nothing to do with their meditative state, so we can totally disregard that feature. So let's first create two arrays. One Array, we'll hold our class labels. The other array, we'll hold our features. We can iterate through every line in our CSV file. Using this for loop. We'll define a row which is a single instance as an array of values. By splitting the line by the comma separator. Using this row, we can first get the associated class label by retrieving the first value in our row array, converting it to an end than adding it to our labels array. Now we can do the same thing for our features are ray. We'll take each feature and convert it to a float. Since we want precision in our values, our feature array is now an array of arrays. Now that we've pulled our data from our dataset file into memory, we've arrived at the last step transforming the data.

Speaker 1:          04:14          One possible transformation is decomposition. Sometimes we have features that are too complex like the date. If we're trying to predict which day in October is most likely to get rainfall this year, we don't really need the month and the year. If we decompose that feature into just the day of the month, that'll make our model more accurate. Since we're satisfied with the features we have in their class, labels will perform. The only transformation we need will transform them into vectors. Vectors are numerical representations of features. All features can be represented as vectors, words, images, videos, all of it. We can take these vectors and feed them into our neural net directly. We'll convert our array of arrays into a two d matrix using numb Pi's matrix function and set the type to float. This is a matrix of feature vectors. Each vector contains a list of features. For an instance, we'll also want to transform our class label array into a num py array. Because a number [inaudible] can easily be converted into a one hot matrix. Then we'll return are fully processed feature matrix and one hot label matrix. So what is one hot encoding yo DJ dropout. Bump it.

Speaker 2:          05:25          How was Portland? So I made it. No, right. I'll put words in like cat, dog, duck in the lake. If I encode them as numbers, it's a o k one, two, three, four, all on display. But if four is more than one, is Palais more than cat? No, but I'm not gonna leave it at that because I'm not trying to rank. Differentiate. Yeah, differentiate. So I'll make each of vector of ones and those different but not right. That's just how with go three, no two, no one hot and code. Yeah. Once we have our

Speaker 1:          05:53          data processed, we'll want to feed it into our graph. In tensorflow, the placeholder object is considered the gateway for data into our computation graph, so we'll want to initialize two placeholders, one for our class labels and one for our associate in feature vectors. And when we finally run our training step using the run function, we can feed our data into the graph using the feed dictionary parameter. The labeled placeholder gets the labels and the feature place holder gets the features. When we run our model, it'll show the classification line that it created two separate the meditating from the non meditating people and if we feed it a new instance, boom, it'll classify it just like that. So to break it down, the steps to prepare a dataset are selecting the right data, processing it and transforming it. You can easily find public datasets on the web.

Speaker 1:          06:45          The a number of sources that I'll link to in the description or you say web scraping tool, like beautiful soup to create them yourself. And once we have our data, we'll convert them into vectors, which are numerical representations that are machine learning model can understand. The winner of the make a game bought challenge is Carl big highs and he created a bot that used deep cue learning. His Bot fed the pixel data, it received to a convolutional neural net that updated its policy over time through trial and error. Also, this was his first get hub repo that ass of the week, and the runner up is Roohan Vermont. While my Demo Bot couldn't finish a lap, his could in just two and a half minutes. The coding challenge for this video is to write a script that classifies a Pokemon by their element using a Dataset that all provide post your get hub, humbling in the comments, and I'll announce the winner in the next video. Please hit that subscribe button. And for now I've got a decentralize the web, so thanks for watching.