Speaker 1:          00:00          Red Pill or blue pill. Let your curiosity guide you. Hello world it Saroj and curiosity is a concept that we're all familiar with. It's the desire to know or learn something. It's something that all of us have experienced at some point. It's only human right, wrong. Recently a team at Berkeley published a paper on curiosity driven learning and they demonstrated how it helped enable their AI agent to learn how to play the popular game of Super Mario brothers very efficiently using the added benefit of curiosity to help Mario explore his options. We'll discuss how it all works in a second, but first we need to understand why they decided to go there. In any kind of real time environment scenario where time is a relevant data point, whether that's a simulation or a smart power grid or robotic movement, we can use a class of algorithms called reinforcement learning to learn an optimal policy for our agent so that it can achieve a predefined objective.

Speaker 1:          01:08          That objective could be any number of things. Transferring power across nodes at the lowest cost, detecting intruders in a real time security system or just winning a game. An Ai agents policy then is a function that given an environment state, we'll return the optimal action that an agent should take to reach that objective. There are several possible functions that our AI agent will need to approximate in order to learn the optimal policy, and several known techniques can help it do that, but of all of the function approximation techniques, neural networks have shown the most promising results. Thus using neural networks to approximate one or several functions involved in a reinforcement learning scenario is considered deep reinforcement learning or deep RL, and it's no secret that deep RL is the hottest subfield of AI right now with many researchers adding to the scope of human knowledge on the topic every single week.

Speaker 1:          02:11          One of them being the research group that we're talking about in this video, they first identified two key problems with all reinforcement learning algorithms. The first is the problem of sparse rewards. That's the time difference between an action and its associated reward. For example, if we use any reinforcement learning algorithm to learn how the game of breakout works, it would get a lot of reward feedback immediately because it's such a fast paced game. But in a more complex scenario like say splinter cell trying to sneak into a high security facility, I miss this game. It's not immediately clear what the rewards are for doing. Some set of actions will have to wait until much later to see how an action plays out towards the larger goal of sneaking in. The second problem is that extrinsic rewards are not scalable. That means rewards that we humans define extrinsic to the agent work well enough for simple environments.

Speaker 1:          03:13          But when we scale to much more complex environments, there are lots of rewards that we'd have to think about and and code into the environment for our agent to learn from hand coding these rewards is not a scalable approach. So their solution to these two problems in reinforcement learning was to define a reward function that was intrinsic to the agent, meaning that the agent generated by itself and they called this reward function, curiosity. They were inspired by the fact that we humans are so used to operating with rewards that are extremely sparse. For example, some entrepreneurs will toil away at building a startup even if there is no immediate reward insight, their businesses could be failing. They could be running out of investor support. Yet some entrepreneurs will continue despite this. I'm not really sure who fits into this category, but uh, anyway, psychologists call this intrinsic motivation, otherwise known as curiosity.

Speaker 1:          04:17          The entrepreneur pushes through the uncertainty guided by the question. I wonder if I can do this and tries to answer it by doing it. The researchers define curiosity as an intrinsic reward that's equal to the error of an agent in predicting the consequences of its own actions given its current state. It needs to be able to successfully predict the next state given the current state and action taken. The basic idea of curiosity is to encourage an agent to perform actions that reduce the uncertainty in the agent's ability to predict the consequences of its own action. This uncertainty will be higher in areas where the agent has spent less time or in areas with more complex dynamics. To measure the agent's error, we need to build a model of environment dynamics that predicts the next state given the current state and action. The question then becomes how do we calculate this error?

Speaker 1:          05:18          Let's take a step back. The question we're trying to answer is how can our agent predict the next state? Given our current state and our action, we can define the curiosity as the error between the predicted states given our current state and action as well as the new real state. In the case of our video game or agent is Mario, and each state will be a game frame which consists of a bunch of pixels. The problem though is that there are so many pixel values to predict in a single game frame, so the entire process will be very computationally expensive. They decided that making a prediction in the raw sensory space of game pixels wasn't the optimal way to frame the problem. So they instead transformed the raw sensory input, which is an array of pixels into a lower dimensional space that only contains relevant information.

Speaker 1:          06:15          But what do we mean by relevant? What makes some data relevant and other data irrelevant to our agent? The way they ensured that it only contained relevant information is by defining a rule for what data could be considered relevant beforehand. Relevant data. It needs to model both environment objects that can be controlled by the agent as well as environment objects that can affect the agent. Everything else is not in the agent's control and has no effect on them. And we can discard that. So in our Super Mario game, our agent, Mario, would need to model himself. Since he's controlled by the agent and as the enemy Coupas approach, we can't control them, but they can affect our agent. So they're relevant. Keep your enemies close, right? We don't need to model the cloud in the sky because it doesn't affect our agent and we can't control it this way.

Speaker 1:          07:15          By constraining what data we represent, we can develop a feature representation with much less noise and our desired embedding space. We'll be compact and informationally dense. So the researchers introduced the intrinsic curiosity module to help the agent generates curiosity. This module is composed of two different neural networks. Instead of making predictions from a raw sensory input space of pixels directly, we'll need to transform the sensory input into a feature vector where only the information relevant to the actions performed by the agent is represented. And in order to learn this feature space, we can train a neural network to predict the agent's action given its current and next states. Because the neural network only predicts the action it has no other incentive to represent within its feature embedding space the factors of variation in the environment that doesn't affect the agent itself. We can then utilize the learned feature space to train a model that predicts the future representation of the next states given the feature representation of the current state.

Speaker 1:          08:26          And the action. The prediction error of this model is an intrinsic reward to encourage it's curiosity. We can formalize these two models mathematically. The first is the inverse model which encodes the state s and s plus one the next state into two feature vectors that are trained to predict an action. The second is the forward model. It takes us input a feature vector and predicts the feature representation of the next state. The curiosity of our agent then will be the difference between our predicted feature vector of the next state and the real feature vector of the next state. Thus, we can construct the optimization problem of this module as a concatenation of both inverse loss and forward loss. They trained their agent in the Super Mario World using only curiosity based signal without any extrinsic reward from the environment. They're Mario agents could learn to cross over 30% of level one the agent who received no reward for killing enemies or avoiding fatal events, yet it automatically discovered these behaviors.

Speaker 1:          09:33          A possible reason for this is that getting killed by the enemy will result in seeing only a small part of the game space satisfying it's curiosity. In order to remain curious, it's in the agent's interest to learn how to kill and dodge enemies so that it can reach new parts of the game space. This suggests that curiosity provides indirect supervision for learning behaviors. Pretty cool stuff, right? As we learn more about Ai, we learn more about ourselves and how we learn. There are three things to remember from this video. Reinforcement learning suffers from sparser awards and extrinsic rewards, which are not scalable. A solution is to create an agent that creates intrinsic rewards itself using curiosity and the intrinsic curiosity module consists of an inverse model used to learn the feature representation of a given state as well as a next state and a forward dynamics model used to generate the predicted representation of the next state. Please subscribe for more programming videos, and for now, I've got to explore, not exploited, so thanks for watching.