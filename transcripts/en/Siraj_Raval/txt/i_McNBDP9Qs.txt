Speaker 1:          00:00          Hello world. It's a Raj and welcome to my new AI for video games series. For the next 10 weeks, I'm going to teach you how a really popular subfield of AI called reinforcement learning works and we're going to be using all kinds of game environments to implement the theory behind the algorithms we learn the majority of the recent advancements in AI have been due to the use of supervised algorithms, namely neural networks applied to big datasets and using lots of gps for computation. Neural Nets have been around since the 50s and they're considered universal function approximators. That means that given any set of inputs and outputs, if given enough examples, they can learn the function, the mapping that relates both of them together. We can then use this function to predict new outputs given some set of inputs. Everything from medical imaging for hospitals to license plate detection for governments to identify and crop yields for farmers.

Speaker 1:          01:07          These incredible applications of deep neural nets have been due to the fact that the data set used to train the algorithms had an associated set of labels. A lot of times though, if labels aren't available, data scientists will hire humans to hand label their datasets using services like Amazon's mechanical Turk, but ideally we don't need labels. We can just train our algorithms on unlabeled data. Since the vast majority of the world's data does not in fact have labels. So if we want to train algorithms, unsupervised meaning no labels, then we can use techniques like clustering and anomaly detection. These are fast improving, but there's also room for another class of learning techniques that are based on trial and error in an environment setting. This is called reinforcement learning. The basic idea is that in reinforcement learning, the labels are time delayed and instead of calling them labels, we call them rewards.

Speaker 1:          02:11          While supervised learning tells you how to achieve your goal. Reinforcement learning tells you how well you achieved the goal. There are lots of problems settings where the idea of time delayed labels makes more sense. Think about if you were tasked with creating an AI that learns how best to control the temperature in a data center. How are you going to tell your algorithm what the correct setting of each hardware component is at any given time step using reinforcement learning, you can use feedback data such as how much electricity was used at a certain time period or the average temperature. This is literally how Google reduced the cost of cooling. It's data centers by a massive amount, so it's the real deal. Unlike magic leap, the human brain probably implements all three of these learning paradigms together. The Neo cortex could be similar to a stack of auto encoders learning low level perceptual feature detectors close to the sensory input with no correction signals.

Speaker 1:          03:13          These networks possibly get fine tuned by top down feedback in a supervised manner using immediate and reliable correction signals and the supervision and behavioral control mechanisms could be largely learned via reinforcement learning with plenty of different rewards signals, but who knows, right? The only way to unlock the secrets of intelligence are to do some experimentation because reinforcement learning revolves around using rewards in some kind of environment instead of using a premade dataset. Robotics has been a testbed for RL research for decades. There have been several successes in getting our l agents to learn to play sports, navigate a helicopter autonomously getting robots to Wa and getting them to fold laundry. We've created some seriously capable robots that are theoretically able to do any task a healthy human could do. But the reason they're still so limited is because of software. Robotics is a software problem, not a hardware problem.

Speaker 1:          04:15          In parallel to the robotics world, game environments have also been a test bed for our l since they are safer than the real world. And the barrier to entry is just having a laptop so anyone can test out their algorithms to of the most popular AI research institutions in the world, open AI and deep mind extensively use game environments to train and test their algorithms and their worldclass algorithms like Alpha go zero. And the Dota two bought both of which be world class players for the first time ever heavily used reinforcement learning. So let's say we have some game environment, the simple game of tic tac toe where the goal is to be the first to successfully create three in a row. And we have our AI, which we'll call an agent. Our goal is to have this AI learn how to become really good at playing tic tac toe against humans rather than just hard coding in a bunch of if then statements, how can we formalize this problem?

Speaker 1:          05:13          That's where the mathematics of Algebra and probability theory come in. We can use the variable s to define a set of game states. These are all the possible configurations the board can be in at any given time step during the game. We also have a, which is a set of actions that our agent can take. In this case, it would be the position on the board they'd like to place their Exxon p represents the probability that a given action a in a given state s will lead to another state. It's a measure of how likely a board state will be a certain way after the agent plays a certain move or represents a reward value. It's what the agent gets after the game transitions from one state to another after a an action. This is the time delayed reward. We were talking about the signal that will tell the agent that the action it's taken is either good or bad, which it can then use to further improve itself.

Speaker 1:          06:08          Lastly, we'll have a discount factor which represents the difference in importance between future awards and present rewards. We can call this formalization a mark Haub decision process. It's a way of framing a problem where at each time step an agent is in some state and may choose some action. This will probabilistically transitioned the agent into the next state and give some reward as a function of the transition. Crucially, the transition obeys the Markov property, meaning the transition and reward probabilities are only dependent on the pair s and a and not dependent on the entire history of previous states. In other words, the state s should encode all of the important information to be able to make good decisions on which action to take. So reinforcement learning is a method of solving this kind of process and we can break down our El methods that do this into two groups.

Speaker 1:          07:05          Policy, iteration and value. Iteration methods, policy methods, perform a search in a policy space. And value methods tried to estimate the value function. The value function is a function that tries to estimate the longterm utility of either a state or a state action pair and the agent just selects the Arg Max action over this function. It represents how good is a state for an agent to be in. It is equal to the expected total reward for an agent starting from state s. The value function depends on the policy by which the agent picks actions to perform. Among all possible value functions. There exists an optimal value function that has higher value than other functions for all states. The optimal policy is the policy that corresponds to the optimal value function. Value function algorithms in reinforcement learning generally follow the current policy as defined by the current value function and whenever a reward is received, the agent propagates it back through its history, assigning each action some of the reward over time.

Speaker 1:          08:14          This gives the agent a good idea of what states and actions result in good rewards. So value iteration computes the optimal state value function by iteratively improving the estimate of the of s both value iteration and policy iteration algorithms assume the MDP model is known by the agent comparing to each other. Policy iteration is computationally efficient as it often takes considerably fewer number of iterations to converge, although each iteration is more computationally expensive. So there are three important points to remember from video. Reinforcement learning is a technique that lets ais learn to complete an objective in an environment using time delayed labels, Aka rewards as a signal. We can formally call this a mark Haub decision process which relates states' actions and rewards for an agent. And two fundamental ways of solving MDPs is by either using a value iteration or policy iteration algorithm. This week's coding challenge is to create a simple value iteration algorithm for an AI using the open AI Jim Environment. I'll announce the top two submissions next week, so make sure to post your get hub links in the comment section and I'll review them personally. Please subscribe for more programming videos and for now I've got to go hack my Nintendo switch. So thanks for watching.