Speaker 1:          00:00          Hello world, it's Saroj. And our task today is to build a recurrent network, a type of recurrent network called an Lstm or long short term memory network to generate m and m lyrics, that star task. And we're going to build this without using any libraries, just num Pi, which is for Matrix math because we want to learn about the math behind LSTM networks. We've talked about recurrent networks earlier on in the series and the LSTs are the next logical step in that progression of neural network learnings. So that's where we're going to do today. And um, we're gonna first talk about recurrent networks. We're going to do a little refresher on what recurrent networks are, how we can improve them, and then we'll talk about LSTM networks and how they are improvements and the mathematics behind them. Then we'll build all of this build as a, we're going to look, we're going to, I'm going to explain the code cause there's quite a lot of code to go over here.

Speaker 1:          00:56          There's a lot of code. Um, but we're going to look at all the code and then I'm going to code out manually the forward propagation parts for both the greater recurrent network and then the LSTM cell itself. So strap on with every math hat you have because this is going to be so hard. We might as well just give up. Let's just forget this. No, no, I'm just kidding. No, this is actually going to be, there's going to be pretty easy. This is going to be pretty easy. If you got the recurring that video, this is going to be pretty easy stuff, although it's a little more complicated, but you will get this. Okay. Get, get ready for this. I'm going to make sure you get this. Okay. So here, here we go. What is recurrent network? Can you tell me what a recurrent network is in one sentence? I'll give you 10 seconds. Go.

Speaker 1:          01:42          Okay. That's all the time you get. I'm not gonna wait a full 10 seconds for current networks are cool. They're useful for learning sequential data. We know this, we know this. They're useful for learning sequential data as series of video frames text. Um, music, anything that is a sequence of data, that's where recurrent networks do really well. That's what they're made for. They're very simple. You have your input, your input data, and then you have your hidden state and then you haven't helped put. And so the different between recurrent nets and feed forward nets are that recurrent nets have something different here. So in a normal feed forward and that we would have our input, our hidden layer and then our output layer and that's it. And then we would have to wait matrices between each of these layers that are the, those are matrices, right?

Speaker 1:          02:26          That we multiply right? Input Times weight at a bias activate. Hopefully you said activate because that is a rap slash mnemonic device. But input times wait at a bias activate repeat over and over for feedforward networks. A difference though for recurrent networks is that we add another weight matrix and this other weight matrix called synapse h in this diagram connects the hidden state back to itself. So it's just a third weight matrix. And what is the reason we add that is so whenever we, whenever we are training our network, we're not just feeding in new data points. So if the data's, so if we're trying to train our current net to remember the next uh, to learn to predict the next number in a series and the series is one through 10, right? We want to predict the 10 we would say, okay, one, two, three, four, five, six, seven, eight, nine, 10.

Speaker 1:          03:18          We wouldn't just give it the numbers. We would also feed in the hidden state, right? So we would say, okay, so given a one, predict the two. Okay, now given the one and to predict the three, okay, give him the one, two and three, predict the four. And that's how we would train every iteration. But that's not all we would give it. We wouldn't just give it the input data. We would also give it the hidden state. We would give it both. And so that's why we have a recurrent matrix that's connecting the hidden state to itself because we are not just giving it, giving it every new data point, the ones and the twos and threes. We're also giving it the hidden state, which is basically a, which is a matrix and we're multiplying it by the hidden state and the new input data we are, we are feeding in the hidden state and the input data at every time step.

Speaker 1:          04:02          And so that's why we have a hidden matrix. And so you could think of this as own rolled as just a series of feedforward networks, right? Or we give it an an an input data point and then we get a hidden states. And then in the next time step, we don't just give it this new data point x one, but we also give it the, the hidden state from the previous time step. So we just continued to do that. So that's, that's one way of looking at our current network as just a big chain of feedforward networks. A chain, right? A block chain. No, not blockchain. Although I do want to talk about blockchain, it's coming. Don't worry, I'm coming for you. Blockchain, but not yet. We're still talking about recurrent networks. Amazing stuff. Amazing stuff. Blockchains coming though. Oh, I don't want to give away too much.

Speaker 1:          04:42          I just did anyway. Ah, okay. Uh, you guys can tell I'm very excited about blockchain anyway. So we give it the hidden state and the input at every time step and we just keep repeating that. And that is a recurrent network. But there is a problem here. The problem is that, uh, there are 99 of these problems and no, I'm just kidding. The problem is that when, when, whenever, oh, so here's the problem. The problem is actually really interesting. It's called the vanishing gradient problem. Okay. So it's called the vanishing gradient problem. So let's say you're, you're like, what is this? So let's say we are trying to predict the next word and a sequence of texts, which is actually what we are trying to do right now. Duh. But let's say that's what we're trying to do. And let's say we're trying to predict the last word in this sentence.

Speaker 1:          05:26          The grass is green, right? So all were given. So we know that the word is you know, green, but let's say we're trying to predict it. So all we're giving is given is the grass is and we're trying to predict that last word. Recurrent networks can do this, right? They, they can easily do this because the distance between what the word we're trying to predict the distance between that data point in the sequence and the previous data points is it's pretty small. You write the different, the distance between grass, which is the context we need. And Green is pretty small. The only difference is that the word is, is between, is between those two words. And so that's easy to predict because um, that's the only context that we need. Let's say it had some tendencies before that said, you know, the, the law in his reign or the grass is, let's say you already said the grass is greener.

Speaker 1:          06:14          So it's already like the previous sentence. So it already made that, that distinct connection between grass and green. And you know those two things are related. So of course it's very easy to do that but it, but let's also say, and you, but let's also say that we want to predict the next word in this sentence. So let's say we have a huge paragraph. Let's not even a parent. Let's say we have a 30 page essay and the beginning of the essay, it is a first person essay and it's about a guy named John. Okay is French dude. He's got a mustache, unnecessary detail, but a guy named John and it starts off with I am French as it should be, I am French and then there's 2000 plus other words and we're trying to predict the, the last sentence, the last word after 2000 words and that is the word French.

Speaker 1:          07:03          I speak fluent what? And now let's say between I am French and I speak fluent French. There's all these other languages like and so this guy is Spanish and I heard some German on the subway and all of that is actually irrelevant to what he speaks fluently, which is French. We've got to know that context. We got to know that he is French. So of course he's gonna speak fluent French. So we've got to find a way for our network to be able to remember longterm dependencies. And so the whole idea behind recurrent networks, the whole reason that we feed in the hidden state from the previous time step for every new iteration it so that we can have a form of neural memory, right? That's why we don't just feed into the, the previous input, but we feed it in the previous hidden state because the hidden state is that matrix that represents the learnings, the learnings of the network so that we can give it a form of memory like what is remember before and the new data points.

Speaker 1:          08:04          But the problem and you would think, okay, so that's all you need, right? Of course the hidden state is going to be updated more and more. And the whole idea of recurrence is made so that we have a form of neural memory for sequences specifically. But the problem is that when we're, whenever we are back propagating, the gradient tends to vantage. So it's called the vanishing gradient problem. So, let me explain this. So whenever we're forward propagating, we have our input data, right? And we are trying to predict the next a word or character or a musical note in the sequence. And to what we do is we say input times wait out of bias, activate, repeat over and over. And then you know, recurring over and over and over again. And you get the output, which is the prediction. The prediction is the next word or whatever.

Speaker 1:          08:47          And so once you had that prediction value, then you're going to say, okay, the doctor rise a prediction into a number and say prediction minus the expected, the, the, the expected output, which is the actual label or next word or character. Cause we know it and because it's a supervised. And so then we have the error value or lost value. And then we use that air value to compute the partial derivative with respect to our weights going backwards in our network. Recursively work, performing gradient descent or in the context of neural networks. Backpropagation because we are back propagating on error gradient across every layer. But what happens is that as the gradient, remember this is a chain of operations. The chain rule I would happens is as we are, uh, as we are propagating this gradient value backwards across every layer we're competing the partial derivative with respect to the air bar, the partial derivative of the error with respect to our weights.

Speaker 1:          09:44          As we are doing that for every layer recursively the gradient value gets smaller and smaller and smaller for the reason, for linear algebraic reasons, the gradient, the gradient just gets smaller and smaller and smaller. And so what this means is that the magnitude of the, so the whole point of that propagation of grading dissent is to improve our weight values such that are expected output is closer to such that our actually sets that our predicted output. It's closer to our expected output, right? We were trying to minimize the error value. And so the whole point of gradient descent is to give our weight values a direction in which to update such that the error is going to be minimized through a forward pass. And so by direction, I'm talking about what, what values in this weight matrix, this set of numbers is optimal such that if we were to multiply this by the input and then you know, do that over and over again, it's going to give us the rights, output value.

Speaker 1:          10:43          And so that's the whole reason we're computing partial derivatives or gradients, which we also call them because calculus is a study of change, right? Change whether it be in moving bodies or change in terms of uh, how to update a set of values. And when I say direction, it doesn't mean like a literal direction, like up, down, left, right? It means a direction in that, um, the numbers are closer to the ideal optimal numbers that they should be in the weight matrix. So when we multiply the input by them, the output is going to be closer to the actual output that we want. I probably repeated that several times, but you know, it's good. It's good, it's good for us. And so, so that's, that's the whole point of a performing gradient descent, Aka backpropagation. And so the grading gets smaller. And so what this means is that the magnitude of change in the, the first layers of the network is going to be smaller than the magnitude of change in the tail end of the network. The last layers, right? So the last layers are going to be, um, more affected by the change, but the, the first layers are going to be not as effective because the grading update is smaller because, because the grade itself is smaller and so,

Speaker 2:          11:57          okay.

Speaker 1:          11:57          Right? And there are two factors that affect the magnitude of these gradients, the weights and the activation functions. And um,

Speaker 1:          12:07          each of these factors is smarter than one. The gradients may vanish in time if larger than one, an exploding might happen. So that's called the exploding gradient problem. The grade itself is too big, so it can go either direction. Usually it's vanishing gradient, but uh, yeah, this is a problem, right? We want to somehow maintain that gradient value as we are back propagating. We want to maintain that error gradient value so that it is at the full magnitude that it should be to update our weight values for every layer recursively. In the correct way. We want to maintain that rating value. We want to remember that creating value as we'd back propagate. And so how do we remember a gradient value? How do we remember, remember is uh, a word, you know, maintain, remember, you know, whatever you want to call it. And so the solution for this is called using an LSTM cell or a long short term memory cells.

Speaker 1:          12:58          And if you think about it, the knowledge of our recurrent network is pretty chaotic. Like let's say we're trying to, you know, a caption, uh, of a video, a set of frames, a video, right? And it's, he's a guy and he's eating a burger and did the statue of Liberty is behind him. So then there were the network things. Okay, he must be in the United States, but then he's eating Sushi and it thinks, oh, he must be in Japan just cause it seemed Sushi, but it forgot that he was just behind the Statue of Liberty. There exists Sushi Sushi places in New York City too, right? And then, you know, he's riding a boat and he thinks, oh, he must be in, you know, the odyssey or something, but he's still in New York. So we need the information to update less chaotically to account for all of the learnings, the memories that it's learned over a vast period of time, a vast sequence to be more accurate.

Speaker 1:          13:49          And so the solution for this is called the LSTM cell, and it replaces the RNN cell. And so the RNN cell is input times weight out of bias activates, right? And would that be a weight matrix that is connecting to itself? And so the difference is an Lstm cell, basically just you just replace it with an Lstm cell. You just take that out and you replace it with an Lstm cell. And what it is, it's is a more complicated or more extensive series of matrix operations. So let me, let me talk about what this is. Okay, let me, let me give this a go. So LSTM cells consist of three gates. You've got an input gates right here. You have your output gate right here. You ever forget gate right here. And then you have a cell state. Now you also see this input modulation gates.

Speaker 1:          14:38          So that's actually only use sometimes, but uh, let's just forget about that. That's, that's just forget that exist. You have an input gates on Kate's and a forget gates. And so there's many, there are many variants of Lstm is and why you see this input modulation gates. But the most used ones are just input, output, forget, and cell state. So just forget about the input modulation gate for now. Okay, so we've got three gate values and then we have a cell state. So you might be thinking, okay, so what are these gates like? Like what, what is a gate? And so a gate is just like a layer. A gate is a series of matrix operations and it is input times weights out of bias activate. So in a way you could think of an LSTM cell as kind of like a May neural network.

Speaker 1:          15:23          These gates all have weights themselves. So they all have their own set of weight matrices. That means that an Lstm cell is fully differentiable. That means that we can compute the derivative of each of these components or gates, uh, which means that we can update them over time. We can learn, we can have them learn over time. And so, uh, so these are the equations for each of these, right? So for forget gate or implicated and applicate, it's input times weight out of bias activate where the input it consists of the input and the hidden state from the previous time step. So, and each of these weights, these gates have their own set of white values. And so what we want is we want a way for our model to know what to forget and what to remember and what to pay attention to in what it's learned, right?

Speaker 1:          16:18          What is the relevant, and that's called the attention mechanism. What is the relevant data and everything that it's learned. What is the, um, relevant part of what it's being fed in this time step to remember and what should forget the cell states is the longterm memory it represents what all of the learnings across all of time. The hidden state is akin to working memory. So it's kind of like a current memory. The forget gate also called the re remember vector learns what to forget and what to remember, right? One or zero binary outcome. This, the input gate determines how much of the input to let into the cell state also called a save vector, what to save and what not to. And the output gate is akin to an attention mechanism. What part of that data should it focus on? And so you might be thinking, well how is like I see you might be thinking like, okay, so I see these equations, I see how like input times wait antibodies activate a t is akin to forgetting and input and output.

Speaker 1:          17:19          I see how there's an ordering to this, right? We have, we have an ordering to this. We first, you know, go through an input. We, we learned what we learned, what to forget, and then we compute that cell state and then we, and then we send it to the output. Um, ultimate. But, and then ultimately we compute the cell and the hidden state. And these are the two key values that we output. So you might see these and you might think, okay, like I see the ordering, I see how they represent for getting and but I don't like make the connection between still like how it knows what to forget and what to remember. And that's again, that's the amazing thing about um, neural networks like these, these gates are essentially perceptrons there. They're like mini networks, right? With, with a single node. The node is the gate itself, right?

Speaker 1:          18:09          It's a single note what they single activation function. All of these gates are perceptrons, they're like many neural networks, like single layer neural networks. And it learns what to forget, what to remember. Um, based on gradient descent. Again, it's the magic of grading dissent. It learns what is necessary over time. And so these, these components represent mechanisms for forgetting, remembering, and what to pay attention to or attention. And so that's what the and provides us. So instead of, so this equation would be for a normal recurrent network. So you would have your input times your weight plus the hidden state times its own hidden state to hidden state matrix. You'd activate that. And that would give you the hidden state at the current time step. The difference is that it would look like this instead. It's a, it's a more extensive series of operations, right?

Speaker 1:          18:59          So that's what it is. And uh, yeah. And so, yeah, so that's kind of the high level of how that works. And now we're going to look at it and code as well. We're just going to help you know, your retention. But what are some use cases of this? Like I said, it's all sorts of sequential data, any kind of texts, any kind of sequence data, it's going to, it's going to be able to learn what the next uh, values in that sequence are, which means it will be able to both generate and discriminate that type of data that you've trained it on, right? In this case, it's, it's, it's handled, it's a characters that I can draw. It can draw. So this is very popular in NLP. Andre Carpathy, the famous AI researcher. He has a great blog post on this, the unreasonable effectiveness of recurrent networks.

Speaker 1:          19:46          I assume you've read that. If not, check it out. Um, I'll also link to it, probably, I'll link to it in the description. And so yet to use for translation and send them an analysis and all sorts of NLP tasks. Uh, it's also used for image classification. Like if you think of a picture as a sequence where each pixel is a sequence, predict the next sequence. So there's a lot of different ways that we can frame our, our, our problem and you know, Ellis teams can be applied to almost any problem. Very useful. And so what are some other great examples where I've got two. One is for automatic speech recognition with tensorflow. So yes, it's a little abstracted with tensorflow, but it's a really cool use case and definitely something that checkout. So check out that demo. And then also, um, this, this repository, which is just a visualization of Lstm, which is going to definitely help with your understanding to check out this visualization.

Speaker 1:          20:41          Very cool. Very cool demo. Uh, it's in Javascript, I think. Yeah, it's in javascript. Oh No, it's in great. So yeah, so there's that. Definitely check that out. And so our steps are going to be the look at the recurrent. Now we're class. So we'll have a recurrent network class, Plano Rucker and network. And we're going to use an LSTM cell instead. So we're going to replace it and then we'll build that LSTM class and then our data loading functions for our text. And then we'll train our model. Okay, so what is this right here? This jumble of numbers with no explanation. This represents the forward pass, the series of Matrix operations that were computing, uh, to get our output, our predicted output. Okay, so, and we'll look at this and code. So, okay, so a recurrent neural network is going to remember. So what we're going to do is given our m and m text, we're going to predict the input, the next word in the sequence, given the previous words.

Speaker 1:          21:34          So what that means is the input is going to be all of those words that we have, just every word and our output will be the same thing, but just moved over one. That means that, you know, it'll, it'll look like this. Like, you know, during training we have a series of, you know, training iterations, right? So we'll say, you know, like, uh, let's say, you know, my name is slim shady would be like the, the text. So we would say my, and we would try to predict name, okay, we've got that, uh, compute the error back propagate, next iteration weights are updated, my name and then we're going to try to predict is right. And so then we say, uh, try to predict is we have an actual output. We have a predicted output, compute the error back propagate, repeat. My name is, I'm trying to predict slim.

Speaker 1:          22:22          You see what I'm saying? So we just keep doing that. And so that's why it's moved over by one. So our inputs and our outputs are going to be our words. We have the number of recurrences that we want to do, which is the number of words total, right? Cause we're gonna, we're gonna, we're going to perform recurrence in our network for as many times as necessary until we iterate through all those words up until we get to that predicted ward that we need to be at. And then we have an array of expected outputs. Uh, and then we have our learning rate, which is our tuning up for, you know, too low and it's never going to converge to high and it'll convert it, it'll overshoot, uh, and so we will never converge as well. So that's where our learning rate is. So now let's look at this.

Speaker 1:          23:03          So we're going to do a bunch of an initialization. So remember the difference between LSTs and Plano recurrent nets, or is our, that we have more parameters, those, those gate values, and we, uh, have new operations for those parameter values. So we're going to initialize all of those right at the beginning. And Look, you might look at this and think, wow, this is very complicated and difficult. But remember with tensorflow, with care os you can, you can do this in 10 lines of code 20, 30, 40 lines of code. We're looking at the, at the details here. That's why it looks so long. Okay. So that's why it looks so long. So now let's look at this. So we're going to initialize our first word, the size of it there, next word, the size of that. And then our weight matrix. This is the weight matrix between the input and the hidden state, which we're going to initialize randomly of this size of our predicted output.

Speaker 2:          23:56          Yeah.

Speaker 1:          23:57          Well, initialize this variable, gee, that's going to be used for r m s prop, which is a, a technique for gradient descent that decays the learning rates. I'll talk about why we're using g later on. We're not ready for that right now, but then we're going to compete, we're going to say, and then you tried the length of the recurrent network, right? That's the number of words that we have. The number of recurrences are learning rates. These are all parameters. And now we're going to have a raise for storing our inputs at every time step on array for storing our cell states on a ready for storing hormone output values are hidden states. And then our, um, take values, right, are for gate values. Well, one of them is actually a cell state. These aren't LSTM values. So before that, these were the, uh, network, the recurrent network values, and now he's the LSTM cell values.

Speaker 1:          24:45          And then we have our, our array of expected output values. And finally we have R, l s t m cell, which we initialize right here, giving it our inputs, our outputs, the amount of occurrence and the learning rate, just like we did for the recurring network. Remember, it's like a mini network. It's like a network in a network. And if you think of the gates as networks, and it's a network in a network inside of a network or no, it's three networks inside of a network. Inside of a network. Okay. So yeah, talk about inception, inception, recurrence, inception. So back to this. Um, those are our initializations and now we have our sigmoid, which is our activation function. It's a simple nonlinearity, right? And then we have our derivative of the sigmoid function, which is used to compute gradients. That's why we have the derivative. Right. We'll talk about that in backpropagation. And then we have four propagation, which is our series of Matrix operations, which I'm going to code. Okay. So for, for propagation,

Speaker 2:          25:43          yeah,

Speaker 1:          25:44          we're going to do this in a loop, right? Because it's a recurrent network for the number of, uh, loops that we want. Right? Um, so we're going to say, okay, so for the number of loops that we want, that we define in our L, we're going to set the input for the LSTM cell, that input for the LSTM cell, which is a combination of inputs. So we'll set, which we can do with the h stack function of num Pi, and we'll say,

Speaker 2:          26:18          okay,

Speaker 1:          26:18          hi. Minus one or minus one. And then self dot x Lstm dot x equals NP softer. And so this is the s, so this is how we set inputs for the LTM sell by. It's a combination of inputs from the previous output and the previous hidden states right here, right? So that's, that's, that's US initializing our LSTM cell, and now we can run for propagation through the LSTM cell itself. So it's gonna Return. Our cell states are hidden, states are, forget gate are. So we can, you know, store it in our array of values, um, c which is a cell state, and then the output. And so we can compute that using the forward prop, a function of the LSTM cell. And so then we're going to,

Speaker 2:          27:13          uh,

Speaker 1:          27:18          say, let's

Speaker 1:          27:22          store the computed cell state, right? So then we've got all these values and now we can store them locally, uh, uh, in this, in this recurrent networks memory, these are all values that were computed from the forward propagation of the LSTM cell. And so we can just, you know, set the, the hidden state. We've got our, uh, what else do we have? We have our, uh, forget gates. Of course the forget gate is going to be a set as well. These are all values that we computed in our Ford propagation states. We have our input gates, uh, which we're going to set like that. We have our cell state, and then we have our output gate, right? These are all values. We competed there. Now we can calculate the output by multiplying the hidden state, uh, with the weight matrix. So input times wait, alibis activate. So we'll say, okay, we'll compute that output by saying self dot sigmoids we'll use a sigmoid function to activate the wait times the input input times we'd had a bias activate, right? Yup. And then we'll set our inputs cause we've computed an output. We want to set our input to the next word in the sequence, right? Cause we're gonna, we're gonna keep going.

Speaker 2:          29:08          Okay.

Speaker 1:          29:09          And then when we're done with that, we can return the output prediction.

Speaker 2:          29:16          Okay.

Speaker 1:          29:18          Right? So that's forward propagation. That's forward propagation. Now for backward propagation, we're going to, um, look at this. So we're going to update our weight matrices. Uh, that's, that's what, that's the point of backpropagation to update our weight matrices with our learnings. So we've computed at predicted next word and we have our actual next word. And we represent these words as numbers, as vectors, so that we can compute the difference between words. Like how do you compute the difference between words? Well you convert them to numbers or vectors, then we can compute the difference to get the air value. So we'll initialize the error is zero. And then we'll um, initialize to a empty res, um, to empty vectors for the cell state and the hidden state. Remember the cell state in the hidden state where those two values right here, which ultimately these forget input output gates are used to help compute the value of right.

Speaker 1:          30:11          Notice how, forget input and output are used here. That the forget gate is multiplied by the working memory state. Um, and then we add the input gate which and multiplied by the working memory state as well, uh, to remember what to forget, to learn, what to forget and what to remember. And that's what's stored in our cells state the learnings of the forget and the input gates. And we then we then activates the cell state and multiply it by the output to get our hidden state. And so we have for our level, with a weight matrix between the input and the hidden the states, we have our hidden state itself and in the cell state. And those are the key like outer level, um, uh, parameters. And then our inner level parameters are those LSTM level gradients, right? So, so we want the creating values for our forget gate, our input gate or sell ourselves unit or state, and then our output gate, the internal ones.

Speaker 1:          31:07          And so we're going to fill these out. So, uh, we're gonna loop backwards our backdrop, the gate through time, through our recurrence. So we're gonna say we have our calculator output, let's compute the air between that and the expected output. And then we're going to say, okay, we're going to compute the partial derivative vice computing, the error times a derivative of the output times the hidden state. And so once we have that, then we can say, okay, it's time to propagate the air back to the, to the exit of the Lstm cell or the, the, the, the way out to, to the outputs of their recurrent network in general. So the way we do that, it's, it's three steps. We do, we compute the error times the recurrent network weight matrix, and then we set the input values of the LSTM cell for recurrence. We set the input values of the LSTM cell for recurrence.

Speaker 1:          31:59          And then finally we set the cell state of the LSTM cell for recurrence pre like that's pre updates and then we recursively called this backpropagation using these newly computed values. Right? So for our, for uh, this is going to compute gradient updates for our forget input cell unit and help locates and the s they'll have the higher level cell state and the hidden state. Those two higher level, uh, parameters. And so these are all of our gradients. And now we can, this is just for air logging. Now we can accumulate those gradient updates by adding them to our existing empty valleys that we initialize right at the start. And then we can update our LSTM major cs with the average of the accumulated gradient updates and then update our weight matrix with the average of the accumulated grade updates and return the total error of this iteration. And that's backpropagation in general. So then,

Speaker 2:          32:57          okay,

Speaker 1:          32:58          uh, so then,

Speaker 2:          33:04          okay,

Speaker 1:          33:05          so notice this update step with this update step is, is rms prop. It's a way of, the cane are, uh, learning rates over time. And this improves convergence. There's a lot of different methodologies for improving on gradient descent. There's Adam, there's rms prop, there's, um, at a Grad, there's a bunch of these in rms properties, one of them. But here's the formula. I'll just put it up there. Okay? So that's what this is.

Speaker 2:          33:30          Okay.

Speaker 1:          33:31          So now we will. And so the sample function is the same thing as a forward propagation function. It's just, it's, it's what we're gonna use once we've trained our model to predict or to generate new words. So it's the same thing, right? We have our input and for a number of words that we defined, we'll say, you know, generate words or predict words for as many iterations as we defined. So it's the same thing. So we can just skip that. Now for our LSTM cell. So for our LSTM cell, we've given it the same parameters as we did for our recurrent network. It is after all a mini network in and of itself. So it gave it the inputs, the outputs, the amount of recurrence and the learning rate. And so what we'll do is very similar at the start to what we did for our recurrent network will in it will initialize, um, our input the size of it or help put the size of it and then our cell state is going to be empty. How often should we perform a current, we'll initialize that variable are learning rate as well. And now we're going to create weight matrices. We'll initialize these weight matrix. He's randomly just like we would for a, any kind of neural network will neutralize of weight matrices for our three gate values for our, forget our implicates and our application as well as our cell state. So the cell state itself, let me go up here,

Speaker 1:          34:46          has uh, uh, uh, set of weight, major CS, right? Just like all recurrent net, all neural networks. And so that node has its own set of weight. Major sees that, that we multiply to get that output value right? It's a part of a series of operations. And the weight matrix is that learning part. It's the non static, it's the dynamic part of that equation that would essentially you can think of these as gates. You can think of them as layers even. Um, but they're called gates,

Speaker 1:          35:20          but layers are very similar. You know what I'm saying? The layers are very similar. Input Times weight added bias activate, but we called them gates to differentiate, not to be confused with so many terms here, not to, not to be confused with the actual mathematical term differentiate but to discriminate. Right. Okay. So back to this. So now, um, we're, we're, we, we initialized our gates, we and then we've initialize our gates and now empty bellies for gradients that we're going to compute for all of these. Remember all of these gates are differentiable. So that's where these grading values, we'll go sit down, we can update them through backpropagation because we back propagate through, that's the cell itself. We don't just back propagate through the recurrent network at a high level. We are back propagating. That is we are competing gray and values for each of these gates. So we were updating waits for the input for get a cell state, the outputs, Gates where, and the, uh, outer level weight matrix as well for the recurring network. So we're competing gradings for everything. So we're updating everything. It's learning what to forget, what to remember, what to pay attention to, the attention mechanism, the output, and um,

Speaker 2:          36:31          okay.

Speaker 1:          36:32          The outer level weight Matrix as well. So, right. So then we have our activation functions sigmoid just like before and our derivative just like before. And so what we do is we add another activation function here. This is good practice in LSTM networks. You usually see the Tan h function applied pretty much all the time. And did the, you might be asking, well, why do we use 10 age over other activation functions? I've got a great video on this. It's called, um, which activation function should I use search, which activation function should I use? CREPE video on this? And you'll, you'll know it very fast within seven minutes if you watch that video. But I'm at a high level, it prevents the vanishing gradient problem. The Tan h function gives us stronger gradients. Uh, since the data is centered around zero, as opposed to the sigmoid, which is not. Um, and so we use that. And then we also, uh, are going to import the derivative function of it because we went to compute gradients, right? Everything is differentiable. So just like before, will compute the for propagation for an LSTM cell. And so remember the four propagation for an Lstm cell is drum roll please. This set of operations, ultimately, uh, we want to compute the output, right? We want to compute the output.

Speaker 1:          37:48          So that's what we'll do back to this. So we'll say the forget gate is going to equal the input times that forgetting. So we're going to activate the dot product of the forget decades and the input input times for get paid and then activate. That's going to give us our forget gates. And then we're going to compute this, the cell state. We're gonna, we're gonna, we're gonna update the cell state by multiplying it by that for decades. So it knows what to forget. And then we're going to compute the input, which is going to be again, the activated version of d, uh, input times the previous input, the current input times or previous input. And so that's going to tell us that. And so then once we've got that, we can compute the cell state,

Speaker 1:          39:07          which we're going to apply this new activation function for the cell states to prevent the vantage and gradient, which is the cell stayed Timesi input. And then we activate bride, that series of operations. And then we're going to update the cell state by adding the input to times the cell, state the cell and put times a cell. And then for a predicted output, we're going to take those. We're going to say let's compete the dot product between the output gates and the cell state or no, the applicate and the input that's going to go to our predicted output. And so what we can do is we can say, oh no, that's going to give us apart how put gate and to get our actual output, our predicted output, we'll we'll multiply our output gate times the activated version of our self state and that's going to give us our predicted output. We can then return our cell states are predicted output, forget gates in implicate cell, and then our output gates as well. That's for propagation. All right, and that's the equation that I showed up there. So now we've got our forward propagation. Now we're going to look at the backward propagation. Remember we back propagate through the self state as well, not just the higher level recurrent network. So what this is

Speaker 2:          40:31          okay

Speaker 1:          40:32          is we're going to compute the the first error, right? So that's the error plus the hidden states derivative and we'll clip those values so it's not too big. We want to prevent the gradient from vanishing. So clipping, it helps that and we'll multiply the air by the activated cell state to compute the output derivative and then will compute the output updates, which is the output derivatives times the activated output times the input. And then we'll compete the derivative of the cell state, which is error times output, time to draw it it. Oh the cell state plus to derivative cell. So we're computing derivatives of all of these components in the backwards order that's before, which means that forget the forget gate computation or update will happen near the end instead of near the beginning. And so then we're competing the driven of the cell and then the cell update, the derivative of the input, the input update, the derivative of the, forget the forget update.

Speaker 1:          41:25          So you see what I'm saying? We're computing gradients and then we're updating them, right? And then derivative of the cell state and then the driven off the hidden states. And then finally we can, we can return all of those great value, these, those updated grading values for the, forget the input, the cell, the output, the self states and the hidden state. So many different parameters that we have computed gradings for and backpropagation is going to let us compute all of those. Right? Recursively um, computing the air with respect to our weights for every single component we have in that now work in the reverse order that we did forward propagation. And so,

Speaker 2:          42:06          okay.

Speaker 1:          42:06          Yeah. It's just, you know, it's, it's more matrix math than we did before. It's like, it's like six or seven more steps than a recurrent network. Maybe it's more like eight or nine more steps, but yeah.

Speaker 2:          42:17          Okay.

Speaker 1:          42:18          And then our, our updates step is going to just be us updating our, forget input cell and Alpa gradients and then we can update our gates using those grading values and there are grading values.

Speaker 2:          42:31          Okay.

Speaker 1:          42:32          Okay. So then that's it for our LSTM cell are recurring network and now we can look at our load text and our export text function, which are not as interesting. But what we do is we load up our eminent text file of lyrics, right? Just like that. And then we compute those unique words for all of them. And so we have a sequence, right? We have a sequence, our input and output sequence, and then we have this export texts. So whenever we've sampled new words, we can write them from memory to disc. That's all. And so for our program we'll say, okay, if so for 5,000 iterations with the learning rate of 0.01 let's load the input and our output data and then we'll initialize a recurrent network using our hyper parameters that we've initialized before. And then for training time for that given number of iterations, 5,000 we'll say compute the predicted next words.

Speaker 1:          43:21          So that's the forward propagation. Then perform backpropagation to update all our weight values, use you aren't error. And then if our air and then just keep doing them and then this, this catch or this, this estate. And we'll say if our error is small enough that it's between this range here, then we can go ahead and sample, which means predict new words, generate new words or network is trained our errors, smallest. Let's go ahead and now just predict new words without having to compute backpropagation to update our weights are weights are already updated enough. So then we can go ahead and define a seed word and then predict some new texts by calling that sample function, which is the four propagation. That's all. And then we can write it all to disk.

Speaker 2:          44:02          Okay.

Speaker 1:          44:03          Right. So let's run this.

Speaker 2:          44:05          Okay.

Speaker 1:          44:06          It's beginning. It's going to take a while.

Speaker 2:          44:09          Yeah.

Speaker 1:          44:11          And then once it's done, I actually have a saved copy.

Speaker 1:          44:17          It's spitting out some pretty dope lyrics, right? These are some pretty dope lyrics, right? That's it for this lesson. Definitely look at this Jupiter notebook afterwards. A look at those links that I have sent you in the description as well as inside of the Jupiter Notebook and make sure that you understand, um, why, how at least why to use LSTM cells. The reason is because it's to remember longterm dependencies. That's at least a high level and you should've got from this video and learns what to forget, what to remember and what to pay attention to. Those are the three things at an LSTM cell as opposed to a regular recurring network lets you do. Okay. So yeah, please subscribe for more programming videos. And for now, I've got to learn to forget. So thanks for watching.