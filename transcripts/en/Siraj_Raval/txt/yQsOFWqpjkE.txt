Speaker 1:          00:00          Hello world, it's Saroj and let's visualize some data, shall we? There was a refill lab study where a group of scientists straps some participants what they fitness tracking device. Then ask them to do a bunch of exercises while recording some physical measurements. And I've got that Dataset, will visualize it in a two d graph so we can make some discoveries from it. We live in a three dimensional world, so we can understand things in one dimension, two dimensions and three dimensions pretty easily. But data can be complex. AAF, sometimes data demands that we tried to reason in hundreds or even thousands of dimensions at some fundamental level are puny. Biological brains just can't do that. So we've invented machine learning to help us learn patterns in our data that we can't recognize ourselves. Take Alphago for example, because they could reason about so many possibilities at once. It made moves that seems strange at first to the world champion it played against, but then it ended up beating him by doing that or IBM's Watson. It consistently diagnosed cancer better than the best doctors because it was able to analyze millions of cancer research papers at once and match a patient's genetic profile to what it had learned. And there are so many things that ml hasn't yet been applied to. So opportunity is right

Speaker 2:          01:13          and turns off the satellites for you. We'll launch his own gym satellite, we're going to collect that data. Our

Speaker 1:          01:23          exercise data is kind of complex as well, but we're going to figure out how to visualize it so that we can understand it. So let's take a look at this data. Each row represents a different person and each column is one of many physical measurements, like the position of their arm or forearm. And each person gets one of five class labels like sitting or standing that represents the activity they've done. So there's a lot going on here, but you'll notice that some of these cells have empty value. So the first thing we'll want to do is clean our data by removing them. Let's first import pandas, which is our data analysis library. That will help us read our CSV file. Then we'll import num pi to help us transform our data into a format our model can understand. Psychic learn will help us create our machine learning model and map plot live will eventually help us visualize our data.

Speaker 1:          02:09          Now that we've imported our dependencies, we can use pandas, read CSV function to download that exercise Dataset directly from the Web and store it and the data frame all variable and we'll also create a variable to store the number of rows in the data by calling the shape function on the zeroth column. To get the count of rows, we'll call the is no function than the sum function to get the total sum of the no or empty columns in our dataset. Then we'll create another variable to count the number of non empty columns in our dataset using the previous variable as a parameter. Now we can remove the columns with missing elements by only using the non empty columns. Also, if we look at our data, the first seven columns don't have information we can use to differentiate between our classes, so let's remove them as well.

Speaker 1:          02:53          Using the I x function which asks for the index of columns we want to delete, we'll specify from the up two column seven we're going to take this clean data and transform it into a set of vectors which we can then feed to our learning algorithm. A vector is a set of numbers and it's how we represent data in machine learning will create directors to represent the features for each person in our data. Let's grab all of our features from our data and store them in a variable we call x. Then we'll want to standardize those features using the standard scaler object of psychic learn. In math terms, this means shifting the distribution of each feature to have a mean of zero and a standard deviation of one, which is a way of saying make all the features operate on the same scale so they are all in proportion to one another.

Speaker 1:          03:38          This will improve. The quality of our results will store the resulting 70 dimensional feature vectors and the ex STD variable. Since 70 is the number of features we have, haven't seen India trying to understand this data set. Each feast is dimension. Then the note f correct. If each feature equals one dimensions, that ain't no sweat check in heartbeat, that's easy. If it goes up, then there's a threat, but behold, once that adds time and it goes up and down, when I add temperature, we can move it all around, but weight and height and strength and a hundred more features. I can't visualize that. I'll get me a seizure. Let's reduce dimensionality, the two or three so we can see and understand this data set. He's the lead data city. There's an entire subfield of machine learning called dimentionality reduction. The Amsa let us represent high dimensional data in a twoD or three d space.

Speaker 1:          04:30          Even a picture can be considered to have 32 million dimensions if we consider every single pixel to be a dimension, but it can also be considered to have just two dimensions, length and width of a photo. We just need to find the intrinsic load dimentionality hidden in our data so we can visualize it. One of the most popular ways to do this is called [inaudible], which stands for distributed. The castic neighbor embedding say that three times fast go. TSMC will allow us to reduce our vectors dimentionality to just to, it does this by taking each one of our 70 dimensional feature vectors and finding the similarity between it and every other vector. The similarities are represented as values and stored in a similarity matrix and then creates a similarity matrix for the projected map points which will contain our final representation of the Dataset. Our first similarity matrix represents where we are in our second represents where we ideally want to be.

Speaker 1:          05:22          We can minimize the distance between these two matrices using a process known as gradient descent. This will slowly bring down the dimensionality of our first similarity matrix by updating its values over time. When it's over, we can use the trained matrix to map the points into the space. We'll initialize our t snd model via psychic learn and set the number of components to to this perimeter is asking how many dimensions do we want our end result to be in. We'll fit it on our feature vectors and store the resulting two dimensional feature vectors and the x test two d variable. Now that we have that, we can plot our points on a two d graph by first creating a legend for our class labels and plotting each point using map plot live. We'll define the location of our legend and show our graph. We can see here that point of the same class tend to cluster together and r, t s and d helped make that happen without knowing the classes of the feature vectors, we fed it.

Speaker 1:          06:14          It learned how to represent the similarity between these classes in a two dimensional space. We can further analyze this plot to study why certain classes are clustering together and what conclusions this gives us. Like these two classes are near each other since the actions are similar. So perhaps the more movement and exercise requires the farther away it will cluster from the rest. There are a couple of live demos of Tsne on the web as well, like this one that visualized as a bunch of tweets. You can see that similar sounding tweets tend to cluster together, so to break it down high dimensional data is everywhere and machine learning can help us understand it. If we were dosed the dimensionality of our data to twoD or Three d space, we can visualize it ourselves and Tsne is a popular dimensionality reduction techniques that you can use via psychic learn.

Speaker 1:          07:03          The challenge winner for last week's video is Keegan Taylor. He can use tensorflow to train a neural net to classify Pokemon by their type and the classifier had a 75% accuracy after training, which is pretty incredible one than anyone else had bad ass of the week and the runner up is Michelle Batu, very well documented and clean code. This week's coding challenges and use Tsn to visualize it. Game of Thrones Dataset that all provide and write down something you discovered once you visualized it, you're getting up submission should go in the comments and I'll announce the winner next time, please subscribe. And for now, I've got to make my new year's prediction. I mean resolution. So thanks for watching.