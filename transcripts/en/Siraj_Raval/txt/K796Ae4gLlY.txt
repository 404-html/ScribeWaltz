Speaker 1:          00:00:10       Oh, live stream is starting Sarah. Okay, there it is. Oh we're hold. It's Raj. Good to see you guys. Welcome to this live session. We're going to, we are so live right now. It's not even funny. We are so, I mean you myself. Okay, great. Okay. We got people in the room. We got a bunch of cool people in the room. We got Brandon Geo. I'd be rude. I'd invite Esteban. Yo. Hi everybody. We Day patchy the pirate. Okay, good to see everybody it pawn. Why Radu. Okay John. Hi. Ooh Korean. I wish I could read Korean someday when we have a neural lace and weak and languages. Not as much of a problem for us. Okay. Are you guys okay? We've got a lot of people in the room right now. Okay. Okay. This is tiny. Okay, so today we are going to talk about dimensionality reduction.

Speaker 1:          00:01:06       Okay. We are going to go deep on this. We are going to actually build TCA and I've talked about this in my last weekly video principle component analysis from scratch. Okay, well that's what we're going to do and it's, it's very useful. We have a lot of use cases and we're going to compare it to two other dimentionality reduction methods. We're going to compare it to t. S n e n l d a. So we are in acronym heaven right now. Okay. Um, that's what we're going to do and we are definitely going to do some act. We are doing some math. I'm cause guys, I am the most excited when we do math because math is awesome. Okay. And it's not taught well. Map is not taught well. So that's what I'm here to do. And let's start off with a five minute Q and a and then we're going to dive right into the math in an eye python notebook. We're going to build PCA from scratch to simplify our Dataset so we can visualize it. All right, so hit me with your best questions, not math. Math. Okay, here we go. Come on guys. Let's see it.

Speaker 1:          00:02:17       Thank you so dark.

Speaker 1:          00:02:22       How did you pruning on Alex net or any deep neural network? I am Newbie and no basic python pruning for the architecture or printing for the data. I will do a freestyle. Um, first of all, Alex net is great for transfer learning. Um, we're going to talk about transforming it in a later episode. Are you in the yes. You like game boy. I mean when I was 10 years old when it was, that was the thing. I want that hairstyle. Any tips? Uh, dye your hair. It's silver right in the middle. Take a picture tweeted. I'll retweet it. Uh, agree. The problem is not the math as a teacher. Exactly. If a neural net can figure out the right weights for itself. What is the purpose of running PCA? The purpose of PCA is to reduce the dimensionality of our data so we can visualize it. It reduces the, I'll talk about why in a second. Actually, what are your thoughts on our use python instead? More modular. So Raj, are you liable? Tensorflow? Summit is live right now. Yes. First of all, they didn't even enabled chat guys. I would be there if they enable chat. They didn't do that, so I'm going to watch it later. They're just spewing facts without there being any community aspects. Love tensorflow. Love you guys. But you guys got an enabled chat. Okay?

Speaker 2:          00:03:28       Okay.

Speaker 1:          00:03:28       So how can I reduce complexity in imperfect information games?

Speaker 2:          00:03:33       Okay.

Speaker 1:          00:03:33       Imperfect information game. Reduce complexity in imperfect information game.

Speaker 2:          00:03:39       Uh,

Speaker 1:          00:03:42       please clarify. What do you mean by that? How do you think about generative models? How do, how do I think about generative models? Um, how do I think about it? I, I think about it as a distribution. It's a distribution of possibilities. It's a curve of possibilities and we give it data and then we kind of generate, we generate new data based on that distribution and possibilities. So it's all of the old, old, what is generated is related to the data that already exists. It's not something completely off the off the rails. How can ml change a common, a common thing in my life? How can ml change with common thing in my life? Okay guys, we are the most important people in the world. Okay? We are the most important community in the world. The people in this community are going to go on to build the most valuable startups to make the biggest contributions to ml.

Speaker 1:          00:04:34       If software is eating the world. As Marc Andreessen of Andreessen Horowitz says an ml is eating software that people building ml are the most important people in the world. Even if you don't know that much about ml, even if you barely know anything about ml, the fact that you are in this livestream stream, the fact that you were even watching this video, that you have the awareness to know how important it says that you have the desire to learn, it puts you in the top bracket of people. Okay, we are, we're going to be the biggest machine when he community in the world. Okay. And we are just getting started. So how can it, it's a common thing in your life. You have to start learning the basics and then inspiration will come. Once you understand the tools that are, that are at your disposal, you will find something that you are passionate about and I refuse to accept that you don't find anything that you're passionate about because you will. Okay, I reject that notion.

Speaker 2:          00:05:28       Yeah,

Speaker 1:          00:05:28       I found you today. Okay. Can de-boning be used in time series forecasting type of application? Like in real time tool breakdown in drilling oil. Yeah, deep learning can be used for time series. An LSTM recurrent neural network. We'll talk about that later. Where are you? I am in San Francisco. What are challenges? Deep learning basis

Speaker 1:          00:05:49       facing pre presently. Um, we need to, okay, so the major problems are learning with unlabeled data. That's the biggest right now. How do we learn an uncertain on unsupervised? How do we perform unsupervised learning without labels and have it be a useful our predictions and, okay, so that's what that is. But five minutes are over. Thanks on top of 1% per for saying that I'm going to do a freestyle and I'm going to use music. So someone shout out and topic or one minute freestyle and then we're gonna get started. Here we go. Someone shout out of topic. Just one word.

Speaker 1:          00:06:21       Yo, shout out on topics. One topic and then we're going to go, I'm waiting for a topic. First ones to say a word first. First one. I see planning, Yo go on to other planets. I you to understand it. See when I go there, it's like I gotta be something else. I go with the state. It's like my on weight, it, all these other things. I tried to close, I tried to float my stage. Patient goes out of control. Yo. When I go past the solar system in the moon, I told Jack like I'm flying, I'm a little zoom, zoom, zoom. I go out. Is the space earth from a machine learning space? That's what I love to do. I do it every day. I do with my weight. I do eight. Oh Hey. Not really. I do a great, it's like I'm from Chile. Not really. I'm from San Francisco. That's not a place where I stay at Yo cause I don't want every day. And I do it when our views tend to flow. Okay. So that was it. My one minute freestyle. Okay. And I use music this time. So let's get, let's get on with this. All right, let's get on with this. Um, I'm going to start sharing my screen and we're going to build the, we're going to do bill PCA. Okay. So here we go. Here we go. Let's get started with this.

Speaker 3:          00:07:41       Okay.

Speaker 1:          00:07:42       Screenshare and guys, I want you to code along with me. So open up an I python notebook and start a coding along with me. Okay, so here we go.

Speaker 3:          00:07:55       Yeah.

Speaker 1:          00:07:57       Okay, here we go. Here we go. So what are we going to do today, guys? Let me, let me, let me also have myself up. I'm going to put myself up in a little quick time window like I like I do. You know how I guys, how I do right guys? Okay,

Speaker 1:          00:08:22       sure. I am in the little corner of the screen and we're going to get started with our iPods on notebook. Okay, so let me make myself a little smaller so I fit in there. Okay, so here we go. Let's do this. Let's do this. So what are we going to do it? We're going to build TCA. Okay. And why do we want to build PCA? Oh, so why do we use dimension that why you dimensionality reduction? What is the reason for that? What is the purpose? There are three reasons. Reason. One reason why, okay, I'll give you a minute to initialize. I'm just writing up some comments. So go ahead. Go ahead and initialize your notebook. We're going to do from scratch, we're going to use num Pi and later we're going to use map plot line to plot our data. Okay, so reason one for doing dimensionality reduction is space efficiency.

Speaker 1:          00:09:05       We want space efficiency, right? Lina is huge right there. We could have 500 gigabytes of data, but if we find those features that are the most important, we could reduce the, the, the, the amount of space our data takes. And cause not everyone has terabytes and terabytes of space. So that's reason one. The other reason, reason too is computing efficiency. Okay? Let's data there is the faster our model will run, right? Make sense, right? The last we have, uh, the less data we have, the more the faster model run. And reason three, and my favorite reason, my favorite reason is visualization. We cannot visualize hundred dimensional data. When in dimensions are the same as features. They are synonyms. Okay? So remember that dimensions are features. So if we have a hundred dimensional data set, we can reduce the dimensionality to two d or three d and then we can view it.

Speaker 1:          00:09:59       And why do we want to view it so we can analyze by human eye and it just looks pretty as well. Okay. We can analyze it by human eyes. There's a lot of insights we can make if we can just look at the data. So today we're going to build PCA. Okay. We're going to build PCA. We will build PCA and then compare it to DSME and Lva and I'll talk about these methods. So, so three dimensionally reduction methods total. Okay. And so that's what we're going to do. I'm going to talk about the math behind this and we're going to talk about the math behind this. Okay? Okay. So here we go. We're going to start off with this. Okay. So let's, let's start off with, let me make the first, uh,

Speaker 3:          00:10:43       okay.

Speaker 1:          00:10:44       The first important. So we're going to build PCA. Okay. So here we go. The first thing we'll do is important. I'm crying. We always want to important on five or any kind of map that we wanted to. Then we're going to introduce a c. Can anyone tell me why we introduced the seat? Three seconds to think about it. Okay. Because it's good for debugging, for debugging. It's, it makes, it makes cause we're gonna, we're gonna randomly generate data and, and we want to start from the same seed, that same starting point. So whenever we tested the randomly generated numbers are always the same. Okay? So that's just for debugging. So it's just, it's just good practice. Not Necessary, but it's good practice. So now let's go ahead and,

Speaker 1:          00:11:29       uh, create two classes. We're going to create two classes and now we're going to put on our map had guys, and this is where I'm so excited because we're going to put on our map tap. So step one is to create our data datasets. We're going to create this space that, okay. And to do this, we're going to, we're going to, okay. Okay. We're going to initialize one variable first and I'm going to call it move back one, which stands for a sample mean. Okay. So, the, so I'll talk about why I'm creating a meat. So, so I'm going to use num Pi to do this. I'm going to say here's a zero uh, and so this is the sample mean of the data. Why do we want the mean? The mean is the average of the day. That's what it's going to represent. That's what it's going to represent.

Speaker 1:          00:12:14       So it's going to be just a matrix of Zeros. And then I'm going to, I'm going to create a covariance, a sample, covariance, and I'm going to talk about what these two things are and et cetera. Okay? So let me just write this out. And uh, so the sample covariance is going to be one zero zero and then, uh, let's see, zero, one zero. So, and then what's the last one we want to fill in that last blank spot just so we have two zero zero one okay, so this is the sample. Covariants why did I initialize these things? What's the point of that? There is a very important one point because we were going to use both of these values, both the mean and the covariants to generate our first data and we're going to call it class one sample. How do we generate this dataset? Well, we're going to use one of num Pi's functions called random dot. Multi-Variate underscored normal. Uh, and I'm going to talk about what this does in a second, but to see these are the parameters we use to generate that data. So let's talk about what, what the hell is actually happening here. And I'm gonna use a transfer to that and let me print it out and I'm going to talk about it. Here we go. So when you print it out and made sure it worked okay?

Speaker 3:          00:13:24       Yeah,

Speaker 1:          00:13:26       I is not defined. Wait a second. What line is that? That is on move back one him that array in court, num, py, SNMP name NP is not defined to important. Hold on a second.

Speaker 3:          00:13:52       Okay.

Speaker 1:          00:13:54       Two non keyword arguments are accepted for this. Okay. Let's see. Let's see what's happening here. Let's see. Let's see. None pilot array, covariance matrix and pot. I got a red one. One. Oh, okay. Okay. I see. So there has to be another one of these. Okay, so good. Okay, so now we generate, so what the hell just happened here? Right? So let's talk about what this is. Let's talk about exactly what I just did it cause this is linear Algebra. Okay. This is linear Algebra. So I generated this multivariate normal. What is a multivariate normal distribution? It's the same thing as a Garcia and distribution. So let me, um, so this is a Gaussian distribution. This is, this is what are generated data. It looks like in three d space there are a set of points. It's a three dimensional matrix of values. Okay? So there aren't, so what this is is it's, it's a three by 20 matrix.

Speaker 1:          00:14:50       Okay? So let me write that down. It says eight, three times 20 majors. There are three columns with, with 20 rows each. Okay? So there's 20 rows I put the is and hey Godsey and distribution is used to create a distribution of possibilities and we use the mean to define a center for it. What does that central point, right in the middle, that little red center points. And we use the covariants as a measure of how much of width of standard deviation, how far off from the mean do we want to, to stretch this data, this distribution of possibilities. Okay, how far do we want to stretch that? And by the way, the word covariants, and we're going to use it a lot, covariance is a measure of how changes in one variable are associated with changes in a second variable. The covariants, how much does one variable very in relation to another variable. And why do we even use this, this value? Because we want to, we want to measure how these two variables are related and the variables are features. What does that, what does that connecting value between these variables? And we're going to use this to, to create our lower dimensional data. And I'll talk about why in a second. Okay. But that's what that is. And

Speaker 3:          00:16:10       okay.

Speaker 1:          00:16:11       Uh, so that's a, it's a gospel and distribution curve of possibilities. So that's for our data. Okay. So, okay.

Speaker 3:          00:16:21       Yeah.

Speaker 1:          00:16:21       Oh, so it doesn't make much sense. So, okay, so, so geo we aren't, we are just Jen, we are generating sample data and we are general, we're just generating sample data and it's a bunch of, so this is the data as we as we printed out and it's just across a distribution of possibilities. So if we were to pick any point off of this, they would be like 0.6 5.3 right? So it's a three dimensional point. It's a triplet of numbers and it's just a set of those. So it kind of looks like this, but that's all it is. Just it's a table. The word Matrix is scary, but a matrix is just a table of values. That's all that a matrix is. Okay. What is the difference between a three dimensional matrix and tenser?

Speaker 3:          00:17:00       MMM,

Speaker 1:          00:17:02       a tensor isn't an end dimensional array. So a three dimensional matrix is a tensor. So attention is the most generalized form of, you know, so a four dimensional majors to be attentional five dimensional matrix. Okay, so let's keep on going. So next, that was it for our first sample. And guess what? I'm going to do this twice. I'm going to do this twice so I can basically just copy and paste this because I'm going to do this twice because I'm gonna create two classes. I'm going to quit Q classes and so I'm going to paste this twice. So I'll say two. Two what? So this is going to be one, one, one. So there are two different, so there are two data sets and the moon is going to be different for this one. Okay, so the starting point and then I'm going to say class two, sample renovar two and to, and why do I went through datasets? Well,

Speaker 3:          00:17:53       okay.

Speaker 1:          00:17:53       No pluck them. So it going to, let's see, let's see. Good. Okay, so I've got two data sets now, right? Okay. Class one and class two. These are my two datasets and now we're going to plot them.

Speaker 1:          00:18:06       So step two is to plot the data so we can look at what this looks like. Okay? And it's going to be much more intuitive when we plot plot the data. So let's start by sliding the data. All right, so we're going to import Matt thought lives so we can plug this data and we're going to call it PLT. And then we're going to define our variables. So figure with, so the finger, which is going to be, and then we're going to use the finger side. There's going to give us a width and the height. Okay, go figure. It's going to be what is the width and the height. So figured size is eight. Eight.

Speaker 1:          00:18:43       Okay. So then we're going to create three d subplot and these, so that's the finger. Okay. So then we're going to say these are subplots grid parameters in coding as a single integer. Okay? So what am I talking about here? So once we have our finger to create a subplot and the figure is kind of a generalized form that we draw our data. And so hold on a second. It's going to be really pretty in a second. I'm going to print this out. It's gonna be awesome. So I'm going to say let's add this subplot and then some point has been to start up. That's what I meant by that word. What is one, one, one. These are the subplot Rick Rick parameters encoded as a single integer. So what this is saying is one, one, one it says, so it's a one by one grid and we'll talk about the first sub plot.

Speaker 1:          00:19:28       So it could have been anything else that it's just defining the rules for our, our, our, our plot. What, what do we want our plot to look like? Okay, so it's going to be a three d plot and we're going to say, and you know what, uh, let's see. Yeah, it looks, let's keep going here. So then the font size is going to be first to find the font size, the font size. These are such little trivial things, but they're necessarily right. We want to make sure that aren't data looks good, right? Whether we're presenting it to people, whether we are looking at it for ourselves, we want our data to look good and we're going to plot their samples. Now, now that we've defined the rules for our graph, now it's time to actually plot arc data. So we'll say, okay, so let's apply it using the plot function. And we're going to plot the first sample. And

Speaker 1:          00:20:23       you know what, this, you know, I can just paste. This is pardon? Because it's not as important as the machine learning that we're about to do. So let me just, let me just, I took that advice before some of them. So, so here's what we're doing. We're plotting both samples, class one and class two, okay. And we're going to label them. One is going to be blue and one is going to be green. And there one is gonna be a little class one and one is going to be able to class two. We're going to have a legend and let's show plot. Let's see. Hope this works. Boom. Okay, so let's see what happened here. It said, can't assign too literal. So what that means is PLT, that RC parameter equals legend on font size. Can't assign to literal. Okay. Legend, dark font size. Let's see what's going on here and not what lie highlight in line. Uh, let's see. We'll just be in line by the way. So Act, start, figure, add subplot, projection, 3-d. So you're size eight, eight PLT figure, RC programs. What do we got going on here? Marcy? Parameter for Plt. Can't assign to literal. Interesting. So what's,

Speaker 1:          00:21:41       Oh yes, equal sign. Yeah, that's what it was. Okay. Populate it here from Num Py. Inmar plot lied. Okay, let's see what's going on. Anon. Projection three d unknown projection three d why do we have an unknown projection of three d? Let's see, even unknown projection of three days because see what we've got here. I will look, I will look, I will look. I am looking now. RC parameter equals I do. Look, I am looking. Okay. Uh, so let's see. Bigger size and then also Rosie, arrogant, unknown projection for Ed. So you're a dot pads. Sell time. Let's see what's going on here so far.

Speaker 3:          00:22:42       Hmm.

Speaker 1:          00:22:44       All right, let's just go for this. UHMM I know. Projection Three d let's see what's going on. But I didn't do maps all like three d in my earlier code. So it's interesting. I mean, what I would do at this point is probably Google it and stack overflow, but I'm going to see what is going on. Let's see, we have,

Speaker 3:          00:23:14       mmm.

Speaker 1:          00:23:17       Capital d, capital d for y. So, oh, is that what it is? Is that what it is? No, it's not. No, it's not. Capital d, lowercase d. Uh, so, so, okay, so this is actually what, there's a lot of noise. No, that's cool. No, it's, it's fine. It's fine. It's fine. So it's just gonna it's a, it's an unknown projection three. Let me just show you this. I'm going to keep going. We don't actually, um, so, okay. Let me just show you what this looks like. Let me show you what this looks like. Here's what it looks like. Check this out. Check this out. When we do this, this is the projection in three d. This is what it looks like. We have two classes in three d. This is what it looks like in three d space. Okay. Our data is valid. It's just the, it's just the tools and,

Speaker 3:          00:24:13       okay.

Speaker 1:          00:24:14       That's what it looks like. It's in three d space. So we're going to get started with the math because that is the important part. This is what it looks like in three d and now we're going to actually take our data and we're going to look at it. All right, so let's, let's keep going here. Okay, so we have our data at the data is valid. It's fine. Okay, let's, let's keep going. Let's keep going. So now what? So that was step two to now we're at step three. So step three is we're going to merge this data into one big data. So would you merge the data into one big ass? They said, I had to say, asked him that. Okay, so that's what we're going to do. So how we do it, we're going to say all samples equals, and we're going to use numb pies, concatenate function to do this and make function. To do this, we're going to say, take our first class sample, take our second class sample, and on one axis we're going to merge it. So it's going to be accessed before is once it's all on one access. What does that mean? One access? Well then let me, let me write this out and then we'll show, we'll show, we'll show it. It looks like we'll show at one x. This looks like this is now what our data looks like. It is now a three by 40 dataset. It's a three by 40 data set and

Speaker 1:          00:25:24       the Saroj reval company is not taking investments right now. We are focused on our media division and I'm also focusing on, I'm going to start building our research division, which includes you guys later on, but right now it is just media. We will be the best media world. Okay. Back to this. So all samples, that's what it looks like three by 40 days said. Okay, so that's our data. So let's get an also, this is what the transpose looks like by the way, because we're going to be using this t. Dot. T at PN. We are going to be using that a lot. What does that do? It gives us the transpose and what does that do? It just flips our data. It just, it flips our matrix. Okay. So inserting horizontal, it'll be vertical. And that's just good for looking at them. It's good for it. Sometimes formulas require the transpose, which they will, but right now we're just doing it just to have it look aligned so we can without having to struck our window. So now that was step. What was that? That was step three. Now we're on step four. Step four,

Speaker 1:          00:26:21       the research division. Okay, so step four is to compute the dimensional mean vector. What is that? Why are we

Speaker 1:          00:26:31       here? Why don't you compute? Why are we competing with me? Becker? It will help compute the covariance matrix. So a lot of this process of reduction for PCA and in general is to compute these little pieces. They're all little pieces of a puzzle and we use each piece to help us find the answers to the next piece. It's a, it's a, it's a process of discovery. It's an incredible process of discovery. Each of these are pieces to the puzzle and eventually it's going to lead us to something that is amazing. So let's find the mean for each feature. Now it's right. So by itself it doesn't seem like it's that important, right? But when we use, we're going to use it to compute our co variants matrix. So the mean for x. So we have three features, right? We're going to use numb highs knee function to do this, we're going to say it's getting all of our examples and just that first row. And that's going to Joe Joe dimensionality reduction is taking a lot of data with a lot of features and squashing it. They're just to, so instead of a hundred features we have just to, and that's great for uh, a lot of things. So we're going to keep going with this all the same. So I can just copy and paste this and just replace we numbers, right? So many for x me for why we perceive we're taking a three dimensional data set and we are squashing it to a two dimensional datasets. The covariants

Speaker 1:          00:27:57       is the measure of your head. I got to keep writing this. So the one, how two variables vary in relation to each other, right? And so now we're going to, so now we have the meat. What does the mean, the mean average value from all of those values. So what is the average, if we add all the values together and divide by the number of values and it gives us the average value. So we want the average value or mean for each of those features. And I'm not picking the, uh, I'm not picking which features I want men, she, so it depends on the method but for, uh, so for principal component analysis, it picks those features that it finds to be the most relevant. And we're going to talk about that. I'm going to visually show what I'm talking about in a second. It's going to make a lot more sense. Stay with me guys. Okay. Stay with me. We are computing the mean right now. So now we're going to take this three d mean vectors. So we're going to create one big vector out of it. So it's called the mean. That is one array. And I'm going to print it so we can see what it looks like. I'm going to print one array and

Speaker 2:          00:29:05       okay,

Speaker 1:          00:29:06       mean of y and then Z. Okay. So those are our three. We create one vector out of this and then we can just print it. So print me vector. Let's see what it looks like.

Speaker 2:          00:29:16       Okay,

Speaker 1:          00:29:19       so you're in love with the sequence. Okay. So this is a

Speaker 2:          00:29:22       Oh, right, there we go. Hold on a second.

Speaker 1:          00:29:28       NPRA mean hacks, right? Close that, close that bracket. It's in texts online. Eight. What, what is it? Oh, this one. Okay. Remove that. Yep. This our mean Bector see these three values are, what do I mean by squashing it? Okay, so, okay. So

Speaker 3:          00:29:50       yeah.

Speaker 1:          00:29:50       MMM, okay.

Speaker 3:          00:29:52       Okay.

Speaker 1:          00:29:53       So if we had a hundred features, if we had like if we had a Dataset of Pokemon, okay, let's say we had a Pokemon data sets and there are a hundred features, the length of it, the fire breathing capability of it, the blink of its tail, who it loves, its gender, all of these features. By squashing it, I mean we give this huge hundred dimensional data set of all the attributes of a pub. Tomorrow we give it to you, this algorithm like Pca. And what is going to do is it's going to squash it. And what that means is it's going to take those hundred features and find the two most relevant features from that data. But it's not the same features that exists. There are two new features. There are two new features that they don't, they don't, they're not cool. They're not like a word. They're not like, they're not like, oh, these are the two features, tail and head size. They're entirely new features.

Speaker 1:          00:30:44       They are entirely new features that they just represent. And I don't talk about this in a second. So let's keep going with this. So now we've computed the mean and now we're going to compute the covariance matrix using this meter. So let's talk about the covariance matrix with a little, uh, with a, with a, with the image. Okay, so I have this interactive website that I found I'm going to, I'm going to link to you guys so you guys can see this. Okay. This is the one right now. This is not the one. What is the one, let's see, what does that, what does that cove or making it covariance matrix in our, this is a great link for it. Okay. So let's see.

Speaker 3:          00:31:29       Okay,

Speaker 1:          00:31:30       so this is also a great link. Let me, let me paste this in as little learning a, so check out that link. Right. Okay. So what are we doing right now? Let, let me,

Speaker 1:          00:31:39       it's back to this. We're computing the covariance matrix. You have a covariance matrix models their relationship between our variables and in our case, those are features. It is the matrix of values that model it model the relationship between all the different features and it's all gonna come together. What? Remember, these are pieces to the puzzle. It's all gonna come together. It measures the, the relationship between all of these variables, the word variants. So a variance it the degree by which a random variable changes with respect to, it's expected that that's variance. Variance is a word for describing a value in relation to itself with nothing else in common, just itself. But covariants is the degree. Oh, the Saudis ones. So covariates it the degree by which two different random variables change with respect to each other. It measured the relationship between each features. That's Coburn there. How are they both during? So let me, let me write this out now. Oh, so, oh, it didn't work. Let me, uh, okay.

Speaker 3:          00:32:42       Okay.

Speaker 1:          00:32:43       Yeah, let me just, there we go. Yep. So let's, let's try this again. We're not live.

Speaker 3:          00:32:51       Okay.

Speaker 1:          00:32:52       No worries. No worries. Antiglobulin yeah, eventually. Not yet. I'm a popular night enough but works on great. Great. Awesome. Okay, so let's, let's do it. So there's a calculate the covariance matrix. So let's calculate the covariance matrix. So we're gonna start off with an empty array of Zeros and herei three by three array of Zeros. And we're going to use high zero song sheets it to do that. So we have an MPRM Zeros. Now we're going to say, okay, so for each value in our grade, I in range of, all right, great. Um, all samples. Dot shake.

Speaker 1:          00:33:32       We're going to say take back covariance matrix and now we're going to add our values driven. What are these values look like? So according to that link I sent you guys even a covariance matrix, it's not just the covariance, it's the variance and the coherence. That's what it looks like. There is a specific formula as you can see here for creating that nature. And we are going to programmatically write down this formula. So it is the same set of rules. That formula is a set of rules that is always the same. So we're going to say we're going to keep continuously add value through our covariance matrix and we're going to say ORC, all the samples starting with the ones in this row. And we're going to iterate through the rough start and this column we're going to reshape and reshape is a new shape twin, right? Without changing its data. So it just gives us a new shapes are all in the same shape minus the mean back then and now are you using the mean bastard? So we take those features and we subtract the mean vector and then we compute a dot product of it. And then one more value. And so the dot product is

Speaker 1:          00:34:38       the don product is the one we take two major seas and we multiply them together. And why do we multiply matrices together so we can too. So it gives us a more generalized value. And is that there's actually a lot of reasons why we would most part two matrices together. Uh, but in this case we're doing it to give us a covariance makers. I mean Becker is going to help us compute that. So let me just out all samples. Oh, okay. Hold on. I'm sorry.

Speaker 3:          00:35:11       Yeah,

Speaker 1:          00:35:12       paste this so then I can talk about it. I mean, great. Okay. This is our covariance matrix. It is a three by three matrix. Okay. We use our main vector to help us calculate this and we computer the dot product between every feature minus the mean teacher minus the mean vector with every other feature minus the mean vectors. Trans. Why do we do the transposed? That is a part of the equation. As you can see here, the transpose with mentioned seed. This tea, it is the same set of rules every time. Okay, now here is my favorite part. Now we're getting to my favorite part. Why do we compute the covariance matrix? This is the, now the pieces of the puzzle are going to start coming together. Now it's going to start making more sense why we computed the mean backyard to compute the covariance Matrix, to Compute d eigen factors in eigen values.

Speaker 1:          00:36:07       That's what we're going to do. Now. We're going to compute the eigen vectors and eigen values. Okay, so that's next six. Step six is to compute the eigen vectors and hiking dot eigen values. Why do we do this? What is the point of, I can backtrack and Heidi di and what, what even is that word? Well, first of all, let me just say I can vectors in [inaudible] values are used throughout all of engineering. It's not about computer science is used in electrical engineering. It's using physics. It's used in the Google page rank system. I come back to as an eigenvalues or use the route engineer and no, so here is what the system, let me, let me give you a quick summary of the series of what this is. But so this is actually a great interactive link. So definitely click on this link. I'm about to paste it into the chat. And it's also going be in the comments when people who are not watching this live, check out this link. Okay.

Speaker 1:          00:37:02       So I can vectors make understanding transformations easy. Okay. They are the acts axes, the directions along which a transformation acts by stretching or complex or or compressing. And I do values give you the factors by which this compression occurs. Okay. So yeah, so basically there's a lot of problems that can be modeled with a linear transformation, but it gives you a sense of direction. So, so that's a, that's a general description. So I think vectors give you a sense of direction as to where this data is going. So let me, let me show you this so you can actually move this around. This is interactive. I can vector is Jeremy. Exactly. So see as I move this, these sets of points. Uh, so this is a, this is, uh, this is a very intuitive explanation on this page, but essentially an eigen vector is a direction that some data is going in. And what do I mean by direction? Direction in this case is like, so there's actually a, there's actually another link that, that that also helped me understand this. Then I want to paste in here. Uh, so hold on a second.

Speaker 3:          00:38:26       Yeah.

Speaker 1:          00:38:27       So um, hold on a second. So principal, I think this is

Speaker 3:          00:38:36       no, there's another one. So basically,

Speaker 1:          00:38:39       or I think this guy had it. Okay. So this is a great explanation. So, so check at this point. Let me show you this. This is an amazing, amazing, like it's an amazing room.

Speaker 3:          00:38:53       Okay.

Speaker 1:          00:38:55       Check out these points. Okay. These are bunch of different data points there, a bunch of different data points and we could just, we want to find the direction of this. We want to find the principle components of this data. What is the principal component? What do I mean by that? Well, if we just draw a random line in this data, it's going to all the how far, what is the variance, how far is this data from this line? And we can draw a red line between every data point and this line. And so that, that's cool. But if we draw a horizontal line, it shows them the, the data is even more spread out. And what, why is that good? That that means that this is the principal component of this data. This is the essence of this data. This is that point where if we were just given this line, if we were just given this line and some and some distribution, we could recreate the data points. That's what I'm trying to say. We can then recreate the data points from this lower dimensional.

Speaker 3:          00:39:52       Okay

Speaker 1:          00:39:52       idea this line and this line in this case, because the principal component and the direction for where should this line go, where, what is she going to be horizontal it should it be vertical. That's what the Eigen vector gives us. It gives us a sense of direction as to what does that point with the most Darien's between data points. And we want the most variance because that's what lets us create these new lower dimensional features from this date so that we can recreate the data with these lower dimensional features. Okay.

Speaker 3:          00:40:22       Okay.

Speaker 1:          00:40:22       So that's what that is. So where were we? So let's, let's compute eigenvectors and eigenvalues. All right. Values and I think I in your doctors, and we're going to use numb pies, linear Algebra functions. I in sell poaching to compute them. Very useful function to have. Okay? So,

Speaker 3:          00:40:54       okay.

Speaker 1:          00:40:54       Worth every value we have. Let's print out all of those.

Speaker 3:          00:41:01       Okay.

Speaker 1:          00:41:01       Um, so that's actually gonna get it. Then we can just print them out. Now we can just print it out to print. I can value and print. I can backwards. So let's just see what this looks like. Okay.

Speaker 3:          00:41:12       Okay.

Speaker 1:          00:41:12       Oh, okay. And so let me print. But so there's a space between them. Don't say what this is.

Speaker 1:          00:41:27       These are our eigen values. An item. So I can vectors give us the direction and I can dial you skip. You give us the magnitude of that direction. These two, uh, values are intrinsically correlated. We need both of them. Okay. And so we need both of them. So what are we going to do to, to combine them together? What we are going to, uh, we're going to create. Um, an I didn't care. I didn't care from both and sort that. So when I compare it is if we were to combine both of these items. Okay. What do you mean by the direction of what did I mean by the direction of the data? So,

Speaker 3:          00:42:10       okay,

Speaker 1:          00:42:11       so like,

Speaker 3:          00:42:15       okay,

Speaker 1:          00:42:16       like what does that point where in the lowest dimensional space we could then recreate all of those dimensions. It has to be this, what is the optimal, what is the optimal line? We could draw what is the optimal set of features we could have such that we can given some formula from these features, we can then recreate all of the data with the most accuracy, the direction of where we should go. Negative positive of should it, you know.

Speaker 3:          00:42:42       Okay.

Speaker 1:          00:42:43       What should it say to look like? That is what the principal components, that was the eigen vectors and I can dial, use, give us and that, that, that lowest Mitchell point is the principal component. And we're gonna we're gonna calculate those in a second. So let's, let's, let's create our ids. Harris. Yeah, I can Paris or I can, parents are going to be refused. Numb, Pi's have suit we're can to use the absolute value of that. And

Speaker 1:          00:43:16       the, actually let me just, let me just pick this one in this one. Can you paste it? So we have to, I didn't carers. This should be eigen values. Let's talk about what's happening here. Let's see. So we get, we made a list. I can tell you I can vector two bulls, two bulls, right? Cause there are three dimensions and we use the absolute value function to make them positive. Why do we make them positive? Because it doesn't matter if they're negative or positive. It's just making them positive. Makes it easier because we don't have to deal with any kind of uh

Speaker 3:          00:43:54       okay. It

Speaker 1:          00:43:55       did. It's the same as like the idea behind the mean squared error. The reason we square it because it doesn't, it's just a magnitude that matters. It's not if it's positive or negative, it's just the magnitude that matters. Okay. So then we sort them with the sort function and then we reverse it so that it's going to be in descending, in decreasing, in descending order. And these are the values of the parents. If we were to combine both of those values together, this is what it gives us, these three values or the most important values we are looking for. I'm going to post this code. Yeah, let me, me, let me paste this code on pacing and yeah, totally. That's a great idea. Let me, let me do that. Um, I don't get home, um, and I haven't actually comment on it yet, but I'm going to, so let me just paste that in and then I'm going to, so here's, here's what's happening guys. Okay. So let me paste this code in and then we're going to keep going. Check this out. Boom. All right, so that's my messy code. I haven't written, read me. I will in a second. Okay, so those are our ID in Paris. So now we've done that. So that was that, that was step seven actually. No, no, that was step seven. Step seven was too, sorry about that. Okay, that was step seven. And now we're going to do step eight now or would you choose the largest one? So step eight is to,

Speaker 1:          00:45:22       what would you stack? We're going to choose the K id vectors we want. What do I mean by kit backwards? So what is the dimensionality of uh,

Speaker 3:          00:45:35       yeah,

Speaker 1:          00:45:37       what needs a dimensionality that we want here? Because we're going to create a matrix and we have one more step after that. So there was the idea of one more step after this. Okay? So we're going to create a matrix. Uh, and

Speaker 3:          00:45:51       yeah,

Speaker 1:          00:45:52       we're going to create a matrix and we're going to say for our, I can pears zero one

Speaker 3:          00:46:05       we shape them, boom,

Speaker 1:          00:46:12       we're going to stack on a res horizontally. That's what we're doing, right? How much stacking or a race in sequence horizontally, that's what appreciate it every day. Uh, we're going to stack them horizontally and how are we going and why are we doing that? Where are you using, how are we doing that? We're using the h stack function, okay? Either way, guys, all of this can be implemented in a single line of code. In a single line of code would say get the loans. But we aren't doing this from scratch with the Mac because we are awesome. And we just want to know how this works. Okay. It does. It never hurts to know at least a general idea of how this works. Okay. Because we are awesome. So now, um, we're going to talk about reshape pretty one. Okay. Print Matrix. Stop you. I bet you there, it's going to be an error in a second, but that's okay because I in pairs I can down juice. Let me just,

Speaker 3:          00:47:21       is that him? Okay.

Speaker 1:          00:47:26       So this is now h. This is now a two dimensional matrix. It's a two dimensional matrix with three values age. We created this using our, our eigen vector eigen values two poles. This can absolutely be a data science interview problem. Yes, yes, it can be. Um, so now we're on our last step. We're on our last step. So step nine is to transform our data is to transform our data using this,

Speaker 2:          00:47:58       uh,

Speaker 1:          00:47:59       I can pair matrix using this eigen major who are going to transform our original data set. So now the pieces of the puzzle are coming together a bit. The piece of the enemy. To clarify, Mikael, it's not that they're going to ask you a code. The Saul from scratch, they'll probably ask you to on the high level idea, like the mind is known to just lightning like lightning interview, like a hundred questions about machine learning. Just off the Bat, what is PCA? Like, what is this? What is this? Okay, so we going to create for my data using this. So we're going to say take our transformed because transformed data and calculate the transpose of it times a doc. And then we do the fence post. So it's to be the same shape. And so remember all samples data, that was our original dataset. We are computing the dot product between our original Dataset and this new eigenvector. I can pair a matrix, am my computing the dot products. It's going to give us a two dimensional,

Speaker 2:          00:49:04       uh,

Speaker 1:          00:49:05       two features in set up three features because then any of us two features instead of three features. And we can use these two features to plot the data. We're going to plot it in a second. Okay.

Speaker 2:          00:49:17       Okay.

Speaker 1:          00:49:17       So and then we'll print out the transformed. That's it. It's just one line of code. It was just one line of code, one line of code. This is our new data sample. We just performed principal component analysis. We now have two dimensional data instead of three dimensional data. Now instead of writing out all this code unnecessary, go just to print it out. I'm just going to paste it in. So this is what it looks like now. This is our, now this is our two dimensional data instead of our three dimensional data for our classes. Okay, it is now two dimensions. Instead of three, we just performed principle component analysis by hand. Now instead of writing out the now instead of writing up tsne and

Speaker 1:          00:50:12       Lda from scratch, let's just talk about them and compare them because that's the really important bit and then writing up the math for those two methods. Going to take quite a while. So let's talk about a comparison. Okay, so we're going to compare PCA versus he has a knee versus Lva. These are the three most popular dimensionality reduction techniques. These are three most popular PR techniques out there. Okay, let's compare them. Pca. Done. Okay. So what are the pros? So why do we use PCA? Pca? Oh, so basically I can sum it to this. So for the best visualizations, this is, this is just something to remember. If you were to take something out of this session. No, that, so the best visualizations, uh, I'm sorry, let me start off with this. The best generic dimensionality reduction method is drum roll, please. Pca. There's a lot of reasons for this. There's a lot of reasons for this book, but that's Generic dimensionality reduction method is PCA. But if we have supervised data, or sorry, if we have a supervisor and that that is for generally for supervised only for unsupervised data, which is most of our data, but the best method for specifically for supervised data that is data that has labels is drum roll. Lda, no, what is Lda Lva? Same for was Lda. That'll be his,

Speaker 1:          00:51:40       when you're a discriminant analysis, linear discriminants analysis. Now if another apartment, all that. But it's the same steps as PCA. So if the same as same steps as PCA, except for one difference, uh, we compute the mean vectors for the different classes. Instead of take the whole dataset. Remember how we got a mean vector for the whole dataset inside of doing that will compute mean directors for each of the classes will compute mean bachelor's for each of the classes. It's the same except we compute mean vectors for each of the classes.

Speaker 3:          00:52:16       Okay.

Speaker 1:          00:52:16       We confuse mean vectors for each of the classes that Lva what is the best for visualization? Is he s visualize a dataset easily. But essentially there are three steps that tsne. Okay. And there's also a great link and I'm going to send you guys in a second. So worried about this. Um, okay. Uh, he had money and it is an O'Reilly page. This is the best explanation for Tsne that I found on the web beside me. So check this out. Okay. So

Speaker 1:          00:52:56       we compute a similarity matrix between all the feature vectors. And then we compute a similar matrix for the map points, which are the projected points where we want to be, who want, we have a hundred dimensional data and we want two dimensional data that are, those are our set of nat points. And basically we use grading dissent to minimize the distance between two major cities. And that resulting matrix is our low dimensional data. That is it at the highest level possible. Now we need for a data visualization, generally in general, it gives us more accurate data visualization than any other method. But in general, remember those three reasons I gave at the beginning? Me, let me, let me, uh, stop screen sharing and go back to me. But in general, we want to use PCA. That is our most, that is our most, uh, Jen generic dimensionality reduction techniques. Okay, so that was it. Let's, uh, let's do a ending a by many Q and a and then we're, we're out of here. Hi Guys. And by the way, thanks for watching. You guys are awesome. And also, um, I assume APP uses k nearest neighbor. Hi. It does? Yes. I assume APIs and other methods. There's actually so many dimensionality reduction methods. There are so many out there, but what I've done is I picked the three that I think are the most important and the most opulent. Okay. I joked to morning,

Speaker 3:          00:54:22       okay,

Speaker 1:          00:54:22       please increase the frequency of live sessions per week.

Speaker 2:          00:54:25       MMM.

Speaker 1:          00:54:27       I'm going to increase my video output in general. Um, that's what I'm focused on right now guys. You're are you Udacity? Nanodegree is too costly, Bro. Yo, I tried to get it for free, but look, grading is not cheap and, and they have the old team. The, these people are awesome. They have to pay themselves. They've got to feed themselves, you know, so not all of them use, you know, what are like the crooning on Alex Net architecture. It's actually, let me get back to you on that. On the, in the slack channel. Okay. I assume APP increases the dimensionality. Uh, okay. So can we have rap? Okay. We do a wrap again. Does the art improve the speed of training?

Speaker 2:          00:55:08       Uh,

Speaker 1:          00:55:10       yes. Yes it does. Because it's less data. It's less data. So then because there's less data, it's time for a model is less. Okay, where can I learn data science, my channel, watch all my videos, I've put my life into this channel and I'm just getting started guys. We are going to be the biggest machine learning community in the world and I'm talking to you. We are the most important people in the world. The biggest companies are going to come out of this community. We are here to help each other. We are here. We're here to help each other learn and grow and get better. Okay? We are the future. Okay? And I do not accept any of you giving up, okay? You ask questions in the comments, you ask questions and it's slack channel. You ask questions here, but you do not. You do not give up. Not one here. Okay. And you, you believe in yourself. I believe in myself. I have failed so many times. Guys, I had failed so many times. It's so many things. I had been rejected from jobs. I have been fired before. I have,

Speaker 3:          00:56:08       okay.

Speaker 1:          00:56:09       Been so many things and yet I just kept going and that's what I want you guys to do. Okay. We are here too.

Speaker 3:          00:56:16       Yeah.

Speaker 1:          00:56:16       To progress our species and to end to just make the most awesome stuff possible.

Speaker 3:          00:56:24       Okay, we need to publish papers. We are going to publish papers. We're going to make startups. We're going to do so much amazing. So many amazing thing. You're just getting started. We are just getting started. We are a 66,000 strong community of machine learning engineers and

Speaker 1:          00:56:40       and the machine learning sub reddit is 80,000 we're going to hit 100,000 by April. First smart mark my words, we're going to hit 100,000 what makes us the biggest machine learning community? Okay, so I'm pumped for this and I hope you guys are as well because we are just getting started and do not tell yourself you can't do this. Don't ever tell yourself that you can't do this. I refuse to accept that the people who actually make valuable contributions believe in themselves. So step one, step zero. Do you remember all the steps that I just pronounced? Step one, do this. Step zero is believe you can do it. That's step zero. Okay, so remember step zero, how do I join your slack channel? I'm going to want to play it in a second. How do I find such excellent sources for understanding topics on the neck? I

Speaker 3:          00:57:27       okay.

Speaker 1:          00:57:28       What about, what's a better answer than I just Google through, I just go through all the results in Google, like optical, like page five. I basically curate data for you guys. So I'm looking through all the shit to find the best sources and then I talk about it.

Speaker 1:          00:57:42       Exactly. We are going to make a difference. I love you guys too. Okay, so that's it for the five stars. Let me do one more question. Oh yeah. I said I would do a wrap around. So someone's brought up someone's right, uh, um, uh, were uh, a topic. What? Okay guys, so the robot coat. Yes. I talked about the robot last live session and I even bought, I should've brought it with me. I bought a real robot kit on Amazon and I'm making this new video and I was going to, and I had, I had the script for it. But the problem was that, um, and this was my feedback from you dad city and they were right in this video that I'm going to release on Friday. I'm going to say, I was saying how to build an Ark. We know how to talk about motors. Had a talk about convolutional nets. I tried to put too much into one video. So I'm, I'm taking that robot and I'm not going to do it in this weekly video. I'm just going to talk about convolutional neural networks, but I'm going to make a robot it's own video in the future. Okay. It's going to be, I already bought the robot kit. It was 90 bucks on Amazon. I'm committed to doing it. It's going to be its own video. Okay. So is there a wrap on tensor flow? Okay. Okay. Tentraflow Oh, here we go.

Speaker 1:          00:58:50       Here we go. And these are ending right? There we go. Yeah.

Speaker 4:          00:58:54       Okay.

Speaker 1:          00:58:54       What does this, because it'd be it. Sorry.

Speaker 4:          00:58:58       Yo, Yo, here we go.

Speaker 1:          00:59:02       I'm on tensor flow. How long roll and you wait every day cause I'm in a community. So 3,600 strong machine learning engineers are do event. I sit back and I drink a beer and coffee and all of that stuff on me. I keep going man. I'm like Zombie. I wake up everyday and do machine learning and then I go to sleep because I'm fucking journey to do something else. Not really. I just do what I want man. Cause I'm really pretty and really it doesn't even matter what I say because I know all of you are going to go out and do something. Okay, I'm going to end it now going back down town and I'll take a bow. Okay, so that was it. So all right guys, thanks for showing up in his life session. Um, and I love you guys and I'm going to post all the links within the hour. Like always. I love you guys and definitely watch the tentraflow live that summit as well later on in the day or right now. I know I'm, I'm going to, so for now, I've got to go edit this video and think about this other video that I want to make on the weekend. So thanks for watching in a microbe. Where's Mike? What do you have in mind? Right, whatever. See you. Bye.