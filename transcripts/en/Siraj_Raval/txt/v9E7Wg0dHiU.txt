Speaker 1:          00:01          Deep mind released a new paper. Hello world. It's Saroj and deep mind. Just drop a paper titled Neural Arithmetic Logic units. It's a really cool idea that I'll demo in this video. Capable of learning a diverse set of tasks, including tracking time, translating words to numbers and counting objects in images. If you're new here, hit the subscribe button to stay up to date on my latest AI content. We all know that deep neural networks are amazing at learning very complex functions. They can learn seemingly anything and have been applied to a wide range of contexts from autonomous vehicles to dialogue generation to generating terrible pickup lines. But one of the longstanding objections do deep learning is that neural networks are not able to deal with numbers. That includes learning numerical functions that extrapolate beyond the training data. Storing numbers outside of the range of values seen during training.

Speaker 1:          01:16          Even basic counting has been nearly impossible for them to learn. Specifically, modern neural architectures can learn to count in the training data. In the testing data sets. They can only count as high as what they saw in the training data. This is more akin to memorization than generalization. It's like memorizing a vocabulary of numbers instead of learning about the idea of numeracy more generally. What makes Nalu so exciting is that it's a breakthrough in what has historically been a crux for neural networks, that they can't extrapolate basic numerical functions to numbers outside of the range they've been trained with. Now, Lou turns out to be an attention mechanism deciding which parts of the input and output should have certain operations applied to them. It can learn a function over a small range of numbers. Then extrapolate that function orders of magnitude later for the first time.

Speaker 1:          02:24          It allows neural networks to learn the general concept of a number. The authors also noted that this learning bias accurately fits a phenomenon in nature of numeracy and arithmetic. Many different types of species from baboons to bees can do this. Yes. Even bees can count be you'd. If I know also, I just like to make a side note here. Most algorithmic breakthroughs in AI like this one or just a novel, clever new combination of differentiable blocks to existing architectures. That means the barrier to entry to algorithmic research is quite low. You too can do this more on that in a later video anyways, to demonstrate that neural networks can't extrapolate well normally they created a series of neural networks called auto encoders. Auto encoders are computation graphs that reconstruct the input data learning a compressed representation of all the input data it's been given, and this representation is valuable for a very wide range of tasks.

Speaker 1:          03:38          From classification to regression. Each auto encoder has the same hyper parameter values, the same learning rates, the same number of layers. The only difference was that each had its own unique activation function. Neural networks are models created with linear Algebra and optimized using calculus. If these models didn't have activation functions included in their chain of operations that they would apply to the input data, they would only be able to learn linear functions, but activation functions allow them to learn both linear and nonlinear functions, giving them the Almighty universal function approximator title, they had each Kado encoder in code numbers between negative five and five that was the training data as noted in the graph. When the auto encoders were done training and given novel numbers to reconstruct as long as those numbers were in the range of the training data, the reconstruction was pretty accurate with a low error, but as soon as any of the auto encoders, we're tasked with reconstructing numbers outside of that range, the error rate increased substantially.

Speaker 1:          04:56          You were supposed to bring convergence to the model, not leave it in darkness. Neural networks can generate new drug molecules for diseases but normally can't count as well as a five year old can. So as a solution, the researchers proposed to new acronyms, I mean models that would be able to learn to represent and manipulate numbers in a systematic way. The first model is called the neuro accumulator or NAC, NAC. It's supports the ability to accumulate quantities. Additively a simple idea that helps form the basis for the next model. The NAC is a special case of a linear layer whose weight matrix w consists of just negative ones, Zeros and ones. It's outputs are additions or subtractions of rose in the input vector instead of arbitrary rescaling and that prevents the layer from changing the scale of the representations of the numbers. When mapping the input to the output, no matter how many operations are chained together.

Speaker 1:          06:07          So given our input data, we multiply it by the weight matrix w to get the output. The weight matrix w is computed by adding the Tan h activation of w hat multiplied by the sigmoid activation of m hat. This whole thing is differentiable. We can compute the derivative with respect to our weights here. What that means is that we can use the popular gradient descent optimization strategy from calculus to help improve this network's prediction, but while addition and subtraction are useful and equally robust, ability to learn more complex math functions like multiplication would be desirable as well, right? That's where the second model, the neural arithmetic logic unit or Lou comes into play. It learns a weighted sum between two sub cells. One is capable of addition and subtraction, while the other is capable of the other operations, multiplication, division, and power functions. It demonstrates how the neck can be extended with gate controlled operations, which allows for end to end learning of new classes of numerical functions not seen in the training data.

Speaker 1:          07:25          The nol Lou consists of two neck cells interpolated by a learned sigmoidal gate g such that if the add subtract sub cells output value is applied with the weight of one or on the multiply and divide sub cell is zero or off and vice versa. So the output a is computed by the first neck. This stores the result of the Nol lose addition and subtraction operations. The second neck learns to multiply and divide the output. Y is computed by applying several operations to the computed variables and altogether Analou, which consists of two knacks can learn arithmetic functions consisting of the major math operations in a way that extrapolates to numbers outside of the range observed during the training process. It can be added to existing neural network architectures as a new module from convolutional networks to recurrent networks. It's a few new variables and operations cleverly ordered in a way that allows these networks to learn fundamental arithmetic.

Speaker 1:          08:38          They experimented with the Nalo on a couple of different tasks. First was learning a simple function, meaning learning to select relevant inputs and apply different arithmetic functions to them. It extrapolated very well to data. Next, they trained it to count how many types of handwritten character images it was shown and it again extrapolated well to new data. They also trained it to be able to translate a text number expression into a scalar representation. A simple mapping that can be learned, which LSTM networks have shown to be the best at, but when the Lstm was augmented with the [inaudible], it achieved even better results. And what would a machine learning experiment be without sprinkling some reinforcement learning on it, right. While the previous tasks all involved making numeric predictions alone, what would it system that uses numeric computations internally to help guide a larger goal look like using a simple grid world environment as a starting point, they built a reinforcement learning agent that learned how to track the time in the game.

Speaker 1:          09:52          The agent has the ability to move in different directions, is given a time and receives a reward if it arrives at a particular location at that time. At each time step, the agent receives as input a pixel representation of the state of the world and has to select an action to take like move left or move up. It also receives an integer instruction t that says when exactly the agent should arrive at the destination. In the baseline model t was given to the agent as a new input concatenated with the output of the visual module, then pass to its LSTM core memory. In the improved model, t was passed through a knack and then back into the LSTM. Of course, the [inaudible] agent performed better than the baseline. There are already almost a dozen implementations of Nalo on get hub right now in many different python frameworks including Pi torch care, Ross and tensorflow, and this paper is only a few days old.

Speaker 1:          10:58          Pretty incredible stuff. Also, shout out to my friend and paper author Andrew Trask at deep mind for his work on the paper. I had a few questions that he helped clarify before I made this video. Numeracy, the ability to truly understand what numbers mean and how they interact with each other. He's different than just memorizing them, and now Lou helps give neural nets that capability. It's very exciting and there's a lot of room to build on this work. To further increase the capabilities of neural networks, links to some awesome educational resources, and the code will be in the video description. Oh Em, gee, you made it to the end. Hit subscribe if you haven't yet, and don't forget to connect with me on Twitter, Instagram, and Facebook for more educational content. Those links are right here for now. I've got to stop computing, so thanks for watching.