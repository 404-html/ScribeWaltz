Speaker 1:          00:00          Hello world. It's the Raj and let's learn about a popular new deep learning framework called Pi Torch. The name is inspired by the popular torch deep learning framework, which was written in the Lua programming language. Learning. Lula is a big barrier to entry if you're just starting to learn deep learning and it doesn't offer the modularity necessary to interface with other libraries like a more accessible language would. So a couple of AI researchers who were inspired by torches programming style decided to implement it in python calling it high torch. They also added a few other really cool features to the mix and we'll talk about the two main ones. The first key feature of Pi Torch is imperative programming. An imperative program performs computation as you typed it. Most Python code is imperative and this num py example, we write four lines of code to ultimately compute the value for d when the program executes c Equals d times a.

Speaker 1:          00:55          It runs the actual computation than in there just like you told it to. In contrast and it's symbolic program, there is a clear separation between defining the computation graph and compiling it. If we were to rewrite the same code symbolically than when c Equals d times a is executed, no computation occurs at that line. Instead, these operations generate a computation or symbolic graph and then we can convert the graft into a function that can be called via the compile step. So computation happens as the last step in the code. Both styles have there tradeoffs. Symbolic programs are more efficient since you can safely reuse the memory of your values. For inplace computation, tensorflow is made to use symbolic program. Imperative programs are more flexible since python is most suited for them. So you can use native python features like printing out values in the middle of computation and injecting loops into the computation flow itself.

Speaker 1:          01:50          The second key feature of Pi Torch is dynamic computation graphing as opposed to static computation graphing. In other words, Pi Torch is defined by run. So at runtime the system generates the graph structure. Tensorflow is defined and run where we defined conditions and iterations. In the graph structure, it's like writing the whole program before running it. So the degree of freedom is limited. So in tensorflow we defined the computation graph once, then we can execute that same graph many times. The great thing about this is that we can optimize the graph at the start. Let's say in our model we want to use some kind of strategy for distributing the graph across multiple machines. This kind of computationally expensive optimization can be reduced by reusing the same graph. Static graphs worked well for neural networks that are fixed size like feedforward networks or convolutional networks, but for a lot of use cases it would be useful if the graph structure could change depending on the input data, like when using recurrent neural networks.

Speaker 1:          02:49          In this snippet, we're using tensorflow to unroll a recurrent network unit overboard vectors. To do this, we'll need to use a special tensorflow function called wild. We have to use special nodes to represent primitives like loops and conditionals because any control flow statements will run only once when the graph is built. But a cleaner way to do this is to use dynamic graphs instead where the computation graph is built and rebuilt as necessary. At runtime, the code is more straightforward. Since we can use standard four and if statements, anytime. The amount of work that needs to be done is variable. Dynamic graphs are useful using dynamic graphs makes debugging really easy since a specific line in our written code is what fails as opposed to something deep under section.run. Let's build a simple two layer neural network in Pi torch to get a feel for the syntax.

Speaker 1:          03:38          We start by importing our framework as well as the autograph package, which we'll let our network automatically implement backpropagation. Then we'll define our batch size, input dimension, hidden dimension and output dimension. We'll then use those values to help define tensors to hold inputs and outputs. Wrapping them in variables we'll set requires gradients to false since we don't need to compute gradients with respect to these variables during backpropagation. The next set of variables we'll define our our weights. We'll initialize them as variables as well, storing random tensors with the float data type and since we do want to compute gradients with respect to these variables, we'll set the flag to true. We'll define a learning rates and we can begin our training loop for 500 iterations. During the forward pass, we can compute the predicted label using operations on our variables. Mm stands for Matrix multiply and clamp clamps.

Speaker 1:          04:31          All the elements in the input range into a range between men and Max. Once we've matrix multiplied for both sets of weights to compute our prediction, we can calculate the difference between them and square it, the sum of all the squared errors, a popular loss function. Before we perform backpropagation, we need to manually zero the gradients for both sets of weights. Since the gray and buffers have to be manually reset before fresh grades are calculated, then we can run back propagation by simply calling the backward function on our loss, it will compute the grain of our loss with respect to all variables we set requires gradient the true for, and then we can update our weights using gradient descent and our outputs. We'll look great. Pretty Dope. To sum up, high torch offers to really useful features, dynamic computation graphs, and imperative programming. Dynamic computation. Graphs are built and rebuilt as necessary at runtime and imperative programs perform computation as you run them.

Speaker 1:          05:25          There is no distinction between defining the computation graph and compiling. Right now, tensorflow has the best documentation on the web for a machine learning library, so it's still the best way for beginners to start learning and it's best suited for production use since it was built with distributed computing in mind. But for researchers, it seems like Pi Torch has a clear advantage here. A lot of cool new ideas will benefit and rely on the use of dynamic grass. Please subscribe for more programming videos. And for now, I've got a torch, my hair, so thanks for watching.