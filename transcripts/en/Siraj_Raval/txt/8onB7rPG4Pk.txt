Speaker 1:          00:00          Hello world is Saraj and the mathematics of machine learning. Is it necessary to know math for machine learning? Absolutely. Machine learning is math. It's all math. In this video, I'm going to help you understand why we use math in machine learning by example, machine learning is all about creating an algorithm that can learn from data to make a prediction. That prediction could be what an object and a picture looks like or what the next price for gasoline might be in a certain country or what the best combination of drugs to cure a certain disease might be. Machine learning is built on mathematical prerequisites and sometimes it feels like learning them might be a bit overwhelming, but it isn't. Or is it? No, it's not. As long as you understand why they're used, it'll make machine learning a lot more fun. You can have a full time job doing machine learning and not know a single thing about the math behind the functions you're using, but that's no fun is it?

Speaker 1:          01:09          You want to know why something works and why one model is better than another. Machine learning is powered by the diamond of statistics. Calculus, linear Algebra and probability. Statistics is at the core of everything. Calculus tells us how to learn and optimize our model. Linear Algebra makes running these algorithms feasible on massive datasets and probability helps predict the likelihood of an event occurring. So let's start from scratch with an interesting problem. The problem is to predict the price of an apartment in an up and coming neighborhood in New York City. Let's say Harlem. Shout out to Harlem, Yo west side represent. Okay. Let's say that all will know when we eventually make a prediction is the price per square foot of a given apartment. That's the only marker we'll use to predict the price of the apartment as a whole. And lucky for us, we've got a Dataset of apartments with two columns.

Speaker 1:          02:12          In the first column, we've got the price per square foot of an apartment. In the second column, we've got the price of the apartment as a whole. There's got to be some kind of correlation here and if we build a predictive model, we can learn what that correlation is so that in the future if all were given is the price per square foot of a house, we can predict the price of it. If we were to graph out this data, let's graph this out with the x axis, measuring the price per square foot and the y axis, measuring the price of a house. It would be a scatter plot. Ideally we could find a line that intersects as many data points as possible and then we could just plug in some input data into our line and outcomes. The prediction, poof. In mathematics, the field of statistics acts as a collection of techniques that extract useful information from data.

Speaker 1:          03:05          It's a tool for creating an understanding from a set of numbers. Statistical inference is the process of making a prediction about a larger population of data based on a smaller sample, as in what can we infer about a population's parameters based on a sample statistic. Sounds pretty similar to what we're trying to do right now. Right? Since we're trying to create a line, we'll use a statistical inference technique called linear regression. This allows us to summarize and study the relationship between two variables. A little mountain one variable x is regarded as the independent variable. The other variable y is regarded as the dependent variable. The way we can represent linear regression is by using the equation y equals mx plus B. Why is the prediction x is the input B as the point where the line intersects. The y axis and m is the slope of the line.

Speaker 1:          04:02          We already know what the x value would be and why is our prediction. If we had M and B we would have a full equation plug and play easy prediction, but the question is how do we get these variables? The naive way would be for us to just try out a bunch of different values over and over and over again and plug the line over time using our eyes. We could try and estimate just how well fit the line we draw is, but that doesn't seem efficient does it? We do know there exists some ideal values for M and B such that the line when drawn using those values would be the best fit for our dataset. Let's say we did have a bunch of time on our hands and we decided to try out a bunch of predicted values for M and B. We need some way of measuring how good our predicted values are.

Speaker 1:          04:53          Will need to use what's called an error function. An error function will tell us how far off the actual why value is from our predicted value. There are lots of different types of statistical error functions out there, but let's just try a simple one called least squares. This is what it looks like. We'll make an apartment price prediction for each of our data points based on our own intuition. We can use this function to double check against the actual apartment price value. It will subtract each predicted value from the actual value. Then it will square each of those differences. The sigma. That little [inaudible] looking thing denotes that we're doing this not just for one data point, but for every single data point. We have m data points to be specific. This is our total error value. We can create a three dimensional graph. Now we know the x axis and the y axis, they will be all the potential m and B values respectively, but let's add another axis.

Speaker 1:          05:55          The Z axis and on the axis would be all the potential error values for every single combination of M and B. If we were to actually graph this out, it would look just like this, this kind of bowl like shape cup. It firmly in your hand like a nice bowl. If we find that data point at the bottom of the bowl, the smallest error value, that would be our ideal m and B values. That would give us the line of best fit, but how do we actually do that? Now we need to borrow from the math discipline known as calculus, the study of change. It's got an optimization technique called gradient descent. That will help us discover the minimum value iteratively it. We'll use the error for a given data point to compute what's called the gradient of our unknown variable and we can use the gradient to update to our two variables.

Speaker 1:          06:47          Then we'll move on to the next data point and repeat the process over and over and over again. Slowly like a ball rolling down a bowl. We'll find what our minimum value is. See, calculus helps us find the direction of change in what direction should we change the unknown variables Mnb in our function such that it's prediction is more optimal, Aka the error is smallest, but apartment prices don't just depend on the price per square foot. Right. Also included our different features like the number of bedrooms and the number of bathrooms as well as the average price of homes within a mile. If we factored in those features as well, our regression line would look more like this. There are now multiple variables to consider, so we can call it a multivariate regression problem. The branch of math concerned with the study of multivariate spaces and the linear transformations between them is called Linear Algebra.

Speaker 1:          07:48          It gives us a set of operations that we can perform on groups of numbers known as matrices are training set now becomes an m by eye and matrix of em samples that have I features instead of a single variable with the weight. Each of the features has a weight, so that's an example of how three of the four main branches of math dealing with machine learning are used. But what about the fourth probability? All right, so let's just scratch this example. What if instead of predicting the price of an apartment we went to predict whether or not it's in prime condition or not, we want to be able to classify a house with the probability of it being prime or not. Prime probability is the measure of likelihood of something. We can use it probabilistic technique called logistic regression to help us do this. Since this time, our data is categorical as in it has different categories or classes.

Speaker 1:          08:43          Instead of predicting a value, we're predicting the probability of an occurrence. Since the probability goes between zero and 100 we can't use an infinitely stretching line. We're left with some threshold past some point x. We are more likely than not looking at a prime house. We'll use an s shaped curve given by the sigmoid function to do this. Once we optimize our function, we'll plug in input data and get a probabilistic class value just like that. So to summarize, machine learning consist mainly of statistics, calculus, linear Algebra, and probability theory. Calculus tells us how to optimize linear Algebra makes executing algorithms feasible on massive datasets. Probability helps predict the likelihood of a certain outcome. And statistics tells us what our goal is. This week's coding challenge is to create a logistic regression model from scratch in python on an interesting dataset. Get humbling SCO in the common section. And winners will be announced in a week. Please subscribe for more programming videos. And for now, I've got to build a movement. So thanks for walking.