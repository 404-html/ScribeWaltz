Speaker 1:          00:00          Hello world. It's Saroj and how would you suppose to train all these deep learning algorithms if you don't have an amazing GPU? We're going to talk about why gps are important for deep learning. Then I'll show you how to train a big style transfer model in the cloud. Super easily. A central processing unit or CPU acts as the brain of a computer. It's a small square like component with a bunch of short rounded metallic connectors on its underside like bender. Hi my shiny metal ash. It's fundamental operation. It's called an instruction. Every computer has both a CPU and memory, so when you run a binary file like an exe which contains instructions, the oos copies the program into memory then feeds it to the CPU one instruction at a time. The basic computation unit of a CPU is called a core. It can run a given task. It does things like maintain the program state and the correct execution order. It can use one or more cores to perform a task. I'd a given time. CPU is generally don't have more than 12 cores and our men for general computing tasks. There are some tasks that can only be executed sequentially like doing the Danny, Danny,

Speaker 2:          01:15          Susan.

Speaker 1:          01:20          Some tasks though can be run in parallel. In fact, they would benefit from parallelization rendered. Graphics is a great example. Each pixel can be processed independently of the others and so to help do this, the graphics processing unit or Gpu was invented. Modern GPS can have up to a thousand cores or more. They're made for parallel computation. A single GPU is like lots of little CPS all running at once. CPS are optimized for latency. They're good at fetching small amounts of memory quickly. While GPU is optimized for bandwidth, they're good at fetching large amounts of memory all at once. Lighting and shading effects require massive matrix operations to be calculated all at once and the GPU helps make this happen. It just so happens that another use case that involves an ungodly amount of matrix operations that can be parallelized is deep learning. Every computer needs a GPU.

Speaker 1:          02:19          Do you have one too? Without it, you wouldn't see an image on your display, but the question for deep learning is should you get a dedicated GPU or should you use the cloud building your own rig if you do it right, can be the most cost efficient method. The Best Gpu overall in terms of computing power is the Titan XP. If you have a moderate budget, then the GTX 10 60 is your best bet and if you're on a tight budget and the GTX 10 50 I should be your goto. It's kind of like when the Internet was first coming of age, everyone had an opinion on whether or not to host their website on their own hardware or use the cloud. Eventually though, the industry standard became the, because maintaining hardware wasn't unwanted hassle. Call one 800 train that shit.com wait, so which cloud providers should you use?

Speaker 1:          03:09          Let's talk about three AWS, Google cloud and Floy hub. Amazon's cloud offering is currently the industry standard and many big names rely on it. There are three types of instances or virtual servers that AWS offers. The first is on demand. This lets you rent an instance with a specific capacity that you can start and stop as you need to. When you restart it, it picks up right where you left off and only running instances are built. Then there are reserved instances for these. You commit to paying for a number of them for a certain period of time beforehand. The payoff for your commitment is that these are half the price of on demand instances. Then there are spot instances. These are the cheapest option. They are the spare computing capacity that Amazon has at any given moment. You have to get on them. If you're outbid, your spot instance is terminated automatically and they can't be stopped and restarted, only terminated so you can't just resume where you left off heady later time.

Speaker 1:          04:09          Aws is dope sauce extreme, but there are a lot of steps to get started with it in the learning curve is quite steep. So how does it compare it to Google cloud when it comes to pricing? Google cloud wins a two CPU eight Gig Ram incense will cost $69 a month with AWS compared to only $52 a month on Google cloud. It offers a pay per minute model instead of a pay per hour model, which is useful if you have short on the flight tests to run. That means that 2.01 hours of compute on Google cloud is equivalent to three hours on AWS. Well, AWS offers a generous one year free trial in which you can use seven 50 hours a month of a small CPU instance who will cloud offers you $300 worth of credit for 12 months and an unlimited time three tier. It still beat by AWS though.

Speaker 1:          04:57          In terms of number of offerings. If you need a cloud sequel solution, you can use my sequel with Google cloud, but AWS offers a bunch of different options. Google cloud is more flexible when it comes to configuring your instances though. They let you customize how many CPS and how much ram to use and it has something called preemptive instances, which is similar to Amazon's spot instances except you don't need to bid. It can run for up to 24 hours but could be interrupted by Google if they need the computing power. So if choosing between the two, while AWS offers more cloud options, Google cloud is cheaper and easier to use, but there's another even newer option. Floyd hub. If you are a beginner or just want to try out cloud computing, this is the best option. You get a hundred free hours of GPU usage and you don't need a credit card to sign up.

Speaker 1:          05:46          They offer per second billing and under the hood they are using AWS reserved instances but are providing simplicity as a service. It's like Heroku for deep learning. Let's test it out by deploying a style transfer model to it for training ourselves. The first step is to create an account on Floyd hub. Easy enough. Then we'll want to install the Floyd command line tool using the python package manager hip. This tool will let us interact with the Floyd hub cloud directly from terminal. Once that's installed, we can use this tool to help us authenticate with the server via Floyd login. We can paste are off token in as well. Then we can clone that style transfer repository directly from get hub to our local machine. Once it's downloaded, we can CD into the main directory and initialize it as a Floyd hub project. We'll call it style transfer.

Speaker 1:          06:36          Now we're ready to run the model. We can run it by wrapping the python run command for the main file inside of the Floyd run command. This repository has some flags that we can use, so we'll go ahead and specify the style we want to use. The directory will save our checkpoint to output is a special path meant to store checkpoints on Floyd hub, we can define what images we want to test on, how much influence we want the content went to have on this style transfer and the number of iterations to train for. Oh, and of course the data source we want to train on. That's all it takes to run and now it's training in the cloud. What this has done is it's saint our local code to the cloud, spun up a GPU instance, set up an environment with tensorflow installed and executed our command in that environment.

Speaker 1:          07:23          Once it's running we can easily check the status of it in terminal using the status command and the ID of our run or we can view it in the web dashboard. Under slash experiments. We can also view the logs using the logs command, which will show all our print statements and we can monitor training right here if we wanted to. If we make a change to this code locally, we can just rerun the project with the run command. Floyd hub will upload a new version of the code and start another run of the project so it's gotten version control built in just like yet. In fact, it versions the entire pipeline code data parameters and environment for exact reproducibility. Floyd hub uses a content addressing scheme for both runs and datasets. If there is a particular Dataset we want to train on again and again, it's useful to just upload it directly.

Speaker 1:          08:13          If we have it in our current directory, we can just create the remote directory for our data and name it using Floyd data in it and our nay. Then we can upload it using data upload. We can use this data id for any model we'd run later on. It takes about eight hours to train this model and when it's done we can test it out by running the evaluate script, giving it some fresh images to style transfer by pointing it to our images directory. When it's done, we can observe the output by running the upper command using the run id. Much deepness. All right, let's cut the Hay. If you want the absolute cheapest option and are willing to deal with the hassle. Building a custom deep learning machine is the way to go and link to how to do that. In the description. Aws and Google cloud are two great cloud platforms. Aws offers more cloud products and is the industry standard. A Google cloud is cheaper and easier to use. And Floyd hub is the easiest way to train your models in the cloud and the best way for beginners to get started, please hit that subscribe button for more programming videos. Check out this related video. And for now, I've got to fly to Amsterdam, so thanks for watching.