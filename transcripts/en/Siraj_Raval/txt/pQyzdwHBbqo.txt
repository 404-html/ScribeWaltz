Speaker 1:          00:00          That feeling when you read a great paper, but there's no code. Hello world. It's the Raj and the practice of actually implementing a technique from a research paper into code is supremely useful to learn how it all works. In this video, we'll implement the model from neural style transfer, a landmark paper that introduced the idea of applying filters in the style of a given artist to any image using deep learning. If we just want the code for the paper, it's best to first search the web to see if that code already exists. This saves us a lot of times since implementing it isn't a simple task. We can find a bunch of research papers using the popular tool archive sanity. It indexes the latest papers submitted to the Open Journal Archive. There's also Twitter and Reddit for keeping up to date with the field, but a lot of time the code isn't linked to the paper. In a post, we can use a tool called [inaudible], which links papers with code to see if the code exists. If it's not there, we can go straight to get hub and search for appeal of the keywords from the paper's title to see if anything promising shows up. If there's no code there, well it's time to code it ourselves.

Speaker 1:          01:27          So how do you choose which paper to implement? Ask yourself what part of the machine learning research pipeline interests you the most? Are you really into neural networks? How about unsupervised learning or attention mechanisms or stochastic models or evolutionary computing or cell folding cardboard? You've got to first figure out what makes you excited. For me personally, it's either novel optimization techniques or generative models using probabilistic programming. List them out in your notes, then start searching for important papers in that field. The best paper is the one you actually enjoy reading. There are a lot of papers out there, so be sure to pick one that's well written. Usually these come out of top tier universities or research teams in smaller universities that have been tackling the problem for years. I tend to look for papers with an industry focus. A lot of papers from academia are cryptic and lacking in detail some intentionally so because their goal is to publish as many papers as possible that look good on the surface. Industry focus papers have real life applicability so they are easier to reproduce.

Speaker 1:          02:53          Go onto our neural style transfer paper. I've got a great video called how read a research paper that I've linked to in the video description. It all boils down to carefully read the paper from start to finish multiple times as necessary. There will be a lot or a few terms that you don't understand as you read it. Make a note of them. You can look them up later. If we read the paper a few times and still don't understand the gist of it, we can follow the tree of citations at the bottom of the page and read relevant papers and if there's a paywall, just pirate it. Because Yolo, once we've traversed the whole tree of knowledge as all papers are built on previous knowledge, we'll be better equipped to interpret this paper before we start building our model. Well, one to first pay attention to the input data that was used by the authors. If we use a different training set with images that aren't, say high definition, but the authors used high definition images, there's a chance our algorithm won't perform as well as it did for the author.

Speaker 2:          04:05          Okay.

Speaker 1:          04:05          Our main task will be to understand the variables and operators of the model that the authors chose to use. We're essentially translating math equations in the paper into code and data. So before jumping into the code, we have to fully understand the equations and processes in these equations. Notations for variables and operators can change from one mathematical convention to another and from one research group to another. We should know what each variable is, whether it's a scaler or a matrix, and what every operator is doing on these variables. A paper is a succession of equations, so we'll need to know how we'll plug the output of equation one into the input of equation to once we've read and understood the paper, it's time to create a prototype. This can be a very time consuming process. The more detail we put into it. So to start off, let's use the highest level library we can to get something working as fast as possible. Cara Ross is a great deep learning library that lets us build neural networks in python focused on fast experimentation, good old special k

Speaker 2:          05:21          wait, that's taken

Speaker 1:          05:22          the paper details. A system that generates an image with the same content as a base image but with the style of a different picture. So there are three parts to the workflow, a content extractor, a style extractor, and a merger. In the first part, the content extractor, they found a way to separate the semantic content of an image. It says they used a convolutional neural network called VGG calmness or neural networks that are well suited for image classification tasks and Vgg 19 was trained on thousands of images and is capable of classifying images right out of the box. It looks like they use the output of one of the hidden layers as a content extractor. That makes sense. The hidden layers of a continent extract, high level features of an image and the deeper the layer, the more high level the attributes will be that the layer identifies between taking an image as input and outputs.

Speaker 1:          06:27          A guess as to what it is. A CNN is doing transformations to turn the image pixels into an internal understanding of the content of the image. We can use one of the intermediate semantic representations in a comnet to compare the contents of two images. If we pass two different images through a comnet after being passed through a few hidden layers, their representations, we'll be very close in raw value. If we pass both the final image and the content image and find the distance between the intermediate representations of those images, we have the content loss. The equation is listed as such. This summation notation makes the concept look harder than it really is. We make a list of layers where we want to compute the content loss. We pass both images through the network until it's at a particular layer in the list, take it out of that layer squared.

Speaker 1:          07:23          The difference between each corresponding value in the output and sum them all up. We do this for every layer in the list and some of those up. We're also multiplying each of the representations by some value Elfa called content weight after finding their differences and squaring. The second part of the workflow was to extract the style of an image. It looks like they used the same idea as the content extractor, meaning they use the output of a hidden layer but they added an additional step. It used a correlation estimator based on the gram matrix of the filters of a given hidden layer. Sounds complicated but if we read on, it seems like what that does is it destroys the semantics of the image but preserves its basic components making an excellent texture extractor. A gram matrix results from multiplying a matrix with the transpose of itself and because every column is multiplied with every row in the matrix, we can think of the spatial information that was contained in the original representations to have been distributed.

Speaker 1:          08:30          This Graham Matrix contains all sorts of information about the image, the texture, shapes, and styles. Once we have that Graham Matrix, we can find the distance between the gram matrices of the intermediate representations of both our image and the style image to find out how similar they are in style and it's all multiplied by some value Beta known as the style weight. For the last part, they needed to blend the content of one image with the style of another and they of course framed it as an optimization problem. As machine learning papers tend to do and in an optimization problem with some cost function is minimized iteratively during training to achieve a goal. Their costs function penalize the synthesized image if it's content was not equal to the desired content and its style was not equal to the desired style. Both the content and the style loss were added together to get the cost function.

Speaker 1:          09:29          They then performed backpropagation to minimize the cost by getting the gradient of the final image and iteratively changing it to look more and more like the stylized content image. They use an optimization technique that's terribly named called El bfgs, which isn't as popular as say, stochastic gradient descent. If we do a bit of research, it looks like it's a second order optimization scheme, meaning it uses the derivative of the derivative that gets closer to the global minimum, but the iteration cost is also bigger. Looks like this will likely be the term we'll need to spend the most time learning about, but first let's create some naming conventions. We've got a content image, a style image, and a final synthesized image. We can start coding this model in care offs sequentially as a list of steps to help us organize our thoughts here. It looks like carrots doesn't use the El bfgs optimizer so we can use psi Pi for that part.

Speaker 1:          10:30          It's going to be important to document everything here. As we code, since there are a lot of moving parts, we'll define some multidimensional arrays to help us create image variables. Then concatenate them all into a single tensor. They first synthesized a white noise image, then extracted the content and style of it. We can input our tensor into the VGG 16 model using carrot cost. They calculated the distance between the content of the image and the original content image as well as this sense between the style of the image and the original style image. We can extract data from specific layers using their numbering for both loss functions. Both distances were used to calculate the cost function and thus the gradient as is the case in machine learning. If the gradient is zero, we are done optimizing, but if it's not we'll run another iteration of optimization that will generate a new final image that's closer to the content image content wise and closer to this style image style wise and if the preset number of iterations is achieved finish.

Speaker 1:          11:39          Otherwise we'll go back to the start. After a couple of iterations we can check the result in our local directory and it seems to work well enough. We can go back and tweak the parameters as necessary to get a result. We're comfortable with. Now that we have a prototype version done, if we want, we can write a more detailed precise version in pure python or a lower level deep learning library like tensorflow. Do you want to be the very best, like no one ever was. We'll hit the subscribe button and it'll happen for now. I've got to use Pi torch, so thanks for watching.