Speaker 1:          00:00          Okay. When, when we, when I go to one, we're going to start. Okay. Cool. Five, four, three, two, one.

Speaker 2:          00:11          Hello world. It's Raj. Welcome to this live session. Today we're going to build a sequence to sequence model in tensorflow. And so, uh, let me mute this. Okay, so we're going to build a sequence, a sequence model in tensorflow. Oh my God, there's so many. There we go. Just muting everything we are going to build. Let me say that again. We're going to build a sequence to sequence model in tensorflow. Are we going to do it for a chat bot? No. Are we going to do it for a, uh, translation system? No. Are we going to do it for a, anything useful? No. The point of this is to learn the architecture. So that's what we're focused on. We're going to focus on learning the architecture. It's not about the application this time and I'm rolling up my sleeves because it's just cool. It doesn't actually have anything to do with anything, but uh, so that's what I'm doing right now and okay. So that's what we're going to do. So we're going to have some toy data and you can see right here, this is the output. This is what the output should look like. So we're going to give it some sequence of inputs.

Speaker 2:          01:15          We're going to give it some secrets of inputs like this. Let me make that bigger. And it's going to be like that. That could be a sequence four, eight, five could be. And then the end is just padding. We just add zeroes at the end randomly. Okay. And, uh, then we try to predict that same sequence. That's, that's the same sequence we want to predict. So we give it an input sequence, we encode it, and then we decode it and then we compare that decoded output to that initial sequence. So it's actually going to be different. And I'm going to show you why, right? This is, this is an example of how memory works in a sequence. A sequence model, okay. Uh, so as you can see, it starts off bad, but eventually at the very, very end, the predicted output is the same as the, uh, initial input.

Speaker 2:          02:05          Okay? So, uh, that's what we're going to do. Are going to try to predict that same. It puts input sequence and in the process, learn how memory works in a sequence, a sequence model. Okay? So that's what we're going to do. And the first question is, well, what type of, uh, this is not an auto encoder, but what it is is a, uh, bi-directional encoder decoder architecture. So there's the simple encoder decoder architecture, which we can look at and it looks like this encoder decoder architecture. And you know, a lot of initial papers use this. They use an initial encoder decoder architecture where the,

Speaker 2:          02:46          are you kidding me? Google? This is your stuff. Google. So it, it just looked like this, right? So you would have a values go one way in the encoder and then they would decode the other way. But what we're gonna do is we're going to do an improvement that we're going to add. We're going to make the encoder by directional. Now recall that we talked about this in the language translator, translator video, and we talked about how having a bi-directional encoder gives you the full context of that input sequence. You could, you could, you could talk about both the future and the past. So, so that's, that's why we're using a bi-directional encoder and uh, yeah, that's what we're doing. And uh, yeah. So let's, let's start building this thing, shall we? So let's just drive right into it. Uh, but first let me answer two minutes worth of questions and we're going to answer questions every, at 15 minute intervals.

Speaker 2:          03:38          So any questions you guys have? Yes, I have a new hairstyle, machine learning deep learning questions. I finally got time to cut my hair. I know time is very hard to come by these days, but that's the way I like it. What is the best way to identify if a feature in a model is statistically significant or not for inputs a, B, c and output? Eat? What is the best way to tell which inputs most contribute to eat? That is a great question. So before deep learning, we had to manually pick what those best features were because we didn't have a way for our model to learn what is the ideal features to use. So that's a whole field of engineering that existed before deep learning, which was called feature engineering. And the, the ideal way to pick that was thinking about what features you personally would use when trying to predict some outputs.

Speaker 2:          04:26          So if it was a classifier and you're trying and you're trying to classify a dog, you would pick, well what would I need to know to classify a dog? Well, I would look at, it's what type of dog it is I would look at, it's the color of its for and you know, the size of its ears and things like that. So that's a good rule of thumb to go by. What are the features you personally would look for? Uh, when trying to classify or do another task. Okay. Uh, is this also in line for you, Udacity nanodegree DL? Yes. This is a part of the university nanodegree and these videos are public for you guys. Okay. How about predicting the alphabet? I have a little snippet that generates a dataset for your previous sequence. A sequence code. You can absolutely predict the alphabet that is a 25 character long sequence.

Speaker 2:          05:11          Okay. And this, this exact code can be used to predict the alphabet. Okay. You can apply this exact code. Well you would just switch out the data, but this, this can be applied to more questions and then we're going to get started with the code. Does this model used gr use? No, it does not. But we will use gru. Gru are being used more and more and I'm not going to say they are objectively better than Lstm, but they are turning out to have better outcomes than LSTs in a lot of cases. Yes. One more question. Would you work on neural links? So I'm very excited about the idea. We need that to happen for us to keep up with Ai, for us to merge with it. That's the goal for us to become the gods and not have, it becomes some, you know, runaway God, we want to merge with it.

Speaker 2:          05:55          Right. And become amazing. So yes, I would, I work with them. No, because I'm focused on making content for you guys. So that's, that's what I'm trying to do and no one else can do what I'm doing. So that's why I'm doing well in the way that I deal with my personality and stuff. So that's what, that's what my job is. That's what I'm focused on. But I will collaborate with neuro link. I will collaborate with open AI, I will collaborate with Google. That's it for questions. So let's get started with the code. Okay. So the first thing we're going to do is we're going to import our dependencies. So dependencies, time. But see, and let me know if this is not big enough, right? Let me know if the text is not big enough. Okay.

Speaker 3:          06:31          Uh,

Speaker 2:          06:34          so, so these are our dependencies. So the first thing we're gonna do is we're going to import num Pi to do our matrix math. And by the way, the code is in the description. Check it out, follow along as I'm writing this. Okay. And I'm going to explain it. There's a lot of comments. There's a lot of documentation for this code. I'd really try to document it as best that I could. Uh, they're both okay. So in markdown and in the comments. So we're going to, uh, important and pie. And then of course, tensor flow for ml. And then we're going to have our helpers are helper is just, it's, it's one class and it had only has two functions. Those functions are formatting the data and, uh, generating the random sequence of data generating, sequenced, sequential, generating random sequence data. And we'll talk about what that data looks like. Okay?

Speaker 3:          07:23          Yeah.

Speaker 2:          07:24          So those are our dependencies. So now what we're going to do is, oh, there's one more thing. We want to, uh, run this reset default graph function, which is going to clear the default graph stack and it's going to reset the global default graph. It's just the one of those initial steps that we just have to do. And we normally wouldn't have to do this, but we're going to do this and tensorflow, because like, if we had multiple graphs, for some reason, this would be a good thing if we wanted to clear our cash, but we're just doing it because it's, you know, uh, it's a good introductory step. And of course we want to activate our session. Okay? We're going to do this at the very beginning. Um, and, uh, the version of tensorflow that I'm using is 1.0, okay.

Speaker 3:          08:09          Hold on. Nope. Do you have.virgin?

Speaker 2:          08:14          Okay, so that's the version of tensorflow.

Speaker 2:          08:19          That's not spam guys. Okay, so here we go with this. So let's start off by defining the vocabulary size. So recall that for encoder decoder architecture, we have to have a fixed size input vector. Now sequences are variable length. Like how are you versus how are you doing today? One is only four characters long, but one is five characters. So how do we solve this? Well, we add padding to the end. What we do is we add zeros to the end. So you know, one sequence could be this, you know, one sequence could be like this, the next sequence could be this, but they have to be the same length. So what do we do? We add zeros just like this. So they are the same length. So that's what we're doing right now. Okay? 350 people live. That's the way I like it. Okay, so let's define, our pad has zero and our end of sentence as one. So Eos stands for end of sentence and it's a token that specifies for our model when our sentence ends. Okay. And now we're going to define our vocabulary size. So you know, that is the, we are predicting what are these, what is the Max Max length for that input sequence. So we're going to say it's going to be 10. So in this case it's going to be 10 because we're generating toy data. But uh,

Speaker 3:          09:31          okay.

Speaker 2:          09:31          Uh, in other cases this can be very, very long. Okay. So then we're going to define our input embedding size.

Speaker 3:          09:39          Uh,

Speaker 2:          09:41          we're just going to be 20, which is the length of the characters.

Speaker 3:          09:45          Okay.

Speaker 2:          09:46          And so vocab sizes, the words and then character length. And then the, uh, and then input in bedding size is the, we take that input sequence and then we converted to a vector. And that vector is what we've then feed into the model. It's not that we just feed this raw, um, these raw words directly and we have to convert them to vectors and embedding vector. It means the same thing. And we're going to talk about that. Okay. Okay. So that's it for that. So the next step is for us to define our hidden units. So let's, let's define these hidden units and then talk about what they are. So in Kotor, hidden units and Dakota hidden units, we have 20 of these. And for our decoder, we're going to say we want 20 times two. So why are we saying 20 times to let's, let's talk about why we are saying 20 times two.

Speaker 2:          10:35          In the original paper they had a thousand units. So in the original paper Bites Sutzkever, uh, for sequence, the sequence model where they introduced this model, the youth a thousand for them, they use a thousand for both the encoder and then a thousand hidden units for the decoder. And generally that's what we want. We want them to be the same number of hidden units. But in this case, I'm saying the decoder is going to be double the size of the encoder. Why? Well, we want it to be a little different. We want that output value to be a little different. It's just going to be the same as the input, right? And we want it to change a little bit. So adding hidden units is going to change ever. So slightly our output so that we can then, um, minimize it. Right? So that's what we're having it be multiplying by two. So,

Speaker 3:          11:20          okay.

Speaker 2:          11:21          Yes. Great.

Speaker 3:          11:25          Okay.

Speaker 2:          11:25          So now we're going to define our placeholder. So placeholders, time, placeholders are, shout it out. What are they? They are gateways. They are gateways for data into our computation graph. These are the gateways and they are, they are primitives and tensorflow and they are,

Speaker 3:          11:43          mmm,

Speaker 2:          11:45          we always have to use them. Right? And what we're gonna do is we're going to have three placeholders. We're going to have placeholders for our inputs, uh, our decoder and the targets. Okay. So then we're going to define a couple of parameters, uh, the size of these. And then we're going to name them as well because these are names for our computation graphs. So we're going to actually talk about a tensor board next live session. Oops. I just spilled something. So this is a good way for us to differentiate between different primitives in tensorflow. That's why we had names to it. And so then we're going to have inputs length, the length of the inputs. So actually that's the next place holder, the length of the inputs. So we have our inputs, the length of them, and then the, the decoder targets. So then we have our shape. The shape is going to be none. And then we have our data type. Okay. So we want them to all be the same data type because that, that's what we're operating in. And if we didn't do that, then we probably have some kind of errors that start popping up. Okay. So it's 32 d type is in 32. And then, uh, the name is going to be a encoder inputs encoder inputs length. Okay. And then we're going to have,

Speaker 2:          13:16          okay, so input or length, and then we actually have one more. Uh,

Speaker 2:          13:28          no. Yeah, that's fine. Uh, Dakota targets. So Dakota targets are going to be, you have to a placeholder shape and then none. None. Because there are empty. Right now we, the, the uh, we're going to define them later in a second. Okay. Data type. I do think a Pi torch is going to be hot. So data type is going to be tf.in 32. And uh, got to remember to answer questions after this. And then finally our a decoder targets. Okay, so those are that. So we're going to get an error right here. So the, I knew it. So it's for this encoder inputs. Uh, let's see here. In Bod Syntax, let's debug this. So we have Incode are inputs and then we have, it's going to be a tensorflow placeholder. Let's see the whole thing here. What's going on over here? We want to maximize that. And so we have encoder inputs and then we have, it's a TF flood placeholder with a shape value, none, none data type. And then we have input. So it's actually not on that. It's on encoder inputs. Dot length and the syntax a yes, that's the one. Great. And now for the length it's going to be too, you have to a placeholder shape, uh, none data type

Speaker 2:          14:48          input of length. So why don't I have w here in my code, shouldn't have that. And then, uh, someone was saying there's a comma missing from shape, non equals none, comma like that. Okay. Encoder inputs, length thought shape. So in my actual code, I put a w there for some reason where I put a w like right. Where'd I put it? At the end? That the comma? Yes. Hold on. So I put a w here at the end in my initial code. And let me add this. Okay. See why did I put a w that, oh, Suraj. Here we go. Okay. So there is no w great. Okay. So now tore embedding layers.

Speaker 2:          15:38          Okay. Oh, let me answer two questions. Okay. So we are at the 15 minute mark. Let me answer two questions. Any questions guys? Yes, TFL and 32. Okay, so now we're going to define our embeddings. Uh, Oh, and I'm here. I don't know if I mentioned this at the beginning, but I'm here at the upload Vr Studios. Uh, so check them out, subscribe to them with Assad. Balaban Yan totally forgot about saying that. Uh, so yeah, just remember that. I remember his name. Okay. Links down below. Links down below. Yes. So, um, and this wouldn't be possible without him. So big round of applause. So now what we're gonna do is we're going to define our yes beddings. So we have our uh, variable.

Speaker 2:          16:24          Uh Oh and then one question. So what's best suited for ml, mac or windows or Linux, Linux and Mac, any kind of unix based system. Why? Because most of these libraries are they, when I look at stack overflow errors, when I look at get hub issues, it seems that people own windows tend to have more issues because the people who are writing these libraries are using them on unix based systems. So it just so happens to be that way. Um, yes. So okay. So now for our embedding, so we want to convert our Dec, our sequences to embeddings, right? So we're going to use the tensorflow primitive variable to do this. Okay. We have going to, we're going to initialize an embedding matrix randomly. So we're going to use tfs built in uniform random function to, to, to build this. And it's going to be the vocab size. So we're going to, this is going to be a matrix of values and it's going, then we're going to fill hit. We're going to fill it. Okay. So now we have our vocab size and then we have our input in bedding size. And then we have, what is that? What is the, what is the interval that we want to generate from, from negative one to one? Uh, it's a, it's a distribution from negative one to one of values. And then we have our uh, data type. Of course, our data type is going to be,

Speaker 2:          17:49          is the

Speaker 1:          17:50          audio really out of sync dot float? I'm looking into that. We have a Zod looking into that. Okay. And then

Speaker 2:          18:00          we have an encoder inputs embedded. So now we're going to take that embedding matrix and we're going to add in the, um, inputs. Okay?

Speaker 1:          18:17          Okay.

Speaker 2:          18:18          So what did we just do? So what we just did is we in randomly initialized an embedding matrix that can fit the inputs sequence. And then,

Speaker 1:          18:27          okay,

Speaker 2:          18:28          uh, once we did that, we then put our encoder inputs into that embedding matrix. And that embedding matrix is what we then feed to our encoder. So let's now build that encoder, right? We have our embedding matrix and now we can feed that directly to our encoder. So for our coder, so define encoder. So for our encounter, tensorflow has so many updates, like they're all over the place and version changes and API changes, like all software. And so the Lstm cell is always changing where it is. Before it used to be, I think it was in the LSTM cell, used to be in, where was it? In the previous version, it was in just a different place and now it's in under opps RNN cell. And in later versions that will probably change. So it's always a good idea to keep up with these API changes. Um, and to best way to do that is a tensorflow documentation. Okay. So we're going to import our LSTM cell and our LSTM states Tupelo. I'll talk about the second one and why we're importing that in a second. Okay. So that's, we're going to define our encoder. So our encoder cell, and it's just one cell is going to be an LSTM cell, right? That we just imported. And we're going to define the number of hidden units, uh, as that number we defined previously. And that's how many, which was, uh,

Speaker 2:          19:53          2020 neurons. 20 of these voice of God is awesome. I'm glad you guys liked the voice of God. So that's 20 of these encoders cells. Okay. And remember each of these is an LSTM itself. Each neuron is an Lstm. Okay. And, um, so that's our encoder cell and see deprecation warning, but it's fine. It's just a warning, but it's will soon be deprecated. I knew it would happen. So, so now, um, so this is actually like a, a spatial,

Speaker 2:          20:31          a very hellish spacial line to write out, which is probably gonna, you know, in python, especially with all of this in tech. So I'm just going to paste this part and let's talk about what this is. This is the dynamic RNN. So when using a standard RNN to make predictions, we're only taking the past into account, right? Um, so for certain tasks just makes sense whenever we're predicting the next word in the sequence. But for some task it would be useful to take both the past and the future into account. Okay. So this is the bi-directional directional part. We're taking both the past and the future into account. So let me talk about, uh, what we're doing here. Okay. So in order to demo this, I'm going to do some VR to demo this. Okay. We're going to talk about how a by directional layer works over time. Okay. So let's get some VR going and I'm just going to draw this out in Vr, uh, for you guys.

Speaker 1:          21:21          Cool.

Speaker 2:          21:22          Thanks. Probably need two of these. Okay. So let's see if we could get our VR mode on. Yup. We're in our VR mode. Okay. So, so, um, so we have our encoder decoder, right? So we have our encoder in coder. This is so fun. And then we have our decoder and this is a high level of abstraction of what it is, right? We all get this idea that we are taking our encounter and so we're giving it a sequence, right? So we're giving you a sequence like, you know, how are you, how are you? And we're taking the sequence, we're actually converting it to are embedding matrix,

Speaker 1:          22:04          okay.

Speaker 2:          22:04          Arts, which is a vector. And then we're feeding the vector into, right? So it's, it's, here's the steps, right? Cause you could see, you guys can see all of this, right? This is dope. Yup. So we have our inputs sequence, we vectorize it and we feed it to our encoder. And then we want that final hidden state, right? We want this final hidden state. So the hidden state, we don't feed the output of the encoder to decoder. Remember we feed the hidden state, which we'll call h. So we'll just say hidden, right? So this is, we embed this into a vector and then the encoder further embeds it across as many as many neurons as we define. And so because this is bi-directional, uh, it's looping over, uh, over the sequence from both left and right. So since it's by bi-directional, it's looping over the sequence, both in both left and right directions, and then what we're doing. And so now it's time for three d. So I'm going to go over here. What we're doing is through time, and this is, this represents through time, we're feeding in the h the hidden states, and we're also feeding in the pre the previous input. So this is back propagation through time. So there's two values. So BP through

Speaker 2:          23:23          time, my handwriting needs to get better in Vr, but just like, bear with me for a second. So, so I'm, I'm trying to demo like through time, so right inputs, sequence, vectorize encoder, get the hidden states and take the hidden state and the previous input and feed it back into the encoder. Okay. And so we feed it back into the encoder and then, uh, that's one full time step. So in one full time step, once we feed that in, then we feed that into the decoder. So we roll it feet, decoder role, feats decoder. Okay. And so over and over and over again for every word that we have in our input sequence. And then we feed it to our decoder, our d Carter will output a, um, uh, a vector. And then from that vector, we convert that into our, uh, in this case it's going to be the initial value. How are you, but in other cases in chatbots, it would be, you know, I'm doing fine or you know, translations is going to be French. Okay. So that's the high level of what we're doing here. Hope you guys like that. Okay. So back to this.

Speaker 2:          24:30          People love the VR people of the VR. Okay. It's super cool, isn't it? Okay. So we have to keep going guys. So it's super cool. I know. So now,

Speaker 3:          24:43          uh,

Speaker 2:          24:44          here we go with this. It's a vibe. It's a vive. Yeah. Uh, great. So now let's, uh, do what I was just talking about. So let's, let's now programmatically do what I was just talking about. Okay. So we're going to concatenate bartenders along one dimension. So we're going to take, so this is the actual step where we're combining our,

Speaker 3:          25:14          uh,

Speaker 2:          25:16          this is the bi directional steps. So this is a bi directional step. We write down by directional step. Okay? So we'll talk about what that means by directional step. So what we have here, what we have here is we have our outputs for our encoder. So we have our encoder outputs and the encoder output. We're going to use TensorFlow's concatenate function. To do this, we're going to have both our forward outputs and our backward outputs. So our forward outputs are what we s we spit out from this dynamic sell, right? We got our forward outputs are backward, outputs are forward final states in our backward final state. So their states and outputs for both the encoder for, sorry, for both the forward and the backward,

Speaker 3:          25:54          uh,

Speaker 2:          25:56          parts of the bi-directional RNN. Okay? Forward and backward. FW and BW means forward and backwards. So we have states for both. How are you? And then you are how. Okay. And what this does is it allows us to take into account the full context. So the both the past and the future. This is definitively better. I'm telling you right now that it is better to do this every time. It is more computationally expensive to have a by directional layer. But just think about it. You want the future and you want the past, when you're trying to make a prediction, it, it's a word, a story I sequence is all about what's the story is about what's happening all over it, whether it be music, whether it be art, words, numbers, if there's a pattern, if there is a pattern book past and the future matters. So having by directional layers, although more computationally expensive, gives us better, uh, predictions. Okay. So, so now we're going to take our encoder outputs and we're going to take our in quarter up forward outputs in our encoder backwards outputs. And we're going to concatenate them along a along, uh, two axes, access axes. It's the poor hall. Uh, and so, uh, that's gonna give us our encoder outputs. And then what we're going to do is we're going to take the final state, the final state. See, uh, so that's, that's for our outputs. And then we're going to have our concatenate function to take our,

Speaker 1:          27:34          okay.

Speaker 2:          27:36          Okay. So we're going to take our encoder forward, final state dot c and then we're going to take the encoder.

Speaker 1:          27:46          Okay.

Speaker 2:          27:47          Backwards final states. Dot C okay. And so what is this, what are these words? What does CN one, let me, let me write this owl and then we're going to, uh, I'm moving a bit too fast. Okay. So

Speaker 1:          28:05          hold on.

Speaker 2:          28:07          So this specific part I'm going to paste because there's a lot to explain here. So, okay, so we have a final state, see and we have a final state h and I can slow it down as well. So what we have here is we have, so h and c are commonly used to denote output, value and sell state. So it's both the output value and the cell state and we want to contaminate both of those for both our forward and are backwards, um, our forward and backward outputs. Okay.

Speaker 1:          28:46          MMM.

Speaker 2:          28:50          So that's what we're, so we're concatenating both of those values and then we get a final state using the LMS. And so now this is why we imported LSTM state to pull at the beginning. This is why we imported it, because we're going to use that final state c and the final state h. So that's the, that's the one I say Cnh I'm talking about the internal state of the cell and the output value. We're combining all of those to get that final state for the encoder. And that this value right here is what we then feed into our decoder. It is a combination of both our forward or backwards, our cell state and our output. We combine it altogether, and that's what we feed into our decoder. So now let's, let's define our decoder. So our decoder think there's an encoder mistake where you misspelled. Oh, e e r at the end. I'm trying to find it here. Someone said there's a encoder to misspelling a bandaid. I don't see it in this,

Speaker 3:          29:49          uh,

Speaker 2:          29:52          someone encoder. Someone said there's an encoder mistake. I don't see it. Yeah, I'm not either or whatever. So where were we? So now we're going to define our decoder. So for our decoder, for our decoder, we want to do, it's going to be very similar but different. And I'll show you what the difference is. So let's define it. So this is going to be similar. We define it the same way. It's still an LSTM cell out. Remember the number of hidden units is different. It's double that of the,

Speaker 3:          30:28          okay.

Speaker 2:          30:33          And then, uh, we're going to define our batch size and then our, we're going to unstack. So let me talk about what I'm doing right here. So what I'm doing right here is, uh, what I'm doing here is I'm going to do fine our encoder and hour.

Speaker 3:          30:58          Yeah.

Speaker 2:          30:59          Do you have to unstack.tf dot shape encoder inputs? Okay. So let's define our Dakota defining our decoder, defining our decoder. So we have our a decoder self and we're going to, we've defined our decoder cell and so what this, this step does is what we really care about is the batch size is going to give us the batch size. We have all of those inputs and we went to feed it into our decoder in batches batches because that's how we train, right? We feed our data into batches, uh, data into our decoder in batches. Like a subsets. There are subsets. Okay. Uh, okay, so, uh, right. So then we're going to have the length of the decoder, which is going to be the encoder inputs length plus three. So why do we have plus three? Because we have two additional steps. One for the leading end of sentence token for the decoder inputs. We want it to be a little bigger because we have an end of sentence token at the end. That's just going to help us help our model. No, uh, that this is the end of a sequence. This is in natural language processing. We have these end of sentence tokens all the time. Um, I'm actually, I don't think we have models that can,

Speaker 2:          32:22          no, when the sentence is over yet, I actually, I haven't seen a model that doesn't use these, uh, either padding or, um, some kind of end of sentence token to specify, you know, that the sentences over yet, we, we need to get there. We need to get there where we just feed it. And we don't have to do any of these, uh, preprocessing steps, but we'll get there. Okay. Uh, so now let me answer some questions since we're 15 minutes in. Uh, does dot dividing into smaller batches? Is, is dividing into smaller batches more computationally expensive? No, no, it's not a, because it's still the same amount of input data. Uh, but what, what having it, uh, in batches does, is it,

Speaker 1:          33:09          okay.

Speaker 2:          33:10          It makes our prediction better because, uh, we have more iterations that are happening. So there are more, uh, wait updates happening. It actually, honestly, it is a little more computationally expensive, but it's so, it's so small. The difference is so small, so don't, don't, don't worry about that.

Speaker 1:          33:27          Okay.

Speaker 2:          33:28          Ah, I appreciate it, Paul, for waking up at 4:00 AM in Melbourne for this. Uh, okay, this is a good question. In what ways are gru is outperforming LSTs? I recently read a paper that says adding a bias of one to the LSTs. Forget gate closes the gap between the LSTM and the GRU. So Gru cells are very similar to LLCs and we're definitely gonna talk about that in this coming weekly video in three days. But to sum it up, so gr use have less gates than LSTs. There's just a reset gate and there's a, uh, update gate. So whereas Ellis teams have a, forget gay, they have three days so it's two gates so it's less computation happening so it's less computationally expensive and not only is it less computationally expensive, it tends to have better results specifically for dynamic memory network, which is a really, really cool model type.

Speaker 2:          34:19          And I'm going to start talking about as we get further into this course. We have four more videos in this course and we are now. We are now guys, if you don't understand all of this, don't worry. We are at the bleeding edge of deep learning. This course is called intro to deep learning, but we went from doing linear regression at the start too. We are about to go, we are about to get into generative adversarial networks and we're about to get into the bleeding edge of all of AI, which is not even deep learning in the last episode, which is I'm most excited for, which is called probabilistic programming, which is some future stuff and you know how much I liked the future. So that's going to be awesome. One more question.

Speaker 1:          34:56          Okay.

Speaker 2:          34:57          Pace. Raj, what model would you use to analyze repetition in speech? Repetition in speech? Great question. Repetition in speak. What do you mean by analyze? Repetition is be, so I've just got to clarify for you. Uh, analyzing repetition in speech. Uh, so maybe the frequency of repetition. So how would I do this? So this does, this is a good way of thinking about it. So you would, um, you could consider it a classification problem. So an easy way to do that would be to an easy way computationally to do that would be to pre label, uh, what each sentence is. So, you know, you would say like, how often does this label, you know, x up here versus y and Z and a, B, c, d, d. So it'd be a classification problem. Uh, and you would, uh, use a deep net and you would use a, you wouldn't even have to use an encoder decoder because you would just define how many times this label shows up in the data. So it would be a multi-class classification, deep net using a supervised learning with labels and, um, inputs. One more question. Evolutionary strategies that is coming Ganzer are coming. And uh,

Speaker 2:          36:10          our large batches always worse than smaller batches in terms of prediction, accuracy, uh, our large batches always worse than smaller batches in terms of prediction accuracy. That as as far as I've seen, yes. But it's not like, it's not like there's like some uh, some exponential in terms of like, you know, you can never a, you know, you can just get infinitely smaller in terms of batches and by infinitely smaller, I mean you can have an infinitely large number of batch, a batch steps. Uh, but there's, there's a, there's a, there's a bell curve, right? So there's a point where it's at the optimal level of batch step and then it goes down again. Remember, batch steps are one of many hyper parameters and all of machine learning, all of deep learning, I'll, sorry, all of deep learning. It's about tuning those hyper parameters and to me what I would like to see, what I would like to see you guys work on, what I would like to see more people work on is ways of learning these hyper parameters. Because this signing these hyper parameters is so annoying sometimes, right? Uh, because they are so arbitrary, there's no way for us to really know. We have to learn them manually. And it takes up a lot of our time from looking at what is really necessary. And that is a higher level architecture. So let's find out easier ways of learning these hyper parameters. Okay, so that's it for questions. Now we're going to, we've defined our decoder

Speaker 2:          37:32          and the lengths and now we're going to define our weights and biases. So

Speaker 1:          37:38          yeah,

Speaker 2:          37:39          hold on. So our weights and biases are going to be

Speaker 2:          37:47          are, so our output projection, so let's define our, define our weights and biases. So remember this is all for the Dakota. We're working on the decoder value. So for our decoder values, we have both weights and biases. And these weights are what we're going to use. We're, we're manually defining these. Remember we didn't define these for the encoder because? Because tensorflow had those built in for us in that dynamic RNN function. But we're going to be a little more detailed with our decoder. Why? Because we are going to implement attention ourselves. Okay. So I want to show you guys how attention works. So I've kind of skipped on attention before I just said, you know, hand wave, this is attention, boom. But now we're going to implement a little bit of what are, what are, what is called soft attention and we'll talk about that. So we're going to find these weights and biases for our decoder manual. We've uh, okay, so where were we, where were we? So where were we? Right. So these weights are going to be initialized random weight using again, tension flows, random uniform function. We use that a lot. And then we want to define the size of that in terms of tensors. So it's going to be, the size is going to be a three dimensional tensor. So we have the number of hidden units, the vocab size.

Speaker 1:          39:10          Okay.

Speaker 2:          39:11          And then the distribution from negative one to one given our data type and it's all in 32 bits and a

Speaker 1:          39:22          yeah,

Speaker 2:          39:23          right. So then our bias, our bias is going to be TFI variable if that's zeroes. And then the vocab size followed by the type. Okay. So then that's the title. And let's talk about this. Okay. And I'll answer questions in five minutes. So think about your questions in five minutes. It's to be the next question mark. I'll answer them too. You have to float 32. Okay. Okay. Weights and biases.

Speaker 2:          39:54          These are our weights and biases guys. And so it's going to be offside. So the number of hidden units, so, right? Th these weights and the reason we said we put the Duke, the hidden units as, uh, as an, as an input to this is because the way you have to connect all of those hidden units to the output, what the output's going to be. So we take all of those values and then we multiply it by the way, it's, which is a initialized as a random matrix and it's going to learn what the optimal weight value should be to get that value, right. So, right. So, okay.

Speaker 1:          40:29          Okay.

Speaker 2:          40:30          Okay, so now we're going to get into the attention part. Okay. So, uh, let's talk about attention. So to get attention, let's see. Let's see

Speaker 2:          40:49          guys, if you're lost, don't worry about it. This stuff is, remember the bleeding edge and I'm going to get better. I'm just going to get better at explaining it. I'll get better examples. You're going to get better just by looking at this. You're getting better crew. Kudos to you. Thank you for being here. By the way. I need to say thank you to you guys. You guys are awesome for being here, for wanting to learn this awesome stuff. This is the most important stuff in the entire world. More important than anything else you can be doing in the world. It is working on AI. If you are a smart person in the world right now, more important than politics, more important than climate change, more important than anything else in the world. You should be working on AI. Why? Because we are the closest to solving it. Oh, across all these problem spaces. And if we can solve this, we can solve everything else. So what we're gonna do is we're going to talk the next steps here and to do that. Let me just paste this in because we have a lot to, to go still and uh, we don't have time to really type out everything. So we are now, now is the padding step. Remember as I defined the pad and Eos, uh, functions before now is now we're going to actually pad those inputs. So we have, uh, we've embedded them those values.

Speaker 1:          41:58          Okay.

Speaker 2:          41:58          We've embedded those values into our projection matrix, right? We took our inputs and we embedded them. And now what we're going to do is we're going to,

Speaker 1:          42:10          yeah,

Speaker 2:          42:11          now we're going to add those paddings so we, so we're going to add the padding's to it. And that's what, so the t, the embedding lookup function of tensorflow, retrieves the roads of the, of the parameters tensor, which is in this case, the, uh, embedding matrix. And the behavior is similar to using indexing with arrays and num Pi. So this is essentially just adding the padding and the end of sentence token to our, um,

Speaker 2:          42:40          two are uh, embedding lookup. Okay. So that's what the end of sentence and padding steps do. And now get ready for attention guys, because I have never like fully, programmatically a defined attention. And so now we're going to define attention. So bear with me. This is gonna be awesome. Bear with me. Okay, so now we're going to, now we're going to implement attention. Okay, and there's there, there are two functions here though. We have to implement. So for attention. Now remember for the encoder we defined a that that function, what was it? W The function that we defined was, let's look at it again just so we remember what it was. It was called bi-directional dynamic coronet. That functionality, the looping functionality that I did in Vr a second ago where we're taking the previous hidden states and the input and feeding both of those backend. That's all done by this one line. All of that is done by this one line, but for the decoder we are going to do that manually because we want to see how it works. So this is what we're going to do manually. We were going to loop. This is this function, does that loop manually for us? So,

Speaker 1:          43:45          okay,

Speaker 2:          43:45          what we are doing in this is your first saying, what is the end of sentence step to get that initial input? What is the initial cell state? And then what is the loop state? And uh, all it does is it's going to initialize these, these values and then return them because the loop isn't actually happening here. The loop happens in the next function. All we're doing in this function is defining what the end a sentence say. It is, what the cell state is, what the initial output is, which is going to be non to start off with, and then return those values to, to then loop in a second. So now, now we're going to loop. Okay, so now we're going to

Speaker 1:          44:27          loop.

Speaker 2:          44:31          Let's get to the looping par. Let's pace this in as well. Got a lot to go. A lot to go. Okay, so for our, so here's our attention mechanism. So this is considered, um, soft attention. Okay? It's considered soft as opposed to hard attention. Uh, because it's a very trivial form of attention. I'm going to explain exactly what that is. Okay. So let's talk about what we're, what we're, what we're doing here. So we have loop fn initial and uh, so, okay, so let's talk about what we're doing here. What this does is it's going to get the next, the next input in the next day. So remember the loop. That's what this is doing. It's transitioning for the loop. And uh, so let me talk about what this does, but I said how would answer questions at the 45 minute mark. So let me answer some questions right now. So

Speaker 4:          45:34          Paul Brady's got a question and he's asking, are we going to be exposed into tensorflow, uh, in the Udacity course? All right, the intro tendon transfer flow or should they start focusing on learning it on their own way?

Speaker 2:          45:45          Yes, we will be learning tensorflow in the U Udacity course and we will be learning tensor flow, um, just from here on out until, um, you know, anything better comes along which pie torch is starting to get pretty hot. So also check out py torch, cause I definitely am, because their documentation is pretty dope

Speaker 2:          46:03          when we write codes to create a predictive model. What is the output? Is it an object, a dll, an exe? Thanks in advance? Uh, the output is a set it, while the output could be anything, but it is, um, it could be numbers, it could be words, it could be depending on what you're trying to predict. You're trying to predict the next word. Are you trying to pick the next number? Our Turner trying to predict a label. Is this a cat or a dog? But it's going to be a variable in code, a programmatic variable, a space in memory that we defined would, they'd word and programmatically like label. So it's a set of integer numbers, but it's not an exe if not some kind of, uh, it's not, it's not a, it's not a binary executable file that you just run like a program. It's just a set of, it's like memory. So if you were to even print, you know, a variable, it'd be a variable. Okay. When we feed words from our, two more questions, when we feed words from our texts, example to the model, are we, how are we handling pronouns? For example, if we have a text on Sir Albert Einstein, many of the times we'll refer to him as he, his, him, et cetera.

Speaker 1:          47:10          Okay.

Speaker 2:          47:10          Okay, so great question. So before deep learning, so again, before deep learning, we had to define these things. We have to, and this was, this was an a whole subfield of natural language processing, which was called, which is called, uh, a part of speech tagging. So we would manually tag these parts of speech and how do we that while were you, there are a bunch of banks like word banks that just, they're like dictionaries where they have like, just give it a word and it'll tell you the part of speech. These are prerecorded. And so that's, that's one way to do it. But with deep learning, we have to think about this at a higher level where it's just data in data out. We don't, don't even, I know, you know, as humans we want to worry about these things, but we won't have to, we don't have to because it'll learn what are pronouns more or less. It's not going to define them as Pronoun. But internally and it's hidden states, the more data we feed it, it will learn what a pronoun is. And by that I mean when to use it, when not to use it. One more question,

Speaker 3:          48:13          uh,

Speaker 2:          48:18          the bioinformatics question. What about deep learning in bioinformatics? What about it? Uh, so I guess you're asking about use cases. So bioinformatics are like, like high scans and stuff, right? Like bio, uh, thumb prints and stuff? Yes, absolutely. We could use machine learning for that to learn what to learn, what a certain biological signature looks like. Like what is this type of person, what is their biological signature looked like by giving a data of a certain type? Remember it's a label, it's a class. It could be a classification problem. That's kind of generally what I, what I see bioinformatic, they use case for deep learning in bioinformatics. I'm also anomaly detection. Like, uh, maybe somebody, uh, is very different. So, uh, somebody who has some off genetics, some one of their genes is like different than the rest. Uh, and it can be used for anomaly detection, which would be unsupervised clustering. And for that I would use state of the art generative adversarial networks. That's it for questions. Let's get back into this. Okay.

Speaker 2:          49:27          Uh, where were we? So back to this, to the loop pumps was just, so here's where the attention mechanism. So the attention mechanism and the only time I'm going to answer questions after this is at the very end. So that's in 10 minutes, the loop function. We're going to get the next input. So what are we doing here? We're going to get the dot product between the previous output and the weights and then add the bias. So Matrix multiply the previous output and the weights and the bias. And you remember to think about that br thing that I just did, the Vr Demo, that's what's happening here. We're looping it over time and that's going to give us our output logics. And then we're going to use the Arg Max function. And this line is attention. This line is attention. And what do I mean? The Arg Max function will return the index with the largest value across the axis of a tensor.

Speaker 2:          50:13          That largest value is the one that we are picking has our prediction. So that picking that choosing step is attention. What you pay attention to. I remember attention is a very broad term. It's a very broad term and we could it to pick what is the best value arbitrarily. We could generate a probability distribution and say, you know, over this certain threshold, those are the values we want. Attention mechanisms are words that we use for how we pick what the best value is from a set by whatever definition of best that we define is. And that depends on your use case. So this is the attention mechanism. Remember it's really not that complicated. It's just one line. You attention mechanism has just one line of code. Okay? Uh, so then once we have our prediction, we're going to embed the prediction for the next input using this embedding look up layer. So we, we have that input and we want to feed it back into our network and that's going to be our next input. And then we're going to say this, this line defines what the, uh, what the ending sequence is going to be. The or the ending scalar. This, this tells us that we are done looping. Uh, and so then what we do

Speaker 2:          51:26          is we have to,

Speaker 1:          51:30          uh,

Speaker 2:          51:31          and so this reduce all is the logical end of elements across dimensions of a tensor. So we are saying this is going to output a boolean scale or are we done or not? And then a conditional that says is it if it's done or not, then continue and get that input value. And it's just like a, um, I remember from data structures and algorithms when we would, uh, for binary trees and for, uh, for any kind of, or for any kind of tree like structure, we would then set the previous day to the current state and then the previous, um, leaf to this leaf that that's kind of what we're doing here. It's the same thing. It's a, it's, it's a data structure. It is a data structure. A cell is a data structure. And the time element is why we're switching the previous to the current. This is that you should remember this from data structures and algorithms. Um, by the way, if you haven't taken data structures and algorithms, definitely do that and see my, uh, how to succeed in any programming interview video. Okay. So we've got to start wrapping up. Okay. So I'm getting notes. We got to start wrapping up. So what I'm going to do is I'm going to start reading this. Can we, we can end in eight minutes.

Speaker 1:          52:34          Fine.

Speaker 2:          52:34          Five minutes. Okay. We have five minutes guys. This is just a onetime thing. Next time I'm going to be, uh, I won't go over this much over the time I'm allotted, so don't worry about it guys. So let's, let's go. So we had that out. So I'm going to be reading off of my code because we are running out of time, so it don't worry. We are, we're, we're almost done. We only have a few more lines of code. Uh, and so let me explain it. So that's our looping mechanism. And so what we're saying is we're doing two loopy and mechanisms, one for just the first date because we have no data. We want to fill the data. And then we do our main looping mechanism, which is what we just did know. A cell is not a tree data structure. It's just ate an analogy I was using. A tree has nothing to do with this. It was just an analogy. Don't worry about it. So,

Speaker 2:          53:23          so this is our loop, right? This is the looping state. This, we are doing this manually and if you don't want to do this, tensorflow has a line of code that does all of this for us. Okay? But we want to look at how this works. And so we are looping, we're taking that input and the previous time step and we are feeding it back into the network. And that's going to give us our decoder output and the Dakota final state. Do we care about the final state? No. We care about the output value. The only final state we care about is for our encoder. The only final state that is true Jordan. Uh, why do I have to listen to someone? It is my channel is true. I'm getting my own studio. Don't worry about it. It's going to happen soon. We are building up, we're building guys were building an ml empire and we're going to get there. Don't worry about it. Uh, so

Speaker 2:          54:11          that's our decoder output. And we want, what we want to do is that decoder output. We want to format it into a valid prediction. We want to format into a valid prediction. And to do that, we're going to flatten the matrix and then we're going to get that prediction value. Okay? So we're going to find the matrix and get that prediction value because we want our tensors to be the same size and shape and the Dakota prediction is going to be our final prediction value. Okay? And we'll, we'll, we're going to do, and I'll talk about what we're going to do is, what we're going to do is, um,

Speaker 1:          54:46          okay,

Speaker 2:          54:47          uh, we, we're going to minimize a loss using cross entropy. So the quarter prediction and the actual value is what we're going to minimize. And let's talk about that. And the training we're going to function, we're going to be using is Adam to do this. So now he's four our helper functions. We defined the helper function. Uh, the, this is the data that it's going to generate. Okay? It's going to be this sequence of numbers and we're going to continuously generate that data using this next feed function for all those batches. We're going to add the end of sentence and pat it, uh, and we're going to continuously do that and feed it into our, uh, placeholders. And this is the training staff and we're just a bunch of print statements and we're saying run the session, um, compared to the loss and then print out the values and minimize a loss every time. So in one minute, let me explain exactly what's happening here. We have our input and our predicted output.

Speaker 1:          55:41          Okay?

Speaker 2:          55:43          Okay. So our member, these are our inputs are all of those generated values. Four, eight, five, four, eight, six. And we're patting them with Zeros and our predicted value. Remember because we, uh, our predicted value is going to be that decoded output and we, and what we're minimizing is the difference between the predicted output and the initial input. And that's the last that we are minimizing over time. So eventually it's going to look like this where it's going to be, the predicted output is going to look exactly like the input. And I have this plot right here, um, that shows just the last minimizing. Okay. So we uh, so that's, that's it for the code. Um, next time I'm going to have more time and I'm going to more, I'm going to have more time. Okay. So, um, and the code is all their undocumented and I'm going to add even more examples to the code for you. I have two other Python ipython notebooks and I'm going to add to this coach. You guys get extra help. Two questions and we're outta here. So

Speaker 2:          56:39          yes, there are girls here. We need more women in machine learning. Women, thank you for being here. We need more women in machine learning. Spread the word and be nice guys. Okay. We, this is a, this is a gender equal, everybody equal opportunity place. Do you rec? Oh my God. Okay. So do you, how do you choose learning rate for faster convergence? Remember, uh, Avi Group learning rate is one of those hyper parameters that we want to, that we kind of guess and check, right? Like all hyper parameters. But if you look at papers, papers are a good source, look at their results and copy them and also get hub code and I python notebooks because you can see the output. One more question and we're out of here. Okay. Is it possible to train a standard RNN on a non reverse translation problem where sentences are reversed? Yeah. Because it would still be a sequence of words, right? I don't see why it wouldn't, it would, it would still be a sequence of words. Uh, you might have to flip the, the direction. So maybe you would just have a backwards, a backwards, uh, layer instead of both a forward and backwards there. Uh, but yeah, you could absolutely do that. It's just a sequence. Okay. So that's it for the questions.

Speaker 2:          57:55          Okay. Thanks guys for showing up. I appreciate it.

Speaker 2:          58:01          We need to learn how to learn our hyper parameters. Okay. Love you guys. Code's going to happen in, in, in at noon. Special things to upload Vr for the space and to Assad. Balaban Ian for hosting this. Uh, and uh, please subscribe if you haven't, tell your friends, subscribe, subscribe, subscribe. I'm trying to hit a hundred k by April 15th. That's the goal. Um, okay. And then 500 k by the end of the year. I'm just saying it right now, so then I have to do it 500 k by the end of the year. For now. I've got to make this empire even bigger. So thanks for watching.