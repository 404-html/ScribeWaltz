Speaker 1:          00:00          When you use AI, a crash is just an opportunity. Hello world. It's a Raj and can we predict the price of Google stock based on a Dataset of price history? I'll answer that question by building a python demo that uses an underutilized technique and financial market prediction, reinforcement learning, but before we look at this problem from a reinforcement learning perspective, let's talk about the more popular algorithmic trading approach involving supervised learning so we can compare the two. If we can predict that the market will go up, we can buy right now and sell once it's gone up or if we can predict if it will go down, we can short then by once the market goes down. But the problem is what price are we actually predicting? There's no single price we are buying at the final price we pay depends on the volume that's available at different levels of the order book including the fees will need to pay.

Speaker 1:          01:00          A naive approach would be to predict the mid price, which is the mid point between the best ask and the best bid. Most researchers do this, but it's just a theoretical price, not something we can execute trade orders at, which means it could differ significantly from the real price we're paying. We also have to pay a transaction fee for whatever platform we use and because the market moves so fast, by the time the order is delivered over the network, that price could have slipped away. Supervised models don't take into account network latencies, these or the amount of liquidity in the best order book levels. The takeaway here is that in order to make money from a simple price prediction strategy, we've got to predict large price movements over longer periods of time or be smart about our fees and order management. That's a nontrivial problem.

Speaker 1:          01:55          There's also no policy involved in supervised learning. We predicted the price would move up and then it did luckily, but what if the price went down? What would we have done then sold huddled like a beast. What if the price went up? Then down, how do we choose the threshold of certainty to place an order? We need more than just an price prediction model. We also need a rule based policy. This policy will take as input our price prediction, then output a decision as to what to do with it as in place, in order, do nothing, dance, et cetera. How are we supposed to come up with the policy and how do we optimize the policy parameters and decision thresholds? Heuristics and human intuition can only get us so far. A solution to our problem is to use reinforcement learning. We can formulate the reinforcement learning problem in the following way.

Speaker 1:          02:49          We have an AI or agent that takes actions inside of an environment. At each time step, the agent will receive the input of the current state and take an action, then receive a reward as well as the next state. The agent will choose which action to take based on some policy. Our goal is to find a policy that maximizes the cumulative reward over some time horizon. In the context of our problem, the agent is the trader. It will decide how and when to make trades on the market. The environment would be the exchange that we use and that includes the other agents, both humanlike like Jordan Belfort and algorithms making trades that are a part of that exchange. In terms of the state of the environment, we can't observe the complete state since we don't know the details of the other agents in the environment like their account balances or what orders they're waiting on.

Speaker 1:          03:43          We have what we observe is not the actual state of the environment, but instead a derivation of it for us. That observation at each time step is a historical log of all exchange events received up until that time. In order for our agent to make decisions, we'll include a few other features like the current account balance, the timescale with which we make trades will matter. Should we do this over a period of days. How about nanoseconds? High frequency trading bots make decisions at nanosecond timescales. Neural networks are popular because they can learn complex functions if given lots of data and compute, but they're also relatively slow. They can't make predictions on nanosecond time scales, so they can't compete with Hoft Algorithms. So we want to act on a timescale where we can analyze data faster than any human possibly could, but we're being smarter allows us to beat the fast, but simple Hoft algorithms, it's a tradeoff.

Speaker 1:          04:46          Reinforcement learning, trading strategies in general are simpler than the supervise. The approach. Instead of hand coding or rule based policy or El, we'll learn a policy, no need for us to specify rules and thresholds like sell. When you are 80% sure of the market will move down then initiate skynet. It's all a part of the policy. It learns to optimize for the metric we care about. And this policy can be parameterized by a deep neural network making it more powerful than any rules a human trader could come up with. Because our l agents are trained in a simulation, we can take into account environmental factors by simulating them as well. We can simulate latency and give the agent a negative reward if it causes it to make a mistake. So let's get to the problem at hand. We want to develop an AI that will predict the price of Google stock so it can buy at a low price and sell at a high price, but this type of prediction isn't easy.

Speaker 1:          05:46          It depends on several parameters like the available stocks, recent prices and the available budget. The states in our situation would be a vector that has information about those features. Let's say we want the 200 last stock prices. Our state would then be a 202 dimensional vector and in terms of actions are agent can either buy, sell or hold. We've got our states and action, but we also need a policy. A policy for our AI consist of three rules. Buying a stock at the current price decreases the budget. Selling of stock, trades it in for money at the current price and holding does neither of these things. It yields no reward. The goal for our AI is to learn a policy that gains the maximum net worth from trading in the stock market. Let's try a technique called Q learning. So our AI will need to select an action and improve its utility.

Speaker 1:          06:41          Function to learning is a table of values for every state, the Rose and action, the columns possible in the environment. Inside of each cell of the table, we learn to value for how beneficial it is to take a given action within a given state. These are cue or a quagmire values. Just kidding. Q stands for quality. We'll start by initializing the table to be all zeros. Then as we observe the rewards we obtain for various actions, we can update it accordingly. The way we make updates to the cute table is by using the bellman equation. The states that the expected long term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. So we're reusing our own Q table when estimating how do update our table for future actions.

Speaker 1:          07:35          More formally, the equation states that the Q value for a given state and action should represent the current reward plus the maximum discounted future. Reward expected according to our own table for the next state we could end up in, so given a state the decision policy, we'll calculate the next action to take and improve the Q function from the new experience of taking an action. Let's start implementing the decision policy based on which action the AI will take for buying, selling, or holding a stock. To start with, we'll create a random decision policy and evaluate its performance. We can do this in the general decision policy abstract class. We can then inherit from this superclass to implement a random decision policy. All it does is pick an action randomly without even looking at the state. We can definitely do better than this neural network based cue.

Speaker 1:          08:30          Learning has proven to be a good strategy for learning a decision policy. We can set the hyper parameters from the Q function in the constructor as well as the number of hidden nodes in the neural networks. This helps define the input and output tensors and the structure of the network. It will also define the operations to compute the utility. Of course our optimizer will be great and descent, which the most popular in this space, the results don't look too bad, and there's a lot of potential to improve this model using reinforcement learning techniques like Sarsat or policy gradients. Applying our l two financial predictions is an exciting and largely untapped area of research. And I've got to link to all the code for you in the video description, so check it out. You made it to the end, but it's really just the beginning. It subscribe and let me help you on your journey to be the best for now, I've got to learn a Q function, so thanks for watching.