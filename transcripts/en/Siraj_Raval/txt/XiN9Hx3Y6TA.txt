Speaker 1:          00:00          Yo google, I'm gonna let you finish, but apple has the best hardware of all time. Hello world. It's Saroj and apple just announced its new iPhone xs, so expected demand for it to be massive. We can use a special reinforcement learning algorithm called policy iteration to help manage Apple's retail inventory and any others and make sure that the demand meets supply. I'll explain how in this video, let's assume that we are a retail manager for apple in San Francisco and there are two retail locations that were in charge of both locations, have different levels of demand in terms of number of iPhone xs purchases per day, and to rate of new deliveries from apple HQ. Since one of our retail locations, we'll have more demand than supply in terms of new iPhone xs orders delivered from Hq. We can move i-phones overnight from one location to the other.

Speaker 1:          00:59          That way we can make sure that we have enough iPhones at each location to maximize our profit. There's a reason where a trillion dollar company, right. The problem that we're trying to solve here is how many I iPhones should we move from one location to the next, given a particular number of i-phones at each retail store and how much should we expect to earn at both locations? This problem requires a real time learning strategy. One that adapts to a dynamic world. We can use the tried and true Mark Haub decision process framework to express this problem mathematically. Then come up with a proper solution. Once all of our variables are defined, our reinforcement learning agent will need to learn how best to optimize our problem using the variables it's given. A stake here can be considered the number of iPhones at each location. At the end of the day, the actions we could take, our, the net number of i-phones moved between the two locations overnight.

Speaker 1:          02:06          The maximum possible being five. Every time an iPhone is bought, we earn $10 in commissions. So that can be art reward value. It's a hard knock life, I know, but we also have a cost to moving. It costs us $2 in shipping fees to move an iPhone overnight from one location to the next, so we can consider that a negative reward and one time step in our case means a full business day. We'll also use a discount factor of 0.9 to prevent us from looking infinitely into the future. We instead look at a certain amount of time into the future. This measures how far ahead in time a reinforcement learning algorithm looks. A discount factor that's closer to zero indicates that only rewards in the immediate future are being considered a one that's closer to one like ours prioritizes rewards in the distant future. So that's it for our mark Cobian variables, but we should define the rest.

Speaker 1:          03:10          Let's say each retail store can only hold 100 iPhones at a time because we're feeling exclusive. We can expect an average of three and four iPhone purchases at the first and second locations respectively, and we can expect an average of three and two new iPhone deliveries at the first and second locations respectively. What that tells us is that the second location we'll have more purchases than deliveries were as the first location will have an equal number of purchases and deliveries. We also know one crucial variable beforehand and that's the state transition probability function, which is a key element in any Mark Cobb decision process. It defines the probability of transitioning from one state to another in a single step. We could even list out the transition probabilities and a matrix and consider it a state transition matrix. You can imagine that for some problems with potentially millions of possible states, this could get way too computationally expensive to compute manually, but there are ways around that that we'll talk about later on.

Speaker 1:          04:23          In statistics, the press on distribution can be thought of as an approximation of the true reality. It focuses on the number of discrete events or occurrences over a specified continuum that can be time, distance, slang, et cetera. We can define a quest on random variable x as a number of events in a given unit of time, which is going to be any positive whole value with no upper bound. In our specific case, we can represent the mean of the [inaudible] distribution as lambda and set it equal to the number of occurrences over a given interval, which depending on the retail store, we have two is going to be number of iPhones per day. This way we can mathematically define the expected probability of each outcome, which is the likelihood of x iPhones to be bought from a particular retail location. So because we know all the elements of our Mark Haub decision process beforehand, we can use dynamic programming.

Speaker 1:          05:27          We've used value iteration to estimate the optimal value function, which we could then use to estimate the optimal policy. But there's another technique called policy iteration that directly computes that optimal policy. Let's use that one. Remember, policies are a mapping of the actions and agent takes from a particular state. For example, if at the end of a business day, if there are 13 and 17 i-phones in stores one and two respectively, how many iPhones should the agent move between stores? There are lots of possible different actions the agent could take. So in order to solve this problem, it needs to know which action will result in the highest longterm return, so how do we determine what gives us the highest value and how do we come up with a better set of policies? Step one is to evaluate the policy and this is known as policy evaluation.

Speaker 1:          06:26          It's the process of taking a policy and calculating the return of every single state ace on following a particular policy. When we arrive at a true value function for our policy, we can try and improve it makes sense, but we can definitely do better than our initialize policy that does nothing. Policy Improvement then is the act of looking at all actions and agent could take from a given state and then choosing the action that gives the highest return. With respect to our value function or system. We'll look through these two processes iteratively. This is policy iteration. It will calculate the true value function for a policy. Then improve the policy based on the value function before going back and refining the value function again, thus improving the policy. It'll just keep doing that until it arrives. At the optimal policy for our solution, which will consequently also give us the optimal value function in our code will have an iterator that examines each state, does a one step look ahead over the action we take as defined by the policy and all the different ways that our algorithm could take us, looks at all the successor states that we could end up in.

Speaker 1:          07:45          Then sums the probabilities of being an each state which is going to give us an updated returned for the particular state we started from. We can call this update the expected update due to the fact that it's the result of going through every possible next state instead of a sample. If we move i-phones from one store to another, we don't know exactly how many I phones will be bought or delivered and thus how many phones we'll have at the end of the day. What we can do though is calculate the probabilities of different outcomes happening and some those probabilities together to come up with an approximation. We will replicate the approximation of it, the state value from the last iteration with an updated version and do this for all states positions between when we first start evaluating and when we end up settling on a true value function. There are many value functions we'll have to create.

Speaker 1:          08:47          This iterative application of the bellman expectation equation is supremely useful. Once we've fitted the function to the random policy we initialize at the beginning, which by the way was choosing to not move. I phones from one store to another, we're ready for some policy improvement. Improving the policy involves testing actions at each state and choosing the best action from them. And unlike evaluation, we iterate through all the actions. Then have a list of returns to look over. So when we plot out the results from our agent, we can see a nifty diagram that shows what our policy would do given a certain number of i-phones in each store. It wouldn't switch iPhones to other stores generally, which is made clear by the huge concentration of policies in the middle of the graph. But for a number of cases where store one has more iPhones, then store to the optimal policy would be to move I iPhones to store too and there are some scenarios where the optimal policy would move I iPhones from store two to store one but the threshold is higher and if we consider our problem statement it makes sense.

Speaker 1:          10:01          We already know store two has a higher purchase rate than both its own delivery rate as well as store ones purchase rate. It's pretty cool that no matter where we started with either our value function or our policy, we ended up with an optimal value function and policy. All of what we've done so far, dynamic programming that is assumes we have complete knowledge of the environment or all possible transitions, but what kind of technique could we use if we don't block chain? Just kidding. That's a topic for the next video. Three closing points here in dynamic programming, policy iteration is a modification of value iteration to directly compute the optimal policy for a given mark cop decision process. Policy Iteration consists of two steps, policy evaluation and policy improvement. And while value iteration is a simpler algorithm, then policy iteration, it's more computationally expensive. You just keep on learning, don't you? I love it. It subscribed to prevent the apocalypse for now, I've got to iterate on my skills, so thanks for watching.