Speaker 1:          00:00          Data is sacred. Every hour of every day. A new sensor is connected to the web. Every trip, every memory, every creation, every discovery joins the ever-growing web work building together amidst a world of ever growing skepticism and falsehood. Data is true. It's transparent, provable. Most of it is unstructured streams of raw numbers being a mass that dizzying rates, but by applying intelligence to it, we can find the patterns and connections that matter. We can find the meaning hidden in the numbers. The Economist says more for you is less for me, but the lover knows more for you is more for me too, and when we visualize the right data, it gives us that most precious feeling at the intersection of art and science. Wonder

Speaker 2:          00:41          hello world. It's Saroj and today we're going to learn how to preprocess a Dataset. Yes, preparing data is one of the most important, yet most overlooked parts of the machine winning pipeline. A lot of introductory tutorial is just have you import a preprocessed version of a Dataset like handwritten characters or movie ratings and just a single line of code. We go, we're all these, not that easy. Once you've decided what problem you're trying to solve or you have a question that you want the answer to, it's time to find the right Dataset.

Speaker 1:          01:11          I want to know what happened to the plans they sent you.

Speaker 2:          01:14          I stored them on Microsoft Azure. The predictions your deep net makes are only as good as the data you give it. Garbage in, garbage out, so you want to make sure your data is relevant to your problem. There are tons of resources to find publicly available datasets and I've linked to some in the description. The defacto standard format for data is CSV. Most software packages out there deal with data in that format and you can convert your data into CSV format just as easily. There's so much we could potentially do to our data, but there are three key preprocessing steps for every dataset will cover cleaning, transformation and reduction. We're going to look at three different datasets and go through these steps for each one to get them ready to be fed into a model. So let's start with our first. The first Dataset will use his music based.

Speaker 2:          02:01          This data was collected from a game called tag attune in the game to players listen to a song and tack it with Shauna is an instruments that they think are relevant when the song is over. The player who had the most correct tags gets a point, so I would win every time. Our data set has 25,000 songs with the correctly labeled tax. We want to train a model on this data so that given a new song, you know correctly classify, it's Chaunra. We'll import pandas to help us parse this dataset. Then the read CSV function will let us store the data in the two dimensional pandas. Data structure known has a data frame. Data frames are easily modifiable and we'll call our variable new data. Let's explore this data first shall we will display the first five rows using the head function with five as a parameter.

Speaker 2:          02:42          So basically each row is numbered, has an id and then either a one or zero next to a tag to indicate whether or not the given MP3 has that tech. It seems simple enough we can use the info function to get some more data. Only 38 megs. So for our cleaning step, is there anything we need to do? Not really. Each has a simple binary tag. It's consistent and luckily our data does not have empty values, but my soul does. We can move right onto the transformation step. What are some modifications we can make to this data that will make it easier for our model to understand? Well, notice how a lot of the tags are pretty similar sounding like female singing female vocals. We can generalize these features into one feature called female. Let's create a two dimensional list of synonyms that we find in our data.

Speaker 2:          03:27          Then we can merge them and drop all the other columns except for the first one for each synonym list in our matrix. Let's get the Max values from each of the features and add them all to our person and I'm in our data frame object, which will effectively merge the values into one column. Then we'll drop the rest of the features from the data frame. Now we've got more generalized features. Next for the reduction step. What can we remove from this data that's not necessary? Everything seems pretty solid, so let's go ahead and split it into training, validation and testing sets that we can feed it into our model. Notice how in this example, I'm not thinking about which features to use and which not to. Before deep learning, we had to pick the right features to use to feed our model, but deep neural nets learn high level features from whatever features we give it, it decides for itself what is relevant to the problem from a Dataset architecture.

Speaker 2:          04:14          Engineering is the new feature engineering. The second data set we'll use is a collection of network connections and they're labeled normal or abnormal. The abnormal connections are intruders trying to break in. We want to be able to classify a connection given the set of other features. When we look at this data, it seems pretty dense, no missing values. Nothing really jumps out as an outlier, so let's skip the cleaning step and move right on to transforming it. Our numerical features are all operating on different scales, so we should normalize them to ensure each feature is treated equally by our model. After storing our data into a pen as data frame it's, I can't learn has a handy sub module called standard scaler, which we'll import that initialize. After that we're ready to move on to our reduction step. We got a lot of features and there are probably a lot that are highly correlated. We could use a technique called dimensionality reduction to reduce the number of features we have. This will also let us visualize our data in twoD or Three d space. This doesn't mean that our model will be more accurate necessarily, just that our data is easier to read. One method of doing this is called PCA, which stands for Porsche Club of America, way prong definition, principal component analysis.

Speaker 3:          05:26          David, it's got so many teachers have squash, a mint

Speaker 2:          05:30          creatures

Speaker 3:          05:31          are so normalized. Then our correlation matrix size pool, Eigen vectors and I's sort them. How many dimensions do I want? I select that many items up front projection matrix from them and use it to turn my data three dimensional, plot them so I could judge them.

Speaker 2:          05:50          So let me summarize this process again. Let's say we had four features and we wanted to reduce them to adjust to using PCA. There are five steps to this. The first is to normalize the data. Once we have it stored in a variable, then we'll want to compute a covariance matrix. To construct this, we compute the covariants between each feature with every other feature. So we'd subtract the mean from the feature Matrix, calculate the transpose and multiply it by the feature matrix minus the meat. Then we take that whole value and divide it by the number of features minus one. This gives us our covariance matrix. Next we'll perform eigen decomposition on it to get the eigenvectors and eigenvalues. I again, isn't it such a fun word to say? Hiking vectors are the principle components of a dataset. They give us the directions along which heart transformation act, the eigenvalues give us the magnitude of each.

Speaker 2:          06:39          We'll sort both in descending order, then create a matrix out of that. We'll use this matrix to transform our original Beecher matrix via the dot product. We could then plot our data in two d space and use these principal components to replace are many features. Let's look at one more dataset. This time for airline prices, for flights between New York and Paris, we want to predict the ticket price from just a departure date. We've got departure and arrival dates, airports and flight prices up to 120 days before departure. Notice how we've got quite a few missing values in our data, so for our cleaning step, we could remove these values, build them with Zeros, build them with the average price across all the days, or try to predict them using a learning algorithm. Let's go ahead and calculate the average price for each row across all days using the mean function and then we'll iterate through the data and if it's no, we'll replace it with the mean price, then we can smooth our data.

Speaker 2:          07:29          That means finding outliers in it that we can remove, define these. We could run clustering or regression algorithms on certain values to find the outliers and then remove them or just remove them by eye. Since our data set is small, let's do the latter. No need to reduce our data. This seems like a good set. Let's break it down. There are three steps to preprocessing a dataset cleaning, transformation and reduction. Deep learning learns to relevant features from our data. So architecture engineering is the new feature and generic and principal component analysis is a popular dimensionality reduction techniques that can be implemented with psychic learn. The winner of the coding challenge from the last video is Charles David Blot Charles David Hughes, just non Pi to build a three layer neural net capable of predicting an earthquake and he used a random search strategy to find the optimal hyper parameters for his model wizard of the week and the runner up is city Jack Grove. He use tensorflow for his prediction using just three inputs. The coding challenge for this video is to use a dating dataset to predict if someone gets a match based on their personality traits. Details are in the read me your get humbling in the comments and I'll announce the winner. Next video, please subscribe. If you want to see more videos like this checkup, this related video, and for now I'm going to predictive roses. Really smell like poo poo poo. So thanks for watching.