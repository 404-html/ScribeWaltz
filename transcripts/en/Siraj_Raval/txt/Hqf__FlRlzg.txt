Speaker 1:          00:00:03       Yeah.

Speaker 2:          00:00:04       Hello, hello world. It's the Raj, welcome to this third live session that I've ever done. I'm excited to be here. Um, I'm going to start off with a Q and a section and this time it's going to be a little more structured. So I'm going to start off with a five minute Q and a section. Let me mute myself. Here we go. I just muted myself. Okay. So I just muted myself and we're going to start. So five minute Q away and then, uh, we're just going to go to concepts coding and then summary and then a less a Q and a at the end. Okay. So let's go. Starting now. This is not beer pong. I know. It should be. That would be cool. Maybe in another video. Thanks for liking my hair. I work hard at it. I die. It actually, this is, this is died. It's dyed silver.

Speaker 1:          00:01:01       MMM.

Speaker 2:          00:01:04       Hi everybody. Hi. Bartos. Hi. Ivy Era tensorflow. We're going to be using tensorflow in this video. Absolutely. I have tried to be omitting a generative adversarial network. Those are amazing and they're going to do good things for generative models in general. Uh, do I do any freelance work? Yeah, I mean sometimes clients approach me to make videos for them. I'm trying to just do this full time. Like I don't want to like, you can't control me. You know what I'm saying? Like I'm just wanna I just want to do the youtube channel full time. There's self driving car team is the only one that I'm like, okay, I'll do something for them.

Speaker 2:          00:01:43       Hi, what is up? Dion and Mitch. I'm going to show us how to build recurrent neural network in tensorflow next time. What are the dependencies? Go to the top of the chat screen. I talked about them there. Thanks. Mokolo hi from Argentina. I want to visit Argentina and learn salsa. I know. Sorry. Oh my God. I meant uh, tango. That's what I meant. All right. Uh, all right, so here we go. Two more minutes. Yes, I'm Indian American. Yes. I'll never get it. You're right. I'll never get your nickname. Will you be using cue learning? Good call Lord. That's exactly what I'll be using. What is Pong? Neural network. Okay. So Pong neural network is we're going to build calm from scratch the game, you know, the two paddles and the ball in the middle. And we're going to train a neural network to get better at it over time.

Speaker 2:          00:02:38       Uh, hi [inaudible]. Hi Antonio. Brazil. Sounds cool. Uh, then big niche. I'm good. How are you? A good pdf tutorial on CNN for beginners. Uh, wait til later, I'll, I'll send you a link. I'll send you a link. I'm the best massively online course for total beginners and machine learning, my channel. Uh, just implement all the code and each of the videos. And, uh, if you do that, you're going to get good at no time. Start from the very beginning of my videos. Um, let's see. Game on game on exactly high Kush. Uh, I am using pylon python three. And can you do this using Java? Yes. You could do this in Java. Have I tried implementing two d models? Restricted Boltzmann machines. I've done videos on restricting both midstreams, Microsoft AI or Google AI, Google Ai. Of course, my mean, Microsoft did hire a 1500% AI team recently, but like, I mean, you know, we'll see how that goes.

Speaker 2:          00:03:49       Okay, so that was it. Five minutes is over. So now we're just going to get started. Okay. So now the next part is me explaining the concept. Okay. So what we're going to do, what we're going to do is build a deep Q network to read and Pixel data from the game upon. You guys know the game of Pong, right? Two paddles and there's a ball in the middle, right? And it's just, you're just trying to get the ball to go pass the paddle. That's it. And so we're going to have to, we're going to have one evil Ayaan okay. This is an evil I, that's the guy we want a beat. It's just going to be really smart and it's going to know everything right from the start. And then there's going to be us and we're going to be super dumb at first.

Speaker 2:          00:04:26       Okay? We're going to be super dumb, but through trial and error, but through trial and error, we're going to get better over time. So it's going to, it's going to notice where the ball is going and if it's losing, it's going to say, okay, I need to get better. So it's going to be using something called deep cue learning. Okay? Deep Cue learning. And guess who else did this deep mind? Deep mind did this. All it does is it reads in the Pixel data. It doesn't know anything about the game. It reads in Pixel data and the score. That's it. Those are the two inputs to the Deq network. If we didn't pixel data and the score and based on those two things, it will get better over time. Reinforcement learning, trial and error better over time. Okay, so, and we call this the agent environment.

Speaker 2:          00:05:12       Luke the agent environment. Look, looking at the environment. The agent sees if there's a reward or not and gets better and better over time. Okay? That's what we're going to do. And uh, Charles, I had coffee but not enough. Uh, but anyway, we'll see. Okay, here we go. Okay, so that's the explanation of the concept. That's a two minute explanation and now we're going to get started with the code. Okay. We've got a lot of coats go over because we are building this palm game from scratch. We are building it from scratch and then we're building the reinforcement learning algorithm, uh, using tensorflow. So there's a lot of coats to go over. That's about 300 lines I want to say. Uh, so, uh, let's just get started. Okay, here we go. Never enough coffee. Um, exactly. So, all right, here we go. I'm going to screen share and we're just going to get started because we have a lot of code to cover. Let's do this. All right. Screen sharing time.

Speaker 1:          00:06:16       Screenshare Holy Shit. Hold on. Oh my God. Moving this. We'll put this

Speaker 2:          00:06:34       right here so I can see it.

Speaker 1:          00:06:37       All right.

Speaker 2:          00:06:40       Screen section. Exactly. Exactly. I'm going to make this really big. This text. Really big. Okay. I know you guys like that. Big Text.

Speaker 1:          00:06:54       Boom, boom, boom, boom, boom.

Speaker 2:          00:06:57       Getting bigger and bigger. No, this isn't in sublime text and writing this in sublime text. Is this big enough? I was that is this big enough? I'll wait for a reply and then I'll keep going. Is this big? Oh, bigger. Okay.

Speaker 1:          00:07:19       How about boom, boom, boom, boom. Okay, here we go.

Speaker 2:          00:07:26       All right, here we go. Here we go. Here we go. That's as big as we're getting. Okay, so the first thing we want to do is build a pong. We're going to build a pawn class, okay? We're going to build pawn from scratch. All right, here we go.

Speaker 1:          00:07:39       Okay.

Speaker 2:          00:07:40       Uh, no. Texas. Big like my mom. Okay, here we go. So we're going to import pie game first. This is going to help us make a graphical user interface games. And Python Pie game is a great library for making any kind of two d game and python. Okay? Whether it's pong, whether it's snake white, their rates, you know, a side scroller adventure game, like even Super Mario. You could build it in pie game. I love it. Check it out. Read the docs. All right, so this is how we're going to build upon the next dependency is going to be random. All right? A pie game and random, and I'm going to upload a copy of that. I'm going to save a copy of this video and save it to my channel. So check it out later if you, if you have to go. Okay. And so the random is going to help us define which direction our ball is going to go in. Okay.

Speaker 2:          00:08:27       All right. So let's start off by defining some of our variables defined variables for game. Okay. The first one is going to be the frame rate. Okay? So let's just say 60. That's going to be the frame rate for, uh, how fast our game is going to move. Next, we want to have the size of our window, our game window. Okay. So that's gonna be the window with and which is going to be 400 pixels. Uh, and then window height, which is to be 400 as well. So it's going to be a square window, same width, same height. All right. The next step is to define the size of our paddle. Okay. So that the paddle is, uh, you know, the stick that the ball hits. Okay? So paddle with a, and they're going to be the same for both us and the evil Ai. They're going to be the same size, so we're just going to have to define it once. We'll say 10 pixels and then uh, we'll define the height at 60 because the height will be, it will be longer than y. Right. Okay. So that's the size of our paddle. And now we want the size of our ball.

Speaker 2:          00:09:35       Size of our ball.

Speaker 1:          00:09:38       Yeah.

Speaker 2:          00:09:39       Okay. So the size of our ball, we'll be uh, let's say ball with, and then ball height. It's going to be 10. All right. So that's the size of our ball. And now we want the speed of our paddle and ball. Okay. So how fast do we want our paddle and ball to move? Yeah, the size of our ball. Because our ball is actually not a circle. It's a rectangle. It's a very small rectangle. All right. Yeah, please. Ball jokes. Go in the comments. Keep 'em up. I love it. Uh, I love it when you have big balls. Here we go. So the speed of our panel on our bolts. We want our paddle speed going to be, let's take two and our ball speed, which is the x direction,

Speaker 2:          00:10:35       uh, uh, of our bottle. And then our ball. Why speed, which is going to be two. And so x and y speed. We have two different speeds for our ball. What, which direction? X and Y, right? There's an x, y coordinate plane. And there's two directions in pike game, right in our window. We're having a ball with these bald jokes, aren't we? I love it. That was clever to top 1%. Okay, so now we want to define our color spectrum. So we're going to have our RGB colors or a paddle ball, paddle and ball. We're going to have, let me just go up. Uh, what is it we're going to, we're going to use white, which is going to be, what is the color for white to 55 to 55 and two 55. That's going to be the color of our paddle in our ball. Um, uh, and

Speaker 2:          00:11:31       now, uh, we're going to define are the black, which is the background for our game. Zero, zero, zero. That's it. Okay. Those are our colors. Now let's initialize our screen, initialize a, we're going to randomize or let me, we could randomize our speed. I mean, it doesn't really, I mean we don't want to get that complex right now. So let's just, um, initialize our, uh, screen. So we're going to say screen is going to be, so we're going to use the pipe in module two, initialize our screen. And so we're gonna use the set mode, uh, method of display to set the size. And we're going to use our variables that we've created, our width and our height to define this screen. Okay? That's our window with and our window height.

Speaker 2:          00:12:20       Okay, great. Now that we've initialize our screen, let's, uh, draw our ball. Okay. So now we can write our first function here called drop ball. We're going to draw our ball. It's going to be a small rectangle. Okay. And did the two parameters are going to be the exposition and the wife position, uh, that we want our Deval, uh, have, we want to, um, draw our ball in? Okay. So our ball is, we're going to define our ball variable and we're going to use the wrecked function of Pie game to, uh, create that rectangle, that ball rectangle. Okay. Um, and so let me make sure I didn't miss spell this and I'm going to say let's, uh, initialize our rectangle with art screen as a parameter. And then, um, sorry, not our screen. We're going to use our exposition or wipe position that we input as parameters where we want to start off with. And then the width or with uh, four. Oh, Suraj for president. I would, I would, I would be down actually I would be down. Uh, that would be awesome. I would a everything, I would just make everything run on code. It would be amazing if I was precedent. Okay. So future plans, future plans. Let's make this youtube thing work first, ball width and then ball height.

Speaker 1:          00:13:56       MMM.

Speaker 2:          00:13:58       Uh, you guys are so awesome man. I really, I love you guys. Seriously. Like I had my ups and downs in life. Like I've been feeling pretty down lately, but like just the fact that like, you know, I've got you guys, it makes me feel better, so, so thank you. Okay, so here we go. Make America learning. And I know I've got to focus. I know I just, the conference are so amazing. Oh, okay. Anyway, so that's our ball. And now we're going to draw our ball. So Pie game dot draw. Okay, so we're going to draw our ball using the draw function. We've already defined the rectangle and we're going to use our screen, our color, and then our ball that we just defined to, to find our ball. Okay. That's it. That's how we draw our ball.

Speaker 1:          00:14:38       Okay.

Speaker 2:          00:14:39       Okay, now we're going through, thank you. Now we're going to draw our first paddle on. Our first pedal is us. Okay. That's us slash R as us and we're going to learn over time. Okay, so our pedal, we're going to draw a paddle one and we're going to use the first why the position as our parameter. Okay? So we're going to do the same thing. We're going to create our paddle using the Pi. Gamed, wrecked meth method. And then we're going to use the, we're going to use our paddle buffer as the first parameter, and then our y position, and then our width and height. Okay. That, so that's going to be how big it's going to be and where it's going to start. And the reason I used a buffer is to show that it's not so that it doesn't hit the edge of the screen. All right? Blah, blah, blah, blah, blah. Okay, so that's our ball. And now we're going to dry. Just like before. It's a pie game. Dot draw.

Speaker 1:          00:15:34       Okay.

Speaker 2:          00:15:34       MMM. Dot. Recht and it's going to be white as well. Screen white paddle one.

Speaker 1:          00:15:43       Okay.

Speaker 2:          00:15:44       All right. Um, uh, okay. So that's our first paddle. All right. Now we're going to do the same thing for our next paddles and draw paddle to, that's going to be our evil Ai. That's the guy we want to beat. He's going to be really good. We want to beat this guy. All right. He's on, he's on some shit. Okay. See, I never know if I can or cannot occur or if I should. So I just do it anyway because like whatever, you know what I'm saying? I have nothing to lose. Okay, so paddle to wipe position. All right. So, uh, okay. Got to focus here. I'm going to answer you guys but I've got to focus. So now paddle to is going to be um, uh, we're going to do the same thing. We're going to draw it using direct function and then we're going to um, say window with minus paddle buffer so it doesn't hit the edge of the screen. And then we're going to say a minus paddle with right petal with, and then paddle to y position and then paddle. And so then the, this, the size of it. Paddle with the paddle. Hi. Okay, I got to keep going here. All right. And then we're going to drive high game dot draw direct and then the screen on their color and then our panel. Okay.

Speaker 2:          00:17:23       I should use it. Linux machine. I agree. I agree. I agree. I will eventually. I, I will. Okay, so now here comes the heart function. Okay. Okay. Here it comes. The heart function. We're going to update this ball. Okay. It says the function where we update the position of the ball. So we're going to say update ball and we're going to take in parameters. The parameters are going to beat the position of our, uh, both of our paddles are ball and the direction that the ball is going. Those are going to be our perimeters. So we're going to take, those were going to say paddle one.

Speaker 1:          00:17:56       Okay.

Speaker 2:          00:17:58       My position paddle too. Um, why position?

Speaker 1:          00:18:06       MMM.

Speaker 2:          00:18:08       And then Paul Exposition, we're going to say the ball wide position. Those are the, that's the position of our, our ball position. The window with is spelled wrong, right? I'm going to blind 40. Let's see, where is it? When did you do that? There you go. Thank you. Appreciate it. That's spelled wrong. All y position, ball exposition and Eh, ah, line 40. You caps are. Thank you. Thank you. Appreciate it. Appreciate it. Boom, boom. All right, so that's a position of our ball and then we want a direction of our ball. The ball's direction is going to be the ball x direction and then the ball. Why Direction? Okay. So those are our parameters. And now we can just go ahead and get started.

Speaker 2:          00:19:00       I am 32 instead of racks. Typo fixed 36 capital also capital Typo fixed at that. That the, okay. So now, okay, we have got to get through with it. Okay, so we're going to update the x and y position. Update X and y position. So the balls, so we went up to the x and y position of our ball. Okay. So that's, that's how we're updating the speed of our ball and where it is, it's a ball exposition. It's going to, we're going to take the original position or we're going to add the direction that it's going in and we're going to multiply it by the speed of the ball. Okay? And so that point, you update the position of our ball. Okay. Depending on how fast it's going and where it's going. All right, so Paul White position plus.

Speaker 1:          00:19:48       MMM,

Speaker 2:          00:19:52       a ball I direction times ball. Why speed? Okay. All right. There we go with that. And we're going to say the score, it's going to be zero. It's going to start off with no score. I mean the score is going to be zero and we're going to update the score and we're going to return the score as well, depending on

Speaker 2:          00:20:15       all right, I'll try not to explain every step. I'm balls. Exactly. So now we're going to check for a collision. Okay, so this is going, going to check for collision. And if the, if the ball hits the left side, then our learning agent, we'll switch the direction, uh, or the ball's going to a ball is going to switch the direction. Okay, so it's going to be a bunch of if statements here. So we're going to say, um, if all exposition is less than the paddle buffer plus paddle with and the ball, why position plucks the ball height is greater than or equal to the paddles position and a balls life position. Um, minus the ball height. It's less than or equal to the paddle. One stitching. Why position plus the paddle heights. All right, so this says, okay, so I'm going to write down what this means. This means we're going to check for a collision. And if the ball, it's the left side.

Speaker 2:          00:21:39       Um, if the ball golfers, a little hits the left side, then we then switch direction. So we're going to say, um, a ball x direction. It's one. Okay else else? If, if it doesn't, so then the ball's position, it's less than or equal to zero. Then we're going to, um, having negative score. So we're going to say we're going to change the direction of the ball. Um, and then we're going to say we're gonna make the score equal to negative one. Okay? And we're going to return and we want to return whatever we've just calculated. So we're going to return to score. We're going to return the paddles position for both of them. We're going to return the ball's position. We're going to both the x and y position, and we're going to return the direction of the ball. We want to return. That's the x and y direction of football. Boom. All right. This is crazy. Okay? There's a lot of variables going on right now. Okay. Um, and then, all right. Can't see code. I know, right? It's

Speaker 2:          00:22:55       okay. Just, we're not gonna be able to see all the code because we have a, we have to switch between like how big the text is and like if there's a trade off between how big the text is and uh, how visible it is. Thank you. It's the score is negative one. Thank you. All right, now we want to check if it hits the other side. So if ball exposition is greater than or equal to the window with window with minus paddle with his paddle buffer and the ball, why position plus the ball height greater than or equal to paddle to y position and ball. Why? Position minus. Okay. Oh,

Speaker 1:          00:23:48       okay.

Speaker 2:          00:23:52       Ball minus ball height is greater than equal to paddle too.

Speaker 1:          00:24:02       Yeah.

Speaker 2:          00:24:03       Why position plus paddle height. Okay. Thank effing God that we are done with those two statements. Okay. Fuck. Um, right. Okay. So we did that and now we're going to switch the direction of the ball. So if it, if it's, so this is basically, it means if it hits the other side then we want to switch directions again. Right? So now we're accounting for both sides that it hits. So ball x direction equals negative one. All right. And then else, if it doesn't hit it so else, if the ball x position, it's greater than or equal to the window with minus ball with.

Speaker 1:          00:24:49       Okay.

Speaker 2:          00:24:50       Then we want the positive scores. So at the Baldor x direction equals negative one. Um, and the score is one. And now we're going to return that same shit, right that we returned up here. That same thing. And that score, it's here. And now I'm going to look at the comments for a second. Boom.

Speaker 1:          00:25:13       Okay.

Speaker 2:          00:25:14       Uh, hi. Hi. Uh, what am I saying? There's too much lag. All right. This salty ball exposition. Alright, here we go. So now, um,

Speaker 1:          00:25:28       okay,

Speaker 2:          00:25:30       cool. So now, um, okay, so let me just explain what I just wrote and let me, let me make it smaller for a second. Hold on. We're, we're checking for a collision here. Okay. So we're saying, uh, if the ball hits the left side, then switched directions. If the ball hits the right side, then switched directions again. So it's basically the logic is, you know, if it hits one of the sides, then we want to switch directions for the ball. All right? That's so that's all. All right. Break the line, break the line, break the line. Great. The line. Lots of line breaks here. Okay, there we go. All right.

Speaker 2:          00:26:13       And I'll keep the font size and I'll break this line and I'll break that line and I'll just fit it all in here. All right. Boom, boom, boom, boom, boom, boom, boom. All right. Okay, so now, um, oh and there's one more thing we need to account for. What if the ball hits the top? What if the ball hits the top? We don't know what to do then. Right? We have to account for that though. If the ball's why position, uh, it's who I center equal to zero, then we want to, um, make that wide position equal zero. So then we want to change the direction of it. Uh, and we want to say,

Speaker 1:          00:26:48       MMM,

Speaker 2:          00:26:51       I'll see if all white position less than or equal to the window height minus the ball height. All right? So if it hits the bottom, then we want to change position as well. So ball by position equals window height minus ball height. Okay. And so then we want to make the white direction all y direction equals negative one. And eventually we returned the score. It returned the score. This is cute player Pong. Exactly. But one of them is an AI, an evil AI that's already really good. And one of them, it's an AI that uh, we want to make better. Okay.

Speaker 1:          00:27:43       Huh?

Speaker 2:          00:27:45       I'm going to fix the colon's. Yes, thank you. Boom. Um, all right. Anyway, so now, okay, that's it for that method. Thankfully we are done with that method. Oh my God, that was so intense. Okay, so now we need, we have two more, uh, start to more functions to do. Okay. So let's do it. Yeah, it's AI versus AI is a bad AI and then a good AI. That's us. That's us. We're going to get better over time. It's already going to be good from the start. All right, so here we go. Update paddle one. Kind of update the pedal position if we move up. So if the ball moves up, so that's going to be [inaudible]. So if the action is one, so it's depending on where we're moving. It's like which direction we're moving in. We want to update the position of our paddle, right? So we could go up or down, right? So the action is just an array of where it's going. Right? So up or down. All right. So, um, paddle one.

Speaker 1:          00:28:53       Okay.

Speaker 2:          00:28:54       Why position equal. So we're going to update the physician of our paddle, um, and we're going to subtract the speed of it. All right. And we're going to add this colon. So now if it moved up, so now it's, if it moves down.

Speaker 1:          00:29:13       Okay.

Speaker 2:          00:29:19       If action as two equals one. So if we try to move down and we went update the position to move the paddle down. Okay. Paddle fine. Why position? Close paddled speed. Yeah, don't let it move off the screen. Okay. So if the paddle, so if it, if it tried to go above where we want it to go, Dan were, so lesson zero then we're going to update that. Let me see where we are right now. Okay. Cool. So paddle one by position equals zero. And then I'm going to say battle on why positions greater than window height minus paddle height.

Speaker 1:          00:30:21       Okay.

Speaker 2:          00:30:22       Okay. So now we have taken into account every possible scenario that this, this our paddle could move in. Okay. All right. Um, thanks acrylic love back from San Francisco. So we want to do the same thing for paddle too, right? So I'm just going to copy this method or function and we're gonna do the same thing but for paddle to right. And the differences were going to say, um, all y position. And uh, this is going to be pedal to battle two oh two.

Speaker 1:          00:31:05       Okay.

Speaker 2:          00:31:08       All right. Now we can define our pong class. The game of Pong, right? We are moving really fast here where we haven't even gotten to our AI yet. Let's define our punk class. F in it.

Speaker 1:          00:31:24       Okay.

Speaker 2:          00:31:25       Self 64 Colin Colin at 64. Where are you talking about? No, I didn't. Okay, so now we want to define our game. Okay. So this sort of game class, this, well, now we have our functions. We want to define our pong yet. Okay, so we're going to start off by saying um, um, by saying random number for the initial direction of the ball, where do we want our bolt to go? We'll define a random number at using the random grant in function. Um, I know it's a pretty long program. This is a pretty long program. I'm thinking like let's just keep going. We're just, we just need to keep going here. So random answer. We're going to start from Jared and ninth. It's going to be a random number between zero and none. Okay. And now we're going to keep the score in order to keep the score. We're going to define Itali variable self dot. Cali equals zero. Now we want to initialize the positions of our paddle positions of our panel. All right, so now we're going to say itself. Dot. Padel one position is window height divided by two minus paddle hides divided by two and self dot Padel.

Speaker 1:          00:32:42       Yeah,

Speaker 2:          00:32:42       is window heights. So it's going to start off in the middle. Basically. That's why we're dividing by two. We wanted to start off in the middle window height by two minus paddle height divided by two.

Speaker 1:          00:32:56       Okay.

Speaker 2:          00:32:56       All right, so now we wanted to find the ball direction all direction, definition, and we're going to say itself. Dot. Ball x direction. It's one self doc ball. Why Direction? It's going to be one. And then the starting point for our ball. The starting point will be the self doc ball exposition. It's going to be the window height divided by two and then so it's kind of start off in the middle as well, right? So ball with divided by two.

Speaker 1:          00:33:32       All right,

Speaker 2:          00:33:35       now we're going to say um, two more functions. Okay. We're going to define the get present frame. All right. These are two more functions and we're done. Cool. In 94 and one oh nine I remember that. Okay, so, so now we want to get the present frame. So that means the frame that we are currently at, why are we defining this function? Because we want to feed our reinforcement learning algorithm, the pixels, right? So we need, in order to get the pixels, we need to get the pixels from the game. All right? So for,

Speaker 2:          00:34:21       okay, so for, for each frame we need to call the event queue. All right, so pie game dot event dot punk. Like this is going to be like if we need to repaint our window or anything and we're going to, we're going to repaint if we need to run anything. So that's why we have pump. So now we want to make our background black. We want to make our background black, so we're going to run our fill function. Okay, so screen dot Phil and we're going to say black. So that's what's going to be our background. All right. I know it might, it might just explode when I run it, but that's okay. Draw. Now we're going to draw our paddles. So we're going to say drop. So we're going to call the functions we just made, right? We're going to draw our paddles and we're gonna use our [inaudible] physician first.

Speaker 2:          00:35:09       And then we're going to draw the next paddle paddle too. And we're going to say self dot Padel to white position. Now that we've drawn our paddles, let's draw our ball and guess what we, we made a draw ball function, let's call it. All right, this is everything we've just done and we're going to say it's going to be at the exposition. We're going initial, we're going to initialize it at that exposition and we're going to use that wide position as well. Okay? Now we've grown our ball, we've done, our paddles are drawn ball, we'd on our screen and now we went to take all of those pixels from the entire game and we're going to return that. And how are we going to do that? Well, let's define what's the binary variable called? Let's, let's say we want to get pixels. So let's define our variable call image data. Okay? We're going to define a variable called image data. And image data is going to use the use the array three d function of pie game to get the pixels from the screen. So we're going to say surf array dot array three d. All right. And we're going to say pie game dot display dot get surface.

Speaker 2:          00:36:16       That's going to get all of our pixels. Now we want to update the window and we're going to say pie game dot display dot flip. All right, Leonardo, I'll definitely take that into account. We're going to update our window and now we want to return the surface data. Return the screen data. All right, so we're going to return image data.

Speaker 1:          00:36:45       Okay.

Speaker 2:          00:36:46       Um, exactly. Okay, so that's, that's um, that's going to return the screen data and we want one more. Okay. Uh, we're going to want one more function to get the next frame. Uh, so it's going to be death yet. Next frame.

Speaker 1:          00:37:02       Okay.

Speaker 2:          00:37:02       Um, and we want to use self and we want to use action. Okay. And the reason that self enacting are going to be our two parameters. Action. We're going to use action because the action is the like what to what direction we want to move in. All right? And we're going to define this later when we create our reinforcement learning algorithm. So for this, we want to, uh, do the same, we're going to start off the same way. We're going to say pie game.events stop pump to uh, call the event queue. And then we're going to fill the background screen and just like we did before, because it's for the next frame frame. Thank you. Um, and sorrow of you're just wait, we're almost done with it or give in four minutes. We'll be done with this and we'll get to the reinforcement stuff. That's going to be super interesting. Okay. Just, just hold tight. So now we're going to update our paddle and we're going to say self dot Padel one wipe position. Um, and then we're going to update our paddle using the paddle method. We just define and we're going to say, uh, we're going to say

Speaker 2:          00:38:12       the action and then the action is going to be fed back to us from our reinforcement learning algorithm that we, that we already do. All right. Action is going to be, hi victor. Thank you. Um, the action and then itself. Dot. Padel one wipe position. All right. And then we want to say self dot Padel one wife position. We filled our screen. We want to draw our paddle, um, one self dot Padel one wipe position. And then we want to update the, the position of itself that paddle to wipe position. We're going to update the position of the Evil Ai, right?

Speaker 1:          00:38:59       MMM.

Speaker 2:          00:39:02       By two people's update. Paddle too. Boom. And then self dot Padel PYP white position. Self dot ball.

Speaker 1:          00:39:13       MMM.

Speaker 2:          00:39:15       Why position? All right. So, okay, so that's going to update both of our paddles over time. And then, um, we want to draw the ball. So draw a ball. Um, self dot ball exposition, self doc ball. Why position? Now we want to get the surface data again and so we're going to do the same thing that we did before. We're going to get the surface data. I can just go ahead and copy and paste that and then

Speaker 1:          00:39:47       okay.

Speaker 2:          00:39:47       Uh, update the window. So Pike in the flip just like we did before and then return it and that's it. Okay. So that um, we're going to say pie game. Oh. And we want to record our Cali, right self. Dot Talley is going to be whatever we are plucks the score that we've, that we've added. Okay. And now I know pet pee pee, pee aid is going to kill me later. Uh, so we want to return the score and the image data. Those are our two things. Okay? All right. These are the two inputs that we, uh, are going to feed into our reinforcement learning algorithm, the pixels on the screen and the score. Okay? So that's a class. All right? Um, and so now, now we have this punk class. Let's do our reinforcement learning algorithm in tensorflow. Okay? We're going to do our reinforcement learning algorithm in flow.

Speaker 2:          00:40:42       All right? So let's just start off by importing tensorflow. Okay? And I'm Adi. Hi Fernando. Okay, we're going to do this intense or flow and we're going to import, uh, so tenter flow is to do machine learning. Uh, CV two is open CV and that's going to help us, uh, um, format our pixel data. So it's, it's, it's, so, it's so that it's better for reading into our tensor flow graph. We're going to import the pong class that we've already created. Um, and things are off. And uh, we're going to import num Pi as NP, which is going to be help us with math. We're going to import random, we were just going to how we're going to initialize some random things later. And then we're going to import the collections library and from the collections library we're going import DQ. Okay. Dq is going to be, it's a, it's a data structure but you queue is a queue data structure and that's what we're going to store our experience replay.

Speaker 2:          00:41:45       Okay. We're going to, we're going to, we're going to store some memory in this queue function and I'm going to explain why we're going to use that. So let's go ahead and start off by defining our hyper parameters. See, I'm just like much more interested when we get right into the machine learning stuff because you know what I'm saying? Like that's, that's where it's at. Okay. So these are our hyper parameters. We're going to start off with our actions variable and our actions variable are up, down or stay. Okay. Um, let me see. All right, so now we want to define our learning rate. Learning rate. Learning rate is going to be a gamma and we're going to set it as 0.99. Now we want to define our epsilon. So I'll, I'll talk about what our initial on is. Um, we're going to say initial epsilon is going to be 1.0 and then our final epsilon and where we want to be after training, it's going to be 0.05.

Speaker 2:          00:42:44       So what is this? This is, uh, when we update our gradient or training over time, we want to make sure our epsilon reaches 0.05. Okay. And we're going to, we're going to apply this later. How many frames do we want? Do we want to anneal are epsilon. Okay, so how many frames we're going to define? Those friends were going to say explore frames. It's going to be 50,000 frames. And then our observe frames are going to be, sorry, these are 50,000. This is 500,000. Okay. Um, so now, thanks Daniel. Uh, what kind of neural net is this? Great question. Let's just, I'm going to define that. Let me, let me say that in a second. Let me just define to more hyper parameters are replay memory, which I'm going to talk about and what kind of neural network this is. So replay memory, there's going to be 50,000 and then our backsides, our backsides is going to be a hundred our rights.

Speaker 2:          00:43:43       So batch. So how many times we want to train. All right. And while we want to create RTF graph, okay, what kind of known that worker is this? This is a convolutional neural network. A convolutional neural network is a type of neural network that reason image data. And we're going to make a five layer convolutional neural network. Um, thank you for that in tensorflow and we're going to feed that pixel data and the score into this convolutional neural network and that's good. It's going to help us do deep reinforcement learning. Okay. And uh, and deep mind did this, they did this for that, all those Atari Games and no matter what Atari game they gave it, it was able to just look at those pixels and become amazing over time. All right. Exactly. Lynn Wang. Exactly. So now let create our tensor flow graph. Let me just go down a little bit.

Speaker 2:          00:44:33       And so we're going to take, okay, so create our graph, create graph, and let's get started. So now we want to say create our first convolutional layer. Our first convolutional layer is going to convolutional layer and our bias vector. That's what we're going to first create. So I'm going to define that burst. I'm going to say first convolutional layer using tentraflow variables. And we're going to define the size of it. Okay. Um, and we're going to say these sizes can be changed to be whatever numbers, but I'm going to have days be these numbers because, uh, so okay. We're defining our firsts convolutional layer. Okay. And we're using the TF dot zero's a function to define those layers. What TF dot zeros does is it creates a, an, um, an array of an empty tensor full of Zeros. It's an empty tensor, which is good for us because our a convolutional network is going to start up empty and we're going to fill it with data over time.

Speaker 2:          00:45:38       And we wanted to find the size of it. Okay. We want to define the size of it. So we're going to say be accomplished in a one t f. Dot. And so our bias vector will help us how much we want, uh, uh, like so where are we want our data to flow and like in what direction. And so our bias will help us in defining like what part of the network we want our data to float in. Okay. So we're going to say TF dot Zeros and we'll define that as 32 because it's 32 bits. Um, so it's eight by eight with 32 bits. And so that's our first player. So now our second layer.

Speaker 2:          00:46:22       Um, and wow, there's 231 people watching this. That's awesome. Wow. I'm so happy. Like this is awesome. Let's get more interest in machine learning. Guys. This is, you guys are going to love it. When I went, oh my God. Okay. You guys, you guys get me pumped. Okay. I love it. You guys get me pumped. All right. So now for a second convolutional layer, we want to, okay, how long is this going to take? This is going to take about, I want to be done within 15 minutes. I want to be done in 15 minutes. Okay. I want to have the entire thing and we'll see how long it takes. So let's define that layer. Um, uh, and we'll define our next convolutional layer. Do you have to have variable pf dot Zeros and four four 32 64 B convolutional two equals TF dot variable.

Speaker 1:          00:47:27       Okay. Okay.

Speaker 2:          00:47:32       And then our third layer, just got to keep defining those layers. Bump bump, bump re equals TF variable DF dot Zeros. Okay. We have a lot of explaining to do and I'm going to explain these magic numbers in a second. All right? So let me just write this shit out and we are going to explain this stuff. Alright. Alright. Alright. That's how we do it. So, um, do you have zeros and we want this to be 64 bits as well. Okay. That those third, fourth and so now our fourth.

Speaker 1:          00:48:19       Okay.

Speaker 2:          00:48:20       Okay. So now our fourth is going to be WC for, okay. Nope. Have you have c? Four is equal to p doc variable and

Speaker 1:          00:48:36       okay.

Speaker 2:          00:48:38       Do you have doctor a ball? And we're going to say Zeros and it's going to be [inaudible] 74 and we're going to use our actions as a parameter here. I'm gonna explain this in a second. And then our bias, let me just cut out all of these layers. Equals F. Dot. Variable gift that Zeros again. And then, um, seven 84

Speaker 2:          00:49:04       I will hook, look at source code. And then last layer. Okay. Um, well after I was going to be w u of c five equals TF doc variable. Can you have got Zeros? Seven 84 actions be fc five, CF. Dot. Variable. So, you know, it's just a very repetitive thing when you're coding your layers because it's just, you know, layer after layer you have to define each of them. And then we want our input. Okay. So what happened here? Let's see. Like all this stuff you guys have said I, okay. So we created five layers and each of them we define a size on input size and, and um, we define an input size. Right. Oh thank you. Print this either.

Speaker 2:          00:49:55       Yeah. I I would, I would be in a ted conference. Yeah. Um, so we define the size for each of these layers. Line 42 is the wrong variable. What are you talking about? Oh, it needs a no. Oh No. Hundred 50 plus people now. Wow. You're right. Do you want to take the three? Oh my God, this is crazy. Oh my God. This is crazy. Okay. Here we go. So we have five layers to our convolutional neural network. Okay. And so now we've got to keep going cause we are running out of time, out of time. How a time. Okay.

Speaker 2:          00:50:29       Analogies are great here. So yeah, that, that, that's a good point. That's a good point. Um, so now we want to create our input. Where is our data going to go? We created, we created this five layer neural network and where do we want our data to go? But we have to create an input, right? A placeholder where the data feeds into the network. Right? Okay. So let's, let's, um, create an input for our pixel data. We're going to feed our pixel data into this input. So we're going to say TF dot. Placeholder, we're going to say float and none. 84, 84, 84. All right. So that, this, this is where our, um, this is where our data's going to flow into. Okay. So now we want to, now we went to do our actual activation function, which is going to be the rectified linear unit every time. Make pixels. Great again. That was great. Every time we feed data into this, at each layer, we need to perform some kind of computation when we need to do something to that data. Okay? Everybody keeps talking about line 42. So let me just quickly look at what is happening here. Um, uh,

Speaker 1:          00:51:41       okay.

Speaker 2:          00:51:42       B F c four. All right, thank you. [inaudible] thank you. Thank you so much. So at every layer we are going to, uh, perform the rectified linear unit activation function. Okay? It's going to take that data and it's going to turn it into a probability. And so really is in use a lot. And Oh, you're right. I only have 28% battery. I need to make sure, oh my God, I have really on a timeline. Okay, so here we go. So let's compute our relu activation function on two d convolutions given for the inputs and filter tensors.

Speaker 1:          00:52:24       Okay.

Speaker 2:          00:52:25       All right. So we're going to do that. We're going to compute it every time using that tensorflow is builtin relu function. Okay. Take our convolutional Qd, convolutional neural network and um, oh man. Okay. All right. Um,

Speaker 1:          00:52:44       okay.

Speaker 2:          00:52:46       To the convolutional neural network. And then, so we're almost done. All right. We have, we, we wanna we have six more lines of code. Okay. And then we're, we're, we're going to just, we're going to get through this. Okay. So, um, no, we have about 12, 12 more lines and then we're good. So we're going to take our convolutional neural network and let me just make sure where I am. Okay. So I need to calm down. I need to calm down. Everything's good. It's all good. Meditation, mindfulness. I know my battery's going to die. I'm gonna die. Everybody's going to die. But hopefully if we can put our minds into computers and machines and merge with them and make the happens, we won't die. Okay. I just said that. So now.

Speaker 1:          00:53:30       Okay.

Speaker 2:          00:53:31       The one to input that. So we're going to perform our convolutional

Speaker 1:          00:53:35       yeah.

Speaker 2:          00:53:36       Is it really more than 300 people in here? Oh my God. It's fucking crazy. Hi everybody. Welcome to becoming awesome with Saroj. So we're going to take strides. So our striver, um, how, um, like the channels that we want our data to float through, I want to say padding is valid. So that's just the, um, so that's a variable for [inaudible]. Okay. Um, I just, I just have to code it. I like literally just have to go over a minute. It's a and N. Dot. Relu TFT. Nn. Actually, I can just literally just copy and paste this. You know what I'm saying? I'm just changed the layers here as accomplish or to accomplish three. And then compositional three.

Speaker 1:          00:54:22       Okay.

Speaker 2:          00:54:23       And then I need to say, I'm going to take,

Speaker 2:          00:54:29       I would say 10 ish years. Yeah. At singularity tennis. Yours. Okay. So, so what's happening here, we're taking all the, all the data that's computed in each layer and we're sending it to the next layer. Okay? Now we're going to, um, take that last convolution layer, convolutional three, and we're going to say, do you have to reshape it? And we're going to take the previous convolution layer and we're going to say negative one, three, one, three, six. And then we're going to say fc four. I know, I know. I have to fix that Typo. Tf Dot. Nn Dark Relu. And we're going to do perform matrix multiplication on that function to make sure that, uh, that is working. Uh, and we're going to use our

Speaker 2:          00:55:12       reshape. Okay. And now our last layer is going to take everything we got from before and we're going to perform matrix multiplication on it. And if we do that, then we have our output tensor, right? Fcf Alright. FC Five is going to be our output tensor. I was going to be the result and we're going to feed that back into the network. Okay. And so we want to return that. We want to return the input tester and the output tensor. Okay. So let's skip right to our main method now. Okay, boom, boom, boom. Deaf name. In our main method, because we are running out of time in our main method, we want you to start off by creating a session I've tentraflow session and our tensorflow session, we use TF got interactive session. That's the method we use to initialize a section. And then when call on it, they find our input layer and our output, our output layer, our, I can't even think right now our output, my hair. Okay. And we're going to what we just defined a method to do that, right? That create graph method or was it, is that what it was called? Yeah. Korea craft create graph is going to create our input and output layer for us and we're going to feed those into our training graph method, which, um, is going to take both of those and it's going to run a bunch of good to run a reinforcement learning algorithm on. And then if Maine,

Speaker 1:          00:56:39       okay,

Speaker 2:          00:56:39       aim equals made

Speaker 2:          00:56:45       Paul Manan. Okay. Okay. So literally 300, 300. Oh my God. Literally, no, you guys are not just, just as smart as me. You guys are like smarter than me. I've got people who are subscribed to me who are just smarter than me. Okay. And in a lot of ways we all shine in different ways. I'm good at being in front of a camera. You guys are good at, you know, other things. I know there's a lot of errors. Um, and we're running out of time because I only allocated an hour for this and this is my third time live streaming and we have a good, we're going to have some errors here and I do not have time to fix the errors here, but let me explain what I've done. Okay. Let me just go over what I've done. I have, okay, what have I done here?

Speaker 2:          00:57:34       I've defined pong from scratch. I've defined a bunch of variables for defining for, for what the game is going to look like. We're going to draw the ball, we're going to draw both of our paddles. We're going to update the position of our ball, and then I have 20% battery. We want to update our paddles, like where they're, where they're going, and then we're going to define a class for our pong game. We're going to say, okay, get the present frame and then get the next frame. We're going to take that pixel data and then we're going to feed it into our convolutional neural network, our deep queue network, which is a convolutional neural network. It's a five layer convolutional neural network, and we built these with tensorflow. Okay? We're going to input that using this placeholder variable. That's where the data flows into the network. Okay? And

Speaker 2:          00:58:19       once we do that, we're going to apply the relu activation function and each layer of that network, and we're going to, by the end of it, it's going to, uh, using our agent environment loop in our training graph method. We're going to be able to train our network to be better and better over time. Okay? So, um, I don't have time to fix all the errors right now. So what I'm going to do is I'm just going to run, uh, what I already have. And so you guys can see what this looks like. Okay? Can you guys see this? Yeah, you guys didn't see this, so I'm not doing anything right. I'm not doing anything. I've got 17% battery. And the thing is like, my Mac book sometimes dies that like 15%, sometimes a dies or like 17. So it might just die right now. So I wanna make sure I'm able to upload this code or this video to my youtube channel and it works. So, so see, there's the evil AI and then there's the rai and it's going to get better and better over time. Right? Right now it's really bad. But if you give it like, you know, a couple hours, it's going to get better and better over time. Typical Mac books, right? So that's how it's gonna look. It's gonna get better and better over time. And now,

Speaker 2:          00:59:25       um, I'm going to stop screen sharing and I'm going to answer two questions and then I'm going to upload the code. I'm going to have it well documented. I'm going to add this stuff that I wasn't able to get to and I'm going to two other awesome stuff for you guys. So two questions. Let's go. 330 people watching. Hi Daniel. Hi. We got, uh, Daniel Shiffman. We have got to do something together. Okay. Um, um, Max, you know, I, I might visit FSS state. Send me an email. Why the real activation function? Because it's good for deep learning specifically. Um, more live maybe next week, maybe next week. No, Daniel, don't worry about it. Hi shamrock. I love you too. Um,

Speaker 2:          01:00:13       well let's be in touch. What is reinforcement learning? We're using the agent environment loop to get better and better over time. It's like trial and error. Like if you give a dog a treats and if it does something that's, that's, that's reinforcement learning, you're reinforcing it and using a reward function. Okay. And uh, cool. Who painted my hair? I do it at a salon. What's next? We're going to do more a neural network stuff, more machine learning stuff and we're going to do it, uh, in a way that's more accessible to more people. I might give a tech talk. You've got to come to me like I don't have time to like go out and like try to find a Ted talk, uh, books for machine learning. I would recommend, um, deep learning, yearning, deep learning, yearning by Andrew. Awesome. How long have you been learning ml?

Speaker 2:          01:00:56       It's been, how long has it been now? Maybe like three years. Blockchain stuff. I used to do that stuff. If you want to see blockchain stuff, go to the beginning of my channel. I used to talk about that a lot. I wrote a book on that called decentralized applications. One of the best selling software engineering books in the world, by the way. Um, uh, can it be done with Cara Ross? Yes, absolutely. Using a piano, blah, backend. A book for Python. Learn python the hard way. Why not use a charger? I don't have time. And for now I've got to go get my charger. So thanks for watching. And I love you guys.