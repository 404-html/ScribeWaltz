Speaker 1:          00:00          Hello world, it's the Raj. And how does an AI get from point a to point B in a game world? That's the question that we're going to answer today using the magic of dynamic programming, okay? This is a type of reinforcement learning and we're going to apply it to the open AI frozen lake environment that you see behind me. Basically, it's a really rough texts version of a frozen lake. And we've got, we've got an AI, an agent that's got to get from the starting point to the ending point without falling into a hole, okay? And there's a, there's an optimal way of doing this, right? There's a, there's an optimal path that this agent can take such that it doesn't fall into any holes and it gets from point a to point B as fast as possible. And we're going to use dynamic programming to help us do that.

Speaker 1:          00:48          Okay? So let's get right on into it. The first thing I want to talk about is this environment. So this is what it looks like, right? So you've got, you've got a grid, right? It's a three by four grid or a four by four grid and it represents a frozen lake. And we have a, we have a series of letters, right? So s means the starting point, that's where our agent starts. F means frozen. That means that an agent can totally walk on it, slide on it, do whatever you want. It's a frozen lake, right? H means a whole, you do not want to fall into the hole. And then g means goal. That's where the agent wants to be. So it's the goal for the agent is to just travel through all these squares until it hits g without touching any of the H's.

Speaker 1:          01:30          And it's going to do this through a process of trial and error, Aka reinforcement learning. So how do we frame this problem? Right? So the mark of decision process is the most used way of, of formally defining an environment with an agent. It's the most used way of formally, mathematically defining a problem setting where we have an agent, we have an environment, we have a set of actions that this agent can take in this environment. And we have a set of states that will occur sequentially as the agent takes new actions in this environment. The reason that we define it as such, instead of just saying, yeah, we have an agent and it's in this game world and yeah, it's going to do some things is because just mathematics is beautiful, right? It's, it's, it's a way of describing all of these different processes as variables and what can we do when we break down these processes into variables?

Speaker 1:          02:22          Well, we can perform all sorts of operations on these variables. We can find relations between these variables. We can create formulas and equations that tell us what the optimal value function for example, or policy for example, what are the best set of rules for an agent to perform in order to complete this objective? That's the reason that we use what's called a Markov decision process, which is a mathematical way of describing all the relations between these variables because it makes things easier later on is we're trying to construct these functions, these equations later on. So in RL, in reinforcement learning, we have an agent that's interacting with an environment in some way and at each time step every second or whatever interval this agent, our AI is performing some action, jumping, moving forward, hitting a ball, whatever. It's going to lead to two things. One, it's going to change the environment state.

Speaker 1:          03:15          That means that, okay, so Mario moved forward one inch. The new state is the environment with Mario. One step forward and the agent possibly receiving a reward or penalty from the environment that you fall in the hole minus one that you not fall into a hole. Did you get a coin plus one? Right? And the goal of the agent is this governor, the optimal policy. So that is the word of the day policy. Okay, so policy means what are the actions to do in each state, right? That's a policy. What are the set of actions that this agent could do in each state? So you could think of it as a table that this agent is looking at. It's like, okay, this is the state of the game. Oh, this is the action that I should perform. Let me do that. Right? And we want to learn the optimal policy.

Speaker 1:          03:56          That means the best actions for that given state, such that the agent is, uh, completing its objective as efficiently as possible, um, to be more specific, to maximize the future, the total future discounted reward. But we'll get to that. So mark of decision processes are a way of describing this agent environment loop in a formal way. We have [inaudible], which is a set of states, the state of the world, right? And we can describe states in any number of ways, right? The number of Coupas, the number of, you know, uh, bots that the layout of the grid, do you know the state, right? This is a very abstract way of thinking about all these problems. That's why it's so generalized. That's why it's used. So often we have a, a set of actions. We have the probability of a given state given us an action and a previous state.

Speaker 1:          04:44          This is called our transition function and we want to estimate it, right? What is the most likely the next state given, uh, an agent takes a certain action in a certain state. We have our starting state distribution, which we can skip for now. We don't need that. We have a discount factor and we have a reward. So the reward is the value given some action in a given state, do we get a plus one? Do we get a minus one? Now sometimes right? Sometimes a reward, sometimes doing some action will give us a reward. Uh, where as if we perform a different action entirely, we get a longer term reward. What do I mean? I mean, let's say I'm Mario, right? And I'm in this game world and I see a star and the star is bouncing towards me and instead of treating the star, I choose to, I choose to step on the Koopa so I get a plus one.

Speaker 1:          05:28          So that's, that's optimizing for short term reward. But if instead I decide, let me, let me instead get the star and not the coupon, if I get the star, I'm not getting any plus one, but it means that I'm more likely to get plus ones later on. So instead of optimizing for the short term reward, I'd be optimizing for the longterm award. So the discount doctor tells us it's a way of weighing what, uh, what, how, um, how optimal and action will be to, to maximize for reward. And the agent learns this, right? It, Lauren's what the optimal discount factor will be, right? So what we're trying to compute is this policy. So policies are generally, they are denoted like this pie symbols. And the, the idea is that the policy is a function that takes a current environment state to return an action. And it's a noted by this symbol, right?

Speaker 1:          06:17          So let's differentiate between the two different types of environments that we could have. So we could have a deterministic environment or we could have a stochastic environment. These are two words that you should just know in general for machine learning because they're used all over the place. It's really simple. Deterministic means you can predict what's going to happen. It's the cast sick needs. You can't, it's random. Simple as that. Uh, but to be a little more specific, in a deterministic environment, the next state is completely determined by the current state and the actions performed by the agent. That means the agent has a, has a real effect on what's going to happen in the environment. Whereas in it's the Catholic environment, which is real life, right? We can't predict what's going to happen, right? And outside of a vacuum, that doesn't matter what an agent does that the environment's going to do, whatever it's going to do, you can't predict it, right?

Speaker 1:          07:06          So clearly it's the CASAC environments are a little harder to learn the optimal policy for, but deterministic ones are easier. So in this video, we're just going to focus on deterministic environments, right? So like I said earlier, I kind of hinted at this. This is the, this is the formula for the total discounted reward. Okay? So don't give, don't get afraid by this math where I'm going to go over it. And the second. So the goal of the agent is to pick the best policy that will maximize the total rewards received from the environment. So let's, let's see what this is. This is a sigma notation. What this means is it's the sum of all of these values right here together. So where I equals one up until t or t is the horizon, the episode length, right? Which can be infinity, write an episode for a game like you know, from start to finish.

Speaker 1:          07:53          Let's take this discounted reward times the reward. So what this would look like if we were to expand this equation, it would be a discount times or award plus a discount, tons of reward plus a discount times or award, right? For, for, for, for each I value. And so we sum all those rewards up and they're, this count generally makes the reward smaller and smaller and smaller. The farther and farther we go into the future or we can flip that and they could go larger and larger and larger. And this is what we're trying to maximize for whizzy. What is the, we're trying to optimize for what, how do we, how do we maximize this value? What do we optimize for to maximize for this value? So okay, so the solution to unmark of decision process is called a policy, right? And it's simply specifies the best action to take for each of the states.

Speaker 1:          08:37          But although the policy is what we're after, what we're actually going to compute is a value function because we can easily derive the policy from the value function, right? So a value function is similar to a policy, except instead of specifying an action for each state, it specifies a numerical value for each state, right? So policies are very straightforward, right? It's, we're given state, this is the optimal action we should take. But for the value function tells us a just a numerical value for each state and using that value, we can compute what the optimal policy should be. So it's kind of, it's, so the value function is to, the policy is a subset of the value function. And we can derive, if we know the value function, we can easily derive what the policy is. So there are two fundamental methods of solving more carb decision processes that are model based.

Speaker 1:          09:27          That means that the agent knows the model of the world beforehand. And these are policy iteration and value iteration, algorithms and d. These are considered dynamic programming algorithms, right? So they assume that the agent knows what the model of the world looks like later on. Will the scourge w we'll discuss model free methods as in the agent doesn't know what the model of the world is and that is Q learning and we'll get into that in a later video. But we're trying to learn what the optimal policy will be and we're going to use either value iteration or policy iteration to do that, right? So let's get into this. Okay, so value iteration, okay, this is, this is, this can be non trivial. It might take a few tries, but eventually you're going to get it. Just keep looking at it over and over. Look at my notes after this video.

Speaker 1:          10:12          If you don't get it, look at the code. It's all there. You just got to look at it for a while, a few hours and boom, you'll get it. But anyway, let's, let's go through this. These are two separate processes. So for value iteration, what we're going to do is we're going to compute the optimal state value function by iteratively improving the estimate of the value function via [inaudible], which is the value function, right? So, um, the algorithm initializes vos to arbitrary random values, it repeatedly updates the Q function if, if we're talking about ACU function, and then it's guaranteed to converge, converge on the optimal values. In our case though, we're not going to talk about the Q function right now, we're just going to focus on value iteration without the cue function. So here's what it looks like. We start out by choosing an initial estimate of the optimal value function.

Speaker 1:          10:56          Just writes whatever our estimate is, it could even be zero, right? We repeat this process until the change and values is sufficiently small. For every given state, we calculate the maximum expected value of neighboring states for each possible action. We then use the maximum value from this list. The Arg Max over that to update the estimate of the optimal value function and after we do that, then we can calculate the optimal value function is okay. If you didn't get that, I'm going to go over it again in a second. In a, in a more planning English way for policy iteration, we're going to first of all choose an initial policy and a value function. Then we're going to repeat this process until the policy is stable. There are two parts here, policy evaluation and policy improvement. So for policy evaluation we're going to repeat this process. For each state.

Speaker 1:          11:41          We're going to calculate the value of neighboring states when taking an action according to the current policy. Then update the estimate of the optimal value function and then for policy improvement, then we're going to update it to get a new policy, right? So Palsy Iteration concenter repeatedly improving the value function estimate will redefine the policy at each step and compute the value according to the new policy until the policy converges. Policy iteration is also guaranteed to converge to the optimal policy and it often takes the less iterations to converge. Then the value iteration algorithm, so check this out. Here is the best plain English explanation that I can give you for the difference between these two. I'm just gonna read this out. Okay, just take, just, just listen to this. In a policy iteration algorithm, you start off with a random policy, right? It's just random, just some number.

Speaker 1:          12:28          Then you'll find the value function of that policy. This is the policy evaluation step. Then find a new improved policy based on the previous value function and so on. In this process, each policy is guaranteed to be a strict improvement over the previous one, unless it's already optimal, in which case we can just stop iterating because now we found the optimal policy, right? But in value iteration, you start off with a random value function and then find a new improved value function in an iterative process until reaching the optimal value function and notice that once we have that value function, like I said earlier, we can easily derive the optimal policy from the value function. Why? Because it's a variable in the, in the, it's one variable that Pi's him. We'll end the value functions equation, we can easily drive that. So what's the difference here?

Speaker 1:          13:18          Policy iteration is generally faster than value iteration as policy convergence more quickly than value than the value function. But it depends, right? It depends on environment and it's good to try both out. So let's look at some code here. So we have open a eyes environments to do this, right? So I'm going to go over this code and then I'm going to go over the, I'm going to go over the high level code and then the lower level of code. So let's just, let's just go through this. Let's just step through this code. What we're doing is we're building this frozen lake world and we're going to apply both policy iteration and value iteration algorithms to this code. So to start off, we're going to import gym, right? Open Ai's gym environment that lets us test out a bunch of different game environments. And then we're going to import num Pi to do some matrix math.

Speaker 1:          14:04          So to start off, we're going to have some action mappings, right? Numbers that correlate to keys that the agent can perform like up, down, left or right. Okay. So in this play episodes function, we're going to assume that we know the optimal value function, the optimal policy. So here's how we would go if we knew those two, right? We've already performed value and policy iteration. So for every episode in the game, while the game is still running, select the best action to perform in a current state. We're taking that right from the policy and we're using Arg Max to find the greatest value for that action. The action that's going to give us the greatest value, then we're going to perform that action and observe how the environment acted in response. So the great thing about Jim's environment, the gym environment, is that we can just use the step function on a given environment and to perform an action and it's going to report, it's going to return the next state.

Speaker 1:          14:54          If there's a reward or not a yes or no boolean did the game terminate or not, and then some logistics, some logging info, that's it. Then we'll render the environment at every time step. So we can see what it looks like and we'll summarize. We'll summarize a total reward. Remember that equation I showed you about the total reward and then we'll update the current state as the next state, right? Because we just stepped into the next time step. We'll calculate the number of wins over each episode and we'll get the average award as a total award divided by the number of episodes and return the number of wins, the total reward, and the average reward. So that is the law. That's actually how the agent plays given its, knows everything, you know, the optimal policy and the optimal value function. So then we have a number of episodes to play.

Speaker 1:          15:38          Let's just say 10,000 and these are both of our solvers, right? We're going to have a policy, iteration solver, and a value iteration solver that I'm going to go into later. So, so for each of these solvers, load up our environment, frozen lake, okay. And then we're going to search for an optimal policy using policy iteration. Okay? And then we're going to print out what that optimal policy is, and then we're going to compute an optimal policy using policy iteration. And we're going to use both value and policy iteration for, because these, this is looping for each of these solvers. And then we'll print out the rewards, the winds, and everything for us to view later on, right? So let's go and see what these two functions look like under DP dynamic programming. Okay, so let's start off with value iteration, right? So for value iteration, we're trying to compute that optimal value function, right?

Speaker 1:          16:26          And if we've got that value function, then we can compute the optimal policy. So here's what we're gonna do. We've got the environment, our discount factor, a theta value. That data volume is a stopping threshold. And the Max number of iterations, we want to perform this right? How many times do we want to iterate to get that optimal value function? We're going to start off by initializing our value function randomly rights. And then for the number of iterations we defined, we have our stopping condition. We're going to update each state. We're going to do one step look ahead to calculate the state action values in the next time step and that's going to give us this action value. Then we're going to select the best action to perform based on the highest state action values. So we'll perform NP dot Max to find the largest number for the actual value and that's going to be our best action value.

Speaker 1:          17:10          We'll calculate the change or the Delta right between this current state and the and our best action value and then we'll update the value function for the current state. We found the change, the difference in those two, and then we'll check. This is our stopping conviction, right? We'll check if we can, we can stop. We've converged and once we have that value right, then we can create a policy using that optimal value function down here. So for each state. So we'll first initialize our policy randomly and then for each state we have, we'll look ahead one step to find the best action for the state and using our optimal value function. Then we'll select the best action based on the highest state action value and update the policy to perform a better action at a current state. At the very end, we were turned the optimal policy and the optimal value function, right? So the value function is at the core, it's at the center and from it we then derive the policy. So now let's look at policy iteration.

Speaker 2:          18:04          Yeah,

Speaker 1:          18:05          so for policy iteration, we start off with the random policy, right? Right. In this case is going to be the number of states times the number of actions divided by the number of actions, right? And so now we have a counter to count how many times we want to evaluate each policy. We're going to repeat this until convergence. We say evaluate the current policy, right? Remember there's two steps, policy, evaluation, and then policy improvement. We're going to evaluate the current policy and that's going to give us a value function. We're going to go through each state and try to improve actions that were taken. We're going to choose the best action in a current state, under the current policy by performing Arg Max. What's the Max value in the policy for a given state? And that's going to be the current action. Well then look, one step ahead and evaluate if the current action is optimal and we're going to try every single possible action in a current state and we're going to select the better action of the two and the f the action doesn't change, then that means we found a stable policy, there's no change in the action.

Speaker 1:          19:02          And then we, we will update the policy, right? Greedily but we'll talk about what greedy means later. This will update the policy and then we have a convergence criteria at the end, right? So that's a difference here, right? Value Iteration is focused on the value function first and foremost, whereas policy iteration is more focused on the policy first and foremost. And there are two differences. Sometimes one works better than the other. Generally, generally the policy iteration algorithm is faster, but you got to try out both. Right? And both of these are examples of dynamic programming and this, these are model based methods. If our agent knows what the world is going to be like,

Speaker 2:          19:38          uh, yeah.

Speaker 1:          19:41          So it looks like this, right? Uh, you know, a better view of this would be just for me to remove this render and then just look at our results

Speaker 2:          19:53          over time.

Speaker 1:          19:58          Right? Here are our policy, right? It's, it's the collection of directions that our agents should move over time using policy could aeration. You can see how many policies it's, it's evaluated, uh, and it's just going to keep repeating, right? Telling. And now it's like, okay, here's value iteration and see when each of these converged, you could see the optimal policy derived from value iteration, and yeah, that's it. Please subscribe for more programming videos. And for now, I've got to optimize my chest hair, so thanks for watching.