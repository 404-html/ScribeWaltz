Speaker 1:          00:00          Regression. Final answer. Yes. Hello world it Saroj and everyone in the entire world needs to understand how regression works. It's a data science technique that can quickly give us superpowers of insight from a massive, sometimes unintelligible dataset. Using regression, we can predict the optimal way to allocate funds in our organization to increase sales, predict demand for a product, understand the effects of certain treatments on a patient. Understand why certain factors contribute to poverty or economic prosperity. I could literally go on for hours. Ain't nobody got time for that. Whether it's in business research or even lifestyle design regression can give us the answers we need from data to improve our lives and the lives of others. Now, there are a lot of different types of regression models in the wild, including logistic and stepwise and elastic net. It can be overwhelming to try and understand all of them from a single video.

Speaker 1:          01:07          So we're going to focus on just a few of the most important ones for our problem. We'll play the role of a newly hired data analysts at a corporation called engine using a Dataset of our marketing expenses and ticket sales. For our newly opened Jurassic Park. We'll use a regression model to predict how to optimally increase ticket sales next month. Data of finds a way identifying whether or not the correlation between two or more variables represent a causal relationship is rarely easy using human intuition alone, but if we could figure that out in a mathematical way, it would be so much easier to make business decisions quickly so we can spend the rest of our time playing Fortnite to help answer these types of questions. Data scientists use regression models. Regressions are used to quantify the strength of the relationship between one variable and other variables that are thought to explain it these days.

Speaker 1:          02:06          Running hundreds of thousands of regressions has become extremely simple. In fact, we don't even need to know how to code. That's right. Microsoft formerly run by Youtube sensation. Bill Gates has built regression functionality into its XL product and almost all of the major tech companies have some form of drag and drop interface that lets us immediately apply regression analysis to a Dataset we upload. But using a programming language like python can help us intuitively understand the mathematics that's happening behind the scenes and if we understand the mathematics, it'll help us build an intuition around when to use which model for whatever task we have. So let's start with the first type of regression, the simple linear regression. This is a model that can show the relationship between two variables. More specifically, it shows how the variation in the dependent variables can be captured I a change in the independent variable in a business context.

Speaker 1:          03:14          The dependent variables can also be called the predictor for sales of a product performance, pricing or risk. The independent variable also called the explanatory variable explains the influence of the dependent variable. A simple linear regression model is linear because all the terms in the model are either a constant value or a parameter multiplied by an independent variable. The core idea is to figure out this model which when plotted is the line of best fit for the data. The best fit line is the one where the total prediction error of all the data points are as small as possible. We can consider the error as the distance between the data point and the regression line. So in our datasets, after we import pandas for data manipulation, num Pi for Matrix math and Plotly for data visualization, we can visualize our data first to see that we're spending money on TV, radio and newspaper ads each week and the amount of sales of our product changes every week as well.

Speaker 1:          04:22          Yes, I said newspaper. Remember this is the 90s let's say we want to find the relationship between just TV, ad money and sales. Thus we'll want to use a simple linear regression model. We can represent this as the model y equals mx plus B. The values of both M and B should be optimal such that the error is as small as possible when plotted. There are several different metrics we can use to compute the error, so let's use a popular one called the sum of squared error. This simply finds the difference between the actual output and the predicted output squares. It adds them all up and that's our final error value. We then use this error to update the values for B and m using a technique called gradient descent. Rather than explain how that works, I'm going to link you to my video aptly titled Linear regression using gradient descent in the video description.

Speaker 1:          05:16          When we perform this simple linear regression will find that there does seem to be a relationship between these two variables. The more we spend on TV ads, the higher our sales are. Makes Sense, but we probably want a more holistic view of our data, meaning we want to take into account the other variables as well, like the newspaper and radio ads so we can see how they all affect sales. If we try to plot just one other feature, the line we have now becomes a plane. We're now in three d. The plane is the function that expresses why as a function of the other two variables. This is the genesis of the multiple linear regression model. There's now more than one input variable use to predict the y value. A model with two input variables is a longer equation than one expressed with one. We can take it a step further and take into account all three input variables and create an equation for that as well.

Speaker 1:          06:15          Alas, if we do this, we are now limited in our capabilities in terms of what we can visualize in a graph since we are now dealing with more than three dimensions, but that's the wild world of data. Many data sets have tens if not thousands of dimensions. If we again use gradient descent to find the equation that best fits our data will receive a model, we can interpret specifically the coefficients in the equation. We can correlate each learned coefficient with a different feature. If all the other predictors are held constant and we change any of these values, we can tell what the corresponding increase in the amount of sales would be, but which coefficients are the most significant? Are they all equally significant? Does this equation explained the meaning of life? Here we turned to the art of model evaluation. Notice that all the coefficients are greater than zero.

Speaker 1:          07:13          That means all the variables have an impact on the sales. Hmm. Before we keep evaluating this model any further, I have a suspicion we could use a better model. My intuition as a digital detective is that TV ads far outweigh the other features in terms of significance and we'll need to prove that are a linear regression model fits the data relatively well? I mean it's not extremely off. If it was, we'd want to try to instead use a nonlinear regression. A nonlinear regression model is one where not all the terms in the model are either a constant value or a parameter multiplied by an independent variable. Both linear and nonlinear models can fit curvature in data, so that's the main factor to consider, but since we're doing pretty well so far, let's not go there right now. It's also important to note that both single and multiple regression models can be either linear or nonlinear.

Speaker 1:          08:13          These four terms are all different ways of defining regression's. Before we kick back, relax and tweet that we've secured funding for Tesla at four 20 we want to try to more types of linear regression models, rich and lasso regressions. Any linear or multiple regression model will be sub optimal. When there is a high colinearity amongst the feature variables. Colinearity is a condition in which some of the feature variables are highly correlated. It's sub optimal because colinearity tends to interfere in determining the precise effect of each feature variable. There are a few different ways of mathematically evaluating this, but we'll get to model evaluation techniques later on. Right now let's just try out ridge regression to see how it works. Ridge regression alleviates colinearity amongst the regression predictor variables in a model. Again, colinearity is we're one of the feature variables in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.

Speaker 1:          09:22          If the feature variables are very much correlated, the final regression model is going to cause overfitting, meaning our model won't be generalized enough to new data points. It'd be perfect, but only for the existing data points. We don't want that. So to solve this issue, Ridge regression as a small squared bias factor to the variables. This squared bias factor pulls the feature variable coefficients away from this rigidness. I introducing a small amount of bias into the model. It reduces the variance, which means the difference between predicted data points and the mean of y immensely. This trade off of variants and bias will produce much more useful coefficient estimates. When colinearity exists for Ridge regression will introduce grid search CV. This is going to allow us to automatically perform cross validation which will assess how the results of our model will generalize to an independent data set with a range of different parameters.

Speaker 1:          10:24          In order to find the optimal value of Alpha, then we can find the best parameter and the best mean squared error using the built in attributes. Thank you. Psychic learn. Our optimal value of Alpha and MSC are both slightly better than the ones we got from art multiple regression model. This is good, but I think we can do even better. Enter Lasso. Regression. Lasso regression is similar to ridge regression and that they both have the same premise. In Lasso regression. We're again adding a bias term to the regression function so that we can reduce the negative effects of colinearity and consequently the model's variants, but instead of using a squared bias like ridge regression does, Lasso regression uses a literal lasso. Just kidding. I had to do that at least once. It instead uses an absolute value bias and this difference has a huge impact on the bias variance.

Speaker 1:          11:21          Trade off Lasso overcomes the disadvantage of Ridge regression by not only punishing high values of the coefficients but actually setting them to zero if they aren't relevant in this way, we might end up with less features included in our model then we started with, but this can be an advantage when we implement lasso. It looks similar to Ridge and our resulting alpha and MSC are the best scores of any model we've used. Looks like Lasso is the way to go. Using this model, we can give it values for how much we will spend on different advertising mediums and it will predict the amount of sales we'll make, which lets us much more efficiently allocate funds towards the medium we want to use saving us money and thus increasing our revenue. Luckily, there are only three points to remember from this video. A regression model is simple if it has only one input and one output, but it's multiple if it has multiple inputs and one output. A regression model is linear when all the terms in the model are either a constant value or a parameter multiplied by an independent variable else. It's nonlinear. Lastly, if a data set has a high colinearity, meaning the inputs are highly correlated, we can use either ridge or lasso regression to help ensure more accurate estimates of the regression coefficients. What do you want to use a regression model to do next? Let me know in the comments section and please subscribe for more technology videos. For now, I've got to find a lasso, so thanks for watching.