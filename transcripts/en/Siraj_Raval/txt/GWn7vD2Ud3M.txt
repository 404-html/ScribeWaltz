Speaker 1:          00:00          Is generating novel data. Hello world, it's the Raj. And today we're gonna learn about a special type of neural network called an auto encoder. Then we're going to implement our own auto encoder using tensorflow to generate handwritten digits. An auto encoder is a simple type of neural network with only three layers. It's got an input layer, a hidden layer, and an output layer just like your mom. Just kidding. What makes an auto encoder special is that the output neurons are directly connected to the input neurons, and the goal is to get the output values to match the input values. So if I added an image of a dog to the input layer, the apple layer would then output that same image. When we input that image, it's a vector in end dimensional space, which is sent to the hidden layer. After some activation function is applied to it, this reduces it to an end dimensional space.

Speaker 1:          00:49          So there are less dimensions. This process is called dimensionality reduction, and it happens in every neural networks. So we can think of the first layer of the network as the encoder, since it's compressing data. And the last part as a decoder fins and tried to reconstruct the original input from the smaller representations in the hidden layer. So when do we use auto encoders all day, all day? Well, one use case is data compression, kind of like creating a zip file for some dataset that you can later unzip. Another is image search where you search Google using an image as your search query instead of text. It's very likely that Google is using auto encoders to help fuel this use case. Whenever one of Google's bots crawling the web and finds an image, it will compress it using an auto encoder. That would mean Google would have all of its indexed images compressed in the form of an easily searchable array.

Speaker 1:          01:35          Whenever you search for a new image, it will compress it and find all the points nearest to it in the compression space. Since there is less noise, then rank them according to their similarity. There's also the one class classification use case. That's when we only have a dataset with a single class, which is called the target class. When we feed it to an auto encoder, it'll learn to detect objects of that class. When it receives an object that doesn't fit that class category, it'll detect it as an anomaly, whether it's a deadly virus or fraud attempt system, downtime or someone who actually doesn't play Pokemon go. That's right. Auto encoders are for you and if you want to train a deep network, you could use what's called a stacked auto encoder. That is a set of auto encoders where the outputs of each layer our wire to the inputs of the successive layer.

Speaker 1:          02:17          Once you train it on some data set, you can use those weights to initialize your deep net instead of randomly initialized weights. One of the more recent applications of auto encoders is generating novel yet similar outputs to our inputs like faces that look really similar but are different than the input basis. For this, we use a newer type of auto encoder called a variational auto encoder, which learns a distribution around data so it can generate similar but different outputs. There are lots of techniques that are used to prevent auto encoders from successfully reconstructing the input image. Like denoising where the input is partially corrupted on purpose. The idea is that if you can reconstruct an image, despite it being corrupted, it will be a more robust decoder. So let's build our own simple autoencoder to learn, detect hen writing digits using tensor flow, shall we first of all import tensorflow, the awesome machine learning library.

Speaker 1:          03:04          Then num Pi, the [inaudible] scientific computing library will also import our input data, which is the collection of handwritten character images. Then we'll define our auto encoder hyper parameters. We know that each character image is 28 by 28 pixels, so we'll set the width to 28 then we'll initialize a variable that represents the number of input notes. We also want to have 500 nodes in the hidden layer. I generally like to have two thirds of number of nodes in my hidden layer as my input layer as a starting point. Once it purposely corrupt our input data later so that our Dakota gets even more robust and it's reconstruction over time. So let's set our corruption level to 0.3 which isn't nearly as high as the u s government's. Once we have these variables, we'll use them to help us build nodes. We'll start by creating a note for the input data.

Speaker 1:          03:45          Then we'll create a note for the corruption mask, which will help us reconstruct the original image. Well then went to create notes for our hidden variables. After we initialize our weights, we'll initialize our hidden layer as well as the prime values for each at tidal weights between the encoder decoder and result in an output value. After that, we define our model via a function that takes in our variable parameters. We'll make sure to get a corrupted version of our input data, then create our neural net and compute the sigmoid function to create our hidden state. Then we'll create our reconstructed input and return that. So our model will accept an input and return a reconstructed version of it despite the self imposed data corruption. So we can utilize this function by building a model graph with it, which we'll call z. So now that we have our model, we need to create a cost function, which is what we want to minimize over time.

Speaker 1:          04:29          The more we minimize it, the more accurate our output results will be. Then we want to create a training algorithm, which we'll call a train op and use the classic gradient descent algorithm to help train our model, which takes the cost function as a parameter so it could continuously minimize when we train it. Before we start training our model, we need to load our data, so let's read from our local directory. We'll set one hot to true, which is a bit widest operation that will make computation faster than initialize the variables for our images and labels for both our training and testing data. Finally, it will begin our training process by initializing a tensorflow session. We'll initialize all of our variables first, then begin our for loop, which will iterate 100 times. For every image and label. We will retrieve an input, then create a mask using the binomial distribution of our input data and use that as a parameter to run our tensorflow session using the training algorithm we defined earlier. We'll calculate a mask for the outer loop as well and print out the results as we go along. Let's take a look at the results. We can see our score gets better and better over time with training, and eventually our neural net is able to reconstruct and classify handwritten characters. For more information, check out the links down below and please subscribe because I'm just getting started for now. I've got to go boost some gradients, so thanks for watching.