Speaker 1:          00:00          Hello world, it's Saroj and in this episode we're going to build our own game bots capable of beating any Atari game that we give it. In 2014 Google acquired a small London based startup called deep mind for $500 million. That is a lot of Benjamin for what seems like pretty simple software. It was a bot for Atari Games, but the reason they paid so much for it was because it was one of the first steps towards general artificial intelligence. That means an AI that can excel in not one, but a variety of tasks. It's capabilities are generalized just like ours are. There paper was later featured on the cover of nature showing that their algorithm could be applied to 50 different Atari Games and achieve superhuman performance. In all of them, they called their bots, the DPU learner, but before we talk about that, let's talk about the concept of reinforcement learning.

Speaker 1:          01:02          Supervised and unsupervised learning techniques are well known in the applied AI community. You give some model, a Dataset with labels and have it learned the mapping between the two or a dataset without labels and try to learn what the labels are by clustering or detecting the anomaly in the Dataset. We can use these data sets to create data classifiers or data generators. But consider this scenario you're playing the game Super Mario Brothers, awesome game and rather than play it yourself, you'd like to train an AI to play it for you. How should we think about this problem? If we screen captured game sessions from expert players, we could use the video frames from the game as input to a model. And the output could be the directions that Mario could move. This would be a supervised classification problem. Since we have labels, the directions to move, assuming we have lots of data and access to some sick gps, it makes sense to try out a neural network here, given video frames in a new game, it would know how best to navigate to beat the level, right?

Speaker 1:          02:12          Yeah. But then we'd need hundreds of hours of gameplay videos to train on, and it doesn't seem like an elegant solution to this specific problem. First of all, we're training a model, not on a static data set, but a dynamic one. The training data is continuous. New Frames are constantly emerging in this game world, this environment, and we want to learn how to act in this world. Humans learn best by interacting with an environment, not by watching others interact in it. Environments are stochastic. Any number of events can occur. It seems best to learn by trying out different possibilities. So rather than framing this problem as solvable by pattern recognition, let's frame it as solvable through a process of trial and error. This is the kind problem that reinforcement learning is made for. We do have a few labels, a plus one. Every time Mario does something positive, they're just not instantly available to us.

Speaker 1:          03:10          They're time delayed. Instead of calling them labels, let's call them. So how do we formalize this process mathematically? Well, we start off with an environment where an AI or agent can perform a number of actions in. Since environments are unpredictable, we want to keep track of its current state. The agent acts in a given state of the environment and based on its actions, it may or may not receive a reward, an increase in the score. We can then represent one full episode of this process. An episode would be a single game from start to finish as a sequence of states actions and rewards for the agent. The probability of each state depends only on the immediately previous states and the performed action, but not on the states or actions before that. This is called the Mark Hoff property in probability theory named after the Russian mathematician Andre Hoff, and since our agent is making decisions based on this property, this process is considered a Mark Cobb decision process. How could you submitted without asking me first, it doesn't matter where on the front page of nature. Hello Luis,

Speaker 1:          04:30          we want our agent to be smart, a f to plan not just for short term rewards, but for long term rewards as well. For our Super Mario example, stepping on a Cooper would increase our score in the short term, but that's it. However, consuming a star would increase our score in the short term, but also increase our score and the longterm we could represent the total future reward for a single episode from a time point t onward like this where we just summed them all up, but remember that our environment is unpredictable, so we can't be sure that we'll get the same rewards in another episode. The farther into the future we go, the farther the rewards could diverge. So we can add a discount factor between zero and one to our equation. What this means is the more into the future, the reward is the less we take it into account.

Speaker 1:          05:21          So we want a balanced value. Ideally, we want to choose an action that maximizes the discounted future reward. So how do we do that? Well, we can represent this discounted future reward when we perform an action in a state and continue optimally from that point on as a function. This function represents the best possible score at the end of a game after performing a given action in a given state. It's a measure of the quality of an action in that state. So we'll call this function Q four quidditch. No, I wish for quality whenever is deciding between several possible actions. The solution is picking the action that has the highest Q value computing. This Q function is where the learning process comes in. The maximum future reward for this state and action is the immediate reward plus the maximum future award for the next state.

Speaker 1:          06:17          This is also called the bellman equation. We can think of the Q function as a matrix where the states are the rows and the actions are the columns. We'll start by initializing the Q matrix randomly and observing the initial state of the game. Then we can approximate it through a training process where we first pick an action, execute it, observe whether or not we received a reward because of it and the new state we're in. This is called Q learning. It's used to find the optimal policy for any mark Haub decision process. It's a very popular algorithm in reinforcement learning. Since we're any finites markup decision process, it's been proven to eventually find the optimal policy. If we apply cue learning to Mario, then the state would be presented by the location of the enemies on the screen and the obstacles as well as some other factors, but that's a game specific state.

Speaker 1:          07:12          What could we represent as a states that could be generalized across many gates? Well, one thing that all games share in common are pixels. If we could somehow convert pixels from the game screen into actions, we could feed it any game and it would learn the optimal policy for that game. If we use a convolutional neural network, we could use game screens as input and output the Q value for each possible action. So deep mind use the CNN with three convolutional layers and to fully connected layers. Normally you'd also want to use pooling layers which makes the network insensitive to the location of an object in an image which is perfect for classification tasks like detecting if a picture has a certain object and it, but for our use case, the location of the player and the enemies is crucial. So we won't use pooling layers.

Speaker 1:          08:06          We'll input for different games, screens as input so that we have a way of representing speed and direction of the game characters and the outputs are the Q values for each possible action. Since we're using a deep network to help approximate the Q function, we can call this process deep cue learning. Deep Mind added a few other tricks to this, but that's really the basic idea. The best part is that this algorithm can learn how to excel in any day. To summarize, we can use reinforcement learning to optimize an agent for sparse time delayed labels called rewards in an environment mark Haub. Decision processes are a mathematical framework for modeling decisions using states actions and rewards and cue is a strategy that finds the optimal action selection policy for any MDP. Noah Dell is the wizard of the week. The challenge was to build Lda and he definitely delivered.

Speaker 1:          09:06          Noah used Lda to mind topics from scientific papers found on nature.com using it. He was able to find important key words that summarize all the jargon very neatly. Such an awesome use case, a plus, and the runner up is he jumped one each on one. Use Lda to mind topics from news articles, then classified them with the CNN to predict stock prices. This week's coding challenge is to implement a cue learning algorithm from scratch for any game you'd like post those get hub links and the comments, and I'll give the two best submissions a shutout next week. I hope you liked this video and if you did, please hit the subscribe button for now. I've got to find my Q matrix, so thanks for watching.