Speaker 1:          00:00          Are Live. Hello old. It's the Raj. We started a little late because I was having some tensor board trouble, which I still am having, but it's all good because when I say I'm going to go live, I'm going to go live. So today we are going to talk about tenser board. So tensor board is the tool that comes with tensorflow. So it's Builtin. So if you've, if you've installed tensorflow, tensor board comes with it and the goal of tensor board is to help you visualize your data. Uh, there's it, it can help you optimize your data. It can help you debug it, it can help you, uh, look at ways that you can improve it. And there's also the plugins feature, which is coming soon. So I can't wait for that. But yeah, it's a pretty cool tool. And I admit I haven't used it much, but I s B and that's because I think the use case for it really shows itself when your data, it starts to get really complex.

Speaker 1:          00:49          That is your model. So when your model start to get really big, that's when tensor board becomes very useful. And what are the reasons I'm demoing it now? So I already made a video intensive board before, but one of the reasons I'm doing one again is because the embedding visualizer is a pretty cool tool and I'd love to talk about that. And it's a new tool and a, it's going to be really cool. So in this demo, what we're going to do is we're going to look at a m and I is tea. Now I know you guys have seen Mni and I 1,000,001 times, but the point of this is not immunized. It's the embedding visualizers specifically. So that's what we're going to look at. And uh, so yeah, I'm going to start off by showing the code, uh, that we're going to use a, and then we're going to visualize it intense or board, and then we're going to look at the details for each of the sessions. All right, so let me start off by looking at the code here. Uh, so what I'm gonna do is I'm going to explain the code and then we're going to go into the,

Speaker 2:          01:49          uh,

Speaker 1:          01:50          the session itself. Okay. Or the code itself. All right, so to start off with, let me remove some things. So to start off with, we're going to look at this code. So I've got it up here. I want you guys to download the code and follow along with me. Okay? So what this code does is it is a convolutional network. It's a convolutional net and it is meant to classify a handwritten characters. Okay? So let's look at what it is and let me say, let me start off with a two minute Q and a. Okay. So ask, go ahead and ask questions and uh, yeah, it's gonna be awesome. We got code, we got questions, we've got sublime, we got audio lag. Yeah. All the things we're here.

Speaker 2:          02:40          I'll do luxury feel.

Speaker 1:          02:42          We are all tensors. Okay. Any questions guys before I get started? Because I'm about to go in, go in on this, no pun intended.

Speaker 2:          02:54          Okay.

Speaker 1:          02:57          How do we do? Okay. How do we, the typology of a neural network? Great question. Uh, so many decisions go into that depending on your use case, what you're trying to do. But I would say there are a few general rules to go by rules of thumb. One would be the more layers, the more computation, but also the more accuracy your model. We'll have, uh, what type of network. It depends on your use case. Convolutional network for images, recurrent network for time series data feed forward network for binary data. Uh, and then more complex data. There are different types of models to more questions. And then we're going to get started. Uh, audio is a little low but we'll take care of that. How do you use tensor board with Floyd hub? Uh, I haven't heard of Floyd hub. I actually, I've heard the term but I don't, I don't know what Floyd hub is.

Speaker 1:          03:46          Uh, how much product goes into that hair? So surprisingly none. It's just I just roll out of bed and I, and I, and I do this one more question, not about my hair. Uh, how do we use tensor board with AWS servers? Now, that's a great question. So I actually have not seen that done before. Uh, using tensor board in the cloud, uh, would be a good idea. If you had, uh, uh, if you were using transfer learning. So if you are using a model that you yourself hadn't trained and it's a huge model, then using it, um, without having to download it directly from the web would be actually, that'd be a great startup idea. So not just AWS, but just visual. So here's a great startup idea for you guys. Visualizing modeled architecture in the web is a problem that has not been tackled at all right now. We have to do this locally and no one wants to have to do that. So, uh, great. That would be a great problem. Okay, so let's go into the code. Okay guys. So let's, so what I'm going to do is I'm not going to type out the code, I'm going to talk about it, and then we're going to look at the tens board. Uh,

Speaker 1:          04:51          then we're going to look at the tender board version. Okay? So to start off with, let me, let me maximize this code so we have a good look at it. Okay, let's maximize. So here are our libraries. Okay. And then for URL live, I have this little versioning because you know, for python three versus python to URL live is annoying. So it's, we have to have this. And so then these two constants, these constant values are what we are doing here. And so this is the, this is the novel part is we are importing the,

Speaker 3:          05:22          okay,

Speaker 1:          05:23          we are importing the embeddings from the web. So let's look at what this link is. So on get hub, if we look at this link, we can see that that's an invalid request. That's what we can see. But there's also, let's see.

Speaker 3:          05:49          Mm.

Speaker 1:          05:50          So okay, hold on.

Speaker 3:          05:54          Okay,

Speaker 1:          05:56          so here's what we're, we're

Speaker 3:          05:58          pulling, okay,

Speaker 1:          06:00          these, so what are these? So let me, I actually have these locally. So let me just open them locally. So there are two things here that we are pulling. The first is a collection of sprites. So these are sprites. So what are these? So what we are doing is we are classifying handwritten characters. Now, most of us are familiar with this, with this example, but this is novel. So for the embedding visualizer, which is a new feature of tensor board, we can visualize the learned vectors that are model creates when training on our input data and input labels. So we have characters, right? One through nine, and these are what are considered sprites. So when we learn these mappings from our model, and our model is training, it's learning what a seven is, it's learning what the end, these vectors represent the correlation between the actual number, the, the, the number seven, and the image of seven.

Speaker 1:          06:51          And that correlation is represented as a vector. So the way that we can then represent that vector is to have a sprite. And these are the sprites. And so this is this, this, uh, image are, uh, is going to be cut into a cut into squares. Okay? So starting from the top left to the bottom right, we can think of it as a matrix. And when we visualize it intense or board, when we visualize it, intention board, these vectors are, these sprites are going to show up as the, um, as the images for what we've just visualized. Okay. So, so just like this, I'm doing this because I haven't actually launched tensor board yet, but it would look like this, right? So see these fours and the Zeros, those, those are the sprites. Those are, those are values that are coming directly from this sprite file. And then associated with that, we also have a, we have a, let me see anywhere where, where'd I put this? Uh, labels file, which is a TSV file. Okay. So the labels file is, it's a file extension. The file extension for Tab d limited. It's like for spreadsheets. So it's important into an export from spreadsheet software. And these are just the, the associated numbers. These are the associated numbers. It's such a weird format, but it every single number associates with one of these images from this a matrix right here. Okay. So

Speaker 1:          08:20          each number is associated. So that's what, that's what we're doing. Okay. Um, okay, so

Speaker 1:          08:29          that's, that's what that is though. Let's keep going here. So the next part is for us to, uh, download these embeddings. So what, what this line does is it's, it is downloading the Mni data and it's splitting it into three parts. You have 55,000 data points of training data, 10,000 points of testing data, and then 5,000 points of validation data. And so we're going to call the images and we're going to call the labels why? And each image is 28 by 28 pixels. Okay? So these are each of these images and that's what the read data sets function is doing. Having downloaded that from the web, okay? And

Speaker 3:          09:10          okay.

Speaker 1:          09:11          And we're going to store that in m and ist the, the, the variable. Okay? So let me increase the font size a little bit. Okay, so now, okay, so now that we've done that, we're going to define our layers. So we have convolutional layers. Okay? So, uh, for our convolutional layers, let's talk about what's happening here. So in a standard convolutional network, uh, we have, what are, we have three layers and I'm going to answer questions in 20 minute intervals. So in seven minutes I'll start answering questions. But let's look at this architecture right here. So I think an inception is a good example. Convolutional net, right? So in standard convolutional networks, we have, uh, what are considered blocks. Okay. So it doesn't matter what these words say, but we can just think of these as blocks. Okay. So, uh, this is actually a huge convolutional net, probably one of the biggest in the world, in the world. Uh, but the point is that we have a convolutional layer. And in each of these blocks we have three things. We have a, uh,

Speaker 1:          10:20          we have an input. So typically a CNN is composed of a stack of convolutional modules that perform feature extraction. So each of those modules consist of a pooling layer. It's gotten the, and then, uh, fully, uh, fully connected layer. And then a, um, what's the next one? And then a soft Max activation function. So we squash it. So we just keep doing that every time. Okay. So we're going to define that programmatically here. Uh, so let's go ahead and do that. So, um, so the first line here is the name scope. So if so, or board we have named scopes, right? So named scopes basically define, so this, this one's not working. So right now, boys, you what? We're going to look at tensor board in a second, but right now let's look at the names go feature. So there are so many variables. There's so many tensors, there's so many components to a computation graph.

Speaker 1:          11:09          So what names, scopes do, is there a way for us to encapsulate all of that complexity under one name so that it's easier for us to view in the graph? Okay. Uh, it's easier for us to view in the graph. So in this case we're using a name scope, and we're going to define it when we call this, this function. But the, the name scope itself is going to encapsulate the weights and biases and the, uh, the convolutional part, the activation part. And then the pooling part of this, this, uh, this convolutional what we can call a block. Okay? So convolutional block consisting of these three layers of pooling, of, uh, of an activation function and the weights and biases. So this, it's all going to be encapsulated. And then we're going to see that in tenser board when we, when we visualize it in a second. Okay? So, uh, so up TF variable maintains the state of the graph. Okay? So that's what we were using variable for and truncated normal is going to output the random values from a truncated normal distribution. And so then the constant is going to create a constant tensor. Okay? So that those, those are our weights and our biases. This end, this is going to be for our first a convolutional block. Okay.

Speaker 1:          12:22          And then we're going to compute a twoD convolution, given a four d input. That is the 40 tensor that is coming directly from the place holder that we're going to define later. So right when we input data into our model, it's going to go right into this convolutional block. Okay? And then once we have that, we're going to create what are called histograms. So let's talk about TF summaries for a second. So what are these summaries and why do we use them? So, so a summary is a flow operation that outputs protocol buffers. So protocol buffers are a way of encoding, are a way of encoding, uh,

Speaker 1:          12:58          data. So it's, it's, it's serializing the data that we have in memory to disc. So we're writing it to disk so that we can then pull it intensive board and visualize it. So that's what the TF summary, uh, function does. And there are different things that we can use a summaries for. So intense or board. We have, we have different types of summaries and we can look, we can see what types they are by looking at the tabs up here. So we have summaries for scalers, for images, audios, graphs, distributions, histograms and embeddings. So we have different summaries for both and we'll talk about when we use those summaries and why we use them. But, um, for, for this case we're going to use histogram summaries because we want to see a distribution of values across the weights, biases and activations. Okay. Because we're randomly initializing these values right now, right? With this truncated normal function. And as, as we update those values over time, there's going to be a distribution of possibilities and we want to show what they actually are and then also what they could be. Uh, so then that, and that's useful for debugging. So we could then rerun our model with a different set of hyper parameters and see how those distributions move. So that's, um, what this function is. And then we have a Max pooling. Uh,

Speaker 3:          14:15          okay.

Speaker 1:          14:16          We have a Max pooling, uh,

Speaker 3:          14:18          okay.

Speaker 1:          14:18          Layer, which pooling in general is really cool. So pooling, I mean pulling, there's so many different types of pooling methods that we could use. And Max pooling seems to be the one that's used most often. So if we could, it's, so for Max pooling, uh, let's say we have a four by four matrix, right? We have a four by four matrix and that is our input image. And we have a two by two filter. So the reason it's called convolutional is because we are taking a filter, which is kind of like a flashlight and we are convening around an image and we're only using the parts of it that we find relevant. So it's like, uh, it's like taking a flashlight to an image and at what those relevant features are. And there are also these, there's a, there's another parameter called strides, which are essentially intervals. So at what interval do we want to look at? Uh, the part of the image, we're looking at every two pixels, every four pixels, every eight pixels. And depending on what your stride length is, uh, it's going to it that we need to tune that. So, so that our model is a better or worse. And we can do that through trial and error. And what, what pooling is, is it says, let's, um,

Speaker 3:          15:22          okay,

Speaker 1:          15:23          let's take the Max of a region and just use that as the input. So if we had a, uh, so if we had a, uh, let's see, let me show a little image for that and then we'll keep going with this. So this is pooling right? So this is if we could look at our image, because images are all matrices, right? Why we use pooling. So images are matrices. An image is a matrix of pixels. So if you think of this image right over,

Speaker 1:          15:56          I want to see if I can, there we go. So this image over here, just that, the big image over there and then we, that that's our image. So what pooling is, is it saying, what portion of the image do we want to use when we take that data and pass it forward through the network and work with Max pooling, we're going to say, well, if we were to split these up into squares like four different squares, if we were to add up the value in each of these squares, which um, which square, which a big square would contain the most, uh, values. And that's the Max values. And that's the, that's the square, the set that we're going to use. And that's a subset we're going to pass forward in the network. Okay. And pooling is the most popular. Taylor of the, of the, um, Max pooling is the most popular of the pooling methods. Okay. So that's what we did for one of our layers and right. So if, so following along we established our weights and biases we are and then are accomplished no layer. We applied a, um, we applied an activation function to it and we used Relu. And so why do we use Relu? Because a relu reduces the likelihood of the vanishing gradient, which it recall for it,

Speaker 1:          17:08          for neural networks, a recall for a, for LFTs and time series data. The vanishing gradient is a huge problem and it's also a problem in convolutional networks. Okay. So

Speaker 3:          17:27          yeah,

Speaker 1:          17:27          that's that layer. Now, now we have another layer and then we're going to get into actually building our model. So this is our fully connected layer. And so what this does is it performs classification. And we use this in all sorts of networks. It's not Esta [inaudible] connected layers are using almost every type of neural network. Um, and we use them pretty much before we use them before we squash it into using an activation function to output a prediction. So fully connected layers are usually found at the end of neural networks, um, at the very end. And what they do is they, and the reason that we use fully connected layers is so that we can use all parts of the data because we're about to because we

Speaker 2:          18:11          mmm.

Speaker 1:          18:14          Because we want to squash in. Okay. So that's what that is. And we're going to do the same thing we're going to use, we're going to use the histograms to create summaries for the weights by and activations. Okay? So that is that. And so now let's build our model. Okay. So we'll establish our graph and initialize our session. And now we're going to have placeholders that, and these are going to be the gateways for data. So imagine we haven't done anything. I've just, all I've done here is I have, uh, talked about what those layers are going to be and then now we're going to actually initialize them. So let's see if we have any questions. And then we're going to talk about what this looks like. How does Max pooling work with an, an image of only ones and Zeros? Oh, that's a good question. So the ones would then, uh, the, the, the part of the matrix that has the most ones would then be the pool that we use. That's the Max value. Two more questions and then we're going to keep going with the model. How did he get those keyword? Okay.

Speaker 1:          19:25          If we had infinite resources, we wouldn't do pooling. Right.

Speaker 2:          19:29          Uh,

Speaker 1:          19:31          that's a good question. So yeah. Yes. The answer is yes. Yeah. I've never thought about it that way. Yeah. Because all of a lot of machine learning is a trade off between computational complexity and, uh, brevity, brevity, be that in code or brevity be that in a training time, like we want to minimize the training time, minimize the amount of code we have, minimized the computational complexity so that we can maximize our, uh, and, but also maximize our prediction accuracy. So one more question and then we're going to get started with this. Does Max pooling use a sig Monday, a sigmoid function? Uh, no, it does not. It does not. Max Pooling isn't a sig, isn't that an sigmoid is an activation function. That's not Max pooling is, um, is an operation. Okay. So not to build our model, we have are placeholders for our images. So that's going to be our tensors. And then we're going to use this summary. Remember the summary, um, function of tensorflow to create a summary Protocol buffer for images. So we can visualize these under the images tab right here. We're going to visualize that in the images tab, and then we're going to define our model. So depending on how many layers we want, we have an if else statement. So if we have two layers, then we can use that function. We just defined it to then, um,

Speaker 1:          20:53          to then call the layer. And then, uh, so if we have to layer, then we're going to use that function. We call it to build a model else. We'll just use one layer. And then once we have those layers, we're going to, at the end, we're going to flatten it and then we're going to feed it to a fully connected layer. And we flatten it, uh, because it's, uh, it's more computationally efficient for our model to read, um, a one dimensional tenser than a two dimensional two dimensional tensor. However, there are ways of not having to fly in an image and then using that two dimensional, a tensor directly. But however it's more computationally efficient, uh, complex, not efficient. Okay. So those are our fully connected layers and uh, oh, sorry. Those are our convolutional layers. And then we'll take the flattened image and feed it to our fully connected layer

Speaker 1:          21:45          and then, uh, we will create those embeddings so that we can visualize them later. So at the end, once we have those fully connected layers, that's where we're taking our embeddings to visualize. So that's the part where the embedding visualization is going to, is going to happen. So data's flowing through our convolutional layers. Okay. Through the Max pooling, through the activation functions, it's getting flattened and then fed into the fully connected layer at the end. And that the result, the result that the fully connected layer creates, that embedding is what we're going to visualize. And it's gonna be so dope when we visualize it. It's so cool. Okay. So, um,

Speaker 3:          22:24          yeah,

Speaker 1:          22:24          and I'll answer questions in five minutes. So that's what we have for that. And so, and then we're going to take the luggage, which is the probabilities that don't equal one. It's, it's the output. It's we have the embedding, we stored that in the embedding variable and now the lug nuts are going to be the output of the fully connected layer. Hey Alexa. Hey computer. Do you like tensorflow?

Speaker 3:          22:54          Okay.

Speaker 1:          22:55          No, that was a little, we had Alexa here but she didn't work anyway. I thought it would be fun. So, uh, anyway, so the next step is for us to define our loss function. So we're gonna use a standard cross entropy loss function with luggage. So we, the log gets are all those values that we output, we squash it and then that's it. Once we have that, we can then generate our prediction from it. So this output is going to be our set of prediction values. Okay. From the, from this name scope, which we call x ENT, which is short for cross entropy loss. And the output of this. So x ENT is going to be our loss and we were going to visualize this loss intenser board as well. So remember member scopes are going to group operations together so that we can then visualize them as a whole intenser board.

Speaker 1:          23:39          Okay. So then, okay, so now we've defined our model and now we can it. So we don't just define name scopes for our model. We also want to define it for our, uh, for our tensors are. So for our, uh, training operation and our testing operation. So for our training operation we're using Adam. And so I've written down a huge explanation here of what, like why do you use Adam and when not to, but in a nutshell, Adam is, is, gives us better results than the standard gradient descent optimizer, but it's more computationally complex. So if you're willing to make that trade off, then I would go for Adam. Uh, and most of the papers these days that are using a convolutional nets, I tend to see Adam used more often than the standard grading dissent optimizer. But if you're a beginner, then I would go for great dissent optimizer. Okay. Okay. So, and let me try Alexa one more time. Do you think I should? Hey Alexa or, hey, hey computer. What Day is it today?

Speaker 2:          24:39          What Day is it today?

Speaker 1:          24:42          It's Wednesday, April 5th. It doesn't like me. Okay, boys, the boys. Okay. So anyway, that works. Anyway. Do we have our training? We have our accuracy. Ho. So hold on guys. We're going to, we're about to visualize this intense or board. I know, you know, I've been waiting for that to let's, let's just get through this code. Okay. So we have 380 people here from all over the world. Okay. We are growing so fast. It's amazing. It is amazing. So I am so honored to be here with you guys. So where were we? We have defined our training function and now we're going to define the, uh, the, the last name, scope. This is the last name, scope that we're defining here, which is the accuracy. So we're going to take the log, it's, and then we're going to, which is a collection of the probabilities. And we're going to choose the, the, the, the, the average, which using the org, or sorry, not the APP, not sorry. We're going to use Arg Max, which is going to get the largest value. Uh, and then we're going to get the average using the reduced mean function and accuracy is going to be one scalar value. That is our prediction right there. The accuracy. Okay. My desktop is pretty crowded. It's a lot of editing. But uh, anyway, where was I?

Speaker 2:          25:49          Okay.

Speaker 1:          25:50          Make this bigger. Okay. So now is it's time to look at the novel stuff about tensor board here. So in a lot of the initial tensor board tutorials, it didn't have this because the embedding visualizer didn't exist, but now it does. So what we're going to do is we're going to, we, the reason we, we, uh, summarize the all of the summer, the reason we merged the summaries before we write to disk is because it's more computationally efficient. Uh, instead of continuously writing them to disk, we just merged them all at switch one, right? Versus like 10. Okay. Okay. So,

Speaker 1:          26:26          uh, where was I? So we did that. And now we're going to initialize our embedding matrix as a, as an array of Zeros. And then we're going to assign it the embedding that we just calculate it. All right? So now it's in our assignment. Okay. So now it's in our assignment and now we're going to initialize a saver. And the saver is used to save and restore all of our variables. Uh, and so then we're going to use initialize the file writers. So we have our summaries and then we have the file rider. So the summary, uh, operation creates the protocol buffers so that we can write it to disk. And the file writer is that, um, it is that

Speaker 1:          27:05          method that we are able to actually write to disk. So we create summaries and then we send them to this using the file rider. So they both, they both, they both go hand in hand. Okay. So that's why we use the fall rider. And so this part needs to be better. Let me just say that this part needs to be better. So in the initial, uh, in tensorflow examples. So there are, there actually are not a lot of examples of tents or board that's use, um, that use this, um, there that you can look at the embedding visualizer in there. Not a lot of examples of tens of board that you can use the embedding visualizer in. So there should be, so this is one of them. So definitely check out the get hub repo because there are not a lot of them. The problem with this is that it's using the config file directly from tensorflow. Now I would like it so that we can define our own configuration file. But if we look at this profile, let's look at what this looks like. It's pulling it from the web. And so let's look at what this looks like.

Speaker 1:          28:05          Four oh four. Here man. We've got to, we've got to clean this, this up. No, it's not this, there it is. So here it is. So this is, these are the, um, these are the options that we're using to build our embedding visualizer. Uh, so we have a tensor name, we have metadata, and then we have our project configuration. Okay. So we can also define these locally if we want it to you by creating a pro, a pro buff file, um, uh, locally, and then calling it from disk. But right now it's calling it from the web right here. Okay. So we could add multiple embeddings, but we're just going to add one. And that's what we've learned from our embedding matrix. Okay. So

Speaker 1:          28:49          we have that and we're going to specify the width and the height of it with a single thumbnail. Okay. And so they're 28 by 28 pixels. Okay. So that's it for that. And then we train it and then we save the checkpoint every 500 iterations. So this thing is going to run for 2000 iterations with, with an, each batch is going to be a hundred. Uh, and then, yeah. And so that's what the training step looks like. And then we have our main function where we call it. And then this, this make h parameter string function just converts a hyper parameter string, uh, to, uh, one that is more detailed for us. It's just for us. Okay. So that's the code. I want it to kind of blow through it a little fast because I wanted to get the tents are board. So before we get the tensor board, let me ask if there are any questions before we get started with this. Okay. I'll take two questions and this was before I started.

Speaker 3:          29:42          Oh my God. You guys know how it is with demos. Like literally it works perfectly. It works. You could rewrite, I literally stopped it and rerun it like three times before starting it. Boom. Tents board every, you know, that little command line with tensor born and it was running. And then five, literally almost like the gods of deep learning. Where of of Demo. Hell, where there this I should say say they didn't make it go or it didn't work. So we're going to figure it out. We're going to debug it in real time and let's see if we can make tens of board work in real time. Okay. So we'll see. So two questions. How do you interpret the histograms? I have trouble giving meaning to the acce axes. That's a great question and I'm going to talk about that when we, when we look at it, are there any third party wrappers for tensor flow so as to convert symbolic programming to kind of simulation for end user in object oriented programming?

Speaker 1:          30:37          That is a big question with a lot of parts to it. Third Party wrappers for tensorflow, yes, there are tenser layer, look at tensor layer, Google that tensor layer. Are you using GPU version of tensorflow? Uh, yes I am. What is printable file do? So protocol buffers, Google invented this, um,

Speaker 1:          30:57          like a couple of years ago, like six, seven years ago. But basically protocol buffers are a serialization methods. So serializing as a way like pickling is a form of serialization. It's a way of taking some data and converting it to some standard format, like some generalized standard format so that you, so that is so you can save it to disk and then recreate that data in a, in a later form. And another form later. One more question and then we'll get started with this. How would you detect fake news using ML and DL? So guys, we are at a point right now where the tools available to us to generates data are getting better and better. So fake news will get more and more realistic. But at the same time, the classifiers, we have to detect what's fake and what's real will get better as well. So it's crazy if you think about it.

Speaker 1:          31:46          Uh, viruses we'll get, we'll learn to fight. Well, we'll get better at learning to find vulnerabilities in systems, but at the same time, virus detection algorithms will learn to get better at detecting what is trying to attack it system. So it's this, it's this battle is this constant battle with both sides getting stronger and stronger. And machine learning is at the forefront of it. And the way we make sure the good side winds is by spreading AI. So making sure everybody has access to it because if only a few have access to it, then bad things can happen. So learn Ai, tell your friends about it. Spread AI awareness, tell everybody about it. We got to get this power distributed to everybody to prevent bad things from happening. So we're moving into a very beautiful world and we have to make sure that everybody has access to this power. Anyway, so that's it for my rant on that. Now what we're going to do is we're going to visualize what we've just written and we're gonna make sure that it works. So what happened here is to run this. So let's see, okay, we want to run this intense or board. What have we done here? We saved it to this log directory right here. So it's in TM, TM m and I s t tutorial, and that's where we saved it. Okay? That's where our file writers saved it. So now what we want to do is we want to,

Speaker 1:          33:09          we want to, um, train our model, right? So let's train this thing. So we'll say like a bigger python, uh, where am I am and I see seed a pie. And that's going to train the model. Hopefully it will train it. So good. So good. So good. Okay, good. So great. So now it's training the model. Okay. So while it's training, and I encourage you guys to train it, it's going to take about five to seven minutes on a CPU and

Speaker 1:          33:48          Oh, it's going to take about five to seven minutes on a standard CPU on your laptop. So don't even worry about not training it locally. You could totally train locally. So while it training, we're going to run tensor board. So to run tensor board, now all of you who have tensorflow, we'll have tensor board. There's nothing extra you have to um, do. So let's initialize sensor board. I can do a 10 I can do a Qa livestream someday, but right now we're going to just run tensor boards. So to run tensor board, we run tensor board and then it's, what was it? It's log directory equals and then the path that we saved the logs too. So what happened is it stopped working five minutes before the demo. So I'm going to paste it and let's see what happened. We're going to debug this together.

Speaker 1:          34:34          It's definitely going to throw an error for some reason, which is so annoying, but it's, it is what it is. Come on baby. Let's do this. Okay, so this is what I'm talking about. So normally this would show the, the URL that we could then visit in our browser to visualize our tensorflow computation graph, but instead it's showing this error, tensorflow starting tender board be 41 on port six oh six. So I don't know what that is. And so I was googling this one minute before the stream started, but I found that someone else had this issue, tends to board isn't showing, and this was a not a recent issue, this was a year ago, but he had the same issue. And then Damn Manet, who I interviewed, who is also the guy who's, you know, in charge of this, this stuff on the tensorflow team said something and I'm like, Blah Blah, blah, blah, blah. I mean this, this thing goes on forever, but what's the result? Is this what I do? I can just kind of like zoom through the, uh, this, the stuff. And so, okay, so he had debug. So he, so maybe adding a debug flag at the end would help. So let's see what happens here. And again, guys, if it doesn't work, we're going to, we're going to visualize it somehow. What? We'll figure it out. Okay. So debug, let's see.

Speaker 3:          35:55          Oh No, fuck right there. Okay. You need this right now?

Speaker 1:          36:12          Um, no, it's okay. Okay. So then we could try it again with the debug flag as the get hub issue says. And um, okay. So yeah, Ben has a good idea. Let's just open up local host and see what happens. Local host 66, what was it, six or six? No, it was, um, six, six, six oh six. How many paws training here? Because it's really taking up compute and I don't need that right now. Okay. So

Speaker 3:          36:57          yeah.

Speaker 1:          36:59          So something here is loading up. Um,

Speaker 1:          37:07          okay. Yes. Okay. Yes. I'm so happy that this is working. I guys, I'm so happy that this is working right now. This is the greatest thing. Who is this? God. Ben. Benjamin Schulz. Larsen. Shout out to you. Wizard of the week. Wizard of the live session. Okay. So here's what's up. Let's visualize this. Okay. So, um, where were we for our scalers? So let's look at the, you know what, let's just go straight into the embedding. Cause that's what I'm most excited to look at. Look at this. You guys who have stuck around how get to see this amazing, amazing, amazing visualization. So let's talk about what this is. So what this is is it is our, these are our embeddings that we generated. So remember in our fully connected layer, we, we fed those into our embedding visualizer. Okay. And the way that we are visualizing them is with those sprites. So remember the sprites are, these are these files and it's like a matrix. We're cutting it. It's like a matrix. All, I dunno how many are there. There's like 28 by 28 or something like that. But these sprites represent each of these embeddings that we've learned. Okay? So what, what we're looking at, right, right now is a visualization of this craft and using PCA, principal component analysis as a technique to map them out. So,

Speaker 1:          38:43          so let's talk about what this, what this is. Okay. So PCA versus Tsne is a, is a, is a good question. So when, what do we use PCA, and when would we use p a tsne. So, um,

Speaker 2:          38:58          actually I had my notes on that. Um, so I forgot about that. Thanks.

Speaker 1:          39:07          So Pca is a technique that we use to visualize data and it's actually most of the time you'd want to use PCA over tsne most of the time. Okay?

Speaker 2:          39:20          Okay.

Speaker 1:          39:21          Most of the time we'd want to use a PCA over Tsne, but, um, in this case, but, and let's look at what a tsne looks like. Boom. Let's find the nearest neighbors using tsne and watch it move. Let me, let me make this big. Nope. Wrong way the hell. Okay. Okay. So you see that they're clustering here. These values are clustering in, in, uh, in, uh, in, uh, where they're supposed to be. So all the six is, and all the threes and all the fours, they're all clustering together. And, uh, what we want to do is we want to,

Speaker 2:          40:02          uh,

Speaker 1:          40:03          compute the distances between them. So there's a lot of things that we can do here. Once we have them visualize and we're going to talk about what all those things are that we can do once they're visualized. Okay. So before, so there's a lot we can do here. Uh, and

Speaker 2:          40:18          okay.

Speaker 1:          40:18          The idea of SME and Tsae is to place neighbors close to each other. So that's what this is doing. And, but the thing is that it almost completely ignores the global structure, but PCA is the opposite. It tries to preserve the global properties and the, those are the eigenvectors with high variance. While it may lose, uh, um, it could lose the low variance deviations between the neighbors. So it's a trade off and there's actually a great a stack overflow link for that in the, uh, get hub like right at the very top, the sack exchange showing like five reasons you do want to use PCA over tsne. Um, does, that's what that is. So what we can do here is we don't even just have to visualize the, uh,

Speaker 2:          41:07          okay.

Speaker 1:          41:07          We don't have to just visualize the, um, okay. So there's a lot of comments here about

Speaker 2:          41:17          my hold on

Speaker 1:          41:19          my name and stuff. How many people we have here live? We're at 440 we have four. So I'm here to say this to 440 people live. Okay. My name is Sarah [inaudible]. I am Indian. My parents are from India. Let me just say this. Okay. When I was 18 years old, I legally changed my name to Jason Scott rebel when I was 18 years old because I wanted to do great things and I felt like the only way to do that was to be white or at least anglicised. It was only three years later that I legally changed my name back to Saroj revolve. I learned to love myself over time, I learned to love my identity and now I am unabashedly Saroj revolve an Indian and an American. So I just wanted to say that publicly. It's embarrassing to admit, but it, it, it gives vulnerability, but it's also a point of, it just needs to be said. It needs to be said to the world. I am Saroj. Yes, my parents are from India and I love being who I am. Okay. So I am very Indian and a, I love it. Love Indian food, love all the Indian culture. Love India. I visited winter for six months and uh, yeah, I just wanted to say that life. So where were we?

Speaker 1:          42:35          Uh, yeah, a lot of racism growing up in Texas. But uh, anyway, back to this, back to PCA and TSN. Yes.

Speaker 2:          42:42          Where were we? Um,

Speaker 1:          42:47          where were we? So, uh, what are we doing here? Dimension. So we have two d versus three d. So we want to cheat. There's, there's several things that we can do here with our graph and there's a lot of things happening at once. Let me focus here. Okay. Where were we?

Speaker 3:          43:02          Yes.

Speaker 2:          43:09          MMM.

Speaker 1:          43:18          Hi. Thanks guys. I appreciate the support. Anyway, I guess I did want to see the comments and see what people thought of, you know, it's, it's one of those things that I haven't really admitted, um, before too. A lot of people. Um, but you guys are my, you guys are my homies. You guys in my crew and uh, I'm just going to continue being real with you guys and you know, I'm good. I'm going to continue giving you all of me, all of Saroj, every part of me, every part that I've always been afraid to show, whether it be rapping about Tsne or whatever else it is. You have a very unique part about you and you probably don't even realize this. You have a very special skill set and if you find a way to encompass all of the things you can do into one, um, go one activity, you will find success. And that's what I'm here for. I'm here to help you be successful. That is my goal. That is a reason that I do this. I want to help you become awesome because if you're awesome, then our society will be awesome. This is power unlike anything we've ever seen before. If you're able to understand this stuff, you're going to be amazing. Okay, you're going to do great things. So where were we?

Speaker 1:          44:20          So Rod, you deserve a plate of Biryani. That's my favorite comment of the day. Where were you? I would love some. Okay. So, um, man, I'm just saying all sorts of things right now. We were somewhere, we were somewhere else right now. We were,

Speaker 2:          44:33          uh,

Speaker 1:          44:36          we were talking about DSME. So we have our tensors and we have our,

Speaker 2:          44:43          mmm.

Speaker 1:          44:45          The color map and our labels and our dimensions. Oh, you can change the colors too. So check this out. You can change the colors too. So we can change the color by label. We can us. So when we sphere eyes, the data, it, it, it puts it into it more ball like structure. And we can also search for a different things that we want. So if I can say seven, it's going to show all the sevens. So this can also be used for word vectors. So remember in one of our videos we talked about the difference between man and woman, like man plus woman equals, well, no, a man plus woman equals child or something like that. But Keon Queen that also works in here. And I would love to show you guys that later, but a natural language processing, um, later on.

Speaker 2:          45:25          Um, uh, so yes. Um, so six,

Speaker 1:          45:37          five, four, three, two, one that was a count down to nothing. I was just typing this out. How cool is this though? You could, you could, you could visualize it and you don't have to just visualize your data. You can also visualize the weights. You can visualize your biases, you can visualize a lot of different things. And in fact, you can visualize stock prices. What I would like to see, and it, this depends on the tensorflow team releasing plugins for tents or board is, um, is, uh, I would like to see more plugins for this. So I'd like to see more people make things for this so we can choose different components. Um, what else can we do? We can,

Speaker 1:          46:16          dude, oh, night view. That's, that's, that's my stuff right there. And we can also select parts of it. So look at this. So if we select this bounding box selection and we select some specific part of it, will we can then get those parts. And then I wanna I want to show the, uh, the coastline distance. So let me show how to do that. So if we were to isolate the points that we've just bounded in a box, we could then find the coastline similarity between those points, which is so by the way, bookmarks are to share this with other people so you can download it and share it with other people. Um,

Speaker 3:          46:51          okay,

Speaker 1:          46:51          now I want to,

Speaker 3:          46:57          yeah,

Speaker 1:          46:59          yeah, sure. So thanks. And we can do custom visualizations as well. Um, we can say like from three to take these labels and then match them using some vector

Speaker 1:          47:16          use case. Um, a random vector. Anyway, let's talk about the other parts as well there because it's not just this, there's, there's more to it than this. We have, uh, nine minutes to go, right? Is that or, okay. We have minutes ago. Those last four minutes are going to be for questions. Let's talk about these other parts. So the scalers tab is for scalar summaries. These, those are single number values that change over time. Okay. And what we have here are the accuracy and the cost function, which is [inaudible]. The x axis shows the time steps and the y axis shows the accuracy or loss. And if we can increase the graph for a closer look, but by doing this just like that and, or to view a wider range of data points depending on the, uh, and which expands the y axis, just like that.

Speaker 1:          47:59          Okay. Um, we can also, um, DoubleClick to zoom out. So double clicking zooms out. So we zoom in and then double click to zoom out. Okay. And the step option shows the time steps. Okay. And so a great thing about this is, so right now we only have two of these, a two scalers, but if we had multiple scalers, so if our computation graph was really complex, then we could group them together. So if I type in accuracy or sorry, if I could type in like lol, it's like what's going to be here? Nothing. But if I type in accuracy, it then encapsulates the accuracy that we already have. So if we have names like, you know, accuracy slash accuracy, one, accuracy slash accuracy, two, accuracies as accuracy three, and then ours to create accuracy right here, it would then encapsulate all of those. Okay.

Speaker 1:          48:49          So that's great for um, doing, uh, in the browser encapsulation, similar to names, scopes, but kind of like a Gui version of that. Okay. Save your question to the end and then smoothing makes the graph smoother or less smooth. Uh, less smooth is if you want more. The trade off here is less pretty but more accurate. What else we got? We got relative and then wall, um, relative showed the time relative to when it started, whereas wall shows the time of training in general, like in, in real world time. What else do we have? We have our images, so these are the um, the image summaries that we created. We can visualize them right here. Okay. Um, and then we have audio. So I haven't actually seen somebody use audio for tensor board and believe me, I have looked, I have looked on the web and I haven't seen anybody do it.

Speaker 1:          49:39          Um, so I would love to see it. I would love to see someone and I actually, you know, I have, let's see, someone use wave wavenet or one of these things that used to look at audio. I think Magenta might, but Magenta is so hard to get started. There's so many scripts and it's very confusing. So let's look at our graph. Uh, I can't believe I saved the graph for last. But the thing here is the graph is these are our names, scopes, right? So we these continents to calmed one fc one. These are the names scopes that we created. And if we double click, we can then see the parts that are encapsulated in the name scopes. So why are these colored, uh, a different, uh, colors? So for every color, except for gray, those are the default colors. So for every color, um, flow automatically looks at the w the data inside of these operations.

Speaker 1:          50:31          And if they're the exact same, they know, color it the same. So it's kind of a, it's, it's a way of organizing data. So it's cleaner and it's a way for us to know that it's going to be the same type of data. Okay. And reason they did this is because in its inception there was some bug and it was because in one of the layers it was like one small difference. So now they have this by default. So colors, it's going to color code itself. And if you look over here at the side, you see this in it all by itself. That's because tensorflow detected, sorry, tents are boring. I'm sorry. Tenser board detected that there was one operation that was continuously used a lot. And so then it put it into its own section because if we didn't, then it would be, it would look even, it would look crazy.

Speaker 1:          51:18          So let me show you this. We could put this in its function back into the graph and then it looks like this because it's used all over the place. But if we removed from the graph, we could see all of its connections and have the graph be cleaner. So the reason it does this, this auxiliary, auxiliary node function is to make it cleaner to view. Okay. And then, um, we can also structured by the, uh, by, by device, whether that's CPU or GPU or the structure. And we can look at everything that's inside of the graph like this and all the layers and increasingly, uh, so much complexity. I love it. I love it, man. I wish I had time to just make my own project and just make it just really go dive, dive in on this. But I'm having so much fun making content right now, so I'm just going to keep going with this. Uh, two more things. I want to show really fast or the distributions and the histograms. So the distributions are for their weights and our biases there for layers. Remember we, we randomly initialize them but they change over time and we want to show what they actually are compared to what they possibly could be. So that, so the possibilities versus the actuality or reality. And uh, one more is our, the histograms. So histograms are, um,

Speaker 1:          52:37          for a histogram plot allows you to plot variables from your graph. So if your model has weights, the histogram showed you the values of those weights and how they change with training. So this is, it's nice little three d looking graph, which, which is pretty cool. And uh, yeah, so that's it for this. Uh, we've got three more minutes for questions. So let me, let me answer some questions and then we're out of here. Uh, wow. That's, so my, there's even more people in here now than there were before, which is insane. Now. Now there are 4,432 people here. Okay. So let me see what people video did not pause in deep neural networks. What is a, what is an alternative for gradient descent for minimizing errors and deep on for progression, for minimizing errors? So that's a great question. Um, there's the castic grading descent, which is different from standard gradient descent. There are, um,

Speaker 1:          53:40          a bunch of different optimizers at a Grad. Um, Adam, if you look on the tensorflow documentation for optimizers, you'll find a list of them. Two more or two more questions. Well, deep learning, solve intelligence. So spoiler alert. I don't think so. I don't think deep learning will solve intelligence. I think it's a pathway to get there. We need to make more computationally efficient models with, um, that need less data for the same result. And I feel like there's still something that we're just missing something, something very fundamental, something very basic. And, um, yeah. Uh, but we're, we'll, we'll get there. We'll get there. Uh, let me know two more questions cause we are, we have two more minutes. How do you explain the evolution of the weights distribution given the evolution of the gradients distribution? So recall, and I have a great video on this backpropagation in five minutes, but the gradients give us a direction to update our weights. So they are, and your, your question, well it's gone now but how do we, something about relating the gradients to the weights. So the weights.

Speaker 2:          54:41          Okay.

Speaker 1:          54:41          A great way to model that is to have, um, histograms for both the gradients and the weights. Right now we only have histograms for the weights so you can see how they both change in real time. One more question. Two more. Does our brand news back prop and no, it does not. Our brain does not use back prop. Um,

Speaker 2:          55:00          uh,

Speaker 1:          55:01          Adam optimizer versus STD. Uh, I would say SGD. And one last question. Is Lb fts a true linear regression technique?

Speaker 2:          55:14          Uh, uh,

Speaker 1:          55:15          Lbf Gis is one of the most complicated things that I've come by. I have to admit, um, I, I, I don't think it's a linear regression technique.

Speaker 2:          55:25          Um, yeah. Anyway, yeah.

Speaker 1:          55:30          Blusher full was the LBF Jess was at blusher fetcher gesture session or something like that. It was the name of four scientists. Um, but, uh, anyway, so that's it for this ice cream. We got 400 people at the end of the livestream. That's amazing. Thanks guys for watching. For now, I've got to go, um, make more technically accurate, more mathematically accurate, uh, content for you guys because I really want to be increased the learning capability of my content. So I love you guys. So thanks for watching.