Speaker 1:          00:03          Hello everybody. Let me load up the chat room. Live stream is starting soon. All right, so let's get into that room with all the live people loading, loading, loading. Oh, all the loading livestreaming events. Boom. I'm in the room.

Speaker 1:          00:33          There. I am. All right, cool. All right. Hey, hello world. It's the Raj. Good to see everybody. I'm going to list some names. Uh, uh, psych, Collins, [inaudible], Ricardo, Spencer, Daniel Preston. Now while happy new year, Sagar, she has on Lance Xavier. How's it going guys? Okay, so today it's good to see you guys. Good to be here. I'm excited for this live stream today. Uh, we're going to build a work, well, first of all, we're going to install opening eyes, universe, and then we're going to build a game Bot to get better at it. And it's our own custom game bots. Um, it's going to be about 70 lines of code in Python. And then, uh, I'm gonna, I'm going along the way. I'm gonna explain how it works. Okay. So, first of all, as always, I'm going to start off with my five minute Q and a and then we're going to get started.

Speaker 1:          01:30          All right. All right, here we go. Um, how much math, you know, behind all this AI and ml? Uh, I, I, I've studied linear Algebra, I studied statistics and calculus. Um, in terms of how much do you need a, I think, I mean, it depends on the level that you're trying to, you, you know, what you're trying to do. If you're trying to do research, you definitely need some math. It's no question. If you're a developer and you're just trying to implement some machine learning into your app or use an Api, you don't obviously don't need math. And if you want to do something in the middle where it's like your developer, but you're also, you know, you're doing some innovations in AI, having some math is good and I'm going to start teaching more math in my, on my channel. All right. Uh, hi, uh, Alexandra and massive Macedonia.

Speaker 1:          02:21          What do you do for living? I do this channel full time. Uh, how do you get started with tensorflow and say my name please? Rajat uh, with started with tensorflow, uh, I have an intro to tensorflow series on my channel. Uh, what is opening at universe capable of other than games? Uh, great question. Uh, so there are environments, so, uh, their environments for doing tasks like sending an email or uh, you know, uh, doing some kind of a mouse clicking keyboard he event. Uh, so there's a lot of different set objectives and they're adding more over time because it's open source. Can we get tensorflow working with goaling? There's gotta be a rapper on get hub, just search tensorflow and then click on go tell me if something shows up. I actually haven't searched for that. I haven't even thought of that. You're doing a great job, sir.

Speaker 1:          03:07          Thank you. Thanks. Thanks. Bartos which is the best tutorial to learn python to be efficient at ml. I have a series called learn python for data science. Check it out. When will the basic deep learning video series come out and will it also feature the math? Uh, yes, it's coming out on Friday. Uh, it's something really big, something really big is coming out on Friday, guys. Uh, so it's a lot of pressure. Uh, but this is going to be something really big on Friday. Uh, how long will it take this video? It's going to take about 45 minutes. Um, I Saroj our Google not releasing their best tensorflow library. Uh, no. I mean, I mean Google does have a lot of internal code that they're not going to share with us. Uh, but tensorflow is, is, you know, I mean that's, that's, that's a good portion of what they have. Are you going to make a Gan live tutorial? That's a great idea. David. I gans are really, really, really cool. And, uh, I made one video called generative adversarial networks. Uh, but I'm going to do more. Um, all right, so three more questions.

Speaker 1:          04:09          Uh, where are you now? Saroj um, in my room in San Francisco. What do you think about Mark Zuckerberg? Ai? I talked about that and like two streams ago I thought I've, alright. Are you going to write code with you? Yes. What do you think about Pandora bots? Um, I haven't heard of that. All right. Um,

Speaker 1:          04:31          will you do something with videos? Uh, I guess you're talking about machine learning with videos. Yes. Is a nanodegree worth it? I, yes, I think it's worth it. And, uh, speaking of nanodegrees, uh, something cool is coming out, so just I can't, I'm not gonna I'm not gonna say what it is. What college did you go to? Columbia. You smoked marijuana? Yes. Um, and no apologies. So, um, anything with d three. Dot. JS and AI. I have a live stream called, uh, uh, well that's a visualization without AI. Anyway. Um, cool. Hi. Lori is all right. So, um, that's it for the five minute Q and. A. Uh, let's get started with this. We're going to install a opening eyes universe and then we're going to, um, build our game bought with it. All right, so let's, let's get started with this. I'm going to go ahead and start screen sharing. So obviously it depends on your environment, like what you're running, but, uh, in this demo we're going to, uh, let me just start screen sharing. Here we go. Screen share desktop. Okay. So,

Speaker 1:          05:44          so, okay, so here we see the actual repo, right? This is the repo for opening eyes, universe. They've got to read me here and they're explaining that it is a software platform for basically training an AI in thousands of different environments that got Flash Games. They've got browser tasks, as you can see here. Um, like, you know, sending emails and you know, more trivial things like that. Uh, and even like, you know, things like grand theft auto five, which isn't included yet, but it's coming very soon and more games like that, three d games are going to come soon. Uh, so, so there's that and then it's got the install instructions. So what I'm gonna do is I'm just going to go through these install instructions with you guys. Uh, and I'm going to do it for Mac because obviously I'm on Mac, but this also applies to Linux.

Speaker 1:          06:28          Uh, and if you're on a windows machine, uh, you know, so the, the intersection here for windows machine is when we talk about docker. Okay. Well, so we'll get, we'll get there. So let's start off by installing universe. Let me check what you guys are up to. Um, cool. All right, cool. So here's what we're going to do with, the first thing we're going to do is clone the environment. So what I did is I went ahead and, um, I'm going to open up terminal. Okay. Uh, and I'm gonna say, uh, let's see. I'm going to create my, let me make this bigger. Let me make it way bigger. Uh, CD fun, fun. Okay. So I'm in my directory right now, of, of, of, of, uh, okay. So let me make a super big, okay. I'm going to make it super big for everybody. Everybody can see that.

Speaker 1:          07:13          All right, here we go. So here's what we're going to do. I'm going to put this over here. Let me see what you guys are up to. All right, cool. All right, so that's installed this baby. All right, so we're going to start off by cloning this thing. Let's go ahead and get clone. Get clone. All right, so that's going to download it. Boom. That was fast. All right, so next seventh, the CD and the universe. All right. Little CDN. All right, we're in our, let's look. Let's look at what it looks like. This, this a font sizes. All right, right guys. It's fun sizes. All right, cool. Uh, right. Okay. So, um, we seeded into university and now we're going to install our dependencies. So what is this line? It's a the dash e variable means in editable mode. All right? So we're doing this an editable mode, uh, which makes installed packages editable, and it's reading which packages to download from setup.py.

Speaker 1:          08:04          So you see this setup.py file here. Let's, let's like Nano into this for a second. I just like, see what this looks like. It's set up. That pie is a list of the, uh, dependencies and you can see them right here, like pillow and Jim and docker, py, things like that. All right, so that's what that is. So we're going to go ahead and run our, uh, install wine, different stall e. So that's going to install, it's going to, it's going to download this dependencies from, oh, shh. All right. Yes, I need to be in pseudo mode, right? Permission denied. I was just denied access. That always happens to me. All right, so we got our dependencies, right? All right. Um, pillow gives you nightmares. Well, anyway, okay, so we've got our dependencies and so then it, then it kind of splits off into whatever platform you're running on Ubuntu, you want to install num Pi.

Speaker 1:          08:56          Um, and just in general you should just have num pi and your system because pretty much everything machine learning is using non Pi these days. Um, so, and then you're using APP to get to install Golang. Uh, and you know, we on, uh, go and you're going to need go lang because we're using that for our B and c driver, which I'm going to talk about in a second. So for us, we are, uh, sorry. So, uh, us as an Osx users, we don't have a package manager like apps get, so we had to, so we have to use something called a homebrew, which by the way, they inventor, it got rejected from Google for not being able to do, I don't know if you guys heard about that. They're the red black tree. This was like a year ago, but I thought, I just thought that was hilarious and a, and a good commentary on the state of our programming interviews, which I still have to make a video on how to ace the programming interview.

Speaker 1:          09:40          I know you guys want that. So that's going to come soon. I just have a lot on my plate right now, so that's coming soon. Anyway, so, uh, so for Osx it's gonna say you're gonna need command line tools, which I already have, so I'm not going to run that. But, um, so there's that. And then, uh, right, so and I have these dependencies, uh, right. So I'm just going to go, I'm going to go ahead and just install them anyway, so I'm going to overwrite what I already have. Boom, I've got that. And then I'm going to use homebrew installed Golang and live jpeg turbo, uh, which is, which is going to help with, uh, like image processing. Right? So it's going to update homebrew. It's going to download these packages. Okay. Um,

Speaker 2:          10:18          okay.

Speaker 1:          10:19          All right. So,

Speaker 2:          10:21          okay.

Speaker 1:          10:22          Right. So for windows too. Um, all right, so now we're on, so we've got those dependencies and now we're going to install docker. So let's talk about docker for a second here. Okay. People need to understand how awesome docker is. So, so this is the docker website. Okay. So this is how we install docker. And depending on what operating system you are, there are different install processes. I recommend the binary, whether you're on windows or Linux or Mac, you should use the binary, um, because it's just easier. It's like a one click install. Uh, but there's also, you know, you can install from source, but I would prefer my preference preferences binary. Okay. See, I've installed a binary docker and so I've seen like right up here, you see docker is running, it's just running up there. I've got, you know, uh, I could get my diagnostics and preferences and I can update them.

Speaker 1:          11:08          So little pretty guy and that's fine. Uh, but so let's talk about Daca for a second. All right, so docker is, so let me explain with docker. So, okay, so what's a good analogy? Docker is kind of like a TV dinner. Okay. Uh, it's, it comes in a box. It's got everything you need right there in the box. And when you're done, you just toss it away. Okay. So that's, so docker is like, it tries to make apps like TV dinners and so virtual machines are kind of like that, but they're not quite the same. So it's like, let me write this down. So like what is docker? Docker, so let me, let me make this bigger. Oh, okay. So let me waving doctor is like a TV dinner. Okay. It's got everything you need, loaded it in a meal all in one. Right? So docker, it's kind of like a virtual machine, except it's less bloated than a virtual machine.

Speaker 1:          12:03          So like a virtual machine, but, but sinner, so it's, it doesn't have the bloat of a virtual machine. So Brooklyn machines typically including an entire operating system, but I've, docker ecosystem has a central engine that can run multiple docker containers. So it cuts out the bloat of having multiple operating system and just simply, it just packages the runtime elements that you need. Okay. So it's just a run 10 elements that your program needs and dockers. Awesome. Because it, you know, it, it removes like all the dependency hell that comes with every app. Someone suggested. I dockerize literally everything I do from now on, which I will seriously consider I need to get better at. So that's a, that's a short primer on docker. Okay. So, so in terms of docker, we're going to install docker and a TV dinner. Is that docker? Okay. So there were, so we've installed, so we're going to install docker and [inaudible] run the binary for docker. Uh, and so once you've installed the binary, um, I'm not going to reinstall the binary for docker. It's just, it's literally just like a three step process, like next, next finish. Uh, but once, once you've installed the binary, it should be linked to your terminal. Okay. So you'll be able to call it, um, from terminal. Okay. So, um, so, and that's it. That's it. So docker was the last step for that.

Speaker 1:          13:22          And, uh, once we have that, we're going to go ahead and create our agent. So they've got a, they've got a sample agent here, but we're going to go ahead and just create our own. Okay. So we're going to write up, um, so we're going to, we're going to go ahead and write up our script. That's where it's going to go. Go right into it. Okay. So, um, let's, let's go ahead and get started with this. Okay. So the first thing I want to do, so let me talk about what this agent it's going to do. It's going to do two things. Well, first of all, the first thing is going to do is our age. Our is going to do, our Bot will do two things. The first thing is going to do is determine, uh, should, should, should I turn right? And then the next step is going to be um, where to turn.

Speaker 1:          14:11          It's a two step process where to turn, okay. And this is going to be for the coaster racer game, okay? With you can see right here, okay, this is the coaster rager get racer game. It's not going to be this default agent right here. We're going to build our own thing. But it's going to be right. So, and uh, and by turning I'm talking about which direction, right? Laughter, right? Which were, which direction do we want to go? Left. Right. And that's what our Bot will do. It's gonna be about 70 lines of code and it's going to, it's going to decide where to turn. Okay. All right. And um, yes, it's reinforced learning. Um, and I'm using sublime text. Uh, you guys are all awesome as well. Thank you. Okay, so let's get started with this. So, um, the first step as always is to import our dependency.

Speaker 1:          14:51          So the first one is going to be Jim. Why are we importing Jim if we're using universe? Well, Jim is universities based off of Jim. Jim was opening eyes, original attempt at creating an environment to run box in and they had games like cart pole, you know, they had things like that, but universe basically expanded on it like way more. So we're going to import boats. Okay. And so we've got both of those dependencies. And then we're going to import random because, uh, we're going to implement a random policy here, but it couldn't be a smart random policy that's using reinforcement learning. Okay? So I'll talk about that in a second. But those are our only three dependencies are, those are our only three dependencies. And now we can go ahead and create our main function. All right. Our main function, the first step is going to be to initialize our environment.

Speaker 1:          15:39          Okay. So we'll say, what is our environment going to be like, what is the game that we want to play? So Jim has a make method that lets us define what the environment is going to be. And I'm going to define it right here. I'm going to say this is calling to be that it's going to be a flash game called poster racer at the flash game called coaster racer. And I think this might be a little too big. So I'm going to just go down a little bit. Okay. So that's what it's, it's initializing our environment. Um, and it's going to be Kosta racer. Okay. Um, so that's what that is. And let me, so, so a little primer on what's happening here. So, uh, so this is, this is going to initialize our universe environment, um, and initializes universe environment. And what does that mean?

Speaker 1:          16:26          There are two things happening when we're initializing or environment. There's a client and there's a remote. So the client is a BNC environment. That's where the agent lives. A VNC is a virtual net is stands for virtual network computing. So, you know, when I was 16, I like a little script kitty. I used to hack into these just for fun. But basically it's a way to share desktop screens with you know, two people. So you can, you know, if you're using the BN, so bncs kind of like a protocol. It lets you see what's happening in another, um, computer. Right? And so the reason they're using DNC is because these games are running inside of it. This, this, um, essentially a virtual machine, but it's a docker container, right? It's like an operating system inside of a docker container, which we're going to see in a second.

Speaker 1:          17:08          So the NCE is the way it communicates. And so it's a client and a server. And so the client is the agents, sorry, not a client serve a client or remote. The client is the agent. That's, that's what's running all of the, that's what's deciding what the keyboard and announced actions are going to be. And then the remote is the environment and the remote is going to be local. I know it's weird that it's called remote, but it's actually, but it's local. It's our local docker container. Now, if we wanted to, we could upload this to a server and then have it all run there. And in fact we can actually say we want multiple docker containers so we could have multiple games running, right? So one agent that's playing multiple games at once, but for this demo we're going to have this running locally and it's going to be a local doctor container, right? So that's a primer on what we're doing with, uh, the environment. So now the next, so the next step is to get our initial set of observations, right? So right when the game stops starts, let's get that. Let's get that by run a Tipi initial set of observation by running the reset method. Okay. Which will initialize our, our environment. Okay. So that's that. That's for that steps for initializing our environment. Now we're going to initialize our variables. Okay. Uh, so we're going to initialize our variables and

Speaker 1:          18:20          the answer to life. Congratulations. Um, okay, so illuminati confirmed always. So we're going to discharge our starting variables. What are these starting variables? Well, there's a number of game iterations and we're going to define these later, but we're going to start off with zero. Both of these defined the number of game iterations, so numb of game iterations. Okay. So that's what those two do. Um, and then this one is going to some, uh, it's going to calculate the sum of our observations. So how many observations do we have? And we'll store that in the total sum variable, which will, we'll initialize a zero. Okay, so that's for total sum of observations. Um, the next one we want to create is the previous, the previous total sum. Right? And why do we want to do that? Well, we're going to compare both of those. We're going to compare the total sum of observations to the previous one when we implement our policy.

Speaker 1:          19:12          Uh, and now, and this is the, I'd probably the most important variable that the turn variable and we're going to based on whether or not this is true or false. A, we're going to turn okay. Because remember there's only, this is a driving game and there's only three moves to make. Turn left, turn right or go forward. Okay. So speaking of, uh, turns, let's define our terms, define our terms or keyboard actions. Okay. So let's define our terms or keyboard actions. We'll start off with left. Okay. So this is, this is going to be a left turn. So a left turn,

Speaker 3:          19:46          okay.

Speaker 1:          19:47          Is we're going to say our key event is aero up. Um, true. And let me explain this in a second, but let me just write this out. So that's our first key events. Um, and then our next, so let me just copy and paste that because I'm going to be reusing this, right? So our next one is key event Arrow left. Uh, which is going to be true. And then our last one is going to be key event Arrow a right? Which is going to be false. Okay. So there's that. So let me just copy this whole thing. Let me make this a little smaller so we can see what, what's going on here. A little bit more.

Speaker 2:          20:34          MMM.

Speaker 1:          20:35          Events or left or right up, up, up, up. All right. So

Speaker 2:          20:42          mmm,

Speaker 1:          20:45          right. So the central left and I want one for a right. So, so that's your left. So aero op is true. Arrow left is false for this right one, and then it's going to be true for going rights. And then the last one's going to be forward. Those are our three key positions, left, right and forward. Um, and so for poor word it's going to be true. False. False. Okay. So let me talk about what, what this is. Okay. So we've got three key events that we defined here, left, right and forward. And so why are we defining three different key events for one direction? Like left? Well, we want these two. So these are going to, um, these are a set of actions, right? And we want them to be like this because, uh, uh, they're going to happen synchronously. They're going to happen synchronously.

Speaker 1:          21:28          So whenever this is true, we want this to be false. So whenever this is false, we want this to be true. So it's a format that we want to specify everything in, right? So we're just standardizing this format. We could easily just say, well, key left is just going to be, you know, key left and none of the other two, but we want to standardize these so they're all in the same format, okay. As we iterate through them, uh, for our game. Okay. So it's just, it's, it's, it's for formatting purposes to in a nutshell, it's for formatting purposes. Okay. Let me see what everybody's saying up here. Area Ubuntu against you. Cool. Okay. So how do we interact with the pixels characters of the game? Great question. So the Pixel data and the Pixel data is returned via the observation variable. Uh, when you run the environments step method, which we're going to talk about in a second.

Speaker 1:          22:10          Um, but right now, let's go ahead and, um, go ahead and run our main logic. Okay. So we've defined our terms. Uh, and so we're going to make a wild loop here. So, wild, true. Um, we're going to run this thing. So this is, you're kind of go, this is just going to run continuously. So that's, so that's for the, that's for the wildlife. So now we're going to increment our counter that a number of iterations. Okay. So this is our counter, so that, so, and we've already defined, but we're going to add one to it because well, we're iterating, right? We just started the game and now we implement, we've iterated it by one. So, um, if at least one iteration, then check if a turn is needed. Okay. So now comes the fun part. So let's check in our observation, um, uh, variable.

Speaker 1:          23:02          Uh, and we want to, so we want the only, the first observation from this observation variable, which is essentially a list. Uh, but we, we only want the first one because we don't, we say zero because we don't have multiple games running, right? We only have one. So that's gonna be the first and only item in that, in that list. Okay? So if there's something there as in we've, we've observed something, then it's time to check if we want to make a turn or not. So we've got some feedback from the game and now we're going to store, uh, the, uh, we're going to store the reward and the previous score. So in the previous score, but we're going to say, okay, we've got this variable called previous score, which we define previously, right? And it's just going to serve as a way for us to compare what we have now to what we had before and we're going to use that later, that comparison.

Speaker 1:          23:55          Okay. Um, so okay, so we've got something from the game, right? We're still in this if statement we go observe something, right? So now we're going to check, okay, is it time to turn? I remember turn was initially subset to false. Uh, but if it's true and we're going to determine whether or not we want to turn in a second, we're going to create a bit, we're going to create a function for that call. Determine whether or not we want to turn. But if, if, if, assuming it's true that we want to pick a random events, okay, so the event is going to be, um, hold on. The event is going to be, it's going to use that. And remember we input, we imported that random, uh, library up there, which is going to help us with this. So random dot choice, pretty aqualina apt we lent aptly named, right?

Speaker 1:          24:40          So, um, so random toys is going to be an array of left. So we're going to choose between going left or right, and that's going to be the events. So based on whether or not we turn, so remember there are two things happening here. So there, you know, uh, should we turn? So that's what it should we turn and then where to turn, where to turn. And so this, this is this answer the question where to turn, but we still haven't answered the question. Should we turn? And that's where the, that's where the reinforcement learning comes into play. And I'm going to talk about that in a second, but let me check it. Everybody's following. Everybody has, every AI is so volatile. Um, everybody's same. It seems everybody's cool. All right, cool. So, so where to turn is the next thing. Okay. So, um, now we're going to perform an action. Let's perform an action. So we're going to perform an action. So the action is going to be, we're going to call it end, right? So, um, so for the event, so okay, so let me just cut this out of bent or ob in observation.

Speaker 2:          25:48          Yeah.

Speaker 1:          25:49          Okay.

Speaker 1:          25:52          And then we're going to, so, so, uh, right, so, so we say, so, so we ask should we turn and then where to turn? We got that event. It's gonna be either left or right, and then we're going to perform an action based on that event. So based on whatever we see performed that random events, and that's going to be encapsulated in this action variable, right? Whether or not, whether or not we want to turn right, it's going to encapsulate it in this action variable, which we're going to, which is going to be the input to our step method. We haven't actually, we haven't actually implemented this action of, of, of, of our choice to randomly turn left. Right? We've only defined it's okay. So now we've, we've done that. Well, let's set turned to false again, because remember it was true in this case.

Speaker 1:          26:32          So now we're going to set it to false again because that's what we already turned. Okay. So now in the, the opposite case and the opposite case, um, let's see. So, um, oh, so guess what guys? So I missed one thing here. So the one thing I missed was saying, I've got this observation, so if n is greater than one, and so if at least one iteration has been made, right? So if, which I've already incremented if at least one iteration is made, check if turned is needed, right? So if at least one iteration is made, let me add that there. Um, this, that means that all of this has had to be indented, boom, boom, boom. I'm going to go ahead and, and dense and dense and dense index. Okay. So I've been dented everything and now I'm going to, um, check and the opposite case. So if, if not, if not so else, if I'm not turned, okay, so it's no turn is needed. So let me, let me write that out. If no turn is needed, go straight. Right? So have no turn is needed and I still haven't determined this is coming up. Like should I turn or not? And that's going to be the big reveal. Like how do we know whether or not the turn and that's a reinforcement learning step. So

Speaker 1:          28:03          the action we're going to get, um, it's going to be forward, right? Which we've already defined based on whenever we see in our observations, observation underscore and right. Okay. So there's that. And then, um, if there is an observation, the game has started and check out attorney is needed or not. The game has started. Check if turn need it. All right, so if there is an observation, so yes. Uh, observation. Hold on man. Words are hard up, sir. Basin zero. No, and zero. Everybody's following. Everybody's calling. Everybody's good. Everybody's good. All right, great. Okay. So, um, observation and if it's not equal, sorry, if it's not equal to none. Okay. So if there is an observation, Amy started checking the attorney as needed. So the, so now we're going to uh, uh, run are a function of robots defined with a reinforcement logic, reinforcement, learning logic. So total sum. So here you're the variable that we're going to get, get back from this. So total som,

Speaker 1:          29:26          uh, previous total. These are our variables. I'm going to define what each of these do. Um, so they're going to be four variables that we get back. So total som, previous, total sum a j and then determine turn. That's our, that's the name of our function that we're going to define based on whether or not we want to turn based on what we see in our environment, the observation and in that first environment and the only environment, which is why we say z j is that iteration variable that we defined earlier, but total sum, which is the total sum of iterations. And then the previous total stone, which is our way of comparing. Okay. Oh and there's one more reward. Those are what we're going to use. Those would be five variables we're going to use to determine whether or not we should be turning.

Speaker 1:          30:14          Okay. Um, so there's that. Um, okay. Ooh, that's a big, big, big thick. They're able, okay. I mean, so a big line or function. Okay. So then we're going to save the new variable or variables for each iteration. So now we've, we've determined, so this is the end game, right? We've, we've run a reinforcement learning. We've determined that yes, we want to turn and that's where we want to turn, right? Or are we saw our two questions and now we're going to, we're going to implement that term or that action, which is a term, right? And it's going to return our four variables, right? That OpenAI gives us whenever we run the environments step function, which is a timestamp, right? So it implements the action, right? So that the action that we defined up here, okay. And then we, the last step is to render our environment, right? So we're always rendering it. We've, we've got to render it for every time step. Okay. Um, and then obviously, um, all have this like ending code, which is like if the name and what was it? It's like equals equals

Speaker 2:          31:27          mmm. Maine. Then

Speaker 1:          31:41          if name Ben made. Okay. So there's that. And um, now we're ready to actually to create our, um, determine turn steps. So this is the reinforcement learning step. Reinforce. So let's see. Hold on. This is our reinforcement learning step. Reinforce learning step. Let me check if everybody's on board reinforcement learning step. Okay. So it's gonna be called death determine turn. Okay. Let me check if everybody's on board real quick. Um, let's see. Main made me made thank you Rodrigo. And shoe bomb. Uh, okay. So let me, let me make sure.

Speaker 2:          32:26          Right. Okay.

Speaker 1:          32:28          Okay. So now we're ready to determine our turn. Okay. Um, okay. So let's go ahead and, and, and write this step. Okay. So, um, so turn, so remember those are, what are our variables? Well, we, we define those down here. Uh, the observation, the reward and um, and the previous total som, the total som, uh, observation and, and then the turn. So these are our variables. I'm just going to copy and paste those, right, cause we already defined them. Uh, and well, we don't need to add this cause it already, we don't need to add this index counter because it already knows that we're just going to make it something a little more pretty. Okay. So now we're ready to run this variable. So, okay, so here's the basic logic. So for, for every 15 iterations, sum the sum, the total observations, okay. And take the average, that's what we're going to, we're going to take the average if it's lower than zero came to direction. Okay?

Speaker 2:          33:37          Okay.

Speaker 1:          33:38          But that's what we're going to do. Okay. So, um,

Speaker 1:          33:44          that's what we're going to do. And um, right, so basically what this is, what this means is like if it's going to use the reward as, as, uh, as a, as a, uh, a pointer as a guide. So if it, basically it's saying that if you've gone for 15 iterations and there's there, so let me just write this down. So if, if we go 15 iterations, uh, with getting, if we go 50 iterations and get a reward, each step, we're doing something right. We're doing something right. So that's when we want to turn, that's when we turn. Okay. So, so, okay. So that's the basic logic goes, let's go ahead and write this out. So if Jay is greater than or equal to 15, right? So, so Jay is going to be the, our reward, okay. For this step. And basically it's saying, okay, if you've gone 15 iterations, um, or more, sorry, it's 15 or more, sorry, 15 plus iterations. So, um, if the total sum of, of the, uh, iterations, uh, sorry. So if the total sum, which is the reward is the same as the iterations and how do we define sameness? Well, sameness is going to be a division, right? If they're the same number, uh, as in there's no remainder and current is going to be true. Okay. Print is going to be true.

Speaker 1:          35:11          Um, well, if it's not the same as in, well you haven't gotten a reward for every iteration that you've actually cracked, sometimes then turn is going to be false. Okay. So this is how it's, it's reading, uh, a reward. It's using the reward as a guide to, to determine whether or not to turn the actual direction to turn is random, but the, but the, but the, but the, um, the reinforcement learning is using the reward to whether or not the term. Okay. So we still haven't defined the reward yet, so that's going to come up. Uh, so assuming that we've done this, we've already defined, you know, whether or not we want to turn, we can go ahead and reset these variables because we've already defined them. They played out what w w what we want it and we're going to go ahead and reset them.

Speaker 1:          35:57          Okay. So that includes the sum of rewards, the iteration cal, which, and also the previous total, sorry. No, and so the previous total that this is, this is when it gets that, uh, the value of the total song because we've reset the total som. And now the previous little is going to get what happened before. Um, okay. And so, and it's a way of storing what we had before. Um, just so we know what, what we had before, it's just, it's, it's good to give a counter of that and we can extend that to, okay. So let me make sure everybody's following. All right. Thanks Frank's world. Uh, all right, cool. All right, cool. So, um, previous total sum. Total sum. Okay. Total sum. Oh zero. Okay. So else we're going to say turn his false. Basically if it, so this is the, so this is if, if it's not, if we're not receiving a reward every 15 iterations, then we're gonna, we're gonna say, okay, so don't turn. Okay. And so now we want to define,

Speaker 2:          37:13          yeah,

Speaker 1:          37:13          now we want to define whether or not we want to hold on. Now we want to make sure, you know, empty white space, not necessary. Keep it clean. You know how it is, you know what it is. So if we have an observation observation, and if we have an observation and

Speaker 2:          37:37          okay,

Speaker 1:          37:38          well there's something there, right? If there's something there, then we're going to say, okay, we want to increment the counter, which, which, which were comparing above. We're going to hit them at the counter and the sum and the reward some. Okay. So let me write this out. So J plus equal. So this is our counter j plus equals one. And then total sum plus equals reward.

Speaker 2:          38:06          And, and then

Speaker 1:          38:12          awesome award.

Speaker 1:          38:15          Okay. So now we're going to return Kern, Jay, total, som, everything that we calculated here and that's it. Okay. Boom. So let's go over what happened here. Okay. So, um, so okay, let's go through this step by step. We initialized our environment and then a set of variables, right? That were, that we were later going to use. Okay. Then we define three actions, right? Three keyboard events going left, going right or going forward, okay. And so because that's all we're going to do. So then we started this while loop because this is just going to keep on running as you know, forever. And we said, and we said, okay, so let's increment our counter because, well, this for every step that we're running. And then we said, if you've met, if our game has made at least one or iteration check, if we need to turn.

Speaker 1:          39:03          And so, okay, so assuming that we've made at least one, one iteration, so the game has moved one time step get well, uh, and we have something that we're seeing. We're right, the, the, the, the uh, observation of the game, um, which depends on whatever game we have. Uh, we're going to store their award and the previous score and then determine if we should turn or not. So if we do turn that, we're going to pick a random turn, either left or right and capsulate that inside of an action variable and then said, turned to false again, because we've already done our action else. If no turn is needed, just keep going straight. And this is how we determine whether, uh, whether or not we want to term with this determines turn function. And we use those variables we defined previously. And then we, uh, actually implement that term.

Speaker 1:          39:51          Has a turn as an action using the environment step function, how do we determine whether or not we want to turn? Well, for every 15 iterations, we sum the total observations and we take the average. If it's lower than zero, we change the direction. If we go 15 plus iterations and get a reward at each step, we're doing something right. And that's when we turned, that's our signal. Right? And why 15 well, it could be 16 it could be 17. It's arbitrarily going to be 15 right now. So if it's greater than or equal to 15, we'll take that some divide by the number of rewards that we got. And if it's the same, if we, if we've been doing it right this whole time, turn or else don't turn reset our variables. And then, uh, this, this code down here is how we get that, some of the awards and the terms. Okay. So that's a basic explanation. I'm going to go ahead and run this. All right. Um, all rights and,

Speaker 3:          40:43          yeah.

Speaker 1:          40:44          Okay. So, so let's go ahead and run this code. All right. Um, we'll go ahead and run this code. Um, I'd saying, what'd I call it? They call this demo. Um, right. So where did I put demo? Put it somewhere. Probably in here, right? So, uh, python. So let me make this bigger. Whoa, wait, digger. So Python, Demo. Dot. Pi Demo Dot Chi. So now we're going to run this baby. Boom. It's connecting to our local docker container, which has our DNC server. Um, web socket connection. Failed. Whoa. Okay. Let's see what's happening here. Uh, let's see. Let's see. Let's see. Let's see. Um, okay, so I'm going to close out this. Let's try that again. Sometimes if you open another window, it starts working. So, oh shit. Okay. So websocket closing on handshake. Um, wow, this was working right before I started the live stream. Okay, so let's figure what is going on here. Um, so I, well first of all, first of all, okay, here's, here's what it is. Uh, so online 80 line 80 in Maine, forward an observation line 80, what is happening for observation, observation. Let's see what's happening here. Um, environment don't render. Uh, okay. So what I've been saying here is,

Speaker 3:          42:30          okay,

Speaker 1:          42:30          ELLs if turn forward for observation in opposite, if you guys have a suggestions open to those too. Um, let me see what people are writing here. Okay. So, oh, capital f, that might be it. That is that what it is? Is it that I defined it as a capital f? Oh yes, yes. Thank you. Okay, so now let's run this thing at that. Uh, Yo, yes. Okay, awesome. Yo Yo yo Yo yo yo yo yo yo yo yo yo. Okay, awesome. Yo Yo yo. Okay, great. So what is due? It's connected to our local docker container using the VNC protocol. Remember? So this is a, this is a, this is a, a kind of like a virtual machine, but it's, it's, it's inside of this docker container as a BNC, um, using the VNC protocol. And so now it's going to connect to get hub and it's going to, let's see, let's see what's happening here. Uh, come on baby. Yes. You know what it is. So here's our game. It's about to start a, it's going to load up and then it's going to run our code. Alright, C'mon coast to race or you know what it is

Speaker 1:          43:48          preparing graphics. How great is that? Preparing graphics. That's some flash shit right there. Okay man, it's laggy, but okay. It's every 15 times steps. So, so iterations move fast in the game, right? So it's like 15 steps are happening every like second. Okay. Um, and as you can see, it's, it's determining like, oh, if I hit something that I shouldn't be doing that. So randomly moving to different directions. If I hit something randomly moving in a different direction, it doesn't, no, it doesn't. No. Like if it hits something on the left move, right, or it's, it has something on the right to move left, but it does know what they hit something with it. If it's not doing something right for 50 iterations, stop. Okay. So there's a demo code. I wrote that run for a little bit. It's kind of laggy, but it's pretty cool. Right? Um, boom. We fat, we've fall, fall in, and it's just going to keep on going. Okay. So I'm going to exit out of that. I'm going to go back to you guys. Quit. Quit out of terminal, uh, terminates as a terminator says and then open up this.

Speaker 3:          44:50          Okay.

Speaker 1:          44:51          Screen sharing. Okay. So I, okay. So, uh, last five minute Q and a and then we're good to go. All right. Okay.

Speaker 1:          45:05          All right. So, uh, I neural network is not in 11 and lines of code such as nine. This is my code, right? Yes. Mick. So Mick, yes, this is, this is, uh, one of our challenge winners code Nick Ben Holes. This was one of his entries for the challenge. I just, I really liked it and I wanted it to demo for you guys because I thought it was not hard enough that it was like going to take like two, three hours, but it wasn't easy enough where it was like, just like trivial. This was it. This was use reinforcement learning and it was a, it was a good example of that. Um, where is the, okay, uh, any good examples of universe? I'll post some examples in the description on tutorial, on gang check search, generative adversarial networks. On getting on Youtube. My video is probably the first one that that's going to pop up.

Speaker 1:          45:47          I'm going to do more in the future for sure. I'm actually kind of waiting for that to get a little better because there's a lot of possibilities with cans, like seriously, like seriously, like generative models for video. Like I mean like we could, we could generate movies and, and then say, I want to be in this movie. Uh, assuming it was good enough and had enough data, like I want you to, to, to, to generate star wars except put me in it and like not have gungan. It's like not have jar jar Binks. Like that's the level of where we could get to. Like that's where I see the trajectory of moving in generative models. It's incredible. With Ai, if you can dream it's, it's possible. If you can dream it, it's possible. Where did you learn machine learning techniques on the Internet from machine learning sub reddit from Twitter, from implementing code myself on get hub, uh, from, uh, courses like, uh, Andrew Ng's course and from the deep learning course on your Udacity, how are, how are you sure this code actually learns because it's receiving the reward and it's moving based on the reward.

Speaker 1:          46:40          What's happening on Friday party? I can't answer that right now, but you're going to love it. What do you think of the new apple paper?

Speaker 3:          46:50          Okay,

Speaker 1:          46:52          I'll give apple. I still don't, I still don't fully, you know, respect apple yet, but if they, if they're, if they keep going with this, you know, this open source stuff, you know, there's, there's some potential there. How did you get your first 10 youtube subscribers just by posting videos? Even though I just,

Speaker 1:          47:12          even though people didn't like him that much, I just had, I just believed in myself. I, I just like, you know, reinforce a positive feedback loop. Um, practical, practical application for this, uh, for this specific Bot. Um, uh, any kind of racing game. That movie that's moving left and right. This could be applied to, did you check Song Hahn's paper? I attached a not yet. I've been really busy. Um, but I will, uh, how to be a good programmer. Um, make a video on that. Watch all my videos. Saroj custom voice assistant video, um, custom voice assistant. Uh, that's a good question. I haven't been voice specifically a voice assistant. I then build a tensor flow chat Bot, but not a voice version of that. So that's gonna come up. Um,

Speaker 3:          47:53          okay.

Speaker 1:          47:54          What do you think of Microsoft Azure ml? Actually, you know, Microsoft is a surprisingly starting to get cool again. I mean it's, it's uh, it's incredible. Like what, what, uh, Satya Nadella has done. He's, he's, he's taking Microsoft in a diff different direction. So I'm, um,

Speaker 1:          48:12          yes, Mick, I'm going to add the link to the descriptions. Haste. Raj, what's next guys? Just wait for Friday. Um, wait for Friday. Seriously? Like, wait for Friday. Are you going to do interview videos? Yes. I'm going to do those soon. Yeah, I mean, it's not any immediate plan. Maybe in two to a weeks wrap on universe. As always, I've got a rap. Um, okay, so wrap on universe. Okay. I've got a game, but I do it without a thought. I'll look at the future and think, man, I got to go away from here and find my flow. Man, my world is so high. I'm so in the sky. I'm like a universe. My mind is so same. Um, like a fly on a wall as the universe comes down with planets, my mind, mind. Okay, that was it. Um, that was my dough. Be Heights, man. Yo, get heights, get hives, get, get hyped, get super hyped because this is, this is huge. I am meeting some important people today. Okay. So are you a vegetarian? No. Um, okay. So bars. Okay. So, uh, have you looked into a hierarchical temporal memory by Numenta? Yo Numenta as great as Jeff Hawkins book on intelligence is, and as, as much as I love the idea of artificial general intelligence, Numenta hasn't really done anything,

Speaker 1:          49:33          know it hasn't published results that, that compared to deep minds. Uh, and they're kind of a joke in the community. So, you know. Yeah. And also there hasn't been any real innovation. I mean, people have known this, you know, this kind of hierarchical thing happening in the brand. But anyway, so Ilan, uh, uh, high vehicles, hype, Elan. Uh, all right, cool. So yeah, it's, it's not Ilan. Uh, but it will be not, not yet. Um, not, not elan. Eventually, you know, maybe later this year I'm taking my time. I know that. I know what's going to happen. Well, we're, we're, we're going to Yo, it's, it's us. You know what I'm saying? It's we, we are a squad guys. We are, we are an army. We are a force. We are a movement and we're going to get everybody. We are going to get the attention of the entire world. Okay? We are going to get the attention of everybody because this is a war path that we're on. We're going to solve intelligence. Okay. So that's it for this video. I love you guys. Thanks for watching. Um, Andy, for now. I've got to go start a revolution or sorry, continue a revolution. So thanks for watching.