Speaker 1:          00:04          I actually didn't play anything. You just heard AI generated music. Oh, a world walkmans rod geology. And this episode we're going to train a neural network that compose music all by itself. Machine generated music. A technical term for this is music language modeling and it has a long history of research behind it. Markov models and restricted Boltzmann machines, which kind of sounds like something out of half life or bioshock. Hold on Babe. I've got to go save the world using the restricted Boltzmann machine music is how we communicate our emotions and passions and it's completely based on mathematical relationships, octaves, chords, scales, keys, all of it is math. At the lowest level. Music is a series of sound waves that create pockets of air pressure. And the pitch we hear it depends on the frequency of changes in this air pressure. She music is the annotation would created to help us map the sounds into a repeatable set of instructions.

Speaker 1:          00:49          So if machine learning is all about feeding data into models to find patterns and make predictions, could we use it to feed in music data to predict new songs, apps of fruit? We, we're going to build an APP that learned how to compose British folk music by training on a British folk dataset. I was going to go with American, but I didn't think it'd be age appropriate. There's too much sacks and violence. Get it. We'll be using tensorflow, the sickest machine learning library ever to do this and just 10 lines of python. We're going to follow our tried and true force that machine learning methodologies to do this collected data set, build a model, train the model and test the model to start off with one to collect our data. So let's import the URL live module, which is going to let us download the file from the West.

Speaker 1:          01:25          Once we import it, we can call the URL retreat methods. To do just that, we'll set the parameters to a Lincoln, a Dataset and a name. We'll call it a downloaded file. We're using the Nottingham Dataset for this demo, which is a collection of a thousand British folk songs in Mitie format. Video format is perfect for us and codes all the note in time information, exactly how it would be written in music annotation. It comes in a zip file. Want to unzip it as well. We can do this programmatically using the zip file module. We'll extract the data from the Zip and place it in the data directory. Okay, so we've got our data. It's time to create the model, but before we do that, we need to think about how we want to represent our input data. There are two things happening. There's the main tune or melody and then they're the supporting.

Speaker 1:          02:00          Those are harmony. Let's represent each as a vector and to make things easier, we'll make two assumptions. The first is that the melody is monophonic. That means only one note is played at each time step. The second is that the harmony at each time step can be classified into a core class, so that's two different vectors, one for melody and one per harmony. Well then combine them into one big vector and use that as our input. We can just import our ml helper class and then call the create model method. To do this, music plays out over a period of time. It's a sequence of notes, so we need to use a sequence learning model. It has to accept a sequence of notes as an input and output. A new sequence of notes, plain old neural nets can't do this. They except fixed size inputs like an image or a number.

Speaker 1:          02:39          We'll need a special kind of neural network, a recurrent neural network. Those can deal with sequences. Since data doesn't just flow one way, it loops. This allows the network to have a kind of short term memory unlike Donald Trump, but wait, we want our network to not just remember the most recent music it's heard but all the music gets heard like a piece of music and have multiple themes in different parts of it. Hopeful, sad and if no one can remember members only. The most recent part which was cheery, it's just going to compose cherry stuff. We need a special type of recurrent neural network called a long short term memory network. This type of network can remember things from way back in the sequence of data and it uses everything I remembered to generate. New sequences are model will generate the sequences and cord mapping file to a file and a data folder.

Speaker 1:          03:19          This is a serialized bytes stream representation of our music that we can input directly into our model to train. By the way, every machine learning model has a set of water called hyper parameters. These are the parameters that we human set for how our bottle operates, like knobs on a control panel. How many layers do we want, how many iterations for training, how many neurons? You can just use an existing model with pre tuned hyper parameters to build something. Awesome, so now we're ready to train our model. We can just call the train model method of our neural net class and do this. It's we'll get the network to start collecting the input data piece by piece. It took me about two hours to train us on my 2013 macbook pro, but you don't have to wait until it's completely done training to test it out.

Speaker 1:          03:54          Just wait until you see the best loss of four encountered saving model message. Once you see that, you can type in RNN sample into terminal with the config file flag and pointed to a newly generated config file in your models folder that will generate a new song. You think the newly trained model you've just created the generate music, we just sampled the melody and harmony at each time step and plug it into our trained model. The model will then predict what the next note will be. The collection of all the predictive notes is our new we generated song.

Speaker 1:          04:26          There's some improvements that could be made for sure the time signature. It's kind of sporadic and in terms of longterm structure, there seems to be a lack of repeated themes and phrases. The solution may well be more data and more computing power. It usually is when it comes to machine learning with deep neural nets, but he only can help us learn the fundamental nature of how music works in ways that we haven't even thought about it. Links with more info below. Check him out and please subscribe for more machine learning videos per now I've got to go fix a runtime error, so thanks for watching.