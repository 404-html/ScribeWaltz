Speaker 1:          00:00          You Suck rhymes like mine can't be beat. That hurts my feelings. Oh, sorry. Do you want to play Pokemon go instead? Sure. Yes. Hello world. It's Saroj and I am a big fan of rap music. There's plenty of rap that degrades women, promotes violence, and glorifies the gangster lifestyle. But for all of its negatives, wrap is also a medium for some incredible storytelling. It's a glimpse into real human suffering in a hopeless place, a cry for justice, a distillation of the human spirit of rebellion, and it's not just the content of rap that's compelling, it's the style. Let's not even think about the subject matter for a second rack gives you a certain lyrical freedom that newly no other musical genre can. It's all about the rhymes. There's the perfect rhyme words that end with the same sound. There's also the assonance rhyme where only vowel sounds are shared like, and so because there's so much creative potential in the rap game, we might need to introduce a digital mc to the scene.

Speaker 1:          01:03          Am I right? What if we could get a machine to understand rack lyrics or even write them? It's not like this stuff hasn't been tried before. It has several attempts have been made to try and understand lyrics using machine learning. But one attempted particular got some really great results. A pair of researchers at Hong Kong University, so I had to download the lyrics to 52,000 rap songs. Then train a model so that given a novel song and model could identify it's rhyme scheme. So you ate a big Mac, your breadth is whack, you need a tic tac. Take the whole pack is an example of the AIA rhyme scheme since all the ending words Ryan, whereas life is a dream future is now I eat ice cream. They're not saved. Gabel is an example of a, B, a, B. Since every other line writes, the model they use to train on the lyrics was called a hidden Markov model.

Speaker 1:          01:47          Let's remove the word hidden for a second and talk about how a plain old Markov model works. Let's say we want to predict the weather and let's also say that the weather can only be one of three states, either sunny, cloudy, or rainy for a hundred days. We record the weather and record the transition between each day. Whenever we want to find the probability of the weather being a certain state after a given day, we can just tally the number of transitions of that type and divide the number of days by 100 that's how Markov models work. They help us predict the likelihood of a future state. Pretty useful. Right. But what about a hidden Markov model? Well, suppose we can't directly observe the weather, so we can't really calculate the transition probability. So the model is hidden from us, but we can observe related phenomenon like the number of people with umbrellas on a given day because Rihanna said so.

Speaker 1:          02:33          So using one of many techniques, we can still find ways to calculate the transition probabilities using these related variables. So Hidden Markov models are pretty cool and their entire textbooks devoted to how they work. In fact, it can be used not just to classify lyrics but generate them as well. That same pair of researchers published a later paper, but you know how during a Cypher one MC challenges another with a verse. Then the opponent is supposed to spit back some sick rhymes. They trained hmm to do this. The results were interesting, but let's just say they're digital. Mc wasn't exactly the second coming of Tupac. Let's take a look at a fresh approach. A group of researchers published a paper just last month called dope learning, a computational approach to rap lyrics, generation legendary. They use an algorithm called rank Svm, which was partially powered by a deep neural network and they fed it a data set of all the songs from the top 100 English speaking rap artists.

Speaker 1:          03:24          No idea how Lil Wayne got on that list. So how does this algorithm work? Well, first they needed to extract features from their rap corpus to feed to their model and they wanted features that represented three metrics, rhyming, structural similarity and semantic similarity. I'll those three metrics. The semantic similarity was the one that required the use of a deep recurrent neural network and neural net did what it does best. It found vector representations of words, lines and groups of lines. Once the features were calculated, they were input into the rink. Svm model rake SBM is a support deck machine by the way, which is just a type of linear classifier and the SPM eventually learn to predict the next line once who was trained on the input features. They also want it to find a way to quantify how good their algorithm was compared to human MCS and so they calculated something called rhyme density, which is the average length of the longest Ryan per word using rhymed density as a metric.

Speaker 1:          04:16          They found that the algorithms generated lyrics had a 21% higher density and the most rhyme dense human artists on their list and inspect a deck play who let's write our own Rappler, his generator using a hidden Markov model in Python. We only need two dependencies here, random, which helps generate random numbers and re which helps deal with text formatting. Our highest level of method is called test Markoff in. It will initialize an empty array called rap, lied and add all of our lyrics to the rap library and returned the wrap generated from the rap library using the stop word that was input by the user. The generated wrap. We'll always start with the stop word. Let's take a look at the ad to live function. It opens the lyrics file and construction. Hmm. It iterates through every word and checks this record to see if it's a new word or sequence.

Speaker 1:          04:56          If a word or a sequence has appeared before, he won't rerecord it, then it changes each count to a percentage value or a transition probability. So once we constructed our model, we can run the make rocks function. It'll take both the set of lyrics and the start word as the parameters. It will continuously generate words. You have the markup next function for up to 50 words. The markup. Next function either returns a random word, it's the word it's novel or find a word from the model. Probabilistically, let's test this out and see what it generates. I'll start off my wrapper with homie. Homie grows Bunani likely on Toten inspired enough, basically, boy coming periods. Damn. So dope. Check out the links down below for more info and please subscribe for more ml videos. For now. I've got to go fix a malformed requests, so thanks for watching.