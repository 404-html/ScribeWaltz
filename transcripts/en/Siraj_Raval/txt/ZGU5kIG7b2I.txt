Speaker 1:          00:00:01       Hello world. It's Raj. Welcome to this live stream. I'm very excited to be here today. We're going to generate some fake news. No, I'm just kidding. It's not fake news. It is generated Wikipedia articles. Okay? That's what we're going to do today. And uh, I'm very excited for this because we are finally going to code and Lstm from scratch. We are going to code an Lstm from scratch. That means that we are doing it from scratch, not num py, scratch, but tensorflow scratch. No care. Os we're going to code out the internals of an LSTM cell. And here's what it's going to look like. It's going to look like this. Okay? It's going to look like this, just like this. And we can see that, right? Let me make the text a little bigger.

Speaker 1:          00:00:53       We can see that. Okay? So we'll give it some input tax. I plan to make the world a better place by, and then it's going to, uh, generate the rest of the text. Now this is actually really bad because it only took one iteration of training, but it's gonna get better over time. Okay? So that's what we're going to do. And we are, we are, we are psyched to do this. Thank you guys. Hi everybody. Let's start with our five minute Q and. A. Okay. So ask me deep learning machine learning, any kind of questions you want Ama, go for it.

Speaker 2:          00:01:28       Yeah.

Speaker 1:          00:01:28       Oh, and we are, I am live at the end. Please subscribe. And if you want to see more live streams there. Every Wednesday at 10:00 AM pst, also at the upload Vr studios with the wizard of Oz, Oz. Balaban. Ian, who was going to be up there. There he is. He's going to be the voice of God later on. He's going to help me answer questions. Okay. So ask me some questions. We're going to get started. We're going to do some virtual reality today to virtual reality, to, to demos, to math as well. Hi, Palosh Mohan Leo Party Tuber. Okay. Questions? Can you explain more on the text? Summarizer the tech summarizer was, um, an LSTM network that you use memory and attention. And this next weekly video that's coming out in three days will definitely explain that more. Okay. Uh, come to Brazil someday. Someday I want to go to Brazil.

Speaker 1:          00:02:15       What is best machine learning? I love that question. This channel, could you train an Lstm with a chat bot? Yes. Could you train a chat Bot with an Lstm is what you meant to say? The answer is yes, you can. What's your opinion about semantic web? Semantic web? The cement. The semantic web is a word that describes the web of words and the web of text and it's all really connected in there. It's hyperlinked together. We still have to improve right now. Links go one way but we want links to go both ways, which was the original vision for the web. Zana do was a project that did that and IPFS. The interplanetary file system is aiming to go back to that, so check out that protocol. Will you use syntax net? Not In this. We not using transfer learning. We are building it from scratch as a learning exercise. Why do you do not use chaos? Because we want to learn all of it. Thank you for joining the class and you'd Udacity. Thanks King Burrito. Best framework for sound processing in python for sound processing fun. There are quite a lot, um, for sound processing

Speaker 1:          00:03:28       Dirac the IRA c is a good one, uh, to, it's by a German, a programmer. What harp hardware do you recommend? Have you tried FPG A's for deep learning? Uh, Nvidia is a good g as a good brand for Gpu, specifically the Titan X. Although if I were to say, you know, between cloud and local, I would choose the cloud specifically AWS. What is your background in mathematics or Raj? I took some math in high school. I took some math in college. Uh, I takes, took some math online. You're really, it's, it's, we have to think about this as a lifelong learning process. I'm still learning math. It's not like I just stopped learning math and now I'm, I'm good to go. We need to switch the view from a PR, from learning being a period in our lives to a lifelong learning process. So I've studied linear Algebra and Calculus and Statistics and I'm still studying these things. Two more questions and then the five minutes is up and we're going to code this thing. Okay. The second question, the first question is, well, you make an AI robot in future, like personal assistant, personal assistant, uh, w I, yes, I have several chat bot videos in terms of hardware. So hardware is a little more effort than just software. I definitely want to do more hardware and I will, uh, let me just finish this course first and then you know, a lot of crazy cool things will, will start coming out. One more question and then we're going to get started with the code. This, the next question is

Speaker 1:          00:04:57       how can I get out of Venezuela with AI? Okay. Wow. I don't know. Okay. No, here's how you do it. You would just study this stuff well enough that one of these big companies would want to hire you and host you. So just study it well enough. You can do it in Venezuela, you can do it in Africa, you can do it anywhere. You can become a world class machine learning expert anywhere in the world because of the Internet. So get good at this stuff. People will want you increase your personal value and people will want you and you can go anywhere you want. The barriers to entry to travel will decrease. That's it for the questions. Now we're going to get started with this code. Okay, let's get started with the code. So what we're gonna do is we're going to generate text and we're going to generate Wikipedia articles.

Speaker 1:          00:05:45       Okay? And by the way, with the questions, I'm not going to let it interrupt my coding, so I'm going to code for something like 15 minutes and then go back to the questions and then we'll just repeat that process. Okay. I was reading feedback from last time and I want to better, so let's, we're going to code some, we're going to generate some text from in the style of Wikipedia. Okay? That's what we're going to do. So let's look at the data set that we're going to use. This is the data set and by the way, check the check the video description for a link to the get hub repository. And I'm going to use VR by the way, to show some math. So it's, so stay tuned for that. Okay. So that's what we're gonna do. And uh, okay, so let's get started with this. This is the Dataset that we're going to use. It is from salesforce, Matt and mine. So metamind got bought out by salesforce. Great purchase. Metamind had some really great technology. And by the way, if you start an AI startup, you will

Speaker 1:          00:06:41       very likely to get bought out if you bring any sort of value. These big companies are snatching up AI startups like they are candy, like they are candies. So it is a good time to be an AI right now is a very lucrative time. This dataset is a collection of 100 million tokens preprocessed for us from Wikipedia articles. So they took a bunch of Wikipedia articles, tokenize them, and then they, uh, this is the Dataset. So this is the data set we're going to use to train our model on, okay, that's our dataset. Let's get started. The first thing we're going to do is import our dependencies, dependencies, time. What are the dependencies that we need? Well, of course we need num Pi for vectorization. We want to vectorize our data before we feed it into our model. The next step is to import, uh, the art shows to map.

Speaker 1:          00:07:30       The next step is to import random. Okay, and why are we going to import random? We want to generate a po probability distribution, right? For generating texts. This is going to help us generate text. Uh, the next is tensorflow from machine learning. And then finally we have one more and that is a date time, date, time. Okay. So this is going to show, this is for clocking the training time. We want to see, you know, where we are in the training process. Okay? So that's it for our dependencies. And now we're going to open our text. So let's check out this text. It's going to be, so I've already downloaded it and here's, here it is, by the way, uh, this is the one that you should download. Where is it? It is right here. This one?

Speaker 2:          00:08:10       Yeah,

Speaker 1:          00:08:10       right here. The 4.3 megs. Okay. Oh, actually no. The one 81 mags. Do you want more data? So download the one 81 megs. It's an a. Dot. Raw increase. The font size, I can do that. It's in a dot.

Speaker 2:          00:08:25       Raw.

Speaker 1:          00:08:26       How's that? That's amazing. I am very excited for this. Okay. It's an a. Dot. Raw format. So that's gonna be Wiki Dot Texstar raw. So I've already downloaded it and it's right in my folder right here. So I'll say Wiki Tex. Dot. Raw

Speaker 1:          00:08:40       Dot Reed. Okay, so that's our dataset. And we're going to print out the length of the text that we have so far in the number of characters. And it's going to be, it's going to show show how long this text is. And then we want to print the head of the tax. We're just going to show the first collection of characters, just so you know, for ourselves. And is the great thing about having a Jupiter notebook is that we can just say how much do we want? And I'm going to say I want the first thousand tokens. So let's see if this prince out and it will for sure, definitely pronounced. Okay. So we have how many characters we have. We have 128 no, we have 1 million, 1.2 million characters. We have a lot of characters. And we're going to print this out. Okay. Character by character. And in fact we are going to feed characters into our model. We're going to feed characters into a model. So it's going to be character by character training and then character by character output. And we're going to talk about ways to improve that.

Speaker 1:          00:09:44       Okay? So that's, that's that's it for our tax, that's what the text we're going to use. And these are Wikipedia articles that are already tokenized. That means that each word is it's own token. It's its own string. And this is good for this is this our preprocessing steps. So now we're going to print out the characters. So now we want to print out our characters and sort them and sort them as well. That should be reversed. So our characters are going to be sorted. Okay. We want to sort them. Python has some great native functions for sorting characters. And, uh, so this is actually a nested,

Speaker 1:          00:10:22       it was, it wasn't my AI, it was, it was the wizard of Oz. He added that. So that was good. I was kind of surprised. I was like, whoa. But eventually it will be an AI. So now we have charts size, which is going to be the length of the characters. And then we want to print out the number of care. So this is an exercise to show the characters that we're going to be dealing with. This is very important because we want to show the characters that we're going to be dealing with here. Okay? So we have our charge size and we want to print out all of this chars. And then lastly, we want to say we don't even need to print. Hello. Okay. So here's what's going to happen.

Speaker 2:          00:10:55       Hold on.

Speaker 1:          00:10:56       So we have sorted let would it,

Speaker 2:          00:11:02       oh, there we go.

Speaker 1:          00:11:03       Okay. We have 259 characters total. So what did we do? You just do a set is an unordered collection with no duplicate elements. So we first remove duplicates, then we uh, and then we turn it back into a list. Okay. And then we sorted the list. And so it's sorted Alpha numerically.

Speaker 2:          00:11:22       Uh,

Speaker 1:          00:11:23       please post the link. Yes, the wizard of article posts a link on my behalf. And so these are our characters sorted alphanumerically all of these are the character of the are the characters are, we're going to use 259 unique characters,

Speaker 2:          00:11:35       sorry.

Speaker 1:          00:11:38       And the link to that, I'm going to post that in the chat room. Here's the link to the Dataset. So that's it for our characters. And now

Speaker 1:          00:11:53       there we go. So that's it for our characters. So we saw a little bit of data preprocessing and then we're going to get to the machine learning. So now we want to have a method to convert characters to ids. Why? Because it's, it's good to have this mapping because we're going to need it to create labels. And I'll talk about specifically what we're considering labels, right? Th the labels here aren't so clear cut. They're not like ones or Zeros. We just have a collection of characters. What are the labels? Well, we'll hold on tight. We're going to figure out exactly what that is in a second. So we'll start off with creating a dictionary to convert our characters to ids. And we're going to enumerate through all of those characters to create ids for each of them. So these are dictionary objects. Okay? These are dictionary objects and I can just paste this in again, but this time it's going to be id to character. So these two functions are going to convert our characters to ids and vice versa. Okay? And that's what that's going to do. Make sure I have all of that. Their character or characters. Yep, Yep, Yep, Yep, Yep. There's something missing here. There we go. Okay. So that's our character to ids isn't gonna convert character. So [inaudible] and bias and numer in new arrayed, right? Spelling is such a bee spelling as a B.

Speaker 2:          00:13:15       Okay?

Speaker 1:          00:13:16       So that's what we're going to do. So now we're going to create this helper method. So this helper method is going to, uh, generates the probability of each character of each next character. Okay? So this is going to help us generate the probability of each next care. So character by character, we're going to predict a character, and then we're gonna generate the next character. So we're going to our model output a character, and we want to generate the next character. So this is going to be a helper method and what's claimed more as we go. So we're going to say sample. So given a prediction. So the prediction is, uh, it is a list of the prediction is going to be a list of possible characters. And this method is going to return the, the, the most likely character. So it's actually a list of characters that were in that we're going to input.

Speaker 1:          00:14:06       And then we're going to use this method to predict what is the most likely of those characters. So it's a multi-class classification problem where the classes themselves are characters. This is gonna make more sense once we start building the model. Okay? So we have samples. Okay. So the first thing we're going to do is want to say random dot uniform zero through one. And so this is going to generate a, a uniform distribution okay. Through the interview interval of zero to one. Why to, okay. So yeah, I was going to say we have Socratic Mishra, a Jew all Daya and get the nerves run and asking why two dictionaries, why do you need to use two of them? Okay, so we're using two dictionaries because we want to convert characters, two ids and to characters. And the reason for this is we want a way to encode, uh, these characters because we want to, first, we want to convert the characters to ids so that, uh, we can convert them back later. And this is for, uh, it's for indexing. We need a way to index these characters, right? More than just a, B, c, d, e. We need an a numeric way of indexing these characters so we can refer to them. We want to store and retrieve them optimally. So this is a way, this is a way for us to index them similar to a, you know, it's a key value pair, right? So the, the character id is going to be the length of the prediction. Minus one, because,

Speaker 3:          00:15:33       uh, because, uh,

Speaker 1:          00:15:37       since the length is greater than the indices starting at zero. So that's what we add a minus one at the end. So we want to generate, so we have a list of a prediction predicted characters. We have a list of predicted characters,

Speaker 3:          00:15:52       uh, and uh,

Speaker 1:          00:15:54       we want to say, so for each character prediction probability. So for each of them. So we're going to iterate through each of these predictions. Okay. So we're going to iterate through the length of the prediction list. This is a list of possible characters, probabilities. It's a list of possible character probabilities. And we want to say for each of them, let's, let's, uh, first say, so as is going to store the prediction character. So there's going to store prediction character.

Speaker 3:          00:16:25       And um,

Speaker 1:          00:16:29       so we're going to say this is our first character. This is our first prediction. And now this is, this is where the unit, this is where the distribution, uh, comes into play. We're going to say if s is greater than or equal to r, and so our is going to be a randomly generated probability. Okay. So we are generating a probability and there are different ways of doing this. But right now we are going to randomly generate p probability. Okay? And uh,

Speaker 2:          00:17:00       okay.

Speaker 1:          00:17:01       So if the, if it, if it's greater. So that means if the, if, if it's a likely probabilities. So this is a generated threshold. If it's greater than that, then we're going to say, okay, so this is the most likely character. Set it to the character id. Okay? And then we're going to break. And so once we have that character, once we have that likely character, we want to one hot encoding, okay, we want to generate a character. So we want to create, create a one hot encoding of that character. So we're going to use a num Pi to do this. And so let me talk about what one hot encoding is. Why are we, why are we one hot encoding these characters? Okay. We are one hot encoding these characters because, let me it. So it's going to be, we're going to one hot and code these characters because we want to, one hot encoding is a way for us to differentiate. So if we could say, uh, uh,

Speaker 1:          00:18:07       we went to differentiate not ranked value. So you know, if we, if we were to say, if we were to just normally encode like fish taco Bob Geyser, we could, you know, if we were to encode this, normally it can be one, two, three, four. But the problem with that is that gives them a ranking. We don't care about her ranking Taco because it's too, is not somehow greater than fish. These are just values. Okay. So one hot encoding is a way for us to differentiate between different values without ranking them. And so we want to differentiate between these values without ranking them. And One hot encoding looks more like this, you know, they're just as collection of ones and Zeros, it's just like that. And so to do that will generate, will create a zero matrix with non Pi's zero function with the size of the jar size. And then we're going to say, uh, at the specify character index, set it to one, so it's gonna be like zero. This is a drought one. There's there where one is the index of that character. Okay. That's it for our sample probability

Speaker 1:          00:19:11       distribution. So hill at averse shopper asking what if you're using two dictionaries will be very inefficient if you're using a big dataset a and e or would it take forever? May Basically, uh, well we actually see a lot of in memory storage for big data sets and the no, so the answer is no, because why? Because we want to a cue, it could be if we start it all in memory, but we are, we are storing it in batches. So we are flushing it, we're flushing the cache. This is our cash story and we're, we're going to flush it every so often during training. So, but great question. And, and, and, and another way, great way to do this, if you don't want to have to flush it, uh, is to store it on disk. And for that we can just, uh, we could, I mean, it's, it's a trade off because if we didn't store it in memory, we would be a continuously retrieving it. So then there's that, uh, computational complexity. So you're trading off memory verses computation time for, for continuously reading it from disk. Right. Every time. Uh, so which one would be the best? I would say to do it in memory but then to flush, but to flush it every so often.

Speaker 2:          00:20:20       Okay.

Speaker 1:          00:20:20       Okay. So that's, that's one hot encoding. Okay. So that's for our sample.

Speaker 2:          00:20:25       Okay.

Speaker 1:          00:20:25       Okay. So now we're going to vectorize our data before we feed it into our model. So vectorize our data. Okay. We are just chugging along here. We're going to vectorize our data to feed it into the model. Okay. So the length per section. So these are, I'm going to first define art, some variables here. So the length per section is the size of what we can consider sentence a sentence. This is how we're going to feed data into the model. They're going to be 50 character long, uh, batches. And then each of those batches, those characters are what's going to go directly into the model. But the size of these batches are 50 characters long. Okay. And then I'm going to say skip because we're going to,

Speaker 2:          00:21:09       okay.

Speaker 1:          00:21:10       I'll show you exactly what I'm using. Skip in a second. Okay. And then we're going to create an empty list, four sections. And then for next characters.

Speaker 2:          00:21:21       Okay.

Speaker 1:          00:21:23       Okay. So,

Speaker 1:          00:21:27       so that's that. Okay. So here's why we use skip. So what we're gonna do is we're going to fill this section's list. We want to fill the sections list with chunks of tax. So every two characters, we want to create a new 50 character long section. So if it was something like this, so okay, that's no. So if our tech was like, hello, I am [inaudible] Raj, this would generate something like, you know, uh, so start off with, hello, I'm Sarah. There'll be the first one and then it will skip to, and then we'll say low, I am Saroj whatever's next because, and then skip to, so then, oh, I am Saroj because more, right? So it's going to keep generating these texts and these, these chunks are overlapping there overlapping chunks. And there are different ways we could feed them in. We don't have to have them overlap.

Speaker 1:          00:22:14       We can have them all be unique. But in this case we're going to have them overlap because we don't have a lot of data. So we want to reuse some of the data if we can. Okay. Does that mean that our model is going to be less accurate than we were if we were to give it just only unique taxed? Yes, it will be less accurate then we, if we were to give it just unique taxed, but uh, we don't have shitloads of computing power, so we're going to do that. So now let's, let's do this. So we're going to, we're going to do this. So we're going to generate, so we're going to iterate through, uh, are taxed. We're going to iterate through our text, skipping where we need to skip and we're going to upend the sections list by saying, okay, so our text,

Speaker 2:          00:23:10       okay,

Speaker 1:          00:23:12       we're going to Upenn or sections in our next characters list. You're good. Okay. So then our next check characters were good to compend text hi. Plus length per section. So we're filling the, the sections with those chunks of texts and the next character is with the next character. Exactly. So these are, this is us cure. No Siri, I hate Siri. Siri is such a terrible assistant. So we're going to say, so now that we have these chunks that I just described, we're going to vectorize them. So now to vectorize them, we're going to use non pies. Opening bracket needed for a list.

Speaker 2:          00:24:16       Hold on

Speaker 1:          00:24:18       text, hold on. So zero length text.

Speaker 2:          00:24:23       Okay.

Speaker 1:          00:24:24       Minus length per section. Skip text sections. Dot. Append. Hold on. Text. Let me just, this is actually, this should be something else. It should be,

Speaker 2:          00:24:37       hold on, hold on. So yeah,

Speaker 1:          00:24:45       sections dot. Upenn should actually be the text I apparently there's a missing bracket that our bond and Tom are saying. Thank you. Thank you guys. Text and then the next characters. Dot append text. Hi. Length per section. Okay,

Speaker 2:          00:25:06       so w so that's it. Oh no wait, wait, wait. You're mic. Your mic just went off. Awesome. One second. We're going to switch over one second. One Sec. Are we gonna switch minds? I don't want, I'm just going to switch the battery. I think that'll be quicker because it really, I mean it's still worth your side. Just went out my side. I think we're good.

Speaker 1:          00:25:47       And Five, four, three, two, one. And we're back. We're back. We're back. We're back. Okay, great. Okay, so where was I? So we're going to vectorize these inputs. Okay. We're going to vectorize these inputs. So to do that, we're going to say, okay, x equals Pi none. Pi Dot Zeros and

Speaker 2:          00:26:14       God, Damn, this is compact code. Okay.

Speaker 1:          00:26:20       Length of the sections.

Speaker 2:          00:26:25       Okay.

Speaker 1:          00:26:27       We need to get to the machine learning. So what I'm gonna do is a paste it from here and then explain it right here. We need to get to the machine learning. Okay, we're going to vectorize our inputs and outputs. They're going to both be zero matrices that are going to be the, so it's going to be this length of the sections by the length per section, by the length PR, uh, by the length per section. Yes, exactly. So, and then, so what are our labels? What are our labels? Are labels are going to be the next character. So we're creating labels. Each of our predictions is going to have a label and the label is going to be the next character. Okay, so, so when we have an output, our output is going to be our predicted next character. That's going to be our output label and the actual label was going to be the real next character.

Speaker 1:          00:27:18       So given some character in our sentence like you know for the word thaw for h, the next word we know is he, so the label is going to be e but are predicting next character is going to be z and w. And so this goes back to that initial question, why convert a charge to ids? We need to wait an Alpha knew we need a numeric way of representing these because we want to minimize our loss and our loss function is going to minimize the distance between our predicted character slash label and our actual next character. Slash. Label. Okay, so that's what our labels are in this case. So we have someone pointing out that range is misspelled actually. Um, he is saying that there's a Typo in range above the function definition. Thank you. Above the function definition ranges. Misspelled. Okay, so range is

Speaker 2:          00:28:09       mm,

Speaker 1:          00:28:13       range is, it says R,N a.G instead of r a n. G. E maybe you can control off it or something. R A. N. G. O. R. N. A. G. A. G. No. Hmm. Okay. No, I fixed that. Don't worry about it. We're okay. Okay. So now we're getting to the machine learning. Guys. Let's, let's do this. Get Ready, get ready. We're about to do some machine learning. Get ready for this. Okay. And we're going to add some VR into the mix. Guys. We're going to add some VR into the mix. This is the shit right now. This is the shit. Here we go. So the first thing, so machine learning

Speaker 2:          00:28:51       time.

Speaker 1:          00:28:54       Okay. So here we go. So the first thing we want to do is we want to define our batch size. So how, so in the in general, let's talk about these terminologies. They are continuously used in machine learning. The batch side defines the number of samples that are going to be propagated, uh, uh, through the network. So, and then one. And so we have batches. We have epochs and then we have um, iterations. So for one epoch is going to be one forward pass and one backward pass. And of all the training samples, the bad sides is a number of training samples in one forward, backward pass. So the higher the batch size, the more memory space we'll need. Okay. So the more memory space we'll need. And so

Speaker 1:          00:29:47       it's ready to go when we are. Okay, cool. Thanks. We're about to do some PR. So, and then to show line numbers. Someone shout how to show line numbers in a second. Showmen shout out if this guy wants to know, well I code this out. So we have batch sizes, five 12, that's the bad size. And now we're going to define the number of iterations, the number of iterations. How many do we want to have? Let's say 72,000 why 72,000 uh, we could have 70,000 we don't actually have a lot of, because I don't have, I'm not running this in the cloud. I'm running this locally. I'm running this locally. Okay. So we've vectorize or input because we want to feed them into our model. We always have to vectorize our data before we feed it into our model. It's a way of converting it into vectors slashed. Tensors okay, so now what we're going to do is we're going to say, how often do we want to log while let's log every 100 steps. Okay? Oh, so and so why did I set 72,000 or 70,000 because we don't have a lot of computing power.

Speaker 1:          00:30:50       So that's how often we want to log. Like where we are just for printing. So you know, we'll, during our training iteration, we'll use this as a checkpoint to say, okay, now time to print something out. Okay, so now we have saved every, and so how often do we want to save our model? This is also a, a, an important thing. We want to continuously save our model so we don't lose where we are in time. Okay? And now we want to say, how many hidden nodes do we want to have? So I'm going to say ten one hundred and twenty four why 1024 well, when we're choosing our hidden knows, if we have too few, then we result in under fitting. That means our model doesn't have enough data to general. It doesn't have enough general generalized storage. It's a hasn't stored enough generalized vectors from what it's trained on.

Speaker 1:          00:31:36       But if we have too many hit hidden nodes, then we're going to going to overfit because it's to fit to the data and it's be able to generalize well to other data. So we have to have that ideal number of hidden notes and there's actually a formula for calculating the ideal number of hidden nodes and you take into account the number of input nodes and the number of output nodes. But right now we're going to choose 1,024 because it's in that range. Okay, so let's give it some starting texts. What does that text that we want to train it on? Some initial texts from which it generates an entire article from, so the starting text is going to be

Speaker 2:          00:32:16       okay.

Speaker 1:          00:32:16       I am thinking that that's our starting text. Okay. That's what we're going to generate data from. Okay. So that's that. And so now we're going to

Speaker 2:          00:32:30       say that's our starting texts. Okay.

Speaker 1:          00:32:41       Oh was so I'll call it test texts. Start Test Star. That's what I'll call it. And so now we're going to save our model, our model. And so we wanted to find a checkpoint directory. So intenser flow, we define checkpoints and checkpoints are just, they are flags. That's that, that define how uh, where, where in the training process we are. So ideally we want several checkpoints so we can, you know, depending on, you know, just for testing purposes, for debugging purposes, obviously the last checkpoint is going to be the where we want. That's the model we want to use is the fully trained model. And so this is actually some,

Speaker 2:          00:33:22       uh,

Speaker 1:          00:33:25       do you have a second for questions? I have a second for questions. So, um, who do all is thing in this games, there'll be too much data and held to be held in weights, will want it slow down. Our processing. And Sahil is asking how did you find the right number of hidden nodes? That is a great question. So heal is killing it with this questions today. Okay, so the question was, hold on. Okay, great. That they can pop up. Say it again. So, so he'll uh, was asking how did you find the right number of hidden nodes? And you want to go with that? Okay, that's a good one. Okay. So the number of hidden nodes, so calculating the, the hidden nodes. So there's a formula towards the formula. The formula is, okay, so this is a great one right here. Hold on. Where's the formula? This right here. So check this out.

Speaker 1:          00:34:19       Okay. There's for supervised learning, which is what we're doing because we have labels, we are creating those labels ourselves. Those labels are the next set of characters and this formula where n is the number of input neurons and, and the next one is the number of output neurons, number of the number of samples in the training data set and some arbitrary scaling factor. If we were to use this formula, this would give us a good estimate for what our head notes should be. Okay. And this. So hopefully that answers your question. So back to this. So now

Speaker 1:          00:34:55       let's build this model we're going to build LSTM and we're about to use VR in is, I'll tell, I'll let you know. I'll let you know. I'll let you know. I'll let you know. So let's build our model. So let's create our graph, our computation graph. And in tensorflow we always initialize a graph like this. And then we're going to say with graph.as default, why? Why as default? This is our computation graph because if we had multiple glip graphs, this is a way for us to define which graph. If we could say graph has default, and then the identifier for this graph is default, the identify for the next graph. But in this case we just have one graph. Having multiple graphs is not where we are. Now. We don't actually need to run multiple Grad, we just need one graph. Okay, so is there no, is there another link that they should be a link to apart from the Dataset? Uh, yes. So let me, let me type that in. What did I just,

Speaker 1:          00:35:55       and the, and I'll put in a description as well for, I just pasted it. Cool. Oh, so, okay. So then, so we're going to define us. The first thing that we build our model is we want you to find our global stats. Okay. We wanted to find our global steps. So our global step, our global step is, refers to the number of batches that are seen by the graph. So every time a batch has provided, the weights are updated in the direction that minimizes the loss. And global step just keeps track of the number of batches seen so far. So we're going to start off as a zero, okay. And it's going to be, hey, tensorflow variable, uh, because variables are tensorflow primitives that can be modified and we are going to modify it, okay. Because we're going to add to it as opposed to a t.

Speaker 1:          00:36:40       F. Dot. Constant that never gets modified. Those are immutable primitives. So let's build the shit out of this. So the first thing we're gonna do is we're going to say, let's define our placeholder. So are placeholders are our gateways for our data. And these are going to be float 30 twos because they are 32 bit integers. And we're going to say we're going to define this tensor size. So the tense or size, it's going to be three dimensional. Okay. It's going to be three dimensional because we have three parameters for it. This is the size of the tea, the data that we're going to feed in to our model. Okay. And so to define this, let me define both. Uh, gateways. We have two place holders, right? One for our data and then one for our labels. They're both going to be 32 bit integers and uh, the size of this one.

Speaker 1:          00:37:28       So here's how it goes down. Our data is going to be those character, a 50 length character, uh, sentences that we are creating and our labels are going to be the likely next character. A list of a likely next characters, no, sorry, not sorry. Wrong. Our labels are going to be the actual next characters depending on given our sentence. What does that next character, the, the, the, the actual next character. We want to predict the next character. So the labels are there for us to treat this as a classification problem. That's what they're there for. So now that we have that, we're going to build our LSTM network. This is what I've been waiting for. We're going to build this LSTM network. So let me show you this shit. Okay, we're going to build this LSTM network and I have a great link to show you guys if I can find it. So here's the link. I'm going to paste it in here. Here's the link and I'm going to look at it on my screen. Where did I say it?

Speaker 3:          00:38:37       Where's the link? No, hold on. Where his, that plank. Hold on. I got this. Did I say it here? No, I didn't.

Speaker 2:          00:38:45       Oh

Speaker 1:          00:38:48       No. So this is actually a better LSTM cell. Look. Let's just say LSTM network. Check this out and then we're going to go into VR in a second.

Speaker 3:          00:38:56       Here it is

Speaker 1:          00:38:58       an LSTM network and this is just, this shows it through time. That's, that's why we have, that's why we have two of them side by side, because it's showing it through time. So if we were to just split this, in fact, I can just split it myself.

Speaker 2:          00:39:12       Okay.

Speaker 1:          00:39:13       That's an LSTM network right there through time. So let me, let me actually just write this down and do our get ready for VR. Get ready for Vr. Here we go.

Speaker 3:          00:39:23       Okay. Okay. So let's do this in Vr.

Speaker 1:          00:39:33       So let's the womb, we are now in virtual reality. We are now in virtual reality. Okay. Here's what it is. So let me see this. Okay, great. So Undo. Yup. Nope. Here is us one to many. This one, yes. To and do that. Okay. So here's what it is. We are, so if you could get switched to the bass. Okay, great. Okay. Yeah. So this is an LSTM cell, right? And we can think of it as a box. So an Lstm cell that's really all in a, an an LSTM recurrent network. The LSTM cell is, is uh, the neuron. It's, it is that neuron, right? So when we feed data into an LSTM recurrent network, this is the neuron. So we have an input gate, an output gate, uh, forget gate. So we'll say Aye.

Speaker 3:          00:40:24       Uh Oh

Speaker 1:          00:40:26       f and then we have an internal hidden state, which we'll define as h. And so the reason I'm doing this in Vr is because, and this represents the copy of it. So this is copy one, this is two, this is three. And so what happens is through time we update this. So we back propagate through time with this. And so what do, I mean each of these gates has a series of weight values. So each of these has a series of wait values. Okay. And these weight values are going to be updated through time. And we have a standard, we have a set, a set of equations that we perform every time to calculate a what this hidden state is going to be. LSTS are a way for us to capture longterm dependencies for us to capture longterm memory. And when we back propagate through time, we are updating the, the state of the cell.

Speaker 1:          00:41:20       Okay. And that's what I mean by through time because as we add new batches, new sentences, ah, we are feeding in, not just uh, the new data we are feeding in the previous state of the STM. We, we're pre fitting in the previous state of the LSTM. It's just going to back propagate through time for as many iterations as we have. Okay. And we're going to, we're going to do that right now. We're going to define all of these gates programmatically. Okay. So that's it for Vr. I hope you guys liked that little VR demo. That was cool. Let's see. Oh, we're really excited about it. Oh, that's awesome. Okay, I'm glad you guys liked the VR. Okay. So that's for the, that's it for the VR. Let's get back to the code. So I'm glad you guys liked that. So here we go with the code. So here we go. So let's define our, uh, gates. So we have our input gate, our output gates and hour we're going to or have our implicate our Applegate, uh, and our, uh, forget the gates as well as our internal state. So these are going to be calculated. They will be calculated. Um,

Speaker 1:          00:42:45       they will be calculated first. Uh, uh, they're not related to each other. They're going to be calculated in, uh, like, what's the word I'm trying to look for it like siphons. They're going to be calculated in vacuums, vacuums, there'll be calculated in vacuums.

Speaker 1:          00:43:04       And then we're going to, then we're going to add it out. So here's what I mean by that. So let's write out the first thing, the first side of weights, and then we'll write out the next one. So we're going to have a TF variable to represent this, this value, okay. Because the why, because these are mutable, mutable, primitive. And so the TF dot. Truncated normal method, we'll define, uh, it's going to define a random distribution. So we're going to initialize these weights randomly as we do in network. In neural nets. We start off with a random distribution, uh, and this, and the, this is going to be our char, char size and our hidden nodes. And then this is our mean and standard deviation. So these are values that define, yeah,

Speaker 1:          00:43:53       how that distribution is going to look like. So remember it's a, it's a distribution, like a golf Sian, right? It's a distribution. So we're going to generate those weights randomly. So that's the weights. And so I'm going to define this. Don't worry, don't worry about it. I'm going to define it. So we have our starting weights and then we have our next set of weights. And so our next set of way I can just actually copy and paste this. Our next set of weights, let me just, it's going to be three lines of code and then I'm going explain it. So TF dot. Truncated normal of hidden nodes and then of hidden nodes, that distribution. And then lastly, our bias. So this is going to happen for are uh, input gates. This is our input gate. Okay. So this is our input gate. This is, this is the low level guys. This is the low level right now. So we have our bias. So remember, think of each of these as a perceptron. These are neural nets in and of themselves. We have neural nets inside of a neural net neuroception, basically neuroception. So

Speaker 2:          00:44:56       okay,

Speaker 1:          00:44:57       here's what it is. Here's what I have. Just coat it. What I've just coded is the input gate. So the input gate has,

Speaker 2:          00:45:06       mmm.

Speaker 1:          00:45:11       The implicate has the, uh, what was I trying to say? The input gate has the weights, um, and it has, it has an initial set of weights for the input and then has waits for the output and then has a bias vector. Okay. So it has, it has an initial set of weights and then un UN un UN outputs that have weights and then a bias vector. And guess what, these are calculated in vacuum. So none of these have anything to do with each other yet. We're just initializing them. So let me, let me just say this three times or three more times. So this is going to be the input. This is going to be our, forget where the weights for the input and the wait for the previous output. So let me say this. So the weights for inputs then waits for previous output as well as bias. All hail the input gate. We have our implicate. So now that's our implicate. And so now we're going to define our for decades.

Speaker 2:          00:46:06       Okay,

Speaker 1:          00:46:07       you guys are champs for sticking through this because this is some, this is some good stuff. Forget gate. And then we have our health book gate. We have our output gate, we have zero Roi. And then we have uh oh and then I, and then see I, this is us, our way of, of um, this our memory cell. This is the, the internal hidden state. Oh, and we're going to get into those calculations in a second. We are just defining these gates. That's all we're doing. We're defining gates. Gates have to be defined. Bill Gates doesn't need to be defined. He's already defined by Microsoft. So we have our memory cell and the rest of our gates. So we, we've named them all respectively. There are unique names and let me make sure that the values inside of them are good to go. They are good to go because they are there are initialize the same way. They're initialized the same way. All of them. Okay. So yes. Great. Those are our cells. And oh wait, hold on. Input. Forget how put, and lastly cell states, those are our, those are our, I will definitely do an outline at the end.

Speaker 2:          00:47:18       Yeah,

Speaker 1:          00:47:19       the eye is duplicating. Thank you. Okay, so now let's, let's calculate the cell. We've defined them. So this is the actual cell. This is how we define ourselves. And these are calculated separately. No overlap until we get to this. And so let me paste this part because I need to explain what is going on here. Okay? So here's what's going on. Here's what, here's what the deal is. Let me make sure it's all visible because we're about to explain this shit.

Speaker 2:          00:47:56       Okay.

Speaker 1:          00:47:56       Oh, don't hold your borrower says hold your horses. No, that's a duplicate. That's a duplicate and no, no, no, that's very useful actually. I wrote that because it's very useful. Okay, so here's what it is. Here's what it is.

Speaker 1:          00:48:10       Here's what it is. We are calculus. So, so here, here are the equations by the way. So these are the set of equations and this is their programmatic. These are the set of equations for calculating our, what do we want to calculate? We want to calculate an output and a state and we're going to answer some questions. Go for on. Not a question actually there seeing the W io. Uh, you have the, you have a twice instead of it should be w underscores CEO. Thank you. Great. We are, these are the, and I linked to this before. These are the set of equations and I know it can be a little confusing. This is math terminology. You have to do a little bit of, you know, refreshing. We're not sitting here doing Algebra homework every day, right? So of course we're going to need a little bit of refreshing.

Speaker 1:          00:49:00       It's okay. These are just, uh, they're just, this is a language. Math is a language. We want to remember this language and how do we remember the language by practicing it? Okay, so this is the language. Okay. The, these are the set of equations that it's going to calculate our output state. And this is how this is what it's represented. Like programmatically. No. So I prefer programmatic implementations to the a and the link is going to happen in the, in the chat. I link to it before, but it is in the description. All the links are in the chat and the description. Yes. Thanks. [inaudible] wizard of Oz. So that's what, that's what we did. And this is the LSTM cell. Okay. So what we're gonna do is we're going to matrix multiply the, which is essentially a dot product. We're going to multiply the input times the input weights plus the output times the weight for the previous output plus the bias. And the bias is our anchor points. It makes sure it helps prevent over fitting. So that's how we calculate the input gate. And that calculation has nothing to do with the rest of the other three calculations. Yet these are all the calculus calculations that basically they're using the previous output and the current input and there's summing them together to, to, to, uh,

Speaker 1:          00:50:19       to represent how data has changed over time. Now that we have these four, these four,

Speaker 2:          00:50:27       yeah, essentially,

Speaker 1:          00:50:28       uh, primitives that we've created the input for gay output and memory cell. Now we can combine them all together. And so this is that last equation. This is how we combine them together. We take the four get date, we multiply it by the current state, and then we add it to the input gate times the memory, still times the memory cell. So the, the, the state right here is the given state, uh, where we are currently and the memory cells is the state where we were before. So we take the input gate time, what we remembered before, plus the forget gate times what we now get because the F and why are we multiplying it because we want to see it. What do we want to forget from what we're given and then what do we want to remember from what we've stored previously. And that output is going to give us a final state and we're going to take that state value and we're going to squash it with a nonlinearity, with a, with a non linearity.

Speaker 1:          00:51:27       And the nonlinearity is going to be Tan h, which is a hyperbolic tangent of x. Uh, which essentially we don't have to get into the details. It is one of many nonlinearities. But what it does here is it squashes it into a probability value. That means that converts the state, whatever. It's, whatever it is, a floating point number into a single probability value. And uh, we're gonna multiply it by the output gate and that's going to be our output. And at the end we were turned our, our final output of the Lstm cell and oh, sorry, the final, not sorry, the final state of the OCM and then the output value that it calculates. So that's what that's going to do. And then,

Speaker 2:          00:52:12       yeah,

Speaker 1:          00:52:14       let me know when you want to take questions. We have some, we have some questions. Yeah, go for the questions. So, um, Kevin Nelson was asking, can you use tensorflow to visualize the network at some point or Nah, I think that might've been a little bit far back and Praveen just ask was it raspberry pi or on tense would flow. Okay. So Kyi, Kevin's question. Let me paste this in and I'm going to explain it to Kevin's question. Can we use tensorflow to visualize it? Yes. With tensor board, which I'm sure you know, we, I need to use tensor board more often. Thank you for the reminder. We're not doing it this time, but we will in the future. In fact, I'm probably going to have a tensor board specific.

Speaker 2:          00:52:52       Okay.

Speaker 1:          00:52:52       Live stream coming up soon. So, and then Praveen and Praveen was asking, um, if the raspberry Pi on transfer flow, and there's people now having conversations about raspberry, raspberry pie on tens of floods. So I'm pretty sure that somebody made a tensorflow raspberry Pi a rapper. So there it is.

Speaker 2:          00:53:19       Yeah,

Speaker 1:          00:53:20       there it is. So here we go with this. So back to this, we want to now, uh, calculate the LSTM value over time. So this is our unrolling over time. So before we start training, we want to feed it in all of those values so that we can compute that those longterm dependencies. And this is what's happening here. We're going to start off with an empty set of matrices for our, our output in our state. And then we're going to unroll the LSTM loop over time. So what does this mean? So for each section we're going to calculate a state and an output from our LSTM. So to start off with, when I equals zero, the first round, we're going to give it into our, give it to our outputs. All I list, this is a list of outputs that we are storing over time. And this is our labels list that we are storing over time. So once we're past that initial iteration, then we can start, uh, concatenating vectors along a dimension access. So we're not multiplying, we're just adding in all of those outputs and all of those labels. Until finally at the last iteration, we, we have our final value. Okay. And so,

Speaker 1:          00:54:35       okay, so then up to the optimizer part, we're going to be okay. So I'm going to paste this in as well. So then for their classifier, our classifier is, is, is, is only gonna run after our saved output and save state were assigned. What does that mean? We have our list of outputs. We have our list of states. I will show a demo at the end.

Speaker 2:          00:54:59       Uh, so

Speaker 1:          00:55:04       now that we have our LSTM unrolled over time, that the dope, the LSTM cell is the only thing that we really, that we worry about in our, in an LSTM network, we just needed to find the LSTM cell. And then we have, uh, the weights from cells, plural. And then we have the weights from the hidden layer of LSTM cells to the output layer. And that's what we're defining here, that last set of weights for the bigger network in and of itself, not the cells, but the bigger network and to those are defined by WNB and they're generated randomly using the truncated normal function. And then we're going to calculate the logics. So logics is so largest is uh, the, the function that operates on the unscaled output of earlier layers. So it's a way for us to think about, uh, what, uh, what is going to be that, that value that we feed to our loss function.

Speaker 1:          00:56:01       Okay. So it is our prediction outputs. It's a, it's a, it's a list of all of our predicted outputs and we're going to compare it with the labels. So we have predictions. And then we have our labels and then we want to minimize. So he had predicted characters and then actual characters. And we'll minimize the loss using cross entropy with low budgets. So why are we using cross entropy? Because Cross entropy, a good optimizer for, uh, multi-class classification, which is what this is. We have a question from go. Cool. Who's seeing many people you normally use relo? Why use sigmoid or Tan and the reasons? Uh, good question. Uh, so [inaudible] is 10. It is becoming more popular these days. Uh, why use raw over tan? Uh, that's a good question. In fact, I should have a video comparing different activation functions and when and why you should use them. In this case, we're not using rally Lou because, uh, the Tan function because we're using, uh, a softmax specifically because we're using softmax cross entropy for multi-class classification. Generally, uh, 10 h is paired with multi-class classification, whereas rail Relo is, uh, with,

Speaker 1:          00:57:16       sorry, Tan h is with multi-class classification for recurrent networks, whereas [inaudible] is more often used for convolutional networks. We have seen them in recurrent networks, uh, but they're mostly used for convolutional networks. And, uh, I'm actually gonna have a video coming out on that soon. Then we're going to use the reduced mean function to calculate the average probability. And that is our loss. So given all of those probabilities across the list of possible characters, we're going to compute the average. Okay. And then we're going to that as our loss and that is a loss that we want to minimize using gradient descent, given our global step values that keeps track of all of the, uh, values. Okay. So then we're going to,

Speaker 2:          00:58:00       sorry.

Speaker 1:          00:58:01       So then we're going to train our model to here. Is it for our training step? Hold on. So let me make sure this is all good. So online hate, batch size, char size.

Speaker 2:          00:58:16       Hold on.

Speaker 1:          00:58:21       Okay. So here's what it is. It's sad. Tf Low 32 batch size charts has invalid syntax because?

Speaker 2:          00:58:31       Because

Speaker 1:          00:58:42       it is an omnium. See how many people are here right now? Okay. Three and four. Nine. Good. So we have an invalid batch size and an invalid char size. And, uh, oh right. There we go.

Speaker 2:          00:58:55       Great.

Speaker 1:          00:58:57       Great. Okay, so now for our training step, so I'm going to paste this in. Uh, so here's how, here's how it's going to work. Uh, so we have our graph. Okay. And we're going to first initialize our variables. So this is our global variable and then an offset value that I'm going to talk about. Their real, we got to, the real things to talk about here is the offset value and then a saver. Cause we're going to just save our model every so often. Okay. And uh, so for each of our training steps, for each of our training steps, we're going to start off as zero. So this is our actual training steps. We are going to feed it in as batches. We are data and labels that we feed in as batches and the offset helps us calculate what those batches are going to be, the size of those batches. Okay. And, and there's, there's a calm next to bat size that you're missing. Yeah, I added that. Thanks.

Speaker 2:          00:59:46       Okay.

Speaker 1:          00:59:47       Up there. Uh, and so,

Speaker 2:          00:59:50       okay,

Speaker 1:          00:59:51       we want to feed it to the model model iteratively. So for the first part we'll calculate those batches and the labels. We'll update the offset until, and we're using that offset to make sure that we have the right data at the end, uh, at the very end right here. Okay. And then we're going to optimize it with our loss function and then we're going to feed it in as dictionaries, batch data, batch labels in batches. We get our training loss, uh, and we and the run functions going to run our optimizer given our loss. Right? And it's going to every step. And then remember that those that's saved. Every variable we use to to uh,

Speaker 2:          01:00:28       okay.

Speaker 1:          01:00:28       Calculate that. So think there might be a semi colon that's needed. It's what people are saying.

Speaker 2:          01:00:40       Okay.

Speaker 1:          01:00:41       Where though, tell us where it guys, hold on. Do you have to have variables? Initializer yeah, coming up on our, on the album coming up on the hour, coming up on the hour. Okay. initializer.run.

Speaker 1:          01:00:57       You're not actually, let's move this for a second. So, okay. So this is actually going to take a while to train. We're running up on the hour. Let's, what did I have trained right here? So, so here's what happened. So when I give it this, so here's the demo and then we're going, I'm going to show more demo code. Uh, so when I give it this value, I plan to make the world a better place. It's going to initialize the graph and load the model. So for every character in the input sentence, it's going to generate the next character for every character is going to generate.

Speaker 1:          01:01:31       Then the next character and okay, so it's character out of character, character out of character at a time. So this is actually pretty bad right here. Why? Because I trained it for one iteration before I started this session. I plan to make the world a better place by off, I guess parts. It's act, it looks like Latin because we are generating it with one iteration. But what happens when we generate it with lots of iterations? Okay, here's the demo of what happens when we generate it with this exact, this exact code, this exact type of code and LSTM network. When we generate lots of iterations and you can see what happens over time here. Uh, where is it? Hold on. Come on baby. Here's what it is. So the evolution of samples, while training is a good example. So it's just like this, right? Just like what I've just showed right here.

Speaker 1:          01:02:19       It starts off really bad like this. And then with each iteration at 308 or so, that was at 100 iterations. At 300 iterations, it's more like two hours. At 500, it's more like five hours. And I'm talking about on a CPU that's like 2.4 gigahertz and then 700 it gets better and better. And by 2000 iterations it is amazing. Okay. It gets better at spelling, it gets better at everything. That's it for our code. Uh, and the, uh, linked to that is in the description and so yeah, last five minute Q and a and then we're done for the day.

Speaker 2:          01:02:55       Okay.

Speaker 1:          01:02:56       Qa time. Yes. And a time. Cool. So we have a couple of things people were asking about different live streams that you want you're planning to do. So party was asking you if you're going to do a drone, a drone image, trucking livestream, and uh, Ozzie Campos asking if you're going to do a stream of bought seek to seek, seek to suite. The sequencing was, yeah. So for the drone, that's going to be the robot video. I'll do a drone video soon. Yeah. For sequence a sequence. That's what I'm in right now. Like I'm in the sequence, the sequence world. We, we made a sequence, the sequence, I made a sequence sequence video last week. I'm making one this weekend. I'm going to make one next week for chatbots. So yeah, that's different sequences. The sequence. Are you a human or AI? I am a human. I am a human for now.

Speaker 1:          01:03:41       Neil Neil Sandy said something really cool. He says it's a great way to build confidence is through your math pep talk that you give. So that's shout out to you. Oh yes. Thank you Neil. Yes, math is just the language voice shop. Ya. Uh, asked earlier if you could please do an outline of the program at the end cause he was kind of losing track of how you're yeah, let me do it. Ending outline. Great. Any other questions before I do the outline? Um, I think that's about, that's the good stuff. Okay. So here's the outline. We started off with our dependencies. This is a fast outline and then we imported our tax, that's Wikipedia text and we said, let's look at let's store only the unique characters because we want to generate unique characters so that we can have these two dictionaries to map the characters to ids and an ids to characters so that we can use that later on, uh, for training.

Speaker 1:          01:04:30       Then we grow, uh, uh, a sample method to generate the probability of each next character given the predicted value for that comes out of our model. We vectorize our data using non pie that we fed in and these are 50 character long sentences. We fed it into our model. Okay. And our model is an LSTM network and LSTM network has an input gate. Uh, forget gate on Applegate and in memory cell. We unrolled it through time. Okay. Before, before we unrolled it through time, we wrote up the equations for that. How data on how data flow through an LSTM cell and rapid. Okay.

Speaker 2:          01:05:05       Hold on.

Speaker 1:          01:05:10       No sleep. That's exactly my life. No sleep. So that's a perfect beat.

Speaker 4:          01:05:19       How has the ship, oh, check your check your sound settings on your laptop. So go gutter system profits.

Speaker 1:          01:05:25       Oh my God. Okay. Stupid. See I will gladly rapid because we were in the last two minutes. So let me just, if the Internet decides to work, could you just play some rap? Like some hip hop beat? Yeah. Like if this just starts.

Speaker 2:          01:05:42       Okay.

Speaker 4:          01:05:43       Um, just whatever.

Speaker 2:          01:05:49       Okay.

Speaker 1:          01:05:52       And the notebook has in the description. Okay. So the notebook is in the description and a

Speaker 4:          01:06:08       want to get a ration before I started this session. Oh, that's violence. To make the world a better place.

Speaker 1:          01:06:17       Does any good. Okay. No, stop that. Stop that. Okay. Here we go. We're, we're, we, we took our LSTM state. We David and input date, we get up, forget gate, we David and how cook cakes. I'm David a memory cell. I calculated that output state. I took the forget gate and multiplied by the state, I give it an input gate and give it a memory sale. Yo. And then I got that output value. I got an output state output and a state. That's what I did for my LSTM cell. I calculated my hair's always either. I unrolled it through time. I was saying, here are all my outputs and here are my labels. Calculate it in the sale, man. I'm like Charles babble. I got a classifier given all of those cells unroll through time. I had our weights and our biases. Man, I'm going online with 350 people live.

Speaker 1:          01:07:19       I got my loss function, um, minimized. It would cross entropy. Ah, this is my function. I optimize gradient descent. Man, I'm online all the time. At the end I ran my training function given the states that came back, we brought back, propagate it through time. And then at the end we calculated our training loss. We optimize, we optimize. All right. Hey, hey, here we go. Yo. That's how we did. We saved the model. All right though. We saved it every day. I'm going crazy. I love rapping, man. It's my day today. Okay. Yeah. So hype man. Let's give him some love at buddy

Speaker 4:          01:07:58       does these kinds of streams where they show machine learned, they teach machine learning and they stream at the same time and they wrap. That's pretty amazing.

Speaker 1:          01:08:05       Thank you. Wizard of Oz coming at you. We are. I am here at the upload VR studios, so thanks everybody for watching this livestream, uh, for being here. I'm going to show the, I'm going to show that code and the code is in the description. I'm going to add comments and read me. Stop to it within the hour. Thanks for coming guys. I love you guys. A little, five less, five more shout outs to names. Sal Load. Javier Anthony, a good CEO. Anthony Sparks, sporanox and James. Hey James. Good to see you. All right. Uh, for now I've got to go practice my, uh, everything because I want to get better at everything. Bigger, better, faster, stronger. Uh, so for, that's what I've got to do. So thanks for watching.

Speaker 3:          01:08:43       Ooh.

Speaker 5:          01:11:12       [inaudible].