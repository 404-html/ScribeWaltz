Speaker 1:          00:00          Hello world. It's Saroj in. Have you ever tried unity before? If you've ever wanted to create a three d game or three d simulation, unity is the easiest way to do that. Recently, unity release a tool to let developers train and test new AI algorithms in a three d world called M l agents. Let's explore how this toolkit works and use it to build a simple simulation that trains ais to balance balls with the rise and machine learning breakthroughs, virtual reality and processing power simulated environments are becoming more and more commonplace across a wide variety of fields. They allow scientists to test out their hypotheses in a safe, repeatable environment for all sorts of experiments, and if you want to test out how powerful your algorithm is, it's a lot funner to watch it perform in a three dimensional world than a two d one or trying to decipher logs in a console when it comes to building a three d environment for gaming or simulations, the two standards that have become commonplace are using unreal engine and unity as a toolkit.

Speaker 1:          01:05          Unreal engine is great for high end graphics, but if you're a beginner, unity is really the way to go. It's allowed small budget indie developers to create really popular games through rapid prototyping games like Minecraft limbo and super meat. Boy, we're all created using unity, but even large studios like CCP, the creators of Eve online use it for rapidly prototyping game concepts. Unity provides a game engine in a box that means a physics and a rendering engine with hooks for several scripting languages. It gives us this really cool visual editor for manipulating the game environment, which lowers the barrier to entry to development substantially. If we started completely from scratch with c plus plus and open GL, it would take us days to get to the point where there's actually something rendered on screen, but with unity, this takes about 10 seconds, take that deadlocks to get started.

Speaker 1:          02:04          With unity, there's a free version and a pro version. We can just use the free version to as little dicky would say, say that money. The big difference between the two is that the free version doesn't have some of the advanced rendering options that creates better looking fast running environments. But for beginners we can just skip that. For Right now, installing unity is pretty straight forward. We can download an executable right from the website and follow the installer instructions. Unity supports both c sharp and Java script. Will you see sharp since that's what's most compatible with the ML agents tool? A class file, we'll lay out the definition of an object and we can instantiate an object from a class. We can attach a new script to an object easily, so if we want an enemy logic object that handles AI for an enemy in our game, we'd write an enemy at logic class.

Speaker 1:          02:58          Then attach that to every enemy entity. When we run our game, each enemy will be equipped with a copy of the enemy logic object. Once we've installed unity, we can clone the MLA agents, get hub repository. Let's build a preexisting example environment, train an agent in it, and then embed the trained model into the unity environment. First we'll launch the unity editor and log in if necessary. Inside the unity environment folder, we'll see the project panel at the bottom of the tool. We can navigate to the ML agent examples folder and select the 3d bowl environment. Once we double click the [inaudible] icon, it'll load all the environment assets. I guess we are now officially balling. Ah No. There are three kinds of objects within any kind of learning environment. The first is the agent which has a unique set of states and observations, takes unique actions within the environment and receives unique rewards for the actions it takes in this environment.

Speaker 1:          04:00          This is classically called the agent environment loop. In reinforcement learning, each agent has a brain. Unlike Sophia, the brain defines a specific state and action space and it's responsible for deciding which actions each of its linked agents will take. The brain can be external, meaning decisions are made using tensorflow, internal meaning and decisions are made using a trained model embedded into the project. The tensorflow sharp player meaning player input decisions or heuristic meaning decisions are made by hand coded behavior. The last kind of object is the academy. It contains all the brains within the environment. We can set multiple agents to a single brain or give them their own brains, decide actions to take in batch fashions and we can even utilize parallel computation. There are lots of options here. Adversarial play, cooperative play. Let's go ahead and expand the ball. Three d game object and locate its child object ball, three d brain within the scene hierarchy in the editor.

Speaker 1:          05:05          We'll ensure that type of brain for this object is set to external. Then it's time to build this thing. Under file build settings. We'll choose development build to log debug messages, then click build. This will save the environment binary to the Python sub directory of are cloned repository. Now we can train our agent brain using reinforcement learning. There's a premade Jupiter notebook in the repository we downloaded that we can use directly to train our agent. It's using the Python Api that unity provides for machine learning. We'll launch Jupiter than find the proximal policy optimization algorithm notebook. I've got a great video on how PPO works to see the link in the description. We'll set the environment to the name of our environment file from earlier. Then set the run path to a directory of our choice. Lastly, we'll run all selves of the notebook except for the last one.

Speaker 1:          06:01          We can use tensor board to observe the training process in more detail as it runs by running the tensor board command from command line. We can observe how rewards are being accumulated, how the loss functions decrease, and how the policy and value functions are changing over time. After the average award is about 75 we can stop the training process since the script periodically saves models that we can use. This is a trained tensorflow model. Our job now is to convert this model into a unity ready format. To do this, we'll use tensorflow sharp. These are.net bindings to the tensorflow library. I know, I know it's Microsoft, but it's okay. We need this right now. We need to enable it so first we'll download it and place it in the assets folder. We'll target our platform, set the scripting runtime version two experimental and add the enabled tensorflow flag to the defined symbols.

Speaker 1:          06:56          We can save the project and restart the editor. Then we'll export the trained tensorflow graph and move the export and bites file to the TF models directory. We can select the three d ball scene in the unity editor and select the ball 3d brain object from the scene hierarchy will change the type of brain to internal and drag the bites file from the project window of the editor to the graph model placeholder into d ball brain inspector window. Then set the graph placeholder size to one. Our placeholder will be called epsilon with the type of floating point and a range of values from zero to zero. Once we pressed the play button, we can visualize the train model being used to control the behavior of the balance ball within the editor itself. The three points to remember from this video are that unity is the easiest way for developers and researchers to train their AI algorithms in a three d environment.

Speaker 1:          07:51          The unity ml agents library lets us build AI using tensorflow and export them to a three d environment directly. And it revolves around three key objects, an agent, a brain, and an academy. One or more agents can share a brain and they all live in the academy environment. Last week's coding challenge winner is Brendan Hunts necked using care. He implemented his own version of deep minds. Alpha go zero, but for a different game called Othello. I'm blown away. A plus 12 plus Brendan and the runner up is Shaad Mon Coochie car who used Karus to detect happiness levels via sentiment analysis. Great work. This week's challenge is to build something with unity's ml agents. Post your get hub link in the comments section, and I'll personally review them and announce the top two entries. Next Friday, please subscribe for more programming videos. And for now I've got to get some sun, so thanks for watching.