Speaker 1:          00:00          Hello world, it's Siraj. And how risky is your credit? That is the question that we're answering today by looking at this credit risk data set. It is a German credit risk data set. We want to know based on your employment history, based on your family history, your in, you know, your income, are you at risk for not paying back your loan? Usually we have people who do this and it takes a long time and you have to, there's a bunch of biases built into the system. You know, you meet the person, whether you're an insurance agent or you work for some bank and you're trying to assess whether or not this person deserves alone. As humans we have a lot of biases and these biases don't necessarily add real value to whether or not this person deserves a loan or not. Right? So the way to fix that is to let machines do the job because machines can find relations in data that we can't.

Speaker 1:          00:51          So the data set we're going to look at is a bunch of financial attributes of somebody, the status of their existing checking account. This is a German data set by the way. And I found it on the UCI, a website also a that is just a great website to find data sets on. So definitely check that website out if you haven't already. We're going to look at their credit history, the duration of payments they've made, the purchases they made, the cars, furniture, all of these things are features, right? They're all features and we can use that to assess whether or not this person has, uh, is at risk for is at risk for not paying back their loans. This is used in a bunch of different fields, insurance of finance, um, whether or not to rent a house to somebody, right? The landlord assesses whether or not you can pay, pay it back savings account that whether or not they're employed, all these features and the label is, this is a history of credit risk.

Speaker 1:          01:48          So there's also a label based on all these features. This person has already been assessed as a at risk or not at risk, right? So that's, that's the mapping that we're going to learn, right? It's a, it's a binary mapping. And the way we're going to do that is by building a random forest. That's really the goal here is to learn about random forests and uh, yeah, so basically what it would look like is something like this, like this picture that we're looking at right now. Eventually once we built this a random forest, which consists of several, what's what are called decision trees, it'll look like this. We'll feed it a new data point and then it will iteratively ask a series of questions like, is there checking account balance above 200 or below 200? And then based on that, it'll ask another set of questions.

Speaker 1:          02:35          Like if you say no, if the answer is no, based on that data point, you'll say, well, what's the length of their current employment? And then are they credible or not credible? Right? So it'll just keep going down this, this, um, this chain of decisions. A decision tree. Okay. So that's what we're going to build. We're going to build a random forest. So what is a random forest? Well, a random forest is a collection of decision trees. So let's talk about what a decision tree is. So a decision tree is actually a relatively simple concept, and so I just didn't even talk about that in this series so far. I'm just going straight into random forests because decision trees are easy stuff. We want to get right into the random forest spot, right? But let's go over decision trees really quickly. So a decision tree is basically a set of decisions on whether or not to classify something.

Speaker 1:          03:23          The technical name for this is the classification and regression tree or cart. And it was invented by a dude named Leo Breman, uh, couple of decades ago, like two decades ago. And it can be used for both, as the name suggests, classification and regression, both of those things. But we're going to use it for classification because that is, that is what we're trying to do, or we're trying to classify whether or not someone is credible or not credible. Okay. So how does this thing work? Well, you have a set of features, right? Let's say the features are the temperature, the wind speed, and the air pressure. And based on these three features, uh, we want to classify whether or not it's rainy. Well, what the decision tree that we build is going to do is it's going to create iteratively, be iteratively, I should say recursively actually it's going to recurs.

Speaker 1:          04:11          The tree itself is built recursively you'll see what I mean when I, when we look at the code, but the tree itself has built recursively and for all of these features they will ask a series of questions until it classifies it as raining or not raining. So the real question is how do we build this thing? What is, how do we build the optimal tree where it is asking the rights a threshold values. Like how does it know that the temperature should be greater than or less than 70 degrees Fahrenheit? And then based on that answer, how does it know whether the wind speed should be greater than 4.5 specifically? Where are these magic numbers coming from? Well, they're coming from [inaudible], the Gini index, that's what it's called. It's called the Gini index genie in a bottle genie in a bottle, not genie in a bottle.

Speaker 1:          04:57          Gd, it's the Italian Jeannie. So, uh, it was some dude named Jeanie. I actually don't know. Uh, we're not going to get into that. Anyway, let's talk about the Gini Index. So the Gini index is the loss function here. But the difference here between what we've done before with grading dissent with Newton's method is that there is no convex optimization happening here. We're not trying to find the minimum of some convex function. There is no error that we're trying to minimize. The Gini index is a cost function that works differently. And here's how it works. Basically for every single point in our data set, right, we've got a bunch of different features. We want to find that ideal threshold value. And so I'm going to explain this once and then explain it again when we get into the code. But we want to find that ideal feature, right?

Speaker 1:          05:43          What that ideal value for a feature. Okay. So what do I mean by that? So here's how it works. Check this out. We've got a Dataset with a bunch of features, right? 10 features, right? So one, let's say, let's say one of them is the income. So the income could be anywhere, and I'm gonna use USD for this example. It could be anywhere from $10,000 a year to $1 million a year. So what the Gini index is going to do is it's going to go through, so what we're gonna do is we're going to iterate through every single data point for that feature. So we're going to say we're going to iterate through every single data point and we're going to compute this Gini index, which is one minus the sum of it is the no. So, so the, the Gini Index is whereas the formula for it, it is one,

Speaker 1:          06:37          it is wine minus the average times the average where the average is proportion, right? One minus the average of all of the class values times that average. And that gives us the Gini Index. And so what we want to do is it comes out to some single value. So I'm a scalar value. And so basically we use, we start from, we start from data 0.0 and we go up to data point and where n is the number of data points and we compute the index for each of these data points for a specific feature, right? So let's say the first data point is 10,000 will compute a Gini index for that data point for that value, for that amount of income. And so what happens is basically it goes on a scale. So

Speaker 1:          07:20          a Gini score of zero is the worst case. Gini score of zero means that based on that index for all the other data points they're gonna be, they're gonna be evenly split between less than that value and greater than that value. But that's not what we want. Ideally we want a Gini score of one that is the ideal Gini core. And what that means is that for that given a value that given value for that specific feature, all the classes from one, all the classes from all the data points from one class will be on one side of that threshold value. And all the data points from the other class will be on the other side of that threshold value. That will give us a one value that is the a one and Jeannie value, right? And so what we do is we just compute the Gini index for every single data point for every single feature.

Speaker 1:          08:07          And we just do that for every feature, right? So we, so for let's say for income or Compute, the Gini index all say, okay, so let's start with 10,000 we'll compare every other day at point of 10,000 and see whether it lies on the left or right, whether it's greater or less than. And that we get we then we compute the Gini index from that. Okay. Which is this formula right here. And we do that for every single data point. And so what's gonna happen is we're going to have a collection of genie indices, will have a set of Jeannie indices and then we'll pick the one that is the highest and the highest one is the one such that the data points are most our most uh, not evenly split. That means the most data points from class a are going to be on the left or right.

Speaker 1:          08:49          And the most data point from the other class will be on the opposite side. Do you see what I'm saying? And by side I mean greater than or less so the, so the worst genie, the worst case is when the data points are evenly split. We don't want that. We want them to be all on one side and all on the other side. That means that when we get a new data point, it'll plop down right into it's, it's, it's a little bucket with all the rest of, it's a related data points, right? So that's the Gini index and there are different measures of a loss when it comes to arithmetic based machine learning models. Instead of Matrix, a matrix, operation based machine learning models like we see with neural networks. Okay, so that's a Gini score, Gini index, whatever you want to call it.

Speaker 1:          09:35          So how do we build this decision tree? Well, it is. There are two parts. We, first we've got to construct the tree. So we, that's a recursive process that you'll see. We have to construct the tree and once we construct the tree, then we prune the tree. So that means we identify and remove the irrelevant branches that might lead to outliers to increase classification accuracy. So wait a second, you might be asking, why are we building a random forest in the first place? Why can't we just build a decision tree alone? Well, what happens is if you just build a decision tree, that's not fun. No, there's a better answer. If you just build a decision tree, then your decision tree could be over fit. That is a big problem when it comes to decision trees, right? It's the decision tree gets over fit to the data, right?

Speaker 1:          10:21          It's like, it's like, uh, the boy, someone might memorize an eye chart. It's not like they can see it properly, right? With one eye closed, they just memorize the position of where everything is. Right? In that same way, we don't want to over fit for a data to our data. So the way to prevent that is to create a bunch of decision trees on random sub samples of the data. So we'll define subset, some set of sub samples, and we'll say for each of these sub samples we'll create a decision tree. Then once we have a bunch of decision trees that we've trained right then by train, I mean we've computed the Gini index for all of the features and then we've recursively built the tree. It's a binary tree. By the way, I didn't mention that the t decision trees are binary trees right there.

Speaker 1:          11:05          Either I left node or write note or no node, right? It's the, it's the last node. And so, um, and if you haven't reviewed binary trees, I mean, we're essentially building a binary tree right now. But if you want to learn more about data structures and algorithms or if you're a curious if you should know data structures and algorithms for machine learning, the answer is yes for two reasons. One, just for the logic sake, you need to know how data is stored because machine learning isn't just about matrix operations. It's all also about storing data, right? Serializing and storing data in the most efficient way possible and retrieving it. And if you want to build algorithms, you got to have your basic data structure and algorithm knowledge intact. Okay. I just wanted to say that back to this, the way random forest work are each of the decision trees that are generated.

Speaker 1:          11:53          Then we'll just, once we have a new data point, we'll run it through all of those decision trees. They'll all make a prediction and then we'll make a majority vote. So we'll, we'll calculate a majority vote. So each of the votes for each of the trees, whatever the majority vote, which is the class that is the class that we're predicting. And what this does is it gives us higher accuracy than just using a decision tree alone. Let me also say that random forests are one of the most used machine learning techniques out there. They can be because they can be used for both classification and regression. And there in lies almost 90 what? 90 plus percent of problems. Right? And it also works well for very small data sets, which we tend to have a lot of.

Speaker 1:          12:38          And so that's, it's so random forest or just use the law. They're used so much that, how can I, how can I, how can I say this? There you so much that the guy, what's his name? Josh Gordon. The Google dude, his name on Twitter is random for us. So they are very useful. Hi Josh. If you're watching this. Okay, back to this. The, okay, so we're training, we're training it on subsets of data, right? One subset per tree. And that is our random forest. It's a forest because it consists of trees as you've probably guessed. And so if you create a giant random forest, you get lord of the rings. Rivendale style. No, you don't, you, the bigger, the better. Generally you'll see at the end, the more trees we add, the better our accuracy score gets, right? So yeah, in each of our nodes are going to represent a set of feature splits.

Speaker 1:          13:26          Right? So what's the color green? Red. Green. Okay. What's the size? Small. Big. Big. Okay. That fruit is a watermelon, right? So we just recursively do that. Are there other good examples of this you might be asking? And the answer is yes, of course there are other good examples. The stock price, prediction and classification. I've got two great examples here. Definitely check them out. The documentation is pretty sparse, but the code itself, it's not using any libraries. So definitely check it out. All right, so now the, let's go into the order of functions that we're going to follow. So we're not gonna have time to write every single function. We're not using any libraries, but we will write the two most important functions split and get split. And that's going to really take, take on the majority of our, uh, logic. But that's what we're going to do.

Speaker 1:          14:10          And that's going to be 40 lines of code, but for the rest of it, this is the order of functions that I'm going to follow the chain of functions. So let's just get right down into what this chain of functions will look like. So I'm going to first of all look at our dependencies here. So I'm going to import seed from random and feed is going to generate pseudo random numbers. This is useful for debugging. You want to do it anytime you have some random numbers and you want to debug your code and production or otherwise. It's always great to have some seed so that the random numbers that are generated start from the same point every time. And so that's just great for reproducibility of results. Or I'm also going to import rand range is, so it's going to return a randomly selected element from a range of numbers, CSV, because our Dataset is, by the way, let me, let's see.

Speaker 1:          14:57          Our dataset, our Dataset is a CSV file. It our Dataset is a CSV file. So let's open our dataset and see what it looks like. It is the numeric data right here, right? So all of it is numeric at the end. The result is either two or one, right? It's a binary level, either two or one, and the rest are like 15 features. Here we're going to use every single one of them, no features election. We're going to use every single one of these features. Okay? And it's using arithmetic so that we're only importing math. We're not even importing num Pi. We're only importing the math library. Okay, so let's, let's look at this thing. So we've got some really basic data loader functions here. Load CSV, initialize a Dataset, has a list, open it as a readable file initialize a CSV reader, and then for every row in the dataset appended to this Dataset Matrix, it's a two d matrix.

Speaker 1:          15:48          Return it. And so we have an in memory version of our data. We know that part that's, that's, that's general to all machine learning really. Whenever you're reading a CSP fall, what else do we have here? We have two more helper methods functions. One to convert a column to an inch and one to convert, one to convert a string to an aunt and one to convert an into a stream. And that's if we have string values. But in this case we don't, we have numerical value so we don't need this. Okay, so let's go into this. This order that I was talking about, the order of algorithms. So the first, um, so the first one went to look at is this main code here, right? So we started off with the seeds so that we always start with the, with the same random numbers. We loaded up our Dataset, right?

Speaker 1:          16:30          That CSV file and then we converted our strings to integers. We don't actually need to do that. But then we said, okay, so how many folds do we want to have? And folds means sub samples of data. So we want five sub samples. What is the nax depth? And the depth means how many nodes, what is the depth of the tree, right? How many levels of that tree do we want to create through, we're going to say Max 10 levels. And these are our hyper parameters. We can tune them, we can make them more or less, and we'll have different results. They're kind of like note the number of neurons in a neural network, right? And so we say, what's the minimum size? What's the minimum size for each of those nodes? How many features do we have? We'll count all of those as well.

Speaker 1:          17:09          And then what we're gonna do is we're going to create three different random forests, or we're going to create one random forest with just one tree. So it's actually a decision tree. And then one with five trees and then one with 10 trees. And then we'll assess and then we'll assess how good each of these random forests are by measuring the number of trees, the accuracy score for each of them. Okay? That's what we're going to do. So notice that here is the big boy right here. This evaluate algorithm is that main function that we're going to use to train our model. So we're going to give evaluate algorithm, our dataset. We're going to give it the, the random forest model that we've built, the number of folds or the sub samples of the data, how big we want to treat a, B, the mid size, the sample size, the number of trees, and the number of features that we've counted.

Speaker 1:          17:58          So let's look at what this evaluates how rhythm function looks like, because that's really the big boy, right? We want to see what is going on in this main, this big function right here. So what I'll do is I will go right here. Okay. So what it's doing is it's going to say this, let's look at this. Okay, ready? Let's, let's look at this. What it's gonna do is it's going to say, okay, so for a given dataset and for a given algorithm, which is the random forest algorithm that we're going to feed it, let's say the folds are the sub samples that are used to train and validate our models. So we'll split our data into a training and a validation set, um, by the number of folds. So what do I mean by that? Well. Okay, let's look at that cross validation split method.

Speaker 1:          18:44          Where is that? It's right here. So, so we basically want to split the data into cables. The original sample is randomly partitioned into k equal size sub samples. And then of those case sub samples, a single sub sample is retained as the validation data and the rest are going to be used for training data. It's splitting the data so that k minus one sub samples are used for training. And then there's one sub sample that's left for validation. That's it. Okay. So back to this, we talked about, uh, that function. So once we've split our data into those folds, then we're going to say, okay, let's score each of them, right? Because we're evaluating our random forest algorithm. So we'll say for each of the sub samples that we have of our data, let's create a copy of the data and then remove the given subsample.

Speaker 1:          19:29          We'll initialize a test set because this algorithm really does two things. It try it, it trains our model on the training data and then it tests it out on the testing data. That is, it makes predictions on the testing data, ignoring the labels. So we'll say okay for each. So we'll add each row in a given sub sample to the test set so that we have test samples as well. And then we'll get the predicted label, right, so for, so we'll use a random forest algorithm. This, this is our random force algorithm that that's the next thing we're going to look at. But it's going to get all the predicted labels from them, right? For, from our training and our testing set. And then it'll get the actual labels, right? So, and once it has the predicted labels and it has the actual labels, then it can compare the two via this accuracy metric.

Speaker 1:          20:16          And the accuracy metric is a scalar value and it is how we assess the validity of every random forest that we've built. And so the accuracy metric to go into this is really simple for each label. If the actual label matches the predicted label, add one to the correct iterator and then we just calculate the percentage of predictions that were correct, which is the number of them divided by the, the number of correct divided by the actual ones times a hundred. Really simple. Like I said, it's all arithmetic. It's all plus minus, plus minus, multiply and divide. That's all this is. There's, there's no linear Algebra here. It's all Algebra. Um, so, but despite how simple this model it is, it is quite powerful, which is why it's awesome. Okay, so that's our evaluate algorithm function. So let's keep going down, right? We're going down the chain, go into the Moon, Moon Ride, Moon Ride, Moon ride mood.

Speaker 1:          21:14          I'm just adding my crazy aunt as well. So back to this, if you got that referenced, you're cool. If you didn't get their references from spongebob back to this, you're still cool. So back to this, so we're evaluating our algorithm here. Let's evaluate this thing. So if we're evaluating our algorithm, so what does this algorithm function even do? What, what is this? Right? Let's, let's see what this algorithm function is. What this algorithm function is, is it is our random forest. That is what it is. We say for the range of trees that we have, let's compute a subsample of those trees. And then for that sub sample we'll build a specific decision tree for that specific sub sample. And then once we built that tree, we'll add it to our list of trees and then we're going to make predictions based on all of those trees and we'll return the list.

Speaker 1:          22:02          The predictions right? Seems simple enough, right? So what does this bagging predict? Now notice how we're, we just keep going down the chain, right? We, we just keep going down the chain. So this is the list of trees responsible for making a prediction for, with each decision tree. So it combines a predictions from each decision tree and we select the most common prediction, the one that comes up the most, that that label, that the sum of that label is greater than the rest. The, some of the others, right? And there are only two labels because this is binary classification. You can also do multi-class classification, but we're not going to talk about that right now. Okay. So then we've talked about bagging predict. So what's next? Sub Sample, right? Where does this the sub sample function? So how are we sub sampling? How are we choosing how to split this data?

Speaker 1:          22:46          Right? That's the question. How are we choosing how to do that? Well, the answer is we are, we are creating a random subsample and this is where our randomness comes in, right? We are creating a random subsample in this random range for the number of samples and the data and we'll add that list of sub samples to this sample array and then return that. So the sample array or list contains all of those samples from our, from our Dataset, right? We split them. And so that's for sub sampling. And so now where were we? Oh, so we talked about sub sampling, the building tree part. Let's, let's see how the tree itself is built. So if we give it some sub sample, the depth and size of the tree, that's too hyper parameters as well as the number of features. We expect this function to build this tree, so let's look at how this, how this function works.

Speaker 1:          23:33          How is it building the tree itself? Well, inside of this function we said we noticed that it's, it's first using this method gets split, which we're going to code and which is where the meat of the code goes, but we're going to build a, and that involves creating the root node and that's going to get that split and that that's going to this, this is going to output the root node, right? That that first node and then we'll call the split function that's going to call itself recursively to build out, build out the whole tree. So once we've got that root node, then we're going to call split recursively and so it's just going to continuously build that tree recursively by calling itself. In case you haven't heard of recursion, it's when a function calls itself, it's like inception, except it's recursion. Yeah. Wow. I never actually made that reference until I just said that, right?

Speaker 1:          24:19          Inception is recursion a dream inside a dream. Whoa. Okay. Back to this. Um, right. Uh, right. So where were we? Uh, so we were at split and get split. So let's, let's write out get split, right? So get split is the first one that we want to write out. This is going to select the best split point in a Dataset, right? That, that key question. How do we know when to split our data in this decision tree of the many decisions trees that we make in our random forest? The answer is we'll have to compute it this way. This is an exhaustive and greedy algorithm. That means it is just going to go through every single iteration that it can, right? There are no heuristics here. There's no educated guesses, there are no educated guesses. It's going to go through every single data point to compute that split.

Speaker 1:          25:07          So let's look at what this looks like. Well, we've got to give it, first of all, our Dataset of course. And we want to give it the number of features, right? Because for each of those features, we're going to compute that split. So given a data set, we got to check every value on each attribute as a candidate split. So we're gonna, what we're gonna do is we're going to say let's get all of those class values right and that, that, that set of class values is going to be a list. That set of class values is going to be a list and it's going to be a list for every single data point. And so we'll save all of the rows from our dataset are our data points, right? We have that, we know that we want to calculate the index, the value, the score, and the groups can.

Speaker 1:          25:55          So we'll initialize all of these as really big numbers and they will be updated with time, but we'll initialize them as very big numbers as well. Okay, so check this out. So the Gini Index, essentially it gives us two things. It gives us an index and it gives us a value, the index of the feature of the feature of the, so it gives us the, it gives us the index of some feature whose value is the optimal value to split the data on for that feature. Do you see what I'm saying? It gives us the index of say for income, let's say that 30,000 is the best feature is going to be that decision node. From then we can split everything based on 30,000 that is where the, the classes are most split. It's going to give us the index of 30,000 in the, in the Dataset as well as the value, whatever it is, 30,000 right? So that's the pair that the Gini index gives us, the index and the value and it will also give us a score and the groups and the groups are the sub samples. And the score is the, is is how good it is, right? It's a measure of how good it is. So we all want to initialize our features here as a list and then we're going to say, okay, so while the

Speaker 2:          27:13          yeah,

Speaker 1:          27:14          number of features is less than, uh, the number of features, whereas this is going to be zero and we're going to increase it every time as we iterate through each of the features. Well, let's see this decide some random range, right? Some, some random index in our dataset to then to a pen, to our features list. So we'll say if the index is not in the features, which it won't be then a pen at first, but eventually it will, will append, it will append the index wherever we are to the list of features that we initialize is empty. And then once we've done that, we'll say, um,

Speaker 2:          28:00          yeah,

Speaker 1:          28:00          every index in the Dataset for each of those indices, let's, let's go through every single row in the datasets. So we're competing groups here, right? So we're competing groups to split our data into, so we're saying, well, we want to decide the, the test values that we want to split as well as the, um, the Gini Index. And so the Gini index is what we're computing right here. This is the point that we're computing are Gini index for the current group of data that we're in, right? So for, for we've picked a feature and we're going to iterate through every row for that feature. We're going to compute the Gini index for all of those values and we're going to pick the GDN decks. That is the, that is the, uh, largest, right? And that is what we're going for. And that once we picked the genie and Nexatus largest, that will give us the index and value to then build out that, that note of this, the decision tree. Okay. So then, um, we've computed that and then we're going to say, okay, if the Gini index is less than the optimal score, uh, then we want to say we're going to update these values to the new values, to the score, the value of the index and the groups.

Speaker 2:          29:22          Okay.

Speaker 1:          29:25          So we're going to use a dictionary to store the node in the decision tree by its name. So we'll say return, where were we? So we'll say return the index.

Speaker 2:          29:37          Okay.

Speaker 1:          29:40          As well as the value,

Speaker 2:          29:43          the value and

Speaker 1:          29:48          the groups, the groups, right? So we went to index the value in the groups, right? Cause we've computed all those, right? So this, that function gives us, gives us the root node, right? And so once we have the root node, then we can actually perform the splitting, write it down the best flooding point. And so now we want to recursively compute this splitting itself. So that's where our split function comes in. Given that root node, how do we build the tree such that it is split along the ideal aligns. Okay, so let's, let's see. Let's write out this split function. Okay, so it's going to, so basically, so this is the binary tree part. If you've, if you've created a binary tree before it is exactly the same. So given some, uh, root node, right, we're going to say some root node will compute a left and right leaf for that node. And then we'll delete the original node. So we'll say, okay, so now we can delete that. And then

Speaker 1:          30:46          once we've done that, we can check if either the left or right group of nodes is empty. So we're checking if either the left note or the right note is empty. And if so, then we create a terminal node using the, the, the records that we do have here, right? So an a terminal node. By the way, let's look at what a terminal note is. We select a class value for a group of rows and then return the most common output value in that list of rose. What is the most common health put value in that list of rose. And that is the most common class. So that's what we're doing. We're selecting the most common class. Okay? So that's the first part. And so we want to check if we've reached our maximum depth, right? So that that depth is our hyper parameter is a threshold for how large we want our tree to be. So we'll check if we've, if we've reached that point. So we'll say, um, if the depth is greater than or equal to the Max depth, then

Speaker 1:          31:49          so if we reached our Max depth and then we create a terminal node, that's what, that's what that's saying. Okay. So then, um, we've, we've got two more parts here. All right, so the next part is to say, okay, so first the two groups of data that are split by the node, we retrieved them when we store them in the left and rights variables here. And then we delete that node. Then we check if either the left or the right group of is empty. And if they are, we create a terminal node using the records that we already have right here. And so the terminal node by the way, is where we just select the class value that's the most used, right? And that is the, that is the output class. The output class, the terminal node. Is that, what is that? What is the, what is the prediction itself?

Speaker 1:          32:34          Right? So that's the end point. And so then, um, we check if, so then we check if either the left or right group of roses empty. And if so we create that terminal and road. So then we check if we've reached our maximum depth. And if so we create a terminal node. And so that's what this part is. And then, and so lastly, if the group of rows is too small, we'll create a terminal node else. We'll add the left node in a depth first fashion until the bottom of the tree has reached on. This branch will do this. So we'll do the same for the right child as well. And so the right side is in process in the same way. And then as we rise back up to construct a tree all the way back up to the route. Okay, so get split.

Speaker 1:          33:11          Notice how gets split has been called here over and over again. Uh, two more functions and then, and then we're, we're good with this. The two more functions I had where the Gini index and so the Gini index is like I said, it is, it was that formula right up here. Okay. This is the Gini index or Gini Score, whatever you want to call it. So the Gini index splits a dataset involving one input feature and one value for that feature, right? What the Gini index gives us is remember that pair that, that the value and the index of some feature, some features for some data points, right? And that's, that's the line that we then split or that's the, the boundary from which we can split database on that feature in the future. And so the way we compute that is it starts off at zero.

Speaker 1:          33:54          It's some scalar value and we're computing it for all of the data points. So for each class value that we have for all of our classes and we only have two fraudulent or not fraudulent, um, and we only have two credit worthy or not credit worthy for each of those classes, we'll select a random subset of that class, will compute the average value for that feature. Um, and then will compute p times one minus p where p is the average and that is our genie scaler. Okay. And we'll, we'll add them all up together because we have all of those cause it's the sum of all of those values and that's where the sigma notation comes in and will return that as a Gini score. Okay. And we compute that for all of, that's the subsets of our data. And so the last function to show you is the predict function and the predict function is right here, right?

Speaker 1:          34:42          So when, whenever we're actually making predictions, this is how it works, it navigates down the tree. This is asking, is this person employed or nuts with this person? Go to school. What is this person's social security number? What does this person, you know, just a bunch of random questions based on the features. Each of the features that we have so predict is recursive. So whereas the node is always changing for a given row, the node could be the left note or the right node. So whether or not the value for some data point is the less than or greater than some nodes, threshold value that we've computed using the Gini index, it will then update the node and then use that as the new parameter to then run, predict again. And eventually once it's reach the terminal node, the last node, the label, it will return the label and that and we, and then because, and that's for one decision tree.

Speaker 1:          35:27          And because we have a random forest, it's computing that for every single decision tree we sum up the values and then we use the one that is the majority vote. And that is our prediction. So then if we test our code, we'll notice that the, uh, we've got our, uh, accuracy scores here. And so the accuracy is getting, is improving every time. So we've tried it for three different random forests. We tried it, we tried it for one with one decision tree. We tried it for one with five decision trees and we tried to it for one with 10 decision trees. And every time the accuracy accuracy score improved. And so what this means is if we give it a 100, uh, 100 tree random forest or a thousand tree forest, it's going to do really, really well. Okay? So, uh, and then we'll be able to predict whether or not someone's someone is worthy of getting their credit assessed or not. And if you made it to the end of this, um, I'm very happy. So thank you. And that's all. Please subscribe for more programming videos. And for now, I've got to do something random. So thanks for watching.