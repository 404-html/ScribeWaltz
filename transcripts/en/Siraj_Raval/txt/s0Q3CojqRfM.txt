Speaker 1:          00:00          Hello world, it's Saroj and what's the deal with factors? You're going to see this word a lot in machine learning and it's one of the most crucial concepts to understand. A huge part of machine learning is finding a way to properly represent some dataset programmatically. Let's say you're a manager at Tesla and you're given a Dataset of some measurements for each car that was produced in the past week. Each car on the list has three measurements or features, its length, width and height, so a given car can then be represented as a point in three dimensional space where the values in each dimension correlates to one of the features we are measuring. This same logic applies to data points that have 300 features. We can represent them in 300 dimensional space. While this is intuitively hard for us to understand as three dimensional beings, machines can do this very well, Kevin, this data point x is considered a vector.

Speaker 1:          01:08          A vector is a one dimensional array. Think of it as a list of values or a row in a table. A vector of n elements is an end dimensional vector with one dimension for each element. So for a four dimensional data points, we can use a one by four array to hold its four feature values. And because it represents a set of features, we call it a feature vector. More general than a vector is a matrix. A matrix is a rectangular array of numbers and a vector is a row or column of a Matrix. So each row in a matrix, it could represent a different data points with each column being its respective features. Less general than a vector is a scaler, which is they single number. The most general term for all of these concepts is a tensor. A tensor is a multidimensional array.

Speaker 1:          01:58          So a first order tensor is a vector. A second order tensor is a matrix and tensions of order three and higher are called higher order tensors. So if a one D or it looks like a lined up, who are you? I think they get it, but I think they get it. You could represent a social graph that contains friends of Friends of friends as a higher order tensor. This is why Google built a library called tensorflow. It allows you to create a computational graph where cancers created from datasets can flow through a series of mathematical operations that optimize for an objective and why they built an entirely new type of chip called a Tpu or tensor processing unit. As computational power and the amount of data we have increases, we are becoming more capable of processing. Multidimensional data vectors are typically represented in a multitude of ways and they're used in many different fields of science, especially physics since factors act as a bookkeeping tool to keep track of two pieces of information, typically a magnitude and a direction for physical quantity.

Speaker 1:          03:30          For example, in Einstein's general theory of relativity, the curvature of space time which gives rise to gravity is described by what's called a reman curvature tensor, which is a tensor of order for so Badass, so we can represent not only the fabric of reality this way, but the gradient of our optimization problem as well. During first order optimization, the weights of our model, our updated incrementally. After each pass over the training data set, given an error function like a sum of squared errors, we can compute the magnitude and direction of the way. Update by taking a step in the opposite direction of the error gradient. This all comes from linear Algebra. Algebra roughly means relationships and it explores the relationships between unknown numbers. Linear Algebra roughly means line like relationships. It's the way of organizing information about vector spaces that makes manipulating groups of numbers simultaneously easy.

Speaker 1:          04:34          It defines these structures like vectors and matrices to hold these numbers and introduces new rules on how to add, multiply, subtract, and divide them. So given two arrays, the Algebraic way to multiply them would be to do it like this, and the linear Algebraic way would look like this. We compute the dot product instead of multiplying each number like this. The Linear Algebraic approach is three times faster. In this case, any type of data can be represented as a vector. Images, videos, stock indices, text, audio signals, Douggie dancing, no matter the type of data. It can be broken down in Qa to set of numbers.

Speaker 2:          05:17          The mom is not really accepting the data. It keeps throwing errors. Let me see. Oh, it looks like you've got to vectorize it. What do you mean? The model you wrote? Expected tensor over a certain size is its input. So we basically got to reshape the input data so it's in the right vector space and then once it is, we can compute things like the coastline distance between data points and the vector norm. Is there a point in library to the day you got to love num? Pi vectorization is essentially just a matrix operation and I can do it in a single line.

Speaker 1:          05:44          Awesome. Well you vectorize it dog. I've got a bad propagate out for the day. Cool. We're to tend to base. All right. See Ya. See Ya. A researcher named Macola off use the machine learning model called a neural network to create vectors for words. Word two VEC given some input corpus of texts like thousands of news articles. It would try to predict the next word in a sentence given the words around it. So a given word is encoded into a vector. The model then uses that vector to try and predict the next word. If it's prediction doesn't match the actual next word. The components of this vector are adjusted. Each words context in the corpus acts as the teacher sending air signals back to adjust the vector. The vectors of words that are judged similarly by their context are iteratively nudged closer together by adjusting the numbers in the vector.

Speaker 1:          06:42          And so after training, the model learns thousands of vectors for words. Give it a new word and it will find its associated word vector. Also called word embedding. Vector is don't just represent data. They help represent our models. Too many types of machine learning models represent their learnings as vectors. All types of neural networks do this. Given some data, it will learn dense representations of that data. These representations are essentially categories akin to if you had a data set of different colored I pictures, it will learn a general representation for all I colors. So given a new unlabeled I picture it would be able to recognize it as an I see factors good. Once data is vectorized, we can do so many things with it. A trained word to VEC model turns words into vectors. Then we can perform mathematical operations on these vectors. We can see how closely related words are by computing the distance between their vectors.

Speaker 1:          07:47          The word Sweden for example, is closely related to other wealthy northern European countries because the distance between them is small. When plotted on a graph. Word vectors that are similar tend to cluster together like types of animals associations can be built like Rome is to Italy as Beijing is to China and operations like performing hotel plus motel gives us Holiday Inn. Incredibly vectorizing words is able to capture their semantic meanings. Numerically. The way we're able to compute the distance between two vectors is by using the notion of a vector norm. A norm is any function g that maps factors to real numbers that satisfies the following conditions. The lengths are always positive. The length of zero implies zero. Scalar multiplication extends lengths in a predictable way and distances add reasonably so in a basic vector space. The norm of a vector would be it's absolute value and the distance between two numbers like this.

Speaker 1:          08:53          Usually the length of the vector is calculated using the Euclidean norm, which is defined like so, but this isn't the only way to define length. The others you'll see the terms l one norm, an l two norm used a lot in machine learning. The l two norm is the Euclidean norm. The l one norm is also called the Manhattan distance. We can use either to normalize a vector to get its unit vector and use that to compute the distance computing. The distance between vectors is useful for showing users recommendations. Both of these terms are also used in the process of regularization. We train models to fit a set of training data, but sometimes the model gets so fit to the training data that it doesn't have good prediction performance. It can't generalize well to new data points. To prevent this overfitting, we have to regularize our model.

Speaker 1:          09:46          The common method to finding the best model is by defining a loss function that describes how well the model fits the data. To sum things up, feature vectors are used to represent numeric or symbolic characteristics of data called features and a mathematical way they can be represented in multidimensional vector spaces where we can perform operations on them like computing their distance and adding them and we can do this by computing the vector norm, which describes the size of a vector. Also useful for preventing overfitting. The wizard of the week award goes to Vishnu Kumar. He implemented both gradient descent and Newton's method to create a model, able to predict the amount of calories burned for cycling a certain distance. The plots are great and the code is architected very legibly. Check it out. Amazing work, Vishnu and the runner up with the last minute entry is Hamas Schick. Loved how detailed your notebook was. This week's challenge is to implement both l one and l two regularization on a linear regression model. Check the get hub, worry me in the description for details and winners will be announced in a week. Please subscribe for more programming videos, and for now, I've got to not be normalized, so thanks for watching.