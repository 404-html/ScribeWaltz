Speaker 1:          00:00          An Ai has probably judged your resume already. Hello World Saroj and what does the perfect resume look like? We'll build an AI app to automatically rates the value of a resume using advances in natural language processing or NLP. The first generation of hiring systems required a lot of human effort hiring teams would need to publish their open positions to newspapers and the cable networks. Interested candidates would then apply to the position by sending in their resumes. These resumes were then reviewed by the hiring team and the best candidate was called in for another round of interviews. The second generation of hiring systems introduced the concept of hiring agencies. They offer a solution in which the candidates submits their resume to an agency. The agency would act as a middleman. They would screen resumes and send to candidates to suitable positions at different companies. But now we're in the age of a Ai.

Speaker 1:          01:09          We are reaching the third generation in hiring systems. Candidates can upload their resumes to an AI system that uses natural language processing to assess the capability for a specific job. A company could then use this tool to call in the best candidates for further interviews. I've got so many rejections, but look at me now. This removes the cost for companies in hiring an agency and results in a faster candidate pipeline. It also serves as a great tool for job hunters rather than trying to use your own intuition and trying to gauge whether certain parts of your resume our optimal, you can just use a tool to analyze your resume for you and tell you what areas need to be improved on for this specific job you're applying to. Red Score is one startup that does that. For candidates. They can improve their chances of landing a specific job by using their tool to optimize their resume and on the recruiting side, sovereign does the same for companies looking to hire candidates.

Speaker 1:          02:18          So how do we do this? Rule based parsers are a traditional way of screening resumes automatically. That means that a developer will line-by-line write out a set of if l statements that check for certain requirements in a resume and if enough of those requirements are met for a given preset threshold, then the resume moves on to the next stage of the interview process else. It's tossed aside, but the problem is that there's a lot of variability and ambiguity in the language used in resumes. There are different ways to write dates and different job and skills. A pure every month someone's name could be a company name or even an it skill. The only way for a resume parser to deal with all of these edge cases is to truly understand the context in which words occur and the relationships between them. No rule is 100% reliable except the one about fight club.

Speaker 1:          03:24          Machine learning solves this problem by estimating the quality of these signals based on annotated data and by combining the evidence from several signals, we can call these signals features. They are the encoded information that is relevant for a prediction task. Lots of different machine learning techniques require us to decide what these features are beforehand. In fact, two of the most important aspects of machine learning models are feature extraction and feature engineering. Those features are what supply relevant information to the ML models. We can represent the word overfitting for example, using various different feature representations and LP tasks like automatic summarization and topic extraction have generally required developers to sort their corpus of text and two different feature groups like bigrams and trigrams. If there are too few features, that model will have a hard time making a useful prediction. If there are too many, that model can be slow and overfit stay balanced with the force.

Speaker 1:          04:40          We don't really know what feature representation is best for a given task and in the off chance that someone does, it still relies on a human being in the loop. Ideally, we just want to be able to give a raw data set to a model and have it produced an insights without any human help. Deep learning though allows these features to be learned automatically for image classification. For example, handcrafted features were required for any model like detecting edges but with convolutional neural networks. Edge detection is instead learned by the first convolutional layer. The filters learned to buy the first layer of CNNS when given images are very reminiscent of Gabor filters which were used for edge and texture detection traditionally and for the next layers, the filters begin to recognize specific patterns and objects with increasing detail. All of these are learned feature extractions as the features become further refined as we go up the model hierarchy.

Speaker 1:          05:48          The change here is that architecture engineering became the new feature engineering. Rather than having data scientists figure out the for deep learning, they have to figure out the most suitable hyper parameters for the model, like the learning rate and the filter size. In a paper from last year called domain adaption for resume classification, a group of students from Finland, Sweden's jealous younger sibling, proposed a method for classifying resume data of job applications into 27 different job categories using CNNS. Even though CNNS are normally used for image related tasks, we found that they can work well for text classification as well. If we think about how we would assess a resume before anything else, we need to make sure that the resume is a right fit for the specific role we are hiring for. Once we have that we can assess the amount of experience that candidate has, the quality of the brands they've worked with in the past and the quality of their past relationships.

Speaker 1:          06:57          So let's start with this basic example of automatically weeding out resumes that aren't a fit for our specific role cover, how exactly it works. Then talk about ways of improving it. Collecting training data for our resume classification use case isn't easy. Resume data is difficult and costly to obtain because it's considered sensitive information, but data about job descriptions can be obtained much easier. The two domains constituted by resume data and job description data, however are intrinsically related. Both domains are related to the same job recommendation task, which is to match applicants to suitable job offers. The resumes of applications have semantic similarities with job descriptions, which belonged to the same job category. They both contain skills, education duties as well as personal characteristics of the desired candidate. So it seems reasonable to use transfer learning in order to implement a domain adoption in order to leverage the information contained in the huge amount of labeled job description data.

Speaker 1:          08:14          In order to classify resume data. That's exactly what they did. They collected 90,000 job descriptions, snippets using the indeed job search Api. It enables access to short jobs summaries given a keyword. The keywords they used were 27 different industrial job categories like accountants and insurance to very boring jobs for testing the model. They use an anonymized data set of about 500 resume data samples. Each sample labeled with one of the 27 categories based on the type of job that candidate is looking for. So the goal was to leverage easily available job description data to train a model for classifying resume summary snippets to better understand how the two domains deferred. They mapped out word frequencies of all words appearing at least five times in both datasets. The results showed that in resumes people were much more likely to use adjectives describing themselves as adaptable and polite.

Speaker 1:          09:20          Whereas job descriptions more often mentioned roles like director and coordinator. An interesting finding. The training data consistent of lots of words like lots and lots and lots. If we observe these words with the human eye, we're able to tell which words relate to which other words. We're able to categorize certain words. That's because our brains have built an internal representation of the intrinsic meaning behind words, so we can tell that even if two words are spelled differently, they both have the same meaning. The way machines can do what we do is he use vectors as that representation. This is a group of numbers and abstraction that is learned from learning word meaning mappings. Word two VEC is a pretrained model that did this. It learned word vectors from a huge collection of generic words. Using this model, when you give it a word, it'll output it's vector.

Speaker 1:          10:20          Turning words into vectors allows us to perform calculations on these words. I'm talking addition, subtraction, numerically defining how similar one word is to another. Math is my city and converting our words into vectors. Then feeding those vectors into art. CNN is an important preprocessing step as it's a numerical representation. Perfect for the Matrix. Operations will apply to it throughout the neural network board propagation process. So once we use word to Vec to convert our words to vectors, we'll build a convolutional network using Carol Ross to then learn the mapping between those word vectors and the job. That category label the model did pretty well and anybody can train their own on a cloud training service like Google cloud or Floyd hub pretty easily given a new resume, it's converted into a vector fed into the model and the percent likelihood that it belongs to a certain job category is output.

Speaker 1:          11:26          If that percent is over a certain threshold we predefined than it valid for our job. This alone greatly increases the efficiency of the hiring pipeline still here. Awesome. Three points to remember. Natural language processing is a subset of machine learning that lets us extract insights from text data rather than engineering features. We can let deep learning learn relevant features and focus on tuning model architectures instead. And the hiring pipeline currently requires lots of unnecessary costly effort. There's a massive, massive opportunity to use machine learning to make the process more efficient for both candidates and recruiters. It is midterm time for this course. This week's coding challenge is to write a script that classifies text using a CNN. Details are in the read me get humbling scope in the comment section, and I'll announce the top two entries next week. You made it to the end. Hit subscribe to be my friend and you'll never be lonely again. For now, I've got to attend Google io, so thanks for watching.