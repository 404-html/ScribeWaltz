Speaker 1:          00:00          Live. So let's go live right now. Start streaming. Good. The event is starting, I think on the Stream is actually going to start in a second. It says it's starting. Yes, it started great. Okay. Okay. Hello world. It's Saroj. Welcome to my live stream on Google. Dopamine. This is a new framework that came out of Google research. There was this blog post that explained how it works. And in this live stream we're going to, I'm going to talk about how it works. So I'm going to dive into its modules. It's features of, we're going to talk a little bit, not a little bit. We're going to talk about reinforcement learning in general. Uh, and then we are going to code it out. So we're going to use Google's Colab to code out a very simple agent implementation in the browser. Anybody can do this. I want, if you're watching this, I want you to do this with me.

Speaker 1:          00:51          So go to colab.research.google.com and open up a notebook and just start and start coding with me. Okay? So, and I'm also taking in feedback from the last live stream. So I don't want to be as sporadic with like looking at audience comments and just being all over the place. So here's how we're going to structure this. The first part of this tutorial, this lecture is going to be me explaining how the framework works, uh, and how reinforcement learning in general works. Then I'm going to take a short Q and a, then I'm going to code this thing out in the browser as a very simple agent so we can get a feel for how this framework works. Uh, and then after training I'm going to take another Q and. A. And at the end they'll take another Q. And. A. And I'm going to start off with just two questions to go ahead and ask two questions.

Speaker 1:          01:38          I'll answer those and then we'll get right into the lecture. Okay. So I'll definitely minimize this chat window when I'm talking because I saw that the, uh, recorded viewers didn't like that that much, you know, seeing all these comments come in. So, um, I'm very excited to talk about this by the way, because reinforcement learning has been just pure reinforcement learning has been the forgotten technique I think about AI of, of Ai because there was the resurgence in deep learning, right? So let's start off with this timeline, right? So there was the resurgence in neural networks with deep learning around like, oh six or oh seven, uh, and then, uh, there was a resurgence in deep reinforcement learning around, I think it was 2013 when a deep mind release, the deep Q learner. And so it went from like reinforcement learning in the 60s to neural networks to taking reinforcement learning, applying it to neural networks.

Speaker 1:          02:32          But no one in general has really focused on reinforcement learning, just pure reinforcement learning. Like if you look at the content out there in terms of blog posts, in terms of, um, videos, in terms of anything for reinforcement learning, it's so small compared to the amount of content available for supervised learning, right? Neural networks, et cetera. So there definitely needs to be more content around reinforcement learning. And I promise you I'm a bar. I'm going to bring that to the world. So starting with this course and then more and more coming. So about those two questions before we get started here. The first question is most important things for a software engineer to get into data science. That's a great question. Data Science is similar and different to software engineering and software engineering. You're not doing, you're not doing any data cleaning, right? Any data preprocessing which takes up the majority unfortunately have a data scientist time.

Speaker 1:          03:28          So one important thing would just be to use the Google data sets search, download some datasets, and then use something very simple like psychic learn in Google Colab to just hack together a very simple linear regression models. So that's a, that's a good starting point. Second question is, do I need a GPU to run what you were about to do today? Can I use my laptop? Only you can use your mobile phone because the GPU is actually in the browser with colab. So you don't need a Gpu. Okay. I would actually not recommend you using a mobile phone. I would recommend you to use a, um, any kind of basic laptop. All right, so, right. So I could talk to you guys all day, but I'm, I'm, I'm being more disciplined here. Okay. So this is for the greater good. So I'm going to minimize the chat window and then I'll get right back into it.

Speaker 1:          04:14          All right. So like I said, deep reinforcement learning is what's responsible for these big victories in Ai. And uh, it started with the deep queue network that deepmind release. And it was so good that Google bought deep mind for $500 million just because of that algorithm, because that one algorithm could, could succeed in 60, I think it was 60 different Atari game environments. It was, it was generalized, it could, it could learn how to master any of these games, one algorithm. And that was very exciting. And so what deep Q used to, to do, what it did was three specific techniques. It used replay memory, large scale distributed training, and a dis, a distributional modeling method. Okay. And so this, this all ties into dopamine by the way. Okay. This is not just like me going off on a tangent here. Deep Q used these three techniques, right?

Speaker 1:          05:07          So replay memory, uh, is it's, it's a really, it's a really cool idea. So we are in week one of this move 37 course. So don't expect me to like go all deep into deep, deep learning or deep reinforcement learning right now. I will, uh, but to start off with, um, I'm just going to like touch on these little concepts. So replay memory is saying, um, let's store all of those states actions and rewards in this giant array while we are learning. So there's two processes happening when it comes to replay memory. There's the, the, the, the cue learning. So it's the process of learning what the appropriate state action parish should be, uh, for whatever your environment is. And then next to that is this idea of storing all of those states actions and rewards that the agent is executing in that environment. And so interpolated these two techniques, acting and learning helps improve the policy.

Speaker 1:          06:05          So that's the basic idea. All right? So when it comes to deep reinforcement learning, all you need to know is that neural networks are an agent that can learn to map state action pairs to rewards, right? So if we are in a state and an environment, right? So let's say we are in grid world, right? We're in the bottom left corner. We want to perform an action, like move up or move to the right. Uh, we need to know, um, what would be the best action to do depending on the reward, right? What's gonna Maximize reward? So if we take the state action pair from this state, move up, you know, or from this state moved to the right, and then we take the reward plus one zero or negative one. If we can learn that mapping, then we can find the optimal queue function, which is going to give us the optimal policy.

Speaker 1:          06:51          Uh, and that's what neural networks can do. And there are a lot of ways to approximate that function. But remember, neural networks are universal function approximators and that's why they work so well when it comes to reinforcement learning to learn what that cue function should be. That's it. Don't worry if you don't understand anything I just said that was just me like taking like a leap into the future, but now we're going to come back. Right? So because those techniques required such rapid experimentation, the researchers at Google decided that they needed a framework that could encapsulates the types of iterative, fast design updates that they required, right? So if you look at reinforcement learning frameworks today, they're not very flexible. Um, they require you to construct a mark of decision process from scratch. Um, and then there's a lot of like, just fill our code that you have to write and then let's say you're writing say policy improvement and in policy evaluation and you know, that's policy iteration as a, as a dynamic programming technique.

Speaker 1:          07:52          If you want to modify that and turn that into a say, Monte Carlo technique or some other technique, it's certainly, it's going to require a lot of code refactoring. So there needs to be a way to enable rapid experimentation. So TLDR needs more flexibility, stability, and reproducibility, right? Let's be real here. Code reproducibility when it comes to our El research is abysmal, right? Just try, just try reimplementing. Um, some technique using a famous RL paper, it won't work because, uh, there's a lot of problems there, right? Not, not that I won't work, but it's very hard. So this is going to help with that. So if we look at this framework and they've got it on get hub, it's brand new. It was just uploaded 19 days ago. They've got some great documentation here, and then they've got some instructions on how to install it locally, depending on what your environment is.

Speaker 1:          08:44          Let me, let me make this bigger, make this bigger as well. And let me make sure everybody's doing well. All right, cool. All right, good. So what does it feature? So if we, if we go to this folder called dopamine, so I want you to go to this folder. These are the, these are the features that it provides. Okay. So basically Tldr, dopamine is taking open Ai's gym, uh, environment. So all those games and tensorflow and combining them, I know that's a very gross of, um, that's a, that's a, that's a gross oversimplification of what the framework is. But Tldr, highest level, you got some environments, you got some agents, let's combine that together and then let's create this RL framework. So the first part of what it offers are these agents. Okay. So all of these agents are basically variations of the deep Q learner, right?

Speaker 1:          09:41          So deep Q like I talked about, uh, was deep mines, you know, famous algorithm that beat all those Atari Games. So it's got a collection of those pre coded and those agents are commented very, very well. I've been looking through this code, check this out. I mean it, it's even got references to the paper and each function, um, the arguments are all explained. If we go into these, um, functions like build networks, well documented code. Um, so that's, that's very cool. So it's got a bunch of agents in there and which specific agents doesn't have, well, it's got the deep Q network. Right? So given some, given some, uh, pixels from some frames of a game, uh, decide what the appropriate action should be. So a neural network will learn to approximate the Q function based on those pixel value. So it's learning that mapping and then taking those pixel values and converting them through actions and whatever action space, uh, whenever environments specific action space, there is now there, there's also see 51, which is a, which is basically deep, do the same thing as deep queue, except instead of approximated x, instead of x approximating expected future reward values, it generates the whole distribution over possible outcomes.

Speaker 1:          10:56          So what this allows for is, um, more variability in what those rewards could be. It's not just a scaler, it's a distribution of possibilities, right? Like, uh, uh, a Gaussian or etc. There's also rainbow. So Rainbow, uh, there've been a bunch of extensions to the deep Q network. Um, no priority, no dueling, don't worry if you don't understand any of these terms. There's a lot of terms to Grok to understand and reinforcement learning. And I'm going to go over the most important ones, so don't worry. Okay. Just, let's just, let's just, let's just chill here. And this is this a lot to take in. I know this is a lot to take in, but, but bear with me. So, so rainbow basically combined six different techniques into one algorithm and it outperforms the rest. And then there's implicit Quanta, which is very new. This actually just came out, I think it was last month at the ICML conference. Uh, but it's similar in that it's kind of based off the idea of, of the deep Q network accepted approximates the full Quan tile function for the state action return distribution. So again, the idea of a distribution is used for the rewards over a single scalar value. And don't worry if you don't understand any of that, but anyway, it offers those agents encapsulated already coded for you and we can then build our own agents off of those. So that's that. The other thing it provides besides these pre coded agents,

Speaker 1:          12:22          our environments, so under a tare, preprocessing is not necessary for us run experiments, trained the Pi's. Uh, okay. So basically it's going to run a UN, UN, UN, whichever environment that you'd like from. So there are 60 different Atari games that you could use fright from the Atari learning environment, the Ale, the arcade learning environment, and it's using Jin. Jin is basically just, um, uh, a configuration framework for python. You're going to see gin, like kind of sprinkled in this code where, you know, if you have a lot of parameters to be able to easily replace those parameters with new values, uh, would be efficient, right? So in the case of neural networks, so we have possibly many layers with possibly many different types of activation functions. If we could just plug in, play different activation functions, that would be great for research, right? And so that's what Jen helps with. So there's that.

Speaker 1:          13:18          Um, there's, and there's common, right? So in addition to having that environment and a bunch of agents to act in their environment, it's got a common library which is going to provide us with logging and checkpointing, right? So any kind of debugging we need to do right? Experiments, statistics, agent variables, including the tensor flow graph, itself's replay, buffered data, right? So sometimes having a replay memory can be a giant memory footprint, right? So imagine just storing all the states, all the actions, all the rewards. Eventually that's going to become a giant table in memory. So, um, this helps minimize that. So that's under common. Okay. So, um, that's under common right here, checkpoint or iterations to sticks, et cetera. And then there's replay memories. So replay memory, there's actually different types of replay memory techniques, circular replay, prioritize, replaced some tree. Now, I haven't talked about any of these before and I cannot wait to start talking about them, but, uh, they're all under here as python files. And of course there's tests sing, right, for production use cases, et Cetera, et cetera. So that was it for that. Now.

Speaker 2:          14:33          Good.

Speaker 1:          14:35          Okay. So, all right, so that's it for the framework. We're going to start coding, but I want to go over some very important concepts right now. Um, when it comes to reinforcement learning. Okay. So, like I said, there are a ton of different, uh, techniques that we could use for reinforcement learning, right? Different types of replay memory, deep cue learning, writes a deep reinforcement learning in general, Implicit Quan tile, you know, et cetera. All of these techniques. But really there are, there are four concepts that are the most important. They are objectively the most important concepts to know in reinforcement learning. And here's what they are. One, the Mark Haub decision process. That's the first one to a policy. Three, the state value function and three and four, the action value function. Okay. And five, the bellman equation. So five, actually there are five really important concepts, you know, and if you know those five, everything else is based off of these five concepts.

Speaker 1:          15:43          The mark of decision process date with me, the mark of decision process. Say, okay, the policy, the state value function, the action value function and the bellman equation, there's four of them. So I was actually pretty amazing. Uh, once you start to get into this RL world, you start framing life and reality in terms of our l and you start seeing how the mark of decision process is a great mathematical framework for framing. Um, any kind of learning agent in an environment. And a, it's just, it's just beautiful. The more, the more you stare at this stuff, the more beautiful it seems. And I guess that applies to all sorts of science and math. But in particular, reinforcement learning is a very, um, this theory that the theories that have been, it's been built on are amazing. And you know, I was kind of skeptical at first because I thought, you know who, who says the mark of decision process should be the way that we're going?

Speaker 1:          16:41          Right? What if we just discard that and think of an entirely new framework? But you know what? They really put a lot of thoughts into this and there's a reason that it's stuck around for so long because it's a very simple yet elegant and extendable framework, mathematical framework, right? So in any environment, you have a set of states, right? Whatever the state could be. If you're skiing, the states could be skiing, down, skiing, up. I don't think skiing up is a thing. Skiing to the size, you to the right, you have a set of actions push forward, push back, um, look right. Whatever those actions should be. You have a transition function that says what's the probability of going to a certain state? If you take an action in this state, you have a starting state distribution, which we're not going to talk about right now. You have a discount factor which says if you're in a state, right, if you're in a state, let's just talk about grid world. I think red world is a great introductory example here in grid world. Looks like this where if you're in a grid world and you're trying to get from point a to point B, you could go up, down, left, right? Then, uh, so just that's, that's the framework here.

Speaker 1:          17:46          What should be the next state that you go to, right? If you're in one state, what would be the best next state to go to to optimize your reward? And what the discount factor does is it is, it helps to compute the expected cumulative reward, the expected cumulative discounted reward. So it could be discounted, it could not be discounted. So there's so much to explain here. So like, okay, if you're in a state, so check this out. If you're in a state, in an environment and you want to see what would be the expected set of rewards, I would get from going from say, you know, let's say I'm in this bottom corner right here and I want to eventually end up in the top right corner. So all of these are states, all of these squares are states and I'm gonna earn a reward based on which state I'm in.

Speaker 1:          18:39          So if I could look ahead into the future and say, well, based on this state that I'm in right now, if I were to go to that state, that state that say that's an eventually to the end, what would be the total reward that I would get? And so we could sum up all of those rewards, right? And then we could add them up and it becomes a scalar value. And that's the ex that's called the return. So the return is the cumulative future reward. So if we look ahead into the future and we try to sum up all of those rewards, that's going to be the cumulative future reward. Now we can apply what's called a discount factor and that's a scalar value. Usually it's 0.9 as it is, but it can be pointed whatever in terms of research papers. And what that will do is it's going to say, how much should we take into consideration the short term versus the future.

Speaker 1:          19:24          And so it's a, it's a, it's a variable that tweaks how much an agent cares about maximizing for short term rewards versus future rewards, longterm awards. And that depends on the environment that you're in. Okay. So that's the idea of a mark Haub decision process. An agent is in an environment, it takes an action in that environment based on its state and it tries to maximize for the reward value, right? So in a given state, it will take an action to go to the next state such that that that action will maximize its reward. Speaking of that, okay, for an example, racing is another example, but I think I just gave the grid world example, which is a, which is pretty nice, right? So what do we call that? Right? So once an agent knows, right, it nodes the, the, the best action to take.

Speaker 1:          20:11          Let's say we, we have trained our agent very well and it knows what the best action should be. Now it knows exactly what to do. What do we call that? We call that a policy. Okay. That is called a policy. What the policy does is it says it knows exactly, it knows exactly, um, what is the best action to take given this state, right? It's, it's a function that takes in a state and an action and returns the probability of taking that action in that state. So what it's going to do is going to take the action that gives the highest of achieving that maximum reward, and that's the action that it's going to take. So get given a state is going to say all of these actions are possible, but this particular action is going to maximize future reward and that is the optimal policy. And we have to learn what that optimal policy will be. And that optimal policy is a function. Right?

Speaker 2:          21:08          Okay.

Speaker 1:          21:09          So how do we compute that optimal policy? So we as humans, and this is, this is, this is what I'm talking about, about relating it to reality. We as humans have an optimal policy, right? So given this environment of teaching this, um, I, the state of teaching, I know to make the next, the next action to take is to continuously scroll down and continue teaching because that's going to maximize my reward in terms of the amount of people that are learning this. That's my objective and that's my policy. That's a very abstract metta. But anyway, you get what I'm saying? I hopefully. Um, so how do we, let me just make sure everybody's on board here. Everybody's good.

Speaker 1:          21:55          Okay. So how do we compute that policy now? Okay. The way we compute the policy is by using value functions and don't go anywhere because here is where I think a lot of people give up on our El because the idea of a value function can be confusing, right? There's, there's some terms that we have to essentially memorize here, but what I'm saying is this is totally doable. Okay. There are two value functions, so right? So remember an agent is in an environment and it's trying to pick the best action to take. So how is it going to do that? How is it going to learn this policy of what's the best action to take? Well, it needs a way of evaluating both the state that it's in right now and

Speaker 2:          22:39          okay,

Speaker 1:          22:39          what the value of each action it could take would be, so there are two value functions. You have the state value function. Let me maximize this.

Speaker 2:          22:52          Okay,

Speaker 1:          22:54          let me just, these equations are what matter here. So I'm just going to make that big. You have. So the, the equation right here at the top, that is the state value function in one way to think about this is by using an apostrophe. So say so by saying that the value function actually belongs to the state. So it's the state's value function. So the state has its own value function,

Speaker 1:          23:18          okay? And so what it does is it describes the value of a state when following a policy, it's the expected return when starting from state ass according to our policy, right? So what is going to do is going to evaluate the current state that I'm in by looking at all the possible actions I could be taking in any direction in this action space and saying, here's a scalar value that represents the value of being in this state. But if I was in a different state, then I have a different value for all those actions, right? So it, it, it measures the value of this particular state. But notice that in order to measure the value of this particular state, we need to find a way to measure the value of each action that we would take in that state. And that's where the action value function comes into play.

Speaker 1:          24:06          Again, think of it like an apostrophe. It belongs to the action. So it's the actions value function. So the state has its own value function and then the action has its own value function. And these two value functions combined help us approximate the policy, right? Because a policy consists of both value function functions. What's the value of the state? And to compute that, I need to know the value of each action I could be taking in this state. And that's how they both relate to each other now, okay, the last part, that was four or five, the last part, the bellman equations. Okay. So there are four of them, but we're going to use two of them. So how do we compute those value functions, correct, or rights? How do we compute those value functions? Well, we need a way of relating the current state that we're in to, uh, to other states.

Speaker 1:          25:02          And if we were able to frame it that way, if we were able to say, here is what this state equals two and here is how it relates to other states, well then we could, we could then compute what the optimal values should be. So what the Bellmont equation lets us do is it lets us express values of states as values of other states. So it relates a current state to other states. And if we can do that, if we can do that, then we can characterize fully the learning problem. And once we've characterized in terms of mathematically defined how these two value functions related to the policy in this specific environment, then we can compute them using a technique like say dynamic programming or Monte Carlo techniques or salsa or deep learning, et cetera. But what the bellman equation does is it lets us frame the problem in a way that these, um, these functions relate to each other.

Speaker 1:          26:00          So we have first of all, the optimal value function, which relates to the optimal action value function. The Q function, the optimal value function is equivalent to the action that's going to give us the highest probability, the Max probability of achieving our reward. And we can then further break that down into this last equation. And this is how it relates. Using the bellman equation, we can compute what the optimal state value function should be using that action value function. And that's the bellman equation. And once we have both of those, we've got our optimal policy. Okay, that's the highest level. Now I've got some great videos coming out on both of these concepts this weekend. Can't wait for that. All right, so now I'm going to take some Q and a before I start coding in this framework.

Speaker 1:          26:50          All right, let me see how many people are in the room here. Oh, 801 hi. Hello world. 800 people are here. I love it. I love it. Um, yes, this video will be, so let me take two questions before we start coding. Okay. Okay. Here's a great question. Sh show. It asks, what if the R L agent chooses Paul a policy with the most reward? But in that time the environment changes a bit and the chosen policy is not the best. Great question. So there are two types of policies on deterministic policy and a stochastic policy. Okay. And depending on your environment, you're going to want to pick one. A deterministic policy will be predictable. You're going to know what the outputs are going to be us. The castic policy on the other hand, is unpredictable and it's usually leveraging the usage of um, distribution, no values. And so you'd want, he's a sarcastic policy in that context. Second question, what is Pi doing there in the equations? Okay. Pi is not the, yes. Okay, check this out. This pi value does not mean pie like 3.14. That's another confusing thing. We should've picked a different, um, we should've picked a different, a symbol for the optimal policy, but that's, it's too late. The Pi symbol in the context of reinforcement learning means the policy, not 3.14. So just take that out of your head for right now. Okay.

Speaker 1:          28:18          Now dopamine versus open Ai, Jim. Um, it wraps, it literally wraps open Ai. Jim, open Ai. Jim is one of the dependencies here, right? And we can do that by saying, uh, we can look for what that is by saying, uh, import gym inside of this repository. And there it is. This repository is importing gym, right? And this trained up py import gym. There we go, right? So it's important Jim. So it's, it's, it's contained. Jim is contained here. All right, let's start coding guys. So here we go. All right, we're going to build a simple agent using Google dopamine. So the first step is to install our packet. Let me just a short Q and a here. So the first step is for us to install our packages and we have some packages to install the me make this bigger here. Um, all right. So the first package we're going to install is the of the dopamine library. So dopamine, our l, and then we're going to install the c make library, which is one of dopamine is dependencies as well as the arcade learning environment. Okay, that's going to help us. No cache directory. Pip install. There we go.

Speaker 2:          29:44          [inaudible]

Speaker 3:          29:45          so let's let this, uh, that, those are our three dependencies that we need to install. Still Real. We'll let those install. Great. They were just installed. Okay. So that's the first part. Now let's import our dependencies. Okay. So the first one, we're going to import his num Pi for Matrix math. The second one we're going to import his hoe ass took for us to help load our files. The third one is our dopamine framework, specifically the Dq an agent, which we're going to use to test as a baseline against our simple agent. We're also going to use the Atari a environment. So the run experiment is the function is the execution engine that's going to relate the agent and the environment together. Then we have our, um, we have a little helper functions specifically for Colab, which is going to let us a download data directly into our colab as well as visualize it very easily. And then we have this important flags just for like warnings that we could potentially have

Speaker 2:          30:46          [inaudible]

Speaker 3:          30:47          dopamine. Right. That's what I wanted to import.

Speaker 2:          30:52          Okay,

Speaker 3:          30:53          good. Okay. Now we're going to create our agent from scratch. Um, but there's actually, oh, you know what, I need to define, um, where to store those training logs. So we're going to store our training logs in as temporary directory. I'm going to call colab dope run. Um, and then which arcade environment should we use? Let's use asterisks. Uh, there's, we could use any of them asteroids or any of them, but let's just use this one. Okay. Make sure that works. Good. Alright, so that's create our agent from scratch now. So before we start, I'm going to, let me make sure everybody can see what I'm writing here. All good? Everybody right? All right, so let's, Oh, also before we even do this, let me also just quickly show you tensor board. So this is a premade notebook and um, what it's going to allow us to do is visualize this intense or board in the browser. Okay? So, um, this is something that I been meaning to do. So this is going to install a bunch of dependencies. It's my take a while. Uh, and then when that's done, dependencies of course, take a while.

Speaker 3:          32:14          Okay. I want to visualize asterisk.

Speaker 2:          32:18          Okay.

Speaker 3:          32:19          Hasta and so now it's training and then I'm going to show you tense or board in the browser. It's very cool. I just want to just start off with something very visual, um, before we go into other things. Okay. So let that train.

Speaker 2:          32:37          Okay.

Speaker 3:          32:38          And let me continue creating my agent

Speaker 2:          32:40          here. So the first step for us is to create a log path. And what this is going to do is tell our agent where to store. It's a where to store the log data. Okay? So I'm going to call it a basic agent inside of this game environment. And that's the log path are right.

Speaker 3:          33:03          Let me just remove this because I need this full space here. Okay. Log Path. Great. No, let's create this agent. So I'm going to call it a basic agent. Basic agent. Now, dopamine expects a certain, um, framework for defining our functions here. And I'm going to, what I'm gonna start off with is defining what those functions should be. Okay? So the first one is choose actions. Um, how to select an action. Basically the next one is what is the bundle and the checkpoint. So this is when it checkpoints during training.

Speaker 2:          33:49          Okay.

Speaker 3:          33:49          Uh, anything we should do, like whenever we check point during training, this is going to be a fired whenever. Um, we are saving something. Actually I'll just put pass here cause we don't really want to do anything. Um, there's also load from, so whenever we're loading from a checkpoint, unbundle is going to be called. So we want to do some, we might want to, um, do something if, if we are loading from a checkpoint, any kind of cleanup or maybe reset our environment or, you know, depending on what we're doing, we might want to, um, you know, there's, there's any number of things that we could be doing a, but for right now, this simple agent on been dope. We don't want to do anything. All right? So there's that. There's also began episodes. So whenever we, um, the, what's the first action that we want to take inside of this environment?

Speaker 2:          34:44          Okay.

Speaker 3:          34:44          Unused observation. All right. Pass. And, um, there's one more and that's the step function. So we can update our policy here actually. So given a reward for whatever environment we're in, and this is where it's, I'm inheriting from Open Ai's gym environment, let's, let's return what the optimal action should be. Okay. Um, and so that's really it for our, this basic step and, and, and once we have that, we can then create another function. But let me go back up here. And so this is kind of the skeleton that we need for Google dopamine, right? And so this is, you know, a way of defining a bunch of whatever our technique we'll be. Okay. So for this basic agent, let's just choose a random action to take. Very simple, and then we could extend that to you say, dynamic programming, value, iteration, policy, iteration, any kind of these, any of these very simple planning techniques where the entire mark of decision process, all of those more Covidien variables are known.

Speaker 3:          35:52          Uh, we can use something like dynamic programming, but to start off with, let us code up a very simple policy. So we're going to first initialize our tensor flow session and then we're going to define this variable called numb actions. And what numb actions is going to tell us. Is it going to say how many possible actions can we take? What is the possibility for this? What is the realm of possibility for this action space? There's also switch Prob, which tells us the probability of switching actions in the next time step, which we're going to use to define what it's going to be. And then we have our last action. And so this is going to be our first action. Actually. I know it's kind of confusing, but we're going to initialize the last action, which is going to be where we start from. And so we're going to keep continuously update it. We're going to initialize it as random and uh, and then

Speaker 3:          36:53          well we'll start one more initialized variable for to not debug. We, we might want to debug, but we won't. Okay, so those are our initial actions. Now let's define our policy. Okay, so define our policy. How do we choose an action, right? Given the state, how do we choose our action? So here's what we're going to do. Our policy is going to be so simple. It's going to say, let's pick some random number out there. Okay, let's say NP dot. Random dot. Random APP, great name by the way. Uh, let's say if that number is less than our switch probability, which is also, um, which is what we're going to define beforehand. If it's less than that, okay? If it's, if it's below our threshold for switching actions, let's set the last action we just took to a new random action to take. And, and that's, that action is going to be contained inside the possibility of actions that we could be taking. And so if we go back to return, so at the end we can return what that new action is going to be, which is stored in last action. Okay. This is our policy. We're essentially choosing a random action as long as it fits into our threshold for whatever we define switch prop to be, it's going to be a value between zero and 100 or zero and one a percentage.

Speaker 3:          38:24          That's it. That's our simple policy. That's our mapping of um, states to actions. Okay. So, um, there's that. Um, and then there's one more thing I need to define here and the step function which is going to return that. In fact, I already have that. And so once we have that, then we can talk about what the class should be, right? So now that we have let we make sure that works. Of course self dot switch prob says invalid syntax of course because

Speaker 3:          39:05          right? Okay. Self dot switch prob boom. [inaudible] wow, that worked great. Okay, now we've defined that and so now we can create our basic agents. We've defined our agents class and now we can define our, we'll create an agent. So given our tents are closed session, given the environment that we're in, let's return a basic agent object using the tensorflow session session and we're going to define the number of actions we could take by looking at the environment that we're in, looking at the action space and returning that number. And then we're going to define a switch probability of say 0.2. And that's going to help us return a basic agent. Now once we have that, we can now create an agent using that function. So run experiment is that dopamine native function to create a runner object. And so if we look inside the runner object of, hold on, this is actually super exciting to me. So like I could take forever on this if we look. So what did we just use? We use the runner. What was it? It was a tare run experiment. So this is the runner object. And so if we look inside of here, we can see, let me hold on, we can see those.

Speaker 2:          40:43          Okay.

Speaker 3:          40:45          Those functions that we defined earlier, like what were they called? They were called began episode began episodes. So right there, right? These, these functions are then called inside of the runner object and they're utilized. So that's why I'm saying that there is a skeleton here that we are following such that it's going to work for this specific framework. And um, right. And so this is, this is, this is a good thing. So we're going to create a basic agent. We're going to define the log path and now we're going to define a bunch of 'em, a bunch of variables here. So the game name is going to be the game that we defined. The um, number of iterations is going to, oh, by the way, I was running this tender board. I forgot about that. Let's start it. Okay, so what, this, what, okay, what this is doing is it's going to visualize, uh, some training in the browser.

Speaker 3:          41:43          So trained models in the browser using tensor board. Anybody can now run tensorflow board in the browser. Check this out. This is real time right here. We can add in some filters. I S. Z, I dunno. So just like that, we've got a bunch of scalars we could change the smoothing values, get more smooth, get less smooth, checkout, different runs until these are all the different runs for Rainbow Implicit Quan tile. There's agents that I talked about before and they've even got this great little link here where we can just see for any environments that we're in, I make this bigger. Let's say it's the game. Frostbites we can see how each of those agents performs in the specific environment that we're in. And it looks like right here. Rainbow outperformed the rest. So just wanted to quickly show you that by the way, they have these colabs inside of the um, framework. Go to dopamine colab and they've got several right here. Okay, so check those out. So, um, where was I? So the number of iterations are going to be, let's say 200, uh, the training steps. And I'm going to answer questions in a second. Like I said, after training, I'm going to answer some questions here. Or evaluation steps are going to be, let's say 10, and then the Max steps per episode are going to be 100.

Speaker 2:          43:07          Okay.

Speaker 3:          43:08          Um, yeah, that's not gonna work. Of course. What is the problem here? Right, I see that. But where, where's the issue? No registered environment asterisk. Really, really? There's not a game called asterisk histories. Okay, let's see what, what happened here? Um, oh, asterisk within capital a think that's how it goes down. Session is not defined. Okay. Session is not defined.

Speaker 2:          44:06          Okay?

Speaker 3:          44:10          So I defined it in here. Oh, right. So in the unit function, I've got to say, I've got a number of actions, switch probability, appoint one

Speaker 2:          44:24          session number of actions and then interesting.

Speaker 3:          44:32          Hold on. Oh Shit. Okay. All right. Let's see here. I see how everybody's doing by the way. Oh yes. I only have one ass. See this is when it's good to, okay. Oh right. Session number of actions with probability, Dah Dah, Dah, Dah. What's the issue where on what line online, 36 sessions not to find, oh, you know what it is is I think I forgot to add session to the um, oh one s that's what it was. Object takes no parameters. The object takes no parameters. Um, let's see. Session number of actions in the environment. That action space off an switch probability is too return basic agent session environment. Um, you know what it is is, um, I think it's like this function needs to go inside of the class 32, right? Line 32. Yup. Maybe it's like that. Interesting. Okay. We've got, we've got much more to do here. We've got our visualizations, we've got our plotting. I've actually, where's my coat here? This is how we do it. Okay. There we go. With all my beautifully commented code, by the way. Definitely check this out.

Speaker 2:          47:07          Okay.

Speaker 3:          47:12          Yes. Good. All right. Now let's try this thing. All right. So, uh, the basic runner that we defined earlier, we're going to run the experiment and that's going to train this thing and then when we're done, we can say we're done training.

Speaker 1:          47:33          That's simple. It's training. Okay. Let me answer some questions. Like I said, Q and a time. All right. What are the questions for today? Okay. Okay. There's some delay here between comments and where I am. You know, he's like a 15 second delay, so it'll take a while. Let's answer some questions here. Okay. Neil degrasse Tyson is this Roger Vol of astrophysics. Thank you though. Okay. Um,

Speaker 2:          48:17          okay.

Speaker 1:          48:19          What application do you use for live sessions, open broadcasting service, obs? How do you create your own environment? Great question. Um, that would be using pie game to create a game environment. Um, you can use unity. Unity is great. Unity has MLA agents, so it's just Google unity, m l agents. Great Framework for both mobile games and desktop games and web applications. And the third question is do people use LSTM is on our l problems? Yes. Long term memory is very important and they do use it for sequential learning. Specifically did Q was a convolutional network, but an Lstm in the context of reinforcement learning would probably be in a text based environment or um, yeah, one of those uh, tech space environment makes sense. Laptop battery 11%. Yes.

Speaker 2:          49:16          Uh,

Speaker 1:          49:18          one more question.

Speaker 2:          49:27          Okay.

Speaker 1:          49:30          Is it possible to do our el Que learning from scratch to learn more accurately?

Speaker 1:          49:38          So, uh, from scratch. Interesting. Interesting point there. So Mark Haub decision processes like this idea is going to follow us throughout the course. It's going to follow off throughout all of reinforcement learning and those mark Javian variables that I talked about in the beginning, right? These, the states, the actions that transition probabilities, the discount factor, the reward not, we're not always going to know what the transition probabilities are going to be. We're not always going to know what the optimal state value function with the optimal action value function. We're not going to know always what they're going to be and that means that we're going to have a partially observable mark Haub decision process. So we have partial Upserve, we have partial insight into what's happening and so we have to learn the rest. And so all of, we're starting off with art model based methods where we know what the model of the environment fully is going to be, but sometimes we don't know what that model is going to be.

Speaker 1:          50:36          So we're going to think we're going to need to use model free techniques and cue learning is one of them. And I am not going to talk about Q learning yet. I'm specifically avoiding it right now because I, like I said, those five concepts you've got to master before you go into queue learning. All right? It's like that meme, like you know, dive straight into get hub models without understanding the math that people are sharing, you know, whatever. But like you gotta understand. Let's go over those five things. Again, this is the five things you need to remember from this live stream. By the way, the policy, the state's value function, the action value function, the bellman equation, and the mark of decision process. Okay. Okay. So where were we? We train that baby. It's done training. Yup. And now we can load the baseline data. Okay. Um, that's it. That, that, that's, that's the most important part. So what we're gonna do is, um,

Speaker 1:          51:45          where are we at? Time wise? 51 minutes. Okay. Uh, this tense or board thing is pretty cool. But yeah, I, I like, I like dopamine. Dopamine is cool. I mean, clearly it's useful. Look, if the Google researchers decided that this was useful for them and they've got a, you know, all the data, they've got all the computing power, they've got some of the best researchers in the world, then I think that that's good enough proof that this is a good framework. And also just looking at the code and how it's structured though the, um, the decisions they made an architect the code. I like this framework and I was skeptical at first, I will admit. But the more I looked at it and I've been kind of deepen this for the past two days, um, the more I like it. So, um, and I, and I really liked this idea of just giving us this link right here of like really quickly just checking for any of these environments, how each of these perform. Not even that it's super necessary, but just the fact that they did that. Google has really been helping, um, the greater amateur AI research community lately with the Dataset search, uh, with, um, colab. So, uh, yeah, we're all very appreciative. Google, if you're watching in, in, in this school of Ai Community, we're very appreciative. Thank you. So, um,

Speaker 1:          52:58          yeah, uh, let's load the baseline data. You know what I'm going to do? I'm just going to do this. Here's what, here's what we're going to do. We're going to go straight into the, just so you know, we'll do this together. We'll go straight to dopamine, will go to dopamine, will go to Colab, we'll go to,

Speaker 2:          53:19          uh,

Speaker 1:          53:21          this colab lab that they have right here

Speaker 2:          53:25          and

Speaker 3:          53:31          these packages,

Speaker 2:          53:34          okay,

Speaker 3:          53:37          we can install just like this. And then we can visualize them. Okay. Uh, let me answer some questions as well. While this is running, dopamine is a happy hormone hormone. Is it a hormone? Dopamine is more of, I guess you could classify it as a neuro transmitter along with serotonin. And uh, that's another one that's for anxiety. I forgot. Uh, can you, can you use dopamine with real life or El Problems? Yes, you can. Can you summarize what we've accomplished so far? So, so far we have trained and RL agent using a random policy in the asterisk, asterix environment. Uh, it, we've trained it to learn an optimal policy by choosing actions randomly. Right? It's a very simple agent, but we don't have to create our own agent. What we could do is just a subclass and existing agent. And uh, once we subclass and existing agent, we can extend it like what rainbow did or what the implicit Quan title agent did. They just extended deep too. And that's really the value that that brings.

Speaker 2:          55:00          Yeah.

Speaker 3:          55:01          As you can tell, I'm an ra, I'm really trying to avoid coding out this last bit. I just think that it's already out there. We can just like compile it right now if these dependencies take their time. But I think in terms of a visual element, um, wow. It's already on. It's already loading the baseline data. Pretty cool. Okay. Um, right. We've got to import our utils are globals the game and then we can load the baseline data. Let me keep answering some questions here if I have that up. All right. You know what? I'm going to end this stream with a rap. Um, but yeah, visuals, visualization tools are here. I'm not going to wait forever. Waste your time with like loading the stuff. Um, they've got a bunch of colab. You've got my co lab. My lesson is available in the video description. Please subscribe if you haven't yet. I'm going to continuously be posting, uh, machine learning, deep learning, reinforcement learning, AI content on this channel. Um, tell all your friends to subscribe. We're almost at 500 k subscribers. I cannot wait. I can taste it. I can taste 500 K it's so close. And I'm going to end this with a, with a, a wrap. So just say a topic. I'm going to wrap that topic. All right.

Speaker 3:          56:20          Which will, when Arele versus supervisor, that's a great. Um,

Speaker 1:          56:24          there is no winner. They're both going to win together. They're, they're, they're both techniques are all, they're both part of a larger, whole of a learning agent in an environment.

Speaker 2:          56:35          Right.

Speaker 1:          56:38          Okay. Now,

Speaker 2:          56:40          yeah,

Speaker 1:          56:45          it's in the various, yeah. Okay.

Speaker 1:          56:55          Kind of love math. I mean, I do it every day. I look at equations, I look at everything, man. It's okay. Listen, I was looking at RL. I was looking at policies. I was looking at all these things. Men, it just got to me. I had to stop. I had to think, what are these value functions really doing? When I was looking at them, man, I was like, I was back in school and I was writing on a chalkboard. I was trying to code it out. Look at how policies and values relate to each other. Without a doubt. I was trying to see what the real meaning of it was, how it all connected to each other, how it was all a part of me, like my cousin in it was like my family member. Math is like my family. And just like you guys, man, we do it right. We do it actual liquor. Okay. That's it. See, I always end on a crazy note like that, but thank you guys so much for watching more live streams every single week. Um, I'm in it for you and yes, I've got 6% laptop battery. Uh, and for now I've got to do a bunch of school of AI work, so thanks for watching.

Speaker 2:          58:00          All right.