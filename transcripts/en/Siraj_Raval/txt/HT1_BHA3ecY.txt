Speaker 1:          00:00          Hello world. It's Saroj and evolution. It's the reason we're all here today. In today's video, we're going to use tensorflow dot js to simulate the process of evolution in our browser. What you're seeing behind me is the demo that we're going to code in today's video. What it's doing is it's using a bunch of different neural networks, and it's using evolutionary strategies to evolve all of these neural networks over time to learn how to walk from the left end of the screen to the right end of the screen. So every generation, a new breed or new generation in every generation, a new generation of neural networks are going to be bread, and then they're all going to try to walk from the left side of the screen to the right side. Now we're at generation seven. They're getting a little better. They're still kind of jumpy, and they don't really move that much.

Speaker 1:          00:46          We can take one and even we can even like move it across like all right, it's gone. But, uh, yeah, it's using tensorflow dot js to do this and and evolutionary algorithms. It's the combination of neural networks and evolutionary techniques put together that can allow us to evolve a population over time. And this, this has some real use cases and I'm going to talk about those in today's video. We're going to code this intention flow dot js at the end of the video. But I want to start off by talking about neuro evolution. Okay, so machine learning models, neural networks in particular, neural networks are function approximators. That means they're really, really, really good at approximating the correlation between some input data and some output data. Right to the input data could be images, that can be videos, it could be numbers and the output data could be labels.

Speaker 1:          01:33          They could be, you know, hot dog, not hot dog, that could be one through 10 they could be the price of a stock on a given day. They could be the notes for a musical composition, whatever. But it's the label and there is some mapping between the inputs and the output data. And neural networks are really good at learning that mapping. And the way we do this is using a technique called gradient descent. Okay. So creating dissent is all about finding those optimal values by finding what's called the, the, the minimum in some valley, right? So if we were to map all of the parameters versus the output values, you'd find some value. And we want to find the bottom of that point, right? So it looks like this, and what gradient descent does is iteratively and every time step it tries to find this minimum value.

Speaker 1:          02:18          And once it finds that minimum value, we could say, okay, we've got this minimum point. What is the, what is the coefficient values at this point? So in this three, um, and this three access model, we have a z access, a y axis and an x axis. So in the x and y axis represent the two parameters of a model. And the Z axis is the objective. It's, it's that output value. And we can try and guess what those values are, but, but by using gradient descent and we can learn what the optimal values are, and the way to do this is right gradient descent. That's how it's done so far. But in the 80s a couple of researchers thought, okay, what if we use a different optimization technique? What if we don't use gradient descent? And if we think about life on earth, you, me, animals, bacteria, we're all neural networks.

Speaker 1:          03:04          We all have these networks of neurons in our head and we're learning through trial and error. And the, the, the local learning procedure could be whatever, right? We don't know what it is, but the more macro learning procedure through our children and their children and their children is this process of evolution, right? So we are adapting to our climate, we're adapting to our, our scenery, we're adapting to our environment, the context of where we lived and we pass on these adaptions in the form of genes to our children. And that process is evolution. And that in and of itself is a learning technique. And what evolutionary algorithms neuro evolution does is it tries to model that process in simulation in Silico, inside of a machine. And so if we think about a bunch of different neural networks, we can randomly initialize all of them. So they all start out with random weight values.

Speaker 1:          03:57          And what we can do is we can pick the best one by some measure of whatever best means for our objective, and then we can take that best neural network out of say a hundred and then say, okay, this neural network is the best of all of them. Let's let's create a bunch of other neural networks that are very slight variation from this best one, and that's the new population. And then out of those there's going to be the best one and we'll take that and we'll breathe it with another and then we'll, we'll repeat that process over and over again. So we're just kind of simulating evolution that's so high level. I'm going to go into the details of how that works in a second, but what I want to say here, what my point is that my point is that gradient descent works really well.

Speaker 1:          04:34          If you look at all of the production systems, grading dissent is the optimization strategy also called backpropagation in the context of neural networks. It is the optimization strategy that is used. However, with Uber, with a bunch of new startups and Uber and some big companies, I'll talk about more and more companies, users, startups, researchers are using evolutionary strategies, neuro evolution, genetic algorithms to optimize their neural networks. And I'll talk about those in a second. Okay, so think about it. Grading dissent is us trying to find that minimum point by starting at some point and then see an Eruv Lee going closer and closer to that ideal value. If you want to look, if you want to know how backpropagation works, how grading, dissent works, search backpropagation and Saroj on Youtube, I've backpropagation and five minutes video could super useful. Okay, so, so it typically goes like this, like I said, we will generate a population of let's say a hundred random neural networks, right?

Speaker 1:          05:30          So we don't know what those initial weight values are. So in the context of neural networks, those perimeters would be wait values, right? So you know this, this is initialized with some random group of numbers of vector, of weight values. Another one is with another group of way to values. And we will then determine the arc. And all of those architectures are determined using some distribution, right? So some golf, Sian distribution, et cetera. Then we'll select, select the best neural network by some fitness function. We'll talk about that. We'll let's say three or four of them. We'll breed them together. By breeding it means, you know, it could be a dot product between those wait values. It could be, uh, finding those optimal parameters and, and then choosing those for the next one, etc. What does weight values are copying them and then create these offspring, which are new neural networks.

Speaker 1:          06:14          And then we'll repeat that process over and over and over again. So there are a couple of genetic algorithms out there. There's neat neuro evolution of augmented topologies. There's hyper neat, there's novelty search. There are quite a lot of examples. So the idea here is that we start off with some neural networks and then we evaluate all of their, how fit each of them are. So that means checking where the models are and the optimization surface, which of the models performed best. And then selection is performed based on that fitness evaluation. We'll select four or five, whatever threshold value we decide and evolutionary strategies. The offspring is reduced to a single model and weighted by the fitness evaluation. So for deep neural networks, the fitness is defined as the loss or the reward, and essentially we're moving around the optimization surface and using the offspring to get in the right direction so that the, the key difference between gradient descent here is that instead of computing the gradients, we're sending out multiple antennas and moving in the direction that looks best.

Speaker 1:          07:09          There's not a single model that we're optimizing. There are several. There's a population of models that's, that's the big difference here, right? So if you think about there's, there's six of these models here. Think of each of them has, as all of these rows, each model, one model equals one row. We'll select the best ones will combine those values using some kind of technique, maybe. Dot. Product, multiplication, division, whatever. Then will mutate them. That means we're very them in some way, and then we'll place those mutated offspring in back into the population and then try and try again. The process repeats over and over again. In this way, it's similar to a structured random search, but the end result is in the selection phase that we have a single model and then we repeat that process over and over and over and over again. So this is an example of Uber.

Speaker 1:          07:54          So this, this map I, I took from Uber, right? So there are engineering a website. So they have this great blog on neuro evolution, which I have a link to, but basically what one technique they used, one, one real life use case. We'll see, use evolutionary strategies to evolve this, uh, pathfinding algorithm to get from the best route from point a to point B. And this is the map of, of, of what they found to be that, that best route. And so, uh, so applied to neural networks specifically, we can use genetic algorithms to optimize any kind of model. It doesn't have to be a neural network and it can be, you know, some simple function. It can be some, um, it could be anything really. And a neural network really is just a nested function. And every layer is another layer of nesting that function, you know, f of x is one layer, and then m of f of x is two layers.

Speaker 1:          08:44          And then g of F of f of x is a three layer network. You see what I'm saying? Because we're taking the output of one function and using it as input to the next function and using the output of that function it and using it as input to the next function. So it's, it's, it's this, it's this nested function, right? So however many layers is another layer of nesting. So applied to a neural network. If we apply neuro evolution to that. So what is, what is that thing that we want to optimize? What we want to optimize, the number of layers, the neurons per layer, the activation function, the network optimizer, these are all different parameters that we could optimize. And so when it comes to use cases, it really just boils down to anything that could use a neural network and that could improve on the backpropagation optimization strategy.

Speaker 1:          09:30          So Uber found actually in their blog was that neuro evolution outperformed graded descent in terms of time to convergence. So they'd spend up this Atari game, this, this deep Q learner that you know took deep mind like days and days on massive sets of gps to compute into a single hour on a desktop CPU. Um, which is really cool. So just check that out. I have a link to it in the video description, but so like I said, it can be applied to anything that neural networks are currently applied to. If there is some, um, if there is some AI that is using backpropagation right now that is slow in some way where a 2%, 3%, 5%, 10% increase can be useful. That would be a perfect time to apply this technique to write. You can outperform the competition by using an evolutionary strategy, right? So this could be applied to self driving cars, better game AI, image classifiers, like clarify our generation.

Speaker 1:          10:24          Really anything that's using neural networks. So you can apply in neuro evolution too to make that optimization process faster. That's what it comes down to. A faster optimization process that converges to a better minimum, faster, right? So you can get better results and faster if you are using neat algorithms. Most of the time from what we've seen here and some of these examples, especially the Uber blog, um, but also Google, so Google uses, uses it for auto ml. They're automatic machine learning algorithm that you can try in the browser. Um, and deepmind used it recently, so did mine, did neuro evolution for pup, they called it population based training. I remember it was like population based training, population based training of neural networks. That's what they called it. Yes. And this was released less than a year ago. But um, yeah, some great blog posts on that as well.

Speaker 1:          11:26          They've got some nice little visualizations of using them on generative adversarial networks. Great use case, popular model. So yeah, there's a ton of use cases. But in today's video I want to talk about using it in the context of tensorflow. Dot. Js. It says the hottest machine learning library right now because it allows us to create an l models in the browser that anybody can access on any kind of device, mobile embedded device, TV, um, laptop, desktop. And it's, it's really utilizing the power of transfer learning, which we haven't really seen when it comes to python or c plus plus. For some reason it's just people are using, uh, pretrained models much more. And I think it's because it's a lot easier for one and two. It's using Java script, which has a much bigger user base right there. More programmers using javascript these days. So before we get to our model, I just want to say two quick things about tensorflow.

Speaker 1:          12:18          Dot. Js to remember there is the core Api, right? So the core Api is what we're going to use, but it lets us build a computation graph and we could think of a computation graph as a neural network. It's because it's just a series of operations that tensors data flows between. And these operations can be layers. They can be, you know, whatever, you know, different operations like adding, subtracting, depending on the type of architecture you want a, but it's very simple to do in Java script. Here's a simple example. We have two tensors a and B. We add them together, we print them out just like that. And that those are two nodes in the computation graph, a, B plus, and that output, that's the computation graph. Three nodes in the graph, a, B, and then the total, the output of a and B. So that's, that's what we're going to use because this is a very simple neural networks that we're going to build.

Speaker 1:          13:03          However, when it comes to more complex neural networks with much more late, many more layers and you know, many different, you know, strategies like attention, um, momentum, et Cetera, then we want to use the high level API, which is the layers API. So layers API lets us it packages together, both the variable and the operations that act on Tim and act on them into a single function. So one example would be this, so that the layers dot dense, uh, line of code, it performs a weighted sum across all inputs of each output and applies an optional activation function. So it's combining, but the variable and the operation into a single line, which doesn't seem like that big a deal. But when you're creating a big neural network, it saves a lot of time. It's basically care os in the browser. That's what it is. Think of that as an analogy.

Speaker 1:          13:49          Care Os and the browser. A single line per layer. Okay, so now let's get to our model. So in our model, what we're going to have are these creatures and each of these creatures is a three layer feed forward neural network. That's their brain that we see right here. That right there was a bunch of creatures. Right now in this generation, there's only one. But if I refresh, we're going to, we're going to get a lot more, but we're going to go, that's a lot of creatures, right? So each of them has their own neural network. So the input data that's fed into the network are four different parameters. The horizontal velocity, how fast they're moving horizontally, the vertical velocity, how fast we're moving up and down. The torque, how fast they're turning, and the height above the ground level. So the score, the, the, the way that we are, um, scoring these, these, uh, creatures is we're saying they can gain points based on the distance that it travels from the starting point.

Speaker 1:          14:43          So the further it travels in the correct direction, the more points that gain. So if it goes from left all the way to the right, it's gonna get a lot of points. And then traveling in the opposite direction is going to reduce the point. So the fitness function, the way we define how fit, um, a neural network is in this context is that how far a creature goes, the selection algorithm that says, let's see what are the best most fit neural networks here is to select based on their fitness value will, which will be a scalar, a single value. And that's going to be that that fitness value is going to act like the probability of being chosen for reproduction. The creatures that perform better have higher fitness values. And hence have a higher chance of reproducing when it comes to crossover breeding, reproduction, W, whatever you want to call it, two creatures.

Speaker 1:          15:28          These parents are selected using the selection algorithm. Their weights are interchange randomly in our case. And then finally, mutation is a single value, a parameter that's going to change, uh, the probability of introducing randomness to each of these networks. So let's get into the code here. Okay, so I want to start off by just creating a simple neural network. This will be an 40 lines of code. We're going to use tensorflow dot js. So I'm going to call this a neural network class, and let me make sure that it's using javascript as its base javascript. There we go. Okay, so it's going to be called class neural network. Okay. And in this class neural network, I want there to be a constructor. The constructor, we'll define what the input to this neural network will be. We want some, a certain number of input nodes. We want a certain number of hidden nodes or neurons, and we want a certain set of output notes.

Speaker 1:          16:19          Okay? So then inside of the constructor, we're going to define what those input nodes are, what those hidden nodes are, and that's going to make them accessible inside of the class, right? So for all three of them, I'm going to do the same thing. And so these values are going to be accessible later on in the class as I manipulate them with inside of other functions. The next step is I'm going to initialize random waits for that neural network, right? Because when our creatures start out, they're going to be randomly initialized weights, right? Because there's no learning happening yet. And so now I'm going to actually use tensorflow dot. JS and like I said before, we're going to use, we're going to, we're going to generate these wait values randomly from a distribution and and distribution. And so to do this, we can use the what's called the random normal function.

Speaker 1:          17:07          So from a normal distribution, we're going to generate a set of weights. And the way to do this is to say, well, how many input notes do we have? And then how many hidden nodes do we have? And then based on those values, we can generate a set of weights. And then once we have that, we could say, well that's our input weights and now we want to have one more set of weights, right? Output weights. Because this is a, a three layer, like I said before, neural network. And then between each of those three layers is a set of weights. And because there are three layers, there are then two sets of weights. And so because there are two sets of weights were we'll create those two sets of weights. And because I had, um, the input notes and then hidden knows for the first, I'll have the hidden nodes and I'll put notes for the next set of weights. Okay. So now we've initialized our random weight values and now that we've done that, we can create the predict function, right? So we have our neural network, it's constructed and we want to create a predict function, right? So that the user will do is they will input

Speaker 1:          18:13          some data, right? So this is going to be some input data and we'll talk about what that input data is, but it's going to take that input data and we're going to define some output variable. So now I'm going to use the tidy function. Tensorflow is a tidy function and what this is going to do is it's going to cry. I'm going to use this as a rapper and then inside of that rapper I'm going to actually create my neural network. So before what I did was I initialize those values and now I'm actually going to create the neural network itself using those values at created before. What tidy does is it executes a function and it frees up any GPU memory. So because we're about to create this network, I'm going to use the tidy function to execute this neural network and as a function and then free up any GPU memory that I have because this is going to require some GPU memory right now. So this is going to end. This is going to take an one d array as input. So for the first layer I'm going to say, well, I'm going to input a tensor using and then that tensor is going to be created using what the input data is from the user rights and the size of that is going to be the input nodes sides.

Speaker 1:          19:24          That's the first part. The next part is for us to say, well, what's the hidden layer? The hidden layer is going to be the input layer, but we're going to matrix multiply it by the the set of weight values that we had that we defined before, right? So inside of the constructor and we're going to apply the sigmoid function to that because input times wait, activate. That's going to activate that result. If you want to know why I'm using the sigmoid function, watch my video, build a neural network in four minutes. Now that we have that, we can just, we can use, we can create the output layer because this is a three layer feed forward neural network and I'm going to do the same exact thing because neural networks are giant nested functions. I'm going to take the output of the first function and use it as input to this function. And I'll say this thought, how put weights dot sigmoid. And lastly, now I have my output, which is going to be the output from the output layer. Dot data sync. We're just going to sink the result into a single scalar value that I can, I can have as my output, and now I can return that output.

Speaker 1:          20:38          So that's it for the predict function. Now I have two more functions that I want to create. So the first function is called clone. So what the clone function doing is it's going to say, let's create a Kloni. And what it's going to do is it's going to create a clone of our neural network because we'll want that at the start. We want to create a bunch of different neural networks, right? And so this function is going to do that. So we'll just call this function over and over and over, over again. And so whenever a population is initialized, we'll clap. We'll call this function. However, for however many neural networks we want there to be. And now that we defined the neural network class, we can just say create a new neural network, use, you know, whatever number of input nodes you want, whatever number of hidden nodes you want. And then whatever number of, I'll put notes you want. Once we have that. So we'll say, well now we can dispose of anything in memory that we don't want so we can clear up some memory. And then we will say, here are the input weights. Clone the ones that we already have, TF dot clone function. And here are d.

Speaker 1:          21:53          Wait.

Speaker 2:          21:57          Okay.

Speaker 1:          22:00          Okay. So now that we've done that, we have our input ways, we have our output weights, we've quoted our neural network, and now we can return it and glass Lee. Now that we have all of that, so we can finally just say dispose, which we'll dispose the input in help, what for memory, which we'll call at the end of a generation. And once we have that, we're done with our neural network class. Now we can look at the rest of the code and I'll just copy paste.dot dot. And then this is going to be output. Okay, so that's our simple class. Say that. So in the context of our genetic algorithm, we have a genetic algorithm class and this is just an example of how it works. We'll initialize the creature. The creature will be a neural network, will define what it looks like in inside of the space, like the visualization by these, these values right here.

Speaker 1:          22:50          And then we'll define some way of picking one of those, one of those, um, neural networks or creatures by defining some threshold value. And if it's over that threshold value that is our selected that as our selected network and will return that. Now we can say it, let's evolve our neural network. So a sign of fitness to each creature. We'll pick the best one and then we'll breed them by picking two parents that define a child. And you think this crossover functions to define what the child's weights are and then push that child to a new generation and repeat that process for all of those values, which are children inside of that generation, Hooray will kill the current generation iteratively. And an add new children to the generation population. So this is the genetic algorithm class as as a whole. This is the neural network class as a whole.

Speaker 1:          23:37          And now we can look at how this has combined in sketch dot j yes, so this is how it works. We define the canvas, what everything looks like. We'll initialize a generation, a person or creature, we can call it whatever, and we'll, we'll say, let's define a bunch of them. We'll add it to the world will define the world's boundaries, the mouse constraint. This is just visual visualization stuff in javascript and we'll say, well, let's restart a generation every five seconds now when we'll have a counter for that and we'll display the stats for that. Okay, and so the real, the real meat of this here, I said, just initialize this population right here and we'll just say, just keep doing that over and over and over again. So generation dot evolve. That's it for today's video. I hope you found it useful. If you want any more cool information, it's going to be in the video description. I've got the code, help her links everything for you. Uh, check it out if you want to evolve to the next level in terms of your programming expertise and your life, hit the subscribe button for now. I've got to evolve myself, so thanks for watching.