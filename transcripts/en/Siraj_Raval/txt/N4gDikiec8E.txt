Speaker 1:          00:04          You. Okay.

Speaker 2:          00:11          Deep learning,

Speaker 3:          00:13          totally. Hello world. Learn about the math needed to do deep learning. Math is in everything, not just every field of engineering and science. It's between every note in a piece of music and hidden in the textures of a painting. The painting is no different. Math helps us define rules for our neural network so that we can learn from our data. If you wanted to, you can use the pointing without ever knowing anything about math. There are a bunch of readily available API Apis for tasks like computer vision and language translation, but if you want to use a library like tensorflow to make a custom model to solve a problem, knowing what math terms mean when you see them pop up is helpful. If you want to advance the field through research, don't even trip. You definitely need to know the Max. The pointing mainly pulls from three branches of math, linear Algebra, statistics, and calculus.

Speaker 3:          01:03          If you don't know any of these topics, I'd recommend a cheat sheet of the important concepts and I've linked to one for each in the description. So let's go over the four step process of building a deep learning pipeline and talk about how math is used at each step. Once we'd got a data set that we want to use, we want to process it. We can clean the data of empty values, remove features that are not necessary, but these steps don't require math. A step that does though is called normalization. This is an optional step that can help our model reach convergence, which is that point when our prediction gives us the lowest error possible faster. Since all the values operate on the same scale, this idea comes from statistics.

Speaker 2:          01:40          You have a 17.4% chance of making it straight.

Speaker 3:          01:46          They're settled strategies to normalize data, although a popular one is called mid next scaling. If we have some given data, we can use the following equation to normalize it. We take each value in the list and subtract the minimum value from it. Then divide that result by the maximum value minus the min value. We then have a new list of data within the range of zero to one, and we do this for every feature we have, so they're all on the same scale. After normalizing our data, we have to ensure that it's in a format that our neural network will accept. This is for linear Algebra comes in. There are four terms in linear Algebra that show up consistently scalers vectors, matrices, and tensors. A scalar is just a single number. A vector is a one dimensional array of numbers. A matrix is a two dimensional array of numbers and a tensor is an end dimensional array of numbers, so a matrix scalar, a vector and specter. Wait, not specter can all be represented as a tenser want to convert our data, whatever form it's in, B that images words, videos into tensors where n is the number of features our data has and defines the dimensionality of our tensor. Let's do the three layer feed forward neural network capable of predicting a binary output. Given an input as our base example to illustrate some more math going forward.

Speaker 4:          03:00          When do we use math and deep learning? When we normalize during processing learning model parameters by searching and random weights be initializing tensors flow from input to output, then measure the era to measure the doubt. It gives us what's real and what's expected. That Cup of gate to get costs corrected.

Speaker 3:          03:19          Import our only dependency num Pi and initialize our input data and help with data as matrices. Once our data is in the right format, we want to build our deep neural networks. Deep Nets have what are called hyper parameters. These are the high level 20 knobs of the network that we define and to help decide things like how fast our model runs, how many neurons per layer, how many hidden layers. Basically, the more complex your neural network gets, the more hyper parameters you'll have. You can tune these manually using knowledge you have about the problem you're solving to guests, probable values and observe the results based on the result. You can tweak them accordingly and repeat that process iteratively, but another strategy you could use is random search. You can identify ranges for each. Then you can create a search algorithm that picks values from those ranges at random from a uniform distribution of possibilities, which means all possible values have the same probability of being chosen.

Speaker 3:          04:15          This process repeats until it finds the optimal hyper parameters. Yay. For statistics, we only have number of [inaudible] as our hyper parameters. Since we have a very simple neural network, we use probability to decide our weight values to one common method is randomly initializing samples of each weight from a normal distribution with a low deviation, meaning values are pretty close together. We'll use it to create a weight matrix with a dimension of three by four since that's the size of our input, so every note in the input layer is connected to every node in the next layer. The Wade values will be in the range of negative one to one. Since we have three layers, we'll initialize to weight matrices. The next set of weights has a dimension four by one, which is the size of our output. As data propagates forward in a neural network.

Speaker 3:          04:56          Each layer applies its own respective operation to it, transforming it in some way until it eventually. How puts a prediction. This is all linear Algebra. It's all tensor Matt. We'll initialize a for loop to train our network 60,000 iterations. Then we'll want to initialize our layers. The first layer, our input gets our input data. The next layer computes the dot product of the first layer and the first weight matrix. When we multiply two major cs together, like in the case of applying wait values to input data, we call that the dot product. Then it applies a nonlinearity to the result which we've decided is going to be a sigmoid. It takes a real value number and squashes it into a range between zero and one. So that's the operation that occurs in layer one and the same occurs in the next layer will take that value from layer one and propagated forward to layer to computing the dot product of it and the next weight matrix.

Speaker 3:          05:45          Then squashing it into output probabilities with our nonlinearity cause we only have three layers. This output value is our prediction. The way we improve this prediction, the way our network learns is by optimizing our network over time. So how do we optimize it? Enter Calculus. The first prediction, our model mates will be inaccurate. To improve it, we first need to quantify exactly how wrong our prediction is. We'll do this by measuring the error or cost. The error specifies how far off the predicted output is from the expected output. Once we have the error value, we want to minimize it because the smaller the error, the better our prediction training a neural network means minimizing the error over time. We don't want to change our input data, but we can change our weights to help us minimize this error. If we just brute forced all the possible waits to see what gave us the most accurate prediction, it would take a very long time to compute.

Speaker 3:          06:40          Instead, we want some sense of direction for how we can update our weights such that in the next round of training, our output is more accurate to get this direction. We'll want to calculate the gradient of our error with respect to our week values. We can calculate this by using what's called deep derivatives. In Calculus, when we set direct to true for our nonlinear function, it'll calculate the derivative of a sigmoid. That means the slope of a sigmoid hadn't given point, which is the prediction values we give it from l two we went to minimize our error as much as possible and we can intuitively think of this process as dropping a ball into a bowl where the smallest air value is at the bottom of the bowl. Once we dropped the ball in, we'll calculate the gradient at each of those positions and if the gradient is negative we'll move the ball to the right.

Speaker 3:          07:25          If it's positive, we'll move the ball to the left and we're using the gradient to update our weights accordingly. Each time. We'll keep repeating this process until eventually the gradient is zero, which will give us the smallest error value. This process is called gradient descent because we are descending our gradient to approach zero and using it to update our weight values. Irritably I understand everything now still understand everything, so to do this programmatically will multiply the derivative we calculated for our prediction by the error. This gives us our error weighted derivative, which we'll call l two delta. This is a matrix of values, one for each predicted output and gives us a direction later uses direction to update this layers associated week values. This process of calculating the error had a given layer and using it to help calculate the error weighted gradient so that we can update our weights in the right direction.

Speaker 3:          08:16          We'll be done recursively for every layer starting from the last back to the first, we are propagating our air backwards after we've completed our prediction by propagating forward. This is called back propagation, so we'll multiply the l two delta values by the transpose of its associated weight matrix to get the previous layers air. Then use that error to do the same operation as before to get direction values to update the associated layers, weights, so air is minimized. Lastly, we'll update the weight matrices for each associated layer by multiplying them by their respective delta. When we run our code, we can see that the air values decreased over time and our prediction eventually became very accurate. Sort of it down deep learning borrows from three branches of math, linear Algebra, statistics and calculus. A neural net performs a series of operations on an input tensor to compute a prediction and we can optimize our prediction by using gradient descent to back propagate our errors.

Speaker 3:          09:10          Recursively updating our weight values for every layer during training. The coding challenge winner from the last video is Jovie on Lynn Jovian and tried out a bunch of different models to predict sentiment from a Dataset of video game reviews wizard of the week. And the runner up is Michelle Batu. He tested out several different recurrent nets and eloquently recorded his experiments and he's read me the coding challenge for this video is to train a deep neural net to predict the magnitude of an earthquake and use a strategy to learn the optimal hyper parameters. And he tells her in the read me post who can help link in the comments and I'll announce the winner. Next video, please subscribe. If you want to see more videos like this, check out this related video. And for now, I've got to get my math turns up to a million, so thanks for watching.