Speaker 1:          00:00          Success. The Ai beat me. Hello world it Saroj and just a few days ago opened a eyes team of bots won a match against a team of some of the best dota players in the world. I'll explain how they did it in this video and if you're new here, hit the subscribe button to stay up to date on my latest AI content. The event was streamed online to over a hundred thousand people and it was massive. I was there in the thick of it amongst the crowd and could feel the energy all around me. I even got to say hi to Andre Carpathia, which was algorithmic. Yes, I'm starting that. This is a big deal. The entire AI community is very excited about the results. Open AI calls their algorithm five because it's a set of five neural networks that work together to defeat the opposing team. Open Ai Five has now played against several opposing teams.

Speaker 1:          00:59          Although it started off playing against open AI employees. It moved onto audience members, valve employees, an amateur team, a Semipro team, and now this, the reason open AI is dedicating all of their time and energy to creating bots that can beat humans at Games is because the idea that if an AI can learn how to navigate and strategize in an unstructured, messy, unpredictable environment like a game world, we can transfer all of those skills that it's learned onto complex real world problems like drug discovery, materials design, climate change, and ASM. Our videos when IBM's deep blue beat the world champion Garry Kasparov at chess in the 90s it surprised a lot of people and the algorithms it used including tree search. Many macs and Alpha Beta pruning proved to be useful in other applications like route planning for vehicles. Just last year, a similar victory occurred when Google's Alphago bought beat the best go player in the world go has a search space that's orders of magnitude larger than chess and many wrongly predicted that it wouldn't be possible for an AI to defeat a human so soon.

Speaker 1:          02:20          Now with open AI, five Dota or defense of the ancients too is the newest game to be conquered. Dota two is an online multiplayer game where two teams consisting of five players each compete to collectively destroy a large structure defended by the opposing team called the ancient while defending their own. It's a real time strategy game and it's presented on a single map. From a three dimensional perspective. The game has 115 playable characters that players can choose from. Each has its own design, strength and weaknesses. Once the game begins, players start off with an experience level of one with just one ability but are able to level up and become more powerful during the course of the game. Up to a maximum level of 25 abilities are gradually unlocked. In 2011 the creators of Dota Valve sponsored 16 teams to compete at the international, uh, Dota specific sports tournament for $1 million prize.

Speaker 1:          03:31          Since then, it's become an annual championship tournament with the prize money increasing each year. In total, Dota two tournaments have earned teams over $100 million in prize money up to now making it the highest earning east sports game of all time. Brb switching careers players dedicate real time and effort to become professionals at this game, which makes open Ai's win that much more impressive. Open AI five consists of five neural networks. Each neural network is a single layer, 10 24 units, long short term memory, recurrent network recurrent networks specialize in sequence learning. Give it a sequence of words and it'll predict the next word in that sequence. Unless the input data is too big, if there are too many words, the recurrent network will start to forget what it learned from the first words. A variance of recurrent networks. LSTM networks account for this problem popularly known as the vanishing gradient problem by adding a clever combination of operations and variables to the network such that during the gradient descent process where the gradient value is what we use to update the weights of the network accordingly during every training iteration doesn't vanish.

Speaker 1:          04:58          It's maintained throughout the network. It's locked into the weights of the LSTM network components called gates. Accordingly, and since the gates are differentiable, the cell learns what's important to remember and what's important to forget over time. During optimization, each open AI single layer LSTM network receives the current Dota two games states as its input, which is extracted from Valves Bot Api. This contains all the relevant data about where players are and what's happening on the map inside of a vector. The output of each LSTM is an action on what to do next. There are several possible action heads and the LSTM chooses the specific action path to take, then executes it in a supervised learning scenario. It's all about learning the input slash label relationships so that given new data it can label it properly. But unfortunately there's no static data set here. Sorry, Kaggle. It's a real time environment and the signal is not an immediate label, but instead a time delayed label called the reward.

Speaker 1:          06:16          This is instead of paradigm called reinforcement learning. The reward is provided by the environment itself and specified by the creator of the environment. Open Ai developed a reward that consisted of a combination of aggregated metrics, including net worth, kills and assists. These rewards can either be positive or negative depending on what the agent does in the environment. The agent receives the state of the game as input takes an action in the environment and either receives a positive or negative reward signal, which it uses to update its weights to select an action more likely to receive her award. Every training iteration. The agent in this case is an LSTM neural network, and we can frame the way it has to make decisions as what's called a Markov decision process. There's a set of possible game states of possible actions, a reward function, and a description of each actions affects in each state.

Speaker 1:          07:27          The reason it's called Markov is because it'll have the mark coff property, which states that the effects of an action taken in a given state depend only on that state and not on the prior history. Although the dynamics of the environment are outside of the agent's control, it will through the training process, develop what's called a policy. A policy is defined as the probability distribution of actions given a state. It can use this policy to decide what the next best action to take would be. The objective of any reinforcement learning agent is to maximize the expected reward when following a policy. To learn this policy, lots of algorithms exist. One popular algorithm is called policy gradients. The idea is that when an agent takes an action in an environment, it might receive a positive reward, but what if that action is actually bad in the long run?

Speaker 1:          08:31          What if we could wait and see how this action played out until someone wins the game? The PG algorithm helps learn a policy that samples actions and then the actions that happened to eventually lead to good outcomes get encouraged in the future. The actions taken that lead to bad outcomes get discouraged, seems legit. Each action has an advantage estimate. It numerically answers the question of how much better is this action than the average or expected action I take at this state. The gradients that are computed during optimization are accumulated over a number of full played episodes, then applied to the network, but it turns out there's a problem with policy gradient methods. They're very sensitive to the choice of step size during the training process. If a step is too small, then the progress is small and if it's too large, the response will be noisy.

Speaker 1:          09:32          Making it very challenging in a policy gradients setting. If the agent ends up in a poor region of action space, it can leave it in a position where it has very few useful state action pairs to learn from. For example, if an agent tries to navigate a maze to find some cheese and had accidentally learned a policy of just spinning in circles, it would have a hard time reaching any actions that had a positive reward. Since it could only learn to increase an action probability. By experiencing that action and seeing that it leads forward, it literally gets stuck and its own learned loop with no way out. How can we make an update that's guaranteed to improve our current policy or at least not make it worst God mode? Of course. Just kidding. Well, if we separate our policy into two policies, a post update policy called the new policy and a pre update policy called the old policy, we can calculate this at each state.

Speaker 1:          10:38          We take a sum over the actions of the new policy probability at that state multiplied by how much better that action is than the average old policy action at that state termed the advantage. We then weigh each state's expected new policy advantage by how likely that state is to be reached by the new policy. This gets us the expected advantage of the new policy. Over the old one and lets us confirmed that expected advantage is positive before we take a move. The approach of the trust region policy optimization algorithm or t RPO is to calculate an estimate of the advantage quantity we've just described, but doing so using the distribution of states from the old policy rather than the state distribution from the new policy. It controls the rate of policy change by placing a constraint on the average difference or KL divergence between the new and old policy after each update.

Speaker 1:          11:46          Even still trp Oh, has a very poor sample efficiency, usually taking a large number of training steps to optimize open AI introduced their own spinoff of Trp Rpo called proximal policy optimization and that's the algorithm they used for their bots. It simplifies the problem by converting trp Rpos KL divergence from a constraint into a penalty term with no need to compute the KL divergence. The Algorithm is less complex. It strikes a balance between ease of implementation, sample complexity and ease of tuning. So Open Ai, trained by LSTM networks using PPO on two 56 gps available on Google cloud, flying 180 years worth of games against itself every day from walking to skill development, to learning coordination. There are bots improved throughout the process. The next step for the team is to move on to the international and play against the best in the world. If you want to learn more about any of the topics I've talked about, check out the links in the video description. All your policy are belong to us. Hit subscribe. If Ai excites you too, and make sure to connect with me on Instagram, Facebook, and Twitter. For now, I've got to play with some data, so thanks for watching.