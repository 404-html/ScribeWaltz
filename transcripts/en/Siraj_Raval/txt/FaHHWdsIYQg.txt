Speaker 1:          00:00          And my weight's keep changing. Helps me making decisions. I got a predicted hello world, it Suraj and in this video I'll teach you how backpropagation the most popular optimization algorithms in machine learning works. Machine learning, deep learning, modern artificial intelligence is in general, centered around a topic called it mathematical optimization. Optimization is a distinct branch of applied math that's useful in a ton of different industries like manufacturing, transportation and economics. Optimization comes from the same root as optimal, which means best. When you optimize something, you are making it the best and the word best varies based on the context. If you're a basketball player, you might want to maximize your jump and minimize your fouls to be like Mike, the concept of both maximizing and minimizing or different types of optimization problems. A basic optimization problem consists of three core ideas. First, a function, let's call it f of x.

Speaker 1:          01:11          We want to maximize or minimize the output of this function. That's our goal or objective, so let's call it an objective function. Secondly, we can't control the output, but we can control the inputs. These are our variables. That could be one or many depending on the use case. Lastly, there are usually some kind of constraints on our objective. These places have limits on how big or small some variables can get. In the case of our basketball player, if he's main goal is to maximize his number of free throws, that becomes his objective function. He can choose to spend his time doing any number of things like running or jump exercises to help achieve that goal. These are variables and there are certain limits to the total amount of training time he has. For example, he can't run for more than say, 30 minutes each day due to schedule conflicts.

Speaker 1:          02:06          Aka Fortnite is too much fun. These are constraints. There are a lot of variations to this scenario. We could have, there could theoretically be no constraints, which would mean this would be an unlimited problem. It could have just one variable or many. These variables could be discreet, having only integer values, or they could be continuous taking on any value. The problem could be static or unchanging over time or dynamic, meaning continual adjustments could be made as changes occur. It could be deterministic, meaning specific inputs cause specific outputs or stochastic meaning there is randomness to the outputs. So despite all of these possible variations, the practice of framing this as an optimization problem helps us solve it. Alternatively, we could just be guessing and checking different inputs for our problem, but never fully knowing what the ideal or optimal inputs should be. Optimization is a better solution than this guess and check strategy.

Speaker 1:          03:16          So how does all of this relates to machine learning? Machine learning is the practice of building mathematical models that we'll learn from datasets to make predictions. There are so many different ways we can describe this process. One way is to say that it consists of three parts. Representation, evaluation and optimization. Let's say we have a Dataset for a list of NFL teams with the number of wins in a given season and the average points per game for each of them does. The more points a team averages in a game lead to more wins in a season. Can we predict how many games a team will win based on a number of points, a team averages in a game? We can use machine learning to find out. My dear Watson representation is the space of models we could use for some dataset. There are so many different machine learning models that we could use a three layer feed forward neural network, a support vector machine, a decision tree, but let's start with something simple.

Speaker 1:          04:25          A linear regression model. It's aligned that intersects as many data points as possible. Then using that line we can make a prediction about any team. We can by hand draw out this line ourselves, but we can't know for sure that it's intersecting as many data points as it can with our human judgment. This process of trying to judge its performance is called evaluation. In machine learning, this is usually called the error function. There is a certain possibility space of parameter values for our model and depending on what those parameters are, the error will change. If we were to graph out all of those possibilities, assuming that there are only two possible parameters, we get a three d plane of possibility of the many popular error functions. Let's try a simple one called mean squared error. We essentially calculate the difference between the models output and the actual data point.

Speaker 1:          05:26          Our graph, we'll have hills and valleys. We are evaluating for the height of this landscape and the lowest area is the most optimal because that's the point where the error is smallest. You'll then also give us the optimal parameter values for our model to achieve that error. The process of searching this space of our represented model to obtain a better valuation is called optimization. We have to traverse the landscape to find the promised land of ideal parameter values for our model optimization is the strategy of getting to where we want to go. We can use gradient descent year to optimize. We'll initialize our parameters or weights with random values. Then calculate the error using our error function. This is a single number. Now we're going to use that error value to compute the partial derivative. With respect to both of our parameter values, we can call this the gradient.

Speaker 1:          06:26          Alright, let's take a step back. Unintended. The derivative is a term from calculus, the study of change. It measures the steepness of a graph of a function at some particular point. On that graph, we can think of the derivative as a slope. That's the ratio of change in the value of the function to the change in a variable. It's represented by this little squiggly character and we want to change both of our parameter values, so will compute the partial derivative of each meaning we derive one and treat the other as a constant respectively. We do that for both of them and this gives us the gradient. The gradient is a multivariable generation of the derivative. While a derivative can be defined on functions of a single variable for functions of several variables, the gradient takes its place. We can use it to update both of our parameter values ever slow, slightly in a certain direction on this plane that will move closer to the smallest possible error and we do this over and over and over again.

Speaker 1:          07:32          Over time, the parameters converging to their optimal values. It's like letting a ball roll down into the bottom of a bowl. The gradient is a compass that helps us descend down the valley to where the error is. Smallest gradient descent can be used in many different machine learning models, but it's most popularly used in neural networks. In the context of neural networks. Grading dissent is just renamed backpropagation and your whole network is a function just like our linear regression model is a function. It's a series of matrix operations that are applied to some input data that results in an output. When we compute the error and use it to compute the partial derivative with respect to its parameters, we get the gradient data is propagated forward through the computation graph of operations. An error is computed and propagated backwards by computing gradients recursively for every set of layers in the network.

Speaker 1:          08:35          So while the models all use different mathematical operations to compute predictions, the optimization strategy of gradient descent remains the same depending on how big our Dataset is, we can choose to run grading dissent in batches and mini batches or in a stochastic way. Batch descent is when we compute the error of all the training samples at once, every iteration of training. This can take a long time to compute for big data sets of though so many batch gradient descent. Let's a split our training data into many batches that can be processed individually. Stochastic gradient descent is when we perform an update. Every single training observation, the size of our batch leads to the fastest learning process. Depending on the size of our dataset. There've been a lot of variations to gradient descent that have been used over the years. Sometimes the gradient gets stuck in a local minimum and doesn't reach the global minimum.

Speaker 1:          09:39          Momentum is a technique that uses the moving average gradient instead of the immediate gradient at each time step. To preserve the momentum has we approach smaller valleys. This can slow the convergence process, but a fixed to this is to use rms prop which will adaptively scale the learning rate. A combination of both ideas is called Adam or adaptive moment estimation. Adam is currently the most popular iteration of gradient descent and you'll see it a lot in code basis. I know that was a lot, but stay with me. Three things to remember from this video. Mathematical optimization is a way of framing a problem in order to maximize or minimize some goal or objective. Machine learning uses optimization to find the optimal parameter values for a model to make the best predictions and gradient descent. Also called backpropagation when applied to neural networks is the most popular optimization strategy.

Speaker 2:          10:43          What's my next topic? Using machine learning, you could probably predict it. Subscribe for more educational videos about Ai, and for now, I've got a guest lecturer at Ucla, so thanks for watching.