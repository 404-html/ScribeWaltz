Speaker 1:          00:00          Okay.

Speaker 2:          00:01          Okay.

Speaker 1:          00:14          Yeah. Okay. Okay.

Speaker 2:          00:47          Okay. It's working. Okay guys. Oh my God. It was open broadcaster software. I can you guys bring everyone over here. So if you still have that other license and you just paste the link to this in that, in that a chat, because I am never going to use open broadcaster software again. I'm only using Google hangout. All right? Okay. So it's fine now. No, everything is working. Everything is better. I'm live. Good to go. We got everybody in here. Great. Works much better. Hi Guys. Okay, so we're gonna start off with a five minute Q and a and then we're going to start building a recommender system. Okay? We're going to talk about several types of recommender systems in this episode and uh, we're going to do it in the, in, in high python notebook. Okay. OPS sucks. Totally. So let's start with the five minute Q and a and then we're gonna get started.

Speaker 2:          01:33          All right? So come on, give it to a of Google hangouts, right? I'm using Google. Hey, I've been using Google hangouts for since forever. I just tried to do something different because I wanted to do up the quality of my stream. But that was just never again. What projects are you programming for fun? Uh, right now I am just making content. I really wish I had the time to just do some research, um, made some publications, uh, but join a research group online. But I love making content even more so that's what I'm doing right now. If I wasn't the project that would be working on is probably trying to push the field forward. In terms of Bayesean like probabilistic programming, like trying to move past the, pointing to something that requires less data and computation. Uh, how are you going to use, are you gonna use tensorflow and not this session? Uh, what do you think of tensor flow fold? I've never heard of that, like came back. Um, okay.

Speaker 1:          02:33          Okay.

Speaker 2:          02:35          One question as soon as the one, okay. You went to Google. I don't, apart from your videos, what do you recommend to learn machine learning from? Uh, you'd asked against great courses. You Demi and also Andrew Young's black is not back like as good videos. Okay. Okay. Okay. No lag. Thank you. Okay, so we're going to answer three more questions and then we're gonna get right into it because been lagging and, and doing some dumb things. Okay. Can you suggest a math book for machine learning? I would recommend Khan Academy. I would actually not recommend a textbook because every time I read a textbook it just, I don't absorb it. You know, like I'll read it and I'll like get it for a while, but I, after like a few months, I'll forget everything. So the way to do it is not just practice makes perfect, but practice makes retention.

Speaker 2:          03:24          You have to continuously practice this stuff. And, and where do you that is a cheat sheets and Khan Academy. Uh, like short bits of information as you go in your journey. Okay. Rather than just sit there, read an entire textbook and then forget it after a few months. Okay. How can I do to get involved in machine when he would search a join a research group online? Uh, there are several, if you go to the machine learning stuff, separate it. They have a, they have a weekly reading groups. So that would be a good place to find people. What do you think of data camp? Data questions, similar. All great projects. I don't know about them. And they go, how many years have you been studying? On and off for like four years. Uh, what are your thoughts on c Plus Plus for MLN? Deep learning? Uh, Steve Muscles is great. I don't have time to deal with deadlocks and uh, like syntax and the STD library we're focused on, uh, algorithms and that's why I use python. Can you recommend research topic for CS student? Uh, focus on, uh, unsupervised learning and generative models. Can you record?

Speaker 1:          04:30          Okay.

Speaker 2:          04:30          And she and management, how they go hand in hand management, like management per company? Yeah.

Speaker 3:          04:35          Uh, uh

Speaker 2:          04:38          hmm. Well you can try to find the, if you, if you, if you model your company as a machine when he problems, you can find what tasks are most profitable for your objective and your objective could be to maximize profit for your company. So if you were to, if you were to look at the data of what people are doing in your company, you were to put that in an excel spreadsheet and then you could find those teachers that are most relevant and then improve efficiency in those areas. Okay. So one more question and then we're going to get started, which is reading books for,

Speaker 2:          05:09          do you think I'm Harrison Kinsley Centex you can channel, yes. In Texas grades. Uh, he's the currently the most viewed guy on youtube, but, uh, yeah, he's, he's a great guy. Okay. So, so we're gonna get started. Uh, but, but sorry, what do you, oh wait, Jason, will you make some time to make a video about hyper brainery Tootie? Yes. Chick that is coming up. I'm grammar tuning that is coming up for sure, because that's something I need to talk about. Practice makes permanent. Exactly. So let's get started. One thing is every time I do this live stream for an athlete to a brief out, so I'm just gonna freestyle off the dumb right now, uh, on some things. If someone just say a topic and then I'm just going to freestyle without music. I felt like last time was good, but it wasn't amazing. So I want to make this one amazing.

Speaker 3:          05:52          Uh,

Speaker 2:          05:54          how do you get control system with tensorflow that get hub link? What is a generated my machine learning human learning robots, robots. When I was in seventh grade, I made my first robot. It flowed on the floor like it was a little flow of bile. I see the desk, Brene enemies looking at me telling me, you can't make that. No, not. I said, no, I'm going to make it anyways. So I went back and went my fiddle in place, Yo, I'll put meteors and other things together. And it made a little construction men and was against the weather. It went out in the rain. It got nos for short circuits. It came back, man. It was my lurk in back. Okay. So that was it. Okay. So now we're gonna get started with this. And I didn't build a robot in 10th grade. It was more like 10th grade.

Speaker 2:          06:41          Okay. So that was the storytelling rap. And now we're going to get started. So let me start screen sharing and then we're going to do this. So guys, let me give you the link to this by the way. So I have the link and we're going to review this together and I'm going to put it in the description. So, okay, so here's the link to the code. Let me send you this link to the code and I want to give a shout out to some people because I forgot about that. Nestor Dance, a dark sedan come Po, uh, Dan. David. Okay. Alpha. Okay, so that was at 417 people here are watching. I'm so excited for recommended systems. Okay. Because there's so much cool shit we can do with this. There's just so much to talk about, about recommended since. Okay. So guys, let me add this to the in real life time. Boom. Link is there in real time. Now we're going to start screen sharing. Okay, here we go. What do I want to screen? Sure. I want to and share my screen. Okay. So now that we have that, let me minimize this Google hangout. And we are going to do this. Okay, here we go. So let's start at the very beginning of this. Let's look at the very, very beginning and I'm going to minimize my screen. So

Speaker 2:          07:56          let's get started with this. Okay. And let me see here. Let me see here. Let me see here. Let me see here. Okay, great. So let me maximize what we've got here. We're going to build a song recommend. Okay. So the data set we're going to use is, uh, so let me show myself, because I need to show myself you spray sprain, importing, not screen. Important. What was it? It was movie report. Okay.

Speaker 2:          08:29          There are, I am in the corner. All right. Okay. All right, so great. So I'm there like always. So we're going to build a song recommender and now how are we going to do that? How are we going to build a song, recommend her a recommender? This is a pro. This is a problem that Amazon has. Netflix has, Gulu has, every website had this problem. How do we personalize content for users and how do we feed it to them? Okay. So what we're going to try to, that's what we're going to think about this life. How do we personalize content for users and feed it to them? Okay. Uh, and, uh, dating recommender, we're going to do that too, uh, in the, in the next weekly video, not, not this video. So before we, even, before we even think about this, let's talk about the goal of recommender systems to identify relevant data for users.

Speaker 2:          09:16          Whether that's articles, movies, games, people, places. In fact, let me just, let's just look at what Amazon recommended for me. Okay. Let's see. What does Amazon recommends for, to Raj? What do we got here? Valentine's Day that's not personalized. Uh, when we got here. Recommend inspired by your browsing history. Okay. Okay. So if you look at this, you would think something's weird is going on in here. I swear I needed costumes for a, a video that I'm making, but what it did was it looked at what I've looked at in the past and the recommended items based on my past history. Okay. And books on machine learning. So it's looking at my past history, but it's not just my task. It would other similar users have looked at as long. We're going to talk about these different types of systems in a second. Okay. So I was looking at, I'm looking at making a video and it needed costumes for it anyway.

Speaker 2:          10:05          I have like a whole choir singing my name anyway, that's consistent. So there are three types of recommender systems okay. That we're going to talk about. But first one is content based. The second one is collaborative and the third one is popularity. Okay. So we're going to go through this step by step. Okay. But before we talk about any of these, and we're going to talk about them in depth, we're going to first load our music data. So let's look at our music data. Okay. What do we have for our music data? For Our music data, we are going to look at two two files. Okay. So the first one is, let's see what this says. This is a lot of songs. Okay. There's, there's a lot of songs here. And then the next one is the, the, the, the next database or some. So let's just, let's just look at the website for, it's like what is this?

Speaker 2:          10:52          What is this data? We always want to analyze that data is that we have first before we do anything, what is that data that we have? So this is the dataset and it's a mixture of songs from a bunch of music websites and user ratings. Okay? So it's a collection of songs and the ratings that user gates users gift gave the songs. So last the times a website thirsty. This is my jam secondhand songs. It's a bunch of different, uh, ratings by users of their favorite songs that what we're going to train our model on, okay? Now we have two different datasets. Our job is to integrate this data, okay? This is a very important part of the data processing pipeline. It is. We want to integrate this data, okay? So to integrate this data, we're going to use pandas, okay? So what we first do is we say, okay, here are your two files.

Speaker 2:          11:42          Okay? And let me make this a little bigger. These are our two files, once the CSD and one is, uh, is a triplet. Okay. These, these are, these are triplets up here with tripling his user Id, Song Id and listen count. Okay. So we want to integrate these two together. So the first thing we'll do is we will read the table, uh, using pandas and we'll store and this variable. And then we'll say, okay, let's define those three columns. He's write a song idea and listen cal. Then we're going to read the Metadata, which is that other file that we had and we're going to store in this variable song, df two. So DF means data frames. So we have two data frame,

Speaker 3:          12:21          uh,

Speaker 2:          12:22          two data frames and we're going to combine them with pandas merge function. We're going to combine both of them. And it's a, it's actually,

Speaker 3:          12:33          uh,

Speaker 2:          12:34          one, one of those. So whenever we, whenever we are integrating two data sets together, sometimes there are duplicate columns and we can, we can drop those duplicates and replaced it. Okay. And the way we do that is what we, we specify,

Speaker 3:          12:49          uh,

Speaker 2:          12:49          what that column is. That's a duplicate. So that we can then replace it. So in our case, some ID is the one duplicates across these two datasets to replace it. And that's how we merge it. And the final result, the integrated, the integrated

Speaker 3:          13:03          uh, uh,

Speaker 2:          13:05          data frame variable is called song df. And we can look at what this, what this now looks like. Okay. So once we, um, once we have that done, let me, let me give you guys the full file as well. Uh, yeah. Let me give you guys a full file. Hold on a second. So you guys have everything. So check this out. So here's the full file, which I haven't paced that. I was just

Speaker 1:          13:37          okay,

Speaker 2:          13:39          sure that each it, okay. So here's the, here's the data. Okay. So let's see what, what it is. We have an index. We have a user id, a song. It was encounter title, release artist's name, and what do we want to do? We want to, it is very similar to an inner join in SQL. Exactly. But it's using pandas. It's the same conceptual idea. It's not SQL, but it's conceptually the same. Exactly. Well, so someone asks, what teachers should we be using? And that's a great question. So

Speaker 2:          14:14          with deep learning, architecture engineering is the new feature engineering. Okay. If there's anything you guys remember from this session, remembered that architecture engineering is the new feature engineer. That means that all of that engineering complexity that we had to do, thinking about what teachers are relevant and what features aren't, doesn't matter. Because with deep learning it learns the high level features from whatever features we give it. More or less, there are some caveats. Okay. More or less, but it learns those. And so the complexity that moves to the architecture rather than hand tuning features for hand tuning models, what are the hyper parameters? What are the, you know, uh, type, what is the type of neural network one he used for this specific data set? Okay. So in our case, what we want to do is we want to predict a song or a user, the songs, sorry.

Speaker 2:          15:04          Plurals songs. That's uh, this user will like, we don't want to predict artists, we just want to predict song. So what is something we could do that could just make it easier for us to look at and just make it easier for our model in general? Well, why don't we just, why don't we do so the first part of this is why don't we do, we're going to be doing the data transformation step. We've already integrated our data into one file. The next step would be to clean our data, but we are getting at is relatively clean. I mean there's not any really, it's, it's a relatively clean data set. So let's just move right on to transformation. And the transformation we're going to do here is we're going to combine the song and the artists are columns together because we don't care about the artists, we just care about the song. So just for simplicity sake, let's just go and combine these two columns. So what's happening here is the first thing it's doing is it is, um,

Speaker 2:          15:58          it is going to put, we're going to create a subset of the data. So the first 10,000 songs, that's what we're going to focus on. The first 10,000 songs, we're going to merge the song, the song and the artist title into one color. Okay? So it's one column that we're going to focus on. Now we're going to show the most popular songs in the Dataset. So what does that look like? We're going to group them by the listen count and the percentage. So there are four lines here. Let's, let's, let's, uh, hey quantum, really appreciate it

Speaker 2:          16:29          Mikael. Really appreciate it. Okay, so we're going to show these most popular songs. Okay, so we're going to start off with this is what it looks like, but let's, let's, let's talk about these four lines of code in detail. So we're going to, so let's talk about [inaudible]. So what we're going to do is we're going to say, uh, what this does is it groups. So the first line is it groups them in order of listen, count, descending, okay, but, but listen, count. Then it gets a total sum of listen counts to calculate the percentage and that is what's on this right hand column. Then it's going to add a new column called percentage two to calculate that percentage. It, it, it, it, it summed up all those listen counts and it had to calculate the percentage and it does this by dividing by listen count times a hundred and that's going to give us this 0.4 5.32 and then finally it's going to list them in, uh, the most popular songs at the top.

Speaker 2:          17:21          Okay, so now we have this subset of data and this, now this is what we're talking about, right? We wanted, we wanted something really simple for us to look at for us to understand. Now if we really wanted to, we did it have to do this though we didn't have to, but it's simpler to look at. Okay. And model will likely be more accurate. And just because, and another thing, just because a feature engineering isn't as relevant with deep learning, that doesn't mean that we shouldn't engineer our features at all. You don't like for simplicity sake, having this is just simpler to look at and it helps. Now it's [inaudible]. [inaudible] helps. Okay. And the, and whatever you do can have a difference. Can get that last mile difference between, you know, your model being 98% and 99% accurate. If you're trying to get there, then yeah, go ahead and, and make it more simple.

Speaker 2:          18:07          Okay. So that can we did for that. When do we hit get to hear the choir? Uh, Fredrick, we'll talk about that later. Uh, okay. So, so that's that. Now let's keep on going. We haven't actually done any recommendations there. We're just, we're just doing some data. Preprocessing. What do I mean by listen, count. Listen, tell is a number of times each song was listened to in general by all of users. Okay. Because we're going to start off with a very naive approach for recommendations of very naive approach. Okay? So let's count the number of unique users in the Dataset. Okay. To do this, we'll say, okay, what is the number of unique values? Really Handy, uh, method right here. Unique. Okay. And then we say, okay, well those 365 users and does dataset just so we know how many users there are. And whenever you're looking at a Dataset, be sure to look at these things. Okay? You use I python notebooks. Look at all of your columns, analyze your data, see what tells you that there are, what count of everything there is. What is the percentage? Shouldn't it? Psalm two, one, um, nope. No, so the percent, it's not, it's not percentage. Uh, in relation to a all songs, it's percentage in relation to, uh, hold on.

Speaker 1:          19:27          Okay.

Speaker 2:          19:28          Actually hold on a second. You are right. This code needs to be recompiled this, this needs to be recompiled it. So the percentage here, it needs to be altered, so, so good. Good, good call. Uh, Tina. Good call. Yeah. Okay. So, but anyway, so let's keep going. That's the basic idea.

Speaker 2:          19:50          Okay. So we're going to count up the number of unique songs of the day is had 5,000, about 5,000. And now we're going to create a song recommender. Okay. So the first thing we'll do is we're going to split it into training and testing data and we're going to use a train test split functions from Psych Hitler. Okay. That's what's I kept learning. Gives us, whenever we do any machine learning before we train our model. And the thing we always want to split our data into training and testing data, okay? That's what we want to do. And we're going to end, we're saying arbitrarily, let's pick 20% as our testing sites and then it'll know that 80% is our training size. Okay? So there's that. Now lets, it says, okay, so sim, so the first thing it's doing here is it saying simple popularity recommended class can be used as a black box.

Speaker 2:          20:36          Well, we're not going to look at it as a black box. We're going to look at this code in a second, okay? But what it does, it says, okay, based on the popularity of each song, uh, create a recommender based on this training data and then printout per user five. Given this user id, what are the recommended songs? And it's going to print out. So what does this look like in code? So to look at this in code, let's, let's look at this class. Okay. So we're going to look at this class actually. So let me go right into this. This class. So in this recommenders file, right? So what is this? What is this metadata loop looking at? Let's look at this together. Recommenders, popularity recommender pop. Okay, let's look at what that is here. So this is the recommenders file and now we want to look at the popularity recommenders class.

Speaker 2:          21:21          So where is that? Okay, here it is. Popularity recommended me. Let me, let me increase the size of this. So let's just look at this. Create a function. Okay, so what is it doing here? And this is what it's doing. Okay, so this is a very naive approach. It's not personalized. What it does is it says, okay, based on your training data and then user id, we want to get account of the user ids for each unique song as a recommendations for. Okay? So basically what it's saying is how many times have had each song, has each song been listened to and then sort the songs based on a recommendation score. Now what does that recommendation score it? The recommendations for is given by the, the score that the user at the scoring that user gave it. And we can find that for them, the score that the user gave it.

Speaker 2:          22:24          So we're calculating the score by the number of times that a song has been listened to in general. And that is that metric that we're using for the score. That's it. Then we rank those. So we, so we, so we rank those in order. So all it's doing, all this is doing is giving you for any user, the top 10 recommended songs in general. And it doesn't focus on the user. Okay. It's not focused on you, it doesn't care about you. It doesn't care. I care about you, but it doesn't care about you. Okay. I'm sorry, but that's just like, okay. But it's saying what are those top 10 songs that are going to be recommended to you? And those top 10 songs are just in general, they're just the most popular songs. There's no, there's no personalization happening here. This is the naive approach.

Speaker 2:          23:10          So if we give it user five it's going to say, okay, you're the top 10 songs. Harmonia undo, and then dog dates. Okay. Well, what about for user eight harmonia undo and dog days. It doesn't care about the user. That's a naive approach. Okay. And this is before machine learning. So we wanted to show that. I want to show the most trivial case before we do machine learning. Okay. That was the most trivial case. Now we're going to focus on exactly Tevin. It's the top cabinet. It's the top 10 songs based on the listened cap. Okay. Oh, we're going to focus on the personalization kid. Okay. This is, now we're going to talk about some machine learning. Okay, you guys ready for this? So to talk about two machine learning. Okay. Now we're going to do a different type of recommender system. It is called an item similarity. The top songs are based on just listen. That's it. So far, hacker Heco or awkward listen cap. Okay, so this on items, similarity based recommender systems. What the hell is this? Let's talk about this. So

Speaker 2:          24:22          here is a great website. So there are two types of, so there are two types of recommender systems. Okay? There are content based and collaborative base content based. Predict what you like based on what you'd like in the past. Collaborative systems predict what you like based on what other users liked. Now, most major services like Netflix and Hulu, you say hybrid approach. So it's not just what you like, it's what you liked in the past and what other users like. And it combines those approaches in a, in a, in a way. But right now we're going to focus on collaborative, uh, Sung Sung ground. You're not too late. Right now we're going to focus on a collaborative approach and we can split the collaborative approach into two different approaches. Okay. Item item, collaborative and user item collaborative. Let's talk about each. Okay, so let's talk about, um, let's see which one do we want to talk about first we want to talk about item item. Okay, so let's talk about this. So user items. So what is user item collaborative approach look like? Well, look at this. We're creating a matrix of values. So for each user here, we list their rating for each item. So you know this, this user right here would let me share this link as well.

Speaker 2:          25:44          Okay. Check out that link. So this user, all the items that they like, okay, all the heightened that this next, you're like all of these items, this snack, these are like from this matrix, we're going to calculate the similarity. Now that's is a user items, okay? And we're going to talk about that now. This is item item, collaborative filtering, it filtering. So this is what item item looks like. So let's just look into the code because that's what we're about to do. Item item. So it's saying, okay, so Craig is items similarity recommender initialize this class and then creative for a user and then print out the recommendations. So let's look at this code. Okay, what is this code doing? So, okay, so here we are in item, uh, in items similarity recommender. Okay, so let's look at what it's doing here. There's a bunch of these helper methods. But what is, what is that main method that we're looking at? Okay, what is that main method? The main method is, okay, right here, generate the top recommendations. Okay, so here we go. So the first thing it's doing is this going to create a co occurrence matrix? Okay. That's what it's doing is create a co occurrence matrix. Let's talk about what a co occurrence matrix is. A co-occurrence Matrix. Here's an example.

Speaker 2:          27:03          Let's assume that someone, a bunch of users bought a bunch of different products. So what we would say is, let's see this, this is the one right here. So for, so for each product, what is the likelihood that I use are also brought, the bottle is not a it for each user. How many times, let me repeat it one more time. I'm down for each product. What is the number of times that a user who bought that said product but another product? Okay. So for product 1001 that user bought whatever the number of times we've brought 1001 they bought a thousand to one time, they bought 1,003, three times where we are creating a co occurrence matrix. Okay?

Speaker 2:          27:47          So uh, co-occurrence matrix in our case would be for item items. So, but for, for so songs, songs or items in our case, songs or items. Okay. So we are creating a matrix of songs. Okay. So we want to calculate the weighted average of the scores in a co occurrence matrix for all users songs. Then we're going to sort the indices based upon their value and maintain the corresponding sport so big. So we pretty a cove occurrence matrix of songs that user's life. Okay. So basically based on what songs you've liked in the past, we can see those, those top songs that you've liked. And then based on those top songs, what are the users that liked that, those songs the most? And then what are those songs that they liked the most? So it's kind of like a second order. Uh, we're, it's like a second order function.

Speaker 2:          28:43          How is it co-occurrence matrix different from a normal matrix, Rick question? Well, I mean a normal matrix is just a matrix of users and songs and a bunch of other features. A co-occurrence matrix if you're taking the same, the same value for both our, our rows and columns. So it would be songs and songs. Okay. So based on this song, how many sometime did you like what other set of songs? Okay, so we're creating a co occurrence matrix and based on what you liked in the past, other users, uh, what you've gotten past, what are the most likely, the other songs you'll like based on what other similar users have lacked. Okay, so that is what we did for the personalize song. Okay. And they tend to be sparse matrices. Kyle, great insight. These coke, these co-occurrence matrices tend to be sparse because not all because? Because why is that? Because the base, the space of possible possible songs, it's so bad that you can't just say that whoever likes this song is going to like every other song, right? There are millions of songs out there. So whenever we're dealing with recommender systems, we have, we tend to have lot of sparsity more so than I've seen in a lot of other applications. So this is one of those fields where dealing with sparsity is very important.

Speaker 2:          30:13          Here's what the result of doing that gave us. He's with the resolve gave us, they gave us this list of scores. Now we can rank things. Okay. And we rate these up to 10 and that's going to be our recommendations. So that is one.

Speaker 1:          30:27          Okay.

Speaker 2:          30:28          Is it feasible to have a matrix with such huge dimensions? Yes, and we routinely do in data science. We routinely have huge ass matrices. Okay. We routinely have huge asked me to see that we load into memory. I'm sure there are better ways, just like there are, there must be better ways or um, sampling data then uniformly random. But we just, you know, we're moving fast and we need people to be focusing on these things. But right now, yes, we just load the entire fucking matrix into writing. Okay. So that is that. Now we've got a personalized model and this is just repeating the same thing for different user. Okay. Let's keep on going here. Let's keep on going. Okay.

Speaker 2:          31:09          And we could use the same matrix for socks. So based on a song, what is it similar saga? Like what, why? Because, well, the first one you say, you know, second order it looked at, not just as something to use the co-occurrence major to see based on the songs with you, what users like. Now this is just looks, looks at a raw song makers and gives us the course for that. Now this is not using deep learning. I want to say that right up front, right up front. This is not using deep learning. It's just using a linear Algebra and matrices. Okay. That all using and we can get good results from this. Okay.

Speaker 2:          31:49          Okay. So some people are saying that they prefer it when I Code and uh, I got feedback last time that they prefer it when I don't code. So we're going to see, we're going to see this is the first live stream I've ever done or I'm not doing any code, I'm just having it there. So I'm going to see feedback based on this and an overall, I'll decide how to move forward. Like I already decided this live stream not to ever use obs ever again. So that was one thing. And then based on the feedback in general from here, I'm going to decide if I'm going to a code next time or just look at it like this. Okay. So we'll see. So, so make sure to give me your feedback. Brutal honesty back. You know, you know me guys, you know how I love it. Okay. I'm here waiting for you to code. Okay. Okay. We are going to, we're going to get to deep learning. Okay.

Speaker 2:          32:37          Clone yourself and do both. I wish that was coding is better. Okay, I'm going to put the notebook. All right. Coding is you. Yeah. Yeah. Okay. Okay. So, okay, so I'm going to coat omics, omics, coding. All right. It's getting difficult to Paul. Okay. Okay. So, okay, so everybody wants to tell it. Okay. Okay. Clearly. Okay. Okay guys. Okay. Okay. Okay. You guys miss me? I promise that next live stream I will code. Okay. But it's just that this one I set up so that I don't code because that was the feedback that I got. Overwhelming feedback. Okay. So, but now I'm promise I'm going to code from now on moving forward, but let's just keep going with this. Okay. Um,

Speaker 2:          33:22          God damn. People are just going back and forth. Yeah. Okay. We'll have a call. We'll have a call. That's a good idea. He attached. Great idea. We'll have a port. Let's keep going. So, okay, what are we doing here? Okay, so we've, we've done our uh, personalized item based collaborative filtering. Okay. Now we're going to do with called calculate the, we want to measure the performance of our two model. What was our first model? The first model was we want we, the first model was a not based on you at all. I remember it. It was just based on the popularity of a song. The next model was based on you using a co-occurrence Matrix, right? It was a collaborative filtering model. Now how are we going to measure the performance of these two models? Okay. The poll will be 50, 50 grade. How are we going to measure the performance of these two models? We're going to use something called precision recall. Now, what does precision recall look like? Let's look at this. Precision recall is a good way of of measuring the value of our recommender system. Now I have gotten a great link for this. Then I'm going to throw up on the screen here. Let me, let me find this link. It's an awesome link. Okay, here it is. And let me, let me paste it for you guys too.

Speaker 2:          34:40          Okay, so check out this link. I'm going to throw it up. Okay. So what is precision recall? Let me, let me blow up this. Uh, this image. This is what it is. So there are two metrics here. Precision and recall. Okay, so precision and recall, precision is the based on some, so let me, let me hold on a second. Precision is the proportion of top results that are relevant considering some definition of relevance to our problem domain. What is that definition of relevant to our problem domain? It could be the number of times a song has been listened to. It could be a, the number of users that

Speaker 1:          35:23          mmm.

Speaker 2:          35:25          Have all liked the song. Some value of relevancy. Okay. That we're going to define and recall is that would measure the proportion of all relevant Saul results included in the top results. Okay, so they're measuring the relevancy of songs in relation to the precision in relation to the top 10 results and then recall is how good are they in relation to all of the songs. So there are two different measures. Okay. There are two different measures. Okay. So I promise. Next live stream, I'm going to code every single bit. Okay. I promise because you know I every live stream, I have coded everything. Okay? So I will continue to do that. That's where I feel most at home. I'm just going to do it this way this one time. All right, so we're going to use precision and recall to calculate this. Now we've got a class that does this. Okay? Then when we get to the graph, we're going to plot the graph. This is what it looks like. This is what it looks like. So precision is the y. Access and recall is the x access. And what it looks like is, let me, let me make this bigger,

Speaker 2:          36:38          but it looks like is it looks pretty much like the item similarity model has higher values for recall and precision up up to a certain point. So basically this tells us that more items, similarity model is better than our popularity mom. It's more accurate now I'm it made a great point. Uh, f one score is also a good measure. Okay, there are there, there's several. Precision is one. Recall is another f one score. There's a lot of ways we could measure how good a um, oh good are, sorry. How good our model is in relation to another mall. Okay, so that was it for our item base. Now we have one more type of recommender system that we're going to look at. Now this is a matrix factorization based recommended recommender system. Okay, so what, so what am I talking? Awesome. Okay. So wow. Okay. DMV calling me. Okay, so now what we're going to do is we're going to create this recommender system. Okay? And we're going to talk about Janzen Gans in a second. But, okay, so let's, my phone is in my year right now. Can you just start to say, okay, so let's,

Speaker 1:          38:17          okay,

Speaker 2:          38:18          focus on our recommender system. Okay. And

Speaker 1:          38:31          Yeah, finally, all these things out of my year. So what would you use? We're going to compute SBD to calculate a recommenders recommendations. Let's talk about what this is doing. Okay. So,

Speaker 2:          38:47          uh, so the, so the singular value decomposition. Okay. So what is this? It is a matrix a with, so let me, let me throw up a, so the SPD is a way that we can perform matrix factorization, which is a technique that is used to build recommender systems. Okay. So SPD is basically, it's a matrix and he said factorized matrix of the original similarity makers. So we'll create some similarity makers and then we'll perform this process right here in this method called singular value decomposition. Now what this does is it ultimately outputs three, okay, what are these three bounds? So you s MBT, you represents user of doctors. Okay. Uh, s represents the, uh, item vectors and, and then VT is points in a two dimensional space. So,

Speaker 1:          39:47          okay.

Speaker 2:          39:47          So what we're doing is we're going to use STD to compute rating. So it's going to, in vector space, it's going to create Jews are vectors. It's gonna create item vectors and instead create a joint embedding vector for both of them in a two dimensional Spitz. And we're going to use these vectors to measure the distance from one users. Uh,

Speaker 1:          40:06          okay.

Speaker 2:          40:06          One user's preferences and another goose was properties. And whoever has the smallest distance between users, we're going to use their songs as recommenders. Okay. That's, that's, that's, that's kind of an explanation that we're talking about. So it's okay. It's a, it's a little like PCA. Yeah. It's a little like principal component analysis. Exactly. We are vectorizing matress seats or vectorizing matrices and the recruits, we're computing the distance between major cs. Uh, okay. So it's, it's not clustering, it's,

Speaker 1:          40:44          mmm. Okay.

Speaker 2:          40:50          Hold on. Okay. So, so let's see what we have here. Do I have any links for STDs? So, so, so this, this, this whole, this whole method here of computing the SPD and then using it to estimate ratings is how Matrix factorization is used to recommend, uh, products for users. So we can look at, I mean, let me show you guys this check, check, check this out for a second. So this,

Speaker 1:          41:23          okay,

Speaker 2:          41:23          is the input that we're going to give it? It's a, it's a user item Matrix, okay. And this is what we perform it singular value decomposition off. So the are going to be songs and the users are the users. Okay. Once we perform SPD on this, it's going to give us a set of directors and what we have those vectors, we're going to measure the distance between vectors to give us recommend recommendations. That's the most simple way of putting it where it's going to measure the distance between vectors to give us recommendations. Okay. And and down here, it's got a little bit of the intuition behind it, but um, so when we plot those and that's where this code ends. But I want to talk about deep learning right now for a second, okay. Because we haven't done deep one. And yet this is the, this is just, um, this is without the pointing. So let's, let's talk about deeper learning right now. Okay. So let me pull this up. Let me pull this up. So what is a good, so, so this code doesn't have the pointing because I think a good one, it's like Hulu has a great, so I think who has a VR yet? Hulu has state of the art right now in recommender systems. So what is doing here? So, so right now I think Hulu has state of the art had they got state of the art and recommended system. So let's look at what they're doing for deep learning. There are a lot of ways to apply deep on it. This problem set, right? So if we,

Speaker 2:          42:58          so here's one way, okay, so their method is called CF nay. Okay. That's what they call their method. They definitely need some marketing to help with that, that name. But, uh, so what they did was they said, okay, let's take an example. Now, here's an example that they're using and let's say user rate four movies, transformers, spongebob, a Ninja Turtles, and interstellar with the spores for two, three and five on a five star scale, they're going to create a joint probability of that vector factorized as a product of conditionals by chain rule. Now what the heck did I just say? So this is what it looks like. It's going to calculate the probability that the user gets transformers at four star rating conditioned on nothing.

Speaker 2:          43:42          There's gonna be the probability that a user gets spongebob, a rink rat tooth already conditioned on what had just happened, and then the probability of teenage mutant Ninja turtles condition or that condition on that condition on that. So you see what I'm saying? This is a chain rule. It's a chain of probabilities based on what previous previously has occurred. So it's a chain of probabilities and each conditional is modeled by its own separate neural network. And the parameters for all of these neural networks are shared amongst all models. So it's got several neural networks for all of these conditionals. Can you imagine how many neural networks,

Speaker 2:          44:22          can you imagine how many neural networks are using for this? Okay, so they've got a lot of neural networks and they're going to, uh, minimize the negative log likelihood, the probability of the Becker among, I'll use it. Okay. So that's one that is like the state of the art way of doing it. Okay. So we did blackbox, all of this, right? Obviously it's a 10 lines of code, but Brittany, we're just looking at it theoretically in detail. Like what is, what is the cutting edge right now in the field? Okay. So let me, let me show you guys just think as well. So leading edge right here. Okay. So, so, uh, let's go back to where we were and yeah, so I think we went through all of it actually. Yeah. Okay. So cool. We went through all that, that code and um, yeah, we're going to talk more about recommender systems. This was nearly not enough to talk about it. We're going to do a lot more, this was a good high level over read. This was good, but we're going to do a better high level overview or sorry, an additional eye level overview,

Speaker 3:          45:34          uh,

Speaker 2:          45:37          later on. Okay. Stop screen sharing. Okay. Back to me. Oh, you guys. Okay. So that was it for the code and we're going to end with another five minute Q and. A. Okay. So what else is on guys? We're going to,

Speaker 1:          45:55          okay.

Speaker 2:          45:57          I will link to code for deep learning based recommender. Right now we want to learn the concepts. Okay. Deep One. There's a lot of, there's a lot of conceptual, um, things we have to learn about recommenders before we get to deep learning because I think that deep learning for recommender systems is the most complex,

Speaker 3:          46:18          uh,

Speaker 2:          46:19          task right now in, in deep learning to me there it's actually more complex than generative adversarial networks. Uh, so we're going to talk about it more. This was clearly not enough. I'm not satisfied, but I'm satisfied. I'm satisfied with this license, but not satisfied. They're talking about it enough. Okay. So wrap about music. Recommenders. I rapped already. I, okay. Um, okay. Uh, bragged about me. Is Greg a minute. Okay. So that was a fourth person. So, uh, here we go.

Speaker 2:          46:50          Yo rap about music recommenders. I see this girl up there and she's like a sender. Do you try and give me some mail? It's like get out of here, but I'm not going back. I got a treatment. They are not really, I don't drink beer. I'm more a coffee drinker, man. This is crazy. You could call me Sean Speaker. I go out everyday on the streets telling people, Hey, I like this song. They like, man, you need a drink of people battle Snapple, all these drinks. But I'm bored man. I got songs, man. I'm done with that. So,

Speaker 3:          47:16          okay.

Speaker 2:          47:17          Is it possible to write a collaborative filtering algorithm without deep learning? Yes. Absolutely. Yes. Yes. That's, that's, that's what we, that's what we just did. Okay. And I should have clarified that. Okay. What do you think about trying to group possible actions and optimized path taking in this group in reinforcement learning to reduce size, size of state space.

Speaker 2:          47:43          Okay. Reinforcement learning in general should be applied everywhere. It is going to be applied everywhere because no pack net. Okay. Path neck by deepmind. Okay. Pack Net. Let me, let me make the path name. But this is, this is so dope because they applied, they applied reinforcement learning to a neural net architecture. Check this out, check out that link. We only put in a description for people watching us, but it used a reinforcement reinforcement learning agent inside of a neural network to find the optimal parameters. So get, get this for a second. It is looking inside of it is using an AI inside of an AI, which is a neural network to find the optimal hyper parameters. How Joe is that? Okay. And so that's that. So yes, we can use reinforcement learning every work including recommender systems. Two more questions and then we are Outta here. Okay. Um, how much usage data is typically required to make a good performing recommender systems? Rosio quake. Great question. You definitely need a lot of data. Like 100,000 plus sets. There's a lot of of reckon. There's a lot of recommender system libraries. How, I mean, sorry, data sets out there publicly available a link to them in the description. But you need a lot. You need a lot. Okay. If you want it to be accurate, you need a lot. And, and don't be intimidated by that because it's everywhere.

Speaker 2:          49:14          Okay, well actually I'll, I'll take two more favorite entrepreneur. What favorite entrepreneur is? Probably my favorite entrepreneur is to POCs, sugar because Tupac face so much oppression. He faced so much suffering and in the face of all of that, Tupac believed in himself enough to broadcast themselves to the world and say, Hey, I'm Tupac Shakur. I don't care how many haters I have. I'm going to be myself and I'm going to be out essentially a messiah for a bunch of people. And so yeah, Tupac is my favorites. Uh, entrepreneur. I also have a book of Tupacs poetry called the rose that grew through concrete. One more question. Yeah. We're going to talk about, let's make it a good one. Okay. When are you making a robot with machine learning?

Speaker 1:          50:20          Okay,

Speaker 2:          50:21          next video. Okay. I just said it. So now it has to happen. So the next weekly video we're going to make a, okay. So

Speaker 2:          50:30          detached. If you want to do research post in the slack channel, guys, we need our own research group, okay? We need our own research group. I want publications coming out of this community, okay? You guys are so smart. We need to build community, okay? We're going to build a research community, okay? We were going to publish, we're going to build a brand that publishes worldclass machine winning research. Okay? So that's what we're gonna do. All right? All right, so, so that's it for the livestream. Okay. And next time I'm going to code everything. This time I didn't, uh, I love you guys. And for now I've got to edit this next video. I just have to edit it myself. Like you, Udacity has offered me an editor, but I just cannot do it. I just have to do it myself right now. Okay? So that's what I got to do. And, uh, thanks for watching. Bye.