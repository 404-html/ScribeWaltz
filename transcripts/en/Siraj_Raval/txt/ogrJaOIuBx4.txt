Speaker 1:          00:00          Her world. It's Saroj and we're going to make an a half that reads an article of texts and creates a one sentence summary out of it. Using the power of natural language processing language is in many ways the seat of intelligence. It's the original communication protocol that we invented to describe all the incredibly complex processes happening in our neocortex. Do you ever feel like you're getting flooded with an increasing amount of articles and links and videos to choose from as this data grows, the importance of semantic entity does as well. How can you say the most important things in the shortest amount of time? Having a generated summary lets you decide whether you want to deep dive further or not and the better it gets, the more we'll be able to apply it to more complex language like that in a scientific paper for even an entire book.

Speaker 1:          00:49          The future of NLP is a very bright one. Interestingly enough, no. One of the earliest use cases for machine summarization was by the Canadian government in the early nineties for a weather system they invented called fog. Instead of sifting through all the meteorological data, they had access to manually, they led fog, read it and generate a weather forecast from it on a recurring basis. It had a set textual template and it would fill in the values for the current weather. Given the data, something like this, it was just an experiment, but they found that sometimes people actually prefer the computer generated forecasts to the human ones, partly because the generated one's used more consistent terminology. A similar approach has been applied in fields with lots of data that needs human readable summaries like finance and in medicine. Summarizing a patient's medical data has proven to be a great decision support tool for doctors.

Speaker 1:          01:43          Most summarization tools in the past or extractive, they selected unexisting subset of words or numbers from some data to create a summary. What you and I do, something a little more complex than that. When we summarize, our brain builds an internal semantic representation of what we've just read. And from that we can generate a summary. This is instead an abstract of method and we can do this with deep learning. What can't we do with it? So let's build a tech summarizer that can generate a headline from a short article using care Os. We're going to use this collection of news articles as our training data. We'll convert it to pickle format, which essentially means converting it into a raw bytes stream. Pickling is a way of converting a python object into a character stream so we can easily reconstruct that object in another python script.

Speaker 1:          02:32          Modularity for the wind, we're saving the data as a tool with the heading description and keywords. The heading and description or the list of headings and their respective articles in order and the keywords are akin to tags, but we won't be using those. In this example, we're going to first tokenize or split up the text into individual words because that's the level we're going to deal with. This data in our headline will be generated one word at a time. We want some way of representing words numerically. Angio coined a term for this called word embeddings back in 2003 but they were first made popular by a team of researchers at Google when they were least word to Vec inspired by boys. To men. Just kidding. Word to Vec is a two layer neural net trained on a big label text corpus. It's a pre trained model you can download.

Speaker 1:          03:20          It takes a word as its input and produces a vector as it's output. One vector per word. Creating word vectors lets us analyze words mathematically. So these high dimensional vectors represent words and each dimension in codes a different property like gender or title. The magnitude along each access represents the relevance of that property to a word. So we could say king plus men minus woman equals queen. We can also find the similarity between words, which equates to distance. Word to VEC offers a predictive approach to creating word vectors, but another approach is count based and the popular algorithm for that. His glove short for global vectors. If first constructs a large co-occurrence matrix of words by context for each word I he row, it'll count how frequently it sees it in some contexts, which is the column. Since the number of contexts can be large, it factorized is the matrix to get a lower dimensional matrix, which represents words by features.

Speaker 1:          04:18          So each row has a feature representation for each word and they also traded on a large text corpus. Both performed similarly well by glove trains a little faster. So we'll go with that. We'll download the pretrained glove word vectors from this link and save them to disk. Then we'll use them to initialize and embedding matrix with our tokenize vocabulary from our training data. We'll initialize it with random numbers and copy all the glove weights of words show up in our training vocabulary and for every word outside this embedding matrix will find the closest word inside the Matrix by measuring the coastline distance of glove vectors. Now we've got this matrix of word embeddings that we could do so many things with. So how are we going to use these word embeddings to create a summary headline for a novel article? We feed it. Let's back up for a second, Ben. A squad first introduce a neural architecture called sequence to sequence in 2014 at later inspired the Google brain team to use it for text summarization successfully. It's called sequence to sequence because we are taking an input sequence and al putting not a single value, but a sequence as well.

Speaker 2:          05:23          We're going to end code. Then we be code. We join it in code. Then we z code went up. He did a book. It gets back to rise and when I decode that as Mariah,

Speaker 1:          05:34          so we use two recurrent networks, one for each sequence. The first is the encoder network. It takes an input sequence and creates an encoded representation of it. The second is the decoder network. We feed it as its input, that same encoded representation and it will generate an output sequence by decoding it. There are different ways we could approach this architecture. One approach would be to let encoder network learn needs in bedding from scratch by feeding in our training data, but we're taking a less computationally expensive approach because we already have learned embeddings from glove. When we build our encoder LSTM network, we'll set those pretrained embeddings as our first layers weights. The embedding layer is meant to turn input integers into fixed size vectors. Anyway, we've just given it a huge headstart. By doing this and when we train this model, it will just find tune or improve the accuracy of our embeddings as a supervised classification problem where the input data is our set of vocab words and the labels are there associated headline words.

Speaker 1:          06:35          We'll minimize the cross entropy loss using rms prop. Now for our decoder, our decoder will generate headlines. It will have the same LSTM architecture as our encoder and we'll initialize its weights using our same pretrained glove embeddings. It will take as input the vector representation generated after feeding in the last word of the input text, so it will first generate its own representation using it's embedding layer and the next step is to convert this representation into a word, but there is actually one more step. We need a way to decide what part of the input we need to remember like names and numbers. We talked about the importance of memory. That's why we use LSTM cells, but another important aspect of learning theory is attention. Basically what is the most relevant data to memorize? Our decoder will generate a word as it's output. And that same word will be fed in as input when generating the next word, until we have a headline, we use an attention mechanism.

Speaker 1:          07:34          When out putting each word in the decoder for each output word, it computes a weight over each of the input words that determines how much attention should be paid to that input word. All the weights some up to one and are used to compute a weighted average of the last hidden layers generated. After processing each of the input at words, we'll take that weighted average and input it into the softmax layer along with the last hidden layer from the current step of the decoder. So let's see what our model generates for this article after training. All right, we've got this headline generated beautifully, and let's do it once more for a different article. Couldn't have said it better myself. So to break it down, we can use retrained word vectors using a model like love easily to avoid having to create them ourselves to generate an output sequence of words given an input sequence of words.

Speaker 1:          08:24          We use a neural encoder decoder architecture, and by adding an attention mechanism to our decoder, it can help it decide what is the most relevant token to focus on when generating new texts. The winner of the coding challenge from the last video is just soon. See he wrote an AI composer in 100 lines of code. Last week's challenge was non trivial and he managed to get a working demo up. So definitely check out his repo wizard of the week. The coding challenge for this video is to use a sequence to sequence model with care os to summarize a piece of text posts or get hub link in the comments and I'll announce the winner. Next video, please subscribe for more programming videos. And for now I've got to remember to pay attention. So thanks for watching.