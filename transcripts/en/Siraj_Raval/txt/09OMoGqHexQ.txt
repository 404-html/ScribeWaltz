Speaker 1:          00:00          Hello world. It's a Raj and my demo today is landing this space x rocket in a simulation using machine learning. This thing has learned how to properly land using all of those gravitational Constance balancing. It's weights, velocity, physics, all these things, but the key here is machine learning. It's not rocket science. It's machine learning, which is even better. So that's what we're going to do today. I'm going to code this out. It's going to be about 150 lines of python. I'm going to be using the tensorflow libraries, the tensorflow library, and the open Ai Jim Library. But before I get there, we've got to learn a bit about reinforcement learning as a bit of background before we get to the technique that it uses, which is called stay with me, proximal policy optimization. I know that's a mouthful and we're going to get there, but first we've got to learn some background on reinforcement learning.

Speaker 1:          00:49          Okay, so open Ai. It was started by Elan Musk, Sam Altman, a bunch of really prominent people in the, in the tech space. But the goal was to build safe AI in a way that was distributed, that was open so everybody had access to it in order to ensure a more beneficial future for humanity. And one of the major contributions that open AI has made to the world is releasing the gym environment. So Jim is this really cool, uh, library on get hub that allows anybody to train their AI algorithms, uh, in a bunch of different environments. It's very cool. A Universe was kind of their next, um, environment, which they hired me to make a video on back in the day. And then Ilan retweeted it. It was awesome. But anyway, that's a tangent. But Jim, Jim is kind of what has stayed in the mainstream university and really take off for whatever reason.

Speaker 1:          01:37          But, but Jim is still there. It's, it's, it's still very active. People are still using it and a bunch of examples, you can find it on get hub and they've got a bunch of different environments from Atari Games to three d simulations using moo Joko at toy tax class control a bunch of different examples, right? So it's a collection of environments. It's all in python and it's really easy to use. The, the whole point of it is so that you don't have to focus on building the environment. You can just focus on the algorithm. And they had this leaderboard as well on the website, but that they took that down for whatever reason. But you can still find a leaderboard that compares the strength of different algorithms on, on get hub. And it's in this, uh, Jim Wikipedia page. It was updated 25 days ago, but, uh, I would still make a PR.

Speaker 1:          02:23          If you have some kind of update and they're gonna, they're likely going to accept that, that request. Okay. So let's start with reinforcement learning. So this is right. So there are different ways that we can class the learning experience, right? Supervised unsupervised reinforcement online, offline. You know, there's, there's a million different ways that we can class learning, but anytime that we have some kind of time element, right? So when we add the element of time, which is where an environment comes into play, that's when we can use reinforcement learning. It's a computational approach. We're an agent, interacts with an environment by taking actions where it tries to maximize a reward. So this is a very simple way of visualizing this. We have an agent that is a Bot, an AI, a person, an animal, a human. Anything that takes an action in the environment. If it's learning how to ride a bike, that action would be to push down on the pedal.

Speaker 1:          03:16          The environment will then update by giving it a reward, right? So if, if I, if I push successfully, then my muscle won't have any pain. So my brain is getting that reward back, right? And I've updated the states in my environment, my, my foot is now lower and this, this just repeats over and over again. But if I push in the wrong direction or my, you know, my foot goes just crazy in a different direction, then I'm going to get a negative reward and I want to avoid that negative reward. So a much more formal mathematical way of describing this agent environment loop is to describe it as what's called a mark Hoff decision process. This is based off of Andre Markoff and early mathematicians. Initial principle to mark off property, but essentially it's a way of estimating probabilities and how they change over time in an environment.

Speaker 1:          04:01          So we have a state, a state is how you, an agent is currently in a game. Like what, what's its position, what's its move, what's it's uh, the, the way it's positioned in relation to other objects in the environment. And then we have an action. This could be, you know, punching a wall or hitting a ping pong ball or jumping up and down. It's something that the agent does. We have a transition probability that that relates the states to the action. We have a reward, which is what the agent receives if the action was correct. So by correct I'm saying how does it affect the objective? What is the objective of the game to win, to lose? How does it affect it if it's uh, if it's a winning move, if it's a winning action, then the reward will be something like a plus one.

Speaker 1:          04:45          If it's not, it's a minus one. And there are different quantities that are involved here, like policies, utilities, Q values, and that's there. There's a lot that we can go into. There's so many different ways and pathways we can go into when we're talking about reinforcement learning. But what the one that I want to focus on is the policy and the value, which I'm going to go into in a second. But before we get into that, let's talk about offline and online learning. Remember I said there's so many different ways that we can class though the different learning processes and one way is offline versus online. So online just means learning in real time as we're playing the game, which is what we're going to do and offline is before or after or in between games. So it's like we've already completed a set of actions in an environment we're done and now we're learning from that.

Speaker 1:          05:35          Whereas online is during the actual process of interacting. That's what we're going to do. So to summarize a bit about Arele, we have an agent. It interacts with an environment and it learns what's called a policy. So this is a very important word. We have a state and an action. And a policy describes how an agent decides how to best interact with the environment using its current state and the list of potential actions it could take, right? So it, it looks at what's called the state transition matrix, which relates all the states to all the actions. And the policy says, what's the, what's the optimal action I can take right now that will maximize my goal, right? How do I get to my goal in the most efficient, optimal way possible? And the policy will tell the agent exactly how to do that. But in addition to the policy, there's also another feature called the value, what's called the value function.

Speaker 1:          06:29          So the value function will evaluate every single possible move and it will create this list of all the possible values for every move. And it will choose what that best value is. The policy uses the value function to then evaluate every move. So the value function is saying, this move is, this is this good, this movie is this bad, this movie, this is even better. And then the, and then the policy will use that value function to then pick the best move. So the value function is a part of the policy. The policy is the higher order function on top of the value function and this value function can be learned and approximated by any learning and approximation approach like in our case, a neural network, right? Function approximation. So we're going to learn about you function and it's going to help us learn a policy and we can broadly categorize reinforcement learning into three different groups.

Speaker 1:          07:20          For our sake, right now for approximate policy optimization, there is the critic only method where we first learn about you function and then use that to define a policy, right? We we learn a value function using a neural network and then use that to define the policy. Then there's an actor only method where we directly searched the policy space. We don't have this. The policy is what's evaluating all the moves without separating. These modules have a value and a policy function, you know, independently. And then there's an actor critic method. This is where we have two different neural networks. One learns the value and one learns the policy, right? So the cue, the critic observes the actor and evaluates its policy, determining when it needs to change. So the critical be a neural network. The actor will be a neural network. The actor will then take action in the environment and then the state will be returned to both the actor and the critics, the critical observe this, the new states, and then send, it's a Q value, which is the quality of that action to the actor so that it can update its weights.

Speaker 1:          08:23          And the process repeats. And what's what's been found is to have to do is when we have two different neural networks, an actor and a critic, this usually outperforms both acthar only and critic only methods. So we're getting closer to proximal policy optimization. Let's keep going here. So there's this idea of the policy gradient approach. Andre Carpathia has a great blog post on this, and I'll link to it in the description, but the idea is that, let's say we're in a game, right? Let's say we're in Pong and we're trying to evaluate how good each move is, right? Let's say one of the moves is moving the paddle up and then the ball hits the paddle and then it goes to the opponent. Well, one, one thing you could say is, well, this was good, right? So returning to reward of plus one, but the problem is, what if that's move was good in the short term, but it was bad in the long term.

Speaker 1:          09:12          What if it was better to move the pedal down? The ball would go down, the agent would hit, the opponent would hit it. You hit it again, and then you were let more likely to win the game. If you would've went down, then you went up. If you just updated your weights immediately, you wouldn't have known that you, instead, you should wait until the game is finished before evaluating how good that move was. And that's where the policy gradient approach comes in. It waits for the game to finish for the episode to finish before using that result as a gradient to then update the weights. So it's kind of like supervised learning where the label is the result, right? The log probability of whether or not that action was good or bad, and then updating the weights. So that's the policy gradient method. So now, now on top of that, there's something called the trust region policy optimization method.

Speaker 1:          10:04          This is the same idea as the policy gradient method, except it doesn't stray too far from the old policy. So one of the downsides of the policy gradient method is that sometimes updates are way too big and this causes a lot of noise in the loss function and it, and it prevents the, the network from, from converging on the, on the proper optimization scheme. So what the trp Oh method does is it says, let's not stray too far from the old policy. Stay near to the old policy making too large a change is a bad thing. So how do we do this? Well, a naive solution is to make a very small step every time. But then how do you take small steps? What trp Oh does is it controls the rate of policy change using a constraint then then comes PPO, right? So Ppo proximate policy, implement optimization is an implementation of trp.

Speaker 1:          10:56          Oh, that adds what's called a KL divergence term to training the loss function. And so there's a, there's a, there's a KL divergence term that trains the loss function, and there's also one that updates the old policies. So check this out. Here's what the graph looks like for what we're going to build in what's called tensor born. We have a po, we haven't fault, we have a policy function, we have a old policy function, we have an, we have a critic, and the actor or the actor is going to be this policy function, okay? This is gonna make a lot more sense when I start coding it, but just recognize that we have what's called an actor, a critic, and this is an implementation of what's called proximate policy optimization. All right, so let's get to the code. So the first thing we're going to do is we're going to import tensorflow, right?

Speaker 1:          11:43          These are our, this is our machine learning library, and we're going to import gym. And then we're going to go ahead and create this PPO class, right? This, this is where our main object is going to be that we're going to use later on. Now we're going to initialize it and it via our python in it function. Go Python. Now, who's ready for this? The first thing we're going to do is initialize our tensor flow session as a TF dot session, right? This allows us to execute graphs or part of graphs. This is the tensor flow graph, right? The, the, the main part of our pipeline that we're going to place all of our objects into. Then we're going to create a place holder. This is just a variable that we will later assign data too at some point. But right now it's going to be a float 32 object, um, the size of the tensor, or he's going to be none. Give it some hyper parameter that I've, that I'm going to define later, and then it's going to be called the state. Okay? Now we're ready to define our critic. Okay, so this critic is going to be a neural network. So we're gonna use tensorflow is variable scope.

Speaker 1:          12:52          So the variable scope keyword is it allows us to create new variables and to to share already created ones. So this is just a way of organizing our code of a a bit better. So the first layer of our network, we're going to use the layers sub module and it's going to be a dense layer because every single note in the first layer is going to be connected to the next layer. We're going to use that. That's a placeholder as a data entry. It's a gateway for data into the network. And then we're going to say the uh, nonlinearity is going to be called [inaudible], which is a very popular nonlinear function that makes sure our network can learn both linear and nonlinear functions. Remember, neural networks are universal function approximators. Now, well, let's create our value, right? So the value is going to be, uh, this dense layer and it's going to be the output of the first layer is going to be fed into this value variable. Now we're going to create a discounted reward. This is a reward in the future that's not worth quite as much as the reward right now. And we'll say, okay, this is two is going to be a place holder.

Speaker 2:          13:59          Okay.

Speaker 1:          14:00          TF, Daf float,

Speaker 2:          14:05          32

Speaker 1:          14:08          the size, we'll keep it small and we're going to call it discounted reward to keep things, keep things, um, organized. And I'll call it discounted reward.

Speaker 2:          14:24          Now, uh,

Speaker 1:          14:26          all right, so now the advantage, this is the next variable we're going to use. It's called the advantage. So we'll say the ta, the advantage is going to be the difference between the discounter to award and the value. And so the basically this estimates rather than just discounted returns, it allows the agent to determine not just how good a its actions were, but how much better they turned out to be than expected. And this should be a minus, not an equals. There we go.

Speaker 1:          14:52          Now for our loss function. So our loss function is going to, uh, minimize that advantage over time, right? We want to make sure that advantage gets smaller and smaller. So our expected, uh, action is going to be closer and closer to what the real, the optimal action should be. And it's the mean squared error. That's why we're using these functions right here. And lastly, we're going to make sure to use gradient descent because wine, because grading dissent is the most popular optimization strategy out there for neural networks. And one implementation of that is called Adam. And I've got a great video on Adam. If you just want to check out, um, what's it called? Which activation function should you use? I talk about the difference between Adam and nesteroff momentum and so many different amazing activation functions using that loss function as his parameter. Okay.

Speaker 2:          15:43          Yeah.

Speaker 1:          15:43          So that's our critic. We've got our critic, uh, everybody's a critic these days, right? So now we're going to go into our actor, the actor himself, Nicolas Cage, because he's on the Internet. So the actor is going to be our policy network by the way. Okay. So the actor's going to be our policy network and our critic is actually our value network. I forgot to mention that part. So the critic is the value network. And the actor's going to be the policy networks. So we separate these two concepts into a different, different modules. So for our actor, we're going to say, okay, it's going to return the, uh, normal distribution of actions and then the trainable parameters. But I'm going to go ahead and have a, uh, simple helper function for this to build a very similar neural network. I'll call it pie. Uh, and then I'll say yes, it is trainable, was very similar to one layer neural network, which I can define later. And so now what? Remember, we want to have two different policies. We have an old policy, and then we have a newer policy. So I'll say, let's have the same exact outputs and we'll use the same exact, uh, help her function, except we'll call this one the old policy. And then this one's not going to be trainable

Speaker 1:          17:08          now for our, uh, for us to sample our actions. Now the next thing we want to do is we want to sample our actions from both the old and the new policy networks so we can compare them. So again, we're gonna use that variable scope, uh, function to then sample actions.

Speaker 2:          17:30          Okay.

Speaker 1:          17:32          And we say, we're going to say, okay, so the sample operation is going to use, uh, this TF dot squeeze function. And then we're going to sample from it.

Speaker 2:          17:45          Okay.

Speaker 1:          17:45          And the access is going to be zero because we don't want multiple dimensions. And this is basically just going to choose an action from, from our distribution, right? So it's going to output a distribution of possible actions we could take. That's what the neural net learns for the Policy Network. And then we're going to just choose an action from that. And then we're gonna say, okay, so that's, that was a sample action. And now we want to have a, another variable scope. We'll call this one the update the old policy. This one's going to update the old policy. So then we'll just call it update old P, I.

Speaker 2:          18:22          Okay.

Speaker 1:          18:23          And we'll say, okay, update old policy cold p. Dot. Assign p four, p, O p and zip. And then we have those, the new policies, parameters.

Speaker 2:          18:43          Okay.

Speaker 1:          18:44          No. Okay. So that's it for our s for us sampling from the actions from both. Now we can, uh, create two placeholders. So these are going to be placeholders for the action and the advantage. And we're gonna use these to help us compute our loss functions in a second. So we'll say, okay, so the action stuff that Tfa is going to be a place holder, not a hotline or a holder. This is not bitcoin. A placeholder, TF dot float 32. It's going to be very similar size and it's going to be called the action. And I'll just copy and paste it again because it's a very similar line. And this time it's going to be called the advantage. How's that TF advantage? And this is going to be one. Okay. So now we have these placeholders

Speaker 1:          19:40          and now we can finally say, let's do cuts, computer lost functions. So again, variable Dusko scope. Notice how I'm, I'm putting all of my modules inside of this variable scope. And another reason this is useful is because in the tensor flow graph we are able to name these things so we can see how all of these uh, tenters are flowing through the nodes. In the computation graph that we create inside of our loss function, we're going to have something called a surrogate. Okay. So you might be thinking, what the hell is a surrogate? So this is actually really interesting. So let me show you this. So a surrogate is, so when it comes to sometimes in machine learning, the air surface doesn't, isn't very smooth, right? So if you think of gradient descent as dropping a ball, dropping a ball into a bowl, and then having that ball find the minimum.

Speaker 1:          20:31          So that's easy if the air surface is curved, but sometimes it's noncontinuous. So this is some discrete math for you, but sometimes it's surface doesn't just easily go from one range to another. It's not smooth. It looks more like this, right? So the ball won't just flow down to the minimum. It's going to hit different steps. It's going to go all over the place. So we want to make sure that air surface is continuous, that it's very flat, that it's smooth, that we can easily find the gradient. And to do that we'll create two loss functions. We have a loss function, and then we have a loss function for the loss function and we call this a surrogate loss function and that the surrogate loss function, we'll first smooth out that earth's surface and then we can find the minimum, which I think is very cool.

Speaker 1:          21:16          All right, we're going to use these two terms to compute the k l penalty. So the Kale by the way stands for Ku Black Liebler, two guys in the in like back in the day, a couple of decades ago, who discovered a really cool loss function. It's used a lot in generative adversarial networks and in proximate policy optimization. But basically let's go, let's go. Let's check this out. So the both of these two terms, the the ratio and the surrogates will help us compute a kal Penn a penalty. And what's going to happen is we're going to use the old policy and the new policy to come to compute the Kale divergence between both. We'll take that Kale divergence term, get the mean, and then compute a loss using the surrogate. So then we're going to find the difference between the surrogates and the lambda that we just computed times that Kale divergence term and that's going to give us a loss function.

Speaker 1:          22:07          We will minimize that loss function using gradient descent. So these are the two separate distinct respective loss functions that we're minimizing, one for the policy and one for the value network, which will also calling the actor and the critic network. We can then write all of that to disk and then run the session. So I actually want to show you some of the helper methods as well. That's the base code for creating the policy and the uh, value networks. But let's look at some of the helper methods as well. So for the build a net function that I talked about, this is it, it's just a single layer neural network. We compute a mean and a an a sigma. And then we use that to compute a normal distribution and return that as well as the other parameters, the global variables, but in the training loop, here's, here's the real, the real magic where we implement what we just created.

Speaker 1:          22:55          We'll say, let's start our training loop, create an initial environment. We'll have a buffer for both the states, the actions and the rewards, and in a single episode we'll use the PPO class we just created to choose the best action dependent on the state and that's going to return that action. Now we'll take that action and then execute that in the environment using the step function really easy part of using a open Ai's gym and that's going to return the state, the reward, and then whether or not we're done and some log, uh, criteria which are not actually using. We'll add all three of those to our buffers and then will compute the total reward over time. Then we update our PPO object. We'll say, well, let's get that value that get the using the states and then get the discounted reward. Then finally we're going to, we're going to pop all of those off the stack and then update our policy using those that that new state, that new action and that new reward and that's it.

Speaker 1:          23:52          Then we can render the environment and then display the frames as a gift for whatever we want to and return all those rewards at the very, very end. This is the training loop where you run it for a set number of epochs and then eventually what's going to happen is that rocket is going to use both the policy and the value networks, the actor and the critic to learn what's the best way to land on that landing pad, right? It's going to say if it doesn't land, it's a minus one, right? If it does lend, it's a plus one and it can use all of these different methods. Turn left, turn right, turn up, go move faster, move slower on its booster, and then there's a bunch of other gravitational constants that are in the that environment set up for it that we, you can see in the documentation I'm going to give you.

Speaker 1:          24:34          Basically it's going to use all of those to learn. Well, what's the best way to try all this out? The policy is computing these stochastically that is, it's using a distribution to sample from all the possible actions. And the value function is going to say, well here are, here's the, here's how I'm going to rate all of these possible actions. And the policy will say, Hey, value function. I see how you're rating all of these possible actions and I'm going to learn from you. And the value functions learning as well. So there are two different sets of gradients that are being composed that are being computed over time and eventually through that plus one minus one feedback loop that we're getting from open Ai's gym environment. These two networks going to work together to help this thing land as efficiently as possible, right? So we can say, um, by Felon Ppo dot pie. And then we can see it run just like that. So this thing is, this is a trained a version of the algorithm. If you want to train it yourself, I've got the source code in the description, check it out. It's the get hub link. And hope you liked this video, please subscribe for more programming videos. And for now I've got to get a seat on the falcon heavy, so thanks for watching.