Speaker 1:          00:00:05       Oh,

Speaker 2:          00:00:14       all right. Hello world. It's the Raj, welcome to this session, this live session today we're going through, as you can see on the screen here, let me make it a little bigger. We're going to defeat the game of Pong using policy gradients. Okay. So I know this is the world's smallest pong game, but just, uh, so actually I said I set the size programmatically so I can't just like switch it right here in real time. But the point is that it's pong and we're going to use a technique called policy gradients to solve this. So policy grades is really cool and I, I really love this idea and it's one of those ideas that has excited me because I've been studying this stuff like every single day for the past couple of months. And, you know, things start to get, start to get repetitive after a while, right? We're, we're taking the gradient of some, uh, of, of, of function and then back propagating it consistently.

Speaker 2:          00:01:04       You know, most of the advances in deep learning is because of supervised learning where we're able to what's called differentiate, um, a computation graph or a neural network. And that's all of the major advances have happened because of that. But policy gradients are really cool because they, they help us back propagate when nodes are stochastic. So I'm going to talk about these terms in a second and these are good terms to know. Okay. So it's the castic versus back propagate. Deterministic. That's my own voice. Hi everybody. Good to see everybody. Okay, so now what we're going to do is we're going to write out the code for this and it's going to be about 130, 140 lines of code in Python. I'm going to explain it as we go. Uh, this is a demo of it running already. So the PR, so it's going to take about five to six hours to train if you run it on the cloud, on a GPU cluster.

Speaker 2:          00:01:59       If I were to run this on my Mac book, it would take probably like three to four nights of full time running. So that's not what it's for. It's it's, it's, it's not to run locally. Okay. But when it's good, when it gets to the point that it is amazing, that's when things get really interesting. It's going to be better than any other type of AI you can think about, which is amazing because it's not looking at the size of the paddle. It's not looking at the size of the ball. It's not looking at the velocity of the ball. It's not, we're not feeding him these hard coded rules. It's learning from sheer pixels alone, which is amazing. Just sheer pixels alone. It's able to do this and it's also better than a deep Q, which was, Google's a deep Q was Google's, I'm sorry. It was deep minds algorithm that Google bought deep mind for like, this is what impressed Google enough to pay several hundred million dollars for deep mind. The acquisition. This algorithm right here, it used a convolutional network to read in the pixels from a game and then it it output actions that then the agent implemented in the environment, which was any Atari game, which was incredible. There were no hard coded rules for any Atari game.

Speaker 3:          00:03:18       Let's just get 30 seconds.

Speaker 2:          00:03:20       Okay. Let me know when we're ready to go.

Speaker 2:          00:03:26       We're back. Okay. All right guys, thanks for waiting. So, uh, there's a question. I'm going to answer two questions before we get started cause there's so much to go over and I'm actually, I'm very excited to, to start coding this cause this is something that I find particularly interesting. The first question comes from Lucas who said, how do I get started with tensorflow or neural networks? I find this topic very interesting but I don't know how I can get into it. Can anyone tell me? That's a great question. Uh, so my answer is very simple. It's very short. It is my videos. I have been making videos on tensorflow and neural networks for the past year and I make a video every single week. The amount of writing that goes into my radio videos is massive. I have covered almost every topic intenser flow in beef bits and pieces throughout my videos.

Speaker 2:          00:04:11       And in fact, I have a playlist on tensorflow. I also have a separate playlist on neural networks. On my youtube channel to check out my channel. I've got two playlists for both of them. Uh, and as and this intro to deep learning course as well. So my channel, if you want to supplement it with some kind of written documentation, I would recommend Chris Olah is blog. That guy is amazing. Also distill, which is a new machine wedding publication that open AI and a bunch of other people are working on. That makes it really easy to look at machine learning papers with visual documentation. Okay. So one more question and then we're going to get started. So Neil ass, when you have, when do you have to test your classifier? Hold on. So when you, when you test your classifier it, I'm paraphrasing, is it a good idea to use synthetic data sets?

Speaker 2:          00:04:55       Can you say something about synthetic datasets? So synthetic datasets, I assume you mean g data sets that are created by humans just for testing and playing around are good for testing and playing around. You wouldn't want to use a classifier in production that millions of people are using or even hundreds of thousands that has only been trade on train, on synthetic datasets. You want it to be trained on real world data, right? Because you're at, you're applying it to a real world use case. Uh, yeah. So storage, simple answer to that. Let's get started. We have a lot of things to do. Okay. So let's get started. So the first thing we want to do is stop this because we're about to start coding and let's do this. So here's the, here's the fun part. I'm not using tensorflow. There's no tensor flow that's happening right now.

Speaker 2:          00:05:44       I'm using Matrix, I'm using num Pi, which is just good for matrix math. That means we're going to do forward propagation, backward propagation by hand, and differentiation. Slash. Backpropagation. We're doing it all hand. So get ready for this. So the first part is non Pi. Okay? So that's going to do all of our matrix math. Okay. We're gonna use this baby a lot. And then we've got pickles and pickles. Great for, um, eating also to import into our libraries. It's good. It's great for that too, to serialize our data so that we can use the, save our model and then load it up later. Um, okay, so save load model. And then our last library is Jim.

Speaker 2:          00:06:25       So Jim is great for running game environments. So let me say a little bit about money. I released Jim a couple, you know, a while ago, a couple months ago. And Jim was great because it allows you to train agents, reinforcement learning agents in game worlds in a whole bunch of different game worlds. And you can specify which game world it really easily. Uh, these are just a bunch of colored boxes. So let me wait for that to load and let me, and then what, what opening I did is they released another, a repository called universe. So university builds on gym with an add way more environments. So Jim and universe are both compatible. It's not like universe replaced gym. They're both compatible. So a lot of the times you don't actually need to use all of university. You just want like something super simple. Go for gym, uh, which is what we're doing right now. We just want to use pawn. Uh, but yeah, and so that's what opening are released and there's a whole also, there's a whole bunch of algorithms that have been made for Jim because it's been around for longer and you could look at it at an open AI's websites. Um, they've gamified the whole process pretty great. Those are three dependencies that we need. Okay. And then, alright, we're not going to get a little Mike fix for a second.

Speaker 1:          00:07:49       Cool. Cool.

Speaker 2:          00:07:50       Thanks. All right, so let's get started with this book. Before we get started, let's talk about how does pong work. Okay. I mean, let's just think about it. I'm like, what do we need to do? Let's just think about this intuitively for a second. Stop everything. Hold your horses is a glorious day or night, wherever you are. And I'm thank you for watching, by the way. How do we get our agent to learn how to hit the ball? Well as humans. Whenever we go into Pong I what do we were doing something like, let's just see if I go into a game, I've never seen pong before. I look at it, I see that the agent is, I see that the AI is hitting a ball after even one iteration. I know that I need to get this ball to go past the agent because I see my score go down or his score go up if he hits the ball. So it's easy for us. However, one more technical thing that's got to get fixed. The mic.

Speaker 1:          00:08:51       Cool. Do you want to hold it too? Maybe that for now

Speaker 3:          00:08:54       what's happening is it just, it's just getting really fuzzy and say they can't quite hear you can't quite it now. Like I just like put it right here. So now that Mike is on the table, is it as fun? Okay. So now I've got, just hold her in front of you. All right guys.

Speaker 2:          00:09:21       All right, so one second. I'm holding the mic up where we're going through some technical difficulties but uh, is with the Mike and Audio and you know, live stream. Can be kind of crazy because they're live. We're doing it live. Okay, so let me, let me talk a little bit more about this while we fix this issue. So, uh, long humans, we're really good at this, but uh, but we need to think about how to do this in a way that we don't even think about pong. We want to focus on the outbreed them. We don't want to focus on the environment. So if we think about it, we're going to do this. Okay. How does palm work? Well, step one is we even from the game that is this game state, this is an 80 pixel game world. And then we're going to move the paddle either up or down.

Speaker 2:          00:10:05       It's a binary decision. So we get some image, we move our paddle up or down, and then we make an action and receive a reward and it moves past the AI. And then we subtract one if we miss the ball and then zero otherwise, so plus one, if it goes past the agent, uh, minus one, if we missed the ball and it goes past us and then zero. Otherwise the agent hit it. So the three possible actions that we could take, let's forget about everything else. That's all we need to know about. Why. Because we are focusing on the algorithm. We're not focusing on care, not policy. Great hands. How can we make the most general algorithm that we can apply to paint? Dang. Okay. So, and this is a type of green hold policy gradients and policy gradients are really cool for several reasons that I'm going to go into in a second. But what we're going to do is uh, we're gonna just start coding this thing. So

Speaker 3:          00:11:07       when I have a free hand, let me actually say something else. So what we're gonna do is we're going to build a two layer neural network. Okay? And that these are agent, it's going to be a two layer neural network and the network is going to read in, we're going to feed it one input and the input is going to be the game state. And that is the, the the pixels, not that, not the states, the game pixel. So a frame, we're going to free pizza in frame by frame, by frame, by frame, and then it's going to output a probability value of whether to move up or down. That's it. And then we sample from that probability value. Should we go up or down and

Speaker 3:          00:11:45       then we say, okay, so should we go up or down? And then from that probability value we can then update our network. We'll get, we'll we're, we're getting the gradient, which is the policy from that value, from that distribution as we back propagate and you'll understand more as well as I start coding this. Okay. So let's just go ahead and get started with this. So the first step is to import our hyper parameters. So let's go ahead and get started with the hyper parameters. Okay. So the hyper parameters in this case are going to be the number of hidden neurons. Okay. There's gonna be 200 hidden neurons, okay. For our two layer network. And

Speaker 4:          00:12:24       okay.

Speaker 3:          00:12:24       Then we're going to import the batch size. So how many episodes do we want in a parameter update? We're going to say 10. Uh, the learning rate, uh, is going to be a very small. So one e minus four just means like 0.0, zero zero one where four is a number of values after the decimal point. And the reason we have a learning right is for convergence. If it's too low then it's going to be slow to converge. If it's too high then it's never going to converge. We always need a learning rate. Well we don't always need a learning, right? But if you want your model to have accurate values, then yes you want to learning rate. Okay, so then we have gamma. Gamma is going to be used for, and this is really cool and this is specific to policy gradients, but for our discount factor, what the hell is this?

Speaker 3:          00:13:09       I will talk about it when we write the method for that later. Rewards are less important. So what we're doing here is we are optimizing for the short term. That means that we are optimizing for short term rewards. We don't know what's going to happen in the future. This is, this isn't a game like legend of Zelda all Corinne up time with many levels and there's a whole bunch of things that we could do and you know, moving up a ball this way or picking up a ferry right now could help us beat the boss five levels of in the future. This is Pong in Paul. You just got to get, get in. You are in the game. Once you are in the game, you are in the game. So that's what that's for. And we'll explain it. I'll explain it when we get to the, uh, the actual code. So now we have our decay rates. So decay rate is going to be 0.99. And this is for rms prop, which means grading dissent. Remember, we're going to do this by hand and you don't actually have to do this with tensorflow. Tensorflow is great because of automatic differentiation. That means that it does this by itself, but we're going to do it by hand because hard mode is on right now. Seriously hard node in life is long. Okay. Um, so right, so,

Speaker 4:          00:14:24       okay.

Speaker 3:          00:14:25       And then resume equals false because this is just our way of saying do we want to start off from a previous checkpoints? Okay. So then let's go ahead and initialize our model. Okay, so in it model, right? Okay, so for anything, our model, and I'm going to answer questions in five minutes, uh, is going to, we're going to say, okay, so what is the,

Speaker 4:          00:14:44       okay

Speaker 3:          00:14:45       input dimensionality. It's going to be an 80 by 80 pixel frame. Okay? We're defining that right here. D 80 biking, and that's what we just saw. If I made it bigger than that, that palm would be bigger. Okay. So now what we're gonna do is we're going to say if resume that is, if we want to load it from a previous [inaudible], this is just, this is why we imported pickle. We're going to say, let's open. That's value from our saved file and that loads it. Okay. So, okay, so that's for resuming. And then I'm going to also see how everybody's doing over here. Everybody's good. Everybody's in the room. All right. Once you are in the game, oh man, you guys are the best. Okay. Okay. So now model equals nothing. So elsewhere you, we were going to initialize it by hand. Okay. So this part is really cool.

Speaker 3:          00:15:41       So this is something that I haven't done before. So, um, it's time to initialize our weights. So how do we initialize our weights? Can anyone tell me? Think about it. You're gone. Because see, we usually say I get these products, have notifications that I don't care about, but I don't have time to remove them because I don't have time for anything but this. And that's the way I like it. Look, where was I? We are going to initialize these models pseudo randomly. So it's going to be smart her, they just clean all random. It's called Xavier initialization. So Dave, you're initialization and there's a whole bunch, there's a whole theory behind this. What essentially what we're doing is we are taking the hidden nodes into account when we initialized our weights. So we can say, let's say for our first set of white, so we have a two layer network, right? So our first set of ways is going to be random but conditioned on are, are um, are hidden nodes as well as our model, our input dimentionality so we're going to say random. Got Rent, uh, and this is the interval that we're going to be using. And we're going to say that's beautiful it from teach to d divided by the square root of heat. Now why do we do this?

Speaker 3:          00:17:11       The reason we're doing this is because we want to, let me, let me just for the equation for a second cause danger and the foundation because this is pretty cool. Xavier initialization. Okay. Xavier initialization. It's pretty cool. So it's the same formula. It runs the same way and,

Speaker 2:          00:17:51       okay. Right? So it's just, we don't actually this, this is the formula, what we're sampling from a distribution between h and d divided by the square root of d. And that is better than random. And what it does is we make sure that the weights are not too small, but not too big, but also not too big to propagate accurately. It's kind of like the vanishing gradient problem, whereas like it's a kind of the same idea where, um, if our weights are initially very close to zero, then what happens is that the signal shrink as it goes through each layer until it becomes too tiny to be useful. But if the weights are too big than the signal grows at each layer. So we do that for this first set of weights and then we do that for the next set of weights as well.

Speaker 2:          00:18:33       Okay. We do it for both sets of weights. So this is going to be weights too. And these are in the model is going to be a key value pair where the key is going to be the, um, the, what is it? The construct, whether it be weights or a layer and in the value is going to be the values inside of it. The vector, you know, the actual values inside of the Matrix. So we're going to say through h and then square root h. Okay. Because now we have a hidden state. So for the next one, right? So the first one hidden c and d because we're inputting that the frame. But now for the wait, wait too. We just, we're just talking about the hidden wait with the input has already been computed into some doctor. Okay. So that's Xavier initialization. Remember this is super low level.

Speaker 2:          00:19:19       We're not using anything but num py right now. So, um, so it's awesome. Cool. So now we're going to initialize two more values here, the gradient buffer and the RMS prop cash, which I'll explain in a second. So for our gradient buffer, we have a collection of Zeros that we're going to initialize. Uh, and then we're going to say for each key value pair in the model. And remember this, this is great about putting them put, our model is essentially a dictionary for this use case. Uh, we're going to say get the gradient buffer. And let me minimize that a little bit so we can see all of that. Okay. Just like that. And then we're going to say rms product. Let me explain. I'll explain this once I type it out. We have our RMS prop cash and I'm going to take questions once I write this, this line out.

Speaker 2:          00:20:07       Okay. So save them from them. Save your questions for then. I assume that I'm talking too fast already just because my uh, algorithm has been trained to detect that from feedback, trial and error of doing this it or items. Okay. So what, what, what the hell did I just do here? So what I did, what, let me explain this and then I'll answer questions was I define a great and buffer and then an rms prop cash. The gradient buffer is going to help us with uh, backpropagation. So it's, it's a way for us to store our gradients. Right now it's initialized as Zeros, but eventually we're going to add a bunch of gradients to it and you'll see that when we get to backpropagation and rms cash is going to store the value of a formula. This, the RMS prop formula and that is including whatever we multiply it by, which is going to be some set concen value into this cash.

Speaker 2:          00:21:07       Again, will be more clear when we start actually using it. But right now they're both initialize with Zeros. Um, for all the values in our model. Okay, so let me answer some questions here. I'll answer two questions and then we'll see. We'll keep getting, we'll get right back into this. Omar Miranda ask please defines the castic and non and and no stochastic but what non stochastic. Okay. It's the castic means unpredictable. Okay. It's kind of like the u s elections. They're sarcastic. We had no idea that would happen, but they're kind of like life. Life is the tats, the castic. If you believe in fate, that is not the castings. Fate is deterministic. We are, we we, there's a predetermined outcome that is fate. Stochastic means we have no idea if it's going to happen or not. Anytime we have a random number that is the castic.

Speaker 2:          00:22:01       Anytime we have a sat set, static number that is deterministic and then the question becomes, what's going on here? Do we have a bunch of deterministic values in here or do we have a bunch of stochastic values? This goes into the question of free will. So this is a really cool thing because it, it, it's a way for us to think about consciousness and human, what it means to be human and determinism and free will mathematically. How cool is that? So I think that it is the castic what's happening here that we do have free will, that we do make decisions that are not predetermined and we're starting to see deep learning move in that direction. We're starting to see people ha, uh, researchers and developers, anybody working on deep learning, adding more stochasticity stochasticity or randomness into their model, which is really cool because we don't know what's going to happen.

Speaker 2:          00:22:53       Variational auto encoders are a great example of that. Okay. So two more or one more question. Actually, I've got a bunch of questions so I'm just going to lightning answer them. Uh, how to choose optimal batch size to train a and n, um, grid search. Alvaro Garcia, how did you choose your hyper PR? How did you choose your hyper parameters? Uh, they have worked. So this is a good rule of thumb if you don't have a learning method for your hyper parameters, use what's worked. And papers beforehand. If you are going to use a learning method, then use grid search. Jessica says, uh, more resources for understanding perceptron weights and biases with and, and, or logic. Uh, Andrew Trask Google him. Great resource for Rune has how you define it in the same question and one more olly asks, what's the best way to use tensorflow for protein prediction model? Quality estimation, uh, protein prediction model, quality estimation. I'm not sure exactly what you mean, but any kind of prediction for some kind of genetic testing. I'm assuming as a classification problem you have some data set, you're trying to classify what type of protein. I assume something is, hopefully you have pre labeled data. If you don't then you went to hand label it with a bunch of people on your team and then run a classifier on it. I would say use a, uh, prob, use a

Speaker 4:          00:24:22       okay.

Speaker 2:          00:24:23       Feed forward network. Uh, that is

Speaker 4:          00:24:27       okay.

Speaker 2:          00:24:27       Youth transfer learning on a feed forward network that has been pretrained on a genetic data and there are several out there. So genetic pretrained transfer network, Google that. Okay. That's it for that. Um, yeah. So back to this. Now what we're going to do is want to keep going. Let's define our activation function. And remember we're doing this by hand. So we've got quite a bit of coats, right? So let's, let's just, let's just keep going. So for activation function, we're going to use sigmoid. Now it's not just sigmoid, we're going to use quite a bit of, uh, stuff.

Speaker 2:          00:25:05       We're going to use another activation function as well called Relu. That's what I meant to say. So we have one divided by, and remember this is always the same per sigmoid and it helps us convert numbers into, say with me probabilities. So this is our squashing. This is, this is what converts are vectors into probabilities. Okay? And then it's going to be a preprocessing step. So preprocessing step. What this does is it's going to take a single game frame as inputs. So I is going to be the game frame and then it's going to convert it into just what we need, which is going to be a, the paddle, the paddles, plural, the ball. And that's it. We don't care about anything else. So we're going to take an image frame, which is that first image that we get and we're going to first crop it. So it's just going to be the part that we need, which contains the paddle. And how did I find these values, these magic numbers? Because they, I mean in general you would do this through trial and error. You would, you would, you would see, you know, you would try out some sub sample, some sub sample of your image, and then display it and see like where am I? And then closing on it, right? So this is the cropping step a. And then once I cropped it, I'm going to say,

Speaker 4:          00:26:25       okay,

Speaker 2:          00:26:26       two, now I'm going to downsample downsample by a factor of two, which means that I only want a subset of what I'm seeing. And then I'm going to say, well, okay, so and remember these are magic numbers that have been looked at beforehand. It's not night. We just just know somehow. But what I'm saying is I'm saying at the pixel value 144 if the pixel value is 144 that is the type of background which is like this orange g background for the sake of the game, it's, it doesn't care about that. So we're going to erase that. So we're going to set it to zero. So that means erase background and then right and this is just something in four games in general that we can do to help our model. If it's not smart enough to do it itself, we can just hand code it, which is what this sample is doing right now.

Speaker 2:          00:27:11       And we're going to say I equals equals one oh nine and this is going to be the other background. So there's, there are two layers of backgrounds here. One on one oh nine I'm sure I made an error, but let me just finish writing up this function and then we'll get to that. When it's not equal to zero, we're going to say one. And so that means that everything else is going to be, so that means paddles and balls. So many balls set to one. Okay, now we're going to return that value as a flattened duray. And how do we do that? Well, we use the rebel function of num Pi. Okay. Which means flatten it into a one dimensional array. Okay. So we've got that.

Speaker 2:          00:27:58       Now it's time. So we're going to keep writing out some helper functions before we actually implement it and talk about what each is doing. Now things will start to get interesting. Okay, we've done the easy stuff, now it's time for the good stuff. That good, good, that good, good. And that in this case is called the discount reward. Since we are optimizing for the short term, right? We are not optimizing for the longterm, we are optimizing for the short term. And the way that we do this is by implementing this strategy called a discount reward. So we're going to uh, receive a set of rewards. Okay. For a bunch of time steps. So our, our agent is learning from every time, every time the ball goes in one direction and it goes past, you know, one of our players than the game resets. Right? And it just keeps going.

Speaker 2:          00:28:43       The score doesn't reset, but the game resets, the ball resets, and we're going to get rewards for every time this happens and it's going to be an array of values and we're going to feed that into this discount reward and what we're gonna do here in the discount reward as we're going to weigh each of those rewards differently and we're going to weigh that the most immediate rewards higher in the later rewards. Okay? And you'll, you'll see what I'm talking about when we implement this. And when I'm done implementing this, I'll answer some questions. So I'm going to say that the discounted reward is going to be Zeros. Okay? So it's going to start off as a matrix of Zeros. Okay. This is our matrix of discount or rewards that all these values have been applied to. And there's a formula for this as well that I'm going to, I'm going to show, let me just, let me actually just show the formula first. So the formula is going to be zero to all. Uh, there's going to be called reinforcement zero to all. And then it's going to be called a discount reward.

Speaker 4:          00:29:53       Okay?

Speaker 2:          00:29:59       Okay. So this count reward formula is a better thing to search. So this is, this is good. You guys can see how I Google things. There we go. Okay, where are we at?

Speaker 2:          00:30:14       Okay, so that, I mean that's one way of looking at it, but you know what I'm, what I'm going to do is I'm going to say let's go to this guy on Kim's get hub. It's got hung Kim and what Hung Kim did is there, there we go. Here it is. So here it is. Great intuitive explanation right here. Okay. Right here we have a discount rate and what we are doing is we are weighing the discounts, we are weighing the rewards that are immediate, higher than later rewards exponentially so it exponentially. So as time steps go forward for every time step, the rewards are exponentially decrease in value in weight and importance to our agent. So we're optimizing for the short term because we don't know what's going to happen in the long term. We, we just care about the now and that's what Pong is all about. The, now it's a very fast paced game and um, right, so we'll start off by initializing a running average or a running variable of the additions. It's going to store the rewards, sums the stuff, the, some of the rewards. And then we're going to say, okay, so for all the values in the range from zero to R. Dot. Size.

Speaker 2:          00:31:35       So for all those rewards, we're going to say if the reward is not equal to zero, then we want to make sure that that running, that, that uh, running addition variable is set to zero. And now it's time to increment the song. So remember it's just a sum. So sigma, that notation, or over here. Now here's the full formula. That was a step by step. Remember Sigma notation, this he think looking thing, it's not scary. It's just a, a notation. It's a, it's a symbol that represents the sum over a set of values. Okay? It's a sum over a set of values in this case. Okay?

Speaker 2:          00:32:18       So now we're going to say, now we're going to do that actual steps. So we have our running that running ad times gamma, which is our, uh, this discount reward factor that that helps us weigh. This is what actually helps us weigh those values in exponentially, uh, uh, decreasing order of importance as time steps, as time moves forward. We're talking about time, we're talking about consciousness and whole bunch of things and we're just, we're just trying to make an agent that beats pong. Where are we right now? So, and when we're done with that, okay. Right. See I can already predict what you guys and say you got to. Yes, I know. Yes. Okay. So then we returned that discounted reward. Okay. So now let me answer some questions cause I'm sure there are some questions we got here are okay. I'll answer some lightening fast.

Speaker 2:          00:33:17       Okay. So palosh asks, can we have some visualizations on tensor board for this short answer? No time. But I will do that in a future livestream stream probably for sports predictions, which I know you guys want. So Rod, will you make a video about differentiable neural computers said Mona Monarch, you know, you know me. You know, that's what I'm all about. Yes, I will. You know what I'm all about. That's coming soon. I'll actually ask what does short term and longterm mean here? So short term means what is the, so whenever we, uh, make, uh, whenever we get our ball to go past the agent, if we get a plus one reward, that's a short term reward. But if we continually hit the ball and then 30 minutes later, uh, there's a reward. Like we're always getting a set of rewards, right? No matter what. But let's just say we have a reward 30 minutes from now.

Speaker 2:          00:34:08       Can we trace that reward? Like how we got that reward all the way back to actions that we took 30 minutes ago should, should we, that's the question. Should we trace it back? This does what we do, you know, 40 times steps beforehand with all these rewards happening matter for the 40 times steps later for a, that happens way later in the future in this case. No, they, it has nothing to do with that, that it's, it's not like incrementally our agent are the AI is adapting to what we've done before. It's just the same, you know, uh, opponent. So short term means did the ball go past the agent or not? Okay. One more question.

Speaker 2:          00:34:47       Uh, I have, so Benjamin has, can you walk through line zero through 25 there was some sound problems in that range. Sure. So high level and we walked through this. Okay. So we import it three libraries, matrix math, save, load, our model, run our game environment, hyper parameters here are hidden state size, how, how, how many episodes in a batch. And we have several batches. What's our learning rate convergence lower high to make it perfect gamma to help us, uh, way rewards that are happening right now. Higher than rewards that happen in the future. Because we are optimizing for the short term decay rate, which we're going to, we're going to talk about that resume. Should we load our model now? Should we load our model from beforehand or do it now? Then we initialize our model 80 by 80 pixel a matrix, load our model now from a saved state or lower Amato from a safe state or load it right now using Xavier initialization, which they special smarter initialization than just normal using the hidden state and the input dimentionality as a inputs define our activation standard sigmoid to squash values into probabilities.

Speaker 2:          00:35:58       Preprocessed the data, the, the, the frame. So it's just the paddle and the ball. And then I just talked about this. Okay, one more question. What kind of network to use in NLP? LSTM

Speaker 1:          00:36:11       okay.

Speaker 2:          00:36:12       Short answer. Yes. LSTM networks are great for text generation. They're great for tax classification. Anything with text l s t m s outperformed almost everything else almost all the time. Okay. Okay. So now we're going to do uh, two more helper functions and then we're going to get right into it. And they are for propagation and backward propagation. So you're going to get to see what this looks like without TensorFlow's. Um, you know, magic, the magic of tensorflow. Okay? So by the way, Pi Torch tenser flow, you guys got, you guys got to, you guys got to keep your game up because Pi Torch is coming at you. You know what I'm saying? Y'All Jala Koons talking about it. The community is, is on that pie. Torch came, I'm even about to make a video on that pie torch in five minutes. So spoiler alert. Okay, so for propagation, the first thing we're gonna do is we're going to get that x value. That is our input. That is our game, uh, value. That is our, what is that? That is the

Speaker 2:          00:37:15       pixels. The pixels for the game. Okay? So we're going to say, okay, so the first thing we're going to do is we're gonna calculate the hidden states using just as a dot product. Remember, it's just a bunch of linear Algebra. It's just a matrix multiplication. The dot product multiplies to weight matrices together. So we have that, um, first weight matrix that we defined and we're multiplying it by what's input. Okay? And that's going to give us our hidden state, which is a vector of values. Okay. And then once we have that, what we're gonna do is we're going to squash it with our first non linearity, which is going to be relu and Relu is, what is it saying? It's saying take the Max between a zero and then what this value is. So if the value is less than zero, then it's going to be zero.

Speaker 2:          00:38:04       If it's greater than zero, then it's going to be that value. And uh, we do this because Relu is actually a, is a, is one of them, one of the most used activation functions for midway through a network where sigmoid is used for the end of the network to, to convert. Whenever we have into probabilities for classification or whatever else they use, case is going to be, so once we have that value squashed, we're going to say, let's take the log probability, which we're calling here. It's not actually the log, we're just calling it that because we're going to use that in a second. And we're going to take the dot product of the next set of weight matrices times what we have here, which is the hidden state. So this is the second layer. And once we have that, we're, we're ready to output those probabilities to what are we going to use?

Speaker 2:          00:38:49       We're gonna use our sigmoid function. Okay. And that's going to output the probabilities for up or down or stay the same. And then we're going to sample from those probabilities and decide what we want to do. And then you that value the policy to get the gradient values by taking the partial derivatives with respect to each of those weights as we back propagate. And we're going to do that in a second. Okay. And now we return p and, h we got, we got 15, and we got, you know, we've got 19 minutes. I'm, I'm, I'm on this, we're, we're gonna, we're gonna finish this. We're going to return the probability, okay. Of taking action too. And Hidden States it state. Okay. Get away from me. There we go. I just talked to some code. Now we are going to talk about policy backward. Okay. So,

Speaker 4:          00:39:49       okay,

Speaker 2:          00:39:51       now it's time to talk about policy backward. Let me answer one intermediate question just because I feel like there will be one. Uh, no, there's not. Okay, great. But let me see what people were saying over here. Hi, everybody's chilling over here.

Speaker 4:          00:40:03       Okay, great.

Speaker 2:          00:40:05       Okay.

Speaker 4:          00:40:08       Where were we?

Speaker 2:          00:40:11       We'll probably go over five minutes is because the audio errors. Okay. So now what we're gonna do is we're going to recursively compute air derivatives for both of those layers. This is the chain rule programmatically. Okay? So this value right here, e p d log p is going to modulate the gradient with advantage. So we'll talk about that. Um, but basically what we're doing as we're computing the update derivative with respect to wait two to start off with. Okay. So this was all going to make sense once we start coding to sal and then actually implementing it. But so we have our dop product here and the dot product says, let's take the transpose of the, and remember, uh, eph is array of intermediate hidden state. So we're feeding it back on a ray of intermediate hidden states for every time for every, um, successful. Every time the ball goes past the agent, that's considered a a game. And then we have several games in an episode. And we're taking the hidden states from all of those that we, that we are going to calculate and we're going to say this is going to give us the derivative with respect to wait to tell and we're gonna use these to update our weights. We're just calculating derivatives here and then now we're going to compute the derivative of the hidden states.

Speaker 4:          00:41:40       Yeah,

Speaker 2:          00:41:41       using the model and then we're going to apply our activation to it. And I'll answer questions right when I finished writing the south equal zero k which is relu in this case and p. Dot. Hauder product taking the outer multiplication. We're doing an outer product of these two values and hold on a second. No, I'm getting deep. H eph lessening. Okay, great. That's what I'm talking about. And then DW one, now we're going to compute the derivative with respect to weight one and using hidden states transpose and the input observation. And then we finally returned both derivatives to update weights. Okay. Return both of these derivatives to update the weights. Great thing about key value pairs. We can just assign them just like this in in one line. And it's great because we only have two layers. Right. Okay.

Speaker 4:          00:42:56       Okay.

Speaker 2:          00:42:59       And now we're going to get started with the code. But let me answer two questions cause there's inevitably going to be some, is it possible to create an opening I environment that is not a game. Absolutely. That's what universe is great for and that's where they want to move to. So they'll have some predefined task whether that be sending emails or sending or clicking through some, I dunno, some bullshit like that. So then we have, is it, is it possible to solving yeah. Solving the, okay. I need to rephrase these questions cause they're not written out correctly possible to solve.

Speaker 4:          00:43:36       Okay.

Speaker 2:          00:43:37       Is it possible to solve the longest common sub sequence problem using machine learning? Yes. Then how, okay. Uh, so if it's a sequence, you want to use a recurrent network because it's a time series and because they're just numeric, it's just numerical data. You wouldn't want to use anything fancy like LSTs or gr use. You just use a standard RNN two more questions. What is dif? The difference in handling data that is one d or Two d with something that is tendi. Great question. So, um, right, so 10 days where think about dimensions as features. So anytime you have 10 features, those are 10 dimensions. Uh, you would want to, if you're using deep learning, you could just feed them all into the model just like that. Because deep networks operate on multidimensional data. However, if you're using a standard linear, a model like linear regression, you went to squash that dimentionality using dimensionality reduction technique like principal component analysis or a t.

Speaker 2:          00:44:35       S. N. E. There's a whole theory of machine learning around that. One more question. Why are you finding to derivatives? We are finding too, we are finding uh, so backpropagation and have a great video on backpropagation called backpropagation in five minutes. Check it out by the way. But W backpropagation is essentially recursively computing derivatives with respect to weights at each layer. And the reason that we say it's using the chain rule is because we are taking the values at every layer and using those values to compute that the next set of derivatives, partial derivatives which equals the gradients, Parson derivative, equal gradient, same thing, and then we're using those values that we've updated across the network to then update our weights.

Speaker 4:          00:45:21       Yeah.

Speaker 2:          00:45:22       One more question actually. Is it possible to see some [inaudible] plots in real time? That's a great question. I don't have one but that would be a great project to do. So let's get started with the code now, right? I mean the actual implementation details because we are, we've implemented our help, our libraries. Now we're going to get the game started and this is what Jim is good for. It's also good to get swollen but not that's not this. That's something else. And we're going to say, boy, we're going to take an observation from the environment and this is why I love open AI guys, because all we have to do is run this one function reset and we get the observation. And I'm not saying that I'm all about simplicity for the sake of it. I like hard coding things, but sometimes when it comes to you pixel values and some games state that I'd never seen before, I don't want to have to worry about hand coding this reset method where, how do I extract an observation, the Pixel from a game say I just want to focus on the algorithm. And that's why open AI does what they do. Do what they do. That's why they do what they do. Nike. Okay. So, oh, okay. Where were we? Observation. I'm, I'm, I'm uh, I'm good. So, oh, I don't know why I'm laughing. I'm laughing at myself. So what were you going to do is we're going to say

Speaker 2:          00:46:50       what we want to calculate motion, right? And we can't calculate motion if frames, our statics frames are static. We want to calculate motion of the ball. And then, and then improve our network based on the motion of the ball. And how do we do that if frames or static or we're going to calculate the difference between two frames. And that difference is what we feed in. So here's how we calculate the difference. Um, we'll do that in a second, but right now we're just defining this previous ex to a store that previous value of the difference. Okay? So now what we're going to do is we're going to

Speaker 2:          00:47:25       import some variables to intermediate variables to store the observations, the hidden state, the gradient, and the reward. So that's what they, that's what they do, okay? And then we have our current reward. So what is a running reward where we're at? We're just defining some variables right now before we, you know, these aren't hyper parameters, they're just, they're there because they don't affect our models. Architecture. They're used for, um, computing the policy, the reward, not the actual network itself. And then where are we? The episode number? Okay? So let's begin the training. So began, began training is training time, okay, let's do this. So for training time we're going to first uh, preprocessed the observation. So get a image, get frame difference. I'll just say that get framed difference. So we're gonna use that. So we're going to calculate this current value of x using the previous value later, but right now we're going to use that and now we're going to use that method, that preprocessed method that we defined beforehand given an observation and that's going to make sure that it takes out all the, like I said before, the stuff that we don't need and then we're going to say, well the current value minus, and this is a different step, we can literally just say minus.

Speaker 2:          00:48:56       When I say difference, I mean it minus the previous value. If the previous value is not none else, we're going to say get those Zeros, initialize it as an empty matrix and then set the previous to the current value. It's kind of like a, it's kind of like a a data structure in and of itself set the previous to the current and current previous. You guys know what I'm talking about. Algorithms class when a one, so x is going to be our image difference. We feed it right in. Okay. The the X, not previous ex occurrent x, but x. Okay, so now let's do some forward propagation forward onward to a glorious future, which is what we're doing. Okay, so we're going to say use that policy forward method to remember feeding. Remember feeding that x value and it's going to give us an output, a probability as well as they hidden states. And we're going to sample from that Paul probability and a yes, it's a, okay. It's going to, we're going to sample from that probability to then update our network with backpropagation and let me answer one question. Okay, so the question is

Speaker 2:          00:50:21       can we create a neural network for changing the code in a PHP file automatically? If so, will this, the pattern of the code changed in the past, changing the code automatically for what purpose? To optimize it? Yes. If you have a label for the code, if you have some thinking about it as a labeled a supervised classification problem, if you have labeled good code, you need a lot of it too as good or bad. I think of it as binary.

Speaker 2:          00:50:49       Yeah. Okay. So now this is this the classic part. Okay, so this is the Catholic part in this case is going to be, let's say we have an action and we're going to say we're going to sample the action from a uniform, a uniform distribution, uh, which is a, and then we're going to say it's less than the probability else. Three. It's essentially like rolling dice. Okay. So what we're doing, here's where we're saying since it's not a part of the model, the model is going to be easily differentiable the stochastic part, the model itself isn't the cast stick, it's deterministic, but this part, the output value, that sampling of of, of rewards, it's going to be to castic. We're going to randomly pick from them. Now if this to cast, this city was in the model, we have to do a re parameterization technique, which is super cool. And I'm making a video on that, but it's not right now. Yeah. Which is super cool. We're starting to look at deep learning through a Basie and Lens. That is where the field is moving. More unpredictability, more randomness, a less determinism as we move forward. Okay, so now what we're gonna do is we're going to append this d log variable with a minus the eight y minus the high probability. So it's a gradient that encourages the action that was, that was taken to be taken. Uh, so

Speaker 4:          00:52:17       okay,

Speaker 2:          00:52:18       let's keep moving forward. Now we're going to step the environment and get the new measurements, step the environment. So we're going to render it. And this step shows the game constantly. While true. It's just going to continuously show the game and then we're going to say, get the reward, get the observation, get the done, get the info, and then step. So what? No, this is the standard open AI format. Whether it's universal gym, you get the observation, which is what you've see the reward plus or minus the binary value or zero done. Which means did the, the did a reward happened at the game finish for an iteration. Like there's the ball go forward or whatever it is. And then info was just for logistics like debugging purposes, but always do say step and that takes an action in whatever game state it is.

Speaker 4:          00:53:05       Okay. And uh,

Speaker 2:          00:53:13       okay. So then we're going to take the reward some and then we're going to add the reward that we got from it to it. So the reward is going to store the total all the rewards that we got and we're going to use that to uh, to back propagate in a second. Okay. And so then,

Speaker 4:          00:53:36       okay,

Speaker 2:          00:53:37       now we're going to record the reward. It has to be done after we call step to get the reward for the previous action. Okay? And now we're going to say if we're done, if an episode has finished, then increment that episode number. And this is just our iterable the variable that we're iterating through to get that to, to keep track of where we are. And now we're going to stack inputs, hidden states, actions, gradients, rewards. For this episode, we're going to stack every single value that we have calculated, uh, so that we can later compute a discounted reward backwards through time. So remember, this is a, uh, a bunch of, uh,

Speaker 4:          00:54:26       okay.

Speaker 2:          00:54:27       Games are happening in an episode, a bunch of, uh, plus one scores we could think about it are happening in each episode. So we'll say if we were to say, let's stack all the observations and now let's stack all the hidden states and this function, just like to stack them all them all in and separate arrays, um, those lists or as stacks if we want to be. Yeah. Stacks. Uh, and then these are all the hidden states that we've computed. And then all the gradients, we're just, we're just, we're just some summing them all together so that we can use them in a second and all of them are within an episode. Okay. And then we say, be stack dogs.

Speaker 4:          00:55:12       Okay.

Speaker 2:          00:55:13       These are all of our gradients. And then we have all of our rewards. And then we have, we're going to reset the array memory. Uh, now that we have all of those in our respective stacks, okay, now that we have all of those in those respective stacks, we can move forward and now we're going to compute the discounted reward. Okay? So this counted for war computation. So,

Speaker 2:          00:55:41       uh, we have our discounted award that we're going to store in this variable EPR, okay? And that, that is what we've just written out, that function. And once we have that, we're going to standardize that reward. So it's going to be, uh, to be unit normal, which means it's going to help control the gradient estimators variance, which, okay, so let me explain that once I write that out. So we're going to say minus equals the mean. So that's the average value from all of those discounted rewards. So when average, once we, once we've applied this discount function to all of those rewards, we'll average them all together. Um, and that's what these two lines do. So both, so, right? That's what these two lines in total do some total. And we're gonna use this discount, a reward to then compute the, uh, gradient values. So we're going to say

Speaker 4:          00:56:35       P P D

Speaker 2:          00:56:39       can we go 10 minutes over actually, is that cool? Yeah, we'll, we'll, we'll go 10 minutes over cause we actually have quite a bit to to finish here and I want to own and make sure that I talk about this. I know I'm kind of blowing through this part right now and I'm going to go over it. So just so just hold tight. Um, so actually what I'm going to do here is I'm going to paste the sand because I want to explain all of this that's happening here. So my code is on the get hub by the way. I am, I get up. So let me just paste this part in so I can explain the rest of this and make sure that we get it all because there's a lot going on here.

Speaker 4:          00:57:12       Okay? Okay. Where are we to Todd, to Todd.

Speaker 2:          00:57:21       And before I started explaining it, I'll answer one question. Okay. So

Speaker 4:          00:57:27       okay. But that, that, that did that

Speaker 2:          00:57:31       boring bookkeeping and all that stuff. So what we're going to do is, let me save that. Let me answer your question. Question is, how did you converge Vivec ask, how did you converge on the steps for preprocessing, like taking difference of pixel values versus true states or feeding two consecutive frames? Oh, so I didn't converge. Those are, those are hand coded a pixel states that like if we were to just guess what the, so if we were to sample those are pixel values, four colors. So we were to say 404140414041 44 would be the RGB value for that orange background that we then remove because we don't actually care about that. We care about the ball and we care about the paddles. So let's talk about what I'm doing here. So,

Speaker 4:          00:58:22       uh,

Speaker 2:          00:58:25       let's me say this. Okay. I'm just gonna start from the beginning. Let's just start from the beginning and then we'll go forward from there. Okay. Cause there's a lot here.

Speaker 4:          00:58:37       Okay.

Speaker 2:          00:58:38       Okay. So from the beginning, okay, we've got our libraries hyper per ams. Um, let's see. What's a good part to start off right here. So let's say, let's say we're ready to be pong, right? Where we want to be. Pong. We've got these values and we want to beat Pong. Okay. So we're going to initialize our model as an 80 by 80 pixel matrix of values. Okay. And then we initialize our weights a pseudo randomly or not suitor randomly. The correct term for this would be s s smarter than random but close to random because they're random. But there were using these predefined values as bounding values between our hidden state in our input dimentionality for both of our weights. And then we're initializing buffers for our gradient and our rms prop this, these are for backpropagation. So these are four backpropagation, the to store values, intermediate values because backpropagation is a process and he's going to store values for both our gradients and the computations from uh,

Speaker 2:          00:59:41       from compute, from the actual rms proper equation, which I'm going to show the image of in a second. We have our activation function sigmoid, our preprocessing step, our discount reward. And so our discount reward is going to weigh the rewards that we get earlier higher than later rewards because we are optimizing for short term rewards because Paul is a very fast paced game. Then recomputing, r p r a them are competing for propagation, which is essentially, remember it's a chain of matrix multiplication. It's, it's linear Algebra, inputs, times, layer times, layer times, layer times layer. And all of those outputs are then fed into the next layers and this is the chain of operations to get that initial, that final value, those, those final sets of weights that we could then uh, uh, get the policy from, which is the reward. We're going to get an output probability, which is going to be either up or down. Let me show a great image of this. An image would help here. Um, so deep a policy gradients.

Speaker 1:          01:00:51       It's a great one.

Speaker 2:          01:00:53       What's a great image? So this is a great image. Okay, hold on. So where are we?

Speaker 2:          01:01:08       Okay, cool image right here. So this is what the network is doing in her forward pass. We are going to output a set of log problem of we could say log probabilities. Actually these values can be anything. They can be 10, it could be a million. But all that matters is if it's positive or negative, it doesn't actually matter what the value is, the magnitude of that matters. But we're s we're going to output a set of probabilities and those probabilities are going to say there's a probability that we should move up versus a problem probability that we should move down. And then we're going to sample randomly from that probability distribution and say, should we go up or down and then use that, that sample like, okay, let's say best probability is to go up. That's our action. We're going to then implement that action and then get a reward value from the game states.

Speaker 2:          01:01:56       Remember that environment step function is gonna Return or reward value. And then we're going to use that reward value to then update our, uh, our, our network by using those as our gradients. Okay? So we're going to use those reward values to then update our weights recursively as we back propagate. So that's why it's called policy gradients. They're not gradients from, uh, some, it's not because it's not a labeled supervise problem. They are grains that we received from the reward value from the reward that we, we get from our policy. That is our, should we take, should we take a, uh, an action here or not and what action should we take? What is the policy in this environment and how should we then use the rewards that we receive to update our network? What is the gradient? Remember gradients give us a direction to update our network during backpropagation. Okay. So that's a great image for that. And then, uh, we're going to say, let's initializing variables to get started with this. And then we're going to say, okay, let's begin the training. We've got the difference between our frames.

Speaker 1:          01:03:06       Yeah.

Speaker 2:          01:03:07       Okay. Because we want to detect motion, which is going to be an ex. We feed that x value, which is a matrix of pixels into our foot policy forward a function, which is then going to output a probability of either moving up or down. It's, it's going to output both. And then we're, we are tasked then with choosing which a probability, uh, to then pick. It's not gonna just going to output one probability. It's going to say a probability for up and down. And then we have to pick which one. And then the, and this is how we pick, it's the cast Stickley randomly. And then we then, and then this is going to get better, not the random part, but the actual outputs, uh, like we're going to randomly pick, okay, this one, this, this is a good probability, right? And then when we update our network, the, the magnitude of those probabilities in, in either direction for either up or down will increase as to maximize that short term reward.

Speaker 2:          01:04:00       So it doesn't matter if we're randomly picking from them because the magnitude in a certain direction has increased. So then no matter what your sampling, what, no matter if you're sampling from them randomly or deterministically, you're still going to pick the one that is the, that is the most likely. And then we stepped through the environment. We say, okay, what is our award value? And then we say, let's begin, uh, the actual episode. And we're stacking all the inputs in the hidden states, in the actions, all the values that were computing into Iras. Because we're going to use all of them. Uh, were we want to use all of them and we want to, it's kind of like work boxing them all together and into one state, one Maddis state so that we could then, uh, back propagate our network and then we're computing the discounted reward.

Speaker 2:          01:04:48       Remember optimizing for the short term, using those values, um, or sorry, using the a gradiance values and then we're competing the advantage. So let me, so this is the part that I didn't explain yet, right? So the advantage is the quantity that describes how good and action is compared to the average of all the actions. Okay. So the, so this is our grading, right? EPD Log. Pete is our gradients and discounted EPR is going to act as our advantage because it's giving an advantage to the one to the rewards that are short term. So the way we do that, it's just simple matrix, matrix multiplication. Okay. And then, and then once we have that, where we've, we've, we've, uh, added that reward that discounted wards not added, but multiplied the discounted war two, our probability we can then backward propagate. We've, we've taken what we've, uh, our reward from our action in the environment and now we're applying it to that backwards policy to get our gradient value.

Speaker 2:          01:05:50       And then, uh, we're gonna get to the final gradients that we are going to have good to use to update our weights using gradient descent. And so this is the rms prop step and you can see here that there are some weird a matrix. There's some weird math operations here, like w the decay rate times the cash of the RMS prop plus one minus the decay rate times the gradient buffer squared. Well guess what, what this is, is it is the formula for rms prop. Now there are a bunch of formulas for gradient descent and all of them are different and all of them have different, different use cases. But rms prop is stochastic gradient descent. And I should do a video on stochastic gradient descent alone. But this is the formula and what we are doing here. And then you can see here, one minus this term times, uh, g squared.

Speaker 2:          01:06:39       We just implemented this formula and it stays the same programmatically. That's what we did and rms prop is, is, is, is, is this set of equations and it's a, it's a relatively, I know it doesn't look simple here because we have to think about this in terms of these mathematical terms as well, but it's a simple equation and I actually prefer looking at it programmatically to the actual equation with, with all of those terms, it's actually more intuitive this way because I look at code most of the time anyway and you probably do as well as a developer. So, uh, we get that gradient value and then we use the decay rate to only update those values that, um, that the decay rate, make sure that as we back propagate over time, uh, it's the values that we, uh, update closer to now are more important than the values that we update later on.

Speaker 2:          01:07:36       And the decay rate is a mathematical representation of that and it relates to the discounted reward. And then we take our model and we update it using the learning rate times that value g and then we reset the bat. And this is our resetting step once we'd done that. But this step right here is the, is the key step of us updating our models. Wait values. This is the key step right here using our learning rate and what we've calculated with RMS prop. And this is book bookkeeping of just, you know, putting those values and then, uh, how putting them, okay. So, uh, I'm going to really comment this. Get hub read me when I, um,

Speaker 2:          01:08:16       when I do, when I, when I do it, but when I, when I do it, when I do it, okay. So look at it. It's cool. It's fun. It's, it's, it's something that is fun to look at. Run this on the cloud. Okay. If you, if you want to actually see it get better or you could run it on your Mac book for three days or whatever, if that's fine too. But, but uh, but an image on Amazon machine image a bit bucket has a good one. Um, you'd asked the actually has one as well that is pre compiled for you where you don't have to install the dependencies. You can just, you know, run your code directly in a Jupiter notebook in the browser. That'd be great. And I should actually do a video on that and that's coming up soon. But this is dope.

Speaker 2:          01:08:56       Okay. And this was a lot to take in. Okay. So don't worry if you didn't understand every single thing. We just did something pretty complicated. We computed backpropagation for propagation, a grading updates policy, uh, and a policy by hand. And this is, and just to, just for learning purposes and it's awesome. And we're going to do more stuff like this and then later on. Okay. So we're going, I'm going to start to wrap up with two more questions, which is a library to read raw pixels. Um, Oh shit, right? Uh, which is the library to read the raw pixels. Um, what was it? In this case it was

Speaker 2:          01:09:36       boom observation. Open Ai is going to give you the raw pixels. And then one more question. Is the input of our model a frame or a sequence of frames video? I mean, will the NMB able to perceive the ball moving or will decide based on a single image? It is the difference between two frames. That difference is that is, it's how we capture motion and we continuously feed, um, the difference between frames. Okay. So that's it for this live session. Thank you guys for showing up. I love you guys. And for now I've got to, uh, stochastically determine my life, which is a paradox, which is also my life. So thanks for watching.