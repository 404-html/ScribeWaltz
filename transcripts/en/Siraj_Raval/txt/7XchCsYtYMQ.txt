Speaker 1:          00:00          Hello world, it's Saroj and there's some serious hyper lately about this Ai Algorithm called deep fakes that's able to swap the face of anyone in a video with anybody else. Some of the examples I've seen are amazingly realistic. I'm going to explain how this algorithm works through theory and code and then talk a bit about the moral implications of this. At the end of the video last month, a redditor by the name of deep fakes of course was making a name for himself by posting, convincing, let's say, not safe for work videos that swapped the face of the person in the original video with a famous celebrity like say Taylor Swift. He did this by training his deep learning algorithm on some publicly available videos with just his home computer and in the past few weeks, the practice of doing this has exploded. There's now an entire subreddit and get hub community dedicated to maintaining this and sharing the results of this algorithm.

Speaker 1:          01:02          The subreddit grew massively in size and there's even this desktop application called fake app that lets anyone recreate one of these videos with their own dataset. No knowledge of programming necessary because of this. People are putting Nicholas Cage's face on literally everything and making a meme out of it because it's the internet. So how does the deep fake algorithm work? Let's say we've got this video of Harrison Ford in Indiana Jones and we want to replace his face with Nicholas Cage because why the hell not our first step is going to be to gather some training data, specifically images of both Harrison Ford and Nicholas cage to later train our models on possible sources of these images. Could include Google duck, duck, go or being image search. Luckily the face swap repository has scripts to automatically download large amounts of images from one of these sources to our home directory.

Speaker 1:          02:01          Once we've got a couple hundred images, we can place them each in their respective folder. Notice though that these images are full of stuff not related to our characters. There are all sorts of objects and distractions in the environments of these images that have nothing to do with our characters. We're going to want to perform some face detection on each of these images. Pretty much all cameras that have been created in the past decade have some sort of real time based detection algorithm. The open computer vision or open CV library will allow us to easily recognize faces in images. We just a function call, but the way it's doing this in the background is by using a method called histogram of oriented gradients or hog for short. It starts by making our image black and white. To simplify it, we don't need color data to find faces. Then it looks at every single pixel in the image on one at a time.

Speaker 1:          03:00          For every pixel it looks at the pixels directly surrounding it. The goal is to figure out how dark the current Pixel is compared to the pixels directly surrounding it. Then it draws an arrow showing in which direction the image is getting darker. Once it does this process for every single pixel in the image, we'll end up with every pixel being replaced by an arrow. These arrows show the flow from light to dark across the entire image. If we analyze pixels directly, then really dark images and really light images of the same person would have totally different pixel values. But because we're only considering the direction that brightness changes, both types of images end up with the same representation. Making the problem easier to solve. Saving all these directions is to space intensive, so we break the image into smaller squares and count how many different directions there are.

Speaker 1:          03:57          Then replace each square with the direction that has the most counts. This turns the original image into a very simple representation that captures the basic structure of a face in a simple way. Once open CV has a representation, it can compare it by a distance metric to another hog face pattern generated from lots of face images and if it's close to it by some threshold value, we can consider the face the tech to once we've got our face detected using open CV, we can just crop the original image so it's just the face and save that instead. Soon we'll have pure face datasets for both of our characters. Now we want to train our model on this Dataset so it learns to encode these faces, meaning tell them apart from one another. There are a bunch of measurements that we could take of a face to try and tell what it looks like.

Speaker 1:          04:48          Ear size knows length, et Cetera, but it turns out that it's better to have our algorithm learn these things for itself. Convolutional neural networks have been shown to offer really great performance in learning to classify faces. It's a neural network with its own distinct set of operations like convolutions and pooling. These operations amount to simple matrix math applied to an input image. The output being a classification, but we don't simply want to classify an image. We want to learn a representation of both faces and somehow morph face a to look like face B in the most realistic way possible. So what we'll use is an auto encoder. This is a convolutional network that tries to reconstruct the input image. This lets it learn a lower dimensional representation of the input image, which will later use to swap basis. Specifically deep fakes uses one encoder and two decoders during training.

Speaker 1:          05:46          It actually trains two networks. Both the networks share an encoder but have different decoders on Incode or transforms an image into a base vector. This is a set of numbers which identify the important features of the face. The decoder transforms that vector back to an image. There is an error function which measures how good the transformation was and the model tries to lower the overall error during training. The first network is only trained on image a and the second network is only trained on image be the encoder learns how to convert an image into a face vector decoder a learns how to convert a base factor to image a and code or B, wanting to try to convert a base factor to image B. So during training we're feeding both images to the same encoder, but using two different decoders for each. After the network is done training, we can feed it a video.

Speaker 1:          06:42          A video is simply a collection of image frames, one by one. We'll first crop out the target face from the video frame, then perform a face swap on it. That means feeding image a to the encoder, creating a base vector, then feeding it to decoder, be resulting in a face that looks like B, which we can then overlay onto the original frame. Then we can concatenate all of those images together and watch the video result for ourselves soon. This technology will get more realistic and more accessible and anyone will be able to make it look like you are doing something you never really did via video, which could be degrading. But this can also be a good thing. Anyone will be able to make all sorts of entertaining content that wouldn't be possible before easily. It's gonna get harder to distinguish what's real and what's fake as AI improves.

Speaker 1:          07:36          And we'll need to work on more deterministic algorithms that offer computational proofs to help us trust sources more. Enter blockchain and cryptography. Three things to remember from this video. We can use open CV to perform facial detection using the histogram of oriented gradients. Approach convolutional networks can learn features from images and auto encoders can encode representations that we can later use to more for one image into another. Last week's coding challenge. Winner is Ivan [inaudible], who used an auto encoder to convert DNA sequences into RNA sequences. This is such a cool application. Great job, Ivan. And the runner up is param deep sing Oberoi, who created a well documented auto encoder on the M and ist Dataset. You already know what this week's coding challenge is. Generate a face, swap yourself and post a link to your github repo in the youtube comments and winners will be announced next week. Please subscribe for more programming videos. And for now, I've got to prepare myself to get baseball, so thanks for watching.