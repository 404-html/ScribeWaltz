Speaker 1:          00:00          To one. Just starting.

Speaker 2:          00:14          All right. We are live hello world. It's to Raj and welcome to this live stream. I'm so excited to be here with uh, with my new live stream set up. Uh, in this video we're going to be predicting stock prices. So financial forecasting using tensorflow dot. Js. Um, in this, so what you're seeing right here is a graph of us predicting stock prices using what's called a recurrent neural network. But the actual aim of this video is to build, um, a financial forecasting model using a convolutional network. So we're going to be looking at both. So I'm actually going to code out a convolutional network, um, and the second half of the video, but in the first half we're going to talk through some math. We're going to talk through the different models out there for financial forecasting. And I'm going to also answer some audience questions because what is a live stream without some audience questions, right?

Speaker 1:          00:59          So, um,

Speaker 2:          01:02          hello everybody. Yes, good to see so many people here. Um, I'm going to just like sporadically be answering questions in five minute intervals, but really it's, it's got to be related to the topic, right? Um, and also in terms of a demo, I've got a great demo for us here in terms of, uh, you know, an, an actual visual demo. What this is, is a nice little web interface that I'm running off of local host. And what this is doing is it's using a neural network to make predictions based on just simple binary data, right? So this is just an example of us being able to visualize what it looks like, right? So what we see here are the actual labels, right? The outputs zero one zero one. And we have our predictions which are, which are right here inside of this tensor. Um, and they look very similar and they're going to get better and better over time, right? So that's, that's the visual demo, but we're going to be predicting apple stock. Okay. So that's, that's the, that's the actual aim of this video. All right. So I want to start off by just answering two questions and then we're going to go right into the, uh, into the code. All right, so the question one is, should I do a masters from the USA, uh, or weights to earn some money here and go for a phd later? Um, okay, so I think

Speaker 2:          02:11          first of all, if you're going to do a masters or a phd, it has to be at a really good school, right? It's gotta be at like a top 10 school. And if you're not going to go to a top 10 school, you might as well just do this yourself using the internet as your university. I mean, look at the most popular research institution in the world, right? Deep Mind, if you look at their jobs page, they don't even have phd listed as a requirement, right? What a phd does is it does, it doesn't just say, you know, you are the greatest. What it's, what it really says is you have published actual research under the advisory of a professor or someone who is more knowledgeable. You can do all of that with the Internet. You can find an advisor online, you can find educational resources online. Right? So, so that's my thought on that. And one more question before we get into the code.

Speaker 1:          02:55          MMM.

Speaker 2:          02:57          Why Not Python? Great. So guys, javascript is really coming up in the machine learning space right now. Right now you can build a model in python important to javascript and vice versa. And uh, yeah, so javascript is really becoming easier and easier to use for machine learning models. And you're going to also start to see more developers build models in javascript. But most of the complex models are built in python, right? So if I want to see someone build Alpha, go in Javascript, and once that happens and it will happen, then I think the playing field will be a lot more, you know, level. Anyway, back to this, to the two of them are right? So time series data is starting to play a larger and larger role in our world, right? So examples in crude examples include self driving. Tesla is right. So Teslas are constantly creating time series data right there, mapping out to where they are.

Speaker 2:          03:50          And then these are three d point clouds. And then over time, this, this comes out to be a time series, right? So by the way, at time series is a sequence of data points, right? Measuring the same thing over time. This could be three d data points through a three d map points for Teslas. This can be stock prices, this could be smart homes, measuring temperature changes over time. This could be open data, publishing a police departments of crimes happening at certain times. So time, time is the keyword here. As data changes over time, that measurement is a time series. So imagine sensors collecting data from three settings, right? Do you have a factory? You have a city and you have a farm. And so it's measuring temperature. It's a measuring the number of people in a room. It's measuring, you know, the output of a factory.

Speaker 2:          04:34          All of these are static, um, data points that change over time. And because time is involved, it's a time series. Exactly. Cool. Right? So when it comes to financial forecasting or Rema is like the Goto model, right? So, uh, Rema is called is the Goto model. Um, and it stands for auto regressive. Um, it's a, it's an auto regressive integrated moving average. So the first question here is, is a Rhema, a machine learning model? Our rema I you can consider it a regression model. It is, it is a regression model. Is it a machine learning model? I would say no because it was handpicked for a specific use case stock prediction. Um, but you can't really use it for things outside of that. Right? It was handpicked and hand created for a specific use case. Whereas a model, a machine learning model, like say a support vector machine is made for a more general use case.

Speaker 2:          05:31          You can use support vector machines to classified cat faces and you can use support vector machines, the classify stock prices, you know, anything really. But a Rhema, you know, have a little algorithmic outline of what our Rima looks like here. But what it comes down to is us approximating three different variables. The first variable is the number of auto regressive terms. The second variable is the non seasonal differences needed for stationarity. And the third variable is the lag forecast errors in prediction equations. So all three of these variables have their own associated equations. So we're calculating all three. And so it turns out that this has been the most popular model, uh, for, for stock price forecasting. But I'm not going to go into that because with the advent of deep learning technology, very recently, we can, we can outperform models like that and there's reasons and there's reason to believe we can because there's a bunch of machine learning models out there, right?

Speaker 2:          06:28          But neural networks when given a lot of data and a lot of computing power tend to outperform all of them most of the time. So why wouldn't they, when it comes to financial forecasting and you might be thinking, well, why don't I hear more about this? Because big banks and you know, hedge funds, of course, they're not going to give away these models, right? That this is not an open source, uh, industry, right? Finance is not an open source industry. We'll talk about bitcoin and all of that later. But like in general, traditional finance. So these are very close source models. Why am I making a video on this? Because I want you guys to win and I don't care. I, I'm, I'm, it's, it's not about, you know, that for me it's about democratizing AI. So this is just for me, this is like, I don't even care about this, but I, I don't even care about like predicting stock prices myself.

Speaker 2:          07:12          But I do care about you guys knowing about recurrent networks, about convolutional networks. And so this is a medium for me to, to explain that to you. Okay? So why deep learning, right? So I, like I said, when you, when you, when you look at the performance versus the amount of data, deep learning outperforms all of those other models when it comes to having a lot of data. So what is the obvious approach here? Now this is very exciting, right? So what is the obvious approach here? The obvious approach when it comes to time series prediction is to use a recurrent neural network. Okay. So I'm going to, I'm going to explain how this works mathematically. So we have some input data, right? So first of all, first of all, we have a sequence, right? So we have a sequence of numbers. Let's say zero one, two, three, four, five, six, seven, eight, nine, 10 and we want, we were going to use a first nine numbers to predict what the next number is.

Speaker 2:          08:02          It's going to be 10 and there is some function that describes this sequence. In our case that function would be f of x equals x plus one, right? Where x is the term that we are trying to predict the next term it right? That that's the function. And we can use an recurrent network to map out this, this, um, this function and learn what this function is. There's a function that represents everything in life. There's a function that represents how much attention you are paying. To me, there's a function that represents how loud my voice is in relation to everything else. There's a function for beauty as a function for love. There's a function for everything and we can learn these functions using machine learning technology, right? So when it comes to recurrent networks, normally would feed forward networks. We are feeding in every new data point, right?

Speaker 2:          08:46          So we have term one, term two term three term four and that's it. But we're current networks. They're fed in not just the new data points, they're fed in the hidden state, the learned weight value, that Matrix that's initializes to zero, zero, zero, zero, right? Just all zeroes. That's getting optimized over time. It's fed that in as well. So it's the input and the old weight matrix from the last time step that are both concatenated together, combined together and then fed to the model. And that just keeps repeating. And because the model is fed in the previously learned matrix values from the weight from the last time step, it's considered a recurrent network. So mathematically speaking, we have our input x of t. Okay, so where t is the time variable where we have x of t equals w of t, which is the input data plus s of t minus one, where that is the hidden state from the last time step.

Speaker 2:          09:40          So it's fed both together. That's the input. And so when it comes to the hidden layers for as many as we have, and that's what the, this sigma value means, right? This big eat, it's sigma notation. It means take this, this a operation or these sets of variables here and whatever operations there are and add them up for as many as you have. So in our case, that would be the number of data points. So we're going to take that input value x of t like you we just talked about here and we're gonna multiply it by the weight value right at the, at that time step. And that's going to it. And the function of that, that operation is our hidden layer. Okay? And that's why it's called SMT. And that's what up here we say sot minus one because this is our new hidden layer, our new hidden state.

Speaker 2:          10:24          And this is our old hidden state from the last time step. And so when now we have this hidden state, we can use it to compute an output. So we take that state SFT, we multiply it by the last weight Value v ofK and we do that for as many inputs as we have. That's why the sigma notation is there. That's our g of, that's our g of x. X is this function and the result is our output and the output is a label or it could be the next term in the sequence and that's why of t. Okay, so that's, that is a recurrent neural network. It's, it's a, it's a universal function approximator that is learning a function by being fed in new data points as well as what it has learned in the previous time step. And this is the Goto strategy for regression models, right?

Speaker 2:          11:08          Predicting the next data point in a, in a sequence. But there is a problem and before I talk about that problem, I'm going to answer two questions again from the livestream. Okay. So the first question is how old are you? That's not related, but I'm going to answer it. I am 27 years old. Can you believe that? I'm 27 years old. What that means is if you are 27 younger, older age doesn't matter, right? Nothing. We live in an absurd reality, right? We're credentials, a age where you're from. All of that doesn't matter anymore. All that matters is how motivated you are. Cause you got the internet, you can literally do anything you want. Now you have the, you can become a world expert in AI as a college dropout. You can do anything. Right? So that's the reality we're living in. I'm 27 and one more question about Ai, specifically 10, we use reinforcement learning for time series data. You guys are speaking my language. Oh, did someone just donate some money a hundred pounds or your euros? Ah, so I got to answer this question. I didn't even know I ne I enabled this but um, uh, what was the first question? Time series? Yes you can. You can. And I have a video coming out on that tomorrow rupees. Okay. Is Learning. So debunk the demonic asks, is learning tensor flow required or being able to use care ROSC is fine. Look up the, the more you learn, the better.

Speaker 3:          12:37          Yeah.

Speaker 2:          12:37          The more you learn, the better you're going to be. Okay. But I would say yes. Learn tensor flow. Don't just learn careless. Start off with care os cause it's easy. Then learn flow, then learn. Uh, just, just progressively get harder and harder over time. Don't just start with like buying a giant Linear Algebra textbook and saying, I'm going to start learning machine learning today. And then you're like learning about things that you don't necessarily need for machine learning. Right? Just give yourself these small incremental rewards to keep you going on the learning path. This is what I do. Okay. And as you get those rewards, you're going to become more and more confident in this space because in the end, a degree, a credential, a phd, all that is, is you being able to tell yourself that you have learned something. Right? So as long as you can tell yourself that you've learned something, you will be more confident to learn the next subject in the future.

Speaker 3:          13:33          Uh,

Speaker 2:          13:35          right. Okay. Anyway guys. So back to this, there's a problem with recurrent networks. The gradients that's computed to optimize the network. Over time it, it diminishes advantage is slowly, right because forward. So there are two processes to processes in a neural network. Okay, I'm going to, I'm about to go into the math right now so you better hold onto your butts. Okay. This is the system. Good stuff. When we are forward propagating the gradients, why we're doing input times weight, add a bias, activate right? Input Tom's weight, add a bias, activate and we just repeat that over and over. And that's, that's the feed forward process. Until we get that output value, we have that output value. We compare it with our actual label. We compute an error, we use the era to compute the partial derivative with respect to our weight values. Wait one week, two week three in the reverse order.

Speaker 2:          14:25          So we go backwards. We use that partial derivative to construct a gradient. And a gradient is a, is a list of partial derivatives that we can use recursively to come to update those wait values using that gradient. But the problem is over time that gradient gets smaller and smaller and smaller and smaller and smaller until it's not. So the, the, the, the layers at the beginning of the network aren't updated as much as the layers at the end of the network and is a problem because we want that gradient to not diminish over time because we want our network to learn sequences. So someone came up with a solution called an LSTM network. What this is, is turning one equation in two, three equations. That's what it is. And we can talk about, you know, gates and all of that. But really it really comes down to is this, this set of four equations here.

Speaker 2:          15:15          The three equations really just make this one equation. So I'll talk about this in a second. What we are really doing and an LSTM network versus a normal recurrent network is we are replacing one weight with three weights. The right, we have that input times. Wait, there's a weight that's a matrix at every layer as a group of numbers that are, that are learning over time. What an Lstm is, is three weights. We have an input gate, uh, forget gate and an update gates. And all of these gates are mini perceptrons. There many neural networks and they all have their own weights and they're represented by these equations. So what we do is when we have an input and it goes into the first layer, which is an LSTM cell, this is a cell, all of this for questions. We multiply the first wait by the hidden state times the input, we apply a sigmoid activation function to it and that gives us a value z of t.

Speaker 2:          16:09          Now we take that value, we, we do the same for that. So that's our, we have an input gates we ever forget gate, we have an update gate. So we do that for the input gate. Then we have a forget gate. We'd, we multiplied that waits by the hin state times x and that gives us our sigmoid. We have, uh, the third weight value, our third gates, weight value times hidden state times the previously computed value here times the input. And we apply the 10 h activation function to it, not the sigmoid. And this gives us h of t with the, with the squiggly over it. And we use all three of those values here to compute the final hidden state. So what are we doing? First of all, this is confusing. That's okay. Um, I'm going at it right now. What are we really doing in every layer of a neural network?

Speaker 2:          16:58          We're computing a hidden state. So that's what this is, this hidden state. It's the same result. We're a computing a Hayden states, but we're using three wait values instead of one weight value to compute that hidden state. And that's what that is. And we repeat that over and over again. And what this does is inside of those three weights, the gradient is being captured, right? It's learning what to forget, what to remember, what's relevant in the sequence. And that's the whole idea behind LSTM networks. So the gradient doesn't diminish, it doesn't vanish it, it stays static over time, which is what we need. We need full upgrades for all of the weights in our network. So when it comes to predicting a sequence, and the sequence is apple stock, we ha we can have a sliding window, right? So we have w which is some interval of prices over time and we say plus one and that's going to give us the next one.

Speaker 2:          17:50          So given these prices, what are the next prices? So we can think of it as sliding a window across a time series. It's that that is what it is. So when we look at feed forward network data, we have the number of examples, right? How many data points times the number of inputs by the number of inputs. But when it comes to recurrent network data, we don't just have, we have a third dimension time, right? Time matters. And we have a number of times, series examples, the values at every time step and the number up and the number of time steps. So it's a three dimensional input for this recurrent network data. It can be more actually, but it comes down to having time as that third dimension. And we can squash the dimentionality using a dimensionality reduction technique to do that. Um, okay. So we have some smart people in this chat room.

Speaker 2:          18:41          By the way guys, I'm so proud of, uh, just the, the, the level of interest in AI and just the, the um, yeah, just you guys are, yeah, really impressive people. Wizards. Okay. So, um, right, so we, we can think about w. So W is that window, we can think about it as the sum of all of those values before it. So w of one is the sun volt values before it. W two is some of all those before it, right? And so inside of our Lstm, we're taking that input, we're giving it to the LSTM and it's going to output the next value in that window. Wt plus one wait, w a subscript t plus one. So the next time step, uh, and so that's called backpropagation through time when we're applying backpropagation to recurrent networks is called back propagation through time. So I'm going to build a model at the end of this, but I first want to just show you really quickly a simple LSTM model in python.

Speaker 2:          19:39          We're going to get to the javascript. I'm literally going to build that, but I want to show you a simple python model first. What we do is we create this dataset right here using this data preprocessing technique. We have our input data, we have our output data. Data x is our input data. Why are our labels, we fixed a random seed because these values are generated randomly inside of this test example, but we want it to be reproducible. That means we want the same random values to be generated every time we run this for testings, for testing purposes and debugging purposes, we'll load up our data set will use pandas to process it. By the way, Panda's great great's Python Library for data preprocessing will normalize it, so it's all on the same scale. We always should have all of our data's on the same scale between zero and one or whatever, so that it's easier for a network to find the relationships between all of these data points.

Speaker 2:          20:31          We don't want one feature to be between zero and a thousand and another feature to be between zero and one right? That's that's way too big, right? We don't want one feature to be a list of strings and another feature to be images, right? That's completely different. We want that data to be similar. We want it so that's where normalization comes in so that our network can see, hey, this is all just data. What does he know? This is all just numbers that are on the same scale. I can easily see the, you know, the line line of best fit, you know, whatever it is between all these features, we split that data up, we reshape it, we feed it, and here it is. Here is that complex LSTM network that I just talked about. We can do this in five lines of care os that's where we are today. Obviously if you want more detail then you can add in a tensorflow or num py. If you're going to be a baller, you're just going to build this straight out of num Pi and then we make a prediction, right? So model dot predict, train it up mild to predict testing data and then we can graph it out and this is what it looks like. Just like this. It seems, you know, the red is our prediction. Blue is the actual, uh,

Speaker 1:          21:33          okay.

Speaker 2:          21:34          The real data, the time series works well enough now to the, to the good stuff. What we've been waiting for it. So I'm actually not going to use our current network. I'm going to use a convolutional network. You might be thinking what Saroj convolutional networks they're used for images. Why would you use them for sequence classification? That's, that's crazy talk. And before I answer that question, two questions from the comments. Here we go. All right. Two questions. Is,

Speaker 2:          22:04          is asynchronous actor critic reinforcement learning practical for trading? Thank you for your live performance. It's quite some effort to improvise. Thank you. Uh, yes. Okay. So short answer is yes. Um, reinforcement learning in general is a very underutilized technique for financial forecasting and it's a very interesting research problem. In general. You have multi-agent, uh, multi-agent, uh, networks. You can think about other traders as agents. You can think about, there's a lot of different ways to frame this and it reinforcement learning context that hasn't been before, where your environment is, your, the prices and the algorithms. And the traders, the humans are all considered bots. And you're interacting with this environment inside of this, what is considered a mark Haub decision process that's partially observable because you don't know the values of their account balances or you know, things about them in general. So that's why it's partially observable and that makes it a very interesting research problem. So yes. Two, how will you save your trained model?

Speaker 1:          23:13          MMM,

Speaker 2:          23:15          we are going to save it as a PB file. Proto buffer, right? This is the standard for tensorflow. It's a standard for care os and it makes it easier to, um, uh, export models between different frameworks and languages. Even you can train a model in tensorflow with python and then, uh, use it intentionally. Dot. Js with one line of code and vice versa as well. Very great for portability back to convolutional networks, right? So convolutional networks are generally used for image classification, right? And they're used for image classification because they outperform most other models when it comes to image classification. And, uh, when it comes down to is, again, it's a series of matrix operations. That's it. That's what all neural networks are. There are a series of, you know, Matrix operations, add, subtract, multiply, divide, simple operations between groups of numbers that are called matrices.

Speaker 2:          24:06          Input Times, wait at Tobias activate, right? So here's how a convolutional network works. So we can think of an image as a matrix, right? With RGB values, right? Between Zero and two 55 it's a, it's a, it's a, an image is really just a, um, a matrix of numbers that specify how red, how green, how blue every pixel should be. Now, in the context of a convolutional network, when we feed in these images into the model, we don't consider it a two dimensional matrix. We consider it a three dimensional matrix. Why? Because the third dimension is RGB, right? So RGB is one dimension. And then the other dimension is the width. And the third dimension is the height. So with height, RGB, there are three dimensions. And so that is what a convolutional network is used to. Three dimensional tensor, right? Because a tensor, there's, okay, so little linear Algebra terminology here.

Speaker 2:          25:07          A scalar is one. Number of vector is two or more numbers. Uh, no, wait, no, no, no. A matrix. Okay. So let me, let me start over. Let me start over. A scalar is one single number. Uh, a matrix is to, is to, um, uh, a row and a column of numbers. A tensor is a generalizable form of all of them, right? So it tends to, it can be a scalar attention can be a matrix that can be a vector of size. And to that, you know, a hundred dimensional, um, it could be a hundred dimensions. So attention is just the most generalized form of saying all of this. That's why tensorflow, it's, that's why it's called tensor flow, because tensors inputs are flowing through different operations in a computation graph, and that's a neural network. So we have our input and so in [inaudible], so here's, here's how it works. The convolutional networks have a really, it's, it's three operations that are repeated over and over again. Convolution Relu Pool, repeat that convolution Relu Pool. What the convolutional layer is, is it's a flashlight that slides over that input image and it computes a filter. Here's what I mean.

Speaker 2:          26:26          So we have our receptive field. Okay. So what that is, is it's a, it's a matrix and we are continuously performing the dot product on that input by that filter. And we're sliding over the image. So that value plus one, that value plus one plus this one go down again, that value plus one plus one plus one. What's up? Yeah, that was one that I plus one, the value plus one, right? And it could be plus two. That's the stride. So the stride is the value that says how, um, how big are the intervals that I'm sliding this? It could be four, it could be eight, right? But we're computing the dot product between this randomly initialized filter and that input valley in that and that input matrix tenser and it's, it's, it's computing this, um, this outputs, right? That, that's our output filter. And we repeat that.

Speaker 2:          27:15          So that's a convolutional layer, right? So once we have computed that filter, we apply, we apply an activation function to, it's called [inaudible], which stands for rectified linear units, which is in the equation for that is y equals Max zero x, y equals Max, a parentheses zero comma x. What that means is it's negating all this, those negative values and that are, that could be computed. So everything's got to be positive. And what this does is two things. One, it makes sure that the network can learn both linear and nonlinear functions. So it's a universal function approximator and it makes sure that the gradient doesn't vanish. So relu was also invented as a way to prevent the vanishing gradient problem that we talked about in recurrent networks. And so once we have that Relu, then we can do pooling. And so what pooling is, is us saying, you know, once we've, this is the con con involving part, right?

Speaker 2:          28:11          This is the flashlight part. It's multiplying by the dot product by that filter, by that input over and over and over and over and over again. And then we have that, we have the output filter. And then so what pooling is, is us saying what is the, what is the, what is the best? I'll talk about best, what is the best values from every region of this filter that we've computed? And that's going to be our output. And what this does is it reduces computational complexity, which makes the model faster. And so we see here is like, let's say this is our filter here with single depth slice. If we perform Max pooling, so that's one form of pooling. We take the maximum value from each region, region, and this region it would be six and this region it would be eight, three, four, right?

Speaker 2:          28:52          You see what I'm saying? And so what's happening is as we move through this convolutional network, there's, there are more and more filters that are be computed. It could be like four, 16, 32 but each of these filters are smaller and denser and they're more specific to certain features, right? So it's, it's, it's, it's this level of abstraction where the biggest filters are like head and then it goes to like, you know, uh, eyes, shapes, curvature, you know, like very little details. And the output then is going to be one of several classes, right? So class values, is this a card? This is a truck. Is this a plane? So that's a convolutional network. Why apply them to time series data? Okay. So two reasons. One is, so a recurrent network was invented to look to, to make sure that we take the context of the previous sequence into account, right?

Speaker 2:          29:47          So if we're going to predict the next word in a, say a sentence, you know I'm going to the bank to deposit some so you know what I'm going to say money, but I'm machine doesn't necessarily know that and we, and we have to take the context of bank today. You know me, what I've done in the past into context. So the recurrent networks are really great at remembering the past. They're not great at remembering or not remembering, but taking the future into context as well. So if we have some testing data, we know what the 10 next points should be. That doesn't matter in a recurrent. Now recalling the past matters. So convolutional networks are good at seeing both sides are better than recurring. That's taking into account both sides of the timeline. Second of all, convolutional networks are faster, there are less computationally expensive, so you can run them on big data sets in a much faster time scale. Third

Speaker 2:          30:37          convolutional networks take into account look locality, right? So the locality of features, because this is a good thing when it comes to images, the same, the same feature that matters locally, like say you know the, the blackness of my eye also matters here, right? So if, if, if it sees like white in this shape, it'll know like locally speaking, well there's probably black and it sees here, oh white, there's probably black. Whereas you know, recurrent networks are more generalized into taking it the entire data set into account. Whereas convolutional networks focused on local, on the locality of features. So when it comes to prices, for example, you know what, what bitcoin's price was five years ago is irrelevant to what its price is going to be. Now, if you think about it right now, there's a whole, everything is different, right? So that can apply to stocks as well, right? So high frequency trading for example, right? Making trades that nanosecond timescales, that is all about locality of where we are, right? Right. Exactly. Now and what's happening. And so that's, that's why convolutional networks are better at, they have fewer sequential calculations. They tend to be more computationally efficient and they focus on, um, not looking at the data holistically as a whole, but more capturing those and to be specific temporal relations from the beginning of the time to the end of the time. All right, so, and here's the, here's a little

Speaker 2:          32:00          complexity chart. Okay. So two more questions then. I'm going to code code time is coming. Okay. So what are the questions here? Can you share that notebook? Yes, it's actually in the video description. And the second question is

Speaker 2:          32:15          why should we not add even more weights than the three in Lstm? Because as Andre Carpoff, he says there is a diminishing return to adding more layers, right? So up to a certain points, generally like three to four or five, six layers, you know, you have marginal returns, but then after that it's a diminishing return and you're adding computational complexity because it's all about trade offs. Okay. So now let's build our recurrent network. Okay. So we're going to build this using tensorflow. Dot. Yes. Now I want to say that we have built out this, uh, front end. So the front end, I talked about his here. What I want to do now is actually build the network itself and focus on that because that's the most important part. We tend to fill it out. Jay Asks, you just import 10 dot and your html and that's it, right?

Speaker 2:          33:02          So there's, you don't even need to install it. Really. It's just learning. As long as you have an internet connection, it's there. So we have that. So let's build our convolution network. That's step one. We're going to build our CNN, we'll call it this constant built CNN that we're going to use later. So this is going to be a function of the data that we feed the model. Okay? And so, um, that's what that is. So what are we going to do inside of this function? What we're, what we're going to do, cause we're going to return a promise. And you might be saying, well, what's a promise? A promise is a, it represents the eventual result that we're going to have of any asynchronous operation, which is in our case is going to be the, the data that we are our, our convolutional network that we, that we compute.

Speaker 1:          33:44          Yeah,

Speaker 2:          33:44          so it's a place holder. Okay. So it's a placeholder and then we're going to computer result later on. And so that promise is value is going to be a function or neural network and function. Great New

Speaker 2:          34:07          promise, function, resolve, reject Coco, Coco. Now let's start building this thing. So the first thing we're going to do is we're going to say we have our model using tensorflow dot sequential because this is a linear stack of layers, you know, layer one, layer two, layer three. For more complex models, we would want a nonlinear stack of layers so it can branch into different directions. A lot of models that incorporate attention mechanisms can do this. Or we're going to start off very simple with the sequence, you know, simple sequential, linear stack of layers. So now we can start adding these layers using the TF dot layers function, right? So each layers, the layers is a high level API. It's like careless, but for Java script, okay, so we're going to say, well our first layer is going to be our convolutional layer, right? Because our convolutional networks have these uh, these layers. And so the first layer we're going to sit, we're going to have to specify some of these.

Speaker 1:          35:03          Okay.

Speaker 2:          35:03          Some of these values, and I'll talk about them in a second. We have a filter filters, we have our strides. What else do we have to specify for layer, we have our activation function. We have par colonel initializer.

Speaker 1:          35:15          Okay.

Speaker 2:          35:16          And right, if you look into the tension for the documentation, all that, it's there. And so I'm just kind of like,

Speaker 2:          35:22          you know, based on what I've seen of doing that, right? So it's so far our input shape, what is our orders are, what is our data going to look like, by the way. So our data is going to look like, by the way, I can't believe I didn't even show that this is what our data looks like. We have the dates and we have the, the closing price, right? So that's, that's what that is. So we have one price for the input data. So given, so this is our input, the date and the, and the, and the price is the output. That's what we're predicting. And so because convolutional networks prefer a 3-d tenser as input because they're used to images. What we're gonna do is we're going to turn this date into a three dimensional tensor, whereas one dimension is the year one dimension is the month than one dimension is the day. And so it's a three d 10 is input. The label is this, is this price.

Speaker 4:          36:08          Okay.

Speaker 2:          36:11          So when it comes to our input shape, we're going to say, you know, assuming that we have that, um, the dates cause we have the data that's being fed into this CNN, which is, which are the, um,

Speaker 4:          36:27          you know, the, the input data,

Speaker 2:          36:31          we're going to say it's going to be the length of the dates, you know, buy one. Now when it comes to our kernel size, let's say I said we want a hundred of those data points to start off with filters. You know, generally there are multiples of Eight, eight, 1632. Well let's start off with the, with the, with the small one, two, eight strides as well. There's those generally increase or decrease in size as we as we go further along. So you want to start off with a, with a bigger number and then decrease it over time. So I'll start off with two. Um, activation. Of course, like I said, Relu and Colonel Initializer is called baryons scaling. What do I mean by variants scaling? This basically is a weight initialization technique. It's also called Xavier and some context and it tries to make the variance of the output of a layer be equal to the variance of the inputs. And this, um, this reduces overfitting, so it's called Barry and scaling. Um, I can make an entire video on Xavier initialization by the way, but we're going to call it that. That's our first layer. Now for a pooling layer, like we, we did our convolution, we did art relu and now we can do are pooling. We'll do TF dot layers dot Max pooling. That's our one D and

Speaker 4:          37:47          okay.

Speaker 2:          37:48          Um, now we can say, well, what's our pooling size is going to be, we'll start off with a big pooling number and then for the next pooling, remember we start off big and will we get smaller? And generally pooling is like, uh, they're in intervals of 100, so I'll do 500. I'll also answer some questions in a second. The striding, like I said, it's going to be, oh, let's just say two again because it's like a part of the same block, you know, in convolution Relu cooling, and then it's going to change over time. So say strides or to, and now we'll just repeat this process again. So we'll say model that ad for the next layer.

Speaker 4:          38:27          Yeah.

Speaker 2:          38:28          Um, the kernel size is going to be much, much smaller. So let's pick something much smaller. Five. What'd I say about this filters? The filters increased, right? Because there are less, they're more filters, but they're smaller. As we progress through the network filter is going to be 16 strides are going to be one because it's smaller. Relu and Varian scaling. Again, we'll repeat that again with one more pooling layer. And it's going to be old. Like I said, the pooling is going to be smaller. Let's pick a hundred. So I was going to be too.

Speaker 4:          38:55          And lastly we want to add are, and I'm going to answer questions right after this. Uh, lastly, we're going to add our, um, what are we going to add? We're going to add our model. Do you have dot layers. Dot Dense are

Speaker 2:          39:11          um, fully connected layer? That's what I'm trying to say are fully connected layer because we're going to now take all that data and make a single class prediction and that's what our, that's what our softmax layer is going to do. It's going to output, you know, class predictions, which is going to be in our case the next price,

Speaker 4:          39:27          uh, so dense and that's going to be units, uh, 10. That's the thing about, um, it's fun because you get to see, you know, what's going to work and what doesn't work. Activation is of course, a softmax function and now we can return all of that as a resolve. Thanks to the promise that we've made at the beginning. We're going to turn the model. We're going to return the data. We're turn the model, returned the data, and that's it. That's where our bill TNN function.

Speaker 2:          40:13          I take two questions. Ms Comma, online 12th. Thank you very much

Speaker 4:          40:21          data that dates dot. Lang. Thank you. Boom. Now where were we?

Speaker 2:          40:29          Why don't you use the adaptive theme on sublime tax? Cause I don't have time. And can I run these on CPU? Yes, you can. That's the great thing about j s uh, you can run these on the CPU.

Speaker 4:          40:40          All right. So

Speaker 2:          40:46          yeah, lots of questions. Okay, so now where were we? Now we're going to train this thing,

Speaker 4:          40:54          right? [inaudible] de. De. De. De. De. De.

Speaker 2:          40:57          So step two is to train our model. We built our CNN, now we're going to train it. So we're going to say CNN equals function model. Given our data and given the number of cycles or training loops,

Speaker 4:          41:09          we're going to say, let's split out that data. So we have that input data into its own tensor, each of them. So we have our gates, we have our testing data. Testing data is going to be t f. Dot. 10 Zero One d again, we have hard data dot test times and we have our how equals model dot get layer. We want to get that last dense layer so we can make the actual prediction right. We want to make that prediction in a second. Colons on 12 1516. Thank you. Thank you. Thank you. Colin's on 1215, 1615 also 14. See Guys, when I'm teaching and I'm coding, I'm getting, I'm going to get better at this. It's been a while. It's been a year since I've done a live coding stream, but thank you very much. Uh, yeah. Cool. Colon, colon, colon, colon, colon, her period period or comma, comma, comma. Good. Yup. Yup. Great. All right. Where were we? Uh, right. So model helper is going to be our helper function for our model and um, now we can train this model and this is going to be inside of it, inside of a promise. Again, if it doesn't work, we have some, it's kind of like a try catch loop in a way. Um,

Speaker 4:          42:40          okay. So we're going to try this out. Speaking of try catch loops, we're going to say compile the model and now we built the model. We want to, we want to run this thing using the optimizer. It's the CASAC gradient descent, which is the Goto optimizer for neural networks. Our loss function is going to be the very probably the most popular one. Binary Cross entropy.

Speaker 1:          43:02          Okay.

Speaker 4:          43:02          We're just going to compute our error value and are learning rate is going to be a very standard 0.1% most often learning rates are at 1.1% I think about deep learning by the way guys is we are basically just jumbling up some of these parameter values and we are learning over time. So that's the art of deep learning is sang. I think these are good strides. I think these are good filters. I think the thing this'll work, compile, run the model and see the results and then change those values as, as you see necessary as necessary. And you know sometimes you'll change those values and you'll break the, you know, your break the Internet because you just beat everything else. And that's the really exciting part about deep learning. And so once we've compiled that model, we're ready to train it with the fit function. Okay? So we're going to say,

Speaker 1:          43:49          okay,

Speaker 4:          43:49          let's make sure to reshape that input data so that it's in three dimensions. Um, and do that one more time. Dot. Reshape. Okay. One 1960 and I will wrap at the end of this. So stick around by the way, one batch size and then the number of epochs. How many cycles do we want to run this for?

Speaker 1:          44:28          Okay.

Speaker 4:          44:32          And now we can tell. I know, I know I got to like fix this up in a second. Just hold on. I'm going to, I'm going to fix all these like little parentheses and brackets, et Cetera, et cetera. We can tell our model like it's running if it's running, nodded out and you know, whatever you want to say. And then, um, printout that prediction.

Speaker 1:          44:56          Okay.

Speaker 4:          45:00          And then since we have a catch loop as well, we have our catch loop is going to be resolve print whenever the exception is. That's where our exception, um, and now we can execute this. So we can say, uh, prep data for the function and take the result. We're going to build our CNN using the result. And then once we built our CNN built function, then we can say no, nope. How answer questions in a second. CNN built dot model, they'll talk data 100 then function.

Speaker 1:          45:59          Yeah.

Speaker 4:          46:03          Print. So it's like that now I will purchase the license at some point. Okay. So I probably got some serious, um, probably got some serious, um, errors here. Let's see, Park city scripts. We're, we're, we see d public CD scripts python to be CNN dot. Js in valid syntax online one.

Speaker 1:          46:57          Yeah.

Speaker 4:          46:59          Oh you know what sometimes with hold on a second. Get up time guys. Cause we are running out of time in this livestream and battery and everything, but yeah. Anyway, I've got, here's a good humbling for you guys by the way, which already have here would you guys should have been following along with anyway, anyway. Um, public scripts.

Speaker 4:          47:32          Boom. Here we go. And it's very well commented for you guys as well. So definitely check this out. When it comes down to it, uh, given either the logical data or the apple data, it's going to be predicting the next in the next data into time series and we can wait and there we go. And so that's for, that's for a test data. We're pulling from the Apple Api and then we made this Jason file right here and then we trained it on that. Um, but yeah, convolutional networks. Super useful for time series forecasting also. Uh, I want you guys to, I want you guys to, uh, say a subject. I'm going to rap about it. All right, so just say some subject. In the meantime, I'm going to answer two questions. Uh, so Raj need a crowdfunding campaign for a sublime license. Uh, yes. No, no, no, no. Don't worry about it. It's kind of like a, it's kind of like a meme now. Like I just never get sublime. And then lastly,

Speaker 1:          48:32          yeah.

Speaker 4:          48:34          Uh, what are the questions? Anyone here from Amsterdam? I used to live their whole hot tits and should I know TF dot js before seeing this video? No, I think this was simple enough. As long as you've, you've recognized, um, you know, Care Ross or tensorflow and you've seen linear models, then you can, you can get this. Um, right. So, thoughts on reinforcement learning for time series. Of course, I like it. Medical image analysis of a great video on that search, medical image analysis. Um, and uh, stationarity how important is it for Lstm? Uh, it's not as important for Lstm as it is for a Rhema. A rhema focuses on that, whereas Lstm is more focused on features, um, across the entire, so it's to holistic overview of all of that sequential data. Um, rap about Egalitarian technocracy um, why did I pick the hardest ones to mention? Seriously Egalitarian. Technocracy all right, here we go.

Speaker 1:          49:42          Yeah.

Speaker 4:          49:43          Here we go. It's a freestyle. Turn up the sound. Oh Shit. I don't have sound. Could you play a beat from there? Anything like, just like pick a instrumental, we're about to do this on youtube. Just search, like rap, instrumental. Um, okay. Here we go. With the, uh, the rap. It's coming in. Five,

Speaker 1:          50:07          four, three, two, one. All right, there we go. Oh, I got it.

Speaker 4:          50:24          Sick trap beat.

Speaker 2:          50:33          You say, I need a technocracy. I'm gonna give you a democracy. How much show you how to break out of monotony and show you the life that you can't see. I'm trying to go with five, so free outside and see all my enemies telling me that you can do it. Hold up. I'm a break it down. Show you that governments don't mean anything. It's the Internet. We've got a new techno utopia. It's less like man, Internet, Internet. I'm out of it. I'm back in it. You don't even see because I'm beyond it. That's the for this wrap. I just wanted to do a little five second demo to keep it interesting guy. So that's my wrap for this. Today's video. I hope you liked this video share. The best thing you could do for me is to hit the subscribe button and tell your friends to subscribe. That is a single metric. I care about. Subscribe. I'm trying. This audience is growing, but it's not growing nearly fast enough. We're trying to get to the top. You know what I'm saying? Million subscribers by the end of the year. That is the goal. Okay? It's a very unrealistic goal, but I'm an unrealistic person. Okay. So thank you guys for watching. I love you guys. You're the reason I do this. Um, and for now, I've got to go take a plane to Houston to visit my parents, so thanks for watching.

Speaker 1:          51:46          Okay.