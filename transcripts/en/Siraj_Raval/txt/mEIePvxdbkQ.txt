Speaker 1:          00:00          How did you know the mighty ducks would win? Luck and reinforcement learning hello world it Saroj and sports betting. It's a popular pastime for many and a great use case for an important concept known as dynamic programming that I'll discuss in this video. We know that reinforcement learning is the subfield of machine learning that focuses on goal oriented algorithms that involve time delayed label's called rewards. And we've also learned that the standard way to represent this problem of an AI trying to maximize its rewards in a time based environment setting is by using the mathematical framework known as a Mark Cobb decision process. Now let's use these concepts to help us frame our problem of betting on the winning hockey team. Assume that we are a professional sports gambler, let's call ourselves JK Rowling Endo and our career is to make money by betting on whether or not a certain hockey team will win a game.

Speaker 1:          01:09          And there's a series of these games that will occur sequentially that are all apart of a tournament. Our goal is to make 100 US dollars by successfully betting on the winning team. What can I say? We dream big for each game. There are only two possible outcomes. Either are chosen, team wins or they lose. We'll place a bet on that team at the start of the game and if they do win, we will win the same amount to that. We bet. Otherwise the opposite will happen and we'll lose all the money. We bet. Let's also make one fundamental assumption here that we'll touch on later. We'll assume that the home team will win 40% of the time, no matter who the home team is. So for every game we have to decide how much of our money we wanted to put at stake in the context of reinforcement learning.

Speaker 1:          02:05          Then we can consider this a Mark Cobb decision process. The state would be how much money we have at any given point. The actions are steaks or bets that we would place. The reward is going to be zero on all transitions except those in which we reach our goal. Then it's plus one. What we'll want to then learn is an optimal policy. Our policy will be a mapping between levels of capital we have to how much we should bet the optimal policies should then maximize the probability of reaching our goal, which is to make $100 and remember we are making an assumption here that the home team will win 40% of the time. So we can set p the probability of the home team winning to 0.4 since P is known, the entire problem space is known and can that be solved? Meaning we have a complete model of this environment.

Speaker 1:          03:05          That's definitely not always the case. But for this introductory example, it will be to make our problem easier and it's good practice to denote any other characteristics of our mark comm decision process that we can find like the fact that it can be considered episodic, meaning it lasts a finite amount of time when the game ends rather than continuous. So how do we find this mythical Unicorn function known as the optimal policy? Well. If we look back at the Bible of RL called Rl, an introduction by sudden and Barto in chapter four will come upon a pretty useful definition. The term dynamic programming refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a mark Haub decision process. That's perfect for us. Since we already know the transition probabilities, which indicates the probability of a change from state s to the next state and we've got a reward function.

Speaker 1:          04:11          Dynamic programming is a way of breaking down complex problems into sub problems. We solve the sub problems, then combine the solutions to those sub problems to solve the larger problem. There are many different types of DP techniques and it's actually used in many real world applications like bio informatics, scheduling and routing algorithms. The specific dynamic programming algorithm will use in this example to solve our problem is called value iteration. It turns out that any optimal policy can be subdivided into two components, an optimal first action followed by an optimal policy from the next state. If our first action is optimal and after that we follow an optimal policy from where we end up, then we can conclude that the overall behavior is optimal. These rules can be formalized by what's called the principle of optimality. A theorem invented by the mathematician, Richard Bellman, decades ago. It states that a policy achieves the optimal value from state s if and only if for any next state reachable from s our policy achieves the optimal value from state s so we can essentially break down the description of what it means to have an optimal policy in terms of finding the optimal policy from all states.

Speaker 1:          05:39          We could end up in after that, we just need to do a one step look ahead and discern what the best first action we could have taken his. That's the principle of optimality applied to policies. So notice this somewhat counterintuitive intermediate step here. Before we can compute the optimal policy, we need to compute the optimal action value function called Kyu and there's a great formula that expresses this very well. Can you guess what it is? We can express the value of any state as a some of the immediate reward plus the value of the next date. This expression is popularly called a bellman equation. It will help estimate the best action to take using the value for a given state in order to find the optimal policy. So if we know the solution to this sub problem, like the optimal value function from the next state, then the value function given this states can be found by a one step look ahead.

Speaker 1:          06:41          Using the bellman equation in the value iteration algorithm, we will iteratively apply the bellman equation to compute the optimal value function. One Way to think about this formula is that from a given state, there are many possible actions so we can accumulate the values for how much each action would get in terms of expected value and just assign the value of the state to that of the action which return the highest value. We've basically taken the bellman equation and turned it into an update rule. The Algorithm initializes via vests with arbitrary random values than it repeatedly updates the state action pair called queue. And the [inaudible] of s values until they converge. Value Iteration is guaranteed to converge on the optimal values. Guaranteed sounds pretty great, doesn't it? Especially when money is involved, but let's take a closer look here in the context of our sports betting problem, where the state is, how much money we have, the actions are the states and the reward is zero on all transitions except those in which we reach our goal.

Speaker 1:          07:54          When it is plus one we can try and compute the optimal policy. The state value function will give the probability of winning from each state. A policy is a mapping from levels of capital to steaks and the optimal policy maximizes the probability of reaching the goal. The reason we can use value iteration is because of the value of p four me four is known. If we were to graph out the value estimates versus the amount of capital we have, we'd see how the value function changes over successive sweeps of value iteration and the final policy is found for the case of p equals 0.4 we can then use that policy to make the most likely bets in the future. Notice however, that we need that value of p to compute the optimal policy using value iteration in general, even though dynamic programming is very useful, it does have its limitations.

Speaker 1:          08:53          We need a perfect environment model in the form of a Mark Cobb decision process to do it and it's very computationally expensive. Solving the bellman equation is a brute force job we're doing by trying out every possible action. We could do it every state. If we tried to do this for say, a game of chess, we'd need computing power that would make Google's global supercomputer looked like a super Nintendo system. Still, it's a useful thought exercise and it is used in many real world situations that don't require massive amounts of compute. Also, did you notice something? We had that intermediate step where we computed the optimal value function to compute the optimal policy, but what if we just skip that intermediate step and just computed the optimal policy directly? What type of algorithms should we use for that? That's the topic for the next video. Three points to remember from this video though, dynamic programming refers to the collection of algorithms that can be used to compute optimal policies given a perfect model of the environment. Value Iteration is a type of dynamic programming algorithm that computes the optimal value function, and consequently, the optimal policy and dynamic programming is useful but limited because it requires a perfect environment model and is computationally expensive. I am proud of you for making it here. Hit subscribe and your enemies will become your friends. For now, I'm going to compute the optimal life policy, so thanks for watching.