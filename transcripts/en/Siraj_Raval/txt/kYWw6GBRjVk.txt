Speaker 1:          00:00          Hello world, it's arrived and the Internet of things. It's definitely a buzz word these days, but it's a real thing and I'll demonstrate how to use reinforcement learning to optimize electricity consumption amongst multiple devices in a smart home. Marc Andreessen's quote, that software is eating the world still rings true today. More and more devices are coming prepackaged with Internet access that would normally be, that includes smart sole shakers, smart rectal thermometers, smart mugs, smart chairs. The list goes on, and because these devices have a connection to the worldwide web, they can communicate with the outside world and each other sharing information and even learning from one another. This internet of physical things is becoming increasingly common across the globe, whether it's a smart home, a fleet of self driving cars, or a pipeline of assets as they are shipped across the globe. Having a web of interconnected physical devices has a huge range of use cases in our modern world for businesses and consumers.

Speaker 1:          01:05          But a major concern regarding these devices is the amount of energy they'll need to consume. Silicon chips need electricity, and the more devices we have, the higher our energy costs will be. It turns out that we can use reinforcement learning to create a system that will minimize our electricity costs substantially. This can apply to a whole range of industries. We're an interconnected system of devices, is required to perform a task, agriculture, manufacturing, and even a personal home, which will be our demo. Google recently used RL to reduce the amount of energy they used in their data center by up to 40% just like a laptop generates heat. They're massive racks of servers generate a lot of heat, but too much heat can damage the servers. So a cooling system is necessary to help maintain a certain temperature and because it's such a dynamic environment, a learning system is required to set that temperature.

Speaker 1:          02:03          A whole host of unforeseen scenarios like changes in weather and power outages can occur. Also, humans interact with the equipment in unexpected ways, regularly probably doing the floss stance on them. And each data center has a unique architecture and environment, so their system needed to be able to adapt to multiple unique environments. Last week we introduced the fundamental way of framing the reinforcement learning problem where an agent is interacting with an environment to maximize our reward. The mark comp decision process. The goal of an agent is to learn a policy so it knows given a state the best action to take in order to maximize or reward and in a complete Mark Cobb decision process or all of the environment variables are known, we can use dynamic programming to learn an optimal policy. But what if we don't know all the environment variables beforehand? Give up? No.

Speaker 1:          03:04          Then it'd be considered not to model based RL, but model free RL in model free RL. The first variable we miss is a transition model, so we won't know what's going to happen after each action we take beforehand. This tells us the probabilities associated with various state changes. The second thing we miss is the reward function, which gives the agent the reward associated with a particular state beforehand. So when we don't know either of these mark Cobian variables, dynamic programming won't work. We need to instead use a different type of method called Monte Carlo. Monte Carlo methods are a broad class of algorithms that rely on repeated random sampling to obtain numerical results. The keyword here is random Monte Carlo methods make use of randomness to solve problems, which turns out to be very useful in mathematics and physics. Stanislaw ou alum invented it in the late 1940s while working on a nuclear bomb as part of the Manhattan project.

Speaker 1:          04:12          Then John von Neumann decided he liked it and programmed a machine to do those same calculations. They decided to name the codebase. Monte Carlo has every super secret project should be named UHMS. Uncle happened to be losing lots of money in the Monte Carlo Casino in Monaco, so that's why uncles, in fact, deep minds alpha go use what's called a Monte Carlo tree search to help it play against the reigning go champion resulting in move 37 more on that at the end of the course. Monte Carlo techniques have several advantages over dynamic programming. First, they allow for learning optimal behavior directly from interaction with the environment without needing the transition or reward function defined beforehand. Second, it's easy and computationally efficient to focus MC methods. On a small subset of the total states and third MC can be used with simulations. So let's say we have a home that consists of a bunch of internet of things devices.

Speaker 1:          05:20          We've got a smart TV, a smart fridge, a smart dog, and a smart giant server in our room. All of this equipment requires a lot of electricity to run, but it also requires cooling or else my room would get too hot so I have a cooling system. Now let's say that we have access to our electricity usage logs, thanks to partnering with a data friendly provider and our smart thermostat can set the temperature accordingly. Depending on the type of system we built, we can imagine electricity flowing into all of these devices. Creating a closed loop system and this constant stream of electricity data can definitely be utilized. It's giving us the electricity, price, cooling demand, and electricity consumption as variables. Using this, we can construct our mark hub decision process. The goal of our system is to minimize our electricity bills, more money for GPU.

Speaker 1:          06:15          Still now our agent will perform an action in this environment. That action will be to either increase or decrease the temperature by one degree Celsius. My fellow Americans, most of this audience uses the metric system. The state then that our agents can be in will be a measure of both how much cooling demand there is as well as the price of electricity. The reward can tell us whether we are saving money or not by switching states by calculating the total electricity consumption multiplied by the price of electricity and depending on if that's greater or less than what existed the time step before we know whether or not that is a positive or negative reward. There exists an optimal policy here such that if we were to give it a state in this case that would be the cooling demand and the electricity price. It would know exactly what temperature the thermostat should be set at such that we are optimally saving money on electricity by cooling our room as much as necessary when necessary.

Speaker 1:          07:22          Our adaptive real time reward based system needs to learn this optimal policy and since we don't know the reward function or the transition function beforehand, we have to compute our rewards and transitions as they happen in real time. We'll want to use a model free technique like Monte Carlo to learn the optimal policy. The basic idea is to calculate the value function of each state backwards with the reward received. After the end of the episode there has to be an ended, then the task has to be considered an episodic task for us to use Monte Carlo. In our case, we can say that an episode last a full eight hours while I'm away at my office working diligently, if we move from the initial state to the terminal state according to the given policy, will receive a reward at each time step. We'll remember all those rewards and when we get to the terminal state, we'll look back and calculate the value function of each state in the case that there are multiple episodes, then Monte Carlo just averages all of the returns.

Speaker 1:          08:29          We know that the return is a sum of the discounted or reward. In the context of Monte Carlo, though we switch it up to obtain the state value function we take instead the expectation of the returns not the sun. We can define a state s to be a discrete random variable which can assume all the variable stats with a certain probability. Every time our agent reaches a state, it's like we are picking a value for the random variable s for each state of each episode. We can calculate the return and store it in a list. Repeating this process. A lot is guaranteed to converge on the true state value function in Monte Carlo Rl. We are estimating the value function for each state ace on the return of each episode and the more episodes we take into account, the more accurate our estimation will be. Notice though that a possible problem could occur.

Speaker 1:          09:24          What if we visit the same state twice in a single episode? Well, there are actually two types of Monte Carlo policy evaluation. First visit and every visit we'll focus on first visit in this video. First visit only recognizes the first visited state. Every second visit does not count the return for that state visit and the return is calculated separately for each visit. Monte Carlo includes randomness because when it updates every episode, depending on where it originated from, it's a different result depending on which action we take in the same state because it contains these random elements. Monte-Carlo has a high variance. When we graph a simulation of our agents solving our problem, we'll see that eventually the policy will converge and then our system will know exactly what temperature to set our room ace on the electricity related variables. There are different kinds of Monte Carlo techniques that can do all sorts of cool things, but we'll talk about those later on. Three points to remember from this video. In model three reinforcement learning as opposed to model based, we don't know the reward function and the transition function beforehand, we have to learn them through experience. A model free learning technique called Monte Carlo uses repeated random sampling to obtain numerical results. And in first visit, Monte Carlo, the state value function is defined as the average of the returns. Following the agents first visit to s and a set of episodes, please subscribe for more programming videos. And for now, I'm going to try a new sample. So thanks for watching.