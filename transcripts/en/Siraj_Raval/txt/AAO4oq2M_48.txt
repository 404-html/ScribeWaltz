Speaker 1:          00:07          Hey, hello world it Saroj. Welcome to my live stream. My other live stream a software was, you know, technical issues. So that's not heavy. But anyway, in this video we're going to perform some quantum machine learning, some very exciting stuff. There's some people in the, in the other life room that need the link to this video. So I'm going to go ahead and post it in that India video description to that. Okay. Um, so anyway, so we're going to talk about quantum machine learning in this video. I've got two great. Um, I've got some great topics to cover. There's a lot, actually, there's, there's so much to cover here. Um, but uh, let me just pack that in nature. Everybody's coming into this live stream that we have going on right now. Everybody's in. Okay, cool. Just slight little technical error. It happens. And now we've got people in here, right?

Speaker 1:          00:58          280 viewers in your, okay. So let's just get right into this code. Um, Oh, before we get into the code, I just want to say something. I want to say that D-Wave Wade went ahead and invited me to their office and then Vancouver and it was a, it was a pretty cool experience and a seeing is believing. So I can, I can see, I can definitely say that I saw a quantum computer myself in person. I met some really cool people there and I, I just, I'm, I'm a believer now I'm a believer in the power of quantum computing to help accelerate machine learning applications and, and to help us create new types of algorithms that we did not think are possible. So in this, in this video I'm going to go over quantum mechanics, quantum computing, including the hardware and how cubits work in terms of the hardware. And then we're going to demo some machine learning applications. Okay. So I'm going to go ahead and screen share. So unfortunately we might not be able to see each other a while we do this, but um, that's just how it goes. And I'll hit, I'll be answering questions as well. So let me go ahead and move that out of the way. And it looks like screen sharing is working great. So let's go right into this a lecture we have here.

Speaker 1:          02:10          Okay, here's our lecture. Let me, let me and make sure everybody's in this, in this chat room. By the way, we've got a lot of people who need to be in here and I just want to make sure that the links are working. You know, everything's working. All right, cool. People are listening. There's got 500 people watching this. Uh, this, this thing right now. Okay? So,

Speaker 2:          02:36          okay.

Speaker 1:          02:36          Okay. So you guys can hear me, right? Right. So here's, here's, here's what we're going to do. This I want to see, uh, the, I want to see people qualities low, right? Let me, let me make it better. Let me make the quality just a little bit better by making this bigger. And

Speaker 1:          02:52          hello from Brazil, right? Everybody's there. Okay, so here we go. With this, let's, let's get right into this. Okay. So first of what is quantum mechanics? Quantum mechanics is cc me, a, uh, sprint. Hey, then that's just going to be in the corner. So I'll be in the corner. Okay, cool. All right. So you know, whenever it works for now, whatever works. Okay. So, um, what is quantum mechanics? Quantum mechanics is, uh, at the subatomic level things or different, okay. Tldr, things are different. At the subatomic level, mattered is quantized energy is [inaudible]. Momentum is quantized meaning. There are some really strange phenomenon happening at the, at the subatomic level, at the level of particles of electrons, things like that. So the official definition is quantum mechanics is the body of scientific laws that describe the motion and interaction of photons, electrons, and other subatomic particles that make up the universe. Okay? So very exciting stuff. So if we were to flip a coin, okay, so here, here's an example. If we were to flip a coin, in our reality, we are used to one truth, okay? That coin either fall on heads or he will fall on tails, right? It's going to be either one or the other and um,

Speaker 1:          04:21          make sure that I'm over here in the, in the corner, okay? Right? It's either going to be heads where it's going to be tales. And so we're used to one truth in our reality. We're used to one truth in our reality

Speaker 1:          04:34          and but at the quantum level, there are multiple truths. A coin can be in both heads and tails at the same time. So in our reality, when we flip a coin, okay, so I'm, I'm demonstrate. We flip a coin, we look at it, we know that it's going to go up. We know that it's going to go down. We know that it's going to hit land on heads or tails. There is one truth, but at the sub atomic level, at the, at the smallest, at the smallest level of the universe, the laws of physics behave differently. There are multiple truths and we can prove those multiple truths mathematically. What do I mean? Let's assume that I'm going to flip a coin as a particle, so I'm, I'm now Saroj at the subatomic level. Now at the subatomic level, when I flipped a coin and I don't look at it, that coin is going to eternally just be flipping in midair.

Speaker 1:          05:23          It's going to be both heads and tails at the same time. It's going to be both heads and tails at the same time. These are both completely true. At the same time, a coin can be in both heads and tails. A particle can be in two different states at the same time. These are both completely in 100% truth. Okay? And I don't say realities because this is happening in our reality. And so if I observed this coin, then immediately it will pick it heads or tails. And when I say observe, that doesn't mean a conscious observer that any kind of measurement, either human nonhuman machine, any time that that coin is measured by any device, anything, then it will pick either heads or tails. And while it's in the air, it is going to be a probability of heads and a probability of tails at the same time, say 70 and 30%.

Speaker 1:          06:16          So how do we describe this kind of crazy behavior? Right? This is wild. This is wild. Well, it turns out we can describe this behavior mathematically and this is what quantum mechanics is all about. Okay? Let me make sure everybody's following here. Uh, rock. Wow, great. 737 people here. Great. So, um, okay. So there was this experiment called the double slit experiment. And I'm going to answer some questions after we get through this. Before we start coding, I will start coding by the way. So, uh, there's this experiment called the double slit experiments. And how it works is you take an electron beam gun, so it's just beaming out light and it goes through two slits and then it's going to land on the screen. And what's going to happen is when we do this, we're going to see this number two that's on the right here.

Speaker 1:          07:02          Let me, let me make this a lot bigger. We're going to see this number two image that's on the right here. So it's, it's to be, we're going to be able to observe what is called interference. So two different waves through the, through. Both of those slits will interfere with each other. And that's kind of weird, right? Because we would assume that either light is made up of particles or it's made up of waves. Well, it turns out that light is both, actually, it's both a particle and a wave. And we can describe this mathematically using what's called Schrodinger's equation. Okay, how do we describe the properties of light? How do we describe this particle wave duality that light is, and this is happening at the quantum level. So Schrodinger's equation helps describe this particle wave duality. And in the red that is Schrodinger's equation and it's actually based off of a lot of different theories in physics.

Speaker 1:          07:57          What the what, how do we compute kinetic energy, potential energy, conservation of energy. So there's actually a lot of theory behind it and we're not going to describe that right now, but just so you know, things behave differently at the quantum level. And so the reason I talked about this example is that quantum computing, quantum algorithms exploit three features from quantum mechanics. And those three features are number one super position. Remember when I talked about the coin flipping and then just being in mid air and being both heads and tails? That's called superposition in mid air a coin to be in both heads and tails at the same time. That's super position. Um, and here's a great image of that. I know it's in French, but a cyber lock on job. No, I love French. But this was the best image I could find on, on the idea of super position.

Speaker 1:          08:48          Now there's a second feature. It's called interference. Now, don't worry if none of this makes sense yet, by the way, I just really want it to describe quantum mechanics. Okay. I, I just think it's so cool. It is so cool. I don't think it is so cool. So where were we? So super position is the second feature, right? So that's the first thing. You're, the second feature is interference, right? So those waves and the double split experiment, when those waves collided with each other, that's interference. They overlapped and canceled each other out. And the third feature is entanglement. Now this is, this is kind of hard to even explain in the context of classical mechanics or in the context of anything that we know of, right? This is actually quite an interesting, um, this is quite an interesting phenomenon, but Tldr, two particles that are entangled affect each other.

Speaker 1:          09:42          So if you observe a particle here, it's going to affect the other. And there is no amount of distance that matters. Like this could be across the universe and to particles could be entangled across the universe. Okay? So those are the features of quantum mechanics that quantum computing, um, utilizes or exploits. So we know that a bit in classical computing can either be one or zero. And in quantum computing a bit can be both at the same time. A cubit is called a cubit. Now a lot of us have heard the term cubits, right? We, we've heard this term, but what really is physically what physically is a cubit, right? So before we get into that, a quantum computer is using humans to supply information and communicate through the system, okay? It can be an, and so one really simple way to demonstrate the power of cubits is to think about the example of me using a thank you.

Speaker 1:          10:40          So Liam, I appreciate that. So an easy example is, uh, thinking about writing a random x on a page, okay, I'm just going to write an x on a page in a book and I'm going to put it in a library with a million other boats. Now classically, we'd have to sort through all of those books to find the x. But in a quantum computer, accubid in super position can be in multiple places at once simultaneously so it can analyze every page at the same time and find x instantly. Now that is a theoretical example because why? Why? Because cubits are a lot of fun. Okay. Unlike classical computers that can be in a super imposed, a super, super imposed state. Now we are all used to classical, uh, sea moss transistors. These are silicon transistors. CMOS stands for complimentary metal oxide semiconductors. And the way they encode and access information is by using voltage, right?

Speaker 1:          11:42          So Zeros and ones. What really represents zeroes and ones? How do we create these states, these binary states? Well, the way we create them is by using electricity, right? Voltage. The voltage of a transistor is addressed by a bus, which is able to set into a state of either zero or one. That's how transistors work and taught the computer you're using works, but in a quantum machine, instead of using that, they used a squid, a superconducting quantum interference device, essentially a quantum transistor. Now what does a quantum transistor look like? Well, it's used, it's instead of using silicon, it's using a metal called neophobia. Okay. The OPO, and this is a very, very interesting Christine Metal, the opium, I saw it myself actually at the way. Very cool stuff. But this metal is called Neovia. And the the, the really interesting property of Neovia is that when we cool it down to absolute near absolute zero temperatures, like really freeze freeze this day, then it's going to, it's going to be become known as, it's going to become a superconductor.

Speaker 1:          12:53          It starts to exhibit quantum mechanical effects. What do I mean? So here's an example of a cubit built with this neodymium metal. And so what happens is instead of encoding states using electricity, we are encoding states using magnetism, which is, you know, still based on the electricity, right? Electromagnetism. But the idea is that when we, when we cool this Neovia metal, it's going to create this magnetic wave in either up or down direction of either up or down. And that's going to allow us to encode a two states as tiny magnetic fields, right? He's going to be plus one. It could be minus one. And that can be plus one and minus one at the same time, which is super cool. And so by adjusting a control knob on our computer, we can put all of these cubits right, these little magnetic, um, like magnetic coils coil is not really the right word here. These magnetic, I'm super imposed a particle cubits states, we can put them all into a state, which

Speaker 1:          14:03          we can entanglement, right? We can entangle them. So what happens when we take a single cubit and we try to couple them with other cubits so that we can have a more powerful quantum computer? How do we do that? So here's what it looks like. So in this, in this image we are seeing these scripts, these gold strips, these are, these are cubits. So there are eight cubits. Your four by four and the blue dots represent what are called couplers there couplers for these cubits. And what the couplers do is there may come superconducting loops and they put many elements together and they allow these cubits to interact with each other,

Speaker 1:          14:39          right? The more Cubans we have, the the fact the, the faster our computer is. And in fact we see an exponential speedup where n is the number of cubits. Um, you know, it's, it is an exponential speed up the more cubits that we add, right? So it's a magnetic field going through the superconductor and we are putting currents on either end of what are called Josephson junctions, which allow for this magnetic field to be created. Um, this quantum mechanical field that can represent two states at the same time when it's cooled. And so the really interesting bit about the Qp, you, the quantum processing unit, is that there are no large areas of memory. We're used to CPU, we're used to von Neumann architecture, right? Where we have, we have a processor and then we have memory but in a quantum computer, but the processor and the memory are interpolated together. So it is both processing data and it's storing data at the same time. In that way it is very similar to neurons in the brain in that we can think of Cubans as neurons that both process data and um, store store data as well. And we can think of the couplers as synopsis that we're connecting these neurons together.

Speaker 2:          15:53          Okay.

Speaker 1:          15:53          Okay. So super, super cool stuff here. And who provides these computers? Well, D-Wave obviously IBM, Google, Microsoft until a lot of big players are working on this right now I'm hitting, we're, we're going to start seeing a lot of startups coming to this space, which is why I am talking about this because the time is now, the time is now for those of you who are hungry to build applications for the world that are going to solve real business problems. The time to use quantum computing is now to speed up existing machine learning algorithms and create entirely new classes of machine running algorithms that did not exist before. And I'm going to show you how to do that in this video. Now, Dewayne has a lot of patents. When I was there, I literally saw a massive wall of patents, uh, which is good for them, right? You know, market share, um, that's going to be good for them. And then there are others and so they've been building a stronger and a bigger and a faster quantum computer over the past couple of years. Um, and so, so that, that, that's what that space is like. Okay. So here's the big question. Are they faster than classical computers, right? That is the big question that we want to answer here. Or are they faster than classical computers?

Speaker 1:          17:11          Right? I liked the way you guys think. So the answer is for some problems. Okay. It's not like they are going to completely replace conventional classical computers, but they will augment that. They will augment them and they will think of them as a six. Right? Think of them. Has specially special devices for a specific set of problems, just like we think of CPU at or CPS or a processing sequential data. GPS are great and matrix operations, uh, quantum computers or great at solving problems that involve quantum data. So anything involving a simulation where you're trying to simulate our universe because our universe operates using both classical and quantum mechanics. And that is in fact why Richard Fiman wrote the paper on this, you know, a couple of decades ago because he envisioned a computer that could simulate reality perfectly. And the only way to do that, it's use a quantum computer so that we can simulate quantum mechanics and um, right.

Speaker 1:          18:17          So, so there's that. We can use it to speed up machine learning algorithms. We'll get into that in a second. Um, and so it's actually really hard to build a quantum computer as well. Um, we can think of this whole process of quantum computing as a process of engineering. The pattern of a complex set of waves, these electromagnetic waves in hopes of channeling that flow towards the correct answer. So in machine learning, we are used to defining an objective function, right? We've got this objective function, we're gonna and then we're going to learn a function with a proper coefficients using some optimization scheme, like say gradient descent to get solved that objective.

Speaker 1:          18:54          And you know, we might have some constraints. Now in the quantum world when we are doing quantum machine learning, that same idea is there of us to finding an objective. But there is much more of an emphasis on defining what the constraints of the problem are because it's like defining the constraints of the problem. And then just like letting these waves interact in ways that we have no idea how they're gonna interact and then hopefully it's going to come up with a solution to our problem. Right? We don't know what that answer is going to be, but we can define the constraints of the problem, you know, threshold values and you know, lower balance, things like that. And then we'll converge on the function. Okay. And that's kind of hazy. We'll get, we'll get more detailed in a second here. But there's another problem with comp Quanta computers is that they, they fall into this decoherence coherence state, right? Uh, where, uh, basically there are leaks there, these quantum leaks, and it's really hard to keep them all in this coherent state where they're entangled, they're working together and they're, they're used to compute things. You know, it's, it's, it's the mixture of variables involved in cooling and, you know, things like that. So it's hard and a lot of times it's not worth the effort to build a quantum computer when you could just solve a problem. Classically.

Speaker 1:          20:10          So what are the applications of quantum computing now? Uh, there are a lot, these are being used in production, so from cryptography to medicine to obviously machine learning, searching for, you know, in big datasets. And I've actually got a giant list here for different industries and how quantum computing can be used to solve problems in these industries. So definitely check this out. You know, a lot of different industries here. Um, and so even Dewayne, they've got this great a landing page that shows their clients and what they're using for using quantum computing. So from Volkswagen, Booz Allen, you know, recruiting and NASA is using it. Um, material simulation. Like I said, I'm like five minute envisioned, Monte Carlo simulations, things like that.

Speaker 2:          21:00          Okay.

Speaker 1:          21:00          And so we can think of this stage that quantum peers are at now at the same stage at classical computers were in the 50s, right? Still very introductory technology. We have some basic idea of how these, how this hardware works. But think about classical computing. Do you, you don't think that the people designing the CPU in the 50s had a idea that apps like Facebook and you know, et Cetera, but the apps that we all use, um, would actually exist. It's such a, it's so, it's so far off from the original idea of a, you know,

Speaker 2:          21:34          okay.

Speaker 1:          21:35          Uh, about the hardware. So here's, here's the real, here's the real question. How can it be used for machine learning? So when I was there at the wave,

Speaker 2:          21:45          yeah,

Speaker 1:          21:46          when I was there at the wave, I was very, um,

Speaker 1:          21:51          happy with, uh, the, the, the team. They answered a lot of my questions, a bunch of quantum scientists. We all sat in a huge circle and just, I was asking them questions left and right. And this is such an exciting field of study of research. Quantum computing is such an exciting field because there's so much to discover still. But there are four known ways that it can be used for machine learning, including optimization, sampling, colonel evaluation. And of course, you know, gleefully we, we, we, we look at this and think, Oh yes, linear Algebra. Why, what did your own networks use it use matrix operations. What speeds up matrix operations? GPU, why? Because they do them in parallel. Could we speed them up even more using quantum computers? I definitely think so. I definitely think so. And in fact we're going to look at a paper in a second that does that.

Speaker 1:          22:45          So they will speed up some existing algorithms and enable a new type of algorithm as well. Okay, so some images and so let's look at the leap, right? So let's look at their software that, this is what they demoed to me in Vancouver when I was there. So if you go to the dashboard here, they've got some, they've got some demos, they've got to an STK installed, they've got a QP you dashboard. And uh, we've, uh, we've got one hour of Cupe you time, which doesn't seem like a lot, but if you think about how fast a quantum computer is, you can compute a lot of things in milliseconds so that that hour is actually quite a lot for a three tier, right? This is free for all developers to use. Now here is the coolest thing here. And then here's where they really got me. Um, there are Jupiter notebooks in the browser and we can compile these quantum algorithms in the browser. So let's, let's look at a demo here. Um, first, and this is called social network analysis. And then we'll look at the code and I will, I will myself as well. Okay. So,

Speaker 2:          23:47          okay,

Speaker 1:          23:48          let's look at this optimization problem here. Okay. So this is an example of social network from Romeo and Juliet. Okay. So we all know the story of Romeo and Juliet, two star crossed lovers from different families that these families were enemies. But then they fell in love. And so there was that new connection that was forged. Right? Okay. Let me move this out of the way so that our great, oh my God. Move, move, move, move on. Sometimes you just got, there we go. Oh, show to continue. Okay. So at the sort of the story, where are we at? The start at the store and there was a river that, that, that, that cuts that divide these two communities, right? Um,

Speaker 1:          24:36          there's a river that divides these two communities and they all are connected with each other and then they have um, connections across the river that are negative, right? These are enemy connections, but between each other they have friend connections and then all of a sudden Romeo and Juliet Fall in love and a new connection is formed in this breath is social graph. And this leads to instability or imbalance in the network, right? Because what ideally, we want there to be no connection between both groups. We want them a balanced network in this case would be one where there are no positive connections between the families. So how do we rebalance this? So the way we would do this kind of quantum computer is we would set the degree to which two variables agree and that's the cubits coupling strength, strength and the degree to which a variable tend to a particular outcome. And that is the body. And we're going to look at this programmatically in a second,

Speaker 1:          25:37          but basically we can rearrange that rap. So such that in a way that uh, there is no enemy connection across the river and the graph is in fact balanced again, and we can use a quantum algorithm to do this. Okay. So you know, this, this applies to, you know, same exact thing is happening here with this series and extremist thing and we hit run and then it's, it computed this structural imbalance problem. And so what better way to demo how this works then to look at code. So if we go to developer tools here, they have, or learning in docs, Jupiter notebooks in the browser that we can just start looking at right now. That's, let's check this out. Beautiful. The colonel's ready. Great. Good. Okay, here's the kernel. So what are we doing here? So let's just compile this. So here's this toy example. Nothing quantum is happening here. And this like little, I mean make this big.

Speaker 2:          26:39          Okay.

Speaker 1:          26:40          I think quantum is happening here. Okay. All that's happening here, it's bigger is we are building this network graph, right? We are building a network graph and um, there is a hostile relationship set up, hostile relationships and instead of friendly relationships, that's it. And then we can initialize a solver. Okay. And the solver hall explained what the solver is doing here.

Speaker 3:          27:10          MMM.

Speaker 1:          27:13          And then we'll take that solver and we'll use it to sample from the possible outcomes. Okay. So we have the solver and it's going through solve that problem by sampling from the set of possible outcomes. And when we perform that, it's going to show us the frustrated relationships and then rebalance the graph. And we can even visualize that right here.

Speaker 1:          27:42          Okay. So it, it rebalanced the graph. We didn't see how it works. You might be wondering. We did see it worked, but we didn't get to see this like little visualization thing because a million things go wrong in demos guys. But the, but the point is that we just come, we just computed I quantum algorithm in the browser and now I'm going to code it myself. So don't worry if you don't understand what the solver itself is doing. I'm going to code that myself now and then we're going to look at what the solvers are. Also. Let me answer some questions. I haven't been answering questions mad fast, right. What are the questions here? Uh, the questions are, uh, numerous, any questions? So SGD Oh, great question. So STD will be useless with quantum ml. No, no, no, no, no, no, no, no, no, no, no. Gradient descent is not going anywhere for the time being. Um, but we can think as the analog of gray dissent in the quantum world as simulated annealing. Well, that's simulated annealing, quantum annealing because quantum annealing is all about energy minimization, right? We were trying to minimize some energy and a system and simulated annealing is used a lot by physicists to try to minimize the energy in a system. Quantum annealing is the analog of that, um, to the classical world. It's also the analog too.

Speaker 1:          29:01          It also the analog to, um, grainy the scent in the quantum world because gradient descent is gradient descent, is trying to optimize, um, a landscape and it always optimize it for the local minimum, right? The local minimum. But what simulated annealing does is it optimizes for the global minimum things that Sdg Sgd stochastic gradient descent cannot necessarily reach. Uh, so there is hope, right? Because a global funding, the global minimum is better than finding a local minimum. Now, gradient descent has a bunch of right, exactly. In Gym. So,

Speaker 1:          29:49          so there are a bunch of trips into the cast, the grading dissent to try to improve its, uh, its abilities to optimize, right momentum. Adam, there's a bunch of techniques and try to get it over the, the giant hill that it has to go over. Um, so yes, um, there is an analog, it's called simulated annealing and there is a lot of research in that direction. Okay. So let me answer one more question then. We're going against some math. How can we learn ml in GPU? So it turns out, check this out, check this out. I'm going to blow your mind. Second, kick up this website,

Speaker 1:          30:24          quadrant. Dot. Ai Deep learning with a lot less data. What, what, what, what the, what is this is a Dewayne business. Okay. The power of generative learning. You have my attention. You have our attention. What is generative? They've got a little demo here in tensorflow. Okay. Okay. And the thing is, I've already looked at this by the way, I'm just being theatrical here so it's not using, I don't think I look at the code here. It's not add download it. It's not using a QP. You correct me if I'm wrong guys, but it's not using the QP you here, it's um, using this probabilistic distribution algorithm instead of the quantum machine, but it's a step in that direction of using the QP you, um, very exciting stuff. I, I met with the machine learning team when I was there. Very, very exciting stuff is happening right now.

Speaker 1:          31:20          Specifically with Boltzmann machines with sampling, right sampling, Gibbs sampling specifically. I met with a scientist working on taking variational auto encoders and improving their generative abilities using quantum algorithms. And, um, there were, I met another scientist that was working on an algorithm that decides which chip to use and when to use it between a CPU, Gpu, Qq and other assets, right. That want me to be perfect low level algorithm where you have this high level model that you define and then that's a low level. It's like, oh, I'll use a coupon for this, for this. And there's that. Right. That would be all right. Cool. So, um, yeah. Anyway, so that's that. Make sure everybody's onboard here. Great. And coding on now. Okay. So you know, I thought when I was going to do this livestream that I would just be coding out four different examples, right?

Speaker 1:          32:13          One for colonel evaluation, one for optimization, one for sampling. Well, it turns out that to even code a single example, there's so much context that I have to, you know, both learn and to convey. And so I'm going to code, I'm going to try to coat two examples, but we'll see. For Time's sake. I might just, I might just go with one. And for simplicity's sake, because there's a lot of, there is so much theory here. Seriously, I have never come upon any subfield of AI that I, I've never seen such amazing, exciting possibilities and literature.

Speaker 2:          32:52          Okay.

Speaker 1:          32:52          There's a lot. There's a lot. That's all. I mean, the rabbit goal, the rabbit hole goes so deep. I mean, check this out. Check this out. Let me show you this paper. I was just reading actually before this stream, check out this paper that I was just reading. It's called, it's literally called quantum machine learning. It was published a couple months ago. Um, huge collaboration between a lot of people. Beautiful. Um, introduction. I mean they start off with um, like human nature and told me and Copernicus and then get into Hockfield networks in Boltzmann machines. So very cool stuff. But, but, but check this out. By the way, here's something that's very cool. Are already measuring the speed ups of these machine learning algorithms. Quantum reinforcement learning, right? So this is a part of move 37. So this is a, here's your, here's our, our El elements here.

Speaker 1:          33:44          Um, and in general, this is just good for reinforcement learning engineers for and researchers and everybody, right? Because quantum can be applied to all sub deals, or I believe, and here's where the belief comes in. I think that there is a possibility to speed up many, not all. I mean, I think all has been disproven. You, you can't speed up everything in machine learning, but there is a huge possibility space and I believe that and we'll see as we learn more. But look, look at this. There are clearly speedups here and they've been denoted those speedups here, right? Um, square root of n speed ups. So for perceptrons, those are neural networks and you know, things like that. So very, very exciting stuff. Alright, so let's start coding here. Let's start coding here. Okay, so here's what we're going to do. We're going to start coding and what am I, what am I going to do is I'm just going to start coding and then talk about what I'm doing. So there is this library called Die Mod. Okay. So Die Mod. Uh, basically he wants us build models that are suited for the quantum, the quantum machine. Okay. Uh, Die Mod. That's what that, that's what diamond does.

Speaker 1:          35:02          And a sheriff Api for binary quadratic samplers were, we'll get into what that is in a second. So let me just import the library. Now let's define our input data, right? We, we have to have to input data and let me make sure this goes away. All right, let me just find our can data. Okay. So our first input data is going to be called H. And so what age is, is it is a binary variable.

Speaker 1:          35:28          H is a binary variable and it's kind of value for both zero and value for one. What Jay will be is it's going to have a single value for both zero and one at the same time. So as you're in it, and that value will be negative one point up. So negative. Now what is this? So each represents our linear biases and j represents our quadratic biases. Now, now it's time to actually look at what we're doing here. Okay? So I just need to start coding something. So this is called the easy model or the, the icing. Actually I model. Okay. Um, the icing model identifies phase transitions. So in physics, right, something is either be solid, a liquid or a gas, or in the right, something is either a solid liquid or gas. And how do we identify the transition from one stays like a solid to a liquid or a liquid to a gas, you know, et cetera. And so the icing model represents this as an objective function that we're going to, that we see right here expecting did we icing model does. Okay. All right. So that's the icing model. And, uh, here's, here's what everything defines here. So, um, this, this [inaudible] looking thing, it's, it's sigma notation. And what we're saying is let's take the sum of all of the, uh,

Speaker 1:          36:54          of all of the variables that are a part of our equation here, where n is the number of variables, um, and as, as represents a spin. And so what this does, but let, let's, let's step back for a second here for a second. What this model is, there is a lot of physical systems that we could want a model, right? So Electron from individual electrons, the collisions of entire galaxies, right? So, uh, good models can represent, are very generalizable. They can represent the collision of galaxies and collisions at the subway on my level. And that's why a IC model is a good model. It can represent a lot. This model was proposed by a guy named Ernst icing and it's used to represent that phase transition, right? So variables represent spin up or spin down states that correspond to plus one or minus one values.

Speaker 1:          37:44          These are, these are magnetic, these are a result of the electromagnetic activity, okay. Up Two particles. And so they, it uses discreet variables to represent the magnetic dipole movements of spin states, which are either plus one or minus one. And the data which are just the spin states are organized as a lattice so each spin can interact with its neighbors. And so this is this, this equation here then then represents the state of that phase transition. And is, it is a quantum mechanical process. It is a process from quantum mechanics because it's dealing with the subatomic level. And so what we need to, what we've, what we're, what we just wrote here, this is quantum data, right? This represents quantum data because it's dealing with phenomenon that occurs in all reality based on quantum mechanics at the subatomic level and it represents a phase transition. So, um, this is our objective function and what we want is for,

Speaker 2:          38:51          okay,

Speaker 1:          38:51          there are to be a UN, an equilibrium here, all of our, of our spin states so that we, we reach this objective function of, of upstate and downstate, some magnetic up and down states at the same time, right? Simultaneously. And so the way we're going to do that is we're going to construct this, uh, missing icing model. And we can call this a binary quadratic model. Is it binary quadratic model, but using the dime odd libraries, there are different types of binary quadratic models. In fact, this is a superclass of icing. We can also create, um, a Cubo model is different, kind of not going to talk about today. Like I said, there's so much happening here to, once we define our model, we can then sample from that model. So remember a quantum algorithm, ports sample from a space of possibilities.

Speaker 2:          39:41          Okay.

Speaker 1:          39:42          And we can sample from that model. It's like that. And that's, that's it. We've, that we've computed our quantum, um, result that's going to satisfy this binary quadratic model, which is an icing model. And then we can pronounce it what we've, you look what we've, um, approximated and seeing what's in the response data. And I'm going to say, uh, this is our sample. We have our energy, right? And we're going to print out those samples and those energies.

Speaker 3:          40:20          Right, right.

Speaker 1:          40:23          All right. Let's see what happens here in Dalton Syntax where, Oh four I'm from.

Speaker 3:          40:36          Okay. Um,

Speaker 1:          40:41          right. Oh, this has to be print cs.

Speaker 3:          40:44          Okay.

Speaker 1:          40:46          All right, cool. So what does it, is it simultaneously computed all the possible, um,

Speaker 1:          40:52          states that it could be in those coefficients and it found a what they are and then we're printing them all out at the same time. So this, this essentially we are simulating this quantum mechanical system that represents a phase transition, uh, particles from a solid to liquid, or I look at your gas, et Cetera. And this can, this has proven to be faster, um, faster computer using quantum algorithms over a classical algorithms. Now optimization in general, we'd like, we find that Romeo and Juliet problem. It's a, it's a, it's a big thing, right? In general. Um, right. So, um, anyway, right as a lot of people. So there's that. Um, and there's also sampling, right? So these quantum computers can be understood as samplers and prepare a special class of distributions and run. So Gibbs sampling, things like that. Um, want to make sure these people are not, are in the right place. There are a hundred people waiting here. That's a, whoa, there's 40 people. Okay, good. I'm gear wrong link. Come to this one.

Speaker 1:          42:17          All right. Back to this. Okay. So I'm sampling, it's another example. I'd like, there's so much give sampling variational audit encoders. Uh, you know, we, sometimes we initialize our weights as random when we shouldn't. Um, there, there are better ways of initializing our weights for our neural network and then random, better than random. And I'm right. So there's a lot of potential here for sampling. Now. Um, there's colonel evaluation, there's the satisfiability problem, uh, to sad problem, which is an NP complete problem. There is way too much for the single stream. There's so much theory. I don't want to overload your guys here. How are you guys feeling? There's so much. I still have so much to learn as well. Like I am totally a student of quantum machine learning and I see a lot of a potential there.

Speaker 1:          43:13          So like, so, uh, colonel evaluation, right? So in machine learning there are kernel methods, um, where we are, we have some high dimensional, uh, data and we want to separate it by a hyperplane, right? So for classification for example, and we can use the kernel method to, um, convert that high dimensional data to lower dimensional data so we can then visualize it and have the hyperlink hyperplane go through. So I support vector machines are a good example of that. Uh, but there's also other examples as well. Um, and we can use a quantum algorithms to estimate certain kernels. One, they're difficult to compete classically in. The last thing here is linear Algebra. Now this is really, I think this is going to blow the, this is going to blow the lid off of everything. Like if we can really get these quantum computers to operate as neural networks, you know, if we can interpret the Matrix, ascribing a quantum gates as a linear layer in a neural network, we can visualize how the Gatewood connect inputs and outputs.

Speaker 1:          44:10          And this is a very, very exciting area of research. And, uh, that's, that's in the future, right? That there's so much theory there. Okay. So it's overwhelming isn't it? It's a lot of stuff. More physics, right? I mean, clearly, clearly. So, um, you know what, I'm just gonna stop there. I don't want to overwhelm you guys. There's a here and, and you know, if you want to see me talk about more quantum machine learning stuff, um, definitely tweetdeck needs, we had D-Wave. Um, and we'll, we'll get, we'll get something set up. Maybe we'll do a full course on this so that that's for later. We're still working on move 37 right now. And, um, we have a lot of, of work to do at school, the ids, um, everything's always going to be free. All my education will always be free. Don't, don't even worry about that. Um, but it's a super exciting time and I'm, I'm glad to, I'm glad that you guys are here. I'm, I'm, I'm excited to continue with this, uh, uh, this stuff and uh, yeah, that's it, that, that's, uh, that's it for this live stream. Thank you guys. Thank you guys for being here. Okay, so I'm going to go ahead and end this live stream. Thanks. Karsten. I'm going to end this live stream. Thank you guys for coming and for now, I've got to go, um, work on some curriculum stuff, so thanks for watching.

Speaker 1:          45:29          All right, let's love you too. Love you guys. Okay.