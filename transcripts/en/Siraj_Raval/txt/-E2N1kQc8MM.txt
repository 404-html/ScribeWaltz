Speaker 1:          00:00          Hello world, it's Saroj and today we're going to learn how to generate videos using the hottest type of model in machine learning. Right now, the generative adversarial network. We'll train our model to generate the alien language from the movie arrival that we can later animate. In 2014 everyone was trenching themselves in ice water for the als challenge, but meanwhile in Canada, a researcher named Ian Goodfellow published a paper introducing the world to Ganz and the Ai community loved it down. Macoun the director of AI at Facebook called it the most interesting idea and the last two decades in machine learning. I recently interviewed Ian and ask him a bit about its history. How did you come up with this idea

Speaker 2:          00:41          for guns? The short story is that I was arguing with my friends in a bar. The thing that clicked there and the bar was this idea of having the discriminator continually learn at the same time that the generator is learning. If both muddles learn, they're driven to this equilibrium where it becomes impossible to fool the description.

Speaker 1:          01:02          Gans are a type of generative model given some input, data x and some labels. Why it learns the joint probability distribution of this data with the joint probability distribution function. Given a y you can calculate or generate its respective x and since this is unsupervised learning, we have no labels handed to us. We use the training data as y and the generated data as acts to so given say a set of three d models of major cities from across the world from Google Earth. There are thousands of different features or dimensions to this data. Can we generate an entirely new city from it? Atlantis is waiting and what constitutes a good city? Sometimes a single input corresponds to many different correct answers, each of which is acceptable. Generative models help us work with multimodal outputs like this. There's been so much activity centered around gans lately.

Speaker 1:          01:55          They've been used to convert low res images into crisp high res images, convert hand drawn sketches into photorealistic images and to generate everything from fashion styles to new product categories. Apple, you should probably take a look into that last one by gans cycle. Gans W gads. There's a lot. So the basic idea here is that we have two neural nets. One is called the generator g and the other is called the discriminator d. We've got some dataset. Let's say it's a collection of Pokemon images and we want to generate new images from this dataset. That means entirely new Pokemon that have similar attributes to those in our training data. The generators job is to try to create fake Pokemon that look really similar to our training Pokemon. Ah, the discriminators job is to classify the generated Pokemon has either real or fake. Think of Ge as a magician and D as his audience.

Speaker 1:          02:50          The magician is constantly trying to make the audience believe that he's illusions are real magic. The audience boos when it can tell that his tricks fake and applauds, when it can't tell the difference, they both improve and in the ideal case, the magician gets so good that no matter what he's able to full his audience every time the type of gain will use in this video is called a deep convolutional Gan or DC. Again, this is because both d and g are deep convolutional neural nets, so the discriminator has several layers of what's called convolution and the generators got several layers of de convolution. Convolution is literally impossible to understand in a parallel universe, but in this one it's pretty easy. Normally neural net layers are fully connected, so all inputs are connected to all outputs with many types of data. This makes sense. We want all parts of the input data to be able to contribute to all parts of the output prediction, but images are considered spatially, locally correlated.

Speaker 1:          03:53          That means if there's a banana and an image, it doesn't matter where it is in the image, it's still a banana. So we exploit that. Instead of connecting all of the input data or pixels to all output values, we use a much smaller filter that we slide across the picture like a flashlight, so that means that much fewer parameters in a convolutional layer as opposed to a fully connected one. Let's look at our code. We're only going to use care os to build our model. Pillow helps us do image processing and num Pi will help us perform some valuable reshaping operations on our images. Let's get started by first defining our discriminator. We'll give it its own function. It's going to be a linear stack of layers, so we'll define it as sequential and we'll start off with two convolutional blocks. That means a convolution to extract the feature map followed by the Tan h activation function to squash real numbers into a range between negative one and one which lets our model learn more complex functions than just linear regression. Then a pulling wire pulling reduces the dimensionality of each feature map but retains the most relevant information. Will flatten the feature map into one dimension. Then applied to fully connected layers to it. The last dense layer, I'll put an end dimensional vector where n is the number of classes we have, so it would be to, in our case and by applying a sigmoid to it, it'll convert the data into probability values for each one.

Speaker 3:          05:17          Dan's got a train them. It's Dng. Let's use cross and true p and g, enhance gradients to this and is it real or just pretend.

Speaker 1:          05:28          Next we'll define our generator which performs similar operations, but in the reverse order. Since it's fed random numbers as its input, it converts them into an image by first going through to fully connected layers with their own associated activation functions. Batch normalization, we'll apply a transformation that maintains the mean activation close to zero and the activation standard deviation close to one. This allows for faster learning and higher overall accuracy. Then we apply to convolutional blocks that will eventually output an image. Upsampling will convert our image into a higher resolution. Now that we've defined models for both D and g, we can combine them to make, again pretty easily with care os we can just reuse the same network objects we've already instantiated and they'll conveniently maintain the same shared weights. With the previously compiled models, we're going to want to freeze waits in the discriminator part of the game when we back propagate the joint models.

Speaker 1:          06:26          So we'll first set the OS trainable flag to false for each element. In this part of the network, we'll also define a low data function which will form our images into vectors that we can feed to our model by making use of the image and num py libraries. Okay, so how do we train this thing? The goal of training our discriminator is to maximize the of x for every image from the true data distribution and minimize the effects for every image, not from the true data distribution. The goal of training the generator GMC is to create samples that full DDI. We trained both D and g by taking the gradients of this expression with respect to their parameters because each player's cost depends on the other players parameters, but each player cannot control the other players parameters. This scenario is most straightforward to describe as a game rather than an optimization problem.

Speaker 1:          07:20          Specifically a mini Max scape mini Max is a strategy of always minimizing the maximum possible loss, which can result from a choice that a player makes. And the Nash equilibrium is where the optimal outcome of a game is. One where no player has an incentive to deviate from his chosen strategy after considering an opponent's choice. So the way we train Ganz is to find the Nash equilibrium of a mini Max game between g and D. We'll load our data and initialize both of our models in our train function. Then we'll combine them both and initialize to castic gradient descent optimizers for both of them will define a loss function. Binary Cross entropy then performed the set of steps outlined in the paper for every time step. We sampled data from both distributions, then update d using our gradients. Then we update g using our gradients. Once we're done training, we can generate some images from GE and we'll see that they do indeed look pretty similar to our other images.

Speaker 1:          08:18          We can even stitch a bunch of these images together to make a video. This can be applied to any image or video data set. Since videos are just collections of images, we just feed them in one at a time. All right, so what are the main takeaways here? Generative adversarial networks are a framework for generating realistic samples from random noise. They consist of two neural nets, a generator that creates fake samples and a discriminator that tries to judge if those samples are real or fake, and we optimize them through back propagation, which helps find the Nash equilibrium of the mini Max game between both g and D. First place for last week's coding challenge goes to an ammonia totemic. Nemanja used a convolutional variational autoencoder to generate videos by training on my own videos. He also added a recurrent network to this architecture, which I haven't seen done before.

Speaker 1:          09:10          Very cool results in the Jupiter notebook. Definitely check it out. Was there of the week and the runner up is Neoss. Muhammad neons used a vie to generate Pokemon and wrote up a really cool blog post on his whole process. You guys inspire me and I vow to you. This week's coding challenge is to use again to generate some video. Decals are in the read me get humbling SCO in the comment and winters are going to be announced next week. Please subscribe for more programming videos and for now I've got to discriminate between data, so thanks for watching.