Speaker 1:          00:00          Oh world, it's Saroj and in this video we're going to make our own chat Bot using tensorflow. 2016 has been the year of the Chat Bot Messenger. We chat Skype and a bunch of other popular messaging platforms now host chatbots that developers have built for them and brands are increasingly using chatbots to engage their customers because the data doesn't lie. 90% of apps that get downloaded or only used once with a chat bot, there's no need to download anything. It lives inside of the chat app that you open up a dozen times a day and competing for space on your phone home screen is really hard, but per space on your next most used screen, your chat app, that's more doable. You can now chat with CNN to get the news or chat what they bought to get flowers delivered to your boo or even chat what they matchmaking

Speaker 2:          00:45          but what,

Speaker 1:          00:48          but even though the chat bot space is getting really hot, it's nowhere near saturated. Just think of an APP that you like and build a chat Bot for it. Chatbots are the new apps. Before deep learning hit the scene a few years ago, all chatbots had hardcoded responses, a that would try to predict everything you would say and build a huge list of responses for every question they could think of. All of them were pretty terrible. Deep learning changed everything and still is as new discoveries are being made. Instead of telling a computer what to do, you can say, this is what I want as an outcome. Make it happen. Some chatbots that have used deep learning do so by taking different components and applying it to each of them. I could create a deep learning based system to interpret the language and another one to track the state of a conversation and then another one to generate a response. Each of these systems would be trained separately to do its own task and the Chat Bot would collectively use the results from each. This is unnecessarily complex to build.

Speaker 1:          01:48          Ah, a better type are called end to end. These are chatbots that use one system that's trained on one dataset. They make no assumptions about the use case or the structure of the dialogue. You just train it on the relevant data and say, I want you to be able to have a conversation with me about this data and to end systems are what we should all be striving for. Intuitively they make sense and they're starting to outperform all other systems. So let's talk about how to do this with deep learning. The most simple type of neural net is feed forward. That means that as it trains data just flows one way from the input node all the way to the output node. It only accepts data that is a fixed size, like an image or a number. Give it a labeled dataset like whether or not a temperature is hot or cold and it'll be able to predict if a given temperature is hot or cold.

Speaker 1:          02:37          But a conversation isn't a fixed size. It's a sequence of words. We need a network that can accept sequences as an input, a recurrent neural net in a recurrent net. We feed the data back into the input while training it in a recurring loop. So we're going to build a chat bot intense or float using recurrent neural nets are steps will be to download our Dataset, create a model, train it on that Dataset, then test out by chatting with it. The first thing we'll want to do is decide what data set we want to use. If we were creating a chat bot for a specific use case like customer service, we want to use a dataset of conversation logs from a real human representative, but for this demo we just want to make a fun conversational Bot, so we'll use a movie dialogue. David said, compiled by Cornell University.

Speaker 1:          03:18          It contains conversations between characters from over 600 Hollywood movies. Hopefully transcendence is not included in that list. Well, download our Dataset and put it in our data directory. Next we'll want to split our data into two different sets. For training, we'll call one set in code or data and the other set decoder data. The encoder data will be the text from one side of the conversation. That decoder data will be the responses. Then we'll want to tokenize our data and give each token an integer. Id. Tokenizing means taking each sentence like a Lomo and chopping it into pieces so that it's easier for a model to train on and giving each token and associated id. We'll make data retrieval faster. Once our data is properly formatted, we can create our model. We can define our own function for this. That takes our tokenized encoder and decoder data as its parameters.

Speaker 1:          04:07          Our function is going to return TensorFlow's built in sequence to sequence model with what's called the embedding attention mechanism. Let's break down what the eff. This means. A sequence to sequence model consists of two recurrent neural networks. One recurrent net is the encoder. It's job is to create an internal representation of the sentence. It's given which we can call a context vector. This is a statistical value that represents that sentence. The other recurrent net is the decoder. It's job is to give it a context vector output. The associated words, the type of recurrent net we'll be using is called our long short term memory network. This type of network can remember words from far back and the sequence, and because we're dealing with large sequences are attention mechanism helps the decoder selectively look at the parts of the sequence that are most relevant for more accuracy.

Speaker 1:          04:55          So our model, we'll be able to create context vectors for existing questions and responses and it'll know to associate a certain type of question with a certain type of response. So once we create our model, we can train it by first creating a tensorflow session, which will encapsulate our computation graph. Then we'll initialize our training loop and call our sessions run function, which we'll run our computation rep, which is our sequence to sequence model and we'll use it as our parameter. Now we can save our model periodically during training using the TF trained not saver function. This will save our model as a checkpoint file, which we can later load once we're done training using the savers restore function. When we run our program, it'll take a few hours to fully train. We can periodically test what kind of responses we get from our Bot in terminal if we'd like.

Speaker 1:          05:39          And as you can see, responses are pretty meaningless at first, but as our model improves through training, eventually it becomes more coherent. So to break it down, deep learning allows us to make chatbots that are way more human lives than any kind of handcrafted Chat Bot we've made before and to end systems are systems that allow us to use a single model to give us our desired outcome and we can use sequence to sequence models using two recurrent neural nets to create conversational chat box. The winner of the coding challenge from the last video is Georgie petkoff. He implemented three different methods to estimate a solution to the traveling salesman problem and benchmark the results in an eye python notebook that ass of the week and the runner up is mic. Then Holst he used both the nearest neighbor and simulated annealing algorithm to estimate a solution. The coding challenge for this video is to use TF. Learn to write a script that generates sentences in the style of Lord of the rings. Don't take at most 50 lines of code to write this and details are in the read me post. You'll get humbling in the comments, and I'll announce the winner in my video one week from today. Please hit that subscribe button for now. I've got a hack snapchat spectacles, so thanks for watching.