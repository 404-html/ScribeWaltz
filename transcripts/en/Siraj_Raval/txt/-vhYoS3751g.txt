Speaker 1:          00:00          I'm going to show you mathematically what's happening in here. Are you ready? Hello world. It's Saroj and the brain. It's deeply influenced so much machine learning theory and in this video I'm going to take a little bit of a different approach to my usual lectures and introduce four unique machine learning algorithms in the context of both human and animal learning. Ultimately using one of them to train a digital mouse to find the exit in a maze. If you're new here, hit subscribe to stay updated with my latest content. I'm going to constantly be pushing educational videos to this channel every single week. In the early 20th century, a physiologist named Ivan Pavlov was studying the physiology of the gestion in dogs in the lab. He noticed that his dogs begin to salivate in the presence of the technician who normally fed them rather than only salivating in the presence of food.

Speaker 1:          01:00          Pavlov was inspired by this informal observation to conduct a more formal experiment. He presented an audio stimuli to a dog, then gave it food. He did this a few times and then when the dog heard the sound, even without the food, it started salivating. That beat must've been fired. He concluded that if a particular stimulus in the dog surroundings was present when the dog was given food, then that stimulus could become associated with food and cause salvation on its own. Due to this positive reinforcement loop. This experiments popularly called Pavlov's dog experiment helped form the basis for associative learning theory. It describes the process by which a person or animal learns an association between two Stimuli. The basic claims of associative learning theory, our that reinforcement learning is the acquisition of associations between states' actions and rewards. The animal is acquiring these associations through learning.

Speaker 1:          02:09          In this case, we can assume the state to be a stimulus. It's useful for predicting the potential reward. It also claims that the modifications of these associations are driven I prediction errors. That means the discrepancy between what the animal expected to get and what it actually got. The most influential idea of associative learning theory was invented by two researchers named rescore law and Wagner. A couple decades later. They're model is a prediction error based learning model in which stimuli acquire value. When there is a mismatch between the prediction and the outcome in the equation. The value of stimulus s at trial t is set equal to the value of stimulus s at the previous time step plus the reward and the expectation. The learning rate defines how much this prediction error is weighted. This prediction error says that when the animal gets more reward than it expected, it leads to strengthening of the associative weights and a negative prediction error leads to a weakening of the associative weights.

Speaker 1:          03:21          The risk score La Wagner model was groundbreaking for two reasons. The first reason is that it was able to explain a huge number of phenomena in fear conditioning. Using that model, researchers were able to map out the detailed structure of how these computations could be implemented by a Mig Dulles circuitry. The second reason is that it helped developed some early natural language processing applications like part of speech tagging. I'm sure they could have named it something more descriptive, but hey, it was the 70s but wild. The risk score low walked her model provided a basis for associative learning theory. It only estimated a single value. We know however, that biological brains are able to represent uncertainty about the world somehow since the world is full of uncertainty, probability theory suggests that to properly represent a biological brains uncertainty of the world, it should utilize a probability distribution over the possible weights instead of a single value.

Speaker 1:          04:31          We can use a famous statistical rule invented by Thomas Bayes called never talk about fight club. Wait, that's something else. It's called Bayes rule. It states that the posterior probability, which is the probability that the hypothesis is true is equal to the prior probability, which is how well the hypothesis fits existing knowledge multiplied by the likelihood of the evidence. If the hypothesis is true divided by the prior probability, that the evidence itself is true where the evidence is a description of how well the hypothesis explains the new evidence. Another way of saying this is that the probability of event a occurring given that be as true is equal to the probability of B occurring. Given that a is true multiplied by the independent probability of a divided by the independent probability of B, the Basie and generalization of the risk score. Low Wagner model is embodied in what's called the Coleman filter.

Speaker 1:          05:30          It states that uncertainty grows over time due to the random diffusion of the weights. It also states that this uncertainty can be reduced by observing the data. It uses a series of measurements observed over time and produce his estimates of unknown variables that tend to be more accurate than those based on a single measurement alone by estimating a joint probability distribution over the variables for each timeframe. The common filter has numerous applications in technology including navigation and control of vehicles like Aircrafts sell flying planes. Yup. It's also applied in time series analysis and robotics. It works by modeling the central nervous system's control of movement because of the time delay between issuing motor commands and receiving sensory feedback. The common filter is a realistic model for making estimates of the current state of a motor system and using commands. It's a two step process in the predictions step.

Speaker 1:          06:34          It produces estimates of the current state variables along with their uncertainties. Once the outcome of the next measurement is observed, these estimates are updated using a weighted average with the most weight given to estimates with higher certainty. It's a recursive real time algorithm using just the present input measurements and the previously calculated state as well as its uncertainty matrix. So far we've broken down tasks into trials, but real life operates in continuous time and our algorithms are short shortsighted. They've only been able to predict the immediate reward, the one that will be received in the very next state to extend the capabilities of what we're able to model mathematically. Let's switch from classical learning theory to modern reinforcement learning theory with a focus on a specific sequential decision problem. A mouse trying to find the exit in a maze full of fire. Nevermind. There are two popular classes of reinforcement learning algorithms we can use to help solve this problem.

Speaker 1:          07:44          The first art model free techniques, these attempt to build up a table of values. The value is the cumulative future award at the animal expects to get when performing a particular action. In a particular state. The idea here is that we are not building an explicit internal model of the world. Instead, it were directly trying to estimate a lookup table from trial and error. Once we have the lookup table, we can use it to choose the optimal action for any particular state. The other strategy is model based learning. The idea is that the animal will construct an internal model of the world. That means two things. First, it will learn a transition function, meaning if it's in a particular state and it takes an action, what's going to be the next state and to a reward function, which estimates how much reward it expects to get in any particular state.

Speaker 1:          08:46          We can use dynamic programming here or a variety of algorithms to simulate different paths in the environment and pick actions that lead us to the optimal path. Both of these algorithms have trade offs. The model free system is fast. We just perform a table look up, but it's inflexible. If the reward function changes. All the values in the lookup table have to change. Whereas in the model based learning system, it's more flexible. If the reward at some state changes, we just change the reward function at that state and that reward will propagate to all of our values via the reward function we've defined, but it's slower. A model free learning technique called a temporal difference. Learning extended the risk score la Wagner model by introducing a discount factor into the prediction error, which helps define how much a reward matters to an agent. Depending on when in time it's received, meaning we can make rewards that happen in the near term worth more.

Speaker 1:          09:56          It was invented by two researchers named son and Barto in the late eighties while they wrote their defining work, an introduction to reinforcement learning. Unfortunately, no one really cared much for TD learning until a paper came out in the late nineties a decade later, which showed that the temporal difference model could accurately represent dopamine response in the brain. That's dope. If we just represent a reward by itself without any cues just to reward, we'll see a burst of dopamine neurons. However, if that reward is predicted by some cue, so the Q reliably predicts a reward, we won't see a dopamine burst in response to the reward. Instead, we'll see a dopamine burst as a response to the queue that predicted that reward. This contradicts the popular view of dopamine as a reward molecule. If it was just reporting reward, then we'd expect it to respond when reward was delivered, regardless of whether that reward was predicted or not.

Speaker 1:          11:01          And in fact, it only responds to unpredicted rewards when reward is surprising, the TD model helps explain why that's happens. Once the reward is fully predicted, there will no longer be any prediction error. And the dopamine response is reporting these prediction errors from the model. It's been used to create game bots that can be humans most popularly by deep mind and it's deep cue learning algorithm able to beat many Atari Games. Td learning helps capture some important properties of temporal dynamics as well as dopamine responses, but it lacks the uncertainty tracking mechanism of the common filter. So we need a Basie and version of TD learning and we can call that the coal mine TD instead of just estimating a single value of the weights. We're estimating amine and covariance matrix for the weights of our model. We can think of all four of these models along two dimensions based on what kind of estimate or they are either Bayesean or point based, and what the target they're trying to estimate is either immediate reward or value.

Speaker 1:          12:10          Here are the three points to remember from this video. Associative learning is a learning process in which a new response becomes associated with a particular stimulus. When we build mathematical models of learning, we can use distributions instead of single values to help represent uncertainty about the world. And Tim poral difference learning is a model free learning technique that predicts the expected value of a variable occurring at the end of a sequence of states. You're good looking and smart a plus for making it to the end of this video. Hit subscribe and I'll give you an a plus. Plus for now, I've got to chase a reward, so thanks for watching.