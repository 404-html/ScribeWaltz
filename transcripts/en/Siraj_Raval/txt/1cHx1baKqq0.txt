Speaker 1:          00:00          Bowling world, it's Saroj and today we're going to do some GPU programming to add the elements of two arrays together. Sound super simple, I know, but it will be a way for us to introduce ourselves to the key concepts that make up the incredible world of parallel computing used heavily in deep learning. The field of computer graphics traces its roots back to the earliest days of computer science, HP and IBM or some of the first companies to introduce silicon hardware capable of generating imagery which helped fuel the decades long transformation of the southern SF bay area into the technology hub known as Silicon Valley. There were endless applications of computer graphics in everything from scientific simulations to movies to video games. Programmers created graphical software and that software was broken down into a series of instructions that the central processing unit of a computer was capable of carrying out.

Speaker 1:          00:58          The CPU is the main chip in a computer responsible for telling all the other components in a computer what to do using it's given instructions just like your friendly neighborhood dictator. If we look at the difference between your early games and modern day games, it is truly remarkable how much of a difference in realism there is. The process of rendering a seen consists of a number of steps a computer must perform in a specific sequence and the sequence is called the graphics pipeline. The first step is to set up the objects in the scene. Object models are a mesh of triangles that describe its shape. This is done by listing the exact coordinates of every corner of every triangle. Coordinate systems need an origin and in a model's file, it's just an arbitrary place, but the place, the model in the scene, all the vertex coordinates need to be transformed so that the origin is now a place in the scene.

Speaker 1:          01:53          Once the objects are placed in the scene, the coordinates are again transformed into camera space. These transformations require the use of matrix operations for older games. With simple models. This was entirely doable by a CPU, but as models got more complex, the amount of transformations outgrew the capabilities of a CPU alone. So a company named and video came along and released. The first graphics processing unit in 99 capable of hardware transform and lighting a CPU consists of a few cores optimize for sequential serial processing. While a GPU has a massively parallel architecture consisting of thousands of smaller, more efficient course designed for handling multiple tasks simultaneously. So while traditionally software has been written for serial computation where instructions are executed one after the other, there are some algorithms that can be framed to utilize parallel computation. Searching for a specific word in a document, for example, can be done serially.

Speaker 1:          02:55          That means iterating through every word from start to finish, but it can also be done in parallel by having 10 different workers search a different section of the document for our target word at the same time. But some algorithms like calculating the Nth FIBONACCI number can't be made parallel since we can only find it after the proceeding to Fibonacci numbers have been calculated in order. Matrix operations for deep learning can easily be parallelized. Deep neural networks can have millions of parameters to train, all of which can be done much faster by utilizing GPU. Yes, there are several programming frameworks out there that led programmers harness GPU course like open cl opiod ACC, but the most popular is Nvidia's Kuda, which stands for compute unified device architecture. Kuda is a toolkit that acts as an extension of c with its own programming model that lets programmers run their code on and videos gps.

Speaker 1:          03:53          The Cu Dnn library built on top of Kuta specifically for deep learning is used by every single major framework from tensorflow two Pi Torch under the hood. So let's talk about Gpu architecture in the way that could have used it. The key part of Kuta code is the kernel program. The kernel is a function that can be executed in parallel on the GPU. It's executed by an array of Kuta threats. All threads run the same code and each thread has an id that it uses to compute memory addresses and make control decisions. You can run thousands of these threads on the GPU and Kuda organizes threads into a grid hierarchy of thread blocks. A grid is a set of thread blocks that can be processed on the device. In parallel. Each thread block is a set of concurrent threads that can cooperate amongst themselves and access a shared memory space private to the block.

Speaker 1:          04:47          It's the programmer's job to specify the grid glock organization on each kernel call since it can be different each time within the limits set by their specific GPU. Let's write some Kuta code to get a better understanding of how all these components play together. We'll start with some basic c plus plus. Our program will simply add the elements of two arrays with a million elements, each only two dependencies io stream, which will help us take input commands and show output results and math, which defines various math operations we can use in the main function. We'll initialize an integer array with space to store a million elements. Then we'll initialize pointers to both of our arrays, x and Y, which space allocated for both of them using our previously initialized variable and we can fill both arrays by creating a for loop and filling each index and each array with float values.

Speaker 1:          05:41          We'll use both filled arrays and the number of elements in each as parameters to an add function. In our ad function. We'll create a for loop to add all end elements in each array and set the result to each index in the y. Ira. After this loop is finished, why contain the sum of each elements in both arrays? When we're done, we can free the memory from both a res and exit the program by returning zero. We can save the code in this file as ad dot CPP and compile it with a c plus plus compiler claim for me since I'm on Mac g plus plus if you're on windows or Linux, that was a simple c plus plus program. No Kuta needed but adding the elements in both arrays can be parallelized on many cores of the GPU, so let's get turnt. The first step for us is a Turner had function into a function that the GPU can run called a colonel in Kuda.

Speaker 1:          06:31          All we have to do is at the global specifier to the function and that'll tell the Kuda c plus plus compiler that this is a function that runs on the Gpu and can be called from CPU code. We also need to allocate memory that is accessible by not just the CPU but the GPU as well. Karuna uses the concept of unified memory to make this easy. It provides a single memory space accessible by all gps ncps on your system. We'll need to replace the calls to new in our code with calls to Kuta Malaak managed. This will allocate data in unified memory and returns a pointer that we can access from CPU or Gpu code. After this, we can launch the ad colonel on the Gpu by using the triple angle bracket syntax. We'll just add it to the call to add before the parameter list and the CPU needs to wait until the kernel is done before it accesses the results.

Speaker 1:          07:25          Because Kuda colonel launches don't block the calling CPU thread. We can make this happen by calling Kuda device synchronize. Before freeing the memory are one liner launched one GPU thread to run ad. We can save this code in a file called ad.cu for Cuda and compile it with NBCC, the Kuda c plus plus compiler as written. The kernel is only correct for a single thread. Since every thread that it runs, we'll perform add on the whole array. We can find out how long it takes to colonel to run by using the GPU profiler as well, but can we make it faster? Our execution configuration tells the cooter runtime how many parallel threads to use for the launch on the GPU. There are two parameters here. Let's change the second one, which is the number of threads in a thread block Kuda gps, Ron kernels using blocks of threads that are a multiple of 32 in size, so let's try to 56 but doing this alone will cause it to do the computation once per thread rather than spreading the computation across parallel threads.

Speaker 1:          08:29          To do it properly, we need to modify the kernel. We can get the indices of the running threads using the thread id keyword for the current thread and it's block and the block dim keyword. For the number of threads in the block, we can modify the loop to stride through the array with parallel threats. If we save it and run it again, we'll see a huge speedup. It went from using one thread to two 56 threads, so this makes sense. To summarize what we've learned. Kuda is a toolkit that lets programmers use Nvidia's Gpu to parallelize their code. It's an extension of see with its own programming model consisting of threads, blocks and grids and the newer version uses a unified memory architecture which lets us access allocated data from CPU and Gpu Code. Winter of last week's coding challenges out there too. He used to neural network based models that he implemented in tensorflow.

Speaker 1:          09:25          We popular indicators from the field of market analysis to predict trends give his costar star on get hub and the runner up is who top, who converted to stock prices into audio data and ran deep minds wavenet on it. What a while and brilliant idea. I love it. This week's challenge is to add Kuda to a simple algorithm and benchmark just how fast the speedup was. Details are in the read me poster, get hope link in the comments and winners will be announced next week. I hope you liked this video. Please subscribe if you did, and for now I've got to free up some memory, so thanks for watching.