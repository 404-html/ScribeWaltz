Speaker 1:          00:00          Hello world. It's a Raj. In this video we're going to use genetic programming to identify if some energy is gamma radiation or not. I'm getting angry gamma rays. No, I wish data science is a way of thinking about discovery. A data scientist need to decide the right question to ask like who's the best candidate to vote for in the U s election? Then decide what data set to use like tweet history of candidates and pass endorsements of each candidate and lastly, decide what machine learning model to use on the data to discover the right answer live goes on with the right data computing power and machine learning model. You can discover a solution to any problem but knowing which model to use can be challenging. For new data scientists. There are so many of them. That's where genetic programming can help. Genetic Algorithms are inspired by the Darwinian process of natural selection and their use of generate solutions to optimization and search problems.

Speaker 1:          01:01          They have three properties, selection, crossover and mutation. You have a population of possible solutions to a given problem and a fitness function. Every iteration. We evaluate how fit each solution is with our fitness function. Then we select the fitness ones and perform crossover to create a new population. We take those children and mutate them with some random modification and repeat the process until we get the fittest or best solution. So take this problem. For instance, let's say you want to take a road trip across a bunch of cities. What's the shortest possible path you could take to hit up each city once and then return back to your home city? This is popularly called the traveling salesman problem in computer science and we can use a genetic algorithm to help us solve it. Let's look at some high level python code. We have the number of generations set to 5,000 and the population size except to a hundred so we start by initializing our population using our size parameter.

Speaker 1:          01:58          Each individual in our population represents a different solution path. Then for each generation, we compute the fitness of each solution and story in our population fitness array. Now we'll perform selection by only taking the top 10% of the population, which are our shortest road trips and produce offspring from them by performing crossover. Then you take those offspring randomly and repeat the process. As you can see in the animation, eventually we will get an optimal solution using this process unlike apple maps. All right, so how does this all fit into data science? Well, it turns out that choosing the right machine learning model and all the best hyper parameters for that model is itself an optimization problem. We're going to use a python library called teapot built on top of psycho learn that uses genetic programming to optimize our machine learning pipeline. So after formatting our data properly, we need to know what features to input to our model and how we should construct those features.

Speaker 1:          02:54          Once we have those features, will input them into our model to train on and will want to tune our hyper parameters or tuning knobs to get the optimal results. Instead of doing this all ourselves through trial and error. Teapot automates the steps for us which genetic programming and it will output the optimal code for us plates done so we can use it later. So we're going to create a classifier for gamma radiation using teapot after installing our dependencies and then analyze the results. Teapot is built on the popular psyche learn machine learning library, so we'll want to make sure that we have that installed first. Then we'll install Penn does to help us analyze our data and num Pi to perform math calculations. Our first step is to load our Dataset we use. Penn does read CSV method and set the parameter to the name of our safe CSV file.

Speaker 1:          03:39          This is data collected from a scientific instrument called a Charin [inaudible] telescope that measures radiation in the atmosphere and these are a bunch of features of whatever type of radiation it picks up. Thanks Putin. Since the class object is already organized, we'll shuffle our data to get a better result. The eye lock function of the telescope variable is pandas way of getting the positions in the index and will generate a sequence of random indices. The size of our data using the permutation function of num Pi's, random submodule. Since all the instances are now randomly rearranged, we'll just reset all the indices so they are ordered. Even though the data is now shuffled using the reset index method of pandas with the drop parameter set to true will now let our Televerde will know what our two classes are by mapping both of them to an integer with the map method.

Speaker 1:          04:27          So g or gamma is at the zero and h or Hadrian is that the one? Let's store those class labels, which we're going to predict in a separate variable called teleclass and use the values attribute to retrieve them. Before we train our model, we need to split our data into training and validation sets. We'll use the train test split method of psychic learn that we imported to create the indices for both. The parameters will be the size of our dataset. We want both sets to be a raise. So we'll set the stratified parameter to our array type and we'll define what percent of our data we want to be training and testing. With these last two parameters. We have a 75 25 split now in our data and we're ready to train our model. We'll initialize the teapot variable using the teapot class with a number of generations set to five on a standard laptop with four gigs of Ram.

Speaker 1:          05:13          It takes five minutes per generation to run, so this will take about 25 minutes. This is so teapots genetic algorithm knows how many iterations to run for and we'll set verbosity to two, which just means show up progress scarred in terminal during the optimization process. Then we can call our fit method on our training data, let it perform optimization using genetic programming. The first parameter is the training feature set, which were retrieved from our tele variable along the first access for every training index. The secondary level is our training class set, which will retreat from our Televerde bubble like so. We can compute the testing error for validation using teapot score method with validation feature set as the first parameter and the validation class set as the second we'll export the computed python code to the pipeline.py class using this method and name it in the perimeter as a string.

Speaker 1:          06:00          Let's demo this thing. After training we'll see that after five generations she pot chose the gradient boosting classifier as the most accurate machine learning model to use. It also shows the optimal hyper parameters like the learning rate and number of estimators for us boy, so to break it down with the right amount of Beta computing power and machine learning model, you can discover a solution to any problem. Genetic Algorithms replicate evolution via selection, crossover and mutation to find an optimal solution to a problem and teapot is a python library that uses genetic programming to help you find the best model and hyper parameters for your use case. The winner of the coding challenge from the last video is Peter Amatrano. He added some great deep dream samples to is repository and even deep dream. My own video, bad ass of the week, and the runner up is Kyle Jordan. Good job stitching all the deep dreams frames together with one line of code. The challenge for this video is to use teapot and a climate change dataset that are provide to predict the answer to a question you decide. This will be great practice and learning to think like a data scientist posts. You're getting humbling in the comments and I'll announce the winner next time for now. I've got to stay fit to reproduce, so thanks for watching.