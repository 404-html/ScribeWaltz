Speaker 1:          00:00          Did her Duh, Duh, Duh Duh Duh Duh Duh Duh Duh, Duh. I've been listening to a lot of classical, hey, my live, I must be live right now because I hit the live button. I can't see myself. So I always have to wait like 15 seconds or so. There I am. Okay, great. I agree. Oh, we're all, it's Saroj. How's it going? Uh, so today we are here live and we're going to make some word vectors out of a series of books called game of Thrones. If you know game of Thrones, give it a shout out in the, in the comments. Let's see who knows what this is. It doesn't matter if you don't. The point is we are learning about the concept of word vectors and we want to take some books and make them into vectors. And once we have these vectors, we're going to do a bunch of really cool stuff with it. All right, so who's in the house? Let me name some names. We got Jake Akash party to hair, teddy recky party. Ricardo Angel got a lot of people in the house. All right, so, okay. Uh, let's go ahead and do a five minute Q and a and then we're going to get into the code. It's going to be an IP on notebook and I'm going to give you guys a the data set as well. So a five minute Q and. A. Let's get started.

Speaker 1:          01:30          Glove or word to Vec. We're using word to VEC, not glove. Uh, although glove is similar also. I haven't used gloves before, but I've heard good things.

Speaker 1:          01:42          Another question. Best character in game of Thrones. Uh, to be honest, I don't even read game of Thrones. I just picked it because I thought it was, I thought the, I think the idea is cool. I don't have time to read anything or watch TV shows and any more I'm just focused on content. Uh, any mass we are going to do. Yes. We're going to do some math. We're going to use the co sign similarity, uh, as a measure of distance between word vectors. It's a corpus. Yes, it's five different books, but we're going to treat it as one big corpus. Please say my name to use. Okay. I'm okay. Doing the deep learning foundation and feeling utterly lost. Keep pushing through or go back to school. Okay guys, listen. Okay. So clearly this is, there's a lot of stuff in deep learning and there's a lot of math and from what I've read, I have taken your feedback into consideration.

Speaker 1:          02:32          We need to really dive into the math and we're going to, we're going to go so deep into the math and the next video. Okay. So the next weekly video, get ready. We are going to really dive into the math. Please say my name. Colon. What's a word? Vector. I'll explain that in a second. Cleaned the camera. There's moisture. Okay. Okay. Um, video isn't clear at bro. Okay. Uh, can't help. That right now is opening. I worth to explore. Yes. One minute. Wrap before I love you rap. I'll do that. Um, yeah, I'll do that. Um, let me answer some more questions. How can I classify images from a live feed and put the output in a file from a live feed? You'd want to, Oh man. In a live feed. I can we, well we know how to do it recorded, but in a live feed you'd want a live classifier to look at this, to look at your screen or maybe segment the screen. So you'd want to use probably Java script for that. Javascript would be easy. Uh, con net dot. Js Andre car, these library would be great for that. The Cam looks dirty. Hey Man, I was just at the beach. I'm recording some cool stuff. Uh, predict who dies using word vectors that that's gonna be possible. Are you going to use tensorflow for this? No, we're going to use a word two VEC and we're going to use a bunch of other smaller libraries. All right. Okay. So one more question and then we're going to get started.

Speaker 1:          03:55          Okay. Here we go.

Speaker 1:          04:04          If we increased vocab size by adding words, will that help? Yes, yes. It helps. It helps if, if, well, it helps if, um, if the words are relevant to that, to the problem we're trying to solve, uh, if, if they're relevant to the, uh, the story which is game of Thrones in this case. Okay. So that's it for the questions. Let's get started with this. I'm going to start screen sharing. It's going to be an eye python notebook. Uh, and then we're going to, oh, that's right. Iraq. Uh, let's, let's do a little wrapper seconds. Um, uh, let me do a wrap on vectors.

Speaker 1:          04:40          Vectors, right. Okay. So it starts with Troy with the wrap. Okay. Uh, here we go. Ah, I rap about vectors. I do it man. Cause I'm victor, victor, Victor. What? The Vector Victor, I'm on a plane going crashing down like my mind is a scalar scalar. Clouds going down. Stay low. Clouds go away. No, I see one, two, three in the sky and it's my enemy. No, it's not. It's just mine. And I'm going to fly back to this screen. So here we go. That was it for the rap. And let's get started because we are getting serious guys. This is all about words that gets us an English favorite movie. The Matrix, the matrix. How about that? I'm just going to add to the list of people who set the matrix. Let's get started. This is an NLP session. We're going to do some natural language processing. Ready. Okay, let's, let's start screen sharing. Let's start screen sharing. Okay. Okay. Let me move you guys over here. Okay. All right. So let's, uh, get started with this. Let me make sure this is showing,

Speaker 2:          05:59          hold on.

Speaker 1:          06:00          Okay. So,

Speaker 2:          06:03          uh,

Speaker 1:          06:04          we want to, let me just first talk. Say we want to the, the, the goal is to create word vectors from, from, uh, game of Thrones, a Dataset. Now look guys, this is, these are just the five books. Let me show you guys now. I'll just show you guys, uh, and play with them and uh, and analyze them to see semantic similarity. Okay, let's, let's look at what we've got here. So hold on. Uh, I can even show you guys the Dataset for a second. So these are just the five books for game of Thrones. Okay? There's literally just a text files for the books. They are these huge text files for the books. That's, that's, that's all they are. Okay. And I just divided them. I downloaded them from pirate bay. No regrets. Uh, you know what I'm saying? So that's what, that's what this is. And that's it. We've just got five books in the series. We're going to take all these books and we're going to create word vectors from them. We're going to treat it as one big corpus. Okay?

Speaker 1:          07:08          So that's what we're going to do. So let's just go dive right into this baby. All right. So the first thing we want to do is we want to import our dependencies. Now, we've actually got a lot of dependencies for this. Um, so I'm going to explain every single one. So the first one we want to do is import future. And why do we want to import future? Can anyone tell me why in the comments as I type this, the reason we want to import future is because it's the missing link between python two and python three. It allows us to use the syntax from both. It's, it's kind of like a bridge between the two languages, and we're going to import three functions that we're going to use for this. Okay? So that's the first step. Uh, once we have that, we want to, uh, we're gonna, we're gonna encode our words. So we're going to end for word and coding. How, how are we going to encode our words while we're going to use codex. The next one is we're going to perform some red rejects and Reggie [inaudible] is basically whenever we want to, uh,

Speaker 1:          08:08          we went to like search and file really fast. That's, that's, that's all about Red Jack's. It's, it's uh, it's a, it's a way of like quickly and efficiently searching through a large text or number database for what you need. Uh, the next one is for logging. So actually we don't need it. We need, we don't need to log. Oh, now I actually have been talking about concurrency before. So this is going to be interesting. We're going to import this multiprocessing library to perform concurrency. And if you don't know, concurrency is a way of running multiple threads and having each thread run a different process. So it's, it's, it's, it's multithreading multiprocessing it's a way of moving. It's a way of having your program run faster. Okay. And I haven't talked about this before, but we're going to do this a lot in the future, especially when we get to distributed machine learning, uh, which is later on in this course.

Speaker 1:          08:57          Okay. So, so that's it for multiprocessing. The next one is dealing with the operating system operating system. Uh, like we want to like reading a file, like reading a file and for that we will want to use pos module and then we want to do some pretty printing, make it human readable. And how do we do that? We were going to import pretty cranky. Cranky for sure. Okay. Uh, the next one is more regular expression. So glob was uh, and I think has a different tier, but this is for, this is for a more granular regular expressions. So this is, so this is like step one by the way. It's like step zero important dependencies and I've got a few more and then we're going to get started with the actual logic of the code. Okay. So, uh, what else do we got? We got the natural language tool kit cause we're gonna be using NLTK.

Speaker 1:          09:42          Okay. Natural language tool. Let me show you guys an Ltk for a second. If you, if you don't know now, you know, let me show you guys lck because NLP, Kay, I made videos about this before is bullshit. Okay? NLTK is awesome. It is so easy to use. Let me zoom in on this thing. Okay. Literally it can tokenize San Francisco and single cent. We'll single lines of code. So if you have a sentence like at eight o'clock on Thursday morning, Arthur didn't feel very good. You, you feed that to NLT and boom, it'll give you the tokens for each word. Why is this useful? Well, you can have part of speech tagging, Pos tagging that, which means like, oh, is this a noun? It's a verb. Is this a CD? How does it know these things? Because it has a pre trained, well, it says it's actually two things.

Speaker 1:          10:24          It's for some, for some of it, it's got a pretrained model, which a trained, uh, and the, and another part of Nltk is, it's got, it's, it's using the, uh, which I talked about in the last video, which is the, uh, uh, like having that, that database, a prerecorded sentiments. The lexicon, that's the word I'm looking for. All right, so then we're going to talk, we're going to use word to Vec now work to beck is the shit that is what we are going to use. That is the real meat of this code. And I'm going to really deep dive into what word to Vec is at a high level right now. Worked with that is what Google created, uh, to basically they trained are, they trained a machine, the their own network on a huge dataset of word vectors for, and it created back and we can use these directors in other ways.

Speaker 1:          11:08          So it's like a generalized collection of word vectors. Okay. So where I'm going to talk about, I'll listen to a second. Let me just keep coming out. These dependencies, the neck length, dimensionality reduction. Once we have our word vectors, they're going to be multidimensional. They're going to be 300 plus dimension word vectors, okay. Because they're so generalized and we want to plot them on a two d graph so we can see them. And how are we going to do that? Well, we're going to perform, I could a technique called dimensionality reduction. I have a video on this called, uh, visualize in a set, easily. Check it out, check it out. Okay. Now, so then we're going to start our math library, which has none Pi. And then we're going to import a plot thing library, which is going to be Matt Line. And finally we're going to, uh, what's the next one? Arts. Well, of days that we got one more after this. So many libraries. But no, it's, it's, it's important because right now we're talking, I, we want to talk about the concepts here and we're going to dive into these specific processes later on. Okay? Uh, so import pandas as PD. And lastly, here's the last one. Visualization. Seaborne. Seaborne is going to help us visualize our data set. Okay. As that's an s okay. That's it for our dependencies. Boom. All right. No. Okay. So now that we've done that,

Speaker 1:          12:35          uh, we are going to, we're not using pretrained word vectors. We are, we are using word vectors that we train in real time. Uh, let's see. No model named pipe ply lot. Oh, pipe plot by plot. No model named bananas. Oh, pandas. So this just why I love python, python notebooks. Okay. So, uh, okay, so that was that. Now our next step is to process our data. Okay. So step one is to process our data. What does this look like? Well, before we do anything, before we do anything, we want to clean our data. So how do we clean our data? Well, NLTK has a really handy function for that as well. The first one is called punk and the next one is called stop words. So what does this do? What this does is it downloads punked downloads. Uh, it's a cocaine izer. It could pretrained tokenize or there's a pretrained tokenize or, and what it's going to let us do is tokenized our text.

Speaker 1:          13:31          And remember I talked about tokenization last time. Tokenization is where we take a sentence or a word and we take, sorry, we take a piece of text and we split it into tokens and those tokens to be sentences or words or even characters, whatever we specialize. In this case we're going to do sentences. Okay? So that's what punk does. And stop words are words like and or sorry and, and, and a no of words that don't really matter. They don't really have a lot of semantic meaning. And we want to remove these words and why do we want to do that? So that are our vectors that we create are, are more accurate. Okay. So, so, so we done that and it's going to download okay. And it says, okay, you've already got him. If we don't have them, it's going to download it. The next step is to get the book name, I think the texts file. And now like I said, we had been right here. So let me, let me make this bigger.

Speaker 1:          14:29          Hey, here's our books right here today and in textiles, so we can just say, okay, so get the book file names, book file names, and before we use Glob, Robin's going to let us get those books that just have that end in dot txt. Right? And it's going to print those out for us. Make sure that we actually, uh, that we, that we actually printed them. All right. The ends there and it starts here. Okay, boom, boom. Okay. No, no, no, no, no, no. There we go. Okay, so that's for our text file. And, uh, let me print out the books. Let's print them out. Let's print them out and make sure that we got them. File names sorted. glob.club. Let's see what we got here. Dot. Txt. Um, let's see. Hmm. I, so we should have them here and I guess we don't, then the problem is that we hold on sorted. Glob, duck, glob. Let's see. The, the, hold on a second. So, um, let's see. Let's see, let's see. Uh, interesting. So what the, what this is, is, um,

Speaker 3:          16:11          fun. Have. Okay,

Speaker 1:          16:23          let's see. Let's see. Uh, okay. So, okay. Uh, so it seems like they might be in the room. Yeah, they might be in the route. One second handle. What I can say is second closing folder.

Speaker 3:          16:56          Okay.

Speaker 1:          17:11          Hold on one second. This is actually not, oh, you're right. Right, right. Oh, what are people saying? Dot. Slash okay. Hold on a second. Oh my God. Somebody's people saying some shit. Hold on. Okay. Okay. Okay. Okay, so let's get that current directory. Swear to God. This is so annoying right now. Hold on. Okay, let's, okay, so, um, hold on a sec. Okay. So, oh my God. Okay. No, it's in tax book file names we talked about. I just named them right here. Let's see. Look file names. Oh, that's like,

Speaker 3:          18:42          yeah.

Speaker 1:          18:59          Endowed Syntax. Hold on. So we really don't have time for errors like this.

Speaker 3:          19:08          Okay.

Speaker 1:          19:09          God. Okay. Okay guys. So I actually had a comments with a lot of up votes and the comment was we should just not spend time actually writing out the code and just show the code since I'm already reading off of it anyway. Uh, so I already have the code anyway, so why don't I just show you guys the coat and I'm gonna explain it as we go. Okay. It's because seriously, I have, don't have time to look at why this is not working right now. Prince book file names.

Speaker 3:          19:44          Okay.

Speaker 1:          19:45          That's exactly what it was. Okay. So we don't, we don't have time for this. Okay. So let's just talk about this. Okay, let me, let me move this over here. Okay. And I've got my notes here as well. Okay. Hold on a second. So, okay, so where were we? Let me, let me see if this actually works. Why, let me just, let me, let me just, let me just talk. Let me just restart the toll I python notebook and run this from scratch. Okay, let's run it from scratch. I python notebook and let me close out everything.

Speaker 3:          20:29          Okay.

Speaker 1:          20:30          And, okay, so here we go. Hold on. That was I missed. Um, okay, so let's see, let's have it. Give it to us. Okay. And try out this one. Okay. So let's, let's just run what we have here. I love in line populating log. Log Log.

Speaker 3:          20:59          Ooh,

Speaker 1:          21:01          okay. Okay. CYL names. Okay. So clearly there is clearly we're having a problem right now. And the point is let's just, okay, so this is, this is pre compiled anyway and,

Speaker 1:          21:16          and talking about what this is doing and we're going to really deep dive into the important concepts here. Okay? So we're going to keep going onwards. We don't have time to stop with this right now. So okay. So we've got our book file names, right? And we next time is to combine the book that you want train. And why do we want to do this? Because we want to have wine, a corpus for all those books. And that's what this does. We initialize a rock corpus, we say you, let me, let me make this bigger because we are, we really want to, we start with you because it's Unicode, right? It's a unicode string and we want to convert it into a format that we, that we can read easily. And what does that format? UTF eight right here. Okay, so you TFA. So this is where the Codex library comes into play. We are using a codex library to read in the book file name and convert it into UTF eight format. Now that remember that corpus raw function we just initialize up here. Well now we want to add all of the books that we see to that Corpus. And the way we're going to do that,

Speaker 1:          22:11          the way we're going to do that is we're going to add,

Speaker 1:          22:18          we're going to add it all to this corporate fraud. And at the end of it, it's going to have all of those books in one in one variable in memory, corpus raw, okay. Which is going to be a very, very, very big Barrick variable. Okay? That's what we're going to do. And so that's the first step. And once we have that, then we're going to split the corpus into sentences. Now remember when I said we downloaded that, uh, pumped a model right up here. Let me show you guys nltk. Dot. Download punk. Well now we're going to actually load that into memory that it's a trained model and it's, it's loaded in, it's in a byte stream. And that's what pickle is. It's a, it's that file format that we can load as a byte stream. Now that's what that does. And it's going to load it up into this tokenize or variable. There's tokenized or is pretrained it turns words into tokens and the type of tokens we want our sentences in our case, right? So we'll use a token izer uh,

Speaker 3:          23:11          okay.

Speaker 1:          23:11          Well use a tokenized or to tokenize that corpus that which is every single word we have, right? And let me, let me, let me open this one. So every single word we have, and this can be anything guys, this could be any piece of text you want, any book, anything you download, any big piece of tax this same, the same principles apply. Okay. Um, and

Speaker 3:          23:32          okay,

Speaker 1:          23:32          we're going to put those all into this raw sentences variable. Once we had that raw sentences variable, we're going to convert it into a wordless. So what do I mean by a wordless? Well, um, I also want a second. Sure. So for our word list or we want a list of words, but, right, but, but, but what, what exactly do I mean by that? Let me, um,

Speaker 1:          24:01          so for our word list, let me, let me see the comments. What are you guys up to? Uh, okay. So this is, okay. So let me, let me come at this as well. So I'm going, you convert into a list of words, remove unnecessary characters. We want to remove unnecessary characters. That's what we have. That A to Z, a to Z. I'm going to split the split into words, no hyphens, you know, and, and it's going to be a list of words. So that's an array. So it's a, sorry, it's a list of words. Okay. Once we had that list of words per each sentence, we're going to take those words and tokenize it. So there's going to be a sentence where each word, a sentence where each word is tokenized. That's what this side, so for every sentence that we have, we're going to initialize an NP sentence list and we're going to add it. We're going to add sentences to it. And,

Speaker 2:          24:51          uh,

Speaker 1:          24:57          and then once we have that, uh,

Speaker 1:          25:02          all right, so I've got some comments that I should slow down a little bit. I can do that. I can do that. Okay. So let me slow down a little bit. There's so much I'm trying to cover, so let me, I'll slow down. Okay. So we have a list of words. Each word is tokenized. Okay. Each list of words is considered a sentence. And we can see that when we print it out. So we have our raw sentence, which is, he was an old man, passed 50. He seen as the Lord links come and go. This is a sentence taking directly from game of Thrones and it's at the, it is, it's at the fifth index. So if, if, because we combine all of our books in order, this is going to be the fifth sentence and the first book because it's one big corporates. Okay. And once we have that,

Speaker 1:          25:49          uh, once we have that, we're going to convert it into a word list, which is tokens and cds. You, uh, characters, they basically help us convert them into their unicode representations, right? So whenever we're dealing with words, any kind of tax, we have to make sure it's in the right format. And unicode is that format. We want four vectors. Okay. Uh, and, and UTF eight is that format for reading from files. So once we have our big book Corpus, let's print, print out how many tokens we have. Okay. Um, and each sentence is, and what we're going to do is we're going to consider each,

Speaker 2:          26:22          uh, uh, uh,

Speaker 1:          26:25          each, each sentence a token, and we can print out a 1,000,800, 10,000 tokens. Okay. So, okay, so no, we're going to get into vectors. I'm gonna explain back to you in a second in detail. We haven't gotten to that part. Okay. So that's what we've, that's what we've just done. Now we're going to train to VEC. Okay. These are our hyper parameters. Let's, let's, let's talk about vectors for a second. Like word. Okay. So I'm trying to find a great image for this kind of second. So word embeddings are here. So tensorflow probably has a great image for this. So, okay, so here's a great one. Here's a great one. Copy image address. Let's blow this image up. Let's get it really big. Okay. So here are vectors, for example. So we'll get the one on the left. Okay. Look at the one on the left. King Man, woman, Queen. These are word vectors. So when we have a set of words, okay, we have a set of words like, so let's say, um, you know, um, masculine, uh, John, uh, you know,

Speaker 1:          27:33          Sarah Gay, you know, like words that are like, like men, right? So, so, so where does that all have the same semantic similarity? We can all, we can generalize all these words into a vector representation. We also call them word embeddings. So there's a lot of terms here, but we want to make sure that it's, it's all really the same thing. It's, we don't want to be confused. Word embeddings word vectors, same thing. So if we have a set of words, we can generalize them into man. Okay? So that's what man is. Okay. So man, woman, King, queen. These are generalized Decter representations that we have created after training on a very large corpus of text. Okay? So when we have a new word, like we say, like, uh, you know, what's, what's something manly? I mean, this is, this is right now, this is a gender landline. I mean, this example, but let's, let's, let's pick an example. Like what do men have that women dotes um, this is like a, you know,

Speaker 1:          28:31          right. So a penis, right? That's the only thing I can think of it. It's like literally true and doesn't cross any kind of, uh, you know, other things. So Penis, right? So penis would then, uh, if we were to feed it to a, a trained word, two VEC model, it would see that penis is closest to man. Okay. Why does it know this? Because it's, it's trained on a corpus of text to know, to eventually find the similarity that, that generalized representation across all of these words. And how does it do this? Well, it converts words into vectors and then it creates vectors of vectors and he's vectors are, when we, when we plot them out, we can see how similar different things are. Yeah. Beard was a great one as well. Okay. So balls. Okay. Coz It's attached. Right? And so then once we, once we have these vectors, we can see what's similar to them, right? We can do all sorts of things. Mainly, there are three things we want to do. Okay, let me write that down. Write this down. So write this down. This is a very important, so once we have vectors,

Speaker 1:          29:33          three main tasks, there are three main tasks, tasks

Speaker 2:          29:42          or some women have beard. So three main tasks. That vector is help quit. Okay. Here are the three main types. Uh, the one of them is resistance. Okay. Hold on. Uh, similarity, similarity and ranking. Okay. So those are generally what they help us with. MMM.

Speaker 1:          30:06          And what do I mean by that? Similarity? Like vectors are good for things. Like if we want to, if we want to think about, if we think about, uh, any kind of,

Speaker 2:          30:17          uh,

Speaker 1:          30:19          what's the word I'm looking for? Like if you want to see what's, how similar are two words are, which we're going to do later on if you want to rank for something, right? So if you wanted to browse all the scientific papers in the world will create vectors out of all of them. And then we want to rank them in terms of some metric that we decide, like what's the one that has the most information on say, a climate change? We could then rank them and semantically. So it would say, here's the number one paper that has the most, uh, verb, verb usage or word usage about the, about the topic, climate change. And it uses vectors for that. Okay. These are really useful. Okay, so let's go ahead and talk about these hyper parameters. So numb features is a dimensionality of these word vectors. Okay. Um, so

Speaker 1:          31:04          we're, so number of features is the dimensionality and we say 300 because we say 300, because this we could say 400, we could say 500, the more dementia. Let me say, let me cut this up. Some more dimensions. The more complex, so more calm, occasionally complex expert, sorry, expensive to train. Uh, but, uh, but also more accurate. Okay. So the more dimensions of vector has, the more general lot more dimensions means more generalized, more generalized. Okay. So we're going to say 300 for now. Okay. Uh, minimum word count threshold is, uh, what is the, the, the smallest a set of words that we want to recognize. Okay. When we convert to a vector, the number of threads to run in parallel. So if we are,

Speaker 1:          31:58          so what does the actual structure of a vector that said it's a great question. Oh, the structure of a vector. So the definition of a vector, and we're going to talk about this in the next video, but the deaf we're going to really deep dive into later. But the definition of a vector is a, uh, it's, it's a set of numbers. Okay. And, uh, in this context, in machine learning and physics, it's got a different context. We talk about the, it talks about a direction. In our case, we're just talking about a set of numbers. So we can think of it as a list of a huge list of numbers. Okay. That's what, that's Kinda what it's represented as. Um, okay. So and intense, or for weeks we use the word tenser because attention is an end dimensional, uh, array of numbers. So I'm factor is a type of tensor.

Speaker 1:          32:43          So Becker is a type of cancer. So this doesn't really belong here. I'm just saying this right now. Vector is a type of tensor. Okay. So, so then the number of threads running parallel, this is where that multiprocessing library that we imported comes into play. Okay. We want to say, how many workers do we have? So the more we have the faster model train more we're workers faster, we train context. Window length is the size of, of the, of, of, of what we're looking at at, at, at a time like this, the size of like, if we think of it as like looking at blocks of seven words at a time, that's the context window down. Downsample setting for frequent words is a hold on. Scalar. A vector Matrix tensor. Exactly. We're going to be talking about those in the next video. But uh, downtempo settings for frequent words is uh, once we have, once we, once we are trained, word to Vec model is noticing a lot of frequent words. We don't want to have to look at them constantly. So any number between zero and one knee negative five is good. For this. It's the best. Generally that's, that's what we found is a, is a, is a good, uh,

Speaker 1:          33:59          it's, it's generally a good use case for this. Basically. How often do we want to look at the same word? The more frequent and where it is, the less we want to use it to create vectors because it's already a part of our train model. Uh, so then the seed is for the random number generator, right? So that's what that is. Random number generator. And why do we use a random number generator? We use it to pick what part of the text we're going to look at your attorney to vectors. Okay. Um, and to see, make sure that it's deterministic. This is good for lugging the terministic. Good for debugging. Okay, so this is our actual model right here. A word to Vec model we imported from the Gen Sim Library. And let me tell you guys Jensen for a second chance. And it's super useful.

Speaker 1:          34:44          Uh, it's for topic modeling. Basically you give it any kind of corpus like this, it'll create a model, it'll train it, you can save it, you can load it later on. And then given some words like woman and King, something that was in the, that was something that was in the actual corpus. It'll give you words like, it'll give you things like the how similar they are. Uh, uh, what does it match? Uh, what's the, what give you the straight up directors so you could use it later on. Jen Sims, a great library. Okay. So that's going to actually train our model. And this is going to take, this is our model is relatively small in the context of deep learning. This is the only going to take 30 seconds or so to train. Okay.

Speaker 3:          35:25          MMM.

Speaker 1:          35:29          All right. So, and it's actually not the same definition as in physics. And there's a lot of debate about this actually. I've been looking at a lot of stack answers and a lot of core answers and it's crazy how much people are debating over these words and machine learning. But the point is, it's just numbers. It's number representations that we, that we create and then we can feed into our model. Okay. So then we're going to build our vocabulary and

Speaker 1:          35:58          uh, so then we're going to build our vocabulary, uh, using those sentences. Okay. This is how we actually, uh, this is how we load the corpus into memory. We haven't actually trained it. We built our model, right? This is step three, build our model by which I should, should have broken up written up here. So step three is built model mark. Okay? So once we built our model, we have loaded our corporates that we cleaned [inaudible] memory and we printed out the size of it. Now we can start training and it's going to train on all of those sentences we gave it. It's going to take 30 or 30 or 40 seconds. And when it's done training, we're going to save the file for use later on. Okay. And easily do that. You can get that OSTP module great for that. We can, we can save it and we can, we can save it and then we can load it later on. In fact, we can load it right now. We'll load it from memory right now. Okay. Um,

Speaker 3:          36:53          and

Speaker 1:          36:56          once we have that,

Speaker 1:          36:59          we're going to compress those. Okay? So that's going to be 300 dimensional word vectors, one of train. So in this Thrones to Beck, uh, model right here that we've trained, it's going to have, it's going to contain all of those word vectors that we just trained. He's going to, everything is in memory right here, but these are 300 dimensional word vectors. Okay? We cannot map a 300 dimensional word vector on a plot for us, puny humans to see how are we going to do that? Well, we're going to use a method called [inaudible], which stands for teeth. What does it taste? The tastic distributed Haber and embedding. Okay. And I have a great day. I hear you on that. Um, basically, let me, let me just, um, it's an, it's an awesome technique. This is not useful, but

Speaker 1:          37:50          basically a great big on that. Let me, let me just have the video name for that. Uh, PCA is another, my video. It's called a, what's it called? How to visualize, I Beta set easily. I really dive into this, this, this method right here, how to visualize the data set easily in a nutshell, TSAE cooks or 300 dimensional vector and squashes it into just two dimensions. Why? So that we can then plot it and view it. How does it do this? It's actually a long explanation and the video is great for that five and explanation. Definitely check it out. Okay. Um, but uh, right, so once we, so that's what Tsmc does. It's going to create those vectors and so it's going to squash it. Okay. And we're going to take all those doctors and put them in one gigantic matrix, right? We've initialized, yes. Any here, but we haven't trained tsne right? So TSMC is a model. It's a machine learning model and we have to train it. Okay? So we'll train it on that word vector matrix. And this is gonna take a minute or two like it says and

Speaker 2:          38:58          uh, uh,

Speaker 1:          39:03          so, uh, it's going to create this word vector. It's a two D Matrix, right? So this is one gigantic matrix and it's got the plots on the points with it. Okay? So then we're going to plot what we've got. Okay. So what do I mean by plot? Well,

Speaker 1:          39:18          we want to apply to it in to d sprays space. So for every word we have in that vocab we went to, we want to have, we want to have three columns. The word, the x, the x coordinate, and the y coordinate. Now, how does it get these coordinates? Well, that's what TSN he does. Not only does it, does it, uh, not only is it, uh, so it's squashing these vectors into a two dimensional vectors, but it's also giving us the x and y coordinates of those vectors and in two dimensional space, okay? So these are all words from that Corpus, right? These are all game of Thrones, the words, right? So that's what that does. And

Speaker 1:          40:03          uh, once we've got that, then we're going to plot them on a graph. So this is where map plot life comes into play, right? We're going to plot these points, we're going to plot them on a graph. And it's a lot. These are our word vectors. There's, there's a lot of them here, right? And we've, we've, we brought it down to scale so we could see a lot of them, but all of our word vectors or word embeddings, whatever you want to call them, are here in to the space. Now what are we gonna do with them? Well, we can see what vectors are close to each other.

Speaker 1:          40:33          Let's, let's start with that. Let's see what word vectors are close to each other and what that tells us about the data. Okay. So, uh, the first thing we want to do is zoom in on this, right? And that's what this function does. It creates a bounding box of x and y coordinates in that graph that we have. And it shows just that bounding box. That's what this function does. Okay. And so then we'll use that. We'll use that to, we'll say, okay, so in the bounds of this and the, the x y balance of, of these coordinates, and we give it, let's see what we, what it gives us. This is what it gives us when we look in this corner. Well, what are all these, um, these are names, Barista and Gregor, Saigon Sander. These are all names and they look like male names as well, right? They, they, they look pretty much mail. Okay. So interesting. Just by training vectors on our, our model, it created, uh, just by training and creating vectors and applying them in a two dimensional graph using TSMC. Uh, I remove stop words and special characters. A software's been included in that list, uh, at least in Nltk. Um,

Speaker 2:          41:42          but

Speaker 1:          41:44          it, it shows that these words are all close to each other cause it knows that, uh, the distance between these worlds, it's small, so it grabs them very close to each other. So it's pretty cool, right? So if you're looking at a different region,

Speaker 1:          41:59          so if you look at a different region, we'll see that, hey, this is food, right? This is a totally different region. Pepper, pickled, you know, cod all lives, turnips. These are all similar words. And I wouldn't really stress the importance that brevity of vectors. Okay. Not The brevity, the wrong word, wrong word vector. The, the, the, the enormous awesomeness of vectors. Okay. Word vectors are word clusters are related. There's so much we can do with this in every field in, in legal and law. We can, we can train an AI judge using this thing. Do you think semantic similarity to see the differences between the different case data or we can doctors, we could, we could see what's similar. We could use this to try new drugs. Like what is, uh, what is the semantic, what is the coastline similarity or what is some, some similarity metrics that we defined between a corpus of scientific hate papers on, uh, some problem that we're trying to solve so we could, so this most similar function it's already created for us, but basically we'll say, well, we'll give it stark and then it'll show this, the similarity by a number.

Speaker 1:          43:11          So it ranked them. All of these names are similar to stark, right?

Speaker 2:          43:16          MMM.

Speaker 1:          43:17          And how are we doing this? Well, there's a lot of methods for measuring semantic similarity. There's a lot of methods for measuring the similarity between vectors. And the one we're using here is the coastline similarity. So let me, let me, let me bring that up. The code sign similarity.

Speaker 2:          43:34          MMM, okay.

Speaker 1:          43:40          Co-Signs clarity. Possible distance metric we can use because turning these words into Vectra, turning our videos into vectors, turning our images into vectors. It gives us a way to mathematically reason about these things. We can, we can reason about them. Just like we've reasoned about numbers in a, in a, in a, in a, in a, in a mathematical way. Right. So, uh, so this is the, this is the formula for the cosine similarity, right? So given two vectors, we can, we can use the dot product and the magnitude of those vectors to calculate them. Okay. So that's one method. There's a lot, there's like the hostel, Ian, I think it's called Halcyon similarity. Um, or the, sorry, the golf scions no, of course the similarity, but it's like the, anyway, there's a lot. And I, and I'll, I'll link him more into, in, in a different dorm. So,

Speaker 2:          44:40          mmm.

Speaker 1:          44:41          Anyway, so then, uh, we'll use that to say, okay, so given these three words, stark winter fell in river run, it'll say we'll start as related to winter, Phil as x is related to river run and what is x? And that's what this does. It says, okay, so it's going to measure the similarity between the first two parameters we give it. And then it's going to say for that similarity would be that like 0.5 or 0.6 and 0.7, what is the, what is, what is something that similar to that last parameter river run. And it'll find that from our list of existing vectors. And in that case, in our case, it will be truly Tyrian Danny, things like that. Okay. Um, okay, so, so there's that, that's the end of this already. And I and I, because I didn't type out the code, it went a little faster. Uh, but

Speaker 2:          45:29          uh,

Speaker 1:          45:31          yeah. So let's wait. Where's my, where's my screen here? Let me stop screen sharing and go back to this. Oh Man. Okay. Hi Guys. Let's, uh, you are ending five minute Q and a and then we're going to end this stream. Okay. I have an awesome video coming up for you guys. Have been working really hard to thank you song Graham. I really appreciate it. Uh, I'm really excited about this. Next video. I'm using a Mac book.

Speaker 2:          46:11          MMM,

Speaker 1:          46:13          thanks. Vocalize.

Speaker 2:          46:15          Uh, what else?

Speaker 1:          46:25          Thanks party. Can you please elaborate on how is the projection of each word of each work to the coordinator is I work to the coordinate. What do you mean? What? Our real life applications of this. Great questions. Thanks. Iraq's, okay. Real life applications of word vectors. Take any, take any, uh, piece of texts. Take a book. Okay. After this live stream, download a book. Okay. Download an ebook and convert it to a text format and then use the code that I give you and you could easily feed it towards the back and create vectors. What do you do with these vectors? You'd, well then you can, besides the similarity and the distance, um, what's up, what's a good application for Bec? Like, uh, then when a corpus of what your friends are saying, you could see what, what's what you can rank personalities like, you know, what you know, chats like this guy's chance versus this guy's chat or a disguise. Um, you know what he said in a speech versus what he said. If you want to compare, you know, Hitler to Trump. I just went political now trying to go political anyway, but I just did, uh, if you want to anything we're doctors are good for that. Um, ranking. Uh,

Speaker 3:          47:36          okay.

Speaker 1:          47:37          Words are everywhere guys. Um, any kind of similarity, a ranking, that's what it's for. And there's a lot of, there's a lot of possibilities. Okay. So how are the words assembled into factors? Is it just context? Are there any ontological network being built? If so, how? Um, right, so Google released worked effect. Okay. So he trained neural network on these vectors and these are labeled, this is, it was a labeled corpus of words and well, actually we can do that. We can do this unsupervised as well. But basically, uh, we, we convert words to vectors and single single words to vectors. And so that gives us a number. I 0.8 or 0.9 and once we have those vectors, then we could create even more generalized vectors by looking at the similarity. And that similarly could be like, you know, this is point 999.9 and an eight. And these are very similar. Okay. So, and that creates even more generalized doctors. Okay. So anyway, the guys I've got to go, ah, I've got some editing to do, some shooting to do some you Udacity talking to do and I love you guys. Uh, we're gonna and uh, next livestream is going to be much more, you know, uh, so and not that this one wasn't there. All Doe. Uh, I'm dope. You guys are dope. We're all do. Okay. So for now, I got to go.

Speaker 3:          49:16          Okay.

Speaker 1:          49:16          Take a chill pill. So thanks for watching. That'd be guys.