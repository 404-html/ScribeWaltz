Speaker 1:          00:00          Hello world, it's Saroj and I've built an automated trading Bot called neuro fund and I built it with a machine learning library called tensorflow and not just any tensorflow, the new version of tensorflow, tensorflow 2.0 and in this video I'm going to show you how I built this and what the important features of tentraflow 2.0 are so that you can make money with it. And by make money, I mean build an AI startup, work at a company that requires that as a dependency of knowledge, do something great with this knowledge that will impact people's lives and make you money. That's the point of this video. And let me start off by showing you how neural fund my Automated Investment Ai Works. What it does is it makes predictions about stock prices in the future for various companies in a sector that the user selects say technology. So within technology, apple, Google, and say Amazon or three different stocks, and it's going to use an AI model to predict future prices for each of these stocks. And then based on the stock that is, it predicts to be the highest price. It will buy that for you. So it's like an automated hedge fund manager. And rather than having a 20% cut, which hedge fund managers take, it's going to take a 2% cut. So let me go ahead and demo this for you.

Speaker 1:          01:14          So the first step for us is to sign up for neuro fund and to sign up, I'm going to give it a name and then I'll type in a password and then I'll hit submit. Great. So I've signed up and now what I have to do is select the industry that I want my AI to invest in. So all select technology and it's going to pick some stocks to invest in for me and I'll click on invest. And now what's going to happen is it's going to ask me to complete my charge. So it's going to ask me for 100 bucks and I'm gonna fill this out using a fake credit card.

Speaker 2:          01:44          Okay.

Speaker 1:          01:45          For testing purposes, and this is using the Stripe Api. And once I pay, it's going to take 2% of that, so two box and it's going to invest the rest for me. So I'll pay

Speaker 1:          02:00          and once I've paid see the balances right now, 98 USD, it took the rest it's going to, and so I chose to invest in apple stock based on its predictions. And so this was just a chart of apple stock, but the point is that in the background it is continuously learning about stock prices in the future using tensorflow serving. I'm going to talk about how that works in the background and it's tensional a 2.0 this is a, this is an automated investment bought and I'm going to show you how I built this. All right, so in this tutorial there are 10 steps. We're going to start by looking at what some prerequisite videos you should watch are. Then we're going to talk about the problems with tensorflow 1.0 how cash flow, 2.0 fixes those problems. We'll build this stock prediction model in in Colab. We'll download that model, we'll serve it using tensorflow serving, and then we'll add some extra functionality like user's authentication payments, and then finally we'll deploy it to the web so users can actually use it at the end.

Speaker 1:          02:55          We'll talk about ways of improving that APP. All right, so first of all, if you haven't, go ahead and watch four videos, right? So one is called how to make money with tensorflow. So that was for 1.0, but a lot of the concepts still apply. Then watch seven ways to make money with machine learning. And then watch one of my most recent videos, watch me build an AI startup and then watch this playlist called intro to tensorflow just on youtube search intro to tensorflow Saroj and it'll show up. And once you do that, let's talk about some of the problems with tensorflow. One Point Oh so first of all, there is this programming paradigm called data flow. And it tends to flow. Team did not invent this. This existed for a while. And the idea behind, uh, behind data flow is that it creates what's called a static computation graph.

Speaker 1:          03:40          So imagine any set of operations in any function or in any equation, we can represent that as a graph where different nodes, our operations like add, subtract, multiply, divide, right? And data will flow through these computation graphs operation by operation. Let's say first that you want to add, then you want to subtract and then you want to divide, right? So there's a, there's a sequence of operations and astatic computation graph can represent these operations. And one way of representing data in these computation graphs is by considering them as tensors and what our tensors tensors our n dimensional arrays. So that means they are groups of numbers with n dimensions. This could be one dimension. This can be two dimensions. This could be three dimensions. This could be a million dimensions, right? Which we can visualize. And the idea is that these tensions flow through the graph from input to output.

Speaker 1:          04:31          And they make a prediction when the prediction could, would be the output of the function. Tensorflow, and this is great and the reason they did this is because creating a computation graph in this way, a static computation graph allows for very easy parallelism. It's easy to distribute computation across different nodes. If you abstract the idea of operations to um, objects in what is considered object oriented programming, it makes it easier to have distributed execution across multiple machines. And it's easier to compile. And it's more portable because when a static computation graph is created in one language, let's say python, uh, we can then export it and load it up in a different language because it is language agnostic. All great things. So the idea behind tensorflow, 1.0 was we import our data, define our model. This is a static model at runtime. The data will flow through this static graph and then we'll get some output and is an example of some tensorflow 2.0 code right here.

Speaker 1:          05:29          And let me also just write out something very simple so you could see what I mean. So once we've imported tensor flow, then we can define two constants, right? So the first constant is going to be called a, the second constant is going to be called B. And both of these represent single values. The first represents two, and the second represents three. And once we have these constants, these tensor flow variables, we can perform operations on these constants. Like let's say C is going to be equal to a plus B, whereas d is going to be equal to a times B. And then once we've done that, then we launch our graph by using what's called the session object. We have a TF dot session. And then inside of that session, we perform our operations. Let's say it's ad or it's multiply. And when we do this, we can see that it computed this operation.

Speaker 1:          06:21          But the problem is that it computed it, when we ran that computation, graphs are right here is where this operation a plus B occurred, right? So if we want it to say, well, let's see what B is right over here, it's not gonna work, right? Because it is a static computation graph. First we have to define the graph, then we can run the graph. We can't just debug inside of the graph as it's being built. And this is a problem. The second problem is verbosity. So there's so many concepts in tentraflow, placeholders, variables, uh, hyper parameter values, formatting convention. There's a lot to learn and we haven't even begun to talk about deep learning theory here. And so this is an example of verbosity. I basically just copied and pasted some code from what's called a DC Gan. Uh, it's a type of generative adversarial network, the convolutional generative adversarial network.

Speaker 1:          07:10          And I basically just copied some tentraflow 1.0 code here to show you that there's a lot happening here. And this can be confusing for beginners. Even if you have coded before, they could still be confusing because there's so many different, uh, tensorflow specific naming conventions here that we have to get to know, you know, global variables and initializer at the session, the different types of optimizers that saver, right? So it's very verbose code and uh, there's also very messy Api APIs because they're always adding more to the API APIs over time. There's a lot of deprecated Apis, a lot of new packages are being added to this. And I made this meme like, should I use this sequence to sequence or this one, I don't know. And lastly, it's very hard to debug, right? So if we have some value here like x, and it's a value called zero, and then we're, we have another value called Y and we say, well, why is going to be this value?

Speaker 1:          08:07          The log of x plus one divided by x? We're dividing by zero and then we say y plus one is c and then inside of our session we'll have to say, well print out that value of, see, let's see what, what z evaluates too. And of course it's going to evaluate as Nan Nan, but the problem was why, but we didn't. We couldn't, we couldn't see that. It was why? Because if we print out why, it's not going to tell us, hey the problem was here because why hasn't been compiled? It hasn't been completed yet. It's waiting until the graph has built. So that's what they wanted to fix with tensorflow 2.0 so they're actually a lot of features that tends to flow 2.0 has that 1.0 doesn't, but what I've done is I picked the three or four of them that I consider it to be the most important.

Speaker 1:          08:50          Sorry, six or seven of them that I consider to be the most important and here they are. The first one is that it allows for rapid prototyping by having what's called eager execution mode as the default mode. Right. And so eager execution mode is it is an imperative programming paradigm that doesn't create a static computation graph. It creates what's called Ed dynamic computation graph, and this is more python Anik. It's more in line with how python was built and this makes it much easier to debug. It makes it much easier to read the code. It's less for boasts, so just having this alone is such an important feature. Pi Torch already does this. In fact, chainer did this about three years ago, but now this is a native concept and tensorflow, which makes things much simpler to to understand, and I had this code right here, which I'll compile.

Speaker 1:          09:42          So right here you can see I'm installing the latest version of tensorflow 2.0 with this pip command and then I'll import tensorflow. It's executing eagerly. I'll create this value for x and then I'll do perform a matrix multiplication using x and notice how I can then print out that value m, which is x times x or X. Dot. Product x before creating a session. In fact, there are no sessions anymore, right? All of this is being computed with each line and this is super valuable. In fact, we can also use num Pi natively, right? So we can use num py functions to perform operations on these TF variables like see this Constance et cetera. And notice there's, there's our output. Exactly as we wanted that to work, there were no errors. And like I said, this makes it easier to debug because we can just print out, hey what's the value of m before computing, you know, anything else later on and it's going to tell us what that value is.

Speaker 1:          10:39          So it makes debugging a lot easier. It's less verbose. And one major reason for this is because they are now using care Ross as the high level, the official high level Api of tentraflow 2.0 which is awesome. And so this code right here is an example of training a model and it's takes about 45 seconds to train and Colab, which is super good, super short. And the, the whole point here is all we have to do is we have to import flow. We're not importing care us. Why? Because Ken Ross is now built into tensorflow, and what that means is we can call carrots just like that. See this TF dot Ken Ross, uh, module right here, and we're downloading the MNI ist dataset. We trained it, we tested it, and this will finish training in about a few more seconds, but that's all it took for us to train our model on data using tensorflow.

Speaker 1:          11:31          We built a neural network right here in a few lines using the carrot sequential API. It trained it, it tested it, and an in five epochs. We're done with training. There you go. There's our it trained right there, right? So that's super easy. And there's more granular control as well in that we not only do they have carrots as a new high level API, they have a full lower API that allows you to access those native internal operations that tensorflow is using using what's called TF dot. Raw ops. But the thing about that tensorflow team, which they're definitely watching, I tried to search for this and the documentation, I did not find it. So definitely make that easier for developers to, to find. Okay. But that's another point that it allows for more granular control at the low level as well as high level. And you might be thinking, well I wrote a bunch of code and tensorflow 1.0 how am I going to convert it to 2.0 they have this nifty little command called TF upgrade Vitu and all you have to do is say, here's my old tensorflow, 1.0 file and here's my new tentraflow 1.0 file and it will make all those changes automatically for you, which is awesome to backwards compatibility.

Speaker 1:          12:39          And here's perhaps my favorite feature. Tenser board is now available inside of Colab. Now tensor board, if you don't know, is tension flows way of visualizing your model during training. You can visualize the hyper parameters, you can visualize a lot while it's happening and it's super, super useful. And using end Grok, which is a tunnel to be able to access this, we can, we have tents board right there and now we can perform all sorts of visual visualizations as per necessary. You know, we don't have a model right now, but we could perform any kind of visualization right in the cloud. We don't have to download anything. So those are the main points about tentraflow 2.0 that I thought were worth mentioning because it makes it easier for anybody to enter into this field and that will make it easier for you to build something of value for other people, right?

Speaker 1:          13:27          If as long as you can understand these tools at a high level, you can put pieces together to make a prototype and then you can generate value using that, right? So here's what we're going to do. We're going to build this stock prediction model with tentraflow 2.0 in this colab notebook and then we're going to train it in the cloud. Then we'll download it and then we'll create a web app around that. All right, so let's go to this stock prediction example right here. So inside of this example, what I'm going to do is I'm going to use what's called a transformer neural network to predict prices for one company stock. And I'm just going to randomly pick a general electric or apple, whatever you want to pick, it doesn't matter. So what I'm first going to do is import the data and once I have that data, then I can view that data and see what it is.

Speaker 1:          14:13          And so what I did was I pulled the data from Yahoo Finance for GE right here as a text file and it downloaded that and it showed it to me right here. And we can actually just, if we wanted to, we could manually download it by saying download data right here and it will download it as a CSV file. And then we upload it to Colab. And so once we have that, we will be able to visualize it as a data frame, right? These are all the prices for, uh, for about 90 days of historical data. And once we have that, we're going to visualize it right here. Okay. The data's going up over time. It makes sense. And then once we have that data, we're going to perform a bunch of preprocessing on it. I'm going to skim through this because there's a lot of preprocessing here, but it's very basic stuff.

Speaker 1:          15:01          We want to reformat a and you can really just find copy and paste a lot of this preprocessing code. Um, and then once we have that we can show what it looks like over time. And then we get to the fun part, which is building our model, which is called the transformer network. Now the transformer network has replaced all variations of recurrent networks. That includes LSTM networks, gru networks for time series prediction for sequence prediction. Let me clarify for sequence prediction. And Google invented this for specifically for language translation and their model. Bert uses a transformer network open AI's modeled gpt to uses a transformer network to make word predictions, which allows for text generation and have two great videos on how this works in detail. See both of these open AI tech generator and natural language processing. Now what we're gonna do is we're going to re purpose this transformer network because it's so new for asset price prediction and that is the exciting part.

Speaker 1:          16:00          Taking some of these bleeding edge models and reapplying them to use cases that nobody thought about. That is the value that we can bring to people. And this is an example of a transformer network right here. Now remember all of these machine learning models, these neural models, there are collections of different matrix operations. Add, subtract, multiply, divide. And what we do is these boxes represent these matrix operations and once you get to know a few neural architecture is feed forward, are current, you know, uh, Hockfield networks. Once you get to know a few, you realize that they're all just the same thing but just different ordering of operations. So you kind of just jumble up this pile of Matrix math and that's your new neural network, that's your new architecture. And sometimes you'll achieve state of the art performance with that. That's really how it works.

Speaker 1:          16:47          That is AI research, jumbling up these what are called differential blocks and seeing what's going to give a better output. And so this is one example. It's an encoder decoder architecture, right? So input sequence of past prices, output will be the sequence of the next price is based on it, what it's going to predict. And uh, both of those videos should show you in detail how the transformer works. So how did I build a transformer for tensorflow 2.0 because I couldn't find one. I could not find a transformer on get hub. Unbelievably that was written in tensorflow 2.0 so what did I do? Did I build it from scratch? No. What I did was I found an existing transformer that this guy built on Kaggle Shoe Gian Shadow to tissue Gian. So this was an existing transformer network that this guy built on Kaggle and it was built for tentraflow 1.0 and what I did was I copied and pasted it and I repurposed it for tentraflow 2.0 so he might be asking how did you repurpose that for tension flow 2.0 and the answer is that it didn't work at first.

Speaker 1:          17:50          Once I installed tensorflow 2.0 when I could have done it's, I could have used that script that I talked about that converts everything to 2.0 but instead what I did was I just like manually went through and I changed it myself just to learn what the differences are. And it turns out that the only difference was rather than using check this out rather than using, uh, from Ken Ross. Dot. Models. All of these imports just said Care Os. What I did was I just had to change it to tensorflow. Dot Care Ross and then a compile for tentraflow 2.0 why? Because like I said before, Ken Ross is now a part of tentraflow 2.0 and so there we go. We got a bleeding edge model. Never been built before and 2.0 at least publicly. Definitely internally. They have this at Google. Uh, and then it's going to compile and hopefully this works great.

Speaker 1:          18:38          Just like that. See, and this a big model. It's, it's got a lot, there's a lot there. We were not going to go into all that theory. Uh, we built it there and then we fit it in. I already have this right here for you. My training code, it's going to be in the, in the video description. This actually took a while to train a while, meaning 20 minutes, which is not that long. And then we have a prediction of the price right here. Okay? So it's not that good, but it's, it's not bad. It's like they're, so we just need more data. But the point is that we train this in the cloud. We didn't have to use any of our local gps and we saved the model by this single line model. Dot. Say my model dot h five. So this will save it and then we can download it by just saying, let me download whatever file is here.

Speaker 1:          19:19          And using this sidebar we can download whatever file is there. So great. Now that we have trained a model, we want to serve this model to a user in the form of a web app. So let me take some time to help explain tensorflow serving to you. I really think this is one of the most powerful tools in the entire machine learning pipeline. And it's because sometimes you want, you want a model to be able to continuously learn from data, right? You don't want to just train a model, it's trained on data, it's static, you serve it to a user and it's just always there. You want it to continuously learn over time. And tensorflow serving allows your model to gracefully do this because there's a lot of things that can go wrong here. And that's why Google built it for themselves because they have these continuous training pipelines internally for tools like search and maps, et cetera.

Speaker 1:          20:05          So the idea is that it's got this version control system built in where you have a version of the model, it's called model one and it's trained on some data and end users are making requests, is model posts, requests, get requests for inference. And this is happening, but in the background, another version of this model is training on new data. And once this model has fully trained on new data, it will gracefully phase out that original model and it will phase in the newly trained model. And then once that sin, it's going to be training another one. Now this is just one version of how you could do this, right? You can do this several ways. You can have multiple models, you can have multiple models training, multiple bottles serving. You can combine data from multiple outputs to create some ensemble technique. There's a lot we can do, but just to be very simple about this tentacle of serving allows you to create models that serve users in a production environment that allows you to experiment very fast.

Speaker 1:          20:58          And so think about how the data science pipeline looks. And if we look at the production grade ml pipeline, brining that ml code that we just did, it's such a small part of it, right? This configuration, there's monitoring, and this is all considered Dev ops, right? Serving analysis, machine resource management. There's a lot. So tentacle of serving takes care of all of that for us, which is awesome. So you might be thinking, well, why can't I just use a regular web framework like Django or, uh, you know, what have you like flask to do this? Well, you could, you could rapid simple model with an API like flask or whatever, but there's some really good reasons we don't want to do that. The first reason is that serving is faster because it was optimized for a continuous versioned model environment because that's what they do at Google, right?

Speaker 1:          21:46          They have CPU and GPU and sometimes TPU and they have to allocate resources efficiently, both in terms of memory and in terms of space. So it has better time complexity to be computer sciency about it and it has better a space efficiency. And this version control system that's built in is just amazing. That's exactly what we want to happen. And more importantly, they use it for their, you know, very scale, very production, great products. So we should use it as well. And um, you might be thinking, well, does it use http? Does it use a GRPC? And it used to use GRPC. And there's this great talk by my previous company Twillio love you Tulio that explains how grpc works. But recently they added http support as well. So that's a great thing as well. So it uses both. Now let's consider some concepts inside of tensorflow serving.

Speaker 1:          22:33          Okay, so this is, uh, this is a, an image of the pipeline. The life cycle of what's called a servable. And a cerebral is the central abstraction of tensorflow serving. It's just a name of an object. You know, an object can be named anything. And inside of this programming paradigm, a servable represents a model. So a truly trained model, but it doesn't just have to be a model. It can also be some other algorithm, you know, some kind of any kind of algorithm. But server bowls are that central layer of abstraction that uses users will perform inference on or width. So the idea is that for a servable, a separable could be, let's say in our case it's going to be that fully trained stock prediction model. Okay, so that's our servable. Now the servable will have versions to it. There's a first version, the second version, the third version, and if we take those versions, we can consider it as a stream and so in gets, the analogy would be a Dag, right?

Speaker 1:          23:25          A directed [inaudible] graph to be computer sciency about it, but in the tentraflow serving paradigm there, the server bowls make up a stream with all of their versions in it and models in in the serving paradigm can represent multiple service goals. So an actual model can represent multiple models, but we don't have to consider models right now let's just consider service goals because we're not going to have to deal with that ourselves. Then there are loaders, so loaders will manage a server, bubbles lifecycle, and in loader we'll pull a cervical using a source. So the source object has direct input output access to your file system. So a source, we'll pull that model from your file. The loader will use a source to load that model into, into memory. And then a manager, which is kind of the controller in this pipeline, will detect what the inspired version is in which we can write logic for like which version of a model do we want to serve to a user.

Speaker 1:          24:19          The newest one, one that's trained in 30 days and tensorflow has some default aspired versions for us, which we can use as well. But the manager will handle the full lifecycle of a servable. The manager will say, okay, we want to use this inspired version. Okay, in court is the highest level, um, superclass in everything tangible serving core, which wraps everything here. So let me outline the steps here. So first of all, we train a model called a transformer on data and use it for inference. Then we train it on newer data. So a user is inferencing this version of our transformer in real time. Uh, meanwhile we're training on it again on newer stock data that it's pulling from the web via an API. And so the Thor's plug and creates a loader for specific version of this model. And this loader has all the metadata necessary to load this servable computation graph.

Speaker 1:          25:10          It points to it on the disc, the source, the source then notifies the manager of the aspired version, which is this one right here, this green graph. The loader will then tell the source, okay, load the new version based on that version. Policy would specify the aspired version and then it will pull that from the file system into the source, into the loader. The loader will then give it to the manager. The manager will then hand that servable that the result of that inference request back to the clients. And that's how it works. So this is, so this version system is happening in real time and uh, that's how tentraflow serving works at a high level. It has http and grpc support and now it's, we're onto step six of this tutorial. We want to build tensorflow serving. So how are we going to do this?

Speaker 1:          25:54          Well, the easiest way I found was use this base repository called simple, simple tentraflow serving great stuff. What this guy did was he implemented all of these serving features, right? Restful http API APIs supporting inference, acceleration for Gpu, supporting dynamic online and offline model versions. Basically everything we would want that we would have to build from scratch, which we don't have to remember. This is a mentality that I want you guys, you wizards, the loves of my life. This is a mentality that I want you to adopt because it's my mentality, this mentality of rapid experimentation, of not doing anything that is unnecessary, fastest method to prototype. If there exists components, if there exists pieces of a puzzle in terms of code on get hub, we don't have to build that ourselves. Right? And the first version of Uber just combined a bunch of existing Api is together and slapped a pretty interface on it, right?

Speaker 1:          26:49          Uber use payments from stripe maps from Google, which did everything it showed where drivers were, it even did routing for them. Of course, nowadays they have their own routing algorithms using deep RL and some amazing work. But my point is when it comes to a prototype, just put piece of the puzzle together to get something very basic out there and then improve it over time. So my point of saying all of that is that we're going to build off of this existing simple tentraflow serving demo at all of our functionality to it. And that is the first version of our APP. So let's get right to it. I'll first of all download this app and it's going to take a while to download. Let's see what it asks us to do. Okay. So we just have to install it. We can either install it with pip from the source. So we'll go ahead and install it from the source. So what we have to do is say setup.py as it's saying right here, and get help. Let me make this bigger terminal is my safe space install.

Speaker 2:          27:56          Okay.

Speaker 1:          27:56          Huh. Awesome. Now we will develop as he says here, or she develop. Great. And lastly, we'll do a Bazell build of it.

Speaker 1:          28:16          Awesome. Okay. So that all of that worked. So now what we're gonna do is we're going to start the server with the saved models. So this APP allows us to load different types of models by specifying, okay, simpletons role serving and then what's the model that we want to load and hopefully this works. Okay, so now we have this app running on our local machine called tit, simple tensorflow serving and it allows us to load up any type of model that we want. And there are a lot of different models that this guy has put into this app and we can see those models right here. So models. In fact, we'll just look at it on our local machine. Let's check it out.

Speaker 1:          28:50          All these different models for em and ist for detecting iris flowers. You know all those different types of models. We have here models that were trained in mx nets that we can then convert into tentraflow using onyx and different language agnostic tools, library agnostic tools. The whole point is notice how the models are stored right here and they have both a proto buff file and a variables file. Those are the only two dependency. So using this boiler plates templates application, we can train a model like we just did in the cloud, load it or serve it via this web app and wrap all sorts of functionality around it, like user authentication, et cetera. And it will be able to predict using that data just like we saw right here. So let's take a look at this code and see what exactly it's doing and how it's doing, what it's doing. So if we go into this code, let me make it little bit bigger.

Speaker 2:          29:43          Okay,

Speaker 1:          29:43          let's go into what looks like the main code server. Server dot. Py. That looks important. So in server dot pie,

Speaker 2:          29:53          okay,

Speaker 1:          29:54          we see that it's using flask and it's wrapping. It's both using flask and tensorflow serving. So flask creates an AP, an http Api, and then internally it's using serving four model versioning. And inside of this we'll see that depending on which model you choose, a different inference service is loaded up, right? So we have, we can have onyx, we could have a Pi torch model, we have a tensor flow model. And so in that example we loaded up a tensorflow model. Then there the route there, there are the routes, the routes for the different web pages that we have and we just need to add routes for user login and then you know a payment like, like purchase and then then we can leverage these existing routes, these existing functions like do inference. So, okay. But I'm very curious as to what this tentraflow infant service looks like because that's the one we're using here.

Speaker 1:          30:47          So for tentraflow inference service, what is doing is exactly what we talked about. See there's a function right here for dynamically reloading models, right? So it starts a new threat to load models periodically. So it'll load up a model to start off. And this is just continuously running. It'll load the model to start off that. We feed it when we, when we launch it, and then it's got functions for dynamically reloading models, creating new threads, loading saved models. And it's going to call these as per the manager's requests, right? So a lot of this boiler plate is already abstracted away from us. So all we have to do is define what that initial model looks like and then it's going to load up new versions of that model. However, we do have to define where that data, that new training data is coming from. And so what we can do

Speaker 1:          31:32          as we can find in this code where it's asking for training data and then we can modify it. So inference right here. So inference is happening within the context of dynamically loading and reloading models, right? So inferences, what's constantly happening. Okay, so inside of this training.py file right here, we'll notice that it is preparing training data from a local directory. So we can modify this so that it will download data using the Yahoo Finance Api from a specific date. It will format that data, it will turn it into a request and it will then make a prediction using it. But right now we don't want to make a prediction, we just want to download that data as a request and then format it as an input. And then we'll say, we'll create a place holder around it, shape equals none. And then we will say this request Jason Data. It's not like that. This is input data.

Speaker 1:          32:35          Tf Dot placeholder. This is our train data actually. So there are, so this is the training file. So whenever a model needs to train, it's going to use this and it's going to continuously use that new data that we are pulling from the web. We have this API. Then inside of inference is going to make, it's going to perform inference and these models are going to be trained dynamically, right? So all of these functions, load saved, model version, dynamically reload models, load custom operations. This is happening in realtime as part as part of our tensorflow inference service, which is all we needed to do was define where that new data input is coming from. We already, we've already trained that data from the previous stock data and then we'll define an interval like let's say every 30 days or sorry, every two hours we'll train a new model every one hour.

Speaker 1:          33:26          You know, we can change that up, but the point is using a timer. But the point is that we now have an infant service. It's running in real time. And now we want to, uh, we want to create user authentication, we want to create a payment functionality and we want to add that to our existing tentraflow serving apps. So how do we do that? Well, it turns out that just last week I made this AI startup prototype, which data a lot of this boiler plate code, which I can copy and paste. So basically in the layout I said, you know, here's my background color. I generated that logo using the tool brand mark, that io I talked about before. And then I said, okay, so here's the, here's the logo right here that I just added it as a source. I uploaded it to injure, right neuro fund named at that, place it there.

Speaker 1:          34:20          And then I use stripe to create that payments file, which is in layout dot. Html. So here's the paint, here's the payment page, right? So if these are as if the user has authenticated and make them pay, and then it's going to make a charge to post. So I'll just take these files and copy them to this existing repository. And once those files are in my templates file, now it's going to, I basically took all that existing code. I'm only using the backend coats and none of that front end code. Whenever a user logs in, it's going to use a sequel database to store that username and password. Then it's going to say, well pay with stripe and then the user pays with stripe a hundred bucks, which I hard coded and it can be anything. It's going to take two of those bucks, send them to my personal stripe accounts, and then the rest of it, the $98, it's going to invest using uh, the Quandl Api. And then any other API's we want to invest with. And we're going to, we're going to store all of those tokens, that token for a specific user id, a token for the specific payment id that this user made and the specific model that the user is using inside of the SQL database as rows. And that makes it easy to find an index. And once we have that, then we can add that the rest of the logic for us, we had the inference, we have the user

Speaker 1:          35:38          off logic, we have the payment logic, it's just a simple stripe. And then the rest of it is just formatting html. So it looks nice. So recall how in the original demo that I had here, I basically just copied and pasted this line chart to just show the apple price. And I also hard coded it to choose apple. But what we can do is we can, but it's predicting using that time series data, right? What the next best price would be. What we can do is have it perform inference on several different stock prices. Find the Delta between the prediction and the actual result and whichever one has a smaller delta, we'll use that one and we'll invest in that. So we'll do a stock price dot invest, whatever the price is, right? So we, we need to authenticate with some sock Api, but which, you know there are several, I like the Quanta one, but there's, there's several of them.

Speaker 1:          36:29          Alpaca Commission, Free Api stockbrokers you can build. And trade with real time market data for free. You get an Api key and then you can use it with zero commission, which is awesome. So we have connected our API for stock price pulling and making purchases. We have integrated that with stripe user auth and inference and not just any inference, dynamically continuously training inference using tensorflow serving and not just any tentraflow serving tensorflow 2.0 there's a lot more to this code. You'll find it all in the video description. If you have any questions, any comments, let me know in the comments section. I'm always trying to improve my content, improve my code quality, improve the topics that I'm talking about to make sure that I am providing as much value as possible for you guys. And it's a Saturday morning and I'm here recording this because I love you guys and I want this to be released very soon. So I hope you found this video educational and inspirational. What's the next AI startup you're going to build? Let me know in the comment section and please subscribe for more programming videos. For now, I've got to invest in myself, so thanks for watching.