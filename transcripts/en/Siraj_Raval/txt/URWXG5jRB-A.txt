Speaker 1:          00:00          Hello world, it's Siraj. And today we're going to talk about deep minds, starcraft two AI environment. So recently they're released this starcraft two environment that lets you train reinforcement learning models. And that's what you're saying right now. It's a demo of it happening. But basically you can use starcraft two the game as a test bed to train and run your AI models on. And these can be reinforcement learning models. That can be deep learning models. That can be really anything. They can just be scripts that aren't, that aren't even machine learning, just like hardcoded bots, whatever. But the point is that it's, it's meant to be a test bed for people to train and test their AI models on. So it's a really exciting time right now for deep reinforcement learning as a field because open AI beat Dota two, uh, recently in the, in the world champion, they'd beat the world champion at Dota two.

Speaker 1:          00:56          And then after that the mine, uh, releases starcraft two AI environment. So there's a lot of exciting things happening right now in deep reinforcement learning. There's a lot of low hanging fruit in this section of machine machine learning as opposed to supervise learning where a lot of it has been solved more or less, right? Gradient based optimization. You compute the gradient and then you update your weights and you know, your labels. It's, it's very, you know, it's, it's been tried and done before, but for deep reinforcement learning, there's a lot of unanswered questions. So it's hard. It's a really exciting time. Right now what we're gonna do is we're going to, we're going to run a pre trained model and what I'm gonna do is I'm going to on my machine set up and install all the required dependencies and uh, the script, everything you need basically to go from zero to having starcraft two running on your computer with deep mines environment installed and modeled a pretrained model running.

Speaker 1:          01:49          Okay. So what the, what the model is is it's called a deep Q learner. I'll talk about what that is, but it's a deep Q learner and it's going to be running on the collect mineral shards mitty game of starcraft two, which means it's just the Bot that's collecting little trinkets called mineral shards. And it'll do this autonomously without you needing to do anything. And so from there you could modify it or run your own algorithms, but once you have something set up, it'll be a lot easier to, to get into the, the, uh, the, the bottom of things. And what, what am I doing with my hands? Okay, so let's get to this for a second. Okay. So first things first. What does the history here, so deep minds, first attempt at running games simulations came, uh, for Atari Games, right? They, that's why Google bought them.

Speaker 1:          02:32          They created an algorithm called the deep Q learner, which is it, which is the algorithm that we're running. And they use that algorithm to be any Atari game. And the way they did this is they combine two different ideas in machine learning. They combined the idea of deep learning, which is all about learning features. You don't have to engineer what those features are like. I'm looking for a dog that has long ears and it has brown for no, it will learn what the necessary features are to map what it sees to some label and so they used a convolutional neural network for this to to create features learned dense representations from game screens, right? So all I got where the pixels of the game, the pixels and then it learns dense representations from those pixels and then it con and then it converted what it saw into an output and that output was an up, down left or right value, right.

Speaker 1:          03:25          Anything that you can use on a joystick for an Atari game. And the, the way it did this is it didn't just take in the input from the what they saw in the game. It also use what's called Q learning. It's cure learning is a type of reinforcement learning where we initialize what's known as a Q matrix and the Q matrix has a, has a collection of possible actions that an agent can take in a game and all of these actions are weighted. Like this is an okay action, this is a better action. This action could be the best action. And what it does is it picks an action from the Q matrix using some strategy that you decide. It could be random, it could be based on some pre weighted value, like an epsilon, whatever. But you pick an action from the Q matrix, you perform it in the game, you observe what's happening and then you see if you got a reward or not, right?

Speaker 1:          04:14          A plus one and minus one. And then based on that reward, you'll update the Q matrix so that the actions are all weighted differently. Right? And so the idea is that eventually the Q matrix will have the best actions for you to perform at whatever time step you're in. Right? The Q matrix acts as a weight similar to how in a neural network, the weights improve over time in reinforcement learning in cue learning, the Q matrix improves over time. So they combined both of those ideas together, right? So deep cue learning and the way it does this is, and I've got a little bit of pseudo code here for how it works and this is the full pseudo code for how their algorithm works. Now keep in mind that that their algorithm wasn't just a simple, you know, take a convolutional network and then run Q learning on top of it.

Speaker 1:          05:03          It's also got, uh, two different features from neuroscience that uh, the first one is called replay memory. And replay memory is essentially a buffer in memory that's a temporary buffer that stores a state's actions and rewards. Basically your experience of what, what, what has happened in the game. And this basically this is to, this improved their AI when they use this concept of replay memory that acted as a temporary buffer that they could pull actions from as well as the Q matrix. So if we look at this a pseudocode, we'll see that first initialize it, some replay memory matrix and initialize as a Q matrix randomly and it observes the initial state of the game. Where are we in the game? And that would be the, the collection of pixels that at first cs, and then it runs the training loop. So in the training loop, it will select an action from the Q matrix, either randomly by using some probability value epsilon or by bike selecting the optimal Q value from the cue that the optimal action from the Q matrix that it sees and then it will execute that action.

Speaker 1:          06:08          In Open Ai's universe environment, they call this the step right, the environment step function. So it will execute the action and then observed the reward that it receives and it will store the this, the, the state, the reward, the old state and the action. All of those that that, those, those four values into replay memory. And then the next step is for it to a compute a loss function. And then we can see the loss function here. It's also called the bellmen equation. Okay. This is also called a bellman equation. Not to get you to confuse, but that's just what it's called as well. But basically it will randomly sample from the replay memory and then it will use a sample that it, that it retrieved to compute a loss function and then it will minimize the square of that loss function at every iteration. And as a loss function is minimized, the Q matrix is values are improved at every time step, which means that eventually every time the agent pulls an action from the Q matrix, it's going to be more and more optimal such that it's going to minimize the loss, which will maximize the reward it receives.

Speaker 1:          07:15          I know that's, that's quite a mouthful to to, you know, say in 30 seconds be, but that's, that's how the deep Q learner works at a high level. The next step for them was to try it out on go that the ancient Chinese game of go. So a lot of AI experts, experts said that it would take 10 years, 20 years, 30 years for an AI to be able to beat the game of go because there were so many possibilities, right? There's so many possibilities and the search space is far too vast for an AI to just brute force through all the options. There are so many different combinations of game states that it's just too hard for an AI to compute with our, with the limits of computing power that we have now, that was their thinking, but they were wrong because they're always wrong when it comes to deep learning and all of these new technologies, but basically Alphago was their attempt.

Speaker 1:          08:06          They're successful attempt at beating the game of go and they used, they use two different neural networks here. They actually use three. They use three different neural networks, these three different neural networks here. One was for the policy network and one was for the value network and both of these computer, two different values. One is the policy and that, and the next is the value. And it used both the policy and the value to help it help guide what is essentially a gigantic tree search. And the tree search is called a Monte Carlo tree search. And here's a brief description of how it works, but basically the Monte Carlo tree search or MCTs t simulates an a search tree and it's the Ai selects an action at each time step based on the action value and prior, which is the output of the policy network and some exploration parameter.

Speaker 1:          09:00          So it uses the values of the policy network and the value network as guides to help it search the tree of possible moves that it can play at every time step. And they trained Alphago on tens of thousands of hours of expert gameplay and then they gave it, they gave Alpha go to the world champion Lisa Dole, and it beat Lisa Dole. So beating the game of go was a much harder challenge than beating the 20 different Atari Games given just the pixel values of the game screen. Right. But now more recently, they decided let's, let's up the ante even more. Right? And they decided, let's not just do this alone. Let's, let's open source this so that everybody gets to use this technology, right? So starcraft, starcraft is arguably one of, if not the greatest PC game of all times, PC fanboys come at me. But anyway, starcraft is one of the best PC games of all time, and it's got hundreds of thousands of players across the world.

Speaker 1:          10:01          It's got people whose day job is to just play starcraft all day competitively, right? In South Korea, specifically, uh, much closer to South Korea if you're out there. But anyway, so, uh, so, uh, starcraft two huge awesome game. If you've never played it before, then this is a great opportunity to download it. It's free. I'll show you a little bit about that in a second. And if you have played it before, this is a great way to help improve your own strategy, right? Because when you're building an AI for starcraft two, you're thinking about all the things that requires to be a good starcraft player, right? When you're thinking about when you should spend your wealth, how you should build your army, what you should invest, your resources and time and energy and all of these things. You're going to try to want to replicate in an AI that you built.

Speaker 1:          10:49          And so if you think about an AI for starcraft there, it has to be able to do a bunch of things that are quite difficult. First of all, it's got to have an effective use of memory, right? It's gotta be able to remember not just the things that have that have happened in the short term. It's got to be able to remember the things that happened back in the longterm, in the past and not just in the past. It's got to be able to plan over a long period of time. Sometimes a, you want to make decisions that help maximize your current value, right? You, you want to kill an enemy because the enemy is next to your, you know, vulnerable troops. But other times you want to make an action that is not as intuitive in the short term, but in the longterm it is, right?

Speaker 1:          11:30          Like, sometimes you want to spend a lot of money on some resource and so you're going to have a little money right now, but in the longterm, that resource that you purchased is going to help you a lot more, right? So it's not as, it's not as, you know, obviously intuitive as something like an Atari game where it's just like all you have to do is defeat, you know, get from point a to point B or remove some block or till all the aliens on the screen. It's not that simple, right? And even as something as even some task as simple as expand your base to some location is actually pretty complicated. You have to coordinate mouse clicks, your camera available resources. And what this does is it makes actions and planning hierarchical. And this is generally very hard for reinforcement learning algorithms to grasp.

Speaker 1:          12:17          The concept of hierarchy is quite hard for reinforcement learning algorithms to grasp, right? Because you're performing an action and you're receiving a reward, right? So there's the agent environment loop. It's not like a deep learning where we have all of these layers and there's all of this structure that's built over time. So deep cue learning was one good example of having a hierarchical structure, a hierarchical model in a reinforcement learning environment and I think it was one of the first, but we're going to see a lot more and a lot of the key discoveries that are going to come out of the field. The entire field of machine learning this year and next year are going to come from deep reinforcement learning when some really smart people combine the ideas that come from deep learning, mainly hierarchical learning and the ideas of reinforcement learning from a learning from an environment in real time.

Speaker 1:          13:10          Andre Karpati had a recent talk at y Combinator, I think it was called, why cons, where he said that Agi, artificial general intelligence is going to result from having simulations right from, from creating an AI that can adapt in a simulation similar to how we adapt in the real world. And this is a simulation, enter a twilight zone music. But anyway, so reinforcement, learning, deep reinforcement, learning, super hot field. And this is your chance to get into it, right? You don't have to work at deep mind. You don't have to work at open Ai. You can just be some kid, uh, who has a time and energy to work on this stuff. And if you have Internet access and you have the time to work on this, youtube can make an amazing algorithm and you post it on get hub, you posted on hacker news on the machine learning sub Reddit, you'll get great feedback.

Speaker 1:          14:00          You can join some online research groups on a slack channel or on several of the forums online and you can just do great work. And all of this can be added to your portfolio, your get hub, your resume for future prospects, whether that be studying at a university or working at one of these fields. But the point is, in order to get anywhere you've got, you've got to do something right. And, and starcraft two is a great test bed. It's a great set of tools. I've tested it out myself and, and I think it's, it's, it's, it's a really great place to get started with deep reinforcement learning. Okay. So, uh, okay now so onto the code, right? So, uh, basically so it was a joint collaboration with blizzard. So blizzard already released an API that lets a user create scripted bots, machine learning base bots that are running from pickle files, you know, pretrained models, replay analysis and tool assisted human play.

Speaker 1:          14:50          And so deep minds environment, it's repository is called Pi s c two Pi, starcraft two. So it's all in python. Thank the [inaudible] right. It's in, it's in python but it has four components to it. The first is the API that it wraps from blizzard in python. The next is a Dataset of anonymized game replays. Okay. So it's got a lot of these anonymize game replace that you can download from right here. I'll go through that in a second. And it's got a, a series of simple RL mini games. One of them is what we're going to do for this demo and to test out different environments, right. To test out the different algorithms. I mean, so this was blizzards initial API and then deepmind wrapped it with their own python repository. Right? So what we're going to do is we're going to just set up everything, right?

Speaker 1:          15:38          So I'm going to go through these installations that there are seven steps here. I'm going to go through all of them to, to get started. Okay. So first of all, before you do anything, you've got to download a starcraft two, the blizzard client, it's free. You just sign up on blizzard and you, you select the starter edition, right? That's it just to like the starter edition and it's free and you can just download that and then you can play it just like that. It'll take, you know, it depends on your bandwidth, but it took me about an hour to download and set up and already I was running through the tutorial in starcraft two, right? Just an hour for me. So definitely download that. And once you've downloaded starcraft two then move on to these seven steps. So the first one is to install pie [inaudible], right? And so luckily they have wrapped it into a nice little python library for us, so I can go ahead and install it using pip. So I'll say pseudo pip three install pie se two. Okay. And it's downloading. Hey Scott, it's, it's building off of all those dependencies that right. It's, it's, it's built on top of the Blizzard Api that I just talked about and,

Speaker 1:          16:53          and some other things. Okay. So now that was step one. We've installed pie se two and so the next step is to install the sample code. So the sample code, you can just clone it directly from my get hub, just like get clone and it's going to download it

Speaker 1:          17:09          just like that, right? And the sample code contains the pre trained model and it contains all the python files you need to run this very simple reinforcement learning bot. And so once you've got that, then that's step two. And then step three is to download the mini games from the starcraft two maps, right? So we can click on this link and just like that, here are our maps, our mini game maps. Okay. And so we can move all of these into our folder for starcraft two and it's got to be in the maps folder right here we go in maps. So I'll just copy and paste it just like that. So now my starcraft two application that I've downloaded with the blizzard client has these maps. And so once I have these maps, now I can install tensorflow and open AI baselines. Oh, okay. So so right if you don't have a tensor flow you can install tensorflow with pip three or pip install, you can install tensorflow, I've already got tensor flow.

Speaker 1:          18:06          And then once you've got tensorflow, tensorflow is to be able to train and run these machine learning models and then you have to install open AI baselines. So baselines is a collection of high quality reinforcement learning algorithms, right? The deep Q network is one of them that we're going to be using and it's got policy gradients, which she said another popular reinforcement learning algorithm. But basically it's your way of being able to implement these reinforcement learning algorithms without having to code them from up from scratch. And then you can modify existing ones. Tweak them to see if you can get better results. So it's a good way to test around with some tests, some different, uh, affects. Okay. So then I can download the baselines environment. And once I've downloaded baselines, I've put my folder, I put my maps into my, uh, starcraft two folder, then I can go ahead and open the project with Intelijay. So the reason I say use Intelijay for this and not just open it with sublime or you know, some regular text editor is because this, oh, hold on. Permission Error. Of course we've got to do Sudo. Of course. There we go. So now the reason I say intelligent is because there's some logs that are really nice too to view a when it comes to,

Speaker 2:          19:25          okay,

Speaker 1:          19:26          how your aging is running, which is easy to do with Intelijay. So if you've never used a, uh, uh, an ide, the integrated development environment, like intelligent, this is a great, uh,

Speaker 1:          19:39          this is a great reason to do it, right? So go ahead and download intelligent, you can download from here. There is a paid version, but there's also a free version of the community version. Get the community version so you don't have to get the paid version. And then if you want to train the model right from scratch, then you can run python three train mineral shards. That Pie. But we're, we're gonna do is we're gonna run a pretrained model. So though we don't have to train it from scratch, we just want to, we just want to get something to work right. So let's go ahead and just open up that model.

Speaker 2:          20:37          Okay.

Speaker 1:          20:41          Okay. So Intelijay is opening right now and once it's opened I can go and open my project. So I'm going to import a project and where is my project? So my project is

Speaker 2:          21:00          okay

Speaker 1:          21:02          in downloads pie, s c two. So go to downloads, Pi s e two examples. All right. And then I'll open it.

Speaker 2:          21:20          Okay.

Speaker 1:          21:21          From existing sources finish, you can install plugins to support python, which I'll do right now. So clearly I'm, I'm doing all this steps as if I've never done any of this before. And then I've got to restart it to initialize the python plugin.

Speaker 2:          21:57          Okay,

Speaker 1:          21:58          Kay. All right. So now I've got the, the code in, import it into Intelijay and it's detected a python framework. And I'm going to say, okay, did this, and then,

Speaker 1:          22:48          and there we have it. And now the client is running in the background. It's, it's, it's executed starcraft two. It's the tech did that. My system has starcraft two installed and then it's run this script to run a pretrained model inside the starcraft two environment. So it acts. So it's able to access the starcraft two game because the deep, because deep minds, Pi se two repository under the hood, it's using blizzards. Api is, but it's a local API. So it's not like it's connecting to more remotely. It's connecting to the game that's right on your desktop or your laptop in terms of the code, the, the model and the environment or bill from us. So in this, in this code, it's got the deep convolutional network right here. As you can see, the parameters are here, the number of hidden layers that are here. And then it's got the, uh, and then it's got the, it's wraps that open AI environment, that step function, right?

Speaker 1:          23:44          So it's, it's given, it's given these parameters and it's combined both the convolutional network and the, uh, Q network together to then train this AI and it saves it as a pickle file. It saves the pretrained models a pickle file. And then once it's trained the pickle file, we can, we can access that pickle file to run the pretrained model in the deepmind environment into starcraft two environment. And if we look at this code, it's actually, you know, it's quite a lot of code and I can make a different video to talk about how all the code works, but right now I just want it to help you install and configure this script so that you can run it yourself. Don't be afraid to run it. It's actually pretty easy. And all in all including downloads, starcraft, two Dowling, the code, installing all your dependencies, it'll take you probably an hour and a half to go from zero to running your own, uh, RL algorithms in this game. Okay. So I hope that, I hope that helped. Please subscribe for more programming videos, and for now, I'm going to play some starcraft too, so thanks for watching.