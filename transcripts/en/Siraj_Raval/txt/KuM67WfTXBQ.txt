Speaker 1:          00:00          Deep learning on a stick. Who needs flying cars? Hello world. It's Saroj and there's this really cool little device called the Intel Mobidius neural compute stick I want to talk about today. It's the world's first USB based deep learning inference kits. It's tiny fan lists and you can use it to learn AI programming on a huge range of hosts devices. I've been having a lot of fun with it and we'll go through a demo of performing computer vision by deploying a deep neural network to it, but more importantly, this represents a trend of AI moving to the edge instead of the cloud. Centralized cloud computing has worked pretty well, but it can't overcome the laws of physics. Only Jeff Dean can do that. The weight of data, the speed of light as people use more realtime services waiting on a data center across the world will become cumbersome.

Speaker 1:          00:58          Latency is important for both mission critical systems like self driving cars and consumer services like video chat. Most of the internets underlying infrastructure runs on rule based systems without any learning technology behind it. Upgrading this infrastructure is going to take a unique understanding of all the different contexts the Internet works with. For example, every manufacturing plant is different. Even though they use the same equipment, an AI model created in the cloud to help plan the supply chain or different plants won't work. Sometimes the edge isn't always connected to the cloud and when it is, it's expensive to move high volume, real time data for training or realtime decisions to the cloud. An AI model trained locally for specific plants needs would allow for fast real time decision making. This also presents an opportunity for hundreds of startups to create AI pipeline tools, not just one winner take all.

Speaker 1:          02:05          Since there's such a diversity in terms of the infrastructure and data and applications specific needs, real money is being invested into putting AI at the edge for all sorts of companies. Since getting even a small sliver of increased productivity will result in a massive impact for their revenues and let's not forget the increased security of keeping data local instead of sending it out to a third party. The neural compute stick or NCS is a perfect example of a device that allows anyone to test and deploy AI models locally. It's super low power since it just consumes a single watt. There's no need for connectivity and you can feel secure knowing that the data is never leaving your device. You can run a whole host of deep learning applications that involve images, Aka computer vision. So let's go through the pipeline of getting a real time object classification demo running using this thing.

Speaker 1:          03:07          We can find the stick on a bunch of sites. I prefer using Amazon since prime is the greatest thing since sliced bread. The NCS supports to deep learning frameworks currently tensorflow and cafe, but models trained using these frameworks need to be converted into an appropriate format to run on the device. The NCS DK has three libraries that help with this. Compile converts a model into the appropriate format profile, gives layer by layer statistics to evaluate model performance and check compares the inference results from running the network on the device versus pure tensorflow or cafe. You can install the NC Sdk by cloning it from get hub. Then running, make install to compile it. Once that's finished, we can clone the NC APP zoo, a collection of community projects that make use of the NCS. We can get started by importing the MBN C API module to access the API.

Speaker 1:          04:11          Then we'll check for the USB device, which the API lets us do using the enumerate devices function. We could even connect multiple NCS devices to the same application processor to scale inference performance. But for now, let's just pick one. We'll find a pre compiled model called Alex Net, which was trained on lots of images with their associated labels and load it onto the NCS. We're going to simply read images from a folder on our local machine and offload it to the NCS. For inference, all the neural network processing is done completely on the NCS device, which frees up our machines, CPU and memory resources to perform other application level tasks. We can load an image onto the NCS, but we'll need to resize it to match the dimensions defined by the pretrained network and we'll convert the color scheme accordingly, as well as its data type to an array and load the image onto the NCS has a tensor.

Speaker 1:          05:16          We can retrieve the results from the device and print them out for us to view. Lastly, in order to avoid memory leaks, we can allocate to any used memory. When we run it on an image, it'll immediately make a class prediction in the order of likelihood. Pretty awesome. Three points to remember from this video. The Mobidius neural compute stick. Let's anyone test and deploy AI models at the edge, AKA locally. This is useful because it has lower latency than the cloud and gives increased security. Since the data stays on your machine and using the NC Sdk, we can test a whole host of deep learning applications on the device easily. With a few lines of code, it does subscribe button and I will make you an AI master. For now. I've got to train myself, so thanks for watching.