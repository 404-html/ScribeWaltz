Speaker 1:          00:01          Hello world gets a Raj and Pong.

Speaker 2:          00:04          Everybody's played it at least once and let's see if we can create an AI to beat the game by using a reinforcement learning technique called policy gradients. When deepmind beat all those Atari Games using the same algorithm a few years ago or when they created the AI that beat the best go player in the world last year, the Ai community was blown away, but if you think about it, the algorithms they used warrant novel at all. In fact, they'd been around for decades. The deep Q learner that beat the Atari Games just use a standard Q learning algorithm with function approximation and you can find an example of that from the standard RL book by Sutton in 1998 as well as a convolutional net which has been around since the 1990s alpha go a strategy called a Monte Carlo tree search as well as something called a policy gradients approach. All of these are standard, well known components.

Speaker 2:          01:03          My point is it's not the algorithms that's been the driver of recent progress in AI, but the amount of data and computing power. We've been able to feed these algorithms recently. So we've talked about a lot of algorithms I've listed here, but one we haven't talked about is called policy gradients and we'll use it to help beat the game of Pong. You might be asking, shouldn't we use a deep learner like deep minded? Well, it turns out that cue learning isn't that great. In fact, most people prefer to use policy gradients as they've been shown to work better than Q learning when tuned. Well, the reason for this is because it's an end to end approach, meaning it's one system that can learn the entire game. There's an explicit policy and a principled approach that directly optimizes the expected reward. Pong is a great example of a simple reinforcement learning task and we can represent it as a mark Haub decision process.

Speaker 2:          02:03          Think of this as a graph where each node is a particular game state and each edge is a possible transition. Each edge also gives a reward and the goal is to compute the optimal way of acting in any state to maximize rewards. You know how the game works? Two players each gets a paddle and you have to bounce the ball, pass the other player at a more technical level. We're going to receive an image frame which is a 210 by 160 by three byte array, basically pixel values and we want to decide if we move our pedal up or down based on that. After every single decision we make the game, we'll execute our action and give us a reward. We get a plus one if the ball goes past the opponent and a minus one reward if we miss the ball or zero. Otherwise the goal is to move this paddle so that we get lots forward.

Speaker 2:          02:56          Something to keep in mind is that we should probably not make any assumptions about the way that the game of Pong works. Why? Because we actually don't care about pong. We care about complex, high dimensional problems are getting a robot to save a person in a natural disaster, but if we can get an AI to learn pong, we can someday get them to do more useful tasks using their ability to generalize to any task. Our first step is going to be to create a neural network that represents our AI. It's job is to learn a policy, so the network, we'll take the state of the game and decide what we should do, move up or down based on that game state, we'll call it the policy network. We can just use a two layer neural network that takes the raw image pixels of the game and produces a single number indicating the probability of going up every iteration we can sample from this distribution to get the actual move.

Speaker 2:          03:54          Since we only have two layers, the neurons in the hidden layer will be able to detect various game states like if the ball is on top or our paddle is in the middle and the neurons in the next layer can then decide if in each case, if we should be going up or down. Think about how difficult this problem is. For a second, we're getting about a hundred thousand 800 numbers and forward. Our policy network, which can easily involve on order of a million parameters for both layers. If we decided to go up for a given time step, the game could return a zero reward and give another hundred thousand 800 numbers for the next frame. It's possible we could repeat this process a hundred times steps before getting any non zero reward. If we finally get a plus one, how can we tell what made that happen?

Speaker 2:          04:40          Was it some move we played just recently or 40 frames ago? Maybe it had something to do with frame 12 and then frame 38 which of them billion knobs of our neural network should we change and how in order to do better in the future. This is a common problem in reinforcement learning called credit assignment. How do we solve this? Well, let's think of the easy case first. Aka supervised learning. Assume we had labels for every single move. As in we could feed an image to the network and get some probabilities for two classes up and down. We could compare that prediction to a label which would tell us the optimal thing to do at that time step like go up or down. Once we find the error, the difference between the label and the prediction, we could use backpropagation to update our neural network, the most common optimization scheme using a gradient vector to update our weights after every single parameter update for every training time step, our network would be slightly more likely to predict a move that more closely relates to the ideal move I. E. The label, but let's get real for a second.

Speaker 2:          05:48          We aren't going to have labels for every single move. This is where the policy gradients solution comes into play. It's our policy network is fed an image of a game state and outputs a probability of going up as say 30% and down as say 70% we can sample an action from this distribution and execute it in the game like down for example. But our problem is that we don't know if down is good yet, but that's okay. We can just wait a bit and see if it is. We'll just wait until the end of the game. Then take the reward. We got either a plus one if we won or a minus one if we lost and use that number as the gradient for the action we've taken. So if going down ended up losing the game, eventually we can find a gradient that discourages the network to take the down action for the input in the future.

Speaker 2:          06:38          And that's how policy gradients work. We have a policy that samples actions and the actions that happen to eventually lead to game wins get encouraged in the future actions that lead us to losing the game, get discouraged for training. We can initialize the policy network with two sets of weights and play a hundred games of pong. Assume each game is made up of 200 frames and we'd made 20,000 decisions for going up or down. And for each one we know the parameter gradients which tells us how we should change the parameter if we wanted to encourage that decision in that state in the future. The only missing step is to label every decision we've made as good or bad. If we won 12 games and lost 88 we can take 200 times 12 decisions we made in the winning game and do a possible update.

Speaker 2:          07:30          We'll take the other 200 times 88 decisions we made in the losing game and do a negative updates. That's it. The network will now become slightly more likely to repeat actions that worked and less likely to repeat actions that didn't work, not that crazy. Right? Three things to remember from this video. None of the algorithms that have achieved state of the art results out of deep mind are novel. It's the data and the computing resources we have now that gives them breakthrough results. The policy gradients method involves running a policy for a while, seeing what actions lead to high rewards than increasing their probability through back propagating gradients. And the credit assignment problem is a common one in our l where we're not sure how to decide which action an AI took contributed to the reward he received. Last week's coding challenge winner is Alex Meiosis dove.

Speaker 2:          08:24          He implemented a simple cue learning Bot using open Ai's gym environment to solve the frozen lake environment. Really efficient code. He did it in just under 40 lines of python and the runner up is Aditya Beer Parmar who solved the same environment using a greedy action selection algorithm. This week's challenge is to use a policy gradients method to solve the game other than palm posts. Your get hub links in the comment section, and I'll announce the winner next week. Please subscribe for more programming videos. And for now I've got to go assign some credit. So thanks for watching.