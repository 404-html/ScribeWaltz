Speaker 1:          00:00          Hello world. It's the Raj and I'm going to show you how I read research papers and give you some additional tips on how you can consume them more efficiently. Reading research papers is an art, whether the topic is machine learning or cryptography, distributed consensus or networking. In order to truly have an educated opinion on a particular topic in computer science, you've got to get yourself acquainted with current research in that subfield. It's easy to agree with the claim if it's got enough hype behind it, but being critical and balanced in your assessment is a skill that can be learned. Phd Students are taught how to do this in Grad school, but you too can learn how to do this. It just takes patience and practice and coffee. Lots of coffee every single week. I read between 10 to 20 research papers in order to keep up with the field and I've gotten better at it over time and I don't have any graduate degrees.

Speaker 1:          01:00          I'm just the guy who really loves this stuff and I teach myself everything using our new collective university, the Internet. One of my favorite resources to find papers on machine learning is the machine learning subreddit. People post papers they find interesting every day and they've also got this cool weekly. What are you reading thread where people post the papers that interest them the most currently. Additionally there is this web app called Archive sanity.com created by Andre carpathy which basically goes through archive and finds the papers that are most relevant. You can filter them by what interests you buy, which ones are most popular or by the ones that are most cited lately. Google and deep mind respectively published their work on their websites for easy access and there are of course journals like nature that you can find some top papers in easily. The pace of research is accelerating and machine learning because of a few reasons not including Schmidt, Huber in academia and in the public sphere.

Speaker 1:          02:02          The democratization of data computing, power, education and algorithms is all steadily happening over the Internet. Because of this, more people are able to make their own insights into this field. In the industry. The big tech companies profit more when their own teams discover new machine learning methods. So there's this race to create faster, more intelligent algorithms. All that is to say that there are a lot of papers you could be reading right now, so how are you supposed to know what to read? Well, what I've found is that every week there are maybe two or three papers that are getting the most attention in machine learning and the tools I've mentioned help me find them and read them, but most of my reading is a result of me having a goal. That goal could be to learn more about activation functions or perhaps probabilistic models that use attention mechanisms.

Speaker 1:          02:55          Once I've got that goal, it makes it much easier create a reading strategy that points towards that goal. Just being a good math heavy machine learning paper reader is not a goal to aspire to. Your stamina is more of a function of human motivation, which is a function of the goals you're trying to accomplish. I found that I can crush through and understand the most difficult papers much more when I have a real reason to do so. So let's take the landmark paper by a friend of mine, Ian Goodfellow on generative adversarial networks as an example. There is a lot in this paper. He synthesizes some ideas here that made young Macoun say that this concept was the coolest idea in deep learning in the last 10 years. The way I read papers is by performing a three pass approach on the first pass. I'll just skim through the paper to get a gist of it, meaning I'll first read the title if the title sounds interesting and relevant, generative adversarial networks.

Speaker 1:          03:56          Yo, let's go. I'll read the abstract, the abstract acts as a short standalone summary of the work of the paper that people can use as an overview. If the abstract is compelling, an adversarial process between two neural networks that resembles a game. All right, this is lit. Then I'll skim through the rest of the paper. By that I mean I'll carefully read the introduction, then read the section and subsection headings, but ignore everything else. Mainly ignore the math. I never read the math on the first pass. Are we the conclusion at the end and maybe glance over the references mentally ticking off the ones I've already read. If there are any, I just assumed the math is correct on the first pass. My goal for this first pass is to just be able to understand the aims of the author. What are the paper's main contributions here?

Speaker 1:          04:48          What problems as the attempt to solve? Is this a paper I'm actually interested in reading more of. Once I've done the first pass, I'll go back to see what other people are saying about this paper and compare my initial observations to. There's basically the aim of this first pass is to ensure that it's worth my time to continue analyzing this paper. Life is short and there are too many things to read. If it does peak my interest, then I'll reread it a second time on the second pass. I'll read it again this time more critically and I'll also take notes as I go. I'll actually read all the English text and I'll try to get a high level understanding of the math that's happening in the paper, so it's a mini Max game that looks to optimize a nash equilibrium. Okay. I kind of get that.

Speaker 1:          05:35          Eventually the generator network creates fake samples that are indistinguishable from the real thing, so the discriminator is powerless. Cool. I'll read the finger descriptions. Any plots and graphs that are available and try to understand the algorithm at a high level. A lot of times the author will break down an equation by factoring it out. I avoid to analyze this on the second pass, I see that it's using a loss function called the colback Leibler divergence. Never heard of that one, but I do get the concept of minimizing a loss function. When I read the experiments, I'll try to evaluate the results. Are they repeatable? Are the findings well supported by evidence. Once I'd done that, hopefully there is some associated code with the repository available on get hub. I'll download the code and start reading it myself. I'll try to compile and run the code locally to replicate the results as well.

Speaker 1:          06:28          Usually comments in the code help further my understanding. I'll also look for any additional resources on the web that help further explain the text articles, summaries, tutorials. Usually a popular paper will have a breakdown that someone else has done online that will help drive the key points home for me. After this second pass, I'll have a Jupiter notebook full of notes and associated helper images. Since I teach this stuff on youtube, teaching is really the best way to fully understand any topic. When it comes to the third pass, it's all about the math. My focus on the third pass is to really understand every detail of the mass. I might just use a pen and paper and break down the equations in the paper myself. I'll use Wikipedia to help me understand any of the more formal math concepts fully like the Kale divergence and if I'm feeling really ambitious, I'll try to replicate the paper programmatically using the hyper parameter settings and equations that it describes.

Speaker 1:          07:28          After all of this I'll feel confident enough to discuss it with other people. Reading papers is not easy and nobody can read long manipulations of complicated equations fast. The key is to never give up. Turn your frustrations into fuel to get better. You will understand this paper. You will master this subject. You will become awesome at this. It gets easier every time as you build your Merkel Dag of knowledge. See what I did there. If you don't get a math concept, guess what? Khan Academy. We'll teach you anything you need to know for free and lastly, do not hesitate to ask for help. There are study groups and communities online that are centered around the latest research in machine learning that you can post your questions too. Don't be afraid to reach out to researchers as well. You're actually doing them a favor by having them explain to you in terms you understand. All scientists need more experience translating complex topics. I've got lots of great links for you in the description and I hope you found this video useful. If you want to learn more about machine learning, AI and blockchain technology, hit the subscribe button, and for now I've got to reread the capital network paper, so thanks for watching.