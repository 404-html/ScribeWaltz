Speaker 1:          00:17          Hello world, it's Saroj and let's talk about optimization.

Speaker 1:          00:24          There are thousands of languages spoken across the world, each one unique in its ability to represent concepts and convey ideas, but there is one language that is shared by all humans. Regardless of where you come from. Mathematics, no matter your culture or your age, you possess the ability to understand this language of numbers that connects us all across continents and time. Like all languages. Fluency requires practice, but unlike any other language, the more fluent you become in math, the more unstoppable you'll be in anything you want to do in life. Math is happening all around us to a degree that most people don't realize. We can think of everything as a set of variables, as metrics and there exists relations between all of these variables. In math we call these relations functions. It's our way of representing a set of patterns, a mapping, a relationship between at many variables, no matter what machine learning model we use, no matter what Ada sent we use.

Speaker 1:          01:36          The goal of machine learning is to optimize or unobjective and by doing so we are approximating a function. The process of optimization helps us iteratively discover a functions hidden in the depths of data. Last week we talked about a popular optimization technique called gradient descent. This can be broken down into a five step process. First, we define some machine learning model with a set of initial weight values. These act as the coefficients of the function at the model represents the mapping between input data and output predictions. These values aren't naive. We have no idea what they should actually be, but we're trying to discover the optimal ones. We'll define an error function and when we plot the graph up, the relationship between all the possible error values and all of the possible weight values for our function will see that there exists a valley, a minimum.

Speaker 1:          02:42          We'll use our error to help us compute the partial derivative with respect to each weight value we have and this gives us our gradient. The gradient represents the change in the air when the weights are changed by a very value from their original value. We use the gradient to update the values of our weights in a direction such that the error is minimized iteratively coming closer and closer to the Minima of the function. We step our solution in the negative direction of the gradient repeatedly when we reach it. We have learned the optimal weight values for our model where our gradient is equal to zero. Our model, we'll then be able to make predictions for input data it's never seen before. Most optimization problems can be solved using gradient descent and its variance. They all fall into a category called first order optimization methods. We call them first order because they only require us to compute the first derivative, but there's another class of techniques that aren't as widely used called second order optimization methods that require us to compute the second derivative.

Speaker 1:          04:02          The first derivative tells us if the function is increasing or decreasing at a certain point and the second derivative tells us if the first derivative is increasing or decreasing, which hints at its curvature. First order methods provide us with a line that is tangential to a point on an air surface and second order methods provide us with a quadratic surface that kisses the curvature of the error surface. Ah, get a room you youtube. The advantage then of second order methods is that they don't ignore the curvature of the error surface and in terms of stepwise performance, they are better. Let's look at a popular second order optimization technique called Newton's method named after the dude who invented calculus, whose name was there are actually two versions of Newton's method. The first version is for finding the roots of a polynomial, all those points where it intersects with the x axis. So if you throw a ball and recorded his trajectory, finding the roots of the equation, we'll tell you exactly what time it hits the ground. The second version is for optimization and it's what we use in machine learning, but let's code the route finding virgin first to develop some basic intuition. Okay.

Speaker 2:          05:26          Elouise of ICML Bay and you're welcome. Hi, good to meet you and your to the how to coordinate on that part. Not yet. It's going to be quite a challenge. We've got to perform anomaly detection on a 500 terabyte data sets. I was thinking just do Snoopy's method, just the progression first to see if we can predict particles. Newton's method. Yeah, it's a form like in the arm optimization, I already build props was training for the past few hours. I'm impressed. Machine Learner, we shoot, let's

Speaker 1:          05:58          say we have a function f of x and some initial guests solution. Newton's method says that we first find the slope of the tangent line at our guest's point. Then find the point at which the tangent line intersects the x axis. We'll use that point to find its projection in the original function. Then we iterate again from our first step this time replacing our first point. With this one we keep iterating and eventually we'll stop when our current value of x is less than or equal to a threshold, so that's the route finding version of Newton's method where we're trying to find where the function equal zero, but in the optimization version we're trying to find where the derivative of the function equal zero. It's minimum. At a high level, given a random starting location, we construct a quadratic approximation to the objective function that matches the first and second derivative values at that point, and then we minimize that quadratic function instead of the original function.

Speaker 1:          07:06          The minimizer of the quadratic function is used as a starting point in the next step and we repeat this process iteratively. Okay, so let's go over to cases of Newton's method for optimization to learn more. A One d case and a two d case. In the first case, we've got a one dimensional function. We can obtain a quadratic approximation at a given point of the function using what's called a Taylor series expansion. Neglecting terms of order three or higher. A Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point. It was invented by an English mathematician named Brooke Taylor Swift. Just kidding. So we take the second order tailored series for our initial point x and minimize it by finding the first derivative and second derivative and equating them to zero in order to find the minimum x value.

Speaker 1:          08:08          We iterate this process. In the second case, let's say we've got a function of multiple dimensions, we can find the minimum of it using the same approach except for two changes. We replaced the first derivatives with a gradient and the second derivatives with a Hessian Hessian is a matrix of the second order partial derivatives of a scaler and it describes the local curvature of a multivariable function. Check this out. Derivatives help us compute gradients which we can represent using aged Cobian matrix for first order optimization and we can use the Hessian for second order optimization. These are four of the five derivative operators using all of, they're the ways that we organize and represent change numerically. So when should you use a second order method? First order methods are usually less computationally expensive to compute and less time expensive, converging pretty fast on large datasets.

Speaker 1:          09:12          Second order methods are faster when the second derivative is known and easy to compute, but the second derivative is often in tractable to compute requiring lots of computation for certain problems. Gradient descent can get stuck along paths of slow convergence around saddle points, whereas second order methods won't. Trying out different optimization techniques for your specific problem is the best way to see what works best. Here are the key points to remember from this video. First order optimization techniques use the first derivative of the function to minimize it. Second order optimization techniques use a second derivative. The Jacoby Ian is a matrix of first partial derivatives and the Hessian is a matrix of second partial derivatives and Newton's method is a popular second order optimization technique that can sometimes outperform gradient descent. Last week's coding challenge winner is Alberto Garcia sets Alberto used gradient descent to find the line of best fit is Jupiter notebook is insanely detail. You could learn gradient descent just from reading it alone. Very well thought out. That was dope. Alberto wizard of the week, and the runner up is Ivan Gustaf who implemented gradient descent from scratch for polynomials of any order. This week's challenge is to implement Newton's method for optimizations from scratch. Details are in the read me poster, get hub link in the comments and winners will be announced next week. Please subscribe for more programming videos, and for now I've got to invent the sixth derivative, so thanks for watching.