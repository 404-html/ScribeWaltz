Speaker 1:          00:04          Raj and welcome to the mat

Speaker 2:          00:06          of intelligence.

Speaker 1:          00:16          For the next three months. We're going to take a journey through the most important math concepts. That underlying machine learning. That means all the concepts you need from the great disciplines of Calculus, linear Algebra, probability theory and statistics. The prerequisites are knowing basic python syntax and Algebra. Every single algorithm we code will be done without using any popular machine learning library because the point of this course is to help you build a solid mathematical intuition around building algorithms that can learn from data. I mean, let's face it, you could just use a black box API for all this stuff, but if you have the intuition, you'll know exactly which algorithm to use for the job or even custom make your own from scratch. As humans, we are constantly receiving data through our five senses. Then somehow we've got to make sense of all of this chaotic input so that we can survive.

Speaker 1:          01:21          Thanks to the evolutionary process. We've developed brains capable of doing this. We've got the most precious resource in the universe. Intelligence, the ability to learn and apply knowledge. One way to measure our intelligence against the rest of the animal kingdom is using a ladder. Arts is indeed the most generalized type of intelligence capable of being applied to the widest variety of tasks, but that doesn't mean that we are necessarily the best kind of intelligence. In the 1960s a primate researcher named Dr Jane Goodall concluded that chimpanzees had been living in the forest for hundreds of thousands of years without overpopulating or destroying their environment at all. Orca is have the ability to sleep with one hemisphere of their brain at a time, which allows them to recuperate while being aware of their surroundings. In some ways, animals are more intelligent than us. Intelligence consists of many dimensions. Think of it like a multidimensional space of possibility when building AI, the human brain is a great roadmap.

Speaker 1:          02:30          After all, neural networks have achieved state of the art performance in countless tasks, but it's not the only roadmap. There are many possible types of intelligence out there that we can and will create. Some will seem familiar to us and some very alien thinking in a way we've never done before. Like when Alpha go played move 37 even the best go players in the world were stunned at the move. It went against everything we've learned about the game from millennia of practice, but it turned out to be an objectively better strategy that led to its win.

Speaker 3:          03:04          The many different types of intelligence or like symphonies, each comprising of different instruments and these instruments very not just their names dynamics,

Speaker 1:          03:13          but in their pitch and tempo and color and melody. The amount of data we're generating is growing really fast. No, I mean really, really fast. In the time since you started watching this video and enough data was generated for you to spend an entire lifetime analyzing and only 0.5% of all data ever is creating intelligence isn't just a nice to have. It's a necessity put in the right hands. It will help us solve problems we never dreamed could be possible to solve. So where do we start? At its core, machine learning is all about mathematical optimization. This is a way of thinking. Every single problem can be broken down into an optimization problem. Once we have some dataset that acts as our input, we'll build a model that uses that data to optimize for an objective, a goal that we want to reach and the way it does this is by minimizing some error value that we defined.

Speaker 1:          04:11          One example problem could be what should I wear today? I could frame this as optimizing for stylishness. Instead of say comfort, then define an error that I want to minimize as the amount of ratings a group of people give me that are negative or even what's the best design for my Ios apps home page. Rather than hard coding. In some elements I could find a Dataset of APP designs and their ratings from users. If I want to optimize for a design that would be the highest rated. I would learn the mapping between design styles and ratings. This is the way every single layer of the stack will be built in the future. Sometimes our data is labeled, sometimes it isn't. There are different techniques we can use to find patterns in this data. And sometimes optimizing for an objective can happen, not through the frame of pattern recognition, but through the exploration of many possibilities and seeing what works and what doesn't.

Speaker 1:          05:10          There are many ways that we can frame the learning process. But the easiest way to learn, it's when we use labeled data, mathematically speaking, we have some input. There's a domain x where every point of x has features that we observed. Then we have a label set y. So the data consists of a set of labeled examples that we can denote this way. The output then would be a prediction rule. So given a new x value, what it's associated, why value, we've got to learn this mapping, which is an unknown distribution over x to be able to answer this. So we have to measure some error function that acts as a performance metric. So what we do is choose from a number of possible models to represent this function well. Initially set some parameter values to represent the mapping and we'd evaluate the initial results, measured the error, update the parameters. And repeat this process, optimizing the model again and again until it fully learns the mapping.

Speaker 4:          06:10          Was it convex or concave functions that were easier to optimize? I think convex. I really hope my lab partners advocate optimization. I guess I should be thankful. Not many data scientists get a grant from cern to detect the Higgs Boson. What was her name again? Oh, Luis. I think. Yeah, she did win an award from ICML. I wonder if she's cute. That doesn't matter. I am not going to mix business and pleasure. Not this time.

Speaker 1:          06:35          Suppose I've got a bunch of data points. These are just data points like what apple probably trained Siri on there. All x y value pairs were x represents the distance a person bikes and why represents the amount of calories they lost. We can just plot them on a graph like so. We want to be able to predict the calories lost for a new person, even their biking distance. How should we do this? Well, we could try to draw a line that it's through all the data points, but it seems like our points are too spaced out for a straight line to pass through all of them so we can settle for drawing the line of best fit a line that goes through as many data points as possible. Algebra tells us that the equation for a straight line is of the form y equals mx plus B where m represents the slope or steepness of the line and be represents.

Speaker 1:          07:26          It's why access intercept points. We want to find the optimal values for B and m such that the line, it's as many points as possible. Given any new x value, we can plug it into our equation and it'll help put the most likely why value our error metric can be a measure of closeness, which we can define like this. So let's start off with a random B and m value and plot this line for every single data point we have. Let's calculate it's associated. Why value? Then we'll subtract the actual why value from it to measure the distance between the two. We'll want to square this error to make our next steps easier. Once we sum all of these values, we get a single value that represents our error. Given that line we just drew. Now, if we did this process repeatedly, say 1,337 times for a bunch of different randomly drawn lines, we could create a three d graph that shows the air value for every associated B and m value.

Speaker 1:          08:23          Notice how there is a valley in this graph at the bottom of this valley, the error is at its smallest and so the associated bnm values would be the line of best fit, where the distance between all our data points and our line would be the smallest, but how do we find it? Well, we'll need to try out a bunch of different lines to create this three d graph, but rather than just randomly drawing lines over and over again with no signal, what if we could do it in a more efficient way such that each successive we draw brings us closer and closer to the bottom of this valley. We need a direction, a way to descend this valley. What hip for a given function, we could find the slope of it at a given point. Then that slope would point in a certain direction towards the Minima of the graph and when we redraw our lines over and over again, we could do so using the slope as our compass, as our guide on how best to redraw

Speaker 1:          09:23          towards the minima until our slope approaches zero. In Calculus, we call this slope the derivative of a function. Since we are updating to value B, and, m, we want to calculate the derivative with respect to both of them, the partial derivative, the partial derivative with respect to a variable means that we calculate the derivative of that variable while ignoring the others, so it will compute the partial derivative with respect to B. Then the partial derivative with respect to em. To do this, we use the power rule. We multiply the exponent by the Coefficient and subtract one from the exponent. Once we have these two values, we can update both of these parameters from our function by subtracting them from our existing DNM values and we just keep doing that for a set number of iterations that we predefined. So this optimization technique that we just performed is called gradient descent and it's the most popular one in machine learning. So what do you need to remember from this video? Three points. The derivative is the slope of a function at a given line. The partial derivative is a slope with respect to one variable. In that function we can use them to compose a gradient which points in the direction of the local Minima of a function and gradient descent is a very popular optimization strategy in machine learning that uses the gradient to do this. The winter of last week's coding challenge is typical hog.

Speaker 4:          10:50          He created an encryption algorithm that's pretty complex written in c plus plus that I've never heard of before. Definitely check it out. Links in the description. Great Work Wizard

Speaker 1:          10:58          of the wick. Now it's your turn. I've got a coding challenge for you. Implement grading dissent on your own on a different datasets that all provide check checkup and get hub link for details. And the winner will be announced in a week. Please subscribe for more programming videos. And for now, I've got to memorize the power rule, so thanks for watching.