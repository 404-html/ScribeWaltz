Speaker 1:          00:00          Oh world, it's Saroj and let's build a chat Bot that can answer questions about any texts you give it, be it an article or even a book using care Os. Just imagine the boost in productivity. All of us we'll have, once we have access to expert systems for any given topic, instead of sifting through all the jargon in a scientific paper, you just give it the paper, then ask it the relevant questions, entire textbooks, libraries, videos, images, whatever. You just feed it some data and it would become an expert at it. All 7 billion people on Earth would have the capability of learning anything much faster. The web democratize information and this next evolution will democratize something just as important guidance, the ideal chatbox and talk intelligently about any domain. That's the holy grail. But domain specific chatbots are definitely possible. The technical term for this is a question answering system.

Speaker 1:          00:54          Surprisingly, we've been able to do this since way back in the 70s. Lunar was one of the first. It was, as you might've guessed, rural based. So it allowed geologists to ask questions about moon rocks from the Apollo missions. Uh, later improvement to rule based QA systems allowed programmers to encode patterns into their Bot called artificial intelligence markup language or AI ml. That meant less code for the same results, but yeah, don't use AI ml. It's so old. It makes Numa Numa look new. Now with deep learning, we can do this without hardcoded responses and have much better results. The generic case is that you give it some docs as input and then ask it a question. It'll give you the right answer after logically reasoning about it. The input could also be that everybody is happy and then the question could be what's the sentiment? The answer would be positive.

Speaker 1:          01:45          Other possible questions are, what's the entity? What are the part of speech tags? What's the translation to French? We need a common model for all of these questions. This is what the Ai community is trying to figure out how to do. Facebook research made some great progress with this just two years ago when they released a paper introducing this really cool idea called a memory network. LSTM networks proved to be a useful tool in tasks like tech summarization, but their memory is encoded by hidden states and weights is too small for very, very long sequences of data be that a book or a movie, a way around this for language translation for example, was to store multiple LSTM states and use an attention mechanism to choose between them, but they developed another strategy that outperform an LSTM for Q and a systems. The idea was to allow a neural network to use an external data structure as memory storage.

Speaker 1:          02:44          It learns where to retrieve their required memory from the memory bank in a supervised way. When it came to answering questions from [inaudible] data that was generated, that Info was pretty easy to come by. But in real world data, it is not that easy. Most recently there was a four month long Kaggle contest that a startup called metamind place in the top 5% four. To do this, they built a new state of the art model called a dynamic memory network that built on Facebook's initial idea. That's the one we'll focus on. So let's build it programmatically using care Os. This dataset, it's pretty well organized. It was created by Facebook AI research for the specific goal of improving textual reasoning. It's grouped into 20 different tasks. Each task test a different aspect of reasoning, so overall it provides a good overview of all the different capabilities of your learning model.

Speaker 1:          03:35          There are a thousand questions for training and a thousand for testing per task. Each question is paired with a statement or a series of statements as well as an answer. The goal is to have one model that can succeed in all tasks easily. We'll use pretrained glove vectors to help create a sequence of word vectors from our input sentences and these vectors will act as inputs to the model. The DMN and architecture defines two types of memory, semantic and episodic. These input vectors are considered the semantic memory, whereas episodic memory may contain other knowledge as well, and we'll talk about that in a second. We can fetch our babbled Dataset from the web and split them into training and testing. Data glove will help convert our words to vectors so they're ready to be fed into our model. The first module, the input module is a gru or gated recurrent unit that runs on a sequence of word vectors. A Gru cell is kind of like an LSTM cell, but it's more computationally efficient since it only has two gates and it doesn't use a memory unit. The two gates control when it's content is updated and when it's erased.

Speaker 2:          04:40          A recent update, recent update or recent,

Speaker 1:          04:47          and the hidden state of the input module represents the input process so far in a vector. It outputs hidden states after every sentence and he's outputs are called facts in the paper because they represent the essence of what is fed. Given a word vector and the previous time step director will compute the current time step vector. The update gate is a single layer neural network. We sum up the Matrix multiplications and add a bias term and then the sigmoid squashes it to a list of values between zero and one the output vector. We do this twice with different sets of weights. Then we use a reset gate that we'll learn to ignore the past time steps when necessary. For example, if the next sentence has nothing to do with those that came before it, the uptake gate is similar in that it can learn to ignore the current time step entirely.

Speaker 1:          05:35          Maybe the current sentence has nothing to do with the answer, whereas previous ones did. Then there's the question module. It processes the question word by word and I'll put a vector by using the same gru as the input module and the same weights. We can encode both of them by creating embedding layers for both. Then we'll create an episodic memory representation. Both the motivation for this in the paper came from the hippocampus function in our brain. It's able to retrieve can portal states that are triggered by some response, like a sight or a sound, both the fact and questioned vectors that are extracted from the input enter the episodic memory module. It's composed of two nested. You are use the energy are you generate what are called episodes. It does this by passing over the facts from the input module, but when updating it's interstate, it takes into account the output of an attention function.

Speaker 1:          06:28          On the current fact, the attention function gives a score between zero and one to each fact and so the gru ignores facts with low scores after each full pass on all the facts, the energy are you outputs an episode which is then fed to the outer gru. The reason we need multiple episodes is so our model can learn what part of a sentence it should pay attention to. After realizing after one pass, that's something else is important. With multiple passes, we can gather increasingly relevant information. We can initialize our model and set its loss function as categorical cross entropy with the stochastic gradient descent implementation rms prop. Then train it on the given data using the fit function. We can test this code in the browser without waiting for it to train because luckily for us, this researcher uploaded a web app with a fully trained model of this code.

Speaker 1:          07:18          We can generate a story which is a collection of sentences each describing an event in sequential order. Then we'll ask it a question. Pretty high accuracy response. Let's generate another story and ask it another question. Hero status. Let's go over the three key facts we've learned. Gru is control the flow of data like LSTM cells but are more computationally efficient using just two gates. Update and reset. Dynamic memory networks offer state of the art performance in question answering systems and they do this by using both semantic and episodic memory inspired by the hippocampus. Drum roll please. Nevermind. Nemanja told Meek is decoding challenge winner from last week. He implemented his own neural machine translator by training it on movie subtitles in both English and German. You can see all the results in his eye python notebook. Amazing work wizard of the week and the runner up is Michelle Batu. Despite the massive amount of training time and empty requires, Michelle was able to achieve some great results. I vow to both of you. This week's challenge is to make your own Q and a chat bot. All the details are in the read me get humbling links going to comments and all announce a winner a week from today. Please subscribe for more programming videos, check out this related video, and for now I've got to ask the right questions, so thanks for watching.