Speaker 1:          00:00          Alexa, get me a new outfit. Yes. Saroj Nice. Hello world. It's Saroj and let me show you how to build, train and serve an APP on AWS that predicts if customers are unhappy using a given mobile carrier. That way we can prevent them from leaving by giving them incentives to stay. You've probably heard about AWS. It's Amazon's gigantic collection of cloud computing services that offer developers and businesses, databases, storage management tools, analytics, networking, deployment options, Jeff Vasos. Basically everything you need to create and scale an APP to millions of users. Amazon currently commands a third of the cloud computing market that's three times more than it's next biggest competitor, Microsoft. But why use the cloud in the first place? Well, if you're doing some machine learning locally and try to load a Dataset into memory, chances are you've seen an error like this before. That's because Ram was exhausted on your machine and the operating system couldn't allocate another, say 500 megabytes of Ram.

Speaker 1:          01:17          One possible solution could be to upgrade your ram, but another is to use a virtual machine in the cloud with more ram and Cpu because hundreds of thousands of customers are aggregated in the cloud. Aws can achieve higher economies of scale, which translates to lower pay as you go prices for the developer and you don't have to think too much about to computing power. Usually you either have too much or too little, but the cloud can give you just as much as you need whenever you need it. If we try and sign up for AWS on the website, it will briefly ask us for credential details and we can have an account set up pretty fast and they'll ask us for a credit or debit card. They won't charge unless we authorize it. After this, we can see the AWS dashboard with a listing of every service it provides, and this can be overwhelming at first to any developer who first sees it.

Speaker 1:          02:21          Since we're doing machine learning, we know that we need compute, so let's go over some of these compute options. Starting with Ece to back when Soulja boy was cranking that. If we wanted to create an APP that could be accessible to people on the internet, we'd need to buy or rent a server, but if our server got less traffic than we expected, we have a huge loss in revenue with easy to, we don't have to worry about that. Isi stands for elastic computing and it's a concept that represents the ability to automatically scale up and the amount of compute necessary for an application. Ece Two is a collection of globally distributed Linux servers, likely running a customized version of Red Hat enterprise Linux that allows customers to create virtual machines that use any one of a variety of operating systems from windows to free. BSD. A virtual machine is a program that acts as a virtual computer.

Speaker 1:          03:27          It runs on top of an operating system and has its own operating system. We can start with an easy two instance that fits our needs and configure it to scale up according to the traffic with whatever dependencies we need installed on it. But while he c two gives us a computing instance with an operating system, sometimes we don't need all of the tools that come with that operating system for our application. That's where the ECE two container service becomes useful. Containers let us only use the necessary libraries and tools to make our service work so it's more lightweight and efficient and if we don't want to configure any of the details of our easy two instance, we can use light. It's a preconfigured easy two instance that makes deployment even faster. It's a trade off between ease of use and configurability and right in the middle of those two is elastic beanstock, which handles a lot of the deployment details for you, but also gives you more configurability than say light sail lambda lets you run code in response to events like changes to your Dataset or say image resizing that's necessary before a customer can classify an image they upload to your service.

Speaker 1:          04:52          It executes your code only when it's needed and scales automatically from a few requests per day to thousands per second. And those are just the compute services. There's a lot more. I've linked to this really great article that explains each in detail for you in the video description. Now remember we're trying to do machine learning, not make some personal website and luckily for us, Amazon recently released a service called sage maker that makes the whole process much easier. Sage maker, let's machine learning, engineers build, train and deploy machine learning models easily. The build module provides us with a hosted environment to work with our Dataset, experiment with any algorithms and visualize our output. It provides fully managed easy two instances running Jupiter notebooks that let us explore training data, preprocess it, all of this in the cloud. Immediately. These notebooks come preloaded with Kuda tensorflow high torch.

Speaker 1:          05:59          Basically all the tools you need to start training models in the cloud. It also provides its own algorithms in the form of an Sdk, both supervised like xg boost and logistic regression and unsupervised like principal component analysis and k means clustering. So let's get started with our APP. Our dataset is publicly available on Kaggle and contains about 3000 data points. Each data point has 21 attributes and describes the profile of a customer of an unknown us mobile operator, including their plan type and the amount of calls they make. So we'll first need to create an s three bucket to store. The data set will be using in s three stands for simple storage solutions. It manages data using an object storage architecture. Objects are called buckets and are the basic storage unit of s three we'll name. It's something that will later. Remember now to our notebook with the click of a button, we can create our own notebook instance that runs on AWS using sage and maker.

Speaker 1:          07:09          We'll select the default instance type, create a new role for ourselves and spin up our notebook. Once we open up our notebook, we can test out the installation of several popular machine learning frameworks by importing them. Then running that code. Let's start by specifying the s three bucket we just created as it's what we'll want to use for training data as well as the role we created for ourselves. Next will import. The python libraries will need to predict churn, including of course sage maker in our data set. The last attribute is the one that matters most to us, whether or not the customer left the service or termed. It's a binary attribute and we'll use it as our label. It's what we want to learn the mapping of from the rest of the attributes, the input data to this, our label. This is now considered a binary classification problem.

Speaker 1:          08:06          Let's explore this dataset a bit using pandas. We can see how frequently each feature appears and using map plot live. We can visualize a histogram of the numeric features. Looks like only 14% of customers have turned and most of the numeric features are nicely distributed. It looks like they are evenly distributed geographically and more likely than not to have an international plan. Also, it seems like some of our features have 100% correlation with one another, which means that we don't need them all. We can just remove some of them to avoid data redundancy. Now for our algorithm, let's use one called xg boost, which is a bunch of decision trees. It actually goes by lots of different names like gradient boosted trees, multiple additive regression trees, stochastic gradient boosting. Boosting is an ensemble technique where new models are added to correct the errors made by existing models.

Speaker 1:          09:09          Models are added sequentially until no further improvements are possible. Gradient boosting is a method where new models are created that predict the errors of previous models and then added together to make the final prediction. It's using gradient descent to minimize the loss when adding new models. This method supports both regression and classification problems. Since it seems like there are some variables in our dataset where both high and low values can predict turn. If we were to use linear regression, we need to generate some polynomial terms. It'd be better to use gradient boosted trees since they naturally account for non linear relationships between features and target variables while also accommodating complex interactions between features. We can start by regularizing our data so that our categorical features are converted into numeric features. Then we can split the data into training validation and test sets as this helps us prevent over fitting.

Speaker 1:          10:18          This is when our motto can't generalize well enough to new data points. When we start training, we need to specify the location of the xg boost algorithm containers, and because we're training in the CSV format, we'll create s three inputs that are training function can use as a pointer to files. In s three we can now decide on the values of our hyper parameters, how deep do we want each tree within the algorithm to go, how many boosting rounds? We can start by guessing these numbers and later on improve them. After training, we can deploy our model to a hosted end point and once we have that end point, we can run inference really easily as simple as making an http post request. When we run our model on our test data, we can see that it's mostly correct at predicting the customers who've turned. I hope you liked the video and it's time to start scaling. Please subscribe for more computer science videos and for now I've got a turn, so thanks for watching.