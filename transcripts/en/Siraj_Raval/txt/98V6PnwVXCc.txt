Speaker 1:          00:00          Simulation ending cheat code up, up, down, down, left, right. Hello world it Saroj and the AI research lab deepmind recently introduced their newest AI dubbed Alpha Star. Alpha star was the first AI to defeat a top professional player in the popular games. Starcraft two what they final score of five Oh, it was an exciting event with both sides uncertain about the outcome, but in the end team liquid's Mana unbelieve accepted defeat back in 2016 when deep minds Alpha go ended up beating the world champion at the popular game of go. A result that experts thought would take decades to accomplish. It resulted in a flurry of interest in AI technology, especially in China. It use a technique called deep reinforcement learning to assess the value of all possible positions on the board. Then use a search algorithm called a Monte Carlo tree search to choose its next move.

Speaker 1:          00:57          Picking the best one last year, deep mine again stunned the world with the release of a new algorithm dumped Alpha fold that was able to predict how proteins fold winning first place in an annual competition made for that specific purpose. It proved that AI could be used to help solve hard scientific problems, that these game environments were indeed just a test bed alpha fold use to residual neural networks to solve a supervised learning problem, predicting protein properties from genetic sequences. The job pattern matching. You did it again, but now with the release of Alpha Star, we've got to ask the question, what are the real world applications of this victory? Alpha star is more impressive than Alphago and Alpha fold combined. Here's why. First of all, Star craft has been played in professional sports tournaments for more than 20 years. It's a Saifai universe realtime strategy game, and the most common way to play is a one B one tournament over five games to start.

Speaker 1:          02:01          A player chooses one of three different alien races. Each race has its own distinct set of characteristics and abilities. One selected a player starts with a number of worker units each which can gather basic resources to build more units, structures and technologies. Using these creations, players can harvest even more resources and build increasingly more sophisticated basis and structures as well as new capabilities to defeat the other player. It's like building an entire city with an armed military and as such, a player has used an intelligent strategy to win. This involves game theory since there's not a single strategy that will win, there are multiple paths to success and unlike Ingo there is imperfect information. A player will actively discover things by scouting. It's not all known all at once. The game takes up to an hour to complete. So actions taken early in the game may not pay off long term and this requires longterm planning.

Speaker 1:          03:02          It's also all real time. So it's not like the alternating turns of go and perhaps the most mind blowing fact is the action space, which is the number of possible actions in agent can take at any point. There are hundreds of different units and buildings that have to be controlled all at once, which results in a common editorial explosion of possibilities go had tend to the 170th possible moves, which is more possibilities and there are atoms in the universe, but starcraft tend to the 1685th even Putin doesn't have that many subs. So back to the real world significance of this discovery. Dennis Hassabis, the CEO of deep mind stated that the longterm planning and strategizing ability that Alpha star exhibits can be used for other longterm planning tasks like weather, prediction, climate modeling, and language understanding. That makes total sense, but what else can we use longterm planning algorithms for?

Speaker 1:          04:01          Can we use them to design swarms of self replicating photosynthetic nanobots to quickly and cheaply suck pollutants from the atmosphere to save our lungs? How about longterm strategizing for our own lives on assistant that tells us the steps we need to take in order to accomplish any goal. We'd like designing drugs to cure all diseases, creating new strategies to combat the weaponization of social media generating better documentation of three blue one Brown's python animation library. We need to think of radically new ideas and AI can help us do that. We have to find the right datasets, properly frame our problem mathematically as well as our objective function and let it work for us. So let's move into the architecture of how this worked. Starting with the data Alpha Star used to distinct data sets. The first was a series of anonymized game replays from expert players.

Speaker 1:          04:58          The largest set ever released since these games were prerecorded. The result of every action ever taken was known making this part a supervised learning problem. The second data set it used was realtime gameplay versus itself and not just one or two games, 200 years worth of gameplay sped up because we can speed up time in a game world on computing devices. Alpha star got pretty good having burst, been trained on the game replay Dataset. Then it became really, really good after playing against itself and the second Dataset, the team hasn't yet released a paper and the only indication we have as to what its architecture could be comes from two sentences. In the blog post. In the first sentence it says the neural network architecture applies a transformer torso to the units combined with a deep LSTM core, an auto regressive policy had with a pointer network and a centralized value baseline. The second architecture specific sentence is that the weight update rule isn't efficient and novel off policy actor, critic reinforcement learning algorithm with experience, replay self imitation learning and policy distillation. That's a lot of concepts, so I'm going to explain just three of the most important components here, the transformer network, the point or network and the multi-agent reinforcement learning setup. Then I'll give my best educated guess as to how these three components were put together to create Alpha star

Speaker 2:          06:32          draft.

Speaker 1:          06:35          Let's first start with the transformer network. This was a neural network that was first proposed in a paper titled attention is all you need by Google brain. Google wanted to make its language translation system more accurate, so they used a model that uses a technique called attention to do that as well as speeding up training time at a high level, a fully trained to transform our network can take an input in one language and output it in a different language. If we look closer, we'll see that it actually consists of two components on encoder model and a decoder model. The encoder is actually a stack of encoders and the decoder is a stack of decoders. Each one of these encoders consist of a feed forward neural network and what's called a self attention mechanism. The inputs to each end coder thus flows first through the self attention layer.

Speaker 1:          07:28          Then to the feed forward network. By flows I mean matrix multiplication not flows in the hip hop sense. The decoder has those two components as well, but it also has an attention layer that helps it focus on relevant parts of the input data. The input data is partitioned and each partition is encoded into a vector using an embedding algorithm. Each of these embeddings are independently fed through each layer which allows for parallelization. This is called multi-headed attention. Each vector is split into several heads multiplied by weight major sees the results are concatenated and multiplied by another weight matrix and that's the output. These weight values are trained. It learns the strength of each vector of its relevance in the larger hole. It knows what to pay attention to for whatever objective function is defined rather than naively treating the entire input vector as equally relevant.

Speaker 1:          08:24          It learns which parts are most likely the most relevant. The decoder components are architected in the same way altogether. The incoders starts by processing the inputs sequence. Then the outputs of the top encoder is transformed into a set of attention vectors. These are used by each decoder and encoder decoder attention layer, which helps the decoder focus on appropriate parts of the input sequence. After the encoding basis over the decoding phase begins. Each component outputs it's results until finally the result is fed into a softmax layer which outputs a set of probabilities of likely values through optimization and knowing what the output should be. This network will learn how to predict the most likely output. That's the first component. Let's now move onto the second component, the auto regressive policy head with a pointer network. A pointer network is actually pretty similar to a transformer and that it's also a sequence to sequence model with attention.

Speaker 1:          09:25          What makes a pointer network unique are two things. The output of a point. Your network is discreet and corresponds to positions in an input sequence, and the number of target classes in each step of the output depends on the length of the input, which is variable. Pointer nets are great for problems like sorting words or numbers, sorry about will soar. No one loves you as for its auto regressive policy had an auto aggressive model is one where every inputs sequence depends not only on the input but also previous outputs. So the point your network is an auto regressive model that outputs a policy. What's the policy you ask? That brings us to the third and final component, the multiagent reinforcement learning architecture. The idea behind reinforcement learning is that in some simulated environment where time is an important element, an agent will take an action, receive a reward, then transitioned to a new state and repeat this process again.

Speaker 1:          10:23          It learns a policy, a mapping of states to actions. When the agent is a deep neural network, it's considered deep reinforcement learning of which there are many different architectures when there are multiple agents learning at the same time. It's called multi-agent. Deep reinforcement learning and a centralized baseline is a value by which all these decentralized agents can agree is the standard that they can compare their learnings against to sync the learning process. A type of deep reinforcement learning algorithm called actor, critic has one neural network be a critic and that it measures how good the action taken was and is value based. The other, the actor measures how an agent behaves and it is policy based. So those are the three major ideas of the system, a transformer network, a pointer network, and a multiagent deep reinforcement learning actor, critic architecture and swag. I guess what I'm now going to do is make an educated guess as to how these components were used together and when the transformer network acts as the critic network.

Speaker 1:          11:27          The actor network is the pointer network and this coupling is fed input data from the raw game interface, which is a list of units and their properties. The output of it is fed to the point your network, the point network. Then using the policy. It's learned thanks to the transformer network and its own weight values, outputs an action in the first part of training. It compared it's actions to those of the gamers they've watched during all those hours of free play, rewarding or punishing. It's weight values depending on how similar they were. Then in the second phase where it played against itself at each iteration, new versions of itself were replicated. All of them learning each of them exploring the huge action space of starcraft game play while ensuring that each competitor performed well against the strongest strategies and doesn't forget how to defeat earlier ones. In the end, the most strategically sound agent is chosen as the final fully trained model to train Alpha Star.

Speaker 1:          12:24          They built a distributed training setup running for 14 days using 16 TPU for each agent and the final Alpha Star agent contain the most optimal mixture of strategies that were discovered and was able to run on a single desktop GPU. As you can see, 2019 is already turning out to be an exciting year for AI. There are three things to remember from this video. Deep minds. Alpha star Algorithm beat one of the top starcraft two players in the world. A feet. Many experts thought would take much longer. Alpha star used deep reinforcement learning to do so. The defacto technique for AI in game environments and this technology has the potential to help us solve problems that require predictions over very longterm sequences in. What's a problem you want to solve with AI? Let me know in the comment section and please subscribe for more technology videos. For now, I'm going to play starcraft, so thanks for watching.