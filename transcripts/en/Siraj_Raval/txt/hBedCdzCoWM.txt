Speaker 1:          00:00          I'm glad I can book a self driving Uber Now. Hey, this car is in self driving. I need to, we'll never be hello world. It's Saroj and in this episode we're going to talk about how self driving cars work. Then people in our own self driving car in a simulated environment, self driving cars aren't in the realm of science fiction anymore. Real companies like Toyota and Ford have millions of dollars in r and d pouring into this technology. Services like Uber and Lyft that currently pay human drivers will soon deploy entire fleet of self driving cars. So prepare per skynet, just kidding. And two or three years we're going to start seeing hundreds of thousands of self driving cars being sold to regular consumers. Get your self driving trucks right here. But how do they work? Well, when we humans are in the driver's seat, we're observing our environment by receiving an input of our surroundings and simultaneously processing it in order to make a decision of which way to move the steering wheel.

Speaker 1:          00:55          This can be translated into a machine problem known as slam slam or simultaneous localization and mapping and it's something all self driving cars do. A self driving car is usually outfitted with GPS unit on inertial navigation system and a range of sensors. It uses the positional information from the GPS and navigation system to localize itself and the sensor data to build an internal map of its environment. Once it has its position and its internal map of the world, it can use that map to find the optimal path to its destination that avoids any kind of obstacles, be that dead babies or Pokemon. Once the car has determine the optimal path to take. That decision is then broken down into a series of motor commands which are fed into the cars, actuators. That's a high level description of how they work, but roads are complex. It's not just about avoiding obstacles.

Speaker 1:          01:41          There are weather conditions that require changes in the way you accelerate different types of road signs and situations that you probably couldn't ever predict. A recent paper came out just this year called longterm planning for short term prediction. These guys proposed a planning algorithm for self driving cars, specifically one that would be able to make immediate actions to as to optimize a longterm objective. An example they used was around about when a car tries to merge in a roundabout, it should decide on an immediate acceleration or breaking command while the longterm effect of the command, it's a success or failure of the merge. Traditionally planning for self driving cars is done via reinforcement learning. The car learns to continuously correct. It's driving capability over time through trial and error. When training the car or agent observes the state s that is a scene that it observes and takes an action a and depending on whether or not the action was good, however we define good, it can receive a reward are then it moves to the next state s and the process repeats.

Speaker 1:          02:37          The goal is to maximize the reward and that depends on a policy which maps state to action that it learns over time. The state action value function is called queue and it helps find the optimal policy, but it can be super hard to learn Q and an environment as dynamic as roads with multiple cars. It's not just a problem of predicting your own cars actions. You have to be able to predict other cars actions as well. So to this problem of learning queue, they used a deeper recurrent neural network to learn a policy and the input to the neural net was a vector that contains both the predictable part, the speed of the car, and an unpredictable part that speed of other cars so it could learn from both. They applied it to just two features, adaptive cruise control and merging roundabouts. But is there a way to make one learning algorithm that can learn everything from the ground up?

Speaker 1:          03:22          The technical term for this is end to end and an even fresh paper that was released about three months ago tried it. A team from Nvidia put three cameras on a car windshield to receive input data fed this video data to a convolutional neural network and features were learned by themselves. They didn't explicitly decompose the problem into sub modules for different scenarios. There's CNN matte what it saw from the input directly to steering commands. It was first trained in a simulation with prerecorded video, then trained by a human driver. They got great results, but it was hard for the paper authors to differentiate the feature extractor part of the neural network from the controller part, so it was difficult to test each. That's why most real world car manufacturers have decided it's not yet possible to test and verify an end to end system. They end up just making software where each module is separate and can be tested on its own.

Speaker 1:          04:10          A hacker named George Hotz built a self driving car in his garage, which is a couple of cell phone cameras and the total cost turned out to be just a thousand bucks. Let's train our own self driving car using cue, learning to drive itself without running into obstacles. After we declare our imports, let's write our training function for our car. First it'll take in a neural net with a set of hyper parameters as the parameters. Then we'll define some variables for the number of frames we want to observe for both training and testing. Well, then define our positional variables for localization. We'll create a new game instance and get the first state of the game instance. We'll also set a timer for tracking purposes. Then when we start building experience replay, we'll update our positional variables and choose an action depending on the state randomly. If the random variable is outside of our constraints, we'll get the Q values for each action to help us find the optimal policy.

Speaker 1:          04:59          We'll take that action and if it is valid, we will get a reward. Once it's done observing the game and building experience replay, we'll start training, sampling the experience, replay memory, and getting the training values. It'll then train the model on this batch that is a neural network. Then update the starting state, and if the car dies, logged the distance and reset the car's life. Finally, we want to save the model every 25,000 frames and the weights file. Let's see how it looks in a simulated environment. It constantly tries to avoid obstacles through a mix of reinforcement learning and a neural net. Once you've got it working in the simulator, you can port it to a real RC car and have it self-drive all over your room. Links down below for more info death, subscribe for more ml videos. I've got to go to descend some gradients, so thanks for watching.