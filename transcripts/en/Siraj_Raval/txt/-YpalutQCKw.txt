Speaker 1:          00:00          Hello world. It's the Raj and in machine learning you'll hear the term Monte Carlo a lot. I'm going to explain what that means and we'll use it to help a house cleaning robot navigate a virtual living room. To recap, last week we talked about how there are two main algorithms for computing optimal policies for an AI, namely value, iteration and policy iteration. A policy is the set of rules for how an AI chooses actions to take. We modeled our environment using a mark Haub decision process and we use the transition model to describe the probability of moving from one state to the other. This is the most common way to formalize a reinforcement learning problem where an agent provides an action to the environment as a function of the state and reward causing the environment to update and provide a new states and reward to the agent in a feedback loop.

Speaker 1:          01:05          So far we've assumed that the agent knows what all the elements of the Mark Cobb decision process are. We can just compute the solution before ever actually executing an action in the environment in AI because we can compute the solution. To this decision making problem before executing the actual decision. We'd typically call this process planning, which is the opposite of what NASA is doing for a Mars mission. Both value iteration and policy iteration algorithms are examples of planning algorithms, but reinforcement learning isn't so kind to us. What makes a problem a reinforcement learning problem instead of a planning problem is that the agent doesn't know all the elements of the markup decision process. So it wouldn't be able to just plan a solution. It doesn't know how the world will change in response to its actions, I. E. The transition function tea nor what immediate reward it will receive for doing so I either reward function are the agent has to try taking actions in the environment and observing what happens until somehow it finds a good policy from doing so.

Speaker 1:          02:25          So the question then is if the agent doesn't know the transition function or the reward function can find a good policy? The answer is yes. And there are two approaches here. The first approach is for the agent to learn a model of how the environment works from its observations. Then plan a solution using that model. So if an agent is currently in a state and takes an action than observes the environment transition to the next state with the, that information can be used to improve the estimate of T. And r. Once the agent has a model for the environment, it can then use a planning algorithm like policy, iteration or value iteration with it's learned model to find a policy reinforcement learning solutions that follow this framework are called model based algorithms. This is when an agent exploits a previously learned model to accomplish a task at hand, but it turns out that an agent doesn't have to learn a model of the environment to find a good policy.

Speaker 1:          03:39          Sometimes our agent can simply rely on trial and error experience for actions election and we can call this model free learning in moto free reinforcement learning. The first thing we miss is a transition model. The second thing we miss is a reward function which gives the agent the reward associated to a particular state. There are two approaches here, a passive and an active approach. In the passive approach. We have a policy which the agent can use to move in the environment in state [inaudible]. The agent always produces the action a given by the policy. The goal of the agent is to learn the utility function. This is a case for Monte Carlo for prediction, but it's also possible to estimate the optimal policy while moving in the environment. In this case we are in an active case, I. E. We are applying Monte Carlo for control. Okay? Okay. Okay. Lots of terms here.

Speaker 1:          04:42          So let's rewind a little bit. Let's say we've got a four by three tiled floor. This represents our kitchen. Don't ask me why it's so tiny. The virtual rent is too damn high and we want our cleaning robot to clean the whole kitchen. This environment has an unknown transition model. The only information about the environment is the state's availability and since our robot doesn't have the reward function, it doesn't know which state contains the charging station and which state contains the stairs. Only in the passive case does the robot have a policy that it can follow to move in the world. And since the robot doesn't know what's going to happen after each action, it can only give unknown probabilities to each possible outcome. In this passive case, our objective is to use the available information to estimate the utility utility function. To do this, our robot could estimate the transition model moving in the environment and keeping track of the number of times an action has correctly executed.

Speaker 1:          05:52          Once the transition model is available, the robot can either use value iteration or policy iteration to get the utility function. There are different techniques which can find out that the transition model, but the problem with this approach is that estimating the values of a transition model can be expensive for our three by four world. That means we have to estimate the values for a table of 12 states by 12 states by four actions. So there is another technique which directly estimates the utility function without using the transition model called the Monte Carlo method. In the 1940s Stanislaw ou alum invented the Monte Carlo method to help with his experiments. The idea is simple, use it randomness to solve problems. It's used a lot in AI. In fact, deep mind used the Monte Carlo method to complete a tree search to find the best move in the game of go, which ended up beating the world champion at the game.

Speaker 1:          06:57          It's a useful technique since it lets our agent learn the optimal behavior directly from its interaction with the environment. In our cleaning robot scenario, the robot starts at a state and follows its internal policy. At each step it records the reward obtained and saves the history of all the states visited until reaching a terminal state. We call an episode the sequence of states from the starting state to the terminal state. If our robot recorded the following three episodes, we'd noticed that the robot followed its internal policy, but an unknown transition model perturbed the trajectory leading to undesired states. In the first two episodes, the robot eventually reached the terminal states obtaining a positive reward, but in the third episode the robot moved along a wrong path, reaching the stairs and falling down. Every occurrence of a state during the episode is called of visit. Using our discount factor, we can calculate the return.

Speaker 1:          08:04          For a given state. After three episodes, we come out with three different returns. We are taking the expectation of the returns here, but we need more than three episodes to properly approximate the utility. Why is this? Well, using Monte Carlo terminology, we can define s to be a discrete random variable which can assume all the available states with a certain probability. Every time our robots steps into a state, it's like we are picking a value for the random variable S. For each state of each episode. We can calculate the return and store it in a list. Repeating this process for a large number of times is guaranteed to converge to the true utility. This is because of the famous theorem known as the law of large numbers, I. E. The more trials the closer to the expected value, our results will be. After several iterations we can see that the utility gets more and more accurate and thus our robot gets better and better at cleaning.

Speaker 1:          09:10          They're just three key points to remember right now. In model based learning. The agent exploits a previously learned model to accomplish the task at hand, whereas in model free learning, the agent simply relies on some trial and error experience for action selection. Monte Carlo methods rely on repeated random sampling to obtain numerical results and for model free learning we can use the Monte Carlo method to either learn the utility function or to estimate the optimal policy. The wizard of the week is Justin Francis. The challenge was to create a simple value iteration algorithm using open Ai's gym environment and Justin did just that. He even managed to create a really cool three d graph of state values over time. Impressive stuff. Justin and the runner up is suck Chum Sharma who created a deep cue learning Bot for a PAC man environment. Great work. This week's coding challenge is to use the Monte Carlo method to help an AI navigate in an environment, posts your get hub links in the comment section and I'll review them personally announcing the top two entries next week. Please subscribe for more programming videos, and for now, I've got to go compute the optimal policy. So thanks for watching.