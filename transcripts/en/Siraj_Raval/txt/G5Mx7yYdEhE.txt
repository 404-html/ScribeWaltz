Speaker 1:          00:00          Hello world. It's the Raj and do you want to predict cryptocurrency prices? Of course you do. Who wouldn't? Everybody's talking about cryptocurrency these days and if you understand machine learning, why not apply that skill to try to predict some cryptocurrency prices? Right? In this video, I'm going to show you how to predict the price of Bitcoin, but this can really apply to etherium. This can apply to any alt coin. It can apply to any cryptocurrency, and we're going to use a very simple model built with care os, a deep learning library that is very popular and I've talked about it before. We were going to use it to predict future prices. Now you might be asking, wait, does this work? Are you sure this is going to work? And let me just tell you this, JP Morgan, Morgan Stanley, all of these big banks, all of these big hedge funds numerate all of these hedge funds.

Speaker 1:          00:48          I promise you they're using some kind of algorithm to predict future prices. Will they share it with you? Definitely not because that is their profit, right? That is their secret. Of course we're not going to share it. So the answer is yes, it's possible. There's millions of variables out there and you can get a greater than 50% a return if you do this the right way. Is this the absolute optimal right way? I don't know, but it, the graph looks really nice and I've made sure to really avoid overfitting. So, uh, using dropout and some other techniques that I'll talk about. So let's just get into this. Okay. All right. So what I'm gonna do is I'm going to show you this code. Now, this code, all of it is available in the description of this video. So definitely check it out and follow along because I'm going to be analyzing this code.

Speaker 1:          01:33          I'm going to be talking about different techniques. Okay? So let's, let's do this. So the first step is for us to import our dependencies, which I've got right here. The first dependencies are of course, all of those carrots. Submodules remember Ken Ross is the easiest way to get started with deep learning. If you haven't searched care os explained Saroj on Youtube. Great video for you. But anyway, in this case, we're going to use what's called a recurrent network specifically. Okay. This is, no, this is a long one. A by directional LSTM recurrent network. I'll talk about exactly what that is, but I've talked about it before in my intro to deep learning course, which you should watch as well if you haven't. Uh, but uh, it's, it's basically a more advanced version of her recurrent network that takes into account future, uh, values in a sequence instead of just the past.

Speaker 1:          02:19          But [inaudible] is awesome. It's a great library. It's got loads of documentations, great examples and uh, yeah, you can basically learn all about deep learning just from reading the chaos documentation, which I've got right here. So it's a great library. Okay, so enough of that. So what I'm doing here for these forts, first four lines is I'm importing the relevant parts of care os that we're going to need for this model. The first part is dense. Dense means a fully connected layer. That means all of the nodes in one layer are connected to all of the nodes or neurons in the next layer, which usually happens at the end of a deep network. Usually the last two or three layers are fully connected, whether it's for convolutional networks or LSTM networks. I've got the activation, a sub module which is going to be used for are activation function, right?

Speaker 1:          03:04          This is our nonlinearity so our network can learn both linear and nonlinear functions, which of course the stock graph is definitely nonlinear and then drop out. Drop out is a technique that I'm going to talk about, but in this case I'm using it to prevent over fitting. Then I've got the LSTM class, which is, which stands for long short term memory. I'll talk about all of this by directional is a type of network. Sequential is a type of graph like in order, you know, ABC d, uh, instead of like, you know, multiple branches like a, like a directed a cyclic graph or something. It's more of a single, it's more of like a like a link, a singly linked to list in terms of a graph in terms of bi directional. It's a doubly linked lists, but we'll, we'll get into that. Okay, so the next part are is psychic learn.

Speaker 1:          03:45          Second learns a great library, but at this point it's really all of it's functionality has been eclipsed by great libraries like tensorflow, Pi torch. But what it's really gotten going for it is it's a metrics module, so like this, you know, it's loggings like s sk, learn dot metrics. It's got so many great sub modules that I use still, but we're going to be used it too important the mean squared error, which is going to be our loss function. This is what we're going to minimize over time to make our network better and better at predictions. Lastly, we've got logging for logging with the training process, num, py and math for standard matrix math operations map plot live for plotting like our graph at the end. And lastly, data processing is going to be pandas. Pandas is the data processing tool that data scientists use.

Speaker 1:          04:29          In fact, I have seen data scientists put pandas expert in their, on their resume, like it's that important, the library. So if you haven't seen pandas yet, definitely check it out. So, uh, this is a huge like infographic, but let me first read this, this first step here. So for data processing, what we're going to do is we're going to input this data as a CSV file. And I'll show you this, this data in a second, but we're going to time series transformed this data. So it's you, it's going to be from number of days by the number of features that was the original array, right? And we're going to convert that to the number of days minus the window size times the number of days per sample times the number of features. So we're turning it from a two d array, two or three d array. And this is just the preprocessing step.

Speaker 1:          05:09          So we can then feed it into our, uh, network, right? Uh, and normalization is what we're, what we're about to do. So the reason we want to normalize data is so that our model will be able to converge faster, right? Normalization means, I mean it means a lot of things, but check, now, let me go to this infographic here, but we have five, five rules of data normalization. The first is eliminating repeating groups. Anytime you have some values that are being repeated, eliminate them. Eliminate redundant data, eliminate columns not dependent on the key. So if attributes to not contribute to a description of the key or move them to a separate table. So it's kind of compartmentalizing all of these like different features, isolating independent, multiple relationships, and isolating semantically related multiple. So you can really read the small text later on. But those are like five key rules for data normalization.

Speaker 1:          06:03          But in general, remember that data normalization is a step that we always perform in data preprocessing. It's just a very important thing to do. And so in this case, what we're going to do is we're going to divide each value in the window by the first value of the window and then subtract one. So I. E. If we had an array like this for three to once normalized, it would become zero negative 0.25 negative 0.5. And notice that these values are much smaller and the interval between these values is much smaller. So it's kind of dense. It's kind of condensing this graph so it's smaller and more readable and easier to converge on. For our model, the normalized basis, we're going to keep them, that means those original numbers, just so we can compare the models predictions with the true prices, we're going to split our data, 90% training, 10% testing.

Speaker 1:          06:48          Okay. So this data, what it's going to look like is like this. It's going to have so many, uh, different columns for all of these features. So you might be wondering how is predicting cryptocurrency any different from predicting stock prices or any of the other, you know, financial predictions? And the answer is cryptocurrency has its own set of variables that the stock market doesn't have. You've got block size, block, high annual Hashcode, Medcalf slaw, market capitalization, Hash rate, you know, minor revenue value. All of these variables that are unique to blockchains, right? Very, very cool stuff. So in terms of where you can get this Dataset, if you just Google a bitcoin blockchain Dataset, you've got several. I got mine from Kaggle, which is the first link. Uh, but you've got several here. In fact, you could just pull it from the blockchain API directly.

Speaker 1:          07:35          Uh, if you search stack overflow, you've got some people who are uh, who have, there we go get bitcoin historical data. This guy is like, you can get the whole trade history and CSV format from here. Api Dot bitcoin charts. Startcom Quandl is another one. Uh, but basically I use Kaggle, uh, just because I really liked Kaggle and um, yeah, but there's a lot, you've got a bunch of CSPS for, you know, all sorts of things. The exchanges you can play around with what data you're going to use, right? What is it, what is the exchange volume between different cryptocurrencies? Uh, but there's, there's so much data out there that it is just a crime to not be training some machine learning models on this data. So back to this. So we've got, we've got lots and lots and lots of data here. We can look at some of it, but that's, that's, that's the gist of it.

Speaker 1:          08:20          So let's keep going here. So then comes our loading data a step, right? So what we're gonna do is we're going to read the data file using pandas Builtin, read CSV function, and we're going to store this, that pandas data frame object in this raw data variable. Then we're going to change all the Zeros to the number before the zero occurs. So in this way, we're removing all the unnecessary zero values. We want very dense data. So that's what this nested loop does. We convert that file to a lists for further and easier preprocessing. And then we take that data variable and we convert it to a three d array. Remember I said we're converting it from a two d array to a three d array, right? And then we fill that three d array with the, uh, normalized values, right? This is that normalization step.

Speaker 1:          09:03          We keep the unformalized prices as well. So we can compare the two later on. We split the data into training. Remember 90% training, 10% testing. We shuffle the data and then we create those training and testing of variables we get the day before, why tests price, so there's so we can use that for prediction later on and then we get the window size and sequenced length. We return all of those variables, right? We computed all of these values and now we can return them, right? So now step two, step two for us is going to be to build our model. Now recall I said we're going to build a recurrent network. To be more specific, we're building a three layer recurrent network with 20% dropout at each layer to reduce overfitting into training data. Specifically, the type of recurrent net we're going to build is called a bidirectional LSTM network.

Speaker 1:          09:53          The model is going to have about 500,000 trainable parameters. Don't be alarmed. This is very normal for deep networks, right? Each of those values inside of the matrices of the weights, those are parameters that are being trained and they are tuneable values that are getting updated through some optimization scheme. In this case, the optimization scheme we'll use is Adam, right? So Adam is the optimization scheme. The loss function is going to be mean squared error. The linear activation function in this model is going to determine the output of each neuron in the model and then it's going to use the chaos is sequential bidirectional LSTM layers. So now I'm going to go into what I just said. So recurrent nets are different from feedforward networks because in feedforward networks, every, at every time step, we're only feeding in the new input data, right? You know, we have, you know, let's say like 10 data points, like every time step we feed in a new one, Duh Duh, Duh Duh Duh Duh Duh.

Speaker 1:          10:45          But with recurrent networks, there's loops, right? Instead of just feeding in the PR, the input data, right? The next data in the set of data, we're also feeding in the hidden state that was learned from the previous time step, hence the loop, right? Hence the looping part. We are feeding in the pin stay from the previous time step and the, the new input data. So we're concatenating both of those values. And we're feeding them in and the reason we do this is because when it comes to sequences of data, memory matters, right? If we have a sequence like one two three four, five, six and we want to predict the next number in that sequence, we have to know that before the next number, which is going to be eight spoiler alert, we have to know that the first seven numbers were one through seven and that's why we feed in the hidden state.

Speaker 1:          11:28          That is learned over time, but what happens is when we have really long sequences, whenever we're optimizing this network, the gradient Spanish slowly, slowly, slowly, slowly as we go from the last layer is back to the first layers and the gradient is what we use to update all of these weights. So in order to prevent this vanishing gradient, so all of the layers, all the weights in each layer are updated properly. Someone invented this technique called a short term memory cell, so the LSTM cell, it has a lot of features to it, but I'm not going to talk about all of them right now. But basically what it does is it's got an input gate of forget gate and an output gates and these act as valves, they act as vowels like plumbing valves that can store and locked in memory over time. And what this does is it prevents the vanishing gradient problem, which for us means we can predict longer term sequences, perfect for predicting cryptocurrency prices over a long period of time.

Speaker 1:          12:24          Now lastly, it's not just an LSTM recurrent network. It's a bi directional LSTM recurrent network. So if you've seen some of my older videos, the only other time I've talked about a by directional LSTM is when I was talking about how to create a language translator. I think it was deep learning number 16 or 15 because Google needed that too to create a really good, you know, state of the art language translation service. But by Directional Arice, rns are based on the idea that the output at time t may not only depend on the previous element in the sequence, but also future elements. It's not just about the past, it's about the future, right? So for example, to predict a missing word in a sequence, you want to look at both the left and the right context, right? Like I went to the gym to get every day.

Speaker 1:          13:14          What was that pause? Swole. Right. Do I want to get, I went to the gym to get swole when every day, so I'm looking at every day, what do I want to do every day I want to go to the gym everyday. What is it that I want to do every day at the gym? I want to get swoll right. That was a ta. That was a example I just came up with. Anyway, let's just keep going. I'll just keep going here. So bi-directional rns are good in this case because the price of Bitcoin, ideally we have, we have this list of all the prices over time, right? And we can predict future prices that are in the past. You know what I'm saying? Like as our network has training in one direction, right? We also have in a recurrent network training in the other direction.

Speaker 1:          13:54          And then we can then concatenate there, learned hidden states together to form a more accurate representation of the, of the, of the sequence, right? So it's essentially two RNN stacked on top of each other. The output is then computed based on both the hidden, both hidden states, so the hidden states of both rns and for the Adam optimizer. What I have here is a graph of the loss functions decreasing over time. And notice how Adam is the fastest one or the best one of all of them on the M and ist datasets. Uh, basically I have a great video on the differences between all of these types of activation functions. Just search which activation function should I use on youtube. And it will be the first link. Look, I'm telling you, I have made so much content on deep learning, anything that you would ever want to know, I have content on it.

Speaker 1:          14:42          So now we're going to initialize this model. Okay? So when we come to initializing the model, like I said, Ken Ross makes it super simple. Uh, bi-directional LSTM recurrent network is a relatively complex a model to me, but with carrots, we can do this in just about 10 lines. We'll initialize it as a sequential model. What had our first recurrent layer with dropout, and let me talk about dropout as well after this. So ad then our second recurrent layer, right? You're just, you're literally naming them like human readable, bi-directional Lstm with those two parameters. A third recurrent layer on output layer. Remember I said that to dense, fully connected layer comes at the end, and then our activation function and then our loss function at the very end, which is going to be Adam. So why do we use dropout? So drop out was a technique that was invented by Hinton, which was one of the godfathers of neural networks.

Speaker 1:          15:32          But basically the idea is that some overfitting is a real problem, right? When we have very homogenous data, it overfitting is a, is a very big problem, right? Because when you learn something, think about grooves in your brain, right? If you're learning the same thing over and over and over again and all your inputs are the same, you're going to have these very specific grooves in your brain. So if you have some kind of new data, your brain's not going to know how to predict it, right? Because it's been trained on such homogeneous data. So what dropout is, is essentially it's this technique which randomly turns neurons on and off. And what this does is it forces the data to create new pathways, right? Create new pathways between weights. In this case, right? Between essentially Matrix multiplications waits in each layer. And because it's creating all of these new pathways randomly, right?

Speaker 1:          16:19          These nodes are turned on and off. These neurons are turned on and off. What happens is the weights become more robust to, um, more heterogeneous data, which means you could give it, you know, new data that's not like the training set and it would be better able to generalize. So it increases the ability to generalize. That's why we've added dropout here and we can tune, drop out on and off. Like we could say, you know, 20%, 30%, 40% and it's one of those things where you're just going to kind of, you know, trial and error it out and see what works best. Now, now we're going to train the model, right? So we're going to train it with the batch size of 10, 24 for a hundred epochs. And if we're going to minimize the loss of its training data using the mean squared error.

Speaker 1:          17:00          And uh, so now we can look at this. So for fitting the model, we say, okay, it's time to start recording and just muddled up fit. That's it. And we give it all of those variables that we computed at the very beginning, the training data, and then we return the model, right? So this is our function for fitting the model right here. And then we test the model, right? We do the same thing for testing the model on the testing data. The models given x values of testing data and will predict normalize prices, which is the why, you know, underscore, predict. And so for testing it, it's, you know, it's relatively simple. We just say, test the model x test, create an empty two d array, fill the two d array with all the predicted values, and then plot those predicted values and see, see what turns out at the end, right?

Speaker 1:          17:41          So we're still riding these functions. We're going to, we're going to actually compute them at this at the end, right? Step five is to evaluate the in price so we can plot the models predicted change in price each day against the real change and each each price daily, right? So we can basically model the prediction versus the actual price, which is the real graph that we want to see in the end to see how good our model is performing. Then we processed a percent change in price at the very end, which is the Delta, right? The change between what the price that it was before and the price that it is now. And then we compare the prediction to the real data. So when it, when, when, when all is said and done, we're going to have some true positives, false positives, true negatives and false negatives, right?

Speaker 1:          18:26          Our model is going to predict the real values sometimes and it's going to predict the fake value sometimes, right? And so we want to know how often it's predicting, predicting real values and how often it's predicting fake or off values. And we want to minimize those fake or off values as much as we can. And so to do that we're going to compete, we're going to minimize our loss function. So remember, recall I said that the loss function that we're going to use here is the mean squared error, right? This is a very popular loss function and this is what it looks like. It's the mean of the squared error. So what I mean is we take the value returned by the model f and we subtract the actual value for the data point, right? That is the error, right? The prediction minus the real value, that's our error.

Speaker 1:          19:07          We take that air value, how far off our model's prediction is and we square it and then we add all of those values up together for every single data point we have. And then we divide by the number of data points there are. And that single value is our mean squared error. And that is what we want to minimize at every single time step while we're training. Okay. Using the Adam optimization scheme, which is a variant of gradient descent. So right? This is just another graph of true positives versus false positives and negatives and negatives. But anyway, when we take these values and we add them and put them on top of each other, we get these other three uh, metrics of how good our model is. Performing precision. Recall an f one score. So position to position means how often is our model getting true positive compared to how often it returns.

Speaker 1:          19:54          A positive recall is how often does the model get a true positive compared to how often it should have gotten. The positive and f one score means a weighted average of both of those two values, right? And so we just compute those right here. See, it means squared error right here. Precision recall f one we literally just took those equations and wrote them out programmatically using just native python. So now we can put it all together, right? We've, we've compute, we've created all of these functions and now we can put it all together. So our first step is to load up all this data, which we're doing right here. And then we initialize the model, right? So I've already trained it beforehand. So we can just look at the results just right here in this Jupiter notebook, right? But remember it's a three layer by directional LSTM network.

Speaker 1:          20:38          Then we train the model, right? So this, the model is training by just running that fit model function that we created earlier. And then we can test the model. And so here is the graph. Here is the good stuff that we have. We have been wanting to see. All right, so this is the bitcoin price over time for a period of over 250 days. Notice how the real price is different from the predicted price, but just by very little, which is kind of exciting, right? So our model, it could be a bit over fit. Let's not, let's not kid ourselves. But as we add more features, as we add more data to this model, it will become more robust. We can implement features, we can implement techniques like dropout, we can implement techniques like you know, all sorts of normalization techniques out there. And I can go into that in a future video, but there are all these ways that we can make our model more robust.

Speaker 1:          21:26          I have other videos on predicting stock prices, which some of the learnings can be applied to here as well. Search predicting stock prices Saroj on youtube and like three videos will show up. And then we have one more really important graph and that is predicting the percent change. So what is it predicted present percent change from the previous day to the current day versus a real percent change. And notice how there is a bit more variance here between the real values and the predicted value. So we can, we can definitely do better here. Uh, and then we compare the predictions and the true data and then we get our statistics at the very, very end position. Recall F1 and mean squared error. Please subscribe for more programming videos. And for now, I've got to predict the future. So thanks for watching.