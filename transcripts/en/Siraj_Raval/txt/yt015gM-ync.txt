Speaker 1:          00:00          Are you a good driver? The answer is no. Hello world, it's Saroj and like it or not self driving cars are the future of transportation. We'll build a surprisingly simple demo in this video and let's a car learn how to mimic a human driver in a simulation using deep learning major technology companies like Lyft and Waymo and total as an auto makers like Toyota and General Motors have spent billions of dollars developing self driving cars and the belief that the market for them could one day be worth trillions of dollars. Autonomous buses and shuttles are currently being deployed in cities and airports. Driverless trucks are already delivering beer long distances and even autonomous flying taxis seem to be in our near future. And there's good reason for this revolution. Self driving cars would greatly reduce the price of transport for consumers since there's no human in the loop. And by using autonomous fleets of shared electric cars, we'd only need 10% of cars on the road currently, which would reduce co two emissions and pickup trucks immensely.

Speaker 1:          01:11          And when that shift happens, we can begin to redesign our cities into beautiful creative spaces meant for people, not cars. Perhaps the most popular reason of all of them is that it would create a safer environment for everyone. Data from the National Highway Traffic Safety Administration indicates that more than 90% of car accidents are caused by human error. That means self driving cars had the potential to save more lives than airbags. Seat belts and stability control combined a truly first of its kind technology for road safety. Unfortunately though one of Uber's autonomous cars made global headlines when it had a fatal collision with a pedestrian in Arizona, engineers at Uber have since been decoding what went wrong and this presents a new realm of legal and regulatory questions. We'll have to learn how to answer who should be held responsible for this kind of accident? How should a self driving car make hard decisions on who's life is worth preserving more?

Speaker 1:          02:17          How do we make self drying laundry? All those self driving cars are inexpensive technology. There is definitely room for lean startups in this space that can create software for them. Level five is building software and collecting data that's needed to scale autonomous vehicles globally. No tow aims to make these cards safer by gathering data from human driver behavior and there's a big space to combine blockchain technology with fleets of these cars to create even more autonomous systems, which Porsche has started trying out to increase transparency of decisions that it's cards mic but enough about the possibilities. Let's dive into how these things work. Right? Self driving cars have five core components that form a pipeline, computer vision, sensor, fusion, localization, path planning and control in that order. Let's go through them. Computer vision is the first step in the pipeline. It's how cameras see the road.

Speaker 1:          03:20          We humans handled a vision problem by handling a car's steering wheels with just our two eyes and a brain. A self driving car uses camera images to find the lane lines and track other vehicles on the road. Most of them have lots of cameras, not just to. Tesla, for example, gives its cars eight surround cameras that provide 360 degrees of visibility around the car at up to 250 meters of range. Providing superhuman vision ability. There are so many tasks that camera's enabled like Layne finding road curvature, estimation, obstacle detection and classification and traffic, light detection. Those are some of the main ones, but imagine a pedestrian is about to cross the road. A car has to first find where that object is in a camera image. We can call this detection, then determine what that object is. We can call that classification. Deep learning has emerged as the most accurate approach to working with camera, video and images.

Speaker 1:          04:21          In order to train a neural network on what a stop sign looks like. It's fed thousands of stop sign images and it gradually learns it's abstract representation, robustly able to classify all variations of a stop sign. This is much more efficient than the traditional or old AAF approach to computer vision. Things that focus on color extraction and other low level features, but the camera is not the only type of sensor a car can have, especially if bit by a radioactive spider sensor. Fusion is how the data from other sensors together with the camera data build a complete understanding of the vehicles environment. As good as cameras are. There are certain measurements like distance and velocity at which other sensors excel and some sensors can work better in adverse weather. By combining all of our sensor data, we get a better understanding of the world. There are different sensors for different use cases.

Speaker 1:          05:19          Radar is good for determining how far away an object is and how fast it's going, but radar signatures can be pretty sparse. It'll tell you that something is ahead, but it won't be able to tell you what that thing is. That's where lidar comes in. It emits an array of pulsed laser beams. The create a Three d point cloud. Lidar is the happy medium between camera and radar that allows a car to detect and track objects from far away. Ultrasonic sensors on the other hand, have a small sensing distance which makes them useful for lateral movements like parking. We can combine all of this sensor data together using what's called a filter. There are a lot of potential filters, but a popular one is called a Kalman filter. This filter relies on probability and a measurement update cycle to put together a probabilistic understanding of the world like predicting the speed at which another vehicle is moving.

Speaker 1:          06:20          The Coleman filter keeps track of the estimated state of the system and the variance of uncertainty of the estimate. The estimate is updated using a state transition model. It's pretty similar to a hidden Markov model. Next localization is how a car figures out what its position in the world is, which is the next step after a understand what the world looks like. Our phones are equipped with gps so they do this for our position, but unfortunately gps is only accurate to within about two meters. If a car were wrong by that much, it could result in fatal accidents. So more sophisticated algorithms are used to help a vehicle localize itself to within two centimeters. By matching the point cloud, it sees to the point cloud that the map has measuring the vehicles distance to specific landmarks around it like mailboxes poles and childish Gambino. The next step is path planning, which is what happens after the car knows what the world looks like and where it is in that world.

Speaker 1:          07:27          The car charts a trajectory through the world to get to where he wants to go. First it predicts what the other vehicles around it will do. Then it will decide which maneuver it wants to take in response to those vehicles. Lastly, a trajectory is built to execute the maneuver safely. The final step of the pipeline is control. Once a car has a trajectory, it has to turn the steering wheel and hit the throttle or break accordingly to follow that trajectory. When we have an idea of the path we want our car to follow, we try to control it and to do this, it can sometimes be tricky like attempting a hard turn at high speed. This is something race car drivers are great at and computers are getting really good at. Control theory is the study of how to apply force to an object to control its movement.

Speaker 1:          08:17          I'm not about to dive into that right now, but Brian Douglas has a great video on controlled theory. Link to that in the video description. Quite a lot of components, right? Let's build something doable for us. You'd ask any has this self driving car nanodegree that's pretty popular and they opened sourced one of the projects involved called the behavioral cloning. They built this custom simulator that lets anyone drive a car and train it to drive by itself. We can build a neural network using care to try and replicate human steering behavior. To do this, the network takes as input the frame of the frontal camera and predicts the steering direction had each instance. So there's a simple mapping. It tries to learn between an image, that camera frame and a label that direction to move. And since all of this data can be recorded while you drive the car with the Arrow keys, we can train a car on that data set easily.

Speaker 1:          09:16          A convolutional network learns from images building increasingly abstract generalized representations of the image data set it learns from. I have a great video on convolutional nets. See the link in the video description. Students have reported that training takes anywhere from one to 24 hours on a laptop with all sorts of GPU gps. It's pretty easy to get this project up and running and I highly recommend you check it out. So three points to keep in mind from this video. Autonomous cars are the future of transportation. They have already started being deployed and will one day become commonplace. Self driving cars, use computer vision, sensor, fusion, localization, path planning and control to navigate their environment. And we can train our own self driving car by trying out that you Udacity simulator on our local machines. Hey, you made it to the end. You win. Hit the subscribe button and all your code will become free for now. I've got to take public transit, so thanks for watching.