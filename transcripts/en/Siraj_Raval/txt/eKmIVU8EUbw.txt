Speaker 1:          00:00          Hey Siri, any songs you'd recommend? The blacker the berry by Kendrick Lamar could pick why that one? Because the police,

Speaker 2:          00:06          hello world, welcome to try geology and this episode we're going to build a movie recommender system recommender systems are all around us. Almost every single major service on the Internet uses them to show you things that you'd like based on your interests. Sometimes it's so subtle you don't even notice. They help personalize content on the web, which makes users happy, which makes companies money rich. The two most ubiquitous types of recommender systems are content based and collaborative content based systems focus on each user individually. It looks at the items you've already expressed, an interest in your lights and ratings and records, their keywords, attributes and tags. Then your profile has gradually built with these attributes. Once your profile is built, the system will start recommending. I didn't do like with similar attributes to the ones you've already expressed interest in. So if you're on an ecommerce site and you buy a bunch of Nickelback tee shirts, those items will have tags like worst band ever and stupid.

Speaker 2:          00:48          Based on that content. The system will suggest similarly tagged items for you, like Cree tee shirts and Walmart brand guitar. Then they're collaborative systems. These are the most ubiquitous types of recommender systems. A collaborative system recommends you items based on what other similar users have expressed interest in it. You really like base jumping and constantly by base jumping gear online. This is the more fine users who have similar purchase history. It can recommend other items they bought that you have it. It's likely that you'll be into those products as well. So we're going to build an APP that can recommend movies you'd like in 10 lines of c plus plus using Amazon's and newly released ml library called deep scalable, sparse tensor network engine or destiny. We've got to think of a better name. Our model will be a neural network because depending on how deep we make it and how much data we feed it, it's just going to outperform everything else.

Speaker 2:          01:26          Let's be real. Then we're going to train it in the cloud using AWS because I ain't got time to train this on my Mac book. That's an e is what Amazon bill for production use specifically to recommend products to customers that they might like. It's optimized for sparse data and multi GPU computation data. It sparks if it contains a lot of Zeros as not a whole lot of valuable information. Rex usually operate on sparse data. Not everything is connected, but you can manage to find some valuable links between people in items. Most central library to implement data parallel training as in it splits training data across multiple GP use. It's works, but there's definitely a trade off between speed and accuracy. That's any use his model parallel training, so instead of splitting the data across multiple GP use, it flips the model across multiple Gpu so all the layers are spread out across multiple gps on the same server automatically for you, Amazon had to do this because the weight matrices to had to racks.

Speaker 2:          02:07          That is all the mappings of users and attributes just didn't fit in the memory of a single GPU. When it comes to the mount libraries. Destiny isn't as general purpose as tensorflow, but it is twice as fast when it comes to dealing with sparse data. So we'll follow our methodology and collect our data set, build a model, train the model and test the model. We'll call it a retrieved data set method, but the perimeter as a URL toward downloadable model. In our case, we're going to use a sample movie lens data set, which contains user rating for a lot of different movies and their associated tags. Once we have that, we want to convert it to a format or MLA in rate. When this case is the net CDF format that's EDF is designed for efficiency realization of the large array of numbers and it's what destiny expects will generate it for both the input and output layer of our neural network and we'll use the name of the downloaded data set as our parameter.

Speaker 2:          02:42          Both of these functions generate a net CDF file, an index file for neurons and the index fall for features. Once we generate in our model, it's time to train our neural networks and destiny. You build your model in a Jason Fall instead of programmatically. We can see in the config dot Jason Falls the structure of the neural network. This is where we set our hyper parameters and most important takeaway here is that we are creating a three layer of equal with neural network. I mean data just flows one way with one 128 node hidden layer and our activation function at each note is the classic sigmoid which turns values into probabilities. We can go ahead and run our train function with the batch size and number of epochs of the parameters or set the batch size or number of examples to two 56 and the number of iPads or runs a 10 once we run this little critter newly trained model fall called GL dot and see which we can then use to predict recommendations.

Speaker 2:          03:20          Our last step is to predict recommendation so we can just call the predict method and set the number of recommendations parameter to 10. This will place the newly created predictions and the reservoir. Now that we have our code Betty to be compiled and run, we want to upload it to AWS. To start off, we'll click on the easy to button, which will take us to Amazon's cloud computing service. They don't want to make sure when the US east North Virginia regions, it's payments on creative preconfigured image with dependencies like Kuda and opening NPR already set up for us in that region. We'll click on Ami to under the images directory of the left sidebar and the search for the instance called Ami, a d six f to e six BC. It should pop up and then we'll click the blue launch button to spin up an instance using that image.

Speaker 2:          03:52          Then it'll show us a list of instance types. Since we want to speed up training time, let's go ahead and choose the GPU option here. Then we'll click review and launch and see the final page before we can launch our instance. Everything looks good to go, so let's click launch. It'll prompt you to create a new key pair. Go ahead and download it so you have it locally. This, we'll have authorized your machine to connect to AWS. Now that we've successfully launched a GPU instance, we need to upload our code to it and train it. I'm a fan of using file Zilla to upload files, so let's use that. We'll click the site manager icon, then pacing the host name. We've got gotta. Be sure to set the protocol to SFTP. Then set the login type of normal, and the user is called Ubuntu. Once we set the field, we can click connect and it will show us all the current files.

Speaker 2:          04:24          In our instance. Let's go ahead and drag and drop our project in to the root folder. Now that our code is in our Ece to incense, we can open up terminal and ssh into it. We can find the SSH snippet for terminal under the instances section. Once we click the connect button, perfect. Let's just paste this baby right into terminal. Boom, we're in. Let's see the into our directory. Before we run our code, we'll need to do two things. Had the NPAC and NBCC compiler is to our path and make the library. We can export NPCC and then we're in, then we'll export NBCC. Now we can run our script, and once that's done, we'll have wrecks and our recs fold. That's pretty much it that you can scale your neural net accordingly. Depending on the size of your data. Check up the links down below, and please subscribe. For more machine learning videos. I've got to go fix a runtime error, so thanks for watching.