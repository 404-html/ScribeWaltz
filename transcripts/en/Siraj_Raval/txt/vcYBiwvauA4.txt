Speaker 1:          00:00          Being human is great. Nothing else can dress itself.

Speaker 1:          00:05          Hello world it Saroj and this video of a virtual character dressing itself is making the rounds in the Ai Community. In a joint collaboration between Google brain and Georgia tech researchers demonstrated that it's possible to automatically discover dressing techniques, meaning they managed to get an AI to learn how to dress itself using some clothing. It's movements are relatively smooth and what's incredible is that these movements are learned from scratch. None of them are hand coded. They use the bleeding edge technique called trust region policy optimization. To do this, and I'll explain how that works later, but first, we can use this technology to help improve the gaming experience. A player can create a custom outfit and a Bot can learn how to dress itself using it, allowing for a more immersive experience. We can also use it to help with the elder care market. Many people in assisted living facilities require a human to help dress themselves.

Speaker 1:          01:05          And if a physical robot can understand how to do that, older humans could instead dress themselves with the help of a personal robot that understands the intricacies of how it should work, enabling more independence and dignity. Also, as the online shopping experience continues to improve with more people buying clothes online, we could use this as a visualization tool, not only showing you how you'd look in a certain outfit, but also how it would look to put it on. And infant AI can learn how to properly move its limbs to dress itself. It could also use a similar technique to learn how to complete any physical task, be it on an assembly line in a factory as a personal training instructor, or even as a personal Netflix and chill companion. So let's go through the thought process these researchers went through to create this incredible algorithm.

Speaker 1:          01:56          We have a starting states where the character is holding a garment and an ending states where the character is fully clothed. We want to create an algorithm that will help our character transition from the starting states to the ending states by dressing itself. This dressing task can actually be split up into multiple sub tasks. For example, dressing a jacket consists of four subtasks pulling this sleeve onto the first arm, moving the second arm behind the back, scooping the second sleep onto the arm, and lastly returning the body to a rest position. Luckily no popped callers necessary here. Since this is a simulated environment where time is a dimension we'll want to use, not a supervised learning, but a reinforcement learning technique to help us for each sub task. We can formulate it as a separate reinforcement learning problem to learn an optimal policy. This policy will know exactly which actions to perform given the state of the character in order to complete the task.

Speaker 1:          02:55          The mathematical framework that all reinforcement learning techniques use called the mark Haub decision process or MDP can help us formulate our problem. And since we have several sub tasks, we'll create a separate one for each and our MDP isn't fully observable. Instead, it's partially observable. This is because we humans don't have direct perception of the full state of the world and of ourselves. During dressing. We have a limited perception of the state of the garment outside of any haptic and visual observations. Also, the full state space of dressing tasks is very highly dimensional, which leads to very large policy networks with hundreds of thousands of variables which would be intractable. So to replicate this reality, they designed a compact observation space, oh, which is a subspace of the full state space s. The observation space includes the characters, joint angles, garment feature locations, haptics, surface information, and a task vectra that suggests a direction for the end effector, which is the robot's hand to move based on dressing progress.

Speaker 1:          04:00          The state space consists of the joint angles and the velocities of the human model, the vertex positions of the garment, contact information from the previous simulation step and the values of the precomputed geodesic field on the garment. So we have a lot of numerical values to work with here. We're getting this data fed to us in real time and there's so many possible actions that our robot could take. It's not as simple as moving up, down left or right in a two d video game. Instead of only having a few set of possible actions. We have many in this continuous action space. Actions consist of the set of required changes to get from one state to another. The reward is a numerical measure of how close the character is to completing its current task as a function of the state of the character and the garment.

Speaker 1:          04:48          They designed it so that it rewards end effector motion in the direction of the task vector. Now that we formulated each sub task and it partially observable MDP, we need to decide on a suitable RL technique that will help us learn the optimal policy for each sub task trust region. Policy optimization is the technique they use and look, I know it sounds difficult to understand, but that's because it is. Luckily, I'll explain it in detail here. So here we go. In a traditional supervised learning problem, we know what the correct answer for every input would be, so we can provide reliable feedback to every prediction of our model. If, for example, we had a classifier that output that a plant belonged to the wrong class, we could just use the error between the prediction and label to optimize our network over time, but in reinforcement learning, we don't always have the same luxury of labeled training data for every input for dressing our character.

Speaker 1:          05:45          We don't know what the correct action is at a given time step. We have to learn it ourselves, but what we do know is whether or not the action we take eventually leads to a success or failure. In accomplishing our task, we can modify our optimization function to focus on increasing the chance of moves that lead to a favorable outcome while decreasing the chance of making moves that eventually lead to bad outcomes. We'll start with an untrained and neural network to represent one of our sub task policies. In the paper they used fully connected neural networks consisting of two hidden layers of 64 nodes each and can age activations with a final linear output layer to represent each policy. Our policy network will accept a state and output a probability distribution of actions. Our character will complete an entire episode until it's finished completing, addressing sub task.

Speaker 1:          06:38          Then we can use the collection of states and actions. We ended up taking over time to help update our network. Yes, we're unable to provide the correct label at every step, but instead we compute the gradient as if the action we took was the correct action. The gradient tells us in which numerical direction we should update the weights of our network so that the error is smaller than next iteration. If the action led to a win, we can update the gradients such that we are encouraging our network to take the same action again, if it led to a loss, we instead apply the negative gradients to the network parameters to discourage the network from taking that action. Again, this is considered a simple policy gradient technique, but should we use a simple policy gradient technique to learn an optimal policy for each sub task? No, because policy gradient techniques suffer from two limiting factors.

Speaker 1:          07:31          The first factor is the sensitive step size. At each time step, the network updates some amount, but it turns out that two largest step leads to a disaster. Think of it like being on a mountain. If the new policy goes too far, it takes an action that may be a meter too far and falls off the cliff. But if the step is too small, the model learns too slowly. The second limiting factor is that it's hard to find a good learning rates in reinforcement learning. One that doesn't cause an explosive policy updates. We want to limit the policy changes and ensure that each change guarantees improvements in rewards. To do this, we need a better optimization method to produce better policies. We can find hope in what are called trust region methods. Trust region methods define a region around the current iteration within which they trust the model to be an adequate representation of the objective function.

Speaker 1:          08:23          They then choose the step to be the approximate minimizer of the model in that specific region. So during the optimization procedure, after we decide the gradient direction, when doing line search, we want to constrain our step length to be within a trust region. So that the local estimation of the gradient remains to be trusted and trust. Region policy optimization. We can use a measurement of the difference between the old policy and the updated policy as a measurement for our trust. This difference is called the coal black Leibler divergence and it helps us measure just how much information we lose when we choose an approximation. Over time. We minimize this difference and our policy improves. Learning a control policy for each sub task works well, but how is a character supposed to know when to start one sub task in relation to another. In addition to giving each sub task its own control policy, the researchers introduced a policy sequencing algorithm that matched the distribution of output states from one task to the input distribution for the next task in the sequence and that was the last piece of the puzzle.

Speaker 1:          09:32          Using this algorithm, the team found that training a single sub task required 24 hours of simulation time on 36 CPU compute nodes. After trial and error, they learned that carefully defining the reward function made a huge difference in their results. Their character controller was able to successfully dress under various initial conditions and moving forward, they like to incorporate lower body dressing as well, which would require incorporating balance into the controller. There are three things to remember from this video. We can use deep reinforcement learning in real time environments to help robots learn how to perform any physical task to help their character learned to dress. These researchers use a technique called trust region policy optimization or Trpa and Trpa Oh is a policy gradient technique that constrains policy changes during updates to avoid exploding parades and world is yours. Please subscribe for more programming videos. And for now, I've got to undress myself, so thanks for watching.