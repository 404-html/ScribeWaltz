Speaker 1:          00:00          This is how I feel when my binary classifier outputs 50% hello world, it's Saroj and logistic regression is a very popular machine learning technique that's often misunderstood. In this video, we'll build a sentiment analysis app in python that uses a logistic regression to classify whatever texts that I input into the app as either positive, negative, or neutral. Every week it seems like an exciting new Dataset is made public on websites like Kaggle, AWS, pirate bay. I'll put a link to some data set websites in the video description. When observing one of these data sets for the first time, a novice might trug it off as useless or too complicated to understand, but AI wizards are delighted because we know that there's so much we can create and discover with the dataset using machine learning. Hopefully the Dataset is labeled. This is a column that describes what each data point is.

Speaker 1:          01:08          Think of it as a summary of what all those signals mean. This could be a dog and emotion, a diagnosis and adolescent phase. So many things. We can then consider this a supervised learning task and if it doesn't have labels than it would be considered an unsupervised learning task. This is more of an exploratory problem where we don't have a target label we're trying to predict. Instead, we use mainly clustering techniques to make discoveries. Supervised learning can be broken down into two types of problems. Regression and classification. Regression is when we want to predict an output value that can have a lot of different values and usually our output is some rational number. Classification is when we want to predict an output that can only have some specific categories. Now, depending on the type of problem we have, we can choose a suitable algorithm to solve it.

Speaker 1:          02:11          One algorithm that is frequently used to solve supervised classification problems is called logistic regression. It measures the likelihood occurrence of an event, so some possible questions that logistic regression could answer include how likely is it that a user will purchase a subscription? How likely is it that France will win the World Cup? How likely is it that getting a certain test score will result in admission to a good university? How likely is it that a tweet from Kanye makes sense? Once our model outputs a class likelihood percentage for a new data points, we can then use that to classify that data point according to a threshold value that we decide. You might be wondering why we use logistic regression and linear regression for this. Well, suppose we had a labeled dataset of cancerous tumor sizes. Each was either a malignant or not denoted by either a zero or one.

Speaker 1:          03:16          If we use linear regression, we'd construct the line of best fit for the data and graph it. We could then decide that all the values to the left of the line are considered not malignant and all the values on the right as malignant, but what if there's an outlier in the data, a data point that is significantly different than the rest. If we accounted for that outlier point, when we built our linear regression line, it would look pretty warped. It put some positive class examples into the negative class. If we were to draw out an imaginary class decision boundary that divides malignant tumors from benign tumors, it would look off. It should have been in a different spot, but a single outlier threw off our linear regression predictions. We need a way to deal with outliers in the data. Logistic regression helps us do just that because it uses the standard logistic function.

Speaker 1:          04:17          This was developed by statisticians to describe properties of population growth and ecology rising fast and maxing out at the carrying capacity of the host environment. It looks like an s shaped curve that takes any real number and maps it into a value between zero and one but never exactly those limits. This is described using the equation F of x equals l over one plus e to the negative k times the difference between x and the x value of the curves. Midpoints l is the curves and Max value. He is the natural logarithm base, a constant value use throughout math to help model growth and k is the steepness of the curve. Logistic Regression actually uses a special case of the logistic function called the sigmoid function. This is when l equals one because it's using binary classification values for its prediction. Either zero or one and k will be set to one as well.

Speaker 1:          05:21          Making a simpler equation. What logistic regression does is it uses a linear function to represent the input values. That's where the input values are combined linearly using weights or coefficient values, often referred to as the Greek capitol letter Ada to predict an output value. Why thanks Greece. The difference from linear regression though is that the output value being modeled is a binary values zero or one rather than a numeric value. We do this by plugging that equation into our sigmoid function, which gives us the full logistic regression equation. When we use this model instead of linear regression for our tumor Dataset, it'll incorporate any outliers into it. So let's start building our APP, shall we? It's a collection of labeled tweets that we can find on Kaggle. It's got both training and testing CSV files attached. So once we download the data set, we can view it ourselves in our Jupiter notebook.

Speaker 1:          06:25          Once we spin it up, we can import the data. Preprocessing tool hand does to help load the data into a modifiable data frame object in memory using the head function. We can see that the data contains three columns, the item sentiment of the tweet, either zero for negative or one for positive and the tweet itself. Notice how these tweets are all different sizes. Some of them are much longer than the rest on important data. Preprocessing step for us is to remove any unnecessary noise from our dataset. When it comes to text, there are a bunch of words that don't really contribute to the sentiment of the phrase as a whole. These are called stop words. Examples include the is an of using the nltk libraries stopped board collection. We can iterate through the Dataset and find any of the stop words that do exist and remove them once they're gone.

Speaker 1:          07:26          We can go one step further in reducing noise by reducing art vocabulary size and consolidating similar words. For example, swag, swagger and swag. Delicious all had the same stem and we can just replace all of those instances with the stem swag and Ltk has got a handy stem module to help us do this automatically and we'll also tokenize our words, meaning break each phrase down into individual words that form a larger vector. Train, train, train. I'm clean and data sets. Yeah, trained it on my laptop. Now it's on the intimate. Yeah, we can programmatically right out our own logistic regression class that we can use on the data later. It'll use to hyper parameters that we initialize when we call it first the learning rate, which controls how much we are adjusting the weights of our network during training. If it's too small, then optimization will be too slow, but if it's too large, our algorithm may fail to converge on the best result.

Speaker 1:          08:34          Second, a number of training iterations we'll use. We can first write out the sigmoid function. Since we already know the formula for our loss function, the way we measure how bad our prediction is compared to the real label, let's use cross entropy. This can be divided into two separate loss functions. One of them for when x equals one and one for when x equals zero. When we look at the loss function graphs for both of them, we can see the benefit of taking the logarithm. These smooth functions are always increasing or always decreasing and make it easy to calculate the gradient for our optimization process. When we compress these functions into one, we get our final loss function. It's like a function megazord Oh and one more helper function before our training loop at intercept. This will add a decision boundary to our graph. In our fit function, we'll initialize the weights of our network as theta.

Speaker 1:          09:35          Then the specified number of iterations will compute the linear function by multiplying our input by our weight matrix. Then use that as input to our sigmoid function to compute. The outputs will then compute the gradients by multiplying the transpose of our input data I the difference between our prediction and actual label. Once we multiply it by the learning rate, we'll adjust our weight value theta to help make a better prediction. Next round. After a few minutes of training, we can visualize our graph in the notebook and see that tweets are pretty accurately separated by our line. We can create a simple html UI where we take the text input and feed it to our python model to make a prediction based on what it outputs. We can set a threshold for it, happy or sad, and then have an Emoji that represents that show up on screen. We performed a binary classification here, but logistic regression can be used for multiple classes as well. That's for another video. There are three summary points you should remember from this video. Logistic regression measures the relationship between categorical dependent variables and one or more independent variables. It's more robust than linear regression to outliers in the data and using python and any freely available dataset. You can build your own logistic regression model in a few lines of code. Please subscribe for more programming videos. And for now, I've got to digress, so thanks for watching.