Speaker 1:          00:00          Hello world. It's a Raj and mx net. It's Amazon's deep learning library. It's right up there with tensorflow and cafe as one of the most used deep learning libraries out there. And this video, I'm going to build a very simple neural network using mx net and I'm going to go over all the different parts of its Api so you could see how it works. It's got this imperative Api, it's got a distributed training done really well. And what we're going to do is talk about its features, compare it to other deep learning frameworks, and then start writing some code in python in this stupid or notebook that I've got up. So an overview of mx net. It was founded by a couple of universities. Uh, it's, it supports pretty much any time. I mean it says here's CNNS and Lstm, but you can really make any type of neural network that's been discovered so far with mx net.

Speaker 1:          00:51          Uh, it scalable so it to flow. But one thing that it has a tensor flow does not have, is multilanguage support specifically for a scholar and for our, and then for a few other languages that we're going to look at. Uh, it's got a vibrant ecosystem both in academia and in the industry. We have some examples of companies that are using mx net right here in this picture right here. Baidu for example, a University of Alberta. And why you Intel, uh, and then for its features, I just kind of like screenshot of this from the mx net website, but it's flexible. I mean that's just kind of a generic term. Let's be real. It's portable. Okay. So it runs on CPU or Gpu? Yes. Sounds good so far. Uh, right. We talked about multiple languages, auto differentiation. What that means is that you can use this to perform gradient descent automatically with a single function call.

Speaker 1:          01:40          So it is tends to flow, so it does a lot of the other libraries, but it has this, it's distributed on the cloud. So it's actually very easy to distribute your computation. And we're going to talk about that more so than I would say even tensorflow. And then performance wise, it's pretty damn fast. I mean I've looked on the web and I've seen several uh, performance metrics like from several articles of comparing mx nets performance versus tensorflow. And when it comes to bigger, bigger datasets, mx net tends to outperform tensorflow. So, which is interesting, but you know, because tensorflow has so much activity on get hub, like more so than any other deep learning framework yet mx net performance, better at scale in terms of datasets. Uh, here's his other great comparison graph comparing the top, you know, seven frameworks to each other based on a, you know, a series of metrics, but then notice how yeah, mx net really excels when it comes to multiple GPU support.

Speaker 1:          02:32          Uh, that's really the, the, the main thing and we're going to go into that in code. If we look at this, a chart here of AI frameworks and then AI infrastructure, mx net was really built with AWS in mind. So if you plan on using AWS, mx net is probably the way to go just because it works so well with AWS. You've got these prebuilt ams that are machine images that you can just preload into an AWS instance, like an [inaudible] or a GPU instance. And then whatever you're running with mx net does tends to work more often, get less errors and if you're using other frameworks, so then right down to tensorflow versus mx net, forget the other frameworks. They're both multi Gpu Mxn is distributed, tens of is not. This is actually changed. So tensorflow is all distributed now. However mx net makes it easier to do.

Speaker 1:          03:18          So they're both mobile friendly. Okay. So now onto its really its core features that that I really liked. The first one is the fact that it has an imperative API, like compared to tensorflow, it's got this imperative APIs. You might be wondering what are you talking about here? So this is an example of imperative programming, right? Okay. So what we're doing here is imperative programming is a way to say, define by run. What that means is we're going to define the parts of this computation graph that we're building. Like these are, these are two matrices. We multiply them together and then we can add, add other operations to that. So Pi Torch is another example of an imperative framework, but then declarative programming is defined and run. So we define the variables first, then we run it. The problem is that this is a static computation graph.

Speaker 1:          04:07          It can't change over time, right? So you define your variables, your operations, and then you run the program. This means that if you have something like what say, let's say a recurrent network, a recurrent network works better with a declarative, uh, with, sorry, an imperative, a programming approach because it's defined during runtime. So any of the newer, deep learning models, they use these features that are real time. So you know, data is changing in real time. So that's why something like Pi Torch works better than tensorflow because intention flow, you define the graph, then you run it. Oh, with Pi torture, with Amex Net, you define the graph and then it can modify itself. It's not a, it's not an immutable graph. It's mutable. So it changes over time, which is good for a lot of the newer models. The big con of this is that it's hard to optimize.

Speaker 1:          04:52          You know, there were some early on decisions that the tensorflow team May to say this is going to be a declarative language framework. And the reason was because it was just easier to optimize. However, as the deep learning space progressed over time, we found that imperative programming was a better model for these real time changing models. These to cast tech models, like one example would be variational auto encoders. So then onto the mixed programming paradigm, right? This is an example of how Amex net actually does both. So you can go full imperative if you, if you'd like, or you could use this symbolic execution, that declarative execution. So you had that option. However, most people, if you look at mx net code on, on get hub, most of it is imperative. So that's it for the explanation. And let's just start coding this thing, right?

Speaker 1:          05:37          So, uh, I'm going to go over three different features that I'm going to go over the Ndra Api. I'm going to go over the module Api and then I'm going to go over the, uh, symbol Api. Okay. So these are the three really the main parts of the mx net framework. So we can install it with this very simple command. Pseudo, so that installs it. The simple line right here, make sure you have Sudo installs. So the NDRA API is a core data structure for mx net. It represents a multidimensional array, an end dimensional array. And this, this enables imperative computation. It executes code leisurely, allow me to automatically parallelize multiple operations across the available hardware. So this is good for neural networks because neural networks are operating on end dimensional data rights. So if you have a single feature, right, that's one dimension. If you had a second feature that's two dimensions.

Speaker 1:          06:26          If you had a third feature that's three dimensions and you can keep going, right? You know, you can have a car's tire length and its speed and its philosophy and its shape and all these things, you know, so it's a multidimensional data structure that we're inputting into a neural networks. You could think of all of these features as dimensions and you can just think of a single data points, right? A single car, for example, as a giant matrix or an end dimensional array. And so the way that, uh, mx had deals with this is, it calls it the end d array API. And so there's an image of a, a handwritten character. If you break it down, it's really just a two dimensional array of numbers. If you add color to it, then it's a three dimensional array and et cetera, et Cetera, et cetera. So let's start coding this so we could see, you know how this works. So what I'll do is I'll create this nd array and I'll say it's going to have an a, it's going to have a two dimensional, a data structure.

Speaker 1:          07:30          Okay. And so now I can say a dot size six. Okay. So it's got six different data points inside of that nd array. I can also say, Hey dot shape. So it's a two by three nd array object, right? So he's got two data points and inside there are three different data points inside of there. So it's two by three dimensions. Okay. So then we can say we can even customize the, um, we can customize what it holds. So we can say using non pies, a dependency which is made from h matrix math. We can say, uh, let's say we've got that same object, right? So let's just copy and paste that in here. It's that same object

Speaker 1:          08:20          and we're going to define the data type has numb pies into 32 data type. And then we'll say, well, what does that data type that we just defined? And it's going to say in 32. Now if we want to print it, we can say what is inside of this object? And then it prints it out. Uh, we can also perform a series of matrix operations on it very easily. If I just say, um, let me just copy that back over here. The equals, I'll call it a equals. Okay. I'll say B equals a times a. So there's, there's are a element wise, matrix multiplication and then be as num Pi. Tell us what's inside of be very easily.

Speaker 2:          09:10          And there we go.

Speaker 1:          09:14          So if we wanted to do something like a dot product, if we want to do a dot product, then we can say something like, uh, let's say we have the same object here and then we say B is going to be the transpose of a right now and let's do a dog product. So we're going to use the NDRA API APIs dot function using both of our matrices. And then we can say, well what did we just get using the as num Pi function?

Speaker 2:          09:44          Okay.

Speaker 1:          09:45          And it's saying uh, mx. Oh yeah, right.

Speaker 1:          09:53          Duh. There we go. So we can also define the, what we want it to run on. Like we could say we want it to run on the CPU, we want it to run on the GPU, et cetera. Right? So right now we're just saying CPU, but we can really have it run on any type of hardware, CPU, Gpu, and we can define that right here and I'll just run that. Just like that. Okay. So that's it for the Ndra Api. Now let's talk about the symbol Api. Okay. So this is a symbolic graph, right? If you've ever visualize a computation graph using tensor board or any of those tools, you'll see that it looks like a graph like this, right? Well, it's happening. Are these tensors are flowing through these computation, um, these operators, right? So it's saying, take this data point, take this data point, apply some matrix multiplication to it, and take that output and then, you know, keep performing all these operations to it. And this is essentially a neural network. It's a computation graph. So,

Speaker 2:          10:48          okay,

Speaker 1:          10:48          here's an example of, of, of what this looks like. So instead of doing it to the imperative way, we can do it the declarative way, which is very similar, similar to tensorflow, if we'd like to write. So it depends on your use case. If you're doing a static, um, if you're doing a perceptron, a feed forward neural network, you'd want to use a static, um, computation graph. If you're doing a recurrent network and LSTM network, then you'd probably want to use the imperative a style. So this here's a stall for both of them. So let's see what else we can do here. So if we say I'm a, let's just, let's just see what, what's contained inside of these variables. So we say run, we can see that these are all nd arrays. So we could see everything that's stored inside of them and he was a result of all of those operations. And there we go. We've got all of that there as well. But what type of a variable is, well, it's an Ndra have an nd array of an India Ray. Okay, three dimensions. So now, um, we'll say, well, what are the arguments to eat? Like what makes up e? So we use the list arguments function on that. So be consistent a, B, c, and d. And we can say, okay, so now let's, um, list the outputs.

Speaker 1:          12:12          So what it says is that he depends on variables a, B, c, and d, that the operation that computes he is a sum and that he is indeed a B plus CD. And so we can do much more than just plus or minus. We can do multiplication and division and a lot of different operations, but those are the ones that we're doing right here. So now if we, like if we, if we want to bind data to operations, we can do that using a binding. Okay. So let's say we've got this data, right? We've got four different data points. They're all of type in 32 and what we want to do is we want to bind each MD or nd array to its corresponding symbol. So to do that we're going to have to initialize an executer. So an executer,

Speaker 2:          12:55          okay,

Speaker 1:          12:57          is e. Dot. Bind? It was, we're going to bind together on the CPU, several different variables with a, we're going to bind the variable a to whatever's in aid, a data and then hold on. And then

Speaker 2:          13:11          B

Speaker 1:          13:13          to whatever's in B data. I also want to bind. See, and I also want to bind

Speaker 2:          13:25          d.

Speaker 1:          13:27          There we go. So that's our executer and we can say, well, what's in this executer? And he'll tell us what, you've got an executer object. Great. So now we can, we can let our input data flow through the graph. And the way we're going to do that, or it's going to say, well he data is going to, we're going to use the x, we're going to use a forward function of the executer. And that's going to afford all that data through that computation graph, using the executer as the medium to do that. And then we can say, well what do we got an e now and that it's got this result 14 that's the result of all of those computations that we performed.

Speaker 1:          14:06          Okay? So the last bit is the module Api. Now we can build our neural network. Okay, so to build our neural network, we're going to do this. Okay? So what we're gonna do is we're going to build a neural network that will predict a co a class or category, like 10 different ones from a sample of a thousand different datas data points. Okay. So these are imaginary data points and we can import these directly. So the first thing we're going to do is obviously in port our dependencies, and then we're going to say, oh, we want a thousand data points. We want 800 of them to be for training. We have, we want the rest to be for testing. Here's how many features are going to be 10 categories. And with a batch size of 10. So every training and a rash iteration, we're training our model on 10 different data points every time.

Speaker 1:          14:48          Okay. So now we can generate our dataset. So to generate our Dataset, or we're going to use the uniform function of, uh, the Ndra Api. And to do that unit form below equal zero Pi equals one shape equals sample count, feature account. And then we can print it out and see what we've just created. And Oh, not Emek am decks. And so that tells us, okay, you've created a data point. It's, you know, it's got, it's using this uniform distribution. So it's just generating all these points randomly according to some distribution. And then we have that and to generate our categories, we'll just iterate through that, uh, this random number generator and say, okay, these are going to be our categories for each of these integers. Then to split our dataset into training and testing, we'll split the data, right? We're going to use a crop function to crop the data in half or not, not in half.

Speaker 1:          15:51          80, 20. It's gonna be an 80, 20 split. Okay, so now we can build the network. So here's what it looks like. It's very similar to carrots or tensor flow. You're defining this neural network by first initializing it with this data variable. You're saying, well, we want the first layer to be fully connected, right? So a single layer corresponds to a single line. It's got 64 neurons in that layer. So every neuron in that layer is going to be connected to the next layer. We'll add this activation function relu or rectified linear unit, a very popular type of activation function, another fully connected layer, a soft max output, which is essentially an activation function. And then we output the result to this mod a variable. Now, uh, we can indeed inner iterate on the Dataset, 10 samples and 10 labeled at a time. And then we're going to call reset to restore the iterator to its original state.

Speaker 1:          16:41          But we have to initialize this iterator. Okay, so we need to build an iterator. That's, that's what we have to do so that we can train in batches. That's what iterators or for. So here's our iterator and it's going to say, well, train on the x data, the wide data and with the batch size being batch, which we defined up here, 10 and then we'll say, train the model using this bind function, here's the optimizer, go for it. And that's it. I already initialize my optimism miser before. So it's, it's taking a while, but basically this runs in like 10 seconds and then that's it. That's it for the mx and an API. I hope you found this video useful. Please subscribe for more programming videos. And for now, I've got to research some deep learning frameworks, so thanks for watching.