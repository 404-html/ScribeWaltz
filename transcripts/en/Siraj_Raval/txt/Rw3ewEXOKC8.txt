Speaker 1:          00:00          Hello world. It's Raj and what's the most efficient way to interact with the database? Anytime we want to create, read, update, or delete data in a database, we'll usually do so in the form of SQL queries. Cql stands for structured query language and it's the standard language use to communicate with relational databases. What about Haskell DB? Each query, it takes a certain amount of time to compute and ideally we can order our queries in the most computationally efficient way. There are several techniques to do this and usually reinforcement learning isn't considered one of them, but a recent paper showed that using a deep queue network, researchers were able to perform queries twice as fast as using standard techniques. We'll learn how they did it in this video, but first we'll need to understand why they used reinforcement learning as opposed to other more common techniques.

Speaker 1:          01:06          We know that reinforcement learning works great for video games. Games are the perfect test bed for implementing RL algorithms since they're constrained environments where time is an element that allows for rapid experimentation. The Mark Haub decision process is the mathematical framework of choice for framing this decision. Problem that our AI or agent is facing. It consists of a few variables that define our environment, our agent, and how our agent interacts with this environment. In reinforcement learning, an agent tries to come up with the best action to take given a state in the video game Pac man, the states would be the two D game world we're in. That includes the surrounding items like enemies, walls and packed dots. The action would be moving through this two d space, which would be going either up, down, left or right. So given the state of our game world, our agent will need to pick the best action to take in order to maximize rewards.

Speaker 1:          02:15          We know that eating packed dots gives us positive rewards and getting eaten by a ghost gives us a negative reward and the possible temper tantrum which we want to avoid through trial and error. Our agent will accumulate knowledge of the environment through state action pairs, meaning it can tell if there would be a positive or negative reward given a state action pair. We can represent this using the Q function where Q stands for quality. As in it assesses the quality of a given state action pair. We can actually learn what the optimal Q value will be at any given point, and this is called Q learning, uh, using what's called the bellman equation. We can write out an equation that relates the value of one state to the value of another state. In our environment, because we are able to relate states across time to each other mathematically using the bellman equation, we can use any number of methods to iteratively approximate or Q function, but in the case of our PAC man environment, the state action space can get really big.

Speaker 1:          03:28          In fact, at some point it will no longer be feasible to store all the state action payers. Of course, we could still perform cue learning, but it'll get harder to approximate the Q function over time. Luckily for us, there exists a universal function approximator called a neural network. If we give it enough input data, it can learn any function. If we use a neural network as an agent that predicts the Q value based on the input state action pair, then we have a much more tractable solution than storing every possible value like we did previously. And to capture all the intricate details of this knowledge present in our queue table will likely need to add a few hidden layers to our network, making it a deep neural network. The extra hidden layers allow the network to internally come up with features that can help it learn complex functions.

Speaker 1:          04:29          That would have been impossible. Using a more shallow network. We can call this whole process deep reinforcement learning and more specifically deep cue learning. Now let's bring this theory back to reality. You and I have neural networks in our head. Networks of neurons are firing and endlessly different combinations to approximate functions that help us perform a wide variety of tasks as we go about our lives and we can consider our reality, uh, Mark Haub decision process. As neural network agents given a state, we take actions to maximize reward for whatever task we are achieving using our function approximation capabilities to make predictions. In that way we can consider our reality deep reinforcement learning. Anytime we use a neural network to approximate some reinforcement learning function, be it a value function, a policy, even the model itself, we can call that deep reinforcement learning. So how does this apply to the problem of query optimization?

Speaker 1:          05:37          Well, we know that CQL statements are used to perform a wide variety of tasks related to the database. They can update data, retrieved data, merged data, delete data. Each sequel query has its own function. Assume we have a database consisting of three tables, employees, salaries and taxes. Let's say we want to calculate the total tax owed by all employees under manager one we can write out a SQL query that does, that will compute the tax owed by each employee by selecting their specific attributes and summing them all up. This query is going to perform a three relation join. We can use J to help denote the cost of accessing a base relation. The cost of each query is the percentage of the total batch cost. It's the time needed to execute a query. It's computed different ways, but almost always takes into account several computation factors such as input, output, CPU and communication.

Speaker 1:          06:42          We want to minimize this cost so that we perform our queries as fast as possible. Using dynamic programming, we can iteratively calculate the cost of optimally accessing the three base relations. After the first iteration, we can build off of this information that we previously computed and enumerate all two relations. When we compute the best cost to join two relations, we'll look up the relevant previously computed results and in the third iteration we'll proceed through the other two relations sets. Eventually finding the vinyl best costs for joining all three tables. Once complete, we'll see that this algorithm has a space and time complexity, exponential in the number of relations, which is why it's usually only used for queries between 10 and 20 relations. When we have more relations than that, we'll need to use a different query optimization technique. Instead of solving this join ordering problem using dynamic programming, what if we formulated this problem as a Mark Cobb decision process and solved it using reinforcement learning?

Speaker 1:          07:54          If we do that, the states can be considered the remaining relations to be joint. The actions would be the valid joints out of the remaining relations. The next states would be the old remaining relations set with two relations removed and the resultant joint added. Lastly, the reward would be the estimated cost of the new join because we defined these mark Covidien variables, we can define a Q function using the bellman equation to describe the longterm costs of each join and since we've defined a Q function, we can order joins in a greedy way. We'd start with the initial query graph, find the join with the lowest Q value than update the query graph and repeat the process. This cue learning algorithm has a computational complexity of n cubed, although that's high, that's still much lower than the exponential runtime complexity of dynamic programming. In reality though we don't have access to the optimal Q function, so we need to approximate it.

Speaker 1:          08:55          To do that, we can use a neural network which would make this deep cue learning to learn the Q function we need to observe past execution data. We can use it as training data for our neural network and since we're using a neural network to represent our Q function, we need to feed this state and actions into the network as fixed length feature vectors. This helps our network perform matrix multiplications gracefully. As it accepts this kind of format. We'll use one hot vectors to encode the set of all attributes present in the query graph and the participating attributes from both the left and right side of the join. We'll use a simple two layer fully connected network as our agent and train it using the standard gradient descent optimization algorithm. Once trained it will accept a SQL query in plain text, parse it into an abstract syntax tree form, feature the tree and use a neural network whenever a candidate join his score.

Speaker 1:          09:59          And because databases are real time used in production environments constantly being updated. And changed. We can periodically retune our network using the feedback from live execution across all costs models. Deep Q is competitive with the optimal solution. Without a priori knowledge of the index structure, we can safely say that learning based optimizers are more robust than hand designed algorithms because they can adapt to changes in data workload or cost models for the largest joints. Deep wins by up to 10,000 x compared to exhaustive in numeration. Three points to remember from this video. Deep reinforcement learning involves using a neural network to approximate reinforcement learning functions like the Q function. We can assess the quality or two of state action pairs. I computing a cute table and cue learning involves approximating the relationship between state action, pears and Q values in this table. Using neural networks, please subscribe for more programming videos. And for now, I've got to take a meeting, so thanks for watching.