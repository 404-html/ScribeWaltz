Speaker 1:          00:00:22       Live stream is starting soon. It's starting soon. It's about to start. It's about to go down. Livestream is about to go down. In fact, it's probably already going down. Oh there I am. Oh, we're all, it's Raj. Good to see you guys. And all squad is in the building. Let me shout out some names. Igor. It's patchy. Mick, Raj Sudan shoe, Brennan. Yo, we got the whole machine wearing squad in the house right now. Who is ready to learn some tensorflow. I'm so excited for him. Some tensor flow as to Gora Spencer. All right. All right. All right. All right. Okay, cool. Hi everybody. Hi. Hi. Okay, so let's get down to business as they say, Mama, let's get down to business and defeat the loss function. Okay. So anyway, let's start off with a five minute Q and a. So, so hit me up with your most intense machine or any questions or you just want to know, you know, anything. I'm just, we'll go for it. And then we're going to use tensor flow to classify housing prices. Okay. It's going to be dope. It's going to be the first time. Reason, tensor flow from scratch in a live session. It's gonna be awesome. Okay, so here we go. Hi. Hi. Hi. Hi. Hi. Let's begin.

Speaker 1:          00:01:49       I'm doing good. I'm doing good. I'm so down. I do have a sore throat, but like, you know, I'm a human. I know, but like, it's crazy, but it's, it's cool. It's cool. Um, what kind of method approach do you use to manage your time, Mick? Great question. I, so, um, what do I use to manage my time? I write down what I'm going to do for the whole week before the week starts. So every Sunday I'll write down, you know, the, the weekly goals I still have, I have a weekly to do, to do, I have a daily to do, I have a monthly to do, I have a yearly to do and then I have a five year to do. So I plan things out in intervals like that. And, and I made it, I made sure my goals are accomplishable goals so that I get that positive feedback so I can keep going.

Speaker 1:          00:02:34       It's Google released its own o s uh, chrome, chrome, ios. Yes or language. No. Are only python recommended books on computer science or cryptography. Um, your cryptography is so dope. I wish I could just deep dive into cryptography. That is like some ancient Egyptian pharaoh mummy. You know what I'm saying? Like you, you have to a good book on photography is my book decentralized applications, the best selling software engineering book on Amazon for 2016. Um, and a good book for machine learning is machine learning a probabilistic approach by uh, I forgot the name, but that's the name of it. What do you think of Star Wars? I Love Star Wars. I'm actually not like, I love the old ones. The new ones, like I was kind of disappointed by episode seven. So it's like whatever cause kind of conservative that offer. Disney's parks, we have to make our own stories.

Speaker 1:          00:03:21       Guys. We are a new generation. We're gonna make our own stories and they're going to be as good, if not better than star wars. And we're gonna have a machine learning. It's gonna be dope. And when we make stories, it's going to be technically are accurate. What if there are more than one local minima and gradient descent Algo? Great question. So that's so um, yeah, so sometimes we have more than so graded descent. It's like it's like a three d graph with the valley, right? We want to remember what I talked about last lecture. You want to drop a ball and football and see where it fits. But what if we have multiple values? Well then that's when I was a second order optimization function comes in. We have defined the local minimum that is closest to that is that it's uh,

Speaker 1:          00:04:03       going to best optimize our function. We're going to talk about that later, but the keyword to remember to say second order optimization function tensor calculus, that's exactly what we're going to do. So you are not at a low level but at a high level because uh, tensorflow is, I'm going to do a little bit of magic for us. Can you explain nats behind tensorflow? I will absolutely find the map to flow is better than torch. Exactly. Hey, sprout, are more layers always better in a neural network? A great question. Tomorrow? No, not always. Not always. Usually it is. But uh, so if you have a smaller Dataset, you don't really need that many layers. So remember with, with machine learning, there's always a trade off. There's always a trade off. And in engineering in general, there's, and in life there's a trade off as well. But, uh, we, we want to, when we, when we optimize for,

Speaker 2:          00:04:51       uh,

Speaker 1:          00:04:54       so when we increase the layers, we increased the computational complexity, but we also increased the accuracy. So there's a trade off. How should I prepare for Google brain residency program interview Kaggle, Kaggle challenges are great. And the tensorflow intro docs, uh, watch all my videos, but you probably already have a, if you do all that and you apply the code, you'll be good to go essentially. Yeah. That's the book name. Can you give new research idea that can implement intention flow? Yeah. So here's an idea I've had for a while. Somebody should go with this, take the synthetic radiant paper from the mind synthetic grading. So they kind of like stopped using backpropagation just for this paper and then take the idea of one shot when, so, so probabilistic programming. And then combine these two ideas together. So synthetic gradients and one shot learning, boom, there's your landmark paper for the year. I don't have time to do it. So someone just rolled with that. It's central, usable in Raspberry Pi. Yeah. Actually somebody made a wrapper for Arduino. So two more questions and then we're gonna get started.

Speaker 1:          00:05:47       How does genetic algorithm bit into machine on the genetic algorithms are good for optimizing hyper parameters for us. And when we talked about that for a second, so I've got some emails from people who are like Ah, on the hosel articles, what people like, what do you think about this? We're a Google are using machine learning. Google is using machine learning to optimize and she's running. So like what do we do guys? Machine learning is not going to take your jobs away. Okay. We will be. Um, so if even if there's a, even if there's a job, apocalypse machine learning engineers will be the last people to have jobs. Not that that's going to happen. We're going to create new jobs and the new data at climate and we're going to have basic income and things like that. Hopefully through cryptocurrency. Sec, check out my posts, social point.

Speaker 1:          00:06:29       And it's already possible. And also branch point. But um, don't worry about machine learning, replacing a, your job as the mission you want to engineer it to. If that actually happens that we solve intelligence and then there's no, the idea of jobs and money and scarcity all goes away anyway. Okay, so let's, let's, let's go ahead and start yet. Let's go ahead and get started if everybody started wanting it. So to start my course, which at this course. Okay, so what, okay, so that's the prominent today or we're gonna do, we're gonna use tensorflow to classified housing prices. Okay. So, um, that's what we're going to do and I'm going to use a Jupiter notebook for one. So let me go ahead and start screen sharing immediately and we'll get right on into this itch. There we go. Okay. So let me move this chat window over here so I can still see you guys. I don't need to see myself, but I just need to see you guys. And then I'm also have a timer so I can see myself. All right. New movie recording. All right. And then that goes away and I'm right here.

Speaker 3:          00:07:52       Okay.

Speaker 1:          00:07:55       Minimize myself.

Speaker 3:          00:07:57       Okay.

Speaker 1:          00:07:59       Yeah, the one time it's kind of blurred. Uh, I'm going to get a better one. Okay. So here's what we're going to do guys. Okay. So we have a housing datasets.

Speaker 1:          00:08:11       Uh, one shot learning was the name was the other thing. Okay. Okay. Someone do that. And just in general, one shot learning. So we need more one, her name, uh, Congress because no, not everybody has a huge datasets and fusion. Fusion power. Okay. So I'm going to explain things slowly this time. Okay. I'm going to, thanks Alexandra. Much low. Okay, so I'm going to explain things slowly this time. Okay. So I'm going to start off by talking about a neural network. Okay? So a lot. Okay. We see this image a lot. I'm a neural network, right? But the one thing I want to say is, it's the only circles we call him neurons. They're not actually objects, they're not classes. These are just representations of numbers. Let me zoom in a little bit. They just represent numbers, okay? Because when we input data into a neural network, into a neural network, in tensorflow, we clot a computation graph.

Speaker 1:          00:09:09       Um, and these are not, these are not actually like neuro objects. When we take, when we take an input, let's say like, you know, well we're going to do housing prices and we apply a, when we apply, uh, some matrix math to it, it's going to take that value and it's going to do so if it's a one value and we, we multiplied by a matrix of safety values, it's what you did to become a three or four values. Those values are what we represent as neurons. They're not actually objects. And we take those values and a place that had matrix operations to them and then boom, it's a next set of neurons. But he's neurons are not actually classes. They are values. Okay? So that, I just want to start off with that. So that's, that's, that's that. Now, let me show you guys the graph.

Speaker 1:          00:09:53       We show you that to date of first. So here's the Dataset. I mean, let me, let me zoom in on anonymous. Okay, so this is a set up housing prices. Okay. So the first one did the index, would you have been a number? Then next one is the area which is the size of the house. The next one is the back, the number of bathrooms, then the price and then this. And then the price per square. The price per square inch. Okay? So that's what that is. What we want to do is we want to classify this, but this isn't a class but we don't have labels right now. We're going to add labels, okay? We're going to add labels where it's going to be a good, where it's going to be either a good bite or a bad box. Cause what we want to do, and these with given one of these features, okay, well we're going to decide what feature for use.

Speaker 1:          00:10:38       But given one of these features we went to then classify the house as either a good or a bad bottle. Okay. So let me, let me shut down my screen for a second just so the bit rate goes up a little bit. Okay. I want to, I want it to be not that lobby. Okay. So, so that's what we're going to do. Okay. So that's our data. So let's go ahead and start doing this in a Jupiter notebook. Okay. So I'm going to do it in a Jupiter notebook. All right. So the first thing I want to do is talk a little bit about, let me just make sure that this works first. No, let's see. Yeah. Okay. Works Great. Okay, so tensorflow, why are we using tensorflow in the first place? So, uh, yeah, so this trip is going to be, let me, let me make the font bigger. Actually, the fun is,

Speaker 3:          00:11:24       let me see. Okay.

Speaker 1:          00:11:27       Where are you central flow? Because Google created it for, oh, this is going to be a supervised problem. Okay. Yeah, it's price per square foot. This is a supervised problem we're using tensorflow because we have intention flow. We are, uh, tentraflow was created by Google and it was built to scale. If you use it for many years, they released it. It was kind of like, you know, use coming down from Olympus and getting fired to the humans. Right. We know that tensorflow can scale. We know that it's reliable because it's used in production. By Google, I mean Google search uses tensorflow. Okay. So this is a reliable library, although high torch is pretty cool too. It just came out this month and it introduces the concept of dynamic computation graphs. Usually we have two interpreters, right? We have a python interpreter and then we have a, um,

Speaker 1:          00:12:13       a computation and interpreter. But what ty torch does is it has one interpreter for everything. Okay. So, um, and so Jupiter notebook is really as a, it's a great concept. Basically you can, it uses an a, it allows us to view our coat and had marked down for code and lets us add, um, a bunch of pretty things to our code. Yes. Prometheus's the rights, uh, answer that I was looking for. Okay. So let's go ahead and start by importing Marmot tenancies. Okay. So we went to classify a problem and also let me show you guys what are your own bed? Looks like. I didn't show you guys what is it going to look like? So, so this is what it's going to look like. Okay. So we're going to feed in to inputs and we're going to apply a set of weights and biases. Use the softmax function and then output, uh, the housing price as well as the, the label, which is good or bad. Okay. So, um, we'll talk about that in a second. Let's go ahead and first import import. Our dependencies are all right. So yes, I'm focused on the content. Thank you. Pull up. So here we go. Import. Candace has PD. Why Candice? This is our beautiful library to, uh, work with data as tables worth with at our first Lego. Okay, so next one is none Pi, none Pi. It's going to help us, uh, use numbers, use number of agencies. Both Penn is intentional needed, so use a number of agencies. Okay. The next thing is mapped up light because we're gonna, we're gonna show this graph. We're going to show this graph and in second, and

Speaker 1:          00:14:01       uh, what else? We want maps out, lives quite clots as PLT. I prefer tensorflow, hi, plot to terrorize. And finally you tend to flow as FTA.

Speaker 1:          00:14:18       Okay, that's it. Those are our four libraries. Then we can go ahead and compile it. Boom. So because there are no errors, we can keep going. Right? So that's a great thing about Jupiter slash I python notebooks is that they like you compile as you go so you can see, uh, you know, what is happening. Okay? So that's what we did there. Now we're going to load the data, not so that was, that was so step one is to load data, load data. Okay? So let's go ahead and locate it. How are we going to load data? But we are going to use the pandas, uh, uh, library to help us do that. So the first thing we're gonna do is we're reaching use. The campaigners reads, gets the function because we have a CSV file. Okay.

Speaker 1:          00:15:01       Um, so if we're going to read this in as a data frame object, so what does a data frame object data frame object. It's an object in memory that Candice creates from a CSV file that we can then easily parse. And I'm going to show you what I mean by easily park. So we're going to take this data frame variable that we just created and we're going to, uh, list what the, we're going to remove the columns we don't care about. So how do we know which columns we don't care about? Right? So let's go back for a data for a second. We have several columns here, which are the ones that we want to use to predict if a house is good or bad. Well, what we're going to do is we're gonna take out the index because that's not going to really give us anything. It's just a numbering of what each column gets. The next one is, uh, the,

Speaker 1:          00:15:50       uh, let's say area and bathrooms, right? So those are the two features that like, we're going to use area and bathroom for not gonna get the price for the square. We're not gonna use this price, this, the square price. Because when I looked for a house, I want to make sure that, uh, it has, I have a certain, and that's just what I'm using, right? Deciding which features to use is its own. It's, it's, it's its own discipline in and of itself. What teachers should be used. And a good rule of thumb to go by it is, is um, what features would you personally use to make a prediction? Okay. Um, yeah, let me, let me, um,

Speaker 1:          00:16:30       give you guys this data. How do I, um, got to stay to, oh, I can create it are just so kind of just, I'll say, let's just take off this, let me give you guys this data real quick and I'll paste it into the chat. So you guys haven't said so data that Tsp and we'll create a public. Just boom and I'll paste it in the chat. Ready? Get ready for this guys and the patient. Then boom, there's your data. That's the data file. All right, so now you guys can do that along with it.

Speaker 1:          00:16:59       All right. So we are going to remove the features we don't care about. Okay? We're going to remove the features we don't care about. We don't care about the Christ. We're not going to care about the square price. Speak only won't care about the, the back, the number of bathrooms. And what was the second thing? The area that's, that's what we're going to be using right now. Okay? So, and access is one because for we're only gonna use the first axis from the Dataset. All right? So, so data frame about drop. So that's who we removed too. Let me, let me say what we just said here. We've removed the, we removed Xe, uh, features we don't care about. All right? So we removed the teachers. We don't care about, now we're going to only use the first 10 rows in the Dataset and this example. So we only want to use the first 10 of these, right? Right up to here. Right up to here. Okay. No Oxford, I don't want to define that. Okay, so, so how do we say that? Well, easily enough. The great thing about a Beta for an object, if we can say, we can say I wouldn't

Speaker 1:          00:18:08       from row zero to write 10. Ooh. So that's going to tell us that we want to only use, we only use the first 10 rows. That's it. Okay. So that's it. So now we have our data frame object. So let's go ahead and print this. And here's the great thing about python notebooks. If we can now print this and see what we have and you'll give us an error if it doesn't work. Okay. So there we go. You've got it in text. So let's see what I did here. So on this line, data frame dot drop, uh, it's pointing to

Speaker 1:          00:18:39       his access equals one. And the problem here is invalid syntax. Uh, so we have index square prize, uh Oh yes. So it did. Okay. So the, this bracket actually goes here. That's where the profit goes. Now let's, let's compile. Okay, so there we go. There is our beautiful clean for medic data and we're only using the first 10 columns and we're just using, uh, the, uh, area and bathrooms as our features. And remember, feature selection is an entire discipline in and of itself. And we could have used the square price and we could have used the price as features, but what I'm looking for or the area in Bachelors, okay. That's what I look for when I try to predict if a house is a good buy or not for me. Okay.

Speaker 1:          00:19:28       Um, so, so that's, so now we have our features. So let's introduce our, our labels. So what we just did, so step one was to load the data and set two is going to be to introduce the labels because right now there are no labels. So we could just do a regression, right? Because we can then prevent the next value. And what do we use regression for for predicting the next value in a continuous set. Uh, so that's what we do for that, but we'll, we're going to do, if you were to convert this into a classification problem, so add labels cause that's our step two. So we're going to take that data frame object, and if everybody doesn't have the data, go ahead and ask and then someone's going to paste it into the chat. The link to that, uh, just, all right. So what we're gonna do is we're to introduced the labels.

Speaker 1:          00:20:13       So we want to say, uh, our labels are going to eat a good buy or a bad product. And so how do we represent that? Well, for simplicity sake, let's just use binary numbers, right? So a good buy is a one and a bad by the zero. So that's just randomly had like, you know, a set of purpose for our 10, for our 10 values. So we have to count 10, right? So we want one, one, one zero, and then, you know, add some variety in there. So let's see how many that is. That's three, six, nine. But we want, uh, we have one, one zero zero one and then zero and okay, so again, it's $10. So we're going to add on labels. Um, and so that's, so what is good and zero is back. The one is good and zero is bad. Luck.

Speaker 1:          00:21:03       Okay, so one is the advisor was that way. Now, um, now you can see the first part of the code. Let me, let me read paste the link. I'm going to replace the link from, okay. You can pick some. Okay. So now you can't see the first part of the code. Well that's, let me go that I just, I can't go back. We have to keep going. Okay, so one is the goodbye is there is a bad bottle. So we can define our set of labels. Now the next step is to, uh, turn true false values into ones and Zeros. So we're going to say the location variable is going to define those labels for us. All right? So, so let's go ahead and add the, uh, the white t values, which, which will, so why too. Um, and we're going to use a data frame object, whereas wine one. Okay, so what did we just do? So what we just did is we said why two is a negation of why one, right? So we defined why too, as a negotiation. And a [inaudible] is a dictation of why. One. It's the opposite. Opposite. Okay. Um,

Speaker 1:          00:22:22       right. So today's the actors. One is, is the, is the column, okay? So now white. So why to me we don't like a house. Why do you need, we don't like a house. Okay. So let's go ahead and define why two. Okay. Different unlock location. And we're saying, okay, so what is that value that we want? Um, why two and then we'll say, okay, so we're going to convert it to my needs. Value, right? We always want to remember that for tide we're converting to is an in. So how do we convert it to an evaluator? Uh, so we're going to take as type. And so what this says is if it turn true false values into ones and Zeros, so turn true false values to ones and Zeros. Um, okay. And now we've done that. And so now let's print out what we have here. Let's see what this looks like. Great thing about high python notebooks. We can just print what we have here. We definitely have a syntax error. So there it is. And I think soccer mentioned this earlier. So we are going to add two line three. So which one is that? Our uh, brackets? Yes. Just like that.

Speaker 1:          00:23:43       Okay. And so now you have an extra. Okay. Okay. So what did we do? So we added labels, but instead of saying good or bad, we had to label. So this is just for simplicity sake, we don't want to convert text to integers are just going to say it in a raw value. So, so why? So why one has a value that means that is a good buck and every time why one has a value every time. Why one is one y two will be zero and every time y two is one, why one would be zero. Okay? So that's just our way of, of labeling our data for now. Okay. So we've added our labels that was set to remember step one was to load the data, which we did into a data frame object. We parsed it. And then step two was to add the label to that data.

Speaker 1:          00:24:26       Right. Which makes this a classification problem is group. Would you classify this data? Okay? Where would you classify housing? Uh, if a house is a good buy are bad by based on the, on the area and the number of of backwards. Okay. Not Bedrooms. Right? Um, well I guess, I mean the data didn't have bedrooms did it? Okay. So, but yeah, bedrooms would probably be better if we have that. So now we have all our data in a data point. We have to shape it in major system, feed into intention clusters. Now step three is to prepare data for tensorflow. So depending on which machine learning, uh, library we're using, uh, we can, we will prepare it a certain way. But in general, in general, no matter what data we have, we're going to convert it into cancers. So 10, let me explain what tensions are.

Speaker 1:          00:25:19       So, um, so tensors are, it's a high level of, so cancers and so tenters are generic virgin of made of vectors and matrices. So, um, of vector is so like vector is a list of numbers. Then vector is who different up. This is important because we see these words. I love to take your list of numbers. Then a a matrix is a list of lists of numbers. Okay. So then, so a vector would behave, one d tensor matrix would be a tutee torture. And then whatever a risk of lists of lists of numbers is, would be a three d 10 Sir. And then that just continues. Okay. So attention is like a very generic term or both matrices, matrices and vectors. And these tensors are half or how we represent data in tensorflow, right? The guy's machine learning neural networks. It's all just matrix map.

Speaker 1:          00:26:26       It's a collection of operations that we apply to some input matrix. And we continually apply him like a chain of these operations until we get to an output. And then we, and then we apply an optimization function to, uh, to minimize a loss. And I'm going to talk about that in a, we're going to do that, but that's what this is. Okay. So, so that was a little short, like a thing on Pinterest for a second. But to get back to what we're doing, so we're going to take our data frame and we're going to say, uh, let's see. We want to take the area and the bathroom. So we're going to take our features, right? We're going to take our features and we're going to convert them into tensors. We're going to convert our Peters and detention. So convert features to 10 to include tensor. That's, that's what we're going to it in burn features in anything tensor. Um, and this session will be 15 minutes or probably an hour. We'll see, uh, as matrix and the we went to, we went to say, okay, so as a matrix, that's an advert. So now we're going to convert our labels, uh, to, uh, but to answer as well, right? So let me show you guys this, this image again. Okay.

Speaker 1:          00:27:50       All right. So, right, so here are our, just like right here, this, these two purple circles, those are impotence for as our features and our label. So we're going to feed that into a second. We haven't created our weights yet. We've just, so if you just look at everything, but these two purple lines that, sorry, these two purple circles and that's what we just created. Okay. Um, so now,

Speaker 1:          00:28:15       uh, so that's what we did for that. So now we're going to convert our labels in future surfing. Okay. So I'm sorry, our labels, if you tend to have a data frame dot low, sometimes they're already probably going to happen. So we'll say, okay, so location and then we'll, when were those labels that we created and you guys are gonna be easily have, um, one label. It's, well, I'm just, you know, is, um, okay, so I got to focus on this. Let's see, let's see. It just compiles. Okay, so syntax error, what do we have here? So, right, so the extra back brackets. All right, took area bathrooms. Um, we have location. Okay, so Disney supplementing and extra bracket here. And then what else do I have here? So for why one, I have a bracket or when you get another bracket and then we'll compile that. Let's see what else we got here. Oh, comma aren't Jupiter notebooks. So great guys, we can just continue. We can just, we can just compile as we go. We can just compile it as we go. A different dot. Location. Y One y two. What's the dealio? Okay, we're going to remove that.

Speaker 1:          00:29:35       We want to say what is going on here. We've got y one y two die location and this is a practice.

Speaker 2:          00:29:45       Oh,

Speaker 1:          00:29:49       let's see. Oh, okay. Let's see what's going on here. Whoa, that's a lot. None area bathrooms are in the columns. Hold on. A second. Bathroom in the cops. Uh, let's see what we did here. So he sat data frame dot location. We converted a area, bathrooms to as a matrix. Um, and then we said, okay, hasn't matrix. Interesting. Okay, so right. But so we did have very about terms. Um, remove the closing brackets. Um, remove the closing brackets on or move this one and we want to add interesting missing quotes. Oh, there it is. Right? Missing quotes. That's what it was. Oh my God. Okay. Yes. Alright, there we go. Okay, good. Sturdy for a second. Let's, let's print out what we had here. So what did we just create? Ooh, so what is this? This is what our [inaudible] matrix looks like. Okay. So we have the area in the backyard and this is what our labels matrix looks like it's pronounce aren't labeled Matrix.

Speaker 1:          00:31:12       Okay? So that's our implementers and events are lady Matrix guys. We have our inputs now we're going to prepare our printers. And so what step are we on? Now we're on step four. So step three was to prepare the data for tensor flow so we can format in our data for tensorflow flow. And now why don't you step four is to write out our hyper Kuranda. Okay. So what are our hyper parameters? So the first one is going to be our learning, right? And so when did we last? You've when you, right, we use the learning rate, uh, when we last life section. Do you sum that up? We, the learning rate controls the rate at which we learn, you know, so with a better recommendation, we're learning rate. You're find, um, uh, how fast we reached convergence to be technical convergence is when our model is that it's optimal fit.

Speaker 1:          00:32:00       When we have that optimal hit where the error is minimized and the learning rate defined how fast we get convergence, we, we had to say, you can just say, oh mate, the loading rate a million and just get there as fast as possible because if you go too high then your, your model is not going to converge. If you go too low, it's gonna be too slow. So remember, like all things, it is a trade off. That is a word of the day, particularly is a series of trade offs. And go as straight off to talk, maxing get the training. We're going to train you think 2000 times. And why don't you thousand? I don't know. I mean we could, we could try 10,000 in a second. I don't know what I'm saying. So, and that's what we, we, we train for a number of the box with your results. And then we say, well, is this, is this prediction accurate or not? And then if not, then we'll change. Or two of are hyper parameters. And again, okay, so then the number of displaced sex. Um, so how often do we want to display? Uh,

Speaker 1:          00:32:59       so then how often do we want to display, uh, the process of training and the number of samples, which is going to be the size of the number of labor, which is 10. Okay, so that was our hydropower. Those who are hyper parameters and now guys we weren't ready to compile and make sure that we're on is to create our computation brand. So a bit uncomfy vision, create our computation graph slash neural network. Okay, so convocation grab neural network that the same thing. But before we get to that, let we do one quick review of what we've done so far. We imported our dependencies. Then we loaded our data, which is a CSV files a dick. The data can take a bunch of features, but we only wanted to use the area and the bathrooms as our features. We removed everything else using those two things.

Speaker 1:          00:33:45       We went to predict if a housing price is good or not. But guess what? There are no labels in our data. So what did we do? We added labels because data frame objects are really easy to Parse, add data from, remove data from. And so we added labels. Why? One is when, why one has a value one, it means it's a good vibe. When y two has a good value, that is uh, that is, uh, it's not a good bottle. Then we prepared our data for tensor flow. How do we do that? We looked at our data for an object and we converted our features into an input tenser and then we convert our labels between a potential as well. We printed them out, we defined our hyper parameters. And now we're going to do, we're going to write out our computation graph. Okay? So,

Speaker 3:          00:34:33       okay,

Speaker 1:          00:34:33       let's go ahead and write out in our computation graph. So let's go ahead and do that. So the person, I'm going to do these crazy, I'm going to create a placeholder for our feature inkling tensor that we just defined. He'd put bets. And what this is doing is so

Speaker 3:          00:34:49       okay

Speaker 1:          00:34:49       or [inaudible] for our feature input teacher input pencil.

Speaker 1:          00:34:57       So in flow we'll feed it will feed an array of examples. Each example will be an array of to float values area, a number of bathrooms. And so none means any number of examples, you know, could the fund, which is usually the backsides, but we can say we're just going to say none. So it's just generic. We can say however many we want, I will allocated. That will help. We allocated a number of beforehand. And this allows us to have a number of examples and are two because that's the, the size of disinfectants are. Okay. So I'm sorry, not 32 to that. My, my mistake. So two, because we have two features. Okay. Um, and, and it's a cutie matrix, right? Because we have two features. So we've done that. And so the next step is to now create our weights. So create another thing that placeholder up vet are gateways.

Speaker 1:          00:35:53       So play folders in flow. Our gateway for data. It's how we feed data in for a computation graph or may wait for data into our computation graph. All right, so the next step is you create wins. So how are we going to create our weights? So we're going to define our weights. Has W and we're going to say F. Dot. Gary. Carryable and so we can upload widget. So we'll define our weights and it's going to start off as a set of zero because we haven't, and that's how you should start off your words. Um, for a simple example like this, um, although with transfer learning, uh, you know, your weight aren't zero. They're there, they've been trained beforehand. So this is a two by two float matrix, two by two float flotations and, and we're going to keep updating them. We'll keep updating through the training process I thought we were going to do.

Speaker 1:          00:36:50       Uh, and so why don't we use a variable well, intenser flow of variable is, uh, when they variables hold an update parameter. So variables in TF, cold and upbeat parameters, and you'd have weights. We have weights or any, any other things you want to, so they're, they're basically in their feed memory buffers, campaigning. Tensors okay. Um, to be a little more technical about it, but these are our ways. Okay. So the next step is to add our biases. So, um, we're going to add our biases. So advisees. And so we want to biopsies, right? Because we have too many points. Um, but we want to buy two because we have two inputs and we're going to, this is also going to be ATF variable. Um, okay, so just say two by two matrix. There are in fact that out. And we're going to close the bracket here and add a closing parentheses.

Speaker 1:          00:37:55       Okay? So let's see what this actually what we're going to do. One. So let me show you that. But we have so far, so now what we've done is with printer input tensor, we have any actually uses little square thing and we've created our weights and now we're going to add our biases. So this is what we're about to do. So we're going to have these other purple square looking, uh, dots. So yes, we're gonna use that combination to update or within a second. But first we have to define our biases. And why don't you use biases? Biases are going to help, uh, our model fits better. So a good example is like, so an example is the in the y equals mx plus B equation like that. So because we take that line, it adds the y intercept would make, which makes it fit better.

Speaker 1:          00:38:44       So that's what biases do date. They are there, there are a part of the larger, uh, uh, goal. They, it's like, you know, if you didn't have, be in the labels and I suppose be the line wouldn't be to the red line that you're looking for, right? So, um, okay. Um, and I'll, I'll go over everything we've done at the end guys. So right now we've added our biases and so the next step is to, uh, calculate. We're going to perform some matrix back. So now we're going to multiply our rates by Howard inputs. So this is our first calculation that's happening here, right? Because weights are what we wait for, how we govern, how data flows in, in our copy, wait for how we govern, how data flows in our communications. Okay. So let's go ahead and perform this. So we'll say we'll define it as y values and we'll say, and um, we'll say yes. Wait, can we can initialize weights as random as well? Um, and depending on, you know, what you're doing, it can be different. But right now we're initializing kind of zeroes. And to perform this actual matrix multiplication step, the has this great built in Matrix multiplication function, um,

Speaker 1:          00:40:02       uh, that we can say, okay, so take mark he puts and then our weights and then our biases and what, what are we going to do here? We're going to calculate the prediction to mold and to do that, to do that, we're going to multiply to input matrix by the way matrix, and then had the biases. So let me write that up. Um, multiplied inputs by weights and cad and pad. Okay? So it's kind of like y equals mx plus B. It's similar kind of operations. Okay? So

Speaker 1:          00:40:33       we've done that and now, now we're going to do mark softmax assumption. Okay? When you start sigmoid softmax is another word for six sigma. So we've done this part, right? And now we're going to do this part, which is we're going to take those values and we're going to apply a soft Max to it. And what does soft Max do? Does anyone remember? Shattered on the comments and then explain it in a second. I gonna type it out. If someone can check that out, I will give you a chat about if you can tell me what the softmax function best. So we're going to say softmax I stopped next to this value that we created, right? Um, we're going to apply herself next to his value applied Max to value we've just created. Okay. So, and then what does, it's an activation function. So now an activation function and it does somethings. Ouch. Okay. So let me go ahead and hand. So I just, yes, it normalizes our value. So what soft Max does is, uh, it normalizes our value. And what do I mean by that? It takes our value and it converts it to a probability that we can then feed to our output. So it's our last step before we feed to our help. But, so let me make sure that I typed all this issue out perfectly. Okay. So online 14, TF variable, that Zeros I said to Barry Zeroes, oh, I need a bracket here. Okay. Zeros.

Speaker 1:          00:42:07       The laser focus on what if I what I've done here. So, um, let's see. Interesting. So there is a, an extra value here. Um, let's see what's going down here. Tea, half done variable. Tf Dot Zeros. Oh, extra dot. Okay. Uh, okay, so now the next step is to say how is good. You know what it is? It's all good. Okay. So I need to add a bracket there, I think, right? Yeah. So I'm heading one pair and I'm, once you add one to here and then we'll say compile. And so then for the wide I use, it's telling me what I've done wrong. So it's saying, hey listen buddy, listen, you need to apply your matrix. Multiply x, W B, TF. Dot Add. Um, okay, what else we got here? Why values

Speaker 1:          00:43:22       and let's see. Name acts is not defined. Tax is not defined. Do you mean said, oh, because I didn't define it. Yes. So x is that value. Okay, great. Grit. So I hit enter. So I need to go ahead and add one more thing for training. So, okay. So what do we just do? We, we added the code is going to go online. Yes, I'm going to post it in the comments when I'm done. We've got 15 minutes. Okay. So what we're going to do is when, and say you've got that soft Max value and then we can get four outputs. So let me type that out. So what, what are we doing here? We're going to feed, oh, so we never did this part. So we're gonna say eat in a matrix of labels. So let me just go over what I've just done. Okay. So,

Speaker 1:          00:44:16       uh, so these are our two placeholders, right? So x is up here and then why is down here, but this, right? So one is our labels and then one, one is our set of features. We created our weights, we and our biases. And then we, this is our step right here. We multiplied our weights by our, by our inputs and we ended our biases. And we, and then we use that value that we calculated and we input it into the softmax layer, which converted into a set of probabilities. Okay, so, so there's that. And now when did you apply? And we're going to do to perform a, our training staff before the trainings. So we've done these steps and now step six is to perform training. Step six is to perform credit. All right? So, uh, let's go ahead. And so this is, this is the magic part of, to folks.

Speaker 1:          00:45:09       So if you were hearing the live session last week, then we did this manual. So totally to write out what we're about to see. What we're gonna do is we're going to create our cost function and the cost function. We're going to using the mean squared error. All right? So we did this last live session. One of the great things about a quarter is that we have continuity. Uh, and so we can say, we can say minus y this is going to be the equation. And I'm going to show the equation in a second. But what this does is it says we want to calculate the error. So between our two wide values and uh, we're going to divide by the number of, so the mean square error is where we, you take the average of the, uh, difference in values, which is our, Eric's the square difference.

Speaker 1:          00:46:03       And that's what this is doing. So, um, so reduce some, basically computes the element of the, some of elements across the dimensions of the or. So let me, let me write that down. We'll use some confused, confused elements, some elements across the dimensions of a cancer. And so then let me show the equation for the 10 seconds what we just did. So the two different values we're using are the, uh, the two different guys are using is, are the predicted the predicted values and then the actual values for the outputs. That is our cost function. We want to minimize this cost function over time. And uh, to do that we're going to perform gradient descent. So we're not, so the great thing about tends to flow if we can just refine in the sent had our optimization function. We don't have to actually write out every step. And you radiate the scent. Every step means where computing the partial derivative with respect to our input variables. What do you know? Our case would be the uh, which in our case we made a set of weights and the biases. So that's it. I could just say we have a cost function. We want to minimize that cost using gradient descent and are running where it will define how fast we want to do that. Okay,

Speaker 1:          00:47:36       so do not call me during the live session. Okay. So let's see what we've got here. We're going to say

Speaker 1:          00:47:47       why is here right and a right. So this should be why underscoring. So that's why that didn't work. And so the next thing is we want to make sure that name, why is not defined. How about now? No, it is different because I just updated it. There we go. Great. So yes, so I updated the wine and now we're going to create our session. Um, okay, so let's initialize our session. Is this still a part of the training stuff? We're going to initialize our variables and tend to float session. So intense or flow, we encapsulates our computation graph.

Speaker 1:          00:48:26       We should, we encapsulate our computation graph using and utilize all variables using a session object. So intense flow whenever you want to do any kind of training because the first initialized all variables and what additional is all variables us, they just, it does exactly what it says, initialize every variable that you declared beforehand. That means those TFR variables, we declared it beforehand and the placeholder objects because we did beforehand, right? So every enter flow generic variable that we defined beforehand, it's conditional as it for the session. So I mean, this could be, you know, this could be done under the hood as well, but for now we're going to, we're going to, the textbook asks us to define a deal. So we print our ascension, I work session, and then we just, we run it by using the, all the variables that we've initialized as our inputs. Okay. And of course there's an, they are so no attribute, initial, all variable. Uh, so what, what was that? So initialize all variables with, hey, uh,

Speaker 1:          00:49:30       a set of parentheses and then he f. Dot. Session. Let's see what else we got here. Tf Dot initialize. I misspelled initialize. Okay. Initial lines. Great. Okay. And so here we go. Let's, let's do that. This is the last bit, uh, that we're going to do. This is the actual trading. This is a training room. Okay. So let's go ahead and do this for every night for every, uh, p that we have for training detox. How many people do we have in this life? Section? 300 people go. Awesome. Okay, so for, so for every training evolve,

Speaker 1:          00:50:11       um, if we define as 300, but what was it? What was the number 2000? We defined as 2000. Well, to run our session, given our optimizer, which is gradient that standard, so this is where we're performing rating descent and we're going to feed in to where we actually feed into the placeholders that we get fine earlier. So those placeholders are going to be that the first is going to be that x value, which is going to be our teachers input x. And the next one is going to be for the labels, would just want it to be art, eclipse. Why? Uh, okay. So,

Speaker 3:          00:50:46       okay,

Speaker 1:          00:50:47       how does that and so that's it. And so now that's it really. And now we can just, you know, write out our debugging methods versus going to be right out of training. So if I display stack and then we'll say, let's run this session, our cost function. And then we went to, um, and we are too hard just to get this stuff. We're doing this just to print it out just so we can see, um, you know, what, what's happening at each step. Okay. So our input is going to be that first, um, tenser and then other than that, the other temps or that we,

Speaker 3:          00:51:36       yeah,

Speaker 1:          00:51:37       that we defined, yeah. Frames that it's going to be,

Speaker 3:          00:51:49       yeah.

Speaker 1:          00:51:50       No. So this part I'm just gonna is this part right here because we don't actually, it's just, it's just long. So that's the best way really. So what we're doing, if we're printing out the training staff, and let me, let me just go ahead and run this now. Okay. So our first impacts there. So it's going to be x. It doesn't mean that

Speaker 3:          00:52:20       I do

Speaker 1:          00:52:22       that. We got here, uh, ex and put x, Y

Speaker 3:          00:52:28       hmm.

Speaker 1:          00:52:32       [inaudible] walls.

Speaker 3:          00:52:34       [inaudible]

Speaker 1:          00:52:35       session dot. Brian.

Speaker 3:          00:52:37       Yeah.

Speaker 1:          00:52:38       Equals Yo. Okay. Yes. Okay. Okay. Awesome. So boom. So that's how fast the train because, because it was only 10 values. Okay. It was only $10. And so what is happening here? Let's look at this. The first column gets, aren't set up training steps, right? The next one is our cost function and the cost of entry is minimized over time. Okay. And, and then eventually it's finished training. So what is the w so it ends up with a cost function of 0.109, whatever. Right? So this, is this good or bad? I don't really know, but it's better than the first cost value, that's for sure. We don't know if it's a good buck cost cheat or not because we kind of arbitrarily decide, uh, once there's labels were Harkes awesome. Let's go ahead and test this out and how are we going to test it?

Speaker 1:          00:53:33       I have a call slash work, Bro. And I'm also like really sick, which is like I don't have time for that guide and I'm kicking this dope ass chorus. Okay. So we're going to feed him arc input and we're going to test it out. We're going to see what happens now. So this is our output. So what is it here? So this is the prediction, right? So we've, we've trained our model and now we're going to do our prediction, right? And so it's guessing, but they're all good houses because these values are all above. Um, so rimmer remember I said we have y one and y zero. So it's saying, so, so, um, let me, let me go, let me go back up for a second so you guys can see what I'm talking about here. So we defined, remember this y one and y two.

Speaker 1:          00:54:17       That's what it's outputting guys. That's what it's out. And it's just a wide one in the white too. And it's saying, um, these values rounded in our closest to one right on the left side. So that means it's all one. And the valleys on the right are all closest to zero, which means zero. So what it's saying, it's, it's, it's saying all houses are a good buck, but that's not the case. So exact seven had a 10. Correct. But the actual thing is it wasn't, they weren't all good. There were just, some of them are good, right? So only three, only some of them are good. So seven out of 10 correct. Which is not bad. How to improve. Um, maybe we could add a hidden layer, add a hidden layer. Had I been there, that's what I would do. I would add a hidden layer and then I would try it again. So that's a kind of, so that's my example for this. Okay. So let's go back to screen sharing. Okay. Subsequent turn. Go back to me. Hi. Okay. So, uh, that's what we did for that and I'm going to, uh, I'm going to add the coaching to get [inaudible] five minutes ended Q and a and then we're good to go. And I haven't, I have a video for you guys to be out on Friday, which I'm super excited about. Hit me up with your questions. Let's go. Anything.

Speaker 1:          00:55:34       Why do you think Jupiter, any special perks? Um, Jupiter because, uh, it allows you to compile it and see what you're doing in real time. It's great for visually looking at, especially for data, it's great for visually looking at what you're doing, exactly what Jake said. How do you choose a number of hidden layers or not of unites while trying to develop a neural network? It depends on how big your data is. The more data you have, generally the more hidden layers, the more data you have, the better it is to add more hidden layers.

Speaker 1:          00:56:09       Um, right. Pi Torch versus tensorflow for newcomer. Absolutely. Tentraflow don't even don't even trip well, but you know, pie for texting, great architectural ideas that I think we can, you know, we'll see where that goes. It's a really exciting space, guys. It's, it's only January and we're already seeing so much innovation in this field. In fact, this pace of innovation is accelerating. It's insane. Um, how many layers should we add for this case? Uh, I just need to act. Um, can you say something about Google's deep mind? Was she running? Um, well, I mean they haven't really done anything this year yet, but just wait, give them like two weeks. I expect to see some state of Vr in something. Yeah. Paper in like within two weeks. How to classify an image data set. We're going to do that in two episodes before this course up been high level. You want to label Dataset a supervisor approach. Um, uh, with labels a look at Google's in section. You don't even have to training herself. They were already treated when a million images and you can use a transfer wanting to apply that and then add in whatever other images you want to classify it. And because of transfer learning and you don't need it back, bigger data set to train it. And I have a video on that called guilty tends to image classifier in five minutes. How much coffee did you have? I had a cop.

Speaker 1:          00:57:28       Is it the same way session? The one we're doing with your Udacity? Yes. Um, anybody who has a recurrent neural nets plans. Yes. It's coming out in three days. Uh, do you think that you guys, the deep learning course apparently for deep on a job? Yes. Uh, what image? If you do the assignments, okay, that does it. Kavya and you have to duty assignments. You can't just watch the videos, you to do it, do it like strive up do is I'm a huge shower but then by the way guys, recently, um, cause he doesn't work. Let's take war. Did you age? I'm 25. Did you study at university? Yes. What computer science, specifically robotics. Um, anything on memory networks? Yo, I want to do memory networks. That's some dance shit. That's like generative adversarial networks. Should that I find super interesting at the cutting edge, the bleeding edge of the field. But we've got to get our basics. Now. If you should take something out of this slide session, what you should take out is, are um, three things. One how to do gradient descent and backpropagation to um, uh, basic, uh, an idea of what the, uh, the main variables are with tensorflow. That means placeholders, TF variables, the optimizer function, uh, weights and biases. Okay. Because everything's going to build off of that can get those simple things, then everything else is going to be much, much busier. And the third thing is,

Speaker 1:          00:58:56       um, a softmax function. Uh, sorry, activation functions. Activation function is what we use to convert numbers of probabilities. There's several types of activation functions. In this case we use softmax, which university? Columbia University. But guys, I want to end this with one piece of advice and then we're good to go.

Speaker 1:          00:59:17       A lot of what we learn is what we tell we are capable of doing. That's what degrees are. Degrees tell us, it allow us to tell ourselves, hey, I now know this. I can do it, but we have to flip that way of thinking. It's not about, oh, I have a degree now I'm a computer scientist. Oh, I have a degree now I can do x. No, if you study something and you do it, if you, if you create the code and it compiles and it runs and it gets you help with that, you want it, you now have the ability to do that. So don't be restricted by the Dogma of, Oh, I need to read this or eat this before it gets to this. Think of something you want to build. Start building and along the way or what you need to, to know that by the end you are going to get really good if you've ever engineered anything reported and you know that if anybody asks you a problem about it, you will know every answer.

Speaker 1:          01:00:08       Why? Because you had to deep dive into it to, to be able to build that. Okay? So that's it. Um, all right. So believe in yourself specifically. Believe in yourself. The world needs you guys. Okay? We need you. We are starting a revolution. Okay? We are in the midst of a revolution and like anything the world has ever seen before and we're going to, we're going to do some awesome shit. Okay? We're going to do some awesome shit. Okay. That's it. Okay. So thanks guys for watching the videos coming out on Friday. It's going to be dope and uh, continue the conversation in our slack channel for now. I'm going to, I'm going to, yes. I'm going to do the math tutorial in a second. Um, okay. So anyways, for now I've got to,

Speaker 3:          01:00:55       okay.

Speaker 1:          01:00:55       Work with you, Udacity on, uh, something I want to go down to if you'd ask me and just hang with them and see what we can do. There's some really cool people for now. That's what I got to do. So thanks for watching. See you guys.