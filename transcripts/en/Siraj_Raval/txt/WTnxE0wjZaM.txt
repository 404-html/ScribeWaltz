Speaker 1:          00:00          Hello world, it's Saroj. And what is the future of deep learning? That's the topic for today, inspired by something that Geoffrey Hinton recently said. This talk is going to be divided into three parts. I'm going to talk about how backpropagation works, what the most popular deep learning algorithms are right now. And finally seven research directions that I have personally handpicked. Okay, that's the talk in three parts. So let's get started here. So this whole video was inspired by what Geoffrey Hinton recently said in an article. So Geoffrey Hinton is the godfather of neural networks. He's the guy who invented the backpropagation algorithm back in [inaudible] 86 which is the work horse of all, almost almost all deep learning. Okay? So without back propagation, all the great things we're seeing in deep learning would not be possible today. Self driving cars, image classification, language translation, almost all of it is because of backpropagation.

Speaker 1:          00:57          So this what Hinton recently said is causing shockwaves in the deep learning community. He said that he was deeply suspicious of backpropagation his own algorithm and said, my view is to throw it all away and start again. And I have to say that I agree with Hinton. I know it's crazy, right? Because backpropagation has just given us so much. But if we really want to get to artificial general intelligence, we've got to do something that's more complicated or just something else entirely because it's not just about stacking layers and then back propagating some Eric gradient recursively that's not going to get us to consciousness. That's not going to get us to systems that are able to learn a huge variety of tasks. Everything from playing games to piloting an aircraft to figuring out the most complex equations in the universe. It's gotta be about more than just gradient based optimization.

Speaker 1:          01:53          So, uh, let's first start. Let's first talk about how backpropagation works. Okay, so the billion dollar question is that it's probably multibillion dollar actually. How does the brain learn so well from sparse, unlabeled data? That's how we learn. We don't sit there, we don't have labels for everything we're learning. If you look at a baby, it's incredible how much it learns without some kind of supervision, right? It can learn how to do all these different tasks, stack blocks in all these little to learn how to speak. And there are no labels per se in the sense that we use them in deep learning. It's all happening unsupervised. And when up and sparse means few, right? So it's not about very dense descriptive data. It's sparse, right? There's a lot of zeroes in it and yet we can still learn from it. So how does it do this? Well, let's first understand how backpropagation works.

Speaker 1:          02:44          So in backpropagation, so first of all, a neural network is a huge composite function. It's a function consisting of other functions. And these functions are all in the form health layers, right? So you've got to input layer, a hidden layer, maybe multiple hidden layers. And then finally an output layer. And you can look at it as this four step process that I've got right here. So the first step is to receive a new observation acts and a target label. Why? So this could be some image of a cancer cell and then the label cancer, right? And it could be either cancer or not cancer. And so you'll take that input and you already have the label, right? That's, that's how backpropagation works. As long as you know that label, you are golden, but you've got to know the label. So you take that input, it's an image.

Speaker 1:          03:32          Think of it as a series of pixels. So it's just a huge group of numbers. So it's a vector, right? So you take that group of numbers and you then you go to step two, you feed it forward through the layers. What do, what do I mean by feeding it forward? You, you continually take that input, multiply it by some weight value, add of bias value, and then activate it by applying a nonlinearity to it. And you continually do that over and over again until you have an output prediction. And I'm going to go over this more in a second. We're going to look at the code, but once you have that output prediction, you compare it to your actual prediction. Real label by doing a difference, the subtraction, right? You, you, you're subtracting the actual from the, from the predicted, right? And that that difference cause this, these are all just numerical values.

Speaker 1:          04:20          That difference is your error. And then you go to the last part, step four, back propagating that error. So once you have that error, you're going to compute the partial derivative of that error with respect to each weight recursively for every layer. So you'll compete the partial derivative with respect to the layer before. And then you take that error gradient that you, that you just calculate it and use it to compute the partial derivative with respect to the next layer. And you'll keep doing that every time. And then what happens is you're going to have a set of gradient values that you can then use to update all the weight's in your network for as many as there are. So the process is input, feed forward, get the error back, propagate uptick, the weights, repeat feedforward, get the error back, propagate, repeat over and over and over and over again a hundred thousand million times.

Speaker 1:          05:09          Right? And that is the backpropagation algorithm. We're going to go into it in more detail, but that's at a high level. Okay. So the paper that I'm talking about where hints and released this, I've got it linked to right here, but it's a very old paper. It's, it was, it was done in 86 and the reason it was created an 86 and the reason that Hinton is such a gene is because everybody was telling him this is not going to work. You've got to think of something else. But Hinton held strong to this belief, okay? And I think that is the mark of a good researcher. If you really believe in something to not let anything else influence what you believe in, right? Stick to your belief. If you're wrong, you're wrong. But at least you stuck to what you believed in and you, and you listen to other opinions as well.

Speaker 1:          05:51          But you really, you really believe in something. So the reason that it works now, and it didn't work in the 80s is because we have the computing power and the data necessary to have these huge, amazing, uh, classifications and these amazing generations, right? For classification and generative models for both. So let's just look at this canonical example of a very basic neural network that uses backpropagation scratch out. The input number four is just three inputs, right? So we have, it's a three layer network and put hidden and output. I'm going to go through this kind of fast because we were going to get to what really matters in a second here. And if you've already, if you already know how backpropagation works, just skip forward probably, I'm going to estimate five minutes from now. So we have this very simple basic neural network and the goal is to take some input data and then predict the output label, right?

Speaker 1:          06:40          So we've got some input data, which is a series of triplets, zero zero one zero one one you know, et cetera. And then we have a set of associated labels, zero one one zero. So four zero zero one the associated output label, why is zero and then et Cetera, et Cetera, right? So those are our inputs and our outputs. We have this nonlinearity which is a sigmoid function. It's an activation function. And, uh, I'll talk about that in a second, but we'll take our input data. We'll take our output data and we want to learn the mapping function rights. And then given some new output, zero one one or one zero one some arbitrary triplet, we'll be able to correctly predict the output label as it should be, right? So the first step is for us to initialize our weight values. So our weight values are a set, are both matrices, the are, they are randomly initialized matrices.

Speaker 1:          07:29          And so what happens is when we have our input, right? So again, remember just scratch out the fourth one. It's cause there's only three. It's a triplet. We'll take that input triplet, multiply it by the weights, and those are the matrices. And so that's why you see these arrows, right? The reason we're using, uh, they say we need linear Algebra in deep learning is because the linear Algebra takes the standard Algebra operations like multiplication and division and addition. And it applies it to groups of numbers that are called matrices. So Linear Algebra defines operations that we can apply to groups of numbers, matrices, for example, the dot product, which is used heavily in deep learning. That's in fact that is the multiplication we use in all types of deep neural networks. It's a way of multiplying groups of numbers together, which is what we're doing, right?

Speaker 1:          08:16          We're taking our input and multiplying by a weight matrix and then we take that result and we add some bias value in a bias acts as our anchor. It's a way for us to have some kind of baseline where we don't go below that in terms of a graph, think of like y equals mx plus B. It's kind of like the y intercept for this function that we are trying to learn, right? And once we, we've multiplied our input times our weight value added a bias and then applied some activation function to it, which is our nonlinearity the sigmoid that's going to give us an output and we just take that output and do the same process for the next layer and the next layer and however many layers we have. Okay. So that's what we're going to do using the dot product. And then we're going to back propagate the error once we compute it.

Speaker 1:          09:00          So back propagation, you don't need to know all of calculus to understand backpropagation. You only need to know really three concepts from calculus, the derivative, the partial derivative, and the chain rule, which I'll go through in a second. So first I'll go through the derivative. So the derivative is the slope of the tangent line of a function at some point. And the uneasy way to compute the derivative for some function like say y equals x squared or any function, is to compute the power rule, which I have right here. So you'll take the exponent and you'll subtract one from it and you'll take the original exponent and move it to the coefficient. So for y equals x squared, you take the to move it to the coefficient, subtract one. So then it becomes two x to the first, which is two x. So the derivative of y equals x squared is two x.

Speaker 1:          09:47          So the reason, and so the derivative tells us the rate of change. It tells us how fast some function is changing and what what is happening for, for gradient based optimization in neural networks and all of deep learning. Most of deep learning is we have some if, if, if we were to map out all of the possible error values on the x axis. So just imagine these are all errors and then all of the possible weight values on the y axis, it would come out to be a parabola just like this. And what we want to do is we want to find that minimum error value. We want to find those wait values such that it's going to give us the minimum air value. And what that means is we want to find the minimum of that parabola. And the way we're going to find that minimum of the parabola is by computing the derivative, which tells us the rate of change of wherever we are and then we're going to use it to update our weights such that we are iteratively, incrementally, continuously moving closer and closer and closer and closer to that minimum point.

Speaker 1:          10:43          And once we have that minimum point, that is our optimal point where the error is smallest and the weights are at their most optimal values such that the errors in the me, the smallest, every time we make a prediction, right? That's why it's called gradient descent in general, right? So when we take this very, very popular optimization formula, gradient descent, which I just described, and we apply it to deep neural networks, we call it back propagation, right? Because we are back propagating an error gradient across every layer that we have. And so the reason we know we need to know the derivative is because we're, because we're going to what we're actually computing is the partial derivative derivative. Because a neural network doesn't just have one variable. It has several variables, right? For however, however complex your function is. So, uh, we want to compute the partial derivative of the error with respect to each weight, right?

Speaker 1:          11:35          So when I say with respect to, I'm talking about that way and none of the others. So we'll you can think of a partial derivative as saying, okay, well what is a partial derivative with respect to x for this equation? What that means is we are only computing the power rule for x and we are ignoring everything else. So y to the fourth is ignored and when we compute the derivative of x, it's going to be one, right? So then we are left with five. Why now if we're doing the partial derivative with respect to you, why, then we don't care about x. We only care about why we do the power rule for why to the forest and it's for y cubed plus five x because y the derivative of y is one. So then x remains. So that's what we're competing. We're competing the partial derivative and that's what's going to give us our air gradient.

Speaker 1:          12:20          The gradient tells us how, which direction to move on that parabola to get to that optimal, that minimum point gradient descent. And the last part is the chain rule, right? Because a neural network is a giant composite function, right? But what did I just describe? I describe taking an input value multiplying it. So input times weight at a bias activate, right? We've talked about this before. Input Times, wait at abayas activate. That is the formula that is happening. That is the function, right? That is happening at every layer. And these layers are nested. So every time you add a layer, you're adding a nested function inside of this giant composite function that is the, that is the neural network. So the chain rule tells us how to derive a composite function. What you would do for a composite function is derive the outside, keep the inside and multiply it by the derivative of the inside.

Speaker 1:          13:13          So that is a rule and that rule applies recursively for as many nested functions as you have. So that's the chain rule. And so now that we understand that we can do back propagation, that there's a, there's your calculus primer on doing backpropagation. So the rest of this very canonical example is saying for 60,000 iterations, let's feed forward that input data through each layer. And so what we do for each layer and say, okay, we've got k zero, that's our input multiplied by the first synapse matrix. Uh, by multiply, I'm talking about doc product. Thank you num Pi apply the activation function or nonlinearity to it. So the activation function and the reason we do that is because a neural network is a universal function. Approximator I am, I am telling you a lot right now. So just uh, don't worry if you don't understand everything, there's a lot more to come and then rewatch this video and I've got a million other videos on this stuff as well.

Speaker 1:          14:05          So I'm very excited right now. Where was I? Okay. So we were taking the input times the weight. We're at input time. And so in this case we don't have a bias, right? Because this is very basic, but usually we have a bias. So we're doing input times wait, activate, okay, bye. Times. I'm talking about doc product. So we say, okay, and then you repeat that again for the last layer. And then two is going to be our output prediction and then we compute the error. But finding the difference between our actual output and our predicted output. Then we perform backpropagation. We take that airway to gradient and we see in what direction is this target value by computing the activation of that output value and multiplying it by the error. And that's going to give us the greatest value that delta, the change, right?

Speaker 1:          14:51          And that Delta is what we're going to use to update our weights in a second. But we've computed the delta the gradient for this layer right though the the the hidden layer. Let's get it. Let's compute the gradient for the next layer. So recursively. So we'll use the K two delta ticket, see how much the k one value contributed to the k two error. And once we've got that k one error, we'll come, we'll do the same exact props process again to compute the k one gradient. So the first layer is radiant. And once we have both gradients, then we can up update both of those wait values using those grades. And we just do that over and over again. 60,000 iterations that is backpropagation. So I want it to go into a tangent, no pun intended, to talk about derivatives and, and, and great aunts and how backpropagation works.

Speaker 1:          15:37          But back propagation is the workhorse of deep learning. And this is a great chart. The neural networks do that shows many different types of neural networks. There are so many types of neural networks out there. It's not just one. There's a lot, right? And backpropagation is the optimization strategy of choice for almost all of them, right? Almost all of them use labeled data and then back. And then backpropagation has an optimization strategy to learn some mapping function, right? Everything is a function in life. Everything is a function, love as a function. Emotions are a function that the sound of the airplane above. And then relating that to how fast velocity and you know, all these different variables, it's all you can represent everything as a function. Math is everywhere. Math is all around us. Math is beautiful. It's beautiful. Seriously. Oh my God, it's awesome.

Speaker 1:          16:28          Anyway, um, everything has a function, right? So we're trying to learn the function and supervise. Learning. Using backpropagation is a way for us to do that. So how do artificial in biological neural networks compare? So this is a very, uh, basic view of how they compare the idea. It, it, it's such a rough, it's such a rough, the, the initial perceptron initial neural network, we're so roughly inspired by biological neural networks. It wasn't like they were saying, well, let's, let's implement a neurotransmitter and let's, let's implement, you know, dopamine and dendrites in all of their details. I mean, neurons are these very complex cells. It's very basic. All the, the only inspiration is saying you have some neuron. It's got a set of dendrites that received some input. It performs some kind of activation, some kind of activation on that neuron. What, what that means is it decides whether or not to propagate that, that signal onward or not using some function.

Speaker 1:          17:27          And if it decides to, then it sends it out. That's it. That's, that's the extent of, of of the inspiration between artificial and biological neural networks. Right? Because we have some input, we compute some activation function like Relu or sigmoid or you know there's, there's many of them out there and then we output the value, right? So the brain has 100 billion of these neurons, numerous stand rights and it uses parallel chaining. So each neuron is connected to 10,000 plus others. Compare this to computers, right? Computers don't have neurons in terms of hardware, they are made of silicon and they are serially changed, which means these transistors on or off switches are each connected to two or three others and they form logic gates. So W and there are great at storage and recall, even though they are not as parallelize as our brain, they are still better than at some things.

Speaker 1:          18:23          We got to admit that we are like, it's better at calculating numbers in, in memory, right? We can't compute a million times a million, but a computer can. However, what our brain is really good at that computers are not his creativity, right? We are able to take some idea that is completely unrelated to another idea and apply it and then it results in some amazing innovation or task. And we are great at connecting different concepts together. We are great at being able to learn many different things and apply our knowledge to many different tasks and that's what we should be trying to do with Ai. And so there are some really key differences between our brain and uh, artificial neural networks. First of all, the everything in the brain is recurrent. That means there is always some kind of feedback loop happening in any type of sensory or motor system, right?

Speaker 1:          19:14          Not all neural networks are recurrent. Um, there's a lot of lateral inhibition, which means that neurons are inhibiting other neurons in the same layer. We haven't seen a lot of that in deep learning. There is no such thing as a fully connected layer in the brain. Connectivity is usually sparse, although not random. Um, we usually, we have fully connected layers at the end of our networks like say for convolutional networks. Uh, but in the brain there are none, right? Everything is sparsely connected, but it's, it's smartly sparsely connected. Brains are born prewired to learn without supervision. So we talked about this a little bit, right? How babies can know things even though they don't, they aren't. There aren't given labels or any kind of supervision. And lastly, the brain is super low power at least compared to deep neural networks, right? The brain's power consumption is about 20 watts.

Speaker 1:          20:04          Compare that to arguably one of the most advanced ais today. Alphago, it used about 1200 CPU and 176 GPU is not to train but just to run. Just imagine how much, how many watts that takes. That's like an order of up an order of magnitude more power than our brain ticks, which is, which is annoyingly inefficient, right? So we can definitely, definitely, definitely improve on that. There's this great book by this Harvard Psychologist Steven Pinker, which I've, I've read and I would highly recommend it called how the mind works. And this book is from a neuroscience perspective, not a machine learning perspective, but we need more of that. We need more of that because there are certainly a lot of secrets here that we haven't figured out, but we're trying. So this is a great book to read and it's a, there's a great quote from that book that I'm going to read out to you, which I particularly like.

Speaker 1:          20:57          The quote is the brain is not a blank slate of neuronal layers waiting to be pieced together and wire it up. We are born with brains already structured for unsupervised learning in a dozen cognitive domains, some of which already work pretty well without any learning at all. Right? Evolution has primed us to be able to do certain things even though we don't have any real time learning happening, it's just wired into us, right? So there is something to be said about structure versus learning everything. Anyway. Okay, so we talked about that. So where are we today? Right? So that was the first part. And here's the second part and then we'll get to the third part. Research directions. So where are we today in UN supervised learning? We know where we are with supervised learning. That means when we have labels, but what if we don't have labels?

Speaker 1:          21:42          What we can divide machine learning into two types besides supervised and unsupervised classification and generation, right? These are two tasks and one metal way of looking at it as is as creativity and discovery when everything else is automated. For us, when all of the, uh, s you know, all of the brainless labor that we don't care about, when all of that is automated, what's going to be left for us humans is our two tasks, creativity and discovery, right? What can we create? What can we discover? And we're, and we, and we can frame though those things as classification, discovery and creativity generation. So for classification, what is something clustering, right? Clustering is perhaps the most popular technique when it comes to classification. And there are many ways to cluster data, right? If you don't have the labels, but you do have the data, maybe you can learn clusters for all of these labels such that there that you'll be able to know what groups each cluster are in.

Speaker 1:          22:42          So it's like learning without labels, right? There are several strategies to learn clusters from data. K-Means is perhaps the most popular dimensionality reduction techniques like t distributed to the castic neighbor embedding or ta Tss or principal component analysis. There's anomaly anomaly detection, but most of them still used some sort of supervised learning. And the ones that don't use backpropagation are not necessarily better. They're actually very simple algorithms. Like k means is just, you know, these four steps right here. It's very simple. It's just basic arithmetic and um, that's where we are right now. There's also auto encoding, right? Auto encoders are really popular for unsupervised learning. The idea is that if you are giving some input, tried to reconstruct that input as your outputs, you have an input, you learned some dense representation and you try to reconstruct it from there. And this is great for dimentionality reduction, uh, learning, some feature, some features, et Cetera for generation.

Speaker 1:          23:48          Perhaps the most popular right now is the generative adversarial network. So I met the Creator, Ian Goodfellow. We had a good conversation in San Francisco. We had, you know, he's a really smart guy. And really, I mean the idea was so basic, right? It was such a basic, very intuitive idea. Yet it is the reason behind a lot of hype and deep learning. Right now the idea is to have two networks. One tries to fool the other, right? You have a discriminator and you have a generator. And so what happens is you have some data set, let's say some images and you want to generate new images that look very similar, but they're new. So what you do is you take one network and it's a, it takes an input of one image. It applies some distribution function to it, right? Enlightened space. So what that means is like a golf Sian or something like that.

Speaker 1:          24:34          So it takes them Gosling distribution, multiply it by that image. And so the image is basically a group of numbers, right? The pixel values. And when you apply some distribution value to it, you change those numbers ever so slightly. So then if you look at it as a picture, it's the, it's, it's a slightly modified picture and that picture is then fake and it only does this sometimes. Sometimes it shows the real one, it shows a fake one and the discriminator is a classifier it, right? So you know what the real images and then you know what, but you don't know if the the fake image is fake or not. Right. The classifier doesn't know. So it's got it's so it, it tries to classify the fake image and if it gets it right or wrong, you could take its prediction, compute an error value between the real and the fake.

Speaker 1:          25:17          And then again back propagate an error gradient value, right? So you are still using backpropagation across it. So the whole thing is what's called end to end differentiable because we can differentiate every weight value in the, in this, in the system. So even though there are no explicit labels, we are still using backpropagation. It's self supervised. So it's like we are creating the labels. Another great example are variational auto encoders where we are embedding stochasticity inside of the model itself. That means inside of the layers we have a random variable. What what that means is the neural network is not deterministic or stochastic. You cannot predict what the output is going to be. That means that if you have some input, you feed it through these layers. One of them is a random variable. So it's a distribution that's applied to that input. What happens is the output is going to be some unpredictable new output that you didn't predict before, which is what you're trying to generate.

Speaker 1:          26:14          Right. And lastly, and these are the bleeding edge of unsupervised learning models by the way. And lastly is the differential neural computer. So I, I am going to go out on a limb and I'm going to say that the DNC is the most advanced algorithm currently that uses, uh, backpropagation out there. Maybe, maybe Alphago is better, but we haven't seen the source code for that. So I wouldn't know. But in terms of openly available a source code, the DNC is, is, is, is amazing. It's also highly complex. There are so many moving parts in a differentiable neural computer. And I have a video on this, just search DNC Suraj but there are so many moving parts here. You've got read and write heads. Um, but basically you are separating memory from the network itself, right? So you have memory and the idea analogy fit that they made was between DNA and the brain, right?

Speaker 1:          27:13          So you have DNA, this, this is encoded external memory. So you have an external memory store, and then you have your, your internal controller, right? And so the, the, the, the, the controller is pulling from the memory and there are read and write heads between the controller and the memory. Uh, between there, there are links between different rows in the memory. Basically, you have, let me, let me show you this. Let me show you this. You have so many different differentiable parameters. All of the, you of, you have read and write heads, you've LSTM cells. Every single one of these matrices are, and every single one of these major streets are differentiable. So this is a gigantic, very complex system and everything is differentiable, right? So there's that. And so now, and what they did was for the DNC was they generated a random graph and have different subways and they use it to try to predict where someone was going to go based on some questions, which is just incredible.

Speaker 1:          28:18          They also trained it on family trees and a bunch of other things. But basically the best unsupervised learning methods still require backpropagation. So my point here is that backpropagation really is the workhorse of deep learning even in the unsupervised setting. But another thing I want to say is that a lot of, uh, deep learning research is all about making small incremental improvements off of existing ideas. And a lot of times academia kind of pushes us in that direction. It pushes you to make incremental changes. Maybe like tweaking one hyper parameter or adding some new layer type or maybe new, some new cell type, like a gru or whatever. But if you have, but if you, if you, if you think of a radically new idea, you can really shake things up seriously. And the idea, it doesn't even have to be that difficult. It really does.

Speaker 1:          29:09          It does. It doesn't even have to be that complex. Like think of games. I think of generative adversarial networks. It's such a simple idea. You have two networks. One tries to fool the other, that's it. It's just two neural networks. One tries to hold the other. And young lacount said this is the hottest idea in the past 20 years in deep learning. And look at this. I mean this idea was invented just two years ago. Look at the number of gains that have been, have been inspired by that first paper. There are so many, and this is in two years. All of these different can go on. You could make an entire four month course on all the different types of guns out there. So my point is, anyone can think of a really good idea when it comes to deep learning it, the, the playing field is level for everyone.

Speaker 1:          29:56          So let's get to the future research directions. Okay, so the first one, so my thesis is this, is that unsupervised learning and reinforcement learning must be the primary modes of learning because labels mean little to a child growing up, right? So we need to use more reinforcement learning, more unsupervised learning, and then we're going to get to somewhere somewhere better than where we are right now. So the first, uh, we research direction is Basie and deep learning, which is not discarding backpropagation is just making it smarter. What do I mean by this? Baze Basie and logic is all about having some prior assumption about how the world works versus frequent tests, which just assumes that we have no assumptions, right? So when you take Basie and reasoning and apply it to deep learning, you can have amazing results. And this has been proven in the case of variational auto encoders, but deep learning struggles to model this uncertainty.

Speaker 1:          30:54          So when I talk one on one, I'm what I specifically mean when I say Basie and deep learning is smarter wait, initialization and perhaps even smarter hyper parameter initialization, right? And this kind of relates back to a child and how evolution has primed us to know certain things before we've learned them in real time. Right? There are certain learnings we already have. We are weights in our head are not initialized randomly. When we start learning, we have some sort of smarter wait initialization. So Basie and logic is is a, is a great direction, is a great research direction. Just combining those two fields, Basie and logic and deep learning. The second one is called spike timing dependent plasticity. And a great analogy for this is saying you know, you're trying to predict if it's going to be rainy or not. You can go out there and you can see if it's going to rain literally with your own eyes or you can look at your roommate who tends to take an umbrella every time he goes out and every single time he walks out with an umbrella it happens to be rain.

Speaker 1:          31:53          So rather than try to go out there yourself, look at it's raining or not, instead you just look at your roommate, see if he picks up an umbrella. And if he does, you know that it's going to rain. So you take an umbrella. So the analogy applies to spike timing dependent plasticity because you can't properly back propagate for weight updates in a graph based network since since it's an asynchronous system, so we trust neurons that are faster than us at the task. So it's all about timing, looking at neurons and how fast they're firing and using those neurons as a signal, as a signal for how we learn. So suppose we have two neurons a and B and a synopsis onto be the Std p rural state that if a fire's and be fires after a short delay the synapse will be potentiated. Okay. So the magnitude of the weight increase is inversely proportional to the delay between a and B firing.

Speaker 1:          32:46          So we're taking timing into consideration which deep pointing currently does not do the time of fire. The third idea is our self organizing maps. So this is not a new idea at all, but that's okay. That's another thing that I want to mention. There is so much machine learning and deep learning literature out there. There is a lot and a lot of times the best ideas are forgotten. They are lost in the mix because there's so much hype around certain ideas and sometimes it's unnecessary hype around certain ideas and some of the best ideas could have been invented 2030 years ago. I mean look at deep learning, right? So it's just all about finding those ideas and self organizing maps are one of those ideas where you know this is an older idea but it has a lot of potential and not many people know how these works, how these work.

Speaker 1:          33:36          But this is a type of neural network that is used for unsupervised learning. So the idea is that we have, we, we randomize the node wait vectors in a map of them. So we have some weight vectors and then we pick some input vector that's our, that's our input data and we traverse each note in the map computing the distance between our input node and all the other notes and then we find the node that is closest, the most similar to our input node that is the best matching unit, the BMU. Then we update the wave vectors of the notes in the neighborhood of the BMU by pulling them closer to the input vector. And what happens is this creates a self organizing map and you can visualize it as different colors, but it's a basically clusters of different data points cause basically clustering. And I think this is a great idea.

Speaker 1:          34:23          It doesn't use it. It doesn't use backpropagation. Uh, and we should look more into that. The fourth idea, the fourth, the fourth direction or synthetic gradients. So, uh, Andrew Trask has a great, great blog post on this that I highly recommend you check out. It's really in depth. But this idea came out of deep mind. Uh, this idea came out of deep mine and it's basically, it's a much faster version of back backpropagation in which you are not waiting as long to update your weights. So individual layers make a best guess for what they think the data will say. Then they update their weights according to that guest and they call this best guessed the synthetic gradient because it's a prediction of what the grading will be, not what it actually is. And that data is only used to help update each layers guesser or synthetic gradient generator.

Speaker 1:          35:15          And what this does is it allows, uh, individual layers to learn in isolation, which increases the speed of training. Individual layers can learn without having to do a full forward and backward pass. Uh, so that's synthetic gradients. And I think, and, and it's weird because even in the machine learning sub Reddit, people were talking about synthetic gradients. Uh, but some of the questions were, hey, we need more of this. Why, why hasn't, why hasn't this been talked about more? And people don't know, right? So this is a great idea. I came out of deep mind and definitely learn more about synthetic gradients. The fifth research direction is our evolutionary strategies. So open AI had a great blog posts on this evolutionary strategies as a scalable alternative to reinforcement learning. But evolutionary strategies have not given us a lot of success so far, but that's okay.

Speaker 1:          36:05          Just intuitively they make a lot of sense, right? Trying to resemble evolution. You have fitness, you have a fitness function that determines how fit some individual is and these individuals mates, right? So there's crossover and you know, it's basically survival of the fittest. You have, you have mutation selection and crossover via a fitness function. And you can do with a lot of games, right? So you can have several neural networks and you can use evolutionary strategies to have the best one win or, or survive longer than the rest. So I think there's a, there's a lot of potential for that. And it's very similar to reinforcement learning. So if I, if I were to pick the lowest hanging fruit, right? The lowest hanging fruit in terms of revolutionary ideas to come to the table of really radical changes, it would be in reinforcement learning, deep reinforcement learning.

Speaker 1:          36:55          Reinforcement learning is all about learning from trial and error, right? You have some, you are some agent in some, in some environments, right? It's called the agent environment loop. You perform an action in that environment, you get a reward, yes or no, and then based on that reward, you update your state, your learnings, and you continue that process. So AlphaGo used reinforcement learning, deep reinforcement learning to get really good at its game. And there are so many low hanging fruits and deep reinforcement learning. How do we learn the best policy? Uh, uh, just there, there are so many unanswered questions. So reinforcement learning in general is a great place to, uh, do to just focus on in terms of research. And the last one is the most capital intensive and perhaps the hardest, but I just had to mention it right? We talked about how transistors are on off switches and uh, they are chained together serially to four to perform to form logic gates, whereas neural networks are, are parallel in their construction so they're connected to 10,000 other ones.

Speaker 1:          37:58          So perhaps instead of trying to replicate the rules of intelligence in Silico or at least, uh, on the current types of chips we have, let's just change the hardware completely right at the hardware level. And IBM's neuromorphic chips are good example of going in this direction. Google's TPU tensor processing unit. But basically the idea is to wire up transistors in parallel. Like the brain. Really, I think, I think anyone can can do this. You just, you know, you, if you have some idea, I mean, think about it, the brain is only running on 20 watts, so it can't be that expensive, right? In terms of hardware wetware right? So, so if you have some idea you can crowd funded for whatever hardware you want to build and then, you know, use we funder or Kickstarter and yes, I think you can even have a startup for hardware, for, for machine learning, for deep learning.

Speaker 1:          38:48          So what is my conclusion? Those are my seven research directions that I wanted to talk about today as a way, as a, as a response to Hinton talking about backpropagation. So what is my conclusion? What do I think? I think. And so I agree with Andre Carpathia who is one of the best deep learning researchers out there. He's a director of Ai, a Tesla now. And the conclusion is this, let's create multiagent simulated environments that heavily rely on reinforcement learning and evolutionary strategies. A carpet. The Ed is great talk at y Combinator, which I didn't attend, but the slides are online. But check this out. He had this one slide that said intelligence. The cognitive toolkit includes but is not limited to all of these different aspects of intelligence, attention, working memory, long term memory, knowledge, representation, emotions, consciousness. There are so many different, uh, topics that encompass learning.

Speaker 1:          39:44          It's this orchestra of different, of different concepts and they all work together to define intelligence. Our intelligence to the conclusion is we need to create environments that incentivize the emergence of this cognitive toolkit. So doing it the wrong way is to use this environment. What does this incentivized incentivizes a lookup table of correct moves, right? For pump. But what is doing it right? This two agents in this world, there's some food, there's some survival. They are learning to adapt to each other. It's much more like real life itself, right? And that incentivizes a cognitive toolkit, cooperation, attention, memory, emotions, even right with more complexity. So it comes down to the exploration versus exploitation dilemma from reinforcement learning. How much do we want to exploit existing algorithms backpropagation by making incremental improvements versus how much do we want to explore an entirely new ideas. And we need people doing both.

Speaker 1:          40:46          We need people improving the deep learning algorithms because there's still a lot to be improved upon. But we also need people working on exploration, like entirely new ideas. In fact, I think we need more people focusing on that then we have currently. So if I, if I were to, you know, take some away, I would say let's take 20% and put them 20% from the exploitation and put them in the exploration category. But, um, yeah, it's just something to think about. I hope this video helped you think more about all of these concepts and where we're headed and where we should go. I hope it gave you some ideas for what you might be more interested in. And I'm going to keep making videos like this, so yeah, please subscribe for more programming videos. And for now, I've got to evolve, so thanks for watching.