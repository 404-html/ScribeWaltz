Speaker 1:          00:00          Hello world. It's Saroj and let's visualize a Dataset of eating habits to see what we can learn from it, shall we? There's some really hard scientific questions out there. Are we alone in the universe? What is consciousness? What is dark matter? Really questions like these have multimillion dollar payouts and have been troubling scientists for hundreds of years, but guess what? We very likely already have the answers to them. The problem is that they aren't in plain sight. They're hidden in data. The former leader of the u s government effort to sequence the human genome said that it took several years of effort, a huge team of researchers and $50 million to find the gene responsible for cystic fibrosis and 89 but that same projects could now be accomplished in a few days by a good Grad students. You watching this right now? Yes, you can make a Nobel worthy breakthrough with just your laptop.

Speaker 1:          00:58          The data you need is freely available. You just have to discover the hidden relationships in it. The field of dimensionality reduction is all about discovering nonlinear nonlocal relationships in data that are not obvious and the original feature space. If we reduce the number of dimensions and some data, we can visualize it because a projection in two or three d space can be plotted really easily unless you use PHP training. A model on a dataset with many dimensions usually requires vast time and space complexity. It also often leads to overfitting. Not all the features we have available to us are relevant to our problem. If we reduced the dimensions, we can reduce the noise, the unnecessary parts of the data and find those that are surprisingly very closely related and once in a smaller sub space we can more easily apply simple learning algorithms to it.

Speaker 1:          01:52          We can divide dimensionality reduction into two different topics. Feature selection and feature extraction selection is all about finding the most relevant features to a problem. It can be based on our intuition as in which features do we think would be the most relevant or we could let a model one of the best features by itself. Extraction means finding new features. After transforming the data from a high dimensional space to a lower dimensional space, we'll perform the ladder using a technique called principle component analysis. Our dataset is going to be a record of the 17 types of food that the average person consumes per week for every country in the UK. So we've got 17 features slash. Dimensions. Let's see what kind of insights we can get from this data. PCA transforms variables into a new set of variables which are a linear combination of the original variables.

Speaker 1:          02:51          These new variables are known as principal components. PCA is an orthogonal linear transformation that transforms data to a new coordinate system such that the greatest variance by some projection of the data locked on the first principal component. The second greatest variance on the second component and so on, the variance is the measure of how spread out the data is. If I were to measure the variance of the height of a team of basketball players, it would be pretty low, but if I added a group of primary school children to the mix, the variance would be pretty high. Our first step is to standardize the data. PCA is a variance maximizing exercise. It projects the original data onto a direction which maximizes variance. If we were to graph a small data set that shows the amount of variance for the different principal components, it will seem like only one component explains all the variants in the data like Putin at the g 20 summit, but if we standardize the data first, then we'll see that the other components do indeed contribute to the variance as well.

Speaker 1:          03:57          Standardizing means putting the data on the same unit scale for us, that would be grams for everything, not a combination of kilograms and grams. That means the data should have a mean of zero and a variance of one. Amit is just the average value of all axes in the set x, which we can find by dividing the sum of all the data points by the number of them. The way we calculate variance is by computing the standard deviation squared. The standard deviation is the square root of the average distance of data points to the mean. It's used to tell how measurements for a group are spread out from the me. Once our data is standardized, we're going to perform. I get it. Decomposition. So if your mama is so fat, she's not embeddable and three space I can Paris will help fix that. I again is a German word that roughly translates to characteristic and in linear Algebra and I can vector is a vector that doesn't change its direction under the associated linear transformation.

Speaker 2:          04:58          Decompose, decompose my sole value parity, compose my soul. Another

Speaker 1:          05:11          way of putting it is that if we have a non zero Vector v then it's an eigenvector of a square matrix. A if a is a scalar multiple of the, so the land of scalar is an eigenvalue or characteristic value associated with the eigenvector. V Eigen values are the coefficients attached to eigen vectors that give the axes magnitude. If we had a sheer mapping and displaced every point in a fixed direction, notice how the Red Arrow changes direction, but the Blue Arrow doesn't. The Blue Arrow is the eigen vector of the mapping because it doesn't change direction and its length is unchanged. It's eigenvalue. It's one. Both terms are important in many fields, especially physics, since they can help measure the stability of rotating bodies and oscillations, fiber rating systems. Many problems can be modeled with linear transformations and I can vectors get very simple solutions.

Speaker 3:          06:06          Okay,

Speaker 1:          06:08          okay.

Speaker 4:          06:09          Interventions. Why set her reasons? PCA's deterministic. Yes, it so the correct fences and it makes data portable on a two d graph, which so we could even painted ourselves. I'm dawn to paying some pretty data. What's data by cameras?

Speaker 1:          06:32          If we had a system of linear differential equations, for example, to measure how the growth of population of two species x and y affect one another, like if one is a Predator of another, solving this system directly is complicated, but if we could introduce two new variables, Z and w, which depends linearly on x, we can decouple the system. So instead we are dealing with two independent functions. The eigenvectors and eigenvalues for this matrix of coefficients do just this. They decoupled the ways in which a linear transformation acts into a number of independent actions along separate directions that can be dealt with independently. So we'll need to construct a covariance matrix. Then we'll perform. I can decompensation on that Matrix. A Matrix is just a table of values. A covariance matrix is symmetric, so the table has the same heading across the top as it does along the sides.

Speaker 1:          07:30          It describes the variants of the data and the covariants among variables. Covariants is a measure of how two variables change with respect to each other. It's positive when variables show similar behavior and negative. Otherwise, PCA tries to draw straight lines through data like linear regression. Each straight line is a principal component, a relationship between an independent and dependent variable. The number of principal components equals the number of dimensions in the data and PCA's job is to prioritize them. If two variables change together, it's very likely because one is acting on the other or they're both subject to that same hidden force. Performing Eigen decomposition on our covariance matrix helps us find the hidden forces at work in our data. Since we can't eyeball inter variable relationships in high dimensional space. When calculating the covariance Matrix, the mean vector that's used to help do so is one where each value represents a sample mean of a feature column in the Dataset.

Speaker 1:          08:34          Once we have our eigen pairs, we'll want to select the principle components. We need to decide which ones can be dropped and that's where our eigen values come in. We'll rank the eigenvalues from highest to lowest, the lowest ones bear the least info about the distribution of the data so we can drop a number of them like they're cold. Next, we'll construct a projection matrix. This is just the Matrix of our concatenated top K eigen vectors. We can choose how many dimensions we want for our sub space by choosing that amount of Eigen vectors to construct our d by k dimensional eigen vector matrix w. Lastly, we'll use this projection matrix to transform our samples onto the subspace via a simple dot product operation. If we project our data onto one dimensional space, then we can already see something interesting. Notice how Northern Ireland is a major outlier.

Speaker 1:          09:29          It makes sense. According to the data. The Northern Irish consume way more potatoes in alcohol and way too few healthy options. The same thing happens if we graft both components. We can see relations between data points that we wouldn't otherwise. To summarize, principal component analysis is a technique that transforms a dataset onto a lower dimensional sub space so we can visualize and find hidden relationships in it. The principal components are eigen vectors coupled with eigen values. They describe the direction in the original feature space with the greatest variance in the data and the variance is a measure of how spread out some data is. The winter of last week's coding challenge his own job. He implemented a self organizing feature, math for colors and for handwritten digits. Really efficient code and well documented. Great job. Oh and wizard of the week and the runner up is Hamas shake who developed a super detailed Jupiter notebook on self organizing maps for class size effects on students. This week's coding challenge is to perform PCA from scratch on a Dataset of your choice posts or get hub link in the comments, and I'll give the winners a shout out next week. Please subscribe for more programming videos. And for now, I've got to release a music video, so thanks for watching.