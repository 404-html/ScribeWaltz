Speaker 1:          00:00          Wow. A big, big thank you to the Youtube Ai Algorithm. Thank you. Ai. Hello world. It's arrived in computers are pretty good at learning from spreadsheets of data filled with numbers, but we humans communicate with words not with numbers. The subfield of AI called natural language processing or NLP is focused on enabling computers to understand and communicate in human language. In this video I'll cover how NLP has progressed over the years. Up until 2019 they don't explain how to use a bleeding edge model called irt that makes it incredibly easy for anyone, not you, for anyone to build NLP apps right now. We'll specifically use Burke to learn from a Dataset of Amazon reviews that perform text classification and automatic summarization. Language is a way to represent information and we humans interpret this information from strings of text by assessing three different criteria, syntax, semantics and pragmatics basically and review syntax describes the form of the language usually specified by grammar.

Speaker 1:          01:15          Natural language is much more complicated than the formal language is used for programming. There are many rules of syntax to abide by. I before e except after c with 923 exceptions. Semantics describes the meaning of words or sentences of the language and pragmatics describes how the words relate to the world at large. It's about considering their context to understand the difference between these three criteria. Take a look at the following four sentences. The first sentence is appropriate at the start of an article, it's syntactically, semantically and pragmatically correct. The second sentence is syntactically and semantically correct, but pragmatically it sounds kind of whack AAF. The third sentence is syntactically correct, but semantically incorrect and the last sentence is incorrect on all three fronts. Syntactically semantically and pragmatically. Computer scientists have been creating automatic systems to attempt to do this for just about 60 years now, which makes it an incredibly young scientific discipline.

Speaker 1:          02:25          We can trace the history of NLP back to 1950 when the prominent computer scientists and it it Cumberbatch, I, I mean Alan Turing published a landmark paper titled Computing Machinery and intelligence, which proposed what's now called the Turing test as a criteria on of true intelligence. The question the Turing test poses is can a computer program fool a human into thinking it's a human via conversation? A few years later, Noam Chomsky, a prominent linguist published a book titled Syntactic structures which detailed a rule based system of how to structure a grammatically correct phrases and this inspired many rule based approaches to NLP generating a sentence was usually done by pulling syntactic information from a database. In fact, up until the 1980s most NLP systems were based on a complex set of hand written rules, but in the late 1980s hair metal became an unfortunate reality. On a more related note, a revolution in NLP occurred when researchers started using machine learning algorithms for language processing instead of rule based algorithms.

Speaker 1:          03:36          Mostly because the increase in available computational power allowed for this strategy to outperform rule based systems. Some of the earliest use learning algorithms like decision trees produce systems of hard if then rules similar to existing handwritten rules, but as time progressed, researchers increasingly favorite statistical models which make probabilistic decisions as to what a word or sentence should sound like or mean instead of rule based decisions. Nowadays and LP systems like speech recognition software rely on such statistical models to predict which words were likely spoken by a user, which are more reliable. Alexa, show me a photograph. Photograph by Nico Bath. Know a class of statistical models called deep neural networks. Have been the key driver in most of the recent NLP successes across a wide variety of tasks like machine translation, automatic summarization and sentiment analysis. And in 2019 free open source tools like Pi, torch colab and various text datasets have enabled individuals and teams from across the globe to create powerful applications that use NLP to solve real world problems.

Speaker 1:          04:53          For example, clever who is a finished startup providing an instance site search solution for ecommerce stores. They're using text classification to provide relevant search results for shoppers and actionable insights for store owners. Another startup called English central aims to make learning English much more fun by giving users instant feedback on their pronunciation using speech recognition techniques. Yummly is building a platform for recipe recommendations and search. They use NLP to understand, analyze and connect users with the recipes they most enjoy old on cowboy or cowgirl. Before you go build an NLP startup immediately you need to understand one concept really well, Burt, which stands for bi-directional and coder representations from transformers. I'll explain what each of those words mean in a second. Bert is a fully trained language model that Google released just a few months ago and it's been the most significant breakthrough in NLP thus far. A language model is able to learn the probability of word occurrence based on examples of text.

Speaker 1:          06:02          Traditionally, language models are trained by using the previous end words to predict the next one, but ERT is a language model that was trained by using both the previous and next words when making predictions, hence the word bi directional. Instead of unit directional. Burt was used to establish a new state of the art in 11 and LP tasks including question, answering sentiment analysis and automatic summarization. All of these tasks involve a two step process, train a deep language model on some text data, then give those representations to a task specific model. For the first step of this process, the Goto technique for the past few years is called word to Vec and it creates word representations also called word vectors. It maps each word in the training Dataset to a vector that represents some aspect of its meaning. So for example, the word vector for teen would include information about state and gender.

Speaker 1:          07:02          These representations are generally trained on large unlabeled data like a Wikipedia dump. Then use a train models on labeled data for tasks like sentiment analysis. This allows models to leverage linguistic data learned from larger datasets. The problem with word two VEC and similar word vector techniques was that they didn't take context into account. The Word Bank, for example, would have a different meaning depending on the context it was used in. They have trouble capturing the meaning of combinations of words. These limitations motivated the use of recurrent networks as language models. Instead, instead of training a model to map a single vector for each word, if these techniques train a neural network to map a vector to each word based on the entire surrounding context. Nowadays, the transformer, a newer type of neural network, has eclipsed all variations of recurrent networks for language modeling. A transformer consists of an encoder network and a decoder network.

Speaker 1:          08:04          So the phrase Bert means using a transformer network to create by Directional encoder representations. These representations can then be fed into another model for some specific NLP task. Unlike recurrent networks, transformer networks like Bert don't use recurrent connections at all. They instead use attention over the word sequence. Instead, attention is defined in neuroscience as the ability to selectively concentrates on one aspect of the environment while ignoring the rest. In deep learning, we mimic this concept through the use of attention mechanisms. One way of doing this is to encode an input sequence into not a single fixed vector, but instead have a model. Learn how to generate a vector for each output time step by adding an additional set of weights that will later be optimized. So it doesn't just learn what to output. It learns how to selectively way parts of the input data to maximize the likelihood of the proper output.

Speaker 1:          09:08          Burt is composed of several attention blocks to prevent it from having ADHD like I do. Each block transforms the input using matrix operations. If we input as sequence of n words, the encoder will output a sequence of tensors. These tensors are used by the decoder to output a sequence of words. The architecture is optimized using gradient descent linked to how that works in the video description. The great thing about Bert is that it comes fully trained out of the box. It took Google four days using several cloud TPU use to train it on several languages. So thank you Google. I guess all we need to do is fine tune the final layer of Burt on our own training Dataset for whatever task we choose and it will benefit from Burt's existing knowledge. So let's look at our data set of Amazon reviews, which we'll use for two tasks.

Speaker 1:          09:58          Text classification and automatic summarization. I text classification. We're talking about classifying chunks of text. That could be anywhere from sentence size to an entire paragraph in length as either a good review or a bad review. We'll first clone Bert into our environment. Then we'll download the Burton model files. These are wait values that represent what it's learned from pre-training. Then we'll need to preprocess our data into a format that Bert expects. Column One will be a row ID. Column Two is the label for the route as an ENT. Column three is a column of all the same letter. It's a throwaway column that we need to include because Burt expects it WTF, right? Just roll with it and the text examples will be in the last column, the ones we want to classify. We can do all this easily with the pandas python library. Once we formatted our data, we can run training.

Speaker 1:          10:52          Once it's finished training, we'll use it to predict on new text data by selecting the newly trained weights file as input, as well as some test review and it will output a classification, either a good or bad review. Now if we want to perform automatic summarization, we can use the same learned embeddings well firstly to cluster them then extract one sentence from each cluster. This is an unsupervised technique. These embeddings will be clustered in high dimensional vector space where the number of clusters is equal to the desired number of sentences we want. In the summary, each cluster of sentence embeddings can be interpreted as a set of semantically similar sentences whose meaning can be expressed by just one candidate sentence. In the summary, this candidate sentence is selected to be the sentence who's vector representation is closest to the cluster center. We then order the candidate sentences to form a summary.

Speaker 1:          11:47          This order is determined by the position of the sentences in their related clusters. This technique is considered extractive summarization. Amazing, right? There are three things to remember from this video. Natural language processing is the study of computational techniques to help computers understand and communicate in human languages. Google's Burt's model. It makes it easy for anyone to create an NLP application greatly reducing the amount of training, time, data and compute necessary. And we can perform NLP tasks like text summarization and text classification using birth. What do you want to do with NLP? Next, let me know in the comments section and please subscribe for more programming videos. For now, I've got to analyze some texts, so thanks for watching.