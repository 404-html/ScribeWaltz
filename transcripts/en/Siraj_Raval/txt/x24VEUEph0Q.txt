Speaker 1:          00:00          My dear Megan Tang. This is our love story, human or machine human. Why I found some drops and soylent on it. Oh, a world walking the third geology. In today's episode we're going to build an AI writer that is an app that can write a short story about an image just by looking at it. Sorry, Stephen King. You're out of the gate. Isn't it weird how just by stringing together an exact combination of words, we can produce something, a profound beauty. When we read these stories, our brains are somehow encoding them into thoughts. When we encode a sentence to a thought, the more semantically similar it is to an existing thought, the more we'll be able to relate to it. So how do we get an AI to write a story if it doesn't have any experience living life in the real world? Well, we're going to build an AI writer in python using the deep learning library Lasagna, and we've got a lot of code to go over.

Speaker 1:          00:45          So I'll explain as we go. Let's get python Anik. At the highest level, we could code this APP in just three lines of code. It's a little ridiculous. We import the generate class and call it a load off function, which we'll initialize all of our machine learning model. Then called a story function with the generated models and image location as the parameter. That's it. It'll output a story, but let's dive a little deeper. A load off function is just one of the plate initialization, so let's take a closer look at the story function where the real magic happens. We'll start off by loading an image into memory. This will be the image that we want to tell a story about. We'll use a load image function to load it and have the parameters set to the location of the image on our machine. The load image function uses the scientific computing library, num Pi to get the byte representation of the image and then resize it so it's smaller, but preserving it's aspect ratio.

Speaker 1:          01:29          Once we've loaded our image, it's time to input the image into a deep convolutional neural network to retrieve its features. In a previous episode, we talked about how convolutional neural nets were great for image recognition since they roughly mimic the human visual cortex. This. CNN is pretrained. We initialize it into build cognitive function, which is called in the boiler plate load. All method. Once we specified all the layers, we load up our pretrained sinaps weights fall called BGG 19th it's fall was trained on a huge Dataset of labeled images, so it will be able to recognize the objects in a novel image. Once we input our image into our CNN, it'll return an array of features for us. He's features of the highest level features in our neural net. The layer right before the output layer, the most abstract representation of the image. It's content.

Speaker 1:          02:09          Once we have our features, we want to encode the image features into a multimodal neural language models. So what is this? Well, it's based off a paper called unifying visual semantic embeddings. In our code, we're using a pretrained models that will input a joint image sentence embedding into a multimodal vector space. It use an Lstm to encode the sentence and a CNN to encode the image. Then a decoder neural language model generates a novel description from the image. Since our model is pretrained and we embed our image into this multimodal space, our features are updated to include the weight of the joint space. Then we compute the nearest neighbors to do this. First we retrieved the array of scores. That is a list of all novel sentences generated from the novel image, which we then sort in order of closeness. Then we'll want to print out the nearest captions.

Speaker 1:          02:50          Now that we have a set of caption sentences, one to compute a set of skip thought vectors for each sentence. Skipped up vectors are a vector representation of a sentence. This is another implementation of the encoder decoder model. The encoder and decoder are both recurrent neural networks. We take an input sentence and encode it into a skip fuck vector by inputting it into the encoding recurrent neural net. Since we are modeling a sequence of words, we use gated recurrent units or gr use at each neuron. Gr used consists of two gates on uptake gate and a reset gate. The gating units modulate the flow of data inside the unit, and unlike LSTM cells, there are no separate memory cells. LSTM cells control the amount of memory content that is seen or used by other units in the network. Gru cells don't. They expose it's full content without any control.

Speaker 1:          03:32          So Jerry use have a less complex structure and are thus more computationally efficient. We're starting to see these be used more and more. There are relatively new, so when we feed that sentences into the RNN, it'll create an abstraction. The vector representation or skip thought vector sentences that share semantic and syntactic properties. We mapped to either the same or similar skip thought vectors. The function returns these vectors, that's say num py or rape, which we can then modify via the style shift function. We'll take our thought vectors and modify them to match the style of stories using a pretrained recurrent neural network. The RNN was trained on a data set of romance novels, or each pathogen was mapped to a thought vector. So we're essentially computing a function that looks like this for a style shift. F of x is a book passage thought vector x is an image.

Speaker 1:          04:11          Caption C is a caption. Stop actor and B is a bookstore vector. We removed the caption, stop on the caption and replace it with the bookstore to create a book passage of vector. Once we have our book passage style vector, we can generate the story by running the decoder function on its. The decoder is another recurrent neural network that given a vector representation of a sentence can predict the previous and the next sentence will run the Dakota on our passage vector and that will generate our story based on the image for us. Let's take a look at what it says about this picture. Let's read the first few sentences. She was taking the man out of her mouth and she gave him a gentle shake of her head. Oh my God, I can't wait to see what happened in the past 24 hours. I had never met a woman before.

Speaker 1:          04:47          This thing is a pro for a small chunk of code. There's a lot of machine learning going on here. We use a convolutional neural net to compute image features. An LSTM recurrent neural net to encode our image into joint space and retrieved the sentence captions, a Gru, recurrent neural net, to calculate the skip thought vectors of those sentences. And after sal, shifting an Rnn to decode our passage vector to a story that's for neural nets, you can run this on your local machine. This unnecessary models are pretrained. For more info, check out the links below. And I just signed up for patriots. So if you guys find my videos useful, I'd really appreciate your support to help me to continue doing this full time. We subscribe, subscribe for more ml videos, and for now I've got to go fix a null pointer exception. So thanks for watching.