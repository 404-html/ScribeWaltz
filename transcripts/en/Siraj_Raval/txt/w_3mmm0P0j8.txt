Speaker 1:          00:00          Hello world. It's the Raj and have you ever played doom before? It's a classic shooter and we're going to use a technique called the actor critic to train an AI to beat the game on its own deep minds, deep Q network, the same algorithm that beats so many different games made history and brought our l back into the limelight. Let's just take a second to appreciate how mind blowing the results it achieved. We're starting with no knowledge on what winning means. It was able to explore the environment and eventually master the game. Think about putting yourself in that same situation. It'd be like asking you to play a game without a rule book or an end goal and saying keep playing until you win. Oh and the possible results states you could reach with a series of actions is infinite, I. E. A continuous observation space.

Speaker 1:          00:51          Only young Macoun could do that despite this scenario. Deep Q was able to converge really quickly on this seemingly impossible task by maintaining and slowly updating action value pairs. But what happens when we have an infinite input space and an infinite output space? Well, deep Q is no longer an effective algorithm, but wait, isn't it completely independent of the structure of the environment actions while it was independent of what the actions were, but it was fundamentally premise on having a finite output space. Think about it. The prediction looks to assign a score to each possible action at each time step given the current environment state and just uses the action that had the highest score. Basically it reduced the problem of our l two assigning scores to actions, but how could this be possible if we have an infinite input space, we basically need an infinitely large table to keep track of all the Q values.

Speaker 1:          01:49          Think of this task this way. Not only are we being given a game without instructions about the game has a controller with infinite buttons on it. That'd be a terrible controller, right? Almost as bad as the playstation mouse was, deep queue is restricted to a finite number of actions and that's because of the way the model is structured. We've got to be able to iterate at each time step to update how our position on a particular action has changed. That's why we're having the model predict Q values rather than directly predicting what action to take. If we had the ladder, we'd have no clue on how to update the model to take into account that prediction and what reward we'd receive for future predictions. So the issue stems from the fact that it seems like our model has to output a tabulated table of the rewards associated with all the possible actions.

Speaker 1:          02:38          But here's an idea. What if instead we split the model into two? What if we had two separate models, one that outputs the desired action and the continuous space and another taking in an action as input to produce Q values that seems like it could work. And guess what? It's exactly the idea behind the actor critic model or actor critic methods are premised on having two models. And this theme of having multiple neural nets interacting seems to be getting more popular, not just in reinforcement learning, but in supervised learning as well. Like in the case of generative adversarial networks and variational auto encoders, seriously, almost everyone's slapped the word Gan on their paper at nips and called it a day. But that's a side note. The actor critic model has two components, an actor and a critic. The actor takes in the current environment state and determines the best action to take.

Speaker 1:          03:31          From there the critic plays the evaluation role by taking in the environment, state and action and we're turning a score that represents how good the action is for the state. One way to think about this is as a playground with a child being the actor and her parent being the critic. The kid is running around and exploring all the possible options in this environment. They could slide down a slide, take a ride on a swing, pull grass from the ground and eat it. The parent's job is to watch the kid and either criticize or compliment them based on what they did. Taking the environment into account. Actor, critic methods are advantageous to policy gradient methods as well. Let's go into a bit more detail of how actor critic works. But first I want to take a second to explain the chain rule because it's used in this method and it's just really important in machine learning in general, like say in the case of backpropagation for neural networks, the chain rule is a rule that comes from calculus that helps us compute the derivative of a composite function that is a function of functions.

Speaker 1:          04:35          The idea is that you derive the outer function and derive the inner function and multiplied them together and that'll give you the derivative of the whole composite function. And it's a recursive rule. So it applies for as many nested functions there are, the derivative of the s is the slope of the tangent line at a point on the curve of a function. It's a way to represent the rate of change, meaning the amount by which a function is changing at a given point. The chain rule will help us update the parameters of our actor critic model. So in our system where the output of one network feeds into the input of the other, updating the parameters of the feeding network will shake up its output, which will then propagate and be multiplied by any further changes through the end of the pipeline. When we implement our model, note that it has the same exact task that deep Q would have, except we have two separate modules.

Speaker 1:          05:30          We start by defining the actor model. Given the current state of the environment determined the best action to take and since the task is given numeric data, there's no need to involve any complex layers in our network. Just fully connected once. So the actor model is just a series of fully connected layers that match from the environment observation to a point in the environment space. We're going to return a reference to the input layer as well, and I'll explain why in a second, but how are we going to determine the best action to take? The Q scores are now calculated separately in the critic network. We're going to use an optimization strategy called gradient ascent. This means attempting to reach a global maximum by moving incrementally in the steepest direction of an incline and the way we get the direction to move is by using the chain rule to compute derivatives I. E. Gradients of the weights of our network.

Speaker 1:          06:25          What change in parameters for our actor model would result in the largest increase in the Q value which would be predicted by the critic model. Since the output of the actor model is the action and the critic evaluates based on an environment state action pair, we can see that the chain rule helps us do this. Changing the parameters of the actor will change the eventual Q value using the output of the actor network as our middle link for our critic network. We're faced with the opposite issue. The network is a bit more complicated, but training is straightforward. The critic must take both the environment, state and action as inputs and calculate a valuation. We use a series of fully connected layers with the layer in the middle that merges the two before combining into the final Q value prediction. We use one extra fully connected layer on the environment.

Speaker 1:          07:19          State input as compared to the action will hold onto references of both the inputs, states and action since we don't need to use them and doing updates for the actor network. The output queue with respect to the action weights I. E. The gradient is directly called in the training code. Since we have two training methods, we separate the code into different functions. We're training on the state action pair and using the critic model to predict the future reward rather than the actor. The actor just needs to be called with the actions and states we encounter and our prediction could just iterate through a trial called predict. Remember and train the agent. Notice how our doom bought after training for about 20 hours on a single CPU is able to kill monsters and navigate successfully through the environment. A lack of boss. All right, three things to remember here.

Speaker 1:          08:10          Actor critic methods work well when we have both an infinite input space and infinite output space. In an actor critic model, the actor takes in the current environment, states and determines the best action to take. The critic plays the evaluation role by taking in the environment states and action and returning an action score. And actor critic methods tend to require much less training time and policy gradient methods. Last week's coding challenge winner. He's fine. He trained an AI to play tic tac toe using the policy gradients strategy and he's documentation makes it easy for you to reuse his code. Great Work Wizard of the week. And the runner up is [inaudible] Kumar who use a convolutional network and cue learning to have an AI learn how to play

Speaker 2:          08:55          flappy birds. Really Clever Algorithm. This week's challenge is to generate some music using deep learning.

Speaker 3:          09:02          So the challenge is to either create a model that generates music or generate music from one of the existing models that we talked about here today. Send me your finished track and or tracks to my Twitter handle at Taryn southern. You can upload it to any site that you prefer. Um, just please don't include viruses and, uh, and I will pick my favorite and I will actually write lyrics and vocals to that.

Speaker 2:          09:27          Karen is going to write lyrics for this song. She's going to work with it, so definitely submit it to her via Twitter. And I'll also announce the two top entries next week in next week's video. Please subscribe for more programming videos. And I think we got to go. We've got to go make some AI music. It's true. So thanks for watching.