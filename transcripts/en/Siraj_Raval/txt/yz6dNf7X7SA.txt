Speaker 1:          00:00          Hello world it Saroj and do you gotta catch them all? When I say catch 'em all I'm talking about Pokemon, I've got to catch them all, but I've also got to train them all. I've got to train them all. You can seeing a generative adversarial network. What we're going to do in this video is we're going to generate all new Pokemon. You heard me correctly, more than the original 150 shadows to the original 150 and the 5 billion other ones that came after that. But we're going to generate all new Pokemon using what's called a generative adversarial network. Now, I've done this in one or two videos before, but this is probably going to be the most clear explanation I've ever done of it. It's a very popular machine learning technique popularized just and created just two years ago. And uh, this is going to be awesome.

Speaker 1:          00:43          So this is a demo of what I've already generated with my Gan Gan for short. So we'll call them gangs from now on. And as you can see, it looks pretty funky. We give it some training images of real Pokemon. And then after training, using this generative adversarial approach, it's going to be able to generate new Pokemon that have never existed before. So let's dive right out right on into it. At the end, I'm going to go into the code, but first let's talk about what these things are. So there's a lot of different ways that we can classify the learning process for computers, right? The machine learning process, we could talk about supervised unsupervised reinforcement learning, but one of the most popular ways is discriminative and generative, right? So for generative models, we are learning the probability, the joint probability of x and y. For discriminative models, we're living the probability of y given x.

Speaker 1:          01:37          So you might be thinking, what does that mean? Right? So here's what it means. Here's a really simple example that I found off of stack overflow. So let's say this is our training data, right? The, it's just a four x y value pairs, right? X, Y, x, y, x, y, x, Y, and they're all integers. And so the, the joint probability that has a probability of x and y is going to be these four values. Okay? And then the probe bill, the the probability of y given x is going to be these four values. So take a second to kind of just stare at this and eventually you're going to get it. But let me help explain. So let's say let's say y equals zero and x equals one. So this top left corner here, what that means is what is the probability that y is going to be zero and x is going to be one out of all the possible cases.

Speaker 1:          02:25          Well, if we look up here, we'll see, okay, here's one case, here's another case. These two are not. So out of the four possible cases, two of them are cases where x is one and wise is zero. So that's two out of four or one half. And so that's how it works for the joint probability for the, for the probability of y given x, it's if, if x is going to be one, how many times is why going to be zero? So here's x equals one. So why zero? Here's x equals one y zero and those are the only two times. So out of all those times it's going to be zero. So that's a hundred percent probability, right? So what's the probability that you're going to get zero given every time that x equals one? Right? So that's the difference between both of those. It still doesn't explain the difference, but that's the mathematical explanation.

Speaker 1:          03:13          So the idea behind generative models is that we are trying to create these systems that generate new data given some training Dataset, right? So the pros of this are that we have knowledge about the data distribution. What that means is any kind of training data that you have, any kind of training data, we can represent all of it as some distribution, right? Some curve, either a Gaussian or a multinomial, all sorts of distribution and what a distribution is. It is, it is a range of possibilities, right? It is a range of possibilities of what some data could be. And if we learn what that distribution is, then we can say, let's take some point on that distribution and generate some data from it. What that means is that point, it doesn't necessarily have to come from our training Dataset, but it's a part of the range of possibilities and thus it, it can be a novel data point, meaning a new type of data that looks or feels very similar to the original data.

Speaker 1:          04:10          Basically we can generate new data if we learn what the distribution of the existing data is. The cons of this is that this is very expensive to learn, right? Degenerative models in general are harder to train than discriminative models and we need lots of data. Discriminative models, on the other hand, they don't tell you to generate anything. They're really easy to use. That's the pro. The con is that all they do is they discriminate, right? They classify, they differentiate. Given this dog, given this cat, is this a dog? Is this a cat? It tries to put labels on things that probability of y a label given some input x, right? So that's why he's the probability of y given X. Whereas for the joint probability it's probability of x and y. So given some new x, we can generate a why or given some new why we can generate an x at the same time.

Speaker 1:          05:03          Right? So that's a difference. So a lot of times we talk about discriminative models, um, and there are a lot of popular ones. So logistic regression support vector machines, neural networks can be discriminative. That can also be generative as we're going to see here. Uh, but for generative models we have naive Bayes mixture of Gaussians Hidden Markov models. And if you're curious about how any of these models work, I would point you to my math of intelligence playlist. I have videos, detailed videos on every single one of these that you're seeing here. They're all in my math of intelligence playlists. So take some coffee and just go through that whole playlist if you want to learn how all these models work. But my idea right now is to just show you that there are two types of machine learning models that we can, we can, uh, think about, right?

Speaker 1:          05:49          So again, discriminative models, probability of y given x and generative models or the probability of x and y. And we can use Bayes theorem to derive that as well. Discriminative models learned the boundaries between different classes. If it learns the boundary, then giving some new data point, we can say, oh, it's on the left side. It must be a dog, or it's, it's on the right side, it must be a cat. Whereas generative models learning the distribution of the clashes and then once we have that distribution, we can say, boom, generate this, generate this, generate this. Okay, so that's kind of how it works. So one class of generative models is the generative adversarial network. Okay. So the generative adversarial network at a high level looks just like this. Ignore the detective part for now. But what happens is we have to neural networks. By the way, I met Ian Goodfellow, the Creator, I interviewed him.

Speaker 1:          06:40          If you're curious, just search Saroj. Ian Goodfellow on Youtube. Great interview. Anyway, we have two neural networks here. We have a generator network and we have a discriminator network and our job is going to be this, right? We input some random noise into the generator network. That is some random group of numbers of vector and we're going to feed it to this neural network, this generator network. It's job is to take that input and do it all known. That works. Do you apply a series of matrix multiplication to that input? Until we have an output and that output, we're going to consider a fake image. Now the discriminator is gonna say is only going to look at the real training dataset that we have. That is the real, that are the real images and once we have these real images, we'll feed those into the discriminator network and it's going to output a probability label, whether it's real or it's fake because the discriminator takes two inputs.

Speaker 1:          07:34          It's going to take the real image and it's going to take the generated fake image and its job is to say is the fake image real or fake? Right? And at first it's going to say, obviously this is fake. You just took some random noise vector and generated an image from it. It had nothing to do with the training data. Of course it's fake. And then what's going to happen is when it makes that prediction, we're going to compute an error value like always. And then what do we do? We optimize moving forward and when we optimize, we are optimizing to minimize a loss function, right? And when we minimize a loss function, we're using a strategy like gradient descent, right? With gradient descent, we can compute a great value to update the weights of both networks, both the discriminator network and the generator network.

Speaker 1:          08:18          What that means is they both get better over time. And what that means, what that really means is that because they get both get better over time, the generator is getting better at generating very realistic looking data until the discriminator cannot decide what's real and what's fake. And that's when the generator has done its job well. Right? So one way to think about this is that the discriminator is a detective is trying to figure out what's real and what's fake. And the generator is a forger. It's trying to say, hey, this is a real copy of the Mona Lisa, but it's not. It's a fake copy of the Mona Lisa and it's just using random noise, random numbers generated as its input. And as a tweets are updated, it's getting better and better at this. Over time we can think of the discriminator as a binary classifier real or fake one or zero.

Speaker 1:          09:06          And, and the way we can do this is because we, we ourselves, we as programmers know what's real and what's fake. So we can assign those labels and because we can assign labels, it's a supervised problem. And because it's a supervised problem, what is a technique that we use? Here we go. Go for it. Say back propagation, right? That's how we update our gradients. We use backpropagation and notice because we have two networks, we have two networks. We are going to be back propagating gradients for two different networks. That means we have two different optimization problems running simultaneously. So the model's played two distinct, literally adversarial roles because they are our adversaries. One is trying to fool the other constantly and the other is trying to not be fooled. Right? And because we are using this optimization technique, they're both getting better and better over time.

Speaker 1:          10:01          So you might be thinking, okay, this sounds really complicated. I don't know what this looks like, but let me tell you this. There are really only four components. I know I put five, but they're really only four components that think about here you have are the genuine Dataset, whatever that is, images, videos, whatever that data set is, and then you have eye, which is the random noise vector, which is trying to, which is the kind of starting point that the generator uses to generate anything. You have g the generator and you have d the discriminator, right? You know how to build a neural network. I made a million videos on that. So you just build two neural networks and then you have some training Dataset, you have a loss function and you go, that's a generative adversarial network. The end. No, I'm just kidding. There's more to it than that.

Speaker 1:          10:43          There's more to it. And I'm going to show you, but first let's talk about the use cases of this thing, right? So obviously a generating Pokemon is a great use case. By the way, please, let's just chill with the M and I s. T I know everybody uses them and ist as the baseline, but can we just start using Pokemon as the new M and ist? I'm just saying it right now. Let me drop this right now. Let's start using Pokemon please. As the baseline, the original 150 as the new baseline for generative models instead of m and ist because as awesome as young raccoon is, the guy who, you know, the godfather of convolutional nets and his, you know, Dataset m and ISD is, it's just time to move on to just to keep things interesting. I've made a million videos about him and ist.

Speaker 1:          11:28          I'm not going to rant about it too much. Anyway, back to this, we can do so many things with gans. For example, we can turn black and white images to color. We can, we can turn maps into a digital maps. We can say we can turn day tonight. You can make a drawing of a purse and then turn it into a real purse. Right? But to get really trippy, you can feed it a, a plain text, plain English input, like the flour is white and pink in color with pedals that have veins. And then it's going to generate images from that text. Extrapolate for a second here. What that means is as these things get better, we can feed them, not just texts of, you know, pretty little flower images. We can say, design me a rocket, design me, forget a rocket. Design me the particle accelerator from Cern, you know, that $50 billion machine under Geneva designed that for me.

Speaker 1:          12:20          Here are all the specs. Here's all the data go. What that means is the cost of the barrier to entry to a million things becomes possible. Design, you know, anything, a living group design, clearly there's a lot of possibilities for design and engineering, right? Three d modeling. But it's not just that, it's also for science. So these researchers that in Silico medicine said, let's, uh, let's generate a new drug using, uh, using Gantz. So the goal was to train the generator to sample drug candidates for given disease as precisely as possible to existing drugs from a drug database. Right? And so what happens is after training, it was possible for them to generate a drug for a previously incurable disease using the generator and using the discriminator to determine whether the sample drug actually cures the given disease. So there are a lot of use cases for this, and it's not just that there are a lot of use cases, there are a lot of gans fantastic gans and where to find them.

Speaker 1:          13:20          That's a great blog post by the way. There are so many different types of gangs out there, seriously, but one huge improvement to the initial Gann paper in 2014 is the deep convolutional Gan or DC Gan. And so the, the, the main difference here, the main main difference is that instead of using feedforward networks as both the generator ended discriminator, they used a convolutional network as the generator and a d convolutional network as the discriminator. So that's just a convolutional network flipped. And what happened is they made a few important discoveries. One important discovery was batch normalization is a must for both networks. Fully kind of fully hidden, connected layers. Not a good idea. Avoid pooling. Simply stride. You're convolutions. So again, pooling recall from my video about capsule networks. Hinton the godfather of neural networks themselves. He's also skeptical of pooling. In fact, he replaced it with that capsule strategy.

Speaker 1:          14:17          So here's a great research idea for you. Apply capital networks to generative adversarial networks. Boom, take it, run with that. Be the first to do that. Okay. Capsule generative adversarial networks. Paper of the year award right there. Paper of the year award. Uh, right. So relu activations are your friend almost always. Why? Because it prevents the vanishing gradient problem. Exactly. If you didn't say it, that's okay. Vanilla Gans can work on simple data sets, but DC gans are far better. And if you have some new state of the art algorithm, compare it with the baseline of using a DC Gan. Another one, conditional gans. Now these are really interesting because notice, remember that image I showed you where it said, here's a flower with petals. It's white. It's pretty generated. They use the conditional again to do that. What that means is what we're feeding into the generator is not just some noise.

Speaker 1:          15:09          We're also feeding in these strings were were conditioning the data on these extra strings like male, black hair, blonde makeup, whatever. And we're also conditioning the discriminator on this as well. And what happens is we are feeding both as vectors, right? So we're converting those strings into numbers vectorizing them, we're converting the noise, two vectors. We're concatenating both vectors and feeding that in his input into the generator as well as a discriminator. And when we back propagate, when we update our weights, the networks are going to be more suited to generate data or discriminate data that is conditioned on those strings because there's a, there's an association between those strings and the training images because we have to pre label them, right? We have to pre labeled the images with these strengths and because it's learning, uh, to generate and discriminate conditioned on these strings, we can input these strings as, as the sole input after training.

Speaker 1:          16:04          And then there's going to be able to generate images like wipe pretty flower or whatever, which is very cool. Uh, lastly, there's one more type of Gan of the many, many guns out there that I want to talk about. That is a very important, again, it's called the Wasserstein or Wasserstein, however you want to say Gan or w again and the, the dif, the real discovery of w Ganz was that if you've ever tried to train again before and you should after this video, if you haven't, do it, do it. If you haven't, it'll be great. You'll learn a lot. Trust me. Uh, the, the great thing about [inaudible] is that it improved the, the loss function. So if you look at a general, again, like a generic vanilla Gan, this is what the loss function looks like, or even a DC Gan. How do you know when to stop training?

Speaker 1:          16:53          Right? How do you know when it's, when it should be done? Usually this is, this is a really ugly a loss function. There's, there's no idea. Uh, there's no way to tell when to stop training. You just have to guess and check. So this looks much prettier. Right now we know when to stop training, when that loss is maybe at the 500,000 iteration mark. Now that loss is pretty low and everything training afterward is going to lead to diminishing results. So that's what w gans do. They replace the loss function, which is normally called the Jensen Shannon divergence with instead the Wasserstein distance. They tried to minimize the Wasserstein distance. So I can go into that, but that's a lot for one video, but that's at a high level of what it is, right? So that's, that's more of the state of the art when it comes to Gantz.

Speaker 1:          17:41          Okay. So, um, that's that. Now let's talk about the code. Okay, let's, let's go into the code now for this Pokemon gap. Let's take a look at it. So this code is about 285 lines. So it's a lot, but it's not too much, right? We, we can fit it all into a single class file. And I've, it's built with tensorflow. Okay. It's built with tensorflow. So the training data, is this right? These are all Pokemon images right here, right? We've got several Pokemon. It's not just the original hundred 50. We got a lot of them. Don't trio, whatever. I only know the original one 50 because when I was a kid, that's, that's all we, uh, which one was this? Ticklet. Tigger. I don't know. Anyway, no, I know it's not ticker. It's, I forgot the name. Anyway, let's look at this. A python file.

Speaker 1:          18:27          Okay. We get ready for this. So we have a processing data function that we can just skip. Basically what that does is it just formats all those images into the same size because we want all those images to be the same size when we feed it into our networks, right? Because that's, that's called, that's the, that's the vectorization process. So we have two functions. Each of these functions represents one of the networks. We have a generator function, and then we have a discriminator function. So don't get afraid by looking at this. This is just standard tensorflow code. If we wanted to, we could, we could have used care Ross, and this all could have been abstracted to 10 or 12 lines, but we want to be very specific about what the details are of what this looks like. So remember with convolutions convolutional networks, this, this Gan, by the way, is, is a w GAM.

Speaker 1:          19:16          So what that means is it's a DC Gan. That means today, convolutional and a deep convolutional network for the generator and the discriminator respectively. And for the loss function, it's minimizing the Wasserstein distance and therefore that makes it a w Gan, right? And so for the generator, we have art convolutional network that we can see here. Now for convolutional networks, we have blocks, I call them blocks, right? So convolution bias activation. And then we repeat, that's one block convolution bias activation repeat. Now a lot of times we have pooling and that's considered in a block and we repeat that, but not in this case because for DC gans, they found pooling was not a good thing. So we have a convolutional layer, we have a bias, and then we have an activation function, which is Relu, right? And then we just repeat that over and over again.

Speaker 1:          20:06          Convolution activation, bias, repeat convolution, convolution bias, activation, repeat. And we do that for five or no, six layers. So it's a sixth layer, convolutional network, and that is our generator. And at the very, very end we have this 10 h function that's going to squash the output and then we return the outputs of that. Okay, so that's for the generator. Now for the discriminator, we just flip it, right? It's, it's flipped the other way because we are trying to discriminate, given an image as input. What is the output probability, what that image is real or fake. So we say convolution activation, bias, repeat. And tensorflow has these functions for each of these operations that are apart of convolutional networks. Thank you tensorflow. So then we do it again. Convolution activation bias, repeat convolution activation, bias, repeat. And we do that for four layers, and at the very end, at the very end, at the very end, we use a sigmoid to squash the outputs and then return the probability values for real or fake.

Speaker 1:          21:10          Okay? So those are our functions for the both the generator and the discriminator. Now in our training loop, it looks like this. We say, okay, well we have placeholders and remember intenser flow, we are building a computation graph that is a graph of where, where data flows through, where the tensors flow through, right? So a computation graph, you can think of a neural network as a computation graph. You can think of a neural network has a function, right? It's a glorified function. And so it's a set up operations, right? So we'll create placeholders in the placeholders are gateways, they're the gateways through which we input data into the computation graph. So what is the type of data that we're going to input? What we're going to input some image from our training dataset. We're going to input some random vector for the generator, right? So we need to create placeholders for both the real image.

Speaker 1:          21:58          And the random input, then we'll have a boolean value just to say, should we train it or not? That's just for us. Now we can say, take our generator and feed. It has its parameters, the random input, right? So we're feeding it a random inputs with the, with the random dimension of that input, and then we're going to say, yes, let's train it and it's going to output a fake image, right? It has nothing to do with the training data sets, right? It's only going to learn to morph, morph that, uh, input into, it's only going to learn to morph that initial random input into something that looks like the training data because of the optimization scheme, which is backpropagation correct? So we're going to, we're going to output a fake image. And then for the discriminator, we give it the real image rights and it's going to output the real result.

Speaker 1:          22:45          We're also going to give the discriminator the fake image and it's going to output the fake results so that the probabilities of both, and we're going to use those, the difference between the fake result and the real result as our loss for the discriminator to update that. And for our generator, it's going to be the fake result. Okay? So remember there are two distinct different optimization schemes happening at the same time as these networks are adversaries of each other. And we're going to use the, uh, and we're going to use the rms prop a optimizer for both of them to optimize both to rms. Prop is a type of gradient descent. Okay? There's, I also have a video on all the different types of gradient descent out there. It's called which activation function should you use? Saroj search that on Youtube. Okay. So we have two distinct optimization schemes great into San for both of them, Aka backpropagation.

Speaker 1:          23:39          And then we have a bunch of, uh, you know, check point in restore variables, all kind of boiler plates. And then let's get to the good stuff, our training loop, right? So in our training loop we're going to stay for a number of epochs and for our batch size that we predefined for given number of iterations, let's update the discriminator and then we'll update the generator and to update them. What we mean is that what we mean is we have a tensor flow session and inside the session the graph is initialized. We'll, we'll feed it both the trainer and the loss function has, we'll feed it, will feed it the training noise and the training image, right? For the, for the discriminator and for the generator we're going to feed it the training, the training noise for the random input. And so we feel when we feed both of those in and we are, and we're also saying, well we know we want to optimize both using this, it's going to optimize both given both of those distinct inputs at the very end we can check point and save our model every 500 epochs and that's it.

Speaker 1:          24:35          And then I also have this testing function here that I've commented out, but if you want to test it after training, which you should then go ahead and comment that out and then you can start Jen, that's how you're going to start generating the Pokemon after you're done training. So I also want to say, do not, do not attempt to train this thing on your Puny Cpu, on your Mac book or whatever you have. Um, I, I tried, it's going to take, you know, 24 to 48 hours. If you try to train on a CPU, use Amazon [inaudible], use Floyd hub, use Google cloud. You can get free credits for this, for a free trial, but use a GPU. It's a hundred x plus times faster than the CPU. Deep learning is meant to be run on a GPU, unless of course you have a deep learning machine, right? But train this thing on a GPU and you can do it in three to five hour. If you like this video, please hit the subscribe button. And for now I've got to go find more gans to train. So thanks for watching.