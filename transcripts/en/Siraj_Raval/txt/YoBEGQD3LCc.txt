Speaker 1:          00:00:03       And today we're going to build style transfer using just tensorflow. Nothing else. So let's answer some questions. Five minutes, five minute Q and a. And then we're going to get started. I need myself. All right, who's in this chat room right now? All right, well we got Dan Shiffman and here from the coating train we got Fernando, we've got some cool people in here right now. We've got some celebrities, guys, we've got some celebrities in here. So Lao who did up Chez Jack, I everybody. Wow. Dan was actually the first one in here. I'm honored Dan to see you here. Uh, we, we watch each other's livestreams occasionally get, get inspiration high from Germany. We've got a bunch of people here. So let me answer some questions. Hello from Moscow.

Speaker 2:          00:00:53       Okay,

Speaker 1:          00:00:54       we are going to do the math behind style transfer guides. It's going to be awesome and we're going to really understand exactly what's happening under the hood. No care os it's gonna be awesome. So check out the link in the description to fall along with my notes. Usually, I wouldn't, you know, show that, but I'm going to show it right now. Hello from the Netherlands. Parvez Arvind. I'll say three more names and then we're good to go. Lunchtime is perfect timing, isn't it? Uh, Drma a Sam and then one more name. We'll guerrea a buggy out from Stockholm, Sweden and of course all these other people. Okay. So give me up with those questions guys. Cause I'm so excited to actually build this thing. Okay. Because I've been looking at my notes and trying to figure out exactly how to explain it so it's gonna be awesome. Uh, we got Granada. Okay. Questions your way. Okay.

Speaker 2:          00:01:41       Okay.

Speaker 1:          00:01:41       Hit me up. What your best questions as they say. As I say right now, hello from Israel. Is tensorflow flow a good library for a deep learning beginner? Yes, because it has the most documentation. It's got the most headway in terms of all the other deep learning libraries. There's so much content creating created around tensorflow, not even just me, like other people that yes, I think it's the best way to get started and it's, it's right now, right now it's the industry standard and see how that, what happens in the future. What is the meaning of style transfer? Style transfer means it's we frame adding a filter, you know, it's like filters, right? Instagram filters. But we framed this as an optimization problem. Okay. In machine learning we try to frame everything that's an optimization problem where we minimize a loss function. So style transfer is an optimization problem where we have two images, we have a styled image, which slate say have Dan go painting and then an image of say whatever we want like yourself.

Speaker 1:          00:02:40       And we're trying to minimize the difference between these two images. Images. That's the transfer. We're transferring the style of this van Gogh painting, all the textures and the waviness and the colors onto this, uh, post painting that or this host image. And we're going to figure out how to do this. Mathematically. It's not magic. We know to do it. All of it. Okay. What is zero shot learning. We're going to talk about zero shot, one shot learning at the end of this course. It's not the bleeding edge of everything. I can't wait to get started with that, but it means, but it means a machine learning with very little data. And that I believe is the future of all machine learning. By the way. There's so much cool shit coming out right now out of deep mines, my homie Oreo and yells it. They had this new paper that basically beat deep queue, which has their own algorithm that we're like cause because Google to buy now with this new model called hierarchical reinforcement learning.

Speaker 1:          00:03:30       So we're starting to see a convergence, no pun intended between different machine learning ideologies like this idea of abstraction between layers and uh, the idea of reinforcement and then combining them together. And then this just, they, they, they were able to, uh, beat Pong in like 10 minutes. It was awesome. Okay. Uh, let me answer three more questions before we get started. How transformative it can be done using tfs tensor flow. We're going to use transfer learning right now. So I'll answer your question as we code. Would this also work with writing styles? I actually have a video on that called build an AI writer. Yes, absolutely. And I'm going to make another video on that literally next week. Uh, can you, can we make a program that extract knowledge from story? Absolutely. Um, text summarization. I will have video on that later on in this course.

Speaker 1:          00:04:18       So Raj, answer my question. If the network is possible to use it in trading, we're not going to use a convolutional net and trading. We're going to use a recurring net and training and we're going to, and I made a video on that. Not this one. This one won't be, where's your girlfriend? No girlfriend. I'm single and happy being single. I have too much to do today. Uh, I'm happy being single. I am a beginner. Go more detailed. I will go so detailed. Siddhartha. Don't even don't even trip. So, Rod, can you recommend ml forum please? Uh, the comments section of all my videos, we are essentially a community guys. Essentially we are a community of 75,000 strong engineers. Everyone from middle school students who are just getting started learning Algebra two PhDs at Google, pushing real research. So we are a very large community and the culture we set up is such that if you ask a question you will get it answered. Okay. So we build on Google inception for this task. Yes, you can. We're going to use a different model called a DGG 16 model. Okay. So one more question and then we're going to get started with this.

Speaker 1:          00:05:21       What is the best area to focus in AI? Uh, so deep learning, but specifically a focus. I mean, you know, focusing, be taken a lot of different ways. Focus for research, focus for production, focus for beginner, uh, focus means just a focus on,

Speaker 3:          00:05:41       okay.

Speaker 1:          00:05:42       Uh, I mean, I mean there's so many things you can focus on, but I would say it's such a generic question, a generative adversarial networks because those are the coolest things right now to me. Uh, all right, so that's it for the questions. Uh, and so now we're going to get started with this. So we're going to today a combo. We're going to build a style transfer and it's not that we're going to build a convolutional net from scratch. We already have that. We're going to chop off the top layer and then we're going to add our own layer on top of that to do, install a fence for it. So let's just get started with this. I will explain things as we go and remember the code is in the description all along with me. It's going to be awesome. Focus. Here we go.

Speaker 3:          00:06:29       Okay.

Speaker 1:          00:06:31       All right. Okay, so let's see.

Speaker 3:          00:06:39       Hmm,

Speaker 1:          00:06:41       let's get started with this. Okay, so what I'm gonna do is I'm not going to code the parts, the parts that are not machine learning. I'm only going to code the machine learning part. So what we're going to do today is we're going to perform style transfer. So let's look at this image for a second and let's go to this link. It's just a paper of a bunch of style transfer images and it's just a way of describing what we're doing here. So the first paper that did this, I talked about this in the weekly video was, oh, let me show myself as well, was the uh, [inaudible] paper, right? A neural algorithm for artistic style. That was much low Anton. That was the first paper that did this. Okay. And we, and since then there've been quite a few papers that have built on top of that, right?

Speaker 1:          00:07:25       So the first one just transferred to style, which is what we're going to do today. We want to transfer the style today. But there were subsequent papers that built on that idea. They did it for video, for example. And what they found when doing it for video was that there was actually a lot of choppiness because think of it, a video is just a collection of images and we want to, and what happened was there was a lot of choppiness between frames. So what they did in the next paper, it wasn't the same group, it was a different group, but for video style transfer was they added a, another loss function called optical flow based loss. So in minimize, not just three or two loss functions. They minimize three or four loss functions. So they're just changing these laws functions together. And what happened was this beautiful video that we can see here, you know, like it's, it's very, you know, you can just think about all the potential this just happened last year. So just think about all the potential it has for artists, for Arthur to creative things. And snapchat now has a real time filters. So this doesn't even take training. You can just do this in real time. Okay. So we've seen a lot of progress starting from that initial paper.

Speaker 1:          00:08:32       And so what we're going to do is we're going to, uh, build style, transferred the original way, the way that the original paper was done, and then we'll talk about all the ways of improving that. So let's just go down to this image and take a look at this image, right? Because these are the, these are the two images that we are using right now. And let's talk about the process of what we're doing. We're going to slice off that top because originally the, okay, so the network we originally used was called BGG 16. Right? This was an image classification, convolutional network, right? So at each layer it would imply a series of operations to our input image. So we can think of our input image as a matrix of values. It's a matrix of value, right? I'll pixel values and we apply a series of operations to that image because at each layer, at each convolutional layer, we have what are called a stack of filters. And these filters are learned over time. And so for Bgg 16, they trained it on thousands of images. So it's already got a bunch of filters at each layer. And at what's happening on each of these filters is it is multiplying those filter values by the input image. Okay. And

Speaker 1:          00:09:46       video is glitching lot skipping a lot of brains. Okay, thanks for that. Let me just remove myself. That's going to help.

Speaker 3:          00:09:52       All right.

Speaker 1:          00:09:54       And so readings from Hong Kong. Okay. So hopefully that helps. Okay. And let me know if this is still glitching. Alright, close up. This other friend, just practicum minimal. Okay. So, uh, we're going to it. Uh, so that's what we're going to do. And so this is the BGG 16 layer. Okay. And so the idea here is that let's, let's, let's also look at, uh, this, this image right here that's helping, right?

Speaker 3:          00:10:21       Okay.

Speaker 1:          00:10:22       So let's take a look at this. So let, let me, let me explain what's happening here for a second. So this is a great, great image right here. So we want the contour lines from the content image. So we want all those lines from the content image and we want the textures from the style image. Okay? Those are the two different things we want. And the way we're going to do that is we're going to add each layer. There are a collection of filters. So what are, what are these filters? Thanks Allen. What are these filters? These filters are three d, uh, vectors. So why are they three d? While they are today, there are a collection of matrices, right? But there are three d because of the, the third, uh, the third dimension is, it's, it's that RGB and it's a collection of them.

Speaker 1:          00:11:06       So we've got like say however many filters we wanted to find, the more filters, the more accurate our, uh, if we were willing to do classification, it's going to be, and these are the filters that we're going to use. And so what happens is at each convolutional filter, we weren't to perform eight matrix multiplication and then a summation operations. And if there, if it detects if it detects the feature and the result is going to be a very large number, but if it doesn't detect the feature is going to be zero. And so we can think of these filters at feature identifiers. Okay. They're identifying certain features. And so what we're going to do for, for this or this is we're going to perform, um, we're going to minimize the loss between the labeled image and the feature. Uh, so what happened actually, what BGG was initially set out to do was he was initially set out to minimize a loss between a label image and a future map output. So at each of these layers, after all of these operations, it's once an outmoded output what's called a feature map or an activation map. It's the same thing, which is just a huge matrix of values. Okay? And we want to minimize the difference between what that feature map is and then what our content is. Okay? So that's what we're gonna do here. And um,

Speaker 1:          00:12:24       so those are our two images, right? We have a content image that's going to be whatever we want to modify and then a style image, which is going to be whenever style that we think is cool that we want to apply to this content image. Lastly, we have this mixed image, which is going to be just an empty random noise initialized matrix that we're going to add to over time. Okay? We're going to use what kind of compute two different style or uh, loss functions. We're going to do the style loss and we're going to do the content loss. And then we're going to combine those loss functions. And then we're going to use a gradient value, which we're going to talk about to update this mixed image. Okay? So that's the high level overview of what this, what's going to happen. And after a hundred to a thousand iterations, this image is going to be mixed.

Speaker 1:          00:13:08       Okay? So that's what the high level is of what this is. Okay? So we're going to start off by importing our for dependencies. Map all live is going to help us plot out what we see tensorflow is our machine learning library num Pi is going to help us calculate what's called the Graham Matrix that we're going to do. I'm going to talk about later all on. And then Pila images is going to show the image that we want to show. Okay? I'm now using tensorflow version a one and everybody else should as well. They have updated it. And the other thing is, uh, uh, I'm using python three so everybody should update to python three. Yes, I finally updated to python three. Okay. And it's really easy to update. Just, you know, Google Update Python Three Jupiter notebook. It's just, um, you know, you could do it in two or three lines, a command lines with Anaconda.

Speaker 1:          00:13:54       So the first thing we're going to do is import BGG 16. This is a 16 layer convolutional neural network. Okay. This was pretrained. Okay. So this network already has all of those filters. They're already has all of those filters and we want to use these filters as tools to help us perform style transfer. Okay. So that's the first step is important. Bgg 16 and never going to download it. And so I'm already downloaded it. It's a big file, half a Gig and it's probably going to take, you know, depending on your bandwidth, anywhere from 10 minutes to an hour to download this thing. But once you download it, you're good to go because everything else is going to be local. Okay. So now the next step is to define our image helper functions. So we've got three of these and you know, they're, they're very, you know, standard load, load the image, save the image and plot the image.

Speaker 1:          00:14:41       Okay. And it's using non Pi to do these three, uh, functions. So these are just standard functions and I'll talk about these later on, but right now, just know that there to an image, save an image and plot an image. Okay? So we haven't done any machine learning yet. Uh, these are just, you know, standard image help her function. So now let's get to the machine learning. Okay? So what we're going to do right now is we are going to define our loss functions. Okay? So before we define our loss function, we want you to find the mean squared error. So let me just type this out and I'm going to talk about what is the mean squared error. Okay? So the mean squared error is a function given input tensors so are in presenters. In this case we're going to be called A and B and I'll talk about what these templates are, but it's just one line of code.

Speaker 1:          00:15:31       And the one line is going to be 10 truckloads reduce mean function. And then it's going to, we're going to take the square value of the difference between both of these tenters. That's it. Okay. So, and then I'll, I'll compile that. Okay. So what is happening here? So the mean squared error is the average of two, uh, is the, is the, is the difference between, it's the average of the square of the difference between both the output feature map and the image that we're going to the army put him in. So let's look at what's happening here. Okay. So it was called, um, BGG 16, because there's 16 layers and uh, you know, more layers mean generally better predictions, but less layers mean less computation times. So it's a trade off. It's up for mean squared error. We're taking two input tents. So one of these tents, hers is going to be our image.

Speaker 1:          00:16:24       Okay. Uh, either our content they made or our style image and we're going to use it for both and the other tents or is going to be the, uh, the feature map. Okay. The result of the, uh, whenever layer that we choose. And so what we're gonna do is we're going to subtract the difference. So these are two matrices, okay. And we're going to subtract the difference, the square of the, okay. And then we're going to square that. And then we're going to do reduce means. So what is reducing meat? Okay, so right. So this just this value right here. Do you have thoughts? Square of the difference between these two tensors is going to be a matrix valley. And then we want to reduce the mean. So what did that mean? So we, if we have a matrix to reduce the mean, so it would just be like this. We're going to find the average of all of those values inside of that Matrix. So for this, a two dimensional, uh, this matrix right here, one, one, two, two,

Speaker 1:          00:17:20       okay? It's going to be a 1.5. So that's the average value of this whole major. So this is going to output a scaler which is a single value, okay? That's what means where an error is doing, okay? And it's a weight. And, and, and the reason we square it so that it's positive. Okay? Uh, we went to a positive value. Oh, okay. So that's what we're going to use as a, as a helper function inside of our content laws and our style loss. Okay? So the first thing we're going to do is write out a function for our content loss. Okay. So we're going to call it create content loss and inside of it we're going to use that, uh, mean squared error function. All right, thanks Erez. Um, so our parameters for this, we're going to be session. Okay. So our are session, the model we're going to use, I need to relax.

Speaker 1:          00:18:07       Okay. I'm trying to talk slower and he's lied sections. So let me do that because at one point I'm height and the other point I want to be as clear and communicative as possible so I will slow down. I'm really trying to improve the speed of my speech. I've gotten a lot of great feedback on that and I'm very open to more feedback. So the parameters for this content laws are going to be the tensorflow session, the model, which is DGG 16, the content image, and then the layer ids, which are the indices for which layers we want to use to get those output feature maps. And these are the, the, the layers that we have decided to use for optimizing for content loss. And these are going to be the higher level layers. Okay. Because those higher level layers in a convolutional nets are akin to the content, right?

Speaker 1:          00:18:52       These are the big high level objects that are inside of an image or like, you know, for a van Gogh it would be the moon and the houses, things like that. And the lower levels are the texture or the contours, like the lungs, quantum may, I actually increases the speed of my videos. Interesting. Okay, well you're in the minority. Okay. But maybe, I don't even know. It'd be cool to get some data. Maybe I can do a boat back to this. So the first thing we're going to do is once you've define HP dictionary, now a Phoenix and is a python dictionary object that's generated with placeholders as Keith's. This essentially is going to generate a set of placeholder values. Okay. And in a place holders are going to be your image, right? So that's it. So what this did is it generated a team. Now you care where the key was, the image and the value was the content image.

Speaker 1:          00:19:43       So we do this every time we want to feed anything into arc computation graph, we feed it in as a dictionary, okay? So that was the first step. And then the next step we're going to do is we're going to define what our layers are. Now we have our model that we defined as our parameter and then we're going to get those cancers, uh, by defining those layer ideas. Okay? So we can play around with what indices we want to use for our layer ids and the researchers in this paper did as well. You're going to get different results from different layers that you use, okay? And this is not to say that these are the best and disease, but they are, there are a set of indices that do give us good results. And abstracting this, uh, question even further, what defines beauty? Can we, can we frame beauty in and of itself as an optimization problem? This is something to think about in the future. How can we minimize a loss for beauty? Beauty is so subjective, but can, can it be objective? So something to blow your mind a little bit. Uh, okay, so those are layers. So the next thing we're going to do is we're going to calculate our values. So these are the output values when we've run our session, given the layers that we've just helped calculated and we beat it in our feet dictionary. Okay.

Speaker 1:          00:20:53       Okay. So what this is going to do is it's going to get me a output values for those layers, for the, for the content image. Okay? So the next step is we're going to

Speaker 3:          00:21:04       okay.

Speaker 1:          00:21:05       Say, okay, so we're going to run the model graph now. Okay. So for the model graph, we're going to say as defaults. All right? See how many,

Speaker 3:          00:21:17       okay.

Speaker 1:          00:21:17       Excellent. Monograph as default. And uh, okay. So now what we're going to do is we're going to initialize our losses. So this is setting, so we can, so we're going to add computational notes to it. And uh, that's what we're going to do right now. So let's first define our losses because we're active. Then I calculate a collection of losses and utilize an empty list. Okay? We'll call it layer, uh, losses. That's the name of our list. And the next step is we're going to move down a little bit. We're gonna say, okay, so for each layer that we have for valued, for each layer that we have, what we you iterate through those layers that we get fined in those indices. Okay? Uh, values layers, okay?

Speaker 3:          00:22:10       Okay.

Speaker 1:          00:22:10       That's where they start contributing to AI is to just start pushing code to get hub that you, that you find fun and documenting it really well. And then, you know, marketing, like posting it to people and sharing it with people. We're all sharing, you know, the line between research and production is quite thin. Everything is really a discovery in and up in, in any, in any way, even as simple hyper parameter change. Uh, since you can result in an incredible discovery, uh, personal, their link is in the description. Okay? So what we're doing is for every layer we are iterating through every single layer. Right now we're waiting to get that value. Okay? So what does that value? We're going to define it as a TF constant with comedic 10, not change. And this is once you, uh, BB, uh, content, images value. Okay. So then what we're going to do now is open to calculate that loss.

Speaker 1:          00:22:56       Okay, now lost. And now this is where we use that mean squared error that we defined earlier. And we're gonna use it to the two in pretenses are going to be the value add a layer and then the constant value. So this is the, the book, the content image and then the, the, the set of features at a layer. Amber, what you minimize that loss. The basketball for deep learning is, um, the deep learning book by Ian Goodfellow. Okay? Uh, no. The one by Yoshua Bengio search Giyasova Nto learning such a great book, especially for the mass loss amines. Right? Air. Okay. So now we're going to say, okay, so these are all of our losses that were calculated and we'll get to append it to that list, that empty Lou when to do it, right? More Games. And I'm a full on game AI next. So we'll see how that goes. I think it'll be very, very popular. So once we have all of those loss functions, like be a good average value, you think that's reduced mean function. Okay? So these are our layers losses and that's after we're done with everything. Okay? And then, uh,

Speaker 1:          00:24:09       we're going to say, okay, so at the end of this we're going to return the total loss. So what this is going to do is this is going to give us a total loss value. And let's talk about what we just did. We said, okay,

Speaker 1:          00:24:20       uh, we're going to get the mean squared or of the teacher had to be in the given layers in the model. So between the content image and the mixed image. And so when this is minimized, it's going to, when we minimize this, it's going to make them mixed image that much more stylized. Okay? So this, this defines our content boss. So ideally we can do the live style loss as well, right? We just take both up, feature the feature maps from a layer and then the, uh, you know, the, the image, uh, where whether the, the style image of the content image and we just calculate the mean square error between them and we minimize that. But we cannot, uh, previous line from the last either a typo. Thank you. But we cannot do that. Okay. So we cannot do that because, uh, for whatever reason, wow. That actually compile without any errors because w okay. So let's, let's actually answer this, why we can't talk about,

Speaker 3:          00:25:21       okay.

Speaker 1:          00:25:21       Right. Total loss. Thank you Allen. So, so we don't do that for style. For style. We add eight, another step called the gram matrix and let's, let's really talk about what is happening here. So the reason we use a gram matrix, so let, let's let, let me show an image of this, this, this, this, this will help. So that's okay.

Speaker 1:          00:25:46       Okay. So this is, so this is kind of scary looking, but don't worry about it. A grime interest. All it is, is we are taking the, uh, measuring the correlation between our teacher chant, like doctors after flattening the featured filter image into vectors. So all this is doing is it's taking, it's, it's, it's the matrix product between our initial matrix and then its transpose, which is just, uh, which is just flipping at 90 degrees. Essentially. We're just multiplying a matrix by its transpose. That's it multiplying a matrix by its transpose that is the grand matrix. Okay. And so that's what we're doing for that layer. Okay. And for our style. So for every uh, style layer that we choose, we're going to calculate a gram matrix. And that Graham Matrix is what we're going to use to minimize via the mean squared error. So let's just start building and then we'll start, start talking about what exactly is happening. So for brand matrix function, we're going to have our input tensor. Okay. So you think better of the dot products for vectors of the feature activity patients have a style layer.

Speaker 3:          00:26:55       Okay.

Speaker 1:          00:26:56       Uh, yeah I did. And it was the equal sign, right? For the total total loss. Okay. So the grand major 10, remember guys we following along and the get help link that I've linked to in the district and it's got all my notes. I'm basically talking through it and, and coding at the same time. So this is going to be a for the tensor, okay. This is going to be a 40 tensor, uh, at a given layer because we have the, we have, right? So it's gonna be the collection of pixels and then the RGB, which makes it three d and it's 40, because we have a collection of those. So we have several of those. So we have a collection of broody tensors, which means it take for the tenser and that's what the shape is going to be. And what we want to do for our style layers is we're going to calculate our,

Speaker 4:          00:27:38       uh, uh,

Speaker 1:          00:27:40       brand matrix hello up Doula. So now we're going to say, let's get those channels. Okay. So channels are just even those. So if you think of, uh, the outputs of each layer, so an output of each layer is going to be, let's see what our history was. CDS, like stacks of layers. Each of these stacks is a matrix and we call those our channels. Each of these is is one of our channels. Okay. So it's the word activation and feature as it's used similarly. So feature map activation happens, same thing. Okay, so back our number of channels. Where we want to do is we want to define these channels and we're going to get the number of Eagle Peter Channels for the input tensor, which is assumed to be a convolutional layer with four dimensions. Okay. And then we're going to say, let's get the matrix, which is going to be the, let's talk about what this matrix and this matrix is going to be. We have a shape, it's going to be negative one. And then number of council, what did we just do here? So

Speaker 1:          00:28:46       hang on one means whatever number makes this state of fit. That's why we put negative one in here. But what we're doing is we're reshaping the tensor. So it is a two dimensional matrix. So it basically just a flattens the contents of each of the feature channels. That's what the, it's flattening the contents of each of the feature channels so that we can multiply it, uh, in a second through ts matrix multiplication function. We are, we are making it, it's a kin to normalization. Okay. It's, we're taking both of these values between art style, uh, and our, uh, mixed image and we are normalizing it. You think this reshape function such so that we can multiply it in a second. Okay. So let's get back Graham Matrix now. Okay. So we're going to use 10 truckloads matrix multiplication function to do this and we've got this great transpose function. So we're going to remember it's a grand matrix and just a matrix multiplied by its transpose. So the same matrix will multiply by its transpose and then

Speaker 2:          00:29:40       okay,

Speaker 1:          00:29:41       okay. So there transposed

Speaker 2:          00:29:42       by the Matrix and remove it's comma cause we don't need it. And then that's going to be art remix and then we can return that. Okay.

Speaker 1:          00:29:57       Uh, so it style transfer is an extension of a convolutional nets. Uh, it has uh, Ganzer a whole different ballgame. There are a whole different ballgame and we're going to talk about that later. But before we understand how gangs work, we have to understand how a transfer learning works and how convolutional nets really work. And that's the goal of this. Okay? So that's it for our grand matrix. So that is the extra step we add freestyle loss. Okay. So let's see. Let's actually compute style loss. All right? So what we're gonna do is we're going to compute soloff's now given that Graham Matrix function that we used earlier. So the input to this are going to be the same basic format that like we did for the content lots, except we're using a solid image as our input, not at content image, right? And we're adding this extra step, which is Graham maker steps. So we'll talk about what this looks like when we get to it, but we're feeding in, remember the dictionary and it's creates placeholders for our, uh, feed dicks and

Speaker 2:          00:30:55       okay,

Speaker 1:          00:30:56       went to, and this is going to create a placeholder value for our style image. And then the next step is to get the layers to our layers are going to be whatever we got from our layer id. So whatever we defined and we defined these earlier or we will define these layer ids, right? These are essentially a can to hyper parameters for this or this particular optimization problem because as we tune these are the results will be different, right? So,

Speaker 2:          00:31:22       okay,

Speaker 1:          00:31:22       because there are layers. So now we're going to define our model. Okay. So it's, you know, very similar. We're going to define our model as defaults and

Speaker 2:          00:31:33       okay,

Speaker 1:          00:31:34       so we're going to define our gram layer. So this is where our, this is the line that is difference comparatively compared to the content, uh, Blas. This style loss means we have to take the grand maple scenes. That is a difference here. We're taking the grant-maker seeds of each layer. We're not just saying here's the raw layer of value, which is just that matrix. We're saying get the gram matrix on that layer. So, and also, you know, to, to give some backup to this gets, he's the guy who initially made this when asked why the grand matrix in a talk, he said the grant Matrix and codes, second order statistics of the set of filters. Oh, I'll talk about the noise and get a second Javier. But basically it's looking at it from a higher level. So it sort of mushes all the features at a given layer, tossing spatial information aside in favor of a measure of how different teachers are correlated. So by, so Graham mates, your seats essentially or like it toss away everything that's unnecessary just to focus on this style. And, and this works for style, but not for content. It's lower layer. And

Speaker 1:          00:32:37       I'm sure there's more theory why reading the Grand Matrix, but, uh, that's the best I could get out of Getty's. Um, and you know, it's better than just saying it just works. Right. Which is a lot of machine learning. Uh, right. So, so that's the grim, uh, layers. And then we have, where were we again? So I went up for a second and go back down. Okay. So where were we? We were rights here. So we said that we're taking our grand layers and we were feeding an art dictionary. So for feeding at that dictionary that we just defined edict, and we're saying that, uh, we're going to define our layers, losses. So I'll just say now we're defining that MP a list that we're going to add all of our layer losses too, right? So, so that's that. Those are our two lines before we get started with our iteration periods.

Speaker 1:          00:33:30       So for our iteration period, what we're going to do is we're going to say, okay, so for all of those brand makeup seats, at each layer, we're going to compute the loss function so that we can minimize it. And so we're happy. We have to, um, input here the value, which is a marketing edge, and then the grand layer, which is the grand later. Okay? So the value is going to be constant because it's a TF dot. Con because we don't want it to change, right? We, it's not a variable because we don't want it to change. It's going to stay the same. And, uh, now we're going to just talk to the loss to loss is going to be the mean squared error between, and now this is the important part. The important part is we're calculating the mean square error of the Graham layer and then the value constants.

Speaker 1:          00:34:18       So this is between the grant matrix and the value of a brand mantra matrix when inputting a style image. Okay? So that's between each of the layers and the input style image layer. Okay? So then once we have all of those losses, we're going to add them all to our wrist style list. So layer equal layer the losses at each layer. So you know, we're combining these losses and we later combine the combination of those losses between, right? So it's like combining and then combining again we'll, we'll talk about the second step of combining. But this is the first step of Kumbaya. We're combining losses for a style of and for content. Okay, so we're appending whatever we calculated at each iteration here. And then now we're going to calculate the total loss with total loss. Total loss is going to be the going to reduce the mean queen players so that we're going to take the average of all of those losses. Remember, there's just takes the average of every single number that we have in our matrix and then we're going to return that value. Okay. And that's our style, floss. And we get, and this is a scalar value, it's a, it's a single number. Okay. The scalar about him. So that's our style loss. And then what do we got here? We've got some invalid syntax. My favorite, my favorites. Okay.

Speaker 2:          00:35:47       Alright. Comma. Yeah. Okay.

Speaker 1:          00:35:52       So that's what we did for that. And uh, right. So feed dick, model dot creates. Okay. Model dots. Create. See dicked.

Speaker 2:          00:36:04       Yeah.

Speaker 1:          00:36:05       Image style. Image. Uh, what is it saying here? Oh, brand made friends. So for Graham Matrix Graham layers, we have, see Dick, where was I? So many things to keep track of your Oh, right. That was what it was. It's okay. So, um,

Speaker 2:          00:36:23       okay,

Speaker 1:          00:36:24       let's see. Oh, right, yeah.

Speaker 2:          00:36:26       Great.

Speaker 1:          00:36:28       Brand Matrix layers and then edict or where we do our majors layers, four layers.

Speaker 2:          00:36:35       Oh, right.

Speaker 1:          00:36:39       Let's see. Hold on. I had a lot of code here that I'm looking at him. Hold on a second. So Graham layers are going to be the gram matrix layers, four layers in theirs. Oh, so what's actually, there's an extra line here that I forgot about. So

Speaker 2:          00:36:59       not about certain line here.

Speaker 1:          00:37:06       Hold on. There's a certain line that I'm missing here. Ah,

Speaker 2:          00:37:09       okay. I'm just remove these right here.

Speaker 1:          00:37:16       Great. So now, so there's actually one more loss. I lied. There's one more loss, but I lied because it's very, very trivial. And we used, it was using the paper, but it's, it's not, you know, it's very trivial and it's, it just starts to confuse if we, if we talk about it at the start, but we're going to talk about it now. So this is just, this is the de noising loss. And so what happened was when, when they, when they didn't do this last one, they just computed the style and the content was, the image was okay, but when they added this denoising loss, which we can also call the total variation loss, it's also called that what happened was the results improved. So,

Speaker 2:          00:37:53       okay.

Speaker 1:          00:37:54       What it does is it calculates the difference between the, so it shifts the input image by one pixel on the x and y axis. And then it calculates a difference between the shifted image and the original image. And so the absolute, and we, and we use the absolute value to make it positive. Okay. So let's talk about this. So this is the one more lost than to never going to add. Very pretty a lot function. It's going to be, you know, two lines of code, but a necessary to just know slightly improve our results. And you know, there could be even more lock function that we can think about and um, great. So there are 16 layers here. Okay.

Speaker 2:          00:38:30       Okay.

Speaker 1:          00:38:30       Or is the absolute value, um, for this and,

Speaker 2:          00:38:35       okay.

Speaker 1:          00:38:36       Uh, in fact I can just, you know, I'm just going to paste this in here because we're going to get right to the meat of the code. Now just talk about it since it's just two lines. It's got a lot of, uh,

Speaker 1:          00:38:49       medical things here. So we are, we are, what we're doing here is we are, uh, taking the absolute value to make this a positive value, right? And we're going to calculate the sum of the pixels in these images and basically it can help suppress noise in the mixed images that were generating. Okay? So we'll calculate the sum of the pixel values between the shifted image and our original adage and the variation. And the variation is referring to that shifted image. And the original image is it removes the noise and the noise is like the blurriness. It's not that clear. And by shifting it in my, you know, minimizing it, we're kind of like recreating this blurriness and we're, we're saying, you know, we could even shift it more than one pixel line and try that. They didn't try that, you know, shifting, you know, we could do other, um, you know, artificial noising step, like, what can we do to like, you know, mess up this image in whatever way.

Speaker 1:          00:39:41       Okay. And then we're going to optimize for minimizing that so that our actual value is smaller. Okay. So, so now what we're going to do is, um, how has the, how has the frame rate, by the way guys, I definitely tell me in the comments how you feel about the frame rates. Um, I'm trying to improve my life live streams. Okay. So now we're going to get to the good stuff. This is the mean of the code. This is where it all comes together to style transfer algorithms. This is where we build our model and we're going to use the loss on that. We calculated, okay, so let's talk about what these parameters are going to be. So the creditors are going to be uh, uh,

Speaker 1:          00:40:23       the text is blurred. Pretty cool frame rate. Thanks sir. Okay, framer seems good. All right, cool. So how much, so how much do we want to weigh these loss functions is a good question. Okay, so you don't like this. This can be played with, right? So what they did, so you can see here that the weight style, the wait for the style is going to be 10. And so it's almost, you know, it's, it's like eight times more than the weight for the content. And we can modify these weights so that we can, we've got 1:44 PM over there. Okay. Um, very the text, I'll make this bigger. Yeah. Seven 20 Pete by C bomb. Okay. Interesting. So we've got all over the place. Um, okay. So, so we're weighing this style way more than we're rang the content. And it doesn't mean that these are the best weights and in fact, we can have learning out rhythms for learning the best ways we could learn the best weeks later.

Speaker 1:          00:41:15       Right? So that's that. Those are the weights that we're going to use. And we're going to define these weights, uh, here in a second, but right. So let's go ahead and just do our first cap. Like, assuming that everything else we did was just a helper function for this. This is the real deal. So we're first going to define model. Okay. It's gonna be easy. 16. We imported it. We're good to go. Right? 16 layer, convolutional net, fully connected layer. At the end, it's thinking like, oh, I'm going to be used for classification. That what is thinking right now? Okay, so now we're going, but it doesn't know any better. He doesn't know any better. We're not going to use it for classification. We're going to use it for style transfer itself. We've got our graph and we're went to eat in our model. Okay? So that's the session. Remember the session always encapsulates our computation graphs. Okay? So then we've got these print functions here that we're going to use to help us see what's happening. And I'll just paste these in because they're really just help her, you know? So now, now is the, the good stuff. So cures are lost for our content. We, our first loss function and we defined the, um, we defined find the

Speaker 1:          00:42:29       lost function, right? So the session is going to, we're going to give it the session value, we're going to get a model value and we're going to give it a content image and the layer ids. Okay? So

Speaker 1:          00:42:46       no buffering dot. Interesting. Okay, good thing. A good thing. All good things to consider. Okay. So once you give it that content, obviously, because we are trying to minimize the loss for the contents and then we're going to say other layer id. So these are the indices. So right. We just feed in those layering the teas and if went to, uh, calculate the difference between the raw activations to after given layers and our content image. And we think of that as a matrix, just the rock divisions, activations, no hidden step and the value, the value is our last content. And remember this is eight scalar value. Okay, this going to give us a scalar value. The next step is to calculate our, our silos. Okay, so for our style loss or is that extra step, the extra step is going to be the brand to these calculation. So it's not that we just give it those raw activations. We also, once you give it the, we take those rocks, the patients and we calculate the gram matrices from that. Okay. And the same thing, but on the, you know, in this and this, uh, you know, function. We're not, we're not talking about the grand mason city's, that's all happening internally because we could find that on the outside it looks the same. We're just good programming, right? We, we, we want it to look the same because I'm

Speaker 2:          00:44:11       okay.

Speaker 1:          00:44:11       There was no reason for us to expose that grant makers brown matrix or calculation, uh, externally for, you know, this, this function for re re usability. Okay. So that's for our loss and that's where our style, and remember we had one more very trivial loss function for de noising. Right? So for noising we'd say we just said, well, take our model then remember to the noises so that, you know, it's just so little less per worried, you know, a little catch up at the end. Okay. So those are our three loss functions. And now what we're going to do,

Speaker 2:          00:44:44       mmm.

Speaker 1:          00:44:45       Is we're going to adjust, be here. That's a great question. Why don't we use the gray mates and sees for content loss. Um, so, uh, so it's a second order updates. It's a second order statistic, meaning for, for stock. So style is actually, you know, it's, it's, you can think of style as more abstract and content content is something we can point out and say that is content. Like for example, the, what is the famous content like the, the, the clock that is bending in the Dolly picture dollies. Um,

Speaker 1:          00:45:21       Geez, where's my mouse? The Dolly pick, you know, with the, with the clock that's bending. That is, that is content. That is something we could look at and we can definitively say that is the content. So we can take the raw activation and say, you know, it's just literally there. It's, it's, it describes that that object, but style is much more nuance. Style is much more embedded and ingrained throughout every layer. It's not just like at one way or we see, oh that is the, that is where the clock is. It's, it's embedded in every layer. So by taking the brand matrix, we're kind of abstracting that. We're taking another layer of abstraction to say that we want to style is even more abstract and content. And so Graham major, she's helped us create that extra layer of abstraction. Okay. So now we're going to create those adjustment weights, right? So for our content and our style. So it's basically just the same thing here. So we'll just, you know, copy and paste the same thing over and over again for three lines. But basically it is creating tentraflow variables for adjusting the values of the loss functions. Okay. So we went to adjust how,

Speaker 2:          00:46:29       uh, uh,

Speaker 1:          00:46:31       we want to adjust how, uh,

Speaker 2:          00:46:34       okay,

Speaker 1:          00:46:35       how much we want to weight each of them. Okay. And so now what we're going to do is we're going to initialize them, right? So we're, we say we're gonna say session not run. We want to commercialize this value so

Speaker 1:          00:46:48       isn't the actual run step. And the next step is we're going to actually adjust them because we haven't actually adjusted them. We've just declined them. Now we actually adjust it. Now we actually, uh, Skoll kinds were, can definitely be applied to audio and wavenet is a great application of that. Um, and there are, you know, there's a definitely a lot of discoveries to be made around speeding up wave net. Okay. Um, but the idea of transferring the style to something can be applied to all forms of media, not just images, audio, video, uh, you know, all sorts of things. But these are updated values. And the reason we're using this one e 10 value is because we, it basically is just taking the reciprocal values of the loss functions. And then it's going to, uh, using this small value, one e minus 10 is added to avoid the possibility of division by zero.

Speaker 1:          00:47:46       So if Los content was equal to zero, this would just throw an error. So that's what we add is very small. You can almost think of it as a bias vector, but for adjusting, adjusting these, uh, these, uh, uh, wait values. Okay. And so now we're going to actually combine those losses. So now that we've defined the law, says we defined how much we want to weight each of them. Now we finally, we combine the losses. One big, huge rocks function, and it's a weighted loss function that we will minimize to generate the mixed images. And the reason we're multiplying these loss, uh, values with the reciprocal adjustment values that we tabulate it up here is because we can use the relative weights for the loss functions that are easier to select the exact choice and the co the client later the style and the content labors. So this is our combined loss function and Guinea. And so give it a five minutes and we're going to get right to the, um, we're going to get right to the actual output value. We're almost there guys. Stick. You guys are very patient for sticking with us, right? There's a lot of stuff we're talking to now. So now we're going to calculate the gradients. So let's, so the gradient is a great thing to talk about, right? So we have our combined loss functions and now we're in these tension built dean radiant function to get full value of Rep. It's not just one

Speaker 2:          00:49:11       you,

Speaker 1:          00:49:13       uh, we're using multiple gradient values and we're going to get the combined loss given our model and could value and they'll combine loss. So this is what it's going to do is going to get the gradient, the combined lost function with regard to the input image, which is the mixed image. So this, this wants to minimize difference between the mixed image and remember that mixed images, just the blurry, you know, uh, nothing image, right? It's initialized random noise image and that's what we want to output as are fully know output value that the transferred style image and the gradient is going to get the um, it's a mathematical function. So the gradient value is going to give us a direction. And so, uh, a great image of this, I haven't. Great. Um, I'll talk about building an AI writer also. I'll do another one.

Speaker 2:          00:50:08       Uh,

Speaker 1:          00:50:10       you know, generates Shakespeare or paintings or Shakespeare or text check out that is not, you have to create an that's Nike and some dude. Um, what's like that gradient act sigmoid. Basically, I'm just trying to visualize what this looks like, but basically we are taking the,

Speaker 1:          00:50:29       sorry. Okay. So we have a sigmoid function and we, the gradient is like if we were to take a line, I'm just showing this with my mouse up or down. That's it. Up or down. We take numbers and do we want to push these numbers up, we want to push these numbers down. And so that's what the gradient is doing. So when we take these radiant values, okay, we take these Brady and values and we multiply it by weights are the weights in this case are going to be the image. It's going to update our image in a direction which is up or down. That is going to minimize a loss. Okay? So the gradient is a collection of those gradients for each of those, uh, feature vectors. Uh, for each of those, remember it's a, it's a stack of features. It's a stack of features, okay? To take the list of 10 search, radiant was important. Super important, pushing numbers up and down so you didn't get that right. So these are all collection of scalar values. Okay? These are a collection of scalar values. And when we take these gradients and we, uh,

Speaker 1:          00:51:35       these are a collection of scalar values and, um, we're going to use them, not here, but in a second. We're going to actually, I'll, I'll talk about them when we, when we get to number, basically, uh,

Speaker 1:          00:51:48       I'll talk about them. Okay. So this is going to give us this list of scalar values. Okay? That's what greatness is doing with regard to the loss function. And we'll talk about what it's doing with this scalar values in a second. But just think about it as scalar values. Right now we're, we're about to get stem. So now we're going to initialize that mixed image. We haven't actually initialize that mixed image yet. Now we initialize it, right? So we're using random, the num pies, random dot rand function to initialize an empty matrix of just random values. Okay? And we're going to update its image to us through iteration. Okay? So, so now it's time for our training staff. So for every, so for number of iterations that we have and we have quite a bit of iterations to do, you know, anywhere from a hundred to thousands. This by the way, it's going to take like an hour on my laptop, which is a Mac book, 2016 forced touch bar, which sucks. I need a better computer anyway. So, um, great. So now, so the first step remember, is going to be to create the feed dictionary. All right? So we're going to create their feed dictionary and then given that mixing the director, we dress initialize them, makes an image, create that feed dictionary. Okay.

Speaker 1:          00:53:11       Now what we're going to do is we're going to calculate the value of the gradient as well as adjusting the dial you, so it's breaking up from time to time. It could just be you. Okay. Interesting to um, notes. So we got a couple minutes left. So what I'm going to do now is I'm going to talk about the gradients and so in order to focus on the gradients that we're going to do, paste the rest so that we can focus on the Greenheck values. Okay. So, so we have those gradients, right? There are a collection of scalar values between the mixed image and our combined loss function. So, you know, think of them as like, there are the representation of like the what we want to use to help us, you know, increase the style in our, that mixed image. What, how do we stylize that image to the gradients are going to give us, tell us how to update our image. Okay. A gradient is a slope value. It's a given, a sigmoid function. So if it would just, you remember, so for you know, fee for next, we took the derivative a sigmoid and it gave us a, a line. A straight line is great. It means slope. It's just a straight line. And the line either po points up or at points down, this is a great image. Either points up or it points down.

Speaker 2:          00:54:40       Okay?

Speaker 1:          00:54:44       Uh, so it's, you know, it could be plus two or minus two, right? And so,

Speaker 1:          00:54:49       so when we take these gradients and we multiply it by that image, it's going to transfer it. It's going to update the image in some way. And every time we, every time we minimize our loss, the gradients is going to be, it's going to be better. And by better, I mean it's going to help us. Uh, it's going to be, it's going to minimize the difference between the output image and our style image, right? It's gonna every time it's going to be minimal, minimal. All right? So eventually when it's zero, a local minimum, uh, the, the output image is going to be the most stylized that it can be for whatever, uh, however we frame this problem. So this step is going to remove the single dimensional entries from the shape of an array so that our greatest value is going to be reduced such that it's going to be the same size so that we can multiply it by our matrix.

Speaker 1:          00:55:36       Okay? So then we're going to scale the gradient. So what do we, what are we doing and why are we doing this? This is the same thing as learning rates. So it's the ratio between the weights and the updates. So if, if, if, if we don't scale it properly, then it's not going to converge fast. But if we, if we don't scale it fast enough, then it's not going to converge ever. If we steal it too. No, sorry. If we scale it too slowly, then it's never going to convert. If we scale it too fast, there's going to overfit so, so we have to update not just our, um, gradients, but our, our weights at each layer so that it's going to converge properly. That's what this step is. And then we're going to perform in gradient descent. And so this is the actual gradient distance part, right?

Speaker 1:          00:56:18       This is what I'm talking about. We take those beck gradients, which is a scaler, and then we multiply it by the scale of value that we calculated here and it's going to give us that output mixed image, which is going to suck. It's going to suck at it. It's not gonna look like anything. But the more we update this gradient, the more this image is going to look doper and doper. Okay. Like more and more stylized. Okay. And so that's gonna be our mix image. And then we went to clip all the values that are not between zero and 255. So everything more, we're going to clip it because we want this to be an art be values. And then we're going to print out the number of iterations and we're going to plot them. Okay? So that's what that's going to do. And then we're ready to, uh, show you know what we're going to, so that's that part style transfer algorithm. Now we can print out load images. Not that fine. Yeah, I probably didn't. Oh, I need to find the load. It makes function earlier. It's all good. I didn't compile that earlier. Let's um,

Speaker 1:          00:57:16       we just show this. Okay. So is it actually like if I were to actually run this right now, it would take a lot of computation power, which would lag the hell out of this livestream. And I definitely don't want to lag this slide stream further. So let me talk about this for two minutes and we're going to do an ending to a, okay. So where were we? Okay, so remember, so Devin d a style image, the content image. And then the indices or whatever we want to use, like what are the layers we're choosing for this? And then how much went away. Each of those loss functions, it's going to slow, minimize that loss. So this, this, even this first iteration is going to take like, like five minutes. So even in this first iteration, you already see that it's getting better. Like, like already you went from nothing to this in one single iteration. Okay. And after the others, it's just gonna be, so it's kind of like an ex, uh, um, uh, negative exponential curve. So, uh, it's, it's super good at first and then every subsequent iteration is going to get, uh, you know, the, the, the, um, improvement is going to get smaller and smaller over time. Okay.

Speaker 2:          00:58:21       Hm.

Speaker 1:          00:58:22       Okay. So let me do five minute Q and a to end

Speaker 1:          00:58:28       and then we're going to get started. Hi Guys, I'm definitely going to improve black. Don. I really want to improve these livestreams and I really want to, and I will, I promise that. Well, because I love you guys and want to make this better for you. Okay. So ask any leftover questions you have from this live stream, which I'm going to read out very clearly, but the people who are watching this later. And then I'm going to ask, you know, whatever other questions you have as well about life or Amin machine learning. So Raj, what do you think of neuro evolution of augmenting topologies needs as a genetic algorithm? And do you think you will do a video on it? Sometimes. So genetic algorithms I've done a video on genetic algorithm is called um, genetic algorithms learn python for data science. They're really cool. The idea of evolution, Darwinian evolution, it's a really cool, and it's, it's how we've evolved, right?

Speaker 1:          00:59:14       But in practice, in practice, we don't see, um, you know, as good results as we see with you know, these with deep learning. But you know, we might later on we, you know, who knows. And I think that there are ideas from Jeanette genetic algorithms that can be applied to deep learning. Remember, what we're seeing in the real discoveries that are happening right now is where we're taking these abstract ideas, this idea of hierarchy from deep learning. This idea of an agent of an other, of reinforcement of trial and error for reinforcing. And by combining these ideas, we're going to get even better results. So I think genetic algorithms have a lot of great ideas to give to us. Okay. And uh, I will, I will definitely do more videos on genetic, how returns in the future, how to communicate with objective c. Uh, don't use objective c, use swift, how to improve because apple is now maintaining that.

Speaker 1:          01:00:06       And actually swift is great for even server side code at this point, not just mobile apps. How to improve CV for Google internship. Mccool I have a video on job interviews that I'm going to release very soon either this week or next week. I already have the footage. I just need to edit and release that. How close are we to humans to solving intelligence? My best guess is five to 10 years. Uh, unless some kind of cataclysmic, you know, anything bad happens that prevents that, the rate of discovery that's happening right now. Are there starter videos that you talked slower in? Uh, I'm just going to keep talking slower and slower. Okay. Uh, okay. So a couple more questions. Do you think an evolutionary slash genetic algorithm to create the structure of a neural network? Absolutely. I mean, absolutely. They've, we've seen them creates a bunch of other things as well. I don't think I've ever seen anyone try to do that. So that's a great project to try to recreate, let's say a simple blue layer before neural networks. You think genetic algorithms, that would be a great project.

Speaker 3:          01:01:15       Okay.

Speaker 1:          01:01:15       What's 10 years is very optimistic, but just look at like the paper that was just released yesterday by deep mine when they already beat their DQ network. That is, can be, uh, generalized to a bunch of different games. Um, what else should we learn in AI ml and d l after ml and DL? That's really the, the bleeding edge right now. Um, but I would say if you want to go just even further like that, the real future one shot learning for specifically probabilistic programming. Okay. Probabilistic programming or you could call it Basie and program learning. It's the same thing. You have any examples of using a neural net on random images from the internet? Uh, yeah. Build a tensorflow image classifier in five minutes.

Speaker 3:          01:02:00       Okay.

Speaker 1:          01:02:00       Okay. How do you keep up with all the white papers that are released? Andre Karpati has a great website called Archive sanity.org. I've Ar Xib, sn, s a n. I, t y, Google that and then a n d. R. E. J. Great. It, I, that's my tool. Okay. I'm going to answer two more questions and then we're done. If it's only five to 10 years, should I start a computer science degree next year or should it rule on my own with you? And did your you Udacity roll on your own? Anton roll on your own. Remember to have your get hub full upgrade projects because get hub is the new resume. Okay. And practice your interviewing skills and a whole video on that very soon. Okay. Two more. Okay. So actually two more questions. Additional resources for style transfer. I will link those to those in the description. Also my last video last week, the video, cause that's some great resources in there. One more question. Okay. What do you think about the new,

Speaker 2:          01:03:02       no,

Speaker 1:          01:03:08       uh, I want to make it a good question.

Speaker 1:          01:03:15       What drives you? Shabaam Goop Jab I. Okay. So what drives me to be totally honest with driving number one, one anything is to solve intelligence by teaching you guys, you know, whatever I, whatever, I know you guys can help us all because it's a, it's a shared journey. We're trying to solve intelligence so that we can solve everything else. We have so many problems in the world. If you've traveled to developing countries, you see just the, the scope of problems is so immense and it is just, I am very impatient and I want to solve all these in my lifetime. I want to see it all sorts and the best way to do that, it's to make better and more intelligent software. It is the best way to do that. Okay. And that's what drives me. And also just, I've always wanted to do great things and this is a great way to do that.

Speaker 1:          01:03:59       And also I just, I want to be out there and I went to, I feel like I, I feel like I have a responsibility to read. It's like this internal, like I, I just feel like I need to dream big. And we, and, and, and I went to and see you guys become leaders as well because you are all the real leaders of the future. I'm here to help you guys become leaders. Okay. So that's what I'm here for and I will, I will keep going. Oh, keep going. No matter what happens, I will just keep going and,

Speaker 1:          01:04:28       oh, keep going until my last breath. Hopefully there's no last breath because we saw, hey, I, and you know, we had this singularity. Okay. Is Deep owning a fad? No, it's not a fad. It's a, it's, it's a next step in, in a series of discoveries and I'm sure the next thing is going to be even better. But Dave, uh, in a way, it's not like it's the end goal, right? There's going to be more accurate, but it's a part of this thing about I'm not working at Twilio. Um, okay. So, uh, this wonderful time. All right, so that's all for my questions. Thanks guys for being here. I have a video coming up this, uh, on Friday, like always. And uh, okay, so that's it for this livestream. Let's see. Anything else I want to do now? I'll just say I'll say the rapper next time. Okay. So love you guys remember to, uh, links are going to be in the description. Everything's going to be awesome. Remember to join our channel. All right, love you guys. And uh, thanks for watching. For now I've got to go drink some coffee and then transfer the style of the coffee into my brain and just minimize a loss between my sleepiness and wakefulness. So, oh, freestyle. Oh, someone said freestyle. So I'll do it. Cause you guys know I love to freestyle. Um, uh, Prince Albert copied. So,

Speaker 5:          01:05:40       okay,

Speaker 1:          01:05:40       I love to drink coffee. No, no, no, that's, that's what I want to do it about machine learning. So, uh, so what's a, what did we just talked about? We talked about a style transfer. So style. Okay. I love to talk about style. I take my boat, drive down the road, but now I do it in a spit out bile. That's not about it's flow. Anyway, that's it for this time. People laughing cause I'm trying to freestyle. It doesn't matter because I'm trying to go with the Nile River. You got ancient secrets that the Egyptians made, but now we got even more men, bars, bars, bars. Okay. Woo. Freestyle transfer. Anyway. All right. Thanks for watching guys. Okay.