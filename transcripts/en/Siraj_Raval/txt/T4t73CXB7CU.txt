Speaker 1:          00:00          Hello world. It's a Raj, and this is a guide to core ml. That is Apple's new machine learning framework for all of its ios devices. We're going to go through the development workflow of what it's like to work with core ml. We're going to talk about its API, we're going to talk about its features, we're going to talk about its pros, we're going to talk about its cons, and then we're going to build a spam classifier for text messages on Ios, on an iPhone device to really implement what we've just learned. And so that's what we're going to do today. I'm very excited for this because Cornell has a lot of hype, right? But there's, it's not very well understood in terms of what's happening under the hood. So let's talk about that, right? So apple released this framework this year at WWDC with a lot of hype.

Speaker 1:          00:46          There's always gotta be high when apple does anything, right? And so actually let's start off with the demo, right? Let's start off with a demo. So I'll type in a text message and then how classify it as hammer spam ham means not spam, right? That's it. That's what we're going to do. Just that simple thing. Okay. That's going to be our demo. So that's what we're going to do. It's going to classify it as ham or spam. And we're going to build that using the messages framework, which, which allows us to build a custom APP that uses iMessage. Or we could build an extension to the existing I iMessage APP, which is what we'll do. Okay. So that's what we're going to do today. And okay. Before we get into the features, let's just talk about some history here. This is not the first machine learning framework that apple has released.

Speaker 1:          01:32          In fact, last year it released two of them, uh, to not much, not as much hype as core Mel had. So get ready for some acronyms. The first acronym is MPs, CNN, which stands for metal performance, shader, convolutional neural network. And the second one is n s, which stands for basic neural network sub routines. Actually it'd be n an s. So there's, there's, there's, there's always more acronyms there, but both of these, the let developers build convolutional neural networks, both of them, uh, MPs, CNN was for the Gpu and then BNS was for the CPU. That was the difference there. And both of them let you run inference on your device. So remember there's a difference. There's a difference between training and inference. None of these frameworks are going to let you train your model on your ios device, but they will let you run inference.

Speaker 1:          02:22          That means running a pre trained a model on your device without needing an internet connection. So both of them were uh, released last year they released a set of data types and function types and then different types of layers. And for all intents and purposes, they let you build CNNS. But that was it. It was also very hard to create a.ml to create a model file. A lot of developers were having trouble with that. So based on the troubles that developers were having and just the fact that machine learning is happening everywhere and apple is seems to be just now catching on it. They built core ml. So what core ml is also, I want to say one more thing. So the reason they built two different frameworks, one for the CPU in one for the GPU is because the CPU is sometimes faster than the GPU for inference not for training, for training, the GPU is always faster.

Speaker 1:          03:14          So you kind of have to guess and check with using both. But what apple did was they said, well we don't have to, we don't want to have, you have to worry about all these things. So we'll build a framework around both of them. So that runs on the CPU and the GPU. So that's what core ml is. Core mill is built on top of the previously to released libraries and acts as a layer of abstraction on top of them. And when I say abstraction, I mean abstraction. Seriously. It is almost too easy to use. Like you'll, you'll see what I'm saying. It's very easy to use. And then on top of core ml, they built three other API APIs. One is for, one is for when it's called vision and it's for all of your image analysis tasks, object recognition, classification. And then there's uh, foundations, which is for natural language processing.

Speaker 1:          03:58          That's for sentiment analysis, predicting the next word in a sequence sequence, all sorts of tax related, uh, machine learning. And then there's game played tip and allows your App to evaluate decision trees. So those are the three domains that build on top of core ml. And so the reason they built all three of these on top of core ml is for modularity, right? So let's say you have an APP and what you want to do is classify where somebody is. So part of your app is tagging everything you see. So let's say you're on a beach, it would tag all the things that, that the APP sees. It would tag the water, it would tag sand, it would tag sunshine, maybe some beach volleyball players. And then once you have all those tags using the vision library, then you can use the NLP library foundations to then take those words and classify what scene it is.

Speaker 1:          04:48          And it would say, based on these words, this must be a beach and then you'd output a beach. So that's just one example of how these libraries can interact together. And so for the development workflow, what apple has done is it's essentially built a pdf for machine learning models, right? The PDF is a standard formats, right? And so that's what they tried to do with machine learning models. It's in the.ml model format. And so you can use the.ml model on any ios device, Mac, Mac, apple, TV, Ios or iPhones, all of it. And I pads all of it. And so it's a very simple process. It's a two step process. The first step is to load a pre trained and l model. And the second step is to make predictions with it, right? The training isn't happening on a device, so it's that simple. Uh, so, but sometimes, and so when I say using a pretrained model, apple has a list of these pretrained ml models that I have a link to in this a Jupiter notebook.

Speaker 1:          05:47          Uh, and they're, they're all these very popular models, mobile net squeeze nets, googling that resonant. These are old convolutional networks, by the way. Inceptions huge VGG all of their pretrained models are a convolutional networks. And so you can just use those, but let's say that you don't want to use their, their, uh, pretrained models. Let's say you want to build your own, then it becomes a three step process. The first step then becomes to convert your pre trained model and whatever machine learning library you're using, whether it's cafe or psychic, learn into a.ml model using a tool that they've created for this called core ml tool. It's a, it's a python library. We'll talk about that. And then you do the next two steps, load the model and make the predictions. And so here, it's an amazing list of all sorts of machine learning models that you could use for core ml, right?

Speaker 1:          06:36          The, there's a bunch of pretrained core ml models and then they have models for different libraries that you can then convert to Cornell. Now, I had do have a very disappointing things to say. They don't have support for tensorflow. I know. What are you? Are you serious? There is no tensor flow. It's okay, whatever, whatever, whatever. You can build your own, uh, conversion script, uh, for your tensorflow models to run on core ml. In fact, there must be something on get hub. I'm just making a prediction right now. A prediction. Anyway, so it's either a two step or three step process and so it's the.ml model is like the pdf of machine learning. All you have to do is once you train it, you drag and drop it into x code. So you just drag and drop the.ml model and then you drag and drop your Dataset or whatever you want to test your model on.

Speaker 1:          07:24          And then you could call it very, very simply. I'll show you the code right here. Here's a code, the sample code. But basically when you import the model into x code, it's going to look like this. You've got it. It's going to parse the data inside of it. It's a very, it's very neatly packed for x code to read. So it will be able to say, Oh, here are your inputs here. You're expected inputs and their data types and its description. Here's the outputs. It'll tell you the learn parameters and the weights. That means the weights in the biases as well. So it's a very clean format for x code. And so once you imported that model, you can call it just like this. You can call it by saying, let model equal resonant 50 let's say for example, that's the one we're using. And then you'd say, here's my image, let Pixel buffer up, type CV pixel buffer equal my image wherever it's located.

Speaker 1:          08:09          And then I'll just make a prediction using model dot prediction feed at the image as the parameter and then print it out, right? That's it. That's your prediction. It's that simple. So let's talk about the pros and cons, right? So here's an image of all of the machine learning libraries that are compatible with core ml. That means that that means that the core ml tools python package can convert a model trained with these frameworks into the.ml model format that you can then run, that you can then use to run inference on your device cafe Caerus xg boost live has SVM psychic learn Turi, right? So the pros are it's optimized for on device performance, which minimize, minimize his memory footprint and power consumption. That's the one thing you know about Apple's library as opposed to any third party framework. You know that it is going to be optimized a f for its devices because they make the hardware and the software.

Speaker 1:          09:02          We also know that apple really cares about user privacy. That means use the privacy of users data. So that means you don't have to send the users' data to the server. It stays local and encrypted and it means that you can, and because inferences were happening on device, you don't need an internet connection. So they could be in like a ditch or a prison or why am I thinking of these weird places, but somewhere without Internet connection and then it will be able to perform inference there, right? You don't need an Internet connection and it decides itself. Remember it's a layer of abstraction on top of the previous two machine learning libraries. It decides itself whether to run on the CPU or the GPU or both. So it's, it's, it's self optimizes what it's running on and because it can run on the CPU, you can run it on the simulator, which you can't if it were to just run on the GPU because the simulator doesn't support the GPU yet and it supports many model types.

Speaker 1:          09:56          That's the last pro it support support vector machines. They're all listed here. It supports three types of neural networks at convolutional network, a recurrent network for sequences, and then a feed forward network. Right? And then you've got tree ensembles as tree ensemble ensembles like random forest and boosted trees, linear regression and logistic regression. Okay? So that's it for the pros and we have to talk about the cons, right? It's not perfect. The first is that there is no, there is only native support for supervised machine learning models. That means that models that require labels, there is no support for unsupervised models or for reinforcement learning. So that's that. That can be a big, uh, uh, pain and no training on device, right? So you can only perform inference on a device. Also, it only PR, uh, support certain layer types. So you can just create a new layer, type yourself and add it to core ml.

Speaker 1:          10:45          It's, it's impossible to extend core Mel's native layer types, and it only supports a specific set of training tools, not tensorflow. However, you can write your own custom conversion script for tensorflow models. You can't look at the output produced by the intermediate layers, only the output layers. It only supports regression and classification. That means no, none of the unsupervised techniques like clustering or ranking or dimensionality reduction. And my biggest gripe is this kind of ties into no training on device, but no federated learning, right? Federated Learning is a technique. Google actually published a blog post on this very recently, but basically you can utilize all the phones that you deploy your app too. You can train it on their data locally instead of having it train on a server. And so you can combine all of that training into one big model that is then deployed to everybody.

Speaker 1:          11:38          So you're, you're learning from all the devices that you're, that you're deployed to. And so Tldr core ml is super simple. We saw very simple code sample already, but it's limited in its functionality. So if you want full control, you're going to have to DIY with either the two native libraries that is built on top of, or you can just use another third party framework. Hormel is not the only way to do machine learning on Ios devices. There are other ways, surprisingly, right? And I've got a list of them right here. There are a lot of third party frameworks that work with Ios. They'll let you do a bunch of different machine learning tasks. So of course ml doesn't fit your needs specifically or you don't want to extend it, then go ahead and use these. All right, so what are steps in this tutorial are our steps are going to be first in python to look at what it means to import a Dataset, train a model, and then convert that trained model into a.ml model.

Speaker 1:          12:30          Then we'll go into x code and then in swift will drag and drop our datasets are trained ml model into x code. Then we'll write our basic prediction code and then we'll run the APP. All right, let's get started with this. So for our python code, so I'll, I'll write out the swift code, but we'll just clients over the python code. So far our dependencies for the python code, we have to, right? We have two. One is for num Pi for matrix math, and then the second is psychic learn and all of its sub modules, right? We're going to run, we're going to train three models and then we're going to pick the best one. And I'll talk about what those three models are, but you can get a little hint from here as well as two techniques to vectorize the all each of these models.

Speaker 1:          13:09          So in total, there'll be six different, uh, pipelines that will build. So for each of the models, we'll try out two different vectorization techniques, right? So two, two and two. So which makes six, right? So that's it for psych, it learn for num Pi. And so core ml tools, the, the python package only supports python two. So if you, if you have python three, you can use these commands to initialize your python environment and then you can import a core ml tools, right? Just like that. So I'll go back up here, compile, but uh, okay. And so now we can import our data. So let's take a look at our data set really quickly here. What does our data set look like? We have an SMS spam collection datasets, and it's a bunch of human labeled, uh, SMS messages. Either Ham or spam, right? Spam or not spam.

Speaker 1:          14:01          All right, so that's the data set we're going to use. And so what we can do is we can open that data data set. It's a txt file. We'll go through it, we'll, we'll split all the lines and then we'll convert it into a training and a testing set. Using psychic learns, train test, split function. We can print out the training set to just see, you know what it looks like. It looks kind of messy, but these are some text messages and it's just kind of label ham or stem. That's it, right? Binary classification, right? So that's it for importing our data and now we can look at our models, right? So we're going to use three models and each of these models, I've created a video just explaining the entire model. So definitely check out each of these videos. In my math of intelligence playlist, I've got one for the multinomial naive Bayes support vector machine, and for the random forest, which are the three models that we're going to use, but to just go over them at a high level.

Speaker 1:          14:48          Multinomial naive Bayes is a version of the naive Bayes classifier. These are all classifiers where it, it's just computing conditional probabilities for all the words in a set of documents, right? It's just iteratively computing conditional probabilities. But definitely check out my naive Bayes video on that. And then I have, we're going to use a support vector machine. And so this is a little refresher here. Writes a support vector machine is a type of machine learning model that can classify two different classes. And what it does is it builds a hyperplane by using the, by using the support vectors that are the points that are the closest to each other between the two classes. And then maximize the margin between them and draws a hyperplane right in between them. And so when we add a class, depending on what side of the line it goes on, we can classify it.

Speaker 1:          15:34          And for a random forest or random forest is a set of decision trees, you know? Yes, no, it just, it asks a series of questions and then it, the result is what it, what something is. It'll classify it. And what we can do is we can say, let's create a bunch of decision trees. Have them all classify some data, have them vote, and then pick the, pick the winner, right? What is the most likely class based on a set of decision trees, random forest rights. That's why I told the forest, because it's a set of trees. And then in terms of vectorization strategies, we're going to use either the account vectorized sir, which is just basically a bag of words. It just counts the number of times a word appears. Or we could use a more advanced way of doing this called TF IDF. And what this says is it's a technique to score each vector, right?

Speaker 1:          16:18          Uh, for a term I in a document, Jay. So for each of the words, what we can do is we can say the score, it's TF. IDF score is the number of occurrences of I in j times the log of the total number of documents or SMS messages in our case over the number of documents containing I. And so we'll, we'll compute both of these, the Callen vectorize are and the TF IDF for all three of these models giving us six different pipelines to use. Right? So here's, here's US building the pipeline, right? We have six different pipelines for each of the models. We'll try out both TF, IDF and count vectorized, sir. Once we have all those pipelines, will put them into a list and then we'll perform classification using all of them. And so we'll find this is running right now, we're running live. We'll find that the support vector machine using TF IDF wins, right?

Speaker 1:          17:10          This has the highest accuracy, 98 98 98. So that's the one that we're going to convert into. Dot Ml model file. So we'll see. What we'll do is we'll say, okay, we'll get all the ordering of the words and from this dot txt file and then we'll vectorize it and then we'll convert that uh, model. We'll say here's the model, the LINEAR SVC fit it to the vectorized data and then use core ml tools to then convert for specifically for psychic learn, it's got converters dot psychic learn dot. Convert the model based on the two rows, the message and the label. And that will create a core ml model in memory. And then we can save it using the save function as a.ml model file. So when we, when we compile this, it's going to save it and then we can then drag and drop it into x code.

Speaker 1:          17:58          Okay, so now that's the first part we did it. We train a model in python on our machine. Now we're going to deploy it to x code. So let's check out x code now. So in x code, what I've, what I have here, it's a is the a d a skeleton file. It's a skeleton file. All it has is eight Scott, a storyboard with you know, a label and then a button right in the the button and the label are wired back to the view controller, right? This is in swift. We've got one view controller with all the logic, which has no logic as of now. And then we have the storyboard, right? What's happening here is that for the view controller, we have an IB action that's going to fire every time the button is pushed. So inside of here, we'll put our prediction logic, and then we have this other function for to perform TF IDF that we're going to fill out.

Speaker 1:          18:47          Okay, so let's, let's do that. But first, let's drag and drop our created ml model to our project. Just like that. We'll drag and drop it. And also our spam collection, our data set, because we can test it out on that, on the training data as well. Okay, so then let's go back here and let's start writing the out. Let's see what this looks like. Okay, so, so first of all, let's retrieve the text that the user has typed in where we want to retrieve that message so then we can classify it as spam or not spam. And so what we'll do is we'll say, if the text is copied, let's go ahead and vectorize that text by saying, let vector equal TF IDF. And now we haven't actually written out the functionality for this function yet, but we can just say, well, if we feed the texts into TF IDF, it'll create a vector for us.

Speaker 1:          19:47          And then in a do statement, we'll make the prediction right. We'll say, let the prediction equal tried the message classifier, right? That's our message classifier. Make the prediction, the message classifier is named right there. So we can just call it just like that. And then we'll feed it as the parameter, the vector that we computed using TF IDF. And that's going to store the prediction. We can print, we can print out the prediction for our, our own logging purposes. And then we can to the W, we can send back that data to the interface or the storyboard by saying, set the label to the prediction. Uh, and then so we'll also have a catch statements. So if it can't make a prediction, then we'll set the label to no prediction, just like that. And that's it for our logic right here. And then we'll go to RTF, DF function. And so la, now it's right out this TFD f code. So what's happening here is we can say,

Speaker 2:          20:56          uh,

Speaker 1:          20:59          first we want to import the words ordering and the words. So we'll go ahead and important both of those just like this. And so it's, it's finding where both of these text files are. And the next step for us is going to say is going to be let's create a do statement. And the next step for us is going to, is going to be, let me just write this out like this, just like that. And so the next step for us, let me just paste this in right here. Boom. Like that or delete this. Okay. It's like that. And so we're going to say, let's retrieve the ordering. So, so inside of our dues statement or retrieved the ordering data will retrieve the SMS data. We'll remove the trailing new line from both of them as well, right? Removing the trailing new line. And so once we've retrieved both of those text files, we're going to vectorize our words just by using this, we're going to vectorize our words.

Speaker 2:          21:57          Okay.

Speaker 1:          21:57          But we're going to have a collection of each of those words that we're going to split by the separator, the empty space. And then we, we will vectorize each of those words. And so now for the vectorization part, that here's where the real meat of it, it goes, right? So we're going to say for each word, let's count the number of times the word shows up in an SMS, right? We're going to count the number of times it shows up in an SMS and then we're going to multiply it by the log of the total SMS messages divided by the SMS messages that contain that word. And so that's going to give us the TF IDF score, which we can then store in the vectorized, uh, array. And then we'll return that array of all the TF IDF scores of each of the words in the SMS documents. And that's it for that. And then we can go ahead and compile this

Speaker 1:          22:48          and it works just like that. So overall, I think this is a very fun library. It's gonna. It's, it's a great way to introduce regular developers who've never done any kind of data science or machine learning to machine learning because it's so simple to use. If you've never coded for Ios before, I would highly recommend it. It's a lot of fun. And if core Mel doesn't give you what you want, and you can always use the two frameworks that it's built on, or you could just use a third party library. All right? That's it. Please subscribe for more programming. Video is, and for now, I've got to be thankful for automatic reference counting, so thanks for watching.