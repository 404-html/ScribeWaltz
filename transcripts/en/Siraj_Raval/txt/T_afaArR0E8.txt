Speaker 1:          00:00          Hello world, it's the Raj. And today we're going to learn how to deploy a tensorflow model in production. And now I've never really talked about this before incredibly. I've talked about how to ride models, how to train models, how to test models, but I've never talked about how to actually deploy them in production so that you can use them at scale and a bunch of people can use them like hundreds of thousands or even millions of people. And now this is super important because this is the way to have your app be used by many people, right? Whether it be in a mobile client that you can call the server, or if you just want to make a web app. Both cases, this applies to both cases. Okay? So what I'm gonna do is I'm going to talk about the architecture around this technology. It's called tensorflow.

Speaker 1:          00:42          Serving is a separate library from tensorflow, but you use it with tensorflow. I'm going to talk about how it works. And then what we're going to do is we're going to deploy a simple web app that lets a user upload photos and it's going to return a classification of what's in that photo via age. Jason Response. Okay. So let's start off with an explanation and then we're going to go over the code and then we're going to go through this step by step process of what it means to upload a trained model to a server and then call it from a client. Okay? So that's what we're gonna do today, deploying a tensorflow model in production. Okay, so this is what the basic architecture looks like. I mean, I have several images of what the architecture looks like, but this is one of them. Okay, let me make sure that it's big enough to see just like that.

Speaker 1:          01:29          Cool. So here's what it looks like. So first of all, let me talk a little bit about tensorflow serving. So tensorflow is the open source machine learning library that we all know and love and attention float serving was built to handle it on a server. And what do I mean by handle? I mean handling all the infrastructure that goes into building a model, using it, uh, versioning, different versioning, the model, maintaining the lifecycle, uh, dealing with, uh, which model you want to use. Uh, taking the output from one model and making it the input for another model. It's actually pretty complex. I mean, Google uses this in production, right? So it's gotta be pretty complex. Google's got like billions of requests probably every single day, right? So we know that this is a battle tested approach. Now I know like Pi Torch is super hot right now. I'm like in Pi towards everybody's liking Pi Torch.

Speaker 1:          02:20          But Pi Torch was meant for research, right? Tentaflow was meant for production and it's, it's been battle tested by Google. So if Google can use it, we can use it. Okay. So in terms of the architecture, this is kind of what it looks like. There's a lot of different components that we're going to go over, but at a high level, once you, so this is all like, just think of this, this, uh, these two green boxes at as the server, this is what's happening on the server server side. And then this blue box is the client and the client could be, you know, a mobile app or uh, somebody's desktop for the Terminal v a variety of things. But so what's happening on the server? Well, we've got these models, right? That could be the Model v one and V two. They could be the same model, but different versions of it, like newer version and an older version, or they could be two entirely separate models.

Speaker 1:          03:07          It doesn't matter. It, it considers them both ways. Right. And you can, you can tell a like, I want it to have multiple versions of one model. Or You could say, I want multiple models, right? It depends on what you want to do. But we have a file system that's on the server that stores these models. And what happens is the library, we'll load up a model. Okay. And then it's going to see which version of the model that we want to use. All of these things we're going to specify, right? We're, we're going to specify programmatically. And then once it's got that latest version, let's just say we want to use the latest version of that model, then it's what you serve that model. And what do I mean by serve that that model then becomes the front facing interface to which any client can request via what's called a signature.

Speaker 1:          03:48          And we'll talk about all this, but it's going to request it via a signature id and it's going to return the results of that model. In this case, it's going to be classification. So, and so that's what the, that's what the client does. And you see this term right here, grpc a grpc request and GRPC response. And you might be wondering, what the Hell is this? So Rpc, uh, is, let me, let me pull this up. RBC versus http. This is a, this is a really good link for this, but basically you've heard of rest, right? So rest is like the way that we, we work with http API. So eight. So, okay, so http is the protocol of the Internet, right? It's how data flows across the web. The most popular way. There's also like bit torrent and like other protocols, but the most popular way is http, right?

Speaker 1:          04:36          All websites run on it. And RPC is a protocol that can, that is a PR is a protocol that can run on top of http. It's really not complicated at all. I know it's like all these acronyms, but it's actually not complicated. Like, uh, so, uh, rest is like the default way, right? Mvc Web Apps, node apps, you know, flask, they all use rest. And what is rest? Well it's got these set of verbs and so the only difference, well the main difference with RPC and rest are that the verbs differ. So for rest you've got like get post, put, patch, delete. Whereas with RPC you only have get and post and if you want to delete or do something else, you to specify that inside of the request. So if you want to delete, you would post a delete. And so one of the parameters of the post would be like a delete request.

Speaker 1:          05:27          That's it. And then there's like a bunch of like, you know, it's semantics, like beauty and design ability concerns. It doesn't really matter. The point is that you can run this over http via an API just like anything else. But that's what RPC is. And GRPC is a type of RPC that's uh, you know, tweaked in a certain way. It's, it's, it's, it's basically negligible. We don't have to, we actually have to worry about it. It's, it's, it's happening under the hood, but that's the high level architecture. You have a client a request, uh, it's, it's either request a, it either gets or posts some data and then the server's going to return it. Um, and the server contains the model. So, okay. So that's the super high level. And then we've got this, I've got this, uh, nvidia graphic, but I wanted to show what inferences because TensorFlow's tentraflow serving is meant for inference for inference and inference means managing models, giving versioned access, a reference counted lookup tables, I. E. Then HDP interface via RPC.

Speaker 1:          06:27          So we have an untrained network, right? And then we train it. The has some training data that is a training stage. And then once it's done training, we have a trained model, then we can test it on our own data to make sure that it's classifying correctly. And when we're done with that, then we can perform inference and inference is when we're applying the capability of this model to some new dataset that the consumer would probably be using, right? So that's what this is for. It's for running inference on our models. That is for production use. Okay. So that's what it's for. And it can serve multiple models simultaneously, right? So we'll talk about how it does this, but if this is great for ab testing, right? Let's say you've got one trained model, let's say one is like a convolutional neural net with like 16 layers.

Speaker 1:          07:13          It's performing really well, but then you've got this other kind of like experimental model that's like us, a convolutional and recurrent net mixed together and you want to test it out, test out which one works best. You could put them on this, you could place them on the server and then have your half tentraflow serving, serve them the a client and then it could try out different versions of it. Or You could even have the different versions of the same model and then test out different versions of it. So okay, so the one bad thing about this is that it's written in c plus plus tensorflow serving is written in c plus plus. So tensorflow is written in c plus plus as well. Because when you are making something industrial grade life for hundreds of thousands, millions of requests, c plus plus is still like the most trusted way to do it.

Speaker 1:          08:00          But tensorflow has got python bindings and sodas tensorflow serving but not like enough. So we still have to deal with some c plus plus a. But that's okay. We will, we'll get through it. Okay, so now to get a little more specific, right, to get a little more specific with what this looks like programmatically we have this is, this is kind of like what the architecture looks. Let me, let me move this over here. So there are four major components. Let me make this look a little smaller. There are four major components here. Okay? So we've got server bowls, loaders, sources and managers, and this is all happening within tensorflow serving. So remember this is separate from tensorflow for for the most part, if you just have one to train a model and put it on a server, you don't actually have to deal with any of this.

Speaker 1:          08:47          All you have to, all you have to worry about is creating a servable and it's going to do all the rest for you. But if you want to, you know, have more fine grained control. It's good to know what these variables are. Okay? So if you don't care about this, just skip forward. But if you care about this, if you want to deal with, um, different versions of your model, if you want to make sure that, uh, the lifecycle of your model is handled well, then this is an important part. So we've got, let's start off with sources, right? So sources basically contained service goals. So a source is basically the gateway. Think of it like a gateway. You know, like in the tensor flow graph we have placeholders. So that's kind of a rough analogy as a gateway for data into the server, into the server, right?

Speaker 1:          09:30          That the tensorflow serving server as a source takes into serving bowls. That's more or less. So the circles are the real fun part, right? So you see here we have a source and it's, it's pointing upward to a loader. And so it's what it, what a source contains is it contains server bowls and they sortable is the central abstraction of tensorflow, right? They are the objects that clients use to perform computation. So I certainly will, can contain one model. It can contain multiple models. You can have one model perceivable or you can have one servable with like 20 different models or you could have one servable with a different versions of the same model, right? So there's a lot of ways that you can architect a servable, but that's like the central, the central class. These are four classes, cervicals loaders, sources and managers. But server are like the central class around which everything works around.

Speaker 1:          10:24          So, uh, an analogy to tensorflow for a sort of a bowl would be like the computation graph, like TF dot graph. Everything is there to support the TF graph. And so servable is kind of like that for the server. So we've got a source and we feed it a servable. So we'll have some model and then we like, and then we will look at this programmatically, but basically we put a model in a servable, put it into a source. The source feeds it to a loader. And so this is the loader right here. And what the loader does is it just, it's, it's basically a placeholder. Uh, not, not a placeholder in terms of the gateway, but it's a not a, not a placeholder. A better word would be kind of like temporary storage, temporary storage for the manager to then come and manage that model's lifecycle.

Speaker 1:          11:08          So a manager is really cool. So a manager manager handles a full lifecycle observables. That means it decides when to load the model, went to serve it for a client, went to unload it, and it listens to the sources and tracks all the versions. So a source, I forgot to say this part. So a source actually tracks different versions of the model, right? So when you, when you, when you put a servable inside of a source, then it's, it's, it's got versions built in and what the source does is it is, it points to different versions and then it tells a loader when the loader loads it, hey, these are the different versions, here's where they are in memory. And then the manager can come in and say, right here, this dynamic manager, it could say, okay, here's the version I want. And so that's what the word inspired version means, right?

Speaker 1:          11:51          Once we have our loader, we feed the aspired version, whatever we want, and we specify that in the request to the manager and the manager is going to be like, okay, I want this version. Okay. And so and so once the manager knows what version of the model to syrup, it will serve it via this sortable handle. And that's, that's how it goes. Right. And so the, so you have, you can have streams of service bowls and what do I mean by streams? It's, it's basically like get, you know how when you like commit constantly you have like new versions. It's just like that for a model. So basically this is your way of being able to deploy a model to production and then continue working on it locally. And then if you have some new version of it, you can then just upload that new virgin to the, to the server and then it's going to save the state of the old version of the model and also serve the new version.

Speaker 1:          12:37          And sometimes you might want to revert back to the old version and you can do so via its signature. And we'll talk about that. But, uh, it's basically a two step process. We're about to go into the code, but it's basically a two step process a source creates. So we put a cervical in a source, the source creates a loader and the loaders are sent as inspired version to the manager, which then loads them and serves them to client requests. Okay. So I mean, that's a lot to take in all at once, right? So what we should do is we should look at, uh, what it means to create a model, right? So or create a servable, right? So let's, let's just like go over this really quickly, right? So what this is, let me, Ooh, okay. Okay. So check this out. What this code is, it's, it's like 160 lines.

Speaker 1:          13:30          We're not going to go over all of it. We're just going to go over like some of the basic bits here so we can understand how these four classes will interact with our model. This is really simple code. All right? All it does is it trains an Mni as t model to recognize, you know, it's to recognize handwritten character digits, right? You have some characters, you know, images one through nine and they've got labels associated labels one through nine and we went over to the mapping between the labels and the images, right? And so once it, once our model is trained on this data, then if we feed it some random image, like a, you know, an image of the number three, it can say, oh that's a three because it learned the mapping. So let's go over what this looks like. We've got our imports, right.

Speaker 1:          14:09          This is tensorflow and tensorflow serving. We've got some training flags. And then in our main function, you know, we, we basically specify how much we want to train and you know, different training options. But you know, it's very similar process to all sorts of machine learning. We read the, we first read the data, thankfully tensorflow has this read data sets function built in. We established our session and then we format our data that's so that it can be fit into our placeholders. Then we build our model, right? Very simple model. It looks like it's a single layer network, writes x, y, WB, you know, one set of weights and biases. And then we squash it with some, uh, with a soft Max. And we're using a cross entropy as our loss and, and dissent to minimize across entropy. And what that's gonna do is it's going to build a perceptron that we can then train.

Speaker 1:          14:59          So once we train this model, we're going to use a minibatch gradient descent, so every, so we're going to train it on batches of 50 training examples at a time. And it's going to, when it's done training, we'll print out done training. So that has nothing to do with tensorflow serving. That's all just like tensorflow, right? This is just how you would build and train a model. Now here's the tension flow serving part, right? So once we built this model locally, we've trained it, now it's time for us to save it, right? Because we want to take this model that we've trained locally and we want to, oh actually we could train it locally, but it could also be on the server. So for all intensive purposes, let's just say that this is going to be on the server. Okay? So what we'll do is we'll say, uh, we'll say, okay, so let's, let's define where we want to save it.

Speaker 1:          15:49          So we'll create an export path. Like we want to save it right here. And once we do that, then we can say, okay, we've got, you know, we've got to saved path for this model. And this could be on the server or on the client, right? Cause you might want to train it on the server as well. But the point is that we've trained a model and now we want to save it. So then we'll create a servable from our model. So like we've got this model and we want to create a servable. And that's what we do with this method right here. This, uh, saved model builder and a sort of bubble is a class, right? It's a class and it's how we feed this model to our, uh, tensorflow serving main server class function, our main tensorflow serving class, right? So then it could do all of that manager loading sources, all that stuff.

Speaker 1:          16:33          But we save our model just like this, using the saved model builder, it's going to save a snapshot of it so we can later, it could later be loaded for inference and it can, we can save as many versions as we want. We said that we wanted to do, you know, we, we defined the model version, but we can create an entire stream of these models. Like we want, you know, 30 different versions. But to keep it simple, we're just going to save one version of the model. It's good, it's gotta be one survivable. And once we do that, then what it's, what is going to do is, uh, the, the, the, the main serving file is it's going to create a source out of it. And the source is going to house state. So there could be multiple service roles, multiple versions, and then it's going to create a loader and it's going to use this function, the TF dot saved model, loader dot load to load that source.

Speaker 1:          17:18          And then a manager class is going to decide how to handle its lifecycle, when to serve it, when not to. Okay. So all of this for is going to be in the main class, but all we've done done here is we've created a servable so that we can then load this up via a the serving file, the main tensorflow serving file. And so what we're doing right now is, so here's a, here's, here's the, here's another very important part. We are creating a signature map. And so the signature map is how we, these are parameters that we can embed into our [inaudible] class. So that are 10 truckloads servings are tensorflow serving main class knows which model to load up. So we're going to basically give identities, ids, Ids, our signatures to all of our components. We'll give it to our inputs, we'll give it to our outputs, we'll give it to our class itself and some build info.

Speaker 1:          18:16          And so were all of these, this is basically Metadata, right? We're giving a bunch of Metadata to the server in the form of a servable so it knows what model it is and it also knows, you know what the ideas are for all of its components so that later on whether on the server or on the client, we can reference those parts. Maybe we want to pull apart like we want to see the output, maybe we want to pull the whole model. Maybe we want to take the output from one model and feed it to it as the input to a different model. That's what signatures are for. So we define it as metadata and so that's why we use service goals instead of just like straight up models. By abstracting that a little bit more, we can add metadata to it so that you know are managed.

Speaker 1:          18:56          Both our manager on the server can know what to interact with as well as our client. Our client can know which model and which input, which output to, to manipulate. Okay. So, and then we add them all into that builder. You have the ad metrograph and variables function and then save it. Right. And once we saved it, then we can call it from our main server file. Okay. And so I just wanted to go over that to like quickly show how those four main classes would interact with a simple trained model. Okay. So, so let's do this. Let's build this. Okay. So let me, let me start out by saying like, this is, this is a, uh, it's definitely doable. It's definitely doable, but it's not like some walking the park to run this. I had to like try this out a couple of times before Dylan, before having it work successfully.

Speaker 1:          19:44          But there's a lot of like possible errors that can happen due to, of course dependencies. But, uh, if you, and you know, for whatever reason it's, it's that way, but, uh, it's going to take some work to get this to work. Let me just say I, that's all I'm trying to say. But if you put in the time, uh, then you can easily do it. Okay. So let's do it right now. And so some of these commands are going to take awhile. So I'm gonna like edit forward through those commands cause they can take like 20 minutes or 30 minutes to, to load up all those dependencies, right? So before we, so before we do anything, let's set up our development environment, right? So we, what we want to do is we want to download this tensorflow serving repo, right? And what we're gonna do is we're going to, uh, create, create a docker image for all of those dependencies because it's got a lot of dependencies.

Speaker 1:          20:36          So we don't have to use docker, but we will use docker because it's easier. So docker, for those of you who don't know, is basically like a super lightweight virtual machine. So what is it virtual, have you ever used parallels on like Linux or Mac based and to run windows? So sometimes you want to run, run a game, right? So you would download a virtual machine, which is basically an entire operating system just to run that game. Because that game has dependencies that are operating system specific. But sometimes we don't want to have to download an entire operating system just because we want to run one APP. Right? So what would be better if we had a lightweight virtual machine that was super, super small, but it contained all the dependencies we need for the APP that we're trying to build. And that's what docker is.

Speaker 1:          21:26          Docker basically con contains all the dependencies that we need. So like let's say one APP needs python too, but another APP needs python three and we, and we had, we only have one version of python on our system. We could use docker to contain self contained, one of those apps, download python to all the other dependencies and then those versions wouldn't affect any of the versions on the rest of our system. It's all self contained. So it's super cool. I should be using docker a lot more often and I will eventually, uh, but yeah, that's a little refresher or tutorial on what docker is. And here's some installed instructions. It's in the, it's in the, uh, I python notebook. Okay. So let's go ahead and download this baby. So the first step is to clone the repo. Oh, hold on. Okay. Sound effects. Let's first clone the repo and we'll put that, we'll use this recursive flag because it's got a few, uh, dependency repos that we will need. And so it's gonna, it's gonna basically clone all of those and it could take awhile.

Speaker 1:          22:36          Okay. And so once we have that, we can then CD into that serving directory. Okay. And so here we are, we have tensorflow serving, so we've got that file locally. And so this file has a lot of dependencies that we're going to need, right? And so what we can do is we can say it, so this, this, this file has a docker file or this repo has a, um, has a docker file. And what the docker file does is it says, Hey, these are the dependents, these are the dependencies that you're going to need. So let's just see what that docker file looks like. See, so here's what the docker file looks like. It's saying you got to get all these dependencies, right? You got to then, uh, you know, if you don't have pip and saw that you got to get the GRPC client for running those http requests, you got to set up Bazell so we don't want to have to do this ourselves, right?

Speaker 1:          23:27          So that's what this docker file is going to do. It's going to do all this, create a docker image out of it, and then we can run our docker image and then self contained in that all of our dependencies are going to be there and we can run a recode is normal, right? By the way, Bazell is Google's tool for downloading, um, dependencies. Very useful, very useful tool. But yeah, that, that's what the docker file looks like internally. And so once we have that, we can build it. So let's build this. It's going to create a docker image with all the required dependencies for us. So let's go ahead and do that. Boom.

Speaker 1:          24:11          Okay, great. Successfully built. And so now we've done that. Now we can go ahead and run the container locally. So this is going to do is it's going to run our doc, it's going to run this docker image that we created using the docker demon. So, or the Damon, however you say it. Daemon Daemon, I'd never know. But yeah, the docker Daemon, I'm going to say Damon. Um, cause it's not evil. It's great. So let's go ahead and run it into docker Daemon and we're going to specify, uh, the bill that we just defined, right? We called it tensorflow container. And so now we can run it. Okay. Error Response. That's cause I have docker already running. So let me quit docker and then rerun it docker. Okay, no time for that. And then see what's going on here. Dockers running. Okay.

Speaker 1:          25:15          Okay, so now, okay, we are in the docker container, right? We can see what's in here. I make sure this is bigger. Boom. Okay. Make that a lot bigger. Okay, so we've created our docker container. Here it is. And now that we're in this container, we've got all of our, uh, we've initialize it using our docker image, our docker file, and now we can clone our repo into, and now that we're here, that now that we're in here, so we'll do the same thing. Now that we're inside of our image and this time we're cloning it, not for the docker file, we're cloning it for the related files, right? The actual tentraflow serving, right? We're in the image and now we can download those files to, it's going to take awhile. Okay. So then once it's downloaded, uh, via get, now we can CD intuit what, see what we got here. We've got a CD into it and then configure it. Okay. And what that's going to do is it's going to use Google's basil bill tool. It's going to use Google's Bazell build tool from inside our container, and then it's going to download all of those third party dependencies. So we'll CD into serving, and then we're going to run the configure. Uh, let's see, CD serving, then tensorflow, tensorflow, and then configure it. Boom. Okay. It's going to ask or specify the location of a bunch of things.

Speaker 1:          26:41          Do I want Mkl? No. Uh, no. Default. Default. Default. I mean, oh shit.

Speaker 1:          26:57          Dammit. Let me Redo this. Configure. Okay. Um, MKL sport default. Default. Default. Yes. I want Google cloud platform. Yes. Hadoop don't care about the experimental compiler verbs. Um, don't care. Open cl don't care. Kuda normally, yes, but for all intensive purposes where this tutorial, I don't care. NPI, no. Okay. And that is going to do it. Okay. So then while that runs, the dependencies that we got that we're going to need our tentraflow serving the library and d pretrained inception model, right. The inception model is going to, let me show you what the inception model is, by the way. Basically, it's a huge convolutional neural network that won the 2014 image net competition with state of the art results. They train it on hundreds of thousands of images and this is like a compressed view of it, right? It's the actual view is up here, but then the compressed view is up here. It's got a bunch of layers, like put like a hundred layers. But basically we don't have to train our whole model, right? We could if we wanted to, but we're going to use transfer learning. We're going to use this pre trained model for our own use case. Okay. So that, that means we don't have to deal with the training part, at least for this example. Okay. And so then once we have that are, once we have our, uh, Bazell dependencies built, we can then test it out by, uh, uh, sorry, once we have our

Speaker 1:          28:29          repo configured, then we can build our dependencies using Bazell. Right? So we'll say Bazell build, and that's going to take 20 to 50 minutes. It's going to take a while to build those dependencies, okay? And then we can run our model server, right? So this is going to take a while to do a base. We'll build, but it's done configuring and now we can do Bazell build and it's going to take a while. Okay. And so once it's built, we have our models server built. Now we're gonna move on to the second dependency. And the second dependency is downloading inception, right? So we're going to curl that into this repo and it's going to pull it. It's going to download it just like that. And while it's downloading, we can see, well, what's the next step?

Speaker 2:          29:12          Yeah,

Speaker 1:          29:13          the next step is to run it and the server locally is to just run it, right? We want to run. So, so look at this, um, command here. So while that downloads, we'll look at the command. So what we're going to do is once it's downloaded, we're going to untarnished, we're gonna, you know, uncompress it, and then we're going to, uh, save it as a checkpoint. And then once it's saved as a check point, we can then refer to it in our flow model server like that. That's the main file of tensorflow serving, right? That's, that's the model that we want to use. So at the beginning, I talked about this custom m and t model, but for the sake of this tutorial, to keep it short and simple, we're going to use this pretrained model and it's going to feed it and we're going to feed it in and be at this parameter model name.

Speaker 1:          29:58          And then the path of where it's located. And then it's going to run the server with that model is going to do everything necessary, is going to check, the version is going to feed it into a loader. The manager is going to maintain its lifecycle and then we can, we can call it from our clients. So right now the server is local and then we can call it with our client. Okay. So, uh, right. So this is going to take a while. Let me, let me fast forward. So then once we have our server running, then we can call it via our client. So this is what our clients, it looks like, let me show you the client, right?

Speaker 1:          30:36          So the client looks like this very small, very small, but all it does is it's sending a grpc request and it's going to receive a response, right? So it's going to send a request. And so this is why we had signatures, right? This is why we named signatures as our parameter. And so once we have these signatures, we can make the request. That's all it does is it's making a simple request that the client file, let's, there's two files that are, that we, that we've talked about, right this client file. And then we had that custom file that on the server that's going to train that model for us and then create a servable with it so that our main TentaFlow serving file, which has this huge c plus plus file can deal with, can handle its lifecycle that we don't have to touch unless we want to be very specific, but not for the purposes of this tutorial.

Speaker 1:          31:24          Okay. So, okay, where were we? So, uh, we, once we've built our server, it's running locally, then we can use that client to then take this image, which is a, what does this image? It's a panda is a panda image. Cute Little Panda Kung Fu panda. And then we're going to, we're going to pull it from the web and then we're going to send it to the client via this, uh, via this parameter right here, image. And I'm using the inception client. If it works, we're going to see a of classification output in terminal from the server. So this server is local. So if we run this right swimming, we haven't locally or we run low, I had to file locally. So if I run this, we get a response, we get a response back from the server. And so the server's going to send a response, the string values or the likelihood for e or the or the classification values.

Speaker 1:          32:17          And then we had the likelihoods for each, the likelihoods for each of these classification labels down here as well as floats. But yeah, see Cha, Chinese panda up hand to a black, white panda, you know, Cardigan pretty bad. But yeah, the best results are up there at the top. And so that, so we get those results back as, as, as, um, as Jason. And so then that's, that's it, right? So now we have the server running locally. We have our clients and now we want to put it on a server, right? We don't want to have a whole local want to put this thing on the server. So to do that, we're going to push it to Google cloud. And so Google has this great tutorial on how to do that and it's going to be using Coobernetti's. And what is Kubernetes? Kubernetes is automatic container management.

Speaker 1:          33:01          So this is what Google uses internally and it's basically production grade container management. So you can have multiple containers, multiple models, and basically there's a lot of infrastructure that goes around dealing with these huge models and when to use them and stuff. And basically it's like a little, it's a tool that helps tend to float serving serve models. And so it's running on the cloud and we can, that's, that's how we're, that's what we're going to use. So we can go right to this part too cause we did all the other parts. Okay. So basically this is how it works, right? So you log into Google cloud via terminal. So assuming you've created account, you log in to be a terminal and then you create a container cluster. Okay. And so this cluster is going to just contain all of your, uh, your server code, right?

Speaker 1:          33:47          So that the whole tensorflow serving repo as well as if you have some custom model or inception that that goes in the container, everything but the clients, right? Tensorflow surveying and the custom model, both are both are contained in a single container. And then we say, okay, since Google cloud is so tightly integrated with Coopernetti's, we can do this all in a series of commands so we can, so we can figure Google cloud to say here's a container that we want. Now that we've created it, here's the one we want. And this is the, that see this inception serving cluster is what we're going to put in that container. And then once we have that, we can upload the docker image once we tag it with some value to that container in Google cloud. And once we've done that, now we're going to deploy three replicates, three replicas of the inception inference server using Coobernetti's, right?

Speaker 1:          34:41          So the, so the rectal cause are exposed externally by a Kubernetes service. And what Kubernetes does is it says, well, I know you have several versions of this container. Let me pick which one I want to use and we can then create a, a Coobernetti's, a configuration file and then deploy our container to it just using these two commands. And finally, when that's done, we can query the model, right? Just like that. And it's going to do the same thing. Return a Jason file that we can then, you know, read and whatever else. Okay. So that's the high level of how that works. Uh, the code is in the, is in the get hub repo. It's in the description. Definitely check it out. And uh, God commented it extensively. What else? So, okay, so to end this session, let me answer some questions from, from the channel and then we're out of here. Okay. So what kinds of questions do we have here? Eyes are two questions randomly. Question wine.

Speaker 2:          35:49          Okay.

Speaker 1:          35:52          So here's a question from my differential neuro computer video. How does it understand the order of problems? Like how can 100% to process text first, then a graph problem? Why doesn't it just go to the graph neural network first? So it doesn't know the order. We tell it the order. So we have to define what we have to train it on one data set and then we can train it on a different dataset. The differentiable neuro computer is definitely like the dopest model I've ever seen before because it's so generalized and it can do so many different things. So once we train it on one Dataset, we didn't have to, uh, synchronously train on the next data set and then just keep going like that. Uh, but he doesn't know to say, well, let me first look at this graph and then let me, let me first look at this, you know, subway graph and then let me look at the text. You've got to train it in order. And so one more question. Um, can you please explain how to save the models in tensorflow once the training is done? So that can be utilized later whenever I want. Or is there any other video in which you explained that? So like we talked about right here is how we saved those models. Okay. And so

Speaker 1:          37:02          we save it just like this was defined and export path. And then we build it using this. So basically, so a server bowl contains a checkpoint file, the PB file, all the related Metadata, things that come with the model. And the reason we even use this, right? Why w w why do we even use this is because how else are you supposed to run a model in the cloud? Right? That's why you need a tensorflow specific library because these computation graphs need some kind of environment to run in. And that's why it's very useful to have tensorflow serving as a file. If I were to, if I were to build a model in production, right? If I wanted to build an APP at scale that hundreds of thousands of people would use, I would use tensorflow serving. I did. There is no better tool out there for production in terms of reliability and in terms of scalability. So those are the two criteria that you should look for when dealing with production. Great apps. Okay. So yeah, that's it for this tutorial. If you like this video, please subscribe for more like it. And for now I'm going to say Coobernetti's three times fast. So thanks for watching.