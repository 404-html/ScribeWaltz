Speaker 1:          00:00          Hello world. It's arrived and welcome to the first video of my new reinforcement learning course titled Move 37 this is a 10 week course. All my videos will be released right here for free on my youtube channel, so hit subscribe to keep updated. The only prerequisites are knowing basic python syntax and understanding the backpropagation algorithm. Educational links to both topics will be in the video description. We humans have been able to solve some hard problems in our relatively short existence on earth using our biological intelligence to analyze data and come up with solutions. But some problems like major diseases, extreme poverty, longterm environmental sustainability, have proven to be very hard to solve. Some of these major problems may take large groups of well trained humans decades to solve, and we just don't have the luxury of waiting that long. We need those solutions now since its inception in the 50s artificial intelligence researchers have always been driven by a mixture of curiosity of how the universe really works and all truism to help augment our human intelligence with machine learning systems.

Speaker 1:          01:17          And it turns out that AI isn't just a luxury, it's a necessity. The amount of data generated on the Internet doubles every two years. It's an exponential growth. And the keys to so many mysteries like the ending of the soprano's lie hidden in that data data that no single human could parse through alone, but perhaps with the right algorithm could. So if you've just started to learn how machine learning works, you'll notice that the vast majority of the introductory algorithms and theories that you'll encounter in blog posts and courses, well mostly all be encompassed under the category of supervised learning in the machine learning pipeline. The first step is to collect a dataset. They come in all sorts of file formats and generally look like an excel table of values. Each data point has a different value for whatever feature it has. In supervised learning, we're trying to predict a value that already exists in our data.

Speaker 1:          02:14          This is called the label. Sometimes it's called the target variable, and it can also be called the dependent variable, whereas the rest of the features are considered independent variables. So given our features like the years of experience, the age and the educational background of an individual, it would be helpful to be able to predict their salary or given the age of a car and the driver's age, it would be helpful to be able to predict the risk of automotive accident. The great thing here is that the data we'd use to train our model, whichever one we pick already contains the desired response that is it contains a dependent variable. It's like having training wheels on a bike. We could choose a linear regression model, logistic regression, a neural network decision. Trees all have different ways of eventually learning the function that relates the input features to the label.

Speaker 1:          03:11          Most data however doesn't have clean labels for us to use but we still want to derive some insights from it. That would be the field known as unsupervised learning. We could then cluster our data using techniques like k means and mixture models, allowing us to visualize groups of data points that are related that we wouldn't know of otherwise. We can also attempt to find a compressed representation of the data using say an auto encoder, then use that representation for a specific task or we could just try to find the anomaly in the data set, meaning identify what data point doesn't fit with the rest, like say a fraudulent transaction. Unsupervised learning algorithms are generally used to preprocess data during say the exploratory analysis phase or for supervised learning algorithms. Supervised and unsupervised learning algorithms are supremely useful. They are tools to recognize patterns in complex data, but consider this scenario.

Speaker 1:          04:13          We're a new online product delivery startup and we've just deployed a fleet of vehicles and factories that help us move a product from point a to point B. So many things could go wrong before the product is successfully delivered. The trucks could break down, bad weather, could cause road closures, the food could go bad. What kind of learning technique should we use to predict the most optimal delivery route given all the other factors? This is a highly dynamic problem space. We need a learning system here that will be highly adaptive to changes and unfortunately we don't have a preexisting data set to learn from. We have to learn in real time what works and what doesn't work in a setting that introduces an entirely new dimension time. This is the task that reinforcement learning attempts to solve. It's somewhere in between supervised and unsupervised learning.

Speaker 1:          05:09          While in supervised learning we have a target label for each training example and in an unsupervised learning we don't have a label at all. In reinforcement learning, we have time delayed labels that are sparse, meaning we don't get that many and based on this signal of time delayed labels which we can call rewards, we can learn how to behave in this environment. It's this powerful combination of pattern recognition networks and real time environment based learning framework's called deep reinforcement learning. That has resulted in some incredible recent AI success stories including the minds Alphago program and open AI fives go to victory. We'll discuss the details of those near the end of the course, but first we need to understand the algorithms and theory of pure reinforcement learning. All of it starts with defining some kind of mathematical framework that encapsulates the idea of an AI interacting with an environment where time is a dimension and it's learning through trial and error.

Speaker 1:          06:14          In 1906 a Russian mathematician named Andre Mark coff was interested in modeling systems that followed a chain of linked events. Blockchain, no, so he defined what's now called a Markov chain to describe this process. A Markov chain has a set of states and a process that can move successively from one state to another. Each move is a single step and is based on a transition model t that defines how to move from one state to the next. This chain is based on a property he also invented called the Markov property. It states that given the present, the future is conditionally independent of the past, meaning the state in which the process is now is dependent only on the state it was at one time step ago. For example, let's say we want to use a Markov chain to model the weather with just two states, sunny and cloudy, using what's called a transition matrix.

Speaker 1:          07:12          We can represent the weather model in which a sunny day is 90% likely to be followed by another sunny day and a rainy day is 50% likely to be followed by another rainy day. Each states of the chain is a node in the chain graph and the transition probabilities are edges with the highest probabilities. Having the thickest edges. The most common framework for representing the reinforcement learning problem of an agent learning in an environment is called a markup decision process. This is an extension of mark off chains. The difference is the addition of actions, meaning allowing choice and rewards giving motivation. Every Mark Cobb decision process is defined by five components, a set of possible states on initial state, a set of possible actions, a transition model and a reward function. The transition model returns the probability of reaching the next state if the action is done in a previous state, but given s and a, the model is conditionally independent of all previous states and actions, which is the mark off property and the reward function.

Speaker 1:          08:21          Our returns a real value. Every time the agent moves from one state to the other, and since we have a reward function, we can conclude that some states are more desirable than others because when the agent moves to these states, it receives a high reward. The opposite is also true. There are states that should be avoided because when the agent moves there, it receives a negative reward. The problem then is that the agent has to maximize the reward by avoiding states which returned negative values and choosing the one which returns positive values. The solution is to find a policy which selects the action with the highest reward. Agents can try different policies, but only one can be considered an optimal policy, which gives us the best utility. So if we, for example, have a delivery drone, we want to get to our friend in the same room using the optimal path, we can use the mark Haub decision process to frame this problem.

Speaker 1:          09:21          We can define our environment as a matrix with the starting state being in one corner. There could be obstacles in our environment like ceiling lights and Postgrad students that we'd want to avoid and our drone can go up down, left or right. We've got a set of states actions and rewards here and each environment will have its own characteristics that make it unique. For example, this particular environment is fully observable. Since our drone always knows which state it is in. We could also say that there's no limit on time to deliver, so the problem has an infinite horizon. These factors will influence the choice of algorithm we use to find the best policy. The question then becomes how does an agent choose the best policy? That's a topic for the next video. Three things to remember from this video though in reinforcement learning and AI learns how to optimally interact in a real time environment using time delayed labels called rewards as a signal. The Mark Haub decision process is a mathematical framework for defining the reinforcement learning problem using states' actions and rewards and through interacting with the environment, an AI will learn a policy which will return an action for a given state with the highest reward. I'm so glad you made it to the end. Hit subscribe and you will always win, will be forward for now. I've got a mastermind environment, so thanks for watching.