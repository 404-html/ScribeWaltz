Speaker 1:          00:00          What lights and yonder window breaks and Juliet is the sun is the father. No, no, no. She's a bright ball of gas so someone's son is Juliet is the star in the sky. Oh world walking this here, geology and this episode we're going to build an AI reader that is an AI that can analyze human language. It's type of task is considered natural language processing or NLP. NLP is the arc of solving engineering problems that need to analyze or generate natural language text. We see it all around us. Google needs to do it to understand exactly what your search query means so they can give you back relevant results. Twitter uses it to extract the top trending topics. Microsoft uses it for Incar, speech recognition and ops basically extremely dope because it deals with language. Kurzweil once said that language is the key to AI.

Speaker 1:          00:43          A computer able to communicate indistinguishably from a human would be true. Ai there 6,500 known languages in the world and each of them have their own rules for syntax and grammar. Some rules are easy like I before e except after c and some are based on intuition. Since there is no consistent use case. So how do we write code to analyze language? Before the 80s NLP was mostly just a bunch of hand coded rules. Like if you see the word dance translated to Zach do or if a word ends in ing, label it as present tense. Well this worked. It was really tedious and there are million corner cases. It's just not a sustainable path to language. Understand the way forward was an is machine learning. So instead of hand coding the rules and the AI learns the rules by analyzing a large corpus or piece of text.

Speaker 1:          01:23          This is proven to be very useful and applying deep learning to NLP is currently the bleeding edge. So when thinking about what tool to demo in this video, I was really torn between an NLP API called API to AI and Google is newly released English parts or a [inaudible]. Wow. Api Dot. Ai Takes an input query and returns and an analysis of the text Carsey has, Google is newly released English parser. Both have similar functionality, but I'm going to go with Parsi because a, it's currently the most accurate parcer in the world be. If you built it into your app, that's one less networking call you have to make, which means you can parse tax offline and see. Building this parsing logic from the source allows you to have more granular control over the details of how you want text and be analyzed. Parsi was built using syntax net, an NLP neural net framework for Google's tensorflow machine learning library, so we could use syntax net to build our own parser or we could use a pretrained parser.

Speaker 1:          02:11          Parsi. Yeah, let's do that. Once you parsed your texts are a whole host of things you can do with it. Let's try it out with our own example. We're going to build a simple python app that uses parts of parse phase to analyze a command by a user and then repeat it back to them. Both word is different. We'll begin by importing our dependencies. Then we'll set up our program to receive in store that user input. The input text has the Corpus. We'll be analyzing. We can get an array of all the part of speech tags and the input text using the Tagore function. So what is part of speech tagging? It's assigning grammatical labels to each word in a given corporates, you know all of those words we learned back in elementary school to take the phrase, I saw her face and now I'm a believer.

Speaker 1:          02:46          If we tagged each word in that phrase individually without looking at the sentence as a whole, we might tag saw as a certain bird, which means this would be a quote from Leatherface. But if we look at this word in the context of the sentence, we realized that it's a different verb. Google trained by interpreting sentences from left to right for each word in the sentence and the words around it. They extracted a set of features like the prefix and suffix, put them into data blocks, concatenated them altogether, and fed them to a feed forward neural net with lots of hidden lake, which would then predict the probability distribution over set a possible pos tags and going in order from left to right was useful. Since they could use a previous words. Tag has a feature in the next word. So what does a parse tree, what part of speech tagging isn't enough?

Speaker 1:          03:25          There's another part. The meaning behind some piece of tax isn't just the type of word that's being used, but also how that word relates to the rest of the sentence. Take the example. Freights. He fed her cat food. There are three possibilities of what this phrase could mean. Number one, he had a woman's cats, some food. That's the obvious one to us, intuitive humans, but there's also a number two, he found a woman. Some food that was intended for a cap or number three, somehow encouraged some cat boot to eat something. The meaning of the sentence depends on the context of each work. The team use something called the head modifier construction to sort ward dependencies. This generated directed arcs between words like that and cat cat being a direct object. The word fat. The sentence starts out unprocessed with an initial stack of words.

Speaker 1:          04:02          The unprocessed segment is called the buffer. As the parser encounters words as it moves from left to right, it pushes words onto the stack. Then it can do one of two things. It can either pop two words off the stack attached to the second to the first, which would create a dependency arch pointing to the left and push the first word back on the stack or create an arc pointing to the right and push the second word back on the stack. This will keep repeating until the entire sentence is processed. The system decides which way to point the art depending on the context I. E. Previous pos tagging. Once that's done, it uses the sequence of decisions to learn a classifier that will predict dependencies in a novel Corpus. It applies a softmax function to each of the decisions which normalizes or adjust them to a common scale and does this globally. By summing up all the softmax scores in log space. So the neural net is able to give probabilities for each possible decision. And a heuristic called beam search helps decide on the best one when predicting, once we have our parse tree and parts of speech variables, let's store the root word and the dependent object and to their own variable will call us in an API to retrieve a synonym for the dependent objects. Then construct a novel sentence that repeats the command that users entered back to them in different words.

Speaker 1:          05:08          Looks like it works pretty well. The scope of what you can do with this is so vast. You can use this text analysis to create a tech summarizer award recognizes the intent of a query or understand if a review is positive or negative, or my personal favorite. Create a political debate back checker links with more info below, please subscribe for more NL videos. For now, I've got to go fix a buffer overflow, so thanks for watching.