Speaker 1:          00:14          See, let's see everybody

Speaker 2:          00:17          okay.

Speaker 1:          00:17          Um, minimize myself. I don't need to see myself. I need to see you guys when you go see you guys. Oh, I am hired for this live session. Yo class is in session. Everybody. We are about to do some math with a lot of sessions so I'm super excited. But first of all, let me take some roll call. All right, so let's see. Calling Brandon Neel David because Sebastian, Raj Spencer and the rash, Nico Clement.

Speaker 1:          00:53          Hi Guys. Michael Benjamin. Alright, so that was road call. Uh, welcome to this live session, uh, for the, for the deep learning deep learning course. Okay, this is going to be so awesome because I had been waiting to do some math and guess what guys, guess what? I bought this a tad to write some math on. Okay. And I've never used this before. The light turns off. Super excited for this. I'm going to show you guys the math behind linear regression. By the end of this video, you guys are going to, are going to know like the back of your hand, how to do linear regression. That includes gradient descent. And guess what, we use gradient descent all over the place and machine learning. Don't worry. If you don't know what that is, I'm going to show it to you. Okay. So we're going to deep dive into this.

Speaker 1:          01:44          So, um, we're gonna start off with a five minute Q and a like always. And, uh, I think we've got some new Udacity peeps in the house as well, uh, through, uh, Nico and, uh, Max, who's a d the other instructor for the course. So, uh, you, Udacity, you guys, if you're, if you're, I think you're, you're here a shout out, like say something so people know who you are. And, uh, so I'm gonna do my five minute Q and a like always. And, uh, I'm going to answer all the questions related to me and you know, my everything. But if you have anything, like you'd ask these specific questions, they will answer those. Okay? So let's start off with a five minute Q and a and then we're gonna get right into the code. And Matt. Okay. Uh, do I have to know about partial derivatives? We are going to do a partial derivative. Um, but I'll show you how that works. Um, I had to cut off cutie pie to catch this. Well, I'm honored. I'm honored a baby girl. Let me see that regression. All right, so that's not a question though. Let's, let's get some, let's, let's get some real questions in there. Some, some quality, some quality questions.

Speaker 2:          02:53          All right.

Speaker 1:          02:58          Would want to check out my vibe

Speaker 2:          03:00          AI assistant demo. Sure. Yes. I post to get home link in the comments of one of my videos. I read all my comments. I answer all my comments. See, I'm not, I'm not, I'm not fake. You know what I'm saying? I answer all my comments. I'm here for you guys. Have I enrolled in his calculus required for linear regression? Yes. Um, a little bit of talk to it, but I'm going to go through that. Don't, don't be afraid by the word capital. This, this is, this is actually very intuitive. Uh, um, okay.

Speaker 1:          03:25          Can you mention some details about the upcoming as well? Uh, uh, looking to predict your genre from all right.

Speaker 1:          03:45          What basic Max will give you? They don't know. You need to know basic Algebra. Okay. And then we're going to learn the calculus necessary to do this in this video. Okay. Our Games, the future. Yes. I mean, the idea of between generating generative models in general, uh, are really exciting because, uh, there's, you can generate things that don't exist. Um, and that has a lot of potential for art and culture. Dan's can change culture, right? We can generate music, we could generate art. We could generate paintings in ways that humans could best book to understand math behind ml machine learning. A probabilistic approach. That's a pretty good ones. Just not today. Or coding to both. Mostly coatings and you were brushing versus other classifiers like SG DC when you're brushing is definitely easier. Uh, what is no free lunch? It's a theorem. Uh, the no free lunch theorem at a very high level. It's like, while you, you can't, you can't make assumptions, you can't make assumptions. Uh, whenever you aren't doing anything about related to, uh, proving something, just, um, when will you do NLP? Yo, I'm going to do so much NLP in this course. I can't wait for NLP. It's, it's coming up.

Speaker 1:          04:56          Will you cover again? I kind of want to just do gans right now. You know what I mean? Like I'm super excited for against, I will do dance. Um, I will give an intuition why to do gradient descent over over. Yes. I will explain that linear Algebra is the way to go. Yes. What's the difference between psych learn and CF learn? So I can learn and CF learning? Great question. So CFR did say high level wrapper on top of tensorflow? Uh, it's very similar looking to the side. Uh, but second learning specifically is, uh,

Speaker 1:          05:29          it did, it does. So TF one only focuses on deep neural networks, so I could learn, use as a support vector machines and all sorts of other machine running models, whereas TFR is the same kind of, uh, it has the same brevity, but it focuses only on ignoring networks. Um, do you prefer a WeChat? No. No? No. Oh, when will you start? When will you start working on Anaconda? Uh, I mean, I'm mostly almost like they start using docker to containerize things. Alright. Wrap for 50 k subs. Okay. When you wrap through the DCA stops, and this time I'm going to uh, uh, play an instrumental on, I'm just going to wrap with no, you know what I mean? Don't be discouraged. Rap, hip hop, instrumental on youtube, whatever tests playing someone say a key word and then we're gonna get started. Try on hip hop comes from around. What is this kind of scale? All right. Can you just unplugged my mind? So you got, yeah, you guys can see this.

Speaker 2:          06:21          Okay.

Speaker 1:          06:22          It wasn't music. What does music? Dad's 50 cakes. I got 50 cases. My mind is still fresh. Looking at this copy mass looks like the best. I got a USB four mile by. I'm going to be waiting back online. I see it. It's all fine. That's all right. These equations online coming back like Russian radio. So, okay, so that was it. Oh, see now we're going to get started with the code. Okay. So let's go ahead and do this.

Speaker 2:          07:04          Okay.

Speaker 1:          07:04          I'm going to start screen sharing and then we're going to get started. Um, all right, here we go.

Speaker 2:          07:11          Yeah,

Speaker 1:          07:11          here we go. We'll hang outs. All right. And what does hangouts want to do? Hangouts wants to screen share out. It's going to screen share entire screen share. All right? So I'll minimize this.

Speaker 2:          07:28          Okay.

Speaker 1:          07:28          And minimize, and then I'll move this out of the way so I can see what you guys are doing. A, okay. And we're going to code this baby. Okay. I'm in the corner here. I mean, make sure that what you guys are saying is what I want you to see.

Speaker 2:          07:44          MMM.

Speaker 1:          07:46          Yes. What you guys are saying is exactly what I want you to see. Perfect. All right.

Speaker 2:          07:54          Okay.

Speaker 1:          07:55          Okay. So here we go.

Speaker 2:          07:58          Okay.

Speaker 1:          07:58          Here's what we're going to do. Beds. I mean, make the state that's a success. It's a big enough, right? Okay. So in this lesson we're going to do linear regression, okay? And what is linear regression right now? Okay, so linear regression in this case. Um, and let me just make sure everything's working. Um, everybody's here. Electronic working live. Video's not working. Okay? So here's, here's how it goes. So we're going to do this. Okay. Uh, so this is going to be called linear regression. This is when your aggression, and let me, let me, uh, let me just show you guys the best way to explain it. It's a show to visual. So I'll show it to visual what exactly we're going to be doing and to show you visually, I will give you a late to this and I will just show it right here.

Speaker 1:          08:39          This is what's happening. So we have a set of points and these points are, uh, these, these points are the test scores of students and the amount of hours study. Okay? So this is what it looks like. So this right on the right, this rack here, uh, these set of points, uh, on the set are the x values are the amount of hours the study and the y values are the test scores they got. Okay. And intuitively to us, there must be some kind of correlation between these two values. But we want to prove this programmatically. We want to prove, I mean, sorry, mathematically we went to prove that there's a relationship and how do we prove that there was a relationship? We draw a line of best fit. So how do we know what that line of best fit is? That linear regression is what we don't know.

Speaker 1:          09:20          We don't know. We went to, we have to find that. And the way we're going to find the line of best fit, it's using gradient descent and that process, that training process looks like this. We're going to draw on a random line, compute the error for that line. And I'll talk about how we're going to compute that error. And that error value is going to say how, how wellfit is this line to the data? And then based on that error is that it's going to act as a company, it's going to tell us, well, how best should you redraw the lines to be closer to the line of best fit. And we'll keep doing that. So it will be like draw line, uh, compute hair, throw line, compute Eric, draw line, compute era until eventually the line that we drop is the optimal line that we should draw.

Speaker 1:          09:58          Okay, so that's at a very high level. But now I'm going to go into the code and we're going to talk about this in detail. All right, so let's go ahead and start it. Um, so to start off, to start off, I'm going to write my main function. Okay? So let me move all this stuff out of the way. It's okay. Right into the code. All right, I'll get right into the code for this. And guys are also, if people have questions and I'm not able to answer them because I'm busy doing separately, please help me answer questions. I very much appreciate it. I very much appreciate it. Okay, so let me just show up by writing the main function. Well, why am I, what is the main function here? Well that's, that's where the read of the cocoa, right? Okay. So we had the main bumps.

Speaker 1:          10:42          We'll write or run function, which is where we're going to store all of our logic. Okay. So let's write up to run functions. So the run functions are a chance for us to show what we're doing at high level, at a high level. So step one is collect our data, right? Always in machine learning. We want to collect our data. So we'll get our data points and we're going to do, oh, what, how are we going to collect our data, right? Well, the collect our data, we have to import the one library that we're using. I know guys were using eight single library and that library of reasons. None. All right. And we're gonna use this little symbol. That means you don't have to continually say hi whenever we call it a method or assumptions. Okay. So what is the function we're going to use from Nepal? So the bunch I'm going to use from num Pi,

Speaker 1:          11:27          sorry. Right Maine. Thank you. Good call. So the front door when they get from [inaudible] hi, it's Jen from tests. And what this is going to do because it's going to get the data points from our uh, David, uh, data file. And let me show you guys the data file as well. But basically we're going to separate, separate it by the comments. Okay. And we're going to get those points. So what, what does this, what does this data look like? Well, let me pull up terminal and show you guys exactly what this data looks good. It looks like eight A. Dot. CSV. Okay, so let me, let me zoom out of this.

Speaker 2:          12:00          Wow.

Speaker 1:          12:01          Do way more do you want him to say? So these are just the hour study on the left side and then the test scores for a bunch of students for our intro to computer science class. Okay. The hour study and the test scores they got. Okay. So that's what we're going to pull. That's our dataset. That's what we're going to pull into our uh, points variable. So it's a bunch of points. Is going to contain a bunch of x, y value parents where x is the amount of our study and why is the test score okay? And it's separated by the,

Speaker 2:          12:32          come

Speaker 1:          12:32          on. Okay. So that's, that's that one. We've done that. And you're from Texas essentially, right in two main leads. The first loop converts each line of the bond with sequence strings and the second one is converting each string into the appropriate data tab. Okay. So that's step one. Now, step two is to define our hyper parameters. Okay. In machine learning we have what are called hyper parameters. These are tuning knobs for our model. They are basically the parameters that define, you know, how our model is, is analyzing certain data, how, how fast it's fitting to the data, what, what, what operations are performing on a day. There's a whole bunch of hyper parameters. Um, thank you for the feedback. There's a whole bunch of hyper parameters and what we're going to use just the learning groups. Now the learning rate is used a lot in machinery and you, and it basically defines how fast, how fast should our uh, model converged.

Speaker 1:          13:29          And converged is a word for convergence. Convergence means when you get the optimal results, the optimal model, like the line of best fit in our case, that's convergence. So how fast should we converge? Well, you might be thinking, well, should the learning rate just the like a million if you went to convert super fast? Well No, like all hyper parameters, it's a balanced, okay, so get the learning rate is too small. We're going to get slow convergence. But if it's too big, then our air function might not decrease. Okay. So it might not converge. So that, so that's our first hyper parameter. Our next type of Predator is going to be the initial value for B and the initial value for m. And what does bnm well, what we're going to do is we're going to calculate the slope, right? So this looks like y equals mx book.

Speaker 1:          14:16          So this is why I said we only need to know basic Algebra. This is the formula. This is the slope formula, okay? All lines follow this formula where, why? So m is the slope be as a liner set. X and y are the points. Okay? So that's the line. Okay? So this is our initial, the value or initial slope and our, and our initial y intercepts, they're going to start off as a zero. Okay? So, and then the last type of parameter is then it's going to be the number of iterations. How much, how much do we want to train this model? Well, we have a very, very dataset. There's only a hundred points. Okay? Um, and for that, we're not going to, we're not going to iterate a million times or 100,000 times. We're just going to iterate a thousand times. Okay. So that's our type of parameters. And now step three is going to be too,

Speaker 2:          15:08          yeah.

Speaker 1:          15:09          Uh, they are trained on model based print, our motto trainer model. Okay. So the first step is going to be to show this starting gradient descent. Okay. At v Equals um, what does it starting gradient descent. It's going to be zero, right? And then Emma is going to be that starting point is going to for that we'll say one, this is just for us to see the difference here. Okay. Um, right dot format initial, the initial m I so, um, okay, so what's happening here? Error or line given points. So let me just write this out. It will be,

Speaker 1:          16:11          and then appointments. So what is, what's happening here? Let's go over what I just wrote here. So in this line we're going to show the starting a B value to starting m value. So what is our starting line and separate as our starting slow and what does our starting error. And I'm going to show you how we're going to calculate that error and to get that error given our be, uh, given, given our B and m values, we have dysfunction. You're called compute air for Lynette given points. It's going to take a beat em and the points and it's going to compute the era for that and it's going to help with that. So that's going to be our starting point. Okay. And then now we're going to actually perform, are graded descent and it's going to give us the optimal slow B and the optimal a slope.

Speaker 1:          16:54          I sorry. It's going to go see optimal slogan, the optimal wider sense. So, because great at the sentence, uh, we're gonna have, we're going to call this method the gradient descent referred. So given points given an initial BW and initial, um, uh, and so an initial end value in our learning rate. So this is where we're going to use all the kinds of printers, right? Cause this is where we're training our models. So a number of iterations. Those are all the things we need to this. Okay. And we're going to define these functions in a second. Okay. We're going to, we're going to go deep dive in and defined exceptions. Okay. So then after we've paid our model, well now we can just try it out, right? So let me just copy and paste this. So now this is not our starting point. It says now are, uh, nd ratings, any point. So face or any point, where are we? Is this a B as to ms too? And an Arab is three. And this is with these numbers just defined, uh, what, uh, we're going to see at the end after a number of iterations or B and then

Speaker 1:          18:02          four. And then for computing the air for line at given points, be that the final value, the final 10 value, and then our cost. Okay. So,

Speaker 1:          18:15          okay, so that is high level what's happening here. So all I did was I just printed out the initial BNN value, which just nothing. And then the air. Uh, and then I computed the gradient descent and then I print out the final values. So I'm about to, I'm about to do this now. Okay. So we haven't actually done, now we're going to do it. So the first thing I'm going to talk about is how are we going to compete that here? Okay. So let's, let's, let's, um, right at that first function, what would that person don't you call? It was called compute error per line given points. Okay. Okay. So, and the data set, I'm going to provide that as well. Um, but, but let's, let's go ahead and run up this method. Okay. So, so this is the first step. We're going to write up this method. You, Eric, for Atlanta I can employ. So excited to show you guys is because I get to use my mac panicked for a second. Okay. So, so let me, let me, right. Okay, hold on. Okay, here we go. Okay. So, um, okay, let me right. Okay, so we got a line here.

Speaker 2:          19:19          Okay.

Speaker 1:          19:20          Oh Man, what a great, what a great line dies is. Okay, so this is our plot. Okay. And so we've got a bunch of data points here. We've got a bunch of data points, right? It's all over the place. And what we're gonna do is we're going to draw a random lot through that data. Okay? We don't know, we don't know the line of best fit. So we're going to go on a random line through the data and then we're going to compute the error of that line. And so that error is going to tell us how good our line hits. Okay? And so how do we know how good our line is? Well, what we're gonna do is we're going to for every single, um, why value on that line, we are going to calculate the distance from each point, uh, that data to go along.

Speaker 1:          20:01          Okay? So all of these distances, all of these distances, instance wine, this is two, this is three, this is four, this is five and six all. And then, you know, we probably have more data points down here, these distances that this is his life. And so we're going to take all those distances and once you someday. And so let me show you the equation for that. Okay? So rather than actually writing out this equation, uh, like really sloppily, I'm with the show it to you, um, uh, using this. Okay? So, okay, so this is the equation. So let me explain what this is, what this says. So we got all those distances, right? We got old as distances, we're going to some of those distances together and that, and then get the average of that. But guess what, we're not just going to have some those values alone.

Speaker 1:          20:47          What do you square those doubts? And why are we swearing as values? Because we're squaring those values because we want and we want to first of all, to be positive. Uh, and it doesn't really matter what the actual value is. It's more about the magnitude of those dogs, right? So, and we went to minimize that magnitude overtime. So this is the equation for that. Okay? So let me explain what this, what the hell does say is, okay, so we're computing the error. We are computing the error of our line given m and d. So given em and B, we're going to compute the care line, MSR slope and be as our wine upset. So this e looking thing is called, it's called sigma notation. It's a little rich giving you guys a little refresher here this evening. We're going to see a lot of machine learning.

Speaker 1:          21:28          It's, it's, it's, it's called sigma notation and basically it taught it. It's a way of describing Capelin the sum of a set of uh, of a set of values. Uh, all right, so the sum of a set of values, which is what we're doing, we're talking to some of a set of points. So for, and so it, the starting point is where I equals one and the ending point heads for an end. It's for every point. Okay. So for every point we went to calculate the difference in, in why dogs, right? So why minus mx plus B, and why do we say mx plus B? Because in the flip equation, y equals mx plus B, right? So it's y minus MSSP, which the SEC, which essentially boils down to just one. So it's y minus y square and we'll do, and then we're doing that for every single point. And so we're going to add all of those points together. Okay? So and then get the average. And so that's why one over n because we're going to get the average of that. And that's value. That value is the error. Okay? So at a high level, that is what that is. So let's now, let's programmatically write this out. Okay. So we're going through startup by initializing the hair, initialize it at zero, okay? So our total error at the store is just gonna be zero. There's, there's not anything that's, that's um,

Speaker 1:          22:45          uh, we don't have an area. Okay. So then for every point, every point, so for, I've been range of starting at zero and then going for the length of the points, right? To all of our data points. So for every data point that we have, we're going to say, let's get the x value. All right, so let's pick that x value. So x equals points I zero and then we're going to get that. Why di you? Right? So then the Wagga, right? So I'm just basically programmatically showing what I just talked about. Mathematical, right? So we, we've, we've got the x value, we've got the Y, y values, and we went to, uh, compute that distance, right? We're going to do this every single time. We're, okay. So,

Speaker 1:          23:33          so then the difference squareds and then and add it, add it to the, to the total. Okay. So, so here's that. Here's the actual equation, right? So we're going to, we're going to be plus people because it's a summation and we're going to programmatically show what I just talked about right here, right? Y minus mx plus B Square, okay? And we're gonna get to some of that. So why minus m times x Plus B? Clear. Okay. And we're going to do that for every point. So this whole interracial group right here, is that equation, okay? Minus the average part. So that's gonna to give us the total value. The last part was do averages. So we'll say total error divided by floats. The length of the point, cause we wanted to be a flood

Speaker 1:          24:31          backs is the equation. That is the equation right there. Okay? So, and then if the average, the average, so that, so this a 10 line function, just describe what I talked about right here and his math equation. Okay. We got, we saw all the distances between all those points. As I showed rights here, we sum them all up, we squared them, and then we, uh, we got the average and that is our error. Okay? And so that it's so, and we're calculating that because we, we want a way for us, a measure of us, something to minimize over time, right? We, something to minimize. Every time we redraw our lines, we want to minimize this error because this era basically is a signal. It's a compass for us. It's, it's telling us this is how bad your line is. You need to get better. You need to make me smaller.

Speaker 1:          25:22          I'm really big right now. Make me smaller. And that's what Grady, the sense does that what gradient descent does? And I'm going to talk to you, I'm going, I'm gonna explain how rated descent works in seconds. But that's, that's that first option, right? So what, okay, so what was the second doctor we wrote? It was called gradient descent runner. So this is our actual, uh, our grading dissent function. Okay. So let's now let's write this out. Okay. Which is our second of three methods, um, before we're done. Okay. So gradients percent. So given a set of points, do you have a starting value for be given a starting value for and even our learning rates and given our number of iteration, if we're going to use all of these things to calculate radiant descent, we're going to use every single frame. Okay?

Speaker 1:          26:13          Okay. So let's get back to starting a, B and m valid. Okay. So the starting value for me, we're going to say it to be, and the starting value for, and we're going to say it too. Okay. Simple enough. And now we're going to perform graded descent. What is great in this at high 10? Not wait to explain. Great to send guys, I found the perfect analogy, great insight and I'm really excited. Okay. So, um, so, uh, okay. So before we, before I explained that let's just, I took perform you ratio because you actual math is third and last function to them about the rights. So for every single iteration that we are fine, we're going to perform what's called gradient descent. So we're going to upbeat P and m with the new, more accurate e and m by for for me, uh, the gradient is right before me this gradient step.

Speaker 1:          27:03          Okay? So we and M, we're going to return and by performing this race, this race that we're not going to explain, this is the way the map is happening given our current VR current men are the array of points that we have. And then finally given to learning group, we're going to talk like that final value of PNA. And guess what? Once this grading dissent is done, we're going to return that optimal he and head, right? And so that's what we talked about at this bottom part, right? It returned that optimal BNF value performing right in a sense. And we then printed it out because that optimal be an end value gave us a line of best fit. We plugged them into the y equals mx plus B equals a formula. It gave us a line of best fit. So now we're going to write out the gradient step.

Speaker 1:          27:50          Okay. And this is gradient mother epic descent. Okay. So this is how it's going to go dad. Okay. All right. Here's how it's going to go down. Separating. So I'm just going to say it's not for the magic, the magic, the greatest, greatest. Okay. So that's how excited am I just wrote the greatest points. Okay. So, okay. So given our DNM values points and the learning rapes, this actually isn't going to help with them. So I'll delete that. So given our learning rates, okay, let's perform ray to say, okay, what is gradient descent? Okay, so let me show you guys, this is so okay. How Big Way? Um, describe it. So we have, let me just show you this image. This is going to help a lot.

Speaker 1:          28:47          Okay. So this is a brass. Okay. So let's just look at the graph on the, um, I mean it's, if the same wrap, it's looking at it from two different angles. It's the same graph. Okay. So let's look at the, let's look at the one on the left. Just A, just to pick one. It's the same graph though. We have a bunch of y values and I'm sorry, a bunch of B values and a bunch of em dots. And then we have that error right and error. Then I just talked about, right. So given the two d graph would be given, are any, every single wide insect we could have given every single m value we could have. What is the error? Okay, so for every y intercept and slope pair, what is the air? And so we'll, we'll find this is a three dimensional graph. This is a three dimensional graph because of the error value. Yeah, it's, it's, it's, it's kind of like, it starts off high and then I do approach what's called the local minima in our case, a local minimum, which is these small dec dot point at the very bottom. That is our, that if we're trying to get to, okay. So,

Speaker 3:          29:46          okay.

Speaker 1:          29:46          Given a set of wine slacks and given us, given a set of sloth possible, why does that's impossible. Slopes. We went to compute the error for for those two things and if we were to graph that, who are to grab the relationship between these three things, it would look like this. Now it tends to always look very similar to this in more complex cases. We have like many minimum, we have many little valleys, but what we're trying to do is at that point where the error is smallest and so how do we get that point where the Arab, the smallest, well, we were going to perform what's called gradient descent to get that smallest point, that valley, that smallest point and a great analogy for this, a great analogy for this is a bowl surgical. Okay?

Speaker 1:          30:30          Okay. In fact, it's kind of like a bowl. It's like we drop a ball into a bowl and we want to find that coins were the, there were the ball stops. That end point, the the the, the lowest point. That's that DNM value is our optimal line of best fit. Okay. And the way we're going to get that is gradient descent and we're going to descend, right? We're descending down the bolts using the gradient and gradients is another word for slope. We're going to descend down that goal until we get through iteration. That's that lowest point. And gradient descent is used everywhere in machine learning. Okay. It is like the optimization method for deep neural networks. It's not that apparent right now, but no, this no and understand grading dissent like the back of your hand because it is going to be very useful in the future. Okay. So I don't know why I'm drinking out of the equation. That was, that wasn't necessary. That was a, that was the equation for the sum of squared errors that we just talked about. Some square distances. Okay. So so, so how are we going to calculate that gradient? The descent? Well now let's, let's actually do it. Okay. So

Speaker 1:          31:36          for our separate in function, we'll start off with um, an initial grading value for our Pete. So these going to be zero and so then m's gradient if going to be zero as well. Okay. You said the starting point for our gradients and Brady means slow. And so the gradient, the gradient is going to act like a compass. It's, and it's going to always point downhill. So this is what I mean by, once we tap into that error, it's going to act as a compass for us. It's going to tell us where we should be going, what direction we should be going, how we should best redraw our lines. So for, okay, what someone has, why was the lowest point the best? The lowest point is the best because it is worth it. Our error is the smallest. And when our error is the smallest, that's, that's when we'd have the line of best fit. When the air is the smallest that be an m value, those two, what we plug into our slope equation is going to give us the line of best fit. So that's why we're calculating the error. Okay? So, so for I range zero to the length of points. Okay. So we're going to get, so what we're gonna do is we're going to iterate through every single point on our scatter plot. Okay? So every single data point that we have, we're going to collect it. Okay? So we're going to say, okay, what is, so for the first point, right first point, which gives us x and Y. Dot. X value and a wider, so let me also write up a little comment to that starting point once for arc, okay?

Speaker 1:          33:11          Now we're going to get the direction with respect to the n no, this is uh, uh, the last part, but it's a very, very important for part, and this is where calculus comes into play. Okay? So I'm going to talk about how we're doing things. Okay? So let me talk about what, what we're, what we're about to do. So what we're going to do is so timid. So for every single point, for every single point that we have, we're going to calculate what's called deep partial derivatives. Okay? It's called the partial derivative with respect to be, and with respect to m, okay? And what that's going to do is it's going to give us a direction to go for both the B. Dot. M. Dot. Right? So remember, I remember in this rack, we want a direction, right? We want to, we want it to be going down to grading.

Speaker 1:          34:05          And so, right, and so and so, right? So on this left hand side, you see this radiant search. You see the value in the feed values are, are, uh, increasingly in the direction that they should be because Grady at the sec is essentially a search policy. Awesome. We're trying to find that minimum, uh, Eric. Dot. Okay. Um, and what we're going to do to get that is we're going to, we're going to compute the partial derivative with respect to be in that. Oh, and let me show you the equation for the perfect. Terrific. Okay. The partial derivative is going to be right here,

Speaker 1:          34:43          renter. So this is what the partial derivative does. The partial derivative, um, is so, um, little, um, uh, so we call it partial. Uh, we call it partial because I, it's not calling us the whole story, right? We say it's partially because we're calculating will calculate for board both B and. M. There are two different things. And so it's going to give us the tangent line. So it's going to give us this line as you see right here, right? So you just line that line is our direction or we're going to use it to update our DNM value, okay? Okay. So that's what that is. And uh, let me also show you the equation for the partial derivative because we're about to write it out. So here's what the equation for the partial derivative with respect to m and d looks like. Okay? There are two different equations, right?

Speaker 1:          35:36          So let's talk about the one on top. So this little curvy thing that you see up here, that that just signifies that this is a partial Gribbon, but that's, that's the map signifier that this is a partial driven. Now, we talked about sigma notation, right? Because it's a summation of values, right? And that's what we're doing. We're Sony, the partial derivatives for all of our points, okay? Uh, for all of them to compute that radiant value, okay? And, uh, the partial derivative with respect to m and D is going to look like this. So let's, let's write this out. Okay. Um, so the be great and it's going to come with a tube to be great. It's going to be fussed people's, uh, and then what was it? Let me look at the place again. Two N, two over n. So negative two over again. All right.

Speaker 2:          36:26          And

Speaker 1:          36:30          thanks. Good vibes. And then we're going to, and then what was it? It was y minus, right? And you started equations. These are laws. There are beautiful laws that always stayed the same. And then they, they, they, they do, they give us humans, uh, uh, a way of understanding the direction that we want to move it. Okay? So we current situation. Okay. So, all right, so then we'll do the same thing. And what was the second equation? It looks pretty much the same. Minus this. It didn't, it doesn't have this x, right? And the second one doesn't have this x, right? So we'll say, but it does have this to end.

Speaker 2:          37:11          Okay.

Speaker 1:          37:12          It does have this to end and then it does have, mmm. Uh, let's see. Let's have this x. It does have,

Speaker 1:          37:27          why minus m times x Plus E. Okay. Okay. So let me one more time. So you guys, it's giving us directions for it to go for both d and m. And remember the partial there. It's not telling us the whole story and telling us what direction should we go for beef and what direction should we go for x? And it's going to tell the direction. Remember for a bowl to get to that bottom point where that error is the smallest right here. Okay. So right here where I'm, where my, where my mouse is that point, that point is what we want to get to. And that's what the parcel driven is going to help us. So once we, once we've computed the partial derivatives, uh, there's some of them with respect to BNS. Now we're going to, um,

Speaker 1:          38:23          now we're going to update our BNN values, right? So we're gonna use that to update our P and N. Dot. And guess what? This is our last set, which just our last step, using this partial driven, our partners, it is right plural. There's two of them. And that's gonna give us any MRI. So we have our current value for me, whatever it is, and we keep stopping every time. This is where our learning rate comes into play. Okay. This is why our learning rate is so important. Important because it defines the rate at which we're, uh, we're updating our brightest 0.01, right. And then, uh, also our end, current,

Speaker 2:          39:11          yeah.

Speaker 1:          39:12          Yes. Learning rates, times the m gradients and then a bit, and then we'll return those values. And we're doing this every time, right? This is this new me, a new amplifier final. Yeah. It's arts. The septum timber. We're, we're, we're doing this every iteration, right? We're doing this, uh, for, for the number of iterations we had a thousand, but it's going to return a new bnm value every time. And guess what guys? That's it for a coat. That was it. So, so let's go over what we, what we've done. Um, okay. But let me actually, let me, let me check for errors. Right.

Speaker 2:          39:52          Okay.

Speaker 1:          39:53          Let me, let me check for errors and then I'm not answering more questions cause I wouldn't really want to make sure you guys understand how this works. Okay. So let me, let me, let me, I'm done with it. So I thought it up by what we'll need and is not defined. Okay. Oh right. Just what I did define n and is the number of points of points. Okay. So let's go. Learning rape is not defined where, where he's learning were not defined. Learning rate is not defined. Uh Oh wait a sec. Yeah. Oh yeah. Right, right. Okay. What else? What else did that overflow for? Double scalers one 14 air y minus.

Speaker 2:          40:53          Okay.

Speaker 1:          40:54          Uh Huh.

Speaker 1:          41:00          Go learning. Right. But anyway, I told her, okay, so what's going on here? Okay, let's say this. So yeah, I mean print out the final guy. It got our final value. I'm right here. Um, and if we wanted to, let's see, hold on a second. If we want it to, again, our backup here just in case. So, right. So let me, let me, let me blow this up like way, way, way up. And let me, let me just separate it. So this is what our output is going to look like, right? So [inaudible] seconds. Why? Because our data set was so small, it okay if David said was so small, all right. So

Speaker 1:          41:45          that could happen. And after a thousand iterations, we've got the optimal B and m. Dot. So right. And we start off with Dnm is zero and we'd have to lay the error for a random line that we drew and it wasn't huge. But eventually after running rating descents, we've got the optimal optimal m and the optimal and the lowest error point. We said that that smallest point in the bowl. And to do that, we use grading dissent, uh, with respect to be an m. Okay. So let, let me go over one last time. Every single thing that we've just done just to really go over it and then I'll do my last five minutes to an hour. Okay. So we started out by collecting our Dataset, right? Our data set was a collection of a t test scores and the amount of power studies, right? The x y value pairs of test scores and the amount of power, uh, to variable dataset.

Speaker 1:          42:29          Then we define our hyper parameters for our linear regression or learning rate, which talks about how fast we should, we should learn our initial bnm values for the slope equation y equals mx plus B, the number of iterations a thousand because our Dataset is pretty small. And then we ran Brady the sense, so what the graded descent look like. Well, for, for every iteration, for a thousand iterations, we computed the gradients with respect to both d and m and we did that constantly until we got debt optimal BN handout that gives us that line of best fit. Now how do we compute the gradients to do that? We said, okay, we'll, we'll have a starting point of zero for both of those gradients. Remember gradient is just another word for slope. And then,

Speaker 1:          43:14          uh, we said, okay. So for every single point in our scatter plot, for every single point on a scatter plot for our data, um, will compute the partial derivative with respect to both B and m and that, that those two values are going to give us a direction. I sent them a direction where we want to go. How do we get to that lowest point in that bowl, right? That three dimensional graph, that lowest point. And we use the learning rates to determine how fast, uh, we want to update our DNM diet. We got the difference between the current value and what we had before and we returned that. So for every point, and we did that for a thousand iterations. Okay. And that's what it gave us the output, and it looks like visually, visually it looks like this, right? But the desserts done right? It's like up, up, up, up, up, up, up, up, up, up, up. It's kind of like wheel of fortune, right? It starts out fast and it gets slower and slower as it approaches convergence. The word we use when we have optimal line of best fit convergence. See, let me do it one more time just like that. Okay. So that was, that was that.

Speaker 1:          44:18          And now I'm going to screen share and do a last five minute Q and. A. All right. Stop Spring Chair. Hi everybody. Okay, let me, let me bring you guys back on screen and do my last five and a two and a ask me anything. Uh, and uh, yeah. How's it going everybody? Any questions? I'm open to questions.

Speaker 1:          44:49          Where did I use [inaudible]? It's at the very top. It's right. What's the practical use of linear regression? Great question. Anytime we want to find the relationship between two, um, two different variables and then, you know, in more complex cases there could be more. But, uh, we want to prove mathematically, right? Math is all about proving things in a way that is unfair certifiable that no one can say, hey, that's not true. Well, I can prove it mathematically. So it's a way to show the, uh, the, the relationship between two value parents, right? So maybe housing prices and uh, the, the time of year, right? So what is the real estate market going to look like? Or, uh, you know, anytime that they're intuitively, you'd think there was a relationship, you can prove it with linear regression and, but really I did this to show gradient descent, that optimization processes that is very popular in deep learning and we're going to use that in our deep neural networks and in the end, the rest of the course.

Speaker 1:          45:45          Okay. Um, why aren't you biased towards Google? I mean, it's not really, uh, yeah, I mean, tensorflow is awesome. It's, it's not like, it's not like Google's like Sarraj I want you to talk about it. I just tend to flow is the best, uh, deep learning library out there right now. That's what, and of course it wouldn't be because Google knows what they're doing. They, they handle billions and billions of queries every day. They have to be able to do machine learning at scale, the problems they solve, problems that no one else has even thought of. Salts. And all of those solutions are found in tensorflow for machine learning. Please make an AI doctor, you can create a classifier to classify between, um, different types of disorders that you see in an x ray. I that's going to augment doctors at first but eventually replace them. Uh, how about fitting a quadratic curve instead of a linear line? Uh, we could do that as well. I'm going to provide a data set and the code, uh, I can talk slower. Sure. How to find the optimum learning rate. Uh, that's a great question. Um, there's several methods of doing that, but that's great intuition. Sometimes we can, we can, uh, use machine learning to find the optimal hyper parameter. So it's kind of like machine learning for machine learning. I all, we'll talk about that later.

Speaker 1:          47:03          This is the first light section at the Udacity course. Uh, teaches calculus. I'll do more of that in the future. I'm going to be to keep, I'm going to keep doing calculus. Okay. Um, two more questions then we're good to go to more.

Speaker 2:          47:19          Okay.

Speaker 1:          47:20          How would you recommend me to start machine learning? Watch this series and watch my learn python for data science areas. Watch my hits, right. Attention series. Watch. Um, my machine learning for hackers series. Watch my videos. Uh, why is your view Udacity too expensive? I didn't decide to price guys. I tried to get hello with whatever you got paid. You got paid graders for that. Okay. In grading is not cheap human raters. But look, all the videos are going to be released here on my channel, so, all right, so I'm here for you guys. Okay. I'm trying to grow my current role myself. So Raj robble. Okay. This was the end. Okay. So that's it for the questions. All right, so for now I've got to shoot a fighting scene for my next video. What? Yeah, so thanks for watching. I love you guys. I'll post a link in the comments right when I'm done. All right. In the video description, I posted a link and then the Dataset, everything is going to go into the district description within the hour. All right. Bye.

Speaker 2:          48:36          Okay.