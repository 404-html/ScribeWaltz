Speaker 1:          00:00          Can we actually predict stock prices with machine learning? Investors make educated guesses by analyzing data. They'll read the news study of the company history industry trends. There are lots of data points that go into making a prediction. The prevailing theory is that stock prices are totally random and unpredictable. A blindfolded monkey throwing darts at a newspaper, financial pages could select a portfolio that would do just as well as one carefully selected by experts. But that raises the question, why do top firms like Morgan Stanley and Citi group higher quantitative analysts to build predictive models? We have this idea of a trading floor being filled with adrenaline and abused men with loose ties running around yelling something into a phone. But these days are more likely to see rows of machine learning experts quietly sitting in front of computer screens. In fact, about 70% of all hoarders on Wall Street are now placed by software.

Speaker 1:          00:53          We're now living in the age of the algorithm. Hello world, it's Saroj and today we're going to build a deep learning model to predict stock prices, records a prices for traded commodities go back thousands of years. Merchants along popular silk routes would keep records of traded goods to try and predict price trends so that they could benefit from them and finance. The field of quantitative analysts is about 25 years old and even now it's still not fully accepted, understood or widely used, just like Google plus. It's the study of how certain variables correlate with stock price behavior. One of the first attempt at this was made in the 70s by two British statisticians and box and Jenkins using mainframe computers. The only historical data they had access to where prices and volume they call their model or Rema, and at the time it was slow and expensive to run, but by the eighties things started to get interesting.

Speaker 1:          01:45          Spreadsheets were invented so that firms could model companies, financial performance and automated data collection became a reality and with improvements in computing power models could analyze data much faster. It was a renaissance on Wall Street. People were excited about the possibilities. They started showing up at seminars and discussing their techniques. You should see what's going on at the bigger firms. I mean, I know all the information, but all of this quickly died down. Once people realize that what works is actually a very valuable secret. Alright, get the fuck off. Since then, the most successful quants have gone underground. In the past few years, we've seen lots of academic papers published using neural nets to predict stock prices with varying degrees of success. But until recently, the ability to build these models has been restricted to academics who spend their days writing very complex code. Now with libraries like tensorflow, anyone can build powerful predictive models trained on massive data sets.

Speaker 1:          02:39          So let's build our own model using kiosk with a tensorflow backend. For our training data, we'll be using the daily closing price of the s and p 500 from January, 2002 August, 2016 this is a series of data points indexed in time order or a time series. Our goal will be to predict the closing price for any given after training, we can load our data using a custom load data function. It essentially just reads our CSV file into an array of values and normalizes them. Rather than feeding those values directly into our model. Normalizing them improves convergence. We use this equation to normalize each value to reflect percentage changes from the starting points, so we'll divide each price by the initial price and subtract one when our model later makes a prediction will denormalize the data using this formula to get a real world number out of it. To build our model, we'll first initialize it as sequential since it will be a linear stack of layers. Then we'll add our first layer, which is an LSTM layer. So what is this? Let's back up for it.

Speaker 2:          03:39          Recognizes feet single Eric's with me. You don't have to say what you did I already know sound out from it's easy.

Speaker 1:          03:49          Call the words forward, but could we sing them

Speaker 2:          03:52          words?

Speaker 1:          03:55          No. The reason for this is because we learned these words in a sequence. It's conditional memory. We can access a word if we access the words before it. Memory matters when we have sequences are thoughts have persistence, but feedforward neural nets don't they except a fixed size vector as input like an image. So we couldn't use it to say predict the next frame and a movie because that would require a sequence of image vectors as inputs, not just one. Since the probability of a certain event happening would depend on what happened every frame before it. We need a way to allow information to persist and that's why we'll use a recurrent neural net. Recurrent Nets can accept sequences of vectors as inputs, so recall that for feedforward neural nets, the hidden layers weights are based only on the input data, but in a recurrent net. The hidden layer is a combination of the input data at the current time step and the hidden layer at a previous time step.

Speaker 1:          04:50          The hidden layer is constantly changing as it gets more inputs. And the only way to reach these hidden states is with the correct sequence of inputs. This is how memory is incorporated in and we can model this process mathematically. So this hidden state at a given time step is a function of the input at that same time step modified by a weight matrix like the ones used in feed forward nets added to the hidden state of the previous time step multiplied by its own hidden state to hidden state matrix, otherwise known as a transition matrix and because this feedback loop is occurring at every time step in the series, each hidden state has traces of not only the previous hidden state but also of all of those that proceeded it. That's why we call it recurrent in a way we can think of it as copies of the same network, each passing a message to the next.

Speaker 1:          05:37          So that's the great thing about recurrent nets. They're able to connect previous data with the present task, but we still have a problem. Take a look at this paragraph. It starts off with I hope Senpai will notice me and end with she is my friend. He is my Senpai. Let's say we wanted to train a model to predict this last word given all the other words. We need the from the very beginning of the sequence to know that this word is probably Senpai, not something like buddy or mate and irregular recurrent nets. Memories become more subtle as they fade into the past. Since the error signal from later time steps doesn't make it far enough back in time to influence it. Network at earlier time steps during backpropagation Yoshua Bengio called this the vanishing gradient problem in one of his most frequently cited papers titled Learning Longterm dependencies with grading.

Speaker 1:          06:24          Dissent is difficult. Love the bluntness. A popular solution to this is a modification to recurrent nets called long short term memory. Normally neurons are units that apply an activation function like a sigmoid to a linear combination of their inputs in an LSTM recurrent net. We instead replaced these neurons with water called memory cells. Each cell has an input gate on output gate and an internal state that feeds into itself across time steps with a constant weight of one. This eliminates the vanishing gradient problem since any gradient that flows into this self recurrent unit during back prop is preserved indefinitely since errors multiplied by one still have the same value. Each gate is an activation function like sigmoid. During the forward pass the input gate learns went to late activation pass into the cell and the Applegate learns went to let activation pass out of it.

Speaker 1:          07:13          During the backward pass the Applegate Lauren's went to let air flow into the cell and the implicate ones went to let it flow out of the cell through the rest of the network. So despite everything else in a recurrent net staying the same, doing this more powerful update equation for our hidden state results in our network, being able to remember longterm dependencies. So for our LSTM layer, we'll set our input dimension to one and say we want 50 units in this layer. Setting returned sequences to true means this layer is output is always fed into the next layer. All Its activations can be seen as the sequence of predictions. This first layer has made from the inputs sequence. We'll add 20% dropout to this layer. Then initialize our second layer as another LSTM with 100 units and set return sequence to false on it. Since it's output is only fed to the next layer at the end of the sequence, it doesn't output a prediction for the sequence.

Speaker 1:          08:04          Instead a prediction director for the whole input sequence, we'll use the linear dense layer to aggregate the data from this prediction vector into one single value. Then we can compile our model using a popular loss function called mean squared error and use grading dissent as our optimizer labeled rms prop. We'll train our model with the fic function. Then we can test it to see what it predicts for the next 50 steps at several points in our graph and visualize it using map plot line. It seems that for a lot of the price movements, especially the big ones, there is quite the correlation between our model's prediction and the actual data. So time to make some money and play some too, but we'll our model be able to correctly predict the closing price 100% of the time. Hell to the no. It's an analytical tool to help us make educated guesses about the direction of the market that is slightly better than random.

Speaker 1:          08:54          So to break it down, recurrent nets can model sequential data. Since at each time step, the hidden state is effected by the input and the previous hidden state. A solution to the vanishing gradient problem for recurrent nets is to use long short term memory cells to remember longterm dependencies and we can use LSTM networks to make predictions for time series data easily using carrots. And tensorflow. The winter of the coding challenge from the last video is the Shaw bought you. We shall use transfer learning to create a classifier for cats and dogs. He chose a layer from a pretrained tensorflow model and built his own custom convolutional net on top of it to make training much faster. Wizard of the week, and the runner up is Jeshurun. See, I loved how he added a command line interface for users to input their images. The coding challenge for this video is to use three different inputs instead of just one to train your LSTM network to predict the price of Google stock detailed during in the read me poser gambling in the comments, and I'll announce the winner in a week.

Speaker 3:          09:51          Please subscribe for more videos like this. And for now, I've got to count my stacks of layers, so thanks for watching.