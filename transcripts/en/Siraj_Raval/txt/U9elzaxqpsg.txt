Speaker 1:          00:00          Hello world, it's Raj. And have you ever wanted to create songs using AI? It's totally possible. And I've got my friend Taryn here to help me explain how it all works.

Speaker 2:          00:10          Thank you for having me at the time. I've been watching your videos for a while and very excited about this moment in time, so thank you. Hey everybody. I'm Taryn southern and I've been a youtuber for quite a long time. Got My start in making music videos on Youtube about 10 years ago and recently released my first single off of my upcoming album, which is composed entirely with artificial intelligence as Suraj said. So really been exciting to work with this new medium.

Speaker 1:          00:40          Totally. It's super cool. You guys got to check it out. Links are going to be in the description. Okay. So why did you want to create songs using AI?

Speaker 2:          00:48          I think it, it, it seemed like a great challenge. You know, I, I just, I liked doing things that I haven't done before. Um, music creation has always been a little bit of a mystery to me because I don't play instruments. I grew up trying to learn how to play guitar, but my hands were so small and not very nimble across the guitar that I basically gave up. I played a little bit of piano, but I had this narcoleptic piano teacher who would fall asleep halfway through the lesson. So that didn't work out so well either. So for me, writing music in my, in my twenties in adulthood, um, really just became about writing by ear. And so when I found the first pieces, a few pieces of software that could actually write music using code, I got really excited because I thought now finally someone like me who doesn't play instruments could potentially make the music that I hear in my head with this new collaboration partner. So it was an exciting challenge and, uh, it was like crossing new frontier.

Speaker 1:          01:43          Does that, I mean, and this new partner being AI.

Speaker 2:          01:46          Yup. And they never get tired or hungry. And if you don't like their song, you just throw it out, throw it out the window.

Speaker 1:          01:57          Scott is out of here. Okay. So, so tell us about the process. You use it to do this with Ai. So

Speaker 2:          02:02          it's different depending on the software. And there are now a number of different AI software companies out there that can compose music from Google, Magenta to ampere to AI music, I'Veh, Ibm Watson, like I'm forgetting some of the main ones I'm collaborating with. So they all function differently depending on the neural network and how the, uh, how the engineer has actually created the interface. So I will say that the, the easiest one to use if you just want to get started and understand what kinds of things AI can actually create musically. Ampere um, and that's the one that I used for the single that I released in September called break free. The cool thing about amber is it actually produces all of the music. So what, what, what I mean by that is all the instrumentation that you hear in that song is actually put together using Ai.

Speaker 2:          02:49          It wasn't, it was not a musical track composed by AI performed by humans. Um, so it gives you a sense of what's possible because it's actually really hard to take random sounds and put them together into something cohesive. That sounds good. Um, so I would start with ampere. That's, that's my cat. Tiggy no, that's just the Raj has backpack. Tiggy don't do that. She's destroying your backpack. I'll just, you have an AI make one. We already have adversarial networks. Right. Okay, perfect. So if you're looking to do the most basic kind of introductory lesson in AI music creation, I would go to amber. If you are familiar with code and you want to get your hands a little bit dirty, I would say Google, Magenta or IBM Watson are great places to go. Google, Magenta their software and they have a number of different programs available for AI music writing that, two different things.

Speaker 2:          03:41          It's all available on get hub I believe. I think um, and Ibm Watson is actually really seeing there a software open source in January and I've been using that the last month to write stuff and I love it. It's cool. So you do need to have knowledge coding knowledge, you'll be using terminal to write most of your songs. Um, yeah, I had to like for the first couple of weeks on it, I was calling their developers being like, wait, how do I do this thing? So basically with a program like Watson or Magenta, you're going to set different musical parameters. Like what type of song you want the Ai to create. Is it pop, is it reggae, is it a mixture of reggae and pop? What's the tempo, what's the, what's the bpm, what are the instruments that you want embedded into the track? Um, and in some cases you can also give the AI a track or a multitude of tracks to teach it.

Speaker 2:          04:32          Um, basic rules based on those tracks that you like. So if you want to make a song that sounds like the Beatles, you basically just feed it a whole catalog of Beatles songs. It'll do its thing. And then you say, I want the song that it creates to be a reggae track. So now you've got a reggae track inspired by the Beatles, right? So, so there are a lot of, depending on how skilled you are at coding, there are a lot of ways that you can take these, these softwares to do really cool things. Um, there's a program at Google called incense, which allows you to actually take two disparate sounds and combine them together. So I could actually record my cat tiggy scratching on Suraj his backpack and then combine that with the sound of nails on a chalkboard to create a truly delightful, um, tiggy decided to say hi, this is the little runt that's ruining the video. I can't even fake it. I don't really like cat stuff.

Speaker 2:          05:24          But essentially, I mean, uh, you, you can, you can create a whole, an entirely new sound using some of these, these tools and inject that into your musical work. So, um, this all goes to say that they function differently depending on which you choose to use and your coding knowledge and your musical knowledge. But it is absolutely untrue that using AI is not a creative process. It's so creative because there are so many different avenues that you can take in creating a song and creating new sounds and I actually think that it has the ability to bring out the super composer and all of us.

Speaker 1:          06:00          Totally awesome. So how does Stanford make this all happen? Well, it's proprietary technology, so we're out of luck. See you next time. Just kidding. We can totally reverse engineer how it's very likely done. Looking at some of the latest research in deep learning, we know that one popular way to class machine learning algorithms isn't the categories of generative and discriminative models. Discriminative models classify data into sets of categories. They allow us to differentiate between different types of data, generative models. On the other hand, offer even more exciting possibilities, creating new data after going through a training phase, one type of generative model or called the audio synthesis models. These are useful across the range of applications that we use day to day, including Texas speech systems. The kind that Google translate uses to help dictate what you've just translated or the kind that voice assistance use to respond to our requests.

Speaker 1:          06:59          Generic synthesizers have been used in music extensively and have a long history in that field from Djs to terrible hip hop and producers like t pain. They're usually hand designed instruments that that except control signals like pitch and velocity to shake. The tone and dynamic of a sound synthesizers have had a huge impact on culture and music the past few decades as some of the most popular songs out there have used them. So the traditional way of doing this well so you some sort of arrangement of oscillators or an algorithm for sampling playback. But a newer way to do this is by using a data driven approach that is letting an algorithm learn from musical data. It's possible to create new types of expressive and realistic instrumental sounds using deep learning. We know that deep learning allows us to learn features from data and in the case of music, these features can represent tone, timbre, dynamics, crunk. Notice all of these can be tunable knobs for music generator. Google has been at the forefront of a lot of progress in AI generated music and they have an open source project called Magenta. Magenta embodies a lot of the recent models that had been published in that field. A more recent model just published a few months ago, used two concepts together. The first was a wavenet style auto encoder that learns Tim poral hidden nodes to capture longer term structure.

Speaker 2:          08:27          Exactly. And the second was called incentive, which is an enormous data set of music that can be used to explore neural audio synthesis of musical notes. Just like I mentioned earlier, so infant contains about 300,000 for second annotated notes from about a thousand musical instruments, an order of a magnitude more than any other similar dataset. So it's pretty powerful. And like I said, you can combine sounds of your cat with, I don't know anything, which is pretty cool. Even that created system

Speaker 1:          08:56          called wavenet. It generated raw audio wave forms and waiting that models a conditional probability of to have a way for him to generate new music that uses all previous samples and other parameters to do this. The authors of our audio synthesis paper, we're inspired by that model to create something similar to weight net. There were two motivations here. The first was to be able to create music that had a consistent long term structure so that the theme would carry out through the end of the piece. I'm like anything off of Jesus. The second was to use the learned features or applications like using meaningful audio interpolations like between different instruments. For example, like what what Taryn did in the original wave net architecture. At each step of training, a stack of dilated convolutions predicted the next sample of audio from a fixed size input, a prior sample values.

Speaker 1:          09:50          The joint probability of some audio was factorized as a product of conditional probabilities between the different ways, but these researchers removed the need for external conditioning. Instead it works as an auto encoder, a generic one taking in raw audio wave form as input from which the encoder produces an embedded. Then they shifted the same input and fed it back into the decoder which reproduced the wit input way form. So using the joint probability they could parameterize a feature as a latent variable to train the model. They use gradient descent. The most popular optimization scheme out there, shout degraded a set and it uses the difference between the predicted output and the actual output as an error value. Then using that error it calculates a great and using that gradient, it subsequently updates the weight values of the network over time to give it better predictions.

Speaker 1:          10:39          It was trained over 1800 thousand iterations are really long time for the training data. Even though end synth annotated every single note using its unique pitch and timber and dynamics, they decided to further annotate it, giving each sound a source, a family of instruments, and a sonic quality. Even though training, this type of deep learning model requires a massive amount of computing power. You could just download a model from get hub and run it on Google cloud or AWS is stuff has democratized, so you have access to computing power if you use the cloud or you could just pull a model off of the Magenta get hub page or use experiments.google.com and play with it in your browser. So three things to remember here. Generative machine learning models that can learn latent variables from a dataset can be used to generate new data. Similar to the training section, you can use a wave net style auto encoder and the end synth Dataset if you'd like to experiment with state of the art machine learning from music generation and wave net is perhaps the most powerful model out there that perform audio generation. You have a really smart audience. It's all about them. They are very smart. If you want to see more AI music, you can follow me at youtube.com/terrane there we go. Please subscribe for more programming videos, and I think we gotta go. We gotta go make some AI music. It's true. So thank you for watching.