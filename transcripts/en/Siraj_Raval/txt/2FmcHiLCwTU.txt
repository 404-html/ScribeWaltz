Speaker 1:          00:05          Oh world. It's the Raj. The most popular machine learning library in the world right now is Google's tensor flow. We're going to use it to build a classifier that can look at an image of a handwritten digit and classify what digit it is in under 40 lines of code. Basic as she is pretty much every single Google product you use machine learning in some way, whether it's image search, image captioning, translation recommendations, Google needs machine learning to take advantage of their god like datasets to give users the dopest experience. There are three different crowds that use machine learning, researchers, data scientists and developers. Ideally, they can all use the same tool set to collaborate with each other and improve their efficiency. Tensorflow was the solution. They create it to help solve this problem. Well, it doesn't just have a lot of data. They have the world's largest computer, so the library was built to scale.

Speaker 1:          00:52          It was made to run on multiple CPU or GPU and even mobile operating systems and it has several rappers in several languages. My favorite one is python, objective C, who broke my heart. We have to install tensorflow. First, we're going to use pip, the python package manager to install it. Once we have pip, we can create an environment variable that points to the download URL for tensor flow. Once we've set the environment variable, we can download tensorflow via pip install with the upgrade flag and the name of our environment variable. Dope. Now that we have our dependencies installed, let's get to the code. We'll start off by importing our handwritten digit Dataset. The input data class is a standard python class that download that Dataset, splits it into training and testing data and formats it for our use later on, and of course we'll import tensorflow.

Speaker 1:          01:36          Now we can set our hyper parameters or tuning knobs for our model. The first one is the learning rate, which defines how fast we want to update our weights. If the learning rate is too big, our model might skip the optimal solution. If it's too small, we might need to many iterations to converge on the best results, so we'll send it to 0.01 because it's a known decent learning rate for this problem. Definitely faster than little Wayne's. Now we want to create our model in tensorflow. A model is represented as a data flow graph. The graph contains a set of nodes called operations. These are units of computation. They can be as simple as addition or multiplication and can be as complicated as some multivariate equation. Each operation takes in as input a tenser and output to tensor as well. A tensor is how data is represented in tensorflow.

Speaker 1:          02:20          They are multidimensional arrays of numbers and they flow between operations, hence the name tensorflow. Oh, makes sense. We'll start by building our model by creating two operations, both our placeholder operations. A placeholder is just a variable that we will assign data to at a later date. It's never initialize and contains no data. Well defined, the type and shape of our data as the parameters, the input images x will be represented by a tutee tensor of numbers. Seven 84 is a dimensionality of a single flattened Mni ist image. Finding an image means converting a two d array to a one d array by unstacking the rose in lining them up. This is more efficient formatting the output classes. Why will consist of a Tutee tensor as well, where each row is a one hot 10 dimensional vector showing which digit class the corresponding Emond and ist image belongs to and we'll define our weights w and biases be for our model.

Speaker 1:          03:07          The weights are the probabilities that affect how data flows in the graph and they will be updated continuously during training so that our results get closer and closer to the right solution. That bias, let's a shift or a regression line to better fit the data well then create a name scope scopes, help us organize nodes in the graph visualizer called tensor board, which will view at the end. We'll create three scopes in the first scope. We'll implement our model logistic regression by matrix multiplying the input images x by the weight matrix w and adding the bias be well. Then create summary operations to help us later visualize a distribution of our weights and biases and the second scope we'll create our cost function. The cost function helps us minimize our error during training and we'll use the popular cross entropy function as it. Then we'll create a scalar summary to monitor it during training so we can visualize it later.

Speaker 1:          03:53          Our last scope is called train and it will create our optimization function that makes our model improved. During training. We'll use the popular gradient descent, how rhythm, which takes our learning rate as a parameter for pacing and our cost function as a parameter to help minimize the error. Now that we have our grasp bill, we'll initialize all of our variables and we'll merge all of our summaries into a single operator because we are extremely lazy. Now we're ready to launch our graph by initializing a session which lets us execute our data flow graph well. Then set our summary writer folder location, which will later load data from to visualize intenser board training time. Let's set our for loop for our specified number of iterations and initialize our average costs, which will print out every so often to make sure our model is improving during training.

Speaker 1:          04:34          Well compute our batch size and start training over each example. In our training data. Next will fit our model using the batch data and the gradient descent algorithm for backpropagation will compute the average loss and write logs for each iteration. The other summary writer for each displaced step will display our logs to terminal. That's it for training. We can then test the model I comparing our model values to our output values. We'll calculate the accuracy and printed out for test data. The accuracy gets better with training and once we've trained and tested our model, it will be able to classify novel Emond Ist digits pretty well. We can then visualize our graph intenser board, yo pretty colors and stuff in our browser. We'll be able to view the output of our cost function over time. Under the events tab under histograms, we'll be able to see the variance in our biases and weights over time.

Speaker 1:          05:17          Under graphs we can view the actual graph we created as well as the variables for weights and bias. We can see the flow of tensors in the form of edges connecting our nodes or operations. We can see each of the three scopes we named in our code earlier, and by double clicking on each, we can see a more detailed view of how tensors are flowing through each. Lots of cooling to the description, and please hit that subscribe button if you want to see more ml videos. For now, I've got to go dockerize my environment, so thanks for watching.