Speaker 1:          00:00          Hello world, it's Saroj. And imagine that you've built a game and you want to predict whether or not your players are going to continue playing next month or not. You want to classify your players as going to churn or not going to churn and you've got a single data point and that is the amount of money that that user has spent in the game this month, either withdrawn or deposited to buy some in game currency or digital gold or some kind of items, right? And based on that data point alone, you want to predict whether or not the user is going to continue playing the game or not. Now, normally when predicting customer churn as it's called, you'd want to use several data points. You'd want to use a time that the user has logged in, you'd want to use a bunch of different features, but for the simple one dimensional example, we're just going to use that because what matters is the model that we're going to learn today and the model is a Gaussian mixture model.

Speaker 1:          00:53          That's what the model is called and it is a universally used model for generative unsupervised learning or clustering, which is what we're going to do. It's also called e em clustering or expectation maximization clustering based on the optimization strategy that we're going to use, which we'll talk about, but the idea is very simple for a given dataset and in our case is going to be an amount of money withdrawn or deposited. Each point is generated by linearly combining multiple Gaussians. Okay, so that's what we're going to do and before we start with this amazing, amazing example and code, I want to say one thing. Okay, this is a message from Saroj. This is the message. The message is, I had a, this is a 32nd message. I went and I met subscribers for this channel. I met the community here in Amsterdam. I met 15 or so people.

Speaker 1:          01:43          I went to London this past weekend and I met about 15 or so people and I was just blown away by the quality of people that have subscribed to this channel. The community that we have built is incredible. Seriously. There are people working for the United Nations. There are people who have met who are working to create a machine learning community in Tunisia there are, there are people who have apps with 500,000 plus users that are trying to stop smoking. There are people who are working on global warming. They are, they are legitimately working on using machine learning to prevent global warming and it is so inspiring and it is amazing and inspires me and it increases my conviction to help you guys get better because it's not about me. You guys are the real inspiration. You are my hero, okay? You are my hero for watching this, for giving your attention to machine learning.

Speaker 1:          02:36          We are the most important community in the world because we have an interest in the most important technology in the world. We are the most important community in the world and we are the ones who are going to solve the problems in this world using machine learning in the way that only we can remember. We are smart and we are cool. I have met you guys and I'm going to continue meeting you guys in person. I'm going to India next month. I am very excited for that and I'm just going to meet all you guys everywhere. I'm going to do a world tour at some point. But anyway, it's a very exciting time to be in this field and we are the, we are the real change makers. Okay. We're the ones we're going to solve these problems. So it's amazing that we exist and we are growing at a rate of 500 to 800 subscribers a day.

Speaker 1:          03:22          Reply to people in the comments. As for people ask people for questions in the slack channel. We are growing, startups have been built in the slack channel by the way. It is incredible what is happening. It is truly incredible. Anyway, I was just excited about my past weekend in London meeting you guys and I'm going to continue to do so back to this. Um, so we're going to build a Gaussian mixture model and first of all, what is a calcium? Now, I know I've kind of talked about this before, but let's, let's go into it a little bit more. A Gaussian is a type of distribution. It's a very popular and mathematically convenient type of distribution, but a distribution is a listing of outcomes of an experiment and the probability associated with each outcome. So let's say we've got some kind of game, right, and you could get a score of one or two, three, four, five, six.

Speaker 1:          04:10          You could get a set of scores and you could, and then you want to calculate the frequency or the amount of times that a person has made this score, right? So the tally and the frequency columns are the same thing. And so what we you could do is you can notice that there is a peak here that the n the number nine, right? So notice how this kind of follows a bell curve. The frequency goes up and then it goes down. Now it's not exact because seven is greater than six, but more or less, it looks like it follows is a kind of bell curve. The frequencies go up as a scores go up. And then it has a peak value and then it goes down again. And we can represent this using a bell curve, otherwise known as a Gaussian distribution. And a Gaussian distribution is a type of distribution.

Speaker 1:          04:53          We're half the data, falls on the left of it and the other half falls on the right of it. So it's an, it's an even distribution and you can notice just by the thought of it intuitively that has been very mathematically convenient. Right? And so what do we need to define a Gaussian or normal distribution? What we need two variables, we need a mean, which is the, uh, which is the average of all the data points. And that's going to define the center of the curve. And then we need the standard deviation, which describes how spread out the data is as well. And once we have that, we can write out this curve. And this is used throughout many fields. And you can think about many examples of where you, you know, anytime you have some, some kind of data that's increasing, has a peak and decreases, very likely a Gaussian distribution would be a great distribution to model this data, whether it's a car or a roller coaster that's increasing in velocity, reaches a peak and then decreases or a sound wave acoustic wave forms.

Speaker 1:          05:52          There's a lot of applications of this now. So that's a Gaussian distribution and I'll show you the formula in a second. But this is mold. These are multiple Gaussian distributions, right? Where we have multiple means and multiple standard deviations. And if we have those we can write out, we can plot out multiple Gaussians, which is what we're going to do in a second. But the formula for a Gaussian distribution using the mean and the standard deviation, once we calculate those two things, we can write out this, this, uh, this formula, this, this graph. And this is, this is the formula for it. And it's called the probability density function. That's what it's called. So we can say for a given point x for a given point x, we can compute the associated why, which is, which are all of those values. Remember in a one dimensional case, we know all the x values, we want to compute the y values and the y values are the probabilities for those x value.

Speaker 1:          06:49          So then given some new x value, we can output the, the probability of that x value being a part of the curve or being a part of the Dataset or being a part of the class. In this case. So we can say one over the standard deviation times the square root of two times Pi Times the natural number e to the negative x value, which is the data point minus the mean squared over two times the standard deviation squared. So this is a function of a continuous random variable whose integral across a interval give the probability that the value of the variable lies within the same interval. So we have some interval. So that's a Gaussian distribution and that's the formula for one. So what does a Gaussian mixture model? Well, we sometimes our data has multiple distributions or it has multiple peaks. It doesn't always just have one peak.

Speaker 1:          07:45          And we can notice that we, we can, we can look at our data set and we can say, well, it looks like there are multiple peaks happen here. There, there are two peak points and the data seems to be going up and down and up and down twice or maybe three times or four times. But if we think that there are multiple distributions Gaussian distributions that can represent this data, then we can build what's called a Gaussian mixture model. And what this is, it is a probability distribution. It's more a probability. You could think of it as just a probability distribution that consists of multiple probability distributions and that are multiple Gaussians. So four d dimensions where d is the number are the number of features in our dataset. The gaussian distribution of a vector x where x equals a number of data points that we have is defined by the following equation and right.

Speaker 1:          08:38          So you can see how you can set the two d for the number of features we have. Uh, this is for the mean and so sigma represents the covariance matrix of the Gaussian. So you notice how we're not using the standard deviation. Instead we're using the covariance matrix when it comes to the Gaussian mixture model. And this is what it would look like, right? So in the case of two dimensions, it would look like this. It would give us a, a curve for all of our, uh, possible, uh, data points. And so why do we use the covariants? Will? The covariants is a measure of how changes in one variable are associated with changes in a second variable, right? They co-vary it's not just about independence of variation two variables and how they change depend on each other. They co very, and the covariants well, to be technical, it's called the variance.

Speaker 1:          09:26          Covariance matrix is a measure of how these variables relate to each other. And in that way, it's very similar to the standard deviation, except when we have more dimensions, the covariance matrix as opposed to the standard deviation when we plug it in here, uh, gives us a better, more accurate result. And so the way we compute the covariance Matrix, I've talked about this before when we talked about eigenvectors and eigenvalue pairs, but it, the number of the parameters that it uses are the number of scores in each of the datasets, the variance, the covariants and um, the s where c is the size of the number of columns in the data, right? It's a column by column Matrix. I will compute this in a second, but the probability given in a mixture of k Gaussians where k is the number of distributions that we, that we believe that there are in our case, we believe that there are going to be too.

Speaker 1:          10:26          Okay. Let's just say we think in our data set they're going to be two. So four K Gaussians to, in our case we're going to multiply, uh, w which is the prior probability of the JF Gaussian Times the value that we just looked at up here right here where this is the Gaussian distribution of the vector of our, of our data points. And so once we have this value, then we can say we can multiply it by w where for each for each of our Gaussians and that is going to give us our probability value x for a given x data points. And so this is what it looks like, right? So if we were to plot multiple Gaussian distributions, it would be multiple bell curves. But what we really want is a single continuous, a curve that consists of multiple bell curves. And so once we had that huge continuous curve then and given our data points, it can tell us the probability that it's going to belong to a specific class.

Speaker 1:          11:24          Okay. And so what class, how do you tell what class something is given this curve? Well, let me go to the bottom here. Uh, once we have that best fit mixture, notice this, this, um, this image right here, we can tell what class it's in by, where it falls on the curve for the one dimensional case. So this would be class one, this would be class two, this would be class three like that. Now in the two dimensional case or three dimensional case, it's easier because there are circles, right? Like k means clustering, right? There's circles and that's what they, that's what they come out to. If we were to measure probability values just like this for not just one feature like we're doing for, for us, but multiple features like two features. So if you have probability values for y the y axis, which is a second feature set, like maybe the, the um, number of, uh, other users that this person has friended.

Speaker 1:          12:17          And in this is a, the probability of the, you know, the amount that the user has either deposited or withdrawn, then we can draw these cluster circles. But for the one dimensional case, it looks like that a bell curve. And so the problem that we're trying to solve here with the GMM is given a set of data points x, let's draw from an unknown distribution, which we're going to assume intuitively is a Gaussian mixture model. That is that the data's that consists of multiple Gaussians estimate the parameters of Feta, which consists of the mean and other values that we'll talk about of the GMM model that fits that data. And the solution. The way we find these, these parameters of our model, which is what machine learning is all about. Mathematical optimization is by maximizing the likelihood p of x, the probability of x given our parameters and x is the data point that we want to predict the probability, the class, the, the, the, we want to maximize the likelihood that x belongs to a particular class.

Speaker 1:          13:18          We went to find, the way we do that is to compute the maximum likelihood estimate, which is, which is this equation right here. We want to find the maximum probability value for a given class. That is, we want to find the class that this data point x is the most likely to be a part of. And so notice how, so I also added this image to show how, uh, you know, given an Indy two dimensional case, these uh, distribution bell curves come out two circles. If you, if you have distributions for not just one access or one dimension, but two dimensions.

Speaker 1:          13:56          So it's, it's very similar to k that we've talked about. TheK means algorithm. It uses the same optimization strategy, which is the expectation maximization algorithm. It's similar to k means in that came means k means finds k to minimize x minus the mean squared, but the Gaussian mixture model finds k to minimize x minus the mean squared over the standard deviation over. And the reason that we add the standard deviation into the mix is because the denominator, the standard deviation squared takes variation into consideration, which it when it calculates its measurement, what does this mean? This means that the k means algorithm gives you a hard, uh, assignment. It either says this is going to be, this data point is a part of this class or it's a part of this class. Now this is great in a lot of cases we just want that hard assignment.

Speaker 1:          14:50          But in a lot of cases it's better to have a soft assignment. Instead, we want the maximum probability, okay, this is going to be 70% likely that it's part of this class class a. But we also want the probability that it's going to be a part of other classes, right? It's not just a single output, it's not a, it's not a discrete out output. Instead of is a continuous output is a list of probability values, right? It could be a part of multiple distributions. It could be in the middle, it could be 60% likely this class with 40% likely this class, not just classic. And so that's why we incorporate, uh, uh, that's why we incorporate the standard deviation. So, so how has this thing optimized what it's optimized using the expectation maximization algorithm. So the basic ideas of the algorithm or to introduce a hidden variable such that it's knowledge would simplify the maximization of the likelihood.

Speaker 1:          15:45          So we pick some random data points, right? And we picked some random data points. We draw a distribution around that data point. We then estimate the print, we didn't update our parameters using that generated distribution. And then we repeat the process. And every time we draw a new data point, it's going to be closer and closer to the data point that best fits the data set that we have, such that if we were to draw a distribution around that data points, it would fit that. It would fit that Dataset the best. And so there are two steps. There's the east step, the expectation step and the expectations step is to estimate the distribution of the hidden variable given the data and the current value of the parameters. And then the m step, the m step, the maximizations step is to maximize the joint distribution of the data and the hidden variable.

Speaker 1:          16:36          So that's the high level. And we're going to talk about the implement the implementation details in the code, but you might be thinking, well wait a second, wait a second. What about gradient descent? You might be thinking about that because we've talked about grading dissent and the entire industry loves gradient descent. And here's the thing, you can obtain the MLE using gradient descent, but what is the deal with gradient descent? Gradient descent is using the, the derivative, you are computing the derivative of a node, right? A node in some kind of graphical model. You're computing the derivative and what the derivative tells you is the direction. It tells you the direction that your data wants to move in, what direction to move the parameters data of your model such that the function, your model is optimized to fit your data. But what if you can't compute a gradient?

Speaker 1:          17:27          What if you can't compute a derivative? You can't compute a derivative of a random variable, right? This, this Gaussian mixture model has a random variable. It is a stochastic model that is, it is non deterministic. You can't compute the derivative of a random variable. That's why we're not using gradient descent in this case because you can't compete the derivative of a random variable. Okay? So there are actually ways to compute. There are actually ways to compute the derivative of a stochastic model like a variational auto encoder. But it's, it's, it's a trick. And, um, and do you want to know more about that? I've got a great video on that search. Variational auto encoders Saroj on Youtube, there'll be the first link. So back to this. So when should you use this thing, right when, when is this actually useful? Besides customer term anomaly detection.

Speaker 1:          18:21          Think about any case where you are trying to cluster data where you are trying to classify data that does not have labels. So one use case is trying to track an object in a video for aim, right? If you know the moving objects distribution in the first frame, we can localize the object in the next frames by tracking its distribution. That is, you know, kind of the probability distribution of all the possible ways that the subject can move. And based on that you can create a bounding box around a subject such that in the next frame it's very likely that that's subjects movement will fit into the bounding box. I have some related, uh, repositories here, uh, for using tensorflow and a Gaussian mixture model to classify song lyrics by genre. Very cool use case using tensorflow. Definitely check it out. It's called word to Gaussian mixture.

Speaker 1:          19:12          Very cool. Check that out. It's got some great instructions and I've got one more, which is a great general purpose tutorial for Gaussian mixture models. It's an, it's a Jupiter notebook. Definitely check that out as well and I've got some great links for you in the description. All right, so check that out as well. Got The great graphs and everything. So let's, let's look at this code. Okay, so in this code we're going to import our dependencies and then test out a distribution graph. Let's just see if we can draw out a distribution orthogonal or unrelated to our Dataset just to see if we can do that. So we're going to import for dependencies here. The first one is not plot live, which is our handy dandy plotting tool. And the next one is Ma is num py, which we always use for matrix math.

Speaker 1:          19:56          Then we've got psi Pi, which is going to help us normalize our data and compute the probability density function, which we talked about earlier. Right? We show, I showed you the equation for that and then we have one more seaborne which is also going to help us plot specifically colors. Okay. So once we have our four dependencies, we can go ahead and start this. So the first step is for us to plot out a d a Gaussian distribution. Let's just see if we can do that first. So we've got um, let's just say we're going to use non pies Lynn space function to return an evenly spaced set of numbers over a specified interval, which is a negative 10 to 10. We have a thousand of these data points that we want to generate and then look at, we have a real data set in a second, but this is just for the sake of an example.

Speaker 1:          20:43          And so then we want to create a normal continuous random variable. And that's what these stats dot norm dot pdf that probability distribution function function is going to help us do. And so we say a loc equal zero and this specifies the mean. So a mean of zero and scale is 1.5 and that's the standard deviation. So we're going to draw a Gaussian distribution using those two parameters. We're going to plug those in into the probability density density function and it's going to output the y values for all of our x values, right? We gave it our x values, it's going to apply to all of our y value. So then to plot it out, always to do is say use map plot live to plot out all of our x values and all of our computed why values and then there it is. So we can write out a Gaussian distribution easily.

Speaker 1:          21:31          We can do that. Okay, so let's go, let's go into our date, our dataset. We're also going to import pandas and we're going to read our dataset here. It's a CSV file and we can read it into a pandas data frame object, which is very easy to manipulate in memory once we have it there. And we want to show the first five examples. And here we are, we have the first five examples. Okay? So for each of these are users, each of these are different players and the amounts that they, that that player has spent in a given month this month in bitcoin. Okay. Bitcoin. So, or it's normalized any currency you can, it doesn't really matter what currency is in, but it's in bitcoin. Okay. And so it's either a positive number, it's a negative number that means that the user withdrew that amount. But the idea is that that's it, that we've got one feature.

Speaker 1:          22:19          So it's a one dimensional problem and we're going to write out a probability distribution for that single feature. And if we do that, if we do that, then we're going to be able to predict, uh, the class of a given user based on how much this person, this user has either spent or withdrawn, whether or not they're going to, uh, uh, turn or not. Okay? So that's our data set and now we're going to show the distribution of the data as a histogram. So let's, let's show it as a histogram as well. So this is where seaborne is going to come in. We're going to say seaborne do create a distribution plot. Given our data that we've, that we've loaded into memory with pandas, we want 20 bins of this data and let's plot it out. Uh, let's see. Disc plot.

Speaker 1:          23:22          Okay. So that's the distribution. That's the histogram of our data, right? So for all of those number of players, this is what it looks like. So we could look at this and we can say, oh, okay, so it looks like one Gaussian might fit this, but two looks better. Let's, let's see. Let's try out one to see. Okay. So that's, I mean, clearly to two would be better than one, right? You can see two peaks here. So two distributions would fit this data better than one. So that's why we're going to use a Gaussian mixture model instead of a single Gaussian, right? So we want one that, a continuous bell curve, a continuous curve that consist of two bell curves. So to define this model to two, normal distributions are going to have five parameters. Four of them are the, the first four are the mean and the standard deviation for each of those distributions.

Speaker 1:          24:14          One and two. And the fifth one is the probability of choosing one of them. And so the way we're going to write out this Gaussian mixture model where theta equals uh, those four values and the probability of choosing one of them. W is just like this and this is the probability density function for a Gaussian mixture model consisting of two Gaussians. Now we'd look at the probability density function for a single galcion and it looked like this right here and this is what it looks like for two distributions, right? Right. We get it so far. So now we're going to fit this model. That's our model, right? All machine learning models are functions. There are functions or where we initialize those parameters randomly or using some kind of, you know, smart sampling method or something, but we initialize those parameters very stupidly. And then we learn what the best or most optimal ones are.

Speaker 1:          25:10          And in neural networks and a bunch of convex optimization problems, we generally use gradient descent because we can compute the derivative of deterministic notes. But this is a stochastic model. So we're going to use the popular expectation maximization algorithm, which is a two step process. We first, uh, perform he, which is to update our variables. And then m which is to update the hypothesis. What do I mean? Well, this is an iterative method for finding the MLE that estimates the parameters in statistical models. So we start with some initial values performed, the expectation step, then the maximisation step check if it's converge or not by some threshold that we define and if it hasn't continue iteratively and if it has stop. So the expectation step given the current parameters of the model, estimate a probability distribution maximisation step given the current data, estimate the parameters to update the model and repeat so more formally using the current estimates for the parameters create a function for the expectation of the log likelihood.

Speaker 1:          26:17          And then from maximisation compute, the perimeter is maximizing the expected log likelihood down on the east step. So the end parameter estimates are used to determine the distributions of the latent variables in the next east step. So e m is trying to maximize the following function, uh, which is what we defined the probability density function up here. Where x is a directly observed a variable that is our data, a theta, our whole of those printers is five parameters of our model in z is not directly observed that the latent variable that is the random value that we plot right to, to continue and it gets more and more smart, it gets more and more optimal where we plot it as our, as our model learns. Okay. And we xe is a joint distribution on x, so it's a distribution between the latent variable that we've plotted and the girl Scouts Ian, that we've already applauded.

Speaker 1:          27:12          So there are four steps. If we think about it, there are four steps. We first initialize the parameters of our model Feta and then we compute the best values for z given data. And then we use the computer values of Czi to compute a better estimate. Fourth data and then repeat. So in another way, another way to say this, and I'll show you a visual way in a second, but we want to initialize the parameters of the model either randomly or usually randomly. And then we find the posts terrier probabilities of the latent variable. Given a given the current parameter values. And then the m step is to re estimate the CR, the parameter values given the current posterior probabilities. And we repeat that process again. So check this out visually, this is what it looks like. So we have two Gaussians. So in this case we have probabilities for two features.

Speaker 1:          27:59          So these are circles instead of those curves, right? Instead of those bell curves. And so we have two Gaussians and they're just randomly, we generated them and we have our data points. So what we do is we say for each of those data points, which galcion generated it, so we give them all probabilities for each Gaussian. So that's the east step for each point here, estimate the probability that each galcion generated. In our case we have two Gaussians, right? So we're going to generate the, we're going to estimate the probability that each data point is from the first EI Ei and in the second beat. Okay. And so when we have that, then we compute m the m step is to modify the parameters according to the hidden variable to maximize the likelihood of the data. And the hidden variable. And that's it. And so every time we do that, these down Sian curves, these Gaussian distributions, these clusters are going to be more and more optimal to fit the data where they need to, to classify, to, to distinguish between classes.

Speaker 1:          29:04          So back to our data. So let's define what a Gaussian looks like, right? So we, so, so we'll give a galaxy in its own class. And so a Gaussian is initialized by the mean and the standard deviation. So we calculate that first from our data points and we can do that very easily, right? For the mean, it's just add all the numbers up and then divide by the number of them. And for the, the center deviation, it's uh, the formula for that is a look up there. That's the formula for it. Okay, so back to this. So once we have those, we can compute the probability density function that we looked at up there. The probability to density function is this, right? So we can compute this programmatically, which I'm going to do right now. So programmatically that looks like this, right? Just like that same thing. And so we can say, uh, let's find the Gaussian of best fit for our data. So we have our data in a data frame and let's find the Gaussian of best fit. And so this, these are our ideal means and our ideal standard deviations for our data. But that's not enough. We want to write, it's not enough to just have one that's too easy. There's no expectation maximization algorithm happening there. We don't really need it. But if we were to, uh,

Speaker 1:          30:25          plot out this galcion very easy. It's a single Gaussian, but it's not, it's not well fit to the data. We want to have them, right? So we're going to keep going here and this is where the expectation maximization algorithm comes into play. So

Speaker 1:          30:40          first we initially, uh, we randomly assigned k cluster centers. In our case k are the number of Gaussians, right? Not k means k where they are the number of Gaussians too. And then we iteratively refine these clusters or you know, a bell curves based on the two steps for the expectation step. We signed each data point x to both clusters, the probability with the following probability. And the maximizations step is to estimate, to create an estimation of the model parameters given those probabilities and then repeat them. So I said the probabilities are, are, are maximized for each class. So then we'll create a class for our Gaussian mixture model. We've already created a class for our Galcion, but here's four, our Gaussian mixture model. So for a mixture model where there are two Gaussians, we don't just need one a mean and one standard deviation. We need four. In fact, we need five parameters, right? We need the mean and the standard deviation for one Gaussian, we the mean and the standard deviation for the second Gaussian. And we need our Dataset, uh, as well as well as the, uh, a mix, which is the, uh, initial w value. Remember the w value. And so we'll initialize both of them well in this shot. So that's why we created that first galcion class. So you could easily initialize to Gaussians just like that, using both means and standard deviations,

Speaker 1:          32:06          how as well as the w value, right? Which is going to be updated over time. The weight value and the weights defines how mixed these distributions are like right. The how mixed both of them are. And so now for the East App and the m step, so basically we make initial guesses for both the assignments, uh, point to the distribution and their parameters and then proceed iteratively. So once we've initialize the assignments and parameters, we can alternate between the expectation and the maximizations steps until our estimates converge. That means that they do not change much between iterations for a mixture of Gaussians. And this is similar to convergence in this came means, right, which I've talked about I think a few, a few lessons ago. K-Means clustering Saroj search that on Youtube. So for the East App, we, we first initialize a log likelihood as zero, which we're trying to maximize.

Speaker 1:          33:01          And then we say for each of our data points, uh, compute the normalized wait values using both of our data points and these are the probability values. And then we compute the denominator by adding both of them together. We normalize them by dividing both of those weights by the denominator and had them both into the log likelihood. But so we compute the log of the sum of both of those values and that gives us the log likelihood and we were turn the weight to pull that our are our parameters of our model, right? And so once we have that, we'll, we'll, we'll use those as parameters for the maximisation step, right? We want to optimize those wait values. So we'll compute, so will compute denominators of our weights, right? So zip tells us the absolute values of our weight. So it's all positive, which just makes things a little more pretty mathematically for, uh, for both of them.

Speaker 1:          33:56          And then we'll take the sum of all of those values, both on the left and the right side and will compute new means. And new standard deviations is for our new distributions, right? We're updating them every time. We're maximizing the likelihood that they belong to the correct class every time. And then a new weight value. So that is our update step are maximisation step is our update step. Okay? And so the probability density function is going to be that function that I talked about up there, right when it comes to two Gaussians, not a single galcion. And so,

Speaker 2:          34:34          okay,

Speaker 1:          34:35          so for the fitting process, now that we know these, we can say, well let's try this for five iterations on our data set using the class that we just built. Let's train this thing. So what does training look like? Train training looks like. We'll take our Gaussian mixture model where we have fitted, fitted it to our dataset well, well that we have given our Dataset to. And we'll say, okay, let's try this thing. So we'll iterate.

Speaker 2:          35:06          Okay.

Speaker 1:          35:07          If the log likelihood, if the current log likelihood is better than the best cycle, that log likelihood then set the best log likelihood to the uh,

Speaker 2:          35:18          yeah,

Speaker 1:          35:19          models, log likelihood, and then set the best, um, wait value to the current weight value. Okay. And so we do that for as many iterations as we can, as we, as we defined five of them. And so once we've done that, notice how our means and our standard and our weight values, that fifth parameter are all being updated every time. And that is the expectation maximization process. And so once we've done that, then we can look at the mixture, what is, what does it look like? And so this is, this is the end result, right? This is the end result. So we have one distribution to rule them all, Lord of the rings style. We've got a single distribution. And once we have this distribution, we can then predict, given some new data points, right? Some new spending amounts, what is the likelihood and what is the likelihood that it's going to be a part of a specific class?

Speaker 1:          36:13          And our case is going to be one of two classes, is there's going to spend, is this person going to spend or not? And so that's what it looks like. And if we had more, uh, if we had more features, we could turn this into a clustering problem with circles, which would make it a lot easier to look at. Okay, so what have we learned here? Let's summarize a little bit about what we've learned. Gaussian mixture models. Take our old friend the Gaussian and add other Gaussians, right plural. Sometimes we could have up to as many as we'd like. And this allows us to model more complex data where there are multiple peaks and valleys and we fit it using the expectation maximization algorithm, the e m algorithm, very popular experts, very popular optimization strategy. And it's a series of steps to find good parameter estimates where there are latent variables.

Speaker 1:          37:01          So we initialize the perimeter estimates randomly given the current parameter estimates, find the minimum log likelihood for Z, which is the data plus the latent variables, the joint probability distribution, and then given the current data, find the better parameter estimates and then repeat that process over and over again. And the distributions that are going to be really bad at first and slowly they're going to converge and they're going to fit our dataset perfectly. Okay. And so this can be used beyond the Gaussian mixture models instead think about, well in this case you have to guess the number of Gaussians, right? You have to guess. Well, I think there's going to be two by looking at your data, but what if he didn't have to guess them? Well, that in, in that case, we could use kernel density estimation, but that's for a d, that's for another time. Before we get there, it's we had to have built the intuition around Gaussian mixture models, a great model. If you know the shape of your data, more or less intuitively see you next week. Please subscribe for more programming videos. And for now, I've got to fit my curves, so thanks for watching.