1
00:00:00,270 --> 00:00:04,470
What happens if we predict the past?
Oh Shit.

2
00:00:05,280 --> 00:00:06,090
Hello world.

3
00:00:06,090 --> 00:00:11,090
It's Saroj and I get a ton of questions
about how to make predictions using time

4
00:00:11,341 --> 00:00:12,091
series data.

5
00:00:12,091 --> 00:00:16,590
So I'm going to explain eight different
time series prediction techniques to

6
00:00:16,591 --> 00:00:21,270
predict the price of gold in this video.
In order of increasing complexity,

7
00:00:21,600 --> 00:00:25,770
we can think of time series data as a
sequence of data points that measured the

8
00:00:25,771 --> 00:00:29,190
same thing over an ordered period of time.

9
00:00:29,580 --> 00:00:33,480
Another way of thinking about it is
that it's a series of numeric values,

10
00:00:33,481 --> 00:00:38,310
each with its own timestamp defined by
a name and a set of labeled dimensions

11
00:00:38,580 --> 00:00:42,390
and we're seeing this type of
dataset become more common. In fact,

12
00:00:42,391 --> 00:00:47,370
if we look at developer usage patterns
in the past two years time series

13
00:00:47,371 --> 00:00:52,080
databases have emerged as the fastest
growing category of databases.

14
00:00:52,410 --> 00:00:57,410
Autonomous Trading Algorithms continuously
collect data on market fluctuations

15
00:00:57,451 --> 00:01:01,710
over a period of time. It's like digital
wolf of Wall Street, smart homes,

16
00:01:01,711 --> 00:01:05,700
monitored data points like the
temperature number of people in the house,

17
00:01:05,760 --> 00:01:09,540
swag level and energy usage.
Jeff Bezos,

18
00:01:09,570 --> 00:01:14,570
I mean Amazon monitors how it's many
assets are moving across the world with a

19
00:01:15,001 --> 00:01:19,500
fine grained level of precision and
efficiency that makes same day delivery

20
00:01:19,530 --> 00:01:23,640
possible. All of these applications
have one thing in common.

21
00:01:23,850 --> 00:01:28,830
They rely on a form of data that
measures how things change over time.

22
00:01:29,310 --> 00:01:32,490
Usually time series data sets
have three home analogies.

23
00:01:32,790 --> 00:01:36,030
That data that does arrive
is recorded as a new entry.

24
00:01:36,330 --> 00:01:39,750
It arrives in time order
and time is a primary axis.

25
00:01:39,810 --> 00:01:44,810
These datasets are generally append only
the data that has been recorded doesn't

26
00:01:44,821 --> 00:01:49,530
change since it was recorded at
some point in the past time series.

27
00:01:49,531 --> 00:01:54,300
Data sets are different than just having
a time field as a column in a Dataset.

28
00:01:54,360 --> 00:01:58,350
In that when we collect a new
data point for time series data,

29
00:01:58,470 --> 00:02:01,080
we have to create a brand new row for it.

30
00:02:01,200 --> 00:02:03,510
We can't overwrite what happened before.

31
00:02:03,750 --> 00:02:08,430
Only by doing this will we
be able to track all changes
to assist them over time.

32
00:02:08,760 --> 00:02:13,290
Recording data points over a series of
time allows us to analyze how something

33
00:02:13,291 --> 00:02:14,970
has changed in the past.

34
00:02:15,150 --> 00:02:20,010
Monitor how is changing in the presence
and even predict how it could change in

35
00:02:20,011 --> 00:02:20,670
the future.

36
00:02:20,670 --> 00:02:25,320
So let's say we have a Dataset of the
price of gold over a period of time from

37
00:02:25,321 --> 00:02:27,090
many days ago up until today,

38
00:02:27,330 --> 00:02:31,350
we want to be able to predict the price
of gold tomorrow using this datasets.

39
00:02:31,650 --> 00:02:36,540
How are we going to do that dark
magic? Well, before we do anything,

40
00:02:36,570 --> 00:02:39,690
let's split our data into
training and testing data.

41
00:02:40,020 --> 00:02:44,100
We can use the popular machine learning
library called psychic learn and pandas

42
00:02:44,101 --> 00:02:47,790
to do this. Let's start with
the most naive approach we can.

43
00:02:47,970 --> 00:02:50,760
If we want to forecast
the price for a given day,

44
00:02:50,940 --> 00:02:55,380
we can just take the last days value of
our training set and use it to estimate

45
00:02:55,381 --> 00:03:00,220
the same value for the next day. We can
represent this as the equation y hat.

46
00:03:00,400 --> 00:03:04,990
The price of gold of the next day is
equal to y the price of gold from the

47
00:03:04,990 --> 00:03:05,320
previous day.

48
00:03:05,320 --> 00:03:09,640
That's the most basic time series
forecasting equation we could make.

49
00:03:10,000 --> 00:03:13,990
Then we can check the accuracy of
our model on the testing data set.

50
00:03:14,140 --> 00:03:17,470
By using psychic learns
mean squared error module.

51
00:03:17,800 --> 00:03:22,300
It'll allow us to compute the
error between our prediction
and the testing data

52
00:03:22,301 --> 00:03:26,020
set, which are the real values for
all the data points we predict.

53
00:03:26,021 --> 00:03:30,730
We can compute the root mean squared
error and it will output a scalar value.

54
00:03:31,060 --> 00:03:35,890
As you can see, this value is pretty
bad. We could do much better, so instead,

55
00:03:35,891 --> 00:03:39,370
let's try using a different
method called the simple average.

56
00:03:39,670 --> 00:03:44,440
Sometimes we get a Dataset that varies
by a small margin through its time

57
00:03:44,441 --> 00:03:48,430
period, but the average at each
time period remains constant.

58
00:03:48,670 --> 00:03:53,670
We can thus try and forecast the price
of the next day using the average of all

59
00:03:54,640 --> 00:03:54,700
the past days.

60
00:03:54,700 --> 00:03:58,570
The expected value will then be equal to
the average of all previously observed

61
00:03:58,571 --> 00:04:02,770
data points. It's not going to be exact,
but it can be somewhat close, right?

62
00:04:04,000 --> 00:04:06,610
Actually,
when we compute the error again,

63
00:04:06,640 --> 00:04:09,550
we'll see that this model
didn't improve our score.

64
00:04:09,940 --> 00:04:14,230
This method works best when the average
at each time period is constant,

65
00:04:14,530 --> 00:04:18,400
but even though the naive methods
scored better than the average method,

66
00:04:18,580 --> 00:04:22,090
it doesn't mean the naive
method is universally better.

67
00:04:22,210 --> 00:04:24,550
It just happened to be better.
In this case.

68
00:04:24,720 --> 00:04:28,900
If in our graph we can see that there's
a big difference in terms of the prices

69
00:04:28,901 --> 00:04:31,570
in the initial period
and the later period.

70
00:04:31,900 --> 00:04:36,900
Perhaps we could improve
our simple average approach
by only taking the average

71
00:04:37,121 --> 00:04:42,121
of the prices for the last time period
only we'd call this the moving average.

72
00:04:42,641 --> 00:04:47,641
We just need to pick the right value for
p where p is the fixed finite number of

73
00:04:48,071 --> 00:04:50,620
previous values.
When we test this out,

74
00:04:50,621 --> 00:04:53,260
we can see that we are
getting better at predictions,

75
00:04:53,500 --> 00:04:58,150
are error is smaller and our prediction
line intersects much more of our test

76
00:04:58,151 --> 00:04:58,984
data.

77
00:04:59,050 --> 00:05:04,050
One thing we could possibly do to improve
our model is add some weights to our

78
00:05:04,331 --> 00:05:08,320
values. Meaning it seems like
the more recent the data points,

79
00:05:08,530 --> 00:05:09,790
the more it matters.

80
00:05:09,910 --> 00:05:13,300
If we gave different weights within
the sliding window of values,

81
00:05:13,301 --> 00:05:15,280
which all add up to one,

82
00:05:15,460 --> 00:05:19,990
we can mathematically define a preference
for values that came later on in the

83
00:05:19,991 --> 00:05:20,824
series.

84
00:05:21,010 --> 00:05:25,750
Notice that both the simple average
and the weighted moving average are on

85
00:05:25,751 --> 00:05:30,310
opposite ends of the spectrum in terms of
how they approach the problem. Ideally,

86
00:05:30,311 --> 00:05:34,990
we can use a method that incorporates
ideas from both techniques combining them

87
00:05:35,050 --> 00:05:35,883
like a megazord.

88
00:05:36,280 --> 00:05:41,140
It's clear that attaching larger weights
to more recent observations and two

89
00:05:41,141 --> 00:05:44,200
observations from the
distant past will work well,

90
00:05:44,230 --> 00:05:47,680
but instead of manually
deciding what those weights are,

91
00:05:47,710 --> 00:05:50,620
let's define a smoothing
parameter called Alpha.

92
00:05:50,800 --> 00:05:54,790
The rate at which the weights
will decrease is controlled
by this parameter and

93
00:05:54,791 --> 00:05:59,540
we can make the equation more simple and
that the of y hat just turns out to be

94
00:05:59,541 --> 00:06:01,250
the sum of just two products.

95
00:06:01,490 --> 00:06:06,290
We've created a weighted moving average
with two weights because the expression

96
00:06:06,291 --> 00:06:09,620
is recursive. This is an
exponential method. Simple,

97
00:06:09,621 --> 00:06:11,750
exponential smoothing to be exact,

98
00:06:12,110 --> 00:06:16,880
but still it's not taking into account
just how much variation there is in our

99
00:06:16,881 --> 00:06:18,380
data.
Charles Holt,

100
00:06:18,440 --> 00:06:23,150
a professor at Carnegie Mellon noticed
this problem a few decades ago and

101
00:06:23,151 --> 00:06:25,760
extended the idea of simple,

102
00:06:25,761 --> 00:06:30,110
exponential smoothing to allow for
forecasting of data with a trend.

103
00:06:30,440 --> 00:06:31,820
This method called Holts.

104
00:06:31,850 --> 00:06:36,850
Linear trend applies exponential
smoothing to both the level and the trend.

105
00:06:37,310 --> 00:06:41,690
The trend is the general pattern of
prices that we observe over a time period

106
00:06:41,840 --> 00:06:44,630
and the level is the average
value in this series.

107
00:06:44,990 --> 00:06:48,980
To Express this mathematically, we
need three equations, one for level,

108
00:06:49,130 --> 00:06:53,570
one for trend and one to combine
both to get the expected forecast.

109
00:06:53,650 --> 00:06:57,380
The level equation demonstrates that
it's the weighted average of the

110
00:06:57,381 --> 00:07:01,580
observation and the within
sample one step ahead forecast.

111
00:07:01,730 --> 00:07:06,020
While the trend equation shows that
it's a weighted average of the estimated

112
00:07:06,021 --> 00:07:11,021
trend at time t based on the other values
as well as the previous estimate of

113
00:07:11,271 --> 00:07:11,930
the trend,

114
00:07:11,930 --> 00:07:16,400
this model is better but it's still not
taking the variation into account as

115
00:07:16,401 --> 00:07:17,480
well as we'd like.

116
00:07:17,481 --> 00:07:22,100
So let's look at one more improvement
from our buddy Holt hold notice that

117
00:07:22,101 --> 00:07:26,330
sometimes there are trends that depend
on certain seasons. For example,

118
00:07:26,331 --> 00:07:30,680
a store that sells winter clothing will
usually make more sales in the winter

119
00:07:30,681 --> 00:07:34,910
instead of this summer and this is a
pattern that repeats as the seasons change

120
00:07:34,911 --> 00:07:35,744
yearly,

121
00:07:35,780 --> 00:07:40,340
we can call this pattern seasonality
and data sets that show a similar set of

122
00:07:40,341 --> 00:07:43,940
patterns after fixed intervals
over a time period. Exhibit.

123
00:07:43,941 --> 00:07:48,830
This Holtz winter method takes into
account both trend and seasonality to

124
00:07:48,831 --> 00:07:50,030
forecast prices.

125
00:07:50,300 --> 00:07:54,560
We apply exponential smoothing to the
seasonal components in addition to level

126
00:07:54,561 --> 00:07:57,890
and trend, each has its own
unique smoothing parameter.

127
00:07:58,160 --> 00:08:01,520
The level equation we'll show the
weighted average between the seasonally

128
00:08:01,580 --> 00:08:05,630
adjusted observations and the non
seasonal forecast from time t.

129
00:08:06,050 --> 00:08:10,610
The trend is the same as holds linear
method and the seasonal equation shows a

130
00:08:10,611 --> 00:08:15,110
weighted average between the current
seasonal index and the seasonal index of

131
00:08:15,111 --> 00:08:18,290
the same season. Previously,
when we plot this,

132
00:08:18,291 --> 00:08:22,760
we get our lowest error yet and our
prediction finally looks like it's fitting

133
00:08:22,761 --> 00:08:23,720
or testing data,

134
00:08:24,200 --> 00:08:27,980
but wait before you try and
collapse the financial markets.

135
00:08:28,220 --> 00:08:30,680
As great as that last method was,

136
00:08:30,800 --> 00:08:35,000
keep in mind that so far we've only
looked at univariate time series data

137
00:08:35,300 --> 00:08:38,660
meeting. It's a series. What they
single time dependent variable.

138
00:08:39,020 --> 00:08:43,310
Usually though in the real world there
are multiple variables at play and

139
00:08:43,430 --> 00:08:47,390
handling all of them at the same
time is a skill that can be mastered.

140
00:08:47,690 --> 00:08:51,530
This is instead considered
multi-variate time series data.

141
00:08:51,860 --> 00:08:56,860
Each variable in a multivariate time
series depends not only on its past values

142
00:08:57,450 --> 00:09:00,270
but also has some dependency
on other variables.

143
00:09:00,640 --> 00:09:04,500
Want to use these dependencies
to help forecast future values.

144
00:09:04,800 --> 00:09:09,800
One of the most commonly used methods
for multivariate time series forecasting

145
00:09:10,140 --> 00:09:14,610
is called vector auto regression
or or bar and the bar model.

146
00:09:14,640 --> 00:09:19,640
Each variable is a linear function of
the past values of itself and the past

147
00:09:19,711 --> 00:09:23,010
values of all other variables.
If we have two variables,

148
00:09:23,011 --> 00:09:27,480
y one and y two and we need to forecast
the value of these two variables at a

149
00:09:27,481 --> 00:09:31,620
given time, we can represent the
relation between both. As such,

150
00:09:31,830 --> 00:09:33,180
we've got constant terms,

151
00:09:33,181 --> 00:09:38,181
coefficients and error terms here in order
to use the multiple variable terms in

152
00:09:38,911 --> 00:09:43,740
each equation we'll use vectors,
but what else could we do here?

153
00:09:43,830 --> 00:09:46,710
Well,
we haven't yet used a learning technique.

154
00:09:46,980 --> 00:09:51,570
Recurrent neural networks are able to
learn how to make predictions in sequences

155
00:09:51,571 --> 00:09:56,520
of data and a variance of recurrent
networks called long short term memory

156
00:09:56,521 --> 00:10:00,660
networks are able to learn from
even longer sequences of data.

157
00:10:01,080 --> 00:10:06,080
We can treat our Dataset as a supervised
problem if we'd like and use an LSTM

158
00:10:06,331 --> 00:10:08,370
network to predict our target.

159
00:10:08,910 --> 00:10:13,910
We could also use reinforcement learning
where we frame the market as a Markov

160
00:10:14,100 --> 00:10:19,100
decision process and use real time reward
signals from the market to help adjust

161
00:10:19,531 --> 00:10:24,150
our predictions as in if our buy and
sell decisions affect the price of gold,

162
00:10:24,600 --> 00:10:28,020
I've got videos on both of those topics.
Link will be in the video description.

163
00:10:28,440 --> 00:10:32,970
There are three things to remember from
this video for univariate time series.

164
00:10:32,971 --> 00:10:37,971
Data Holtz winter method works pretty
well for multivariate at time series data.

165
00:10:38,731 --> 00:10:41,970
We can use a model called
[inaudible] auto regression,

166
00:10:42,360 --> 00:10:44,250
and if we're feeling ambitious,

167
00:10:44,370 --> 00:10:49,230
we can use a learning technique like
recurrent networks to help learn trends in

168
00:10:49,231 --> 00:10:51,840
the data rather than using a static model.

169
00:10:52,110 --> 00:10:53,790
I'm so proud of you for
making it to the end.

170
00:10:54,030 --> 00:10:56,670
Please subscribe for more
programming videos, and for now,

171
00:10:56,730 --> 00:10:59,490
I'm going to go forward in time,
so thanks for watching.

