1
00:00:00,640 --> 00:00:05,020
Starting broadcast.
All right.

2
00:00:07,220 --> 00:00:09,210
Live stream is starting soon.

3
00:00:11,460 --> 00:00:16,140
We got people in the house.
Hello world. It's Raj.

4
00:00:16,141 --> 00:00:17,190
Good to see everybody.

5
00:00:18,540 --> 00:00:22,380
We are here or I am here
in Portland, Oregon. Uh,

6
00:00:22,770 --> 00:00:25,320
just for a week just to check it out,
see what it's like here.

7
00:00:25,321 --> 00:00:28,740
It's actually pretty cool up here in
the Pacific northwest. Hi everybody.

8
00:00:28,741 --> 00:00:31,350
I'm going to list some names.
Uh, everybody who's here.

9
00:00:31,351 --> 00:00:34,170
Alexandros Radu Marco Max.

10
00:00:34,171 --> 00:00:38,550
David said to Raphael for Ya.

11
00:00:39,270 --> 00:00:42,890
Sausa hi everybody.
So Yasha um,

12
00:00:43,140 --> 00:00:46,590
today we are going to,
uh,

13
00:00:46,650 --> 00:00:51,650
make a bot for the game of
asteroids and we're going to,

14
00:00:52,910 --> 00:00:57,180
uh, have it get better and better at
the game until it's able to beat it.

15
00:00:57,181 --> 00:01:00,360
And it's going to use a
technique called neuro evolution.

16
00:01:00,750 --> 00:01:04,440
It's called neuro evolution. Okay.
Uh, that's the name of this technique.

17
00:01:04,740 --> 00:01:08,700
And uh, we're going to do
this in javascript. Okay.

18
00:01:08,701 --> 00:01:12,650
And this is a part of a collaboration
with another youtuber named Daniel

19
00:01:12,650 --> 00:01:17,490
Shiffman, uh, that I am excited to do.
Okay. So that's what we're going to do.

20
00:01:17,820 --> 00:01:20,400
I'm going to start off with a
five minute Q and a like always.

21
00:01:20,401 --> 00:01:22,920
And then we're going to get started
with the code. All right, here we go.

22
00:01:22,950 --> 00:01:27,180
Five minute Q and. A. Let's do
this. I'm doing pretty well.

23
00:01:27,210 --> 00:01:29,510
I'm excited to be here in Portland.

24
00:01:29,530 --> 00:01:32,490
I'm going to make an
experimental video here.

25
00:01:36,770 --> 00:01:40,940
Is Dan joining live?
Know which tool do we have to use?

26
00:01:41,390 --> 00:01:45,470
It's going to be just pure
javascript neural evolution.

27
00:01:45,471 --> 00:01:49,070
Like a genetic algorithm. Yes. Hi
from Israel. Did you go to nips?

28
00:01:49,071 --> 00:01:52,970
I didn't go to nips. I didn't go to
nips. But next time I will for sure.

29
00:01:53,480 --> 00:01:56,960
Are you building the game to not the game,
just the Ai Code.

30
00:02:00,340 --> 00:02:03,590
You should come on up to Seattle. I
would love to. I would love to, um,

31
00:02:04,940 --> 00:02:09,770
advice for upcoming computer scientists.
Um, find a paper that you really like,

32
00:02:09,771 --> 00:02:12,890
just find a paper that you really
like and then try to replicate it.

33
00:02:12,891 --> 00:02:16,230
And then if you need help using an
advisor or someone who has a lot of

34
00:02:16,231 --> 00:02:19,790
experience. But I found that replicating
papers has gotten me really good,

35
00:02:19,791 --> 00:02:23,910
really fast. Uh, add a lot of, uh, at
least for ml, and I'm sure, I mean,

36
00:02:23,930 --> 00:02:27,170
the same can be applied to computer
science theory. What do you do?

37
00:02:27,171 --> 00:02:30,830
Saroj I do this youtube channel
fulltime. Any news about rocket AI? Guys,

38
00:02:30,831 --> 00:02:34,340
I'm sorry to break the news for you,
but rocket AI is a total hoax.

39
00:02:34,370 --> 00:02:38,570
It's not real that everybody was in on
it. They trolled everybody. Tim portally,

40
00:02:38,571 --> 00:02:42,620
recurrent optimal learning
troll. Okay. That's the acronym.

41
00:02:42,860 --> 00:02:44,270
Are you drinking coffee?
Yes.

42
00:02:44,570 --> 00:02:47,630
What can a high school do in
machine learning right now? Um,

43
00:02:48,520 --> 00:02:51,740
you can get started with TF,
learn or a psychic learn.

44
00:02:51,770 --> 00:02:53,750
Those are two great libraries
to get started with.

45
00:02:54,020 --> 00:02:58,850
Find a Dataset on Kaggle and then run a
model on it and see what you can find.

46
00:02:59,500 --> 00:03:02,890
Well, you ever cover where you cover
other AI concepts beyond neural networks?

47
00:03:02,920 --> 00:03:07,460
Absolutely. Um, tensorflow
versus care Os. Um,

48
00:03:08,140 --> 00:03:11,890
tensorflow, which AI tool will you
be using in this video? I'm not.

49
00:03:11,891 --> 00:03:16,780
I'm using pure javascript. Um, uh,
please buster wrap on screen right now.

50
00:03:16,781 --> 00:03:19,940
I will say that till the end for the end.
Uh,

51
00:03:20,200 --> 00:03:22,330
papers from where I triple e archive.

52
00:03:22,420 --> 00:03:27,310
Arx id.org archive is a good place.
Shall I tweet you for technical help?

53
00:03:27,340 --> 00:03:30,900
Yes. You can tweet me at the same time.
Remember we have a community guys.

54
00:03:30,910 --> 00:03:33,220
It's not just me.
We're all here to help each other learn.

55
00:03:33,221 --> 00:03:36,400
So there's a slack channel with
a link in all of my videos,

56
00:03:36,401 --> 00:03:40,270
so all remember to always ask
questions there as well. Um,

57
00:03:40,420 --> 00:03:44,250
two more questions and then we're going
to get started. Thanks to Alexander. Uh,

58
00:03:44,830 --> 00:03:46,330
is that a promise about busing the wrap?

59
00:03:46,420 --> 00:03:50,080
Yesterday's did you please remind me
though difference between Siri and Google?

60
00:03:50,130 --> 00:03:53,860
Uh, Siri is a worse in every
way because like, wow, apple,

61
00:03:53,861 --> 00:03:57,040
you just totally missed the boat on AI
and just now you just like publish your

62
00:03:57,041 --> 00:04:01,630
first paper. Congrats. Um, but Google now
is just like way better. Like they're,

63
00:04:01,640 --> 00:04:04,390
they're the, the learning algorithms
they have the data that it's trained on.

64
00:04:04,391 --> 00:04:07,270
It's just way more. What do you think
about octave? It's not that great.

65
00:04:07,420 --> 00:04:10,690
How do I implement decompression by
song? Hahn? Deep compression? Well,

66
00:04:10,691 --> 00:04:12,660
first of all, anything with
compression and neural networks,

67
00:04:12,750 --> 00:04:14,440
it has to do with an auto encoder.

68
00:04:15,110 --> 00:04:16,810
So you're probably going
to use an auto encoder.

69
00:04:17,290 --> 00:04:19,090
The word deep implies many layers.

70
00:04:19,091 --> 00:04:23,920
So like lots of layers and
the specific one that he made,

71
00:04:23,921 --> 00:04:28,000
I haven't seen, but show me a Lincoln,
then I might make a video on it. Okay.

72
00:04:28,001 --> 00:04:30,630
So those are my questions.
Those are my questions. Um,

73
00:04:31,060 --> 00:04:32,980
now we're going to get
started with the code. Okay.

74
00:04:33,220 --> 00:04:37,480
So I'm going to go ahead and screen share.
Let's do this.

75
00:04:43,920 --> 00:04:44,411
All right,

76
00:04:44,411 --> 00:04:48,160
so I want to start off by showing you
guys a demo of this code so you can see

77
00:04:48,161 --> 00:04:52,270
what it looks like. Okay. I'm going
to show you guys a demo of this code.

78
00:04:52,810 --> 00:04:57,330
So, oh, look like this.

79
00:05:01,950 --> 00:05:03,300
So what happens is,

80
00:05:04,430 --> 00:05:04,790
okay,

81
00:05:04,790 --> 00:05:09,290
uh, Amy ass playing and
many different generations,

82
00:05:09,291 --> 00:05:14,100
many different, uh, space,
uh, ships are created. Uh,

83
00:05:14,150 --> 00:05:16,190
this is, so, this is neuro evolution.

84
00:05:16,191 --> 00:05:19,040
I'm going to talk about what's
happening here, but as you can see,

85
00:05:19,041 --> 00:05:21,410
it's just there are a lot of
them that are being spawned,

86
00:05:21,440 --> 00:05:23,690
a lot of ships and they're
all trying different moves.

87
00:05:23,960 --> 00:05:27,260
And the one that has the best move is
the one that's going to survive and

88
00:05:27,261 --> 00:05:30,380
reproduce and create a better one.
And so over time it's going to get better.

89
00:05:30,470 --> 00:05:33,170
But you can see here that the basic
idea I want to show you by looking,

90
00:05:33,350 --> 00:05:36,160
by showing you this, uh, game is that, uh,

91
00:05:36,380 --> 00:05:38,750
these things get better over
time and there are many of them,

92
00:05:38,810 --> 00:05:42,230
many of them that respond. Okay?
So that's the basic idea. I'm,

93
00:05:42,231 --> 00:05:43,700
the link will be in the code.
Okay?

94
00:05:46,030 --> 00:05:46,810
Okay.

95
00:05:46,810 --> 00:05:49,840
Are we lagging? We're lagging. Cool. Okay,

96
00:05:49,900 --> 00:05:52,610
so now let's go ahead and
get started with this. Um,

97
00:05:53,500 --> 00:05:55,240
I'm going to go ahead and
get started with the code.

98
00:05:55,420 --> 00:05:58,310
I'll make this bigger so
everybody can see what I'm typing.

99
00:05:58,490 --> 00:06:00,290
So this is going to be neuro evolution.

100
00:06:00,320 --> 00:06:03,050
That's the technique we're
using is the technique.

101
00:06:05,000 --> 00:06:09,590
Okay? So that's a technique that
we're going to be using. And uh, yeah,

102
00:06:09,591 --> 00:06:13,790
so there's that. And so, so that's what
we're going to do. And so what happened?

103
00:06:13,800 --> 00:06:16,100
What's happening here is like what most,
uh,

104
00:06:16,170 --> 00:06:19,620
learning methods focus on modifying just
the strength of the neural connections.

105
00:06:20,110 --> 00:06:24,060
Uh, this doesn't, this modifies not
just, uh, the neuro connections.

106
00:06:24,120 --> 00:06:27,750
It modifies the actual
structure of the network.

107
00:06:27,751 --> 00:06:31,530
So adding neurons, um, it's, uh,

108
00:06:31,860 --> 00:06:35,690
adding connections. It's uh, and even, uh,

109
00:06:35,920 --> 00:06:38,400
modifies the learning rates.

110
00:06:38,910 --> 00:06:43,170
So a lot of hyper parameters are modified
by this. It's not just the connections.

111
00:06:43,290 --> 00:06:47,790
So that's what makes it different
from like, from regular,
uh, backpropagation. Uh,

112
00:06:47,791 --> 00:06:52,650
okay, so bigger font, okay,
let me make it a little bigger.

113
00:06:52,920 --> 00:06:57,270
Okay. Boom. That's a technique that
we're going to be learning. Uh,

114
00:06:57,510 --> 00:06:59,760
and this is a type of
reinforcement learning.

115
00:07:00,030 --> 00:07:03,510
This is a type of reinforcement learning
that's we're going to be learning right

116
00:07:03,511 --> 00:07:06,190
now. And because it's happening
through trial and error, uh,

117
00:07:06,510 --> 00:07:09,770
so what's going to happen
is we're going to do,

118
00:07:09,990 --> 00:07:12,870
and the reason we're doing this is because
the optimal actions at each point in

119
00:07:12,871 --> 00:07:17,190
time are not always known. Like, remember,
this is a game. This is a game. Uh,

120
00:07:17,500 --> 00:07:19,170
things are random.
Things are random.

121
00:07:19,200 --> 00:07:23,760
And neuro evolution proves to be a
good technique for this because it,

122
00:07:23,790 --> 00:07:27,990
you're able to optimize behavior
given only a sparks feedback.

123
00:07:27,991 --> 00:07:31,470
So as far as feedback is
available is available.

124
00:07:32,850 --> 00:07:34,180
All right?
Um,

125
00:07:35,210 --> 00:07:36,043
okay.

126
00:07:36,130 --> 00:07:38,410
But that's what's happening
here. So, so the steps are,

127
00:07:38,420 --> 00:07:40,560
we're going to do this
in increasing order.

128
00:07:40,740 --> 00:07:44,850
So we're going to go ahead and create
art, neuro evolution, uh, object.

129
00:07:44,880 --> 00:07:48,540
But in that we're going to
create increasingly larger, uh,

130
00:07:49,560 --> 00:07:52,230
objects. So the first one is the
neuron and the next one is the layer,

131
00:07:52,650 --> 00:07:55,740
then it's going to be the network,
then the genome,

132
00:07:56,340 --> 00:07:59,130
and then finally the generations.
So as you can see,

133
00:07:59,160 --> 00:08:01,520
each of these gets bigger over time.
So I'm,

134
00:08:01,590 --> 00:08:05,370
so we'll create new revolution
as our big master object.

135
00:08:05,700 --> 00:08:08,620
Then we're going to create a
neuron, a. And so a neuron it,

136
00:08:08,640 --> 00:08:10,950
there are many neurons inside
of a layer of a network.

137
00:08:11,190 --> 00:08:13,140
And then there are many
layers inside of a network.

138
00:08:13,440 --> 00:08:16,320
And then each network
is considered a geno.

139
00:08:16,380 --> 00:08:19,550
A genome is another word for
individual or being in a, in,

140
00:08:19,560 --> 00:08:22,800
in what's called a generation.
Okay? So there's a generation,

141
00:08:22,830 --> 00:08:26,670
a generation has multiple genomes.
Each genome has its own neural network.

142
00:08:26,880 --> 00:08:31,170
Each neural network has its own set of
layers and each layer has it's own set of

143
00:08:31,171 --> 00:08:35,860
neurons. Okay? So it's all
obstructions. Okay? Everything is, um,

144
00:08:38,450 --> 00:08:41,270
all right, everything is just
like this. All right? So,

145
00:08:41,271 --> 00:08:43,460
so that's what's happening
right now in sports.

146
00:08:43,461 --> 00:08:45,290
Feedback means that you
don't have a lot of feedback.

147
00:08:45,320 --> 00:08:48,190
Like you don't have a lot of data. The
word sparts is applied to data. What?

148
00:08:48,191 --> 00:08:51,890
We don't have a lot of it. So if there's
a lot of zeroes in our data, uh, that,

149
00:08:51,891 --> 00:08:56,270
that would be sparse. Okay? So let's go
ahead and get started by doing this. Um,

150
00:08:56,810 --> 00:08:58,800
all right, so let's go ahead
and create our first variable.

151
00:08:58,801 --> 00:09:00,990
It's called neuro evolution,
neuro evolution.

152
00:09:00,991 --> 00:09:05,040
That is our big master variable and it's
going to be a function and we're going

153
00:09:05,041 --> 00:09:07,020
to get options.
So what are these options?

154
00:09:07,021 --> 00:09:08,970
Options are another word
for hyper parameters.

155
00:09:08,971 --> 00:09:12,420
And I'm going to talk about what the
specific hyper parameters are for this.

156
00:09:12,630 --> 00:09:16,590
Okay? So we're going to start off by
defining defining ourself self variable.

157
00:09:16,680 --> 00:09:19,710
By calling this, okay,
that's the first step.

158
00:09:20,010 --> 00:09:23,290
Now we want to create those
options. So remember, options are,

159
00:09:23,300 --> 00:09:25,630
are hyper parameters,
um,

160
00:09:26,890 --> 00:09:31,270
is unsupervised learning.
Indeed. Exactly. Um, options are,

161
00:09:31,271 --> 00:09:32,280
are hyper parameters.

162
00:09:32,281 --> 00:09:35,970
So we're going to start off by saying
the first option we want is our sigmoid

163
00:09:35,971 --> 00:09:38,130
function. Let me talk about
what the sigmoid function is,

164
00:09:38,430 --> 00:09:43,230
but first let me type it out. So sigma
is an activation function, okay? Um,

165
00:09:43,500 --> 00:09:47,180
sigma is an activation function and it's
a function that we run in every neurono

166
00:09:47,190 --> 00:09:50,040
bar network. Okay? So let
me write, write this out.

167
00:09:50,220 --> 00:09:55,220
I'm going to create a variable called
AP that is negative a over one.

168
00:09:55,980 --> 00:09:59,880
And then I'm going to return this
equation. What does this equation, well,

169
00:09:59,910 --> 00:10:04,910
this is the actual sigmoid equation
and it is a static equation.

170
00:10:05,310 --> 00:10:07,050
Let me explain the hyper
parameters in a second.

171
00:10:07,380 --> 00:10:11,040
But this is a static equation and what
it's doing is it's converting every

172
00:10:11,041 --> 00:10:14,190
number into a set up into a probability.
So this,

173
00:10:14,191 --> 00:10:16,140
this sigmoid is run in,

174
00:10:17,070 --> 00:10:21,090
this function is run in every
neuron of our network, okay?

175
00:10:21,091 --> 00:10:25,770
And it's how we can convert our numbers
into probabilities. Return. Okay,

176
00:10:25,771 --> 00:10:27,380
return.
What are hyper printers?

177
00:10:27,390 --> 00:10:31,920
Hyper parameters are the tuning knobs of
our network. They change over time. Okay?

178
00:10:31,921 --> 00:10:35,580
They are the tuning knobs of our network.
We could change what they are. Okay?

179
00:10:35,581 --> 00:10:37,890
And I'm going to show you a list
of hyper parameter in a second.

180
00:10:38,310 --> 00:10:41,730
The next function we want to create,
it's called random clamps. Okay?

181
00:10:43,080 --> 00:10:45,540
Random. Uh, it's called random clamped.

182
00:10:45,930 --> 00:10:50,930
And what this function does is it returns
a random value and we can use that to

183
00:10:51,631 --> 00:10:54,330
generate weights as well.
So that's a different point,

184
00:10:54,331 --> 00:10:58,320
but we're going to use this to generate
a set of weights later, okay? And

185
00:11:00,310 --> 00:11:01,143
okay,

186
00:11:01,370 --> 00:11:04,460
so there's that.
And I want to return math dot random.

187
00:11:04,461 --> 00:11:05,660
We're going to use the random function.

188
00:11:05,990 --> 00:11:07,810
We're going multiply it
by two to make sure it,

189
00:11:07,820 --> 00:11:11,240
that the number is big enough for
us. And it's a crack by. Whoa. Okay.

190
00:11:11,241 --> 00:11:12,560
So there's those,
there's that.

191
00:11:12,800 --> 00:11:15,020
Now we're going to create our
list of hypochlor amateurs.

192
00:11:15,290 --> 00:11:18,500
The first one is going to be a population,
which is the size of our population,

193
00:11:18,501 --> 00:11:21,410
right? How big do we want
this, this population to be?

194
00:11:21,980 --> 00:11:26,130
The next one is called elite elitism.
What? It's Alita zone. This is the,

195
00:11:26,630 --> 00:11:31,310
this is the number that we're
going to apply to each of. So, so,

196
00:11:31,370 --> 00:11:35,660
so elitism is whenever, um, we are,

197
00:11:36,260 --> 00:11:39,090
uh, so let me give you a quick
explanation of, of, of, of uh,

198
00:11:39,110 --> 00:11:41,920
genetic programming for a second
genetic algorithms. So you,

199
00:11:41,921 --> 00:11:44,270
you have a population,
so it's step one is a,

200
00:11:44,330 --> 00:11:48,380
you have a population and then what
happened is they performed crossover,

201
00:11:48,560 --> 00:11:53,020
which is they, they, they breed together
and then the end result is then you,

202
00:11:53,320 --> 00:11:56,890
those children and you keep going,
right? There are three, right? So, so,

203
00:11:56,891 --> 00:12:00,970
so you select the best ones selection
and then you perform crossover.

204
00:12:00,971 --> 00:12:02,320
You take those and you make them.

205
00:12:02,321 --> 00:12:05,560
And I'm going to show you guys what mate
means in this context and then we're

206
00:12:05,561 --> 00:12:08,320
going to mutate them.
And so this is what elite tourism does.

207
00:12:08,480 --> 00:12:10,240
Elitism is dealing with the mutation.

208
00:12:10,770 --> 00:12:14,920
Polytheism means we're taking
how w 20%. Right? At 0.2.

209
00:12:14,930 --> 00:12:19,450
So we're going to take 20% of
the best offspring. Okay? Um,

210
00:12:20,070 --> 00:12:20,620
okay.

211
00:12:20,620 --> 00:12:22,210
So that's what's going to be happening.
Um,

212
00:12:22,480 --> 00:12:24,970
so right now we're going
to say random behavior is,

213
00:12:25,600 --> 00:12:30,550
is going to be a 0.2. Okay.
So what does random behavior,

214
00:12:30,750 --> 00:12:35,300
um, ran behavior is the new
random networks for the next
generation. So the rate,

215
00:12:35,650 --> 00:12:39,760
so how, how, how, how random
do we want this to be? Right?

216
00:12:40,060 --> 00:12:44,430
The next one is the mutation rates.
You Taishan rate is saying, uh,

217
00:12:45,240 --> 00:12:48,540
what the, the rate of the, that
we're updating the synapsis, how,

218
00:12:48,541 --> 00:12:52,750
how much do we want to mutate them? Okay.
And then we have a mutation range. Okay.

219
00:12:53,130 --> 00:12:56,160
Uh, why the tradeoff between
20%? Good question. Uh,

220
00:12:56,200 --> 00:12:59,830
20% as opposed to something larger,
like I don't know, more than 50.

221
00:13:00,070 --> 00:13:03,550
You want something that's less than 50,
because you only want the best ones,

222
00:13:03,820 --> 00:13:07,620
you only want the best ones and you
want to diminish the rate, uh, that,

223
00:13:07,660 --> 00:13:11,020
that you want to diminish the,
the rate that the population is,

224
00:13:11,021 --> 00:13:15,400
is a breeding so that by the
time your son, uh, first of all,

225
00:13:15,401 --> 00:13:19,090
it's faster that the network trains
faster and you only have a few at the end.

226
00:13:19,091 --> 00:13:20,410
Okay.
Um,

227
00:13:21,870 --> 00:13:22,630
okay.

228
00:13:22,630 --> 00:13:26,260
Okay. So there's that. So the
mutation range is going to be 0.5.

229
00:13:26,940 --> 00:13:30,940
Then we have our actual network. So
the network is going to be one. Uh,

230
00:13:30,970 --> 00:13:34,090
and so what is the network?
Well, now I'm defining, um,

231
00:13:34,450 --> 00:13:38,570
now I'm defining the size of the network.
What does that mean? It means, uh,

232
00:13:39,230 --> 00:13:43,990
uh, like the, the structure of the
neural network. I want one layer. Okay.

233
00:13:43,991 --> 00:13:47,560
I went three of them. So this is a three
layer feed forward neural network. Okay?

234
00:13:47,830 --> 00:13:52,090
So that's the network. And let me define
that as an array. So that's an array.

235
00:13:52,091 --> 00:13:55,570
And so now we want another
hyper parameter called historic,

236
00:13:55,840 --> 00:13:59,200
which saves the adult,
the latest generation that's safe.

237
00:13:59,260 --> 00:14:01,600
That's what it's going in store.
What's the last generation?

238
00:14:01,601 --> 00:14:04,870
Right now we'll initialize it as
zero because there are no, uh,

239
00:14:05,050 --> 00:14:07,720
generations right now.
The next one is going to be low historic.

240
00:14:07,720 --> 00:14:11,920
So we're only going to save
this the score, right? It's
a boolean that says like,

241
00:14:12,700 --> 00:14:16,210
we're only going to save this score.
Now we want to sort sort it, right?

242
00:14:16,211 --> 00:14:18,250
So we want to sort it
up till negative ones.

243
00:14:18,251 --> 00:14:19,990
So what order do we want
this sort things in?

244
00:14:20,290 --> 00:14:23,200
We're going to sort it right now in the
sending wardrobe because it's negative

245
00:14:23,201 --> 00:14:27,350
one. If we wanted it to be
positive, we would do positive
one. Okay. This is a Bot.

246
00:14:27,700 --> 00:14:31,270
So now and be trialed one.
All right,

247
00:14:31,390 --> 00:14:33,940
so this is the number of children
that we want to breathe. Okay.

248
00:14:34,120 --> 00:14:38,620
So that's it for our a hyper parameters.
Let me just move that up,

249
00:14:39,100 --> 00:14:44,100
that different hyper parameters and
now we can go ahead and initialize.

250
00:14:45,640 --> 00:14:50,390
Yes. All right, so let me go ahead
and move that here. I see. Okay.

251
00:14:50,391 --> 00:14:55,130
So now we're going initialize our are
set variables to have options available.

252
00:14:55,550 --> 00:14:59,040
Okay. So what is this?
Now I'm going to um,

253
00:14:59,840 --> 00:15:04,640
behave. Oh, good call typo
on behavior. Behavior.

254
00:15:05,370 --> 00:15:10,060
All right, great. And
variables. No variable. It's
not very, almost scary though.

255
00:15:10,220 --> 00:15:14,560
So we want to create a sense, what do I
mean by a sec? We want to set up options.

256
00:15:14,600 --> 00:15:19,250
We're going to initialize
that like this. Um, okay.

257
00:15:19,400 --> 00:15:24,080
So, um, so now that we've
initialize our variables,

258
00:15:24,081 --> 00:15:28,490
we're going to say, we're going to
loop, we're going to do in this loop,

259
00:15:28,610 --> 00:15:32,750
we're going to say for every option that
we have, for every option that we have,

260
00:15:32,990 --> 00:15:36,630
I want you to do to, uh, say if, if,

261
00:15:36,830 --> 00:15:38,240
if there's nothing there right now,

262
00:15:38,310 --> 00:15:42,860
then I want you to fill it into our
existing list of, of options. Okay.

263
00:15:43,610 --> 00:15:48,530
Uh,
got to add those in.

264
00:15:48,950 --> 00:15:51,200
Okay. So I've got up
my comments here. Okay.

265
00:15:51,201 --> 00:15:56,201
So then if the option is there
and it's not defined undefined,

266
00:15:57,320 --> 00:16:01,160
then we are going to add it
to our options are ray. Okay?

267
00:16:01,161 --> 00:16:06,020
So we want every option to be,
uh, in my options are ray. Okay?

268
00:16:06,021 --> 00:16:10,730
So options up til I, alright, so now,

269
00:16:11,440 --> 00:16:15,560
um, so there's that. And so
now let's create our neurons.

270
00:16:15,620 --> 00:16:19,760
We're going to create our
neuron. Um, all right,

271
00:16:19,761 --> 00:16:22,620
so our neuron is going to be,
um,

272
00:16:24,670 --> 00:16:28,940
it's going to be a function to, so the
neuron has an internal value and as,

273
00:16:29,000 --> 00:16:33,280
as it has a set of connections to every
other neuron in the next layer. Okay.

274
00:16:33,281 --> 00:16:37,530
So we're going to define
both of those to, uh, uh, uh,

275
00:16:37,690 --> 00:16:40,690
both of those two variables. Okay?
So the first one is the value.

276
00:16:40,691 --> 00:16:43,180
It's going to be zero. Like what
is the value of this neuron? Well,

277
00:16:43,181 --> 00:16:46,900
if we're going to initialize it a zero
and then the weights or connections to

278
00:16:46,901 --> 00:16:47,681
every other node,

279
00:16:47,681 --> 00:16:50,620
which we were going to store in
an array which is empty right now,

280
00:16:50,920 --> 00:16:55,630
the next step is going to be to
randomly initialized our weight values.

281
00:16:55,890 --> 00:16:58,120
Uh, we want, we want them to be distinct.

282
00:16:58,540 --> 00:17:02,410
We want them to be distinct,
okay?

283
00:17:05,170 --> 00:17:09,890
All right? So now we want
them to be distinct, right?

284
00:17:09,891 --> 00:17:11,210
So let's go ahead and do this.

285
00:17:11,300 --> 00:17:13,550
So the first one we're going to
do is we're going to create a,

286
00:17:13,880 --> 00:17:16,630
we're going to create a neuron.
It's going to be a prototype objects,

287
00:17:17,420 --> 00:17:20,260
which is the higher level, uh,
object. And we're going to,

288
00:17:20,570 --> 00:17:22,760
we're going to create our
population function, okay?

289
00:17:22,790 --> 00:17:24,480
Or we're going to populate it,
but,

290
00:17:24,530 --> 00:17:27,050
and we're going to do this by randomly
initializing wait values, okay?

291
00:17:27,051 --> 00:17:28,340
So let me show you what I mean by this.

292
00:17:28,640 --> 00:17:32,540
Well initialize a function and the
perimeter is going to be n be okay,

293
00:17:32,990 --> 00:17:37,910
which is, uh, the number of weights that
we once a why randomize the weights?

294
00:17:37,960 --> 00:17:42,890
Uh, because, uh, well
I mean there are many,

295
00:17:42,920 --> 00:17:43,970
many methods of like,

296
00:17:44,330 --> 00:17:48,320
like how do initial initialize
weights random is the most popular,

297
00:17:48,900 --> 00:17:53,470
but there could be a better,
a more, uh, more, uh,

298
00:17:53,600 --> 00:17:56,810
more, uh, efficient way of
doing that. But right now, uh,

299
00:17:56,820 --> 00:17:58,410
and that's an area of research as well,

300
00:17:58,411 --> 00:18:02,250
but generally we randomly initialized
our weights and then we make them better

301
00:18:02,251 --> 00:18:06,900
over time. But just like randomly
sampling data, uh, it could be,

302
00:18:06,901 --> 00:18:10,530
we could do that better as
well, but random is just an
easy way to, to start off.

303
00:18:10,531 --> 00:18:15,460
Okay. So we'll initialize
our, uh, weights array and uh,

304
00:18:15,510 --> 00:18:18,180
we're going to iterate through
every week that we're given.

305
00:18:18,181 --> 00:18:22,800
So bar I equals to zero.
I is less than the number of weights.

306
00:18:23,130 --> 00:18:26,370
And then I've left plus, okay. I put flux.

307
00:18:26,700 --> 00:18:31,700
So we're going to say add every weight
to the list of weights that we have and

308
00:18:33,871 --> 00:18:34,501
what are we using?

309
00:18:34,501 --> 00:18:39,501
We're using that random clamped function
to add a randomly initialized set of

310
00:18:39,571 --> 00:18:43,080
weights to our weights array.
Okay, so that's for our neuron.

311
00:18:43,081 --> 00:18:46,680
Now let's do our layer.
Remember we're increasing the size of,

312
00:18:47,000 --> 00:18:51,720
of how big we are in a of the objects
that we are creating every time.

313
00:18:52,050 --> 00:18:54,330
So we did our weights and now
we're going to do our layer.

314
00:18:54,331 --> 00:18:59,120
And the layer has an ID and a number of
neurons. Okay? Those are, it's too, uh,

315
00:18:59,130 --> 00:19:01,470
attributes,
an ID and a number of neurons.

316
00:19:01,740 --> 00:19:04,350
So we'll start off by creating our layer.
Uh,

317
00:19:04,410 --> 00:19:07,860
and I'll definitely do a recap
at the end of the code. Uh,

318
00:19:07,861 --> 00:19:09,300
so we'll start off with creating a layer.

319
00:19:09,450 --> 00:19:13,770
And a rent a layer is going
to be initialized by this
index variable that is the

320
00:19:13,771 --> 00:19:16,080
parameter of the index parameter.
And what does that mean? Well,

321
00:19:16,081 --> 00:19:18,240
that's going to give us our ID.
That's how we initially,

322
00:19:18,270 --> 00:19:22,020
that's how we identified this layer,
right?

323
00:19:22,110 --> 00:19:24,480
So it's going to be index or zero.
Okay.

324
00:19:24,750 --> 00:19:27,720
And so then we want to initialize
our neurons. How many do we have?

325
00:19:27,721 --> 00:19:31,080
We create a layer, none. So it's still
gonna be an empty list. Okay. It's an, uh,

326
00:19:31,290 --> 00:19:32,400
it's going to be an empty list.

327
00:19:33,030 --> 00:19:37,620
Now we are going to create our layer
prototype prepopulating the lake. Okay.

328
00:19:37,621 --> 00:19:41,130
So we're going to populate the layer with,
um,

329
00:19:43,620 --> 00:19:46,080
thank you for that. Uh,
options. That was a typo.

330
00:19:46,260 --> 00:19:49,440
Now we're going to populate the
layer, uh, with neurons, okay.

331
00:19:49,441 --> 00:19:52,800
Which we've just created. We've
just created neurons, okay.

332
00:19:52,801 --> 00:19:57,390
Layered up prototype to populate.
Uh, and we want there to be

333
00:19:58,890 --> 00:20:01,740
two parameters, the number of
neurons and the number of inputs.

334
00:20:02,010 --> 00:20:03,450
What are the input going to be?

335
00:20:03,451 --> 00:20:06,720
It's going to be the set of actions
that we take during the game. Okay?

336
00:20:06,721 --> 00:20:08,400
That's what we're going
to update our way Twitter.

337
00:20:08,850 --> 00:20:13,800
So we're going to initialize an array
of neurons and we're going to say,

338
00:20:13,860 --> 00:20:17,550
okay, so we're going to iterate through
every single one of our neurons and we're

339
00:20:17,551 --> 00:20:22,260
going to add them all
to our layer. Okay? Um,

340
00:20:23,340 --> 00:20:27,590
let me see this question. Can a layer be
iterated or is it explicitly specified?

341
00:20:27,960 --> 00:20:32,540
We can, uh, we can iterate through
the layer. Um, generally, uh,

342
00:20:32,580 --> 00:20:36,510
in machine learning, we don't update
the number of neurons inside of a layer.

343
00:20:36,510 --> 00:20:39,990
We don't update the actual layer
itself other than the, uh, weights.

344
00:20:40,200 --> 00:20:41,190
But in this case,

345
00:20:41,191 --> 00:20:45,510
in neuro evolution as opposed to just
regular backpropagation neural evolution

346
00:20:45,511 --> 00:20:48,400
in neuro evolution,
we modify all parts of the network.

347
00:20:48,401 --> 00:20:50,530
We can modify the number
of neurons in each layer.

348
00:20:50,710 --> 00:20:55,090
We can modify the number of layers, all,
nothing is safe from neuro revolution.

349
00:20:55,091 --> 00:20:58,500
Okay? So it's not just backpropagation,
we were not just updating the weights.

350
00:20:58,530 --> 00:21:01,870
We're updating the entire neural net,
okay?

351
00:21:02,050 --> 00:21:07,030
So we're going to iterate through the
number of neurons and we're going to say

352
00:21:07,360 --> 00:21:11,800
initialize our first neuron. It's
going to be a new neuron object, okay?

353
00:21:11,890 --> 00:21:15,250
So we're going to say we want a
new neuron. And this is, remember,

354
00:21:15,251 --> 00:21:18,550
this is a class that we've just created
and we want to use that populate

355
00:21:18,551 --> 00:21:22,450
function that we already wrote,
given the number of inputs, okay?

356
00:21:22,510 --> 00:21:24,460
So that's the parameter
for that populate function.

357
00:21:24,460 --> 00:21:28,080
And then when we're done with
that, once we populated, uh, uh,

358
00:21:28,330 --> 00:21:32,740
the number of neurons, we can just
push those neurons, uh, to this,

359
00:21:32,830 --> 00:21:37,570
this layers, uh, array. Okay. So we can
just by using the push function, okay.

360
00:21:37,571 --> 00:21:40,000
So that's our layer and now
we're going to go even higher.

361
00:21:40,001 --> 00:21:42,970
What was the next step? The network,
right? We've created our neurons,

362
00:21:42,971 --> 00:21:47,000
we've created our layers, and the next
step is the actual networking sets. Okay.

363
00:21:47,020 --> 00:21:51,080
And a network consists of layers.
So it's initialized our network, uh,

364
00:21:51,100 --> 00:21:54,700
variable by saying, well, what does
a network habits think about this?

365
00:21:55,060 --> 00:21:59,480
A network has a set of layers. That's
it's though that's the activity that has.

366
00:21:59,481 --> 00:22:04,120
So we'll initialize a layers, a
parameter. Okay. So the number of layers.

367
00:22:04,800 --> 00:22:05,633
MMM.

368
00:22:05,980 --> 00:22:10,750
And now that we've initialize that,
let's go ahead and create a prototype for,

369
00:22:10,930 --> 00:22:15,760
uh, giving it the layer parameters. Okay.
So we're going to call this prototype.

370
00:22:16,100 --> 00:22:19,650
Um,
we're going to call this prototype,

371
00:22:20,500 --> 00:22:21,300
uh,

372
00:22:21,300 --> 00:22:25,050
perceptron generation. Okay. Because
that's another word for neural net.

373
00:22:25,051 --> 00:22:27,930
Perceptron although it's not as sexy,
like people don't use it as much.

374
00:22:28,550 --> 00:22:33,240
But whenever, um, you know,
usually whenever genetic
algorithms come into play,

375
00:22:33,241 --> 00:22:35,840
someone like throws around
the word perceptron. Uh,

376
00:22:36,090 --> 00:22:39,060
but I think neuro evolution
in genetic programming,

377
00:22:39,061 --> 00:22:43,290
it's going to make a huge comeback.
Like, like it had a taytay in the 80s,

378
00:22:43,291 --> 00:22:44,960
and then people kind of gave up with it.
But like,

379
00:22:45,480 --> 00:22:47,310
I think we're going to
see some great results.

380
00:22:47,311 --> 00:22:49,530
It's actually coming out of a
opening the eyes universe like this,

381
00:22:49,531 --> 00:22:52,320
that sandbox environment. There's
a lot of possibility here. Uh,

382
00:22:52,440 --> 00:22:55,950
which makes me very excited about it.
Okay. Anyway, so what we're doing here is

383
00:22:56,070 --> 00:22:56,903
mmm,

384
00:22:57,400 --> 00:23:00,670
uh, noted. If Jess, Jess is a little
confusing for beginners. Noted. Okay.

385
00:23:00,880 --> 00:23:05,020
So we're going to create, what am I doing
here? So I'm creating three variables,

386
00:23:05,080 --> 00:23:08,890
right? Be Index the number of previous
neurons and in the layer that we're on,

387
00:23:08,891 --> 00:23:12,580
which we're going to initialize because
we just created that using the index as

388
00:23:12,581 --> 00:23:16,240
a brander. Okay? So what we're
about to do is, well, first of all,

389
00:23:16,241 --> 00:23:19,600
we want to populate the layer because
we are creating a neural network.

390
00:23:19,840 --> 00:23:23,650
This is going to create the
network itself, okay. By
creating a stack of layers.

391
00:23:23,950 --> 00:23:28,430
Okay? Okay. So that's what we're doing.

392
00:23:28,710 --> 00:23:30,660
Um,
uh,

393
00:23:30,790 --> 00:23:33,830
I knew there would be a Westworld
reference at some point. Some of one,

394
00:23:34,000 --> 00:23:36,430
one of my friends was like, dude,
you need to watch Westworld.

395
00:23:36,520 --> 00:23:38,440
Someone on your channel
is going to mention it.

396
00:23:38,620 --> 00:23:40,450
It's going to be great for
me and been references.

397
00:23:40,451 --> 00:23:43,360
So you need to start watching it.
And Lo and behold,

398
00:23:43,361 --> 00:23:47,030
someone has mentioned Westworld,
so I might need to check
up on that show. Anyway,

399
00:23:47,031 --> 00:23:50,230
so,
so we've created our three variables here.

400
00:23:50,240 --> 00:23:53,600
So let's populate the layer with the
inputs that we've already created.

401
00:23:53,601 --> 00:23:57,110
Right up here, we have our
input, we have our, um,

402
00:23:59,040 --> 00:24:01,320
let's see, we have our previous
number of neurons. Okay.

403
00:24:01,321 --> 00:24:04,650
So each of our layers is going
to be populated just like that.

404
00:24:04,950 --> 00:24:09,480
And we're going to add each layer to
our existing array of layers. Okay?

405
00:24:09,481 --> 00:24:13,320
So later, that's that. And
then we went to the index.

406
00:24:13,321 --> 00:24:15,960
We're going to iterate through the
index and the index tells us which layer

407
00:24:15,961 --> 00:24:20,400
we're on. Okay? So we're going to iterate
that, that index a counter variable.

408
00:24:20,730 --> 00:24:23,850
So now we're going to go, we're going
to iterate through every single, uh,

409
00:24:25,100 --> 00:24:29,990
layer in our network. And we're going
to, we're going to add it to our,

410
00:24:30,260 --> 00:24:31,820
uh, we're going to iterate through, sorry,

411
00:24:31,880 --> 00:24:34,640
we're going to tech with a technical
term for this is we're going to iterate

412
00:24:34,641 --> 00:24:36,050
through every single layer in our,

413
00:24:36,260 --> 00:24:39,890
a list of layers and we're going
to add that to our network. Okay?

414
00:24:39,891 --> 00:24:43,730
So we've stored a list of layers and we
want to add each of them to our network.

415
00:24:44,010 --> 00:24:47,980
It's, we'll initialize our first layer
and this is for the number of hiddens.

416
00:24:48,200 --> 00:24:51,560
And what do we mean by hiddens?
Hiddens is, uh, the, the,

417
00:24:51,561 --> 00:24:55,280
the number of hidden layers that we've,
that we've specified in the parameter.

418
00:24:55,640 --> 00:24:56,241
So we say,

419
00:24:56,241 --> 00:24:59,110
so for each of these layers we went
up to populate it with a number of,

420
00:24:59,300 --> 00:25:03,230
of kittens. Okay. Um,
oh, sorry. Never, no.

421
00:25:03,231 --> 00:25:06,440
Let me Redo the UN. So what I just
said, undo that. It's not hidden.

422
00:25:07,070 --> 00:25:11,360
It's not number of hidden layers. It's
number of hidden neurons. Okay. Um,

423
00:25:14,770 --> 00:25:16,300
to answer your question bus card,
yes.

424
00:25:16,301 --> 00:25:20,500
I will do a much more simple neural
network tutorial a, it's coming up.

425
00:25:20,530 --> 00:25:25,120
It's going to be, uh, it's
coming up in January. Okay. So,

426
00:25:25,180 --> 00:25:28,480
uh, so we were going to neutralize it by
using the number of hidden neurons and a

427
00:25:28,481 --> 00:25:31,610
number of previous neurons. And why do
we want the number of previous neurons?

428
00:25:31,800 --> 00:25:34,540
Well, we want to wait. We want
something to point to, right?

429
00:25:34,630 --> 00:25:39,520
Because our weights are going
to be pointing. Okay. So,
uh, now we want to say, well,

430
00:25:39,521 --> 00:25:43,270
now that we initialize that layer while
replace what we have in our previous

431
00:25:43,271 --> 00:25:46,030
neurons with whatever we have in
our hiddens, right? Because we've,

432
00:25:46,180 --> 00:25:47,770
we've propagated forward and we're,

433
00:25:48,130 --> 00:25:53,130
we're replacing the old variable
with a what we've already done.

434
00:25:53,500 --> 00:25:54,100
Okay.

435
00:25:54,100 --> 00:25:59,100
So now we're going to push this layer
onto the list of layers and we're going to

436
00:26:00,970 --> 00:26:04,450
iterate that index counter variable.
So let's look over what we've just done.

437
00:26:04,740 --> 00:26:07,330
What we've just done is we've iterated
through a number of hidden neurons that

438
00:26:07,331 --> 00:26:10,390
we were given into input parameter.
We have initialize a layer.

439
00:26:10,420 --> 00:26:13,110
We have populated it with
those neurons. We've, uh,

440
00:26:13,120 --> 00:26:17,050
set the previous neurons to whatever
we have, whatever we had now, uh,

441
00:26:17,051 --> 00:26:21,190
and then we've pushed that layer
to the list of layers. Okay. Uh,

442
00:26:21,191 --> 00:26:25,600
and so now that we've done that,
we'll create another layer.

443
00:26:25,870 --> 00:26:28,630
And why do we want to create
another layer? Well, there's one,

444
00:26:28,690 --> 00:26:31,480
there's one more thing
that we didn't think about.

445
00:26:31,481 --> 00:26:33,670
We didn't think about the output.
Remember we,

446
00:26:33,910 --> 00:26:38,320
we fed this function on out
a list of outputs, right?

447
00:26:38,350 --> 00:26:42,910
That we want. So we want to create that
last layer for just the health, but right?

448
00:26:42,960 --> 00:26:45,270
So as, so these are our
hidden layers, right?

449
00:26:45,271 --> 00:26:47,880
So remember we created the first layer,
which is our input layer.

450
00:26:48,100 --> 00:26:49,870
We create a list of,
sorry,

451
00:26:50,090 --> 00:26:54,330
a list of hidden layers and now we're
creating art output layer. Okay?

452
00:26:54,840 --> 00:26:58,990
So that's what we're doing
and we'll create it by, uh,

453
00:26:59,310 --> 00:27:02,010
initializing with the number of output
neurons and then the number of previous

454
00:27:02,011 --> 00:27:04,380
neurons. And lastly,
once we've created that,

455
00:27:04,470 --> 00:27:09,030
we can just go ahead and push it to
our list of layers. That's it. Okay.

456
00:27:09,150 --> 00:27:13,020
So our input neuron was created. Our list
of hidden, uh, sorry, our input layer,

457
00:27:13,170 --> 00:27:17,610
our list of hidden layers,
and then our output layer.
Okay. So now we've done that.

458
00:27:17,611 --> 00:27:22,140
We've created our network. Um, and
before we create our generation, uh,

459
00:27:22,170 --> 00:27:23,010
before we do that,

460
00:27:23,400 --> 00:27:28,260
we want to create a set of
a helper functions. Okay?

461
00:27:28,350 --> 00:27:32,940
So the first one I want to do
is, is called compute. Okay?

462
00:27:32,941 --> 00:27:35,780
So this is an important step. So, um,

463
00:27:37,020 --> 00:27:40,640
computation step. Okay. So
let's do the computations step.

464
00:27:40,650 --> 00:27:43,560
This is an important step and
it's a part of our network.

465
00:27:43,740 --> 00:27:47,220
So what do I mean by computation?
Well, as data flows to our network,

466
00:27:47,260 --> 00:27:52,020
it's not static. Uh, we're,
we're actually applying, uh, uh,

467
00:27:52,200 --> 00:27:55,200
we're applying operations to it,
mathematical operations.

468
00:27:55,500 --> 00:27:59,010
And I'm going to talk about what
these operations are. Okay? So,

469
00:28:00,750 --> 00:28:04,140
so the first thing we're going to do is
we're going to check if the layer and

470
00:28:04,141 --> 00:28:07,620
the neurons are not empty and if they're
not, we're going to assign it, uh,

471
00:28:07,680 --> 00:28:10,260
the input value. So we're going to
check, we're going to say, okay,

472
00:28:10,261 --> 00:28:15,240
so check each of those input values. Okay,
let me check ethers, those input values.

473
00:28:15,510 --> 00:28:20,410
And I want to see if, uh, uh,

474
00:28:20,940 --> 00:28:24,630
if the initial layer, uh, and the,

475
00:28:25,650 --> 00:28:29,640
it's number of neurons. So I want to
see if it's number of neurons is empty.

476
00:28:29,700 --> 00:28:33,970
That's what I'm looking for. Um, uh,

477
00:28:34,290 --> 00:28:37,360
what's a neuron. Dot Neurons, uh,

478
00:28:37,530 --> 00:28:41,940
and then wherever we are right now.
And so if there's something there,

479
00:28:42,120 --> 00:28:47,100
then we want to add it to our, the
current, uh, layer that we're on

480
00:28:49,300 --> 00:28:52,570
and we're going to say,
uh,

481
00:28:52,600 --> 00:28:56,840
so whatever layer that we're currently
on, add all of those neurons to it. Uh,

482
00:28:56,950 --> 00:29:01,720
and then, and it's going to
be those input values, right?

483
00:29:02,140 --> 00:29:04,060
The input values. So, okay,

484
00:29:04,061 --> 00:29:07,420
so that was that introductory step where
we actually filled that layer with,

485
00:29:07,540 --> 00:29:11,620
with neurons. Okay. So now that we've
done that, now that we've done that,

486
00:29:11,980 --> 00:29:13,450
now it's time to,

487
00:29:15,160 --> 00:29:19,930
now it's time to actually
do the computation. Okay?
So what's happening here?

488
00:29:20,140 --> 00:29:21,780
We're going to,
the data's going to flow to the layers.

489
00:29:21,781 --> 00:29:24,010
I'm going to apply an
activation function to it,

490
00:29:24,011 --> 00:29:27,790
and then we're going to return the
output. Okay? So let's do that. Uh,

491
00:29:27,791 --> 00:29:31,720
activation function step. We're going
to say, okay, let's go to start off. Um,

492
00:29:34,420 --> 00:29:38,920
this step is unclear. Okay. So
let me, um, let me see. Okay. So,

493
00:29:40,180 --> 00:29:45,010
okay, so the function, so
let's, let's talk about this.
So what is happening here?

494
00:29:45,610 --> 00:29:49,780
I am, what am I doing? I'm
updating the weights. Okay.

495
00:29:49,810 --> 00:29:53,320
I'm updating the way to using
a sigmoid function. Exactly.

496
00:29:53,321 --> 00:29:56,200
So someone said sigmoid. Exactly.
So that's exactly what I'm doing.

497
00:29:56,410 --> 00:30:00,130
So what I'm gonna do is I'm going to
iterate through each of my neurons. Okay.

498
00:30:00,131 --> 00:30:01,780
So let's, so, so let's see.

499
00:30:01,960 --> 00:30:06,760
Iterate through neurons in layer.

500
00:30:06,790 --> 00:30:11,680
Okay. So let me just say it. Iterate their
neurons in a layer. Okay. So for, um,

501
00:30:12,890 --> 00:30:15,380
Vivar I equals one.
Uh,

502
00:30:15,440 --> 00:30:19,520
and I is less than this,
the layers, the length,

503
00:30:21,530 --> 00:30:25,480
and let me to, let me, when
did you better check a pistol?

504
00:30:25,670 --> 00:30:29,010
I Lutyens it'll never change.
Let's see.

505
00:30:30,240 --> 00:30:34,570
Outside the loop, since
it'll never change. MMM.

506
00:30:40,940 --> 00:30:45,710
Actually, actually, um, that's a
good point. That's a good point.

507
00:30:45,711 --> 00:30:50,570
And we could just do that if
we knew the size of, right.

508
00:30:50,571 --> 00:30:53,000
So we could actually write
that differently. You're
right. That's a good point.

509
00:30:53,001 --> 00:30:54,920
We could, we could write that
differently. It's still works,

510
00:30:54,921 --> 00:30:58,340
but we could've written that differently.
Right? It's just a null check. Exactly.

511
00:30:58,640 --> 00:31:02,550
Okay. So where was I? We want to iterate
through every neuron in the layer. Uh,

512
00:31:02,600 --> 00:31:07,560
and where was I? So length, um,

513
00:31:08,690 --> 00:31:12,450
and we've got iPad plots. Okay.

514
00:31:12,451 --> 00:31:15,810
So what we're doing re iterate through
every neuron in each of our layers.

515
00:31:16,170 --> 00:31:20,490
And we're going to do a double iteration.
So we're going to say four bars, j

516
00:31:22,140 --> 00:31:25,480
four j and this dot layers.
Uh,

517
00:31:25,710 --> 00:31:30,180
so for the length of the neuron and then
for each of the layers. Okay. So, um,

518
00:31:32,950 --> 00:31:37,830
so for this stop layers, uh, where
was I was at? This isn't layers. I,

519
00:31:38,210 --> 00:31:41,680
uh,
dot neurons then.

520
00:31:42,660 --> 00:31:45,300
Where was I? Okay, so,
so the first thing, okay,

521
00:31:45,301 --> 00:31:48,600
so let me talk about what we're
about to do. Well, before I do this,

522
00:31:48,601 --> 00:31:50,790
I need to initialize this
variable called some,

523
00:31:51,060 --> 00:31:52,950
and let me talk about
what some is going to do.

524
00:31:52,980 --> 00:31:54,630
So we're going to initialize
a variable called some.

525
00:31:54,930 --> 00:31:59,290
And with some is going to do is, uh,
that is what we're going to apply the,

526
00:31:59,300 --> 00:32:02,910
the activation. We're going to use
to create the activation function.

527
00:32:03,330 --> 00:32:07,380
So the sum is going to be if we
take the previous layers, neurons,

528
00:32:07,710 --> 00:32:11,390
if we take the previous layer neurons,
wherever we are with that, um,

529
00:32:12,860 --> 00:32:14,520
uh,
and then we take the value,

530
00:32:15,030 --> 00:32:19,420
we're going to multiply that by
this layer a weights. So that were,

531
00:32:19,421 --> 00:32:21,900
that were to the layer
that we're currently on.

532
00:32:22,140 --> 00:32:25,650
We're going to multiply it by the
weights. So this salt layers, uh,

533
00:32:25,740 --> 00:32:28,980
I uh,
doc neurons,

534
00:32:29,430 --> 00:32:32,100
it's not as good as gradient descent.
Um,

535
00:32:32,130 --> 00:32:36,590
I mean we haven't seen someone
do it as well as, you know,

536
00:32:36,600 --> 00:32:40,790
state of the art, a gradient descent with
backdrop. Uh, but I mean, think about it.

537
00:32:40,791 --> 00:32:44,300
I mean we, we've all been a in a,
in an evolutionary environment.

538
00:32:44,301 --> 00:32:47,900
So I think there's still
hope for uh, uh, uh,

539
00:32:47,990 --> 00:32:52,870
genetic programming in general. Okay.
So we're creating this, the sum,

540
00:32:52,880 --> 00:32:56,910
right? And so this psalm is what
we're going to apply to the, um,

541
00:32:58,310 --> 00:33:01,160
we're going to apply it to
the activation function.

542
00:33:01,161 --> 00:33:05,720
So I've created this loop and I'm
going to say for each of those layers,

543
00:33:08,790 --> 00:33:13,640
neurons, j. Dot. Value. I'm
going to say self duck options.

544
00:33:14,270 --> 00:33:16,110
Okay?
So here's the activation function step.

545
00:33:16,310 --> 00:33:19,670
So I take that activation function and
then the parameter is going to be the son

546
00:33:19,671 --> 00:33:21,560
that I've just calculated.
So what happened here,

547
00:33:21,561 --> 00:33:23,630
the activation function
is going to take the sum,

548
00:33:24,140 --> 00:33:28,670
which is the combined weight of the
neurons in this layer. And I'm going to,

549
00:33:28,880 --> 00:33:30,410
uh,
I'm going to,

550
00:33:31,240 --> 00:33:32,073
uh,

551
00:33:32,270 --> 00:33:34,220
run the activation function on that some,

552
00:33:34,221 --> 00:33:36,170
and it's going to convert that
number into a probability.

553
00:33:36,530 --> 00:33:39,830
And so the probability is going to help
the data decide where to flow. Okay?

554
00:33:39,831 --> 00:33:42,680
It's going to decide where to flow. And
finally, once I'm done with all of this,

555
00:33:42,681 --> 00:33:45,110
I can just say, well, now that
I've iterated through that layer,

556
00:33:45,350 --> 00:33:49,070
make sure that the previous layer is now
the current layer where I, where I work,

557
00:33:49,100 --> 00:33:53,930
right? Just was, okay.
Okay. So we've done that,

558
00:33:53,931 --> 00:33:55,910
that activation step.
Um,

559
00:33:57,510 --> 00:33:58,343
uh,

560
00:34:00,290 --> 00:34:03,750
right? Okay. So let's go ahead
and write our genome now.

561
00:34:03,800 --> 00:34:08,150
So our genome is just another word
for a being or a, you know, a,

562
00:34:08,151 --> 00:34:11,880
a living,
a living being inside of our generation.

563
00:34:12,120 --> 00:34:16,740
So each genome has its own neural
network, right? So if you or I,

564
00:34:16,741 --> 00:34:20,880
or genomes, and we have our own neural
network. So we'll call this genome. Okay.

565
00:34:20,881 --> 00:34:25,881
This function or this variable genome
and genome has a score and it has a,

566
00:34:28,400 --> 00:34:32,910
uh, previous layer. Dot Neurons.
Dot value. Is it, is it, let's see.

567
00:34:35,410 --> 00:34:36,250
Let's see.

568
00:34:37,090 --> 00:34:38,200
MMM,

569
00:34:39,590 --> 00:34:43,970
no. Okay. So, uh, for genome we're
going to say a score in the network.

570
00:34:43,971 --> 00:34:48,830
Those are our two printers. And
okay. So now it's time to, um,

571
00:34:50,960 --> 00:34:54,770
Jay does exist. Jay is a part of this
bar. J up here on line one 22. Okay.

572
00:34:54,771 --> 00:34:57,170
So for far genome, I want to say, okay.

573
00:34:57,171 --> 00:35:00,500
So it has a score and it
has its own network, right.

574
00:35:00,501 --> 00:35:03,560
And the score is like where it's at in
the game and that's how it's going to,

575
00:35:03,620 --> 00:35:07,520
that's how we are going to keep tracking
internally of how well it's doing and

576
00:35:07,521 --> 00:35:11,840
decide whether or not we want it to
breed for the next generation. Okay. Uh,

577
00:35:11,841 --> 00:35:15,470
so we know those are our two attributes
of score and it's network. Okay.

578
00:35:15,590 --> 00:35:19,590
So that's a genome. And then now it's
time for that highest level object.

579
00:35:19,880 --> 00:35:22,700
We're ready to code our generation.
And what is our generation?

580
00:35:23,000 --> 00:35:26,420
A generation consists of several genomes.
Okay?

581
00:35:26,570 --> 00:35:28,520
So at generation is just a list.

582
00:35:28,820 --> 00:35:33,290
We can think of it as a list of genomes
and that's the only parameter that we

583
00:35:33,291 --> 00:35:38,090
have. Uh, the number of genomes. Okay. Oh,

584
00:35:38,100 --> 00:35:42,870
good call. Don't capitalize genomes.
Genomes to deter. Okay, great.

585
00:35:43,290 --> 00:35:47,160
Uh, this stuff genomes and
it's an empty list, right?

586
00:35:47,161 --> 00:35:51,930
So it's going to be an empty list.
Awesome fitness functions for the win.

587
00:35:51,960 --> 00:35:53,130
Okay.
All right guys.

588
00:35:53,131 --> 00:35:57,030
So let's go ahead and create our
function for adding the genome to our a

589
00:35:57,180 --> 00:35:58,260
generation.
Okay.

590
00:35:58,350 --> 00:36:02,610
So add the genome at a
genome torque generation.

591
00:36:02,611 --> 00:36:04,470
So we need a function for this ad,

592
00:36:04,480 --> 00:36:07,740
a genome to current generation.

593
00:36:09,570 --> 00:36:13,290
Hey, let me just shut this
off for a second. Okay. Okay.

594
00:36:13,291 --> 00:36:18,060
So for our current generation, Hi
guys. Hi everybody. Um, okay, so

595
00:36:19,710 --> 00:36:21,690
let's go ahead and do this.
So for our generation,

596
00:36:21,691 --> 00:36:26,430
we want a prototype that point to be
called add genome. That's what this is.

597
00:36:26,940 --> 00:36:30,960
That's what this does. Okay. And we're
going to say, okay, so given a genome,

598
00:36:30,961 --> 00:36:33,750
which is our parameter,
we want to add it to this generation,

599
00:36:34,140 --> 00:36:36,500
that that's our task. So, um,

600
00:36:38,250 --> 00:36:39,670
we're going to say,
um,

601
00:36:43,090 --> 00:36:46,690
well we're going to say we're going
to iterate through a, wherever we are,

602
00:36:46,691 --> 00:36:51,070
and we're going to say,
uh, I is less than, uh,

603
00:36:51,670 --> 00:36:55,030
the length of the genome that
we're apps, which says, okay,

604
00:36:55,031 --> 00:36:58,180
so what we want to do
right now is we want to,

605
00:37:00,410 --> 00:37:04,810
uh, made sure that, uh, we are,

606
00:37:05,530 --> 00:37:08,200
uh, we, we have sorted it, right? So
that's the first thing we want to do.

607
00:37:08,201 --> 00:37:11,890
We want to sort it, and we don't want
to add a genome if it's not sortable.

608
00:37:11,891 --> 00:37:13,210
So we're going to create a check for that.

609
00:37:13,450 --> 00:37:16,150
We're going to iterate through the list
of genomes and we're going to check the

610
00:37:16,151 --> 00:37:20,590
score. And if the score is less
than zero, so if it's a bad score,

611
00:37:20,860 --> 00:37:24,590
then what we want to do is we, uh,
that's, that's the first check. Okay.

612
00:37:24,660 --> 00:37:28,720
And there are two interjects.
So if there's, if it's less
than zero, and if this,

613
00:37:28,840 --> 00:37:33,190
if the genome score is greater than what
we already have in our list of genomes

614
00:37:33,191 --> 00:37:33,650
or rapes,

615
00:37:33,650 --> 00:37:36,810
remember we have one in memory and then
we have one that we're looking at, uh,

616
00:37:36,940 --> 00:37:37,960
gets greater than that score.

617
00:37:37,960 --> 00:37:40,240
Then we just want to break because
it's not going to be a part,

618
00:37:40,370 --> 00:37:42,730
it's not going to be in order.
Okay. So we want to break.

619
00:37:42,970 --> 00:37:45,910
What's the other breaking? Uh,
what's the other edge case? There's,

620
00:37:45,970 --> 00:37:48,820
there's one more edge case that we
want to code. And that edge case is,

621
00:37:49,010 --> 00:37:54,010
let me back up a little bit and let
me say else if the genomes score

622
00:37:57,100 --> 00:38:00,490
is less than where we currently,
uh, what we currently have.

623
00:38:00,790 --> 00:38:04,960
If the genome score is less
than this genomes a score,

624
00:38:05,350 --> 00:38:08,740
then we also want to break.
So remember,

625
00:38:08,741 --> 00:38:12,820
we only want it to add it if it's
sorted and if it, and that this,

626
00:38:12,821 --> 00:38:15,580
this prevents us from adding
things out of order. Uh,

627
00:38:15,581 --> 00:38:18,280
and what do I mean by sorted?
We, we're sorting by the score.

628
00:38:18,600 --> 00:38:21,340
We have a list of genomes and
they are sorted by their score.

629
00:38:21,341 --> 00:38:26,170
So now we want to add our genome.
Okay. So this stuck genomes,

630
00:38:26,830 --> 00:38:30,970
that splice. Okay. And
splice is the, uh, uh,

631
00:38:31,000 --> 00:38:34,830
the verb that we're using to
add the genome tool to our, uh,

632
00:38:35,980 --> 00:38:40,030
are our array of our list of genomes.
Okay.

633
00:38:40,090 --> 00:38:44,590
So now let's write our breeding
function. Okay? So we've, we've,

634
00:38:44,680 --> 00:38:49,060
once we've ticked the genome that we want,
the next step is to breed them.

635
00:38:49,061 --> 00:38:51,880
So let's, let's say, okay,
it's time to breathe.

636
00:38:52,270 --> 00:38:57,010
Time to breed that the down
time to breathe. Okay? So,

637
00:38:57,011 --> 00:39:01,610
um, so that's what we're
going to do. And this is our,

638
00:39:01,970 --> 00:39:04,430
uh, breeding function.
This is our last, uh,

639
00:39:04,670 --> 00:39:08,000
major function that we're going to
write. Okay? So time's a breed. Ah,

640
00:39:08,030 --> 00:39:11,720
let's go ahead and write this. Write out
this function. Okay. Stick with me guys.

641
00:39:11,721 --> 00:39:14,030
We've got a, we're almost done. Okay.

642
00:39:14,031 --> 00:39:17,440
So we're going to create a breeding,
uh,

643
00:39:18,860 --> 00:39:20,330
uh, function. Okay.

644
00:39:20,510 --> 00:39:22,790
And what does a breeding function
going to take as his cranberry? Well,

645
00:39:22,800 --> 00:39:24,950
it's going to take two sets of weights,
okay.

646
00:39:24,951 --> 00:39:28,490
The set of weights from one parent and
a set of weights from the other parents.

647
00:39:29,030 --> 00:39:32,660
Um, okay, so we've probably got
about 10 more minutes. Okay.

648
00:39:32,661 --> 00:39:37,520
So everybody relax, sit back, enjoy
the ride. Okay, so we're going to have,

649
00:39:37,910 --> 00:39:40,250
um, and a number of children, right?

650
00:39:40,280 --> 00:39:43,730
So these are our two parents and
in a number of children, okay?

651
00:39:44,750 --> 00:39:48,350
That's what we're going to do. A Pan.
So we're going to do breed these two.

652
00:39:48,351 --> 00:39:51,290
So remember, these are two
good networks, okay? We've,

653
00:39:51,291 --> 00:39:54,950
we've identified them as good networks
because the scores that they provide are

654
00:39:55,010 --> 00:39:57,920
over a certain threshold
that we predefined, okay?

655
00:39:57,921 --> 00:40:00,350
So what we're gonna do is we're going to
iterate through the number of children,

656
00:40:00,380 --> 00:40:04,160
okay? However many children we want
our at least parents to have, okay?

657
00:40:04,161 --> 00:40:06,230
So we're going to say NB zero.

658
00:40:06,590 --> 00:40:08,990
As long as it's less than a number
of children that you've defined,

659
00:40:09,020 --> 00:40:12,830
we want to iterate through all of them
and we're going to of course iterate

660
00:40:12,831 --> 00:40:14,510
through each of them.
All right?

661
00:40:16,310 --> 00:40:19,850
So the first thing we want to do
is create our, uh, data variable.

662
00:40:19,851 --> 00:40:24,260
And what does our data variable, dude,
well, we take, we use the Builtin, uh,

663
00:40:24,770 --> 00:40:27,390
uh, Jason Function, uh,

664
00:40:27,420 --> 00:40:30,450
Jason Builtin parts function to,
uh,

665
00:40:31,100 --> 00:40:34,840
get the list of weights from
the first parent. And if so,

666
00:40:34,980 --> 00:40:38,030
the weights are going to be just
a huge mishmash of numbers, right?

667
00:40:38,240 --> 00:40:41,240
These are just a huge list of numbers.
It's a matrix of numbers,

668
00:40:41,480 --> 00:40:46,040
but we want it to be machine
readable for readable. Right? Um,

669
00:40:46,100 --> 00:40:50,720
and so if we want it to be readable,
then we're going to,

670
00:40:51,170 --> 00:40:53,710
um, you know what? Uh, so he attached,

671
00:40:53,720 --> 00:40:57,200
it seems complicated and I'm going to
get better and making it more accessible.

672
00:40:57,201 --> 00:40:58,034
But you know,

673
00:40:58,090 --> 00:41:01,340
it all depends on the syntax and this
could definitely be less complicated and

674
00:41:01,341 --> 00:41:04,160
it will get less complicated over
time as I do more live streams.

675
00:41:04,370 --> 00:41:08,300
So don't give up hope. Okay. Uh, and this
is definitely gonna get easier over time.

676
00:41:08,510 --> 00:41:12,680
So Jason. Dot. Fi and what do
we string string applying? Um,

677
00:41:14,530 --> 00:41:19,050
this is indeed a neuro evolution.
Uh, okay, so we're gonna.

678
00:41:19,480 --> 00:41:22,000
So we're going to string a Fye the first
wait so that it's readable and we're

679
00:41:22,001 --> 00:41:23,980
going to store it in a data variable.
Okay?

680
00:41:23,981 --> 00:41:28,960
So now it's time for us to say,
ah, let's do a little bit of, of,

681
00:41:28,961 --> 00:41:30,700
uh,
of mutations.

682
00:41:30,701 --> 00:41:34,060
So we're going to take the first parent
and we're going to iterate through, uh,

683
00:41:34,160 --> 00:41:37,700
every single list of weights
in our networks. Okay?

684
00:41:40,480 --> 00:41:45,450
MMM. All right. So let's
go ahead and say, Gee,

685
00:41:45,500 --> 00:41:48,290
not network a ways.
By the way,

686
00:41:48,291 --> 00:41:51,500
you guys should next time for the next
live stream. I didn't tell you this time,

687
00:41:51,501 --> 00:41:55,280
but you should totally be a coding along
with me. Okay? So maybe not this time,

688
00:41:55,281 --> 00:41:58,580
but next time I want you all
to be coding along with me, uh,

689
00:41:58,610 --> 00:42:02,660
in future livestreams. Okay. Uh, but
for right now, just, just, just, uh,

690
00:42:02,690 --> 00:42:03,260
check this out.

691
00:42:03,260 --> 00:42:06,410
So we're going to iterate through all
of the weights that we have. Okay.

692
00:42:06,411 --> 00:42:09,140
We're going to iterate through every
single one of those weights and we want to

693
00:42:09,170 --> 00:42:13,130
now perform some mutation, right? So in
order to mutate, we want to say, okay,

694
00:42:13,131 --> 00:42:17,540
well let's arbitrarily define
some threshold. So we'll rent,

695
00:42:17,541 --> 00:42:22,180
we'll randomly generates a
number using the math dot.
Random function. Okay, we'll,

696
00:42:22,181 --> 00:42:25,520
we'll randomly generate that and we'll
say, well, if it's less than 0.5,

697
00:42:25,790 --> 00:42:30,200
then we want to update our,
uh, our weights from our,

698
00:42:30,230 --> 00:42:32,060
that we sorted our data variable,
right?

699
00:42:32,210 --> 00:42:36,170
And the data variable is just a temporary
variable that's going to store our

700
00:42:36,171 --> 00:42:38,840
weights and rep, right? So if it's,

701
00:42:38,990 --> 00:42:43,400
if it's rent were arbitrarily and
randomly going to update those weights.

702
00:42:43,810 --> 00:42:44,643
Um,

703
00:42:46,460 --> 00:42:48,920
now we've created teacher.network
and we're going to,

704
00:42:48,980 --> 00:42:52,370
we're going to add all this weight from
Jeetu. Okay, blah, blah, blah, blah, blah.

705
00:42:52,580 --> 00:42:57,040
So there's that. And so now we're
going to add some mutation to each way.

706
00:42:57,041 --> 00:42:58,790
And so now here's the, here's
the, here's the fun part.

707
00:42:59,090 --> 00:43:01,760
We're going to perform
to mutation. And, um,

708
00:43:03,730 --> 00:43:07,110
so, uh, we've, we've mutated.
So what's happened? Uh,

709
00:43:07,450 --> 00:43:12,220
we've mutated the weights in
the temporary data variable, uh,

710
00:43:12,250 --> 00:43:15,460
using one of the parents, the
weight of one of the parents, g two,

711
00:43:15,910 --> 00:43:20,170
and now we want to update the weights.
Um,

712
00:43:21,970 --> 00:43:26,260
given, uh, this other, so they're
here. Here's our other arbitrarily.

713
00:43:30,220 --> 00:43:34,090
Okay. So a note it. So I need to
heavily, heavily comics the code.

714
00:43:35,040 --> 00:43:38,490
Okay. So math dot random, uh,

715
00:43:39,580 --> 00:43:42,590
less than or equal to. So,
okay, so what do we have here?

716
00:43:42,820 --> 00:43:46,690
So one of our hyper parameters
was called the mutation rates. Um,

717
00:43:47,020 --> 00:43:50,110
the mutation rate defined like
how fast do we want to mutate?

718
00:43:50,290 --> 00:43:53,680
So that's the kind of the arbitrary think
are arbitrary number we're looking at.

719
00:43:54,020 --> 00:43:57,040
Um,
but we basically want to say like,

720
00:43:57,041 --> 00:44:02,041
take those weights and update
them by adding a whatever,

721
00:44:02,650 --> 00:44:03,430
random,

722
00:44:03,430 --> 00:44:08,430
a random variable times
whatever our mutation ranges.

723
00:44:08,990 --> 00:44:11,710
Um, right. So like that's going to be,

724
00:44:11,770 --> 00:44:14,620
and we can tune that differently and
that's going to give us different results,

725
00:44:14,680 --> 00:44:18,730
like for the mutation rate. Uh, and
then we want you subtract the range.

726
00:44:18,910 --> 00:44:22,960
So we want it to be within
a certain range. Okay.

727
00:44:24,480 --> 00:44:25,830
But we've added some mutation.

728
00:44:25,831 --> 00:44:30,600
And now finally we can go ahead and
say we've added our mutation, go ahead.

729
00:44:30,601 --> 00:44:35,160
And what we've just created a list of
mutated weights toward data is variable.

730
00:44:35,370 --> 00:44:38,590
And at the very end, which you
go ahead and return. So let me,

731
00:44:38,610 --> 00:44:42,570
let me write a comment as well.
Return the list of Brita genomes.

732
00:44:42,571 --> 00:44:46,930
These are the genomes that have been
bred. So these are the updated, uh, uh,

733
00:44:47,010 --> 00:44:50,310
more advanced, stronger,
more robust neural networks,

734
00:44:50,490 --> 00:44:52,560
and that's just going
to keep going into our,

735
00:44:52,980 --> 00:44:55,890
each new generation and they're
going to get better over time. Okay,

736
00:44:56,160 --> 00:45:00,750
so let's have Brita genomes
and we want to return to that.

737
00:45:01,530 --> 00:45:05,910
Okay? So that's the, that's the winner.
That's the code that we're going to do.

738
00:45:06,460 --> 00:45:07,110
Let me go,
go,

739
00:45:07,110 --> 00:45:09,870
go all the way up and I'm going to do a
short walk through what I've just done.

740
00:45:10,290 --> 00:45:13,300
Um, and so let me, uh,

741
00:45:13,500 --> 00:45:15,540
so remember this is the
link to the demo code.

742
00:45:15,541 --> 00:45:18,150
If you guys want to see it on the web.
I showed at the beginning,

743
00:45:18,151 --> 00:45:22,050
and let me show it again, uh, for a
second. It is the game of asteroids.

744
00:45:22,051 --> 00:45:24,440
It looks like this,
right?

745
00:45:24,450 --> 00:45:27,810
So many of them are generated many little
spaceships and they're all trying out

746
00:45:27,811 --> 00:45:30,420
different things every time.
And you can see them all dying.

747
00:45:30,630 --> 00:45:33,750
And once they're all dead, new ones will
spring up. Okay. So it's just like that.

748
00:45:34,470 --> 00:45:36,180
And so let me explain
what's happening here. Okay.

749
00:45:36,181 --> 00:45:40,080
So we're using something called neuro
evolution, neuro evolutions at technique,

750
00:45:40,081 --> 00:45:43,380
okay? And this is, so here's,
here's how we starting off. Okay.

751
00:45:44,700 --> 00:45:46,950
We want to create our
neuro evolution variable.

752
00:45:46,951 --> 00:45:49,320
Then our lists of neurons are layers.
Our network,

753
00:45:49,321 --> 00:45:52,750
our genome and our generation's. It
gets bigger and bigger. Each member,

754
00:45:53,070 --> 00:45:54,810
each object gets bigger and bigger.

755
00:45:55,050 --> 00:45:58,590
So it's a type of reinforcement learning
because we are learning in real time

756
00:45:58,591 --> 00:46:01,050
from what the game is giving being us.
Okay?

757
00:46:01,051 --> 00:46:04,950
So we will initialize our neural
evolution, uh, variable, well, well,

758
00:46:05,010 --> 00:46:09,300
all of our hyper parameters that are
going into a dark going to signify of how

759
00:46:09,301 --> 00:46:13,470
we're going to generate these, uh,
uh, new neural networks. So remember,

760
00:46:13,471 --> 00:46:17,070
every spaceship has its own neural
network, will define our set of options.

761
00:46:17,071 --> 00:46:20,430
And we'll start out by initializing
our net neuron, our neuron variable.

762
00:46:20,700 --> 00:46:24,090
Each neuron has its value
and a set of weights. Okay?

763
00:46:24,180 --> 00:46:26,820
And we went to randomly initialized
each of those weights for each of those

764
00:46:26,821 --> 00:46:27,720
neurons.
Okay?

765
00:46:27,721 --> 00:46:31,230
And once we've created a neuron will
create a layer and a layer has an ID and a

766
00:46:31,231 --> 00:46:33,150
number of neurons,
which started off as zero,

767
00:46:33,360 --> 00:46:37,810
but then we'll populate the layer
by adding every neuron, uh, uh,

768
00:46:37,860 --> 00:46:42,060
by however many would define like number
of neurons. Then we'll create a network.

769
00:46:42,061 --> 00:46:45,630
And a network consists of a number of
layers, which we just defined previously,

770
00:46:45,631 --> 00:46:47,070
which contains a number of neurons.

771
00:46:47,490 --> 00:46:51,900
And we're going to generate that layer
by first generating the, and let me,

772
00:46:51,930 --> 00:46:56,430
let me, let me add a comment here.
This is, um, uh, input layer, right?

773
00:46:56,460 --> 00:46:57,390
The input layer.

774
00:46:57,630 --> 00:47:02,630
Then we create our hidden layers and then
our output layer by using those three

775
00:47:02,851 --> 00:47:05,880
parameters that were given
input hiddens and outputs. Okay.

776
00:47:05,881 --> 00:47:07,320
And then we have a computation step.

777
00:47:07,321 --> 00:47:11,610
This is where we actually
apply the activation function
in each neuron of every

778
00:47:11,611 --> 00:47:15,480
layer in our network, we
turn those numbers into
probabilities of the score of,

779
00:47:15,880 --> 00:47:20,250
and the weights of, of, uh, of,
of, of the, of each of the bots.

780
00:47:20,520 --> 00:47:23,490
And each of those is a genome, right?
That's how we define them. As each,

781
00:47:23,491 --> 00:47:28,350
as a genome and a generation contains a
selection of genomes and we want to add

782
00:47:28,380 --> 00:47:29,640
a genome to the current network.

783
00:47:29,650 --> 00:47:33,220
I'm making sure that it's sorted in order
and you will use a splice function to

784
00:47:33,221 --> 00:47:34,780
add it.
And lastly,

785
00:47:34,781 --> 00:47:39,781
we'll breed our network by taking
the weights of one of our parents,

786
00:47:41,320 --> 00:47:42,790
uh,
mutating it,

787
00:47:43,060 --> 00:47:47,290
and then creating an entirely new set
of weights in the state of variable, uh,

788
00:47:47,291 --> 00:47:48,610
by mutating the other one.

789
00:47:48,880 --> 00:47:52,600
And then we'll return the list of those
weights and what we can use those ways

790
00:47:52,601 --> 00:47:57,100
to up update the next generation
of, uh, of neurons. Okay.

791
00:47:57,220 --> 00:48:02,050
So now let me end the screen
sharing. Okay. All right.

792
00:48:02,051 --> 00:48:04,240
So we'll do a last minute,
uh,

793
00:48:04,270 --> 00:48:09,130
last Q and a and then we're done
for the day. All right, so any,

794
00:48:09,131 --> 00:48:12,890
any last, last questions? Go
for it. Ask me anything. Um,

795
00:48:13,330 --> 00:48:15,460
why does it need to be thwarted?
And he said it's so, it's,

796
00:48:15,461 --> 00:48:19,150
it's better for computational complexity.
If it's foreign it, then we can, uh, it's,

797
00:48:19,160 --> 00:48:24,160
it's going to be a retrieval is going
to be faster retrieval for this force.

798
00:48:24,710 --> 00:48:27,340
Um, so that's just for
computational complexity.

799
00:48:27,490 --> 00:48:30,910
Does it make sense a change in network
and put what genetic algorithm? Uh, yes,

800
00:48:30,911 --> 00:48:31,780
absolutely.
Um,

801
00:48:32,050 --> 00:48:35,680
well the network input is already going
to change because the Games are dynamic.

802
00:48:36,130 --> 00:48:39,310
Um, uh, right. So there's that.
What screen sharing app do you use?

803
00:48:39,311 --> 00:48:42,760
I use Google hangouts. I guess you owe
us a wrap now. You're absolutely right.

804
00:48:42,940 --> 00:48:46,380
I mean decide what to rap about. Someone
said something about care Os. Uh, Yo,

805
00:48:46,400 --> 00:48:48,730
I'm going to do this without
it being okay. Here we go. Um,

806
00:48:50,560 --> 00:48:55,450
I was trying out care Roth man. My life
is so lost. I'm sitting here in Portland,

807
00:48:55,600 --> 00:48:59,710
man, what is this? It's
like, I don't want to curse,

808
00:48:59,711 --> 00:49:02,710
but horror horrorlands not really.
And things are going wrong.

809
00:49:02,740 --> 00:49:06,730
My mind is so loose that
I'm going all around. Okay.

810
00:49:06,940 --> 00:49:10,180
So that was that.
Wow Man.

811
00:49:10,300 --> 00:49:14,200
Got that Portland jam happening
right now it's on point.

812
00:49:14,260 --> 00:49:16,600
It is 10:00 AM on a, on
a Wednesday. Everybody,

813
00:49:16,630 --> 00:49:20,760
it is 10:00 AM on a Wednesday for me.
So that's, that's what happened. Um,

814
00:49:21,730 --> 00:49:26,560
uh, so that was my impromptu rep
and how am I, I'm doing good.

815
00:49:26,561 --> 00:49:31,510
Am I going to test it? I'm
going to add the code to the,
uh, link description. Okay.

816
00:49:31,511 --> 00:49:35,470
So remember to check it out. Um, that just
happened Brian. You're absolutely right.

817
00:49:36,490 --> 00:49:38,750
That just happened. It is a place. Um,

818
00:49:38,780 --> 00:49:43,780
I'm going to shoot an
experimental video here and cool.

819
00:49:43,930 --> 00:49:45,280
One more question and we're good to go.

820
00:49:45,310 --> 00:49:49,180
Is there an easy way to save the data
that's created by the network, um,

821
00:49:49,600 --> 00:49:53,970
in order to implement, in order
to implement it? Uh, yeah, no,

822
00:49:53,980 --> 00:49:57,490
the data is saved and the data is variable
and it's just a list of weights and

823
00:49:57,491 --> 00:50:02,470
we can, um, we can use those weights to
update other types of networks as well.

824
00:50:02,650 --> 00:50:05,590
And that's that, that would be a
part of transfer learning. Okay.

825
00:50:05,770 --> 00:50:08,620
So we could learn from one game and then
ideally applied those weights to any

826
00:50:08,621 --> 00:50:12,520
game. And that's what deepmind did
for the Atari DQ learner. Okay.

827
00:50:13,080 --> 00:50:16,910
Um, do you have a job? No,

828
00:50:17,260 --> 00:50:20,420
this is my job data set of
all reddit comments available.
Anything in the future?

829
00:50:20,421 --> 00:50:24,320
I'm sure I'm going to do something with
a red, uh, comments. Uh, I'm not, uh,

830
00:50:24,350 --> 00:50:26,960
in India. My parents are from
India. I was born in Houston, Texas,

831
00:50:27,200 --> 00:50:30,380
and now I live in San Francisco,
California. I'm new to the channel.

832
00:50:30,381 --> 00:50:35,040
What languages do you use?
Mostly Python and javascript and ah,

833
00:50:35,480 --> 00:50:40,480
cool. Okay guys. Uh, uh, Tara
didn't use a neuro evolution. Uh,

834
00:50:40,610 --> 00:50:44,990
they use something called the DQ learner.
Okay. Uh, right now I've got to go.

835
00:50:45,250 --> 00:50:49,250
Um, did I dye my hair? Yes.
What's new for 2017? Uh,

836
00:50:50,060 --> 00:50:54,330
my opus magnum opus, magnum, uh, my
beautiful dark, twisted fantasy, uh,

837
00:50:54,680 --> 00:50:57,620
which is this new machine learning course,
which I'm not going to say much about,

838
00:50:57,621 --> 00:51:02,180
but just know that something big is
coming guys. Okay. Um, cool. So yeah,

839
00:51:02,181 --> 00:51:06,250
that's it for this livestream. Um, uh,

840
00:51:06,280 --> 00:51:09,190
thank you guys so much for showing up.
I'll add the coaching, the comments.

841
00:51:09,540 --> 00:51:13,570
I love all of you. Thanks for showing up.
Uh, during Christmas. I know, you know,

842
00:51:13,571 --> 00:51:18,340
you have a, a lot of things that can be
doing. So, uh, for now I've got to go,

843
00:51:18,670 --> 00:51:23,470
uh, Heiko waterfall, so thanks
for watching. I love you guys.

