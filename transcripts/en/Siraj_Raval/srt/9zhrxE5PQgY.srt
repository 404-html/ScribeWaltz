1
00:00:00,090 --> 00:00:01,500
Hello world,
it's Saroj.

2
00:00:01,501 --> 00:00:05,490
And our task today is to
build a recurrent network,

3
00:00:05,730 --> 00:00:10,730
a type of recurrent network called an
Lstm or long short term memory network to

4
00:00:11,731 --> 00:00:14,580
generate m and m lyrics,
that star task.

5
00:00:14,760 --> 00:00:18,780
And we're going to build this without
using any libraries, just num Pi,

6
00:00:18,960 --> 00:00:23,880
which is for Matrix math because we
want to learn about the math behind LSTM

7
00:00:23,881 --> 00:00:24,714
networks.

8
00:00:25,230 --> 00:00:29,460
We've talked about recurrent networks
earlier on in the series and the LSTs are

9
00:00:29,461 --> 00:00:34,461
the next logical step in that
progression of neural network learnings.

10
00:00:34,800 --> 00:00:37,110
So that's where we're going to do today.
And um,

11
00:00:38,010 --> 00:00:39,520
we're gonna first talk
about recurrent networks.

12
00:00:39,540 --> 00:00:42,480
We're going to do a little refresher
on what recurrent networks are,

13
00:00:42,510 --> 00:00:43,770
how we can improve them,

14
00:00:43,890 --> 00:00:48,090
and then we'll talk about LSTM networks
and how they are improvements and the

15
00:00:48,091 --> 00:00:52,030
mathematics behind them.
Then we'll build all of this build as a,

16
00:00:52,031 --> 00:00:53,280
we're going to look,
we're going to,

17
00:00:53,340 --> 00:00:56,550
I'm going to explain the
code cause there's quite a
lot of code to go over here.

18
00:00:56,730 --> 00:00:58,500
There's a lot of code.
Um,

19
00:00:58,560 --> 00:01:02,550
but we're going to look at all the code
and then I'm going to code out manually

20
00:01:02,700 --> 00:01:07,470
the forward propagation parts for both
the greater recurrent network and then

21
00:01:07,471 --> 00:01:09,120
the LSTM cell itself.

22
00:01:09,330 --> 00:01:14,330
So strap on with every math hat you have
because this is going to be so hard.

23
00:01:14,610 --> 00:01:18,180
We might as well just give up. Let's just
forget this. No, no, I'm just kidding. No,

24
00:01:18,181 --> 00:01:20,160
this is actually going to be,
there's going to be pretty easy.

25
00:01:20,161 --> 00:01:22,710
This is going to be pretty easy.
If you got the recurring that video,

26
00:01:22,711 --> 00:01:26,160
this is going to be pretty easy stuff,
although it's a little more complicated,

27
00:01:26,640 --> 00:01:28,950
but you will get this. Okay.
Get, get ready for this.

28
00:01:28,951 --> 00:01:31,980
I'm going to make sure you get
this. Okay. So here, here we go.

29
00:01:33,240 --> 00:01:34,470
What is recurrent network?

30
00:01:34,530 --> 00:01:36,780
Can you tell me what a recurrent
network is in one sentence?

31
00:01:36,840 --> 00:01:39,240
I'll give you 10 seconds.
Go.

32
00:01:42,210 --> 00:01:43,260
Okay.
That's all the time you get.

33
00:01:43,261 --> 00:01:45,660
I'm not gonna wait a full 10 seconds
for current networks are cool.

34
00:01:45,840 --> 00:01:48,800
They're useful for learning sequential
data. We know this, we know this.

35
00:01:48,810 --> 00:01:53,810
They're useful for learning sequential
data as series of video frames text.

36
00:01:54,420 --> 00:01:57,630
Um, music, anything that
is a sequence of data,

37
00:01:57,660 --> 00:02:02,040
that's where recurrent networks do really
well. That's what they're made for.

38
00:02:02,100 --> 00:02:04,950
They're very simple. You have
your input, your input data,

39
00:02:05,130 --> 00:02:07,530
and then you have your hidden state
and then you haven't helped put.

40
00:02:07,740 --> 00:02:12,000
And so the different between recurrent
nets and feed forward nets are that

41
00:02:12,001 --> 00:02:13,800
recurrent nets have
something different here.

42
00:02:13,860 --> 00:02:16,350
So in a normal feed forward and
that we would have our input,

43
00:02:16,560 --> 00:02:19,950
our hidden layer and then our
output layer and that's it.

44
00:02:19,951 --> 00:02:24,180
And then we would have to wait matrices
between each of these layers that are

45
00:02:24,610 --> 00:02:27,570
the, those are matrices,
right? That we multiply right?

46
00:02:27,600 --> 00:02:31,560
Input Times weight at a bias activate.

47
00:02:31,590 --> 00:02:36,090
Hopefully you said activate because
that is a rap slash mnemonic device.

48
00:02:36,270 --> 00:02:39,990
But input times wait at a bias activate
repeat over and over for feedforward

49
00:02:39,991 --> 00:02:40,361
networks.

50
00:02:40,361 --> 00:02:44,550
A difference though for recurrent networks
is that we add another weight matrix

51
00:02:44,700 --> 00:02:49,290
and this other weight matrix called
synapse h in this diagram connects the

52
00:02:49,291 --> 00:02:53,310
hidden state back to itself.
So it's just a third weight matrix.

53
00:02:53,460 --> 00:02:57,000
And what is the reason we
add that is so whenever we,

54
00:02:57,180 --> 00:03:01,990
whenever we are training our network,
we're not just feeding in new data points.

55
00:03:02,020 --> 00:03:02,860
So if the data's,

56
00:03:02,930 --> 00:03:06,550
so if we're trying to train our
current net to remember the next uh,

57
00:03:06,580 --> 00:03:11,580
to learn to predict the next number in
a series and the series is one through

58
00:03:12,401 --> 00:03:17,260
10, right? We want to predict the
10 we would say, okay, one, two,

59
00:03:17,261 --> 00:03:20,410
three, four, five, six, seven,
eight, nine, 10. We wouldn't
just give it the numbers.

60
00:03:20,411 --> 00:03:23,890
We would also feed in the hidden
state, right? So we would say, okay,

61
00:03:23,891 --> 00:03:28,030
so given a one, predict the two. Okay, now
given the one and to predict the three,

62
00:03:28,031 --> 00:03:30,280
okay, give him the one, two
and three, predict the four.

63
00:03:30,281 --> 00:03:32,020
And that's how we would
train every iteration.

64
00:03:32,140 --> 00:03:35,290
But that's not all we would give it.
We wouldn't just give it the input data.

65
00:03:35,320 --> 00:03:38,380
We would also give it the hidden state.
We would give it both.

66
00:03:38,500 --> 00:03:42,010
And so that's why we have a recurrent
matrix that's connecting the hidden state

67
00:03:42,160 --> 00:03:46,180
to itself because we are not just giving
it, giving it every new data point,

68
00:03:46,390 --> 00:03:49,900
the ones and the twos and threes.
We're also giving it the hidden state,

69
00:03:49,901 --> 00:03:50,800
which is basically a,

70
00:03:50,830 --> 00:03:55,830
which is a matrix and we're multiplying
it by the hidden state and the new input

71
00:03:56,651 --> 00:03:57,580
data we are,

72
00:03:57,640 --> 00:04:02,200
we are feeding in the hidden state
and the input data at every time step.

73
00:04:02,530 --> 00:04:04,120
And so that's why we have a hidden matrix.

74
00:04:05,120 --> 00:04:09,040
And so you could think of this as own
rolled as just a series of feedforward

75
00:04:09,041 --> 00:04:09,850
networks,
right?

76
00:04:09,850 --> 00:04:14,080
Or we give it an an an input data
point and then we get a hidden states.

77
00:04:14,290 --> 00:04:18,730
And then in the next time step, we don't
just give it this new data point x one,

78
00:04:18,850 --> 00:04:22,090
but we also give it the, the hidden
state from the previous time step.

79
00:04:22,270 --> 00:04:23,740
So we just continued to do that.
So that's,

80
00:04:23,830 --> 00:04:27,430
that's one way of looking at our
current network as just a big chain of

81
00:04:27,431 --> 00:04:31,450
feedforward networks. A chain, right?
A block chain. No, not blockchain.

82
00:04:31,480 --> 00:04:34,480
Although I do want to talk about
blockchain, it's coming. Don't worry,

83
00:04:34,930 --> 00:04:36,700
I'm coming for you.
Blockchain, but not yet.

84
00:04:36,701 --> 00:04:40,450
We're still talking about recurrent
networks. Amazing stuff. Amazing stuff.

85
00:04:40,690 --> 00:04:42,850
Blockchains coming though. Oh, I
don't want to give away too much.

86
00:04:42,850 --> 00:04:46,480
I just did anyway. Ah, okay. Uh,

87
00:04:46,481 --> 00:04:48,820
you guys can tell I'm very
excited about blockchain anyway.

88
00:04:50,470 --> 00:04:54,550
So we give it the hidden state and the
input at every time step and we just keep

89
00:04:54,551 --> 00:04:58,030
repeating that. And that is a recurrent
network. But there is a problem here.

90
00:04:58,300 --> 00:05:01,800
The problem is that,
uh,

91
00:05:01,840 --> 00:05:06,130
there are 99 of these problems and no, I'm
just kidding. The problem is that when,

92
00:05:06,131 --> 00:05:07,420
when, whenever, oh, so here's the problem.

93
00:05:07,450 --> 00:05:09,190
The problem is actually
really interesting.

94
00:05:09,420 --> 00:05:11,320
It's called the vanishing
gradient problem. Okay.

95
00:05:11,321 --> 00:05:14,410
So it's called the vanishing gradient
problem. So let's say you're, you're like,

96
00:05:14,411 --> 00:05:15,070
what is this?

97
00:05:15,070 --> 00:05:18,770
So let's say we are trying to predict
the next word and a sequence of texts,

98
00:05:18,771 --> 00:05:21,250
which is actually what we are
trying to do right now. Duh.

99
00:05:21,790 --> 00:05:23,320
But let's say that's
what we're trying to do.

100
00:05:23,350 --> 00:05:26,350
And let's say we're trying to predict
the last word in this sentence.

101
00:05:26,350 --> 00:05:30,860
The grass is green, right?
So all were given. So we know
that the word is you know,

102
00:05:30,880 --> 00:05:32,740
green,
but let's say we're trying to predict it.

103
00:05:32,741 --> 00:05:36,370
So all we're giving is given is the
grass is and we're trying to predict that

104
00:05:36,371 --> 00:05:39,680
last word. Recurrent networks
can do this, right? They,

105
00:05:39,681 --> 00:05:43,210
they can easily do this
because the distance between
what the word we're trying

106
00:05:43,211 --> 00:05:47,470
to predict the distance between that data
point in the sequence and the previous

107
00:05:47,471 --> 00:05:50,050
data points is it's pretty small.
You write the different,

108
00:05:50,051 --> 00:05:53,590
the distance between grass,
which is the context we need.

109
00:05:53,740 --> 00:05:57,830
And Green is pretty small.
The only difference is that
the word is, is between,

110
00:05:57,920 --> 00:05:59,540
is between those two words.

111
00:06:00,590 --> 00:06:03,860
And so that's easy to predict because um,

112
00:06:04,460 --> 00:06:06,650
that's the only context that we need.

113
00:06:06,770 --> 00:06:10,020
Let's say it had some tendencies
before that said, you know, the,

114
00:06:10,080 --> 00:06:12,290
the law in his reign or the grass is,

115
00:06:12,470 --> 00:06:14,090
let's say you already
said the grass is greener.

116
00:06:14,090 --> 00:06:17,440
So it's already like the previous
sentence. So it already made that,

117
00:06:17,441 --> 00:06:20,570
that distinct connection
between grass and green.

118
00:06:20,571 --> 00:06:22,100
And you know those two things are related.

119
00:06:22,250 --> 00:06:26,270
So of course it's very easy to do that
but it, but let's also say, and you,

120
00:06:26,700 --> 00:06:31,700
but let's also say that we want to
predict the next word in this sentence.

121
00:06:32,061 --> 00:06:35,130
So let's say we have a huge paragraph.
Let's not even a parent.

122
00:06:35,150 --> 00:06:39,740
Let's say we have a 30 page essay
and the beginning of the essay,

123
00:06:39,741 --> 00:06:44,330
it is a first person essay and it's about
a guy named John. Okay is French dude.

124
00:06:44,331 --> 00:06:46,430
He's got a mustache,
unnecessary detail,

125
00:06:46,431 --> 00:06:51,431
but a guy named John and it starts
off with I am French as it should be,

126
00:06:52,700 --> 00:06:57,230
I am French and then there's 2000 plus
other words and we're trying to predict

127
00:06:57,400 --> 00:06:59,420
the,
the last sentence,

128
00:06:59,450 --> 00:07:02,840
the last word after 2000 words
and that is the word French.

129
00:07:03,080 --> 00:07:04,970
I speak fluent what?

130
00:07:05,240 --> 00:07:08,810
And now let's say between I am
French and I speak fluent French.

131
00:07:09,020 --> 00:07:14,020
There's all these other languages like
and so this guy is Spanish and I heard

132
00:07:14,781 --> 00:07:19,160
some German on the subway and all of
that is actually irrelevant to what he

133
00:07:19,161 --> 00:07:22,730
speaks fluently, which is French.
We've got to know that context.

134
00:07:22,731 --> 00:07:27,620
We got to know that he is French. So of
course he's gonna speak fluent French.

135
00:07:27,770 --> 00:07:32,750
So we've got to find a way for our
network to be able to remember longterm

136
00:07:32,780 --> 00:07:37,370
dependencies. And so the whole
idea behind recurrent networks,

137
00:07:37,371 --> 00:07:41,300
the whole reason that we feed in the
hidden state from the previous time step

138
00:07:41,510 --> 00:07:46,370
for every new iteration it so that we
can have a form of neural memory, right?

139
00:07:46,400 --> 00:07:49,610
That's why we don't just feed into the,
the previous input,

140
00:07:49,880 --> 00:07:54,290
but we feed it in the previous hidden
state because the hidden state is that

141
00:07:54,291 --> 00:07:56,690
matrix that represents the learnings,

142
00:07:57,230 --> 00:08:02,060
the learnings of the network so that we
can give it a form of memory like what

143
00:08:02,061 --> 00:08:05,660
is remember before and the
new data points. But the
problem and you would think,

144
00:08:05,661 --> 00:08:07,010
okay, so that's all you need, right?

145
00:08:07,011 --> 00:08:10,160
Of course the hidden state is
going to be updated more and more.

146
00:08:10,280 --> 00:08:15,110
And the whole idea of recurrence is made
so that we have a form of neural memory

147
00:08:15,200 --> 00:08:19,990
for sequences specifically.
But the problem is that when we're,

148
00:08:20,000 --> 00:08:24,410
whenever we are back propagating,
the gradient tends to vantage.

149
00:08:24,440 --> 00:08:27,110
So it's called the vanishing gradient
problem. So, let me explain this.

150
00:08:27,230 --> 00:08:30,740
So whenever we're forward propagating,
we have our input data, right?

151
00:08:30,741 --> 00:08:34,700
And we are trying to predict the next a
word or character or a musical note in

152
00:08:34,701 --> 00:08:39,380
the sequence. And to what we do is we say
input times wait out of bias, activate,

153
00:08:39,381 --> 00:08:42,650
repeat over and over. And then you know,
recurring over and over and over again.

154
00:08:42,830 --> 00:08:44,720
And you get the output,
which is the prediction.

155
00:08:45,170 --> 00:08:47,090
The prediction is the
next word or whatever.

156
00:08:47,450 --> 00:08:50,330
And so once you had that prediction
value, then you're going to say, okay,

157
00:08:50,530 --> 00:08:54,680
the doctor rise a prediction into a number
and say prediction minus the expected,

158
00:08:54,710 --> 00:08:57,500
the, the, the expected output,

159
00:08:57,600 --> 00:09:00,300
which is the actual label
or next word or character.

160
00:09:00,301 --> 00:09:03,090
Cause we know it and
because it's a supervised.

161
00:09:03,300 --> 00:09:05,880
And so then we have the
error value or lost value.

162
00:09:05,881 --> 00:09:09,870
And then we use that air value to compute
the partial derivative with respect to

163
00:09:09,871 --> 00:09:13,040
our weights going backwards in
our network. Recursively work,

164
00:09:13,041 --> 00:09:16,440
performing gradient descent or in
the context of neural networks.

165
00:09:16,520 --> 00:09:21,240
Backpropagation because we are back
propagating on error gradient across every

166
00:09:21,241 --> 00:09:25,140
layer.
But what happens is that as the gradient,

167
00:09:26,130 --> 00:09:27,780
remember this is a chain of operations.

168
00:09:27,781 --> 00:09:31,800
The chain rule I would
happens is as we are, uh,

169
00:09:31,890 --> 00:09:36,210
as we are propagating this gradient
value backwards across every layer we're

170
00:09:36,211 --> 00:09:39,530
competing the partial derivative
with respect to the air bar,

171
00:09:39,780 --> 00:09:44,760
the partial derivative of the
error with respect to our weights.

172
00:09:44,820 --> 00:09:49,230
As we are doing that for
every layer recursively the
gradient value gets smaller

173
00:09:49,231 --> 00:09:52,260
and smaller and smaller for the reason,

174
00:09:52,320 --> 00:09:55,070
for linear algebraic reasons,
the gradient,

175
00:09:55,110 --> 00:09:57,090
the gradient just gets smaller
and smaller and smaller.

176
00:09:57,360 --> 00:09:59,850
And so what this means is
that the magnitude of the,

177
00:09:59,900 --> 00:10:04,900
so the whole point of that propagation
of grading dissent is to improve our

178
00:10:05,041 --> 00:10:10,041
weight values such that are expected
output is closer to such that our actually

179
00:10:10,910 --> 00:10:15,270
sets that our predicted output. It's
closer to our expected output, right?

180
00:10:15,271 --> 00:10:17,340
We were trying to
minimize the error value.

181
00:10:17,520 --> 00:10:20,970
And so the whole point of gradient
descent is to give our weight values a

182
00:10:20,971 --> 00:10:25,260
direction in which to update such that
the error is going to be minimized

183
00:10:25,290 --> 00:10:29,910
through a forward pass.
And so by direction,

184
00:10:29,911 --> 00:10:34,050
I'm talking about what,
what values in this weight matrix,

185
00:10:34,051 --> 00:10:38,400
this set of numbers is optimal such
that if we were to multiply this by the

186
00:10:38,401 --> 00:10:40,710
input and then you know,
do that over and over again,

187
00:10:40,830 --> 00:10:44,250
it's going to give us the
rights, output value. And so

188
00:10:46,030 --> 00:10:49,140
that's the whole reason we're computing
partial derivatives or gradients,

189
00:10:49,141 --> 00:10:53,370
which we also call them because
calculus is a study of change, right?

190
00:10:53,550 --> 00:10:58,340
Change whether it be in moving
bodies or change in terms of uh,

191
00:10:58,380 --> 00:11:02,250
how to update a set of values.
And when I say direction,

192
00:11:02,370 --> 00:11:05,100
it doesn't mean like a literal
direction, like up, down, left, right?

193
00:11:05,101 --> 00:11:07,060
It means a direction in that,
um,

194
00:11:07,110 --> 00:11:10,560
the numbers are closer to the ideal
optimal numbers that they should be in the

195
00:11:10,561 --> 00:11:13,380
weight matrix.
So when we multiply the input by them,

196
00:11:13,530 --> 00:11:16,680
the output is going to be closer
to the actual output that we want.

197
00:11:16,681 --> 00:11:19,770
I probably repeated that several times,
but you know, it's good. It's good,

198
00:11:19,800 --> 00:11:22,830
it's good for us. And so, so that's,

199
00:11:22,831 --> 00:11:27,540
that's the whole point of a performing
gradient descent, Aka backpropagation.

200
00:11:27,750 --> 00:11:29,070
And so the grading gets smaller.

201
00:11:29,220 --> 00:11:33,300
And so what this means is that
the magnitude of change in the,

202
00:11:34,170 --> 00:11:39,090
the first layers of the network is going
to be smaller than the magnitude of

203
00:11:39,091 --> 00:11:42,330
change in the tail end of the
network. The last layers, right?

204
00:11:42,331 --> 00:11:47,100
So the last layers are going to be,
um, more affected by the change,

205
00:11:47,190 --> 00:11:47,631
but the,

206
00:11:47,631 --> 00:11:51,240
the first layers are going to be not as
effective because the grading update is

207
00:11:51,241 --> 00:11:55,960
smaller because, because the
grade itself is smaller and so,

208
00:11:57,120 --> 00:11:57,360
okay.

209
00:11:57,360 --> 00:12:00,870
Right? And there are two factors that
affect the magnitude of these gradients,

210
00:12:01,050 --> 00:12:04,530
the weights and the activation functions.
And um,

211
00:12:07,020 --> 00:12:08,480
each of these factors is smarter than one.

212
00:12:08,510 --> 00:12:13,100
The gradients may vanish in time if larger
than one, an exploding might happen.

213
00:12:13,101 --> 00:12:16,760
So that's called the exploding gradient
problem. The grade itself is too big,

214
00:12:16,850 --> 00:12:20,990
so it can go either direction. Usually
it's vanishing gradient, but uh, yeah,

215
00:12:20,991 --> 00:12:21,920
this is a problem,
right?

216
00:12:21,921 --> 00:12:26,921
We want to somehow maintain that gradient
value as we are back propagating.

217
00:12:27,741 --> 00:12:31,880
We want to maintain that error gradient
value so that it is at the full

218
00:12:31,881 --> 00:12:35,690
magnitude that it should be to update
our weight values for every layer

219
00:12:35,691 --> 00:12:39,530
recursively. In the correct way. We
want to maintain that rating value.

220
00:12:39,531 --> 00:12:43,640
We want to remember that creating
value as we'd back propagate.

221
00:12:44,060 --> 00:12:48,260
And so how do we remember a
gradient value? How do we remember,

222
00:12:48,261 --> 00:12:51,860
remember is uh, a word, you know,
maintain, remember, you know,

223
00:12:51,861 --> 00:12:52,730
whatever you want to call it.

224
00:12:52,940 --> 00:12:57,920
And so the solution for this is called
using an LSTM cell or a long short term

225
00:12:57,921 --> 00:12:59,570
memory cells.
And if you think about it,

226
00:12:59,571 --> 00:13:02,030
the knowledge of our recurrent
network is pretty chaotic.

227
00:13:02,130 --> 00:13:06,710
Like let's say we're trying to, you
know, a caption, uh, of a video,

228
00:13:06,770 --> 00:13:09,140
a set of frames, a video, right? And it's,

229
00:13:09,141 --> 00:13:12,860
he's a guy and he's eating a burger and
did the statue of Liberty is behind him.

230
00:13:12,861 --> 00:13:16,760
So then there were the network things.
Okay, he must be in the United States,

231
00:13:16,880 --> 00:13:19,370
but then he's eating Sushi and it thinks,
oh,

232
00:13:19,550 --> 00:13:21,470
he must be in Japan just
cause it seemed Sushi,

233
00:13:21,471 --> 00:13:24,950
but it forgot that he was just
behind the Statue of Liberty.

234
00:13:25,130 --> 00:13:29,840
There exists Sushi Sushi places in New
York City too, right? And then, you know,

235
00:13:29,900 --> 00:13:32,570
he's riding a boat and he thinks,
oh, he must be in, you know,

236
00:13:32,990 --> 00:13:35,300
the odyssey or something,
but he's still in New York.

237
00:13:35,450 --> 00:13:40,450
So we need the information to update less
chaotically to account for all of the

238
00:13:41,931 --> 00:13:46,700
learnings, the memories that it's
learned over a vast period of time,

239
00:13:46,730 --> 00:13:49,340
a vast sequence to be more accurate.

240
00:13:49,730 --> 00:13:52,310
And so the solution for this
is called the LSTM cell,

241
00:13:52,550 --> 00:13:55,610
and it replaces the RNN cell.

242
00:13:55,670 --> 00:14:00,380
And so the RNN cell is input times
weight out of bias activates,

243
00:14:00,590 --> 00:14:05,210
right? And would that be a weight
matrix that is connecting to itself?

244
00:14:06,560 --> 00:14:10,550
And so the difference is an Lstm cell,

245
00:14:10,551 --> 00:14:13,340
basically just you just
replace it with an Lstm cell.

246
00:14:13,341 --> 00:14:16,700
You just take that out and you replace
it with an Lstm cell. And what it is,

247
00:14:16,950 --> 00:14:21,950
it's is a more complicated or more
extensive series of matrix operations.

248
00:14:21,951 --> 00:14:24,680
So let me, let me talk about what this
is. Okay, let me, let me give this a go.

249
00:14:24,920 --> 00:14:28,400
So LSTM cells consist of three gates.

250
00:14:28,490 --> 00:14:32,360
You've got an input gates right here.
You have your output gate right here.

251
00:14:32,570 --> 00:14:36,290
You ever forget gate right here.
And then you have a cell state.

252
00:14:36,380 --> 00:14:38,600
Now you also see this
input modulation gates.

253
00:14:38,750 --> 00:14:42,410
So that's actually only use sometimes,
but uh, let's just forget about that.

254
00:14:42,411 --> 00:14:44,420
That's,
that's just forget that exist.

255
00:14:44,570 --> 00:14:48,640
You have an input gates on Kate's and
a forget gates. And so there's many,

256
00:14:48,830 --> 00:14:53,000
there are many variants of Lstm is and
why you see this input modulation gates.

257
00:14:53,150 --> 00:14:56,420
But the most used ones are just
input, output, forget, and cell state.

258
00:14:56,421 --> 00:15:00,410
So just forget about the input
modulation gate for now. Okay,

259
00:15:00,411 --> 00:15:03,800
so we've got three gate values
and then we have a cell state.

260
00:15:04,040 --> 00:15:07,910
So you might be thinking, okay, so
what are these gates like? Like what,

261
00:15:07,970 --> 00:15:12,080
what is a gate?
And so a gate is just like a layer.

262
00:15:12,200 --> 00:15:17,200
A gate is a series of matrix operations
and it is input times weights out of

263
00:15:17,421 --> 00:15:18,410
bias activate.

264
00:15:18,411 --> 00:15:22,940
So in a way you could think of an
LSTM cell as kind of like a May neural

265
00:15:22,941 --> 00:15:26,990
network.
These gates all have weights themselves.

266
00:15:27,080 --> 00:15:29,720
So they all have their own
set of weight matrices.

267
00:15:29,900 --> 00:15:34,220
That means that an Lstm cell
is fully differentiable.

268
00:15:34,430 --> 00:15:39,230
That means that we can compute the
derivative of each of these components or

269
00:15:39,231 --> 00:15:43,880
gates, uh, which means that we can
update them over time. We can learn,

270
00:15:43,881 --> 00:15:47,300
we can have them learn over time.
And so,

271
00:15:49,370 --> 00:15:51,900
uh, so these are the equations
for each of these, right?

272
00:15:51,901 --> 00:15:54,670
So for forget gate or
implicated and applicate,

273
00:15:54,770 --> 00:15:58,910
it's input times weight out
of bias activate where the
input it consists of the

274
00:15:58,911 --> 00:16:03,290
input and the hidden state from
the previous time step. So,

275
00:16:04,630 --> 00:16:08,120
and each of these weights, these gates
have their own set of white values.

276
00:16:08,360 --> 00:16:13,360
And so what we want is we want a way
for our model to know what to forget and

277
00:16:13,491 --> 00:16:18,110
what to remember and what to pay
attention to in what it's learned, right?

278
00:16:18,170 --> 00:16:21,230
What is the relevant,
and that's called the attention mechanism.

279
00:16:21,410 --> 00:16:26,030
What is the relevant data and everything
that it's learned. What is the,

280
00:16:26,130 --> 00:16:27,190
um,

281
00:16:27,710 --> 00:16:32,090
relevant part of what it's being fed
in this time step to remember and what

282
00:16:32,091 --> 00:16:37,091
should forget the cell states is the
longterm memory it represents what all of

283
00:16:37,731 --> 00:16:42,290
the learnings across all of time. The
hidden state is akin to working memory.

284
00:16:42,291 --> 00:16:43,970
So it's kind of like a current memory.

285
00:16:44,480 --> 00:16:49,480
The forget gate also called the re
remember vector learns what to forget and

286
00:16:49,881 --> 00:16:54,110
what to remember, right? One
or zero binary outcome. This,

287
00:16:54,111 --> 00:16:58,520
the input gate determines how much of
the input to let into the cell state also

288
00:16:58,521 --> 00:17:01,310
called a save vector,
what to save and what not to.

289
00:17:01,520 --> 00:17:04,550
And the output gate is akin
to an attention mechanism.

290
00:17:04,790 --> 00:17:08,630
What part of that data should it focus on?
And so you might be thinking,

291
00:17:08,631 --> 00:17:13,280
well how is like I see you
might be thinking like, okay,
so I see these equations,

292
00:17:13,370 --> 00:17:18,240
I see how like input times wait antibodies
activate a t is akin to forgetting

293
00:17:18,270 --> 00:17:22,460
and input and output. I see how there's
an ordering to this, right? We have,

294
00:17:22,490 --> 00:17:27,040
we have an ordering to this. We first,
you know, go through an input. We,

295
00:17:27,070 --> 00:17:29,570
we learned what we learned,
what to forget,

296
00:17:29,720 --> 00:17:33,530
and then we compute that
cell state and then we,

297
00:17:34,130 --> 00:17:37,430
and then we send it to the
output. Um, ultimate. But,

298
00:17:37,520 --> 00:17:40,130
and then ultimately we compute
the cell and the hidden state.

299
00:17:40,280 --> 00:17:43,580
And these are the two key
values that we output.

300
00:17:43,880 --> 00:17:47,420
So you might see these and you might
think, okay, like I see the ordering,

301
00:17:47,540 --> 00:17:51,420
I see how they represent for getting
and but I don't like make the connection

302
00:17:51,421 --> 00:17:55,530
between still like how it knows
what to forget and what to remember.

303
00:17:55,590 --> 00:17:59,330
And that's again,
that's the amazing thing about um,

304
00:17:59,880 --> 00:18:04,400
neural networks like these, these gates
are essentially perceptrons there.

305
00:18:04,401 --> 00:18:07,350
They're like mini networks,
right? With, with a single node.

306
00:18:07,380 --> 00:18:09,000
The node is the gate itself,
right?

307
00:18:09,270 --> 00:18:11,490
It's a single note what they
single activation function.

308
00:18:11,820 --> 00:18:14,670
All of these gates are perceptrons,
they're like many neural networks,

309
00:18:14,671 --> 00:18:19,470
like single layer neural networks. And it
learns what to forget, what to remember.

310
00:18:19,940 --> 00:18:24,390
Um, based on gradient descent. Again,
it's the magic of grading dissent.

311
00:18:24,480 --> 00:18:28,260
It learns what is necessary over time.
And so these,

312
00:18:28,290 --> 00:18:32,070
these components represent mechanisms
for forgetting, remembering,

313
00:18:32,071 --> 00:18:34,200
and what to pay attention to or attention.

314
00:18:34,890 --> 00:18:37,680
And so that's what the and provides us.
So instead of,

315
00:18:37,740 --> 00:18:41,100
so this equation would be for
a normal recurrent network.

316
00:18:41,190 --> 00:18:45,630
So you would have your input times your
weight plus the hidden state times its

317
00:18:45,631 --> 00:18:48,810
own hidden state to hidden state matrix.
You'd activate that.

318
00:18:48,930 --> 00:18:51,360
And that would give you the hidden
state at the current time step.

319
00:18:51,930 --> 00:18:55,160
The difference is that it would
look like this instead. It's a,

320
00:18:55,161 --> 00:18:59,610
it's a more extensive
series of operations, right?

321
00:18:59,610 --> 00:19:03,780
So that's what it is. And
uh, yeah. And so, yeah,

322
00:19:03,781 --> 00:19:06,660
so that's kind of the high
level of how that works.

323
00:19:06,810 --> 00:19:08,670
And now we're going to look
at it and code as well.

324
00:19:08,671 --> 00:19:10,470
We're just going to help you know,
your retention.

325
00:19:10,800 --> 00:19:13,320
But what are some use cases of this?
Like I said,

326
00:19:13,321 --> 00:19:16,410
it's all sorts of sequential data,
any kind of texts,

327
00:19:16,411 --> 00:19:19,320
any kind of sequence data,
it's going to,

328
00:19:19,321 --> 00:19:22,950
it's going to be able to
learn what the next uh,

329
00:19:23,250 --> 00:19:24,600
values in that sequence are,

330
00:19:24,601 --> 00:19:29,280
which means it will be
able to both generate and
discriminate that type of data

331
00:19:29,281 --> 00:19:32,710
that you've trained it on, right? In
this case, it's, it's, it's handled,

332
00:19:32,820 --> 00:19:36,210
it's a characters that I can draw.
It can draw.

333
00:19:37,080 --> 00:19:41,510
So this is very popular in NLP.
Andre Carpathy,

334
00:19:41,520 --> 00:19:44,520
the famous AI researcher.
He has a great blog post on this,

335
00:19:44,521 --> 00:19:47,610
the unreasonable effectiveness
of recurrent networks. I
assume you've read that.

336
00:19:47,611 --> 00:19:50,850
If not, check it out. Um, I'll
also link to it, probably,

337
00:19:50,851 --> 00:19:52,200
I'll link to it in the description.

338
00:19:52,590 --> 00:19:56,580
And so yet to use for translation and
send them an analysis and all sorts of NLP

339
00:19:56,581 --> 00:19:59,370
tasks. Uh, it's also used
for image classification.

340
00:19:59,371 --> 00:20:03,240
Like if you think of a picture as a
sequence where each pixel is a sequence,

341
00:20:03,390 --> 00:20:04,920
predict the next sequence.

342
00:20:05,100 --> 00:20:08,370
So there's a lot of different
ways that we can frame our, our,

343
00:20:08,460 --> 00:20:10,680
our problem and you know,

344
00:20:10,940 --> 00:20:15,390
Ellis teams can be applied to
almost any problem. Very useful.

345
00:20:15,660 --> 00:20:17,940
And so what are some other great
examples where I've got two.

346
00:20:18,090 --> 00:20:21,870
One is for automatic speech
recognition with tensorflow. So yes,

347
00:20:21,871 --> 00:20:23,370
it's a little abstracted with tensorflow,

348
00:20:23,400 --> 00:20:26,260
but it's a really cool use case and
definitely something that checkout.

349
00:20:26,280 --> 00:20:30,540
So check out that demo.
And then also, um, this,

350
00:20:30,990 --> 00:20:33,420
this repository,
which is

351
00:20:35,460 --> 00:20:37,650
just a visualization of Lstm,

352
00:20:37,651 --> 00:20:40,380
which is going to definitely help with
your understanding to check out this

353
00:20:40,500 --> 00:20:45,480
visualization. Very cool. Very cool demo.
Uh, it's in Javascript, I think. Yeah,

354
00:20:45,481 --> 00:20:49,590
it's in javascript. Oh No, it's in
great. So yeah, so there's that.

355
00:20:49,670 --> 00:20:50,501
Definitely check that out.

356
00:20:50,501 --> 00:20:53,740
And so our steps are going to be the
look at the recurrent. Now we're class.

357
00:20:53,860 --> 00:20:56,620
So we'll have a recurrent network class,
Plano Rucker and network.

358
00:20:57,010 --> 00:20:59,660
And we're going to use
an LSTM cell instead.

359
00:20:59,680 --> 00:21:03,430
So we're going to replace it and then
we'll build that LSTM class and then our

360
00:21:03,431 --> 00:21:06,970
data loading functions for our text.
And then we'll train our model. Okay,

361
00:21:06,971 --> 00:21:10,900
so what is this right here? This
jumble of numbers with no explanation.

362
00:21:11,290 --> 00:21:13,180
This represents the forward pass,

363
00:21:13,181 --> 00:21:17,410
the series of Matrix operations that
were computing, uh, to get our output,

364
00:21:17,440 --> 00:21:22,120
our predicted output. Okay, so, and
we'll look at this and code. So,

365
00:21:22,450 --> 00:21:25,270
okay, so a recurrent neural
network is going to remember.

366
00:21:25,271 --> 00:21:28,390
So what we're going to do
is given our m and m text,

367
00:21:28,540 --> 00:21:32,740
we're going to predict the input,
the next word in the sequence,

368
00:21:32,741 --> 00:21:33,910
given the previous words.

369
00:21:34,090 --> 00:21:38,650
So what that means is the input is going
to be all of those words that we have,

370
00:21:38,651 --> 00:21:42,910
just every word and our output will be
the same thing, but just moved over one.

371
00:21:43,090 --> 00:21:46,360
That means that, you know, it'll,
it'll look like this. Like, you know,

372
00:21:46,361 --> 00:21:49,480
during training we have a series of,
you know, training iterations, right?

373
00:21:49,600 --> 00:21:52,630
So we'll say, you know, like,
uh, let's say, you know,

374
00:21:52,660 --> 00:21:57,430
my name is slim shady would be like
the, the text. So we would say my,

375
00:21:57,550 --> 00:22:00,490
and we would try to predict
name, okay, we've got that, uh,

376
00:22:00,670 --> 00:22:03,580
compute the error back propagate,

377
00:22:03,820 --> 00:22:05,920
next iteration weights are updated,

378
00:22:06,220 --> 00:22:10,690
my name and then we're going to try to
predict is right. And so then we say,

379
00:22:11,650 --> 00:22:14,980
uh, try to predict is we
have an actual output.

380
00:22:15,520 --> 00:22:19,480
We have a predicted output, compute
the error back propagate, repeat.

381
00:22:19,600 --> 00:22:22,690
My name is, I'm trying to predict
slim. You see what I'm saying?

382
00:22:22,691 --> 00:22:26,110
So we just keep doing that.
And so that's why it's moved over by one.

383
00:22:26,170 --> 00:22:28,660
So our inputs and our outputs
are going to be our words.

384
00:22:28,840 --> 00:22:31,420
We have the number of
recurrences that we want to do,

385
00:22:31,421 --> 00:22:35,050
which is the number of words total,
right? Cause we're gonna, we're gonna,

386
00:22:35,320 --> 00:22:39,730
we're going to perform recurrence in our
network for as many times as necessary

387
00:22:39,880 --> 00:22:43,780
until we iterate through all those words
up until we get to that predicted ward

388
00:22:43,781 --> 00:22:48,680
that we need to be at. And then we
have an array of expected outputs. Uh,

389
00:22:49,130 --> 00:22:52,090
and then we have our learning rate,
which is our tuning up for, you know,

390
00:22:52,900 --> 00:22:56,170
too low and it's never going to
converge to high and it'll convert it,

391
00:22:56,180 --> 00:23:00,400
it'll overshoot, uh, and so we
will never converge as well.

392
00:23:00,940 --> 00:23:03,460
So that's where our learning rate is.
So now let's look at this.

393
00:23:03,490 --> 00:23:06,190
So we're going to do a
bunch of an initialization.

394
00:23:06,350 --> 00:23:10,500
So remember the difference between LSTs
and Plano recurrent nets, or is our,

395
00:23:10,510 --> 00:23:15,090
that we have more parameters, those,
those gate values, and we, uh,

396
00:23:15,190 --> 00:23:17,680
have new operations for
those parameter values.

397
00:23:17,800 --> 00:23:22,690
So we're going to initialize all of
those right at the beginning. And Look,

398
00:23:22,720 --> 00:23:25,540
you might look at this and think, wow,
this is very complicated and difficult.

399
00:23:25,660 --> 00:23:28,480
But remember with tensorflow,
with care os you can,

400
00:23:28,510 --> 00:23:32,980
you can do this in 10 lines of
code 20, 30, 40 lines of code.

401
00:23:33,310 --> 00:23:37,480
We're looking at the, at the details
here. That's why it looks so long. Okay.

402
00:23:37,481 --> 00:23:40,780
So that's why it looks so long.
So now let's look at this.

403
00:23:40,810 --> 00:23:45,770
So we're going to initialize our first
word, the size of it there, next word,

404
00:23:45,800 --> 00:23:48,840
the size of that.
And then our weight matrix.

405
00:23:48,870 --> 00:23:51,920
This is the weight matrix between
the input and the hidden state,

406
00:23:52,190 --> 00:23:56,000
which we're going to initialize randomly
of this size of our predicted output.

407
00:23:56,910 --> 00:23:57,420
Yeah.

408
00:23:57,420 --> 00:24:01,370
Well, initialize this variable, gee,
that's going to be used for r m s prop,

409
00:24:01,550 --> 00:24:06,080
which is a, a technique for gradient
descent that decays the learning rates.

410
00:24:06,200 --> 00:24:09,350
I'll talk about why we're using g later
on. We're not ready for that right now,

411
00:24:09,680 --> 00:24:11,720
but then we're going to compete,
we're going to say,

412
00:24:12,130 --> 00:24:14,210
and then you tried the length
of the recurrent network, right?

413
00:24:14,211 --> 00:24:16,220
That's the number of words that we have.

414
00:24:16,490 --> 00:24:19,850
The number of recurrences are learning
rates. These are all parameters.

415
00:24:20,030 --> 00:24:24,260
And now we're going to have a raise for
storing our inputs at every time step on

416
00:24:24,261 --> 00:24:28,730
array for storing our cell states on a
ready for storing hormone output values

417
00:24:28,910 --> 00:24:33,410
are hidden states. And then
our, um, take values, right,

418
00:24:33,411 --> 00:24:36,230
are for gate values. Well, one
of them is actually a cell state.

419
00:24:36,231 --> 00:24:40,980
These aren't LSTM values. So before
that, these were the, uh, network,

420
00:24:41,030 --> 00:24:45,080
the recurrent network values,
and now he's the LSTM cell values.

421
00:24:45,320 --> 00:24:49,520
And then we have our, our
array of expected output
values. And finally we have R,

422
00:24:49,550 --> 00:24:54,470
l s t m cell, which we initialize right
here, giving it our inputs, our outputs,

423
00:24:54,471 --> 00:24:56,180
the amount of occurrence
and the learning rate,

424
00:24:56,181 --> 00:24:59,990
just like we did for the
recurring network. Remember,
it's like a mini network.

425
00:25:00,020 --> 00:25:03,770
It's like a network in a network.
And if you think of the gates as networks,

426
00:25:03,771 --> 00:25:08,630
and it's a network in a network
inside of a network or no,

427
00:25:08,631 --> 00:25:11,900
it's three networks inside of a
network. Inside of a network. Okay.

428
00:25:12,170 --> 00:25:16,430
So yeah, talk about inception, inception,

429
00:25:16,700 --> 00:25:19,790
recurrence, inception.
So back to this. Um,

430
00:25:20,820 --> 00:25:24,050
those are our initializations
and now we have our sigmoid,

431
00:25:24,051 --> 00:25:27,860
which is our activation function.
It's a simple nonlinearity, right?

432
00:25:27,861 --> 00:25:30,710
And then we have our derivative
of the sigmoid function,

433
00:25:30,890 --> 00:25:34,160
which is used to compute gradients.
That's why we have the derivative. Right.

434
00:25:34,250 --> 00:25:37,490
We'll talk about that in backpropagation.
And then we have four propagation,

435
00:25:37,491 --> 00:25:41,420
which is our series of Matrix operations,
which I'm going to code. Okay. So for,

436
00:25:41,421 --> 00:25:42,620
for propagation,

437
00:25:43,620 --> 00:25:44,453
yeah,

438
00:25:44,670 --> 00:25:46,680
we're going to do this in a loop,
right?

439
00:25:46,681 --> 00:25:51,570
Because it's a recurrent network for
the number of, uh, loops that we want.

440
00:25:51,960 --> 00:25:54,210
Right? Um, so we're going to say, okay,

441
00:25:54,211 --> 00:25:58,880
so for the number of loops that we want,
that we define in our L,

442
00:26:01,770 --> 00:26:05,730
we're going to set the
input for the LSTM cell,

443
00:26:06,120 --> 00:26:07,920
that input for the LSTM cell,

444
00:26:08,550 --> 00:26:12,160
which is a combination of inputs.
So we'll set,

445
00:26:12,180 --> 00:26:16,290
which we can do with the h stack
function of num Pi, and we'll say,

446
00:26:18,420 --> 00:26:18,980
okay,

447
00:26:18,980 --> 00:26:21,380
hi.
Minus one or minus one.

448
00:26:22,070 --> 00:26:24,110
And then self dot x

449
00:26:27,090 --> 00:26:31,030
Lstm dot x equals NP softer.

450
00:26:32,320 --> 00:26:35,900
And so this is the s, so this is how
we set inputs for the LTM sell by.

451
00:26:36,230 --> 00:26:39,860
It's a combination of inputs from the
previous output and the previous hidden

452
00:26:39,861 --> 00:26:42,800
states right here,
right? So that's, that's,

453
00:26:42,830 --> 00:26:44,940
that's US initializing our LSTM cell,

454
00:26:45,210 --> 00:26:50,040
and now we can run for propagation
through the LSTM cell itself.

455
00:26:50,220 --> 00:26:53,550
So it's gonna Return. Our cell
states are hidden, states are,

456
00:26:53,551 --> 00:26:58,380
forget gate are. So we can, you know,
store it in our array of values,

457
00:26:58,680 --> 00:27:02,440
um,
c which is a cell state,

458
00:27:02,441 --> 00:27:06,820
and then the output. And so we can
compute that using the forward prop,

459
00:27:07,060 --> 00:27:09,460
a function of the LSTM cell.

460
00:27:09,910 --> 00:27:12,550
And so then we're going to,

461
00:27:13,140 --> 00:27:13,973
uh,

462
00:27:18,080 --> 00:27:20,210
say,
let's

463
00:27:22,660 --> 00:27:25,450
store the computed cell state,
right?

464
00:27:25,451 --> 00:27:30,190
So then we've got all these values
and now we can store them locally,

465
00:27:30,340 --> 00:27:35,140
uh, uh, in this, in this
recurrent networks memory,

466
00:27:35,590 --> 00:27:39,820
these are all values that were computed
from the forward propagation of the LSTM

467
00:27:39,821 --> 00:27:40,654
cell.

468
00:27:42,880 --> 00:27:46,690
And so we can just, you know,
set the, the hidden state.

469
00:27:46,900 --> 00:27:51,670
We've got our, uh, what else do we
have? We have our, uh, forget gates.

470
00:27:51,700 --> 00:27:56,140
Of course the forget gate is
going to be a set as well.

471
00:27:56,320 --> 00:28:00,040
These are all values that we computed
in our Ford propagation states.

472
00:28:00,400 --> 00:28:05,070
We have our input gates,
uh,

473
00:28:05,071 --> 00:28:08,070
which we're going to set like that.
We have our cell state,

474
00:28:15,440 --> 00:28:17,480
and then we have our output gate,

475
00:28:22,700 --> 00:28:24,140
right? These are all
values. We competed there.

476
00:28:24,170 --> 00:28:28,240
Now we can calculate the output by
multiplying the hidden state, uh,

477
00:28:28,250 --> 00:28:32,270
with the weight matrix. So input
times wait, alibis activate.

478
00:28:32,510 --> 00:28:33,860
So we'll say,
okay,

479
00:28:35,000 --> 00:28:39,170
we'll compute that output by saying
self dot sigmoids we'll use a sigmoid

480
00:28:39,171 --> 00:28:41,540
function to activate the

481
00:28:43,430 --> 00:28:45,620
wait times the input

482
00:28:47,930 --> 00:28:51,680
input times we'd had a
bias activate, right? Yup.

483
00:28:52,100 --> 00:28:56,180
And then we'll set our inputs
cause we've computed an output.

484
00:28:56,420 --> 00:28:59,350
We want to set our input to the
next word in the sequence, right?

485
00:28:59,351 --> 00:29:01,130
Cause we're gonna,
we're gonna keep going.

486
00:29:08,490 --> 00:29:09,110
Okay.

487
00:29:09,110 --> 00:29:11,180
And then when we're done with that,
we can return

488
00:29:13,250 --> 00:29:14,450
the output prediction.

489
00:29:16,590 --> 00:29:17,423
Okay.

490
00:29:18,390 --> 00:29:20,880
Right? So that's forward propagation.
That's forward propagation.

491
00:29:21,180 --> 00:29:25,890
Now for backward propagation,
we're going to, um, look at this.

492
00:29:25,950 --> 00:29:29,370
So we're going to update our weight
matrices. Uh, that's, that's what,

493
00:29:29,510 --> 00:29:32,400
that's the point of backpropagation
to update our weight matrices with our

494
00:29:32,401 --> 00:29:33,120
learnings.

495
00:29:33,120 --> 00:29:36,570
So we've computed at predicted next
word and we have our actual next word.

496
00:29:36,840 --> 00:29:39,300
And we represent these words as numbers,
as vectors,

497
00:29:39,420 --> 00:29:41,590
so that we can compute the
difference between words.

498
00:29:41,591 --> 00:29:43,690
Like how do you compute the
difference between words?

499
00:29:43,840 --> 00:29:45,970
Well you convert them
to numbers or vectors,

500
00:29:46,180 --> 00:29:48,970
then we can compute the
difference to get the air value.

501
00:29:49,150 --> 00:29:52,220
So we'll initialize the error is zero.
And then we'll um,

502
00:29:52,270 --> 00:29:56,620
initialize to a empty res,

503
00:29:56,820 --> 00:30:01,360
um, to empty vectors for the
cell state and the hidden state.

504
00:30:01,361 --> 00:30:05,770
Remember the cell state in the hidden
state where those two values right here,

505
00:30:05,920 --> 00:30:10,060
which ultimately these forget input
output gates are used to help compute the

506
00:30:10,061 --> 00:30:14,770
value of right. Notice how, forget
input and output are used here.

507
00:30:14,980 --> 00:30:19,120
That the forget gate is multiplied
by the working memory state. Um,

508
00:30:19,180 --> 00:30:22,030
and then we add the input gate which and
multiplied by the working memory state

509
00:30:22,031 --> 00:30:26,140
as well, uh, to remember what to forget,

510
00:30:26,200 --> 00:30:28,750
to learn,
what to forget and what to remember.

511
00:30:28,960 --> 00:30:32,560
And that's what's stored in our cells
state the learnings of the forget and the

512
00:30:32,561 --> 00:30:33,394
input gates.

513
00:30:33,670 --> 00:30:38,670
And we then we then activates the cell
state and multiply it by the output to

514
00:30:39,941 --> 00:30:43,450
get our hidden state.
And so we have for our level,

515
00:30:43,451 --> 00:30:46,060
with a weight matrix between the
input and the hidden the states,

516
00:30:46,330 --> 00:30:50,170
we have our hidden state
itself and in the cell state.

517
00:30:50,200 --> 00:30:54,100
And those are the key like
outer level, um, uh, parameters.

518
00:30:54,490 --> 00:30:58,520
And then our inner level parameters are
those LSTM level gradients, right? So,

519
00:30:58,521 --> 00:31:00,820
so we want the creating
values for our forget gate,

520
00:31:00,821 --> 00:31:05,320
our input gate or sell ourselves unit
or state, and then our output gate,

521
00:31:05,740 --> 00:31:10,490
the internal ones. And so we're
going to fill these out. So, uh,

522
00:31:10,540 --> 00:31:13,150
we're gonna loop backwards our backdrop,
the gate through time,

523
00:31:13,151 --> 00:31:16,990
through our recurrence. So we're gonna
say we have our calculator output,

524
00:31:16,991 --> 00:31:19,540
let's compute the air between
that and the expected output.

525
00:31:20,020 --> 00:31:21,580
And then we're going to say,
okay,

526
00:31:21,940 --> 00:31:25,000
we're going to compute the
partial derivative vice computing,

527
00:31:25,001 --> 00:31:29,470
the error times a derivative of
the output times the hidden state.

528
00:31:29,950 --> 00:31:32,890
And so once we have that,
then we can say, okay,

529
00:31:32,891 --> 00:31:35,380
it's time to propagate
the air back to the,

530
00:31:35,560 --> 00:31:40,360
to the exit of the Lstm cell or
the, the, the, the way out to,

531
00:31:40,361 --> 00:31:44,280
to the outputs of their recurrent network
in general. So the way we do that, it's,

532
00:31:44,281 --> 00:31:45,610
it's three steps.
We do,

533
00:31:45,700 --> 00:31:49,450
we compute the error times the
recurrent network weight matrix,

534
00:31:49,780 --> 00:31:54,490
and then we set the input values
of the LSTM cell for recurrence.

535
00:31:55,420 --> 00:31:58,900
We set the input values of
the LSTM cell for recurrence.

536
00:31:59,230 --> 00:32:04,170
And then finally we set the cell state
of the LSTM cell for recurrence pre like

537
00:32:04,180 --> 00:32:06,520
that's pre updates and then

538
00:32:08,350 --> 00:32:13,350
we recursively called this backpropagation
using these newly computed values.

539
00:32:13,690 --> 00:32:16,060
Right? So for our, for uh,

540
00:32:16,150 --> 00:32:19,750
this is going to compute gradient updates
for our forget input cell unit and

541
00:32:19,751 --> 00:32:24,040
help locates and the s they'll have the
higher level cell state and the hidden

542
00:32:24,041 --> 00:32:27,010
state. Those two higher
level, uh, parameters.

543
00:32:27,310 --> 00:32:30,520
And so these are all of our gradients.
And now we can,

544
00:32:30,550 --> 00:32:32,500
this is just for air logging.

545
00:32:32,680 --> 00:32:37,660
Now we can accumulate those gradient
updates by adding them to our existing

546
00:32:37,690 --> 00:32:40,250
empty valleys that we
initialize right at the start.

547
00:32:40,730 --> 00:32:44,510
And then we can update our LSTM major
cs with the average of the accumulated

548
00:32:44,511 --> 00:32:48,200
gradient updates and then update our
weight matrix with the average of the

549
00:32:48,201 --> 00:32:52,820
accumulated grade updates and return
the total error of this iteration.

550
00:32:53,030 --> 00:32:56,210
And that's backpropagation in general.
So then,

551
00:32:57,780 --> 00:32:58,613
okay,

552
00:32:58,830 --> 00:32:59,760
uh,
so then,

553
00:33:04,740 --> 00:33:05,180
okay,

554
00:33:05,180 --> 00:33:09,860
so notice this update step
with this update step is,
is rms prop. It's a way of,

555
00:33:09,861 --> 00:33:13,430
the cane are, uh,
learning rates over time.

556
00:33:13,580 --> 00:33:14,990
And this improves convergence.

557
00:33:14,991 --> 00:33:18,920
There's a lot of different methodologies
for improving on gradient descent.

558
00:33:19,160 --> 00:33:23,900
There's Adam, there's rms
prop, there's, um, at a Grad,

559
00:33:23,901 --> 00:33:26,870
there's a bunch of these
in rms properties, one of
them. But here's the formula.

560
00:33:26,900 --> 00:33:29,840
I'll just put it up there.
Okay? So that's what this is.

561
00:33:30,890 --> 00:33:31,710
Okay.

562
00:33:31,710 --> 00:33:33,510
So now we will.

563
00:33:33,540 --> 00:33:36,840
And so the sample function is the same
thing as a forward propagation function.

564
00:33:36,990 --> 00:33:37,800
It's just,
it's,

565
00:33:37,800 --> 00:33:41,610
it's what we're gonna use
once we've trained our model
to predict or to generate

566
00:33:41,611 --> 00:33:43,380
new words. So it's the same thing, right?

567
00:33:43,381 --> 00:33:47,640
We have our input and for
a number of words that we
defined, we'll say, you know,

568
00:33:48,240 --> 00:33:51,900
generate words or predict words for
as many iterations as we defined.

569
00:33:52,050 --> 00:33:56,100
So it's the same thing. So we can
just skip that. Now for our LSTM cell.

570
00:33:56,280 --> 00:33:57,990
So for our LSTM cell,

571
00:33:59,400 --> 00:34:02,490
we've given it the same parameters
as we did for our recurrent network.

572
00:34:02,491 --> 00:34:06,300
It is after all a mini network in and
of itself. So it gave it the inputs,

573
00:34:06,301 --> 00:34:09,570
the outputs, the amount of
recurrence and the learning rate.

574
00:34:09,840 --> 00:34:12,900
And so what we'll do is very similar
at the start to what we did for our

575
00:34:12,901 --> 00:34:15,920
recurrent network will in
it will initialize, um,

576
00:34:16,290 --> 00:34:21,290
our input the size of it or help put the
size of it and then our cell state is

577
00:34:21,361 --> 00:34:24,000
going to be empty.
How often should we perform a current,

578
00:34:24,001 --> 00:34:27,270
we'll initialize that variable
are learning rate as well.

579
00:34:27,390 --> 00:34:29,580
And now we're going to
create weight matrices.

580
00:34:29,581 --> 00:34:33,210
We'll initialize these weight matrix.
He's randomly just like we would for a,

581
00:34:33,240 --> 00:34:37,200
any kind of neural network will neutralize
of weight matrices for our three gate

582
00:34:37,201 --> 00:34:37,741
values for our,

583
00:34:37,741 --> 00:34:41,760
forget our implicates and our
application as well as our cell state.

584
00:34:41,761 --> 00:34:44,670
So the cell state itself,
let me go up here,

585
00:34:46,280 --> 00:34:49,730
has uh, uh, uh, set of
weight, major CS, right?

586
00:34:49,760 --> 00:34:53,060
Just like all recurrent net,
all neural networks.

587
00:34:53,480 --> 00:34:57,770
And so that node has its own
set of weight. Major sees that,

588
00:34:58,010 --> 00:35:01,700
that we multiply to get
that output value right?

589
00:35:01,730 --> 00:35:04,130
It's a part of a series of operations.

590
00:35:04,280 --> 00:35:08,000
And the weight matrix is that
learning part. It's the non static,

591
00:35:08,001 --> 00:35:13,001
it's the dynamic part of that equation
that would essentially you can think of

592
00:35:14,361 --> 00:35:18,710
these as gates. You can think of them as
layers even. Um, but they're called gates,

593
00:35:20,240 --> 00:35:22,220
but layers are very similar.
You know what I'm saying?

594
00:35:22,430 --> 00:35:25,130
The layers are very similar.
Input Times weight added bias activate,

595
00:35:25,630 --> 00:35:27,890
but we called them gates to differentiate,

596
00:35:28,430 --> 00:35:31,250
not to be confused with so
many terms here, not to,

597
00:35:31,490 --> 00:35:35,090
not to be confused with the actual
mathematical term differentiate but to

598
00:35:35,240 --> 00:35:39,780
discriminate. Right. Okay. So back to
this. So now, um, we're, we're, we,

599
00:35:40,140 --> 00:35:42,540
we initialized our gates,

600
00:35:42,570 --> 00:35:47,570
we and then we've initialize our gates
and now empty bellies for gradients that

601
00:35:47,701 --> 00:35:49,110
we're going to compute for all of these.

602
00:35:49,111 --> 00:35:52,170
Remember all of these
gates are differentiable.

603
00:35:52,800 --> 00:35:54,810
So that's where these grading values,
we'll go sit down,

604
00:35:54,811 --> 00:35:59,220
we can update them through backpropagation
because we back propagate through,

605
00:35:59,221 --> 00:36:00,660
that's the cell itself.

606
00:36:00,840 --> 00:36:03,900
We don't just back propagate through
the recurrent network at a high level.

607
00:36:04,110 --> 00:36:05,070
We are back propagating.

608
00:36:05,071 --> 00:36:08,730
That is we are competing gray and
values for each of these gates.

609
00:36:08,880 --> 00:36:12,900
So we were updating waits for
the input for get a cell state,

610
00:36:13,200 --> 00:36:16,840
the outputs, Gates where, and the, uh,

611
00:36:17,010 --> 00:36:20,310
outer level weight matrix as
well for the recurring network.

612
00:36:20,640 --> 00:36:23,940
So we're competing gradings for
everything. So we're updating everything.

613
00:36:23,941 --> 00:36:27,510
It's learning what to forget, what to
remember, what to pay attention to,

614
00:36:27,690 --> 00:36:30,720
the attention mechanism,
the output, and um,

615
00:36:31,920 --> 00:36:32,260
okay.

616
00:36:32,260 --> 00:36:36,790
The outer level weight
Matrix as well. So, right.

617
00:36:37,960 --> 00:36:41,620
So then we have our activation functions
sigmoid just like before and our

618
00:36:41,621 --> 00:36:42,820
derivative just like before.

619
00:36:42,821 --> 00:36:46,150
And so what we do is we add
another activation function here.

620
00:36:46,420 --> 00:36:48,520
This is good practice in LSTM networks.

621
00:36:48,521 --> 00:36:52,900
You usually see the Tan h function applied
pretty much all the time. And did the,

622
00:36:53,410 --> 00:36:56,590
you might be asking, well, why do we use
10 age over other activation functions?

623
00:36:56,860 --> 00:36:59,520
I've got a great video
on this. It's called, um,

624
00:36:59,620 --> 00:37:01,810
which activation function
should I use search,

625
00:37:01,820 --> 00:37:05,230
which activation function should I
use? CREPE video on this? And you'll,

626
00:37:05,231 --> 00:37:07,900
you'll know it very fast within seven
minutes if you watch that video.

627
00:37:07,901 --> 00:37:11,860
But I'm at a high level, it prevents
the vanishing gradient problem.

628
00:37:12,250 --> 00:37:14,720
The Tan h function gives
us stronger gradients. Uh,

629
00:37:14,850 --> 00:37:18,190
since the data is centered around zero,
as opposed to the sigmoid, which is not.

630
00:37:18,550 --> 00:37:22,490
Um, and so we use that.
And then we also, uh,

631
00:37:22,570 --> 00:37:25,810
are going to import the derivative
function of it because we went to compute

632
00:37:25,811 --> 00:37:29,590
gradients, right? Everything is
differentiable. So just like before,

633
00:37:29,710 --> 00:37:32,440
will compute the for
propagation for an LSTM cell.

634
00:37:32,770 --> 00:37:37,770
And so remember the four propagation
for an Lstm cell is drum roll please.

635
00:37:39,730 --> 00:37:42,430
This set of operations, ultimately, uh,

636
00:37:42,520 --> 00:37:46,690
we want to compute the output,
right? We want to compute the output.

637
00:37:48,600 --> 00:37:50,250
So that's what we'll do

638
00:37:52,930 --> 00:37:54,970
back to this.
So we'll say

639
00:37:56,840 --> 00:38:01,840
the forget gate is going to equal
the input times that forgetting.

640
00:38:01,880 --> 00:38:04,330
So we're going to activate the

641
00:38:08,370 --> 00:38:13,370
dot product of the forget decades and
the input input times for get paid and

642
00:38:13,921 --> 00:38:17,130
then activate.
That's going to give us our forget gates.

643
00:38:17,640 --> 00:38:21,510
And then we're going to compute this,
the cell state. We're gonna, we're gonna,

644
00:38:21,750 --> 00:38:25,830
we're gonna update the cell state by
multiplying it by that for decades.

645
00:38:25,831 --> 00:38:30,450
So it knows what to forget.
And then we're going to compute the input,

646
00:38:30,750 --> 00:38:33,840
which is going to be again,
the activated version of

647
00:38:35,810 --> 00:38:37,590
d,
uh,

648
00:38:38,280 --> 00:38:41,490
input times the previous input,

649
00:38:42,420 --> 00:38:44,490
the current input times or previous input.

650
00:38:45,420 --> 00:38:50,070
And so that's going to tell us that.
And so then

651
00:38:51,970 --> 00:38:53,560
once we've got that,

652
00:38:55,770 --> 00:38:57,000
we can

653
00:39:00,970 --> 00:39:02,380
compute the cell state,

654
00:39:07,510 --> 00:39:10,990
which we're going to apply this new
activation function for the cell states to

655
00:39:10,991 --> 00:39:14,520
prevent the vantage and gradient,
which is the cell stayed Timesi input.

656
00:39:14,530 --> 00:39:17,980
And then we activate bride,
that series of operations.

657
00:39:18,940 --> 00:39:21,220
And then we're going to
update the cell state

658
00:39:25,000 --> 00:39:27,580
by adding the input to times the cell,

659
00:39:27,581 --> 00:39:30,940
state the cell and put times a cell.

660
00:39:31,960 --> 00:39:35,680
And then for a predicted output,
we're going to take those.

661
00:39:35,800 --> 00:39:40,800
We're going to say let's compete the
dot product between the output gates and

662
00:39:40,811 --> 00:39:44,920
the cell state or no,
the applicate and the input

663
00:39:46,620 --> 00:39:51,130
that's going to go to our
predicted output. And so
what we can do is we can say,

664
00:39:51,700 --> 00:39:55,470
oh no, that's going to give us apart how
put gate and to get our actual output,

665
00:39:55,480 --> 00:39:56,380
our predicted output,

666
00:39:56,560 --> 00:40:01,560
we'll we'll multiply our output gate
times the activated version of our self

667
00:40:01,811 --> 00:40:04,690
state and that's going to
give us our predicted output.

668
00:40:06,130 --> 00:40:09,940
We can then return our cell
states are predicted output,

669
00:40:10,120 --> 00:40:14,800
forget gates in implicate cell,
and then our output gates as well.

670
00:40:15,040 --> 00:40:19,000
That's for propagation. All
right, and that's the equation
that I showed up there.

671
00:40:19,240 --> 00:40:21,430
So now we've got our forward propagation.

672
00:40:21,431 --> 00:40:23,320
Now we're going to look at
the backward propagation.

673
00:40:23,321 --> 00:40:26,080
Remember we back propagate
through the self state as well,

674
00:40:26,081 --> 00:40:29,590
not just the higher level
recurrent network. So what this is

675
00:40:31,190 --> 00:40:32,020
okay

676
00:40:32,020 --> 00:40:35,800
is we're going to compute
the the first error, right?

677
00:40:35,801 --> 00:40:39,700
So that's the error plus the hidden
states derivative and we'll clip those

678
00:40:39,701 --> 00:40:42,550
values so it's not too big. We want to
prevent the gradient from vanishing.

679
00:40:42,640 --> 00:40:43,211
So clipping,

680
00:40:43,211 --> 00:40:47,350
it helps that and we'll multiply the air
by the activated cell state to compute

681
00:40:47,351 --> 00:40:51,160
the output derivative and then
will compute the output updates,

682
00:40:51,340 --> 00:40:55,450
which is the output derivatives times
the activated output times the input.

683
00:40:56,170 --> 00:40:58,570
And then we'll compete the
derivative of the cell state,

684
00:40:58,690 --> 00:41:00,730
which is error times output,
time to draw it it.

685
00:41:00,750 --> 00:41:02,680
Oh the cell state plus to derivative cell.

686
00:41:02,890 --> 00:41:07,890
So we're computing derivatives of all of
these components in the backwards order

687
00:41:08,390 --> 00:41:09,223
that's before,

688
00:41:09,730 --> 00:41:13,810
which means that forget the forget gate
computation or update will happen near

689
00:41:13,811 --> 00:41:15,280
the end instead of near the beginning.

690
00:41:15,820 --> 00:41:20,350
And so then we're competing the driven
of the cell and then the cell update,

691
00:41:20,500 --> 00:41:24,100
the derivative of the input, the
input update, the derivative of the,

692
00:41:24,101 --> 00:41:26,500
forget the forget update.
So you see what I'm saying?

693
00:41:26,770 --> 00:41:30,670
We're computing gradients and
then we're updating them, right?

694
00:41:30,700 --> 00:41:35,700
And then derivative of the cell state and
then the driven off the hidden states.

695
00:41:36,170 --> 00:41:40,820
And then finally we can, we can return
all of those great value, these,

696
00:41:40,821 --> 00:41:45,470
those updated grading values for the,
forget the input, the cell, the output,

697
00:41:45,530 --> 00:41:47,420
the self states and the hidden state.

698
00:41:47,600 --> 00:41:50,990
So many different parameters that
we have computed gradings for and

699
00:41:50,991 --> 00:41:54,620
backpropagation is going to let
us compute all of those. Right?

700
00:41:54,621 --> 00:41:56,390
Recursively um,

701
00:41:56,420 --> 00:42:00,380
computing the air with respect to our
weights for every single component we have

702
00:42:00,381 --> 00:42:03,800
in that now work in the reverse order
that we did forward propagation.

703
00:42:04,610 --> 00:42:05,443
And so,

704
00:42:06,350 --> 00:42:06,850
okay.

705
00:42:06,850 --> 00:42:10,960
Yeah. It's just, you know, it's, it's more
matrix math than we did before. It's like,

706
00:42:11,200 --> 00:42:14,110
it's like six or seven more
steps than a recurrent network.

707
00:42:14,290 --> 00:42:16,780
Maybe it's more like eight
or nine more steps, but yeah.

708
00:42:17,550 --> 00:42:18,150
Okay.

709
00:42:18,150 --> 00:42:21,840
And then our, our updates step is
going to just be us updating our,

710
00:42:21,960 --> 00:42:25,920
forget input cell and Alpa gradients
and then we can update our gates using

711
00:42:25,921 --> 00:42:29,640
those grading values and
there are grading values.

712
00:42:31,390 --> 00:42:32,223
Okay.

713
00:42:32,320 --> 00:42:32,561
Okay.

714
00:42:32,561 --> 00:42:37,330
So then that's it for our LSTM cell are
recurring network and now we can look at

715
00:42:37,331 --> 00:42:42,010
our load text and our export text
function, which are not as interesting.

716
00:42:42,011 --> 00:42:46,060
But what we do is we load up
our eminent text file of lyrics,

717
00:42:46,540 --> 00:42:48,190
right?
Just like that.

718
00:42:48,910 --> 00:42:52,600
And then we compute those
unique words for all of them.

719
00:42:52,630 --> 00:42:54,340
And so we have a sequence,
right? We have a sequence,

720
00:42:54,341 --> 00:42:58,330
our input and output sequence,
and then we have this export texts.

721
00:42:58,331 --> 00:43:01,530
So whenever we've sampled new words,
we can write them from memory to disc.

722
00:43:01,540 --> 00:43:04,420
That's all. And so for our
program we'll say, okay,

723
00:43:04,421 --> 00:43:09,250
if so for 5,000 iterations with the
learning rate of 0.01 let's load the input

724
00:43:09,251 --> 00:43:12,700
and our output data and then we'll
initialize a recurrent network using our

725
00:43:12,701 --> 00:43:14,800
hyper parameters that
we've initialized before.

726
00:43:15,070 --> 00:43:18,670
And then for training time for
that given number of iterations,

727
00:43:18,671 --> 00:43:21,850
5,000 we'll say compute
the predicted next words.

728
00:43:21,850 --> 00:43:23,200
So that's the forward propagation.

729
00:43:23,830 --> 00:43:27,350
Then perform backpropagation to
update all our weight values,

730
00:43:27,351 --> 00:43:28,270
use you aren't error.

731
00:43:28,540 --> 00:43:31,570
And then if our air and then just
keep doing them and then this,

732
00:43:31,571 --> 00:43:33,300
this catch or this,
this estate.

733
00:43:33,301 --> 00:43:37,540
And we'll say if our error is small
enough that it's between this range here,

734
00:43:37,840 --> 00:43:42,370
then we can go ahead and sample,
which means predict new words,

735
00:43:42,371 --> 00:43:45,790
generate new words or network
is trained our errors, smallest.

736
00:43:45,880 --> 00:43:48,730
Let's go ahead and now just predict
new words without having to compute

737
00:43:48,750 --> 00:43:52,600
backpropagation to update our weights
are weights are already updated enough.

738
00:43:52,630 --> 00:43:55,780
So then we can go ahead and define a
seed word and then predict some new texts

739
00:43:55,781 --> 00:43:59,320
by calling that sample function, which
is the four propagation. That's all.

740
00:44:00,040 --> 00:44:01,570
And then we can write it all to disk.

741
00:44:02,660 --> 00:44:03,240
Okay.

742
00:44:03,240 --> 00:44:04,440
Right.
So let's run this.

743
00:44:05,780 --> 00:44:06,040
Okay.

744
00:44:06,040 --> 00:44:08,800
It's beginning.
It's going to take a while.

745
00:44:09,970 --> 00:44:10,803
Yeah.

746
00:44:11,140 --> 00:44:15,070
And then once it's done,
I actually have a saved copy.

747
00:44:17,130 --> 00:44:19,200
It's spitting out some pretty dope lyrics,
right?

748
00:44:19,201 --> 00:44:23,190
These are some pretty dope lyrics,
right? That's it for this lesson.

749
00:44:23,370 --> 00:44:26,070
Definitely look at this
Jupiter notebook afterwards.

750
00:44:26,250 --> 00:44:29,760
A look at those links that I have sent
you in the description as well as inside

751
00:44:29,761 --> 00:44:34,410
of the Jupiter Notebook and make
sure that you understand, um,

752
00:44:35,360 --> 00:44:38,370
why,
how at least why to use LSTM cells.

753
00:44:38,670 --> 00:44:42,090
The reason is because it's to
remember longterm dependencies.

754
00:44:42,091 --> 00:44:46,020
That's at least a high level and you
should've got from this video and learns

755
00:44:46,021 --> 00:44:49,380
what to forget, what to remember
and what to pay attention to.

756
00:44:49,381 --> 00:44:53,470
Those are the three things at an LSTM
cell as opposed to a regular recurring

757
00:44:53,471 --> 00:44:56,940
network lets you do. Okay. So yeah,

758
00:44:57,030 --> 00:44:59,460
please subscribe for more
programming videos. And for now,

759
00:44:59,490 --> 00:45:02,670
I've got to learn to forget.
So thanks for watching.

