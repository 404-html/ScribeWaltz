1
00:00:00,060 --> 00:00:01,590
Hello world,
it's a Raj.

2
00:00:01,620 --> 00:00:06,620
And today we're going to build a support
vector machine to classify two classes

3
00:00:07,141 --> 00:00:10,200
of data.
I've already recorded this video before,

4
00:00:10,230 --> 00:00:14,580
but I'm rerecording it because the sound
quality and the video quality was not

5
00:00:14,581 --> 00:00:18,900
good. And we have a standard to
keep on this channel. So listen up.

6
00:00:19,230 --> 00:00:19,980
Here we go.

7
00:00:19,980 --> 00:00:24,000
We're going to build a support vector
machine to classify two classes of data,

8
00:00:24,090 --> 00:00:28,050
right? And the way we're going to
optimize the support vector machine,

9
00:00:28,230 --> 00:00:32,490
this type of machine learning model
is to use gradient descent. Okay?

10
00:00:32,491 --> 00:00:36,600
So that's what we're going to do.
And this is what it looks like it,

11
00:00:36,690 --> 00:00:38,760
it looks like this.
So say we've got two classes.

12
00:00:38,761 --> 00:00:43,560
We have one class that's going to be
denoted by red dots and the other class is

13
00:00:43,561 --> 00:00:45,390
going to be to noted by blue dots.

14
00:00:45,510 --> 00:00:48,840
So if we were plucked both
classes on a two d graph,

15
00:00:48,990 --> 00:00:53,520
an x y graph,
then we could draw a line,

16
00:00:53,700 --> 00:00:58,410
a decision boundary that best
separates both of these classes.

17
00:00:58,650 --> 00:01:01,200
And that line is called a hyperplane.

18
00:01:01,350 --> 00:01:04,620
And our support vector machine
helps us create it. Okay?

19
00:01:04,621 --> 00:01:09,060
That's what are support vector machine
helps us do and we're going to talk about

20
00:01:09,061 --> 00:01:10,590
all the details of how this thing works,

21
00:01:10,591 --> 00:01:14,610
but this is at a high level what it looks
like and we're going to build it with

22
00:01:14,670 --> 00:01:18,810
just num Pi and nat plot
libe okay to graph it.

23
00:01:18,840 --> 00:01:21,180
So no tensorflow or any of that.
All right,

24
00:01:21,181 --> 00:01:25,380
so I hope you're as excited as I am for
this because we are going to go into the

25
00:01:25,381 --> 00:01:29,310
theory as well as the code
as well. Okay, here we go.

26
00:01:29,460 --> 00:01:32,940
So to start off with,
what are some use cases for an Svm?

27
00:01:33,000 --> 00:01:37,050
So classification is one which we're
going to do and that is in fact the main

28
00:01:37,051 --> 00:01:40,710
case that svms are used for.
That is the most popular case,

29
00:01:40,950 --> 00:01:45,950
but they can also be used for other
types of machine learning problems.

30
00:01:46,080 --> 00:01:46,913
Regression,

31
00:01:46,950 --> 00:01:50,970
that is if we have some set of data points
and we're trying to predict the next

32
00:01:51,120 --> 00:01:54,930
point in that set of data. So to stock
prediction would be a good example.

33
00:01:55,320 --> 00:01:58,680
And also type Oracle also
called time series prediction.

34
00:01:59,100 --> 00:02:00,660
There's also outlier detection.

35
00:02:00,690 --> 00:02:05,040
So say you have a security system and
you're tracking all of your users and all

36
00:02:05,041 --> 00:02:09,240
of your users have a set of metrics I
set of features that identifies them like

37
00:02:09,330 --> 00:02:12,060
the time that they've logged
in and what they're doing.

38
00:02:12,390 --> 00:02:15,210
There could be an anomaly and
you anomaly could be or bad guy,

39
00:02:15,240 --> 00:02:19,950
the guy who's trying to break into your
security system and the spms help you

40
00:02:19,951 --> 00:02:21,690
detect who that person is.

41
00:02:21,920 --> 00:02:24,690
And there's also clustering but
we're not going to talk about which,

42
00:02:24,691 --> 00:02:26,300
which is also um,

43
00:02:26,530 --> 00:02:30,510
which is a form of unsupervised learning
learning as his outlier detection.

44
00:02:30,750 --> 00:02:33,360
But we are going to do
supervised classification.

45
00:02:33,360 --> 00:02:35,910
That is when our data has labels,
right?

46
00:02:35,911 --> 00:02:39,690
We are trying to learn the mapping
between the labels and the data.

47
00:02:39,780 --> 00:02:42,390
And if we learn the mapping,
that is a function.

48
00:02:42,450 --> 00:02:46,110
The function represents the mapping,
the relationship between these variables.

49
00:02:46,410 --> 00:02:50,490
If we learn this function, then where
machine learning model has done its job,

50
00:02:50,610 --> 00:02:55,610
then we can use this function to plug
in some new input data and it's going to

51
00:02:55,651 --> 00:02:59,710
output the prediction, right? And
this is all of machine really.

52
00:03:00,880 --> 00:03:04,390
So let's look at the toe. In this example,
we're going to use toy data, right?

53
00:03:04,391 --> 00:03:06,490
Because it's more, it's, it's about the,

54
00:03:07,480 --> 00:03:09,460
it's about the math and the algorithms.

55
00:03:09,461 --> 00:03:13,270
But I have these two other examples
for you just in case you want to do

56
00:03:13,271 --> 00:03:15,130
something a little more useful.

57
00:03:15,400 --> 00:03:18,820
So the first one is for handwritten
digit classification, right?

58
00:03:18,821 --> 00:03:23,020
You have are we have a set of digits and
they all had their labels and we want

59
00:03:23,021 --> 00:03:26,860
to learn the mapping between
the labels and the digits.

60
00:03:27,040 --> 00:03:30,160
And so that's what this
repository will help us do.

61
00:03:30,430 --> 00:03:34,330
And the great thing about this
repository is it's using psychic learn,

62
00:03:34,331 --> 00:03:38,290
which is a very popular machine
learning library. In one line of code,

63
00:03:38,410 --> 00:03:40,000
you could implement an Svn,

64
00:03:40,240 --> 00:03:43,540
but we're going to implement it from
scratch because we want to learn how this

65
00:03:43,541 --> 00:03:45,580
thing works.
But once you've done that,

66
00:03:45,670 --> 00:03:49,900
then you can go on to using something
easier, something simpler like this. Okay?

67
00:03:49,901 --> 00:03:53,860
And it's a real world use case.
I've got one more example here that I've,

68
00:03:53,890 --> 00:03:57,040
that I have for you guys.
And that's where pulse classification.

69
00:03:57,340 --> 00:03:59,710
So the idea is that for a human given,

70
00:03:59,711 --> 00:04:03,100
some metrics like their age and their eye,
what was the other one?

71
00:04:03,101 --> 00:04:07,330
Their pulse rate. Obviously we can
predict what their emotions will be.

72
00:04:07,331 --> 00:04:12,331
So it's emotion classification and we're
using an SVM in this repository as well

73
00:04:12,670 --> 00:04:15,160
as well as psych yet learned
to implement that STM.

74
00:04:15,400 --> 00:04:19,630
So check out those two repositories once
you really understand the math behind

75
00:04:19,631 --> 00:04:24,130
support vector machines from this video
and from the associated code. Okay. So,

76
00:04:24,640 --> 00:04:26,590
uh, yeah, so those are two other examples.

77
00:04:26,620 --> 00:04:30,010
So how does this thing compare to the
other machine learning algorithms?

78
00:04:30,011 --> 00:04:34,090
And there are so many of them.
There are random forests,

79
00:04:34,120 --> 00:04:37,810
there are neural networks.
Well, as a rule of thumb,

80
00:04:37,900 --> 00:04:41,280
svms are great if you have small datasets.

81
00:04:41,290 --> 00:04:45,220
So I'm saying like a thousand
rows or less of data, right?

82
00:04:45,221 --> 00:04:49,090
1,000 data points or less.
If we have that,

83
00:04:49,091 --> 00:04:53,290
then svms are great for classification
and they are very popular.

84
00:04:54,490 --> 00:04:58,990
However, other algorithms, random
forests, deep neural networks,

85
00:04:58,991 --> 00:05:00,760
et Cetera,
require more data,

86
00:05:00,790 --> 00:05:04,540
but almost always come up
with a very robust model.

87
00:05:05,140 --> 00:05:05,973
And the decision of,

88
00:05:06,130 --> 00:05:10,450
of which classifier to use depends
on both your problem and your data.

89
00:05:10,630 --> 00:05:13,300
And as you build this
mathematical intuition,

90
00:05:13,360 --> 00:05:15,940
all of these choices will
become very clear to you.

91
00:05:16,210 --> 00:05:18,790
So we're starting off with the
support vector machine. Okay?

92
00:05:19,270 --> 00:05:23,110
And so I also have this quote by this
famous a computer science professor,

93
00:05:23,111 --> 00:05:24,190
Donald Knuth,

94
00:05:24,460 --> 00:05:28,240
who now I know the chaos silent
Donald Knuth who said that premature

95
00:05:28,241 --> 00:05:33,241
optimization is the root of all evil
or at least most of it in programming.

96
00:05:33,460 --> 00:05:34,480
And what does he mean by that?

97
00:05:34,870 --> 00:05:38,860
That means if you can make a performance
gain by using something way more

98
00:05:38,861 --> 00:05:41,170
complicated,
like a deep neural network,

99
00:05:41,410 --> 00:05:46,410
but it's only going to be like it's
only going to be by like 0.1% then it's

100
00:05:46,541 --> 00:05:47,410
unnecessary.

101
00:05:47,680 --> 00:05:51,880
Your time as a programmer is very
valuable and you will only want to do the

102
00:05:51,881 --> 00:05:55,510
minimum amount of work that you have to,
to get the results that you want.

103
00:05:55,720 --> 00:05:59,870
So if you're trying to use a neural
network for problem that only requires

104
00:05:59,871 --> 00:06:02,270
something very simple,
like a support vector machine,

105
00:06:02,510 --> 00:06:05,690
you should use the support vector machine,
not the deep neural network,

106
00:06:05,691 --> 00:06:09,290
just because it's hot and
it's outperforming everything
else almost all the time

107
00:06:09,560 --> 00:06:13,350
because it requires more
computing data and way more, uh,

108
00:06:14,300 --> 00:06:18,980
it requires more computing power and
way more data, right? Two things, um,

109
00:06:19,100 --> 00:06:22,910
which if you don't have, don't use it,
use a support vector machine, right?

110
00:06:22,911 --> 00:06:24,760
These are all methods of intelligence.

111
00:06:24,980 --> 00:06:28,100
This is all about the
math of intelligence.

112
00:06:28,280 --> 00:06:30,080
And there are many ways to approach it.

113
00:06:31,430 --> 00:06:33,680
So what is a support vector machine?

114
00:06:33,740 --> 00:06:37,550
So this thing can be used
for both classification is
that this is it, this is it.

115
00:06:37,551 --> 00:06:41,630
This and regression got these points.
What's the next point in a series?

116
00:06:42,020 --> 00:06:45,080
So given two or more
labeled classes of data,

117
00:06:45,110 --> 00:06:47,390
remember we are using supervised learning.

118
00:06:47,960 --> 00:06:52,340
It can create a discriminative
classifier that is a classifier that can

119
00:06:52,341 --> 00:06:56,660
discriminate between different classes.
Is it this, is it this is it this?

120
00:06:56,990 --> 00:07:00,980
Right? And the opposite to
discriminative, by the way, is generative.

121
00:07:01,010 --> 00:07:02,630
Where we generate new data.

122
00:07:02,750 --> 00:07:07,400
We take some training data variated very
it in some way using our model and that

123
00:07:07,430 --> 00:07:11,000
output is very similar to the
train data but it's novel data.

124
00:07:11,210 --> 00:07:12,590
But that's for later on anyway.

125
00:07:12,740 --> 00:07:17,180
So the way we build this hyperplane
and we'll talk about that term,

126
00:07:17,270 --> 00:07:19,430
but the way we build
this hyper plane or line,

127
00:07:19,431 --> 00:07:24,431
this decision boundary between the classes
is by maximizing the margin that is

128
00:07:24,621 --> 00:07:28,520
the space between that line
and both of those classes.

129
00:07:28,880 --> 00:07:31,460
What do I mean by that?
When I say both of those classes,

130
00:07:31,461 --> 00:07:35,990
what I actually mean are the points in
each check out this image or the points

131
00:07:35,991 --> 00:07:40,991
and each of those classes that are
closest to the decision boundary and these

132
00:07:41,061 --> 00:07:44,390
points are called support vectors.
Okay.

133
00:07:44,391 --> 00:07:46,940
We call them support vectors
because they are vectors.

134
00:07:46,941 --> 00:07:51,941
They are data point vectors that support
the creation of this hyperplane that

135
00:07:52,191 --> 00:07:56,030
are support vector machine.
We'll create, right?

136
00:07:56,031 --> 00:07:58,670
So we are maximizing the margin.
And why do we do that?

137
00:07:58,671 --> 00:08:03,290
Because we want to draw a line
that is in the absolute perfectly,

138
00:08:03,350 --> 00:08:08,210
that perfect middle spot between
both of these sets of data such as,

139
00:08:08,211 --> 00:08:11,160
so that when we plot a new data point,
if,

140
00:08:11,210 --> 00:08:13,310
if it is of a certain class,

141
00:08:13,490 --> 00:08:18,440
it will have the maximum likelihood of
falling on that side of the decision

142
00:08:18,441 --> 00:08:22,070
boundary where it should.
And the only way to do that,

143
00:08:22,071 --> 00:08:27,071
to maximize the space with which a new
data point can fall into its correct

144
00:08:27,861 --> 00:08:32,861
class category is to maximize the space
between data points and put a line right

145
00:08:33,231 --> 00:08:37,010
in the middle of that space. You see
what I'm saying? I can't get feedback,

146
00:08:37,011 --> 00:08:41,360
but I'm just going to assume that, that,
that that was intuitive, right? So right,

147
00:08:41,361 --> 00:08:42,440
so small margin,

148
00:08:42,530 --> 00:08:46,280
we're maximizing the margin and
we're Tryna draw a decision boundary,

149
00:08:46,281 --> 00:08:48,440
a line of best,
not a line of best fit,

150
00:08:48,441 --> 00:08:51,980
but align of best classification
between both of those.

151
00:08:52,250 --> 00:08:57,120
And we call this line a hyperplane.
Okay. So what is a hyperplane?

152
00:08:57,360 --> 00:09:02,310
Well, I hyperplane is a decision
surface. So given end dimensions, right?

153
00:09:02,311 --> 00:09:04,200
So let's say our data is end dimensional,

154
00:09:04,201 --> 00:09:09,201
where n is the number of features that
you have length with high tongue color,

155
00:09:09,630 --> 00:09:11,460
tongue color.
Where did that come from?

156
00:09:11,790 --> 00:09:16,380
Tongue color and skin color and
whatever other colors. And so,

157
00:09:17,010 --> 00:09:17,843
um,

158
00:09:19,410 --> 00:09:22,230
a hyperplane is n minus one dimensions.

159
00:09:22,260 --> 00:09:26,280
So if you have a two dimensional graph
where, just like this on the left,

160
00:09:26,281 --> 00:09:28,470
right here where I'm pointing my mouse,
we're with this,

161
00:09:28,471 --> 00:09:32,250
our symbol with a two exponent
denotes a two dimensional graph.

162
00:09:32,550 --> 00:09:37,290
A hyperplane would then be two minus one,
right? And minus one. So one dimension.

163
00:09:37,440 --> 00:09:40,120
So it would be a lie, right? But if,

164
00:09:40,150 --> 00:09:43,530
if we are in three dimensional
space or are two the three,

165
00:09:43,770 --> 00:09:47,580
then a hyperplane is going to be two
dimensional because it's three minus one,

166
00:09:47,610 --> 00:09:51,270
which is to, right? So we
have a plane and so you,

167
00:09:51,271 --> 00:09:56,160
we can extrapolate this to
many dimensions. So if we
had a 400 dimensional space,

168
00:09:56,340 --> 00:09:58,080
which we often do in machine learning,

169
00:09:58,081 --> 00:10:02,370
our data doesn't just have two
or three features. It has many,

170
00:10:02,520 --> 00:10:06,180
many features, right? It's not so
neatly packaged for us to visualize.

171
00:10:06,360 --> 00:10:09,090
And that's where techniques
like dimensionality reduction
and all this come into

172
00:10:09,091 --> 00:10:10,530
play,
which we'll talk about.

173
00:10:10,830 --> 00:10:14,430
But right now if we have
a 400 dimensional space,

174
00:10:14,580 --> 00:10:19,530
a graph of points than a hybrid
plan would be 399 dimensions,

175
00:10:19,710 --> 00:10:22,650
which we can't really visualize.
I mean, think about it, humans,

176
00:10:22,651 --> 00:10:27,651
we are not that good at visualizing
or in fact it's impossible for us to

177
00:10:27,841 --> 00:10:31,050
visualize anything and
more than three dimensions.

178
00:10:31,230 --> 00:10:36,180
But for machines it's very easy. It's very
intuitive and that's all that matters.

179
00:10:36,330 --> 00:10:41,130
As long as our machine is able to draw
this decision boundary of n minus one

180
00:10:41,131 --> 00:10:43,950
dimensions,
then given some new data point,

181
00:10:44,130 --> 00:10:48,670
if we put it into that model, that
functions, that functions, um,

182
00:10:49,170 --> 00:10:50,370
if we put it into that function,

183
00:10:50,371 --> 00:10:54,330
if we plug it in and it's going to output
the correct class of whatever it is.

184
00:10:55,140 --> 00:10:57,540
Okay, so, and this actually
comes from geometry.

185
00:10:57,541 --> 00:11:00,990
So I guess there is a little bit
of geometry in machine learning,

186
00:11:02,820 --> 00:11:05,460
right? So nonlinear versus linear. Well,

187
00:11:05,461 --> 00:11:10,461
we're only going to talk about linear
classification because nonlinear

188
00:11:10,831 --> 00:11:13,140
classification is more
complicated and we'll get to that.

189
00:11:13,141 --> 00:11:17,250
But the idea here is that
let's say you have some, uh,

190
00:11:17,670 --> 00:11:21,870
some sets of data, some,
some two datasets, right? Two
classes of data, the best,

191
00:11:21,900 --> 00:11:23,030
let's say the best,
uh,

192
00:11:23,040 --> 00:11:27,300
the line that best separates these
two classes of data isn't linear.

193
00:11:27,420 --> 00:11:30,720
Let's say it's got curves like in this,
in this example right here,

194
00:11:30,900 --> 00:11:35,030
how are we supposed to build a
hyperplane like that? It would, well,

195
00:11:35,040 --> 00:11:39,240
it would be more complicated and there
is actually a trick to do this called the

196
00:11:39,241 --> 00:11:43,050
kernel trick and we'll talk about that
later. But there's a way to take that map,

197
00:11:43,051 --> 00:11:47,640
that input space into a feature space
such that the hybrid plan that you draw is

198
00:11:47,641 --> 00:11:49,650
linear even though it
wouldn't be otherwise.

199
00:11:49,860 --> 00:11:53,500
And so that's called the kernel trick
and we'll talk about that later, right?

200
00:11:53,501 --> 00:11:58,501
So we're only talking about
linear classification for
support vector machines,

201
00:11:59,050 --> 00:12:03,760
supervised linear classification, right?
As opposed to unsupervised. Anyway,

202
00:12:03,761 --> 00:12:06,070
there's so many different ways that
we can frame this problem, right?

203
00:12:06,071 --> 00:12:10,450
There's so many different ways we can
frame the learning process and more will

204
00:12:10,451 --> 00:12:15,370
be discovered. He was a very exciting time
to be in this field. Okay. So let's get,

205
00:12:15,371 --> 00:12:18,730
let's go ahead and uh, get to
building shall we? But first of all,

206
00:12:18,731 --> 00:12:20,740
I also want to say a one more thing.

207
00:12:21,580 --> 00:12:24,280
So no matter what model you're using,

208
00:12:24,281 --> 00:12:27,490
a random forests and support vector
machine, a deep neural network,

209
00:12:27,640 --> 00:12:32,290
in the end we are approximating, we
are guessing, uh, iteratively close.

210
00:12:32,350 --> 00:12:34,430
We are getting,
we're educated guests.

211
00:12:34,431 --> 00:12:36,460
I'm trying to think of a
different word for approximation,

212
00:12:36,640 --> 00:12:40,720
but we are approximating a function,
right? We are trying to find what is the,

213
00:12:40,780 --> 00:12:44,620
what is that optimal function and that
function represents the relationship

214
00:12:44,710 --> 00:12:48,250
between all the variables in our
data, right? That function is,

215
00:12:48,251 --> 00:12:50,050
that is that is that relationship.

216
00:12:50,051 --> 00:12:54,580
It's that mapping and if we can find
that function then we have learned,

217
00:12:55,030 --> 00:12:58,720
we have learned from our data and so every
machine learning model under the hood

218
00:12:58,840 --> 00:13:03,070
is just a function that we are trying to
approximate and it's coefficients are,

219
00:13:03,071 --> 00:13:05,980
it's weights and they are
being updated over time,

220
00:13:06,100 --> 00:13:07,840
through some optimization technique.

221
00:13:07,930 --> 00:13:11,080
Be that gradient descent
usually or Newton's method,

222
00:13:11,081 --> 00:13:14,650
which we'll learn about or you know,
whatever it is. Okay. So, yeah,

223
00:13:15,010 --> 00:13:18,370
so whatever it is, we're just trying
to approximate a function, right?

224
00:13:18,371 --> 00:13:22,270
This is a way of thinking approximating
a function, whatever we're using,

225
00:13:22,720 --> 00:13:26,740
decision forest, whatever we're using,
it's all about approximating a function.

226
00:13:26,741 --> 00:13:31,030
Decision. Trees. All right,
so let's go ahead and get and
get to building, right? So

227
00:13:33,160 --> 00:13:35,440
first we're going to uh,

228
00:13:35,500 --> 00:13:40,360
important num pi and so num Pi is going
to help us perform math operations,

229
00:13:40,361 --> 00:13:44,620
matrix math and then we're going to
plot our data using map plot line. Okay,

230
00:13:44,740 --> 00:13:47,560
so this first step is for
us to define our data.

231
00:13:47,710 --> 00:13:51,970
So our data is going to be of this form x,
y bias.

232
00:13:52,180 --> 00:13:55,630
So the first, so they're five
data points here, right there,

233
00:13:55,631 --> 00:13:59,320
five data points and we've got the x
coordinate the y coordinate and we've just

234
00:13:59,350 --> 00:14:03,160
input our bias into our data to make
things easier later on. But we can,

235
00:14:03,161 --> 00:14:06,130
for all intensive purposes,
ignore this bias term,

236
00:14:06,430 --> 00:14:10,300
but we are basically just
ha we had these set of x,

237
00:14:10,301 --> 00:14:14,920
y coordinate Paris that we can plot on a
graph and each of these data points has

238
00:14:14,921 --> 00:14:17,200
an associated label,
an output label.

239
00:14:17,470 --> 00:14:22,030
That output label is either a
negative one or a one. Okay.

240
00:14:22,150 --> 00:14:24,250
So for the first two,
they're going to be negative one.

241
00:14:24,460 --> 00:14:27,130
And for the last three they're going
to be one. So these last three,

242
00:14:27,490 --> 00:14:32,140
so what we can do is we can plot
these examples on a two d graph.

243
00:14:32,470 --> 00:14:35,020
Okay? So we can say, let's plot the,

244
00:14:35,230 --> 00:14:39,790
let's plot the first two with the negative
marker and let's plot the last three

245
00:14:39,820 --> 00:14:43,900
with the positive marker. Okay? And
so when we plot, it looks like this.

246
00:14:45,130 --> 00:14:48,060
And what we're also going to
do is we're going to print as,

247
00:14:48,160 --> 00:14:52,640
we're going to plot a possible hyperplane
that is a hyperplane that is just the

248
00:14:52,641 --> 00:14:55,580
line. And we don't know,
it's just our naive guests.

249
00:14:55,581 --> 00:14:58,640
We don't know if it's the optimal
hyperplane. In fact it's not,

250
00:14:58,880 --> 00:15:02,990
but it just so happens to perfectly
separate our training data classes.

251
00:15:02,990 --> 00:15:05,150
Just so for us to just see
what it looks like, right?

252
00:15:05,151 --> 00:15:09,230
This is just for that
example. Okay, so that's that.

253
00:15:09,231 --> 00:15:12,830
So now what we can do is get into the
math. So I hope you're ready for this.

254
00:15:13,100 --> 00:15:15,470
All right, so let's get into
our calculus. All right,

255
00:15:15,920 --> 00:15:19,850
so right machine learning machine
learning is all about optimizing for an

256
00:15:19,851 --> 00:15:21,020
objective function.

257
00:15:21,230 --> 00:15:26,230
And the way we optimize for an objective
function is by minimizing a hope.

258
00:15:26,421 --> 00:15:29,450
You said loss or error function,
because that is the correct answer.

259
00:15:29,810 --> 00:15:32,330
We are minimizing a loss for air function.

260
00:15:32,540 --> 00:15:35,240
So let's go ahead and first
define our loss function.

261
00:15:35,570 --> 00:15:39,200
Our loss function in this case is
going to be called the hinge loss.

262
00:15:39,470 --> 00:15:44,150
So the hinge loss is a very popular
type of loss function for support vector

263
00:15:44,151 --> 00:15:45,620
machines.
Okay.

264
00:15:45,621 --> 00:15:50,390
And the class of algorithms that support
vector machines fall under our maximum

265
00:15:50,391 --> 00:15:52,820
margin classification algorithms,
right?

266
00:15:52,820 --> 00:15:57,350
We are trying to maximize the margin
that is the distance between classes such

267
00:15:57,351 --> 00:15:59,020
that we can draw the best,
uh,

268
00:15:59,210 --> 00:16:02,990
decision boundary between those classes
that best separates both of those

269
00:16:02,991 --> 00:16:05,870
classes. Okay, so this
is what it looks like.

270
00:16:05,871 --> 00:16:07,550
This is what the hinge loss looks like.

271
00:16:07,730 --> 00:16:09,770
The hinge loss looks like
this with the word. See,

272
00:16:09,771 --> 00:16:14,480
that's how we did note the hinge loss.
Given these three terms,

273
00:16:14,690 --> 00:16:18,020
the three terms are going
to be x, Y, and f of x,

274
00:16:18,140 --> 00:16:20,060
where x is the sample data.

275
00:16:20,570 --> 00:16:25,220
Why is the true label and f
of x is the predicted label,

276
00:16:25,310 --> 00:16:30,200
right? So it's going to be
one minus y times F of x,

277
00:16:30,470 --> 00:16:34,190
and this little plus sign down here
just means that if this result,

278
00:16:34,220 --> 00:16:38,450
if the result of this op,
these sets of operations is negative,

279
00:16:38,630 --> 00:16:42,770
then we're going to just set it to zero
because we always want the results to be

280
00:16:42,800 --> 00:16:44,030
positive,
okay?

281
00:16:44,030 --> 00:16:48,620
So what this means is that we can break
this down into this equation right here

282
00:16:48,810 --> 00:16:49,643
where my,

283
00:16:49,790 --> 00:16:54,790
where my mouse is now over where we can
say if y times F of x is greater than or

284
00:16:55,581 --> 00:16:56,414
equal to one,

285
00:16:56,420 --> 00:16:59,870
which would make this come out to be one
minus a number that's greater than one,

286
00:17:00,020 --> 00:17:01,820
which would be zero or a negative number,

287
00:17:02,030 --> 00:17:06,560
then set the result to zero because we
want it to be positive. And if it's not,

288
00:17:06,620 --> 00:17:10,700
then it's going to be some non negative
number greater than zero. Okay?

289
00:17:10,701 --> 00:17:14,480
So that's our loss. That's how we
define our loss. And remember these,

290
00:17:14,930 --> 00:17:19,190
this why and this f of x. Both of
these values, the scalar values,

291
00:17:19,191 --> 00:17:22,670
these single values are going to be a
single number, right? And that's what,

292
00:17:22,700 --> 00:17:24,020
that's why we can multiply them.

293
00:17:25,010 --> 00:17:29,270
And so our objective function then is
going to consist of the loss function,

294
00:17:29,420 --> 00:17:31,790
which noticed how it looks a
little different, but it's,

295
00:17:31,970 --> 00:17:35,930
it's really the same thing. This
one minus y times x, uh, XW.

296
00:17:35,931 --> 00:17:39,500
It's the same as this lost up here.
It's just a different way of denoting it.

297
00:17:41,120 --> 00:17:45,080
And we can say the sigma term means that
we are we to take a sum of terms where

298
00:17:45,081 --> 00:17:49,230
the number of terms is n and n is the
number of data points that we have.

299
00:17:49,350 --> 00:17:50,820
So for all five data points,

300
00:17:50,940 --> 00:17:54,660
we'll find the loss of each of those data
points using this, this loss function,

301
00:17:54,661 --> 00:17:57,990
the hinge loss,
and we'll sum them all up together.

302
00:17:58,110 --> 00:18:02,970
And that that total sum will
represent our total loss for our data,

303
00:18:03,090 --> 00:18:06,630
right? That's a single number.
It's going to be a single number.

304
00:18:07,230 --> 00:18:10,230
And then once we have that, we're
going to define our objective function.

305
00:18:10,231 --> 00:18:12,330
So our objective function,
in this case,

306
00:18:12,570 --> 00:18:17,370
it's going to be denoted by this men
lambda w okay, with the square sign.

307
00:18:17,550 --> 00:18:18,740
And so what is this?
All right,

308
00:18:18,810 --> 00:18:22,590
so our objective function is going to
be denoted by the loss plus this regular

309
00:18:22,591 --> 00:18:25,530
riser term, which is denoted right
here with this men and the lambda.

310
00:18:25,830 --> 00:18:28,830
So a regular riser is,
is a tuning knob.

311
00:18:28,831 --> 00:18:31,920
And what the regular
riser does is it tells us

312
00:18:33,630 --> 00:18:37,590
how best to fit our data.
So if the regular riser term is too high,

313
00:18:37,800 --> 00:18:40,560
then our model will be over
fit to the training data.

314
00:18:40,740 --> 00:18:43,560
And it's not going to generalize
well to new data points.

315
00:18:43,590 --> 00:18:47,580
It's going to be over fit,
but at the regularize or term is too low,

316
00:18:47,730 --> 00:18:49,500
then our model is going to be under fit.

317
00:18:49,590 --> 00:18:53,490
So that means going to be to generalize
and it will have your large training

318
00:18:53,491 --> 00:18:53,940
error.

319
00:18:53,940 --> 00:18:58,590
So we need the perfect regularize
or term two for our model to be as

320
00:18:58,591 --> 00:19:03,240
generalizable as possible and fit to our
training data. It's that balance term,

321
00:19:03,420 --> 00:19:07,200
right? And it says it's also, it's
also comes out to be a single scalar.

322
00:19:07,230 --> 00:19:10,650
So given our weights are we square
that? Um, and then we use this lamp,

323
00:19:10,710 --> 00:19:15,360
lend the term here. Uh, we multiply
by the, by this slime to term. Okay.

324
00:19:15,360 --> 00:19:17,010
So, um, right?

325
00:19:17,011 --> 00:19:20,850
So that's our objective function or
objective function consists of our regular

326
00:19:20,851 --> 00:19:24,630
riser and our loss function.
We add them both together.

327
00:19:24,930 --> 00:19:28,530
So what we wanna do is we want
to optimize for this objective.

328
00:19:28,710 --> 00:19:30,720
And by optimizing for this objective,

329
00:19:30,870 --> 00:19:35,010
we're going to find the optimal regularize
their term, and we're going to, uh,

330
00:19:35,011 --> 00:19:36,180
minimize a loss.

331
00:19:36,270 --> 00:19:39,330
So we're going to do two things
by optimizing for this objective.

332
00:19:39,840 --> 00:19:44,840
And so the way we're going to optimize
is we're going to perform gradient

333
00:19:45,091 --> 00:19:46,260
descent,
right?

334
00:19:46,261 --> 00:19:50,580
And so the way we're going to perform
gradient descent is by taking the partial

335
00:19:50,581 --> 00:19:54,180
derivative of both of these two
terms of both of these terms.

336
00:19:54,450 --> 00:19:57,330
We're going to take the partial
derivative of the regular riser,

337
00:19:57,750 --> 00:20:02,670
and we're going to take the partial
derivative of the, um, of the loss term.

338
00:20:02,700 --> 00:20:04,230
Okay? So this is what
it looks like, right?

339
00:20:04,231 --> 00:20:07,020
So remember from the power
rule for partial derivatives,

340
00:20:09,670 --> 00:20:14,670
all we have to do is move the power to
the coefficient and then subtract one

341
00:20:14,681 --> 00:20:19,360
from the coefficient.
And then for the other term we do the,

342
00:20:19,840 --> 00:20:20,740
we do the same thing.

343
00:20:20,770 --> 00:20:25,450
And so for the last term we do the same
thing and it comes out to this and so

344
00:20:25,480 --> 00:20:29,770
it's, it's going to be zero or it's going
to be negative y times x. So there's a,

345
00:20:29,771 --> 00:20:31,090
there's a case for both of them.

346
00:20:33,020 --> 00:20:33,290
Okay.

347
00:20:33,290 --> 00:20:37,640
Okay. So then what we, what we then
have is a misclassification condition.

348
00:20:37,820 --> 00:20:40,790
Any classification condition.
So we can, so we can,

349
00:20:40,791 --> 00:20:44,900
so basically we can say
if it's misclassified,

350
00:20:45,050 --> 00:20:48,820
so if we miss classify our data
depending on these partial derivatives,

351
00:20:49,120 --> 00:20:51,700
then we can update our weights
a certain way. And what I'm,

352
00:20:51,730 --> 00:20:52,930
what do I mean by certain way?

353
00:20:53,110 --> 00:20:57,490
I mean we can update our weights by
using both the regular riser term and the

354
00:20:57,491 --> 00:21:00,310
loss function term. Okay.
Because this isn't zero,

355
00:21:00,311 --> 00:21:05,230
it's going to be negative y times
x, but else, yeah. If, if we,

356
00:21:05,231 --> 00:21:09,100
if we have correctly classified,
then this value is going to be zero.

357
00:21:09,430 --> 00:21:12,340
It's going to be a zero.
So we don't need to update our loss.

358
00:21:12,430 --> 00:21:17,260
We only update our reg. We only update
our weights using our regular riser term.

359
00:21:17,470 --> 00:21:19,600
And so this term right here is
a learning rate, by the way.

360
00:21:19,600 --> 00:21:23,350
So it's weights plus learning
rate times the regular riser term.

361
00:21:23,710 --> 00:21:27,100
The learning rate, by the
way, is how we, um, it's our,

362
00:21:27,160 --> 00:21:31,030
is another tuning knob for
how we, uh, how fast we learn.

363
00:21:31,031 --> 00:21:34,630
So if the learning rate is too high, as
our model is learning, it could just,

364
00:21:35,200 --> 00:21:37,960
it could just miss, it could just
overshoot that minimum entirely.

365
00:21:37,961 --> 00:21:40,000
You could just keep going,
but if it's too low,

366
00:21:40,090 --> 00:21:44,140
you can take way too long to converge,
or in fact, a good, just never converge.

367
00:21:44,350 --> 00:21:47,890
So we want to have that
optimal learning rate. Okay?

368
00:21:47,891 --> 00:21:51,490
So those are our terms.
And so now,

369
00:21:53,930 --> 00:21:54,763
mmm,

370
00:21:56,210 --> 00:21:59,150
now let me plug into some
power here. Okay, so then,

371
00:21:59,151 --> 00:22:02,330
so now let's get into the code for this,
right? We've talked about the math.

372
00:22:02,510 --> 00:22:06,830
Let's get into the code. So for the
code part, we can say, all right, well,

373
00:22:06,831 --> 00:22:09,230
we want to initialize a
support vector machine.

374
00:22:09,260 --> 00:22:12,260
We're going to perform stochastic
gradient descent, by the way,

375
00:22:12,860 --> 00:22:17,210
but we're going to initialize a support
vector machine with a set of wage

376
00:22:17,211 --> 00:22:20,630
factors and he's weight factors are
the coefficient of the model that we're

377
00:22:20,631 --> 00:22:21,770
trying to approximate.

378
00:22:21,771 --> 00:22:25,430
There are three values that we
initialized with a set of Zeros.

379
00:22:25,850 --> 00:22:30,070
Then we have a learning rates,
which has won a number of epox,

380
00:22:30,071 --> 00:22:34,190
which is a number just iterations to
train for over the Dataset over the entire

381
00:22:34,200 --> 00:22:35,033
Dataset.

382
00:22:35,420 --> 00:22:40,010
And then a list of errors we're going
to store all of our errors in. Okay.

383
00:22:40,011 --> 00:22:43,280
So basically we could say,
and now here's the machine learning part.

384
00:22:43,300 --> 00:22:45,650
Here's all of that math that we just did.

385
00:22:45,830 --> 00:22:48,890
We can fit into these 10 lines of code,
right?

386
00:22:48,891 --> 00:22:52,640
We are literally just
taking those equations and
converting them into code right

387
00:22:52,641 --> 00:22:56,860
now.
So we'll say for the number of epochs for,

388
00:22:56,870 --> 00:23:01,870
so for 100,000 times we'll set up an
error to initially zero let's iterate

389
00:23:02,091 --> 00:23:05,810
through every single data point that
we have. So for all five data points,

390
00:23:06,080 --> 00:23:07,190
so we have five data points.

391
00:23:07,191 --> 00:23:11,690
We're going to iterate through
all of them 100,000 times. Okay.

392
00:23:11,691 --> 00:23:16,130
And so then we have our first case,
which is the misclassification case.

393
00:23:16,280 --> 00:23:19,820
So in the misclassification case,
let's see what that was.

394
00:23:20,090 --> 00:23:22,700
Y times x w is less than one.

395
00:23:23,030 --> 00:23:25,400
This is exactly what it comes
out to programmatically.

396
00:23:25,401 --> 00:23:29,270
Why Times the dot product of acts
in w is when it's less than one.

397
00:23:29,450 --> 00:23:30,770
So if it's less than one,

398
00:23:30,980 --> 00:23:35,930
then we have misclassified our data so we
can update our weights using that full.

399
00:23:36,100 --> 00:23:38,240
A equation that we saw were right up here,

400
00:23:38,810 --> 00:23:43,810
right up here where we're using both our
regular riser and our loss function to

401
00:23:44,691 --> 00:23:47,240
update weights, right?
Because this is non zero,

402
00:23:47,241 --> 00:23:50,840
it's going to be negative y times
x using our partial derivatives.

403
00:23:50,990 --> 00:23:54,290
And our Parkville derivatives are
the gradient, right? They are the,

404
00:23:54,570 --> 00:23:58,160
they are the gradient value that so
that we can update our weights in a

405
00:23:58,161 --> 00:24:03,161
direction such that such that the
such that the error is minimized.

406
00:24:03,680 --> 00:24:05,690
That's what they're going to help us do.
Okay.

407
00:24:05,691 --> 00:24:08,060
And so we'll update our weights
doing that and then we'll say,

408
00:24:08,061 --> 00:24:12,230
well we got an error,
so let's make our error account one else.

409
00:24:12,260 --> 00:24:13,790
If we've correctly classified,

410
00:24:14,090 --> 00:24:17,780
then we update our weights using
just the regular riser term,

411
00:24:17,900 --> 00:24:22,160
which is one over the number of epochs.
By the way, are regularize a term.

412
00:24:22,161 --> 00:24:26,030
I think I might've forgotten to say that
our regular riser term is one over the

413
00:24:26,031 --> 00:24:26,990
number of epochs.

414
00:24:27,020 --> 00:24:31,760
So it's inversely correlated so that the
regular riser parameter will decrease

415
00:24:31,940 --> 00:24:34,220
as a number of epochs increase.

416
00:24:35,450 --> 00:24:40,130
And so the update rule for correct
classification looks like this.

417
00:24:40,270 --> 00:24:44,930
We're only using the regular riser
term. All right? So we've got that.

418
00:24:45,080 --> 00:24:50,040
And so once we have that, we can
plot it out and we'll also, uh,

419
00:24:50,420 --> 00:24:53,860
add all those errors to this list of
errors. So we just have that, you know,

420
00:24:53,920 --> 00:24:58,310
we want to see how the error frequency
decreases over time during training.

421
00:24:58,400 --> 00:25:01,700
That's the actual machine learning.
And so when we plot that,

422
00:25:01,701 --> 00:25:06,530
we'll see that the error decreases over
time. The air value decreases over time,

423
00:25:06,531 --> 00:25:10,040
and that's what we want, right?
So we've trained our model, right?

424
00:25:10,041 --> 00:25:12,530
We've trained it on those
five toy data points.

425
00:25:12,830 --> 00:25:17,750
And so now what we can do is
we can now plot this model.

426
00:25:18,110 --> 00:25:22,460
We can plot this model and uh,
we can,

427
00:25:22,580 --> 00:25:25,910
we can add testing data as
well. Bright. But we have it,

428
00:25:25,940 --> 00:25:30,710
we had a misclassification case and we
had a correct classification case and

429
00:25:30,711 --> 00:25:33,740
depending on whether he was
misclassified or classified correctly,

430
00:25:33,950 --> 00:25:37,730
we updated or weights using
a different strategy. Okay.

431
00:25:37,731 --> 00:25:41,930
And the whole goal is to make
those weights the optimal
values that they should

432
00:25:41,931 --> 00:25:44,930
be such that our error value is minimized.

433
00:25:45,110 --> 00:25:47,270
And we have the equations
for that right here.

434
00:25:47,480 --> 00:25:51,560
And the reason we took the
partial derivative derivative
is so that we can best

435
00:25:51,561 --> 00:25:55,400
up there weights to optimize
for our objective function,

436
00:25:55,520 --> 00:25:57,200
which consists of these two terms,

437
00:25:57,380 --> 00:26:02,000
minimizing the loss and optimizing the
regular riser term such that our data is

438
00:26:02,030 --> 00:26:06,320
best fit to both our training data and
any novel data points we give it. That is,

439
00:26:06,321 --> 00:26:11,300
it's generalize ability, right.
So hopefully that all makes sense.

440
00:26:11,390 --> 00:26:15,140
That's essentially what we just did. If
you, if you understood like 80% of that,

441
00:26:15,141 --> 00:26:19,550
pat yourself on the back because you are
a boss. All right. We are all bosses.

442
00:26:19,610 --> 00:26:23,150
All right. So machine learning
bosses. Okay. So back to this.

443
00:26:24,860 --> 00:26:28,880
Okay. So now we're going to plot this
data. All right, so let's plot it.

444
00:26:28,881 --> 00:26:33,200
So if we have less than two sample exam
samples, then we'll apply it again.

445
00:26:33,201 --> 00:26:35,260
So this is the same thing that
we did before. So we'll do,

446
00:26:35,300 --> 00:26:39,080
we'll do it again and then we're
going to add our test samples.

447
00:26:39,290 --> 00:26:43,380
So our test samples are going to be just
two points and they're also going to be

448
00:26:43,381 --> 00:26:45,000
two toy data points.

449
00:26:45,180 --> 00:26:48,690
But just assume that we know what
class this data points, you know,

450
00:26:48,691 --> 00:26:51,420
consist of until add our two test samples,

451
00:26:51,690 --> 00:26:56,690
plot our hyperplane that train on the
training data and hopefully it classifies

452
00:26:57,000 --> 00:27:02,000
both the training data and the testing
data accurately such that they all lie on

453
00:27:02,011 --> 00:27:03,330
the perfect,
uh,

454
00:27:03,331 --> 00:27:06,690
side of the decision boundary so
that they are correctly classified.

455
00:27:08,580 --> 00:27:10,950
And so when we plot it,
we'll see that that is the case.

456
00:27:11,160 --> 00:27:15,750
All the positive labeled data points
are on one side of the line and all the

457
00:27:15,751 --> 00:27:19,110
negative label data points are
on the other side of the line.

458
00:27:19,290 --> 00:27:23,640
We have correctly classified both
our training and our testing data by

459
00:27:23,760 --> 00:27:27,210
optimizing for our objective.
And by doing so,

460
00:27:27,240 --> 00:27:32,240
we are minimizing our hinge loss so
that we maximize the margin or the space

461
00:27:32,700 --> 00:27:37,700
between the two data classes so that we
can draw the optimal hyperplane and we

462
00:27:38,551 --> 00:27:41,240
use a regular riser term that we also,
uh,

463
00:27:41,400 --> 00:27:45,480
found the optimal value for
it by including it in the
objective function so that

464
00:27:45,481 --> 00:27:50,070
our model was best fit to both the
training and the testing data. All right.

465
00:27:50,071 --> 00:27:54,090
So that in a nutshell is how
support vector machines work.

466
00:27:54,510 --> 00:27:56,550
So, um, two

467
00:27:58,370 --> 00:28:02,890
to go over, uh, this again,

468
00:28:02,891 --> 00:28:04,840
let me just say that.

469
00:28:06,330 --> 00:28:07,100
Okay.

470
00:28:07,100 --> 00:28:11,210
Right. So the gradient is a vector whose
components consist of the derivative.

471
00:28:11,360 --> 00:28:13,490
So in all of Calculus we have derivatives.

472
00:28:13,491 --> 00:28:18,491
And so the reason we take the derivative
is because calculus is this is the math

473
00:28:18,621 --> 00:28:22,940
of the rate of change. We want to know
how something changes and the way we,

474
00:28:23,270 --> 00:28:25,160
we study how it changes.

475
00:28:25,460 --> 00:28:28,340
We use it in physics a lot
to write for moving bodies.

476
00:28:28,550 --> 00:28:32,130
The derivative is how
we is, is, is how we is,

477
00:28:32,180 --> 00:28:36,380
how we understand which direction and
something is moving in, right? It's,

478
00:28:36,500 --> 00:28:38,240
we derive the direction from it.

479
00:28:38,600 --> 00:28:43,520
And there's several ways of representing
the derivative. In Calculus, we use the,

480
00:28:43,580 --> 00:28:46,790
obviously the derivative,
which is the derivative operator,

481
00:28:46,940 --> 00:28:50,000
but then we also use the gradient
operator. And so the gradient,

482
00:28:50,001 --> 00:28:51,560
and so you hear these terms a lot,

483
00:28:51,561 --> 00:28:54,120
you hear gradient and you
hear derivative a lot,

484
00:28:54,230 --> 00:28:58,070
but they're really the same thing in
that the gradient is a vector whose

485
00:28:58,071 --> 00:29:03,071
components consist of the partial
derivatives of whatever coefficients of

486
00:29:03,261 --> 00:29:06,440
whatever function that we're
trying to approximate. Okay.

487
00:29:06,441 --> 00:29:09,140
And so that's the gradient and
the derivative and [inaudible].

488
00:29:09,170 --> 00:29:12,380
In the next week's lesson,
we're going to talk about the Jacobian,

489
00:29:12,500 --> 00:29:16,670
which is a matrix of first order partial
derivatives and also the Hessian,

490
00:29:16,820 --> 00:29:19,460
which is a matrix of the second
order partial derivatives.

491
00:29:19,550 --> 00:29:24,550
And these are all words that represents
how we organize and represent change in

492
00:29:25,011 --> 00:29:27,590
data in in functions, right? And we,

493
00:29:27,591 --> 00:29:31,280
the reason we want to represent change
is so that we can minimize her loss,

494
00:29:31,400 --> 00:29:32,960
we can optimize for an objective,

495
00:29:33,080 --> 00:29:38,080
we can iteratively get closer and closer
to approximating to educatedly guessing

496
00:29:38,810 --> 00:29:43,730
what the optimal function is. So that we
learned the mapping between data points.

497
00:29:43,970 --> 00:29:48,140
Okay. So, hopefully that all made sense.
All right, so that's it for this lesson.

498
00:29:48,200 --> 00:29:49,940
Please subscribe for
more programming videos,

499
00:29:49,941 --> 00:29:54,110
and for now I'll get to go invent a new
type of Svm. So thanks for watching.

