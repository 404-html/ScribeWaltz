1
00:00:00,090 --> 00:00:02,670
Hello world. It's Saroj. In this video,

2
00:00:02,700 --> 00:00:06,720
we're going to write a script that
reads in a Dataset of movie ratings and

3
00:00:06,721 --> 00:00:09,630
recommends new movies for a
user in 40 lines of python.

4
00:00:09,780 --> 00:00:14,190
We are in the midst of a new renaissance
right now and the reason for it is data.

5
00:00:14,400 --> 00:00:15,750
In the past two years,

6
00:00:15,870 --> 00:00:20,010
we've created more data than in the
entire previous history of the human race.

7
00:00:20,040 --> 00:00:24,270
Your options to learn or experience
or create are limitless on the wet.

8
00:00:24,420 --> 00:00:27,120
This is awesome. Oh my
God, I'm going to throw up,

9
00:00:27,420 --> 00:00:29,760
but all of this abundance
presents a problem.

10
00:00:29,880 --> 00:00:33,270
The paradox of choice because
we have so many choices,

11
00:00:33,390 --> 00:00:35,790
we could spend way too much
time trying to pick one.

12
00:00:36,000 --> 00:00:38,580
We could end up not picking
anything at all or worse,

13
00:00:38,640 --> 00:00:41,310
pick something and then get
fomo or fear of missing out.

14
00:00:41,520 --> 00:00:44,850
If there was a way to have an abundance
of options and have certainty in our

15
00:00:44,851 --> 00:00:49,140
decisions, that would be ideal, right?
That's how recommendation systems help us.

16
00:00:49,290 --> 00:00:52,110
They use machine learning to
go over all of our options,

17
00:00:52,260 --> 00:00:56,100
learn what we like and recommend which
option we would like best and they're

18
00:00:56,101 --> 00:00:59,580
getting insanely accurate like they
know us better than we know ourselves.

19
00:00:59,581 --> 00:01:04,050
Sometimes Amazon recommends products we'd
like. Google recommends search results.

20
00:01:04,051 --> 00:01:06,480
We'd like Facebook
recommends friends we'd like.

21
00:01:06,600 --> 00:01:08,280
Pretty much every service does it.

22
00:01:08,281 --> 00:01:10,980
Now there are two main types
of recommender systems,

23
00:01:11,160 --> 00:01:13,860
collaborative systems and
content based systems.

24
00:01:13,950 --> 00:01:17,520
Collaborative systems predict what you
like based on what other similar users

25
00:01:17,521 --> 00:01:20,040
have liked in the past.
Content based systems,

26
00:01:20,100 --> 00:01:24,060
predictably you like based on what you've
liked in the past and some services

27
00:01:24,061 --> 00:01:27,720
like Netflix combined both
approaches to be even more accurate.

28
00:01:27,780 --> 00:01:31,650
So let's write our own script that
recommends movies a user would like.

29
00:01:31,740 --> 00:01:33,960
We'll install our dependencies,
then write our script.

30
00:01:34,170 --> 00:01:37,260
We're going to use three dependencies,
both num pi and PSI.

31
00:01:37,270 --> 00:01:41,400
Pi will help us do some math and lite FM
is a library that allows us to perform

32
00:01:41,580 --> 00:01:44,250
any number of popular
recommendation algorithms.

33
00:01:44,400 --> 00:01:48,300
We'll start by importing num py and call
it NP. I'd have him as a big library,

34
00:01:48,301 --> 00:01:50,880
so we're only going to import the
sub modules of it that we need.

35
00:01:51,090 --> 00:01:55,500
We'll use the from import combination to
import specific methods from submodules

36
00:01:55,530 --> 00:01:56,363
as necessary.

37
00:01:56,490 --> 00:02:00,060
We'll import the fetch movie lens method
from the Datasets sub module and the

38
00:02:00,061 --> 00:02:02,430
light FM class directly
from light offense,

39
00:02:02,460 --> 00:02:06,360
which will later create a model for us.
Now that we've imported our dependencies,

40
00:02:06,450 --> 00:02:07,680
we're going to fetch our dataset.

41
00:02:07,890 --> 00:02:11,940
Let's create a variable called data to
store our data set and we use the fetch

42
00:02:11,941 --> 00:02:15,930
movie lens method we imported earlier
from our light FM dependency that fetch

43
00:02:15,931 --> 00:02:16,650
our dataset.

44
00:02:16,650 --> 00:02:20,970
The movie Lens Dataset is a big CSV file
that contains a hundred k movie ratings

45
00:02:21,060 --> 00:02:23,820
from one k users on 1700 movies.

46
00:02:23,850 --> 00:02:28,080
Each user has rated as least 20 movies
on a scale of one to five so hopefully

47
00:02:28,081 --> 00:02:31,710
they gave this a zero
basic.

48
00:02:31,840 --> 00:02:33,330
See if I can track an IP address.

49
00:02:35,150 --> 00:02:38,160
If that's movie lens method takes in
an auction parameter called Min rating.

50
00:02:38,350 --> 00:02:40,740
I mean writing is a minimum rating
we want to include in our data.

51
00:02:40,890 --> 00:02:44,070
We'll send it to 4.0 which means we're
only collecting the movies with a rating

52
00:02:44,071 --> 00:02:45,030
of four or higher.

53
00:02:45,150 --> 00:02:48,780
So this method will create an interaction
matrix from our CSV file in story in

54
00:02:48,781 --> 00:02:50,460
our data variable as a dictionary.

55
00:02:50,640 --> 00:02:52,830
A dictionary is a way to
store data just like a list,

56
00:02:53,010 --> 00:02:55,170
except instead of just using
numbers to retrieve data,

57
00:02:55,200 --> 00:02:58,010
you can use anything in our case
where your strengths are fetched.

58
00:02:58,011 --> 00:03:01,300
Movie Lens Method Splits Dataset into
training and testing data and we could

59
00:03:01,301 --> 00:03:04,660
retrieve each by using the train and test
strings as keys. We'll print out both.

60
00:03:04,810 --> 00:03:07,930
Let's quickly see what this looks
like in terminal. When we compile it,

61
00:03:07,990 --> 00:03:10,120
it'll print out both our
training and testing matrices.

62
00:03:10,300 --> 00:03:13,450
We can see that our training day that
contains 10 times more items that are

63
00:03:13,451 --> 00:03:16,420
testing data. We'll store our model
in a variable that we call model.

64
00:03:16,570 --> 00:03:20,290
We'll initialize a lite FM class using a
single parameter called loss loss means

65
00:03:20,291 --> 00:03:24,070
our loss function and it measures the
difference between our model's prediction

66
00:03:24,280 --> 00:03:25,480
and the desired output.

67
00:03:25,690 --> 00:03:29,440
We want to minimize it during training
so our model gets more accurate over time

68
00:03:29,441 --> 00:03:31,750
and its predictions.
We can choose between several.

69
00:03:31,840 --> 00:03:33,880
We'll go ahead and choose
a loss called warp,

70
00:03:33,910 --> 00:03:36,160
which stands for weighted
approximate rank.

71
00:03:36,161 --> 00:03:40,870
Pairwise warp helps us
create recommendations for
each user by looking at the

72
00:03:40,871 --> 00:03:44,260
existing user rating,
pears and predicting rankings for each.

73
00:03:44,380 --> 00:03:48,220
It uses the gradient descent algorithm
to iteratively find the weights that

74
00:03:48,221 --> 00:03:49,960
improve our prediction over time.

75
00:03:50,140 --> 00:03:53,110
This takes into account both
their users' past rating history,

76
00:03:53,230 --> 00:03:57,700
content based and similar users ratings
collaborative. It's a hybrid system.

77
00:03:57,790 --> 00:04:00,280
Now that we have our model,
we can go ahead and train.

78
00:04:00,281 --> 00:04:03,310
It will use the fit method of
the model object to train it.

79
00:04:03,490 --> 00:04:05,320
The fit method takes three parameters.

80
00:04:05,410 --> 00:04:08,800
The data set we want to train it on
the number of epoxy we want to run the

81
00:04:08,801 --> 00:04:12,970
training for and the number of threads
we want to run this on. For our data set,

82
00:04:12,971 --> 00:04:16,600
we use our data dictionary we created
earlier and pointed to the training data

83
00:04:16,660 --> 00:04:17,740
using the train strength.

84
00:04:17,770 --> 00:04:21,670
We'll set the number of epochs or runs
for this training session to 30 and the

85
00:04:21,671 --> 00:04:25,180
number of threads or parallel computations
to to now we want to generate a

86
00:04:25,181 --> 00:04:27,400
recommendation from our model.
To do that,

87
00:04:27,460 --> 00:04:30,730
we'll write a sample recommendation
function, which takes three parameters.

88
00:04:30,790 --> 00:04:33,010
Our model,
our data analyst of User Ids.

89
00:04:33,160 --> 00:04:35,590
These are users we want to
generate recommendations for.

90
00:04:35,620 --> 00:04:39,310
First we'll get the number of users and
the number of items which are movies.

91
00:04:39,311 --> 00:04:43,480
In our case using the shape attribute
of the data dictionary we created.

92
00:04:43,660 --> 00:04:46,960
Now we can create a for loop to iterate
through every user id that we would

93
00:04:46,961 --> 00:04:50,650
input and say that we want the list
of known positives for each light.

94
00:04:50,651 --> 00:04:54,580
FM considers ratings that
are five positive and ratings
that are four or below

95
00:04:54,581 --> 00:04:57,130
negative.
To make the problem binary much simpler,

96
00:04:57,160 --> 00:05:01,000
we'll get the list of positive ratings
from our data in compressed sparse row

97
00:05:01,001 --> 00:05:04,060
format. This is a suburb way
inside of our data matrix,

98
00:05:04,090 --> 00:05:06,250
which will retrieve using
the indices attribute.

99
00:05:06,280 --> 00:05:10,090
Next we'll generate our recommendations
and store them in the scores variable.

100
00:05:10,180 --> 00:05:12,010
Using the predict method of our model.

101
00:05:12,040 --> 00:05:15,610
We'll use a user id as the first
parameter and then a list of each movie.

102
00:05:15,670 --> 00:05:19,420
The a range method of num py will give
us every number from zero up to the

103
00:05:19,421 --> 00:05:22,300
number of items so we can predict
the score for every movie.

104
00:05:22,450 --> 00:05:24,790
Then we'll sort them in
order of their score.

105
00:05:24,910 --> 00:05:28,840
The Arg sort method of [inaudible] will
return the score indices in descending

106
00:05:28,841 --> 00:05:31,900
order thanks to the negative sign.
Let's go ahead and print them.

107
00:05:31,930 --> 00:05:34,150
First we'll print out the
user's ID percent has,

108
00:05:34,151 --> 00:05:35,590
we'll convert the ID to a string.

109
00:05:35,830 --> 00:05:39,250
Then we'll print the top
three known positive movies
that the user has picked by

110
00:05:39,251 --> 00:05:42,430
creating a for loop ending
in the third index. Lastly,

111
00:05:42,460 --> 00:05:44,320
we'll create one more loop for printing.

112
00:05:44,321 --> 00:05:47,110
The top three recommended movies
that are modeled predicts,

113
00:05:47,140 --> 00:05:51,130
we'll call it at the end using the model
data and Alyssa three random user ids

114
00:05:51,220 --> 00:05:52,180
as the parameters.

115
00:05:52,210 --> 00:05:56,200
Each user has movies that they like listed
as well as movies that our system has

116
00:05:56,201 --> 00:05:58,490
recommended for them.
So to break it down,

117
00:05:58,610 --> 00:06:02,780
recommendation algorithms help us make
decisions by learning our preferences.

118
00:06:02,810 --> 00:06:05,210
There are two main types of
recommendation algorithms,

119
00:06:05,400 --> 00:06:08,390
content based and collaborative.
And lastly,

120
00:06:08,450 --> 00:06:12,380
why FM is a great library to get started
with building recommendation systems.

121
00:06:12,440 --> 00:06:15,710
The winner of the coding challenge
from the last video is row Han Vermont

122
00:06:15,800 --> 00:06:18,950
[inaudible] went above and beyond by
creating a web app where you can search a

123
00:06:18,951 --> 00:06:23,180
topic and it'll list a bunch of tweets on
that topic with highlighted sentiments.

124
00:06:23,300 --> 00:06:26,510
Bad Ass of the week,
and the runner up is our nod Delani,

125
00:06:26,630 --> 00:06:28,700
well documented and well defined methods.

126
00:06:28,730 --> 00:06:32,510
The challenge for this week is to modify
this code so that instead of using the

127
00:06:32,511 --> 00:06:34,310
default fetch movie lens method,

128
00:06:34,340 --> 00:06:38,540
you'll write a new method that fetches
and formats some other recommendation

129
00:06:38,541 --> 00:06:40,910
dataset.
Train it on three different models,

130
00:06:41,030 --> 00:06:43,740
compare the results and
print the best one post.

131
00:06:43,750 --> 00:06:46,520
You're getting humbling in the comments
and I'll announce the winner in the next

132
00:06:46,521 --> 00:06:48,950
video.
Links to everything in the description.

133
00:06:49,040 --> 00:06:52,940
Please share this video and subscribe
for more programming videos. For now.

134
00:06:52,970 --> 00:06:56,600
I've got to wonder why Yahoo still exists,
so thanks for watching.

