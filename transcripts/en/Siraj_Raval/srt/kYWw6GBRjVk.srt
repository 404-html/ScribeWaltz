1
00:00:00,180 --> 00:00:03,660
Hello world,
it's arrived and the Internet of things.

2
00:00:03,690 --> 00:00:05,850
It's definitely a buzz word these days,

3
00:00:05,851 --> 00:00:10,851
but it's a real thing and I'll demonstrate
how to use reinforcement learning to

4
00:00:11,190 --> 00:00:16,190
optimize electricity consumption amongst
multiple devices in a smart home.

5
00:00:16,830 --> 00:00:18,240
Marc Andreessen's quote,

6
00:00:18,241 --> 00:00:21,750
that software is eating the
world still rings true today.

7
00:00:21,960 --> 00:00:26,370
More and more devices are
coming prepackaged with
Internet access that would

8
00:00:26,371 --> 00:00:31,350
normally be, that includes smart sole
shakers, smart rectal thermometers,

9
00:00:31,380 --> 00:00:34,110
smart mugs, smart
chairs. The list goes on,

10
00:00:34,410 --> 00:00:37,860
and because these devices have a
connection to the worldwide web,

11
00:00:38,010 --> 00:00:42,540
they can communicate with the outside
world and each other sharing information

12
00:00:42,541 --> 00:00:44,550
and even learning from one another.

13
00:00:44,820 --> 00:00:49,440
This internet of physical things is
becoming increasingly common across the

14
00:00:49,441 --> 00:00:53,430
globe, whether it's a smart home,
a fleet of self driving cars,

15
00:00:53,520 --> 00:00:56,970
or a pipeline of assets as they
are shipped across the globe.

16
00:00:57,180 --> 00:01:02,070
Having a web of interconnected physical
devices has a huge range of use cases in

17
00:01:02,071 --> 00:01:05,070
our modern world for
businesses and consumers.

18
00:01:05,430 --> 00:01:09,750
But a major concern regarding
these devices is the amount
of energy they'll need

19
00:01:09,751 --> 00:01:14,550
to consume. Silicon chips need
electricity, and the more devices we have,

20
00:01:14,610 --> 00:01:16,560
the higher our energy costs will be.

21
00:01:16,770 --> 00:01:21,240
It turns out that we can use reinforcement
learning to create a system that will

22
00:01:21,241 --> 00:01:24,630
minimize our electricity
costs substantially.

23
00:01:25,020 --> 00:01:27,330
This can apply to a whole
range of industries.

24
00:01:27,331 --> 00:01:31,950
We're an interconnected system of devices,
is required to perform a task,

25
00:01:32,190 --> 00:01:36,570
agriculture, manufacturing, and even a
personal home, which will be our demo.

26
00:01:36,990 --> 00:01:41,990
Google recently used RL to reduce the
amount of energy they used in their data

27
00:01:42,361 --> 00:01:46,470
center by up to 40% just
like a laptop generates heat.

28
00:01:46,680 --> 00:01:49,770
They're massive racks of
servers generate a lot of heat,

29
00:01:50,040 --> 00:01:52,020
but too much heat can damage the servers.

30
00:01:52,021 --> 00:01:57,021
So a cooling system is necessary to
help maintain a certain temperature and

31
00:01:57,091 --> 00:01:59,370
because it's such a dynamic environment,

32
00:01:59,400 --> 00:02:02,820
a learning system is required
to set that temperature.

33
00:02:03,150 --> 00:02:08,040
A whole host of unforeseen scenarios
like changes in weather and power outages

34
00:02:08,100 --> 00:02:13,080
can occur. Also, humans interact with
the equipment in unexpected ways,

35
00:02:13,081 --> 00:02:16,410
regularly probably doing
the floss stance on them.

36
00:02:16,650 --> 00:02:21,090
And each data center has a unique
architecture and environment,

37
00:02:21,240 --> 00:02:26,240
so their system needed to be able to
adapt to multiple unique environments.

38
00:02:26,550 --> 00:02:31,440
Last week we introduced the fundamental
way of framing the reinforcement

39
00:02:31,441 --> 00:02:36,441
learning problem where an
agent is interacting with
an environment to maximize

40
00:02:36,691 --> 00:02:39,960
our reward.
The mark comp decision process.

41
00:02:40,230 --> 00:02:45,120
The goal of an agent is to learn a
policy so it knows given a state the best

42
00:02:45,121 --> 00:02:50,121
action to take in order to maximize
or reward and in a complete Mark Cobb

43
00:02:50,221 --> 00:02:54,450
decision process or all of the
environment variables are known,

44
00:02:54,690 --> 00:02:58,050
we can use dynamic programming
to learn an optimal policy.

45
00:02:58,530 --> 00:03:02,860
But what if we don't know all the
environment variables beforehand?

46
00:03:03,310 --> 00:03:08,140
Give up? No. Then it'd be
considered not to model based RL,

47
00:03:08,170 --> 00:03:12,400
but model free RL in model free RL.

48
00:03:12,430 --> 00:03:15,970
The first variable we miss
is a transition model,

49
00:03:16,000 --> 00:03:21,000
so we won't know what's going to happen
after each action we take beforehand.

50
00:03:21,610 --> 00:03:25,960
This tells us the probabilities
associated with various state changes.

51
00:03:26,260 --> 00:03:29,020
The second thing we miss
is the reward function,

52
00:03:29,021 --> 00:03:34,021
which gives the agent the
reward associated with a
particular state beforehand.

53
00:03:34,390 --> 00:03:38,110
So when we don't know either
of these mark Cobian variables,

54
00:03:38,320 --> 00:03:40,480
dynamic programming won't work.

55
00:03:40,750 --> 00:03:44,890
We need to instead use a different
type of method called Monte Carlo.

56
00:03:45,250 --> 00:03:50,250
Monte Carlo methods are a broad class of
algorithms that rely on repeated random

57
00:03:51,191 --> 00:03:54,190
sampling to obtain numerical results.

58
00:03:54,490 --> 00:03:59,490
The keyword here is random Monte Carlo
methods make use of randomness to solve

59
00:03:59,861 --> 00:04:04,690
problems, which turns out to be very
useful in mathematics and physics.

60
00:04:04,900 --> 00:04:09,900
Stanislaw ou alum invented it in the late
1940s while working on a nuclear bomb

61
00:04:10,271 --> 00:04:12,190
as part of the Manhattan project.

62
00:04:12,460 --> 00:04:17,460
Then John von Neumann decided he liked
it and programmed a machine to do those

63
00:04:17,531 --> 00:04:21,550
same calculations.
They decided to name the codebase.

64
00:04:21,580 --> 00:04:26,450
Monte Carlo has every super secret
project should be named UHMS.

65
00:04:26,470 --> 00:04:31,300
Uncle happened to be losing lots of money
in the Monte Carlo Casino in Monaco,

66
00:04:31,301 --> 00:04:35,140
so that's why uncles,
in fact,

67
00:04:35,170 --> 00:04:40,170
deep minds alpha go use what's called a
Monte Carlo tree search to help it play

68
00:04:40,421 --> 00:04:45,421
against the reigning go champion resulting
in move 37 more on that at the end of

69
00:04:46,541 --> 00:04:47,374
the course.

70
00:04:47,440 --> 00:04:52,300
Monte Carlo techniques have several
advantages over dynamic programming.

71
00:04:52,540 --> 00:04:53,140
First,

72
00:04:53,140 --> 00:04:57,850
they allow for learning optimal behavior
directly from interaction with the

73
00:04:57,851 --> 00:05:02,470
environment without needing the
transition or reward function defined

74
00:05:02,710 --> 00:05:04,510
beforehand.
Second,

75
00:05:04,600 --> 00:05:09,100
it's easy and computationally
efficient to focus MC methods.

76
00:05:09,250 --> 00:05:14,250
On a small subset of the total states and
third MC can be used with simulations.

77
00:05:15,010 --> 00:05:19,030
So let's say we have a home that
consists of a bunch of internet of things

78
00:05:19,031 --> 00:05:23,710
devices. We've got a smart TV,
a smart fridge, a smart dog,

79
00:05:23,740 --> 00:05:26,110
and a smart giant server in our room.

80
00:05:26,440 --> 00:05:30,100
All of this equipment requires
a lot of electricity to run,

81
00:05:30,101 --> 00:05:34,690
but it also requires cooling or else
my room would get too hot so I have a

82
00:05:34,691 --> 00:05:35,530
cooling system.

83
00:05:35,920 --> 00:05:40,390
Now let's say that we have access
to our electricity usage logs,

84
00:05:40,540 --> 00:05:45,540
thanks to partnering with a data friendly
provider and our smart thermostat can

85
00:05:45,641 --> 00:05:49,840
set the temperature accordingly.
Depending on the type of system we built,

86
00:05:50,140 --> 00:05:53,980
we can imagine electricity
flowing into all of these devices.

87
00:05:53,981 --> 00:05:58,981
Creating a closed loop system and this
constant stream of electricity data can

88
00:05:59,571 --> 00:06:04,040
definitely be utilized. It's giving us
the electricity, price, cooling demand,

89
00:06:04,070 --> 00:06:07,430
and electricity consumption as variables.
Using this,

90
00:06:07,460 --> 00:06:10,190
we can construct our mark
hub decision process.

91
00:06:10,610 --> 00:06:14,480
The goal of our system is to
minimize our electricity bills,

92
00:06:14,720 --> 00:06:15,770
more money for GPU.

93
00:06:15,780 --> 00:06:20,210
Still now our agent will perform
an action in this environment.

94
00:06:20,450 --> 00:06:25,310
That action will be to either increase
or decrease the temperature by one degree

95
00:06:25,340 --> 00:06:29,780
Celsius. My fellow Americans, most of
this audience uses the metric system.

96
00:06:30,080 --> 00:06:34,460
The state then that our agents can be
in will be a measure of both how much

97
00:06:34,461 --> 00:06:38,420
cooling demand there is as well
as the price of electricity.

98
00:06:38,840 --> 00:06:43,840
The reward can tell us whether we are
saving money or not by switching states by

99
00:06:43,941 --> 00:06:48,680
calculating the total electricity
consumption multiplied by the price of

100
00:06:48,681 --> 00:06:53,540
electricity and depending on if that's
greater or less than what existed the

101
00:06:53,541 --> 00:06:58,541
time step before we know whether or not
that is a positive or negative reward.

102
00:06:59,060 --> 00:07:04,060
There exists an optimal policy here such
that if we were to give it a state in

103
00:07:04,221 --> 00:07:08,420
this case that would be the cooling
demand and the electricity price.

104
00:07:08,540 --> 00:07:13,540
It would know exactly what temperature
the thermostat should be set at such that

105
00:07:14,061 --> 00:07:19,061
we are optimally saving money on
electricity by cooling our room as much as

106
00:07:19,371 --> 00:07:21,740
necessary when necessary.

107
00:07:22,040 --> 00:07:27,040
Our adaptive real time reward based
system needs to learn this optimal policy

108
00:07:27,620 --> 00:07:31,400
and since we don't know the reward
function or the transition function

109
00:07:31,401 --> 00:07:32,234
beforehand,

110
00:07:32,480 --> 00:07:37,190
we have to compute our rewards and
transitions as they happen in real time.

111
00:07:37,490 --> 00:07:42,380
We'll want to use a model free technique
like Monte Carlo to learn the optimal

112
00:07:42,381 --> 00:07:43,214
policy.

113
00:07:43,370 --> 00:07:48,370
The basic idea is to calculate the value
function of each state backwards with

114
00:07:48,441 --> 00:07:49,580
the reward received.

115
00:07:49,700 --> 00:07:53,680
After the end of the episode
there has to be an ended,

116
00:07:53,690 --> 00:07:58,690
then the task has to be considered an
episodic task for us to use Monte Carlo.

117
00:07:59,690 --> 00:08:00,321
In our case,

118
00:08:00,321 --> 00:08:05,150
we can say that an episode last a full
eight hours while I'm away at my office

119
00:08:05,151 --> 00:08:06,290
working diligently,

120
00:08:06,560 --> 00:08:11,150
if we move from the initial state to the
terminal state according to the given

121
00:08:11,151 --> 00:08:14,600
policy,
will receive a reward at each time step.

122
00:08:14,720 --> 00:08:18,470
We'll remember all those rewards and
when we get to the terminal state,

123
00:08:18,560 --> 00:08:23,560
we'll look back and calculate the value
function of each state in the case that

124
00:08:23,961 --> 00:08:28,910
there are multiple episodes, then Monte
Carlo just averages all of the returns.

125
00:08:29,240 --> 00:08:32,630
We know that the return is a
sum of the discounted or reward.

126
00:08:32,990 --> 00:08:34,790
In the context of Monte Carlo,

127
00:08:34,791 --> 00:08:39,770
though we switch it up to obtain the
state value function we take instead the

128
00:08:39,800 --> 00:08:42,970
expectation of the returns not the sun.

129
00:08:43,460 --> 00:08:48,460
We can define a state s to be a discrete
random variable which can assume all

130
00:08:48,951 --> 00:08:51,350
the variable stats with
a certain probability.

131
00:08:51,680 --> 00:08:53,720
Every time our agent reaches a state,

132
00:08:53,750 --> 00:08:58,750
it's like we are picking a value for
the random variable s for each state of

133
00:08:59,040 --> 00:09:02,850
each episode. We can calculate
the return and store it in a list.

134
00:09:03,120 --> 00:09:04,290
Repeating this process.

135
00:09:04,291 --> 00:09:09,291
A lot is guaranteed to converge on the
true state value function in Monte Carlo

136
00:09:10,600 --> 00:09:15,510
Rl. We are estimating the value function
for each state ace on the return of each

137
00:09:15,511 --> 00:09:18,960
episode and the more episodes
we take into account,

138
00:09:19,110 --> 00:09:21,300
the more accurate our estimation will be.

139
00:09:21,540 --> 00:09:24,150
Notice though that a
possible problem could occur.

140
00:09:24,360 --> 00:09:28,620
What if we visit the same state
twice in a single episode? Well,

141
00:09:28,650 --> 00:09:33,270
there are actually two types of
Monte Carlo policy evaluation.

142
00:09:33,570 --> 00:09:37,410
First visit and every visit we'll
focus on first visit in this video.

143
00:09:37,980 --> 00:09:41,160
First visit only recognizes
the first visited state.

144
00:09:41,310 --> 00:09:45,450
Every second visit does not count the
return for that state visit and the return

145
00:09:45,451 --> 00:09:47,910
is calculated separately for each visit.

146
00:09:48,410 --> 00:09:52,800
Monte Carlo includes randomness
because when it updates every episode,

147
00:09:52,801 --> 00:09:55,020
depending on where it originated from,

148
00:09:55,290 --> 00:09:59,430
it's a different result depending on
which action we take in the same state

149
00:09:59,910 --> 00:10:02,520
because it contains these random elements.

150
00:10:02,760 --> 00:10:05,190
Monte-Carlo has a high variance.

151
00:10:05,490 --> 00:10:08,760
When we graph a simulation of
our agents solving our problem,

152
00:10:08,790 --> 00:10:12,870
we'll see that eventually the policy will
converge and then our system will know

153
00:10:12,900 --> 00:10:17,580
exactly what temperature to set our
room ace on the electricity related

154
00:10:17,581 --> 00:10:18,414
variables.

155
00:10:18,660 --> 00:10:23,070
There are different kinds of Monte Carlo
techniques that can do all sorts of

156
00:10:23,071 --> 00:10:25,980
cool things,
but we'll talk about those later on.

157
00:10:26,340 --> 00:10:28,800
Three points to remember from this video.

158
00:10:29,250 --> 00:10:34,250
In model three reinforcement
learning as opposed to model based,

159
00:10:34,590 --> 00:10:39,590
we don't know the reward function and
the transition function beforehand,

160
00:10:40,020 --> 00:10:42,180
we have to learn them through experience.

161
00:10:42,690 --> 00:10:47,690
A model free learning technique called
Monte Carlo uses repeated random sampling

162
00:10:49,080 --> 00:10:53,700
to obtain numerical results.
And in first visit, Monte Carlo,

163
00:10:53,880 --> 00:10:57,840
the state value function is defined
as the average of the returns.

164
00:10:57,841 --> 00:11:01,680
Following the agents first visit
to s and a set of episodes,

165
00:11:02,310 --> 00:11:05,100
please subscribe for more
programming videos. And for now,

166
00:11:05,130 --> 00:11:07,980
I'm going to try a new sample.
So thanks for watching.

