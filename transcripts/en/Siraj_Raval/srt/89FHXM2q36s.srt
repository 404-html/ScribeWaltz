1
00:00:00,060 --> 00:00:04,410
How does that make you feel? I'm
not sure. It's hard to say. Yeah.

2
00:00:04,411 --> 00:00:06,180
Well there's an episode of
Syrah geology coming in.

3
00:00:06,181 --> 00:00:08,070
I just don't have time for this right now.
So

4
00:00:13,320 --> 00:00:14,790
hello world.
It's Saroj.

5
00:00:14,880 --> 00:00:18,630
Have you ever had a hard time trying to
understand what someone is trying to say

6
00:00:18,631 --> 00:00:20,310
to you?
People aren't confusing.

7
00:00:20,370 --> 00:00:22,710
I don't even understand my
own emotions half the time,

8
00:00:22,890 --> 00:00:25,440
much less what someone else
is feeling or thinking.

9
00:00:25,530 --> 00:00:28,350
When we read a piece of text
or listen to someone speak.

10
00:00:28,500 --> 00:00:30,600
Our brains are performing tone analysis.

11
00:00:30,630 --> 00:00:33,180
We're trying to understand
the meaning behind the words.

12
00:00:33,270 --> 00:00:36,060
That is the sentiment and
the emotions and the style.

13
00:00:36,090 --> 00:00:39,960
It's not an easy task at all, but this
is something machines can do pretty well.

14
00:00:39,961 --> 00:00:43,950
Now it's all a part of a subfield of
machine learning called natural language

15
00:00:43,951 --> 00:00:44,670
processing.

16
00:00:44,670 --> 00:00:48,300
Standard approaches to NLP a couple of
years ago in both extracting a set of

17
00:00:48,301 --> 00:00:51,060
features from some labeled
piece of text that feature,

18
00:00:51,061 --> 00:00:54,840
and we're usually end grams and grams
have nothing to do with Graham crackers

19
00:00:54,990 --> 00:00:58,830
even though they are extremely delicious
and grams are just sequences of words.

20
00:00:58,890 --> 00:01:02,730
So a Unigram is one word by
grandma's to and Trigram is three.

21
00:01:02,760 --> 00:01:04,680
Once the end grims were extracted,

22
00:01:04,740 --> 00:01:08,520
the next step was to train a linear model
on some pre labeled data so they could

23
00:01:08,521 --> 00:01:09,990
classify similar texts.

24
00:01:10,020 --> 00:01:14,190
The process of feature extraction saw a
hugely forward when Google open source,

25
00:01:14,191 --> 00:01:15,990
something called word to Vec,

26
00:01:16,050 --> 00:01:19,380
word to Vec is a toolkit that
helps encode words into vectors.

27
00:01:19,410 --> 00:01:23,460
These vectors are representations of
words called word embeddings that are

28
00:01:23,461 --> 00:01:25,500
learned by training on a given corpus.

29
00:01:25,530 --> 00:01:27,780
The tool kit consists
of two distinct models,

30
00:01:27,900 --> 00:01:30,210
skip gram and continuous bag of words.

31
00:01:30,240 --> 00:01:33,650
The skip grant model predicts
the neighboring words given
the current word and a

32
00:01:33,651 --> 00:01:35,430
given window.
In contrast,

33
00:01:35,460 --> 00:01:38,940
the bag of words model
predicts the current word
given the neighboring words and

34
00:01:38,941 --> 00:01:42,360
given window. These models both
help predict and code words.

35
00:01:42,390 --> 00:01:43,620
Once we had these vectors,

36
00:01:43,710 --> 00:01:48,360
we can use them to do all sorts of text
classification, including tone analysis,

37
00:01:48,810 --> 00:01:49,643
deep learning.

38
00:01:51,030 --> 00:01:54,450
One of the first papers to show that deep
learning could improve text analysis,

39
00:01:54,480 --> 00:01:57,810
who's called convolutional neural
networks for sentence classification.

40
00:01:57,840 --> 00:02:02,130
Although CNNS were intended for computer
vision. These guys applied it to NLP.

41
00:02:02,160 --> 00:02:07,080
They first train the CNN on a set of word
vectors that Google extracted from 100

42
00:02:07,080 --> 00:02:10,740
billion word corpus using their own
word to Vec toolkit. After training,

43
00:02:10,741 --> 00:02:14,130
the CNN had built representations
of all sorts of word categories.

44
00:02:14,180 --> 00:02:16,350
Since the vectors weren't
labeled in any way,

45
00:02:16,410 --> 00:02:18,750
the training was considered unsupervised.

46
00:02:18,810 --> 00:02:21,210
After they had a train on
a set of Google vectors,

47
00:02:21,360 --> 00:02:24,870
they then train it using labeled data
so that he could perform sentiment

48
00:02:24,871 --> 00:02:25,680
analysis.

49
00:02:25,680 --> 00:02:29,970
A later paper called Tex categorization
using LSTM for region embeddings

50
00:02:30,030 --> 00:02:33,000
improved on it.
They did the same basic experiment,

51
00:02:33,030 --> 00:02:36,720
but the key difference is that they didn't
just use a CNN to create embeddings.

52
00:02:36,750 --> 00:02:39,990
They also use a long
short term memory network.

53
00:02:40,050 --> 00:02:43,800
On LSTM is a type of recurrent neural
network that can remember dependencies

54
00:02:43,890 --> 00:02:47,340
from way back in the sequence of data
and they found that they had the best

55
00:02:47,341 --> 00:02:48,480
classification result.

56
00:02:48,540 --> 00:02:52,110
When they combine the embeddings
from both the LSTM and the CNN,

57
00:02:52,170 --> 00:02:56,700
just like two peas in a algorithm,
that was terrible.

58
00:02:56,820 --> 00:03:00,280
They also found that embedding
regions that sets of words.

59
00:03:00,370 --> 00:03:02,560
What's more effective than
embedding single words.

60
00:03:02,590 --> 00:03:07,270
This idea of abstractions of a hierarchy
of knowledge in a given document helped

61
00:03:07,271 --> 00:03:12,160
inspire a fresh paper released just two
months ago called hierarchical attention

62
00:03:12,161 --> 00:03:15,220
networks for document classification.
These guys said,

63
00:03:15,280 --> 00:03:18,310
let's create a new neural
architecture to model a document.

64
00:03:18,400 --> 00:03:22,600
It starts by encoding words and applying
an attention mechanism to extract the

65
00:03:22,601 --> 00:03:26,080
most important words. Then it encodes
a sentences using the weights.

66
00:03:26,110 --> 00:03:28,990
It learned applies an attention
mechanisms to that to,

67
00:03:29,080 --> 00:03:31,060
to extract the most important sentences.

68
00:03:31,120 --> 00:03:34,300
It uses those weights to
build document level vectors.

69
00:03:34,390 --> 00:03:38,530
So it's creating vectors for each layer
of attraction within a document and

70
00:03:38,531 --> 00:03:39,820
building them off of each other.

71
00:03:39,880 --> 00:03:43,060
The encoder for each of these
levels is called a gru neural net.

72
00:03:43,120 --> 00:03:45,250
So after they initialize
the model using vectors,

73
00:03:45,251 --> 00:03:49,210
they got using word to beck a trended on
labeled data and doing this with their

74
00:03:49,211 --> 00:03:53,020
novel neural architecture pretty much
outperformed all previous attempts.

75
00:03:54,670 --> 00:03:58,630
So how do we implement this
stuff ourselves? Well as
professor [inaudible] says,

76
00:03:58,750 --> 00:04:02,290
deep learning requires a rocket,
the model and rocket fuel that data,

77
00:04:02,500 --> 00:04:06,520
but sometimes you just want to get shit
done and training is to time expensive.

78
00:04:06,610 --> 00:04:10,510
That's why we're going to use IBM's
Watson Api to perform tone analysis on an

79
00:04:10,511 --> 00:04:13,670
example set. First we'll need to sign up
for their cloud service called blue mix.

80
00:04:13,690 --> 00:04:15,610
Once we registered,
we'll click on Watson,

81
00:04:15,660 --> 00:04:19,210
then tone analyzer or create a new service
using the standard plan which lets us

82
00:04:19,211 --> 00:04:21,730
try it out for free so no
need to enter a credit card,

83
00:04:21,760 --> 00:04:23,200
which is perfect because I'm poor.

84
00:04:23,230 --> 00:04:26,500
Then we'll click on service credentials
and record our username and password

85
00:04:26,590 --> 00:04:28,840
since we'll need them to
authenticate from our web APP.

86
00:04:28,900 --> 00:04:32,620
Then we can generate a new note express
web app using NPM and the express

87
00:04:32,621 --> 00:04:36,820
generator module in our APP will import
our Watson developer cloud npm module,

88
00:04:36,940 --> 00:04:39,670
which acts as a thin javascript
wrapper around their API.

89
00:04:39,730 --> 00:04:41,830
We have two routes here,
both of our index page.

90
00:04:41,860 --> 00:04:44,680
We'll start with the get route and we
make a get request to the index page.

91
00:04:44,770 --> 00:04:47,560
We want to display some html that
we can send programmatically. Here.

92
00:04:47,590 --> 00:04:49,960
We'll send an input form that
asks for some corporates.

93
00:04:50,170 --> 00:04:53,610
Then in our post trout will retrieve
the input that the user submits via the

94
00:04:53,620 --> 00:04:56,620
post request. Then initialize
the tone analyzer variable,

95
00:04:56,650 --> 00:05:00,580
filling our credentials and call the tone
function using the user input variable

96
00:05:00,581 --> 00:05:03,400
as the parameter. This will return
to the analysis of the text as Jason,

97
00:05:03,430 --> 00:05:05,260
which we can view in terminal.
Let's try it out.

98
00:05:05,320 --> 00:05:09,280
As you can see it race the Corbett's on
three levels, emotions, language, style,

99
00:05:09,281 --> 00:05:10,420
and social tendencies.

100
00:05:10,450 --> 00:05:14,140
You'll get back a percentage for each
of three language styles and five social

101
00:05:14,141 --> 00:05:17,440
tendencies. It does this at both the
document level and sentence level.

102
00:05:17,470 --> 00:05:21,130
Tone analysis is a tool you can use to
better your writing and be more clear

103
00:05:21,131 --> 00:05:24,430
about the message you're trying to convey.
Dope links for you down below.

104
00:05:24,460 --> 00:05:28,000
Please subscribe for more ml videos.
I've got to go fix an indentation error,

105
00:05:28,030 --> 00:05:29,230
so thanks for watching.

