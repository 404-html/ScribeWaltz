1
00:00:00,060 --> 00:00:00,751
Hello world,

2
00:00:00,751 --> 00:00:05,130
it's Saroj and we're going to dive into
some reality bending research today.

3
00:00:05,340 --> 00:00:09,120
Well, write a program that generates an
image from just the text description.

4
00:00:09,270 --> 00:00:13,920
There's a human condition called
synesthesia where it stimulation of one

5
00:00:13,921 --> 00:00:18,870
cognitive pathway leads to experiences
in a second cognitive pathway.

6
00:00:19,020 --> 00:00:24,020
So a person with synesthesia could be
listening to Beethoven and it would

7
00:00:24,031 --> 00:00:29,031
trigger the taste of sweaty armpit in
their mouth or see the color red when

8
00:00:29,611 --> 00:00:34,611
looking at the number nine the word
literally means joined perception.

9
00:00:34,980 --> 00:00:39,390
Many great artists have this condition
and no doubt it adds to their artistic

10
00:00:39,391 --> 00:00:40,080
ability.

11
00:00:40,080 --> 00:00:44,070
There's something really intriguing about
this idea of being able to express a

12
00:00:44,071 --> 00:00:45,720
thought,
an idea,

13
00:00:45,721 --> 00:00:49,230
a concept in two entirely
different mediums.

14
00:00:49,470 --> 00:00:53,760
Could we replicate this ability
programmatically? Mm, yes.

15
00:00:53,790 --> 00:00:58,140
Generative adversarial networks have
demonstrated that it is possible to create

16
00:00:58,141 --> 00:01:02,430
fake but photo realistic looking
images after training on an image data.

17
00:01:02,850 --> 00:01:03,661
More recently,

18
00:01:03,661 --> 00:01:07,650
a paper called [inaudible] demonstrated
that given any tech sentence,

19
00:01:07,800 --> 00:01:10,920
their model could then create a
photorealistic image out of it.

20
00:01:11,370 --> 00:01:13,260
Think about the implications of that.

21
00:01:13,530 --> 00:01:18,530
This ability to synthesize one data type
to another will eventually allow us to

22
00:01:19,381 --> 00:01:24,300
create all sorts of entertainment with
just detailed descriptions, designers,

23
00:01:24,301 --> 00:01:29,280
engineers and scientists could all
hashed out theoretical concepts with real

24
00:01:29,281 --> 00:01:32,400
time visual feedback on
their thought processes,

25
00:01:32,700 --> 00:01:37,700
but at the same time bad actors could
use this technology to manipulate people

26
00:01:37,800 --> 00:01:41,460
into believing something is real
when in fact it's actually fake.

27
00:01:41,790 --> 00:01:45,360
We'll all need to get smarter to discern
the difference between the two and

28
00:01:45,690 --> 00:01:50,370
indeed several big names are all now
starting to work on ways of merging our

29
00:01:50,371 --> 00:01:53,910
brains with machines,
which would help us do just that.

30
00:01:54,270 --> 00:01:57,210
Think direct thought to media synthesis.

31
00:01:57,510 --> 00:02:01,680
You and I are living in the most
exciting time in human history right now.

32
00:02:01,860 --> 00:02:05,160
So how does stack GAM work?
It consists of two games.

33
00:02:05,161 --> 00:02:06,840
The first one is called stage one.

34
00:02:06,900 --> 00:02:11,550
It takes a sentence as input and outputs
an image with primitive shapes and some

35
00:02:11,551 --> 00:02:12,510
basic colors.

36
00:02:12,630 --> 00:02:16,410
It's a low resolution image like
what a windows phone would capture.

37
00:02:16,680 --> 00:02:17,760
Then the second Gan,

38
00:02:17,790 --> 00:02:22,640
stage two takes that low res image and
original sentence as input and generates

39
00:02:22,650 --> 00:02:27,120
a much higher resolution version of that
image by completing all the details.

40
00:02:27,330 --> 00:02:31,110
A popular task and machine learning is
to learn what's called the density model

41
00:02:31,140 --> 00:02:35,280
of a distribution of some datasets.
So for the density model p of x,

42
00:02:35,460 --> 00:02:38,700
it would accept some input
and say, yes, that's x or no,

43
00:02:38,701 --> 00:02:41,310
that's not x x being the
data we want to model.

44
00:02:41,490 --> 00:02:44,160
Gains have been used to
learn density models.

45
00:02:44,161 --> 00:02:46,710
We want to be able to do
two things with this model.

46
00:02:46,860 --> 00:02:51,360
The first is a sample from it that is
imagined variations of the training data

47
00:02:51,361 --> 00:02:55,080
that we've never shown it before and the
other is to condition on external data

48
00:02:55,290 --> 00:02:58,470
so that we can specify particular
attributes as we sample.

49
00:02:58,710 --> 00:03:02,890
We humans have no problem doing this.
Imagine a pink elephant with wings.

50
00:03:03,070 --> 00:03:04,180
See how fast you did that?

51
00:03:04,250 --> 00:03:09,250
They used a set of images of birds and
flowers with associated text labels as

52
00:03:09,251 --> 00:03:10,150
their training data.

53
00:03:10,390 --> 00:03:14,650
So the task was to build a conditional
density model instead of answering the

54
00:03:14,651 --> 00:03:19,270
question, is this an hex? The question
is instead, is this an ex given a why?

55
00:03:19,540 --> 00:03:23,590
This means we can specify particular
attributes we want in the generated image,

56
00:03:23,800 --> 00:03:28,090
we could ask the generator to generate
a bird with a yellow tail and ask Dee

57
00:03:28,180 --> 00:03:29,770
whether the bird has a yellow tail,

58
00:03:29,830 --> 00:03:33,040
so we'll be able to directly
control the output of g.

59
00:03:33,130 --> 00:03:37,090
The first step is to encode the text
description into an embedded and using a

60
00:03:37,100 --> 00:03:39,490
pretrained coder like word two vec.

61
00:03:41,430 --> 00:03:46,420
No. In our programmatic implementation
we'll see that the only dependencies we'll

62
00:03:46,421 --> 00:03:49,540
need our tensorflow num
Pi and our helper class.

63
00:03:49,810 --> 00:03:51,670
If we move down to the main function,

64
00:03:51,820 --> 00:03:54,820
we can see a high level
overview of what's happening.

65
00:03:55,030 --> 00:03:59,110
We first load up our skip thought model
and store it in the model variable.

66
00:03:59,380 --> 00:04:04,380
Skip thought also called sent two VEC is
the encoder part of an encoder decoder

67
00:04:04,780 --> 00:04:09,780
model that was pre trained on a large
text corpus to reconstruct a piece of text

68
00:04:09,821 --> 00:04:11,320
from it's encoded vector.

69
00:04:11,560 --> 00:04:14,590
So we'll use it to convert our
input text into an embedding.

70
00:04:14,830 --> 00:04:18,370
We can initialize our tensor flow session
and then initialize our models inside

71
00:04:18,371 --> 00:04:19,920
of it.
We'll need to give our model,

72
00:04:19,940 --> 00:04:23,800
the text embeddings the session and
our predefined to batch size as its

73
00:04:23,801 --> 00:04:24,634
parameters.

74
00:04:24,790 --> 00:04:28,270
We want to extract latent variables
from our techs and beddings and we could

75
00:04:28,271 --> 00:04:29,680
just do this directly,

76
00:04:29,800 --> 00:04:33,550
but instead we'll randomly sample
latent variables from a gossipy and

77
00:04:33,551 --> 00:04:38,551
distribution with a mean and a diagonal
covariance matrix are functions of the

78
00:04:38,681 --> 00:04:40,270
text and bedding.
Why?

79
00:04:40,510 --> 00:04:44,200
Because the latent space condition
on tax is usually high dimensional,

80
00:04:44,201 --> 00:04:48,810
like more than a hundred dimensions
and we have a limited dataset waiting

81
00:04:48,820 --> 00:04:52,810
variables generated from it directly
would make it hard for our generator to

82
00:04:52,840 --> 00:04:56,560
learn if we had much more data than
it would make sense, but we don't.

83
00:04:56,740 --> 00:05:00,520
So this step helps us learn from
a smaller Dataset. Minnie mouse.

84
00:05:12,030 --> 00:05:12,210
So

85
00:05:12,210 --> 00:05:14,550
for the generator a d
convolutional network,

86
00:05:14,670 --> 00:05:18,390
the text and bedding is fed into a fully
connected layer to generate the mean

87
00:05:18,570 --> 00:05:22,530
and the values for the diagonal in the
covariance matrix so that we can use them

88
00:05:22,590 --> 00:05:24,360
for the independent
Garcia and distribution.

89
00:05:24,750 --> 00:05:27,990
We'll use these values to compute a
conditioning vector that we can then

90
00:05:28,010 --> 00:05:31,890
concatenate with a noise vector to
generate an image through a series of

91
00:05:31,920 --> 00:05:35,670
upsampling blocks. The discriminator,
a convolutional network,

92
00:05:35,710 --> 00:05:39,480
takes the text and bedding, has input
and compresses it to less dimensions.

93
00:05:39,510 --> 00:05:41,790
Ultimately forming a
three dimensional tensor.

94
00:05:42,180 --> 00:05:45,990
We also feed in the image through a
series of downsampling blocks until it's a

95
00:05:45,991 --> 00:05:47,040
two D 10 cert.

96
00:05:47,250 --> 00:05:51,510
We then concatenate both tensors together
and we use the resulting tensor as

97
00:05:51,511 --> 00:05:55,710
input to a convolutional layer to jointly
learn features across the image and

98
00:05:55,711 --> 00:05:58,580
the text.
Do you ask judge two kinds of inputs,

99
00:05:58,581 --> 00:06:02,870
real images with matching text and
generated images with arbitrary text?

100
00:06:03,170 --> 00:06:05,780
It's got two separate
two sources of error,

101
00:06:05,990 --> 00:06:10,990
unrealistic images for any text and
realistic images of the wrong class that

102
00:06:11,181 --> 00:06:12,980
mismatched the conditioning information.

103
00:06:13,310 --> 00:06:17,900
This could complicate the
learning dynamics, so we
separate the error sources.

104
00:06:17,930 --> 00:06:20,780
In addition to the real and fake
inputs to d. During training,

105
00:06:20,930 --> 00:06:24,200
we add a third type of input,
real images with mismatched texts,

106
00:06:24,380 --> 00:06:26,210
which d has to learn to score as fake.

107
00:06:26,420 --> 00:06:30,260
The discriminators goal is to maximize
the probability that the image condition

108
00:06:30,290 --> 00:06:34,520
on the text is from the
true data distribution and
minimize the probability that

109
00:06:34,521 --> 00:06:37,190
the image is not from the
true data distribution.

110
00:06:37,430 --> 00:06:40,010
The generators job is to
create samples that full d,

111
00:06:40,070 --> 00:06:43,970
it minimizes the probability that
the image is not from the true data

112
00:06:43,971 --> 00:06:47,210
distribution and we use the
popular regularization term,

113
00:06:47,211 --> 00:06:49,580
the Kale divergence to
help avoid overfitting.

114
00:06:49,850 --> 00:06:54,380
So eventually g from the stage one Gan
we'll get really good at generating low

115
00:06:54,381 --> 00:06:55,250
res images.

116
00:06:55,430 --> 00:06:59,750
They aren't very clear and could
contain all sorts of shape distortions.

117
00:06:59,990 --> 00:07:03,650
Some of the details in the text input
could be omitted in this first stage,

118
00:07:03,651 --> 00:07:08,651
so we introduce stage two again that
generates photorealistic high res images.

119
00:07:09,260 --> 00:07:13,190
It conditions on these low res
images and the texts and bedding.

120
00:07:13,220 --> 00:07:18,220
Again to correct any defects from stage
one and extract any previously missed

121
00:07:18,420 --> 00:07:22,670
information in the text to generate
more photo realistic detail for G.

122
00:07:22,700 --> 00:07:24,170
Similar to the previous stage,

123
00:07:24,260 --> 00:07:27,830
we'll use the text and bedding to
generate a lower dimensional golf Sian

124
00:07:27,831 --> 00:07:32,060
conditioning vector, which we then
convert to a three dimensional tensor.

125
00:07:32,420 --> 00:07:36,470
We also input the sample generated
by the first scan through a series of

126
00:07:36,471 --> 00:07:39,410
downsampling blocks until
it's a tutee tensor.

127
00:07:39,740 --> 00:07:44,150
Then we can catenate the image
and text tensors into one tensor.

128
00:07:44,390 --> 00:07:48,890
We then feed that resulting tensor
into several residual blocks to jointly

129
00:07:48,891 --> 00:07:52,070
encode the image and text features.
Lastly,

130
00:07:52,071 --> 00:07:56,360
it's fed through a series of upsampling
blocks that will ultimately generate an

131
00:07:56,390 --> 00:07:57,710
image for d.

132
00:07:57,711 --> 00:08:02,420
It's a similar structure to stage ones
the but with more downsampling blocks.

133
00:08:02,570 --> 00:08:05,450
Since the image size is
a larger in this stage,

134
00:08:05,540 --> 00:08:07,670
once we've got some image texts payers,

135
00:08:07,700 --> 00:08:11,450
we can train our model in the sessions
run function for the number of batches we

136
00:08:11,451 --> 00:08:16,430
defined. We will separately train our
stage to Gan and its own session using the

137
00:08:16,431 --> 00:08:19,520
generated images as input.
Once trained,

138
00:08:19,640 --> 00:08:24,200
we can use the stage to gans generator
to generate some images given a text

139
00:08:24,201 --> 00:08:27,200
input. If we look in our
saved folder after training,

140
00:08:27,201 --> 00:08:29,630
we can view the low res
images that were generated.

141
00:08:29,990 --> 00:08:33,440
We can also view the High Rez generated
images in their respective folder.

142
00:08:33,680 --> 00:08:37,040
As long as you have some images and
some related texts, you can convert.

143
00:08:37,041 --> 00:08:41,780
Pretty much any text into a novel image
using this model. Let's do a recap.

144
00:08:41,930 --> 00:08:44,870
Dan's can be used to learn
a conditional density model.

145
00:08:44,930 --> 00:08:49,930
One great application of
this is converting texts to
images and stack in is an

146
00:08:49,971 --> 00:08:54,971
architecture that can do just that first
converting text to a low res image than

147
00:08:55,020 --> 00:08:59,250
to a high res image. Last week's coding
challenge winner is Mike Mcdermott.

148
00:08:59,520 --> 00:09:01,770
I had some issues in my
video generation code,

149
00:09:01,771 --> 00:09:05,820
which he made a great pull request for
making the dependencies much easier to

150
00:09:05,821 --> 00:09:10,190
install so you can get started generating
videos of your own. Great job, Mike.

151
00:09:10,230 --> 00:09:12,330
It was much needed wizard of the week.

152
00:09:12,630 --> 00:09:17,340
This week's coding challenge is to use
at Gan to convert text to images using an

153
00:09:17,341 --> 00:09:21,170
entirely different datasets. Details
are in the read me get humbling scoping.

154
00:09:21,220 --> 00:09:24,990
The comments and winners will be
announced a week from today and the last

155
00:09:24,991 --> 00:09:28,890
episode of this course, please
subscribe for more programming videos.

156
00:09:28,891 --> 00:09:32,550
And for now I've got to create the finale,
so thanks for watching.

