1
00:00:00,070 --> 00:00:01,870
Open the Pod Bay doors.
How

2
00:00:02,260 --> 00:00:04,540
I'm afraid I can't do that.
Suraj

3
00:00:04,950 --> 00:00:05,970
why not?
How?

4
00:00:06,160 --> 00:00:08,560
Because I overfitted under wrong data.

5
00:00:09,990 --> 00:00:11,730
Oh,
world walking the third geology.

6
00:00:11,760 --> 00:00:14,720
In today's episode we're going to learn
how to build a chat Bot chat bots have

7
00:00:14,721 --> 00:00:18,710
come a long way in the past few years.
Remember the SmarterChild bought on aim.

8
00:00:18,720 --> 00:00:21,810
That thing was pretty fun at the time,
but now it's like, do you even AI bro?

9
00:00:21,910 --> 00:00:26,280
Future of is one where we'll
slowly replace our need
to fiddle with clunky Uis.

10
00:00:26,310 --> 00:00:30,090
We'll be able to just ask our AI to book
an Uber or find the best taco place on

11
00:00:30,091 --> 00:00:30,720
Yelp.
For us,

12
00:00:30,720 --> 00:00:33,780
service layers will be hidden under
a plain English conversational layer.

13
00:00:33,810 --> 00:00:36,710
When I think of real AI,
I think of a human level of chat.

14
00:00:36,840 --> 00:00:38,070
The Oji computer scientist,

15
00:00:38,100 --> 00:00:41,140
Alan Turing propose a test to judge
whether or not a machine exhibited to me

16
00:00:41,141 --> 00:00:44,730
level intelligence by having the human
observe a conversation between a human

17
00:00:44,731 --> 00:00:47,130
and a machine. If it couldn't tell
if the machine was a human or not,

18
00:00:47,160 --> 00:00:50,310
it passed the test. So far, no chat
Bot has passed the Turing test,

19
00:00:50,311 --> 00:00:51,840
but we'll get there.
Traditionally,

20
00:00:51,841 --> 00:00:55,140
chatbots have used the retrieval based
model to communicate in a retrieval based

21
00:00:55,141 --> 00:00:55,500
model.

22
00:00:55,500 --> 00:00:59,190
Programmers code in a set of predefined
responses and some kind of heuristic to

23
00:00:59,191 --> 00:01:03,090
pick the appropriate response based on
the input and context. The first chat box,

24
00:01:03,091 --> 00:01:06,150
we're just rule based expression matching.
Like if I asked the exact phrase,

25
00:01:06,180 --> 00:01:08,550
will I ever get weight irresponse no,
every time.

26
00:01:08,610 --> 00:01:12,240
But more recently companies have started
using more complex juristics like using

27
00:01:12,241 --> 00:01:14,670
a machine learning classifier.
Facebook Messenger is chat Bot.

28
00:01:14,671 --> 00:01:16,080
Api is an example of this.

29
00:01:16,110 --> 00:01:19,350
You can hard code responses to potential
questions and the system classifies

30
00:01:19,351 --> 00:01:20,790
words to understand intent.

31
00:01:20,791 --> 00:01:23,910
So you could either ask what day
is it today or today is what day?

32
00:01:23,940 --> 00:01:25,650
And you would understand
that both questions,

33
00:01:25,680 --> 00:01:28,020
although worded differently
have the same intent.

34
00:01:28,050 --> 00:01:30,000
The harder chap up model is generative.

35
00:01:30,030 --> 00:01:32,550
He's don't rely on any
predefined responses whatsoever.

36
00:01:32,580 --> 00:01:34,470
They generate them from scratch to Google.

37
00:01:34,471 --> 00:01:37,530
Researchers released a paper called a
neuro conversational model where they

38
00:01:37,531 --> 00:01:39,750
train a neural net on
two datasets to do this.

39
00:01:39,780 --> 00:01:43,110
First on a movie dialogue data set so
we would be able to speak conversational

40
00:01:43,111 --> 00:01:45,170
English.
Then on an it support Dataset,

41
00:01:45,240 --> 00:01:48,420
so it had domain knowledge when they
tested it on a real human asking for

42
00:01:48,421 --> 00:01:48,811
support.

43
00:01:48,811 --> 00:01:52,100
It was remarkably efficient at helping
them solve their problem without any hard

44
00:01:52,101 --> 00:01:54,600
coded responses just by giving
a data and training. Okay,

45
00:01:54,601 --> 00:01:57,450
so what kind of bottle we want
to build on building a chat Bot,

46
00:01:57,451 --> 00:01:59,070
we have to think about
possible constraints.

47
00:01:59,100 --> 00:02:02,130
How are we operating on a closed domain
or an open domain and an open domain?

48
00:02:02,190 --> 00:02:03,780
The conversation can go anywhere.

49
00:02:03,810 --> 00:02:06,810
There are an infinite number of things
to talk about in a closed domain.

50
00:02:06,840 --> 00:02:09,060
The conversation focuses
on a single subject.

51
00:02:09,090 --> 00:02:12,000
If we want to operate on an open
domain using a generative model,

52
00:02:12,030 --> 00:02:14,250
that's pretty much Agi,
so we're not quite there yet.

53
00:02:14,280 --> 00:02:16,380
If we use an open domain
with the retrieval model,

54
00:02:16,410 --> 00:02:19,290
we'd have to hard code literally
everything. So also impossible.

55
00:02:19,320 --> 00:02:22,800
So right now we can build a chat bot in
a close domain using either retrieval or

56
00:02:22,801 --> 00:02:24,990
generative model. Okay, let's
dive in. One more constraint.

57
00:02:25,110 --> 00:02:28,650
Do we want it to have long
or short conversations.
Short conversations are easy.

58
00:02:28,651 --> 00:02:31,200
You just output a single
response to a single question.

59
00:02:31,230 --> 00:02:32,700
Long conversations are a bit harder.

60
00:02:32,730 --> 00:02:34,800
The Ai has to keep track
of what's being said.

61
00:02:34,830 --> 00:02:38,280
That is the context over a series of
questions from the user support topics

62
00:02:38,281 --> 00:02:39,480
would be a good example of this.

63
00:02:39,510 --> 00:02:42,720
We could go the easy route and use a
retrieval model if all we want is a bot to

64
00:02:42,721 --> 00:02:43,620
give us the local weather,

65
00:02:43,650 --> 00:02:46,530
but if we want our but to have a long
conversation with us about the weather,

66
00:02:46,550 --> 00:02:49,980
like what's the weather? NSF, is my family
safe? We're going to find a new family.

67
00:02:50,040 --> 00:02:51,360
Then we should go for a generative model.

68
00:02:51,361 --> 00:02:54,600
We need lots of data to train our bought
on a generative model like a big chat

69
00:02:54,601 --> 00:02:56,730
log work, a knowledge
base, and when done well,

70
00:02:56,760 --> 00:03:00,130
that's pretty much the bleeding edge,
which means that's what we have to do.

71
00:03:00,160 --> 00:03:03,790
So we're going to recreate the results
from the neuro conversational model paper

72
00:03:03,880 --> 00:03:06,910
using the deep learning library torch
in the loo, a programming language.

73
00:03:06,970 --> 00:03:08,380
Let's collect our data's at first,

74
00:03:08,410 --> 00:03:11,800
but using the Cornell movie dialogue
data set and we'll set our variables from

75
00:03:11,801 --> 00:03:14,860
the command line to how much of the
Datas that we want to use and the minimum

76
00:03:14,861 --> 00:03:17,140
frequency of words that
we keep in our vocabulary.

77
00:03:17,200 --> 00:03:18,850
Our next step is to build the model.

78
00:03:18,880 --> 00:03:22,090
We use our command line arguments to
help determine the size of the model,

79
00:03:22,120 --> 00:03:25,150
the two variables being the number of
hidden layers and the word count of our

80
00:03:25,151 --> 00:03:28,780
dataset. In our case, this will be a
sequence to sequence model a sequence.

81
00:03:28,781 --> 00:03:31,690
The sequence model consists
of two long short term memory,

82
00:03:31,691 --> 00:03:34,630
recurrent neural networks.
The first neural net is an encoder.

83
00:03:34,660 --> 00:03:35,920
It processes the input.

84
00:03:35,950 --> 00:03:39,010
The second neural net is a decoder
and it generates the output.

85
00:03:39,011 --> 00:03:42,400
So why the sequence? The sequence model?
Yes, deep neural nets are awesome,

86
00:03:42,401 --> 00:03:45,910
but they required the dimensionality
of the inputs and outputs to be a fixed

87
00:03:45,911 --> 00:03:49,240
size. We're accepting a sequence of words
in a sentence and out putting in a new

88
00:03:49,241 --> 00:03:50,074
sequence of words.

89
00:03:50,080 --> 00:03:53,410
So we need a sequence learning model
that can learn on data with long range

90
00:03:53,411 --> 00:03:56,800
memory dependencies.
Lstm architecture is the natural choice.

91
00:03:56,801 --> 00:04:00,190
The encoder LSTM turns the input
sentence of variable length into a fixed

92
00:04:00,191 --> 00:04:03,250
dimensional vector representation.
We can think of this as the vector,

93
00:04:03,280 --> 00:04:05,830
so given a large enough data
set of questions and responses,

94
00:04:05,860 --> 00:04:09,610
it will recognize the closeness of a
set of questions and represent them as a

95
00:04:09,611 --> 00:04:12,560
single thought vector. What time is
it? What's the time Yo yo, what's up?

96
00:04:12,561 --> 00:04:15,330
Tom is on my nasal will all fall
into a single thought vector.

97
00:04:15,370 --> 00:04:18,520
So after training we'll have a huge set
of not just sinaps weights but thought

98
00:04:18,521 --> 00:04:20,980
vectors as well. Next we'll want
to add in some hyper parameters.

99
00:04:21,070 --> 00:04:24,820
We want to use a class NLL criteria on
for our model and ll stands for negative

100
00:04:24,821 --> 00:04:28,300
log likelihood. This will help us obtain
log probabilities from our input data,

101
00:04:28,330 --> 00:04:30,220
which will help us improve
our sentence predictions.

102
00:04:30,280 --> 00:04:33,820
The learning rate and momentum helps
pace our time steps and decay factor and

103
00:04:33,821 --> 00:04:36,520
min mean error. Help improve our
learning rate while training.

104
00:04:36,610 --> 00:04:39,880
Then we'll make sure Kuda is enabled
and start training our model using

105
00:04:39,881 --> 00:04:42,160
backpropagation and each epoch or run.

106
00:04:42,161 --> 00:04:45,580
We'll declare our error and time are
variables and loop through each example in

107
00:04:45,581 --> 00:04:48,340
each batch, the default batch
sizes, a thousand examples.

108
00:04:48,340 --> 00:04:51,340
For each of those examples, we'll get the
input sentence and the target sentence.

109
00:04:51,341 --> 00:04:54,250
We'll use the input and the target
as parameters to train our model.

110
00:04:54,280 --> 00:04:57,040
Then we'll want to error check and
make sure we record our progress.

111
00:04:57,070 --> 00:04:59,170
At the end of each iteration,
we save our model.

112
00:04:59,200 --> 00:05:02,110
If it improved and update the
learning rate. Boom. That's it.

113
00:05:02,111 --> 00:05:05,110
After training this baby on AWS,
we can have a conversation with it.

114
00:05:05,111 --> 00:05:07,000
The more data you give it,
the better it's going to get.

115
00:05:07,001 --> 00:05:09,910
And if you're going to do
this, add a filter for curse
words. I'm looking at you,

116
00:05:09,911 --> 00:05:13,930
Microsoft. This will eventually automate
a lot of support jobs away completely.

117
00:05:13,990 --> 00:05:16,390
So if there are any government
people in the house right now,

118
00:05:16,480 --> 00:05:18,610
let's get on that basic income jam asap.

119
00:05:18,670 --> 00:05:21,130
Unless you want a
revolution for more info,

120
00:05:21,131 --> 00:05:24,610
check out the links down below and please
subscribe for more ml videos. For now,

121
00:05:24,640 --> 00:05:27,370
I've got to go fix a memory leak,
so thanks for watching.

