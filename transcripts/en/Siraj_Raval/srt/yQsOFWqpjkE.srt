1
00:00:00,100 --> 00:00:03,520
Hello world, it's Saroj and let's
visualize some data, shall we?

2
00:00:03,880 --> 00:00:06,950
There was a refill lab study where
a group of scientists straps some

3
00:00:06,951 --> 00:00:09,130
participants what they
fitness tracking device.

4
00:00:09,280 --> 00:00:12,820
Then ask them to do a bunch of
exercises while recording some physical

5
00:00:12,821 --> 00:00:15,310
measurements.
And I've got that Dataset,

6
00:00:15,450 --> 00:00:19,750
will visualize it in a two d graph so
we can make some discoveries from it.

7
00:00:20,170 --> 00:00:22,000
We live in a three dimensional world,

8
00:00:22,001 --> 00:00:24,520
so we can understand
things in one dimension,

9
00:00:24,730 --> 00:00:29,710
two dimensions and three dimensions pretty
easily. But data can be complex. AAF,

10
00:00:29,950 --> 00:00:34,030
sometimes data demands that we tried to
reason in hundreds or even thousands of

11
00:00:34,031 --> 00:00:36,910
dimensions at some
fundamental level are puny.

12
00:00:36,911 --> 00:00:39,310
Biological brains just can't do that.

13
00:00:39,520 --> 00:00:43,300
So we've invented machine learning to
help us learn patterns in our data that we

14
00:00:43,301 --> 00:00:46,420
can't recognize ourselves.
Take Alphago for example,

15
00:00:46,540 --> 00:00:49,360
because they could reason about
so many possibilities at once.

16
00:00:49,570 --> 00:00:52,930
It made moves that seems strange at
first to the world champion it played

17
00:00:52,931 --> 00:00:57,220
against, but then it ended up beating
him by doing that or IBM's Watson.

18
00:00:57,400 --> 00:01:01,630
It consistently diagnosed cancer better
than the best doctors because it was

19
00:01:01,631 --> 00:01:06,310
able to analyze millions of
cancer research papers at
once and match a patient's

20
00:01:06,311 --> 00:01:08,650
genetic profile to what it had learned.

21
00:01:08,800 --> 00:01:11,830
And there are so many things that
ml hasn't yet been applied to.

22
00:01:11,831 --> 00:01:13,260
So opportunity is right

23
00:01:13,840 --> 00:01:17,460
and turns off the satellites for you.

24
00:01:17,470 --> 00:01:20,600
We'll launch his own gym satellite,
we're going to collect that data.

25
00:01:23,430 --> 00:01:23,610
Our

26
00:01:23,610 --> 00:01:25,680
exercise data is kind of complex as well,

27
00:01:25,681 --> 00:01:29,340
but we're going to figure out how to
visualize it so that we can understand it.

28
00:01:29,370 --> 00:01:30,990
So let's take a look at this data.

29
00:01:31,050 --> 00:01:35,160
Each row represents a different person
and each column is one of many physical

30
00:01:35,161 --> 00:01:38,160
measurements,
like the position of their arm or forearm.

31
00:01:38,430 --> 00:01:42,600
And each person gets one of five class
labels like sitting or standing that

32
00:01:42,601 --> 00:01:46,080
represents the activity they've done.
So there's a lot going on here,

33
00:01:46,081 --> 00:01:49,050
but you'll notice that some of
these cells have empty value.

34
00:01:49,051 --> 00:01:52,680
So the first thing we'll want to do
is clean our data by removing them.

35
00:01:52,920 --> 00:01:56,130
Let's first import pandas,
which is our data analysis library.

36
00:01:56,280 --> 00:01:58,050
That will help us read our CSV file.

37
00:01:58,290 --> 00:02:02,130
Then we'll import num pi to help us
transform our data into a format our model

38
00:02:02,131 --> 00:02:03,030
can understand.

39
00:02:03,350 --> 00:02:06,840
Psychic learn will help us create our
machine learning model and map plot live

40
00:02:06,841 --> 00:02:08,970
will eventually help
us visualize our data.

41
00:02:09,300 --> 00:02:11,820
Now that we've imported our dependencies,
we can use pandas,

42
00:02:11,821 --> 00:02:16,680
read CSV function to download that
exercise Dataset directly from the Web and

43
00:02:16,681 --> 00:02:20,550
store it and the data frame all variable
and we'll also create a variable to

44
00:02:20,551 --> 00:02:24,600
store the number of rows in the data by
calling the shape function on the zeroth

45
00:02:24,601 --> 00:02:26,490
column.
To get the count of rows,

46
00:02:26,730 --> 00:02:30,750
we'll call the is no function than the
sum function to get the total sum of the

47
00:02:30,751 --> 00:02:32,790
no or empty columns in our dataset.

48
00:02:33,180 --> 00:02:36,930
Then we'll create another variable to
count the number of non empty columns in

49
00:02:36,931 --> 00:02:39,870
our dataset using the previous
variable as a parameter.

50
00:02:40,200 --> 00:02:44,190
Now we can remove the columns with missing
elements by only using the non empty

51
00:02:44,191 --> 00:02:46,560
columns. Also, if we look at our data,

52
00:02:46,590 --> 00:02:50,940
the first seven columns don't have
information we can use to differentiate

53
00:02:50,941 --> 00:02:53,460
between our classes,
so let's remove them as well.

54
00:02:53,460 --> 00:02:57,510
Using the I x function which asks for
the index of columns we want to delete,

55
00:02:57,810 --> 00:03:02,170
we'll specify from the up two column
seven we're going to take this clean data

56
00:03:02,171 --> 00:03:06,430
and transform it into a set of vectors
which we can then feed to our learning

57
00:03:06,431 --> 00:03:07,110
algorithm.

58
00:03:07,110 --> 00:03:10,990
A vector is a set of numbers and it's
how we represent data in machine learning

59
00:03:11,230 --> 00:03:15,400
will create directors to represent the
features for each person in our data.

60
00:03:15,670 --> 00:03:19,000
Let's grab all of our features from our
data and store them in a variable we

61
00:03:19,001 --> 00:03:19,834
call x.

62
00:03:20,080 --> 00:03:23,830
Then we'll want to standardize those
features using the standard scaler object

63
00:03:24,010 --> 00:03:25,990
of psychic learn.
In math terms,

64
00:03:25,991 --> 00:03:30,310
this means shifting the distribution of
each feature to have a mean of zero and

65
00:03:30,311 --> 00:03:31,900
a standard deviation of one,

66
00:03:32,110 --> 00:03:36,010
which is a way of saying make all the
features operate on the same scale so they

67
00:03:36,011 --> 00:03:38,920
are all in proportion to one another.
This will improve.

68
00:03:38,921 --> 00:03:42,820
The quality of our results will store
the resulting 70 dimensional feature

69
00:03:42,821 --> 00:03:45,310
vectors and the ex STD variable.

70
00:03:45,340 --> 00:03:47,920
Since 70 is the number
of features we have,

71
00:03:48,480 --> 00:03:53,370
haven't seen India trying to understand
this data set. Each feast is dimension.

72
00:03:53,400 --> 00:03:56,970
Then the note f correct.
If each feature equals one dimensions,

73
00:03:56,971 --> 00:04:00,360
that ain't no sweat check in
heartbeat, that's easy. If it goes up,

74
00:04:00,361 --> 00:04:04,320
then there's a threat, but behold, once
that adds time and it goes up and down,

75
00:04:04,500 --> 00:04:07,800
when I add temperature,
we can move it all around,

76
00:04:07,920 --> 00:04:10,980
but weight and height and strength
and a hundred more features.

77
00:04:11,250 --> 00:04:15,960
I can't visualize that. I'll get me a
seizure. Let's reduce dimensionality,

78
00:04:16,020 --> 00:04:19,530
the two or three so we can see
and understand this data set.

79
00:04:19,680 --> 00:04:21,300
He's the lead data city.

80
00:04:21,340 --> 00:04:25,030
There's an entire subfield of machine
learning called dimentionality reduction.

81
00:04:25,240 --> 00:04:29,710
The Amsa let us represent high dimensional
data in a twoD or three d space.

82
00:04:30,070 --> 00:04:34,180
Even a picture can be considered to have
32 million dimensions if we consider

83
00:04:34,210 --> 00:04:36,070
every single pixel to be a dimension,

84
00:04:36,460 --> 00:04:39,100
but it can also be considered
to have just two dimensions,

85
00:04:39,240 --> 00:04:40,510
length and width of a photo.

86
00:04:40,690 --> 00:04:44,770
We just need to find the intrinsic load
dimentionality hidden in our data so we

87
00:04:44,771 --> 00:04:48,940
can visualize it. One of the most popular
ways to do this is called [inaudible],

88
00:04:48,970 --> 00:04:50,680
which stands for distributed.

89
00:04:50,710 --> 00:04:53,890
The castic neighbor embedding
say that three times fast go.

90
00:04:54,250 --> 00:04:58,480
TSMC will allow us to reduce our
vectors dimentionality to just to,

91
00:04:58,810 --> 00:05:02,530
it does this by taking each one of
our 70 dimensional feature vectors and

92
00:05:02,531 --> 00:05:05,560
finding the similarity between
it and every other vector.

93
00:05:05,770 --> 00:05:10,630
The similarities are represented as values
and stored in a similarity matrix and

94
00:05:10,631 --> 00:05:14,470
then creates a similarity matrix for the
projected map points which will contain

95
00:05:14,471 --> 00:05:16,660
our final representation of the Dataset.

96
00:05:16,960 --> 00:05:20,890
Our first similarity matrix represents
where we are in our second represents

97
00:05:20,891 --> 00:05:22,510
where we ideally want to be.

98
00:05:22,690 --> 00:05:26,680
We can minimize the distance between these
two matrices using a process known as

99
00:05:26,681 --> 00:05:27,760
gradient descent.

100
00:05:28,030 --> 00:05:31,840
This will slowly bring down
the dimensionality of our
first similarity matrix by

101
00:05:31,900 --> 00:05:34,420
updating its values over time.
When it's over,

102
00:05:34,421 --> 00:05:37,930
we can use the trained matrix to
map the points into the space.

103
00:05:38,290 --> 00:05:42,160
We'll initialize our t snd model via
psychic learn and set the number of

104
00:05:42,161 --> 00:05:46,570
components to to this perimeter is asking
how many dimensions do we want our end

105
00:05:46,571 --> 00:05:47,404
result to be in.

106
00:05:47,500 --> 00:05:50,980
We'll fit it on our feature vectors
and store the resulting two dimensional

107
00:05:50,981 --> 00:05:55,000
feature vectors and the x test two
d variable. Now that we have that,

108
00:05:55,001 --> 00:05:59,330
we can plot our points on a two d graph
by first creating a legend for our class

109
00:05:59,331 --> 00:06:02,030
labels and plotting each
point using map plot live.

110
00:06:02,270 --> 00:06:05,120
We'll define the location of
our legend and show our graph.

111
00:06:05,360 --> 00:06:09,530
We can see here that point of the same
class tend to cluster together and r,

112
00:06:09,531 --> 00:06:13,580
t s and d helped make that happen without
knowing the classes of the feature

113
00:06:13,581 --> 00:06:14,540
vectors,
we fed it.

114
00:06:14,840 --> 00:06:19,010
It learned how to represent the
similarity between these classes in a two

115
00:06:19,011 --> 00:06:20,150
dimensional space.

116
00:06:20,420 --> 00:06:24,020
We can further analyze this plot to
study why certain classes are clustering

117
00:06:24,021 --> 00:06:26,240
together and what
conclusions this gives us.

118
00:06:26,450 --> 00:06:30,110
Like these two classes are near each
other since the actions are similar.

119
00:06:30,111 --> 00:06:34,370
So perhaps the more movement and exercise
requires the farther away it will

120
00:06:34,371 --> 00:06:35,450
cluster from the rest.

121
00:06:35,750 --> 00:06:38,540
There are a couple of live demos
of Tsne on the web as well,

122
00:06:38,541 --> 00:06:41,420
like this one that visualized
as a bunch of tweets.

123
00:06:41,690 --> 00:06:45,200
You can see that similar sounding
tweets tend to cluster together,

124
00:06:45,230 --> 00:06:49,280
so to break it down high dimensional data
is everywhere and machine learning can

125
00:06:49,281 --> 00:06:50,600
help us understand it.

126
00:06:50,930 --> 00:06:55,130
If we were dosed the dimensionality
of our data to twoD or Three d space,

127
00:06:55,460 --> 00:07:00,320
we can visualize it ourselves and Tsne
is a popular dimensionality reduction

128
00:07:00,321 --> 00:07:02,870
techniques that you can
use via psychic learn.

129
00:07:03,200 --> 00:07:06,140
The challenge winner for last
week's video is Keegan Taylor.

130
00:07:06,410 --> 00:07:10,580
He can use tensorflow to train a neural
net to classify Pokemon by their type

131
00:07:10,940 --> 00:07:14,240
and the classifier had a
75% accuracy after training,

132
00:07:14,480 --> 00:07:18,650
which is pretty incredible one than anyone
else had bad ass of the week and the

133
00:07:18,651 --> 00:07:22,250
runner up is Michelle Batu,
very well documented and clean code.

134
00:07:22,610 --> 00:07:25,840
This week's coding challenges
and use Tsn to visualize it.

135
00:07:25,841 --> 00:07:29,840
Game of Thrones Dataset that all provide
and write down something you discovered

136
00:07:29,841 --> 00:07:30,890
once you visualized it,

137
00:07:31,130 --> 00:07:33,920
you're getting up submission should go
in the comments and I'll announce the

138
00:07:33,921 --> 00:07:36,410
winner next time, please
subscribe. And for now,

139
00:07:36,440 --> 00:07:39,020
I've got to make my new year's prediction.
I mean resolution.

140
00:07:39,320 --> 00:07:40,700
So thanks for watching.

