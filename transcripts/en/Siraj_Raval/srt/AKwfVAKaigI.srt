1
00:00:00,060 --> 00:00:04,690
What lights and yonder window breaks and
Juliet is the sun is the father. No, no,

2
00:00:04,691 --> 00:00:05,100
no.

3
00:00:05,100 --> 00:00:10,020
She's a bright ball of gas so someone's
son is Juliet is the star in the sky.

4
00:00:10,410 --> 00:00:11,570
Oh world walking this here,

5
00:00:11,580 --> 00:00:15,330
geology and this episode we're going to
build an AI reader that is an AI that

6
00:00:15,331 --> 00:00:16,830
can analyze human language.

7
00:00:16,890 --> 00:00:20,460
It's type of task is considered
natural language processing or NLP.

8
00:00:20,520 --> 00:00:24,240
NLP is the arc of solving engineering
problems that need to analyze or generate

9
00:00:24,241 --> 00:00:26,610
natural language text.
We see it all around us.

10
00:00:26,611 --> 00:00:29,880
Google needs to do it to
understand exactly what your
search query means so they

11
00:00:29,881 --> 00:00:31,620
can give you back relevant results.

12
00:00:31,621 --> 00:00:36,180
Twitter uses it to extract
the top trending topics.
Microsoft uses it for Incar,

13
00:00:36,181 --> 00:00:39,780
speech recognition and ops basically
extremely dope because it deals with

14
00:00:39,781 --> 00:00:43,170
language. Kurzweil once said
that language is the key to AI.

15
00:00:43,230 --> 00:00:46,980
A computer able to communicate
indistinguishably from
a human would be true.

16
00:00:46,981 --> 00:00:50,670
Ai there 6,500 known languages in the
world and each of them have their own

17
00:00:50,671 --> 00:00:52,050
rules for syntax and grammar.

18
00:00:52,080 --> 00:00:55,560
Some rules are easy like I before e
except after c and some are based on

19
00:00:55,561 --> 00:00:58,170
intuition.
Since there is no consistent use case.

20
00:00:58,200 --> 00:01:00,360
So how do we write code
to analyze language?

21
00:01:00,420 --> 00:01:03,420
Before the 80s NLP was mostly
just a bunch of hand coded rules.

22
00:01:03,421 --> 00:01:08,421
Like if you see the word dance translated
to Zach do or if a word ends in ing,

23
00:01:08,551 --> 00:01:10,380
label it as present tense.
Well this worked.

24
00:01:10,381 --> 00:01:13,140
It was really tedious and
there are million corner cases.

25
00:01:13,170 --> 00:01:15,420
It's just not a sustainable
path to language.

26
00:01:15,421 --> 00:01:18,270
Understand the way forward
was an is machine learning.

27
00:01:18,330 --> 00:01:21,900
So instead of hand coding the rules and
the AI learns the rules by analyzing a

28
00:01:21,901 --> 00:01:23,670
large corpus or piece of text.

29
00:01:23,700 --> 00:01:27,270
This is proven to be very useful and
applying deep learning to NLP is currently

30
00:01:27,271 --> 00:01:30,930
the bleeding edge. So when thinking
about what tool to demo in this video,

31
00:01:30,960 --> 00:01:34,920
I was really torn between an NLP API
called API to AI and Google is newly

32
00:01:34,921 --> 00:01:38,070
released English parts or a
[inaudible]. Wow. Api Dot.

33
00:01:38,071 --> 00:01:41,690
Ai Takes an input query and returns
and an analysis of the text Carsey has,

34
00:01:41,691 --> 00:01:44,970
Google is newly released English parser.
Both have similar functionality,

35
00:01:44,971 --> 00:01:46,860
but I'm going to go with Parsi because a,

36
00:01:46,861 --> 00:01:49,800
it's currently the most
accurate parcer in the world be.

37
00:01:49,830 --> 00:01:53,160
If you built it into your app, that's one
less networking call you have to make,

38
00:01:53,161 --> 00:01:55,740
which means you can parse
tax offline and see.

39
00:01:55,770 --> 00:01:58,740
Building this parsing logic from the
source allows you to have more granular

40
00:01:58,741 --> 00:02:01,800
control over the details of how
you want text and be analyzed.

41
00:02:01,860 --> 00:02:03,660
Parsi was built using syntax net,

42
00:02:03,661 --> 00:02:06,960
an NLP neural net framework for Google's
tensorflow machine learning library,

43
00:02:06,961 --> 00:02:10,950
so we could use syntax net to build our
own parser or we could use a pretrained

44
00:02:10,980 --> 00:02:12,720
parser. Parsi. Yeah, let's do that.

45
00:02:12,750 --> 00:02:15,870
Once you parsed your texts are a whole
host of things you can do with it.

46
00:02:15,900 --> 00:02:17,430
Let's try it out with our own example.

47
00:02:17,460 --> 00:02:21,060
We're going to build a simple python
app that uses parts of parse phase to

48
00:02:21,061 --> 00:02:23,970
analyze a command by a user and
then repeat it back to them.

49
00:02:23,971 --> 00:02:26,910
Both word is different.
We'll begin by importing our dependencies.

50
00:02:26,940 --> 00:02:30,000
Then we'll set up our program to
receive in store that user input.

51
00:02:30,001 --> 00:02:31,950
The input text has the Corpus.
We'll be analyzing.

52
00:02:31,980 --> 00:02:35,130
We can get an array of all the part of
speech tags and the input text using the

53
00:02:35,131 --> 00:02:37,650
Tagore function.
So what is part of speech tagging?

54
00:02:37,680 --> 00:02:41,130
It's assigning grammatical labels
to each word in a given corporates,

55
00:02:41,160 --> 00:02:43,740
you know all of those words we learned
back in elementary school to take the

56
00:02:43,741 --> 00:02:46,290
phrase,
I saw her face and now I'm a believer.

57
00:02:46,320 --> 00:02:49,230
If we tagged each word in that phrase
individually without looking at the

58
00:02:49,231 --> 00:02:52,080
sentence as a whole,
we might tag saw as a certain bird,

59
00:02:52,081 --> 00:02:54,000
which means this would be
a quote from Leatherface.

60
00:02:54,030 --> 00:02:56,760
But if we look at this word in
the context of the sentence,

61
00:02:56,790 --> 00:02:58,470
we realized that it's a different verb.

62
00:02:58,500 --> 00:03:03,070
Google trained by interpreting sentences
from left to right for each word in the

63
00:03:03,071 --> 00:03:04,540
sentence and the words around it.

64
00:03:04,541 --> 00:03:07,810
They extracted a set of features
like the prefix and suffix,

65
00:03:07,840 --> 00:03:10,330
put them into data blocks,
concatenated them altogether,

66
00:03:10,360 --> 00:03:13,270
and fed them to a feed forward
neural net with lots of hidden lake,

67
00:03:13,300 --> 00:03:17,110
which would then predict the probability
distribution over set a possible pos

68
00:03:17,111 --> 00:03:19,720
tags and going in order from
left to right was useful.

69
00:03:19,750 --> 00:03:22,690
Since they could use a previous words.
Tag has a feature in the next word.

70
00:03:22,691 --> 00:03:25,540
So what does a parse tree,
what part of speech tagging isn't enough?

71
00:03:25,540 --> 00:03:26,370
There's another part.

72
00:03:26,370 --> 00:03:29,290
The meaning behind some piece of tax
isn't just the type of word that's being

73
00:03:29,291 --> 00:03:32,140
used, but also how that word
relates to the rest of the sentence.

74
00:03:32,141 --> 00:03:34,630
Take the example. Freights.
He fed her cat food.

75
00:03:34,660 --> 00:03:37,750
There are three possibilities of what
this phrase could mean. Number one,

76
00:03:37,751 --> 00:03:41,800
he had a woman's cats, some food. That's
the obvious one to us, intuitive humans,

77
00:03:41,801 --> 00:03:43,510
but there's also a number two,
he found a woman.

78
00:03:43,511 --> 00:03:45,760
Some food that was intended
for a cap or number three,

79
00:03:45,890 --> 00:03:47,920
somehow encouraged some
cat boot to eat something.

80
00:03:47,950 --> 00:03:50,700
The meaning of the sentence depends
on the context of each work.

81
00:03:50,710 --> 00:03:54,340
The team use something called the head
modifier construction to sort ward

82
00:03:54,341 --> 00:03:55,090
dependencies.

83
00:03:55,090 --> 00:03:58,870
This generated directed arcs between
words like that and cat cat being a direct

84
00:03:58,871 --> 00:03:59,940
object.
The word fat.

85
00:03:59,980 --> 00:04:02,830
The sentence starts out unprocessed
with an initial stack of words.

86
00:04:02,860 --> 00:04:04,840
The unprocessed segment
is called the buffer.

87
00:04:04,870 --> 00:04:07,420
As the parser encounters words
as it moves from left to right,

88
00:04:07,450 --> 00:04:10,600
it pushes words onto the stack.
Then it can do one of two things.

89
00:04:10,630 --> 00:04:13,780
It can either pop two words off the stack
attached to the second to the first,

90
00:04:13,810 --> 00:04:16,840
which would create a dependency arch
pointing to the left and push the first

91
00:04:16,841 --> 00:04:20,740
word back on the stack or create an
arc pointing to the right and push the

92
00:04:20,741 --> 00:04:21,960
second word back on the stack.

93
00:04:21,970 --> 00:04:25,150
This will keep repeating until
the entire sentence is processed.

94
00:04:25,151 --> 00:04:29,230
The system decides which way to point
the art depending on the context I. E.

95
00:04:29,230 --> 00:04:30,910
Previous pos tagging.
Once that's done,

96
00:04:30,940 --> 00:04:34,480
it uses the sequence of decisions to
learn a classifier that will predict

97
00:04:34,481 --> 00:04:35,980
dependencies in a novel Corpus.

98
00:04:36,010 --> 00:04:39,490
It applies a softmax function to each
of the decisions which normalizes or

99
00:04:39,491 --> 00:04:41,890
adjust them to a common
scale and does this globally.

100
00:04:41,891 --> 00:04:44,410
By summing up all the
softmax scores in log space.

101
00:04:44,440 --> 00:04:47,920
So the neural net is able to give
probabilities for each possible decision.

102
00:04:47,921 --> 00:04:51,420
And a heuristic called beam search helps
decide on the best one when predicting,

103
00:04:51,490 --> 00:04:53,920
once we have our parse tree
and parts of speech variables,

104
00:04:53,921 --> 00:04:57,550
let's store the root word and the
dependent object and to their own variable

105
00:04:57,580 --> 00:05:00,970
will call us in an API to retrieve
a synonym for the dependent objects.

106
00:05:01,000 --> 00:05:04,600
Then construct a novel sentence
that repeats the command
that users entered back

107
00:05:04,601 --> 00:05:05,680
to them in different words.

108
00:05:08,730 --> 00:05:09,930
Looks like it works pretty well.

109
00:05:09,960 --> 00:05:12,390
The scope of what you can
do with this is so vast.

110
00:05:12,420 --> 00:05:15,870
You can use this text analysis to create
a tech summarizer award recognizes the

111
00:05:15,871 --> 00:05:19,320
intent of a query or understand if
a review is positive or negative,

112
00:05:19,350 --> 00:05:20,700
or my personal favorite.

113
00:05:20,701 --> 00:05:24,060
Create a political debate back
checker links with more info below,

114
00:05:24,061 --> 00:05:26,010
please subscribe for more NL videos.
For now,

115
00:05:26,040 --> 00:05:28,800
I've got to go fix a buffer overflow,
so thanks for watching.

