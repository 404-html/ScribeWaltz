1
00:00:00,070 --> 00:00:00,761
Hello world,

2
00:00:00,761 --> 00:00:05,620
it's Saroj and let's talk
about how gradient descent
has evolved over the years.

3
00:00:05,920 --> 00:00:09,340
Tensorflow gives us quite a few
options for picking a gradient,

4
00:00:09,341 --> 00:00:11,680
descent based optimization strategy.

5
00:00:12,100 --> 00:00:16,330
This is what causes our neural
network to actually learn from data,

6
00:00:16,570 --> 00:00:19,510
but it's not immediately
clear how we should pick one.

7
00:00:19,750 --> 00:00:23,470
Once we understand the underlying
details of the most popular ones,

8
00:00:23,620 --> 00:00:27,520
it will become much more
clear which one we should use.

9
00:00:27,850 --> 00:00:30,190
When you study machine
learning for a while,

10
00:00:30,310 --> 00:00:34,900
you start making connections between
these mathematical techniques and the way

11
00:00:34,930 --> 00:00:36,640
we learn.
At this point,

12
00:00:36,641 --> 00:00:41,080
I view pretty much everything through
the lens of data and computation and it's

13
00:00:41,081 --> 00:00:43,480
a beautiful feeling.
For example,

14
00:00:43,600 --> 00:00:47,260
sometimes if our Dataset is
too homogenous after training,

15
00:00:47,290 --> 00:00:51,010
our model will be to fit to this data.
It'll be overfit,

16
00:00:51,040 --> 00:00:53,020
meaning it won't be
able to generalize well,

17
00:00:53,140 --> 00:00:54,880
so if given some different data points,

18
00:00:55,030 --> 00:00:57,070
it will be able to make
an accurate prediction.

19
00:00:57,520 --> 00:01:00,880
We have to keep our training
data diverse and in the same way,

20
00:01:00,881 --> 00:01:05,560
if we keep our brains training data
diverse by traveling and seeking out novel

21
00:01:05,561 --> 00:01:06,430
experiences,

22
00:01:06,730 --> 00:01:10,750
we'll be able to generalize better
and generalization is the hallmark of

23
00:01:10,751 --> 00:01:14,880
intelligence,
so that's why I like anchovies.

24
00:01:15,150 --> 00:01:18,360
If you were to ask what the most
important machine learning technique is,

25
00:01:18,660 --> 00:01:21,180
the answer is without a
doubt gradient descent.

26
00:01:21,370 --> 00:01:25,410
It is the foundation of how we train
intelligent systems and it's based off a

27
00:01:25,411 --> 00:01:26,760
very simple idea.

28
00:01:27,230 --> 00:01:31,560
Time travel instead of immediately
guessing the best solution to a given

29
00:01:31,561 --> 00:01:32,394
objective.

30
00:01:32,460 --> 00:01:37,460
We guess an initial solution
and iteratively step in
a direction closer to a

31
00:01:37,711 --> 00:01:38,730
better solution.

32
00:01:39,030 --> 00:01:43,680
The algorithm just repeats that process
until it arrives at a solution that's

33
00:01:43,681 --> 00:01:48,420
good enough. Since there is no way we can
know the best solution from the start,

34
00:01:48,480 --> 00:01:52,170
this educated guess and check
method is supremely useful.

35
00:01:52,550 --> 00:01:57,440
Gradient descent, find the ideal
minimum, control our variance,

36
00:01:58,100 --> 00:02:01,970
update our parameters and
lead us to convergence.

37
00:02:02,410 --> 00:02:06,520
A simple direct example of
gradient descent would be
if we want it to calculate

38
00:02:06,521 --> 00:02:11,320
the minimum value of x squared. That is
where the wide value is. The smallest.

39
00:02:11,560 --> 00:02:14,680
We could just randomly guess x
values from all over the place,

40
00:02:14,890 --> 00:02:19,750
but if we used gradient descent, we
arrive at a solution way more efficiently.

41
00:02:20,170 --> 00:02:24,490
The derivative of x squared is two x
and we used that derivative to calculate

42
00:02:24,491 --> 00:02:26,230
the gradient at a given point,

43
00:02:26,470 --> 00:02:30,520
so we could first guess three and take
a baby step in the direction opposite of

44
00:02:30,521 --> 00:02:34,960
the gradient and x equals three which is
negative six then the next guest might

45
00:02:34,961 --> 00:02:39,340
be at 2.3 then 1.4 then 0.7
until we finally reached zero,

46
00:02:39,460 --> 00:02:40,480
which is the local minimum.

47
00:02:40,720 --> 00:02:44,530
Both we and the machine can see that
this is the optimal solution since we can

48
00:02:44,531 --> 00:02:48,820
follow the trail of optimization visually
and mathematically as it leads us to

49
00:02:48,821 --> 00:02:49,630
convergence.

50
00:02:49,630 --> 00:02:53,620
Another way of thinking about the idea
of gray and dissent is examining how a

51
00:02:53,621 --> 00:02:55,810
professional athlete improves that a gate.

52
00:02:56,260 --> 00:03:00,640
As long as there is some
objective function that can
measured like the number of

53
00:03:00,641 --> 00:03:05,560
wins in a season and some data that
contributes or detracts from that function

54
00:03:05,680 --> 00:03:09,100
say passes, number of
three pointers, steroids,

55
00:03:09,280 --> 00:03:14,110
that player can iteratively take baby
steps in his routines after analyzing the

56
00:03:14,111 --> 00:03:15,250
data to improve,

57
00:03:15,430 --> 00:03:19,570
so to decide which of the gradient descent
optimization techniques we should use

58
00:03:19,571 --> 00:03:20,404
in our model.

59
00:03:20,500 --> 00:03:24,910
Let's learn about the various discoveries
around grading the sense over the

60
00:03:24,910 --> 00:03:25,000
years.

61
00:03:25,000 --> 00:03:29,500
Traditional gradient descent computes
the gradients of the loss function with

62
00:03:29,501 --> 00:03:34,210
regards to the perimeters for the entire
training datasets for a given number of

63
00:03:34,211 --> 00:03:35,044
epochs.

64
00:03:35,050 --> 00:03:39,190
Since we need to calculate the gradient
of the whole Dataset for just a single

65
00:03:39,191 --> 00:03:39,850
update,

66
00:03:39,850 --> 00:03:44,260
this has relatively slow and even in
tractable for data sets that don't fit in

67
00:03:44,261 --> 00:03:44,830
memory.

68
00:03:44,830 --> 00:03:49,180
So to get around this intractability
we can use to castic gradient descent.

69
00:03:49,630 --> 00:03:54,550
This is where we perform a
parameter update for each
training example and label.

70
00:03:54,670 --> 00:03:59,080
So we just add a loop over our training
data points and calculate the gradient

71
00:03:59,200 --> 00:04:01,210
with regard to each and every one.

72
00:04:01,600 --> 00:04:06,280
These more frequent updates with high
variance because the objective function to

73
00:04:06,281 --> 00:04:07,900
fluctuate more intensely.

74
00:04:08,110 --> 00:04:11,950
This is a good thing in that it helps
it jump to new and possibly better local

75
00:04:11,951 --> 00:04:13,700
minimum.
Whereas standard grading dissent,

76
00:04:13,750 --> 00:04:17,770
we'll only converge to the minimum of
the basin that the parameters are placed

77
00:04:17,771 --> 00:04:22,540
in, but it also complicates convergence
to the exact minimum since it could keep

78
00:04:22,570 --> 00:04:23,403
overshooting.

79
00:04:23,440 --> 00:04:28,120
So an improvement would be to use
minibatch gradient descent as it takes the

80
00:04:28,121 --> 00:04:32,410
best of both worlds. By performing an
update for every subset of training.

81
00:04:32,411 --> 00:04:36,700
Examples that we can decide the size of
training in many batches is usually the

82
00:04:36,701 --> 00:04:40,420
method of choice for training neural
networks. And we usually use the terms,

83
00:04:40,421 --> 00:04:44,020
the Cassick rating dissent.
Even when many batches are used,

84
00:04:44,170 --> 00:04:48,190
the oscillations in plain old SGD make
it hard to reach convergence though.

85
00:04:48,310 --> 00:04:52,750
So a technique called momentum was
invented that lets it navigate along the

86
00:04:52,751 --> 00:04:57,190
relevant directions and softens the
oscillations in the irrelevant directions.

87
00:04:57,460 --> 00:05:01,630
All it does is it adds a fraction of
the direction or update vector of the

88
00:05:01,631 --> 00:05:03,280
previous step to the current step,

89
00:05:03,460 --> 00:05:06,340
which amplifies the speed
and the correct direction.

90
00:05:06,580 --> 00:05:10,540
So it's just like momentum from
classical physics. Thanks Aristotle.

91
00:05:10,900 --> 00:05:14,260
When a ball is pushed down a hill,
it accumulates momentum,

92
00:05:14,350 --> 00:05:17,290
meaning it gets faster and faster.
In the same way,

93
00:05:17,320 --> 00:05:19,870
our momentum term
increases four dimensions,

94
00:05:19,871 --> 00:05:24,310
who's creating Ann's point in the
same direction and reduces updates for

95
00:05:24,311 --> 00:05:26,380
dimensions?
Who's gradients changed direction?

96
00:05:26,560 --> 00:05:30,580
This means faster convergence
and reduced oscillations,

97
00:05:30,820 --> 00:05:34,750
but a researcher named Yuri Nesteroff
saw a problem with momentum.

98
00:05:35,110 --> 00:05:37,000
Once we get close to our goal point,

99
00:05:37,060 --> 00:05:41,740
the momentum is usually pretty high and
it doesn't know that it should slow down,

100
00:05:41,920 --> 00:05:44,380
which could cause it to
miss the minimum entirely.

101
00:05:44,530 --> 00:05:49,290
He solved this problem in a paper he
released in 1983 and we now call this

102
00:05:49,291 --> 00:05:53,650
strategy the nests rob accelerated
gradient in the momentum method.

103
00:05:53,740 --> 00:05:54,880
We compute the gradient,

104
00:05:55,000 --> 00:05:58,820
then make a jump in that direction
amplified by the previous momentum.

105
00:05:59,150 --> 00:06:02,270
In this method we do the same
thing but in a different order.

106
00:06:02,450 --> 00:06:06,440
We first make a jump based on our
previous momentum, calculate the gradient,

107
00:06:06,650 --> 00:06:09,500
then make a correction
which results in an update.

108
00:06:09,770 --> 00:06:14,150
This more anticipatory update prevents
us from going too fast and were more

109
00:06:14,151 --> 00:06:15,650
responsive to changes.

110
00:06:15,950 --> 00:06:19,550
So this idea of more dynamic learning
of adapting our updates to the slope of

111
00:06:19,551 --> 00:06:21,260
our air function is a good one.

112
00:06:21,590 --> 00:06:24,290
Perhaps we could apply it to
our learning rate as well.

113
00:06:24,320 --> 00:06:28,670
Good ideas are good.
That's what Adec Grad does.

114
00:06:28,700 --> 00:06:33,050
It stands for adaptive gradient and
allows the learning rate to adapt based on

115
00:06:33,051 --> 00:06:33,830
the parameters.

116
00:06:33,830 --> 00:06:37,730
So it makes big updates for infrequent
parameters and small updates.

117
00:06:37,731 --> 00:06:38,630
For frequent ones.

118
00:06:38,780 --> 00:06:43,070
It uses a different learning rate for
every parameter at a given time step based

119
00:06:43,071 --> 00:06:46,160
on the past gradients that were
computed for that parameter.

120
00:06:46,460 --> 00:06:49,190
This means that we don't have to
manually tune the learning rate.

121
00:06:49,370 --> 00:06:50,720
It's main weakness though,

122
00:06:50,721 --> 00:06:55,460
is that the learning rate
is always decreasing since
the accumulation of squared

123
00:06:55,461 --> 00:07:00,461
gradients and the denominator
grows because each added
term is always positive.

124
00:07:00,860 --> 00:07:04,670
At some point the learning rate could
get so small that the model is just stops

125
00:07:04,671 --> 00:07:09,500
learning entirely at a delta was
invented to solve this and add a grad.

126
00:07:09,620 --> 00:07:13,340
We're constantly adding a square root
to the some causing the learning rate to

127
00:07:13,341 --> 00:07:16,880
decrease. So instead of summing
all the past square roots,

128
00:07:17,090 --> 00:07:21,470
we restrict the window of accumulated
pass gradients to a fixed size.

129
00:07:21,680 --> 00:07:26,600
We defined the sum of great as a decaying
average of all past squared gradients

130
00:07:26,810 --> 00:07:29,150
instead of just stored
previous squared grades.

131
00:07:29,360 --> 00:07:34,190
So the running average at a time step
depends only on the previous average and

132
00:07:34,250 --> 00:07:36,920
the current gradient.
So now we're getting somewhere,

133
00:07:37,010 --> 00:07:39,980
we're calculating individual
learning rates for each parameter,

134
00:07:40,040 --> 00:07:44,660
calculating momentum values, and we're
preventing a vanishing learning rate.

135
00:07:45,110 --> 00:07:49,550
What could we possibly do to improve here?
Sprinkle in some adaptive momentum.

136
00:07:49,580 --> 00:07:52,370
Since we're calculating learning
rates for each parameter,

137
00:07:52,670 --> 00:07:56,000
why not also store momentum changes
for each of them separately?

138
00:07:56,420 --> 00:08:00,440
That's what Adam does.
It stands for adaptive moment estimation.

139
00:08:00,740 --> 00:08:02,480
We calculate the first moment,

140
00:08:02,540 --> 00:08:06,490
the mean and the second moment the
uncentered variants of the gradients

141
00:08:06,500 --> 00:08:07,333
respectively.

142
00:08:07,520 --> 00:08:11,840
Then we use those values to update the
parameters just like in at a delta.

143
00:08:11,930 --> 00:08:15,230
If we were to visualize these optimization
algorithms during the learning

144
00:08:15,231 --> 00:08:19,550
process, we'll see that
the adaptive learning rate
methods quickly find the right

145
00:08:19,551 --> 00:08:24,470
direction and converge super fast while
momentum and the nest rob accelerated

146
00:08:24,471 --> 00:08:26,810
gray and go in the wrong direction.

147
00:08:26,870 --> 00:08:30,260
So which optimizers should you
use and your neural network?

148
00:08:30,440 --> 00:08:35,000
It seems that Adam is the best overall
choice since it usually outperforms the

149
00:08:35,001 --> 00:08:39,330
rest followed very closely by the other
adaptive learning rate methods out of

150
00:08:39,331 --> 00:08:41,540
Grad and add a delta momentum.

151
00:08:41,541 --> 00:08:46,400
Plain STD and nesteroff strategy are cool,
but when data is sparse,

152
00:08:46,430 --> 00:08:50,390
which it usually is in real world
data sets, they don't perform well.

153
00:08:50,840 --> 00:08:55,260
I know we've talked about a
lot of different optimization
strategies and it's a

154
00:08:55,261 --> 00:08:56,940
lot to take in all at once.

155
00:08:57,120 --> 00:09:01,440
So what better way to apply this knowledge
then by implementing one by yourself?

156
00:09:01,710 --> 00:09:05,760
The coding challenge for this week is to
implement Adam from scratch in python.

157
00:09:05,960 --> 00:09:07,470
Details are in the read me get hub leads,

158
00:09:07,480 --> 00:09:11,880
go into comments and winners will be
announced in one week. If you subscribe,

159
00:09:12,000 --> 00:09:13,470
all of your dreams will come true.

160
00:09:13,780 --> 00:09:18,420
Check out this related video and for now
I'm going to go, so thanks for lunch.

