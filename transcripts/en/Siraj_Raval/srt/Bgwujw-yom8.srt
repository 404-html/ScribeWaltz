1
00:00:00,070 --> 00:00:00,731
Hello world.

2
00:00:00,731 --> 00:00:05,020
It's Saroj and how would you suppose to
train all these deep learning algorithms

3
00:00:05,290 --> 00:00:07,510
if you don't have an amazing GPU?

4
00:00:07,690 --> 00:00:11,350
We're going to talk about why gps
are important for deep learning.

5
00:00:11,560 --> 00:00:15,490
Then I'll show you how to train a big
style transfer model in the cloud.

6
00:00:15,520 --> 00:00:16,450
Super easily.

7
00:00:16,650 --> 00:00:20,950
A central processing unit or CPU
acts as the brain of a computer.

8
00:00:21,190 --> 00:00:25,450
It's a small square like component
with a bunch of short rounded metallic

9
00:00:25,451 --> 00:00:29,650
connectors on its underside like bender.
Hi my shiny metal ash.

10
00:00:29,710 --> 00:00:32,680
It's fundamental operation.
It's called an instruction.

11
00:00:32,770 --> 00:00:35,830
Every computer has both a CPU and memory,

12
00:00:35,860 --> 00:00:40,690
so when you run a binary file like
an exe which contains instructions,

13
00:00:40,930 --> 00:00:45,930
the oos copies the program into memory
then feeds it to the CPU one instruction

14
00:00:46,361 --> 00:00:50,830
at a time. The basic computation
unit of a CPU is called a core.

15
00:00:51,100 --> 00:00:52,480
It can run a given task.

16
00:00:52,720 --> 00:00:56,800
It does things like maintain the program
state and the correct execution order.

17
00:00:56,940 --> 00:01:00,880
It can use one or more cores to
perform a task. I'd a given time.

18
00:01:01,270 --> 00:01:06,220
CPU is generally don't have more than 12
cores and our men for general computing

19
00:01:06,221 --> 00:01:07,054
tasks.

20
00:01:07,090 --> 00:01:11,830
There are some tasks that can only be
executed sequentially like doing the

21
00:01:11,831 --> 00:01:12,664
Danny,
Danny,

22
00:01:15,270 --> 00:01:16,103
Susan.

23
00:01:20,260 --> 00:01:23,020
Some tasks though can be run in parallel.
In fact,

24
00:01:23,140 --> 00:01:27,250
they would benefit from parallelization
rendered. Graphics is a great example.

25
00:01:27,490 --> 00:01:32,230
Each pixel can be processed independently
of the others and so to help do this,

26
00:01:32,380 --> 00:01:35,740
the graphics processing
unit or Gpu was invented.

27
00:01:36,100 --> 00:01:39,760
Modern GPS can have up to
a thousand cores or more.

28
00:01:40,060 --> 00:01:42,280
They're made for parallel computation.

29
00:01:42,520 --> 00:01:46,540
A single GPU is like lots of
little CPS all running at once.

30
00:01:46,750 --> 00:01:48,940
CPS are optimized for latency.

31
00:01:48,941 --> 00:01:51,940
They're good at fetching small
amounts of memory quickly.

32
00:01:52,120 --> 00:01:54,850
While GPU is optimized for bandwidth,

33
00:01:55,120 --> 00:01:58,210
they're good at fetching large
amounts of memory all at once.

34
00:01:58,480 --> 00:02:03,340
Lighting and shading effects require
massive matrix operations to be calculated

35
00:02:03,520 --> 00:02:07,030
all at once and the GPU
helps make this happen.

36
00:02:07,390 --> 00:02:12,390
It just so happens that another use
case that involves an ungodly amount of

37
00:02:12,431 --> 00:02:17,020
matrix operations that can be
parallelized is deep learning.

38
00:02:17,350 --> 00:02:20,500
Every computer needs a GPU. Do
you have one too? Without it,

39
00:02:20,530 --> 00:02:22,720
you wouldn't see an image on your display,

40
00:02:23,140 --> 00:02:27,610
but the question for deep learning is
should you get a dedicated GPU or should

41
00:02:27,611 --> 00:02:30,520
you use the cloud building your
own rig if you do it right,

42
00:02:30,550 --> 00:02:32,770
can be the most cost efficient method.

43
00:02:33,100 --> 00:02:37,660
The Best Gpu overall in terms of
computing power is the Titan XP.

44
00:02:37,900 --> 00:02:39,130
If you have a moderate budget,

45
00:02:39,160 --> 00:02:44,160
then the GTX 10 60 is your best bet and
if you're on a tight budget and the GTX

46
00:02:44,471 --> 00:02:46,390
10 50 I should be your goto.

47
00:02:46,780 --> 00:02:49,390
It's kind of like when the
Internet was first coming of age,

48
00:02:49,600 --> 00:02:54,220
everyone had an opinion on whether or
not to host their website on their own

49
00:02:54,221 --> 00:02:58,780
hardware or use the cloud. Eventually
though, the industry standard became the,

50
00:02:59,590 --> 00:03:03,190
because maintaining hardware
wasn't unwanted hassle.

51
00:03:03,370 --> 00:03:06,940
Call one 800 train that shit.com wait,

52
00:03:07,090 --> 00:03:11,710
so which cloud providers should you use?
Let's talk about three AWS,

53
00:03:11,711 --> 00:03:13,240
Google cloud and Floy hub.

54
00:03:13,600 --> 00:03:18,280
Amazon's cloud offering is currently
the industry standard and many big names

55
00:03:18,281 --> 00:03:19,114
rely on it.

56
00:03:19,210 --> 00:03:24,210
There are three types of instances
or virtual servers that AWS offers.

57
00:03:24,460 --> 00:03:25,810
The first is on demand.

58
00:03:26,020 --> 00:03:30,070
This lets you rent an instance with a
specific capacity that you can start and

59
00:03:30,071 --> 00:03:32,530
stop as you need to.
When you restart it,

60
00:03:32,531 --> 00:03:36,940
it picks up right where you left off
and only running instances are built.

61
00:03:37,090 --> 00:03:39,850
Then there are reserved
instances for these.

62
00:03:39,851 --> 00:03:43,600
You commit to paying for a number
of them for a certain period of time

63
00:03:43,601 --> 00:03:44,434
beforehand.

64
00:03:44,560 --> 00:03:48,580
The payoff for your commitment is that
these are half the price of on demand

65
00:03:48,610 --> 00:03:53,200
instances. Then there are spot
instances. These are the cheapest option.

66
00:03:53,350 --> 00:03:57,490
They are the spare computing capacity
that Amazon has at any given moment.

67
00:03:57,580 --> 00:03:59,590
You have to get on them.
If you're outbid,

68
00:03:59,591 --> 00:04:04,150
your spot instance is
terminated automatically and
they can't be stopped and

69
00:04:04,151 --> 00:04:04,840
restarted,

70
00:04:04,840 --> 00:04:09,160
only terminated so you can't just resume
where you left off heady later time.

71
00:04:09,280 --> 00:04:11,500
Aws is dope sauce extreme,

72
00:04:11,710 --> 00:04:14,920
but there are a lot of steps to get
started with it in the learning curve is

73
00:04:14,921 --> 00:04:19,630
quite steep. So how does it compare it
to Google cloud when it comes to pricing?

74
00:04:19,660 --> 00:04:24,660
Google cloud wins a two CPU eight Gig Ram
incense will cost $69 a month with AWS

75
00:04:26,680 --> 00:04:29,410
compared to only $52 a
month on Google cloud.

76
00:04:29,740 --> 00:04:33,370
It offers a pay per minute model
instead of a pay per hour model,

77
00:04:33,580 --> 00:04:36,520
which is useful if you have
short on the flight tests to run.

78
00:04:36,670 --> 00:04:41,500
That means that 2.01 hours of compute
on Google cloud is equivalent to three

79
00:04:41,501 --> 00:04:43,390
hours on AWS.
Well,

80
00:04:43,391 --> 00:04:48,040
AWS offers a generous one year free trial
in which you can use seven 50 hours a

81
00:04:48,041 --> 00:04:53,041
month of a small CPU instance who will
cloud offers you $300 worth of credit for

82
00:04:53,141 --> 00:04:57,970
12 months and an unlimited time three
tier. It still beat by AWS though.

83
00:04:57,970 --> 00:05:01,540
In terms of number of offerings.
If you need a cloud sequel solution,

84
00:05:01,690 --> 00:05:03,910
you can use my sequel with Google cloud,

85
00:05:04,090 --> 00:05:06,820
but AWS offers a bunch
of different options.

86
00:05:07,150 --> 00:05:10,510
Google cloud is more flexible
when it comes to configuring
your instances though.

87
00:05:10,780 --> 00:05:15,010
They let you customize how many CPS and
how much ram to use and it has something

88
00:05:15,011 --> 00:05:17,110
called preemptive instances,

89
00:05:17,140 --> 00:05:21,160
which is similar to Amazon's spot
instances except you don't need to bid.

90
00:05:21,460 --> 00:05:25,300
It can run for up to 24 hours but could
be interrupted by Google if they need

91
00:05:25,301 --> 00:05:28,300
the computing power.
So if choosing between the two,

92
00:05:28,330 --> 00:05:30,820
while AWS offers more cloud options,

93
00:05:31,120 --> 00:05:34,000
Google cloud is cheaper and easier to use,

94
00:05:34,090 --> 00:05:37,240
but there's another even newer option.
Floyd hub.

95
00:05:37,480 --> 00:05:40,570
If you are a beginner or just
want to try out cloud computing,

96
00:05:40,780 --> 00:05:41,980
this is the best option.

97
00:05:42,070 --> 00:05:45,880
You get a hundred free hours of GPU
usage and you don't need a credit card to

98
00:05:45,881 --> 00:05:46,570
sign up.

99
00:05:46,570 --> 00:05:51,520
They offer per second billing and under
the hood they are using AWS reserved

100
00:05:51,550 --> 00:05:54,880
instances but are providing
simplicity as a service.

101
00:05:54,970 --> 00:05:56,530
It's like Heroku for deep learning.

102
00:05:56,990 --> 00:06:00,710
Let's test it out by deploying a style
transfer model to it for training

103
00:06:00,711 --> 00:06:04,910
ourselves. The first step is to create
an account on Floyd hub. Easy enough.

104
00:06:05,120 --> 00:06:09,050
Then we'll want to install the Floyd
command line tool using the python package

105
00:06:09,051 --> 00:06:09,950
manager hip.

106
00:06:10,370 --> 00:06:14,510
This tool will let us interact with the
Floyd hub cloud directly from terminal.

107
00:06:14,750 --> 00:06:16,130
Once that's installed,

108
00:06:16,131 --> 00:06:20,750
we can use this tool to
help us authenticate with
the server via Floyd login.

109
00:06:21,080 --> 00:06:23,300
We can paste are off token in as well.

110
00:06:23,450 --> 00:06:27,590
Then we can clone that style transfer
repository directly from get hub to our

111
00:06:27,591 --> 00:06:29,750
local machine.
Once it's downloaded,

112
00:06:29,751 --> 00:06:34,340
we can CD into the main directory and
initialize it as a Floyd hub project.

113
00:06:34,610 --> 00:06:38,090
We'll call it style transfer.
Now we're ready to run the model.

114
00:06:38,180 --> 00:06:42,980
We can run it by wrapping the python run
command for the main file inside of the

115
00:06:42,981 --> 00:06:47,570
Floyd run command. This repository
has some flags that we can use,

116
00:06:47,690 --> 00:06:51,200
so we'll go ahead and specify
the style we want to use.

117
00:06:51,410 --> 00:06:55,880
The directory will save our checkpoint
to output is a special path meant to

118
00:06:55,881 --> 00:07:00,110
store checkpoints on Floyd hub, we can
define what images we want to test on,

119
00:07:00,290 --> 00:07:04,280
how much influence we want the content
went to have on this style transfer and

120
00:07:04,310 --> 00:07:06,410
the number of iterations to train for.
Oh,

121
00:07:06,470 --> 00:07:08,630
and of course the data
source we want to train on.

122
00:07:08,930 --> 00:07:11,960
That's all it takes to run and
now it's training in the cloud.

123
00:07:12,350 --> 00:07:15,800
What this has done is it's saint
our local code to the cloud,

124
00:07:16,070 --> 00:07:17,840
spun up a GPU instance,

125
00:07:17,960 --> 00:07:22,400
set up an environment with tensorflow
installed and executed our command in that

126
00:07:22,401 --> 00:07:23,234
environment.

127
00:07:23,240 --> 00:07:26,960
Once it's running we can easily check
the status of it in terminal using the

128
00:07:26,961 --> 00:07:31,961
status command and the ID of our run
or we can view it in the web dashboard.

129
00:07:32,420 --> 00:07:36,560
Under slash experiments. We can also
view the logs using the logs command,

130
00:07:36,770 --> 00:07:40,910
which will show all our print statements
and we can monitor training right here

131
00:07:40,911 --> 00:07:44,570
if we wanted to.
If we make a change to this code locally,

132
00:07:44,720 --> 00:07:47,180
we can just rerun the
project with the run command.

133
00:07:47,450 --> 00:07:51,800
Floyd hub will upload a new version of
the code and start another run of the

134
00:07:51,801 --> 00:07:56,570
project so it's gotten version control
built in just like yet. In fact,

135
00:07:56,600 --> 00:08:01,600
it versions the entire pipeline code
data parameters and environment for exact

136
00:08:01,911 --> 00:08:02,900
reproducibility.

137
00:08:02,930 --> 00:08:07,370
Floyd hub uses a content addressing
scheme for both runs and datasets.

138
00:08:07,550 --> 00:08:10,910
If there is a particular Dataset we
want to train on again and again,

139
00:08:11,210 --> 00:08:15,140
it's useful to just upload it directly.
If we have it in our current directory,

140
00:08:15,290 --> 00:08:19,460
we can just create the remote directory
for our data and name it using Floyd

141
00:08:19,490 --> 00:08:23,870
data in it and our nay.
Then we can upload it using data upload.

142
00:08:24,200 --> 00:08:28,040
We can use this data id for
any model we'd run later on.

143
00:08:28,160 --> 00:08:32,300
It takes about eight hours to train this
model and when it's done we can test it

144
00:08:32,301 --> 00:08:34,100
out by running the evaluate script,

145
00:08:34,340 --> 00:08:38,540
giving it some fresh images to style
transfer by pointing it to our images

146
00:08:38,541 --> 00:08:40,100
directory.
When it's done,

147
00:08:40,101 --> 00:08:43,760
we can observe the output by running
the upper command using the run id.

148
00:08:43,880 --> 00:08:46,130
Much deepness. All
right, let's cut the Hay.

149
00:08:46,370 --> 00:08:50,270
If you want the absolute cheapest option
and are willing to deal with the hassle.

150
00:08:50,510 --> 00:08:54,860
Building a custom deep learning machine
is the way to go and link to how to do

151
00:08:54,861 --> 00:08:59,610
that. In the description. Aws and Google
cloud are two great cloud platforms.

152
00:08:59,820 --> 00:09:03,600
Aws offers more cloud products
and is the industry standard.

153
00:09:03,870 --> 00:09:06,930
A Google cloud is cheaper
and easier to use.

154
00:09:07,200 --> 00:09:11,520
And Floyd hub is the easiest way to train
your models in the cloud and the best

155
00:09:11,521 --> 00:09:13,080
way for beginners to get started,

156
00:09:13,230 --> 00:09:15,900
please hit that subscribe button
for more programming videos.

157
00:09:15,901 --> 00:09:19,560
Check out this related video. And for
now, I've got to fly to Amsterdam,

158
00:09:19,770 --> 00:09:21,060
so thanks for watching.

