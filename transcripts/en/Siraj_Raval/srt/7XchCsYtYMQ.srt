1
00:00:00,150 --> 00:00:00,901
Hello world,

2
00:00:00,901 --> 00:00:05,901
it's Saroj and there's some serious hyper
lately about this Ai Algorithm called

3
00:00:06,120 --> 00:00:11,120
deep fakes that's able to swap the face
of anyone in a video with anybody else.

4
00:00:12,210 --> 00:00:15,720
Some of the examples I've
seen are amazingly realistic.

5
00:00:15,960 --> 00:00:20,730
I'm going to explain how this algorithm
works through theory and code and then

6
00:00:20,731 --> 00:00:23,940
talk a bit about the moral
implications of this.

7
00:00:24,120 --> 00:00:26,400
At the end of the video last month,

8
00:00:26,430 --> 00:00:31,430
a redditor by the name of deep fakes of
course was making a name for himself by

9
00:00:31,711 --> 00:00:34,110
posting, convincing, let's say,

10
00:00:34,111 --> 00:00:38,670
not safe for work videos that swapped
the face of the person in the original

11
00:00:38,671 --> 00:00:42,030
video with a famous celebrity
like say Taylor Swift.

12
00:00:42,330 --> 00:00:46,920
He did this by training his deep learning
algorithm on some publicly available

13
00:00:46,921 --> 00:00:51,450
videos with just his home computer
and in the past few weeks,

14
00:00:51,600 --> 00:00:54,180
the practice of doing this has exploded.

15
00:00:54,510 --> 00:00:59,510
There's now an entire subreddit and get
hub community dedicated to maintaining

16
00:00:59,971 --> 00:01:02,700
this and sharing the
results of this algorithm.

17
00:01:02,880 --> 00:01:07,260
The subreddit grew massively in size and
there's even this desktop application

18
00:01:07,410 --> 00:01:12,270
called fake app that lets anyone recreate
one of these videos with their own

19
00:01:12,330 --> 00:01:16,260
dataset. No knowledge of programming
necessary because of this.

20
00:01:16,261 --> 00:01:21,261
People are putting Nicholas Cage's face
on literally everything and making a

21
00:01:21,361 --> 00:01:23,610
meme out of it because it's the internet.

22
00:01:23,970 --> 00:01:26,820
So how does the deep fake algorithm work?

23
00:01:27,060 --> 00:01:31,860
Let's say we've got this video of Harrison
Ford in Indiana Jones and we want to

24
00:01:31,861 --> 00:01:36,861
replace his face with Nicholas Cage
because why the hell not our first step is

25
00:01:37,531 --> 00:01:39,870
going to be to gather some training data,

26
00:01:40,020 --> 00:01:45,020
specifically images of both Harrison
Ford and Nicholas cage to later train our

27
00:01:45,781 --> 00:01:50,070
models on possible sources of these
images. Could include Google duck, duck,

28
00:01:50,071 --> 00:01:51,900
go or being image search.

29
00:01:52,140 --> 00:01:57,140
Luckily the face swap repository has
scripts to automatically download large

30
00:01:57,331 --> 00:02:01,410
amounts of images from one of these
sources to our home directory.

31
00:02:01,770 --> 00:02:03,990
Once we've got a couple hundred images,

32
00:02:04,110 --> 00:02:07,260
we can place them each in
their respective folder.

33
00:02:07,560 --> 00:02:12,560
Notice though that these images are full
of stuff not related to our characters.

34
00:02:12,870 --> 00:02:16,710
There are all sorts of objects and
distractions in the environments of these

35
00:02:16,711 --> 00:02:20,130
images that have nothing
to do with our characters.

36
00:02:20,490 --> 00:02:25,290
We're going to want to perform some
face detection on each of these images.

37
00:02:25,560 --> 00:02:30,560
Pretty much all cameras that have been
created in the past decade have some sort

38
00:02:30,661 --> 00:02:33,150
of real time based detection algorithm.

39
00:02:33,330 --> 00:02:38,280
The open computer vision or open CV
library will allow us to easily recognize

40
00:02:38,281 --> 00:02:40,890
faces in images.
We just a function call,

41
00:02:40,920 --> 00:02:45,240
but the way it's doing this in the
background is by using a method called

42
00:02:45,270 --> 00:02:49,380
histogram of oriented
gradients or hog for short.

43
00:02:49,740 --> 00:02:53,190
It starts by making our image
black and white. To simplify it,

44
00:02:53,370 --> 00:02:55,650
we don't need color data to find faces.

45
00:02:56,010 --> 00:02:59,680
Then it looks at every single pixel
in the image on one at a time.

46
00:03:00,010 --> 00:03:03,610
For every pixel it looks at the
pixels directly surrounding it.

47
00:03:03,970 --> 00:03:08,970
The goal is to figure out how dark the
current Pixel is compared to the pixels

48
00:03:09,101 --> 00:03:10,390
directly surrounding it.

49
00:03:10,840 --> 00:03:14,920
Then it draws an arrow showing in which
direction the image is getting darker.

50
00:03:15,340 --> 00:03:18,850
Once it does this process for
every single pixel in the image,

51
00:03:19,120 --> 00:03:22,570
we'll end up with every pixel
being replaced by an arrow.

52
00:03:22,990 --> 00:03:27,910
These arrows show the flow from light
to dark across the entire image.

53
00:03:28,270 --> 00:03:30,130
If we analyze pixels directly,

54
00:03:30,250 --> 00:03:34,630
then really dark images and really light
images of the same person would have

55
00:03:34,631 --> 00:03:36,640
totally different pixel values.

56
00:03:36,910 --> 00:03:41,230
But because we're only considering
the direction that brightness changes,

57
00:03:41,560 --> 00:03:45,280
both types of images end up
with the same representation.

58
00:03:45,520 --> 00:03:47,440
Making the problem easier to solve.

59
00:03:47,560 --> 00:03:50,740
Saving all these directions
is to space intensive,

60
00:03:50,830 --> 00:03:55,660
so we break the image into smaller
squares and count how many different

61
00:03:55,661 --> 00:03:56,860
directions there are.

62
00:03:57,160 --> 00:04:01,540
Then replace each square with the
direction that has the most counts.

63
00:04:01,840 --> 00:04:06,760
This turns the original image into a
very simple representation that captures

64
00:04:06,761 --> 00:04:09,430
the basic structure of
a face in a simple way.

65
00:04:09,850 --> 00:04:12,460
Once open CV has a representation,

66
00:04:12,640 --> 00:04:17,640
it can compare it by a distance metric
to another hog face pattern generated

67
00:04:17,711 --> 00:04:22,210
from lots of face images and if it's
close to it by some threshold value,

68
00:04:22,450 --> 00:04:26,470
we can consider the face the tech to once
we've got our face detected using open

69
00:04:26,471 --> 00:04:27,220
CV,

70
00:04:27,220 --> 00:04:30,760
we can just crop the original image so
it's just the face and save that instead.

71
00:04:31,000 --> 00:04:34,240
Soon we'll have pure face datasets
for both of our characters.

72
00:04:34,660 --> 00:04:39,370
Now we want to train our model on this
Dataset so it learns to encode these

73
00:04:39,371 --> 00:04:42,700
faces,
meaning tell them apart from one another.

74
00:04:42,790 --> 00:04:47,090
There are a bunch of measurements that
we could take of a face to try and tell

75
00:04:47,140 --> 00:04:50,320
what it looks like. Ear size
knows length, et Cetera,

76
00:04:50,350 --> 00:04:54,820
but it turns out that it's better to
have our algorithm learn these things for

77
00:04:54,821 --> 00:04:55,654
itself.

78
00:04:55,720 --> 00:05:00,250
Convolutional neural networks have been
shown to offer really great performance

79
00:05:00,251 --> 00:05:02,470
in learning to classify faces.

80
00:05:02,830 --> 00:05:07,600
It's a neural network with
its own distinct set of
operations like convolutions

81
00:05:07,601 --> 00:05:08,434
and pooling.

82
00:05:08,620 --> 00:05:12,670
These operations amount to simple
matrix math applied to an input image.

83
00:05:12,940 --> 00:05:14,980
The output being a classification,

84
00:05:15,220 --> 00:05:17,950
but we don't simply want
to classify an image.

85
00:05:18,190 --> 00:05:23,190
We want to learn a representation of both
faces and somehow morph face a to look

86
00:05:23,741 --> 00:05:26,770
like face B in the most
realistic way possible.

87
00:05:27,040 --> 00:05:29,950
So what we'll use is an auto encoder.

88
00:05:30,190 --> 00:05:34,180
This is a convolutional network that
tries to reconstruct the input image.

89
00:05:34,510 --> 00:05:38,920
This lets it learn a lower dimensional
representation of the input image,

90
00:05:39,040 --> 00:05:41,440
which will later use to swap basis.

91
00:05:41,650 --> 00:05:46,650
Specifically deep fakes uses one encoder
and two decoders during training.

92
00:05:46,750 --> 00:05:48,820
It actually trains two networks.

93
00:05:49,030 --> 00:05:53,860
Both the networks share an encoder but
have different decoders on Incode or

94
00:05:53,861 --> 00:05:56,500
transforms an image into a base vector.

95
00:05:56,800 --> 00:06:01,070
This is a set of numbers which identify
the important features of the face.

96
00:06:01,370 --> 00:06:04,910
The decoder transforms that
vector back to an image.

97
00:06:05,140 --> 00:06:09,710
There is an error function which measures
how good the transformation was and

98
00:06:09,711 --> 00:06:12,980
the model tries to lower the
overall error during training.

99
00:06:13,280 --> 00:06:17,870
The first network is only trained on
image a and the second network is only

100
00:06:17,871 --> 00:06:22,871
trained on image be the encoder learns
how to convert an image into a face

101
00:06:23,001 --> 00:06:28,001
vector decoder a learns how to convert
a base factor to image a and code or B,

102
00:06:28,160 --> 00:06:30,680
wanting to try to convert
a base factor to image B.

103
00:06:30,830 --> 00:06:35,330
So during training we're feeding
both images to the same encoder,

104
00:06:35,480 --> 00:06:38,450
but using two different decoders for each.

105
00:06:38,810 --> 00:06:42,020
After the network is done training,
we can feed it a video.

106
00:06:42,350 --> 00:06:46,220
A video is simply a collection
of image frames, one by one.

107
00:06:46,250 --> 00:06:49,520
We'll first crop out the target
face from the video frame,

108
00:06:49,760 --> 00:06:54,560
then perform a face swap on it.
That means feeding image a to the encoder,

109
00:06:54,740 --> 00:06:57,830
creating a base vector,
then feeding it to decoder,

110
00:06:57,860 --> 00:07:00,650
be resulting in a face that looks like B,

111
00:07:00,740 --> 00:07:03,320
which we can then overlay
onto the original frame.

112
00:07:03,710 --> 00:07:08,570
Then we can concatenate all of those
images together and watch the video result

113
00:07:08,600 --> 00:07:10,160
for ourselves soon.

114
00:07:10,161 --> 00:07:15,080
This technology will get more realistic
and more accessible and anyone will be

115
00:07:15,081 --> 00:07:19,520
able to make it look like you are doing
something you never really did via

116
00:07:19,521 --> 00:07:24,050
video, which could be degrading.
But this can also be a good thing.

117
00:07:24,110 --> 00:07:28,340
Anyone will be able to make all sorts
of entertaining content that wouldn't be

118
00:07:28,341 --> 00:07:30,020
possible before easily.

119
00:07:30,380 --> 00:07:35,380
It's gonna get harder to
distinguish what's real and
what's fake as AI improves.

120
00:07:36,110 --> 00:07:41,110
And we'll need to work on
more deterministic algorithms
that offer computational

121
00:07:41,391 --> 00:07:44,450
proofs to help us trust sources more.

122
00:07:44,870 --> 00:07:48,740
Enter blockchain and cryptography.
Three things to remember from this video.

123
00:07:49,010 --> 00:07:54,010
We can use open CV to perform facial
detection using the histogram of oriented

124
00:07:54,231 --> 00:07:54,770
gradients.

125
00:07:54,770 --> 00:07:59,770
Approach convolutional networks can learn
features from images and auto encoders

126
00:08:00,530 --> 00:08:05,530
can encode representations that we can
later use to more for one image into

127
00:08:05,690 --> 00:08:09,470
another. Last week's coding
challenge. Winner is Ivan [inaudible],

128
00:08:09,620 --> 00:08:14,620
who used an auto encoder to convert
DNA sequences into RNA sequences.

129
00:08:14,960 --> 00:08:17,630
This is such a cool
application. Great job, Ivan.

130
00:08:17,960 --> 00:08:20,450
And the runner up is
param deep sing Oberoi,

131
00:08:20,570 --> 00:08:24,980
who created a well documented auto
encoder on the M and ist Dataset.

132
00:08:25,460 --> 00:08:29,050
You already know what this week's
coding challenge is. Generate a face,

133
00:08:29,051 --> 00:08:33,540
swap yourself and post a link to your
github repo in the youtube comments and

134
00:08:33,541 --> 00:08:35,150
winners will be announced next week.

135
00:08:35,210 --> 00:08:37,820
Please subscribe for more
programming videos. And for now,

136
00:08:37,880 --> 00:08:41,390
I've got to prepare myself to get
baseball, so thanks for watching.

