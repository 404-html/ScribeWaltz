1
00:00:00,030 --> 00:00:04,350
Live. So let's go live right
now. Start streaming. Good.

2
00:00:04,740 --> 00:00:06,150
The event is starting,

3
00:00:06,600 --> 00:00:09,840
I think on the Stream is actually
going to start in a second.

4
00:00:09,841 --> 00:00:14,670
It says it's starting. Yes, it started
great. Okay. Okay. Hello world.

5
00:00:14,700 --> 00:00:18,630
It's Saroj. Welcome to my live
stream on Google. Dopamine.

6
00:00:18,930 --> 00:00:21,660
This is a new framework that
came out of Google research.

7
00:00:21,661 --> 00:00:24,210
There was this blog post
that explained how it works.

8
00:00:24,510 --> 00:00:28,710
And in this live stream we're going to,
I'm going to talk about how it works.

9
00:00:28,711 --> 00:00:33,590
So I'm going to dive into its modules.
It's features of,

10
00:00:33,600 --> 00:00:35,430
we're going to talk a little bit,
not a little bit.

11
00:00:35,431 --> 00:00:38,760
We're going to talk about
reinforcement learning in general. Uh,

12
00:00:38,761 --> 00:00:41,250
and then we are going to code it out.

13
00:00:41,251 --> 00:00:45,360
So we're going to use Google's Colab
to code out a very simple agent

14
00:00:45,361 --> 00:00:49,010
implementation in the browser.
Anybody can do this. I want,

15
00:00:49,020 --> 00:00:51,630
if you're watching this,
I want you to do this with me.

16
00:00:51,720 --> 00:00:56,720
So go to colab.research.google.com and
open up a notebook and just start and

17
00:00:57,181 --> 00:00:58,890
start coding with me. Okay? So,

18
00:00:59,460 --> 00:01:02,670
and I'm also taking in feedback
from the last live stream.

19
00:01:02,671 --> 00:01:06,600
So I don't want to be as sporadic with
like looking at audience comments and

20
00:01:06,601 --> 00:01:09,540
just being all over the place. So here's
how we're going to structure this.

21
00:01:09,840 --> 00:01:11,820
The first part of this tutorial,

22
00:01:11,821 --> 00:01:16,020
this lecture is going to be me
explaining how the framework works, uh,

23
00:01:16,050 --> 00:01:18,900
and how reinforcement
learning in general works.

24
00:01:19,260 --> 00:01:21,360
Then I'm going to take a short Q and a,

25
00:01:21,630 --> 00:01:26,010
then I'm going to code this thing out
in the browser as a very simple agent so

26
00:01:26,011 --> 00:01:29,460
we can get a feel for how
this framework works. Uh,

27
00:01:29,461 --> 00:01:32,130
and then after training I'm
going to take another Q and. A.

28
00:01:32,220 --> 00:01:33,990
And at the end they'll
take another Q. And. A.

29
00:01:33,991 --> 00:01:37,830
And I'm going to start off with just
two questions to go ahead and ask two

30
00:01:37,831 --> 00:01:41,640
questions. I'll answer those and then
we'll get right into the lecture. Okay.

31
00:01:41,641 --> 00:01:46,620
So I'll definitely minimize this chat
window when I'm talking because I saw that

32
00:01:46,621 --> 00:01:49,650
the, uh, recorded viewers didn't
like that that much, you know,

33
00:01:49,651 --> 00:01:51,990
seeing all these comments come in. So, um,

34
00:01:52,290 --> 00:01:54,270
I'm very excited to talk
about this by the way,

35
00:01:54,271 --> 00:01:59,271
because reinforcement learning has been
just pure reinforcement learning has

36
00:01:59,761 --> 00:02:02,900
been the forgotten technique
I think about AI of,

37
00:02:02,901 --> 00:02:06,630
of Ai because there was the
resurgence in deep learning, right?

38
00:02:06,631 --> 00:02:08,310
So let's start off with this timeline,
right?

39
00:02:08,311 --> 00:02:11,880
So there was the resurgence in neural
networks with deep learning around like,

40
00:02:11,881 --> 00:02:14,900
oh six or oh seven, uh, and then, uh,

41
00:02:14,910 --> 00:02:18,120
there was a resurgence in deep
reinforcement learning around,

42
00:02:18,121 --> 00:02:22,110
I think it was 2013 when a deep
mind release, the deep Q learner.

43
00:02:22,440 --> 00:02:27,440
And so it went from like reinforcement
learning in the 60s to neural networks to

44
00:02:29,161 --> 00:02:32,280
taking reinforcement learning,
applying it to neural networks.

45
00:02:32,550 --> 00:02:36,840
But no one in general has really
focused on reinforcement learning,

46
00:02:36,841 --> 00:02:38,520
just pure reinforcement learning.

47
00:02:38,521 --> 00:02:43,230
Like if you look at the content out there
in terms of blog posts, in terms of, um,

48
00:02:43,260 --> 00:02:46,140
videos, in terms of anything
for reinforcement learning,

49
00:02:46,141 --> 00:02:50,790
it's so small compared to the amount
of content available for supervised

50
00:02:50,791 --> 00:02:52,800
learning, right? Neural
networks, et cetera.

51
00:02:53,010 --> 00:02:57,450
So there definitely needs to be more
content around reinforcement learning.

52
00:02:57,451 --> 00:03:00,820
And I promise you I'm a bar.
I'm going to bring that to the world.

53
00:03:00,821 --> 00:03:03,730
So starting with this course
and then more and more coming.

54
00:03:03,731 --> 00:03:06,700
So about those two questions
before we get started here.

55
00:03:06,850 --> 00:03:11,850
The first question is most important
things for a software engineer to get into

56
00:03:11,980 --> 00:03:14,350
data science.
That's a great question.

57
00:03:15,160 --> 00:03:19,390
Data Science is similar and different
to software engineering and software

58
00:03:19,391 --> 00:03:23,350
engineering. You're not doing, you're
not doing any data cleaning, right?

59
00:03:23,351 --> 00:03:27,370
Any data preprocessing which takes up
the majority unfortunately have a data

60
00:03:27,371 --> 00:03:28,330
scientist time.

61
00:03:28,570 --> 00:03:32,380
So one important thing would just be
to use the Google data sets search,

62
00:03:32,530 --> 00:03:33,910
download some datasets,

63
00:03:34,060 --> 00:03:38,020
and then use something very simple like
psychic learn in Google Colab to just

64
00:03:38,260 --> 00:03:41,170
hack together a very simple linear
regression models. So that's a,

65
00:03:41,171 --> 00:03:44,200
that's a good starting point.
Second question is,

66
00:03:44,380 --> 00:03:48,880
do I need a GPU to run what you were
about to do today? Can I use my laptop?

67
00:03:48,910 --> 00:03:53,910
Only you can use your mobile phone
because the GPU is actually in the browser

68
00:03:54,251 --> 00:03:56,950
with colab. So you don't need a Gpu. Okay.

69
00:03:57,010 --> 00:03:59,410
I would actually not recommend
you using a mobile phone.

70
00:03:59,650 --> 00:04:04,390
I would recommend you to use a, um, any
kind of basic laptop. All right, so,

71
00:04:05,560 --> 00:04:08,200
right. So I could talk to you
guys all day, but I'm, I'm,

72
00:04:08,201 --> 00:04:11,740
I'm being more disciplined here.
Okay. So this is for the greater good.

73
00:04:11,741 --> 00:04:14,830
So I'm going to minimize the chat window
and then I'll get right back into it.

74
00:04:14,830 --> 00:04:17,230
All right.
So like I said,

75
00:04:17,260 --> 00:04:22,030
deep reinforcement learning is what's
responsible for these big victories in Ai.

76
00:04:22,390 --> 00:04:26,410
And uh, it started with the deep
queue network that deepmind release.

77
00:04:26,411 --> 00:04:31,411
And it was so good that Google bought
deep mind for $500 million just because of

78
00:04:31,511 --> 00:04:34,450
that algorithm,
because that one algorithm could,

79
00:04:34,510 --> 00:04:36,850
could succeed in 60,

80
00:04:36,851 --> 00:04:40,870
I think it was 60 different Atari game
environments. It was, it was generalized,

81
00:04:40,871 --> 00:04:44,200
it could, it could learn how to master
any of these games, one algorithm.

82
00:04:44,470 --> 00:04:48,400
And that was very exciting. And
so what deep Q used to, to do,

83
00:04:48,401 --> 00:04:52,660
what it did was three specific techniques.
It used replay memory,

84
00:04:52,900 --> 00:04:55,890
large scale distributed training,
and a dis,

85
00:04:56,140 --> 00:04:59,860
a distributional modeling
method. Okay. And so this,

86
00:04:59,980 --> 00:05:02,230
this all ties into dopamine by the way.
Okay.

87
00:05:02,231 --> 00:05:04,510
This is not just like me
going off on a tangent here.

88
00:05:05,410 --> 00:05:09,130
Deep Q used these three techniques,
right? So replay memory, uh, is it's,

89
00:05:09,131 --> 00:05:10,930
it's a really,
it's a really cool idea.

90
00:05:10,931 --> 00:05:15,730
So we are in week one
of this move 37 course.

91
00:05:15,760 --> 00:05:18,610
So don't expect me to like
go all deep into deep,

92
00:05:18,730 --> 00:05:23,440
deep learning or deep reinforcement
learning right now. I will, uh,

93
00:05:23,441 --> 00:05:27,190
but to start off with, um, I'm just going
to like touch on these little concepts.

94
00:05:27,191 --> 00:05:30,370
So replay memory is saying,
um,

95
00:05:30,910 --> 00:05:35,910
let's store all of those states actions
and rewards in this giant array while we

96
00:05:37,121 --> 00:05:40,900
are learning. So there's two processes
happening when it comes to replay memory.

97
00:05:41,140 --> 00:05:43,420
There's the, the, the, the cue learning.

98
00:05:43,421 --> 00:05:47,650
So it's the process of learning what the
appropriate state action parish should

99
00:05:47,651 --> 00:05:50,590
be, uh, for whatever your environment is.

100
00:05:50,740 --> 00:05:55,330
And then next to that is this idea of
storing all of those states actions and

101
00:05:55,331 --> 00:05:58,610
rewards that the agent is
executing in that environment.

102
00:05:58,970 --> 00:06:02,600
And so interpolated these two techniques,

103
00:06:02,630 --> 00:06:07,490
acting and learning helps
improve the policy. So that's
the basic idea. All right?

104
00:06:07,491 --> 00:06:09,830
So when it comes to deep
reinforcement learning,

105
00:06:10,910 --> 00:06:15,910
all you need to know is
that neural networks are an
agent that can learn to map

106
00:06:16,220 --> 00:06:21,110
state action pairs to rewards, right? So
if we are in a state and an environment,

107
00:06:21,111 --> 00:06:24,980
right? So let's say we are in grid world,
right? We're in the bottom left corner.

108
00:06:25,100 --> 00:06:29,180
We want to perform an action, like
move up or move to the right. Uh,

109
00:06:29,240 --> 00:06:33,590
we need to know, um, what would be the
best action to do depending on the reward,

110
00:06:33,591 --> 00:06:35,000
right?
What's gonna Maximize reward?

111
00:06:35,120 --> 00:06:38,930
So if we take the state action pair
from this state, move up, you know,

112
00:06:38,960 --> 00:06:40,670
or from this state moved to the right,

113
00:06:40,700 --> 00:06:43,790
and then we take the reward
plus one zero or negative one.

114
00:06:44,120 --> 00:06:45,560
If we can learn that mapping,

115
00:06:45,920 --> 00:06:49,160
then we can find the
optimal queue function,

116
00:06:49,220 --> 00:06:52,060
which is going to give us
the optimal policy. Uh,

117
00:06:52,070 --> 00:06:53,300
and that's what neural networks can do.

118
00:06:53,301 --> 00:06:56,300
And there are a lot of ways to
approximate that function. But remember,

119
00:06:56,390 --> 00:07:00,170
neural networks are universal function
approximators and that's why they work so

120
00:07:00,171 --> 00:07:03,560
well when it comes to reinforcement
learning to learn what that cue function

121
00:07:03,561 --> 00:07:05,150
should be.
That's it.

122
00:07:05,180 --> 00:07:08,480
Don't worry if you don't understand
anything I just said that was just me like

123
00:07:08,780 --> 00:07:11,630
taking like a leap into the future, but
now we're going to come back. Right?

124
00:07:11,631 --> 00:07:16,631
So because those techniques
required such rapid experimentation,

125
00:07:17,960 --> 00:07:21,590
the researchers at Google decided that
they needed a framework that could

126
00:07:21,591 --> 00:07:26,360
encapsulates the types of iterative,

127
00:07:26,390 --> 00:07:30,050
fast design updates that they required,
right?

128
00:07:30,051 --> 00:07:32,990
So if you look at reinforcement
learning frameworks today,

129
00:07:32,991 --> 00:07:34,830
they're not very flexible.
Um,

130
00:07:34,970 --> 00:07:39,130
they require you to construct a mark
of decision process from scratch. Um,

131
00:07:39,200 --> 00:07:40,460
and then there's a lot of like,

132
00:07:40,700 --> 00:07:43,820
just fill our code that you have to write
and then let's say you're writing say

133
00:07:43,821 --> 00:07:47,550
policy improvement and in
policy evaluation and you know,

134
00:07:47,560 --> 00:07:50,990
that's policy iteration as a,
as a dynamic programming technique.

135
00:07:52,820 --> 00:07:55,340
If you want to modify that
and turn that into a say,

136
00:07:55,341 --> 00:07:58,150
Monte Carlo technique or some
other technique, it's certainly,

137
00:07:58,210 --> 00:08:00,560
it's going to require a
lot of code refactoring.

138
00:08:00,740 --> 00:08:03,800
So there needs to be a way to
enable rapid experimentation.

139
00:08:03,801 --> 00:08:07,790
So TLDR needs more flexibility,
stability,

140
00:08:07,791 --> 00:08:10,490
and reproducibility,
right? Let's be real here.

141
00:08:10,500 --> 00:08:15,230
Code reproducibility when it comes
to our El research is abysmal, right?

142
00:08:15,440 --> 00:08:18,920
Just try, just try reimplementing. Um,

143
00:08:18,921 --> 00:08:23,170
some technique using a famous RL
paper, it won't work because, uh,

144
00:08:23,210 --> 00:08:26,270
there's a lot of problems there,
right? Not, not that I won't work,

145
00:08:26,271 --> 00:08:29,150
but it's very hard.
So this is going to help with that.

146
00:08:29,270 --> 00:08:33,920
So if we look at this framework and
they've got it on get hub, it's brand new.

147
00:08:34,040 --> 00:08:38,780
It was just uploaded 19 days ago.
They've got some great documentation here,

148
00:08:38,781 --> 00:08:41,810
and then they've got some instructions
on how to install it locally,

149
00:08:41,811 --> 00:08:45,410
depending on what your environment
is. Let me, let me make this bigger,

150
00:08:45,880 --> 00:08:46,970
make this bigger as well.

151
00:08:48,110 --> 00:08:52,940
And let me make sure everybody's
doing well. All right, cool.

152
00:08:54,920 --> 00:08:58,800
All right, good. So what
does it feature? So if we,

153
00:08:58,860 --> 00:09:03,390
if we go to this folder called dopamine,
so I want you to go to this folder.

154
00:09:04,140 --> 00:09:08,100
These are the, these are the
features that it provides. Okay.

155
00:09:08,101 --> 00:09:12,720
So basically Tldr,
dopamine is taking open Ai's gym,

156
00:09:12,800 --> 00:09:17,010
uh, environment. So all those games
and tensorflow and combining them,

157
00:09:17,280 --> 00:09:19,650
I know that's a very gross
of, um, that's a, that's a,

158
00:09:19,651 --> 00:09:23,520
that's a gross oversimplification
of what the framework is. But Tldr,

159
00:09:23,521 --> 00:09:26,460
highest level, you got some
environments, you got some agents,

160
00:09:26,550 --> 00:09:30,120
let's combine that together and
then let's create this RL framework.

161
00:09:30,240 --> 00:09:33,300
So the first part of what it
offers are these agents. Okay.

162
00:09:33,301 --> 00:09:38,301
So all of these agents are basically
variations of the deep Q learner,

163
00:09:40,980 --> 00:09:45,190
right? So deep Q like I talked
about, uh, was deep mines, you know,

164
00:09:45,270 --> 00:09:47,580
famous algorithm that beat
all those Atari Games.

165
00:09:47,910 --> 00:09:52,910
So it's got a collection of those pre
coded and those agents are commented very,

166
00:09:53,101 --> 00:09:56,660
very well. I've been looking through
this code, check this out. I mean it,

167
00:09:56,800 --> 00:10:00,190
it's even got references to the
paper and each function, um,

168
00:10:00,210 --> 00:10:03,120
the arguments are all explained.
If we go into these, um,

169
00:10:03,670 --> 00:10:08,490
functions like build networks,
well documented code. Um,

170
00:10:08,520 --> 00:10:09,810
so that's,
that's very cool.

171
00:10:09,900 --> 00:10:13,920
So it's got a bunch of agents in there
and which specific agents doesn't have,

172
00:10:13,921 --> 00:10:18,180
well, it's got the deep Q network.
Right? So given some, given some,

173
00:10:18,210 --> 00:10:22,790
uh, pixels from some frames of a game, uh,

174
00:10:22,800 --> 00:10:25,170
decide what the appropriate
action should be.

175
00:10:25,171 --> 00:10:29,100
So a neural network will learn to
approximate the Q function based on those

176
00:10:29,101 --> 00:10:29,871
pixel value.

177
00:10:29,871 --> 00:10:33,480
So it's learning that mapping and then
taking those pixel values and converting

178
00:10:33,481 --> 00:10:36,510
them through actions and
whatever action space, uh,

179
00:10:36,690 --> 00:10:41,610
whenever environments specific
action space, there is now there,

180
00:10:41,611 --> 00:10:44,580
there's also see 51, which is
a, which is basically deep,

181
00:10:44,760 --> 00:10:48,400
do the same thing as deep queue,
except instead of approximated x,

182
00:10:48,750 --> 00:10:52,620
instead of x approximating
expected future reward values,

183
00:10:52,740 --> 00:10:56,370
it generates the whole distribution
over possible outcomes.

184
00:10:56,550 --> 00:11:01,140
So what this allows for
is, um, more variability in
what those rewards could be.

185
00:11:01,141 --> 00:11:05,700
It's not just a scaler, it's a
distribution of possibilities,
right? Like, uh, uh,

186
00:11:05,701 --> 00:11:10,260
a Gaussian or etc. There's also rainbow.
So Rainbow,

187
00:11:10,490 --> 00:11:15,300
uh, there've been a bunch of extensions
to the deep Q network. Um, no priority,

188
00:11:15,301 --> 00:11:17,880
no dueling, don't worry if you
don't understand any of these terms.

189
00:11:17,881 --> 00:11:22,650
There's a lot of terms to Grok to
understand and reinforcement learning.

190
00:11:22,980 --> 00:11:26,400
And I'm going to go over
the most important ones,
so don't worry. Okay. Just,

191
00:11:26,790 --> 00:11:30,150
let's just, let's just, let's just chill
here. And this is this a lot to take in.

192
00:11:30,151 --> 00:11:32,910
I know this is a lot to take
in, but, but bear with me. So,

193
00:11:32,911 --> 00:11:37,911
so rainbow basically combined
six different techniques
into one algorithm and it

194
00:11:39,810 --> 00:11:43,770
outperforms the rest. And then there's
implicit Quanta, which is very new.

195
00:11:43,771 --> 00:11:48,630
This actually just came out, I think it
was last month at the ICML conference. Uh,

196
00:11:48,631 --> 00:11:52,690
but it's similar in that it's
kind of based off the idea of,

197
00:11:52,740 --> 00:11:57,460
of the deep Q network
accepted approximates the full
Quan tile function for the

198
00:11:57,461 --> 00:12:00,250
state action return distribution.
So again,

199
00:12:00,251 --> 00:12:04,510
the idea of a distribution is used for
the rewards over a single scalar value.

200
00:12:04,810 --> 00:12:07,180
And don't worry if you don't
understand any of that, but anyway,

201
00:12:07,300 --> 00:12:12,300
it offers those agents
encapsulated already coded
for you and we can then build

202
00:12:13,691 --> 00:12:16,080
our own agents off of those.
So that's that.

203
00:12:16,510 --> 00:12:20,260
The other thing it provides
besides these pre coded agents,

204
00:12:22,160 --> 00:12:24,300
our environments,
so under a tare,

205
00:12:24,600 --> 00:12:28,960
preprocessing is not necessary for us
run experiments, trained the Pi's. Uh,

206
00:12:29,310 --> 00:12:33,990
okay. So basically it's
going to run a UN, UN, UN,

207
00:12:34,320 --> 00:12:36,080
whichever environment
that you'd like from.

208
00:12:36,090 --> 00:12:40,410
So there are 60 different Atari games
that you could use fright from the Atari

209
00:12:41,010 --> 00:12:44,760
learning environment, the Ale,
the arcade learning environment,

210
00:12:46,200 --> 00:12:49,730
and it's using Jin. Jin
is basically just, um, uh,

211
00:12:49,780 --> 00:12:52,350
a configuration framework for python.
You're going to see gin,

212
00:12:52,380 --> 00:12:54,910
like kind of sprinkled in this code where,
you know,

213
00:12:54,930 --> 00:12:59,640
if you have a lot of parameters to be
able to easily replace those parameters

214
00:12:59,790 --> 00:13:03,630
with new values, uh, would be efficient,
right? So in the case of neural networks,

215
00:13:03,631 --> 00:13:08,130
so we have possibly many
layers with possibly many
different types of activation

216
00:13:08,131 --> 00:13:11,670
functions. If we could just plug in,
play different activation functions,

217
00:13:11,850 --> 00:13:14,880
that would be great for research, right?
And so that's what Jen helps with.

218
00:13:15,480 --> 00:13:20,470
So there's that. Um, there's,
and there's common, right?

219
00:13:20,471 --> 00:13:24,370
So in addition to having that environment
and a bunch of agents to act in their

220
00:13:24,371 --> 00:13:25,204
environment,

221
00:13:25,210 --> 00:13:28,660
it's got a common library which is
going to provide us with logging and

222
00:13:28,661 --> 00:13:32,370
checkpointing, right? So any
kind of debugging we need
to do right? Experiments,

223
00:13:32,371 --> 00:13:37,240
statistics, agent variables, including
the tensor flow graph, itself's replay,

224
00:13:37,241 --> 00:13:38,650
buffered data,
right?

225
00:13:38,651 --> 00:13:43,540
So sometimes having a replay memory
can be a giant memory footprint, right?

226
00:13:43,541 --> 00:13:47,860
So imagine just storing all the states,
all the actions, all the rewards.

227
00:13:48,070 --> 00:13:52,600
Eventually that's going to become
a giant table in memory. So, um,

228
00:13:52,630 --> 00:13:56,920
this helps minimize that. So
that's under common. Okay.

229
00:13:57,190 --> 00:14:01,150
So, um, that's under common right here,

230
00:14:02,260 --> 00:14:06,960
checkpoint or iterations to sticks, et
cetera. And then there's replay memories.

231
00:14:06,961 --> 00:14:10,690
So replay memory, there's
actually different types of
replay memory techniques,

232
00:14:10,720 --> 00:14:14,350
circular replay, prioritize,
replaced some tree. Now,

233
00:14:14,351 --> 00:14:17,830
I haven't talked about any of these
before and I cannot wait to start talking

234
00:14:17,831 --> 00:14:21,100
about them, but, uh, they're
all under here as python files.

235
00:14:21,760 --> 00:14:24,880
And of course there's tests sing, right,
for production use cases, et Cetera,

236
00:14:24,881 --> 00:14:28,780
et cetera.
So that was it for that.

237
00:14:29,050 --> 00:14:30,010
Now.

238
00:14:33,210 --> 00:14:34,043
Good.

239
00:14:35,760 --> 00:14:39,900
Okay. So, all right, so that's it for the
framework. We're going to start coding,

240
00:14:39,901 --> 00:14:43,640
but I want to go over some very
important concepts right now. Um,

241
00:14:43,740 --> 00:14:46,590
when it comes to reinforcement
learning. Okay. So, like I said,

242
00:14:46,591 --> 00:14:49,000
there are a ton of different,
uh,

243
00:14:49,320 --> 00:14:52,580
techniques that we could use for
reinforcement learning, right?

244
00:14:52,581 --> 00:14:55,400
Different types of replay memory,
deep cue learning,

245
00:14:55,401 --> 00:15:00,260
writes a deep reinforcement learning in
general, Implicit Quan tile, you know,

246
00:15:00,261 --> 00:15:03,890
et cetera. All of these
techniques. But really there are,

247
00:15:05,090 --> 00:15:09,320
there are four concepts
that are the most important.

248
00:15:09,321 --> 00:15:14,321
They are objectively the most important
concepts to know in reinforcement

249
00:15:14,781 --> 00:15:19,280
learning. And here's what they are.
One, the Mark Haub decision process.

250
00:15:19,340 --> 00:15:23,690
That's the first one to a policy.
Three,

251
00:15:23,900 --> 00:15:28,900
the state value function
and three and four,

252
00:15:29,240 --> 00:15:33,590
the action value function. Okay.
And five, the bellman equation.

253
00:15:33,591 --> 00:15:37,420
So five, actually there are five
really important concepts, you know,

254
00:15:37,430 --> 00:15:38,840
and if you know those five,

255
00:15:39,260 --> 00:15:43,430
everything else is based
off of these five concepts.

256
00:15:43,670 --> 00:15:48,530
The mark of decision process date with me,
the mark of decision process. Say, okay,

257
00:15:48,950 --> 00:15:53,300
the policy,
the state value function,

258
00:15:54,170 --> 00:15:58,340
the action value function
and the bellman equation,

259
00:15:59,270 --> 00:16:03,660
there's four of them. So I was
actually pretty amazing. Uh,

260
00:16:03,710 --> 00:16:06,170
once you start to get into this RL world,

261
00:16:06,440 --> 00:16:11,000
you start framing life and reality in
terms of our l and you start seeing how

262
00:16:11,001 --> 00:16:15,740
the mark of decision process is a great
mathematical framework for framing. Um,

263
00:16:15,800 --> 00:16:19,400
any kind of learning agent in an
environment. And a, it's just,

264
00:16:19,401 --> 00:16:22,220
it's just beautiful. The more,
the more you stare at this stuff,

265
00:16:22,221 --> 00:16:23,330
the more beautiful it seems.

266
00:16:23,331 --> 00:16:27,020
And I guess that applies to all sorts
of science and math. But in particular,

267
00:16:27,021 --> 00:16:28,760
reinforcement learning is a very,
um,

268
00:16:30,140 --> 00:16:34,130
this theory that the theories that have
been, it's been built on are amazing.

269
00:16:34,340 --> 00:16:37,840
And you know, I was kind of skeptical at
first because I thought, you know who,

270
00:16:37,970 --> 00:16:41,750
who says the mark of decision process
should be the way that we're going? Right?

271
00:16:41,870 --> 00:16:45,320
What if we just discard that and
think of an entirely new framework?

272
00:16:45,321 --> 00:16:46,154
But you know what?

273
00:16:46,430 --> 00:16:49,940
They really put a lot of thoughts into
this and there's a reason that it's stuck

274
00:16:49,941 --> 00:16:54,410
around for so long because it's a
very simple yet elegant and extendable

275
00:16:54,411 --> 00:16:57,890
framework, mathematical framework,
right? So in any environment,

276
00:16:57,891 --> 00:17:01,400
you have a set of states, right? Whatever
the state could be. If you're skiing,

277
00:17:01,401 --> 00:17:04,940
the states could be skiing, down, skiing,
up. I don't think skiing up is a thing.

278
00:17:05,030 --> 00:17:09,680
Skiing to the size, you to the right,
you have a set of actions push forward,

279
00:17:09,710 --> 00:17:13,220
push back, um, look right.
Whatever those actions should be.

280
00:17:13,490 --> 00:17:17,030
You have a transition function that says
what's the probability of going to a

281
00:17:17,031 --> 00:17:19,760
certain state?
If you take an action in this state,

282
00:17:20,120 --> 00:17:21,710
you have a starting state distribution,

283
00:17:21,711 --> 00:17:23,180
which we're not going
to talk about right now.

284
00:17:23,540 --> 00:17:28,340
You have a discount factor which says
if you're in a state,

285
00:17:28,400 --> 00:17:30,770
right, if you're in a state,
let's just talk about grid world.

286
00:17:30,771 --> 00:17:35,060
I think red world is a great
introductory example here in grid world.

287
00:17:35,061 --> 00:17:38,360
Looks like this where if you're in a
grid world and you're trying to get from

288
00:17:38,361 --> 00:17:42,490
point a to point B, you could go
up, down, left, right? Then, uh,

289
00:17:42,590 --> 00:17:44,750
so just that's,
that's the framework here.

290
00:17:46,760 --> 00:17:50,310
What should be the next state that you
go to, right? If you're in one state,

291
00:17:50,520 --> 00:17:55,230
what would be the best next state to
go to to optimize your reward? And

292
00:17:56,770 --> 00:17:59,880
what the discount factor does is it is,

293
00:17:59,881 --> 00:18:04,800
it helps to compute the expected

294
00:18:06,390 --> 00:18:08,010
cumulative reward,

295
00:18:08,520 --> 00:18:13,050
the expected cumulative discounted reward.
So it could be discounted,

296
00:18:13,200 --> 00:18:16,890
it could not be discounted. So there's
so much to explain here. So like, okay,

297
00:18:17,280 --> 00:18:20,550
if you're in a state, so check
this out. If you're in a state,

298
00:18:20,580 --> 00:18:25,410
in an environment and you want to see
what would be the expected set of rewards,

299
00:18:25,411 --> 00:18:28,140
I would get from going from say,
you know,

300
00:18:28,170 --> 00:18:31,830
let's say I'm in this bottom corner right
here and I want to eventually end up

301
00:18:31,831 --> 00:18:34,050
in the top right corner.
So all of these are states,

302
00:18:34,080 --> 00:18:38,910
all of these squares are states and I'm
gonna earn a reward based on which state

303
00:18:38,911 --> 00:18:42,090
I'm in. So if I could look ahead
into the future and say, well,

304
00:18:42,091 --> 00:18:45,120
based on this state that I'm in right now,
if I were to go to that state,

305
00:18:45,121 --> 00:18:47,550
that state that say that's
an eventually to the end,

306
00:18:47,730 --> 00:18:49,950
what would be the total
reward that I would get?

307
00:18:50,070 --> 00:18:52,830
And so we could sum up all
of those rewards, right?

308
00:18:52,831 --> 00:18:55,170
And then we could add them up
and it becomes a scalar value.

309
00:18:55,230 --> 00:18:57,240
And that's the ex that's
called the return.

310
00:18:57,660 --> 00:19:00,150
So the return is the
cumulative future reward.

311
00:19:00,210 --> 00:19:04,440
So if we look ahead into the future and
we try to sum up all of those rewards,

312
00:19:04,620 --> 00:19:06,750
that's going to be the
cumulative future reward.

313
00:19:06,990 --> 00:19:11,490
Now we can apply what's called a discount
factor and that's a scalar value.

314
00:19:11,700 --> 00:19:14,490
Usually it's 0.9 as it is,

315
00:19:14,491 --> 00:19:17,160
but it can be pointed whatever
in terms of research papers.

316
00:19:18,300 --> 00:19:20,460
And what that will do
is it's going to say,

317
00:19:20,520 --> 00:19:24,210
how much should we take into consideration
the short term versus the future.

318
00:19:24,390 --> 00:19:25,201
And so it's a,
it's a,

319
00:19:25,201 --> 00:19:29,550
it's a variable that tweaks how much an
agent cares about maximizing for short

320
00:19:29,551 --> 00:19:32,670
term rewards versus future rewards,
longterm awards.

321
00:19:32,700 --> 00:19:35,760
And that depends on the
environment that you're in. Okay.

322
00:19:35,761 --> 00:19:39,360
So that's the idea of a
mark Haub decision process.

323
00:19:39,480 --> 00:19:41,160
An agent is in an environment,

324
00:19:41,190 --> 00:19:44,820
it takes an action in that environment
based on its state and it tries to

325
00:19:44,821 --> 00:19:48,570
maximize for the reward value,
right? So in a given state,

326
00:19:48,600 --> 00:19:52,980
it will take an action to go to the next
state such that that that action will

327
00:19:52,981 --> 00:19:57,060
maximize its reward.
Speaking of that, okay,

328
00:19:58,440 --> 00:20:00,270
for an example,
racing is another example,

329
00:20:00,271 --> 00:20:03,750
but I think I just gave the grid world
example, which is a, which is pretty nice,

330
00:20:03,870 --> 00:20:08,790
right? So what do we call that? Right? So
once an agent knows, right, it nodes the,

331
00:20:08,940 --> 00:20:11,830
the, the best action
to take. Let's say we,

332
00:20:11,910 --> 00:20:16,710
we have trained our agent very well and
it knows what the best action should be.

333
00:20:17,520 --> 00:20:22,080
Now it knows exactly what to do. What
do we call that? We call that a policy.

334
00:20:22,350 --> 00:20:24,540
Okay.
That is called a policy.

335
00:20:24,840 --> 00:20:29,430
What the policy does is
it says it knows exactly,

336
00:20:29,520 --> 00:20:32,130
it knows exactly,
um,

337
00:20:32,790 --> 00:20:36,780
what is the best action to take
given this state, right? It's,

338
00:20:36,781 --> 00:20:40,920
it's a function that takes in a state
and an action and returns the probability

339
00:20:40,921 --> 00:20:43,350
of taking that action in that state.

340
00:20:43,470 --> 00:20:48,130
So what it's going to do is going to
take the action that gives the highest of

341
00:20:48,131 --> 00:20:51,880
achieving that maximum reward, and that's
the action that it's going to take.

342
00:20:51,881 --> 00:20:55,630
So get given a state is going to say
all of these actions are possible,

343
00:20:55,660 --> 00:21:00,640
but this particular action is going to
maximize future reward and that is the

344
00:21:00,700 --> 00:21:05,200
optimal policy. And we have to learn
what that optimal policy will be.

345
00:21:05,370 --> 00:21:07,990
And that optimal policy is a function.
Right?

346
00:21:08,860 --> 00:21:09,693
Okay.

347
00:21:09,970 --> 00:21:13,820
So how do we compute that optimal policy?
So we as humans, and this is, this is,

348
00:21:13,930 --> 00:21:17,440
this is what I'm talking about,
about relating it to reality.

349
00:21:17,470 --> 00:21:19,720
We as humans have an optimal policy,
right?

350
00:21:19,721 --> 00:21:24,430
So given this environment
of teaching this, um, I,

351
00:21:24,490 --> 00:21:27,730
the state of teaching,
I know to make the next,

352
00:21:27,760 --> 00:21:32,410
the next action to take is to continuously
scroll down and continue teaching

353
00:21:32,500 --> 00:21:36,610
because that's going to maximize my reward
in terms of the amount of people that

354
00:21:36,611 --> 00:21:40,150
are learning this.
That's my objective and that's my policy.

355
00:21:40,151 --> 00:21:44,650
That's a very abstract metta. But anyway,
you get what I'm saying? I hopefully.

356
00:21:44,740 --> 00:21:48,430
Um, so how do we, let me just make
sure everybody's on board here.

357
00:21:49,330 --> 00:21:50,290
Everybody's good.

358
00:21:55,700 --> 00:21:59,300
Okay. So how do we compute
that policy now? Okay.

359
00:21:59,330 --> 00:22:04,330
The way we compute the policy is by using
value functions and don't go anywhere

360
00:22:04,641 --> 00:22:09,641
because here is where I think a lot of
people give up on our El because the idea

361
00:22:10,341 --> 00:22:12,770
of a value function can be
confusing, right? There's,

362
00:22:12,771 --> 00:22:15,680
there's some terms that we have
to essentially memorize here,

363
00:22:15,830 --> 00:22:18,950
but what I'm saying is this
is totally doable. Okay.

364
00:22:19,160 --> 00:22:22,220
There are two value functions,
so right?

365
00:22:22,221 --> 00:22:26,090
So remember an agent is in an environment
and it's trying to pick the best

366
00:22:26,091 --> 00:22:28,670
action to take.
So how is it going to do that?

367
00:22:28,820 --> 00:22:33,500
How is it going to learn this policy of
what's the best action to take? Well,

368
00:22:33,560 --> 00:22:38,560
it needs a way of evaluating both
the state that it's in right now and

369
00:22:39,530 --> 00:22:39,700
okay,

370
00:22:39,700 --> 00:22:43,000
what the value of each action
it could take would be,

371
00:22:43,210 --> 00:22:45,790
so there are two value functions.

372
00:22:45,970 --> 00:22:48,430
You have the state value function.

373
00:22:49,740 --> 00:22:51,030
Let me maximize this.

374
00:22:52,590 --> 00:22:53,423
Okay,

375
00:22:54,930 --> 00:22:56,910
let me just,
these equations are what matter here.

376
00:22:56,911 --> 00:23:00,210
So I'm just going to make
that big. You have. So the,

377
00:23:00,220 --> 00:23:02,070
the equation right here at the top,

378
00:23:02,340 --> 00:23:07,050
that is the state value function in one
way to think about this is by using an

379
00:23:07,051 --> 00:23:07,884
apostrophe.

380
00:23:08,100 --> 00:23:11,940
So say so by saying that the value
function actually belongs to the state.

381
00:23:12,090 --> 00:23:16,410
So it's the state's value function.
So the state has its own value function,

382
00:23:18,140 --> 00:23:22,730
okay? And so what it does is it describes
the value of a state when following a

383
00:23:22,731 --> 00:23:23,564
policy,

384
00:23:23,600 --> 00:23:27,710
it's the expected return when starting
from state ass according to our policy,

385
00:23:27,980 --> 00:23:28,191
right?

386
00:23:28,191 --> 00:23:31,670
So what is going to do is going to
evaluate the current state that I'm in by

387
00:23:31,671 --> 00:23:36,560
looking at all the possible actions I
could be taking in any direction in this

388
00:23:36,561 --> 00:23:38,180
action space and saying,

389
00:23:38,270 --> 00:23:43,270
here's a scalar value that represents
the value of being in this state.

390
00:23:43,490 --> 00:23:45,050
But if I was in a different state,

391
00:23:45,230 --> 00:23:48,970
then I have a different value for
all those actions, right? So it, it,

392
00:23:48,980 --> 00:23:51,980
it measures the value of
this particular state.

393
00:23:52,640 --> 00:23:56,270
But notice that in order to measure
the value of this particular state,

394
00:23:56,750 --> 00:24:01,520
we need to find a way to measure the
value of each action that we would take in

395
00:24:01,521 --> 00:24:06,500
that state. And that's where the action
value function comes into play. Again,

396
00:24:06,620 --> 00:24:09,620
think of it like an apostrophe.
It belongs to the action.

397
00:24:09,830 --> 00:24:12,620
So it's the actions value function.

398
00:24:12,860 --> 00:24:17,360
So the state has its own value function
and then the action has its own value

399
00:24:17,361 --> 00:24:18,110
function.

400
00:24:18,110 --> 00:24:22,670
And these two value functions combined
help us approximate the policy,

401
00:24:22,820 --> 00:24:23,653
right?

402
00:24:23,720 --> 00:24:27,860
Because a policy consists of
both value function functions.

403
00:24:28,100 --> 00:24:30,680
What's the value of the state?
And to compute that,

404
00:24:30,681 --> 00:24:34,250
I need to know the value of each
action I could be taking in this state.

405
00:24:34,580 --> 00:24:39,080
And that's how they both
relate to each other now,

406
00:24:39,560 --> 00:24:44,360
okay, the last part, that
was four or five, the last
part, the bellman equations.

407
00:24:44,690 --> 00:24:47,870
Okay. So there are four of them,
but we're going to use two of them.

408
00:24:48,080 --> 00:24:52,430
So how do we compute those value
functions, correct, or rights?

409
00:24:52,580 --> 00:24:56,840
How do we compute those value functions?
Well,

410
00:24:56,841 --> 00:25:01,210
we need a way of relating the
current state that we're in to, uh,

411
00:25:01,350 --> 00:25:05,360
to other states. And if we were able to
frame it that way, if we were able to say,

412
00:25:05,540 --> 00:25:09,800
here is what this state equals two and
here is how it relates to other states,

413
00:25:10,520 --> 00:25:14,870
well then we could, we could then compute
what the optimal values should be.

414
00:25:16,010 --> 00:25:21,010
So what the Bellmont equation lets us do
is it lets us express values of states

415
00:25:22,310 --> 00:25:27,200
as values of other states. So it
relates a current state to other states.

416
00:25:27,290 --> 00:25:30,620
And if we can do that,
if we can do that,

417
00:25:30,950 --> 00:25:34,100
then we can characterize
fully the learning problem.

418
00:25:34,820 --> 00:25:39,200
And once we've characterized in terms
of mathematically defined how these two

419
00:25:39,201 --> 00:25:42,740
value functions related to the
policy in this specific environment,

420
00:25:42,890 --> 00:25:47,890
then we can compute them using a technique
like say dynamic programming or Monte

421
00:25:47,930 --> 00:25:51,500
Carlo techniques or salsa
or deep learning, et cetera.

422
00:25:51,920 --> 00:25:56,920
But what the bellman equation does is it
lets us frame the problem in a way that

423
00:25:57,021 --> 00:26:02,000
these, um, these functions relate to
each other. So we have first of all,

424
00:26:02,120 --> 00:26:04,250
the optimal value function,

425
00:26:04,310 --> 00:26:08,120
which relates to the optimal action
value function. The Q function,

426
00:26:09,680 --> 00:26:14,680
the optimal value function is equivalent
to the action that's going to give us

427
00:26:14,991 --> 00:26:17,870
the highest probability,
the Max probability of

428
00:26:19,580 --> 00:26:20,840
achieving our reward.

429
00:26:21,350 --> 00:26:25,040
And we can then further break
that down into this last equation.

430
00:26:25,041 --> 00:26:27,500
And this is how it relates.
Using the bellman equation,

431
00:26:27,680 --> 00:26:32,030
we can compute what the optimal state
value function should be using that action

432
00:26:32,031 --> 00:26:36,020
value function. And that's the bellman
equation. And once we have both of those,

433
00:26:36,290 --> 00:26:40,310
we've got our optimal policy.
Okay, that's the highest level.

434
00:26:40,311 --> 00:26:44,130
Now I've got some great videos coming out
on both of these concepts this weekend.

435
00:26:44,490 --> 00:26:45,480
Can't wait for that.
All right,

436
00:26:45,481 --> 00:26:48,810
so now I'm going to take some Q and a
before I start coding in this framework.

437
00:26:50,410 --> 00:26:54,590
All right, let me see how many people are
in the room here. Oh, 801 hi. Hello world.

438
00:26:54,650 --> 00:26:58,940
800 people are here. I love
it. I love it. Um, yes,

439
00:26:58,941 --> 00:27:02,180
this video will be, so let me take two
questions before we start coding. Okay.

440
00:27:03,620 --> 00:27:07,880
Okay. Here's a great
question. Sh show. It asks,

441
00:27:07,910 --> 00:27:12,740
what if the R L agent chooses Paul
a policy with the most reward?

442
00:27:12,890 --> 00:27:17,750
But in that time the environment changes
a bit and the chosen policy is not the

443
00:27:17,751 --> 00:27:19,640
best.
Great question.

444
00:27:19,641 --> 00:27:24,641
So there are two types of policies on
deterministic policy and a stochastic

445
00:27:25,131 --> 00:27:28,700
policy. Okay. And depending
on your environment, you're
going to want to pick one.

446
00:27:28,870 --> 00:27:31,160
A deterministic policy
will be predictable.

447
00:27:31,161 --> 00:27:33,800
You're going to know what the
outputs are going to be us.

448
00:27:33,801 --> 00:27:35,420
The castic policy on the other hand,

449
00:27:35,421 --> 00:27:39,580
is unpredictable and it's usually
leveraging the usage of um,

450
00:27:39,710 --> 00:27:42,830
distribution, no values.
And so you'd want,

451
00:27:42,831 --> 00:27:45,830
he's a sarcastic policy in that context.
Second question,

452
00:27:47,150 --> 00:27:50,660
what is Pi doing there in the equations?
Okay.

453
00:27:51,020 --> 00:27:53,780
Pi is not the, yes. Okay, check this out.

454
00:27:53,870 --> 00:27:58,370
This pi value does not mean pie like 3.14.
That's another confusing thing.

455
00:27:58,400 --> 00:28:02,260
We should've picked a different,
um, we should've picked a different,

456
00:28:02,261 --> 00:28:06,590
a symbol for the optimal policy,
but that's, it's too late.

457
00:28:07,100 --> 00:28:11,780
The Pi symbol in the context of
reinforcement learning means the policy,

458
00:28:11,840 --> 00:28:16,280
not 3.14. So just take that out
of your head for right now. Okay.

459
00:28:18,880 --> 00:28:22,150
Now dopamine versus open Ai, Jim. Um,

460
00:28:24,430 --> 00:28:27,400
it wraps, it literally
wraps open Ai. Jim, open Ai.

461
00:28:27,401 --> 00:28:31,510
Jim is one of the dependencies here,
right? And we can do that by saying, uh,

462
00:28:31,540 --> 00:28:33,510
we can look for what that is by saying,
uh,

463
00:28:33,640 --> 00:28:37,540
import gym inside of this repository.

464
00:28:38,830 --> 00:28:43,600
And there it is. This repository
is importing gym, right?

465
00:28:43,601 --> 00:28:47,350
And this trained up py import gym. There
we go, right? So it's important Jim.

466
00:28:48,120 --> 00:28:50,590
So it's, it's, it's contained.
Jim is contained here. All right,

467
00:28:50,591 --> 00:28:53,500
let's start coding guys.
So here we go. All right,

468
00:28:54,430 --> 00:28:57,430
we're going to build a simple
agent using Google dopamine.

469
00:28:59,750 --> 00:29:04,340
So the first step is to install our
packet. Let me just a short Q and a here.

470
00:29:04,670 --> 00:29:08,690
So the first step is for us to install
our packages and we have some packages to

471
00:29:08,691 --> 00:29:11,960
install the me make this
bigger here. Um, all right.

472
00:29:11,990 --> 00:29:16,990
So the first package we're going to
install is the of the dopamine library.

473
00:29:18,860 --> 00:29:20,540
So dopamine,
our l,

474
00:29:20,960 --> 00:29:24,350
and then we're going to
install the c make library,

475
00:29:24,351 --> 00:29:27,950
which is one of dopamine is dependencies
as well as the arcade learning

476
00:29:27,951 --> 00:29:32,240
environment. Okay,
that's going to help us.

477
00:29:36,930 --> 00:29:41,160
No cache directory.
Pip install.

478
00:29:41,740 --> 00:29:42,573
There we go.

479
00:29:44,150 --> 00:29:44,983
[inaudible]

480
00:29:45,130 --> 00:29:47,060
so let's let this, uh, that,

481
00:29:47,190 --> 00:29:49,570
those are our three dependencies
that we need to install. Still Real.

482
00:29:49,780 --> 00:29:52,600
We'll let those install. Great.
They were just installed. Okay.

483
00:29:52,601 --> 00:29:55,480
So that's the first part. Now let's
import our dependencies. Okay.

484
00:29:55,750 --> 00:29:58,870
So the first one, we're going to
import his num Pi for Matrix math.

485
00:29:59,080 --> 00:30:02,410
The second one we're going to import
his hoe ass took for us to help load our

486
00:30:02,411 --> 00:30:06,010
files.
The third one is our dopamine framework,

487
00:30:06,011 --> 00:30:08,170
specifically the Dq an agent,

488
00:30:08,171 --> 00:30:12,250
which we're going to use to test as
a baseline against our simple agent.

489
00:30:14,200 --> 00:30:17,650
We're also going to use
the Atari a environment.

490
00:30:17,710 --> 00:30:22,710
So the run experiment is the function
is the execution engine that's going to

491
00:30:22,841 --> 00:30:26,440
relate the agent and the
environment together.

492
00:30:27,430 --> 00:30:29,760
Then we have our,
um,

493
00:30:29,800 --> 00:30:32,890
we have a little helper
functions specifically for Colab,

494
00:30:33,160 --> 00:30:37,570
which is going to let us a download
data directly into our colab as well as

495
00:30:37,571 --> 00:30:38,980
visualize it very easily.

496
00:30:39,520 --> 00:30:43,990
And then we have this important flags
just for like warnings that we could

497
00:30:44,140 --> 00:30:45,100
potentially have

498
00:30:46,280 --> 00:30:47,113
[inaudible]

499
00:30:47,300 --> 00:30:51,140
dopamine. Right. That's
what I wanted to import.

500
00:30:52,530 --> 00:30:53,140
Okay,

501
00:30:53,140 --> 00:30:58,090
good. Okay. Now we're going to
create our agent from scratch. Um,

502
00:30:58,120 --> 00:31:01,030
but there's actually, oh, you know what,

503
00:31:02,020 --> 00:31:06,040
I need to define, um, where
to store those training logs.

504
00:31:06,370 --> 00:31:10,790
So we're going to store our training
logs in as temporary directory.

505
00:31:10,930 --> 00:31:14,620
I'm going to call colab dope run.
Um,

506
00:31:14,950 --> 00:31:19,510
and then which arcade environment should
we use? Let's use asterisks. Uh, there's,

507
00:31:19,570 --> 00:31:23,890
we could use any of them asteroids or
any of them, but let's just use this one.

508
00:31:24,310 --> 00:31:26,380
Okay. Make sure that works. Good. Alright,

509
00:31:26,440 --> 00:31:30,790
so that's create our agent from
scratch now. So before we start,

510
00:31:31,120 --> 00:31:34,300
I'm going to, let me make sure everybody
can see what I'm writing here. All good?

511
00:31:34,301 --> 00:31:38,360
Everybody right?
All right,

512
00:31:38,900 --> 00:31:41,420
so let's, Oh, also before we even do this,

513
00:31:41,450 --> 00:31:44,630
let me also just quickly
show you tensor board.

514
00:31:44,660 --> 00:31:47,830
So this is a premade notebook and um,

515
00:31:49,270 --> 00:31:52,180
what it's going to allow us to do is
visualize this intense or board in the

516
00:31:52,181 --> 00:31:55,090
browser. Okay? So, um,

517
00:31:56,680 --> 00:31:58,780
this is something that
I been meaning to do.

518
00:31:59,260 --> 00:32:02,500
So this is going to install
a bunch of dependencies.

519
00:32:02,980 --> 00:32:07,660
It's my take a while. Uh,
and then when that's done,

520
00:32:08,740 --> 00:32:11,560
dependencies of course,
take a while.

521
00:32:14,890 --> 00:32:17,830
Okay.
I want to visualize asterisk.

522
00:32:18,710 --> 00:32:19,430
Okay.

523
00:32:19,430 --> 00:32:20,263
Hasta

524
00:32:23,950 --> 00:32:28,480
and so now it's training and then I'm
going to show you tense or board in the

525
00:32:28,481 --> 00:32:31,750
browser. It's very cool. I just want to
just start off with something very visual,

526
00:32:32,080 --> 00:32:36,690
um, before we go into other
things. Okay. So let that train.

527
00:32:37,600 --> 00:32:38,340
Okay.

528
00:32:38,340 --> 00:32:40,250
And let me continue creating my agent

529
00:32:40,250 --> 00:32:43,940
here. So the first step for
us is to create a log path.

530
00:32:43,941 --> 00:32:46,520
And what this is going to do is tell our

531
00:32:48,620 --> 00:32:53,030
agent where to store. It's a
where to store the log data. Okay?

532
00:32:53,240 --> 00:32:56,870
So I'm going to call it a basic agent
inside of this game environment.

533
00:32:56,871 --> 00:32:59,840
And that's the log path are right.

534
00:33:03,290 --> 00:33:07,190
Let me just remove this because I
need this full space here. Okay.

535
00:33:07,970 --> 00:33:10,730
Log Path. Great. No,
let's create this agent.

536
00:33:10,731 --> 00:33:15,530
So I'm going to call it a
basic agent. Basic agent. Now,

537
00:33:16,550 --> 00:33:20,080
dopamine expects a certain,
um,

538
00:33:21,830 --> 00:33:26,510
framework for defining our functions here.
And I'm going to,

539
00:33:26,511 --> 00:33:31,070
what I'm gonna start off with is
defining what those functions should be.

540
00:33:31,190 --> 00:33:36,050
Okay? So the first one
is choose actions. Um,

541
00:33:36,080 --> 00:33:37,130
how to select an action.

542
00:33:37,131 --> 00:33:42,131
Basically the next one is what
is the bundle and the checkpoint.

543
00:33:43,850 --> 00:33:46,160
So this is when it
checkpoints during training.

544
00:33:49,170 --> 00:33:49,480
Okay.

545
00:33:49,480 --> 00:33:52,300
Uh, anything we should do, like
whenever we check point during training,

546
00:33:52,301 --> 00:33:55,960
this is going to be a fired whenever.
Um,

547
00:33:56,860 --> 00:33:58,600
we are saving something.

548
00:33:58,660 --> 00:34:02,950
Actually I'll just put pass here cause
we don't really want to do anything. Um,

549
00:34:03,790 --> 00:34:07,430
there's also load from, so whenever
we're loading from a checkpoint,

550
00:34:07,600 --> 00:34:11,890
unbundle is going to be called. So
we want to do some, we might want to,

551
00:34:12,130 --> 00:34:16,620
um, do something if, if we
are loading from a checkpoint,

552
00:34:16,690 --> 00:34:20,740
any kind of cleanup or maybe reset
our environment or, you know,

553
00:34:20,741 --> 00:34:25,130
depending on what we're doing, we
might want to, um, you know, there's,

554
00:34:25,210 --> 00:34:28,900
there's any number of things that we
could be doing a, but for right now,

555
00:34:28,901 --> 00:34:33,310
this simple agent on been dope.
We don't want to do anything.

556
00:34:33,860 --> 00:34:38,720
All right? So there's that. There's also
began episodes. So whenever we, um, the,

557
00:34:38,780 --> 00:34:42,460
what's the first action that we want
to take inside of this environment?

558
00:34:44,170 --> 00:34:44,630
Okay.

559
00:34:44,630 --> 00:34:49,340
Unused observation. All right. Pass.

560
00:34:50,150 --> 00:34:54,260
And, um, there's one more
and that's the step function.

561
00:34:54,261 --> 00:34:56,540
So we can update our policy here actually.

562
00:34:56,541 --> 00:35:00,740
So given a reward for whatever environment
we're in, and this is where it's,

563
00:35:00,741 --> 00:35:04,190
I'm inheriting from Open
Ai's gym environment, let's,

564
00:35:04,430 --> 00:35:09,230
let's return what the
optimal action should be.

565
00:35:09,890 --> 00:35:10,910
Okay.
Um,

566
00:35:12,170 --> 00:35:16,960
and so that's really it for our,
this basic step and,

567
00:35:16,961 --> 00:35:21,080
and, and once we have that, we
can then create another function.

568
00:35:21,081 --> 00:35:21,920
But let me go back up here.

569
00:35:21,921 --> 00:35:26,450
And so this is kind of the skeleton
that we need for Google dopamine, right?

570
00:35:26,451 --> 00:35:29,870
And so this is, you know, a
way of defining a bunch of

571
00:35:31,400 --> 00:35:35,000
whatever our technique we'll be.
Okay. So for this basic agent,

572
00:35:35,001 --> 00:35:38,280
let's just choose a random action to take.
Very simple,

573
00:35:38,430 --> 00:35:42,690
and then we could extend that to you say,
dynamic programming, value, iteration,

574
00:35:42,840 --> 00:35:44,700
policy, iteration, any kind of these,

575
00:35:45,000 --> 00:35:49,110
any of these very simple
planning techniques where
the entire mark of decision

576
00:35:49,111 --> 00:35:52,540
process, all of those more
Covidien variables are known. Uh,

577
00:35:52,560 --> 00:35:57,060
we can use something like dynamic
programming, but to start off with, let us

578
00:35:58,680 --> 00:36:00,840
code up a very simple policy.

579
00:36:01,050 --> 00:36:04,470
So we're going to first initialize our
tensor flow session and then we're going

580
00:36:04,471 --> 00:36:07,140
to define this variable
called numb actions.

581
00:36:07,141 --> 00:36:10,140
And what numb actions is going to tell us.

582
00:36:10,170 --> 00:36:13,440
Is it going to say how many
possible actions can we take?

583
00:36:13,441 --> 00:36:15,750
What is the possibility for this?

584
00:36:15,780 --> 00:36:19,290
What is the realm of possibility
for this action space?

585
00:36:19,650 --> 00:36:21,030
There's also switch Prob,

586
00:36:21,031 --> 00:36:25,230
which tells us the probability of
switching actions in the next time step,

587
00:36:25,260 --> 00:36:28,380
which we're going to use to
define what it's going to be.

588
00:36:28,381 --> 00:36:32,520
And then we have our last action. And
so this is going to be our first action.

589
00:36:32,521 --> 00:36:33,870
Actually.
I know it's kind of confusing,

590
00:36:33,871 --> 00:36:37,050
but we're going to
initialize the last action,

591
00:36:37,051 --> 00:36:38,730
which is going to be where we start from.

592
00:36:38,731 --> 00:36:40,650
And so we're going to keep
continuously update it.

593
00:36:41,010 --> 00:36:42,870
We're going to initialize it as random

594
00:36:45,490 --> 00:36:46,410
and uh,

595
00:36:50,550 --> 00:36:51,600
and then

596
00:36:53,690 --> 00:36:57,860
well we'll start one more initialized
variable for to not debug.

597
00:36:59,380 --> 00:37:03,620
We, we might want to debug, but we won't.

598
00:37:04,670 --> 00:37:07,070
Okay,
so those are our initial actions.

599
00:37:07,610 --> 00:37:10,520
Now let's define our policy.
Okay,

600
00:37:10,670 --> 00:37:14,750
so define our policy. How do we choose
an action, right? Given the state,

601
00:37:14,870 --> 00:37:17,000
how do we choose our action?
So here's what we're going to do.

602
00:37:17,540 --> 00:37:20,600
Our policy is going to be so simple.
It's going to say,

603
00:37:21,140 --> 00:37:25,310
let's pick some random number out there.
Okay, let's say NP dot. Random dot.

604
00:37:25,311 --> 00:37:29,250
Random APP, great name by the way. Uh,

605
00:37:29,900 --> 00:37:34,100
let's say if that number is less
than our switch probability,

606
00:37:34,101 --> 00:37:38,390
which is also, um, which is what
we're going to define beforehand.

607
00:37:38,900 --> 00:37:41,370
If it's less than that, okay? If it's,

608
00:37:41,390 --> 00:37:45,200
if it's below our threshold
for switching actions,

609
00:37:45,470 --> 00:37:50,470
let's set the last action we just
took to a new random action to take.

610
00:37:54,230 --> 00:37:55,520
And,
and that's,

611
00:37:55,521 --> 00:38:00,470
that action is going to be contained
inside the possibility of actions that we

612
00:38:00,471 --> 00:38:03,590
could be taking.
And so if we go back to return,

613
00:38:03,910 --> 00:38:07,880
so at the end we can return what
that new action is going to be,

614
00:38:08,030 --> 00:38:11,870
which is stored in last action.
Okay. This is our policy.

615
00:38:11,900 --> 00:38:15,590
We're essentially choosing a random action
as long as it fits into our threshold

616
00:38:15,730 --> 00:38:17,780
for whatever we define switch prop to be,

617
00:38:17,910 --> 00:38:21,920
it's going to be a value between zero
and 100 or zero and one a percentage.

618
00:38:24,040 --> 00:38:28,690
That's it. That's our simple
policy. That's our mapping
of um, states to actions.

619
00:38:30,160 --> 00:38:32,970
Okay. So, um, there's that. Um,

620
00:38:33,010 --> 00:38:36,250
and then there's one more thing I need
to define here and the step function

621
00:38:36,490 --> 00:38:39,640
which is going to return that.
In fact, I already have that.

622
00:38:39,910 --> 00:38:41,890
And so once we have that,

623
00:38:42,130 --> 00:38:46,780
then we can talk about what
the class should be, right?

624
00:38:46,781 --> 00:38:51,500
So now that we have let
we make sure that works.

625
00:38:52,130 --> 00:38:56,450
Of course
self dot switch prob

626
00:38:58,340 --> 00:39:02,690
says invalid syntax of course because

627
00:39:05,880 --> 00:39:07,350
right?
Okay.

628
00:39:08,310 --> 00:39:13,240
Self dot switch prob boom.
[inaudible]

629
00:39:15,420 --> 00:39:17,340
wow, that worked great. Okay,

630
00:39:17,341 --> 00:39:22,050
now we've defined that and so now
we can create our basic agents.

631
00:39:22,200 --> 00:39:26,770
We've defined our agents class and now
we can define our, we'll create an agent.

632
00:39:26,771 --> 00:39:30,120
So given our tents are closed session,
given the environment that we're in,

633
00:39:30,360 --> 00:39:35,360
let's return a basic agent object using
the tensorflow session session and we're

634
00:39:36,391 --> 00:39:40,080
going to define the number of actions
we could take by looking at the

635
00:39:40,081 --> 00:39:43,980
environment that we're in, looking at the
action space and returning that number.

636
00:39:44,370 --> 00:39:47,850
And then we're going to define
a switch probability of say 0.2.

637
00:39:48,810 --> 00:39:52,560
And that's going to help us
return a basic agent. Now

638
00:39:54,240 --> 00:39:55,230
once we have that,

639
00:39:55,440 --> 00:39:59,400
we can now create an
agent using that function.

640
00:39:59,550 --> 00:40:04,550
So run experiment is that dopamine native
function to create a runner object.

641
00:40:07,440 --> 00:40:09,900
And so if we look inside the runner object

642
00:40:13,260 --> 00:40:16,890
of,
hold on,

643
00:40:17,580 --> 00:40:19,380
this is actually super exciting to me.

644
00:40:19,381 --> 00:40:23,670
So like I could take forever on this
if we look. So what did we just use?

645
00:40:23,671 --> 00:40:27,090
We use the runner.
What was it?

646
00:40:27,120 --> 00:40:28,890
It was a tare run experiment.

647
00:40:30,420 --> 00:40:34,800
So this is the runner object. And so
if we look inside of here, we can see,

648
00:40:34,830 --> 00:40:39,630
let me hold on, we can see those.

649
00:40:43,500 --> 00:40:44,333
Okay.

650
00:40:45,100 --> 00:40:50,080
Those functions that we defined earlier,
like what were they called?

651
00:40:50,320 --> 00:40:55,030
They were called began episode
began episodes. So right there,

652
00:40:55,150 --> 00:40:55,990
right?
These,

653
00:40:56,020 --> 00:40:59,680
these functions are then called inside of
the runner object and they're utilized.

654
00:40:59,681 --> 00:41:03,340
So that's why I'm saying that there is
a skeleton here that we are following

655
00:41:03,430 --> 00:41:07,750
such that it's going to work for this
specific framework. And um, right.

656
00:41:07,751 --> 00:41:09,550
And so this is, this is,
this is a good thing.

657
00:41:09,880 --> 00:41:11,470
So we're going to create a basic agent.

658
00:41:11,770 --> 00:41:14,540
We're going to define the log path and
now we're going to define a bunch of 'em,

659
00:41:16,720 --> 00:41:17,560
a bunch of variables here.

660
00:41:17,561 --> 00:41:21,100
So the game name is going to
be the game that we defined.

661
00:41:22,030 --> 00:41:26,920
The um, number of
iterations is going to, oh,

662
00:41:26,921 --> 00:41:30,700
by the way, I was running
this tender board. I forgot
about that. Let's start it.

663
00:41:31,930 --> 00:41:34,520
Okay, so what, this, what, okay,

664
00:41:34,521 --> 00:41:38,240
what this is doing is
it's going to visualize,

665
00:41:39,060 --> 00:41:43,160
uh,
some training in the browser.

666
00:41:43,190 --> 00:41:46,220
So trained models in the
browser using tensor board.

667
00:41:46,730 --> 00:41:50,030
Anybody can now run tensorflow board
in the browser. Check this out.

668
00:41:50,031 --> 00:41:53,570
This is real time right here.
We can add in some filters.

669
00:41:53,930 --> 00:41:58,730
I S. Z, I dunno. So just like that,

670
00:41:58,760 --> 00:42:01,310
we've got a bunch of scalars we
could change the smoothing values,

671
00:42:01,490 --> 00:42:03,540
get more smooth, get
less smooth, checkout,

672
00:42:03,590 --> 00:42:08,270
different runs until these are all the
different runs for Rainbow Implicit Quan

673
00:42:08,271 --> 00:42:08,551
tile.

674
00:42:08,551 --> 00:42:13,430
There's agents that I talked about before
and they've even got this great little

675
00:42:13,431 --> 00:42:16,850
link here where we can just see
for any environments that we're in,

676
00:42:16,910 --> 00:42:21,350
I make this bigger.
Let's say it's the game.

677
00:42:21,380 --> 00:42:26,380
Frostbites we can see how each of
those agents performs in the specific

678
00:42:27,111 --> 00:42:29,180
environment that we're in.
And it looks like right here.

679
00:42:29,181 --> 00:42:30,800
Rainbow outperformed the rest.

680
00:42:30,950 --> 00:42:32,840
So just wanted to quickly
show you that by the way,

681
00:42:32,960 --> 00:42:37,400
they have these colabs inside of the um,
framework.

682
00:42:37,610 --> 00:42:41,090
Go to dopamine colab and they've
got several right here. Okay,

683
00:42:42,050 --> 00:42:44,960
so check those out. So, um, where was I?

684
00:42:45,140 --> 00:42:50,030
So the number of iterations are going to
be, let's say 200, uh, the training steps.

685
00:42:50,060 --> 00:42:52,790
And I'm going to answer questions in
a second. Like I said, after training,

686
00:42:53,000 --> 00:42:56,360
I'm going to answer some questions here.
Or evaluation steps are going to be,

687
00:42:56,690 --> 00:42:57,523
let's say 10,

688
00:42:57,740 --> 00:43:02,740
and then the Max steps per
episode are going to be 100.

689
00:43:07,260 --> 00:43:08,093
Okay.

690
00:43:08,360 --> 00:43:11,930
Um, yeah, that's not
gonna work. Of course.

691
00:43:12,620 --> 00:43:16,790
What is the problem here?
Right, I see that. But where,

692
00:43:17,350 --> 00:43:18,183
where's the issue?

693
00:43:24,290 --> 00:43:26,720
No registered environment asterisk.

694
00:43:29,280 --> 00:43:30,113
Really,

695
00:43:35,400 --> 00:43:39,170
really? There's not a game called
asterisk histories. Okay, let's see what,

696
00:43:39,171 --> 00:43:42,480
what happened here? Um, oh,

697
00:43:42,481 --> 00:43:46,710
asterisk within capital a
think that's how it goes down.

698
00:43:52,840 --> 00:43:55,960
Session is not defined.
Okay.

699
00:43:58,350 --> 00:44:03,350
Session is not defined.

700
00:44:06,530 --> 00:44:07,363
Okay?

701
00:44:10,430 --> 00:44:14,420
So I defined it in here. Oh, right.

702
00:44:14,421 --> 00:44:19,310
So in the unit function, I've got to
say, I've got a number of actions,

703
00:44:19,370 --> 00:44:22,190
switch probability,
appoint one

704
00:44:24,700 --> 00:44:29,020
session number of actions
and then

705
00:44:31,110 --> 00:44:31,943
interesting.

706
00:44:32,340 --> 00:44:36,690
Hold on. Oh Shit. Okay.
All right. Let's see here.

707
00:44:40,560 --> 00:44:42,120
I see how everybody's doing by the way.

708
00:44:46,450 --> 00:44:49,930
Oh yes. I only have one ass.
See this is when it's good to,

709
00:44:59,940 --> 00:45:02,160
okay.
Oh right.

710
00:45:06,670 --> 00:45:10,420
Session number of actions with
probability, Dah Dah, Dah, Dah.

711
00:45:10,750 --> 00:45:15,470
What's the issue
where on what line online,

712
00:45:15,490 --> 00:45:17,830
36 sessions not to find,
oh,

713
00:45:17,831 --> 00:45:22,831
you know what it is is I think I
forgot to add session to the um,

714
00:45:29,770 --> 00:45:31,480
oh one s that's what it was.

715
00:45:36,350 --> 00:45:38,060
Object takes no parameters.

716
00:45:42,500 --> 00:45:46,160
The object takes no
parameters. Um, let's see.

717
00:45:46,430 --> 00:45:50,450
Session number of actions
in the environment.

718
00:45:50,451 --> 00:45:53,900
That action space off an
switch probability is too

719
00:45:55,410 --> 00:45:59,850
return basic agent session environment.

720
00:46:00,690 --> 00:46:01,710
Um,

721
00:46:04,270 --> 00:46:06,220
you know what it is is,
um,

722
00:46:12,390 --> 00:46:17,390
I think it's like this function
needs to go inside of the class 32,

723
00:46:18,481 --> 00:46:22,700
right? Line 32. Yup.

724
00:46:24,140 --> 00:46:25,310
Maybe it's like that.

725
00:46:33,540 --> 00:46:35,940
Interesting. Okay. We've got,
we've got much more to do here.

726
00:46:35,941 --> 00:46:39,510
We've got our visualizations,
we've got our plotting.

727
00:46:41,340 --> 00:46:44,310
I've actually,
where's my coat here?

728
00:46:49,690 --> 00:46:51,370
This is how we do it.

729
00:46:57,100 --> 00:47:01,840
Okay. There we go. With all
my beautifully commented code,

730
00:47:01,841 --> 00:47:03,850
by the way.
Definitely check this out.

731
00:47:07,500 --> 00:47:08,333
Okay.

732
00:47:12,030 --> 00:47:16,530
Yes. Good. All right.
Now let's try this thing.

733
00:47:17,130 --> 00:47:20,880
All right. So, uh, the basic
runner that we defined earlier,

734
00:47:20,910 --> 00:47:25,910
we're going to run the experiment and
that's going to train this thing and then

735
00:47:26,491 --> 00:47:29,520
when we're done,
we can say we're done training.

736
00:47:33,320 --> 00:47:36,770
That's simple. It's training. Okay. Let
me answer some questions. Like I said,

737
00:47:36,800 --> 00:47:39,800
Q and a time.
All right.

738
00:47:41,840 --> 00:47:43,730
What are the questions for today?

739
00:47:49,960 --> 00:47:54,340
Okay.
Okay.

740
00:47:54,400 --> 00:47:57,220
There's some delay here between
comments and where I am. You know,

741
00:47:57,221 --> 00:47:59,230
he's like a 15 second delay,
so it'll take a while.

742
00:47:59,620 --> 00:48:01,420
Let's answer some questions here.

743
00:48:05,130 --> 00:48:05,963
Okay.

744
00:48:06,510 --> 00:48:11,070
Neil degrasse Tyson is this Roger Vol
of astrophysics. Thank you though.

745
00:48:12,030 --> 00:48:13,110
Okay.
Um,

746
00:48:17,170 --> 00:48:18,003
okay.

747
00:48:19,520 --> 00:48:23,900
What application do you use for live
sessions, open broadcasting service, obs?

748
00:48:24,410 --> 00:48:27,740
How do you create your own
environment? Great question. Um,

749
00:48:28,280 --> 00:48:32,460
that would be using pie game to
create a game environment. Um,

750
00:48:32,540 --> 00:48:37,100
you can use unity. Unity is great. Unity
has MLA agents, so it's just Google unity,

751
00:48:37,101 --> 00:48:38,000
m l agents.

752
00:48:38,150 --> 00:48:42,680
Great Framework for both mobile games
and desktop games and web applications.

753
00:48:43,070 --> 00:48:47,030
And the third question is do people
use LSTM is on our l problems? Yes.

754
00:48:47,250 --> 00:48:52,250
Long term memory is very important and
they do use it for sequential learning.

755
00:48:53,570 --> 00:48:58,310
Specifically
did Q was a convolutional network,

756
00:48:58,311 --> 00:49:02,990
but an Lstm in the context
of reinforcement learning
would probably be in a text

757
00:49:02,991 --> 00:49:05,990
based environment or um,

758
00:49:09,980 --> 00:49:14,690
yeah, one of those uh, tech
space environment makes
sense. Laptop battery 11%.

759
00:49:14,710 --> 00:49:15,543
Yes.

760
00:49:16,270 --> 00:49:17,103
Uh,

761
00:49:18,130 --> 00:49:19,090
one more question.

762
00:49:27,470 --> 00:49:28,303
Okay.

763
00:49:30,250 --> 00:49:34,780
Is it possible to do our el Que learning
from scratch to learn more accurately?

764
00:49:38,210 --> 00:49:41,450
So, uh, from scratch. Interesting.
Interesting point there.

765
00:49:41,451 --> 00:49:46,040
So Mark Haub decision processes like this
idea is going to follow us throughout

766
00:49:46,041 --> 00:49:46,551
the course.

767
00:49:46,551 --> 00:49:51,551
It's going to follow off throughout all
of reinforcement learning and those mark

768
00:49:51,631 --> 00:49:55,970
Javian variables that I talked about in
the beginning, right? These, the states,

769
00:49:55,971 --> 00:49:58,700
the actions that transition probabilities,
the discount factor,

770
00:49:58,701 --> 00:50:01,160
the reward not,

771
00:50:01,270 --> 00:50:04,790
we're not always going to know what the
transition probabilities are going to be.

772
00:50:04,791 --> 00:50:08,840
We're not always going to know what the
optimal state value function with the

773
00:50:08,841 --> 00:50:11,150
optimal action value function.

774
00:50:11,210 --> 00:50:15,380
We're not going to know always what
they're going to be and that means that

775
00:50:15,381 --> 00:50:19,250
we're going to have a partially
observable mark Haub decision process.

776
00:50:19,430 --> 00:50:21,460
So we have partial Upserve,

777
00:50:21,560 --> 00:50:26,000
we have partial insight
into what's happening and so
we have to learn the rest.

778
00:50:26,210 --> 00:50:27,043
And so all of,

779
00:50:27,140 --> 00:50:31,730
we're starting off with art model based
methods where we know what the model of

780
00:50:31,731 --> 00:50:33,350
the environment fully is going to be,

781
00:50:33,740 --> 00:50:36,560
but sometimes we don't know
what that model is going to be.

782
00:50:36,620 --> 00:50:40,370
So we're going to think we're going to
need to use model free techniques and cue

783
00:50:40,400 --> 00:50:44,810
learning is one of them. And I am not
going to talk about Q learning yet.

784
00:50:44,811 --> 00:50:48,110
I'm specifically avoiding it
right now because I, like I said,

785
00:50:48,560 --> 00:50:52,250
those five concepts you've got to master
before you go into queue learning.

786
00:50:52,390 --> 00:50:54,230
All right? It's like
that meme, like you know,

787
00:50:54,290 --> 00:50:56,990
dive straight into get hub models without
understanding the math that people are

788
00:50:56,991 --> 00:51:00,860
sharing, you know, whatever.
But like you gotta understand.

789
00:51:01,100 --> 00:51:02,720
Let's go over those five things.
Again,

790
00:51:02,721 --> 00:51:05,210
this is the five things you
need to remember from this
live stream. By the way,

791
00:51:07,600 --> 00:51:11,440
the policy,
the state's value function,

792
00:51:11,441 --> 00:51:15,400
the action value function,
the bellman equation,

793
00:51:16,750 --> 00:51:19,450
and the mark of decision process.
Okay.

794
00:51:23,080 --> 00:51:26,500
Okay. So where were
we? We train that baby.

795
00:51:27,550 --> 00:51:29,260
It's done training.
Yup.

796
00:51:31,210 --> 00:51:35,990
And now we can load the
baseline data. Okay. Um,

797
00:51:37,740 --> 00:51:40,000
that's it. That, that, that's,
that's the most important part.

798
00:51:40,001 --> 00:51:42,310
So what we're gonna do is,
um,

799
00:51:45,170 --> 00:51:48,230
where are we at? Time
wise? 51 minutes. Okay.

800
00:51:49,970 --> 00:51:54,230
Uh, this tense or board thing
is pretty cool. But yeah,
I, I like, I like dopamine.

801
00:51:54,680 --> 00:51:56,990
Dopamine is cool. I mean,
clearly it's useful. Look,

802
00:51:57,020 --> 00:52:01,280
if the Google researchers decided that
this was useful for them and they've got

803
00:52:01,660 --> 00:52:04,410
a, you know, all the data, they've
got all the computing power,

804
00:52:04,450 --> 00:52:06,380
they've got some of the best
researchers in the world,

805
00:52:06,560 --> 00:52:09,500
then I think that that's good enough
proof that this is a good framework.

806
00:52:09,501 --> 00:52:13,800
And also just looking at the code and
how it's structured though the, um,

807
00:52:13,910 --> 00:52:16,100
the decisions they made
an architect the code.

808
00:52:16,400 --> 00:52:19,250
I like this framework and I was
skeptical at first, I will admit.

809
00:52:19,280 --> 00:52:22,910
But the more I looked at it and I've
been kind of deepen this for the past two

810
00:52:22,911 --> 00:52:26,750
days, um, the more I
like it. So, um, and I,

811
00:52:26,751 --> 00:52:30,020
and I really liked this idea of just
giving us this link right here of like

812
00:52:30,290 --> 00:52:33,980
really quickly just checking
for any of these environments,

813
00:52:33,981 --> 00:52:37,370
how each of these perform.
Not even that it's super necessary,

814
00:52:37,371 --> 00:52:41,930
but just the fact that they did that.
Google has really been helping, um,

815
00:52:42,080 --> 00:52:47,000
the greater amateur AI research
community lately with the Dataset search,

816
00:52:47,250 --> 00:52:52,040
uh, with, um, colab. So, uh, yeah,
we're all very appreciative. Google,

817
00:52:52,041 --> 00:52:55,670
if you're watching in, in, in this school
of Ai Community, we're very appreciative.

818
00:52:55,910 --> 00:53:00,070
Thank you. So, um, yeah, uh,

819
00:53:00,130 --> 00:53:04,590
let's load the baseline data.
You know what I'm going to do?

820
00:53:04,591 --> 00:53:06,570
I'm just going to do this. Here's
what, here's what we're going to do.

821
00:53:06,660 --> 00:53:09,690
We're going to go straight into the,
just so you know, we'll do this together.

822
00:53:09,691 --> 00:53:14,150
We'll go straight to dopamine, will
go to dopamine, will go to Colab,

823
00:53:14,540 --> 00:53:16,020
we'll go to,

824
00:53:19,430 --> 00:53:20,290
uh,

825
00:53:21,150 --> 00:53:22,650
this colab lab that they have right here

826
00:53:25,620 --> 00:53:26,640
and

827
00:53:31,880 --> 00:53:33,320
these packages,

828
00:53:34,760 --> 00:53:35,593
okay,

829
00:53:37,910 --> 00:53:42,530
we can install just like this.
And then we can visualize them.

830
00:53:47,770 --> 00:53:52,710
Okay. Uh, let me answer some questions
as well. While this is running,

831
00:53:56,020 --> 00:54:00,640
dopamine is a happy hormone hormone.
Is it a hormone? Dopamine is more of,

832
00:54:00,670 --> 00:54:04,450
I guess you could classify it as a
neuro transmitter along with serotonin.

833
00:54:04,451 --> 00:54:05,310
And uh,

834
00:54:06,990 --> 00:54:11,940
that's another one that's
for anxiety. I forgot. Uh,

835
00:54:13,970 --> 00:54:18,380
can you, can you use dopamine with
real life or El Problems? Yes, you can.

836
00:54:18,860 --> 00:54:21,020
Can you summarize what we've
accomplished so far? So,

837
00:54:21,021 --> 00:54:23,900
so far we have trained and RL agent

838
00:54:26,340 --> 00:54:29,280
using a random policy in the asterisk,

839
00:54:29,550 --> 00:54:32,670
asterix environment. Uh, it,

840
00:54:32,820 --> 00:54:37,240
we've trained it to learn an optimal
policy by choosing actions randomly.

841
00:54:37,750 --> 00:54:41,220
Right? It's a very simple agent, but
we don't have to create our own agent.

842
00:54:41,221 --> 00:54:45,420
What we could do is just a
subclass and existing agent.

843
00:54:46,020 --> 00:54:48,480
And uh,
once we subclass and existing agent,

844
00:54:48,481 --> 00:54:53,481
we can extend it like what rainbow did
or what the implicit Quan title agent

845
00:54:53,881 --> 00:54:58,740
did. They just extended deep too. And
that's really the value that that brings.

846
00:55:00,650 --> 00:55:01,483
Yeah.

847
00:55:01,890 --> 00:55:04,980
As you can tell, I'm an ra, I'm really
trying to avoid coding out this last bit.

848
00:55:05,280 --> 00:55:06,870
I just think that it's already out there.

849
00:55:06,871 --> 00:55:11,700
We can just like compile it right now
if these dependencies take their time.

850
00:55:11,701 --> 00:55:16,400
But I think in terms of a visual element,
um,

851
00:55:17,550 --> 00:55:22,320
wow. It's already on. It's already loading
the baseline data. Pretty cool. Okay. Um,

852
00:55:22,740 --> 00:55:23,310
right.

853
00:55:23,310 --> 00:55:27,870
We've got to import our utils are
globals the game and then we can load the

854
00:55:27,871 --> 00:55:28,830
baseline data.

855
00:55:30,600 --> 00:55:34,350
Let me keep answering some
questions here if I have that up.

856
00:55:35,780 --> 00:55:40,500
All right. You know what? I'm going to
end this stream with a rap. Um, but yeah,

857
00:55:40,740 --> 00:55:44,760
visuals, visualization tools are
here. I'm not going to wait forever.

858
00:55:44,761 --> 00:55:48,330
Waste your time with like loading the
stuff. Um, they've got a bunch of colab.

859
00:55:48,360 --> 00:55:51,990
You've got my co lab. My lesson is
available in the video description.

860
00:55:52,510 --> 00:55:56,250
Please subscribe if you haven't yet. I'm
going to continuously be posting, uh,

861
00:55:56,310 --> 00:55:58,470
machine learning, deep learning,
reinforcement learning,

862
00:55:58,950 --> 00:56:02,790
AI content on this channel. Um,
tell all your friends to subscribe.

863
00:56:02,880 --> 00:56:06,300
We're almost at 500 k subscribers.
I cannot wait. I can taste it.

864
00:56:06,570 --> 00:56:11,130
I can taste 500 K it's so close. And
I'm going to end this with a, with a,

865
00:56:11,470 --> 00:56:15,900
a wrap. So just say a topic.
I'm going to wrap that topic.

866
00:56:16,830 --> 00:56:21,230
All right.
Which will,

867
00:56:21,231 --> 00:56:23,710
when Arele versus supervisor,
that's a great. Um,

868
00:56:24,550 --> 00:56:26,890
there is no winner. They're both going
to win together. They're, they're,

869
00:56:26,891 --> 00:56:29,320
they're both techniques are all,
they're both part of a larger,

870
00:56:29,590 --> 00:56:32,380
whole of a learning
agent in an environment.

871
00:56:35,360 --> 00:56:36,193
Right.

872
00:56:38,470 --> 00:56:39,910
Okay.
Now,

873
00:56:40,510 --> 00:56:41,343
yeah,

874
00:56:45,160 --> 00:56:50,100
it's in the various,
yeah.

875
00:56:50,110 --> 00:56:50,943
Okay.

876
00:56:55,900 --> 00:57:00,070
Kind of love math. I mean, I do
it every day. I look at equations,

877
00:57:00,071 --> 00:57:04,440
I look at everything, man. It's
okay. Listen, I was looking at RL.

878
00:57:04,600 --> 00:57:08,530
I was looking at policies. I was
looking at all these things. Men,

879
00:57:08,531 --> 00:57:11,320
it just got to me. I had
to stop. I had to think,

880
00:57:11,650 --> 00:57:15,490
what are these value functions really
doing? When I was looking at them, man,

881
00:57:15,520 --> 00:57:19,390
I was like, I was back in school
and I was writing on a chalkboard.

882
00:57:19,391 --> 00:57:21,040
I was trying to code it out.

883
00:57:21,130 --> 00:57:24,880
Look at how policies and values
relate to each other. Without a doubt.

884
00:57:25,120 --> 00:57:28,120
I was trying to see what
the real meaning of it was,

885
00:57:28,180 --> 00:57:31,810
how it all connected to each other,
how it was all a part of me,

886
00:57:32,020 --> 00:57:36,340
like my cousin in it was like my
family member. Math is like my family.

887
00:57:36,341 --> 00:57:41,170
And just like you guys, man, we do it
right. We do it actual liquor. Okay.

888
00:57:41,350 --> 00:57:44,980
That's it. See, I always end
on a crazy note like that,

889
00:57:45,370 --> 00:57:50,220
but thank you guys so much for watching
more live streams every single week. Um,

890
00:57:50,240 --> 00:57:54,640
I'm in it for you and yes,
I've got 6% laptop battery. Uh,

891
00:57:54,641 --> 00:57:58,450
and for now I've got to do a
bunch of school of AI work,

892
00:57:58,510 --> 00:57:59,680
so thanks for watching.

893
00:58:00,330 --> 00:58:00,750
All right.

