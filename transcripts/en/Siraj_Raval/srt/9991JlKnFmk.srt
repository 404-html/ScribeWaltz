1
00:00:00,060 --> 00:00:00,751
Hello world,

2
00:00:00,751 --> 00:00:05,370
it's to Raj and we're going to detect
who the intruder is in our security

3
00:00:05,371 --> 00:00:06,060
system.

4
00:00:06,060 --> 00:00:10,560
So say we've got a website and we're
monitoring all of this network data and

5
00:00:10,561 --> 00:00:12,540
there's one guy,
one bad dude,

6
00:00:12,541 --> 00:00:16,950
one bad own Bray who's trying to
break into our security system.

7
00:00:17,190 --> 00:00:19,770
We want to find who this guy is.

8
00:00:19,771 --> 00:00:23,850
And the way we're going to do this is
by implementing a machine learning model

9
00:00:24,090 --> 00:00:28,290
called k means clustering.
And I've never implemented this before,

10
00:00:28,320 --> 00:00:31,560
so I'm really excited and it falls right
in line with the rest of the stuff I've

11
00:00:31,561 --> 00:00:35,160
been talking about in this series.
So let's get started. First,

12
00:00:35,161 --> 00:00:38,940
I've got this little image to show you
what the algorithm looks like. This,

13
00:00:38,941 --> 00:00:43,350
it's a five step process and this gift
kind of shows what that looks like.

14
00:00:43,560 --> 00:00:48,330
We start off with data that has no labels,
it's just a cluster of unlabeled data.

15
00:00:48,330 --> 00:00:51,960
And we don't really know anything
about the data other than the features.

16
00:00:51,961 --> 00:00:54,330
We don't know what classes
the data belongs to.

17
00:00:54,540 --> 00:00:55,920
That's what we're trying to learn.

18
00:00:56,370 --> 00:01:00,690
And what the algorithm will do is
it will iteratively take this data,

19
00:01:00,691 --> 00:01:04,800
this unlabeled data, and it will
create clusters from this data.

20
00:01:05,100 --> 00:01:08,220
And we don't know what,
where these clusters are going to be.

21
00:01:08,280 --> 00:01:12,990
The algorithm is going to help learn
where those clusters should be. Okay.

22
00:01:12,991 --> 00:01:15,000
So,
and I'll talk about what these steps are.

23
00:01:15,860 --> 00:01:18,840
Well let me define what this term is.
K means clustering.

24
00:01:19,110 --> 00:01:22,140
This is one of the most popular
techniques in machine learning.

25
00:01:22,141 --> 00:01:26,870
You see it all the time in Kaggle contests
and the machine learning sub reddit

26
00:01:27,120 --> 00:01:31,680
everywhere. It's a very popular algorithm
and it's very easy, uh, more or less,

27
00:01:31,681 --> 00:01:34,260
I mean more than other things
that I've been talking about.

28
00:01:34,261 --> 00:01:37,830
So that's a good thing. But let's
talk about what we've learned so far.

29
00:01:37,831 --> 00:01:42,030
What we've learned is that machine
learning is all about optimizing for an

30
00:01:42,031 --> 00:01:45,150
objective, right? We are trying to
optimize for an objective that's,

31
00:01:45,390 --> 00:01:46,800
that's the goal of machine learning.

32
00:01:47,130 --> 00:01:50,700
And we've learned about first order
and second order optimization,

33
00:01:50,910 --> 00:01:54,750
what's first order gradient
descent and its variance, right?

34
00:01:54,870 --> 00:01:55,740
Where we are trying to,

35
00:01:55,741 --> 00:02:00,040
if we were to graph the error of a
function versus it's weight values,

36
00:02:00,060 --> 00:02:04,320
we want to find the minimum
of the function so that we
can find the ideal weight

37
00:02:04,321 --> 00:02:08,340
value so that our error is minimized.
And to do that with grading dissent,

38
00:02:08,400 --> 00:02:10,530
we compute the first derivatives,
right?

39
00:02:10,830 --> 00:02:13,800
The partial derivatives with respect
to our weights using our error.

40
00:02:14,250 --> 00:02:15,810
But for second order optimization,

41
00:02:15,811 --> 00:02:19,470
we do the same thing except we compute
the second derivative that is the

42
00:02:19,471 --> 00:02:23,520
derivative of the derivative.
And there's pros and cons to both.

43
00:02:23,760 --> 00:02:26,370
And we talked about when you would use
one over the other and the previous

44
00:02:26,371 --> 00:02:30,330
videos. But what happens, here's
the, here's the big question.

45
00:02:30,720 --> 00:02:33,570
What happens if you don't have the label?

46
00:02:33,720 --> 00:02:36,250
How are you supposed to
compute the air? Right? It's,

47
00:02:36,280 --> 00:02:39,300
it's usually the predicted
label minus or sorry,

48
00:02:39,301 --> 00:02:41,880
the actual label minus
the predicted label.

49
00:02:42,030 --> 00:02:45,780
That's the air use it to compute the
partial derivatives with respect to each

50
00:02:45,781 --> 00:02:48,300
weight value.
But if you don't have the label,

51
00:02:48,301 --> 00:02:50,010
how are you supposed to compute the air?

52
00:02:50,250 --> 00:02:54,780
And that's where unsupervised learning
comes into play. Specifically.

53
00:02:54,781 --> 00:02:58,830
K means clustering. So I've got this
diagram here to show the differences here.

54
00:02:59,860 --> 00:03:02,560
So there are two outcomes that
we could possibly want, right?

55
00:03:02,830 --> 00:03:07,830
Either a discrete outcome that is some
contained outcome like red or blue or

56
00:03:08,200 --> 00:03:13,200
blue or black or white or up or down
or not guilty or guilty right there.

57
00:03:13,931 --> 00:03:17,620
These containerized outcomes,
binary outcomes,

58
00:03:18,510 --> 00:03:20,980
not just binary cause it could
be more than zero and one,

59
00:03:20,981 --> 00:03:24,910
it could be multi-class,
but outcomes that are containerized into

60
00:03:26,370 --> 00:03:26,870
okay,

61
00:03:26,870 --> 00:03:28,010
specific labels.

62
00:03:28,280 --> 00:03:33,280
Whereas continuous outcomes are like time
series data where it could be a value

63
00:03:33,561 --> 00:03:38,561
between two and three or between two
and 2.5 or between two and 2.25 and it

64
00:03:39,981 --> 00:03:44,270
could just go infinity in that direction
of that, of that numerical interval.

65
00:03:44,271 --> 00:03:47,090
Right?
So with supervised learning,

66
00:03:47,091 --> 00:03:50,720
we learn how to do predict a continuous
outcome using linear regression.

67
00:03:50,721 --> 00:03:51,890
That was our first video.

68
00:03:52,190 --> 00:03:56,270
And then the next thing we learned
was how to predict a discrete outcome.

69
00:03:56,271 --> 00:03:59,840
And we use logistic regression for that,
and that's when we have labels.

70
00:03:59,990 --> 00:04:01,700
But if we don't have labels,

71
00:04:01,970 --> 00:04:06,590
then we use clustering to
predict a discrete outcome.
That's what we're gonna do.

72
00:04:06,591 --> 00:04:08,840
We're gonna predict a discrete outcome.

73
00:04:10,100 --> 00:04:14,960
And by defining these classes for people,
it's these discrete classes.

74
00:04:15,290 --> 00:04:17,750
And then we're going to find the
anomaly that is the intruder.

75
00:04:18,260 --> 00:04:20,680
And so then for a continuous outcome,
you,

76
00:04:20,750 --> 00:04:23,930
you'll want to perform dimentionality
reduction. And that's next week.

77
00:04:23,931 --> 00:04:25,880
So we're not, we're not
there yet, but we'll talk.

78
00:04:25,910 --> 00:04:29,240
We talked about this one and this
one and now we're going to talk about

79
00:04:29,241 --> 00:04:31,280
clustering.
So let's keep going here.

80
00:04:31,970 --> 00:04:35,720
So on supervised versus
unsupervised learning, what
are the pros and cons? Well,

81
00:04:35,721 --> 00:04:38,840
for supervised learning it's more
accurate. I mean, you've got the labels,

82
00:04:38,841 --> 00:04:41,150
of course, it's more
accurate, right? It's like,

83
00:04:41,151 --> 00:04:42,980
it's like having training
wheels on your bike,

84
00:04:44,270 --> 00:04:48,410
but you have to have a human who labels
this data or it's just labeled itself

85
00:04:48,411 --> 00:04:52,280
somehow. But unsupervised learning
is more convenient cause you,

86
00:04:52,580 --> 00:04:54,020
cause most data is unlabeled,
right?

87
00:04:54,021 --> 00:04:58,370
You don't just have this neatly labeled
data like, oh this is this or it.

88
00:04:58,700 --> 00:05:01,910
No, data is messy, the world
is messy, life is messy.

89
00:05:02,210 --> 00:05:07,070
So that's what we want, ideally to
run our algorithms unsupervised.

90
00:05:07,280 --> 00:05:12,170
But the problem is that these algorithms
are usually less accurate. Right?

91
00:05:12,171 --> 00:05:15,750
But it requires minimum human effort cause
no one's got a label. These, these, uh,

92
00:05:15,830 --> 00:05:18,080
data points by hand.
Okay.

93
00:05:18,081 --> 00:05:21,500
So let's talk about how this algorithm
works mathematically and then we'll get

94
00:05:21,501 --> 00:05:22,430
right into the code.
Okay?

95
00:05:22,431 --> 00:05:25,340
And so what I'm gonna do is I'm going
to glaze over most of the code and I'll

96
00:05:25,341 --> 00:05:28,460
write a few of the most
important bits. Okay? So

97
00:05:29,960 --> 00:05:33,950
how does this work? So we've got a set
of points, right? So a set of points x.

98
00:05:34,190 --> 00:05:38,510
And so what I'm gonna do is I'm going to
take this and I'm gonna make two copies

99
00:05:38,511 --> 00:05:42,670
of it so we can see this Gif
while I talk about the algorithm.

100
00:05:42,671 --> 00:05:43,730
So let me do this,

101
00:05:44,030 --> 00:05:49,030
hold on just like that and put
that here and then put this here.

102
00:05:49,580 --> 00:05:50,413
Okay.
So,

103
00:05:53,650 --> 00:05:54,130
okay,

104
00:05:54,130 --> 00:05:59,000
so the way this algorithm works
is

105
00:06:00,560 --> 00:06:03,530
we've got a set of points, right?
I set of points x and then,

106
00:06:03,590 --> 00:06:05,000
and then we define a value.

107
00:06:05,001 --> 00:06:09,020
Kay and Kay is the number of
clusters that we want, right?

108
00:06:09,021 --> 00:06:13,550
K Means Algorithm. That's where k comes
from. So we've got a set of points x,

109
00:06:13,551 --> 00:06:16,610
all of our data points,
and we've got k that's our input.

110
00:06:16,640 --> 00:06:18,020
And so we're going to say two,

111
00:06:18,021 --> 00:06:21,650
let's just say two for now and we'll
talk about how to realize what the best k

112
00:06:21,651 --> 00:06:26,030
value is. But let's just say two right
now. And so then once we have that,

113
00:06:26,120 --> 00:06:30,080
we're going to place a set of
centroids at random locations.

114
00:06:30,200 --> 00:06:35,180
How many centroidsK centroids.
So if we chose to for k,

115
00:06:35,420 --> 00:06:39,290
then the centroids are just data points
that are just randomly plotted on the

116
00:06:39,291 --> 00:06:39,831
graph.
Okay.

117
00:06:39,831 --> 00:06:43,940
These are called centroids
because eventually they're
going to be the center of

118
00:06:43,970 --> 00:06:48,140
each cluster that we learn. So the
center, so the centroid points are there,

119
00:06:48,141 --> 00:06:51,290
k of them, and we just plot them
randomly. Okay. So we definedK ,

120
00:06:51,920 --> 00:06:55,070
we have our set of data points and
then our set of centroids k of them,

121
00:06:55,071 --> 00:06:59,930
and we just plot them randomly. Okay,
now what? Now here's the steps we do,

122
00:06:59,960 --> 00:07:02,060
and we repeat them until convergence,

123
00:07:02,240 --> 00:07:04,880
which is what we predefined
beforehand with threshold value.

124
00:07:05,840 --> 00:07:09,320
So what we do is we say for each
point in the data set, so if,

125
00:07:09,321 --> 00:07:12,620
let's say for we have 40 data points,
so for each point,

126
00:07:12,650 --> 00:07:15,440
let's say for one of them we're
going to find the nearest centroid.

127
00:07:15,920 --> 00:07:16,880
How do we do that?
Well,

128
00:07:16,881 --> 00:07:21,881
we compute the distance between that
data point and and each of the central

129
00:07:22,041 --> 00:07:25,100
points. So there's two, in our case
there's going to compute the distance,

130
00:07:25,130 --> 00:07:30,130
the Euclidean distance between that point
and both of the centroid and isn't to

131
00:07:30,341 --> 00:07:32,150
find the one that's closest to it.

132
00:07:32,150 --> 00:07:34,010
And that's where this Arg
Min function comes in.

133
00:07:34,130 --> 00:07:38,030
What is the minimum value
in this set of values?

134
00:07:38,150 --> 00:07:41,960
So we're going to find the
shortest distance and that's
going to be our cluster.

135
00:07:41,990 --> 00:07:45,050
So we're going to assign that
data point to the, to the cluster,

136
00:07:45,051 --> 00:07:50,051
Jay or Jay is for the centroid that
is closest to that data points.

137
00:07:50,450 --> 00:07:50,721
Okay?

138
00:07:50,721 --> 00:07:55,220
So then what happens is we've got a set
of clusters now and we do this for every

139
00:07:55,221 --> 00:07:56,054
single data point.

140
00:07:56,270 --> 00:08:01,250
So every single data point will belong
to a cluster and that cluster will be

141
00:08:01,251 --> 00:08:04,760
defined as a centroid point. That
is closest to that data point. Okay,

142
00:08:04,940 --> 00:08:07,910
so that's the initial cluster
that's going to be defined.

143
00:08:08,210 --> 00:08:10,790
Then for each of those clusters,
Jay,

144
00:08:11,180 --> 00:08:13,760
we're going to take all of those
data points in that cluster.

145
00:08:13,761 --> 00:08:16,730
We're going to add them all up and
then divide by the number of them.

146
00:08:16,730 --> 00:08:19,820
And what is this called? It's called
the mean, right? Or the average.

147
00:08:20,180 --> 00:08:23,450
So now you're getting to see where this
name comes from. Right? K means, right.

148
00:08:23,451 --> 00:08:25,070
It all makes sense. I was
like, God, I love that. Okay,

149
00:08:25,550 --> 00:08:30,550
so k means is the name of the algorithm
and so we find the mean point or the

150
00:08:31,221 --> 00:08:35,660
mean value for all the points in that
Dataset and that mean is going to be

151
00:08:35,720 --> 00:08:38,810
become our next centroid point.
Okay.

152
00:08:39,050 --> 00:08:41,630
So we've defined centroids,
we've,

153
00:08:41,660 --> 00:08:46,660
we've grouped our data points into each
of these centroids and then we will then

154
00:08:48,260 --> 00:08:50,630
find the mean of all those values.
And that will be,

155
00:08:50,690 --> 00:08:54,110
those values will be our new centroid.
So in our case there will be two, right?

156
00:08:54,111 --> 00:08:58,260
So there'll be two new centroids that
we then plot and then we just keep

157
00:08:58,261 --> 00:08:59,220
repeating the process.

158
00:08:59,221 --> 00:09:03,900
So we go back to for each point acts
and for those new centroids find the

159
00:09:03,901 --> 00:09:07,530
distance for all the closest data points
and it's going to be a new cluster.

160
00:09:07,530 --> 00:09:10,290
And so that's what you're seeing here.
It's going to be a new cluster.

161
00:09:10,410 --> 00:09:14,310
And then we just kept keep
repeating that process until when,

162
00:09:14,400 --> 00:09:17,580
until none of the cluster assignments
change and then we're good.

163
00:09:19,020 --> 00:09:23,940
So right. So that's kind
of how that works. And so,

164
00:09:24,000 --> 00:09:25,890
uh,
right.

165
00:09:25,891 --> 00:09:29,240
So we can also terminate the program
when it reaches an iteration budgets and

166
00:09:29,241 --> 00:09:32,850
we'll say after, you know, x number of
iterations, just stop running. Right.

167
00:09:32,851 --> 00:09:37,851
So then one of one great question that
I hope you're asking is how do we know

168
00:09:38,251 --> 00:09:40,710
what value forK we should use?
Well,

169
00:09:40,711 --> 00:09:45,711
it's very simple if we know what classes
we want to classify or how many classes

170
00:09:46,470 --> 00:09:48,510
would you say that that's the value for k.

171
00:09:48,511 --> 00:09:52,590
So if we know that we want
to classify people as either

172
00:09:54,160 --> 00:09:58,260
a, from a certain region
or from a certain place,

173
00:09:58,410 --> 00:10:01,680
then we'll just say like of these
three places, you know, Spain, Mexico,

174
00:10:01,681 --> 00:10:04,830
and I don't know, Argentina,
random the countries,

175
00:10:05,010 --> 00:10:06,420
well I guess those are on my mind.

176
00:10:06,750 --> 00:10:10,200
Then we know that k should be three
because we have three countries that we're

177
00:10:10,201 --> 00:10:14,070
targeting. But if we don't
know how many classes we want,

178
00:10:14,310 --> 00:10:16,740
these are just unknown unknowns.

179
00:10:17,040 --> 00:10:20,640
Then we will have to decide
what the best case value is.

180
00:10:20,970 --> 00:10:23,250
And that can be a guess and check method,

181
00:10:23,340 --> 00:10:26,880
but it's actually a smarter way to do
that. And it's called the elbow method.

182
00:10:26,910 --> 00:10:30,750
So the elbow method is a very
popular method. Well you have to do,

183
00:10:30,820 --> 00:10:32,640
you have to rub your elbow
and I'm just kidding.

184
00:10:32,880 --> 00:10:36,600
So what happens is you've got this graph
here, okay. And it looks like an elbow,

185
00:10:36,601 --> 00:10:39,930
right? It's got this elbow point
right here. So what you do,

186
00:10:39,960 --> 00:10:43,380
and I've got this part in Javascript to
show you what this algorithm looks like.

187
00:10:44,430 --> 00:10:49,410
I know Javascript, right? So here's what
it looks like. We perform. K means once.

188
00:10:49,440 --> 00:10:50,273
So we,

189
00:10:50,370 --> 00:10:54,210
so we define a set of key values between
let's say between one and 10 okay?

190
00:10:54,210 --> 00:10:55,380
So that's a good starting point.

191
00:10:55,620 --> 00:10:58,320
K could be either one cluster
or it could be 10 clusters.

192
00:10:58,710 --> 00:11:01,110
Let's try k means for all 10 of those.

193
00:11:01,440 --> 00:11:05,940
And so what we do is we perform k
means for all of those key values.

194
00:11:06,150 --> 00:11:07,780
And then so that's what this is.
So for,

195
00:11:07,860 --> 00:11:12,780
for as many k values as we have between
zero and 10 let's perform k means to

196
00:11:12,781 --> 00:11:16,110
find all those clusters.
And then for each of the clusters,

197
00:11:16,200 --> 00:11:20,700
let's find the mean value. And what is
the mean value? It is the error value.

198
00:11:20,701 --> 00:11:24,180
So what is the distance between
the centroid and all of its points?

199
00:11:24,660 --> 00:11:27,060
And that's going to be
the mean or the error.

200
00:11:27,180 --> 00:11:31,500
And what we want to do is
we want to compute the sum
of the squared error values.

201
00:11:31,620 --> 00:11:34,650
And so that's what this line is right
here, the sum of the squared hairs.

202
00:11:35,040 --> 00:11:36,510
And so we say math dot pal,

203
00:11:36,511 --> 00:11:40,170
which means square the data point
for each data point minus the mean.

204
00:11:40,560 --> 00:11:44,540
So for every data point in ours,
in our Dataset,

205
00:11:44,640 --> 00:11:47,610
we're going to take a data point
minus the mean value and square it.

206
00:11:47,940 --> 00:11:51,600
And so what that's gonna do is if we were
to graph that for all of those number

207
00:11:51,601 --> 00:11:53,860
of cluster values for all those k values,

208
00:11:54,040 --> 00:11:58,420
we'll see that the sum of the squared
errors for each of those iterations of k

209
00:11:58,421 --> 00:12:00,670
means makes this elbow like graph.

210
00:12:00,880 --> 00:12:05,470
And what we want to do is we want to
pick the k value that is right at the

211
00:12:05,471 --> 00:12:07,480
elbow,
right at that tipping point right here.

212
00:12:07,600 --> 00:12:09,940
So it would be six in
the case of this graph.

213
00:12:10,240 --> 00:12:14,140
And that is our optimal k value.
Okay.

214
00:12:14,141 --> 00:12:17,230
Because after that there's very
diminishing returns. As you can see,

215
00:12:17,350 --> 00:12:19,150
we want to find the minimal error value.

216
00:12:19,150 --> 00:12:24,000
And we've found that for this k
value of six, the error is uh, at,

217
00:12:24,010 --> 00:12:25,450
it's not at its smallest,

218
00:12:25,451 --> 00:12:28,930
but at the point where it's everything
after that is just diminishing returns.

219
00:12:29,110 --> 00:12:33,640
And we could say 10 or 12 or 14 but
then for computational efficiency sake,

220
00:12:33,790 --> 00:12:37,030
we could just say six. So we don't
have to run that many iterations.

221
00:12:37,180 --> 00:12:41,800
So we'll just say six. Okay, so that's
the elbow method. All right, so,

222
00:12:41,801 --> 00:12:45,820
uh, three more points and then we're
gonna get started with the code.

223
00:12:46,030 --> 00:12:49,570
So how is it, this is computer between
the centroids in the data points,

224
00:12:49,810 --> 00:12:51,940
the Euclidean distance,
right?

225
00:12:52,030 --> 00:12:55,780
We've talked about the Euclidean
distance for linear regression, right?

226
00:12:56,140 --> 00:12:59,500
Euclidean distance.
It's a very simple distance formula.

227
00:12:59,650 --> 00:13:02,590
You just take each of the data
points and then you say, uh,

228
00:13:03,040 --> 00:13:08,040
x one minus x two plus c squared
plus y one minus y two squared.

229
00:13:08,380 --> 00:13:09,400
And then if you have z values,

230
00:13:09,430 --> 00:13:12,190
Z one minus z two squared and
the square root of all of those.

231
00:13:12,370 --> 00:13:13,600
And that's the Euclidean distance.

232
00:13:13,601 --> 00:13:17,620
And what that does is it takes two data
points and it gives us a scalar value.

233
00:13:17,621 --> 00:13:20,620
And that is the distance
between all those data points.

234
00:13:20,860 --> 00:13:25,860
So you can apply the Euclidean distance
to a multidimensional dataset where you

235
00:13:27,161 --> 00:13:28,930
have, you know, any number of dimensions.

236
00:13:28,931 --> 00:13:31,810
You just subtract the values
for each of those dimensions,

237
00:13:31,811 --> 00:13:36,100
for each of those data points, uh, the
square, and then you find the square root.

238
00:13:36,130 --> 00:13:38,170
Anyway, I'll show you the equation
when we get to it. But yeah,

239
00:13:38,171 --> 00:13:42,280
the Euclidean distance and to answer
any questions about why the Euclidean

240
00:13:42,281 --> 00:13:44,530
distance as opposed to
other distance metrics.

241
00:13:44,860 --> 00:13:49,860
The answer is because k means technically
minimizes the within cluster variance.

242
00:13:52,240 --> 00:13:54,250
And I know we haven't talked
about this term yet and we will,

243
00:13:54,251 --> 00:13:56,740
but I'm just trying to
be very detailed here.

244
00:13:56,980 --> 00:13:58,660
And if you look at a
definition of variance,

245
00:13:58,661 --> 00:14:01,360
which I'll define more in detail later,
it is a,

246
00:14:01,420 --> 00:14:06,100
it is identical to the sum of the squared
Euclidean distances from the center.

247
00:14:06,101 --> 00:14:10,270
So in Euclidean space and because it
is identical to the sum of Euclidean

248
00:14:10,290 --> 00:14:11,123
distances,

249
00:14:11,230 --> 00:14:15,040
we use the Euclidean distance as our
distance metric as opposed to something

250
00:14:15,041 --> 00:14:17,350
else like the Manhattan distance.
Okay.

251
00:14:17,380 --> 00:14:21,970
So lastly or two more actually,
when should you use this?

252
00:14:22,060 --> 00:14:26,080
If your data is numeric, and that means
if your features are numeric, right,

253
00:14:26,081 --> 00:14:30,940
you have numbers for your features.
If you have a categorical feature like uh,

254
00:14:31,060 --> 00:14:34,870
the shoe color, uh, or

255
00:14:36,430 --> 00:14:40,600
true or false, a boolean value, you
can't really map that in Euclidean space,

256
00:14:40,601 --> 00:14:44,470
right? These are, these are categorical
features. So, but if your data is numeric,

257
00:14:44,700 --> 00:14:46,750
k means is your, is
your, is your algorithm.

258
00:14:46,870 --> 00:14:48,640
It's also the simplest
algorithm you'll see.

259
00:14:48,640 --> 00:14:51,960
It's actually a very simple algorithm.
And when we talked about the pseudo code,

260
00:14:52,190 --> 00:14:53,780
it's a relatively simple algorithm.

261
00:14:53,960 --> 00:14:58,880
And the advantage it has over other
techniques is that it is fast.

262
00:14:58,940 --> 00:15:01,920
That's the real key value. It's
simple and it's fast. You know,

263
00:15:01,921 --> 00:15:06,230
a quick and dirty clustering
algorithm. It's great for that. Okay.

264
00:15:07,820 --> 00:15:09,890
And it really shines when
you have multivariate data.

265
00:15:10,280 --> 00:15:13,850
So that is more than one
dimension. Okay. So lastly,

266
00:15:13,851 --> 00:15:15,320
two other examples of God here.

267
00:15:15,321 --> 00:15:19,190
One for fraud detection and then
one for m and ist without labels.

268
00:15:19,191 --> 00:15:23,690
I know what that is possible and eyes
he without labels. Yes, it's possible.

269
00:15:23,720 --> 00:15:27,350
Anything is possible.
So anything is really possible here.

270
00:15:27,351 --> 00:15:32,040
So for credit card fraud detection and
for finding these labels for these, uh,

271
00:15:32,240 --> 00:15:35,540
for these MSI is t images where let's
just say we don't know the labels.

272
00:15:35,541 --> 00:15:36,890
Usually we know the labels.
Well,

273
00:15:36,891 --> 00:15:40,280
let's just say we don't know the labels
and if we don't know the labels and it's

274
00:15:40,281 --> 00:15:42,530
going to cluster the
images into what their,

275
00:15:43,190 --> 00:15:45,170
what their respective clusters should be.

276
00:15:45,290 --> 00:15:49,370
And those are clusters for ones and
twos and threes in terms of the image,

277
00:15:49,760 --> 00:15:53,120
the images. Okay, so check
those other two examples out.

278
00:15:53,121 --> 00:15:57,740
And now let's get into the
code. Okay, so I'm moving here.

279
00:15:57,741 --> 00:15:58,461
I'm moving.
Okay,

280
00:15:58,461 --> 00:16:02,750
so the first thing we want to do is we're
want to import num Pi for matrix math

281
00:16:02,780 --> 00:16:05,960
map plot line to map plot
live to plot out our graph.

282
00:16:06,230 --> 00:16:10,160
And then the animation module of
plotline cause we're going to graph,

283
00:16:10,400 --> 00:16:13,100
we're going to animate some
graphs in a second. Okay.

284
00:16:13,101 --> 00:16:17,810
So then let's take a look at our data
set. Okay. So where is our datasets here?

285
00:16:17,840 --> 00:16:21,530
Our dataset is as what our
data set looks like. Okay.

286
00:16:21,680 --> 00:16:23,150
So we've got two dimensions.

287
00:16:23,151 --> 00:16:27,170
We've got two features in our dataset
and the feature on the left is how many

288
00:16:27,171 --> 00:16:28,850
packets are sent per second.

289
00:16:29,120 --> 00:16:32,510
And the feature on the right is
what's the size of a packet. Okay.

290
00:16:32,511 --> 00:16:36,160
So what we're trying to do is we're trying
to detect the anomaly and that is a d.

291
00:16:36,161 --> 00:16:37,730
Dot. Sir. If you know what ddosing is,

292
00:16:37,731 --> 00:16:42,530
it's basically flooding a server with
packet requests until a server goes down.

293
00:16:42,770 --> 00:16:46,970
So we're trying to see how many packets
are sent per second from this user and

294
00:16:46,971 --> 00:16:50,630
then what is the size of that
packet. Okay? And then that's that.

295
00:16:50,631 --> 00:16:53,660
Those are our data points and we have
a few of these data points. Okay.

296
00:16:53,661 --> 00:16:57,500
So that's it. Just two features for our
data and we'll talk about, like I said,

297
00:16:57,501 --> 00:17:01,220
dimentionality reduction later on.
If we have a million features,

298
00:17:01,221 --> 00:17:04,190
how do we reduce it to two or
three so that we can visualize it.

299
00:17:04,670 --> 00:17:07,910
So that's our dataset that
we want to load. Okay.

300
00:17:07,911 --> 00:17:10,790
So then here's what we've got here.
This is the Euclidean distance.

301
00:17:10,791 --> 00:17:15,020
Now this is the formula that I was
talking about. So given two data points,

302
00:17:15,021 --> 00:17:19,430
p and Q, take each of the values.
So if it has, you know, Q,

303
00:17:19,431 --> 00:17:22,430
it could be x, it could be
y, z take, he did the values,

304
00:17:22,460 --> 00:17:25,430
subtract them to get the difference,
square it.

305
00:17:25,550 --> 00:17:29,840
And then add them all together and
then square root, that whole equation.

306
00:17:30,050 --> 00:17:33,590
And so that's what the sigma notation
to notes. It's a sum of a set of values.

307
00:17:33,591 --> 00:17:36,560
We're starting at equals
one for end values.

308
00:17:36,710 --> 00:17:38,870
Find the difference difference
between all the data,

309
00:17:38,871 --> 00:17:43,871
all the feature values
in a data points squared,

310
00:17:44,060 --> 00:17:45,980
and then find the square
root of all of that for the,

311
00:17:45,981 --> 00:17:49,980
some of the sum of the
squared errors. Okay. Uh,

312
00:17:50,280 --> 00:17:53,520
technically the sum of the squared errors.
Yeah, and that's Euclidean distance.

313
00:17:54,210 --> 00:17:56,100
So that's a Euclidean distance.

314
00:17:57,450 --> 00:18:00,600
And so now let's look at
the actual algorithm itself.

315
00:18:00,720 --> 00:18:04,320
So what I've done here is
I've defined it's hyper,

316
00:18:04,321 --> 00:18:08,730
it's hyper parameters, and then I'm
going to code out the algorithm itself.

317
00:18:09,060 --> 00:18:09,421
Okay?

318
00:18:09,421 --> 00:18:14,220
So four k means we've got a k
value that is a number of clusters.

319
00:18:14,221 --> 00:18:16,500
And since we were just going to say too,
okay,

320
00:18:16,501 --> 00:18:20,220
we have two clusters that we want
and then we have an epsilon value.

321
00:18:20,221 --> 00:18:23,550
And the epsilon value is,
it's a zero. It's a threshold.

322
00:18:23,790 --> 00:18:27,240
It's the minimum air to be used
in the stop condition. Okay.

323
00:18:27,241 --> 00:18:30,360
When we want to stop training,
okay. When their air is zero.

324
00:18:31,110 --> 00:18:33,860
And then we have our distance.
So what, what type of distance?

325
00:18:33,880 --> 00:18:38,670
We want to compute the EUCLIDEAN
distance. Okay, so let's keep going.

326
00:18:39,960 --> 00:18:44,340
Let me make it a little bigger. So what
we're going to first do is store this,

327
00:18:44,341 --> 00:18:47,640
the past centroids in this
history of centroids list.

328
00:18:47,790 --> 00:18:51,180
Now this is not really a part of
the algorithm. This is just for us.

329
00:18:51,300 --> 00:18:55,830
So we can then graph how the centroids
move over time later on so that we can

330
00:18:55,831 --> 00:18:58,890
visualize it. Okay? So then
we're going to say, okay,

331
00:18:58,891 --> 00:19:00,750
the distance metric is
going to be Euclidean.

332
00:19:00,751 --> 00:19:04,440
So we define it right here as this method,
that variable.

333
00:19:04,800 --> 00:19:08,430
And then we set the data set.
So we load up that data set from txt.

334
00:19:08,580 --> 00:19:11,610
We've got those two dimensions, right?
The amount of packets that are sent,

335
00:19:11,910 --> 00:19:16,080
and then the size of each packet.
Okay? So we've got our data set,

336
00:19:16,081 --> 00:19:19,620
we've defined the distance metric,
and now we're going to say, okay,

337
00:19:19,830 --> 00:19:23,460
let's get the number of instances and the
number of features from our Dataset by

338
00:19:23,461 --> 00:19:26,200
taking this shape
attribute of our dataset.

339
00:19:26,250 --> 00:19:30,270
So our rows and our columns or rows are
going to be the number of data points we

340
00:19:30,271 --> 00:19:33,300
have. And there are columns are going
to be the number of features we have.

341
00:19:33,301 --> 00:19:35,070
So we want to store those two values.

342
00:19:36,600 --> 00:19:40,710
Then we're going to define K
centroids, right? So like I said,

343
00:19:40,890 --> 00:19:45,030
we randomly plot the centroids on the
graph and where they're going to be,

344
00:19:45,031 --> 00:19:46,290
how many centroids they're going to be.

345
00:19:46,620 --> 00:19:51,570
K centroids special k
the cereal. Okay. So,

346
00:19:51,571 --> 00:19:53,550
um,
okay,

347
00:19:53,730 --> 00:19:57,900
so we're going to say our Dataset is
going to be, we're going to say, okay,

348
00:19:57,901 --> 00:20:02,700
so from our Dataset, how many clusters do
we want to find right? Chosen randomly.

349
00:20:02,701 --> 00:20:04,320
So we're going to say size Kay.

350
00:20:04,560 --> 00:20:08,820
So we're gonna find a random
number between zero and a
number of instances that

351
00:20:08,821 --> 00:20:13,560
we have that that is a number of data
points minus one of size. K. Okay.

352
00:20:14,670 --> 00:20:16,200
And so we're going to store that the,

353
00:20:16,270 --> 00:20:19,800
the centroid values in this
prototype's variable, right?

354
00:20:19,801 --> 00:20:22,950
So we've got our centroids and then,
uh,

355
00:20:23,010 --> 00:20:26,550
we want to set these to our
list of past centroids as well.

356
00:20:26,610 --> 00:20:30,090
So we're going to take those centroids
that we just defined randomly and set

357
00:20:30,091 --> 00:20:33,330
them to our history centroids list.

358
00:20:33,360 --> 00:20:37,020
So then we can just keep a copy of it
over time and we'll keep adding our

359
00:20:37,021 --> 00:20:41,130
centroids that are calculated to this
history centroids list so we can graph it

360
00:20:41,131 --> 00:20:44,040
later, you know, for our
own visualization. Okay.

361
00:20:44,041 --> 00:20:47,280
And then we have our prototypes old list.

362
00:20:47,350 --> 00:20:50,260
It's going to be initialized as
a bunch of Zeros. It's empty,

363
00:20:50,650 --> 00:20:53,800
it's an empty vector or tensor.

364
00:20:54,850 --> 00:20:57,700
And so that's going to keep track
of centroids every iteration. Right?

365
00:20:57,880 --> 00:21:01,000
So we've got our prototypes, which is
our current and our prototypes old,

366
00:21:01,001 --> 00:21:04,180
which is are the set of
centroids that we had before.

367
00:21:04,330 --> 00:21:07,360
And those two are what we're going
to use for the actual algorithm,

368
00:21:07,660 --> 00:21:10,050
the histories or the histories.

369
00:21:10,051 --> 00:21:13,630
Centroids is just for us to see
how it changes over time. Okay.

370
00:21:13,990 --> 00:21:17,770
And then we have one more list and that
is the belongs to list to store the

371
00:21:17,771 --> 00:21:20,110
clusters over time.
The clusters themselves,

372
00:21:20,320 --> 00:21:23,290
all the data points
contained in a cluster. Okay.

373
00:21:23,291 --> 00:21:25,330
And then we have our distance method,

374
00:21:25,331 --> 00:21:27,460
which is going to take
the current prototypes.

375
00:21:27,461 --> 00:21:29,980
And then the prototypes old
and the distance between them.

376
00:21:29,981 --> 00:21:33,310
And it's going to store that in
the norm. Uh, variable. Okay.

377
00:21:33,311 --> 00:21:37,450
And then the number of iterations,
which we'll start up as zero. Okay,

378
00:21:37,451 --> 00:21:42,370
so now let's go ahead and write
out this algorithm, shall we?

379
00:21:43,120 --> 00:21:45,250
So we're going to say,
okay, so while the norm,

380
00:21:45,251 --> 00:21:49,180
the distance is greater than epsilon,
where epsilon is zero,

381
00:21:49,181 --> 00:21:50,770
in our case we're going to say,
well,

382
00:21:50,771 --> 00:21:52,690
the number of iterations
we're going to add one,

383
00:21:52,691 --> 00:21:57,340
cause this is our first iteration. We have
begun training our model. So we'll say,

384
00:21:57,341 --> 00:22:01,450
okay, well the iteration is going to be
one. And so let's compute with enormous.

385
00:22:01,451 --> 00:22:04,840
So given our prototypes
where we are currently,

386
00:22:05,050 --> 00:22:09,040
let's compute the distance between those
prototypes and the prototypes that we

387
00:22:09,041 --> 00:22:13,450
had before. And that's going to be our
norm. Then we're going to say, okay,

388
00:22:13,451 --> 00:22:18,280
so for each instance in the Dataset,
so let's go through every single instance.

389
00:22:18,550 --> 00:22:19,091
So we'll say,
okay,

390
00:22:19,091 --> 00:22:23,170
well what is the index of an intense
and what is the actual value of that

391
00:22:23,171 --> 00:22:23,681
instance?

392
00:22:23,681 --> 00:22:27,790
Those are the two variables that we're
going to use to enumerate or iterate over

393
00:22:27,791 --> 00:22:30,580
the data sets.
We'll say,

394
00:22:30,850 --> 00:22:34,420
let's define a distance vector of size.
Kay.

395
00:22:34,750 --> 00:22:38,590
So now it's time for us to define
what this distance vector looks like.

396
00:22:38,800 --> 00:22:39,820
So we'll say,
okay,

397
00:22:39,910 --> 00:22:43,960
so the distance vector is going to first
of all be initialize as a set of zero

398
00:22:43,961 --> 00:22:47,210
values of size k. Okay. So it's uh,

399
00:22:47,290 --> 00:22:51,250
it's going to be empty at first and
we're going to add value to it over time.

400
00:22:51,610 --> 00:22:53,890
Okay. So we'll say, okay,
so for each centroid value,

401
00:22:53,920 --> 00:22:58,750
we have our random centroid values,
right? We already computed the,

402
00:22:58,751 --> 00:23:01,540
there's values randomly at the beginning,
right? So we're going to say, okay,

403
00:23:01,541 --> 00:23:03,550
so eat for each centroid value.

404
00:23:03,551 --> 00:23:08,020
Let me just make this a
comment for each century value.

405
00:23:08,021 --> 00:23:11,360
So for the, for the prototype, and
there's a stored in our prototype,

406
00:23:11,361 --> 00:23:15,100
so we'll say for each index and then
the actual value for the prototype,

407
00:23:16,720 --> 00:23:19,490
let's go ahead and do this
for call of those prototypes.

408
00:23:19,491 --> 00:23:22,780
So for each instance in our Dataset,
and then for each centroid,

409
00:23:22,781 --> 00:23:24,400
so we have two nested for loops,
right?

410
00:23:25,360 --> 00:23:28,780
And then for each centroid
and numerate prototypes,

411
00:23:30,400 --> 00:23:34,000
we're going to say we want to compute the
distance between what do we have to do.

412
00:23:34,100 --> 00:23:39,100
We have to compute the distance between
each data point and its closest and each

413
00:23:40,241 --> 00:23:40,841
centroid.

414
00:23:40,841 --> 00:23:44,290
So for every data point we went to
compute the distance between it and every

415
00:23:44,291 --> 00:23:48,860
other centroid and we want to find the
minimum distance centroid that closest

416
00:23:48,861 --> 00:23:52,430
centroid to it so we can put
it into that specific cluster.

417
00:23:52,850 --> 00:23:54,650
So it was like compute
the distance between

418
00:23:56,150 --> 00:23:59,780
x and x is the data point and centroid.

419
00:24:01,220 --> 00:24:06,220
So we'll say distance vector is
going to be index prototypes.

420
00:24:06,581 --> 00:24:11,581
So for the centroid value,

421
00:24:11,900 --> 00:24:12,733
so here's what we're going to do.

422
00:24:12,980 --> 00:24:16,490
We're going to compute the distance using
distance method between the prototype,

423
00:24:16,610 --> 00:24:20,420
which is the centroid and the instance
value. So for all of those x values,

424
00:24:20,421 --> 00:24:21,500
for all those data points,

425
00:24:21,680 --> 00:24:25,340
we're going to compute the distance
and story and a distance vector list,

426
00:24:25,550 --> 00:24:30,140
and then it's going to be indexed by
the index prototype. Okay? Uh, and then

427
00:24:31,640 --> 00:24:34,130
we're going to do that for all of them.
So that's why we have a nested for loops.

428
00:24:34,131 --> 00:24:37,700
So that's going to store all of those
distances in that distance vector method.

429
00:24:38,090 --> 00:24:43,090
And then when we want to do is we want
to find the smallest distance and assign

430
00:24:44,120 --> 00:24:48,800
that distance to a cluster. So we're
going to say, okay, what is the what,

431
00:24:48,801 --> 00:24:53,801
what cluster does each disk does
each a data point belonged to?

432
00:24:54,800 --> 00:24:55,700
And we'll say,
well,

433
00:24:55,730 --> 00:25:00,730
we'll define it in his belongs to array
and define it by its index instance.

434
00:25:01,250 --> 00:25:04,580
And so now we'll do that arguments that
where we'll find the minimum value for

435
00:25:04,581 --> 00:25:08,780
between all those distance
vectors that we calculated. Okay.

436
00:25:09,290 --> 00:25:13,130
Between all of those distance values and
we use the Euclidean distance that we

437
00:25:13,131 --> 00:25:17,930
define previously in a, in
its own function. And so as
we were going to say, okay,

438
00:25:17,931 --> 00:25:21,590
so we're going to store them all in
there and then we're going to say, okay,

439
00:25:21,591 --> 00:25:25,010
so then we have a list
of temporary prototypes.

440
00:25:25,070 --> 00:25:28,760
Those are temporary centroids because
we're going to update our centers in a

441
00:25:28,761 --> 00:25:32,090
second and we want to save where we are
right now so we could store it in our

442
00:25:32,091 --> 00:25:36,440
history list. So say we'll create, um,

443
00:25:36,500 --> 00:25:37,333
a little

444
00:25:38,810 --> 00:25:43,010
tenser right here and it's going to be
up sized k by the number of features,

445
00:25:43,430 --> 00:25:46,940
right? Because that's the number of
clusters we have. And we're going to say,

446
00:25:48,200 --> 00:25:49,033
okay,

447
00:25:49,390 --> 00:25:50,650
for each cluster,

448
00:25:51,940 --> 00:25:56,380
hold on for each cluster
and our k of them,

449
00:25:56,800 --> 00:26:01,000
let's say. Okay, so for, for each
cluster and there are k of them,

450
00:26:04,930 --> 00:26:08,200
let's go ahead and say, let's get
all the points assigned to a cluster.

451
00:26:09,100 --> 00:26:11,800
Let's get all of those points assigned to
a cluster, right? So we're going to say,

452
00:26:11,920 --> 00:26:14,290
okay, I'm just going to paste it in
for it cause it's a little faster.

453
00:26:14,291 --> 00:26:17,770
We don't have a little bit left. Okay.
So for each cluster in their k of them,

454
00:26:18,010 --> 00:26:20,800
we're going to get all the points
assigned to a cluster. Okay?

455
00:26:20,801 --> 00:26:23,830
And that's what this, what
this uh, line does right here.

456
00:26:23,980 --> 00:26:27,550
We're going to get all those points
assigned to a cluster and store them in

457
00:26:27,551 --> 00:26:30,790
instances close. Okay? So those instances,

458
00:26:30,791 --> 00:26:34,270
close contains all the data
points within a specific cluster.

459
00:26:34,390 --> 00:26:36,610
And we're doing this for each
of the clusters, so, right.

460
00:26:36,730 --> 00:26:38,350
And for each cluster k of them,

461
00:26:39,580 --> 00:26:41,380
we're going to find the
mean of those data points.

462
00:26:41,381 --> 00:26:44,640
And so now you get the
k means for k clusters,

463
00:26:45,000 --> 00:26:49,110
compute the mean of the data points
in that cluster using n p. Dot. Mean.

464
00:26:49,410 --> 00:26:50,460
So that is the average.

465
00:26:50,461 --> 00:26:54,750
We add them all up and then divide by
the number of them for all of those

466
00:26:54,751 --> 00:26:57,720
instances and store
that mean in prototype.

467
00:26:57,960 --> 00:27:00,540
And then we're gonna that's
gonna be our new centroid.

468
00:27:00,690 --> 00:27:05,690
So we'll add our new centroid to our
temporary prototypes list and we did that

469
00:27:05,880 --> 00:27:10,050
so we can then take that
temporary prototype and assign
it to our main prototype

470
00:27:10,080 --> 00:27:11,190
list.
Okay.

471
00:27:11,191 --> 00:27:14,940
So then were the temporary prototype
variable acts as a buffer as well,

472
00:27:14,941 --> 00:27:18,780
computing new centroids to store them
and then we can add it to our main

473
00:27:18,810 --> 00:27:22,380
prototypes variable. And then we
have our history centroids list,

474
00:27:22,500 --> 00:27:27,150
which will also append to just for us to
calculate this for us to visualize how

475
00:27:27,151 --> 00:27:29,220
this centroids move over time later on.

476
00:27:29,221 --> 00:27:31,650
And you'll see exactly what I
mean by that. At the end of this,

477
00:27:31,651 --> 00:27:35,420
we were turned our calculated centroids
and that is at the end of our algorithm

478
00:27:35,421 --> 00:27:39,570
and we have reached convergence and
then our history of centroids all those

479
00:27:39,580 --> 00:27:43,800
centuries that we've calculated over
time and then belongs to which is the c,

480
00:27:43,930 --> 00:27:48,930
the all the clusters or the where we
cluster all those data points and then the

481
00:27:49,321 --> 00:27:52,620
index is going to be the cluster
that it belongs to that k cluster.

482
00:27:52,980 --> 00:27:57,510
And so that's how we define all those
data points in a single cluster. Okay,

483
00:27:57,511 --> 00:27:59,160
so that's it for the algorithm.

484
00:27:59,161 --> 00:28:01,560
And then we're going to
graph what this looks like.

485
00:28:01,950 --> 00:28:04,140
So in our plotting algorithm
we're going to say, okay,

486
00:28:04,141 --> 00:28:08,060
well we're going to have two colors,
red and green for our central,

487
00:28:08,090 --> 00:28:10,950
we'll define them here. We'll
split our graph up, bites,

488
00:28:10,980 --> 00:28:13,530
acce access and it's actual plot.

489
00:28:14,040 --> 00:28:16,170
And then for each point in our Dataset,

490
00:28:16,440 --> 00:28:18,720
and we've got several points in our
data set, we're going to say, okay,

491
00:28:18,900 --> 00:28:23,070
we want to graph all these points
by their cluster color, which we've,

492
00:28:23,280 --> 00:28:24,870
which we're going to define.
So all the,

493
00:28:24,960 --> 00:28:28,800
all the data points in one cluster are
going to be one color and all the data

494
00:28:28,801 --> 00:28:31,560
points and another cluster are
going to be another color. Okay?

495
00:28:31,561 --> 00:28:36,390
So then we'll get all those points
by looking in. D belongs to list,

496
00:28:36,600 --> 00:28:41,190
okay? By its index, right? So for the
index, for each point in our Dataset,

497
00:28:41,370 --> 00:28:42,570
get the instances,

498
00:28:42,571 --> 00:28:46,140
those are all the data points that
belonged to a specific cluster and then

499
00:28:46,141 --> 00:28:51,141
assign each data point in that cluster
a color and plot it its own respective

500
00:28:51,211 --> 00:28:53,610
color, okay? And so
that's what we do here.

501
00:28:53,760 --> 00:28:55,930
And then we're going to log
the history of centroids.

502
00:28:55,950 --> 00:29:00,000
Remember how he said how we want to see
what the history of centroids that our

503
00:29:00,001 --> 00:29:03,720
calculators are so we can check, see how
it changes. That's what we're doing here.

504
00:29:03,721 --> 00:29:08,430
So in the history points list, we'll
say for each centroid ever calculated,

505
00:29:08,580 --> 00:29:12,570
print them all out and then plot
them in their own graph. Okay?

506
00:29:12,571 --> 00:29:14,310
So that's what's happening in the method.

507
00:29:14,790 --> 00:29:16,740
And then we're going to
actually execute it, right?

508
00:29:16,741 --> 00:29:19,470
We've written methods for
our K-means Algorithm.

509
00:29:19,470 --> 00:29:24,120
We've written a method to plot out our
results and now we can execute these

510
00:29:24,121 --> 00:29:26,400
methods.
So we're going to load our Dataset,

511
00:29:26,700 --> 00:29:29,730
train the model on that
data where k equals two.

512
00:29:29,731 --> 00:29:34,170
So we want to clusters and then it's
going to give us back our computed

513
00:29:34,171 --> 00:29:39,171
centroids the history of all the centers
that were computed over time and in our

514
00:29:39,211 --> 00:29:44,200
list of data points defined by
the cluster that each belongs to.

515
00:29:45,220 --> 00:29:48,660
So we could take those three values
and then use them as parameters for our

516
00:29:48,661 --> 00:29:53,020
plotting function. And that's going to
plot our results. And so when we run that,

517
00:29:53,200 --> 00:29:57,440
we're going to get these
two clusters, right? So if,

518
00:29:57,480 --> 00:30:00,760
if we didn't run our k-means algorithm
and we just plotted our data,

519
00:30:00,880 --> 00:30:03,700
it would look just like this,
except there would be no color.

520
00:30:03,701 --> 00:30:05,380
Cause we don't know what the clusters are.

521
00:30:05,710 --> 00:30:08,920
But what happened was the
algorithm learned that the,

522
00:30:09,220 --> 00:30:13,360
that the right clusters were red
and green for the, for these,

523
00:30:13,660 --> 00:30:15,100
for these respective clusters.

524
00:30:15,430 --> 00:30:18,700
And the blue points are
the central central points.

525
00:30:18,880 --> 00:30:21,760
And so those are the centroids.
Okay.

526
00:30:21,761 --> 00:30:26,230
And so over time what happens is, look,
here's, here's what I mean by over time,

527
00:30:26,390 --> 00:30:26,891
over time,

528
00:30:26,891 --> 00:30:30,760
what happens is it learns the ideal
center points for the center for the

529
00:30:30,761 --> 00:30:34,060
centroids. Okay? It starts,

530
00:30:34,150 --> 00:30:37,060
it learns these ideal center
points over time. The first,

531
00:30:37,061 --> 00:30:40,330
if they're randomly blue and then it
gets better and better iteratively,

532
00:30:40,331 --> 00:30:44,590
it finds better and better centerpoint's
find the most optimal center point for

533
00:30:44,591 --> 00:30:48,430
the cluster that we're going to
learn. And then it plots it. Okay?

534
00:30:48,431 --> 00:30:52,330
So that's it. Please subscribe for
more programming videos. And for now,

535
00:30:52,360 --> 00:30:55,150
I've got to find my center.
So thanks for watching.

