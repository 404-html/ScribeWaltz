1
00:00:03,150 --> 00:00:03,361
Hello,

2
00:00:03,361 --> 00:00:08,361
we're all did SAROJ and welcome to this
live stream of my new 100 k a youtube

3
00:00:08,821 --> 00:00:12,120
channel livestream.
I'm very excited for this.

4
00:00:12,330 --> 00:00:14,010
Thank you guys for a hundred
k subscribers. By the way.

5
00:00:14,030 --> 00:00:17,190
I am very happy and we are the fastest
growing artificial intelligence community

6
00:00:17,400 --> 00:00:18,570
in the world.
Okay,

7
00:00:18,571 --> 00:00:23,571
so today we are going to be talking
about Disco Ganz and the name is pretty

8
00:00:24,781 --> 00:00:29,260
interesting, right? But it has
nothing to do with disco balls. But Q,

9
00:00:29,310 --> 00:00:31,260
the disco mute. No, I'm just kidding.
No, we're not going to do that.

10
00:00:32,010 --> 00:00:35,670
We're going to be
building a model that is,

11
00:00:36,270 --> 00:00:39,000
that consists of six
different neural networks now.

12
00:00:39,001 --> 00:00:41,940
So it sounds complicated and it is,
but,

13
00:00:42,090 --> 00:00:46,260
but it's going to be really easy because
I'm building a very bare bones version

14
00:00:46,261 --> 00:00:49,890
of it and it's going to be in tenser flow.
Okay? And you're, you might be asking,

15
00:00:50,040 --> 00:00:52,860
why should we be doing this?
Saroj what is the point of this?

16
00:00:52,861 --> 00:00:57,720
I see purses and I see shoes.
Uh, now here's, here's,

17
00:00:57,721 --> 00:01:01,560
here's the cool thing about
this. Okay. Using Disco Gan,

18
00:01:01,710 --> 00:01:03,540
this is a new type of
generative adversarial network.

19
00:01:03,541 --> 00:01:07,470
I mean there's paper literally just
came out last month, like last month.

20
00:01:07,800 --> 00:01:11,670
And it's very exciting because I was
just going over this paper and trying to

21
00:01:11,671 --> 00:01:14,610
make sure that I understood all the
details before I did this stream.

22
00:01:14,760 --> 00:01:17,490
And now I've got a good, uh,
I've, I've dropped it pretty well.

23
00:01:17,491 --> 00:01:19,560
So I'm excited to share
with you what I've learned.

24
00:01:20,040 --> 00:01:22,890
So this paper came out not
last month but two months ago,

25
00:01:23,070 --> 00:01:27,540
but it was called learning to discover
cross domain relations with generative

26
00:01:27,541 --> 00:01:31,110
adversarial networks. Okay. And I'm
going to answer questions in a second.

27
00:01:31,111 --> 00:01:34,770
Let me just explain this first. But
the, the, the idea behind the paper was,

28
00:01:34,771 --> 00:01:39,000
so the word disco comes from discover,
so, right. It's like disco discover,

29
00:01:39,090 --> 00:01:43,080
cross the main relations. So the idea
was we have two different image datasets.

30
00:01:43,320 --> 00:01:47,460
One of them is let's say purses
and the other one is a set of,

31
00:01:47,520 --> 00:01:48,450
so let's say shoes.

32
00:01:48,660 --> 00:01:53,660
And what we can do using disco Gan is we
can generate images from one dataset in

33
00:01:53,941 --> 00:01:57,480
the style of another.
So here you see exactly what it can do.

34
00:01:57,481 --> 00:01:59,250
Like here's two sets of examples.

35
00:01:59,251 --> 00:02:02,820
The first set of examples is
saying in a fully trained district,

36
00:02:02,850 --> 00:02:07,590
this Gauguin on these two datasets,
purses and shoes, given some novel purse,

37
00:02:08,280 --> 00:02:11,250
you can say,
generate this purse in the style,

38
00:02:11,790 --> 00:02:15,750
generate a shoe in the style
of this purse. Right? So this,

39
00:02:15,780 --> 00:02:19,230
this is really interesting, right?
So it's style transfer essentially,

40
00:02:19,380 --> 00:02:21,840
but it's not like the style
transfer that we've seen before.

41
00:02:21,841 --> 00:02:25,700
So recall that we've, we've talked
about style transfer before. Uh,

42
00:02:25,710 --> 00:02:29,210
but it was more like, you
know, Mona Lisa, uh, you know,

43
00:02:29,250 --> 00:02:32,190
starry nights style transfer.

44
00:02:33,120 --> 00:02:37,640
The style transfer that we did
before was basic. Okay. Basic Aif.

45
00:02:37,890 --> 00:02:42,600
It was essentially like an, like an
Instagram image filter where we just have,

46
00:02:42,601 --> 00:02:45,340
if we consider two
images to be to, uh, uh,

47
00:02:45,360 --> 00:02:50,100
matrices of pixel values,
and of course, you know,

48
00:02:50,640 --> 00:02:54,270
it's gotta be bad results, but you
know what I'm talking about, right?

49
00:02:54,271 --> 00:02:57,510
We have a starry night picture and then
we have some image and we just overlaid

50
00:02:57,511 --> 00:03:01,460
that style onto that other picture, like
Van Gogh or whatever it is. And that's,

51
00:03:01,670 --> 00:03:06,430
that was a naive approach, right?
We, why is it naive? Why is it naive?

52
00:03:06,670 --> 00:03:11,190
Because we're not, we're not focusing on
some subject. We're not saying like it.

53
00:03:11,260 --> 00:03:16,260
The machine doesn't actually know like
this exact Mona Lisa is all we want to

54
00:03:16,690 --> 00:03:19,210
transfer this style too. Let's
just transfer everything, right?

55
00:03:19,390 --> 00:03:22,030
So what this Gauguin does, it is, it is,

56
00:03:22,770 --> 00:03:26,530
it's like smart or style transfer,
right?

57
00:03:26,800 --> 00:03:31,690
Because what it's generating isn't like
a purse and it's kind of like molded

58
00:03:31,691 --> 00:03:36,490
into a shoe. No, it's an actual, it's
an actual shoe. It looks like a shoe.

59
00:03:36,620 --> 00:03:40,150
It, it looks like it would be something
that just wasn't generated. Right.

60
00:03:40,151 --> 00:03:43,210
It looks very realistic.
Like a style in and of itself.

61
00:03:43,420 --> 00:03:47,890
And Gans are really good at this.
They're really good at this.

62
00:03:47,891 --> 00:03:50,950
Like insanely good. This is
a cycle again, video, right?

63
00:03:51,340 --> 00:03:56,140
Notice how we've, we've transferred
the style of a zebra to a horse,

64
00:03:56,500 --> 00:03:58,930
but it's not like we're making
everything black and white.

65
00:03:58,960 --> 00:04:02,290
We're just making the horse black
and white, which is incredible.

66
00:04:02,320 --> 00:04:06,190
It's just incredible. This technology is
incredible and it's just moving so fast.

67
00:04:06,430 --> 00:04:08,500
So I'm really excited to talk about this.

68
00:04:08,740 --> 00:04:13,250
And what I found is that it's
actually very intuitive to,

69
00:04:13,251 --> 00:04:18,190
to understand, okay. Uh, so we,
let's, and here's the other thing.

70
00:04:18,191 --> 00:04:22,960
So we humans, we do this very well, right?
Cross domain relations. If I have a,

71
00:04:23,020 --> 00:04:25,390
you know,
if I've got a black suit,

72
00:04:25,690 --> 00:04:30,430
I know that what goes well with it,
the cross domain, the domain of suits,

73
00:04:30,610 --> 00:04:32,560
what's the cross domain relation?

74
00:04:32,561 --> 00:04:36,430
What's the other domain that's related
to this domain? I could say, you know,

75
00:04:36,640 --> 00:04:41,610
some dope like leather boots would go
well with it or something like, um,

76
00:04:41,770 --> 00:04:44,600
we're just able to do it well
or like Burger and fries,
like this type of Burger.

77
00:04:44,601 --> 00:04:47,170
It goes well with this hyper fries,
but can machines do it?

78
00:04:47,171 --> 00:04:50,590
And the answer is yes they can because
we are able to make them do it.

79
00:04:50,710 --> 00:04:54,280
So we can frame it as a conditional
image generation problem, right?

80
00:04:54,430 --> 00:04:59,430
Given one image purse con generate a
new new image condition on a different

81
00:04:59,681 --> 00:05:02,260
dataset. And that would be like
shoes, right? So we've done,

82
00:05:02,320 --> 00:05:05,380
we've talked about conditional
probability distribution before, right?

83
00:05:05,800 --> 00:05:10,180
Probability of x, given why rather
than the probability of x alone.

84
00:05:10,600 --> 00:05:13,240
So that's essentially what it is.
That's one way of for us to frame it.

85
00:05:13,270 --> 00:05:18,160
And in terms of use cases, games, design,

86
00:05:18,190 --> 00:05:19,810
science,
engineering,

87
00:05:19,960 --> 00:05:22,840
anytime you want any kind of visual
feedback of what you're thinking,

88
00:05:22,960 --> 00:05:23,800
just imagine,

89
00:05:24,040 --> 00:05:28,210
just imagine a a text or
voice interface or you say,

90
00:05:28,330 --> 00:05:33,330
I want to take this three d model that
I have is cad model and I want to apply

91
00:05:34,421 --> 00:05:38,980
all of these features to it and an
a disco Gan that's been generally,

92
00:05:39,010 --> 00:05:42,130
that's been trained on all of these
different datasets. We'll be able to say,

93
00:05:42,131 --> 00:05:46,800
okay, here it is. Just like that because
it's already been trained, right? So the,

94
00:05:46,801 --> 00:05:49,150
the actual generation
process doesn't take forever.

95
00:05:49,180 --> 00:05:51,250
It's a training process
that takes forever.

96
00:05:51,490 --> 00:05:55,720
So there's a lot of possibilities
for pretty much any a science,

97
00:05:55,721 --> 00:05:58,790
engineering or art especially,
uh,

98
00:05:58,850 --> 00:06:02,720
any field where there is some kind
of creation or discovery involved.

99
00:06:03,140 --> 00:06:06,200
Perfect for it. And Games of course.
Well, any kind of virtual world,

100
00:06:06,400 --> 00:06:11,180
there is so much possibility. Think,
think beyond the purse and the shoe. Okay.

101
00:06:11,181 --> 00:06:16,160
That's just the example that we're
using. Think big. Okay. Think big.

102
00:06:16,280 --> 00:06:19,490
Okay. And that, that is,
that is where the money is.

103
00:06:19,491 --> 00:06:22,210
That's where the opportunity is.
That's where, that's where it's,

104
00:06:22,250 --> 00:06:25,580
that's where it's, that's where it's at.
That's what I would be doing if I was, um,

105
00:06:26,120 --> 00:06:27,710
doing, you know, a startup or whatever.

106
00:06:27,940 --> 00:06:31,070
I would be looking at ways of taking
this technology and applying it to real

107
00:06:31,071 --> 00:06:35,000
world problems that people encounter.
Okay. So that's the high level.

108
00:06:35,120 --> 00:06:40,120
So let me answer two questions and
then we'll get started with the code.

109
00:06:41,270 --> 00:06:43,710
Okay. So the first, uh,

110
00:06:44,930 --> 00:06:48,110
question that is going to,

111
00:06:48,500 --> 00:06:53,500
I'm going to answer once I
pull up my question window is,

112
00:06:55,270 --> 00:06:58,960
uh,
just logging in.

113
00:06:59,400 --> 00:07:01,310
This is pretty cool. Uh, let
me, let me make this bigger.

114
00:07:01,320 --> 00:07:02,750
The fun definitely needs to be bigger,

115
00:07:03,160 --> 00:07:08,130
but we no longer support
this version of your browser.

116
00:07:08,940 --> 00:07:09,990
That's what happens when

117
00:07:11,830 --> 00:07:14,800
it happens. Okay. So we've
got 324 people watching.

118
00:07:14,950 --> 00:07:19,240
Let me see who in the room.
Why do you use particularly tensorflow?

119
00:07:19,300 --> 00:07:21,760
That's a great question.
Why do you use tensorflow?

120
00:07:22,180 --> 00:07:25,930
So the reason I use tensorflow and
up, and you're right because the, the,

121
00:07:25,931 --> 00:07:28,390
the authors of the paper
did this in Pi Torch.

122
00:07:28,690 --> 00:07:32,770
Pi Torch is getting really
cool. It's getting really, uh,

123
00:07:32,800 --> 00:07:34,570
used by a lot of cool people.

124
00:07:34,660 --> 00:07:37,120
Remember that Pi Torch in five
minutes video that I just made,

125
00:07:37,210 --> 00:07:41,230
Yann Macun shared that on Facebook and
it got like a Ha. I was like, oh my God,

126
00:07:41,231 --> 00:07:44,470
y'all look good dude. That guy
is so cool. Anyway, okay. Um,

127
00:07:45,520 --> 00:07:47,380
Geez status.
So yeah.

128
00:07:47,381 --> 00:07:50,320
So why don't I do an intention flow
because this course is intensive low and I

129
00:07:50,321 --> 00:07:53,280
didn't want to just, you know, drop Pi
Torch on you guys. All of a sudden we got,

130
00:07:53,380 --> 00:07:56,800
we've got to stick to tensorflow for now,
but I promise you Pi Torch is coming.

131
00:07:56,990 --> 00:08:00,400
Tensorflow is great. Okay. It's not,
it's not a bad, it's not a bad framework.

132
00:08:00,401 --> 00:08:01,360
It's just,
you know,

133
00:08:02,350 --> 00:08:05,320
it's what we're used to and we want to
focus on the algorithms right now instead

134
00:08:05,321 --> 00:08:07,390
of syntax. So that's what I'm
going to keep using tensorflow.

135
00:08:07,540 --> 00:08:10,510
Let me answer one more question and then
we'll get started with the code. Uh,

136
00:08:10,720 --> 00:08:11,860
the other question is,

137
00:08:19,470 --> 00:08:23,710
the other question is how do you select,
oh no,

138
00:08:23,780 --> 00:08:27,550
this is a great question. What is the
best way in resource to learn algorithms?

139
00:08:27,730 --> 00:08:31,030
What is the best systematic way to
learn machine learning? Best Resource.

140
00:08:31,031 --> 00:08:33,550
I know it's your videos and I'm
already following them. So, okay,

141
00:08:33,551 --> 00:08:37,660
those are two separate questions.
The first algorithms is the best way.

142
00:08:37,661 --> 00:08:38,860
So I have a great video on this.

143
00:08:38,861 --> 00:08:41,440
It's called how to succeed
in any programming interview.

144
00:08:41,441 --> 00:08:45,340
I give you a great study guide to get
your data structures and algorithms skills

145
00:08:45,341 --> 00:08:49,060
on point. So check that out. In terms
of machine learning, guests, my videos,

146
00:08:49,090 --> 00:08:52,180
this playlist, I have, you
know, several courses on it.

147
00:08:52,240 --> 00:08:55,650
You Udacity has several great machine
learning courses. Uh, but yeah,

148
00:08:55,710 --> 00:08:58,070
that's what I would say. You
Udacity, my courses, you know,

149
00:08:58,071 --> 00:09:02,380
just together great resources and if you
want really in depth and you have the

150
00:09:02,381 --> 00:09:06,180
time and you've got the, you've
got the motivation to not to, to,

151
00:09:06,181 --> 00:09:09,500
to learn without needing it to
be entertained whatsoever. Uh,

152
00:09:09,600 --> 00:09:14,350
which I need to be entertained. I just
entertain myself with music and stuff. Uh,

153
00:09:14,430 --> 00:09:16,620
then probably the deep learning book,
uh,

154
00:09:16,621 --> 00:09:21,210
by Ian Goodfellow and
Yoshua Bengio. Okay. Uh,

155
00:09:21,240 --> 00:09:26,130
but more than that, you know,
I, I'm beginning to, you know,

156
00:09:26,170 --> 00:09:28,890
it's, it's something that I believed
I believed in my head for a while,

157
00:09:28,891 --> 00:09:33,090
but I've never actually said it.
Um, because it's so, uh, radical,

158
00:09:33,240 --> 00:09:37,170
but textbooks are just no stop,
stop reading textbooks.

159
00:09:37,320 --> 00:09:40,810
It's not about the textbooks we
live in the age of, uh, you know,

160
00:09:40,920 --> 00:09:44,070
the Internet and Twitter and all
this stuff. We don't need to,

161
00:09:44,160 --> 00:09:47,370
we don't have to have, we don't
have, we don't need to read books.

162
00:09:47,371 --> 00:09:50,880
Book should read themselves to us.
Right? Right. We need an easier way up.

163
00:09:51,020 --> 00:09:52,620
It's just data and algorithms,
right.

164
00:09:52,621 --> 00:09:56,070
Learning is just a process of data and
algorithms and we need to get it into our

165
00:09:56,071 --> 00:10:00,150
head and what's a great medium for that
video? Uh, and all sorts of, um, you know,

166
00:10:00,151 --> 00:10:04,050
very condensed, you know, stuff. Anyway,
okay, let's get started with this.

167
00:10:04,200 --> 00:10:07,320
I'm going to talk about most of the code
and I'm going to code the parts that

168
00:10:07,321 --> 00:10:11,040
really matter. Okay. So
the first part is, okay,

169
00:10:11,400 --> 00:10:15,900
the first part of this is going to
be us defining our dependencies.

170
00:10:15,901 --> 00:10:17,760
Let me make sure the text is big enough.

171
00:10:18,630 --> 00:10:22,860
Big like a boughs big, like a boss. Okay.

172
00:10:23,250 --> 00:10:25,350
Oh there's, there's uh, one
more thing I want to talk about.

173
00:10:25,351 --> 00:10:28,320
So how should we structure
this architecture?

174
00:10:28,321 --> 00:10:31,110
Let's do a thought exercise
for a second, right? We want,

175
00:10:31,290 --> 00:10:32,610
we have to image data sets.
Okay.

176
00:10:32,611 --> 00:10:36,210
We have to image data sets and we want
to generate an image in the style of

177
00:10:36,211 --> 00:10:39,390
another. Okay, so conditional
image generation. Well,

178
00:10:39,391 --> 00:10:43,350
one naive way we can think about this is
an encoder decoder architecture, right?

179
00:10:43,560 --> 00:10:47,340
So we have some image and then we
encode it into a Lenten space, right?

180
00:10:47,341 --> 00:10:50,340
We have some latent space in
coding and then we decode it again.

181
00:10:50,520 --> 00:10:51,630
But when we decode it,

182
00:10:51,660 --> 00:10:55,200
we keep that light and space
and we condition it on the
other image data set so

183
00:10:55,201 --> 00:10:59,100
that that image is the one that's decoded
is exactly like a reconstruction of

184
00:10:59,101 --> 00:11:01,980
the original,
but it's conditioned on the other dataset.

185
00:11:02,460 --> 00:11:06,240
That's a naive way of doing it because
what that would do is like the original

186
00:11:06,241 --> 00:11:09,510
style transfer paper, it would, it
would just make everything styled,

187
00:11:09,630 --> 00:11:12,330
but it wouldn't be that you wouldn't be
that specific transfer that we're doing

188
00:11:12,331 --> 00:11:16,410
right now. So that's, that's
the naive way. It's like a
camera filter on Instagram.

189
00:11:16,800 --> 00:11:21,240
A better way is what if we had two
encoder decoders write one for each image

190
00:11:21,241 --> 00:11:23,250
data set and to both
conditioned on each other,

191
00:11:23,970 --> 00:11:26,850
that would be good and that it
would make it backwards compatible.

192
00:11:26,880 --> 00:11:27,713
But it's still naive,

193
00:11:27,720 --> 00:11:31,050
backwards compatible in that we could
generate us chew in the style of a purse

194
00:11:31,170 --> 00:11:34,740
and a person that's stylish shoe,
but the dope boy of doing this,

195
00:11:34,741 --> 00:11:39,480
and that's the one that we're going to
do is doing to encoder decoders in an

196
00:11:39,510 --> 00:11:43,080
adversarial Tom context.
Let me talk about what this looks like,

197
00:11:43,081 --> 00:11:45,240
but before we show,
I show you the architecture.

198
00:11:45,390 --> 00:11:48,060
Let's define our dependencies
and our datasets. Okay.

199
00:11:48,870 --> 00:11:52,610
So those are some questions
that I wanted to answer. Uh,

200
00:11:52,650 --> 00:11:54,850
and I'll answer questions later.
But let's start off with this.

201
00:11:54,940 --> 00:11:58,570
So we first bridge the gap between
Python two and three with the future,

202
00:11:58,660 --> 00:12:01,870
which is totally necessary
a lot of the time. Okay.

203
00:12:01,871 --> 00:12:03,820
Is that for things like the
print function, whatever.

204
00:12:04,060 --> 00:12:06,070
And then we're going to use
[inaudible] for saving files.

205
00:12:06,071 --> 00:12:10,720
And then num Pi for matrix math map plot
live is our Handy Dandy visualization

206
00:12:10,900 --> 00:12:14,920
library for data. We're using
tensorflow for machine learning and uh,

207
00:12:14,980 --> 00:12:19,720
this is a custom class. These two are
custom classes for generating data,

208
00:12:19,870 --> 00:12:22,300
using what's called a
golf Sian mixture model,

209
00:12:22,301 --> 00:12:25,750
which is a really cool idea that I haven't
talked about in this course and I'm

210
00:12:25,780 --> 00:12:30,190
excited to talk about that. Uh, and then
we have a progress bar for, for training.

211
00:12:30,490 --> 00:12:34,090
And then, uh, we're also going
to be using the TF slim library,

212
00:12:34,091 --> 00:12:36,490
which I haven't also used before,
but don't worry, it's just,

213
00:12:36,491 --> 00:12:39,640
it's just tension flow. It just
tentraflow neatly wrapped, neatly wrapped,

214
00:12:39,760 --> 00:12:44,110
similar to Carrie Ross. I'll talk about
the differences. And then we have our, uh,

215
00:12:45,730 --> 00:12:49,330
this distributions class that's going
to represent batches of statistical

216
00:12:49,331 --> 00:12:54,240
distributions. Uh, that's going to help
us generate our data. And then lastly, uh,

217
00:12:54,250 --> 00:12:58,600
this graph replace a method
which is going to let a function,

218
00:12:58,601 --> 00:13:00,330
which is going to let us,
uh,

219
00:13:01,030 --> 00:13:05,520
to regenerate a network,
uh, with replaced 10 stores.

220
00:13:05,520 --> 00:13:07,800
And I'll talk about why that's
that. That is okay. If, so,

221
00:13:07,990 --> 00:13:11,950
those are our dependencies and let's start
defining our hyper parameters, right?

222
00:13:12,040 --> 00:13:16,450
So what are our hyper parameters will
be four. We define our hyper parameters.

223
00:13:16,451 --> 00:13:18,730
We need to know how many networks
we're going to use, right?

224
00:13:19,060 --> 00:13:23,440
We're gonna use a one, two, three,
four, five, six, six different networks.

225
00:13:23,441 --> 00:13:26,620
This is what our architecture looks like
right here and move out of the way just

226
00:13:26,621 --> 00:13:30,070
like that. So this is what our
architecture looks like. Okay?

227
00:13:30,310 --> 00:13:32,930
It's kind of confusing if you,
if it's your first glance.

228
00:13:32,931 --> 00:13:34,960
So like I've been looking at those for
a while. It's actually not confusing.

229
00:13:35,050 --> 00:13:38,260
So here's how it works.
We have four generators.

230
00:13:38,470 --> 00:13:42,400
Each of these boxes is a generator.
This one, this one, this one, this one.

231
00:13:42,700 --> 00:13:47,680
And then we have two discriminators
discriminator, one discriminator too.

232
00:13:47,860 --> 00:13:52,150
Okay. And then what is going on
here? Well, each of these generators,

233
00:13:52,640 --> 00:13:55,180
let's just look at these two on top
of your right, like right here. Okay,

234
00:13:55,660 --> 00:13:58,570
this one is an encoder
and this one is a decoder.

235
00:13:58,660 --> 00:13:59,950
We can just think of it that way.
That's,

236
00:13:59,960 --> 00:14:02,740
that's how we're framing
these generators. Okay? Okay.

237
00:14:02,741 --> 00:14:06,310
So it's an encoder and a decoder.
And it's the same for this one.

238
00:14:06,430 --> 00:14:10,090
It's an encoder and a decoder.
And then we have two discriminators.

239
00:14:10,210 --> 00:14:13,090
So these generators are both
encoders and decoders. Okay?

240
00:14:13,091 --> 00:14:16,690
So what am I talking about here? So we'll
give it an image. Let's say it's this,

241
00:14:16,750 --> 00:14:19,920
uh, this blonde woman, and
then we will encode it and it,

242
00:14:19,921 --> 00:14:22,720
so what do I mean by encoding? So why
don't we normally think about encoding.

243
00:14:22,900 --> 00:14:26,650
We're thinking about taking some image
and converting it into a latent space.

244
00:14:26,651 --> 00:14:28,600
So rights and representation
of that image.

245
00:14:30,070 --> 00:14:32,560
And so that's exactly
what we're doing here. Uh,

246
00:14:32,590 --> 00:14:37,540
but we are a conditioning
it when we encode it on the,

247
00:14:37,570 --> 00:14:40,900
uh, separate, the other image data sets.
So let's say we had the blonde woman,

248
00:14:41,110 --> 00:14:43,600
but then the other image data
set was dark haired women.

249
00:14:43,750 --> 00:14:46,220
So this would be a mix
of the two, right? What,

250
00:14:46,230 --> 00:14:49,960
what Incode is going to be a mix of
the two. And then when we decode,

251
00:14:50,080 --> 00:14:51,740
we're taking that conditioned,
uh,

252
00:14:51,830 --> 00:14:54,770
image that's generated and we're
reconstructing the original.

253
00:14:55,250 --> 00:14:56,990
We're trying to best
reconstruct the original.

254
00:14:57,230 --> 00:15:00,920
And what happens here is then we can
measure the loss between the reconstructed

255
00:15:00,921 --> 00:15:04,400
image and the original image and we'll
use that loss and feed that to the

256
00:15:04,401 --> 00:15:08,930
discriminator. So the same thing is
happening for this encoder decoder para,

257
00:15:09,140 --> 00:15:12,500
but it's the opposite. So we,
so for this one, it's image.

258
00:15:12,530 --> 00:15:15,680
Let's say it's like data set one,
two Dataset to, for this one,

259
00:15:15,681 --> 00:15:20,000
it's from data set to data set one as
in instead of generate a shoe from the

260
00:15:20,001 --> 00:15:22,160
purse, get generate a
purse from a shoe. So it's,

261
00:15:22,250 --> 00:15:25,160
so it's like the backwards version of it.
It's like vice versa, you know, like the,

262
00:15:25,161 --> 00:15:28,120
the opposite. We do the same exact
thing, but, but the opposite,

263
00:15:28,170 --> 00:15:31,910
what this does and the reason we have
this is so that we can do it both ways.

264
00:15:31,911 --> 00:15:35,600
We can generate images in both,
uh, crossed up in both domains.

265
00:15:36,200 --> 00:15:40,970
Just as well. And we feed both of those
losses to our discriminators. Okay.

266
00:15:41,960 --> 00:15:44,390
So we have two separate
discriminators for each of them.

267
00:15:44,660 --> 00:15:47,780
So these are both generative networks.
So you might be thinking,

268
00:15:47,960 --> 00:15:50,420
wait a second generators,
they're just,

269
00:15:50,810 --> 00:15:54,770
they're just going to be
continuously upsampling. Right?
Well, when we decode your,

270
00:15:54,771 --> 00:15:57,500
you're normally thinking about doing the
opposite of what you just did before.

271
00:15:57,650 --> 00:16:00,470
So for an encoder decoder with, uh,
with two convolutional networks,

272
00:16:00,650 --> 00:16:04,010
you would think that for encoding
with convolutional nets, you're,

273
00:16:04,130 --> 00:16:06,800
you're downsampling. And then for
the Di Carter, you're upsampling,

274
00:16:07,040 --> 00:16:11,780
but in this case, because
they're both generators, we're
essentially continuously,

275
00:16:11,840 --> 00:16:16,130
uh, upsampling. So that is weird.
Generating an image, right?

276
00:16:16,550 --> 00:16:19,460
And then we're going to continue, we're
going to generate again from that image.

277
00:16:19,610 --> 00:16:24,320
So it's just like one big huge generator
pair, just continuously generating. Okay.

278
00:16:24,590 --> 00:16:27,080
Uh, and so that's, that's
how that, that's how that is.

279
00:16:27,140 --> 00:16:28,070
And we can use different things.

280
00:16:28,071 --> 00:16:31,290
We can use convolutional nets
for images and we can use, uh,

281
00:16:31,700 --> 00:16:33,530
all sorts of things like that.
So

282
00:16:35,860 --> 00:16:36,850
that is that.

283
00:16:37,120 --> 00:16:42,120
And now what I want to do is I want to
talk about the data generation part.

284
00:16:43,900 --> 00:16:46,060
Okay?
So let's get to the data generation part.

285
00:16:48,820 --> 00:16:50,500
Okay.
So for the data generation part,

286
00:16:52,000 --> 00:16:56,620
I'm going to talk about how we're going to
do this. Okay. So the first step is, oh,

287
00:16:56,650 --> 00:16:58,930
the hyper parameters. So
those are our models, right?

288
00:16:58,931 --> 00:17:02,260
So these are our hyper parameters
for those models. The first,

289
00:17:02,470 --> 00:17:04,750
and I'm going to answer
questions in five minutes.

290
00:17:05,020 --> 00:17:09,160
So the first set of hyper parameters,
standard hyper parameters,

291
00:17:09,161 --> 00:17:13,590
and they apply to our, uh, uh,
generators and or discriminators.

292
00:17:13,600 --> 00:17:15,340
These are general use cases,
right?

293
00:17:15,460 --> 00:17:18,430
Because we're going to have some weight
sharing happening between these models.

294
00:17:18,640 --> 00:17:22,150
Some of these, some of these models like
the two first generators share weights.

295
00:17:22,750 --> 00:17:24,260
And what this does is it,
it's just a lot.

296
00:17:24,370 --> 00:17:28,300
It's just improved across the main
generation. So then we have discriminator,

297
00:17:28,330 --> 00:17:31,930
you know, uh, hyper parameters and
then generate our hyper parameters.

298
00:17:32,140 --> 00:17:33,760
And then I have this inference network.

299
00:17:33,850 --> 00:17:36,280
So the reason we call it an inference
network, it's just a generator.

300
00:17:36,340 --> 00:17:39,430
It's just the generator. But we're calling
it an inference network just for like,

301
00:17:40,670 --> 00:17:41,503
uh,

302
00:17:41,750 --> 00:17:44,000
having that difference. So
we don't get confused, right?

303
00:17:44,001 --> 00:17:45,380
So we can just call it generator again,

304
00:17:45,410 --> 00:17:48,680
but we want to call it something different
just so we don't get super confused

305
00:17:48,681 --> 00:17:51,630
because there are a lot of terms,
right? Encoder, decoder, generator,

306
00:17:51,750 --> 00:17:56,660
adversary discriminator. So there's
like a lot of terms a, but it's two g,

307
00:17:56,820 --> 00:18:00,600
two generator pairs, like January janitor,
January, January two. Discriminators.

308
00:18:01,050 --> 00:18:01,883
That's it.
Okay.

309
00:18:01,920 --> 00:18:05,460
So these are our hyper parameters and
these are the hyper parameters that were

310
00:18:05,461 --> 00:18:09,180
implemented in the original
paper. Okay. So then, uh,

311
00:18:09,210 --> 00:18:11,610
the next step is first who
save our results to our folder,

312
00:18:11,611 --> 00:18:13,830
our disco again folder whenever,
whenever we're done.

313
00:18:14,100 --> 00:18:15,960
So we'll create a directory for that.
Okay.

314
00:18:15,961 --> 00:18:19,800
So then how are we going to generate this
data? Are we going to use images? No,

315
00:18:19,801 --> 00:18:23,310
we're going to use a more
simple dataset because it's, uh,

316
00:18:23,400 --> 00:18:26,370
and I'll show you the results from
the image generation and, and stuff.

317
00:18:26,371 --> 00:18:30,390
But for this, you know, just to, to
get, uh, grokking of the architecture,

318
00:18:30,570 --> 00:18:33,360
we're going to do some really simple
datasets and that's what we're going to

319
00:18:33,361 --> 00:18:36,480
generate it. Okay. Let me show you
what it looks like when we generate it,

320
00:18:36,660 --> 00:18:39,930
but we're going to use something called
a golf Sian mixture model to generate

321
00:18:39,931 --> 00:18:44,820
this data. Okay. So when,
what we're going to do is to,

322
00:18:45,010 --> 00:18:49,450
is we're going to generate data using a
golf Sian mixture model. Okay. And these,

323
00:18:49,780 --> 00:18:52,390
there are two data sets,
right? There are two datasets.

324
00:18:52,480 --> 00:18:54,760
There's the Red Dataset and
there's the green dataset.

325
00:18:54,940 --> 00:18:57,730
And we want to generate that using
gosh, in mixture model. So what is this,

326
00:18:57,731 --> 00:19:01,390
what is a golf swing mixture model?
Well, recall that Gosselin is a,

327
00:19:01,450 --> 00:19:03,520
is the bell curve, right? It's a, it's,

328
00:19:03,521 --> 00:19:07,390
it's a distribution between two uh,
numbers.

329
00:19:07,420 --> 00:19:10,390
It's a distribution in
two and an interval pair.

330
00:19:10,810 --> 00:19:14,860
A mixture model uses several golf scions.
Okay.

331
00:19:14,861 --> 00:19:19,060
He uses several golf seems to generate
data and some of that data might be, uh,

332
00:19:19,870 --> 00:19:23,080
it uses those distributions,
settled distributions to generate data.

333
00:19:23,470 --> 00:19:26,830
And why do we use Garcia mixture
models? Uh, and so like this,

334
00:19:26,831 --> 00:19:27,461
this data right here,

335
00:19:27,461 --> 00:19:30,460
it was generated using a handoff Sian
mixture model and we saved it from a

336
00:19:30,461 --> 00:19:33,190
previous training session. This is
what our data is that will look like.

337
00:19:33,460 --> 00:19:37,300
And then what we want to do is we can
consider these two different domains and

338
00:19:37,301 --> 00:19:38,620
we want to find the intersection.

339
00:19:38,770 --> 00:19:42,500
So whenever we're going to generate
new images that are intersected, uh,

340
00:19:42,700 --> 00:19:44,530
between these two domains,
so where would they be?

341
00:19:44,680 --> 00:19:47,650
They would end up being like right here.
So here's what it would look like.

342
00:19:47,651 --> 00:19:51,160
Once we're done with this test Dataset,
it will look like this.

343
00:19:51,340 --> 00:19:55,780
Let me just pull up this pdf.
You see these red values,

344
00:19:57,600 --> 00:20:01,440
he's red values. That's what we're going
to generate. And what this shows is it,

345
00:20:01,441 --> 00:20:02,430
it is a proof,

346
00:20:02,600 --> 00:20:06,630
is that proof that we can generate new
images that are across domain, that is,

347
00:20:06,631 --> 00:20:09,180
they apply to both
different both datasets.

348
00:20:09,870 --> 00:20:14,870
And then we literally just replace the
datasets with something more interesting.

349
00:20:14,881 --> 00:20:16,500
And we can do all sorts of cool things,

350
00:20:16,501 --> 00:20:21,060
but let's focus on the architecture
right now. Okay. So, yeah.

351
00:20:21,061 --> 00:20:24,360
So the way that's, that's how we
generate this using gosh and Mr models.

352
00:20:24,361 --> 00:20:25,194
And you might be wondering,

353
00:20:25,320 --> 00:20:27,950
well is there another way for us to
generate this without using, gosh,

354
00:20:27,951 --> 00:20:32,610
your mixture models. Sure. You
could just use a, you know,
a golf Sian alone. But uh,

355
00:20:32,670 --> 00:20:36,150
what this does is it's,
it's better for clustering.

356
00:20:36,180 --> 00:20:39,690
It's better for finding relationships
between the two data points.

357
00:20:39,930 --> 00:20:41,310
If we're using a mixture model,

358
00:20:41,370 --> 00:20:44,460
there's already an inherent relationship
between the two data points.

359
00:20:44,550 --> 00:20:46,300
And what this does is it,
it,

360
00:20:46,720 --> 00:20:51,040
it gives us some ground truth from
which we can generate data from. Okay.

361
00:20:51,041 --> 00:20:54,100
So this is the formula for a gosh,
you mixture model and

362
00:20:55,850 --> 00:20:58,310
here's what it looks like.
We say the probability of x,

363
00:20:58,370 --> 00:21:00,810
that is some data set of elements.
What,

364
00:21:00,811 --> 00:21:04,730
what we're going to generate depends
on these three variables right here.

365
00:21:05,000 --> 00:21:08,840
The mean, okay. The uh,

366
00:21:08,870 --> 00:21:13,240
the mixing weights and then the,
the wolves.

367
00:21:13,250 --> 00:21:17,300
This one Alpha, which is the, no,

368
00:21:17,301 --> 00:21:19,490
so the variance,
the,

369
00:21:19,780 --> 00:21:20,613
yeah,

370
00:21:22,030 --> 00:21:24,460
I wrote these down. So this is going
to be the variance, the mixing, wait,

371
00:21:24,461 --> 00:21:28,690
and then the, and then there's the sigma,
which is the, the density function.

372
00:21:28,720 --> 00:21:32,440
That's what it is. Okay, so this,
okay, so what we say is we say,

373
00:21:32,500 --> 00:21:34,000
and we're going to write
this out programmatically,

374
00:21:34,240 --> 00:21:36,620
what we say is sigma notation for k were,

375
00:21:36,621 --> 00:21:40,750
wereK is a number of elements in our
dataset. We first applied this alpha,

376
00:21:41,050 --> 00:21:45,130
this Alpha value, which is,
which is a mixing wait, which
we can decide beforehand.

377
00:21:45,400 --> 00:21:49,190
And then this end looking thing means
the golf Sian distribution. So to golf,

378
00:21:49,200 --> 00:21:52,180
Sian,
probably distribution of x,

379
00:21:52,300 --> 00:21:54,760
which is our data set of elements given

380
00:21:56,560 --> 00:22:00,190
our mean and our variants. Okay,
what were the square of our variance?

381
00:22:00,370 --> 00:22:03,490
And so we need these three values and
mean the barriers and the mixing weights

382
00:22:03,640 --> 00:22:08,080
to be able to generate a data
points in this distribution. Okay?

383
00:22:08,081 --> 00:22:09,460
So that's what we're going to,
we're going to do here.

384
00:22:09,461 --> 00:22:12,460
But let me first answer some
questions here. Uh, okay.

385
00:22:13,670 --> 00:22:14,490
Okay.

386
00:22:14,490 --> 00:22:19,490
So how to determine the number of hidden
layers required for a particular deep

387
00:22:20,941 --> 00:22:23,040
neural net question. Okay.
So that's a great question.

388
00:22:23,160 --> 00:22:26,910
How to determine the number
of layers in a deep net.

389
00:22:27,450 --> 00:22:32,450
The way we do that is by looking at what
has been done before in papers and then

390
00:22:33,451 --> 00:22:38,451
reimplementing that because hyper
parameter optimization is still a field of

391
00:22:40,921 --> 00:22:41,754
discovery.

392
00:22:41,790 --> 00:22:46,380
We need more algorithms to learn how
to learn hyper parameters, right?

393
00:22:46,381 --> 00:22:50,100
One way to do this manually and since
you just have the same model and each

394
00:22:50,101 --> 00:22:53,580
model has a different set
of hyper parameters and
then we run them on the same

395
00:22:53,600 --> 00:22:57,600
dataset and see which one works best,
right? In parallel distributed training,

396
00:22:57,810 --> 00:22:59,730
right?
And then see which one works best.

397
00:22:59,940 --> 00:23:01,890
And that's essentially what
a lot of machine learning is.

398
00:23:01,891 --> 00:23:04,950
It's just like different people trying
out different sense of hyper parameters.

399
00:23:05,160 --> 00:23:10,080
And there's not like one true way of
doing it, right? It's use case dependent.

400
00:23:10,081 --> 00:23:15,081
And I think it definitely
needs to be better,

401
00:23:15,300 --> 00:23:16,980
right?
It needs to be more efficient.

402
00:23:16,981 --> 00:23:21,340
It needs to be something that we learn to
learn. And that's actually, you know, um,

403
00:23:21,420 --> 00:23:25,160
my video is coming out on this, so
it's literally in a few days, uh,

404
00:23:25,170 --> 00:23:27,180
which is really cool.
And then two more questions.

405
00:23:29,800 --> 00:23:30,633
Uh,

406
00:23:35,790 --> 00:23:40,640
looks like I lost my connection
here. Uh, so it's taking a well,

407
00:23:40,700 --> 00:23:42,710
but I see it,
these two older questions.

408
00:23:42,860 --> 00:23:47,060
So can we use tensorflow for
you? Can't. Absolutely. I mean,

409
00:23:47,210 --> 00:23:52,040
a lot of great researchers use
tensorflow for research and it's, uh,

410
00:23:52,780 --> 00:23:56,310
it's been used a lot, but, but although
Pi Torch has been used a lot more now for,

411
00:23:56,311 --> 00:24:00,590
for, for research by serious
researchers. So. Cool.

412
00:24:00,591 --> 00:24:03,260
So that's it for the questions.
Let me get back to the code. Okay.

413
00:24:03,261 --> 00:24:06,940
So for the GAA Sian mixture model,
um, we're going to, we're still,

414
00:24:06,950 --> 00:24:08,810
we're still good, right guys?
Where's the lighting hunger? Great.

415
00:24:09,110 --> 00:24:12,760
So let's go ahead and do this. So the
first step is for us to generate our, uh,

416
00:24:12,950 --> 00:24:13,783
intervals,
right?

417
00:24:13,910 --> 00:24:17,480
Defining our intervals that we're going
to generate this gossipy mixture model

418
00:24:17,481 --> 00:24:20,510
from.
So each of these sets of intervals is a,

419
00:24:20,720 --> 00:24:24,350
is we can consider them components,
right? We call them components, right?

420
00:24:24,351 --> 00:24:26,630
So from zero to zero,
from two to two,

421
00:24:26,750 --> 00:24:29,180
from negative one to negative one
from one tonight and one, right?

422
00:24:29,210 --> 00:24:34,060
So when we define that using the
lambda or anonymous function, uh,

423
00:24:34,150 --> 00:24:39,050
given x. So we say we want to create a
num, py, IRA from these intervals. Okay.

424
00:24:39,080 --> 00:24:43,100
And then we're going to store that
in means. So that's our set of means.

425
00:24:43,310 --> 00:24:47,120
We convert that to a list and it's just a,
and why do we convert it to a list?

426
00:24:47,121 --> 00:24:50,540
Because we can perform a lot of
interesting operations with the list.

427
00:24:50,541 --> 00:24:54,940
The list is a great data type in, in
python. And then we say, well, here's the,

428
00:24:55,040 --> 00:24:57,530
here's our standard deviation that
we defined. We want it to be point,

429
00:24:57,770 --> 00:25:01,610
0.1 and then our variances or
we're going to say, so the,

430
00:25:01,880 --> 00:25:05,570
so the eye function of num Pi
returns an identity matrix,

431
00:25:05,660 --> 00:25:07,780
which is a two d array with, with, uh,

432
00:25:07,790 --> 00:25:10,190
one's on the diagonal and
an zero everywhere else.

433
00:25:10,370 --> 00:25:11,990
And so why do we need that identity?

434
00:25:11,991 --> 00:25:15,740
Major CS are necessary to
generate variances, variances.

435
00:25:15,800 --> 00:25:19,730
So we take the identity matrix and we
multiply it by the standard deviation for

436
00:25:19,731 --> 00:25:22,880
every value in the means.
So for all of those components,

437
00:25:23,120 --> 00:25:27,290
and that's going to give us our, our
variances, right? Then we're going to,

438
00:25:27,360 --> 00:25:30,230
we're going to use our PR,
we're going to generate our prior,

439
00:25:30,470 --> 00:25:33,850
which is this value right up here,
which is our prior beliefs.

440
00:25:33,851 --> 00:25:36,800
So when it comes to probability theory,
we have a prior,

441
00:25:36,830 --> 00:25:39,410
which is your belief before
you come into a problem.

442
00:25:39,411 --> 00:25:43,490
So before you examine a problem,
it's a Bayesian approach, right?

443
00:25:43,491 --> 00:25:47,240
Bayesians believe in priors, frequent
his stone. It's a huge debate.

444
00:25:47,420 --> 00:25:52,230
It's an ongoing debate. So
it's a holy war. Uh, but yes,
I, I followed the Bayesean

445
00:25:54,470 --> 00:25:55,760
Basie and logic.
Okay.

446
00:25:55,761 --> 00:26:00,530
So for priors we're going to say one
divided by the means for all of the means

447
00:26:00,531 --> 00:26:04,430
is going to give us our
prior value. Okay. Uh,

448
00:26:04,431 --> 00:26:07,760
and so then we're going to use those
values that we just computed to generate

449
00:26:07,790 --> 00:26:10,400
our, generate our golf
Sian mixture model. Okay.

450
00:26:10,640 --> 00:26:14,450
And then once we have our
golf Sian mixture model,
we're going to sample from it.

451
00:26:14,570 --> 00:26:16,610
Okay. So we've defined what
this model looks like, this,

452
00:26:16,640 --> 00:26:17,990
this architecture and this,
this,

453
00:26:17,991 --> 00:26:20,780
this structure from which we
can now generate data from.

454
00:26:20,930 --> 00:26:23,600
We've defined our parameters for it
and we can generate data from it.

455
00:26:23,601 --> 00:26:27,680
And so this is our helper function to do
that. And we'll get to our dataset. Okay.

456
00:26:27,770 --> 00:26:31,010
And then we will save the data set and I,

457
00:26:31,011 --> 00:26:36,011
and which I visualize and I showed you
guys and then we're going to plot it

458
00:26:37,250 --> 00:26:40,190
showed you guys, and then we're going
to store the samples and the labels.

459
00:26:40,370 --> 00:26:44,110
So this is interesting. So is
we have samples, the labels, uh,

460
00:26:44,250 --> 00:26:47,310
but we're not going to use the samples,
they're just there. So we have just,

461
00:26:47,540 --> 00:26:49,860
and so we can think of this
as x and y points, right?

462
00:26:49,861 --> 00:26:54,820
The samples or the x points and the, and
the labels or the y points. Okay. Uh,

463
00:26:54,930 --> 00:26:57,480
and so we'll do that twice
for two different datasets.

464
00:26:57,630 --> 00:27:02,010
We have one data set that we call x and
we had the other that we call Z, right?

465
00:27:02,011 --> 00:27:05,280
Two different datasets.
And so we all have both of the datasets.

466
00:27:05,310 --> 00:27:08,940
We just repeat the process twice and
then we can sample from them. Right?

467
00:27:08,941 --> 00:27:13,380
So we have eventually we'll take both a
non pyre raise for both sets of data and

468
00:27:13,381 --> 00:27:16,080
we'll have our x Dataset
Nrz did. I said, okay,

469
00:27:16,110 --> 00:27:21,110
so that's our data and now we
can start defining our networks,

470
00:27:21,481 --> 00:27:24,450
our model. Okay, so they, they
looked at this problem, right?

471
00:27:24,451 --> 00:27:28,140
And so they were considering like
what type of model should we use?

472
00:27:28,140 --> 00:27:28,861
And we had this,

473
00:27:28,861 --> 00:27:32,850
we just had this interesting thought
exercise earlier on in this live stream

474
00:27:32,851 --> 00:27:35,130
where I talked about,
well should we use an encoder decoder?

475
00:27:35,220 --> 00:27:37,110
Should we use to in Cody cutters,

476
00:27:37,230 --> 00:27:40,320
should we use to encoder decoder
is within our adversarial approach.

477
00:27:40,680 --> 00:27:42,360
They had the same thought process,
right?

478
00:27:42,361 --> 00:27:45,360
And so this happens a lot in
research where we are like thinking,

479
00:27:45,361 --> 00:27:48,330
well what if this would be the way I
know it would if this would be the way,

480
00:27:48,510 --> 00:27:51,240
well let's just try out all the
ways and see what works best.

481
00:27:51,360 --> 00:27:55,350
That's exactly what they did. So they,
so they tried out one very simple model,

482
00:27:55,530 --> 00:27:56,363
another model,

483
00:27:56,400 --> 00:27:59,130
and then they came to this model that
we're going to hit them it and they found

484
00:27:59,131 --> 00:28:03,900
that this far outperformed the other
models and this is just a thing in machine

485
00:28:03,901 --> 00:28:08,901
learning and deep learning where it seems
like the more complexity we add to a

486
00:28:09,571 --> 00:28:13,710
model, the better the
results are. Generally,

487
00:28:13,770 --> 00:28:15,540
obviously there are going to be anomalies,

488
00:28:15,750 --> 00:28:17,760
no pun intended when it
comes to anomaly detection,

489
00:28:17,761 --> 00:28:21,360
but they're going to be an old lease.
But generally the more complexity we add,

490
00:28:21,361 --> 00:28:25,050
the more we chain these models together,
the more interesting the results are.

491
00:28:25,390 --> 00:28:30,300
Wavenet is a great example,
a pixel RNN pixel,

492
00:28:30,301 --> 00:28:34,560
CNN and a path net.
They're a bunch of examples,

493
00:28:34,561 --> 00:28:38,370
but that's what our brain is, right? We
don't just have a convolutional network,

494
00:28:38,460 --> 00:28:41,250
we have a convolutional net for our
eyes and we have a recurrent net in our

495
00:28:41,260 --> 00:28:45,780
hippocampus and we have feet forward
nets happening probably in our somewhere

496
00:28:45,781 --> 00:28:48,260
else, but we have all sorts of networks
that are happening, right? As we,

497
00:28:48,390 --> 00:28:50,520
when we combine them,
we get an interesting result.

498
00:28:50,850 --> 00:28:55,440
And when the network observes itself
and it goes through this loop of self

499
00:28:55,740 --> 00:28:59,010
observation and just leave the loop
just keeps going faster and faster.

500
00:28:59,160 --> 00:29:02,700
That's when we get consciousness.
But that's a whole different thing.

501
00:29:02,760 --> 00:29:06,360
Godel Escher, Bach, lots of
philosophy there. Okay. So

502
00:29:08,620 --> 00:29:12,820
where were we? Consciousness.
No. Yeah. Okay. This, this go
again. Just go again. Okay.

503
00:29:12,821 --> 00:29:16,090
So here we are. Uh, okay. Where were we?

504
00:29:16,330 --> 00:29:19,930
So this is another image that they had
in the paper where they said, okay,

505
00:29:20,140 --> 00:29:23,320
we have our, our datasets. And so
let's talk about the failure cases,

506
00:29:23,321 --> 00:29:26,920
what we don't want. So if we look
here at this and make this bigger,

507
00:29:27,570 --> 00:29:30,130
I know I answer questions.
I know I haven't enhanced
your questions in a while.

508
00:29:30,640 --> 00:29:31,630
Uh,
so,

509
00:29:32,560 --> 00:29:33,340
okay.

510
00:29:33,340 --> 00:29:34,480
So sometimes

511
00:29:37,360 --> 00:29:41,080
sometimes the internet doesn't
work, so that's okay. So where was,

512
00:29:43,550 --> 00:29:46,340
what we don't want is we
don't want the images,

513
00:29:46,610 --> 00:29:50,470
the same image to generate a dif,

514
00:29:50,540 --> 00:29:51,650
different images.

515
00:29:51,860 --> 00:29:55,940
We want us a straight
image to image mapping.

516
00:29:56,150 --> 00:30:00,590
What do I mean by that? Something that
is predictable. So we know that, uh,

517
00:30:04,040 --> 00:30:08,220
that there is no failure case where we
give it to different types of images and

518
00:30:08,221 --> 00:30:10,800
it will generate the same image, even
though these are two different images,

519
00:30:10,801 --> 00:30:12,060
right?
We want variety,

520
00:30:12,061 --> 00:30:15,870
we want novel images to be generated no
matter what kind of image we give it,

521
00:30:15,871 --> 00:30:19,500
right? We don't want the same mapping.
So those are failure cases that it's,

522
00:30:19,501 --> 00:30:22,680
and it's a good practice to think about.
Before we were building models,

523
00:30:22,730 --> 00:30:26,610
what are possible failure cases? What,
what do we don't want to happen? Okay?

524
00:30:26,611 --> 00:30:29,880
And it's good to just as a thought
exercise to map those out to, to,

525
00:30:29,881 --> 00:30:32,730
to put those down on paper. Okay? So

526
00:30:34,530 --> 00:30:35,363
that's what that is.

527
00:30:35,370 --> 00:30:39,900
And so now what we're gonna do is we're
going to define these networks, okay?

528
00:30:39,901 --> 00:30:43,810
So let's define these networks. So
remember, we have six networks, okay?

529
00:30:43,980 --> 00:30:48,980
And the way that I'm going to define
these is we're going to use tensorflow

530
00:30:48,990 --> 00:30:50,760
slim. Now, this is just beautiful, right?

531
00:30:50,761 --> 00:30:54,600
So I've never actually used tensorflow
slim in a, in a, in a video before,

532
00:30:54,870 --> 00:30:58,070
but it's, it's actually, you know,
it's a, it's a very beautiful, uh,

533
00:30:58,970 --> 00:31:01,380
a library of tensorflow because,

534
00:31:06,970 --> 00:31:11,560
because, uh, there are,
it's very compact and dense.

535
00:31:11,650 --> 00:31:15,220
So let me, let me answer two questions
before start starting to build this.

536
00:31:16,960 --> 00:31:20,920
Uh, let's see. One great question is

537
00:31:24,130 --> 00:31:28,390
how can we make money with this?
Okay. I mean, yeah, there's,

538
00:31:28,391 --> 00:31:29,500
there's a lot of potential here.

539
00:31:29,590 --> 00:31:34,590
What are some problems where people
right now need to visualize something but

540
00:31:35,291 --> 00:31:40,090
they, they can't, they, it takes a lot of
effort. The barrier to entry is very high.

541
00:31:40,091 --> 00:31:44,890
How can you reduce the barrier to entry
for say, designing or visualization?

542
00:31:45,070 --> 00:31:50,070
Cad engineers are very well paid experts
and there are very few of them and it

543
00:31:50,081 --> 00:31:54,760
takes a lot of time and energy and skill
and practice to be able to generate

544
00:31:54,761 --> 00:31:55,720
images with cad.

545
00:31:56,170 --> 00:31:59,590
What if you use a generative adversarial
network to let anyone generate images

546
00:31:59,591 --> 00:32:03,460
with a text query? Okay. With
a simple description, boom,

547
00:32:03,610 --> 00:32:06,340
billion dollar idea,
one more billion dollar idea.

548
00:32:06,400 --> 00:32:07,810
What can I think of on the spot here?

549
00:32:08,110 --> 00:32:12,730
When it comes to a special effects
of know. Here's, here's a great one.

550
00:32:12,820 --> 00:32:13,900
When it comes to video effects,

551
00:32:13,901 --> 00:32:17,980
when it comes to after effects
or Photoshop or an editor,

552
00:32:18,020 --> 00:32:20,230
an editing program, it's
a bunch of effects, right?

553
00:32:20,590 --> 00:32:23,230
But none of them really use
generative adversarial networks.

554
00:32:23,380 --> 00:32:25,720
Imagine the effects that
you could create with this.

555
00:32:25,830 --> 00:32:28,270
It would blow whatever they
have that's like, you know,

556
00:32:28,271 --> 00:32:30,940
some kind of linear combination
under the hood, out of the water.

557
00:32:31,060 --> 00:32:35,020
You would just dazzle people. Okay? So
that's where we are right now. Okay.

558
00:32:35,230 --> 00:32:36,400
So anyway,

559
00:32:38,960 --> 00:32:39,793
yeah,

560
00:32:39,920 --> 00:32:43,550
hairstyle transfers. Not happening today.
I know, but uh, eventually it will happen.

561
00:32:44,060 --> 00:32:46,190
By the way, if you want to,
here's, here's what it is.

562
00:32:46,940 --> 00:32:50,000
A Dye your hair with
the silver stripe here,

563
00:32:50,900 --> 00:32:54,260
send me the picture on Twitter.
I'll retweet it. Okay. Just do it.

564
00:32:54,530 --> 00:32:56,960
It's going to be dope.
Uh, cool. So where are we?

565
00:32:58,250 --> 00:33:01,250
Everything about me is open source,
guys, my hair, all about everything.

566
00:33:01,280 --> 00:33:04,220
It's all open source. Take, take what
you need, take what you need. Okay,

567
00:33:04,221 --> 00:33:07,460
so the first thing is the generator.
So we have two generators here.

568
00:33:07,461 --> 00:33:10,850
And remember we're calling one generator
and we're calling one inference network

569
00:33:10,910 --> 00:33:13,580
just for the, just for a difference.
We could call a generator too,

570
00:33:13,730 --> 00:33:17,690
but they're both generators. Okay? So in
this case, we don't have images, right?

571
00:33:17,691 --> 00:33:20,330
We don't have images. So we're not
going to use convolutional blocks.

572
00:33:20,540 --> 00:33:24,620
We're going to use fully connected layers
because these are just numbers, right?

573
00:33:24,620 --> 00:33:26,840
This is, these are just
numbers who they are. This is,

574
00:33:26,930 --> 00:33:31,190
this is a set of feedforward networks.
What they set a fully connected layers.

575
00:33:31,340 --> 00:33:34,640
Okay? So, uh, how many layers do we want?

576
00:33:34,641 --> 00:33:36,380
What we define that right
here with number of layers.

577
00:33:36,620 --> 00:33:41,620
So the great thing about tensorflow slim
is that we can define a set of layers

578
00:33:43,640 --> 00:33:48,020
by one line of code. What do I mean
by that? We can say slim dot repeat.

579
00:33:48,110 --> 00:33:52,610
So given some data set z and then the
parameters for what we want the network to

580
00:33:52,611 --> 00:33:56,300
look like, we can say, okay, so
z equals h. Okay? So h is now,

581
00:33:56,301 --> 00:34:00,440
it's our data stored in h. So we'll
say slimmed RRP, take the Dataset,

582
00:34:00,740 --> 00:34:02,720
take the data that we input,
the input data,

583
00:34:03,080 --> 00:34:08,080
and then we want this many
number of layers type of lyric,

584
00:34:08,180 --> 00:34:11,570
fully connected with this
many hidden, uh, nodes.

585
00:34:11,690 --> 00:34:14,270
And then this is the activation function.
We want to apply to it.

586
00:34:14,271 --> 00:34:18,170
Relu and it does that, that number of
times. And then we had at the very end,

587
00:34:18,290 --> 00:34:22,130
one last fully connected layer and
that's returned the output of that.

588
00:34:22,190 --> 00:34:25,160
And that is our prediction would,
in this case, our generation,

589
00:34:25,370 --> 00:34:28,760
what we've generated. And it's going to
be a set of numbers in this case, right?

590
00:34:28,850 --> 00:34:30,950
Because these are numbers
that we're inputting.

591
00:34:31,400 --> 00:34:34,340
And so that's the great thing
about tensorflow slim. And if that,

592
00:34:34,341 --> 00:34:36,470
if that doesn't make sense,
then just like

593
00:34:39,430 --> 00:34:41,920
I forgot, there's no internet.
So, so that's what that is. Okay.

594
00:34:41,921 --> 00:34:45,190
So that's the first step. And uh,
we're going to do this twice, right?

595
00:34:45,191 --> 00:34:47,050
So we're going to do that for our
generator and we're going to do,

596
00:34:47,051 --> 00:34:48,490
that's for our inference network.

597
00:34:48,520 --> 00:34:52,540
We do it twice to set the fully connected
networks and that's gonna eventually

598
00:34:52,541 --> 00:34:56,470
generate our data, right? So
that's it for our generators.

599
00:34:56,471 --> 00:35:00,700
And then for our discriminators, our
discriminators, we're going to use a

600
00:35:02,260 --> 00:35:04,540
fully connected layers as well.
Okay.

601
00:35:04,600 --> 00:35:07,960
So both of these are going to
use fully connected layers and

602
00:35:10,040 --> 00:35:13,220
we're going to,
for the reason we concatenate x,

603
00:35:13,280 --> 00:35:17,300
which is our hand put data with one is
to approximate the log data density.

604
00:35:17,360 --> 00:35:20,340
And why do we want that? Well
we are just out putting a, uh,

605
00:35:20,990 --> 00:35:25,370
we're out putting a binary value, right?
Whether it's, whether it's real or fake,

606
00:35:25,730 --> 00:35:29,990
whether the image is real or fake. And in
this case real or fake is if it is, uh,

607
00:35:30,750 --> 00:35:33,810
if the,
if the generated image image is uh,

608
00:35:33,920 --> 00:35:38,610
from the true data distribution,
which is the original image data set,

609
00:35:38,611 --> 00:35:42,330
or it's from the not true data
distribution, which is something that's

610
00:35:42,630 --> 00:35:43,463
uh,

611
00:35:44,190 --> 00:35:48,120
a mix of the two. So we want to
optimize for the mix of the two, right?

612
00:35:48,150 --> 00:35:51,690
So we'll minimize the opposite. That's
what our discriminator does. It,

613
00:35:51,691 --> 00:35:55,770
it tries to judge if something is real.

614
00:35:55,771 --> 00:35:59,460
But real in this case is you can't
tell that it is a mix of the two.

615
00:35:59,490 --> 00:36:00,420
It just looks like it's a,

616
00:36:00,540 --> 00:36:04,440
an image of its own and it comes from
both distributions and you would think

617
00:36:04,441 --> 00:36:07,020
that it comes and he thinks that
it comes from both distributions,

618
00:36:07,230 --> 00:36:10,980
but it's actually a novel distribution.
I'm mixed distribution. So if it's a bad,

619
00:36:11,010 --> 00:36:13,110
if it's, if our, if our model is bad,

620
00:36:13,200 --> 00:36:16,950
it's going to be able to detect that
this sloppy purse looking shoe thing is

621
00:36:16,951 --> 00:36:20,070
actually fake, right? It's,
it's from a novel distribution.

622
00:36:20,160 --> 00:36:23,190
It's not as intertwined with the
other tooth distribution sets.

623
00:36:23,220 --> 00:36:24,510
So say liberal fake,

624
00:36:24,690 --> 00:36:29,250
but if it's really well generated in
that it is balanced in terms of the

625
00:36:29,251 --> 00:36:32,040
distributions that it's
generated from that you can,

626
00:36:32,070 --> 00:36:35,300
you can't even tell then it's
real. Okay. So that's what,

627
00:36:35,490 --> 00:36:40,380
that's what we mean by real in this case.
Real versus fake. Okay. So then, uh, yeah,

628
00:36:40,381 --> 00:36:42,030
they're there for the discriminator.

629
00:36:42,070 --> 00:36:44,760
There are also a set of fully
connected layers are both feet forward.

630
00:36:45,540 --> 00:36:50,540
And then we're going to output a TF
dot squeeze for the generated data.

631
00:36:51,540 --> 00:36:54,960
And then we say the reason we're using
squeeze is because it removes the

632
00:36:54,961 --> 00:36:59,730
dimensions of size one from the
shape of a, of a tensor. Uh, cool.

633
00:36:59,970 --> 00:37:04,840
So that's it for our generator. Okay.

634
00:37:04,870 --> 00:37:07,630
And our discriminator is plural.
Okay.

635
00:37:07,690 --> 00:37:10,000
So then I could code these out,

636
00:37:10,001 --> 00:37:14,140
but let me answer some questions
to see what the deal is here.

637
00:37:14,760 --> 00:37:17,650
Uh, who else? We got? Q
and Q and. A. All right,

638
00:37:17,651 --> 00:37:20,260
so we have some people has it for tonight.
So here's what the Q and a is.

639
00:37:20,680 --> 00:37:22,450
How can you use deep learning?

640
00:37:22,480 --> 00:37:26,740
How can you use deep learning for
detecting a spoofed frank fingerprint?

641
00:37:27,910 --> 00:37:31,600
A spoofed fingerprint.
So you would have to have a Dataset of,

642
00:37:34,720 --> 00:37:36,520
oh, interesting. That's
an interesting problem.

643
00:37:36,700 --> 00:37:41,700
So it depends on who's the,

644
00:37:41,830 --> 00:37:45,190
who's the spoofing that we are trying to
predict, right? If it, if it's one person,

645
00:37:45,191 --> 00:37:46,210
it's very easy,
right?

646
00:37:46,330 --> 00:37:49,240
Because you just have a bunch of samples
of their real fingerprint, right?

647
00:37:49,241 --> 00:37:53,740
So it's a supervised learning problem, a
supervised classification problem, right?

648
00:37:53,741 --> 00:37:56,710
So we have a supervised classification
problem, left one person's fingerprint,

649
00:37:56,950 --> 00:37:59,710
and then you would just say,
these are all the real ones.

650
00:37:59,711 --> 00:38:02,410
So you train it on the real ones
where they're labeled railroad real.

651
00:38:02,740 --> 00:38:05,320
And then ideally you could,
here's what I would do.

652
00:38:05,321 --> 00:38:08,920
I would take those real images and
then I'd apply some kind of, uh,

653
00:38:09,490 --> 00:38:13,330
some distribution on those images so that
it would create some buried version of

654
00:38:13,331 --> 00:38:14,170
them, some, you know,

655
00:38:14,171 --> 00:38:17,290
so maybe even use the generative
adversarial network to generate fake but

656
00:38:17,291 --> 00:38:20,800
realistic looking images of that Dataset,
but then label them fake. So then you,

657
00:38:20,820 --> 00:38:23,620
you've tried at home what's real and
what's fake and given a fake one,

658
00:38:23,621 --> 00:38:28,180
you could just say that that's fake.
Okay. Okay. So then two more questions.

659
00:38:29,590 --> 00:38:30,423
Uh,

660
00:38:32,310 --> 00:38:36,430
can I use a Gan for anime
related purposes? Yes, you can.

661
00:38:36,520 --> 00:38:40,060
So literally, literally on my
Internet's not working right now,

662
00:38:40,061 --> 00:38:44,320
but literally just search anime Gan
like one word and you will find a,

663
00:38:44,350 --> 00:38:47,650
I swear to God or result on get hub for
this. I was actually looking at this,

664
00:38:47,830 --> 00:38:49,570
but it generates animate characters using,

665
00:38:50,080 --> 00:38:54,280
using the generative adversarial network.
And then one more question is, uh,

666
00:38:56,270 --> 00:38:59,000
what approach should I
follow to get a job in this?

667
00:38:59,420 --> 00:39:02,270
So that's a good question as well.
That's a good question as well.

668
00:39:02,600 --> 00:39:07,600
So a good approach to follow if you want
to get a job in this is whether you're

669
00:39:07,821 --> 00:39:10,850
in college or whether you are,
you have a full time job.

670
00:39:11,030 --> 00:39:14,120
You need to find the time to learn
this stuff. So that's step one.

671
00:39:14,360 --> 00:39:16,190
Find the time to learn this stuff.

672
00:39:16,460 --> 00:39:21,460
So dedicate some set amount of
time every day to just study this,

673
00:39:21,741 --> 00:39:25,610
whether it be papers, whether it be
code. Here's the, here's the best way,

674
00:39:26,600 --> 00:39:30,950
besides consuming content, my
videos, besides consuming content,

675
00:39:31,190 --> 00:39:35,090
you need to try to reimplement papers.
That's going to be really hard at first.

676
00:39:35,270 --> 00:39:37,670
But eventually, like anything,
you're going to get better at it.

677
00:39:37,780 --> 00:39:40,670
Whenever you keep doing,
you get better at whether it's kickboxing,

678
00:39:40,671 --> 00:39:43,760
whether it's machine learning, whatever
you keep repeating, you get better at.

679
00:39:43,790 --> 00:39:48,320
That's how our brain works. So just keep
looking at code, keep writing out code,

680
00:39:48,350 --> 00:39:51,200
keep trying to read them and papers
and eventually before you know it,

681
00:39:51,230 --> 00:39:53,810
you won't even realize it. You're
like, oh, now I'm an expert.

682
00:39:54,200 --> 00:39:57,350
It happens really fast, but you have
to put in the time and the effort.

683
00:39:57,380 --> 00:39:59,540
That's all you need with the Internet.
Okay.

684
00:40:01,200 --> 00:40:04,320
I might freestyle at the end depending on
how I feel. Okay, cool. So back to this.

685
00:40:04,410 --> 00:40:05,280
So I was going to code this,

686
00:40:05,281 --> 00:40:09,450
but honestly there is a
lot like the Internet's not
working so I'm just going to

687
00:40:09,780 --> 00:40:10,950
look at this and it's all good though.

688
00:40:10,951 --> 00:40:15,951
So here's how it is for our
last functions and our networks,

689
00:40:17,520 --> 00:40:20,710
we define the functions for our networks,
right? We've defined them there.

690
00:40:20,711 --> 00:40:23,890
They're done. Now we can actually
start implementing our functions. So,

691
00:40:23,940 --> 00:40:25,770
so check this out.
Okay. So check this out.

692
00:40:26,400 --> 00:40:29,010
First we're going to define placeholders
for both of our data sets, right?

693
00:40:29,011 --> 00:40:30,540
We have x and we have a seat.

694
00:40:30,720 --> 00:40:34,620
Remember we generated both using a
Gaussian mixture model. Those are,

695
00:40:34,770 --> 00:40:37,210
those are both of our data sets.
We've got a data sets,

696
00:40:37,410 --> 00:40:39,090
let's feed them into our model.
That's the next step.

697
00:40:39,450 --> 00:40:41,850
So now we'll define both far generators.

698
00:40:41,970 --> 00:40:43,950
And remember we defined
them both respectively.

699
00:40:44,010 --> 00:40:47,370
One is the generative network and
what is he? Inference. Network. Okay.

700
00:40:48,000 --> 00:40:52,080
He's our incoders, right? So these are
incoders. They are, where are they?

701
00:40:52,081 --> 00:40:56,060
Where are they? They are right here.

702
00:40:56,390 --> 00:41:01,250
One the blue one right here,
and this yellow orange one right here.

703
00:41:01,760 --> 00:41:03,770
That's what we've just defined.
Okay.

704
00:41:04,310 --> 00:41:09,170
Those are our encoders in coder
generators. Okay. And then

705
00:41:10,620 --> 00:41:15,240
now talk about more use cases after this
we've got both of our encoders and now

706
00:41:15,241 --> 00:41:17,670
we're going to define our discriminators,

707
00:41:18,000 --> 00:41:20,550
which were the two discriminators
on the side here. Okay,

708
00:41:20,700 --> 00:41:24,690
so and did to generate these, we gave
it the two respective data sets, right?

709
00:41:24,691 --> 00:41:28,290
Z and x and then our hype and then our
hyper parameters that we defined forehand.

710
00:41:29,190 --> 00:41:32,610
Then we have our discriminators and we're
going to use data network x and then

711
00:41:32,611 --> 00:41:35,930
data network Z, right? We have two
different discriminators and we'll just,

712
00:41:35,931 --> 00:41:36,950
what we're going to feed it,

713
00:41:39,310 --> 00:41:42,920
what we're going to do is we're going
to feed it are the outputs of our

714
00:41:42,921 --> 00:41:45,980
generators, right? We wanna, we
wanna feed it the output, right?

715
00:41:45,981 --> 00:41:48,620
What have we generated? I,
let's see if it's real or fake.

716
00:41:48,630 --> 00:41:52,760
Let's discriminate if you're real or fake.
So that's what we feed both of these.

717
00:41:53,030 --> 00:41:58,030
And the reason we use graph replace here
on are our discriminators a is because

718
00:41:58,700 --> 00:42:03,620
it is going to help us
calculate our loss function.

719
00:42:03,830 --> 00:42:05,210
Because we need two of these.
We need,

720
00:42:05,211 --> 00:42:09,320
it's essentially a copy in the computation
graph so that we can do this next

721
00:42:09,321 --> 00:42:14,180
step and you'll see why we
create two discriminators
and then two copies of them.

722
00:42:14,420 --> 00:42:15,770
So then we could then,
uh,

723
00:42:15,950 --> 00:42:19,880
compute the soft plus
function on all four of them.

724
00:42:20,120 --> 00:42:22,010
It's just too discriminators.
But there copies of each.

725
00:42:22,400 --> 00:42:26,810
And if soft plus function is taking the
law of probability of the exponential,

726
00:42:26,900 --> 00:42:31,220
the exponent of our features plus one,
which is an activation function. Okay,

727
00:42:31,221 --> 00:42:34,460
so we, and that's what the soft
plus function I'll tension does.

728
00:42:34,940 --> 00:42:39,800
So we have four a sigmoid is
that we've calculated. Okay.

729
00:42:40,550 --> 00:42:44,690
And then we'll take those
values and we'll use those c.

730
00:42:44,691 --> 00:42:48,740
And this is why we will just add them
together for the original and then the

731
00:42:48,741 --> 00:42:51,830
copy to Compute d code or loss.

732
00:42:51,980 --> 00:42:55,310
And then our encoder loss
for both discriminators.

733
00:42:55,460 --> 00:43:00,460
So we have the laws for
both discriminators and
then we will combine them to

734
00:43:02,091 --> 00:43:06,050
have one big huge discriminator loss.

735
00:43:06,320 --> 00:43:10,370
And we say let's get the average of the
encoder loss minus the average of the

736
00:43:10,371 --> 00:43:14,450
decoder loss. So we take the difference
between the two. What is the difference?

737
00:43:15,110 --> 00:43:17,180
What is the difference here?
Okay,

738
00:43:17,210 --> 00:43:22,210
between what is the original image
and what is the generated image?

739
00:43:23,540 --> 00:43:27,800
Original data distribution generated data,
data distribution, find a difference,

740
00:43:27,890 --> 00:43:30,470
minimize it.
We went there to be no difference.

741
00:43:30,530 --> 00:43:34,250
So it looks very similar
statistically speaking,

742
00:43:34,251 --> 00:43:38,930
to hit to the discriminator. And
that's gonna be our discriminator loss.

743
00:43:39,080 --> 00:43:43,250
Okay? And then we've
defined our loss function,

744
00:43:43,251 --> 00:43:46,280
redefine our discriminators
and then two of our generators.

745
00:43:46,490 --> 00:43:47,390
Now what do we have to do?

746
00:43:47,600 --> 00:43:52,600
We did you find our generator loss
and then our two next generators,

747
00:43:53,090 --> 00:43:54,710
right?
So that's what we're going to do now.

748
00:43:54,711 --> 00:43:57,680
So now we have two more
generators and these same deal.

749
00:43:57,710 --> 00:43:59,750
We've got an inference that
work and are generative network.

750
00:43:59,780 --> 00:44:03,050
And what do we feed it in the
output of the previous generators,

751
00:44:03,051 --> 00:44:07,130
the incoders px and Qsi.
You remember these were right up,

752
00:44:08,600 --> 00:44:12,980
where were they? Right up here.
See Peak px and Qsi. We'd,

753
00:44:12,990 --> 00:44:17,150
if we feed those right into
our next set or coders,

754
00:44:17,300 --> 00:44:18,830
and what does that look like?
That looks like,

755
00:44:19,700 --> 00:44:24,170
that looks like this decoder,
one decode or two.

756
00:44:24,680 --> 00:44:27,950
That's it. We define all six of
them. See how simple that was.

757
00:44:27,951 --> 00:44:31,140
We define all six of them.
Now where was I?

758
00:44:33,150 --> 00:44:37,830
Where was I? Lots of TF
warnings. Warning, warning. Okay,

759
00:44:38,640 --> 00:44:39,451
we've got a generator generators,

760
00:44:39,451 --> 00:44:42,390
we've got our discriminators and now we
want to compute the generator loss we

761
00:44:42,391 --> 00:44:44,880
have are discriminated or loss.
Let's compute the generator loss.

762
00:44:45,090 --> 00:44:49,320
So what do we do? We compute the
sum of squared errors. All right,

763
00:44:49,321 --> 00:44:51,660
so we have generator one,

764
00:44:52,560 --> 00:44:56,640
the compute that computed the generated
the ultimate generated product, right?

765
00:44:56,670 --> 00:45:00,960
That's what rex Z, the reconstruction
of z and reconstruction of Xr.

766
00:45:01,380 --> 00:45:05,280
Those are the ultimate outputs of
our network. The ultimate outputs,

767
00:45:05,430 --> 00:45:10,430
what that mixture of both image data
sets and we want to compute the loss and

768
00:45:12,601 --> 00:45:16,270
that is our predicted
output. The generated output
minus the original Dataset.

769
00:45:16,290 --> 00:45:19,260
So the difference squared and the average.

770
00:45:19,320 --> 00:45:24,240
So the sum of squared errors for
both datasets Casi and then cost x.

771
00:45:25,650 --> 00:45:29,310
And then we compute the advantage loss,

772
00:45:29,340 --> 00:45:33,330
which basically ties into discriminator
loss into the generators loss.

773
00:45:33,420 --> 00:45:37,320
But finding the average of those,
all of the points in that vector,

774
00:45:37,350 --> 00:45:40,560
the average of all those. And we use
that to compute the generator loss,

775
00:45:40,770 --> 00:45:43,950
which is one times the,
uh, advantage loss plus one

776
00:45:45,870 --> 00:45:50,870
times the cost plus one times
the cost of the other image.

777
00:45:51,690 --> 00:45:52,523
So two,

778
00:45:52,560 --> 00:45:57,150
two costs for two image data sets and
that's gonna give us our final generator

779
00:45:57,180 --> 00:45:59,940
loss and I'll answer questions
right after this. Uh,

780
00:45:59,990 --> 00:46:04,860
what after I get done with this
block right here, so then, uh,

781
00:46:04,861 --> 00:46:08,010
we are, we are going to want to
update our, our weights, right?

782
00:46:08,700 --> 00:46:12,090
We want to use our gradients to update
our weights. So that's what we do.

783
00:46:12,091 --> 00:46:17,091
We say we'll use the get collection
method to say within TensorFlow's

784
00:46:17,190 --> 00:46:21,900
computation graph,
I want you to get these values that are,

785
00:46:21,930 --> 00:46:25,590
that have the trainable variable.
And then we define what that variable is,

786
00:46:25,591 --> 00:46:28,650
inference generative discriminator.
And we define those up here.

787
00:46:28,950 --> 00:46:31,500
We define those appear seat,
the scopes.

788
00:46:31,830 --> 00:46:35,860
That's how we call the scopes really
easily. Very, very, very uh, good, uh,

789
00:46:35,940 --> 00:46:39,450
programming syntax.
Syntactic sugar is the, uh,

790
00:46:40,110 --> 00:46:43,410
official term for it. Very,
very cool term. Thank you.

791
00:46:43,411 --> 00:46:48,000
Francoise Chalet for continuously
using that term. Okay, so,

792
00:46:48,060 --> 00:46:51,490
uh, creator of chaos. So the,

793
00:46:51,590 --> 00:46:55,290
so now let's optimize. We've got our,
we've got, we've defined our networks,

794
00:46:55,530 --> 00:46:57,360
we define our loss functions.
Let's optimize,

795
00:46:57,361 --> 00:47:00,120
we'll use Adam for grading
dissent to optimize it,

796
00:47:01,320 --> 00:47:03,540
and then we can minimize both losses,

797
00:47:03,541 --> 00:47:06,450
the minimize the loss for the generator
and then minimize the loss for the

798
00:47:06,451 --> 00:47:09,630
discriminator using the generators
loss and the discriminators loss.

799
00:47:09,840 --> 00:47:13,050
And then what are the values that we
want to update when we can compute our

800
00:47:13,051 --> 00:47:17,430
gradients? What do we want to back
propagate drone network? Well,

801
00:47:17,460 --> 00:47:19,770
these are the,
these are the variables that will,

802
00:47:19,800 --> 00:47:24,800
that will point our point tensor flow
to the memory location of where we these

803
00:47:25,321 --> 00:47:30,160
networks are. Okay, so that's that part.
And let me talk about the training part.

804
00:47:30,161 --> 00:47:30,461
Now we're,

805
00:47:30,461 --> 00:47:33,970
we're done with that part now where
the last part is the training part,

806
00:47:34,240 --> 00:47:38,740
but let me answer some
questions. Let's see. Okay. So

807
00:47:40,760 --> 00:47:43,220
can again be used for audio generation?
Yes.

808
00:47:43,250 --> 00:47:44,800
Gains can be used for audio generation.

809
00:47:44,810 --> 00:47:49,070
That is a novel field and not many
people have attempted that before.

810
00:47:49,250 --> 00:47:50,510
Wave Net was,

811
00:47:50,690 --> 00:47:54,680
it was a great attempt at generating
audio in the style of someone else.

812
00:47:54,860 --> 00:47:56,870
It didn't use a generative
adversarial network though.

813
00:47:56,960 --> 00:48:01,010
So there's a lot of possibility here
that there's so much you could do with

814
00:48:01,011 --> 00:48:03,820
audio generation generating
speech of the style.

815
00:48:03,840 --> 00:48:08,580
Somebody else generating
music and the song.

816
00:48:09,020 --> 00:48:12,080
That's a great one as well. Generative
adversarial networks for music generation.

817
00:48:12,320 --> 00:48:15,890
How cool would that be? Han
Zimmer. Han Zimmer music. Okay,

818
00:48:16,130 --> 00:48:17,900
two more question questions.

819
00:48:20,170 --> 00:48:22,300
Saroj is wrong about textbooks.
Here's the thing.

820
00:48:23,800 --> 00:48:25,510
People have different learning styles.

821
00:48:25,540 --> 00:48:29,440
People have different learning styles
in the age of the Internet where our

822
00:48:29,441 --> 00:48:34,150
attention is split 300 x, right? We pay,

823
00:48:34,840 --> 00:48:35,860
we split our attention.

824
00:48:35,980 --> 00:48:39,570
I saw some statistic recently where it
was like even 10 years ago, our spa,

825
00:48:39,571 --> 00:48:43,780
our attention was split across maybe two
or three different domains across the

826
00:48:43,780 --> 00:48:48,130
entire day, but now it's split across
300 different domains. Twitter, Facebook,

827
00:48:48,190 --> 00:48:52,300
snapchat, life, girlfriend, boyfriend.
This that. There's so many things, right?

828
00:48:52,450 --> 00:48:57,450
So a textbook requires a lot of
attention and a lot of focus,

829
00:48:58,690 --> 00:49:03,520
but we live in an age of instant
gratification. Let's be real. Right?

830
00:49:03,550 --> 00:49:06,970
So we want that instant gratification.
We get that from everywhere else.

831
00:49:07,060 --> 00:49:09,370
We get that from, you know,
mindless entertainment.

832
00:49:09,490 --> 00:49:12,190
We can get that from education
to that's all I'm saying.

833
00:49:12,730 --> 00:49:15,430
But if a textbook works for
you, it works for you. Right.

834
00:49:15,640 --> 00:49:19,150
But I'm just saying that if you are
trying out the textbook for learning and

835
00:49:19,151 --> 00:49:22,810
it's just not clicking, there are other
ways. It's not like the way to learn.

836
00:49:24,100 --> 00:49:28,900
Two more questions. Can we detect
movement in video with Gan? Yes, you can.

837
00:49:29,260 --> 00:49:31,540
Detecting movement is a vague term,

838
00:49:31,870 --> 00:49:35,500
but detecting the direction that
someone is moving. Sure you can.

839
00:49:35,770 --> 00:49:37,000
You can generate,

840
00:49:39,120 --> 00:49:39,950
okay.

841
00:49:39,950 --> 00:49:42,350
Different possibilities of
given some set of frames,

842
00:49:42,440 --> 00:49:45,950
what is going to be the possible next
frames than possible next range and then

843
00:49:46,130 --> 00:49:50,840
and then take those generated frames
across all the generated frames.

844
00:49:50,841 --> 00:49:53,540
Like he moves this way. He was
this way, it moves this way.

845
00:49:53,810 --> 00:49:56,330
Vectorize all those vector vector,
vector vector a vector,

846
00:49:56,540 --> 00:50:01,070
and then compute the distance between
those vectors, perhaps a co-sign, uh,

847
00:50:01,550 --> 00:50:05,300
the coastline distance or you know,
one of, one of the distance functions.

848
00:50:05,540 --> 00:50:06,373
And then,

849
00:50:09,490 --> 00:50:14,350
and then multiply those by some
distance vector and you can see a set of

850
00:50:14,351 --> 00:50:17,590
possible distances and also
get the most likely distance.

851
00:50:18,850 --> 00:50:21,300
That's never been done before.
I just, I just made that, uh,

852
00:50:21,340 --> 00:50:24,480
can you suggest me an NLP books Raj?
Uh,

853
00:50:25,500 --> 00:50:30,020
an LP.
Stanford has a great course.

854
00:50:30,200 --> 00:50:33,470
Michael Collins is a great,
my favorite NLP professor,

855
00:50:33,530 --> 00:50:38,500
he's a professor at Columbia who had now
has an online course on Coursera and a,

856
00:50:38,501 --> 00:50:42,930
a textbook NLP textbook.
Um,

857
00:50:43,460 --> 00:50:47,460
bishop, if you, okay, if you want a
textbook then and uh, bishops, uh,

858
00:50:47,500 --> 00:50:50,480
pattern recognition book is great.
Great section on NLP in there.

859
00:50:51,020 --> 00:50:55,520
One more question. This is fire.
I know it's fire. What is the,

860
00:51:02,410 --> 00:51:06,400
can you show us how to do facial
recognition next? Yeah, I will eventually.

861
00:51:06,850 --> 00:51:11,680
What happens when none of the Ganz
generated images can be classified as a

862
00:51:11,681 --> 00:51:14,230
positive match within one
of the initial data sets?

863
00:51:17,560 --> 00:51:19,150
What,
uh,

864
00:51:20,050 --> 00:51:23,170
what happens if none of the generated
images from again can be classified as a

865
00:51:23,171 --> 00:51:27,040
positive match within one of
the initial data sets? Oh yeah.

866
00:51:27,041 --> 00:51:29,080
So if you're Gan is bad,
like it's not predicting,

867
00:51:29,081 --> 00:51:33,400
well then you tooth, I mean, there's
several things that could be happening,

868
00:51:33,401 --> 00:51:35,020
but it's usually going to be two things.
One,

869
00:51:35,230 --> 00:51:38,390
you got to change your loss function,
try out a different loss function, right?

870
00:51:38,391 --> 00:51:39,850
And your loss function
is just not working.

871
00:51:40,030 --> 00:51:41,500
And it's a very common thing with gans,

872
00:51:41,620 --> 00:51:45,490
although a Wasserstein gans
definitely improve on this. Uh,

873
00:51:45,510 --> 00:51:47,740
so that's one possibility
change a loss function,

874
00:51:47,980 --> 00:51:52,510
but the other is change your Dataset,
add more data and more data. Okay.

875
00:51:52,540 --> 00:51:54,910
So that's it for that.
And so now let's look at the training.

876
00:51:55,420 --> 00:51:58,600
And for training we're going to say,

877
00:52:00,430 --> 00:52:02,050
let's begin our session.

878
00:52:02,110 --> 00:52:05,950
And then inside of our session we're
going to say stable from both data sets.

879
00:52:05,980 --> 00:52:08,350
So from our hex state of saffron,
from Rosita Dataset,

880
00:52:08,500 --> 00:52:12,100
we use our shuffle function to
sample some random values from them.

881
00:52:12,460 --> 00:52:14,770
And then we're going to say,
okay, so for each of those values,

882
00:52:15,280 --> 00:52:16,960
for each of those values,
so we're going to do,

883
00:52:17,140 --> 00:52:22,120
we train this six network
model simultaneously. So
it's happening all at once,

884
00:52:22,210 --> 00:52:25,030
right? It's not like we have some
session and we have another session.

885
00:52:25,170 --> 00:52:29,050
We've announced no, it's one session
and we train them simultaneously.

886
00:52:29,260 --> 00:52:31,410
So we say for,
uh,

887
00:52:33,100 --> 00:52:35,530
for each of those data sets,
let's compute the,

888
00:52:37,170 --> 00:52:39,160
let's compute the,
uh,

889
00:52:39,190 --> 00:52:42,370
let's minimize a loss for
both our discriminator.

890
00:52:42,400 --> 00:52:45,550
So this is for a discriminator
feeding hand, both,

891
00:52:45,910 --> 00:52:47,800
both sets of data x andZ ,

892
00:52:48,520 --> 00:52:52,420
and then for our generator feeding
in, again, both sets of data.

893
00:52:52,560 --> 00:52:55,330
And so you might be thinking, well,
shouldn't this be generated or loss?

894
00:52:55,870 --> 00:53:00,010
These three components make up the
generator lost the advantage of loss,

895
00:53:00,220 --> 00:53:04,180
the cost of x and the cost of Z.
Recall up here.

896
00:53:04,870 --> 00:53:06,670
See,
these are the three components.

897
00:53:06,760 --> 00:53:09,590
We could just say January
generator loss as well, but, um,

898
00:53:10,240 --> 00:53:14,500
this is how they implement it in the
paper when in Pi Torch, interestingly,

899
00:53:14,560 --> 00:53:17,900
I mean, but you could say to January
or loss as well. Uh, this isn't it.

900
00:53:17,950 --> 00:53:20,530
There's no reason to do
this specifically. Uh, so

901
00:53:22,480 --> 00:53:26,790
let's, uh, let me just compile
this whole thing and, and, and see,

902
00:53:27,120 --> 00:53:31,710
see what's good. See what happens. Bop,
Bop, Bop, compile, compile, compile,

903
00:53:31,711 --> 00:53:34,170
compile,
and then we get to the end.

904
00:53:43,660 --> 00:53:46,990
Boom training. Just like that
training, training, training.

905
00:53:47,200 --> 00:53:49,780
2% now too.
So he's going to take a while.

906
00:53:49,781 --> 00:53:53,920
So I'm gonna stop it before
my computer gets crazy. Crazy.

907
00:53:53,921 --> 00:53:58,240
Like Charles Barkley. No
Way. Charles. Gnarly. GNARLS
Barkley. I was a great song.

908
00:53:58,450 --> 00:54:02,650
Where was I? Oh, okay. So open this.

909
00:54:03,910 --> 00:54:08,080
Let's look at our results. The results
are going to be saved. We're not,

910
00:54:08,140 --> 00:54:12,250
we're not showing them in the tension
flow graph. They're gonna be saved. And

911
00:54:19,260 --> 00:54:23,340
so check out this,
check out our loss function.

912
00:54:23,920 --> 00:54:27,510
It's like not doing that thing where
it has to go down. It's like going,

913
00:54:28,140 --> 00:54:32,130
it's like going wildly crazy for both
the generator handed discriminator.

914
00:54:32,340 --> 00:54:35,730
So that's just, that's generative
adversarial networks for you.

915
00:54:35,880 --> 00:54:38,670
Although I'm going to talk
about Wasserstein Gans and
show how they improve on

916
00:54:38,671 --> 00:54:40,630
this,
but like Tuck to sell,

917
00:54:45,840 --> 00:54:50,070
right? So we have our true samples
and then we have our inferred samples.

918
00:54:50,250 --> 00:54:55,250
So the inferred samples right here showed
that the combination of the two and so

919
00:54:55,471 --> 00:54:59,840
it just plots that it just creates
those novel red points in from,

920
00:54:59,841 --> 00:55:03,420
from the two initial
data distributions and

921
00:55:07,120 --> 00:55:10,090
yeah, more or less simple
simple idea there. But uh,

922
00:55:10,150 --> 00:55:13,990
that's what the paper was all about.
And remember like when reading papers,

923
00:55:14,230 --> 00:55:17,140
it's really like, let me just, let me just
like, I think this will provide value.

924
00:55:17,141 --> 00:55:19,370
This will provide values to you guys.
So just kind of go for it.

925
00:55:19,380 --> 00:55:21,040
Like how I read papers really quickly.

926
00:55:21,220 --> 00:55:24,280
So the first thing I do is I definitely
read the abstract and the introduction

927
00:55:24,430 --> 00:55:27,670
because those are super simple, right?
It's just, it's just like, you know,

928
00:55:27,700 --> 00:55:30,250
motivations and why they're doing it.
Look at the image.

929
00:55:30,370 --> 00:55:33,040
Usually like 90% of the
time I'll be able to,

930
00:55:33,250 --> 00:55:37,600
you'll be able to Grok what the entire
paper does just by reading the first page.

931
00:55:37,750 --> 00:55:39,220
And a good paper should be like that.

932
00:55:39,221 --> 00:55:42,790
You should be able to understand at a
high level what is happening without

933
00:55:43,000 --> 00:55:45,040
having to look at the entire thing,
right?

934
00:55:45,040 --> 00:55:47,440
That's what the abstract is meant to do.
Okay.

935
00:55:47,441 --> 00:55:50,740
And some people say you read the
abstract at the end rather than at the

936
00:55:50,741 --> 00:55:55,150
beginning. Um, but, and so it depends.

937
00:55:55,480 --> 00:55:57,250
If you are trying to

938
00:55:59,320 --> 00:56:03,060
just get a quick idea, quick and dirty
idea of what the paper is, then I read,

939
00:56:03,070 --> 00:56:06,790
read the abstract at the beginning. Uh,
but some people say read it at the end,

940
00:56:07,140 --> 00:56:09,680
you know, all around. I would
say that's a bad idea. I just, I,

941
00:56:09,700 --> 00:56:12,340
this is how I read papers and
it's worked very well for me.

942
00:56:12,520 --> 00:56:16,090
So I'm going to speak on my own experience
here. So, so that's the first step.

943
00:56:16,091 --> 00:56:20,050
And then so then they'll talk about
their methodologies and bill talk about

944
00:56:20,080 --> 00:56:20,950
usually a diff,

945
00:56:21,010 --> 00:56:24,760
like many different methodologies and
many ideas that they're talking about.

946
00:56:24,940 --> 00:56:28,390
But what you can do is you can go
right to the meat of what it is.

947
00:56:28,450 --> 00:56:29,740
And if you read a lot of papers,

948
00:56:29,860 --> 00:56:32,890
you'll get better at this because right
here in the talking about notation and

949
00:56:32,891 --> 00:56:36,610
architecture, talking about a single
Gann with reconstruction loss,

950
00:56:36,730 --> 00:56:40,960
this is irrelevant to the actual,
um, the, the model that they built.

951
00:56:41,140 --> 00:56:45,490
But if you keep going, you'll
find our proposed model.

952
00:56:45,700 --> 00:56:48,750
This is the actual model. So
you can just, if you want to,

953
00:56:48,760 --> 00:56:50,710
you could skip the rest
and then just read this.

954
00:56:50,711 --> 00:56:54,220
And which is what I did at first. It's
the first pass. I'm like, oh, this is,

955
00:56:54,250 --> 00:56:55,090
this is what they built.

956
00:56:56,050 --> 00:56:58,300
And then we'll talk about their
experiments and they'll have many,

957
00:56:58,301 --> 00:56:59,950
many different steps for the experiment.

958
00:57:00,250 --> 00:57:02,650
And it just kind of like go through it
and you just want to get to the results.

959
00:57:02,800 --> 00:57:05,110
You want to know why did it,
why they made the paper,

960
00:57:06,220 --> 00:57:09,850
what's the model they used and
what are the results and once,

961
00:57:09,910 --> 00:57:12,540
so that's how I look at it.
And our first pass I was going.

962
00:57:12,550 --> 00:57:17,020
So I go all the way down here and I see
the results and see can I see that while

963
00:57:17,021 --> 00:57:20,260
they got state of the art, you
know, in, in these areas. And,

964
00:57:20,500 --> 00:57:22,090
and their conclusion is
like how they would improve.

965
00:57:22,390 --> 00:57:25,330
Once you got that first pass and the
first passion take you like maybe,

966
00:57:25,331 --> 00:57:29,740
I don't know, it takes me,
it took me like 25 minutes,

967
00:57:29,741 --> 00:57:30,940
30 minutes first pass.

968
00:57:31,210 --> 00:57:35,890
Then if you want to then go back and
really like read through all the details

969
00:57:36,160 --> 00:57:39,670
and once you read through all the details,
then look at the code. And there's an,

970
00:57:39,671 --> 00:57:43,360
a good paper always has associated
code. If you, if you are writing papers,

971
00:57:43,540 --> 00:57:45,640
you need to be publishing code.
Okay.

972
00:57:45,920 --> 00:57:48,220
You because you have to help
other people out this stuff.

973
00:57:49,390 --> 00:57:52,300
So that's how I read the paper for
this. Okay. So it's a very new stuff.

974
00:57:52,301 --> 00:57:55,780
It's very new paper and the
next live stream is going to be,

975
00:57:56,120 --> 00:57:56,953
uh,

976
00:57:58,550 --> 00:58:00,890
the last live stream and it will be for,

977
00:58:01,220 --> 00:58:04,910
for this course and it's going to
be a on not deep learning. It's a,

978
00:58:04,911 --> 00:58:06,200
it's a surprise.
Okay,

979
00:58:06,201 --> 00:58:09,200
so let me answer some ending
questions here before we're done.

980
00:58:12,030 --> 00:58:14,970
I'm Hagers all love the show. We have
an eight year old. That's awesome.

981
00:58:15,210 --> 00:58:18,720
That's awesome. Thanks for being
here. Is it the code in the paper?

982
00:58:18,750 --> 00:58:22,110
How do I begin reading source
code from a paper? So source code,

983
00:58:22,440 --> 00:58:24,780
he's usually not in the paper,
but it's on get hub.

984
00:58:24,930 --> 00:58:29,550
So what I do is when I went to find
source code is I will copy and paste the

985
00:58:29,551 --> 00:58:34,551
name of the paper like this into Google
or get hub sometimes get up and it's

986
00:58:35,641 --> 00:58:37,110
going to show up.
If they publish code.

987
00:58:37,230 --> 00:58:40,410
If it doesn't show up on Google's first
page, they haven't published the code.

988
00:58:40,860 --> 00:58:44,430
And so that's, that's, that's not
good. Okay. Two more questions.

989
00:58:44,970 --> 00:58:45,803
Uh,

990
00:58:46,450 --> 00:58:48,790
what's the source of
papers you're mentioning?

991
00:58:48,970 --> 00:58:52,240
A great source for the papers

992
00:58:54,390 --> 00:58:58,140
is archive sanity.
Check this out.

993
00:59:00,350 --> 00:59:03,260
Make that bigger is the
word archive sanity.

994
00:59:04,340 --> 00:59:06,050
Let me paste that right here.

995
00:59:11,030 --> 00:59:11,863
Where is it

996
00:59:14,470 --> 00:59:19,300
checked out? This, there's the link right
here. Archive Sanity and outcome. You can,

997
00:59:19,330 --> 00:59:19,931
you know what it does?

998
00:59:19,931 --> 00:59:23,780
It's a web app that looks at the best
papers for you and it just picks and you

999
00:59:23,781 --> 00:59:24,110
can just,

1000
00:59:24,110 --> 00:59:27,470
it just lists them chronologically and
you can look at what the top high papers

1001
00:59:27,471 --> 00:59:32,330
are. Great. Great tool. Okay,
so, uh, one more question.

1002
00:59:40,680 --> 00:59:44,430
Can we use RNN based architecture
and an adversarial setting,

1003
00:59:44,730 --> 00:59:49,730
but realistic text generation tasks mainly
are recurrent net architecture and an

1004
00:59:49,801 --> 00:59:54,120
adversarial setting protect
generation tasks. Absolutely.

1005
00:59:54,360 --> 00:59:57,150
Absolutely. So first of all,
recurrent nets for text generation.

1006
00:59:57,180 --> 01:00:00,630
It's been used before Shakespearian
text, uh, all sorts of texts,

1007
01:00:00,840 --> 01:00:04,230
well documented problem,
well defined solutions.

1008
01:00:04,330 --> 01:00:06,130
But you could do this
in 10 lines of chaos.

1009
01:00:06,300 --> 01:00:09,690
Andre [inaudible] is the king
of this, uh, lots of 10 lines,

1010
01:00:09,691 --> 01:00:11,490
snippets on the web to do this.

1011
01:00:11,760 --> 01:00:15,690
Very easy to do using LSTM recurrent
networks and an adversarial setting.

1012
01:00:19,220 --> 01:00:22,940
I haven't seen it,
but you could,

1013
01:00:22,970 --> 01:00:25,970
I would bet you could beat
the state of Vr if you tried.

1014
01:00:26,030 --> 01:00:28,850
I would bet you could because
I haven't seen it before.

1015
01:00:29,090 --> 01:00:32,900
And overall Gans are just blowing
the state of yard on many,

1016
01:00:33,200 --> 01:00:38,030
many different fields so that, that would
be on my bet. Okay. So enough of that.

1017
01:00:38,390 --> 01:00:43,050
Let me, uh, let me run me rap.
So I'm going to freestyle, uh,

1018
01:00:43,610 --> 01:00:46,120
so someone child to be,

1019
01:00:46,130 --> 01:00:48,440
and let me answer one question in the
meantime because it takes a while for that

1020
01:00:48,680 --> 01:00:51,900
question to come in. Uh,
one more question. Uh, and,

1021
01:00:52,070 --> 01:00:55,340
and it's guys shout out topics for
the freestyle by the way. Okay.

1022
01:00:55,950 --> 01:00:56,783
Uh,

1023
01:00:58,520 --> 01:01:01,580
can we train a net to
configure a good gan?

1024
01:01:04,590 --> 01:01:08,610
That's irrelevant. What was your
uni? I went to Columbia, but look,

1025
01:01:08,670 --> 01:01:13,230
don't think like, Oh shit.
You know, he went to Columbia
like he's a, he's a genius.

1026
01:01:13,240 --> 01:01:18,060
I can't be like that guys. I
am just like you. I am not some

1027
01:01:19,700 --> 01:01:23,660
God. As much as I would like to say that
I am, I'm really not. I'm just like you.

1028
01:01:23,990 --> 01:01:27,820
I'm a human. I'm a, I'm a guy. You know,
I'm not some, I'm not, I'm, I don't know.

1029
01:01:27,830 --> 01:01:31,160
I don't have a phd or have, you know,
I'm not like sitting there, you know,

1030
01:01:31,161 --> 01:01:34,070
I'm just the guy who's really interested
in this stuff and that's all it takes

1031
01:01:34,310 --> 01:01:38,510
is an interest. Okay. Uh,
and, and a desire to learn.

1032
01:01:39,500 --> 01:01:41,870
Okay. So. All right, so we've got some.

1033
01:01:47,130 --> 01:01:52,060
Okay. All right. 10 Sir. All right,

1034
01:01:52,061 --> 01:01:54,370
so I'm going to, I'm going to
freestyle about tents. Hit it.

1035
01:01:56,310 --> 01:01:57,900
Can we go jazzy?

1036
01:01:57,910 --> 01:02:00,660
Tensors it's going to go down in a
second where I want to hear that.

1037
01:02:01,110 --> 01:02:05,510
I want to hear the beat drop.
It's all for Tensor, Huh?

1038
01:02:07,610 --> 01:02:11,510
It's going to come in a
second. Here we go. 10 Sir.

1039
01:02:12,300 --> 01:02:15,490
Here we hear we, we do. We,

1040
01:02:16,050 --> 01:02:20,190
I love 10 stores. Try to give back
and give you some a bit lectures.

1041
01:02:20,400 --> 01:02:24,720
Every day I come out, it's like
I'm a section leader of a person.

1042
01:02:24,840 --> 01:02:27,120
Every time I give it,
you don't even know if I'm leading.

1043
01:02:27,360 --> 01:02:31,500
I see these different variables
every single day. I see
you like one, two, three,

1044
01:02:31,501 --> 01:02:32,430
four.
Okay.

1045
01:02:32,610 --> 01:02:36,690
I think I can define a tense or out of
that my way or use it back to her class

1046
01:02:36,720 --> 01:02:41,070
analysis. Okay. There are different
ways of putting this out there. Anyway,

1047
01:02:41,190 --> 01:02:45,180
I'm going to talk about demo one, two,
and three. All right. Hey, drop the beat.

1048
01:02:45,420 --> 01:02:48,450
Drop it back. Like if you take a
seat, not while I'm rapping, man.

1049
01:02:48,451 --> 01:02:51,000
I'm not your enemy.
I see Jupiter notebooks.

1050
01:02:51,001 --> 01:02:52,920
Can't you see that's
the only way to do this.

1051
01:02:52,921 --> 01:02:55,680
You got a beat on top
whenever you will lease it.

1052
01:02:55,860 --> 01:02:59,070
Whenever you show it to the world.
Do you want to make sure it is creasing,

1053
01:02:59,190 --> 01:03:03,300
creasing, like paper? You fold it,
you show it to professors like you're

1054
01:03:05,040 --> 01:03:09,320
the man. That's it for this. That's it
for the, that's it really. The beat.

1055
01:03:09,410 --> 01:03:11,450
The beat didn't drop,
but I just roll with it.

1056
01:03:11,451 --> 01:03:15,950
You know the beat in life will
drop when you ask it to. Okay.

1057
01:03:16,100 --> 01:03:19,010
So that's it for the live stream.
Thank you everybody for watching.

1058
01:03:19,430 --> 01:03:21,340
And uh,

1059
01:03:21,350 --> 01:03:25,940
for now I've got to go create the finale.
And don't worry,

1060
01:03:26,000 --> 01:03:29,780
I have so much more to come. This, this is
nothing. I'm just getting started. Okay.

1061
01:03:30,020 --> 01:03:34,160
So I am just getting started.
Don't be worried like, oh no.
The course is, no, no, no,

1062
01:03:34,161 --> 01:03:37,970
no, no. I'm just getting started. Okay,
so for now I've got to write the finale,

1063
01:03:38,060 --> 01:03:39,140
so thanks for watching.

