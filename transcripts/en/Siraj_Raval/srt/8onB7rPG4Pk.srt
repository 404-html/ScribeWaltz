1
00:00:00,030 --> 00:00:04,080
Hello world is Saraj and the
mathematics of machine learning.

2
00:00:04,290 --> 00:00:08,550
Is it necessary to know math for
machine learning? Absolutely.

3
00:00:08,760 --> 00:00:13,080
Machine learning is math.
It's all math. In this video,

4
00:00:13,081 --> 00:00:18,081
I'm going to help you understand why we
use math in machine learning by example,

5
00:00:19,680 --> 00:00:24,680
machine learning is all about creating
an algorithm that can learn from data to

6
00:00:25,291 --> 00:00:26,490
make a prediction.

7
00:00:26,880 --> 00:00:31,880
That prediction could be what an object
and a picture looks like or what the

8
00:00:32,011 --> 00:00:37,011
next price for gasoline might be in
a certain country or what the best

9
00:00:37,171 --> 00:00:41,070
combination of drugs to cure
a certain disease might be.

10
00:00:41,490 --> 00:00:46,490
Machine learning is built on mathematical
prerequisites and sometimes it feels

11
00:00:47,761 --> 00:00:51,870
like learning them might be a
bit overwhelming, but it isn't.

12
00:00:52,110 --> 00:00:56,580
Or is it? No, it's not. As long as
you understand why they're used,

13
00:00:56,640 --> 00:00:59,310
it'll make machine
learning a lot more fun.

14
00:00:59,670 --> 00:01:04,650
You can have a full time job doing machine
learning and not know a single thing

15
00:01:04,680 --> 00:01:09,120
about the math behind the functions
you're using, but that's no fun is it?

16
00:01:09,240 --> 00:01:14,240
You want to know why something works and
why one model is better than another.

17
00:01:15,510 --> 00:01:19,710
Machine learning is powered
by the diamond of statistics.

18
00:01:19,860 --> 00:01:22,770
Calculus,
linear Algebra and probability.

19
00:01:23,130 --> 00:01:25,740
Statistics is at the core of everything.

20
00:01:25,860 --> 00:01:29,130
Calculus tells us how to
learn and optimize our model.

21
00:01:29,520 --> 00:01:34,520
Linear Algebra makes running
these algorithms feasible
on massive datasets and

22
00:01:35,701 --> 00:01:39,810
probability helps predict the
likelihood of an event occurring.

23
00:01:40,140 --> 00:01:43,500
So let's start from scratch
with an interesting problem.

24
00:01:43,920 --> 00:01:48,240
The problem is to predict the price
of an apartment in an up and coming

25
00:01:48,241 --> 00:01:52,050
neighborhood in New York City. Let's
say Harlem. Shout out to Harlem,

26
00:01:52,051 --> 00:01:53,430
Yo west side represent.
Okay.

27
00:01:53,940 --> 00:01:58,940
Let's say that all will know when we
eventually make a prediction is the price

28
00:01:59,490 --> 00:02:02,160
per square foot of a given apartment.

29
00:02:02,190 --> 00:02:06,960
That's the only marker we'll use to
predict the price of the apartment as a

30
00:02:06,961 --> 00:02:09,060
whole.
And lucky for us,

31
00:02:09,180 --> 00:02:13,560
we've got a Dataset of apartments
with two columns. In the first column,

32
00:02:13,740 --> 00:02:18,270
we've got the price per square foot
of an apartment. In the second column,

33
00:02:18,390 --> 00:02:21,120
we've got the price of
the apartment as a whole.

34
00:02:21,180 --> 00:02:25,560
There's got to be some kind of correlation
here and if we build a predictive

35
00:02:25,561 --> 00:02:26,280
model,

36
00:02:26,280 --> 00:02:31,280
we can learn what that correlation is so
that in the future if all were given is

37
00:02:31,441 --> 00:02:35,730
the price per square foot of a house,
we can predict the price of it.

38
00:02:35,790 --> 00:02:39,480
If we were to graph out this data,
let's graph this out with the x axis,

39
00:02:39,481 --> 00:02:42,390
measuring the price per
square foot and the y axis,

40
00:02:42,391 --> 00:02:46,110
measuring the price of a house.
It would be a scatter plot.

41
00:02:46,170 --> 00:02:51,170
Ideally we could find a
line that intersects as many
data points as possible and

42
00:02:51,241 --> 00:02:55,560
then we could just plug in some input
data into our line and outcomes.

43
00:02:55,561 --> 00:02:58,230
The prediction, poof. In mathematics,

44
00:02:58,260 --> 00:03:03,260
the field of statistics
acts as a collection of
techniques that extract useful

45
00:03:03,491 --> 00:03:04,930
information from data.

46
00:03:05,350 --> 00:03:09,550
It's a tool for creating an
understanding from a set of numbers.

47
00:03:09,850 --> 00:03:14,170
Statistical inference is the process
of making a prediction about a larger

48
00:03:14,171 --> 00:03:17,710
population of data based
on a smaller sample,

49
00:03:17,711 --> 00:03:22,711
as in what can we infer about a
population's parameters based on a sample

50
00:03:23,171 --> 00:03:26,680
statistic. Sounds pretty similar to what
we're trying to do right now. Right?

51
00:03:27,040 --> 00:03:29,230
Since we're trying to create a line,

52
00:03:29,410 --> 00:03:33,730
we'll use a statistical inference
technique called linear regression.

53
00:03:34,030 --> 00:03:39,030
This allows us to summarize and study
the relationship between two variables.

54
00:03:39,520 --> 00:03:44,080
A little mountain one variable x is
regarded as the independent variable.

55
00:03:44,350 --> 00:03:48,370
The other variable y is regarded
as the dependent variable.

56
00:03:48,670 --> 00:03:53,670
The way we can represent linear regression
is by using the equation y equals mx

57
00:03:54,610 --> 00:03:55,443
plus B.

58
00:03:55,570 --> 00:03:59,920
Why is the prediction x is the input B
as the point where the line intersects.

59
00:03:59,921 --> 00:04:02,440
The y axis and m is the slope of the line.

60
00:04:02,830 --> 00:04:06,820
We already know what the x value
would be and why is our prediction.

61
00:04:07,180 --> 00:04:12,180
If we had M and B we would have a full
equation plug and play easy prediction,

62
00:04:12,490 --> 00:04:15,370
but the question is how
do we get these variables?

63
00:04:15,730 --> 00:04:20,320
The naive way would be for us to just
try out a bunch of different values over

64
00:04:20,321 --> 00:04:24,850
and over and over again and plug
the line over time using our eyes.

65
00:04:24,880 --> 00:04:29,200
We could try and estimate just
how well fit the line we draw is,

66
00:04:29,500 --> 00:04:31,480
but that doesn't seem efficient does it?

67
00:04:31,630 --> 00:04:36,630
We do know there exists some ideal values
for M and B such that the line when

68
00:04:37,181 --> 00:04:41,710
drawn using those values would
be the best fit for our dataset.

69
00:04:41,980 --> 00:04:46,240
Let's say we did have a bunch of time
on our hands and we decided to try out a

70
00:04:46,241 --> 00:04:48,910
bunch of predicted values for M and B.

71
00:04:49,270 --> 00:04:53,170
We need some way of measuring how
good our predicted values are.

72
00:04:53,590 --> 00:04:56,440
Will need to use what's
called an error function.

73
00:04:56,800 --> 00:05:01,800
An error function will tell us how far
off the actual why value is from our

74
00:05:02,051 --> 00:05:02,980
predicted value.

75
00:05:03,340 --> 00:05:07,780
There are lots of different types of
statistical error functions out there,

76
00:05:07,900 --> 00:05:12,760
but let's just try a simple one called
least squares. This is what it looks like.

77
00:05:13,180 --> 00:05:17,680
We'll make an apartment price prediction
for each of our data points based on

78
00:05:17,681 --> 00:05:18,910
our own intuition.

79
00:05:19,330 --> 00:05:23,470
We can use this function to double
check against the actual apartment price

80
00:05:23,471 --> 00:05:27,670
value. It will subtract each
predicted value from the actual value.

81
00:05:28,150 --> 00:05:31,900
Then it will square each of
those differences. The sigma.

82
00:05:31,960 --> 00:05:35,410
That little [inaudible] looking thing
denotes that we're doing this not just for

83
00:05:35,411 --> 00:05:38,290
one data point,
but for every single data point.

84
00:05:38,291 --> 00:05:41,320
We have m data points to be specific.

85
00:05:41,650 --> 00:05:43,600
This is our total error value.

86
00:05:44,290 --> 00:05:46,690
We can create a three dimensional graph.

87
00:05:46,691 --> 00:05:49,420
Now we know the x axis and the y axis,

88
00:05:49,450 --> 00:05:53,020
they will be all the potential
m and B values respectively,

89
00:05:53,530 --> 00:05:55,120
but let's add another axis.

90
00:05:55,150 --> 00:06:00,150
The Z axis and on the axis would be all
the potential error values for every

91
00:06:01,101 --> 00:06:05,120
single combination of M and B.
If we were to actually graph this out,

92
00:06:05,150 --> 00:06:09,010
it would look just like this,
this kind of bowl like shape cup.

93
00:06:09,011 --> 00:06:12,200
It firmly in your hand like a nice bowl.

94
00:06:12,380 --> 00:06:16,820
If we find that data point at the bottom
of the bowl, the smallest error value,

95
00:06:16,910 --> 00:06:20,060
that would be our ideal m and B values.

96
00:06:20,120 --> 00:06:24,140
That would give us the line of best fit,
but how do we actually do that?

97
00:06:24,470 --> 00:06:28,310
Now we need to borrow from the
math discipline known as calculus,

98
00:06:28,460 --> 00:06:33,290
the study of change. It's
got an optimization technique
called gradient descent.

99
00:06:33,500 --> 00:06:37,460
That will help us discover the
minimum value iteratively it.

100
00:06:37,461 --> 00:06:41,720
We'll use the error for a given data point
to compute what's called the gradient

101
00:06:41,900 --> 00:06:46,310
of our unknown variable and we can
use the gradient to update to our two

102
00:06:46,311 --> 00:06:47,144
variables.

103
00:06:47,450 --> 00:06:52,450
Then we'll move on to the next data point
and repeat the process over and over

104
00:06:52,701 --> 00:06:56,300
and over again.
Slowly like a ball rolling down a bowl.

105
00:06:56,450 --> 00:06:59,150
We'll find what our minimum value is.
See,

106
00:06:59,180 --> 00:07:04,130
calculus helps us find the direction
of change in what direction should we

107
00:07:04,131 --> 00:07:09,131
change the unknown variables Mnb in our
function such that it's prediction is

108
00:07:09,231 --> 00:07:12,170
more optimal,
Aka the error is smallest,

109
00:07:12,500 --> 00:07:17,210
but apartment prices don't just depend
on the price per square foot. Right.

110
00:07:17,420 --> 00:07:21,920
Also included our different features like
the number of bedrooms and the number

111
00:07:21,921 --> 00:07:25,880
of bathrooms as well as the average
price of homes within a mile.

112
00:07:26,090 --> 00:07:28,790
If we factored in those features as well,

113
00:07:28,820 --> 00:07:31,220
our regression line would
look more like this.

114
00:07:31,460 --> 00:07:34,370
There are now multiple
variables to consider,

115
00:07:34,430 --> 00:07:38,000
so we can call it a
multivariate regression problem.

116
00:07:38,120 --> 00:07:42,980
The branch of math concerned with the
study of multivariate spaces and the

117
00:07:42,981 --> 00:07:47,000
linear transformations between
them is called Linear Algebra.

118
00:07:48,170 --> 00:07:52,670
It gives us a set of operations that we
can perform on groups of numbers known

119
00:07:52,671 --> 00:07:57,671
as matrices are training set now becomes
an m by eye and matrix of em samples

120
00:07:58,520 --> 00:08:01,940
that have I features instead of a
single variable with the weight.

121
00:08:02,060 --> 00:08:03,680
Each of the features has a weight,

122
00:08:03,920 --> 00:08:08,920
so that's an example of how three of the
four main branches of math dealing with

123
00:08:09,231 --> 00:08:13,610
machine learning are used. But what
about the fourth probability? All right,

124
00:08:13,611 --> 00:08:15,680
so let's just scratch this example.

125
00:08:15,710 --> 00:08:20,710
What if instead of predicting the price
of an apartment we went to predict

126
00:08:20,750 --> 00:08:23,810
whether or not it's in
prime condition or not,

127
00:08:24,110 --> 00:08:29,030
we want to be able to classify a house
with the probability of it being prime or

128
00:08:29,031 --> 00:08:32,900
not. Prime probability is the
measure of likelihood of something.

129
00:08:33,110 --> 00:08:37,010
We can use it probabilistic technique
called logistic regression to help us do

130
00:08:37,011 --> 00:08:38,510
this.
Since this time,

131
00:08:38,511 --> 00:08:43,250
our data is categorical as in it
has different categories or classes.

132
00:08:43,640 --> 00:08:48,020
Instead of predicting a
value, we're predicting the
probability of an occurrence.

133
00:08:48,080 --> 00:08:52,670
Since the probability goes between
zero and 100 we can't use an infinitely

134
00:08:52,671 --> 00:08:53,570
stretching line.

135
00:08:53,750 --> 00:08:57,870
We're left with some
threshold past some point x.

136
00:08:58,140 --> 00:09:01,200
We are more likely than not
looking at a prime house.

137
00:09:01,560 --> 00:09:06,090
We'll use an s shaped curve given
by the sigmoid function to do this.

138
00:09:06,420 --> 00:09:07,950
Once we optimize our function,

139
00:09:08,040 --> 00:09:12,840
we'll plug in input data and get a
probabilistic class value just like that.

140
00:09:12,900 --> 00:09:14,490
So to summarize,

141
00:09:14,760 --> 00:09:18,780
machine learning consist
mainly of statistics, calculus,

142
00:09:18,900 --> 00:09:21,240
linear Algebra,
and probability theory.

143
00:09:21,660 --> 00:09:26,250
Calculus tells us how to optimize linear
Algebra makes executing algorithms

144
00:09:26,251 --> 00:09:28,230
feasible on massive datasets.

145
00:09:28,380 --> 00:09:31,800
Probability helps predict the
likelihood of a certain outcome.

146
00:09:31,980 --> 00:09:34,530
And statistics tells us what our goal is.

147
00:09:34,830 --> 00:09:39,180
This week's coding challenge is to
create a logistic regression model from

148
00:09:39,181 --> 00:09:42,630
scratch in python on
an interesting dataset.

149
00:09:42,870 --> 00:09:46,320
Get humbling SCO in the common section.
And winners will be announced in a week.

150
00:09:46,380 --> 00:09:48,960
Please subscribe for more
programming videos. And for now,

151
00:09:49,110 --> 00:09:51,740
I've got to build a movement.
So thanks for walking.

