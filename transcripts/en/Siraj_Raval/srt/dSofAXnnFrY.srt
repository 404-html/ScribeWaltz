1
00:00:00,060 --> 00:00:01,680
Hello world.
It's a Raj.

2
00:00:01,740 --> 00:00:05,970
In this video we're going to use genetic
programming to identify if some energy

3
00:00:05,971 --> 00:00:10,320
is gamma radiation or not.
I'm getting angry gamma rays.

4
00:00:11,990 --> 00:00:16,780
No, I wish data science is a
way of thinking about discovery.

5
00:00:16,930 --> 00:00:21,450
A data scientist need to decide the
right question to ask like who's the best

6
00:00:21,451 --> 00:00:23,580
candidate to vote for in the U s election?

7
00:00:23,970 --> 00:00:28,740
Then decide what data set to use like
tweet history of candidates and pass

8
00:00:28,741 --> 00:00:31,170
endorsements of each candidate and lastly,

9
00:00:31,171 --> 00:00:35,490
decide what machine learning model to
use on the data to discover the right

10
00:00:35,491 --> 00:00:40,491
answer live goes on with the right data
computing power and machine learning

11
00:00:40,741 --> 00:00:41,400
model.

12
00:00:41,400 --> 00:00:46,350
You can discover a solution to any problem
but knowing which model to use can be

13
00:00:46,351 --> 00:00:49,800
challenging. For new data scientists.
There are so many of them.

14
00:00:50,040 --> 00:00:51,990
That's where genetic programming can help.

15
00:00:52,140 --> 00:00:56,460
Genetic Algorithms are inspired by the
Darwinian process of natural selection

16
00:00:56,700 --> 00:01:00,930
and their use of generate solutions
to optimization and search problems.

17
00:01:01,110 --> 00:01:05,250
They have three properties,
selection, crossover and mutation.

18
00:01:05,460 --> 00:01:09,480
You have a population of possible
solutions to a given problem and a fitness

19
00:01:09,481 --> 00:01:11,100
function.
Every iteration.

20
00:01:11,130 --> 00:01:14,970
We evaluate how fit each solution
is with our fitness function.

21
00:01:15,210 --> 00:01:20,040
Then we select the fitness
ones and perform crossover
to create a new population.

22
00:01:20,190 --> 00:01:25,170
We take those children and mutate them
with some random modification and repeat

23
00:01:25,171 --> 00:01:29,610
the process until we get the fittest
or best solution. So take this problem.

24
00:01:29,611 --> 00:01:33,360
For instance, let's say you want to take
a road trip across a bunch of cities.

25
00:01:33,540 --> 00:01:38,100
What's the shortest possible path you
could take to hit up each city once and

26
00:01:38,101 --> 00:01:39,720
then return back to your home city?

27
00:01:39,990 --> 00:01:44,250
This is popularly called the traveling
salesman problem in computer science and

28
00:01:44,251 --> 00:01:46,830
we can use a genetic
algorithm to help us solve it.

29
00:01:47,070 --> 00:01:48,840
Let's look at some high level python code.

30
00:01:48,900 --> 00:01:53,760
We have the number of generations set to
5,000 and the population size except to

31
00:01:53,761 --> 00:01:58,110
a hundred so we start by initializing
our population using our size parameter.

32
00:01:58,350 --> 00:02:01,860
Each individual in our population
represents a different solution path.

33
00:02:02,190 --> 00:02:03,540
Then for each generation,

34
00:02:03,600 --> 00:02:07,560
we compute the fitness of each solution
and story in our population fitness

35
00:02:07,590 --> 00:02:08,190
array.

36
00:02:08,190 --> 00:02:12,600
Now we'll perform selection by only
taking the top 10% of the population,

37
00:02:12,840 --> 00:02:16,920
which are our shortest road trips and
produce offspring from them by performing

38
00:02:16,921 --> 00:02:21,300
crossover. Then you take those offspring
randomly and repeat the process.

39
00:02:21,600 --> 00:02:23,190
As you can see in the animation,

40
00:02:23,340 --> 00:02:28,110
eventually we will get an optimal solution
using this process unlike apple maps.

41
00:02:28,190 --> 00:02:31,350
All right, so how does this all
fit into data science? Well,

42
00:02:31,351 --> 00:02:34,890
it turns out that choosing the right
machine learning model and all the best

43
00:02:34,891 --> 00:02:39,150
hyper parameters for that model
is itself an optimization problem.

44
00:02:39,210 --> 00:02:43,290
We're going to use a python library
called teapot built on top of psycho learn

45
00:02:43,380 --> 00:02:47,040
that uses genetic programming to
optimize our machine learning pipeline.

46
00:02:47,250 --> 00:02:49,200
So after formatting our data properly,

47
00:02:49,350 --> 00:02:53,130
we need to know what features to input
to our model and how we should construct

48
00:02:53,131 --> 00:02:55,410
those features.
Once we have those features,

49
00:02:55,500 --> 00:02:59,380
will input them into our model to train
on and will want to tune our hyper

50
00:02:59,381 --> 00:03:02,710
parameters or tuning knobs
to get the optimal results.

51
00:03:02,890 --> 00:03:05,620
Instead of doing this all
ourselves through trial and error.

52
00:03:05,740 --> 00:03:10,300
Teapot automates the steps for us which
genetic programming and it will output

53
00:03:10,301 --> 00:03:13,570
the optimal code for us plates
done so we can use it later.

54
00:03:13,690 --> 00:03:17,830
So we're going to create a classifier
for gamma radiation using teapot after

55
00:03:17,831 --> 00:03:20,770
installing our dependencies
and then analyze the results.

56
00:03:20,830 --> 00:03:23,980
Teapot is built on the popular psyche
learn machine learning library,

57
00:03:24,040 --> 00:03:26,710
so we'll want to make sure that
we have that installed first.

58
00:03:26,980 --> 00:03:30,850
Then we'll install Penn does to help us
analyze our data and num Pi to perform

59
00:03:30,851 --> 00:03:34,450
math calculations. Our first step
is to load our Dataset we use.

60
00:03:34,451 --> 00:03:39,010
Penn does read CSV method
and set the parameter to the
name of our safe CSV file.

61
00:03:39,250 --> 00:03:42,710
This is data collected from a scientific
instrument called a Charin [inaudible]

62
00:03:42,910 --> 00:03:46,810
telescope that measures radiation in
the atmosphere and these are a bunch of

63
00:03:46,811 --> 00:03:50,650
features of whatever type of
radiation it picks up. Thanks Putin.

64
00:03:51,010 --> 00:03:53,260
Since the class object
is already organized,

65
00:03:53,350 --> 00:03:55,300
we'll shuffle our data
to get a better result.

66
00:03:55,570 --> 00:03:59,260
The eye lock function of the telescope
variable is pandas way of getting the

67
00:03:59,261 --> 00:04:03,310
positions in the index and will
generate a sequence of random indices.

68
00:04:03,400 --> 00:04:07,030
The size of our data using the
permutation function of num Pi's,

69
00:04:07,031 --> 00:04:11,080
random submodule. Since all the
instances are now randomly rearranged,

70
00:04:11,260 --> 00:04:14,080
we'll just reset all the
indices so they are ordered.

71
00:04:14,110 --> 00:04:18,400
Even though the data is now shuffled using
the reset index method of pandas with

72
00:04:18,401 --> 00:04:23,170
the drop parameter set to true will now
let our Televerde will know what our two

73
00:04:23,171 --> 00:04:27,100
classes are by mapping both of them
to an integer with the map method.

74
00:04:27,250 --> 00:04:31,330
So g or gamma is at the zero and
h or Hadrian is that the one?

75
00:04:31,540 --> 00:04:33,280
Let's store those class labels,

76
00:04:33,281 --> 00:04:37,540
which we're going to predict in a separate
variable called teleclass and use the

77
00:04:37,541 --> 00:04:40,840
values attribute to retrieve them.
Before we train our model,

78
00:04:40,841 --> 00:04:44,200
we need to split our data into
training and validation sets.

79
00:04:44,410 --> 00:04:48,190
We'll use the train test split method
of psychic learn that we imported to

80
00:04:48,191 --> 00:04:52,210
create the indices for
both. The parameters will
be the size of our dataset.

81
00:04:52,420 --> 00:04:54,400
We want both sets to be a raise.

82
00:04:54,460 --> 00:04:58,240
So we'll set the stratified parameter
to our array type and we'll define what

83
00:04:58,241 --> 00:05:00,730
percent of our data we want
to be training and testing.

84
00:05:00,731 --> 00:05:02,260
With these last two parameters.

85
00:05:02,410 --> 00:05:06,010
We have a 75 25 split now in our data
and we're ready to train our model.

86
00:05:06,220 --> 00:05:09,760
We'll initialize the teapot variable
using the teapot class with a number of

87
00:05:09,761 --> 00:05:13,180
generations set to five on a standard
laptop with four gigs of Ram.

88
00:05:13,330 --> 00:05:17,260
It takes five minutes per generation to
run, so this will take about 25 minutes.

89
00:05:17,500 --> 00:05:21,640
This is so teapots genetic algorithm
knows how many iterations to run for and

90
00:05:21,641 --> 00:05:23,170
we'll set verbosity to two,

91
00:05:23,290 --> 00:05:26,830
which just means show up progress scarred
in terminal during the optimization

92
00:05:26,831 --> 00:05:29,950
process. Then we can call our
fit method on our training data,

93
00:05:30,070 --> 00:05:32,680
let it perform optimization
using genetic programming.

94
00:05:32,950 --> 00:05:35,020
The first parameter is
the training feature set,

95
00:05:35,140 --> 00:05:38,680
which were retrieved from our tele
variable along the first access for every

96
00:05:38,681 --> 00:05:41,800
training index. The secondary
level is our training class set,

97
00:05:41,920 --> 00:05:44,170
which will retreat from our
Televerde bubble like so.

98
00:05:44,380 --> 00:05:48,460
We can compute the testing error for
validation using teapot score method with

99
00:05:48,461 --> 00:05:52,390
validation feature set as the first
parameter and the validation class set as

100
00:05:52,391 --> 00:05:57,391
the second we'll export the computed
python code to the pipeline.py class using

101
00:05:57,621 --> 00:06:01,730
this method and name it in the perimeter
as a string. Let's demo this thing.

102
00:06:01,910 --> 00:06:05,930
After training we'll see that after five
generations she pot chose the gradient

103
00:06:05,931 --> 00:06:09,440
boosting classifier as the most
accurate machine learning model to use.

104
00:06:09,680 --> 00:06:13,520
It also shows the optimal hyper parameters
like the learning rate and number of

105
00:06:13,521 --> 00:06:16,200
estimators for us boy,

106
00:06:16,880 --> 00:06:20,660
so to break it down with the right amount
of Beta computing power and machine

107
00:06:20,661 --> 00:06:23,960
learning model, you can discover
a solution to any problem.

108
00:06:24,230 --> 00:06:27,620
Genetic Algorithms replicate
evolution via selection,

109
00:06:27,680 --> 00:06:32,600
crossover and mutation to find an optimal
solution to a problem and teapot is a

110
00:06:32,601 --> 00:06:37,280
python library that uses
genetic programming to help
you find the best model and

111
00:06:37,281 --> 00:06:39,260
hyper parameters for your use case.

112
00:06:39,470 --> 00:06:42,980
The winner of the coding challenge
from the last video is Peter Amatrano.

113
00:06:43,040 --> 00:06:47,120
He added some great deep dream samples
to is repository and even deep dream.

114
00:06:47,121 --> 00:06:51,260
My own video, bad ass of the week,
and the runner up is Kyle Jordan.

115
00:06:51,440 --> 00:06:55,130
Good job stitching all the deep dreams
frames together with one line of code.

116
00:06:55,280 --> 00:06:59,660
The challenge for this video is to use
teapot and a climate change dataset that

117
00:06:59,661 --> 00:07:03,110
are provide to predict the
answer to a question you decide.

118
00:07:03,160 --> 00:07:06,950
This will be great practice and learning
to think like a data scientist posts.

119
00:07:06,951 --> 00:07:10,040
You're getting humbling in the comments
and I'll announce the winner next time

120
00:07:10,220 --> 00:07:14,120
for now. I've got to stay fit to
reproduce, so thanks for watching.

