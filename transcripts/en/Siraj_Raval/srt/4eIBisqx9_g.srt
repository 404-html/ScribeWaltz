1
00:00:00,180 --> 00:00:04,470
Hello world. It's a Raj and
Yolo. You only look once.

2
00:00:04,500 --> 00:00:05,640
That is the demo.

3
00:00:05,641 --> 00:00:10,470
For today's video we're going to build
an object detection algorithm called

4
00:00:10,500 --> 00:00:11,160
Yolo.

5
00:00:11,160 --> 00:00:16,160
You only look once for real time video
so you can feed this thing on Avi Mov any

6
00:00:18,331 --> 00:00:22,980
kind of MP four file and it's going to
be able to in real time like this demo

7
00:00:22,981 --> 00:00:26,970
that you're seeing behind me that I'm
running off of my laptop in real time,

8
00:00:27,030 --> 00:00:29,820
be able to detect what objects are where.

9
00:00:29,970 --> 00:00:34,290
Now this is one of the most popular and
one of the most new techniques out there

10
00:00:34,440 --> 00:00:35,790
for object detection.

11
00:00:35,940 --> 00:00:39,810
So in this video I'm going to go over
a little bit of the history of object

12
00:00:39,811 --> 00:00:44,430
detection techniques just over the past
decade or so so we can really see why

13
00:00:44,460 --> 00:00:45,840
Yolo is so impressive.

14
00:00:46,080 --> 00:00:49,920
Then I'm going to go over the architecture
of Yolo and then at the end we're

15
00:00:49,921 --> 00:00:53,340
going to go through the code and we're
going to use it on both a pre trained

16
00:00:53,341 --> 00:00:57,720
model and we're going to learn how to
build our own Yolo model as well. Okay.

17
00:00:57,721 --> 00:01:00,510
So I hope you're excited
for this cause I am. Right.

18
00:01:00,511 --> 00:01:02,010
So let's get right on into it.

19
00:01:02,250 --> 00:01:06,360
So let's start off with some object
detection history starting from 2001 so in

20
00:01:06,361 --> 00:01:11,361
2001 the first really good facial
detection algorithm came out and it was

21
00:01:11,851 --> 00:01:15,390
created by two guys,
Paul Viola and Michael Jones.

22
00:01:15,540 --> 00:01:18,750
And so it was aptly named
the Viola Jones Algorithm.

23
00:01:18,900 --> 00:01:22,560
And this was the first time that facial
detection really worked for a Webcam.

24
00:01:22,890 --> 00:01:27,150
And so, I mean object detection has
been around since the sixties this is,

25
00:01:27,360 --> 00:01:28,530
this is nothing new,
right?

26
00:01:28,560 --> 00:01:32,640
But this was the first time that it
really worked better than anything else.

27
00:01:32,880 --> 00:01:37,110
And they're out rhythm did what all the
other algorithms in general did before

28
00:01:37,111 --> 00:01:41,490
them. They hand coded in features
and fed them into a classifier,

29
00:01:41,520 --> 00:01:46,410
namely a support vector machine. So it
was trained on a Dataset of bases. Right?

30
00:01:46,620 --> 00:01:47,340
And they would,

31
00:01:47,340 --> 00:01:52,140
they would hand code the location
of eyes of the mouth of the nose,

32
00:01:52,260 --> 00:01:56,490
and then their relations to each
other. Right. So as great as it was,

33
00:01:56,550 --> 00:01:58,620
as at detecting faces,

34
00:01:58,800 --> 00:02:03,780
it was really bad at detecting faces
in any other kind of configuration than

35
00:02:03,781 --> 00:02:07,310
what the hand coded features
said. Right? So wanted,

36
00:02:07,380 --> 00:02:11,880
once they collected these
features by hand, by manually
for each of these images,

37
00:02:12,210 --> 00:02:13,800
these features were vectors,
right?

38
00:02:13,801 --> 00:02:18,030
And they would feed all these vectors
together for a single image into a support

39
00:02:18,031 --> 00:02:19,590
vector machine. A classifier, right?

40
00:02:19,591 --> 00:02:24,480
I support vector machine can
act as a linear classifier,
a binary classifier. Yes,

41
00:02:24,481 --> 00:02:29,010
no, and that was it. Real face or
not a real face. And this worked.

42
00:02:29,040 --> 00:02:30,870
This worked really well at the time.

43
00:02:31,740 --> 00:02:33,870
And here you can see some
pseudocode of how it worked,

44
00:02:34,080 --> 00:02:37,590
but basically for each of the images they
would down sample the image to create

45
00:02:37,591 --> 00:02:41,370
an image, which is, which in this
case is actually a feature, right?

46
00:02:41,550 --> 00:02:44,610
And then they would iteratively
add all those features up together.

47
00:02:44,610 --> 00:02:47,250
They would concatenate them
and then accumulate a set,

48
00:02:47,280 --> 00:02:51,210
a series of filter outputs and feed all
of those outputs to a support vector

49
00:02:51,211 --> 00:02:53,790
machine. If you want to know how
support vector machines work,

50
00:02:53,791 --> 00:02:56,430
check out my math of
intelligence playlist. Okay.

51
00:02:56,431 --> 00:03:00,070
So then in 2005 a much more
efficient technique came out,

52
00:03:00,250 --> 00:03:02,500
still using features,
hand coded features,

53
00:03:02,770 --> 00:03:07,770
but it was called hog or histograms of
oriented gradients and it was invented by

54
00:03:08,181 --> 00:03:12,940
it, a veep Delo and build tricks. So
the basic idea of this algorithm was,

55
00:03:13,690 --> 00:03:18,460
was for for every image. So for faces,
right? Namely they applied it to faces,

56
00:03:18,610 --> 00:03:21,970
but they also applied it to
pedestrian detection on roads,

57
00:03:22,180 --> 00:03:24,460
which is really useful,
right? For self driving cars,

58
00:03:24,640 --> 00:03:27,850
for traffic lights for things
like that. But the idea was, okay,

59
00:03:27,851 --> 00:03:30,340
you've got an image of a face,
right? You've got an image of a face.

60
00:03:30,520 --> 00:03:34,840
And the goal is how dark is the current
pixel compared to all the surrounding

61
00:03:34,841 --> 00:03:37,810
pixels.
So for every single pixel in an image,

62
00:03:37,990 --> 00:03:41,440
they would detect how dark it was
compared to the surrounding pixels.

63
00:03:41,740 --> 00:03:46,420
And then they drew an Arrow showing in
which direction the image got darker.

64
00:03:46,450 --> 00:03:47,283
Okay.

65
00:03:47,590 --> 00:03:51,670
And so they repeated that process
for every single pixel in the image.

66
00:03:51,820 --> 00:03:55,060
So every pixel was then
replaced by an arrow.

67
00:03:55,210 --> 00:03:57,430
And this Arrow was called a gradient,

68
00:03:57,490 --> 00:04:01,180
not a gradient in the sense
of backpropagation for
neural networks, a gradient,

69
00:04:01,181 --> 00:04:03,550
just, they're just using
that word for this, right?

70
00:04:03,551 --> 00:04:07,810
So the gradient show the flow from
light to dark across the entire image.

71
00:04:07,930 --> 00:04:10,180
So this is what picture
would eventually look like.

72
00:04:10,360 --> 00:04:12,040
It would go from a series of pixels,

73
00:04:12,041 --> 00:04:16,300
a matrix of pixels into a
matrix of gradients or arrows,

74
00:04:16,301 --> 00:04:18,730
which point which direction
things are getting darker.

75
00:04:19,090 --> 00:04:22,720
And so then once they had that, they
broke up the image into small squares,

76
00:04:22,750 --> 00:04:24,460
16 by 16 pixels each.

77
00:04:24,760 --> 00:04:28,510
And then in each square they counted up
how many gradients point in each major

78
00:04:28,511 --> 00:04:29,344
direction.

79
00:04:29,500 --> 00:04:33,250
And they were placed that square in the
image with the Arrow directions that

80
00:04:33,251 --> 00:04:38,080
were the strongest. So in that, in that
region, what direction was most prevalent?

81
00:04:38,081 --> 00:04:38,560
Right?

82
00:04:38,560 --> 00:04:41,650
And they would replace the square and
the image with arrows in that direction.

83
00:04:42,040 --> 00:04:46,450
And the end result of this is that the
original image was converted into a

84
00:04:46,451 --> 00:04:49,210
simple representation
that looked like this.

85
00:04:49,390 --> 00:04:53,980
That was basically it captured the
basics. The essence of a face, right?

86
00:04:53,981 --> 00:04:57,580
And so and so they can use this
image to then given a new face,

87
00:04:57,760 --> 00:05:00,760
they can use some sort of
similarity metric like you know,

88
00:05:00,790 --> 00:05:05,020
you know the Euclidean distance or any
of the various distance metrics that we

89
00:05:05,021 --> 00:05:09,580
could use to detect how similar it
was to whatever image it was given.

90
00:05:09,790 --> 00:05:13,810
And based on some threshold value, they
could say, yes, this is a face or no,

91
00:05:13,811 --> 00:05:14,740
this is not a face.

92
00:05:14,920 --> 00:05:19,920
So recall that this kind
of basic feature map right,

93
00:05:20,981 --> 00:05:24,950
that they've generated looks very similar
to what convolutional nets learned

94
00:05:24,951 --> 00:05:25,784
themselves.

95
00:05:25,960 --> 00:05:29,890
But in this case they had to hand
code what this feature map looks like.

96
00:05:30,100 --> 00:05:34,030
So then in 2012 the era of
deep learning began, right?

97
00:05:34,450 --> 00:05:37,360
This is when every so often
in in computer science,

98
00:05:37,390 --> 00:05:42,340
an idea comes along that just makes you
not only rethink your existing ideas but

99
00:05:42,341 --> 00:05:46,540
completely discard your existing ideas.
And that's what deep learning was, right?

100
00:05:46,541 --> 00:05:49,510
So in 2012 for the image net competition,

101
00:05:49,540 --> 00:05:53,620
this is a yearly competition to train
for, for researchers to try to, you know,

102
00:05:53,860 --> 00:05:57,800
try out their algorithms and see who can
outperform the other Kurczewski and his

103
00:05:57,801 --> 00:06:02,210
team with Hinton use a convolutional
network that outperformed everybody else

104
00:06:02,211 --> 00:06:03,270
for image net.
Right?

105
00:06:03,280 --> 00:06:08,280
So for this was in 2012 but the thing
is that convoluted is convolutional nets

106
00:06:09,891 --> 00:06:13,250
had been around since the 90s why did
they work this time? If you can answer,

107
00:06:13,430 --> 00:06:18,140
you get bonus points. There are
two reasons. It's because of a,

108
00:06:18,141 --> 00:06:21,140
lots of gps and lots of data that's,
that's a difference.

109
00:06:21,141 --> 00:06:23,510
They gave it lots of computing
power and lots of data.

110
00:06:23,750 --> 00:06:27,980
Now the thing about humans, we're really
good at classifying images, right?

111
00:06:27,981 --> 00:06:32,480
We're really good at this,
but we do more than just classify images.

112
00:06:32,660 --> 00:06:37,640
So notice that whenever there's
a classification task, yes or no,

113
00:06:37,670 --> 00:06:40,340
is it there or not? That's it,
right? That's classification.

114
00:06:40,790 --> 00:06:42,740
Whenever there is a classification task,

115
00:06:42,741 --> 00:06:46,170
the images that we feed
these models just have the,

116
00:06:46,171 --> 00:06:50,840
the the class in question as a center
of the image. Like, if it's a dog,

117
00:06:50,841 --> 00:06:54,380
you'll just see a dog, maybe a white
background, but it's just a dog.

118
00:06:54,770 --> 00:06:58,370
But the thing is when we look around
at the world, right, it's not just,

119
00:06:58,550 --> 00:07:02,030
we're not just classifying things,
we are detecting things.

120
00:07:02,031 --> 00:07:05,390
That means that we are not just
identifying what an object is,

121
00:07:05,510 --> 00:07:06,740
but we are classifying.

122
00:07:06,890 --> 00:07:10,670
But we're detecting the
boundaries around that.

123
00:07:10,671 --> 00:07:14,540
Object to the differences of that object
with the other objects in the images

124
00:07:14,690 --> 00:07:17,030
and the relations between all the objects.

125
00:07:17,240 --> 00:07:21,500
So the question is can convolutional
nets as great as they are,

126
00:07:21,501 --> 00:07:25,070
a classification also help at detection.

127
00:07:25,100 --> 00:07:28,910
That is not just detecting whether
or not an image is in a picture,

128
00:07:29,050 --> 00:07:33,560
right as a single image, but, but,
but detecting if that image exists,

129
00:07:33,830 --> 00:07:35,780
what its relation is to other images.

130
00:07:35,900 --> 00:07:40,070
Can we build a bounding box around that
image? Is the image partially occluded?

131
00:07:40,130 --> 00:07:43,670
But we can still see it? Yes.
The answer is yes. Right.

132
00:07:43,820 --> 00:07:48,050
And this was proven by a team that
was very recently. This was proven.

133
00:07:48,500 --> 00:07:51,890
But one very basic thing we
can do is we can say, okay,

134
00:07:52,070 --> 00:07:55,760
let's take an existing
convolutional net and repurpose it,

135
00:07:55,940 --> 00:08:00,380
repurpose it as an object, a texture
detector. How are we going to do that?

136
00:08:00,410 --> 00:08:01,880
Now let me tell you how
we're going to do that.

137
00:08:02,150 --> 00:08:06,350
We can take any existing classifier,
VGG net inception.

138
00:08:06,470 --> 00:08:09,620
These are huge convolutional nets
that were trained on huge data sets,

139
00:08:09,621 --> 00:08:11,380
right by Google,
et cetera.

140
00:08:12,140 --> 00:08:15,800
And what we can do is we can take our
image in question. Let's say it's,

141
00:08:15,920 --> 00:08:20,600
let's say it's this image here and we
want to detect her car and Vg Nett and

142
00:08:20,601 --> 00:08:24,290
obviously the tech a car because it was
trained on car images amongst many other

143
00:08:24,291 --> 00:08:25,124
classes.

144
00:08:25,190 --> 00:08:29,940
What we can do is we can slide the
classifier over every single, um,

145
00:08:30,350 --> 00:08:32,780
over a bunch of squares of
that image, right? So we can,

146
00:08:33,080 --> 00:08:38,080
we can classify every single region as
we define maybe a 12 by 12 pixel region

147
00:08:38,180 --> 00:08:41,600
for every single one. And so as
we're sliding it across the image,

148
00:08:41,601 --> 00:08:46,280
we're just continuously classifying
every single box, right? Of this image.

149
00:08:47,280 --> 00:08:49,640
We're going to get a bunch
of different classifications.

150
00:08:49,930 --> 00:08:53,630
We were going to get a bunch of different
classifications and then we can only

151
00:08:53,631 --> 00:08:56,520
keep the ones at the classifier
is the most certain about,

152
00:08:56,730 --> 00:09:00,660
and we can use that to then draw
a bounding box around the image.

153
00:09:00,990 --> 00:09:04,380
But this is a very brute force approach,
right? We don't, we're not just going to,

154
00:09:04,530 --> 00:09:08,030
this is a very computationally expensive
approach. We don't want to just do that,

155
00:09:08,031 --> 00:09:10,500
that that's, that's done
this. That's not a good way.

156
00:09:10,620 --> 00:09:13,350
There's gotta be a more efficient
way of doing things. Right?

157
00:09:13,710 --> 00:09:18,390
So a better approach was invented
just two years ago called CNN.

158
00:09:18,510 --> 00:09:23,100
Okay.
So the idea behind our CNN was,

159
00:09:23,370 --> 00:09:26,610
before they feed it,
they fed it to a convolutional network.

160
00:09:26,910 --> 00:09:31,530
They would use a process called selective
search to create a set of bounding

161
00:09:31,531 --> 00:09:35,850
boxes. Or in the paper they called
them region proposals in an image.

162
00:09:36,000 --> 00:09:36,960
So at a high level,

163
00:09:36,961 --> 00:09:40,950
the selective search process process
looks at an image through a series of

164
00:09:40,951 --> 00:09:44,460
windows of different sizes, right?
These are just randomly size,

165
00:09:44,461 --> 00:09:45,930
randomly placed windows.

166
00:09:46,200 --> 00:09:50,910
And for each size it tries to group
together adjacent pixels by texture,

167
00:09:50,940 --> 00:09:55,770
color, or intensity to identify
objects. So given some input and image,

168
00:09:55,830 --> 00:10:00,480
the first step is to generate a set of
region proposals for bounding boxes for

169
00:10:00,481 --> 00:10:04,710
however many we want by some threshold,
right? And then once we have that,

170
00:10:04,740 --> 00:10:08,380
we run those images in the bounding
boxes through a pretrained,

171
00:10:08,400 --> 00:10:10,560
Alex snap or whatever kind of CNN.

172
00:10:10,561 --> 00:10:14,100
We want to compute the
features for that bounding box.

173
00:10:14,310 --> 00:10:18,720
And then finally a support vector machine
to classify to see what the object in

174
00:10:18,721 --> 00:10:22,620
the image in the box, what the
object, the image in the box is of.

175
00:10:23,070 --> 00:10:26,930
Then we run the box through a linear
regression model to output tighter

176
00:10:26,940 --> 00:10:30,300
coordinates for the box once
the object has been classified.

177
00:10:31,380 --> 00:10:35,400
And this proved to be an effective
approach for object detection.

178
00:10:35,550 --> 00:10:40,290
The most effective by
repurposing a convolutional
network for object detection,

179
00:10:40,440 --> 00:10:45,440
by using this selective search algorithm
to create bounding boxes beforehand and

180
00:10:45,451 --> 00:10:50,451
then feeding all of those boxes to a CNN
to compute a list of features and then,

181
00:10:50,910 --> 00:10:52,680
and then computing class values from them.

182
00:10:52,920 --> 00:10:55,950
So for RCA are for our CNN,

183
00:10:56,130 --> 00:11:00,680
there had been lots of improvements
like lots of fast. Our CNN baster,

184
00:11:00,681 --> 00:11:02,220
our CNN mask,

185
00:11:02,221 --> 00:11:07,020
our CNN and I've got papers and links
to them all here in the description.

186
00:11:07,100 --> 00:11:09,570
Uh, actually mask our CNN
just came out this year.

187
00:11:10,080 --> 00:11:13,260
But yellow I think is the best approach
and not just because of the name.

188
00:11:14,010 --> 00:11:18,270
Yolo is the best approach, um,
because it outperformed all of them.

189
00:11:18,330 --> 00:11:22,290
It outperformed our CNN and all
of its variance that I know of.

190
00:11:22,291 --> 00:11:26,640
Maybe some guy just yesterday invented
some new version of our CNN, I don't know.

191
00:11:26,641 --> 00:11:29,430
But as far as I know,
Yolo is uh,

192
00:11:29,520 --> 00:11:33,400
the state of the art in
object detection and Yolo,

193
00:11:33,420 --> 00:11:36,660
it takes a completely different approach,
which is very exciting. All right,

194
00:11:36,661 --> 00:11:38,400
so check this out.
So Yolo,

195
00:11:38,640 --> 00:11:43,140
Yolo does is it's not a
traditional classifier that's
repurposed to be an object

196
00:11:43,141 --> 00:11:44,520
detector like our CNN.

197
00:11:44,730 --> 00:11:48,330
What it does is it actually just looks
at the image once hence its name.

198
00:11:48,331 --> 00:11:50,670
You only look once,
but in a clever way.

199
00:11:51,000 --> 00:11:55,180
Let's say we've got this image right and
we want to is tact all the classes that

200
00:11:55,181 --> 00:11:58,150
are in the image. We've got
a dog, we've got a bike,

201
00:11:58,180 --> 00:12:01,090
we've got a car in the background.
How are we going to do this?

202
00:12:01,270 --> 00:12:04,810
So what Yolo says is let's
divide up the image into a grid,

203
00:12:04,930 --> 00:12:06,820
a 13 by 13 cells.

204
00:12:07,210 --> 00:12:11,560
And so each of these cells is responsible
for predicting five bounding boxes.

205
00:12:11,680 --> 00:12:16,680
So some part of a bounding box is going
to intersect into that little square

206
00:12:16,890 --> 00:12:19,390
that the, that that is that
little square, right? So it's,

207
00:12:19,430 --> 00:12:21,730
it's responsible for
detecting up to five of them.

208
00:12:21,760 --> 00:12:25,120
Like it could be part of this one part
of this one part of this one or this one,

209
00:12:25,121 --> 00:12:28,840
right,
but five and so what a bounding box does,

210
00:12:28,841 --> 00:12:32,020
cause it describes the rectangle
that encloses some object.

211
00:12:32,470 --> 00:12:37,450
Yolo is also going to output a confidence
score that tells us how certain it is

212
00:12:37,570 --> 00:12:41,230
that the predicted downing box
actually encloses some object.

213
00:12:41,380 --> 00:12:45,640
So it's not just PR trying to predict
whether or not a set of bounding boxes

214
00:12:45,760 --> 00:12:46,593
exist.

215
00:12:46,630 --> 00:12:51,040
It's also each of these
squares is predicting what
the class is for each of the

216
00:12:51,041 --> 00:12:54,250
bounding boxes that it's
predicted the existence of.

217
00:12:54,790 --> 00:12:58,630
And so the score doesn't say anything
about what kind of object is in the box.

218
00:12:58,750 --> 00:13:01,150
Just if the shape of the box is any good.

219
00:13:01,630 --> 00:13:05,020
So the predicting bounding boxes may
look something like the following,

220
00:13:05,080 --> 00:13:08,410
the higher the confidence score,
the fatter the box that is drawn.

221
00:13:08,411 --> 00:13:11,050
So notice for our Doggie,
for our bike,

222
00:13:11,170 --> 00:13:14,140
we've got some really thick
bounding boxes, meaning that it,

223
00:13:14,280 --> 00:13:16,270
it can tell that there is
something significant there.

224
00:13:17,620 --> 00:13:19,300
So each of the bounding boxes,

225
00:13:19,370 --> 00:13:22,600
the s for each bounding box to
sell also predicts a class, right?

226
00:13:22,601 --> 00:13:26,020
It's predicting two things,
whether or not a bounding box exists.

227
00:13:26,260 --> 00:13:30,460
And then the class of that bounding box,
and this works just like a classifier,

228
00:13:30,490 --> 00:13:34,810
it gives a probability distribution over
all the possible trick classes that the

229
00:13:35,050 --> 00:13:37,780
network has been trained on.
It's a convolutional network.

230
00:13:38,200 --> 00:13:41,200
So Yolo was trained on
the Pascal voc data set,

231
00:13:41,410 --> 00:13:45,490
which has 20 different classes like
bicycles and boats and cars and dogs,

232
00:13:46,060 --> 00:13:48,280
and the confidence score
for the bounding box.

233
00:13:48,340 --> 00:13:52,690
And the class prediction are combined
into one final score that tells the

234
00:13:52,691 --> 00:13:57,130
probability that this bounding box
contains a specific type of object.

235
00:13:57,490 --> 00:13:58,031
For example,

236
00:13:58,031 --> 00:14:03,031
the big fat yellow box here on the left
is 85% sure that it contains the object

237
00:14:03,610 --> 00:14:07,510
dog. So based on some kind of
threshold value, like 80% plus,

238
00:14:07,720 --> 00:14:10,330
we can just leave that bounding
box there that we decide.

239
00:14:11,350 --> 00:14:16,270
So since there are 13 by 13 grid cells
and each cell is predicting five bounding

240
00:14:16,271 --> 00:14:21,220
boxes, we end up with 845
downing box boxes in total.

241
00:14:21,250 --> 00:14:22,630
That's a lot of bounding boxes,

242
00:14:23,140 --> 00:14:26,980
but it turns out that most of these boxes
are going to have very low confidence

243
00:14:26,981 --> 00:14:27,730
scores.

244
00:14:27,730 --> 00:14:32,440
So we only keep the boxes whose
final score is 30% or more.

245
00:14:32,441 --> 00:14:36,340
Like I said, whatever threshold
value we want. And once we do that,

246
00:14:36,610 --> 00:14:38,380
the end result is going to be this.

247
00:14:38,530 --> 00:14:41,170
We're going to have only
three bounding boxes left,

248
00:14:41,350 --> 00:14:45,400
and these three bounding boxes had scores
that were higher than our threshold

249
00:14:45,401 --> 00:14:49,510
value, 30% and as you can see, that's
exactly what we wanted to detect.

250
00:14:49,960 --> 00:14:51,800
But the, this, the key here,

251
00:14:51,801 --> 00:14:56,801
the secret of why Yolo is so good is
because it's not just that from the 845

252
00:14:58,401 --> 00:15:00,800
total bounding boxes,
we only kept three of them,

253
00:15:01,160 --> 00:15:05,060
but it's that even though there
were 845 separate predictions,

254
00:15:05,390 --> 00:15:08,720
they were all made at the
same time, right? Yolo,

255
00:15:08,721 --> 00:15:13,721
you only look once the
convolutional network is only
looking once we are feeding

256
00:15:13,850 --> 00:15:18,110
all of these boxes to the neural
network all at once, right?

257
00:15:18,140 --> 00:15:23,000
So it is a convolutional network,
right? And the architecture is very,

258
00:15:23,001 --> 00:15:26,270
very simple. This is what it looks like,
right? You've got input, convolution,

259
00:15:26,271 --> 00:15:28,880
pooling, convolution,
pooling, convolution, pooling,

260
00:15:29,090 --> 00:15:30,770
right over and over and over again.

261
00:15:30,950 --> 00:15:34,400
And there are actually several versions
of Yolo out there, or there's Yolo.

262
00:15:34,401 --> 00:15:38,330
And then there's Yolo [inaudible] also
called yellow, 9,000 better, faster,

263
00:15:38,331 --> 00:15:41,630
stronger. I know these names are
just going crazy. I love it though.

264
00:15:41,631 --> 00:15:45,650
[inaudible] memes and signs together
when total win, right? So, uh,

265
00:15:45,690 --> 00:15:49,310
this is what it looks like.
Uh, and so by the way,

266
00:15:49,311 --> 00:15:51,860
we're going to be looking at
yellow to just skip Yolo one.

267
00:15:52,100 --> 00:15:56,000
Cause Yolo too is obviously
better. Uh, so, uh, we,

268
00:15:56,090 --> 00:16:00,590
what we're to do is once we feed our
bounding boxes into our convolutional

269
00:16:00,591 --> 00:16:05,060
network, we're going to end up with 125
channels for every single Brits Self.

270
00:16:05,330 --> 00:16:09,710
And these 125 numbers contain the data
for the bounding boxes and the class

271
00:16:09,711 --> 00:16:10,544
predictions.

272
00:16:10,760 --> 00:16:15,760
And the reason it's 125 is because each
grid cell predicts five bounding boxes

273
00:16:16,520 --> 00:16:20,930
and a bounding boxes described by 25 data
elements. And these are the elements,

274
00:16:20,931 --> 00:16:23,660
the x fly, that's the
locations, the coordinates,

275
00:16:23,900 --> 00:16:26,210
the width and height for the
bounding box was rectangle,

276
00:16:26,390 --> 00:16:31,370
the confidence score and the probability
distribution over the classes using it.

277
00:16:31,370 --> 00:16:34,070
It's very simple. You give
it an input image, right?

278
00:16:34,250 --> 00:16:38,360
It goes through the convolutional network
in a single pass and it comes out the

279
00:16:38,361 --> 00:16:42,350
other end as a 13 by 13 by 125,
uh,

280
00:16:42,410 --> 00:16:46,100
sized tensor describing the
bounding boxes for the grid cells.

281
00:16:46,340 --> 00:16:50,030
And all you need to do then is compute
the final scores for the bounding boxes

282
00:16:50,240 --> 00:16:53,630
and throw away the ones that
are scoring less than 30%.

283
00:16:54,260 --> 00:16:57,080
And so yola two is better
because it's faster,

284
00:16:57,200 --> 00:17:02,000
it understands more generalized object
representations and you can find a paper

285
00:17:02,001 --> 00:17:05,960
for it here. Okay. So what I want
to do now is go through some code.

286
00:17:05,990 --> 00:17:08,030
So let's go through some code.
This is,

287
00:17:08,100 --> 00:17:13,100
this is probably the best implementation
of a Yolo too that I found on get hub

288
00:17:13,761 --> 00:17:16,950
so far. So, uh, originally the,

289
00:17:17,010 --> 00:17:20,360
the authors of Yolo wrote it in this,
uh,

290
00:17:20,780 --> 00:17:24,590
wrote it in dark nets.
So dark net is a,

291
00:17:24,860 --> 00:17:27,500
is an open source neural network library.
And See,

292
00:17:27,501 --> 00:17:31,880
I know why didn't they just
use tensorflow, but they
wrote it in dark net and um,

293
00:17:32,060 --> 00:17:35,540
yeah, it's written foresee and
Kuta. It's fast. It's great if you,

294
00:17:35,541 --> 00:17:37,160
if you want to work with see work with,
see,

295
00:17:37,161 --> 00:17:40,850
but basically the best implementation
I've found is called dark flow.

296
00:17:41,000 --> 00:17:46,000
So what this guy did is he translated
dark net to a tensorflow virgin and we can

297
00:17:46,221 --> 00:17:48,970
use dark flow to do several things.
And I'm going,

298
00:17:49,020 --> 00:17:51,960
I'm going to do it with you right now.
Okay, let's get started to hear right.

299
00:17:51,961 --> 00:17:55,710
So we can choose one of the following
three ways to get started with dark flow.

300
00:17:55,711 --> 00:17:59,310
I've downloaded it here so you can
just clone it, calling it locally,

301
00:17:59,311 --> 00:18:02,790
and then you can set it up.
So the first step is to run setup.py.

302
00:18:03,660 --> 00:18:08,220
No module named Scythe on. Okay,
so then I'll install Saigon.

303
00:18:10,460 --> 00:18:12,140
Okay,
so that's the first step we're going to,

304
00:18:12,170 --> 00:18:15,920
we're going to set up cy
thon and once it's done that,

305
00:18:15,950 --> 00:18:17,840
then we can install all
of our dependencies.

306
00:18:18,110 --> 00:18:21,950
And the reason we're using Psiphon is
because right darknet is written in c and

307
00:18:21,951 --> 00:18:25,700
we need some kind of rapper to introduce
these c libraries to our python

308
00:18:25,701 --> 00:18:29,420
environment.
So now I can go ahead and install

309
00:18:31,520 --> 00:18:34,610
Paul our dependencies.

310
00:18:35,180 --> 00:18:39,800
That'll take some time. Okay, that
was it. That was our dependencies.

311
00:18:40,040 --> 00:18:44,210
And so now let's go ahead
and download some weights.

312
00:18:50,270 --> 00:18:51,760
Okay,
so downloaded this,

313
00:18:51,790 --> 00:18:56,240
this pretrained weights
and I'm going to open them.

314
00:18:56,720 --> 00:18:57,830
Tonya will awaits

315
00:19:06,200 --> 00:19:07,880
right here.
Okay,

316
00:19:12,390 --> 00:19:17,240
now I want to load these weights up.
Okay.

317
00:19:17,241 --> 00:19:19,850
So let's look at this code
for a bit. Um, all right,

318
00:19:19,851 --> 00:19:24,500
so for the code we can see
that it is quite a big project,

319
00:19:24,530 --> 00:19:25,970
but if we look under net,

320
00:19:26,000 --> 00:19:29,210
we're going to see some of the versions
of the different yellow implementations.

321
00:19:29,510 --> 00:19:32,630
So we can just go to, uh,
let's see where we can go to.

322
00:19:32,900 --> 00:19:36,290
We can go to Yolo v two.
Okay.

323
00:19:36,291 --> 00:19:40,910
So if we go to build.py, we can see the
tensorflow implementation of dark net.

324
00:19:41,090 --> 00:19:43,620
So remember, because this
was written in c, uh,

325
00:19:43,820 --> 00:19:48,800
we are not using tensorflow native
functions to build this convolutional

326
00:19:48,801 --> 00:19:52,790
network. So when we go into
the in function, we'll see
that the s there are some,

327
00:19:52,791 --> 00:19:56,120
you know, boiler plate flags for
how to train this network. Uh,

328
00:19:56,310 --> 00:19:58,760
and there are different ways
of building this network.

329
00:19:58,880 --> 00:20:01,340
We can build it from a
pretrained PB file and you know,

330
00:20:01,341 --> 00:20:05,150
that's how we save files and tensorflow
as dot PB. That's one of the ways.

331
00:20:05,570 --> 00:20:07,430
And when we build the Ford Pass,

332
00:20:07,431 --> 00:20:09,890
like this is kind of where the
convolutional network is built.

333
00:20:10,190 --> 00:20:13,700
We have some placeholder that the input
data can go through that is namely the

334
00:20:13,701 --> 00:20:15,830
image or the video,
whatever it is.

335
00:20:16,820 --> 00:20:19,870
And then iteratively they're
taking all the ex, the,

336
00:20:19,880 --> 00:20:24,260
the author took the existing dark net
layers and they're adding them to tensor

337
00:20:24,261 --> 00:20:26,690
flow just like right here. Okay. So,

338
00:20:26,691 --> 00:20:29,720
and then we have a bunch of flags and
then we have a way to save the file once

339
00:20:29,721 --> 00:20:30,950
we've trained it.
So,

340
00:20:30,951 --> 00:20:35,510
but the basic idea is that we can use
a pretrained version of this model that

341
00:20:35,511 --> 00:20:38,550
was trained in Si by
downloading weights and then uh,

342
00:20:38,610 --> 00:20:43,610
using the train.py file to train
it on our video or on our images.

343
00:20:45,500 --> 00:20:46,400
And then lastly,

344
00:20:46,401 --> 00:20:51,400
we can then we can then predict what the
bounding boxes are going to be by using

345
00:20:51,401 --> 00:20:55,690
predictive, the predict our
pi a class. Right. Okay.

346
00:20:55,691 --> 00:20:57,070
So there are several things we can do.

347
00:20:57,071 --> 00:21:01,350
We can train our model using our
own data set. We can use a PR,

348
00:21:01,390 --> 00:21:05,080
we can use pretrained weights, we can
train it on images, we can train on video.

349
00:21:05,230 --> 00:21:06,041
But for this demo,

350
00:21:06,041 --> 00:21:08,680
why don't we just use a pre trend
model that they already have, right?

351
00:21:08,681 --> 00:21:13,480
The author has weights that
we can download and then
we can train it on one of

352
00:21:13,481 --> 00:21:16,840
our own videos, right? So
if I just paste this in,

353
00:21:17,080 --> 00:21:21,580
we can train it on one of our own videos
and then we can see the results for

354
00:21:21,581 --> 00:21:24,700
ourselves, right? Just like that.

355
00:21:24,970 --> 00:21:28,180
And so now this is detecting
all the objects in the video,

356
00:21:28,210 --> 00:21:31,960
in the video using yellow.
It's awesome. And uh, yeah,

357
00:21:31,961 --> 00:21:35,020
if you want to do this yourself, it's
pretty simple. I've got the code,

358
00:21:35,260 --> 00:21:38,050
the description,
the slides that I was talking about,

359
00:21:38,110 --> 00:21:41,860
everything is in the video description
and the get hub link. So check them out.

360
00:21:41,861 --> 00:21:43,060
And I hope that you found this useful.

361
00:21:44,170 --> 00:21:46,060
Please subscribe for
more programming videos.

362
00:21:46,061 --> 00:21:49,540
And for now I've got to think of some
Yolo maims. So thanks for watching.

