1
00:00:22,110 --> 00:00:25,790
Live stream is starting soon.
It's starting soon.

2
00:00:26,300 --> 00:00:29,960
It's about to start. It's about to go
down. Livestream is about to go down.

3
00:00:30,500 --> 00:00:35,090
In fact, it's probably already going
down. Oh there I am. Oh, we're all,

4
00:00:35,100 --> 00:00:37,010
it's Raj.
Good to see you guys.

5
00:00:37,011 --> 00:00:40,130
And all squad is in the building.

6
00:00:40,580 --> 00:00:44,570
Let me shout out some names.
Igor. It's patchy. Mick,

7
00:00:45,470 --> 00:00:50,240
Raj Sudan shoe, Brennan. Yo,

8
00:00:50,241 --> 00:00:53,300
we got the whole machine wearing
squad in the house right now.

9
00:00:53,301 --> 00:00:55,990
Who is ready to learn some tensorflow.
I'm so excited for him.

10
00:00:56,000 --> 00:01:00,820
Some tensor flow as to Gora Spencer.
All right.

11
00:01:00,821 --> 00:01:05,570
All right. All right. All right.
Okay, cool. Hi everybody. Hi. Hi.

12
00:01:05,600 --> 00:01:08,830
Okay, so let's get down to
business as they say, Mama,

13
00:01:08,900 --> 00:01:13,900
let's get down to business
and defeat the loss function.

14
00:01:16,520 --> 00:01:17,450
Okay.
So anyway,

15
00:01:18,410 --> 00:01:23,000
let's start off with
a five minute Q and a.

16
00:01:23,090 --> 00:01:23,780
So,

17
00:01:23,780 --> 00:01:28,370
so hit me up with your
most intense machine or any
questions or you just want to

18
00:01:28,371 --> 00:01:30,890
know, you know, anything.
I'm just, we'll go for it.

19
00:01:30,891 --> 00:01:35,150
And then we're going to use tensor
flow to classify housing prices. Okay.

20
00:01:35,210 --> 00:01:37,280
It's going to be dope. It's going
to be the first time. Reason,

21
00:01:37,281 --> 00:01:42,020
tensor flow from scratch in a live
session. It's gonna be awesome. Okay,

22
00:01:42,260 --> 00:01:45,140
so here we go. Hi. Hi.
Hi. Hi. Hi. Let's begin.

23
00:01:49,480 --> 00:01:53,890
I'm doing good. I'm doing good. I'm so
down. I do have a sore throat, but like,

24
00:01:53,891 --> 00:01:58,210
you know, I'm a human. I know,
but like, it's crazy, but
it's, it's cool. It's cool.

25
00:01:58,560 --> 00:02:01,780
Um, what kind of method approach do
you use to manage your time, Mick?

26
00:02:01,810 --> 00:02:06,250
Great question. I, so, um,

27
00:02:07,480 --> 00:02:08,650
what do I use to manage my time?

28
00:02:08,680 --> 00:02:12,700
I write down what I'm going to do for
the whole week before the week starts.

29
00:02:12,701 --> 00:02:17,020
So every Sunday I'll write down, you
know, the, the weekly goals I still have,

30
00:02:17,320 --> 00:02:21,400
I have a weekly to do, to do, I have
a daily to do, I have a monthly to do,

31
00:02:21,520 --> 00:02:24,370
I have a yearly to do and
then I have a five year to do.

32
00:02:24,610 --> 00:02:28,690
So I plan things out in intervals
like that. And, and I made it,

33
00:02:28,750 --> 00:02:32,470
I made sure my goals are accomplishable
goals so that I get that positive

34
00:02:32,471 --> 00:02:33,580
feedback so I can keep going.

35
00:02:34,150 --> 00:02:38,120
It's Google released its own o s uh,

36
00:02:38,820 --> 00:02:41,710
chrome, chrome, ios. Yes or language. No.

37
00:02:41,711 --> 00:02:46,510
Are only python recommended books on
computer science or cryptography. Um,

38
00:02:46,720 --> 00:02:50,380
your cryptography is so dope. I wish I
could just deep dive into cryptography.

39
00:02:50,381 --> 00:02:54,520
That is like some ancient Egyptian
pharaoh mummy. You know what I'm saying?

40
00:02:54,521 --> 00:02:54,891
Like you,

41
00:02:54,891 --> 00:02:59,530
you have to a good book on photography
is my book decentralized applications,

42
00:02:59,560 --> 00:03:03,150
the best selling software engineering
book on Amazon for 2016. Um,

43
00:03:03,490 --> 00:03:05,770
and a good book for machine learning
is machine learning a probabilistic

44
00:03:05,771 --> 00:03:09,550
approach by uh, I forgot the
name, but that's the name of it.

45
00:03:09,610 --> 00:03:13,090
What do you think of Star Wars? I
Love Star Wars. I'm actually not like,

46
00:03:13,180 --> 00:03:14,830
I love the old ones.
The new ones,

47
00:03:14,890 --> 00:03:17,200
like I was kind of
disappointed by episode seven.

48
00:03:17,201 --> 00:03:20,260
So it's like whatever cause kind of
conservative that offer. Disney's parks,

49
00:03:20,350 --> 00:03:22,840
we have to make our own stories.
Guys. We are a new generation.

50
00:03:22,841 --> 00:03:25,210
We're gonna make our own stories
and they're going to be as good,

51
00:03:25,240 --> 00:03:27,900
if not better than star wars.
And we're gonna have a machine learning.

52
00:03:27,901 --> 00:03:29,650
It's gonna be dope.
And when we make stories,

53
00:03:29,651 --> 00:03:31,210
it's going to be technically are accurate.

54
00:03:31,990 --> 00:03:35,260
What if there are more than one local
minima and gradient descent Algo?

55
00:03:35,470 --> 00:03:38,060
Great question. So that's so um, yeah,

56
00:03:38,061 --> 00:03:41,250
so sometimes we have more
than so graded descent.

57
00:03:41,320 --> 00:03:45,120
It's like it's like a three d
graph with the valley, right?

58
00:03:45,140 --> 00:03:46,940
We want to remember what I
talked about last lecture.

59
00:03:46,950 --> 00:03:49,030
You want to drop a ball and
football and see where it fits.

60
00:03:49,090 --> 00:03:50,510
But what if we have multiple values?

61
00:03:50,800 --> 00:03:54,280
Well then that's when I was a second
order optimization function comes in.

62
00:03:54,490 --> 00:03:59,490
We have defined the local minimum that
is closest to that is that it's uh,

63
00:04:03,070 --> 00:04:05,800
going to best optimize our function.
We're going to talk about that later,

64
00:04:05,801 --> 00:04:09,520
but the keyword to remember to say
second order optimization function tensor

65
00:04:09,521 --> 00:04:10,930
calculus,
that's exactly what we're going to do.

66
00:04:10,931 --> 00:04:14,940
So you are not at a low level but at a
high level because uh, tensorflow is,

67
00:04:15,200 --> 00:04:17,290
I'm going to do a little
bit of magic for us.

68
00:04:17,470 --> 00:04:19,210
Can you explain nats behind tensorflow?

69
00:04:19,240 --> 00:04:24,160
I will absolutely find the map to flow is
better than torch. Exactly. Hey, sprout,

70
00:04:24,161 --> 00:04:28,450
are more layers always better in a neural
network? A great question. Tomorrow? No,

71
00:04:30,250 --> 00:04:34,750
not always. Not always. Usually it is.
But uh, so if you have a smaller Dataset,

72
00:04:34,780 --> 00:04:39,130
you don't really need that many layers.
So remember with, with machine learning,

73
00:04:39,131 --> 00:04:41,670
there's always a trade off.
There's always a trade off.

74
00:04:41,690 --> 00:04:44,950
And in engineering in general, there's,
and in life there's a trade off as well.

75
00:04:45,100 --> 00:04:49,150
But, uh, we, we want to, when
we, when we optimize for,

76
00:04:51,600 --> 00:04:52,433
uh,

77
00:04:54,050 --> 00:04:56,870
so when we increase the layers,
we increased the computational complexity,

78
00:04:56,880 --> 00:04:59,900
but we also increased the accuracy.
So there's a trade off.

79
00:05:00,080 --> 00:05:04,010
How should I prepare for Google brain
residency program interview Kaggle,

80
00:05:04,310 --> 00:05:08,210
Kaggle challenges are great. And
the tensorflow intro docs, uh,

81
00:05:08,450 --> 00:05:10,520
watch all my videos,
but you probably already have a,

82
00:05:10,521 --> 00:05:13,260
if you do all that and you apply the code,
you'll be good to go essentially. Yeah.

83
00:05:13,270 --> 00:05:14,103
That's the book name.

84
00:05:14,300 --> 00:05:17,600
Can you give new research idea that
can implement intention flow? Yeah.

85
00:05:17,660 --> 00:05:20,360
So here's an idea I've had for a while.
Somebody should go with this,

86
00:05:21,110 --> 00:05:23,970
take the synthetic radiant paper
from the mind synthetic grading.

87
00:05:23,971 --> 00:05:27,170
So they kind of like stopped
using backpropagation just
for this paper and then

88
00:05:27,171 --> 00:05:29,720
take the idea of one shot when,
so, so probabilistic programming.

89
00:05:30,050 --> 00:05:31,910
And then combine these two ideas together.

90
00:05:32,090 --> 00:05:34,670
So synthetic gradients and
one shot learning, boom,

91
00:05:34,700 --> 00:05:36,920
there's your landmark paper for the year.
I don't have time to do it.

92
00:05:36,921 --> 00:05:40,760
So someone just rolled with that.
It's central, usable in Raspberry Pi.

93
00:05:41,690 --> 00:05:44,060
Yeah. Actually somebody
made a wrapper for Arduino.

94
00:05:44,300 --> 00:05:46,010
So two more questions and
then we're gonna get started.

95
00:05:47,970 --> 00:05:51,420
How does genetic algorithm bit into
machine on the genetic algorithms are good

96
00:05:51,421 --> 00:05:53,610
for optimizing hyper parameters for us.

97
00:05:53,700 --> 00:05:54,720
And when we talked
about that for a second,

98
00:05:54,721 --> 00:05:58,670
so I've got some emails from people
who are like Ah, on the hosel articles,

99
00:05:58,671 --> 00:05:59,810
what people like,
what do you think about this?

100
00:05:59,900 --> 00:06:02,060
We're a Google are using machine learning.

101
00:06:02,650 --> 00:06:05,450
Google is using machine learning
to optimize and she's running.

102
00:06:05,480 --> 00:06:06,860
So like what do we do guys?

103
00:06:07,880 --> 00:06:12,290
Machine learning is not going to take
your jobs away. Okay. We will be.

104
00:06:12,670 --> 00:06:15,800
Um, so if even if there's
a, even if there's a job,

105
00:06:15,801 --> 00:06:20,750
apocalypse machine learning engineers
will be the last people to have jobs.

106
00:06:20,780 --> 00:06:21,830
Not that that's going to happen.

107
00:06:21,831 --> 00:06:24,740
We're going to create new jobs and the
new data at climate and we're going to

108
00:06:24,741 --> 00:06:27,560
have basic income and things like that.
Hopefully through cryptocurrency. Sec,

109
00:06:27,620 --> 00:06:31,430
check out my posts, social point. And it's
already possible. And also branch point.

110
00:06:32,030 --> 00:06:35,440
But um, don't worry about
machine learning, replacing a,

111
00:06:35,610 --> 00:06:37,260
your job as the mission
you want to engineer it to.

112
00:06:37,460 --> 00:06:40,430
If that actually happens that we solve
intelligence and then there's no,

113
00:06:40,610 --> 00:06:44,680
the idea of jobs and money and scarcity
all goes away anyway. Okay, so let's,

114
00:06:44,690 --> 00:06:46,320
let's,
let's go ahead and start yet.

115
00:06:46,520 --> 00:06:48,130
Let's go ahead and get started
if everybody started wanting it.

116
00:06:48,131 --> 00:06:51,590
So to start my course, which at
this course. Okay, so what, okay,

117
00:06:52,010 --> 00:06:54,410
so that's the prominent
today or we're gonna do,

118
00:06:55,400 --> 00:06:59,980
we're gonna use tensorflow to
classified housing prices. Okay. So, um,

119
00:07:00,020 --> 00:07:02,870
that's what we're going to do and I'm
going to use a Jupiter notebook for one.

120
00:07:02,871 --> 00:07:07,160
So let me go ahead and start screen
sharing immediately and we'll get right on

121
00:07:07,550 --> 00:07:09,350
into this itch.
There we go.

122
00:07:20,740 --> 00:07:21,780
Okay.
So

123
00:07:23,400 --> 00:07:27,120
let me move this chat window over
here so I can still see you guys.

124
00:07:27,860 --> 00:07:30,240
I don't need to see myself,
but I just need to see you guys.

125
00:07:30,450 --> 00:07:33,180
And then I'm also have a

126
00:07:34,970 --> 00:07:38,730
timer so I can see myself.

127
00:07:41,220 --> 00:07:44,420
All right.
New movie recording.

128
00:07:47,440 --> 00:07:51,840
All right. And then that
goes away and I'm right here.

129
00:07:52,960 --> 00:07:53,793
Okay.

130
00:07:55,830 --> 00:07:57,000
Minimize myself.

131
00:07:57,970 --> 00:07:58,803
Okay.

132
00:07:59,880 --> 00:08:04,140
Yeah, the one time it's kind of blurred.
Uh, I'm going to get a better one. Okay.

133
00:08:04,141 --> 00:08:06,660
So here's what we're going to do guys.
Okay.

134
00:08:06,780 --> 00:08:11,400
So we have a housing datasets.
Uh,

135
00:08:11,470 --> 00:08:16,030
one shot learning was the name
was the other thing. Okay. Okay.

136
00:08:16,031 --> 00:08:19,370
Someone do that. And just in general,
one shot learning. So we need more one,

137
00:08:19,470 --> 00:08:24,080
her name, uh, Congress because no, not
everybody has a huge datasets and fusion.

138
00:08:24,081 --> 00:08:29,050
Fusion power. Okay. So I'm going
to explain things slowly this time.

139
00:08:29,110 --> 00:08:33,070
Okay. I'm going to, thanks
Alexandra. Much low. Okay,

140
00:08:33,071 --> 00:08:36,710
so I'm going to explain
things slowly this time. Okay.

141
00:08:37,040 --> 00:08:40,730
So I'm going to start off by talking
about a neural network. Okay?

142
00:08:40,731 --> 00:08:45,200
So a lot. Okay. We see this image
a lot. I'm a neural network, right?

143
00:08:45,380 --> 00:08:47,840
But the one thing I want to say is,

144
00:08:47,920 --> 00:08:52,430
it's the only circles we call him neurons.
They're not actually objects,

145
00:08:52,490 --> 00:08:56,550
they're not classes.
These are just representations of numbers.

146
00:08:56,730 --> 00:09:00,660
Let me zoom in a little bit. They
just represent numbers, okay?

147
00:09:00,720 --> 00:09:04,680
Because when we input data into a
neural network, into a neural network,

148
00:09:04,710 --> 00:09:07,200
in tensorflow,
we clot a computation graph.

149
00:09:09,790 --> 00:09:12,740
Um,
and these are not,

150
00:09:12,741 --> 00:09:17,660
these are not actually like neuro objects.
When we take, when we take an input,

151
00:09:17,661 --> 00:09:21,910
let's say like, you know, well we're
going to do housing prices and we apply a,

152
00:09:21,920 --> 00:09:25,700
when we apply, uh, some matrix math to it,

153
00:09:25,730 --> 00:09:29,850
it's going to take that value and it's
going to do so if it's a one value and we,

154
00:09:29,920 --> 00:09:32,390
we multiplied by a
matrix of safety values,

155
00:09:32,480 --> 00:09:34,520
it's what you did to become
a three or four values.

156
00:09:34,790 --> 00:09:38,480
Those values are what we represent as
neurons. They're not actually objects.

157
00:09:38,560 --> 00:09:42,680
And we take those values and a place that
had matrix operations to them and then

158
00:09:42,681 --> 00:09:46,120
boom, it's a next set of neurons. But
he's neurons are not actually classes.

159
00:09:46,130 --> 00:09:50,210
They are values. Okay? So that, I just
want to start off with that. So that's,

160
00:09:50,211 --> 00:09:52,770
that's, that's that. Now, let
me show you guys the graph.

161
00:09:53,020 --> 00:09:56,330
We show you that to date of first. So
here's the Dataset. I mean, let me,

162
00:09:56,360 --> 00:10:00,830
let me zoom in on anonymous. Okay,
so this is a set up housing prices.

163
00:10:00,980 --> 00:10:03,830
Okay. So the first one did the
index, would you have been a number?

164
00:10:04,130 --> 00:10:06,950
Then next one is the area
which is the size of the house.

165
00:10:07,100 --> 00:10:11,180
The next one is the back, the number of
bathrooms, then the price and then this.

166
00:10:11,270 --> 00:10:15,770
And then the price per square.

167
00:10:16,610 --> 00:10:20,450
The price per square inch.
Okay? So that's what that is.

168
00:10:21,110 --> 00:10:23,240
What we want to do is we
want to classify this,

169
00:10:23,241 --> 00:10:25,910
but this isn't a class but we
don't have labels right now.

170
00:10:26,090 --> 00:10:28,070
We're going to add labels,
okay?

171
00:10:28,071 --> 00:10:30,230
We're going to add labels
where it's going to be a good,

172
00:10:31,050 --> 00:10:34,280
where it's going to be either a good bite
or a bad box. Cause what we want to do,

173
00:10:34,281 --> 00:10:36,680
and these with given one
of these features, okay,

174
00:10:36,710 --> 00:10:38,330
well we're going to decide
what feature for use.

175
00:10:38,420 --> 00:10:41,780
But given one of these features we went
to then classify the house as either a

176
00:10:41,781 --> 00:10:45,950
good or a bad bottle. Okay. So let me,

177
00:10:45,951 --> 00:10:50,120
let me shut down my screen for a second
just so the bit rate goes up a little

178
00:10:50,121 --> 00:10:54,260
bit. Okay. I want to, I want it
to be not that lobby. Okay. So,

179
00:10:54,800 --> 00:10:56,960
so that's what we're going to
do. Okay. So that's our data.

180
00:10:57,410 --> 00:11:01,160
So let's go ahead and start doing
this in a Jupiter notebook. Okay.

181
00:11:01,161 --> 00:11:04,010
So I'm going to do it in a
Jupiter notebook. All right.

182
00:11:04,011 --> 00:11:09,011
So the first thing I want to
do is talk a little bit about,

183
00:11:09,560 --> 00:11:14,140
let me just make sure that this works
first. No, let's see. Yeah. Okay.

184
00:11:14,160 --> 00:11:17,750
Works Great. Okay, so tensorflow, why are
we using tensorflow in the first place?

185
00:11:17,860 --> 00:11:21,920
So, uh, yeah, so this trip is going to
be, let me, let me make the font bigger.

186
00:11:22,190 --> 00:11:23,420
Actually,
the fun is,

187
00:11:24,660 --> 00:11:25,870
let me see.
Okay.

188
00:11:27,660 --> 00:11:30,390
Where are you central flow?
Because Google created it for, oh,

189
00:11:30,410 --> 00:11:34,500
this is going to be a
supervised problem. Okay. Yeah,

190
00:11:34,501 --> 00:11:35,790
it's price per square foot.

191
00:11:35,970 --> 00:11:39,580
This is a supervised problem we're using
tensorflow because we have intention

192
00:11:39,581 --> 00:11:44,070
flow. We are, uh, tentraflow was created
by Google and it was built to scale.

193
00:11:44,110 --> 00:11:47,430
If you use it for many
years, they released it. It
was kind of like, you know,

194
00:11:47,431 --> 00:11:51,060
use coming down from Olympus and
getting fired to the humans. Right.

195
00:11:51,150 --> 00:11:53,160
We know that tensorflow can scale.

196
00:11:53,170 --> 00:11:56,380
We know that it's reliable because
it's used in production. By Google,

197
00:11:56,530 --> 00:12:00,760
I mean Google search uses tensorflow.
Okay. So this is a reliable library,

198
00:12:00,790 --> 00:12:03,100
although high torch is pretty cool too.

199
00:12:03,101 --> 00:12:07,090
It just came out this month
and it introduces the concept
of dynamic computation

200
00:12:07,091 --> 00:12:09,340
graphs. Usually we have
two interpreters, right?

201
00:12:09,341 --> 00:12:12,400
We have a python interpreter
and then we have a, um,

202
00:12:13,980 --> 00:12:15,420
a computation and interpreter.

203
00:12:15,450 --> 00:12:20,350
But what ty torch does is it has one
interpreter for everything. Okay. So, um,

204
00:12:20,400 --> 00:12:23,220
and so Jupiter notebook is really as a,

205
00:12:23,230 --> 00:12:27,770
it's a great concept. Basically
you can, it uses an a,

206
00:12:28,730 --> 00:12:33,690
it allows us to view our coat and had
marked down for code and lets us add,

207
00:12:33,710 --> 00:12:37,600
um, a bunch of pretty
things to our code. Yes.

208
00:12:37,700 --> 00:12:41,760
Prometheus's the rights, uh, answer
that I was looking for. Okay.

209
00:12:41,790 --> 00:12:44,850
So let's go ahead and start by
importing Marmot tenancies. Okay.

210
00:12:44,851 --> 00:12:48,900
So we went to classify a problem and also
let me show you guys what are your own

211
00:12:48,901 --> 00:12:52,210
bed? Looks like. I didn't show you
guys what is it going to look like? So,

212
00:12:52,310 --> 00:12:55,170
so this is what it's going to look like.
Okay.

213
00:12:55,440 --> 00:13:00,150
So we're going to feed in to inputs and
we're going to apply a set of weights

214
00:13:00,151 --> 00:13:04,520
and biases. Use the softmax
function and then output, uh,

215
00:13:05,190 --> 00:13:10,050
the housing price as well as the,
the label,

216
00:13:10,051 --> 00:13:14,190
which is good or bad. Okay. So, um,
we'll talk about that in a second.

217
00:13:14,460 --> 00:13:18,000
Let's go ahead and first import import.
Our dependencies are all right. So

218
00:13:19,600 --> 00:13:24,260
yes, I'm focused on the content. Thank
you. Pull up. So here we go. Import.

219
00:13:24,320 --> 00:13:29,220
Candace has PD. Why Candice? This
is our beautiful library to, uh,

220
00:13:30,100 --> 00:13:32,260
work with data as tables worth with

221
00:13:34,870 --> 00:13:39,010
at our first Lego. Okay, so
next one is none Pi, none Pi.

222
00:13:39,250 --> 00:13:43,240
It's going to help us, uh, use numbers,

223
00:13:44,770 --> 00:13:47,270
use number of agencies.
Both Penn is intentional needed,

224
00:13:47,320 --> 00:13:51,190
so use a number of agencies.
Okay.

225
00:13:51,580 --> 00:13:54,040
The next thing is mapped up
light because we're gonna,

226
00:13:54,280 --> 00:13:58,240
we're gonna show this graph. We're going
to show this graph and in second, and

227
00:14:01,220 --> 00:14:02,680
uh, what else? We want maps out,

228
00:14:02,681 --> 00:14:06,700
lives quite clots as PLT.

229
00:14:08,750 --> 00:14:12,430
I prefer tensorflow,
hi, plot to terrorize.

230
00:14:13,070 --> 00:14:16,680
And finally you tend to flow as FTA.

231
00:14:18,800 --> 00:14:22,450
Okay, that's it. Those are
our four libraries. Then we
can go ahead and compile it.

232
00:14:22,451 --> 00:14:25,460
Boom. So because there are no
errors, we can keep going. Right?

233
00:14:25,461 --> 00:14:30,230
So that's a great thing about Jupiter
slash I python notebooks is that they like

234
00:14:30,231 --> 00:14:34,460
you compile as you go so you can
see, uh, you know, what is happening.

235
00:14:35,210 --> 00:14:37,720
Okay? So that's what we did there.
Now we're going to load the data,

236
00:14:37,790 --> 00:14:42,470
not so that was, that was so step
one is to load data, load data.

237
00:14:42,670 --> 00:14:45,560
Okay? So let's go ahead and locate
it. How are we going to load data?

238
00:14:45,750 --> 00:14:49,420
But we are going to
use the pandas, uh, uh,

239
00:14:49,580 --> 00:14:50,840
library to help us do that.

240
00:14:50,990 --> 00:14:55,400
So the first thing we're gonna do is
we're reaching use. The campaigners reads,

241
00:14:55,401 --> 00:14:59,360
gets the function because
we have a CSV file. Okay.

242
00:15:01,740 --> 00:15:05,340
Um, so if we're going to read
this in as a data frame object,

243
00:15:05,341 --> 00:15:07,740
so what does a data frame
object data frame object.

244
00:15:07,970 --> 00:15:12,970
It's an object in memory that Candice
creates from a CSV file that we can then

245
00:15:13,860 --> 00:15:16,920
easily parse. And I'm going to show
you what I mean by easily park.

246
00:15:17,170 --> 00:15:20,880
So we're going to take this data frame
variable that we just created and we're

247
00:15:20,881 --> 00:15:24,150
going to, uh, list what the,

248
00:15:25,380 --> 00:15:27,660
we're going to remove the
columns we don't care about.

249
00:15:27,840 --> 00:15:30,210
So how do we know which columns
we don't care about? Right?

250
00:15:30,240 --> 00:15:33,480
So let's go back for a data for a second.
We have several columns here,

251
00:15:33,660 --> 00:15:38,550
which are the ones that we want to use
to predict if a house is good or bad.

252
00:15:38,970 --> 00:15:41,760
Well, what we're going to
do is we're gonna take out
the index because that's not

253
00:15:41,761 --> 00:15:46,260
going to really give us anything. It's
just a numbering of what each column gets.

254
00:15:46,650 --> 00:15:50,520
The next one is, uh, the, uh,

255
00:15:50,560 --> 00:15:54,820
let's say area and bathrooms, right? So
those are the two features that like,

256
00:15:55,240 --> 00:15:58,120
we're going to use area and bathroom for
not gonna get the price for the square.

257
00:15:58,480 --> 00:16:00,280
We're not gonna use this
price, this, the square price.

258
00:16:00,460 --> 00:16:04,420
Because when I looked for a house, I
want to make sure that, uh, it has,

259
00:16:04,450 --> 00:16:07,120
I have a certain, and that's
just what I'm using, right?

260
00:16:08,230 --> 00:16:12,570
Deciding which features to
use is its own. It's, it's,

261
00:16:12,590 --> 00:16:16,300
it's its own discipline in and of itself.
What teachers should be used.

262
00:16:16,301 --> 00:16:21,300
And a good rule of thumb to go by it is,
is um,

263
00:16:21,340 --> 00:16:25,150
what features would you personally
use to make a prediction? Okay.

264
00:16:25,990 --> 00:16:29,130
Um, yeah, let me, let me, um,

265
00:16:30,920 --> 00:16:34,460
give you guys this data. How
do I, um, got to stay to, oh,

266
00:16:34,461 --> 00:16:38,420
I can create it are just so kind of just,
I'll say,

267
00:16:38,470 --> 00:16:39,530
let's just take off this,

268
00:16:40,010 --> 00:16:42,320
let me give you guys this data real
quick and I'll paste it into the chat.

269
00:16:42,470 --> 00:16:45,920
So you guys haven't said so data
that Tsp and we'll create a public.

270
00:16:45,921 --> 00:16:49,760
Just boom and I'll paste it in the chat.
Ready?

271
00:16:49,790 --> 00:16:53,030
Get ready for this guys and the
patient. Then boom, there's your data.

272
00:16:54,310 --> 00:16:57,440
That's the data file. All right, so
now you guys can do that along with it.

273
00:16:59,220 --> 00:17:03,340
All right. So we are going to remove
the features we don't care about.

274
00:17:05,080 --> 00:17:07,090
Okay? We're going to remove the
features we don't care about.

275
00:17:07,091 --> 00:17:09,960
We don't care about the Christ. We're
not going to care about the square price.

276
00:17:09,961 --> 00:17:13,210
Speak only won't care about the,
the back, the number of bathrooms.

277
00:17:13,240 --> 00:17:16,100
And what was the second thing?
The area that's,

278
00:17:16,101 --> 00:17:18,880
that's what we're going to
be using right now. Okay? So,

279
00:17:20,590 --> 00:17:22,510
and access is one because

280
00:17:24,050 --> 00:17:28,990
for we're only gonna use the first
axis from the Dataset. All right? So,

281
00:17:29,530 --> 00:17:32,800
so data frame about drop. So
that's who we removed too. Let me,

282
00:17:32,830 --> 00:17:37,670
let me say what we just said here.
We've removed the, we removed Xe,

283
00:17:37,780 --> 00:17:42,020
uh,
features we don't care about.

284
00:17:44,110 --> 00:17:46,600
All right? So we removed the
teachers. We don't care about,

285
00:17:46,790 --> 00:17:50,820
now we're going to only use the first
10 rows in the Dataset and this example.

286
00:17:50,821 --> 00:17:54,510
So we only want to use the first 10
of these, right? Right up to here.

287
00:17:55,080 --> 00:17:59,330
Right up to here. Okay. No Oxford, I
don't want to define that. Okay, so,

288
00:17:59,790 --> 00:18:02,370
so how do we say that?
Well, easily enough.

289
00:18:02,910 --> 00:18:06,630
The great thing about a Beta
for an object, if we can
say, we can say I wouldn't

290
00:18:08,180 --> 00:18:10,500
from row zero to write 10.
Ooh.

291
00:18:10,560 --> 00:18:13,260
So that's going to tell us
that we want to only use,

292
00:18:13,800 --> 00:18:18,460
we only use the first 10 rows.
That's it.

293
00:18:18,461 --> 00:18:21,850
Okay. So that's it. So now we
have our data frame object.

294
00:18:21,851 --> 00:18:23,260
So let's go ahead and print this.

295
00:18:23,440 --> 00:18:25,300
And here's the great thing
about python notebooks.

296
00:18:25,301 --> 00:18:28,540
If we can now print this and see what we
have and you'll give us an error if it

297
00:18:28,541 --> 00:18:31,120
doesn't work. Okay. So there
we go. You've got it in text.

298
00:18:31,210 --> 00:18:35,940
So let's see what I did here. So on
this line, data frame dot drop, uh,

299
00:18:36,010 --> 00:18:37,480
it's pointing to

300
00:18:39,740 --> 00:18:41,660
his access equals one.

301
00:18:41,900 --> 00:18:46,020
And the problem here is invalid syntax.
Uh,

302
00:18:46,640 --> 00:18:51,110
so we have index square prize,
uh Oh yes.

303
00:18:51,111 --> 00:18:54,500
So it did. Okay. So the, this
bracket actually goes here.

304
00:18:55,190 --> 00:18:58,820
That's where the profit goes. Now let's,
let's compile. Okay, so there we go.

305
00:18:58,910 --> 00:19:03,050
There is our beautiful clean for medic
data and we're only using the first 10

306
00:19:03,051 --> 00:19:07,130
columns and we're just using, uh, the, uh,

307
00:19:07,190 --> 00:19:10,130
area and bathrooms as our features.
And remember,

308
00:19:10,160 --> 00:19:12,980
feature selection is an entire
discipline in and of itself.

309
00:19:13,370 --> 00:19:17,240
And we could have used the square price
and we could have used the price as

310
00:19:17,241 --> 00:19:21,320
features, but what I'm looking for
or the area in Bachelors, okay.

311
00:19:21,350 --> 00:19:26,270
That's what I look for when
I try to predict if a house
is a good buy or not for

312
00:19:26,271 --> 00:19:30,910
me. Okay. Um, so, so that's,

313
00:19:30,970 --> 00:19:33,570
so now we have our features. So
let's introduce our, our labels.

314
00:19:33,571 --> 00:19:34,900
So what we just did,

315
00:19:34,901 --> 00:19:38,360
so step one was to load the data and
set two is going to be to introduce the

316
00:19:38,361 --> 00:19:43,030
labels because right now there are no
labels. So we could just do a regression,

317
00:19:43,031 --> 00:19:45,310
right? Because we can then
prevent the next value.

318
00:19:45,340 --> 00:19:49,090
And what do we use regression for for
predicting the next value in a continuous

319
00:19:49,091 --> 00:19:53,920
set. Uh, so that's what we do for
that, but we'll, we're going to do,

320
00:19:53,921 --> 00:19:56,540
if you were to convert this
into a classification problem,

321
00:19:56,800 --> 00:20:00,100
so add labels cause that's our step two.

322
00:20:00,340 --> 00:20:01,930
So we're going to take
that data frame object,

323
00:20:03,150 --> 00:20:04,810
and if everybody doesn't have the data,

324
00:20:04,870 --> 00:20:07,600
go ahead and ask and then someone's
going to paste it into the chat.

325
00:20:07,990 --> 00:20:10,390
The link to that, uh, just, all right.

326
00:20:10,391 --> 00:20:14,710
So what we're gonna do is
we're to introduced the
labels. So we want to say, uh,

327
00:20:14,770 --> 00:20:17,070
our labels are going to eat
a good buy or a bad product.

328
00:20:17,230 --> 00:20:20,170
And so how do we represent
that? Well, for simplicity sake,

329
00:20:20,171 --> 00:20:21,610
let's just use binary numbers,
right?

330
00:20:21,611 --> 00:20:26,230
So a good buy is a one and a bad by the
zero. So that's just randomly had like,

331
00:20:26,500 --> 00:20:30,850
you know, a set of purpose
for our 10, for our 10 values.

332
00:20:30,940 --> 00:20:35,480
So we have to count 10, right? So we want
one, one, one zero, and then, you know,

333
00:20:35,560 --> 00:20:40,120
add some variety in there. So let's see
how many that is. That's three, six,

334
00:20:40,180 --> 00:20:43,300
nine. But we want, uh, we have one,

335
00:20:43,301 --> 00:20:48,280
one zero zero one and then zero and okay,
so again,

336
00:20:48,290 --> 00:20:51,060
it's $10. So we're going
to add on labels. Um,

337
00:20:52,900 --> 00:20:55,480
and so that's,

338
00:20:55,630 --> 00:20:58,090
so what is good and zero is back.

339
00:20:58,091 --> 00:21:02,640
The one is good and zero is bad.
Luck.

340
00:21:03,220 --> 00:21:06,890
Okay, so one is the advisor
was that way. Now, um,

341
00:21:07,510 --> 00:21:11,260
now you can see the first part of the
code. Let me, let me read paste the link.

342
00:21:11,261 --> 00:21:15,040
I'm going to replace the link from,
okay. You can pick some. Okay.

343
00:21:15,041 --> 00:21:17,800
So now you can't see the first part
of the code. Well that's, let me go

344
00:21:19,310 --> 00:21:21,790
that I just, I can't go back.
We have to keep going. Okay,

345
00:21:21,820 --> 00:21:24,210
so one is the goodbye is
there is a bad bottle.

346
00:21:24,250 --> 00:21:28,740
So we can define our set of labels.
Now the next step is to, uh,

347
00:21:29,980 --> 00:21:32,200
turn true false values
into ones and Zeros.

348
00:21:32,201 --> 00:21:37,201
So we're going to say the
location variable is going
to define those labels for

349
00:21:39,431 --> 00:21:41,650
us. All right? So,

350
00:21:44,140 --> 00:21:49,080
so let's go ahead and add the,
uh, the white t values, which,

351
00:21:49,150 --> 00:21:51,070
which will,

352
00:21:52,540 --> 00:21:56,200
so why too.
Um,

353
00:21:56,620 --> 00:22:01,530
and we're going to use a data
frame object, whereas wine one.

354
00:22:02,700 --> 00:22:05,280
Okay,
so what did we just do?

355
00:22:05,300 --> 00:22:09,520
So what we just did is we said why
two is a negation of why one, right?

356
00:22:09,521 --> 00:22:12,370
So we defined why too,
as a negotiation.

357
00:22:12,580 --> 00:22:17,390
And a [inaudible] is a dictation of
why. One. It's the opposite. Opposite.

358
00:22:17,890 --> 00:22:22,770
Okay. Um, right.

359
00:22:22,771 --> 00:22:26,100
So today's the actors. One is,
is the, is the column, okay?

360
00:22:27,720 --> 00:22:31,270
So now white. So why to me we don't
like a house. Why do you need,

361
00:22:31,271 --> 00:22:35,940
we don't like a house. Okay. So let's
go ahead and define why two. Okay.

362
00:22:36,600 --> 00:22:40,170
Different unlock location.
And we're saying, okay,

363
00:22:40,171 --> 00:22:42,570
so what is that value that we want?

364
00:22:44,800 --> 00:22:49,620
Um, why two and then we'll say, okay,

365
00:22:49,621 --> 00:22:52,430
so we're going to convert it
to my needs. Value, right?

366
00:22:52,450 --> 00:22:56,010
We always want to remember that for
tide we're converting to is an in.

367
00:22:56,011 --> 00:22:59,970
So how do we convert it to an evaluator?
Uh, so we're going to take as type.

368
00:23:01,680 --> 00:23:03,200
And so what this says is

369
00:23:04,830 --> 00:23:07,080
if it turn true false
values into ones and Zeros,

370
00:23:07,290 --> 00:23:10,290
so turn true false

371
00:23:12,960 --> 00:23:17,070
values to ones and Zeros. Um, okay.

372
00:23:17,100 --> 00:23:21,240
And now we've done that. And so now
let's print out what we have here.

373
00:23:21,270 --> 00:23:23,940
Let's see what this looks like.
Great thing about high python notebooks.

374
00:23:23,941 --> 00:23:26,640
We can just print what we have here.
We definitely have a syntax error.

375
00:23:26,970 --> 00:23:30,840
So there it is.
And I think soccer mentioned this earlier.

376
00:23:31,050 --> 00:23:35,910
So we are going to add two line three.

377
00:23:35,911 --> 00:23:40,650
So which one is that?
Our uh, brackets? Yes.

378
00:23:40,950 --> 00:23:44,930
Just like that. Okay. And
so now you have an extra.

379
00:23:46,100 --> 00:23:50,390
Okay. Okay. So what did
we do? So we added labels,

380
00:23:50,510 --> 00:23:53,540
but instead of saying good or bad,
we had to label.

381
00:23:53,541 --> 00:23:55,700
So this is just for simplicity sake,

382
00:23:55,701 --> 00:23:59,210
we don't want to convert text to integers
are just going to say it in a raw

383
00:23:59,211 --> 00:24:00,590
value. So, so why?

384
00:24:00,630 --> 00:24:05,630
So why one has a value that means that
is a good buck and every time why one has

385
00:24:05,811 --> 00:24:07,910
a value every time.

386
00:24:07,911 --> 00:24:11,210
Why one is one y two will be
zero and every time y two is one,

387
00:24:11,300 --> 00:24:14,050
why one would be zero. Okay?
So that's just our way of,

388
00:24:14,470 --> 00:24:16,580
of labeling our data for now.
Okay.

389
00:24:16,581 --> 00:24:21,200
So we've added our labels that was set to
remember step one was to load the data,

390
00:24:21,410 --> 00:24:24,020
which we did into a data frame object.
We parsed it.

391
00:24:24,350 --> 00:24:27,020
And then step two was to add
the label to that data. Right.

392
00:24:27,021 --> 00:24:30,890
Which makes this a classification problem
is group. Would you classify this data?

393
00:24:31,550 --> 00:24:35,920
Okay? Where would you
classify housing? Uh,

394
00:24:36,440 --> 00:24:38,860
if a house is a good buy
are bad by based on the,

395
00:24:39,080 --> 00:24:43,670
on the area and the number of of
backwards. Okay. Not Bedrooms. Right?

396
00:24:43,700 --> 00:24:48,350
Um, well I guess, I mean the data didn't
have bedrooms did it? Okay. So, but yeah,

397
00:24:48,351 --> 00:24:50,510
bedrooms would probably
be better if we have that.

398
00:24:50,660 --> 00:24:53,990
So now we have all our data
in a data point. We have to
shape it in major system,

399
00:24:54,010 --> 00:24:55,070
feed into intention clusters.

400
00:24:55,071 --> 00:25:00,071
Now step three is to
prepare data for tensorflow.

401
00:25:03,350 --> 00:25:07,490
So depending on which machine
learning, uh, library we're using,

402
00:25:08,760 --> 00:25:13,130
uh, we can, we will prepare it
a certain way. But in general,

403
00:25:13,340 --> 00:25:17,150
in general, no matter what data we have,
we're going to convert it into cancers.

404
00:25:17,420 --> 00:25:21,890
So 10, let me explain what tensions
are. So, um, so tensors are,

405
00:25:22,670 --> 00:25:23,490
it's a high level of,

406
00:25:23,490 --> 00:25:28,490
so cancers and so tenters are generic
virgin of made of vectors and matrices.

407
00:25:33,920 --> 00:25:36,980
So, um, of vector is so like

408
00:25:38,490 --> 00:25:41,060
vector is a list of numbers.

409
00:25:41,150 --> 00:25:45,320
Then vector is who different up.

410
00:25:45,450 --> 00:25:46,900
This is important because
we see these words.

411
00:25:46,901 --> 00:25:48,290
I love to take your list of numbers.

412
00:25:49,100 --> 00:25:54,100
Then a a matrix is a
list of lists of numbers.

413
00:25:55,610 --> 00:25:58,830
Okay. So then, so a vector
would behave, one d tensor

414
00:26:00,590 --> 00:26:02,150
matrix would be a tutee torture.

415
00:26:03,230 --> 00:26:08,230
And then whatever a risk of
lists of lists of numbers is,

416
00:26:08,510 --> 00:26:12,260
would be a three d 10 Sir. And
then that just continues. Okay.

417
00:26:12,590 --> 00:26:17,570
So attention is like a very generic term
or both matrices, matrices and vectors.

418
00:26:17,810 --> 00:26:21,800
And these tensors are half or how we
represent data in tensorflow, right?

419
00:26:21,801 --> 00:26:26,490
The guy's machine learning neural
networks. It's all just matrix map.

420
00:26:26,840 --> 00:26:30,890
It's a collection of operations
that we apply to some input matrix.

421
00:26:31,040 --> 00:26:34,550
And we continually apply him like a chain
of these operations until we get to an

422
00:26:34,790 --> 00:26:39,740
output. And then we, and then we
apply an optimization function to, uh,

423
00:26:40,220 --> 00:26:43,590
to minimize a loss. And I'm going to talk
about that in a, we're going to do that,

424
00:26:43,591 --> 00:26:46,710
but that's what this is. Okay.
So, so that was a little short,

425
00:26:46,740 --> 00:26:50,160
like a thing on Pinterest for a second.
But to get back to what we're doing,

426
00:26:51,120 --> 00:26:55,620
so we're going to take our data frame
and

427
00:26:57,210 --> 00:27:01,350
we're going to say, uh, let's see.

428
00:27:01,530 --> 00:27:06,330
We want to take the area and the bathroom.
So we're going to take our features,

429
00:27:06,331 --> 00:27:06,541
right?

430
00:27:06,541 --> 00:27:09,870
We're going to take our features and
we're going to convert them into tensors.

431
00:27:09,900 --> 00:27:11,440
We're going to convert
our Peters and detention.

432
00:27:11,470 --> 00:27:16,470
So convert features to
10 to include tensor.

433
00:27:18,410 --> 00:27:22,540
That's, that's what we're going to it
in burn features in anything tensor. Um,

434
00:27:23,880 --> 00:27:28,620
and this session will be 15 minutes
or probably an hour. We'll see, uh,

435
00:27:28,980 --> 00:27:33,980
as matrix and the we went to,

436
00:27:34,980 --> 00:27:37,080
we went to say, okay, so as
a matrix, that's an advert.

437
00:27:37,290 --> 00:27:41,220
So now we're going to convert
our labels, uh, to, uh,

438
00:27:41,820 --> 00:27:46,200
but to answer as well, right?
So let me show you guys this,

439
00:27:46,201 --> 00:27:50,840
this image again. Okay. All right. So,

440
00:27:52,240 --> 00:27:57,030
right, so here are our, just like right
here, this, these two purple circles,

441
00:27:57,090 --> 00:28:00,030
those are impotence for as
our features and our label.

442
00:28:00,031 --> 00:28:02,430
So we're going to feed that into a second.
We haven't created our weights yet.

443
00:28:02,550 --> 00:28:04,800
We've just,
so if you just look at everything,

444
00:28:04,801 --> 00:28:08,160
but these two purple lines that,
sorry,

445
00:28:08,161 --> 00:28:11,760
these two purple circles and that's
what we just created. Okay. Um,

446
00:28:13,200 --> 00:28:18,080
so now, uh, so that's
what we did for that.

447
00:28:18,081 --> 00:28:22,850
So now we're going to convert our
labels in future surfing. Okay.

448
00:28:22,851 --> 00:28:26,750
So I'm sorry, our labels, if you
tend to have a data frame dot low,

449
00:28:27,370 --> 00:28:29,960
sometimes they're already probably
going to happen. So we'll say, okay,

450
00:28:29,961 --> 00:28:32,450
so location and then we'll,

451
00:28:32,570 --> 00:28:37,100
when were those labels that we created
and you guys are gonna be easily have,

452
00:28:37,420 --> 00:28:41,430
um, one label. It's, well,
I'm just, you know, is,

453
00:28:42,970 --> 00:28:47,840
um, okay, so I got to
focus on this. Let's see,

454
00:28:47,841 --> 00:28:52,680
let's see. It just compiles.
Okay, so syntax error, what
do we have here? So, right,

455
00:28:52,681 --> 00:28:57,260
so the extra back brackets.
All right, took area bathrooms.

456
00:28:57,720 --> 00:29:01,430
Um, we have location. Okay,

457
00:29:01,431 --> 00:29:03,800
so Disney supplementing
and extra bracket here.

458
00:29:05,420 --> 00:29:09,470
And then what else do I have here?
So for why one,

459
00:29:09,710 --> 00:29:14,710
I have a bracket or when you get another
bracket and then we'll compile that.

460
00:29:16,100 --> 00:29:20,930
Let's see what else we got here.
Oh, comma aren't Jupiter notebooks.

461
00:29:20,931 --> 00:29:25,220
So great guys, we can just continue. We
can just, we can just compile as we go.

462
00:29:25,221 --> 00:29:29,630
We can just compile it as we go. A
different dot. Location. Y One y two.

463
00:29:29,990 --> 00:29:32,450
What's the dealio? Okay,
we're going to remove that.

464
00:29:35,320 --> 00:29:37,960
We want to say what is going on here.

465
00:29:37,961 --> 00:29:42,961
We've got y one y two die
location and this is a practice.

466
00:29:45,830 --> 00:29:46,663
Oh,

467
00:29:49,390 --> 00:29:53,590
let's see. Oh, okay. Let's see what's
going on here. Whoa, that's a lot.

468
00:29:53,620 --> 00:29:57,730
None area bathrooms are in the
columns. Hold on. A second.

469
00:29:58,260 --> 00:30:02,080
Bathroom in the cops. Uh,
let's see what we did here.

470
00:30:04,160 --> 00:30:08,960
So he sat data frame dot location.
We converted a area,

471
00:30:09,000 --> 00:30:13,430
bathrooms to as a matrix.
Um, and then we said,

472
00:30:13,460 --> 00:30:17,230
okay,
hasn't matrix.

473
00:30:18,670 --> 00:30:23,170
Interesting. Okay, so right. But
so we did have very about terms.

474
00:30:23,410 --> 00:30:27,610
Um, remove the closing brackets. Um,

475
00:30:27,670 --> 00:30:29,890
remove the closing brackets on

476
00:30:34,670 --> 00:30:38,880
or move this one and we
want to add interesting

477
00:30:40,470 --> 00:30:45,180
missing quotes. Oh, there it is. Right?
Missing quotes. That's what it was.

478
00:30:45,420 --> 00:30:49,470
Oh my God. Okay. Yes. Alright,

479
00:30:50,070 --> 00:30:54,180
there we go. Okay, good.
Sturdy for a second. Let's,

480
00:30:54,181 --> 00:30:58,050
let's print out what we had here.
So what did we just create? Ooh,

481
00:30:58,140 --> 00:31:01,860
so what is this? This is what our
[inaudible] matrix looks like. Okay.

482
00:31:01,920 --> 00:31:04,790
So we have the area in the backyard and
this is what our labels matrix looks

483
00:31:04,791 --> 00:31:06,690
like it's pronounce aren't labeled Matrix.

484
00:31:12,190 --> 00:31:15,700
Okay? So that's our implementers
and events are lady Matrix guys.

485
00:31:15,760 --> 00:31:19,830
We have our inputs now we're
going to prepare our printers.

486
00:31:19,840 --> 00:31:21,920
And so what step are we on?
Now we're on step four.

487
00:31:21,921 --> 00:31:26,200
So step three was to prepare the data
for tensor flow so we can format in our

488
00:31:26,201 --> 00:31:27,090
data for tensorflow flow.

489
00:31:27,250 --> 00:31:32,250
And now why don't you step four
is to write out our hyper Kuranda.

490
00:31:34,180 --> 00:31:36,070
Okay.
So what are our hyper parameters?

491
00:31:36,130 --> 00:31:39,430
So the first one is going to be our
learning, right? And so when did we last?

492
00:31:39,431 --> 00:31:42,340
You've when you, right, we
use the learning rate, uh,

493
00:31:42,370 --> 00:31:46,180
when we last life section.
Do you sum that up? We,

494
00:31:46,390 --> 00:31:49,740
the learning rate controls
the rate at which we learn,

495
00:31:51,010 --> 00:31:55,720
you know, so with a better recommendation,
we're learning rate. You're find, um, uh,

496
00:31:55,840 --> 00:31:59,540
how fast we reached convergence to be
technical convergence is when our model is

497
00:31:59,541 --> 00:32:00,640
that it's optimal fit.

498
00:32:00,850 --> 00:32:05,080
When we have that optimal hit where the
error is minimized and the learning rate

499
00:32:05,140 --> 00:32:08,530
defined how fast we get
convergence, we, we had to say,

500
00:32:09,750 --> 00:32:11,050
you can just say,
oh mate,

501
00:32:11,051 --> 00:32:13,720
the loading rate a million and just get
there as fast as possible because if you

502
00:32:13,721 --> 00:32:17,500
go too high then your, your model is not
going to converge. If you go too low,

503
00:32:17,501 --> 00:32:20,880
it's gonna be too slow. So remember,
like all things, it is a trade off.

504
00:32:21,100 --> 00:32:24,820
That is a word of the day,
particularly is a series of trade offs.

505
00:32:25,300 --> 00:32:28,720
And go as straight off to talk,

506
00:32:29,390 --> 00:32:32,830
maxing get the training.
We're going to train you think 2000 times.

507
00:32:33,040 --> 00:32:34,810
And why don't you thousand? I
don't know. I mean we could,

508
00:32:34,840 --> 00:32:38,290
we could try 10,000 in a second.
I don't know what I'm saying. So,

509
00:32:38,440 --> 00:32:43,010
and that's what we, we, we train for
a number of the box with your results.

510
00:32:43,100 --> 00:32:47,900
And then we say, well, is this,
is this prediction accurate
or not? And then if not,

511
00:32:47,901 --> 00:32:51,740
then we'll change. Or two of are
hyper parameters. And again, okay,

512
00:32:51,741 --> 00:32:55,320
so then the number of displaced sex.
Um,

513
00:32:55,460 --> 00:32:57,830
so how often do we want to display?
Uh,

514
00:32:59,910 --> 00:33:02,620
so then how often do we want to display,
uh,

515
00:33:03,150 --> 00:33:05,550
the process of training
and the number of samples,

516
00:33:05,840 --> 00:33:09,870
which is going to be the size of the
number of labor, which is 10. Okay,

517
00:33:10,080 --> 00:33:11,310
so that was our hydropower.

518
00:33:11,340 --> 00:33:15,630
Those who are hyper parameters and now
guys we weren't ready to compile and make

519
00:33:15,631 --> 00:33:18,470
sure that we're on is to
create our computation brand.

520
00:33:19,140 --> 00:33:20,880
So a bit uncomfy vision,

521
00:33:20,881 --> 00:33:25,260
create our computation graph
slash neural network. Okay,

522
00:33:25,410 --> 00:33:27,840
so convocation grab neural
network that the same thing.

523
00:33:28,230 --> 00:33:32,100
But before we get to that, let we do one
quick review of what we've done so far.

524
00:33:32,430 --> 00:33:35,340
We imported our dependencies.
Then we loaded our data,

525
00:33:35,341 --> 00:33:38,910
which is a CSV files a dick.
The data can take a bunch of features,

526
00:33:39,090 --> 00:33:42,870
but we only wanted to use the area
and the bathrooms as our features.

527
00:33:42,930 --> 00:33:45,540
We removed everything else
using those two things.

528
00:33:45,540 --> 00:33:49,170
We went to predict if a housing
price is good or not. But guess what?

529
00:33:49,171 --> 00:33:51,270
There are no labels in our data.
So what did we do?

530
00:33:51,420 --> 00:33:55,710
We added labels because data frame
objects are really easy to Parse,

531
00:33:55,800 --> 00:34:00,440
add data from, remove data from. And
so we added labels. Why? One is when,

532
00:34:00,450 --> 00:34:04,410
why one has a value one, it means it's a
good vibe. When y two has a good value,

533
00:34:04,710 --> 00:34:09,180
that is uh, that is,

534
00:34:10,470 --> 00:34:13,590
uh, it's not a good bottle. Then we
prepared our data for tensor flow.

535
00:34:13,620 --> 00:34:14,460
How do we do that?

536
00:34:14,520 --> 00:34:19,060
We looked at our data for an object and
we converted our features into an input

537
00:34:19,061 --> 00:34:22,230
tenser and then we convert our
labels between a potential as well.

538
00:34:22,260 --> 00:34:26,220
We printed them out, we defined our hyper
parameters. And now we're going to do,

539
00:34:26,730 --> 00:34:31,500
we're going to write out
our computation graph. Okay?

540
00:34:31,620 --> 00:34:32,453
So,

541
00:34:33,100 --> 00:34:33,570
okay,

542
00:34:33,570 --> 00:34:35,520
let's go ahead and write out
in our computation graph.

543
00:34:35,730 --> 00:34:38,370
So let's go ahead and do that. So the
person, I'm going to do these crazy,

544
00:34:38,380 --> 00:34:42,520
I'm going to create a placeholder for
our feature inkling tensor that we just

545
00:34:42,521 --> 00:34:43,550
defined.
He'd put bets.

546
00:34:43,890 --> 00:34:48,100
And what this is doing is so

547
00:34:49,050 --> 00:34:49,610
okay

548
00:34:49,610 --> 00:34:53,650
or [inaudible] for our feature
input teacher input pencil.

549
00:34:57,860 --> 00:35:01,370
So in flow we'll feed it will
feed an array of examples.

550
00:35:01,400 --> 00:35:05,090
Each example will be an array of to
float values area, a number of bathrooms.

551
00:35:05,510 --> 00:35:10,310
And so none means any number of examples,

552
00:35:10,630 --> 00:35:13,240
you know, could the fund,
which is usually the backsides,

553
00:35:13,250 --> 00:35:16,310
but we can say we're just going
to say none. So it's just generic.

554
00:35:16,311 --> 00:35:20,400
We can say however many we want,
I will allocated. That will help.

555
00:35:20,510 --> 00:35:22,100
We allocated a number of beforehand.

556
00:35:22,310 --> 00:35:27,310
And this allows us to have a number of
examples and are two because that's the,

557
00:35:27,900 --> 00:35:31,700
the size of disinfectants are.
Okay.

558
00:35:31,701 --> 00:35:36,180
So I'm sorry, not 32 to
that. My, my mistake.

559
00:35:36,210 --> 00:35:41,190
So two, because we have two features.
Okay. Um, and, and it's a cutie matrix,

560
00:35:41,191 --> 00:35:43,830
right? Because we have two
features. So we've done that.

561
00:35:43,831 --> 00:35:47,790
And so the next step is
to now create our weights.

562
00:35:47,940 --> 00:35:52,940
So create another thing that
placeholder up vet are gateways.

563
00:35:53,700 --> 00:35:57,090
So play folders in flow.
Our gateway for data.

564
00:35:57,091 --> 00:36:01,590
It's how we feed data in for a computation
graph or may wait for data into our

565
00:36:01,591 --> 00:36:05,250
computation graph. All right, so
the next step is you create wins.

566
00:36:05,310 --> 00:36:09,300
So how are we going to create our weights?
So we're going to define our weights.

567
00:36:09,320 --> 00:36:12,430
Has W and we're going to say F. Dot. Gary.

568
00:36:12,440 --> 00:36:14,970
Carryable and so we can upload widget.

569
00:36:16,080 --> 00:36:21,080
So we'll define our weights and it's going
to start off as a set of zero because

570
00:36:21,451 --> 00:36:24,990
we haven't, and that's how you
should start off your words. Um,

571
00:36:25,080 --> 00:36:29,850
for a simple example like this, um,
although with transfer learning, uh,

572
00:36:29,970 --> 00:36:33,270
you know, your weight aren't zero. They're
there, they've been trained beforehand.

573
00:36:33,540 --> 00:36:36,030
So this is a two by two float matrix,

574
00:36:36,150 --> 00:36:39,840
two by two float flotations and,

575
00:36:40,200 --> 00:36:44,340
and we're going to keep updating them.
We'll keep updating

576
00:36:46,080 --> 00:36:50,470
through the training process I
thought we were going to do. Uh,

577
00:36:50,790 --> 00:36:53,920
and so why don't we use a variable well,

578
00:36:53,980 --> 00:36:56,970
intenser flow of variable is,
uh,

579
00:36:57,030 --> 00:37:00,380
when they variables hold
an update parameter.

580
00:37:00,410 --> 00:37:04,680
So variables in TF,
cold and upbeat parameters,

581
00:37:05,010 --> 00:37:09,970
and you'd have weights.
We have weights or any,

582
00:37:10,410 --> 00:37:13,700
any other things you want to,
so they're,

583
00:37:14,040 --> 00:37:18,820
they're basically in their feed memory
buffers, campaigning. Tensors okay.

584
00:37:18,940 --> 00:37:21,960
Um, to be a little more
technical about it, but

585
00:37:23,550 --> 00:37:28,140
these are our ways. Okay. So the next
step is to add our biases. So, um,

586
00:37:28,680 --> 00:37:33,480
we're going to add our
biases. So advisees. And so
we want to biopsies, right?

587
00:37:33,481 --> 00:37:35,910
Because we have too many points.
Um,

588
00:37:37,130 --> 00:37:39,630
but we want to buy two because we
have two inputs and we're going to,

589
00:37:39,660 --> 00:37:44,160
this is also going to be
ATF variable. Um, okay, so

590
00:37:47,350 --> 00:37:49,930
just say two by two matrix.
There are in fact that out.

591
00:37:50,280 --> 00:37:55,080
And we're going to close the bracket
here and add a closing parentheses.

592
00:37:55,260 --> 00:37:58,500
Okay? So let's see what this
actually what we're going to do. One.

593
00:37:58,560 --> 00:38:00,030
So let me show you that.
But we have so far,

594
00:38:00,120 --> 00:38:03,000
so now what we've done is
with printer input tensor,

595
00:38:03,450 --> 00:38:08,450
we have any actually uses little square
thing and we've created our weights and

596
00:38:08,701 --> 00:38:11,070
now we're going to add our biases.
So this is what we're about to do.

597
00:38:11,071 --> 00:38:16,050
So we're going to have these other
purple square looking, uh, dots.

598
00:38:16,230 --> 00:38:18,660
So yes, we're gonna use that combination
to update or within a second.

599
00:38:18,661 --> 00:38:22,170
But first we have to define our biases.
And why don't you use biases?

600
00:38:23,460 --> 00:38:28,290
Biases are going to help, uh, our model
fits better. So a good example is like,

601
00:38:29,070 --> 00:38:34,070
so an example is the in the y
equals mx plus B equation like that.

602
00:38:37,910 --> 00:38:42,550
So because we take that line,
it adds the y intercept would make,

603
00:38:42,551 --> 00:38:47,100
which makes it fit better. So that's
what biases do date. They are there,

604
00:38:47,120 --> 00:38:50,860
there are a part of the larger, uh, uh,

605
00:38:51,730 --> 00:38:54,910
goal. They, it's like, you
know, if you didn't have,

606
00:38:54,911 --> 00:38:58,150
be in the labels and I suppose be the
line wouldn't be to the red line that

607
00:38:58,151 --> 00:39:02,130
you're looking for,
right? So, um, okay. Um,

608
00:39:03,250 --> 00:39:06,370
and I'll, I'll go over everything
we've done at the end guys.

609
00:39:06,371 --> 00:39:10,740
So right now we've added our biases
and so the next step is to, uh,

610
00:39:11,200 --> 00:39:13,450
calculate.
We're going to perform some matrix back.

611
00:39:13,451 --> 00:39:17,950
So now we're going to multiply
our rates by Howard inputs.

612
00:39:18,040 --> 00:39:22,150
So this is our first calculation
that's happening here, right?

613
00:39:22,151 --> 00:39:26,520
Because weights are what we wait for,
how we govern, how data flows in,

614
00:39:26,750 --> 00:39:29,890
in our copy,
wait for how we govern,

615
00:39:30,160 --> 00:39:33,690
how data flows in our communications.
Okay.

616
00:39:33,760 --> 00:39:35,410
So let's go ahead and perform this.

617
00:39:35,411 --> 00:39:40,060
So we'll say we'll define it
as y values and we'll say,

618
00:39:40,320 --> 00:39:42,630
and um, we'll say yes. Wait,

619
00:39:42,631 --> 00:39:47,560
can we can initialize weights as
random as well? Um, and depending on,

620
00:39:47,590 --> 00:39:50,920
you know, what you're
doing, it can be different.

621
00:39:50,921 --> 00:39:53,080
But right now we're
initializing kind of zeroes.

622
00:39:53,100 --> 00:39:56,200
And to perform this actual
matrix multiplication step,

623
00:39:56,680 --> 00:40:00,480
the has this great built in Matrix
multiplication function, um,

624
00:40:02,940 --> 00:40:04,260
uh, that we can say, okay,

625
00:40:04,261 --> 00:40:08,480
so take mark he puts and then our
weights and then our biases and what,

626
00:40:08,481 --> 00:40:09,480
what are we going to do here?

627
00:40:09,690 --> 00:40:13,500
We're going to calculate the prediction
to mold and to do that, to do that,

628
00:40:13,501 --> 00:40:15,630
we're going to multiply to
input matrix by the way matrix,

629
00:40:15,631 --> 00:40:18,190
and then had the biases. So
let me write that up. Um,

630
00:40:18,450 --> 00:40:23,450
multiplied inputs by
weights and cad and pad.

631
00:40:24,840 --> 00:40:27,470
Okay?
So it's kind of like y equals mx plus B.

632
00:40:27,490 --> 00:40:32,460
It's similar kind of operations. Okay? So

633
00:40:33,910 --> 00:40:36,940
we've done that and now, now we're going
to do mark softmax assumption. Okay?

634
00:40:36,941 --> 00:40:39,610
When you start sigmoid softmax
is another word for six sigma.

635
00:40:39,611 --> 00:40:42,730
So we've done this part, right? And
now we're going to do this part,

636
00:40:42,731 --> 00:40:45,880
which is we're going to take those values
and we're going to apply a soft Max to

637
00:40:45,881 --> 00:40:48,850
it. And what does soft Max
do? Does anyone remember?

638
00:40:48,851 --> 00:40:51,610
Shattered on the comments and then explain
it in a second. I gonna type it out.

639
00:40:51,800 --> 00:40:53,380
If someone can check that out,

640
00:40:53,410 --> 00:40:56,700
I will give you a chat about if you can
tell me what the softmax function best.

641
00:40:56,710 --> 00:40:58,350
So we're going to say softmax

642
00:41:00,910 --> 00:41:05,210
I stopped next to this value
that we created, right? Um,

643
00:41:05,350 --> 00:41:08,090
we're going to apply herself
next to his value applied

644
00:41:12,400 --> 00:41:17,240
Max to value we've just created. Okay. So,

645
00:41:17,241 --> 00:41:19,190
and then what does,
it's an activation function.

646
00:41:19,610 --> 00:41:24,610
So now an activation function
and it does somethings.

647
00:41:25,390 --> 00:41:29,870
Ouch. Okay. So let me go ahead and hand.

648
00:41:31,900 --> 00:41:35,030
So I just, yes, it normalizes our value.

649
00:41:35,031 --> 00:41:38,020
So what soft Max does is,
uh,

650
00:41:38,570 --> 00:41:41,840
it normalizes our value.
And what do I mean by that?

651
00:41:41,900 --> 00:41:45,320
It takes our value and it converts it to
a probability that we can then feed to

652
00:41:45,321 --> 00:41:48,240
our output. So it's our last step
before we feed to our help. But,

653
00:41:48,380 --> 00:41:52,640
so let me make sure that I typed
all this issue out perfectly. Okay.

654
00:41:52,641 --> 00:41:54,350
So online 14,
TF variable,

655
00:41:54,351 --> 00:41:59,200
that Zeros I said to Barry Zeroes,

656
00:41:59,390 --> 00:42:03,110
oh, I need a bracket here. Okay.

657
00:42:05,310 --> 00:42:10,190
Zeros. The laser focus on
what if I what I've done here.

658
00:42:10,191 --> 00:42:15,080
So, um, let's see.

659
00:42:16,820 --> 00:42:19,040
Interesting.
So there is a,

660
00:42:19,490 --> 00:42:23,630
an extra value here.
Um,

661
00:42:25,340 --> 00:42:26,540
let's see what's going down here.

662
00:42:30,510 --> 00:42:35,120
Tea, half done variable. Tf
Dot Zeros. Oh, extra dot.

663
00:42:35,180 --> 00:42:39,510
Okay. Uh, okay,

664
00:42:39,511 --> 00:42:43,950
so now the next step is to say
how is good.

665
00:42:43,951 --> 00:42:48,930
You know what it is? It's all good. Okay.
So I need to add a bracket there, I think,

666
00:42:49,500 --> 00:42:51,940
right? Yeah. So I'm
heading one pair and I'm,

667
00:42:51,941 --> 00:42:55,080
once you add one to here
and then we'll say compile.

668
00:42:57,220 --> 00:43:00,820
And so then for the wide I use,
it's telling me what I've done wrong.

669
00:43:00,821 --> 00:43:04,810
So it's saying, hey
listen buddy, listen, you

670
00:43:06,940 --> 00:43:11,860
need to apply your matrix.
Multiply x, W B, TF.

671
00:43:11,940 --> 00:43:14,950
Dot Add. Um, okay,

672
00:43:17,020 --> 00:43:19,150
what else we got here?
Why values

673
00:43:22,010 --> 00:43:26,000
and let's see.
Name acts is not defined.

674
00:43:26,740 --> 00:43:31,430
Tax is not defined. Do you mean said,
oh, because I didn't define it. Yes.

675
00:43:31,640 --> 00:43:36,200
So x is that value. Okay, great. Grit.

676
00:43:36,500 --> 00:43:38,720
So I hit enter.

677
00:43:38,990 --> 00:43:42,860
So I need to go ahead and add one
more thing for training. So, okay.

678
00:43:42,861 --> 00:43:46,760
So what do we just do? We, we added
the code is going to go online. Yes,

679
00:43:46,761 --> 00:43:50,600
I'm going to post it in the comments when
I'm done. We've got 15 minutes. Okay.

680
00:43:50,601 --> 00:43:52,000
So what we're going to do is when,

681
00:43:52,001 --> 00:43:56,870
and say you've got that soft Max value
and then we can get four outputs.

682
00:43:57,350 --> 00:44:00,320
So let me type that out.

683
00:44:02,140 --> 00:44:05,750
So what, what are we doing
here? We're going to feed, oh,

684
00:44:05,751 --> 00:44:07,200
so we never did this part.

685
00:44:07,201 --> 00:44:11,290
So we're gonna say eat
in a matrix of labels.

686
00:44:11,340 --> 00:44:14,930
So let me just go over what
I've just done. Okay. So,

687
00:44:16,630 --> 00:44:19,840
uh, so these are our
two placeholders, right?

688
00:44:19,870 --> 00:44:22,840
So x is up here and then why
is down here, but this, right?

689
00:44:22,841 --> 00:44:27,790
So one is our labels and then one,
one is our set of features.

690
00:44:27,910 --> 00:44:30,570
We created our weights, we
and our biases. And then we,

691
00:44:30,580 --> 00:44:34,370
this is our step right here.
We multiplied our weights by our,

692
00:44:34,710 --> 00:44:38,010
by our inputs and we ended our biases.
And we,

693
00:44:38,430 --> 00:44:43,260
and then we use that value
that we calculated and we
input it into the softmax

694
00:44:43,261 --> 00:44:47,040
layer, which converted into
a set of probabilities. Okay,

695
00:44:48,390 --> 00:44:51,100
so, so there's that. And now when did you

696
00:44:53,050 --> 00:44:56,160
apply?
And we're going to do to perform a,

697
00:44:56,220 --> 00:44:57,870
our training staff before the trainings.

698
00:44:57,880 --> 00:45:01,620
So we've done these steps and now
step six is to perform training.

699
00:45:01,680 --> 00:45:06,010
Step six is to perform
credit. All right? So, uh,

700
00:45:06,210 --> 00:45:09,740
let's go ahead. And so this is,
this is the magic part of, to folks.

701
00:45:09,740 --> 00:45:13,800
So if you were hearing the live session
last week, then we did this manual.

702
00:45:14,250 --> 00:45:16,940
So totally to write out
what we're about to see.

703
00:45:16,980 --> 00:45:21,980
What we're gonna do is we're going to
create our cost function and the cost

704
00:45:22,891 --> 00:45:26,790
function. We're going to using
the mean squared error. All right?

705
00:45:26,791 --> 00:45:28,710
So we did this last live session.

706
00:45:29,730 --> 00:45:32,930
One of the great things about a
quarter is that we have continuity. Uh,

707
00:45:33,190 --> 00:45:34,230
and so we can say,

708
00:45:36,480 --> 00:45:41,180
we can say minus y this is
going to be the equation.

709
00:45:41,190 --> 00:45:45,060
And I'm going to show the equation in
a second. But what this does is it says

710
00:45:46,980 --> 00:45:49,320
we want to calculate the error.

711
00:45:49,380 --> 00:45:52,850
So between our two wide values and uh,

712
00:45:53,070 --> 00:45:57,250
we're going to divide by the number of,
so the mean square error is where we,

713
00:45:57,260 --> 00:46:01,620
you take the average of the, uh,
difference in values, which is our,

714
00:46:01,621 --> 00:46:06,470
Eric's the square difference.
And that's what this is doing.

715
00:46:06,480 --> 00:46:11,130
So, um, so reduce some, basically
computes the element of the,

716
00:46:11,131 --> 00:46:13,470
some of elements across the
dimensions of the or. So let me,

717
00:46:13,480 --> 00:46:16,890
let me write that down.
We'll use some confused,

718
00:46:19,350 --> 00:46:20,880
confused elements,

719
00:46:22,350 --> 00:46:26,190
some elements across the
dimensions of a cancer.

720
00:46:26,580 --> 00:46:30,390
And so then let me show the equation
for the 10 seconds what we just did.

721
00:46:31,800 --> 00:46:35,870
So the two different
values we're using are the,

722
00:46:36,440 --> 00:46:37,273
uh,

723
00:46:40,630 --> 00:46:42,070
the two different guys are using is,

724
00:46:42,130 --> 00:46:47,080
are the predicted the predicted values
and then the actual values for the

725
00:46:47,081 --> 00:46:49,090
outputs.
That is our cost function.

726
00:46:49,091 --> 00:46:52,330
We want to minimize this
cost function over time. And

727
00:46:53,830 --> 00:46:57,570
uh, to do that we're going
to perform gradient descent.

728
00:46:57,960 --> 00:46:59,910
So we're not,

729
00:46:59,911 --> 00:47:04,010
so the great thing about tends to flow
if we can just refine in the sent had our

730
00:47:04,020 --> 00:47:07,590
optimization function. We don't have
to actually write out every step.

731
00:47:07,830 --> 00:47:09,510
And you radiate the scent.

732
00:47:09,720 --> 00:47:14,400
Every step means where computing the
partial derivative with respect to our

733
00:47:14,580 --> 00:47:18,290
input variables. What do you
know? Our case would be the uh,

734
00:47:19,660 --> 00:47:24,470
which in our case we made a
set of weights and the biases.

735
00:47:24,570 --> 00:47:26,980
So that's it.
I could just say we have a cost function.

736
00:47:27,010 --> 00:47:30,040
We want to minimize that cost using
gradient descent and are running where it

737
00:47:30,041 --> 00:47:33,100
will define how fast we want to do that.
Okay,

738
00:47:36,610 --> 00:47:41,420
so do not call me during
the live session. Okay.

739
00:47:41,480 --> 00:47:45,830
So let's see what we've got here.
We're going to say

740
00:47:47,860 --> 00:47:52,860
why is here right and a right.

741
00:47:53,211 --> 00:47:56,050
So this should be why underscoring.
So that's why that didn't work.

742
00:47:56,530 --> 00:48:01,360
And so the next thing is we want to
make sure that name, why is not defined.

743
00:48:01,480 --> 00:48:05,860
How about now? No, it is different
because I just updated it. There we go.

744
00:48:06,160 --> 00:48:10,810
Great. So yes, so I updated the wine and
now we're going to create our session.

745
00:48:11,310 --> 00:48:14,590
Um, okay, so let's initialize our session.

746
00:48:14,810 --> 00:48:16,180
Is this still a part
of the training stuff?

747
00:48:16,181 --> 00:48:19,150
We're going to initialize our
variables and tend to float session.

748
00:48:19,151 --> 00:48:23,710
So intense or flow,
we encapsulates our computation graph.

749
00:48:26,500 --> 00:48:26,781
We should,

750
00:48:26,781 --> 00:48:31,781
we encapsulate our computation graph
using and utilize all variables using a

751
00:48:34,281 --> 00:48:36,170
session object.

752
00:48:36,350 --> 00:48:40,340
So intense flow whenever you want to do
any kind of training because the first

753
00:48:40,610 --> 00:48:43,940
initialized all variables and what
additional is all variables us, they just,

754
00:48:44,690 --> 00:48:45,650
it does exactly what it says,

755
00:48:45,660 --> 00:48:47,780
initialize every variable
that you declared beforehand.

756
00:48:48,230 --> 00:48:49,840
That means those TFR variables,

757
00:48:49,850 --> 00:48:54,230
we declared it beforehand
and the placeholder objects
because we did beforehand,

758
00:48:54,231 --> 00:48:59,000
right? So every enter flow generic
variable that we defined beforehand,

759
00:48:59,060 --> 00:49:02,300
it's conditional as it for the session.
So I mean, this could be, you know,

760
00:49:02,360 --> 00:49:05,390
this could be done under the hood as well,
but for now we're going to,

761
00:49:05,420 --> 00:49:08,550
we're going to,
the textbook asks us to define a deal.

762
00:49:09,170 --> 00:49:11,870
So we print our ascension, I
work session, and then we just,

763
00:49:11,900 --> 00:49:16,850
we run it by using the, all the variables
that we've initialized as our inputs.

764
00:49:17,000 --> 00:49:20,470
Okay.
And of course there's an,

765
00:49:20,500 --> 00:49:24,800
they are so no attribute,
initial, all variable. Uh,

766
00:49:24,801 --> 00:49:29,000
so what, what was that? So initialize
all variables with, hey, uh,

767
00:49:30,830 --> 00:49:35,570
a set of parentheses and
then he f. Dot. Session.

768
00:49:35,870 --> 00:49:40,420
Let's see what else we got here. Tf Dot
initialize. I misspelled initialize.

769
00:49:41,130 --> 00:49:44,900
Okay.
Initial lines.

770
00:49:45,710 --> 00:49:50,610
Great. Okay. And so here we go. Let's,
let's do that. This is the last bit, uh,

771
00:49:50,611 --> 00:49:54,590
that we're going to do. This is the actual
trading. This is a training room. Okay.

772
00:49:54,620 --> 00:49:57,490
So let's go ahead and do this
for every night for every, uh,

773
00:49:57,550 --> 00:50:00,830
p that we have for training detox.

774
00:50:02,310 --> 00:50:06,560
How many people do we have in this life?
Section? 300 people go. Awesome. Okay,

775
00:50:06,561 --> 00:50:08,560
so for,
so for every training evolve,

776
00:50:11,950 --> 00:50:16,570
um, if we define as 300, but what
was it? What was the number 2000?

777
00:50:16,571 --> 00:50:21,010
We defined as 2000. Well, to run
our session, given our optimizer,

778
00:50:21,040 --> 00:50:22,800
which is gradient that standard,

779
00:50:23,080 --> 00:50:27,460
so this is where we're performing rating
descent and we're going to feed in to

780
00:50:27,490 --> 00:50:30,390
where we actually feed into the
placeholders that we get fine earlier.

781
00:50:30,730 --> 00:50:34,210
So those placeholders are going to be that
the first is going to be that x value,

782
00:50:34,450 --> 00:50:37,370
which is going to be our teachers input x.

783
00:50:37,490 --> 00:50:41,180
And the next one is going to be for the
labels, would just want it to be art,

784
00:50:41,250 --> 00:50:44,990
eclipse. Why? Uh, okay. So,

785
00:50:46,270 --> 00:50:47,103
okay,

786
00:50:47,190 --> 00:50:50,340
how does that and so that's it. And so
now that's it really. And now we can just,

787
00:50:50,610 --> 00:50:50,881
you know,

788
00:50:50,881 --> 00:50:55,650
write out our debugging methods
versus going to be right out

789
00:50:57,830 --> 00:50:59,850
of training.
So

790
00:51:02,160 --> 00:51:05,320
if I display stack

791
00:51:07,110 --> 00:51:10,110
and then we'll say,
let's run this session,

792
00:51:11,100 --> 00:51:14,250
our cost function.
And then we went to,

793
00:51:16,640 --> 00:51:18,900
um, and we are too hard
just to get this stuff.

794
00:51:18,920 --> 00:51:22,720
We're doing this just to print it out
just so we can see, um, you know, what,

795
00:51:22,721 --> 00:51:27,340
what's happening at each step.
Okay.

796
00:51:27,341 --> 00:51:30,760
So our input is going to be that first,
um,

797
00:51:31,660 --> 00:51:35,380
tenser and then other than that,
the other temps or that we,

798
00:51:36,520 --> 00:51:37,100
yeah,

799
00:51:37,100 --> 00:51:39,290
that we defined,
yeah.

800
00:51:43,060 --> 00:51:47,650
Frames that
it's going to be,

801
00:51:49,370 --> 00:51:50,100
yeah.

802
00:51:50,100 --> 00:51:54,090
No.
So this part I'm just gonna

803
00:51:57,180 --> 00:52:01,830
is this part right here because
we don't actually, it's just,

804
00:52:02,010 --> 00:52:04,170
it's just long. So that's the best
way really. So what we're doing,

805
00:52:04,171 --> 00:52:06,460
if we're printing out the training staff,
and let me,

806
00:52:06,510 --> 00:52:11,470
let me just go ahead and run this now.
Okay.

807
00:52:11,471 --> 00:52:12,910
So our first impacts there.

808
00:52:13,180 --> 00:52:16,900
So it's going to be x.

809
00:52:17,570 --> 00:52:18,680
It doesn't mean that

810
00:52:20,640 --> 00:52:21,473
I do

811
00:52:22,680 --> 00:52:27,240
that. We got here, uh, ex and put x, Y

812
00:52:28,410 --> 00:52:29,243
hmm.

813
00:52:32,310 --> 00:52:33,480
[inaudible] walls.

814
00:52:34,150 --> 00:52:34,983
[inaudible]

815
00:52:35,410 --> 00:52:37,060
session dot.
Brian.

816
00:52:37,940 --> 00:52:38,773
Yeah.

817
00:52:38,820 --> 00:52:43,290
Equals Yo. Okay. Yes. Okay. Okay.

818
00:52:43,320 --> 00:52:47,280
Awesome. So boom. So that's
how fast the train because,

819
00:52:47,580 --> 00:52:51,210
because it was only 10 values.
Okay.

820
00:52:51,211 --> 00:52:54,720
It was only $10. And so what is
happening here? Let's look at this.

821
00:52:54,900 --> 00:52:58,830
The first column gets, aren't
set up training steps, right?

822
00:53:00,810 --> 00:53:05,100
The next one is our cost function and
the cost of entry is minimized over time.

823
00:53:05,430 --> 00:53:08,580
Okay. And, and then eventually
it's finished training.

824
00:53:08,581 --> 00:53:13,500
So what is the w so it ends up with a
cost function of 0.109, whatever. Right?

825
00:53:13,650 --> 00:53:15,810
So this, is this good or
bad? I don't really know,

826
00:53:15,811 --> 00:53:18,630
but it's better than the first cost value,
that's for sure.

827
00:53:18,720 --> 00:53:21,990
We don't know if it's a good buck
cost cheat or not because we kind of

828
00:53:22,020 --> 00:53:24,210
arbitrarily decide,
uh,

829
00:53:25,060 --> 00:53:28,330
once there's labels were Harkes awesome.

830
00:53:28,550 --> 00:53:30,790
Let's go ahead and test this out
and how are we going to test it?

831
00:53:33,650 --> 00:53:37,620
I have a call slash work, Bro.
And I'm also like really sick,

832
00:53:37,890 --> 00:53:41,670
which is like I don't have time for
that guide and I'm kicking this dope ass

833
00:53:42,100 --> 00:53:44,100
chorus. Okay. So we're going to feed him

834
00:53:47,360 --> 00:53:50,970
arc input and we're going to test it out.
We're going to see what happens now.

835
00:53:51,530 --> 00:53:55,380
So this is our output. So what is it
here? So this is the prediction, right?

836
00:53:55,381 --> 00:53:58,170
So we've, we've trained our model and now
we're going to do our prediction, right?

837
00:53:58,230 --> 00:54:00,240
And so it's guessing,

838
00:54:00,241 --> 00:54:04,350
but they're all good houses because
these values are all above. Um,

839
00:54:05,760 --> 00:54:10,160
so rimmer remember I said we have y
one and y zero. So it's saying, so,

840
00:54:10,200 --> 00:54:12,240
so, um, let me, let me go,

841
00:54:12,270 --> 00:54:14,640
let me go back up for a second so you
guys can see what I'm talking about here.

842
00:54:14,850 --> 00:54:18,930
So we defined, remember this y one and
y two. That's what it's outputting guys.

843
00:54:19,020 --> 00:54:21,900
That's what it's out. And it's just a wide
one in the white too. And it's saying,

844
00:54:22,110 --> 00:54:26,550
um, these values rounded in our
closest to one right on the left side.

845
00:54:26,760 --> 00:54:29,970
So that means it's all one.
And the valleys on the right
are all closest to zero,

846
00:54:30,150 --> 00:54:32,700
which means zero. So what it's
saying, it's, it's, it's saying

847
00:54:35,130 --> 00:54:40,080
all houses are a good buck, but that's
not the case. So exact seven had a 10.

848
00:54:40,081 --> 00:54:45,000
Correct. But the actual thing is
it wasn't, they weren't all good.

849
00:54:45,210 --> 00:54:47,640
There were just, some of them
are good, right? So only three,

850
00:54:48,370 --> 00:54:52,920
only some of them are good. So seven
out of 10 correct. Which is not bad.

851
00:54:53,070 --> 00:54:57,060
How to improve. Um, maybe we could add
a hidden layer, add a hidden layer.

852
00:54:57,940 --> 00:54:59,310
Had I been there,
that's what I would do.

853
00:54:59,311 --> 00:55:02,940
I would add a hidden layer and then I
would try it again. So that's a kind of,

854
00:55:03,090 --> 00:55:06,180
so that's my example for this.
Okay.

855
00:55:06,181 --> 00:55:09,450
So let's go back to screen sharing.
Okay.

856
00:55:11,650 --> 00:55:15,500
Subsequent turn. Go back
to me. Hi. Okay. So, uh,

857
00:55:15,580 --> 00:55:19,360
that's what we did for
that and I'm going to, uh,

858
00:55:19,390 --> 00:55:23,320
I'm going to add the coaching to get
[inaudible] five minutes ended Q and a and

859
00:55:23,321 --> 00:55:25,420
then we're good to go.
And I haven't,

860
00:55:25,690 --> 00:55:28,450
I have a video for you guys to be out on
Friday, which I'm super excited about.

861
00:55:28,830 --> 00:55:31,570
Hit me up with your
questions. Let's go. Anything.

862
00:55:34,580 --> 00:55:39,020
Why do you think Jupiter, any
special perks? Um, Jupiter because,

863
00:55:40,630 --> 00:55:43,610
uh, it allows you to compile it and
see what you're doing in real time.

864
00:55:43,611 --> 00:55:46,340
It's great for visually looking at,
especially for data,

865
00:55:46,520 --> 00:55:50,840
it's great for visually looking at what
you're doing, exactly what Jake said.

866
00:55:50,900 --> 00:55:55,700
How do you choose a number of hidden
layers or not of unites while trying to

867
00:55:55,701 --> 00:55:59,720
develop a neural network?
It depends on how big your data is.

868
00:55:59,721 --> 00:56:03,860
The more data you have, generally the more
hidden layers, the more data you have,

869
00:56:04,760 --> 00:56:06,740
the better it is to
add more hidden layers.

870
00:56:09,390 --> 00:56:13,900
Um, right. Pi Torch versus
tensorflow for newcomer. Absolutely.

871
00:56:13,901 --> 00:56:17,740
Tentraflow don't even don't even trip
well, but you know, pie for texting,

872
00:56:17,770 --> 00:56:21,610
great architectural ideas
that I think we can, you know,

873
00:56:21,880 --> 00:56:24,290
we'll see where that goes. It's a
really exciting space, guys. It's,

874
00:56:24,320 --> 00:56:28,510
it's only January and we're already
seeing so much innovation in this field.

875
00:56:28,720 --> 00:56:33,500
In fact, this pace of innovation
is accelerating. It's insane. Um,

876
00:56:33,820 --> 00:56:37,450
how many layers should
we add for this case? Uh,

877
00:56:38,230 --> 00:56:41,380
I just need to act. Um, can you say
something about Google's deep mind?

878
00:56:41,381 --> 00:56:44,410
Was she running? Um, well,

879
00:56:44,440 --> 00:56:48,100
I mean they haven't really done
anything this year yet, but just wait,

880
00:56:48,370 --> 00:56:53,140
give them like two weeks. I expect to
see some state of Vr in something. Yeah.

881
00:56:53,170 --> 00:56:56,830
Paper in like within two weeks.
How to classify an image data set.

882
00:56:56,831 --> 00:57:00,880
We're going to do that in two episodes
before this course up been high level.

883
00:57:00,940 --> 00:57:04,760
You want to label Dataset a
supervisor approach. Um, uh,

884
00:57:04,900 --> 00:57:07,240
with labels a look at Google's in section.

885
00:57:07,270 --> 00:57:08,420
You don't even have to training herself.

886
00:57:08,440 --> 00:57:11,770
They were already treated when a million
images and you can use a transfer

887
00:57:11,771 --> 00:57:16,520
wanting to apply that and then add in
whatever other images you want to classify

888
00:57:16,521 --> 00:57:18,800
it. And because of transfer
learning and you don't need it back,

889
00:57:18,801 --> 00:57:19,960
bigger data set to train it.

890
00:57:20,200 --> 00:57:23,650
And I have a video on that called
guilty tends to image classifier in five

891
00:57:23,651 --> 00:57:26,380
minutes. How much coffee
did you have? I had a cop.

892
00:57:28,200 --> 00:57:31,610
Is it the same way session? The one
we're doing with your Udacity? Yes. Um,

893
00:57:32,110 --> 00:57:35,670
anybody who has a recurrent neural nets
plans. Yes. It's coming out in three days.

894
00:57:36,210 --> 00:57:38,710
Uh,
do you think that you guys,

895
00:57:38,720 --> 00:57:42,870
the deep learning course apparently
for deep on a job? Yes. Uh, what image?

896
00:57:42,900 --> 00:57:46,020
If you do the assignments,
okay, that does it.

897
00:57:46,660 --> 00:57:50,190
Kavya and you have to duty assignments.
You can't just watch the videos,

898
00:57:50,340 --> 00:57:51,390
you to do it,

899
00:57:51,630 --> 00:57:55,590
do it like strive up do is I'm a
huge shower but then by the way guys,

900
00:57:56,430 --> 00:58:00,900
recently, um, cause he doesn't
work. Let's take war. Did you age?

901
00:58:00,901 --> 00:58:04,650
I'm 25. Did you study at university?
Yes. What computer science,

902
00:58:04,770 --> 00:58:09,120
specifically robotics. Um,
anything on memory networks? Yo,

903
00:58:09,240 --> 00:58:12,990
I want to do memory networks.
That's some dance shit.

904
00:58:13,020 --> 00:58:14,670
That's like generative
adversarial networks.

905
00:58:14,671 --> 00:58:16,860
Should that I find super
interesting at the cutting edge,

906
00:58:17,060 --> 00:58:19,680
the bleeding edge of the field. But
we've got to get our basics. Now.

907
00:58:20,030 --> 00:58:22,140
If you should take something
out of this slide session,

908
00:58:22,440 --> 00:58:26,740
what you should take out is, are um,

909
00:58:26,840 --> 00:58:27,471
three things.

910
00:58:27,471 --> 00:58:32,471
One how to do gradient descent
and backpropagation to um,

911
00:58:34,520 --> 00:58:37,130
uh, basic, uh, an idea of what the, uh,

912
00:58:37,610 --> 00:58:41,510
the main variables are with tensorflow.
That means placeholders,

913
00:58:41,600 --> 00:58:46,520
TF variables, the optimizer function,
uh, weights and biases. Okay.

914
00:58:46,521 --> 00:58:49,220
Because everything's going to build off
of that can get those simple things,

915
00:58:49,280 --> 00:58:52,090
then everything else is going to be much,
much busier.

916
00:58:52,960 --> 00:58:56,870
And the third thing is,
um,

917
00:58:57,570 --> 00:59:00,870
a softmax function. Uh,
sorry, activation functions.

918
00:59:01,070 --> 00:59:03,930
Activation function is what we use
to convert numbers of probabilities.

919
00:59:04,110 --> 00:59:07,410
There's several types of activation
functions. In this case we use softmax,

920
00:59:07,800 --> 00:59:10,140
which university? Columbia
University. But guys,

921
00:59:11,250 --> 00:59:15,750
I want to end this with one piece
of advice and then we're good to go.

922
00:59:17,700 --> 00:59:22,700
A lot of what we learn is what
we tell we are capable of doing.

923
00:59:23,150 --> 00:59:27,710
That's what degrees are. Degrees tell
us, it allow us to tell ourselves,

924
00:59:27,860 --> 00:59:32,060
hey, I now know this. I can do it, but
we have to flip that way of thinking.

925
00:59:32,120 --> 00:59:35,900
It's not about, oh, I have a degree
now I'm a computer scientist. Oh,

926
00:59:35,940 --> 00:59:40,400
I have a degree now I can do x. No,
if you study something and you do it,

927
00:59:40,610 --> 00:59:41,030
if you,

928
00:59:41,030 --> 00:59:44,600
if you create the code and it compiles
and it runs and it gets you help with

929
00:59:44,601 --> 00:59:48,350
that, you want it, you now
have the ability to do that.

930
00:59:48,351 --> 00:59:51,320
So don't be restricted by the Dogma of,
Oh,

931
00:59:51,321 --> 00:59:54,050
I need to read this or eat
this before it gets to this.

932
00:59:54,410 --> 00:59:55,850
Think of something you want to build.

933
00:59:55,880 --> 00:59:59,360
Start building and along
the way or what you need to,

934
00:59:59,361 --> 01:00:02,660
to know that by the end you are going
to get really good if you've ever

935
01:00:02,661 --> 01:00:06,800
engineered anything reported and you
know that if anybody asks you a problem

936
01:00:06,801 --> 01:00:11,050
about it, you will know every answer. Why?
Because you had to deep dive into it to,

937
01:00:11,070 --> 01:00:15,360
to be able to build that.
Okay? So that's it. Um,

938
01:00:16,190 --> 01:00:19,190
all right. So believe in yourself
specifically. Believe in yourself.

939
01:00:19,220 --> 01:00:23,660
The world needs you guys. Okay? We need
you. We are starting a revolution. Okay?

940
01:00:23,661 --> 01:00:27,410
We are in the midst of a revolution and
like anything the world has ever seen

941
01:00:27,411 --> 01:00:32,000
before and we're going to, we're
going to do some awesome shit. Okay?

942
01:00:32,001 --> 01:00:35,540
We're going to do some awesome
shit. Okay. That's it. Okay.

943
01:00:35,541 --> 01:00:38,360
So thanks guys for watching the
videos coming out on Friday.

944
01:00:38,390 --> 01:00:40,730
It's going to be dope and uh,

945
01:00:40,760 --> 01:00:43,610
continue the conversation in our
slack channel for now. I'm going to,

946
01:00:44,480 --> 01:00:49,370
I'm going to, yes. I'm going to do
the math tutorial in a second. Um,

947
01:00:50,420 --> 01:00:54,050
okay. So anyways, for now I've got to,

948
01:00:55,290 --> 01:00:55,660
okay.

949
01:00:55,660 --> 01:00:57,590
Work with you, Udacity on, uh,

950
01:00:58,480 --> 01:01:02,170
something I want to go down to if you'd
ask me and just hang with them and see

951
01:01:02,171 --> 01:01:04,990
what we can do. There's some really cool
people for now. That's what I got to do.

952
01:01:05,050 --> 01:01:08,320
So thanks for watching.
See you guys.

