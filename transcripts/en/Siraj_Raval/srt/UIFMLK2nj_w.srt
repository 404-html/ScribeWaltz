1
00:00:17,250 --> 00:00:20,010
Hello world, it's Saroj and
let's talk about optimization.

2
00:00:24,340 --> 00:00:28,330
There are thousands of languages
spoken across the world,

3
00:00:28,600 --> 00:00:33,600
each one unique in its ability to
represent concepts and convey ideas,

4
00:00:34,090 --> 00:00:37,750
but there is one language
that is shared by all humans.

5
00:00:37,751 --> 00:00:41,050
Regardless of where you come from.
Mathematics,

6
00:00:41,410 --> 00:00:43,900
no matter your culture or your age,

7
00:00:44,080 --> 00:00:49,080
you possess the ability to understand
this language of numbers that connects us

8
00:00:49,181 --> 00:00:53,230
all across continents and time.
Like all languages.

9
00:00:53,231 --> 00:00:57,580
Fluency requires practice,
but unlike any other language,

10
00:00:57,730 --> 00:01:00,070
the more fluent you become in math,

11
00:01:00,400 --> 00:01:04,840
the more unstoppable you'll be in
anything you want to do in life.

12
00:01:05,200 --> 00:01:10,200
Math is happening all around us to a
degree that most people don't realize.

13
00:01:11,440 --> 00:01:14,080
We can think of everything
as a set of variables,

14
00:01:14,110 --> 00:01:19,110
as metrics and there exists relations
between all of these variables.

15
00:01:19,840 --> 00:01:22,930
In math we call these relations functions.

16
00:01:23,140 --> 00:01:27,220
It's our way of representing
a set of patterns, a mapping,

17
00:01:27,221 --> 00:01:30,460
a relationship between at many variables,

18
00:01:30,520 --> 00:01:33,610
no matter what machine
learning model we use,

19
00:01:33,940 --> 00:01:35,980
no matter what Ada sent we use.

20
00:01:36,070 --> 00:01:41,070
The goal of machine learning is to
optimize or unobjective and by doing so we

21
00:01:42,431 --> 00:01:44,500
are approximating a function.

22
00:01:44,710 --> 00:01:49,710
The process of optimization helps us
iteratively discover a functions hidden in

23
00:01:50,501 --> 00:01:51,880
the depths of data.

24
00:01:52,300 --> 00:01:57,300
Last week we talked about a popular
optimization technique called gradient

25
00:01:57,430 --> 00:02:02,350
descent. This can be broken
down into a five step process.

26
00:02:02,560 --> 00:02:03,340
First,

27
00:02:03,340 --> 00:02:08,260
we define some machine learning model
with a set of initial weight values.

28
00:02:08,560 --> 00:02:13,560
These act as the coefficients of the
function at the model represents the

29
00:02:14,441 --> 00:02:18,670
mapping between input data
and output predictions.

30
00:02:19,000 --> 00:02:21,100
These values aren't naive.

31
00:02:21,130 --> 00:02:24,340
We have no idea what
they should actually be,

32
00:02:24,341 --> 00:02:27,640
but we're trying to
discover the optimal ones.

33
00:02:27,880 --> 00:02:31,750
We'll define an error function
and when we plot the graph up,

34
00:02:31,751 --> 00:02:36,751
the relationship between all the possible
error values and all of the possible

35
00:02:37,391 --> 00:02:41,590
weight values for our function will
see that there exists a valley,

36
00:02:41,770 --> 00:02:42,750
a minimum.

37
00:02:42,940 --> 00:02:47,940
We'll use our error to help us compute
the partial derivative with respect to

38
00:02:48,520 --> 00:02:52,660
each weight value we have and
this gives us our gradient.

39
00:02:53,080 --> 00:02:58,080
The gradient represents the change in
the air when the weights are changed by a

40
00:02:58,451 --> 00:03:01,960
very value from their original value.

41
00:03:02,380 --> 00:03:07,380
We use the gradient to update the values
of our weights in a direction such that

42
00:03:08,381 --> 00:03:13,381
the error is minimized iteratively coming
closer and closer to the Minima of the

43
00:03:13,781 --> 00:03:14,614
function.

44
00:03:14,830 --> 00:03:19,830
We step our solution in the negative
direction of the gradient repeatedly when

45
00:03:20,381 --> 00:03:21,220
we reach it.

46
00:03:21,430 --> 00:03:26,430
We have learned the optimal weight values
for our model where our gradient is

47
00:03:26,921 --> 00:03:29,560
equal to zero.
Our model,

48
00:03:29,561 --> 00:03:34,540
we'll then be able to make predictions
for input data it's never seen before.

49
00:03:34,660 --> 00:03:39,010
Most optimization problems can be
solved using gradient descent and its

50
00:03:39,011 --> 00:03:39,844
variance.

51
00:03:40,120 --> 00:03:45,120
They all fall into a category called
first order optimization methods.

52
00:03:45,880 --> 00:03:50,800
We call them first order because they
only require us to compute the first

53
00:03:50,830 --> 00:03:51,610
derivative,

54
00:03:51,610 --> 00:03:56,610
but there's another class of techniques
that aren't as widely used called second

55
00:03:56,710 --> 00:04:01,710
order optimization methods that require
us to compute the second derivative.

56
00:04:02,380 --> 00:04:07,380
The first derivative tells us if the
function is increasing or decreasing at a

57
00:04:07,871 --> 00:04:12,871
certain point and the second derivative
tells us if the first derivative is

58
00:04:13,211 --> 00:04:17,530
increasing or decreasing,
which hints at its curvature.

59
00:04:17,800 --> 00:04:22,800
First order methods provide us with a
line that is tangential to a point on an

60
00:04:22,871 --> 00:04:27,871
air surface and second order methods
provide us with a quadratic surface that

61
00:04:28,451 --> 00:04:32,730
kisses the curvature of the error
surface. Ah, get a room you youtube.

62
00:04:32,770 --> 00:04:37,270
The advantage then of second order
methods is that they don't ignore the

63
00:04:37,271 --> 00:04:42,070
curvature of the error surface and
in terms of stepwise performance,

64
00:04:42,220 --> 00:04:43,240
they are better.

65
00:04:43,360 --> 00:04:47,650
Let's look at a popular second order
optimization technique called Newton's

66
00:04:47,651 --> 00:04:52,330
method named after the dude who
invented calculus, whose name was

67
00:04:54,880 --> 00:04:57,850
there are actually two
versions of Newton's method.

68
00:04:57,940 --> 00:05:01,930
The first version is for finding
the roots of a polynomial,

69
00:05:02,200 --> 00:05:05,560
all those points where it
intersects with the x axis.

70
00:05:05,710 --> 00:05:08,590
So if you throw a ball and
recorded his trajectory,

71
00:05:08,860 --> 00:05:10,720
finding the roots of the equation,

72
00:05:10,750 --> 00:05:13,540
we'll tell you exactly what
time it hits the ground.

73
00:05:13,630 --> 00:05:18,630
The second version is for optimization
and it's what we use in machine learning,

74
00:05:19,210 --> 00:05:24,210
but let's code the route finding virgin
first to develop some basic intuition.

75
00:05:25,280 --> 00:05:26,113
Okay.

76
00:05:26,330 --> 00:05:31,040
Elouise of ICML Bay and you're welcome.
Hi,

77
00:05:31,780 --> 00:05:36,340
good to meet you and your to the
how to coordinate on that part.

78
00:05:36,700 --> 00:05:38,620
Not yet.
It's going to be quite a challenge.

79
00:05:38,621 --> 00:05:42,040
We've got to perform anomaly
detection on a 500 terabyte data sets.

80
00:05:42,160 --> 00:05:44,110
I was thinking just do Snoopy's method,

81
00:05:44,570 --> 00:05:48,790
just the progression first to see if we
can predict particles. Newton's method.

82
00:05:49,030 --> 00:05:51,580
Yeah,
it's a form like in the arm optimization,

83
00:05:51,640 --> 00:05:55,420
I already build props was training
for the past few hours. I'm impressed.

84
00:05:55,421 --> 00:05:58,040
Machine Learner, we shoot, let's

85
00:05:58,040 --> 00:06:03,040
say we have a function f of x
and some initial guests solution.

86
00:06:03,440 --> 00:06:08,420
Newton's method says that we first find
the slope of the tangent line at our

87
00:06:08,421 --> 00:06:09,260
guest's point.

88
00:06:09,500 --> 00:06:14,500
Then find the point at which the
tangent line intersects the x axis.

89
00:06:14,960 --> 00:06:19,220
We'll use that point to find its
projection in the original function.

90
00:06:19,700 --> 00:06:24,700
Then we iterate again from our first
step this time replacing our first point.

91
00:06:25,040 --> 00:06:29,600
With this one we keep
iterating and eventually we'll
stop when our current value

92
00:06:29,601 --> 00:06:33,050
of x is less than or equal to a threshold,

93
00:06:33,260 --> 00:06:37,760
so that's the route finding version of
Newton's method where we're trying to

94
00:06:37,761 --> 00:06:40,340
find where the function equal zero,

95
00:06:40,910 --> 00:06:45,020
but in the optimization version we're
trying to find where the derivative of the

96
00:06:45,021 --> 00:06:48,260
function equal zero. It's
minimum. At a high level,

97
00:06:48,320 --> 00:06:50,420
given a random starting location,

98
00:06:50,600 --> 00:06:55,600
we construct a quadratic approximation
to the objective function that matches

99
00:06:56,540 --> 00:07:00,170
the first and second derivative
values at that point,

100
00:07:00,530 --> 00:07:05,530
and then we minimize that quadratic
function instead of the original function.

101
00:07:06,050 --> 00:07:11,050
The minimizer of the quadratic function
is used as a starting point in the next

102
00:07:11,211 --> 00:07:14,810
step and we repeat this
process iteratively. Okay,

103
00:07:14,811 --> 00:07:19,370
so let's go over to cases of Newton's
method for optimization to learn more.

104
00:07:19,700 --> 00:07:22,940
A One d case and a two d case.
In the first case,

105
00:07:22,941 --> 00:07:24,890
we've got a one dimensional function.

106
00:07:25,100 --> 00:07:30,100
We can obtain a quadratic approximation
at a given point of the function using

107
00:07:30,411 --> 00:07:33,260
what's called a Taylor series expansion.

108
00:07:33,510 --> 00:07:36,230
Neglecting terms of order three or higher.

109
00:07:36,650 --> 00:07:41,650
A Taylor series is a representation of
a function as an infinite sum of terms

110
00:07:43,250 --> 00:07:47,870
that are calculated from the values of
the functions derivatives at a single

111
00:07:47,871 --> 00:07:48,704
point.

112
00:07:48,830 --> 00:07:53,810
It was invented by an English
mathematician named Brooke Taylor Swift.

113
00:07:53,990 --> 00:07:54,620
Just kidding.

114
00:07:54,620 --> 00:07:59,620
So we take the second order
tailored series for our
initial point x and minimize

115
00:08:00,171 --> 00:08:04,850
it by finding the first derivative and
second derivative and equating them to

116
00:08:04,851 --> 00:08:08,480
zero in order to find the minimum x value.

117
00:08:08,630 --> 00:08:11,960
We iterate this process.
In the second case,

118
00:08:12,050 --> 00:08:15,350
let's say we've got a function
of multiple dimensions,

119
00:08:15,710 --> 00:08:20,710
we can find the minimum of it using the
same approach except for two changes.

120
00:08:21,110 --> 00:08:25,880
We replaced the first derivatives with
a gradient and the second derivatives

121
00:08:26,030 --> 00:08:31,030
with a Hessian Hessian is a matrix of
the second order partial derivatives of a

122
00:08:31,731 --> 00:08:36,620
scaler and it describes the local
curvature of a multivariable function.

123
00:08:37,100 --> 00:08:37,933
Check this out.

124
00:08:38,120 --> 00:08:43,120
Derivatives help us compute gradients
which we can represent using aged Cobian

125
00:08:43,371 --> 00:08:48,371
matrix for first order optimization and
we can use the Hessian for second order

126
00:08:48,981 --> 00:08:49,814
optimization.

127
00:08:50,240 --> 00:08:54,800
These are four of the five
derivative operators using all of,

128
00:08:55,770 --> 00:08:59,670
they're the ways that we organize
and represent change numerically.

129
00:08:59,790 --> 00:09:02,850
So when should you use
a second order method?

130
00:09:03,150 --> 00:09:08,150
First order methods are usually less
computationally expensive to compute and

131
00:09:08,371 --> 00:09:12,780
less time expensive,
converging pretty fast on large datasets.

132
00:09:12,840 --> 00:09:17,840
Second order methods are faster when the
second derivative is known and easy to

133
00:09:17,941 --> 00:09:18,774
compute,

134
00:09:18,780 --> 00:09:23,780
but the second derivative is often in
tractable to compute requiring lots of

135
00:09:23,821 --> 00:09:26,040
computation for certain problems.

136
00:09:26,070 --> 00:09:31,070
Gradient descent can get stuck along
paths of slow convergence around saddle

137
00:09:31,561 --> 00:09:34,380
points,
whereas second order methods won't.

138
00:09:34,530 --> 00:09:39,510
Trying out different
optimization techniques for
your specific problem is the

139
00:09:39,511 --> 00:09:42,690
best way to see what works best.

140
00:09:43,290 --> 00:09:45,990
Here are the key points to
remember from this video.

141
00:09:46,620 --> 00:09:51,620
First order optimization techniques use
the first derivative of the function to

142
00:09:51,721 --> 00:09:55,590
minimize it. Second order optimization
techniques use a second derivative.

143
00:09:55,950 --> 00:10:00,950
The Jacoby Ian is a matrix of first
partial derivatives and the Hessian is a

144
00:10:01,801 --> 00:10:06,801
matrix of second partial derivatives
and Newton's method is a popular second

145
00:10:07,741 --> 00:10:12,741
order optimization technique that can
sometimes outperform gradient descent.

146
00:10:13,290 --> 00:10:18,000
Last week's coding challenge winner is
Alberto Garcia sets Alberto used gradient

147
00:10:18,001 --> 00:10:23,001
descent to find the line of best fit
is Jupiter notebook is insanely detail.

148
00:10:23,190 --> 00:10:26,250
You could learn gradient descent
just from reading it alone.

149
00:10:26,520 --> 00:10:30,030
Very well thought out. That was
dope. Alberto wizard of the week,

150
00:10:30,090 --> 00:10:35,090
and the runner up is Ivan Gustaf who
implemented gradient descent from scratch

151
00:10:35,400 --> 00:10:37,350
for polynomials of any order.

152
00:10:37,410 --> 00:10:41,550
This week's challenge is to implement
Newton's method for optimizations from

153
00:10:41,551 --> 00:10:44,110
scratch.
Details are in the read me poster,

154
00:10:44,130 --> 00:10:47,580
get hub link in the comments and
winners will be announced next week.

155
00:10:47,610 --> 00:10:49,650
Please subscribe for
more programming videos,

156
00:10:49,651 --> 00:10:53,760
and for now I've got to invent the sixth
derivative, so thanks for watching.

