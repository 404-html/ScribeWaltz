1
00:00:00,120 --> 00:00:04,860
Hello world. It's Saroj and evolution.
It's the reason we're all here today.

2
00:00:05,220 --> 00:00:06,031
In today's video,

3
00:00:06,031 --> 00:00:11,031
we're going to use tensorflow dot js to
simulate the process of evolution in our

4
00:00:11,041 --> 00:00:11,760
browser.

5
00:00:11,760 --> 00:00:15,000
What you're seeing behind me is the
demo that we're going to code in today's

6
00:00:15,001 --> 00:00:18,750
video. What it's doing is it's using
a bunch of different neural networks,

7
00:00:18,930 --> 00:00:22,950
and it's using evolutionary strategies
to evolve all of these neural networks

8
00:00:22,951 --> 00:00:27,270
over time to learn how to walk from the
left end of the screen to the right end

9
00:00:27,271 --> 00:00:29,100
of the screen.
So every generation,

10
00:00:29,130 --> 00:00:32,700
a new breed or new generation
in every generation,

11
00:00:32,730 --> 00:00:36,720
a new generation of neural
networks are going to be bread,

12
00:00:36,930 --> 00:00:40,020
and then they're all going to try to
walk from the left side of the screen to

13
00:00:40,021 --> 00:00:43,290
the right side. Now we're at generation
seven. They're getting a little better.

14
00:00:43,291 --> 00:00:46,050
They're still kind of jumpy,
and they don't really move that much.

15
00:00:46,050 --> 00:00:49,170
We can take one and even we can even
like move it across like all right,

16
00:00:49,171 --> 00:00:50,700
it's gone. But, uh, yeah,

17
00:00:50,701 --> 00:00:55,200
it's using tensorflow dot js to do
this and and evolutionary algorithms.

18
00:00:55,201 --> 00:01:00,120
It's the combination of neural networks
and evolutionary techniques put together

19
00:01:00,300 --> 00:01:03,600
that can allow us to evolve a
population over time. And this,

20
00:01:03,601 --> 00:01:07,890
this has some real use cases and I'm going
to talk about those in today's video.

21
00:01:07,891 --> 00:01:10,860
We're going to code this intention
flow dot js at the end of the video.

22
00:01:11,040 --> 00:01:14,310
But I want to start off by talking
about neuro evolution. Okay,

23
00:01:14,311 --> 00:01:18,570
so machine learning models,
neural networks in particular,

24
00:01:18,600 --> 00:01:22,470
neural networks are function
approximators. That means
they're really, really,

25
00:01:22,471 --> 00:01:26,970
really good at approximating
the correlation between
some input data and some

26
00:01:26,971 --> 00:01:30,660
output data. Right to the input data
could be images, that can be videos,

27
00:01:30,661 --> 00:01:34,320
it could be numbers and the output data
could be labels. They could be, you know,

28
00:01:34,400 --> 00:01:35,760
hot dog,
not hot dog,

29
00:01:35,880 --> 00:01:39,600
that could be one through 10 they could
be the price of a stock on a given day.

30
00:01:39,601 --> 00:01:42,570
They could be the notes for a
musical composition, whatever.

31
00:01:42,571 --> 00:01:45,570
But it's the label and there is some
mapping between the inputs and the output

32
00:01:45,571 --> 00:01:49,350
data. And neural networks are really
good at learning that mapping.

33
00:01:49,770 --> 00:01:53,790
And the way we do this is using a
technique called gradient descent. Okay.

34
00:01:53,791 --> 00:01:58,020
So creating dissent is all about finding
those optimal values by finding what's

35
00:01:58,021 --> 00:02:01,440
called the, the, the minimum
in some valley, right?

36
00:02:01,441 --> 00:02:05,730
So if we were to map all of the
parameters versus the output values,

37
00:02:05,940 --> 00:02:10,230
you'd find some value. And we want to
find the bottom of that point, right?

38
00:02:10,231 --> 00:02:11,160
So it looks like this,

39
00:02:11,910 --> 00:02:16,910
and what gradient descent
does is iteratively and
every time step it tries to

40
00:02:16,921 --> 00:02:20,370
find this minimum value. And once it finds
that minimum value, we could say, okay,

41
00:02:20,550 --> 00:02:22,620
we've got this minimum point.
What is the,

42
00:02:22,650 --> 00:02:26,760
what is the coefficient values at
this point? So in this three, um,

43
00:02:27,120 --> 00:02:32,070
and this three access model, we have
a z access, a y axis and an x axis.

44
00:02:32,280 --> 00:02:35,820
So in the x and y axis represent
the two parameters of a model.

45
00:02:36,270 --> 00:02:39,120
And the Z axis is the objective.
It's, it's that output value.

46
00:02:39,210 --> 00:02:41,970
And we can try and guess
what those values are, but,

47
00:02:41,971 --> 00:02:45,360
but by using gradient descent and we
can learn what the optimal values are,

48
00:02:46,700 --> 00:02:50,580
and the way to do this is right gradient
descent. That's how it's done so far.

49
00:02:50,970 --> 00:02:53,820
But in the 80s a couple of
researchers thought, okay,

50
00:02:53,821 --> 00:02:56,250
what if we use a different
optimization technique?

51
00:02:56,251 --> 00:03:00,970
What if we don't use gradient descent?
And if we think about life on earth, you,

52
00:03:00,971 --> 00:03:04,900
me, animals, bacteria,
we're all neural networks.

53
00:03:04,900 --> 00:03:08,620
We all have these networks of neurons
in our head and we're learning through

54
00:03:08,621 --> 00:03:11,640
trial and error. And the, the,

55
00:03:11,641 --> 00:03:15,880
the local learning procedure could be
whatever, right? We don't know what it is,

56
00:03:16,000 --> 00:03:20,050
but the more macro learning procedure
through our children and their children

57
00:03:20,051 --> 00:03:23,470
and their children is this
process of evolution, right?

58
00:03:23,471 --> 00:03:27,910
So we are adapting to our climate,
we're adapting to our, our scenery,

59
00:03:27,911 --> 00:03:29,440
we're adapting to our environment,

60
00:03:29,441 --> 00:03:33,640
the context of where we lived and we
pass on these adaptions in the form of

61
00:03:33,641 --> 00:03:36,700
genes to our children.
And that process is evolution.

62
00:03:36,701 --> 00:03:39,700
And that in and of itself
is a learning technique.

63
00:03:40,000 --> 00:03:44,890
And what evolutionary algorithms neuro
evolution does is it tries to model that

64
00:03:44,891 --> 00:03:48,220
process in simulation in Silico,
inside of a machine.

65
00:03:48,460 --> 00:03:51,670
And so if we think about a bunch
of different neural networks,

66
00:03:51,700 --> 00:03:53,770
we can randomly initialize all of them.

67
00:03:53,800 --> 00:03:56,590
So they all start out
with random weight values.

68
00:03:57,010 --> 00:04:01,060
And what we can do is we can pick the
best one by some measure of whatever best

69
00:04:01,061 --> 00:04:02,230
means for our objective,

70
00:04:02,530 --> 00:04:06,070
and then we can take that best neural
network out of say a hundred and then say,

71
00:04:06,071 --> 00:04:08,410
okay, this neural network
is the best of all of them.

72
00:04:08,620 --> 00:04:12,970
Let's let's create a bunch of other
neural networks that are very slight

73
00:04:12,971 --> 00:04:15,760
variation from this best one,
and that's the new population.

74
00:04:16,090 --> 00:04:18,880
And then out of those there's going to
be the best one and we'll take that and

75
00:04:18,881 --> 00:04:20,680
we'll breathe it with
another and then we'll,

76
00:04:21,070 --> 00:04:23,050
we'll repeat that process
over and over again.

77
00:04:23,051 --> 00:04:25,630
So we're just kind of simulating
evolution that's so high level.

78
00:04:25,631 --> 00:04:27,640
I'm going to go into the details
of how that works in a second,

79
00:04:27,641 --> 00:04:29,140
but what I want to say here,

80
00:04:29,141 --> 00:04:33,970
what my point is that my point is that
gradient descent works really well.

81
00:04:34,000 --> 00:04:35,860
If you look at all of
the production systems,

82
00:04:36,040 --> 00:04:40,390
grading dissent is the
optimization strategy also
called backpropagation in the

83
00:04:40,391 --> 00:04:44,920
context of neural networks. It is the
optimization strategy that is used.

84
00:04:45,160 --> 00:04:46,840
However,
with Uber,

85
00:04:46,841 --> 00:04:50,560
with a bunch of new startups
and Uber and some big companies,

86
00:04:50,561 --> 00:04:54,550
I'll talk about more and more
companies, users, startups,

87
00:04:54,551 --> 00:04:58,150
researchers are using evolutionary
strategies, neuro evolution,

88
00:04:58,330 --> 00:05:01,390
genetic algorithms to optimize
their neural networks.

89
00:05:01,480 --> 00:05:04,840
And I'll talk about those in a
second. Okay, so think about it.

90
00:05:04,900 --> 00:05:09,430
Grading dissent is us trying to find
that minimum point by starting at some

91
00:05:09,431 --> 00:05:13,390
point and then see an Eruv Lee going
closer and closer to that ideal value.

92
00:05:13,630 --> 00:05:17,090
If you want to look, if you want to know
how backpropagation works, how grading,

93
00:05:17,100 --> 00:05:20,860
dissent works, search
backpropagation and Saroj on Youtube,

94
00:05:21,060 --> 00:05:24,250
I've backpropagation and five minutes
video could super useful. Okay, so,

95
00:05:24,400 --> 00:05:26,530
so it typically goes like this,
like I said,

96
00:05:26,740 --> 00:05:30,370
we will generate a population of let's
say a hundred random neural networks,

97
00:05:30,371 --> 00:05:33,550
right? So we don't know what
those initial weight values are.

98
00:05:33,551 --> 00:05:36,970
So in the context of neural networks,
those perimeters would be wait values,

99
00:05:36,971 --> 00:05:38,020
right?
So you know this,

100
00:05:38,021 --> 00:05:41,590
this is initialized with some
random group of numbers of vector,

101
00:05:41,591 --> 00:05:45,040
of weight values. Another one is
with another group of way to values.

102
00:05:45,041 --> 00:05:46,730
And we will then determine the arc.

103
00:05:46,900 --> 00:05:50,410
And all of those architectures are
determined using some distribution, right?

104
00:05:50,411 --> 00:05:53,070
So some golf, Sian distribution,
et cetera. Then we'll select,

105
00:05:53,100 --> 00:05:56,680
select the best neural network by some
fitness function. We'll talk about that.

106
00:05:56,890 --> 00:05:59,870
We'll let's say three or four of them.
We'll breed them together.

107
00:05:59,871 --> 00:06:01,400
By breeding it means,
you know,

108
00:06:01,401 --> 00:06:04,630
it could be a dot product between
those wait values. It could be, uh,

109
00:06:04,790 --> 00:06:08,510
finding those optimal parameters and,
and then choosing those for the next one,

110
00:06:08,560 --> 00:06:13,160
etc. What does weight values are copying
them and then create these offspring,

111
00:06:13,161 --> 00:06:14,390
which are new neural networks.

112
00:06:14,390 --> 00:06:16,430
And then we'll repeat that process
over and over and over again.

113
00:06:16,730 --> 00:06:19,850
So there are a couple of
genetic algorithms out there.

114
00:06:19,910 --> 00:06:23,660
There's neat neuro evolution
of augmented topologies.

115
00:06:23,870 --> 00:06:27,860
There's hyper neat, there's novelty
search. There are quite a lot of examples.

116
00:06:27,950 --> 00:06:31,070
So the idea here is that we start off
with some neural networks and then we

117
00:06:31,071 --> 00:06:33,710
evaluate all of their,
how fit each of them are.

118
00:06:33,980 --> 00:06:37,100
So that means checking where the models
are and the optimization surface,

119
00:06:37,101 --> 00:06:38,600
which of the models performed best.

120
00:06:38,870 --> 00:06:41,660
And then selection is performed
based on that fitness evaluation.

121
00:06:41,661 --> 00:06:42,710
We'll select four or five,

122
00:06:42,711 --> 00:06:46,190
whatever threshold value we decide
and evolutionary strategies.

123
00:06:46,220 --> 00:06:49,700
The offspring is reduced to a single
model and weighted by the fitness

124
00:06:49,701 --> 00:06:51,710
evaluation.
So for deep neural networks,

125
00:06:51,711 --> 00:06:54,530
the fitness is defined as
the loss or the reward,

126
00:06:54,531 --> 00:06:58,490
and essentially we're moving around
the optimization surface and using the

127
00:06:58,520 --> 00:07:01,530
offspring to get in the
right direction so that the,

128
00:07:01,540 --> 00:07:04,970
the key difference between gradient
descent here is that instead of computing

129
00:07:04,971 --> 00:07:05,804
the gradients,

130
00:07:05,870 --> 00:07:09,290
we're sending out multiple antennas and
moving in the direction that looks best.

131
00:07:09,440 --> 00:07:12,290
There's not a single model that
we're optimizing. There are several.

132
00:07:12,320 --> 00:07:17,060
There's a population of models that's,
that's the big difference here, right?

133
00:07:17,061 --> 00:07:19,530
So if you think about there's,
there's six of these models here.

134
00:07:19,550 --> 00:07:23,000
Think of each of them has, as
all of these rows, each model,

135
00:07:23,030 --> 00:07:24,710
one model equals one row.

136
00:07:25,010 --> 00:07:28,640
We'll select the best ones will
combine those values using some kind of

137
00:07:28,641 --> 00:07:31,430
technique, maybe. Dot. Product,
multiplication, division, whatever.

138
00:07:31,880 --> 00:07:34,760
Then will mutate them.
That means we're very them in some way,

139
00:07:34,761 --> 00:07:38,810
and then we'll place those mutated
offspring in back into the population and

140
00:07:38,811 --> 00:07:42,830
then try and try again. The process
repeats over and over again. In this way,

141
00:07:42,831 --> 00:07:44,810
it's similar to a
structured random search,

142
00:07:44,840 --> 00:07:49,340
but the end result is in the selection
phase that we have a single model and

143
00:07:49,341 --> 00:07:52,460
then we repeat that process over
and over and over and over again.

144
00:07:52,940 --> 00:07:56,540
So this is an example of Uber. So this,
this map I, I took from Uber, right?

145
00:07:56,541 --> 00:07:58,310
So there are engineering a website.

146
00:07:58,340 --> 00:08:01,760
So they have this great blog on neuro
evolution, which I have a link to,

147
00:08:02,050 --> 00:08:06,950
but basically what one
technique they used, one, one
real life use case. We'll see,

148
00:08:06,951 --> 00:08:10,020
use evolutionary strategies
to evolve this, uh,

149
00:08:10,160 --> 00:08:13,550
pathfinding algorithm to get from the
best route from point a to point B.

150
00:08:13,910 --> 00:08:18,560
And this is the map of, of, of what
they found to be that, that best route.

151
00:08:20,540 --> 00:08:23,330
And so, uh, so applied to
neural networks specifically,

152
00:08:23,331 --> 00:08:26,810
we can use genetic algorithms
to optimize any kind of model.

153
00:08:26,811 --> 00:08:29,630
It doesn't have to be a neural
network and it can be, you know,

154
00:08:29,631 --> 00:08:33,630
some simple function. It can be some, um,

155
00:08:33,650 --> 00:08:34,670
it could be anything really.

156
00:08:34,671 --> 00:08:37,460
And a neural network really
is just a nested function.

157
00:08:37,461 --> 00:08:40,580
And every layer is another layer
of nesting that function, you know,

158
00:08:40,850 --> 00:08:44,780
f of x is one layer,
and then m of f of x is two layers.

159
00:08:44,780 --> 00:08:48,800
And then g of F of f of x is a three
layer network. You see what I'm saying?

160
00:08:49,010 --> 00:08:52,160
Because we're taking the output of one
function and using it as input to the

161
00:08:52,161 --> 00:08:56,070
next function and using the output of
that function it and using it as input to

162
00:08:56,071 --> 00:09:00,420
the next function. So it's, it's, it's
this, it's this nested function, right?

163
00:09:00,421 --> 00:09:02,940
So however many layers is
another layer of nesting.

164
00:09:03,930 --> 00:09:08,010
So applied to a neural network. If we
apply neuro evolution to that. So what is,

165
00:09:08,040 --> 00:09:10,890
what is that thing that we want to
optimize? What we want to optimize,

166
00:09:10,891 --> 00:09:14,730
the number of layers, the neurons
per layer, the activation function,

167
00:09:14,731 --> 00:09:19,380
the network optimizer, these
are all different parameters
that we could optimize.

168
00:09:20,730 --> 00:09:22,350
And so when it comes to use cases,

169
00:09:22,560 --> 00:09:26,880
it really just boils down to anything
that could use a neural network and that

170
00:09:26,881 --> 00:09:29,880
could improve on the backpropagation
optimization strategy.

171
00:09:30,000 --> 00:09:34,350
So Uber found actually in their blog
was that neuro evolution outperformed

172
00:09:34,351 --> 00:09:37,200
graded descent in terms
of time to convergence.

173
00:09:37,410 --> 00:09:39,160
So they'd spend up this Atari game,
this,

174
00:09:39,161 --> 00:09:44,161
this deep Q learner that you know took
deep mind like days and days on massive

175
00:09:44,671 --> 00:09:49,570
sets of gps to compute into a
single hour on a desktop CPU. Um,

176
00:09:49,710 --> 00:09:51,270
which is really cool.
So just check that out.

177
00:09:51,271 --> 00:09:53,970
I have a link to it in the video
description, but so like I said,

178
00:09:54,000 --> 00:09:57,330
it can be applied to anything that
neural networks are currently applied to.

179
00:09:57,540 --> 00:09:59,190
If there is some,
um,

180
00:09:59,280 --> 00:10:04,280
if there is some AI that is
using backpropagation right
now that is slow in some

181
00:10:04,861 --> 00:10:07,920
way where a 2%, 3%, 5%,

182
00:10:07,921 --> 00:10:09,900
10% increase can be useful.

183
00:10:09,930 --> 00:10:13,350
That would be a perfect time to
apply this technique to write.

184
00:10:13,351 --> 00:10:17,870
You can outperform the competition by
using an evolutionary strategy, right?

185
00:10:17,880 --> 00:10:21,660
So this could be applied to self
driving cars, better game AI,

186
00:10:21,840 --> 00:10:24,930
image classifiers,
like clarify our generation.

187
00:10:24,930 --> 00:10:27,570
Really anything that's
using neural networks.

188
00:10:27,571 --> 00:10:32,571
So you can apply in neuro evolution
too to make that optimization process

189
00:10:32,610 --> 00:10:33,990
faster.
That's what it comes down to.

190
00:10:34,230 --> 00:10:39,180
A faster optimization process
that converges to a better
minimum, faster, right?

191
00:10:39,181 --> 00:10:44,181
So you can get better results and
faster if you are using neat algorithms.

192
00:10:44,730 --> 00:10:49,730
Most of the time from what we've
seen here and some of these examples,

193
00:10:50,221 --> 00:10:55,110
especially the Uber blog, um,
but also Google, so Google
uses, uses it for auto ml.

194
00:10:55,120 --> 00:11:00,120
They're automatic machine learning
algorithm that you can try in the browser.

195
00:11:00,380 --> 00:11:04,380
Um, and deepmind used it
recently, so did mine,

196
00:11:04,381 --> 00:11:09,240
did neuro evolution for pup,
they called it population based training.

197
00:11:09,241 --> 00:11:11,940
I remember it was like
population based training,

198
00:11:12,770 --> 00:11:14,520
population based

199
00:11:16,800 --> 00:11:18,870
training of neural networks.
That's what they called it. Yes.

200
00:11:19,170 --> 00:11:23,640
And this was released less
than a year ago. But um, yeah,

201
00:11:23,650 --> 00:11:25,920
some great blog posts on that as well.

202
00:11:26,310 --> 00:11:29,580
They've got some nice little
visualizations of using them on generative

203
00:11:29,581 --> 00:11:33,690
adversarial networks. Great use
case, popular model. So yeah,

204
00:11:33,940 --> 00:11:35,370
there's a ton of use cases.

205
00:11:35,371 --> 00:11:39,090
But in today's video I want to talk about
using it in the context of tensorflow.

206
00:11:39,091 --> 00:11:39,924
Dot.
Js.

207
00:11:40,050 --> 00:11:43,710
It says the hottest machine learning
library right now because it allows us to

208
00:11:43,711 --> 00:11:48,711
create an l models in the browser that
anybody can access on any kind of device,

209
00:11:48,871 --> 00:11:52,740
mobile embedded device,
TV, um, laptop, desktop.

210
00:11:53,470 --> 00:11:56,620
And it's, it's really utilizing
the power of transfer learning,

211
00:11:56,621 --> 00:12:00,040
which we haven't really seen when
it comes to python or c plus plus.

212
00:12:00,640 --> 00:12:04,480
For some reason it's just people are
using, uh, pretrained models much more.

213
00:12:04,481 --> 00:12:07,750
And I think it's because it's
a lot easier for one and two.

214
00:12:08,140 --> 00:12:11,610
It's using Java script, which has a
much bigger user base right there.

215
00:12:11,620 --> 00:12:16,570
More programmers using javascript these
days. So before we get to our model,

216
00:12:16,760 --> 00:12:18,760
I just want to say two quick
things about tensorflow. Dot.

217
00:12:18,761 --> 00:12:21,580
Js to remember there is the core Api,
right?

218
00:12:21,581 --> 00:12:23,500
So the core Api is what
we're going to use,

219
00:12:23,710 --> 00:12:26,980
but it lets us build a computation graph
and we could think of a computation

220
00:12:26,981 --> 00:12:28,450
graph as a neural network.

221
00:12:28,520 --> 00:12:33,520
It's because it's just a
series of operations that
tensors data flows between.

222
00:12:33,661 --> 00:12:37,210
And these operations can be layers. They
can be, you know, whatever, you know,

223
00:12:37,240 --> 00:12:40,030
different operations like adding,
subtracting,

224
00:12:40,031 --> 00:12:42,670
depending on the type of
architecture you want a,

225
00:12:42,671 --> 00:12:46,000
but it's very simple to do in Java script.
Here's a simple example.

226
00:12:46,060 --> 00:12:48,700
We have two tensors a and B.
We add them together,

227
00:12:48,701 --> 00:12:50,230
we print them out just like that.

228
00:12:50,231 --> 00:12:55,000
And that those are two nodes
in the computation graph,
a, B plus, and that output,

229
00:12:55,030 --> 00:12:58,270
that's the computation graph. Three nodes
in the graph, a, B, and then the total,

230
00:12:58,271 --> 00:13:00,580
the output of a and B.
So that's,

231
00:13:00,581 --> 00:13:03,100
that's what we're going to use because
this is a very simple neural networks

232
00:13:03,101 --> 00:13:04,390
that we're going to build.
However,

233
00:13:04,391 --> 00:13:07,390
when it comes to more complex
neural networks with much more late,

234
00:13:07,420 --> 00:13:11,560
many more layers and you know,
many different, you know,
strategies like attention,

235
00:13:11,860 --> 00:13:15,460
um, momentum, et Cetera, then we
want to use the high level API,

236
00:13:15,461 --> 00:13:20,020
which is the layers API. So layers
API lets us it packages together,

237
00:13:20,021 --> 00:13:24,190
both the variable and the operations
that act on Tim and act on them into a

238
00:13:24,191 --> 00:13:28,840
single function. So one example would
be this, so that the layers dot dense,

239
00:13:29,080 --> 00:13:30,310
uh,
line of code,

240
00:13:30,580 --> 00:13:35,230
it performs a weighted sum across all
inputs of each output and applies an

241
00:13:35,260 --> 00:13:37,750
optional activation function.
So it's combining,

242
00:13:37,930 --> 00:13:40,810
but the variable and the
operation into a single line,

243
00:13:41,080 --> 00:13:42,910
which doesn't seem like that big a deal.

244
00:13:42,911 --> 00:13:45,940
But when you're creating a big neural
network, it saves a lot of time.

245
00:13:46,180 --> 00:13:48,460
It's basically care os in the browser.
That's what it is.

246
00:13:48,461 --> 00:13:52,030
Think of that as an analogy. Care Os and
the browser. A single line per layer.

247
00:13:53,260 --> 00:13:56,020
Okay, so now let's get to
our model. So in our model,

248
00:13:56,021 --> 00:13:59,470
what we're going to have are these
creatures and each of these creatures is a

249
00:13:59,471 --> 00:14:01,390
three layer feed forward neural network.

250
00:14:01,420 --> 00:14:03,490
That's their brain that we see right here.

251
00:14:03,491 --> 00:14:06,040
That right there was a bunch of creatures.
Right now in this generation,

252
00:14:06,041 --> 00:14:09,370
there's only one. But if I refresh, we're
going to, we're going to get a lot more,

253
00:14:09,520 --> 00:14:11,170
but we're going to go, that's
a lot of creatures, right?

254
00:14:11,171 --> 00:14:13,000
So each of them has
their own neural network.

255
00:14:13,360 --> 00:14:18,360
So the input data that's fed into the
network are four different parameters.

256
00:14:20,080 --> 00:14:22,720
The horizontal velocity,
how fast they're moving horizontally,

257
00:14:22,870 --> 00:14:26,110
the vertical velocity, how fast
we're moving up and down. The torque,

258
00:14:26,170 --> 00:14:29,350
how fast they're turning,
and the height above the ground level.

259
00:14:30,730 --> 00:14:33,680
So the score, the, the,
the way that we are, um,

260
00:14:35,020 --> 00:14:37,300
scoring these, these, uh, creatures

261
00:14:39,070 --> 00:14:42,250
is we're saying they can gain points
based on the distance that it travels from

262
00:14:42,251 --> 00:14:45,400
the starting point. So the further
it travels in the correct direction,

263
00:14:45,430 --> 00:14:48,670
the more points that gain. So if it
goes from left all the way to the right,

264
00:14:48,940 --> 00:14:49,990
it's gonna get a lot of points.

265
00:14:50,470 --> 00:14:53,630
And then traveling in the opposite
direction is going to reduce the point.

266
00:14:53,900 --> 00:14:57,080
So the fitness function, the
way we define how fit, um,

267
00:14:57,140 --> 00:15:01,460
a neural network is in this context
is that how far a creature goes,

268
00:15:01,790 --> 00:15:03,470
the selection algorithm that says,

269
00:15:03,590 --> 00:15:08,590
let's see what are the best most fit
neural networks here is to select based on

270
00:15:08,721 --> 00:15:11,630
their fitness value will, which
will be a scalar, a single value.

271
00:15:12,110 --> 00:15:14,840
And that's going to be that that
fitness value is going to act like the

272
00:15:14,841 --> 00:15:17,210
probability of being
chosen for reproduction.

273
00:15:18,320 --> 00:15:21,260
The creatures that perform better
have higher fitness values.

274
00:15:21,290 --> 00:15:25,250
And hence have a higher chance of
reproducing when it comes to crossover

275
00:15:25,251 --> 00:15:28,850
breeding, reproduction, W, whatever
you want to call it, two creatures.

276
00:15:28,880 --> 00:15:31,670
These parents are selected
using the selection algorithm.

277
00:15:31,970 --> 00:15:36,110
Their weights are interchange randomly
in our case. And then finally,

278
00:15:36,111 --> 00:15:40,600
mutation is a single value, a
parameter that's going to change, uh,

279
00:15:40,700 --> 00:15:43,610
the probability of introducing
randomness to each of these networks.

280
00:15:43,790 --> 00:15:45,980
So let's get into the code here.
Okay,

281
00:15:46,070 --> 00:15:48,650
so I want to start off by just
creating a simple neural network.

282
00:15:48,651 --> 00:15:51,500
This will be an 40 lines of code.
We're going to use tensorflow dot js.

283
00:15:51,800 --> 00:15:54,020
So I'm going to call this
a neural network class,

284
00:15:54,021 --> 00:15:58,340
and let me make sure that it's using
javascript as its base javascript.

285
00:15:58,370 --> 00:16:01,490
There we go. Okay, so it's going to
be called class neural network. Okay.

286
00:16:02,240 --> 00:16:05,570
And in this class neural network,
I want there to be a constructor.

287
00:16:05,600 --> 00:16:10,070
The constructor, we'll define what the
input to this neural network will be.

288
00:16:10,460 --> 00:16:13,310
We want some,
a certain number of input nodes.

289
00:16:13,311 --> 00:16:15,530
We want a certain number
of hidden nodes or neurons,

290
00:16:15,800 --> 00:16:19,850
and we want a certain set of output notes.
Okay?

291
00:16:19,851 --> 00:16:21,500
So then inside of the constructor,

292
00:16:21,501 --> 00:16:25,430
we're going to define what those input
nodes are, what those hidden nodes are,

293
00:16:25,460 --> 00:16:30,230
and that's going to make them
accessible inside of the class, right?

294
00:16:30,231 --> 00:16:32,930
So for all three of them,
I'm going to do the same thing.

295
00:16:33,170 --> 00:16:37,400
And so these values are going to be
accessible later on in the class as I

296
00:16:37,401 --> 00:16:40,700
manipulate them with
inside of other functions.

297
00:16:41,030 --> 00:16:45,230
The next step is I'm going to initialize
random waits for that neural network,

298
00:16:45,231 --> 00:16:47,780
right?
Because when our creatures start out,

299
00:16:48,050 --> 00:16:51,080
they're going to be randomly
initialized weights, right?

300
00:16:51,110 --> 00:16:53,510
Because there's no learning happening yet.

301
00:16:53,780 --> 00:16:57,560
And so now I'm going to actually use
tensorflow dot. JS and like I said before,

302
00:16:57,561 --> 00:16:58,700
we're going to use,
we're going to,

303
00:16:58,790 --> 00:17:03,100
we're going to generate these wait values
randomly from a distribution and and

304
00:17:03,101 --> 00:17:04,730
distribution.
And so to do this,

305
00:17:04,731 --> 00:17:07,620
we can use the what's called
the random normal function.

306
00:17:07,620 --> 00:17:10,880
So from a normal distribution,
we're going to generate a set of weights.

307
00:17:12,650 --> 00:17:16,250
And the way to do this is to say,
well, how many input notes do we have?

308
00:17:16,520 --> 00:17:21,350
And then how many hidden nodes do we have?
And then based on those values,

309
00:17:21,530 --> 00:17:25,640
we can generate a set of weights. And
then once we have that, we could say,

310
00:17:25,641 --> 00:17:29,210
well that's our input weights and now
we want to have one more set of weights,

311
00:17:29,211 --> 00:17:33,080
right? Output weights. Because this is
a, a three layer, like I said before,

312
00:17:33,480 --> 00:17:37,460
neural network. And then between each of
those three layers is a set of weights.

313
00:17:37,670 --> 00:17:40,250
And because there are three layers,
there are then two sets of weights.

314
00:17:40,550 --> 00:17:43,550
And so because there are two
sets of weights were we'll
create those two sets of

315
00:17:43,551 --> 00:17:46,180
weights. And because I had, um,

316
00:17:46,460 --> 00:17:48,470
the input notes and then
hidden knows for the first,

317
00:17:48,471 --> 00:17:52,740
I'll have the hidden nodes and I'll put
notes for the next set of weights. Okay.

318
00:17:52,741 --> 00:17:57,741
So now we've initialized our random weight
values and now that we've done that,

319
00:17:59,070 --> 00:18:03,150
we can create the predict function,
right? So we have our neural network,

320
00:18:03,180 --> 00:18:06,570
it's constructed and we want to
create a predict function, right?

321
00:18:06,571 --> 00:18:09,810
So that the user will
do is they will input

322
00:18:13,650 --> 00:18:14,431
some data,
right?

323
00:18:14,431 --> 00:18:17,490
So this is going to be some input data
and we'll talk about what that input data

324
00:18:17,491 --> 00:18:21,030
is, but it's going to take that input
data and we're going to define some output

325
00:18:21,031 --> 00:18:24,150
variable.
So now I'm going to use the tidy function.

326
00:18:24,170 --> 00:18:27,650
Tensorflow is a tidy function and what
this is going to do is it's going to cry.

327
00:18:27,780 --> 00:18:31,200
I'm going to use this as a rapper and
then inside of that rapper I'm going to

328
00:18:31,350 --> 00:18:32,920
actually create my neural network.

329
00:18:32,921 --> 00:18:37,020
So before what I did was I initialize
those values and now I'm actually going to

330
00:18:37,021 --> 00:18:40,380
create the neural network itself
using those values at created before.

331
00:18:40,650 --> 00:18:45,090
What tidy does is it executes a
function and it frees up any GPU memory.

332
00:18:45,091 --> 00:18:47,730
So because we're about
to create this network,

333
00:18:47,910 --> 00:18:52,910
I'm going to use the tidy function to
execute this neural network and as a

334
00:18:53,251 --> 00:18:58,251
function and then free up any GPU memory
that I have because this is going to

335
00:18:59,191 --> 00:19:02,490
require some GPU memory right now.
So this is going to end.

336
00:19:02,550 --> 00:19:05,160
This is going to take
an one d array as input.

337
00:19:05,340 --> 00:19:07,500
So for the first layer I'm going to say,
well,

338
00:19:07,501 --> 00:19:12,501
I'm going to input a tensor using and
then that tensor is going to be created

339
00:19:12,720 --> 00:19:15,510
using what the input data
is from the user rights

340
00:19:17,760 --> 00:19:22,440
and the size of that is going
to be the input nodes sides.

341
00:19:24,870 --> 00:19:27,900
That's the first part. The next
part is for us to say, well,

342
00:19:27,901 --> 00:19:29,070
what's the hidden layer?

343
00:19:29,580 --> 00:19:33,480
The hidden layer is going
to be the input layer,

344
00:19:35,210 --> 00:19:40,210
but we're going to matrix multiply it
by the the set of weight values that we

345
00:19:40,441 --> 00:19:42,420
had that we defined before,
right?

346
00:19:42,421 --> 00:19:46,230
So inside of the constructor and we're
going to apply the sigmoid function to

347
00:19:46,231 --> 00:19:50,850
that because input times wait, activate.
That's going to activate that result.

348
00:19:51,210 --> 00:19:54,120
If you want to know why I'm using
the sigmoid function, watch my video,

349
00:19:54,121 --> 00:19:58,450
build a neural network in four minutes.
Now that we have that, we can just,

350
00:19:58,530 --> 00:19:59,363
we can use,

351
00:19:59,400 --> 00:20:03,480
we can create the output layer because
this is a three layer feed forward neural

352
00:20:03,481 --> 00:20:07,230
network and I'm going to do the same
exact thing because neural networks are

353
00:20:07,260 --> 00:20:08,720
giant nested functions.

354
00:20:08,721 --> 00:20:12,780
I'm going to take the output of the first
function and use it as input to this

355
00:20:12,781 --> 00:20:14,250
function.
And I'll say this thought,

356
00:20:14,251 --> 00:20:17,790
how put weights dot sigmoid.

357
00:20:19,860 --> 00:20:21,510
And lastly,
now I have my output,

358
00:20:21,511 --> 00:20:25,950
which is going to be the output from
the output layer. Dot data sync.

359
00:20:27,450 --> 00:20:30,630
We're just going to sink the result
into a single scalar value that I can,

360
00:20:31,350 --> 00:20:35,010
I can have as my output,
and now I can return that output.

361
00:20:38,520 --> 00:20:40,470
So that's it for the predict function.

362
00:20:40,471 --> 00:20:43,110
Now I have two more functions
that I want to create.

363
00:20:43,410 --> 00:20:45,240
So the first function is called clone.

364
00:20:45,870 --> 00:20:49,510
So what the clone function
doing is it's going to say,

365
00:20:50,260 --> 00:20:51,640
let's create a Kloni.

366
00:20:53,190 --> 00:20:56,260
And what it's going to do is it's going
to create a clone of our neural network

367
00:20:56,261 --> 00:20:57,790
because we'll want that at the start.

368
00:20:58,110 --> 00:21:00,250
We want to create a bunch of
different neural networks, right?

369
00:21:00,251 --> 00:21:01,660
And so this function is going to do that.

370
00:21:01,661 --> 00:21:04,030
So we'll just call this function
over and over and over, over again.

371
00:21:04,420 --> 00:21:08,620
And so whenever a population
is initialized, we'll clap.
We'll call this function.

372
00:21:08,990 --> 00:21:12,970
However, for however many neural
networks we want there to be.

373
00:21:13,300 --> 00:21:15,790
And now that we defined
the neural network class,

374
00:21:15,791 --> 00:21:18,490
we can just say create a new
neural network, use, you know,

375
00:21:18,491 --> 00:21:20,590
whatever number of input nodes you want,

376
00:21:20,591 --> 00:21:23,980
whatever number of hidden nodes you want.
And then whatever number of,

377
00:21:24,040 --> 00:21:28,660
I'll put notes you want. Once
we have that. So we'll say,

378
00:21:28,661 --> 00:21:30,190
well now we can

379
00:21:32,940 --> 00:21:37,680
dispose of anything in memory that we
don't want so we can clear up some memory.

380
00:21:38,280 --> 00:21:40,290
And then we will say,

381
00:21:42,430 --> 00:21:46,960
here are the input weights.
Clone the ones that we already have,

382
00:21:47,280 --> 00:21:51,010
TF dot clone function.
And here are d.

383
00:21:53,100 --> 00:21:53,933
Wait.

384
00:21:57,940 --> 00:21:58,773
Okay.

385
00:22:00,840 --> 00:22:03,510
Okay. So now that we've done
that, we have our input ways,

386
00:22:03,511 --> 00:22:05,160
we have our output weights,
we've quoted our neural network,

387
00:22:05,161 --> 00:22:08,950
and now we can return it and glass Lee.
Now that we have all of that,

388
00:22:08,951 --> 00:22:13,330
so we can finally just say dispose,
which we'll dispose the input in help,

389
00:22:13,331 --> 00:22:15,930
what for memory, which we'll
call at the end of a generation.

390
00:22:16,380 --> 00:22:19,890
And once we have that,
we're done with our neural network class.

391
00:22:20,190 --> 00:22:24,900
Now we can look at the rest of the
code and I'll just copy paste.dot dot.

392
00:22:24,990 --> 00:22:29,190
And then this is going to be output.
Okay, so that's our simple class.

393
00:22:29,730 --> 00:22:34,440
Say that. So in the context
of our genetic algorithm,

394
00:22:34,740 --> 00:22:39,740
we have a genetic algorithm class and
this is just an example of how it works.

395
00:22:39,751 --> 00:22:43,020
We'll initialize the creature.
The creature will be a neural network,

396
00:22:43,260 --> 00:22:46,500
will define what it looks
like in inside of the space,

397
00:22:46,501 --> 00:22:49,770
like the visualization by these,
these values right here.

398
00:22:50,490 --> 00:22:55,020
And then we'll define some way of
picking one of those, one of those, um,

399
00:22:55,920 --> 00:22:59,160
neural networks or creatures by
defining some threshold value.

400
00:22:59,161 --> 00:23:02,520
And if it's over that threshold value
that is our selected that as our selected

401
00:23:02,521 --> 00:23:05,490
network and will return that.
Now we can say it,

402
00:23:05,491 --> 00:23:09,480
let's evolve our neural network.
So a sign of fitness to each creature.

403
00:23:09,720 --> 00:23:13,980
We'll pick the best one and then we'll
breed them by picking two parents that

404
00:23:14,350 --> 00:23:15,300
define a child.

405
00:23:15,301 --> 00:23:19,020
And you think this crossover functions
to define what the child's weights are

406
00:23:19,260 --> 00:23:23,010
and then push that child to
a new generation and repeat
that process for all of

407
00:23:23,011 --> 00:23:26,010
those values, which are children
inside of that generation, Hooray

408
00:23:27,510 --> 00:23:29,280
will kill the current
generation iteratively.

409
00:23:29,281 --> 00:23:31,590
And an add new children to
the generation population.

410
00:23:31,830 --> 00:23:35,130
So this is the genetic
algorithm class as as a whole.

411
00:23:35,300 --> 00:23:37,050
This is the neural
network class as a whole.

412
00:23:37,260 --> 00:23:40,150
And now we can look at how this
has combined in sketch dot j yes,

413
00:23:40,160 --> 00:23:43,740
so this is how it works. We define the
canvas, what everything looks like.

414
00:23:43,890 --> 00:23:47,240
We'll initialize a generation, a person
or creature, we can call it whatever,

415
00:23:47,600 --> 00:23:49,670
and we'll, we'll say, let's
define a bunch of them.

416
00:23:49,730 --> 00:23:52,550
We'll add it to the world will
define the world's boundaries,

417
00:23:52,580 --> 00:23:53,480
the mouse constraint.

418
00:23:53,481 --> 00:23:56,750
This is just visual visualization stuff
in javascript and we'll say, well,

419
00:23:56,751 --> 00:24:01,520
let's restart a generation every five
seconds now when we'll have a counter for

420
00:24:01,521 --> 00:24:03,680
that and we'll display the stats for that.
Okay,

421
00:24:05,480 --> 00:24:08,630
and so the real, the real
meat of this here, I said,

422
00:24:08,631 --> 00:24:11,540
just initialize this population
right here and we'll just say,

423
00:24:11,541 --> 00:24:14,480
just keep doing that over and over and
over again. So generation dot evolve.

424
00:24:15,050 --> 00:24:17,600
That's it for today's video.
I hope you found it useful.

425
00:24:17,601 --> 00:24:20,840
If you want any more cool information,
it's going to be in the video description.

426
00:24:20,841 --> 00:24:23,880
I've got the code, help her
links everything for you. Uh,

427
00:24:23,900 --> 00:24:27,080
check it out if you want to evolve
to the next level in terms of your

428
00:24:27,081 --> 00:24:31,100
programming expertise and your life,
hit the subscribe button for now.

429
00:24:31,130 --> 00:24:33,710
I've got to evolve myself,
so thanks for watching.

