1
00:00:00,090 --> 00:00:00,811
Hello world,

2
00:00:00,811 --> 00:00:05,250
it's Saroj and let's make our own
language translator using tensorflow.

3
00:00:05,550 --> 00:00:10,550
Today there are about 6,800 different
languages spoken across the world and in

4
00:00:10,921 --> 00:00:12,780
an increasingly globalized world,

5
00:00:12,990 --> 00:00:17,040
nearly every culture has interactions
with every other culture in some way.

6
00:00:17,370 --> 00:00:22,080
That means there are an incalculable
number of translation requirements every

7
00:00:22,081 --> 00:00:26,460
second of every day across the world.
Translating is no easy task.

8
00:00:26,461 --> 00:00:30,870
A language isn't just a collection of
words and if rules of grammar and syntax,

9
00:00:30,990 --> 00:00:35,340
it's also a vast interconnected system
of cultural references and connotations.

10
00:00:35,550 --> 00:00:39,870
And this reflects a centuries old problem
of two cultures wanting to communicate

11
00:00:40,050 --> 00:00:42,090
but are blocked by the language barrier.

12
00:00:42,420 --> 00:00:44,860
Our translation systems
are fast improving though.

13
00:00:44,861 --> 00:00:49,770
So whether it be an idea or story or
request each new advanced means one less

14
00:00:49,771 --> 00:00:53,590
message will be lost in translation.
During the Second World War,

15
00:00:53,650 --> 00:00:57,090
the British government was hard at
work trying to decrypt the Morse code.

16
00:00:57,100 --> 00:01:02,100
Did radio communications that Nazi Germany
used to send messages securely known

17
00:01:02,141 --> 00:01:06,940
as enigma. They decided to hire a man
named Alan Turing to help in their effort.

18
00:01:07,180 --> 00:01:10,090
And when the American government
learned of their translation effort,

19
00:01:10,330 --> 00:01:14,590
they were inspired to try it themselves
postwar specifically because they needed

20
00:01:14,591 --> 00:01:17,650
a way to keep up with Russian
scientific publications.

21
00:01:18,100 --> 00:01:22,780
The first public demo of a machine
translation system translated 250 words

22
00:01:22,781 --> 00:01:27,781
between Russian and English in 1954 it
was dictionary based so it would attempt

23
00:01:28,121 --> 00:01:31,900
to match the sorts language to
the target language word for word.

24
00:01:32,230 --> 00:01:35,710
The results were poor since it
didn't capture syntactic structure.

25
00:01:36,040 --> 00:01:38,580
The second generation of
systems used intro lingua.

26
00:01:38,770 --> 00:01:42,940
That means they changed a source language
to a special intermediary language

27
00:01:43,060 --> 00:01:45,010
with specific rules and coded into it.

28
00:01:45,280 --> 00:01:49,450
Then from that generated
the target language. This
proved to be more efficient,

29
00:01:49,451 --> 00:01:53,830
but this approach was soon over shadowed
by the rise of statistical translation

30
00:01:53,831 --> 00:01:58,460
in the early nineties primarily from
engineers at IBM innovation at IBM.

31
00:02:02,080 --> 00:02:06,310
A popular approach was to break the source
texts down into segments and compare

32
00:02:06,311 --> 00:02:10,870
them to an aligned bilingual corpus using
statistical evidence and probabilities

33
00:02:10,871 --> 00:02:13,810
to choose the most likely translation.
Nowadays,

34
00:02:13,811 --> 00:02:17,710
the most used statistical translation
system in the world is Google translate

35
00:02:17,860 --> 00:02:18,730
and with good reason.

36
00:02:19,090 --> 00:02:23,020
Google uses deep learning to translate
from a given language to another with

37
00:02:23,021 --> 00:02:25,990
state of the art results.
So how do they do this?

38
00:02:26,380 --> 00:02:29,830
Let's recreate their results in tensorflow
at to find out the data set we'll be

39
00:02:29,831 --> 00:02:33,790
using to train our language translation
model is a corpus of transcribed the Ted

40
00:02:33,791 --> 00:02:36,760
talks. It's got both the English
virgin and the French version,

41
00:02:36,761 --> 00:02:40,690
and our goal will be to create a model
that can translate from one to the other.

42
00:02:40,691 --> 00:02:41,524
After training,

43
00:02:41,830 --> 00:02:45,910
we'll be using TensorFlow's built in data
utiles class to help us preprocess our

44
00:02:45,911 --> 00:02:48,760
dataset and we'll start by
defining our vocab size,

45
00:02:48,761 --> 00:02:52,030
which is the number of words we
want to train on from our dataset.

46
00:02:52,180 --> 00:02:55,600
We'll send it to 40 k for each,
which is a small portion of the data.

47
00:02:56,020 --> 00:02:59,590
Then we'll use the data utilized class
to read the data from the data directory,

48
00:02:59,830 --> 00:03:04,000
giving it our desired vocab size and it
will return the formatted and tokenized

49
00:03:04,001 --> 00:03:05,950
words in both languages.

50
00:03:06,220 --> 00:03:10,360
Well then initialize
tensorflow placeholders for
our encounter in Dakota inputs.

51
00:03:10,630 --> 00:03:13,450
Both will be integer tensors
that represent discrete values.

52
00:03:13,480 --> 00:03:16,510
They will be embedded into a
dense representation later.

53
00:03:16,600 --> 00:03:20,770
We'll feed our vocabulary words to the
encoder and the Incode had representation

54
00:03:20,771 --> 00:03:24,070
that's learned to the decoder.
Now we can build our model.

55
00:03:24,220 --> 00:03:28,240
Google published a paper more recently
discussing a system they integrated into

56
00:03:28,241 --> 00:03:31,600
their translation service called
neural machine translation.

57
00:03:31,870 --> 00:03:35,950
It's an encoder decoder model inspired
by similar work from other papers on

58
00:03:35,951 --> 00:03:39,580
topics like tech summarization.
So whereas before Google translate,

59
00:03:39,581 --> 00:03:42,580
we translate from language
a to English to language B.

60
00:03:42,820 --> 00:03:44,980
With this new NMT architecture,

61
00:03:45,190 --> 00:03:47,890
it can translate directly from
one language to the other.

62
00:03:48,130 --> 00:03:50,470
It doesn't memorize phrase
to phrase translations.

63
00:03:50,471 --> 00:03:53,680
Instead it encodes the
semantics of the sentence.

64
00:03:53,950 --> 00:03:57,910
This encoding is generalized so it can
even translate between a language pair

65
00:03:57,911 --> 00:04:02,140
like Japanese and Korean that it
hasn't explicitly seen before.

66
00:04:02,260 --> 00:04:06,730
So I guess we can use an LSTM recurrent
network to encode a sentence in language

67
00:04:06,731 --> 00:04:11,560
a. The RNN spits out a hidden
state s which represents
the vectorized contents of

68
00:04:11,561 --> 00:04:14,350
the sentence.
We can then feed as to the decoder,

69
00:04:14,351 --> 00:04:16,660
which will generate the
translated sentence in language.

70
00:04:16,661 --> 00:04:19,840
B Word by word sounds
easy enough, right? Wrong.

71
00:04:19,900 --> 00:04:23,230
There's a drawback to this architecture.
It has limited memory.

72
00:04:23,380 --> 00:04:26,890
That hidden state pass of the LSTM is
where we're trying to cram the whole

73
00:04:26,891 --> 00:04:28,390
sentence.
We want to translate.

74
00:04:28,720 --> 00:04:32,260
S is usually only a few hundred
floating point numbers long.

75
00:04:32,620 --> 00:04:36,550
The more we try to force our sentence
into this fixed dimentionality vector,

76
00:04:36,790 --> 00:04:39,700
the more lossy our neural
net is forced to be.

77
00:04:39,970 --> 00:04:43,030
We could increase the hidden
side of the Lstm after all,

78
00:04:43,031 --> 00:04:45,580
they are supposed to remember
longterm dependencies,

79
00:04:45,850 --> 00:04:50,170
but what happens is as we increase
the hidden size h of the Lstm,

80
00:04:50,380 --> 00:04:53,920
the training time increases exponentially.
So to solve this,

81
00:04:53,921 --> 00:04:56,080
we're going to bring
attention into the mix.

82
00:04:56,260 --> 00:04:58,210
If I was translating a long sentence,

83
00:04:58,211 --> 00:05:01,840
I probably glanced back at the source
sentence a couple times to make sure I was

84
00:05:01,841 --> 00:05:03,550
capturing all the details.

85
00:05:03,820 --> 00:05:07,750
I iteratively pay attention to the
relevant parts of the source sentence.

86
00:05:08,080 --> 00:05:11,830
We can let neural nets do the same by
letting them store and refer to previous

87
00:05:11,831 --> 00:05:13,420
outputs of the LSTM.

88
00:05:13,870 --> 00:05:17,620
This increases the storage of our model
without changing the functionality of

89
00:05:17,621 --> 00:05:18,454
the LSTM.

90
00:05:18,610 --> 00:05:22,960
So the idea is once we have LSTM
outputs from the encoder stored,

91
00:05:23,170 --> 00:05:27,520
we can query each output
asking how relevant they are
to the current computation

92
00:05:27,521 --> 00:05:31,390
happening in the decoder. Each
encoder output gets a relevancy score,

93
00:05:31,391 --> 00:05:36,391
which we can convert to a probability
score by applying a softmax activation to

94
00:05:36,401 --> 00:05:39,040
it.
Then we extract a context vector,

95
00:05:39,160 --> 00:05:42,040
which is a weighted summation
to the Incode outputs.

96
00:05:42,190 --> 00:05:46,450
Depending on how relevant they
are. Memory enough, pay attention.

97
00:05:46,560 --> 00:05:51,090
You have to buddy up spam.
He sneaks past health,

98
00:05:51,280 --> 00:05:56,000
ammonia know as soupy center president
cell memory. Enough pay attention.

99
00:05:56,001 --> 00:05:59,760
We'll build our model using tensorflow
is built in embedding attention sequence

100
00:05:59,761 --> 00:06:00,770
to sequence function,

101
00:06:00,771 --> 00:06:04,970
giving it our encoder and decoder inputs
as well as a few hyper parameters.

102
00:06:04,971 --> 00:06:06,650
We define like the number of layers.

103
00:06:07,160 --> 00:06:09,650
It builds a model that is just
like the one we discussed.

104
00:06:09,950 --> 00:06:13,850
Tensorflow has several built in models
like this that we can drop into our code

105
00:06:13,880 --> 00:06:14,530
easily.

106
00:06:14,530 --> 00:06:18,110
So normally this alone would be fine and
we could run this and the results will

107
00:06:18,111 --> 00:06:18,741
be decent,

108
00:06:18,741 --> 00:06:23,480
but they added another improvement to
their model that requires more code,

109
00:06:23,540 --> 00:06:26,870
a hundred gps and a week of training.
Seriously, that's what it took.

110
00:06:27,260 --> 00:06:31,040
We won't implement it all
programmatically, but let's
dive into it. Conceptually,

111
00:06:31,130 --> 00:06:33,680
if the outputs don't
have sufficient context,

112
00:06:33,890 --> 00:06:35,990
then they won't be able
to give a good answer.

113
00:06:36,380 --> 00:06:40,270
We need to include info about
future words so that the encoder,

114
00:06:40,280 --> 00:06:43,220
how put is determined by
words on the left and right.

115
00:06:43,520 --> 00:06:47,540
We humans would definitely use this
kind of full context to determine the

116
00:06:47,541 --> 00:06:49,430
meaning of a word we see in a sentence.

117
00:06:49,520 --> 00:06:53,900
The way they did this is to use a
bi-directional encoders, so it's two rns,

118
00:06:54,080 --> 00:06:57,590
one that goes forward over the
sentence and the other goes backwards.

119
00:06:57,650 --> 00:07:00,320
So for each word it
concatenates the vector outputs,

120
00:07:00,530 --> 00:07:05,330
which produces a vector with context
from both sides and they added a lot of

121
00:07:05,331 --> 00:07:09,290
layers to their model. The encoder
has one bi-directional RNN layer,

122
00:07:09,440 --> 00:07:11,780
then seven unidirectional corn.
And layers.

123
00:07:11,990 --> 00:07:15,710
The Decoder has eight unit directional
RNN layers. The more layers,

124
00:07:15,711 --> 00:07:19,460
the longer the training time, so that's
why we use a single by directional layer.

125
00:07:19,670 --> 00:07:21,380
If all the layers were by directional,

126
00:07:21,381 --> 00:07:23,720
the whole layer would have
to finish before later.

127
00:07:23,721 --> 00:07:27,500
Dependencies could start computing,
but by using unidirectional layers,

128
00:07:27,740 --> 00:07:29,840
the computations can be more parallel.

129
00:07:30,050 --> 00:07:33,170
We'll initialize our tensor flow
session then our model inside of it.

130
00:07:33,560 --> 00:07:36,170
Let's see some results
after training first,

131
00:07:36,171 --> 00:07:40,580
I'll give it this phrase looks
good, and now another phrase, nope.

132
00:07:40,880 --> 00:07:43,310
While it's not perfect and
we still have a ways to go,

133
00:07:43,311 --> 00:07:46,970
we're definitely getting closer to
having a universal translation model,

134
00:07:46,971 --> 00:07:50,390
breaking it down and coder decoder
architectures offer state of the art

135
00:07:50,450 --> 00:07:52,040
performance in machine translation.

136
00:07:52,340 --> 00:07:54,890
By soaring the previous
outputs of LSTM cells,

137
00:07:55,100 --> 00:07:59,540
we can judge the relevancy of each to
decide which to use via an attention

138
00:07:59,541 --> 00:08:02,390
mechanism and by using
a bi-directional RNN.

139
00:08:02,570 --> 00:08:07,280
The context of both past and future words
is used to create an accurate encoder

140
00:08:07,430 --> 00:08:11,210
output vector. The coding challenge
winner from last week is Ryan Lee.

141
00:08:11,420 --> 00:08:12,530
This was very impressive.

142
00:08:12,531 --> 00:08:17,531
He created a recipe summarizer by
scraping 125,000 recipes from the web and

143
00:08:17,631 --> 00:08:21,140
documented it all beautifully
with installation steps
so you can reproduce the

144
00:08:21,141 --> 00:08:25,070
results yourself wizard of the week.
And the runner up is Sarah Collins.

145
00:08:25,250 --> 00:08:29,600
Her Code Converts Scientific Papers to
texts and prioritizes them by topic.

146
00:08:29,750 --> 00:08:33,380
This week's coding challenge is to create
a simple translation system using an

147
00:08:33,410 --> 00:08:35,060
encoder decoder model.

148
00:08:35,300 --> 00:08:38,180
All the details are in the read me
poster gambling in the comments,

149
00:08:38,181 --> 00:08:39,920
and I'll announce a winner next week.

150
00:08:39,950 --> 00:08:43,280
Please subscribe for more programming
videos, check out this related video,

151
00:08:43,460 --> 00:08:47,120
and for now I've got to get a better GPU,
so thanks for watching.

