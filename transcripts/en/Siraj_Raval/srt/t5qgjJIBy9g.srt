1
00:00:00,270 --> 00:00:00,930
Oh world,

2
00:00:00,930 --> 00:00:05,400
it's Saroj and let's build a chat Bot
that can answer questions about any texts

3
00:00:05,401 --> 00:00:08,970
you give it, be it an article
or even a book using care Os.

4
00:00:09,120 --> 00:00:12,120
Just imagine the boost in productivity.
All of us we'll have,

5
00:00:12,121 --> 00:00:15,780
once we have access to expert
systems for any given topic,

6
00:00:16,050 --> 00:00:19,050
instead of sifting through all
the jargon in a scientific paper,

7
00:00:19,200 --> 00:00:23,820
you just give it the paper, then ask it
the relevant questions, entire textbooks,

8
00:00:23,821 --> 00:00:26,130
libraries, videos, images, whatever.

9
00:00:26,370 --> 00:00:29,310
You just feed it some data and
it would become an expert at it.

10
00:00:29,460 --> 00:00:34,200
All 7 billion people on Earth would
have the capability of learning anything

11
00:00:34,260 --> 00:00:35,310
much faster.

12
00:00:35,430 --> 00:00:39,360
The web democratize information and
this next evolution will democratize

13
00:00:39,361 --> 00:00:41,760
something just as important guidance,

14
00:00:41,820 --> 00:00:46,530
the ideal chatbox and talk intelligently
about any domain. That's the holy grail.

15
00:00:46,531 --> 00:00:50,280
But domain specific chatbots
are definitely possible.

16
00:00:50,520 --> 00:00:54,900
The technical term for this is a
question answering system. Surprisingly,

17
00:00:54,901 --> 00:00:59,370
we've been able to do this since way back
in the 70s. Lunar was one of the first.

18
00:00:59,400 --> 00:01:01,530
It was, as you might've
guessed, rural based.

19
00:01:01,650 --> 00:01:06,330
So it allowed geologists to ask questions
about moon rocks from the Apollo

20
00:01:06,331 --> 00:01:07,260
missions.
Uh,

21
00:01:07,261 --> 00:01:11,370
later improvement to rule based QA
systems allowed programmers to encode

22
00:01:11,371 --> 00:01:16,371
patterns into their Bot called artificial
intelligence markup language or AI ml.

23
00:01:17,250 --> 00:01:22,020
That meant less code for the same results,
but yeah, don't use AI ml. It's so old.

24
00:01:22,021 --> 00:01:24,900
It makes Numa Numa look new.
Now with deep learning,

25
00:01:24,901 --> 00:01:28,830
we can do this without hardcoded
responses and have much better results.

26
00:01:29,190 --> 00:01:33,750
The generic case is that you give it
some docs as input and then ask it a

27
00:01:33,751 --> 00:01:37,920
question. It'll give you the right answer
after logically reasoning about it.

28
00:01:38,220 --> 00:01:42,600
The input could also be that everybody
is happy and then the question could be

29
00:01:42,630 --> 00:01:47,130
what's the sentiment? The answer would be
positive. Other possible questions are,

30
00:01:47,190 --> 00:01:50,070
what's the entity?
What are the part of speech tags?

31
00:01:50,250 --> 00:01:52,230
What's the translation to French?

32
00:01:52,530 --> 00:01:55,440
We need a common model for
all of these questions.

33
00:01:55,770 --> 00:01:59,070
This is what the Ai community is
trying to figure out how to do.

34
00:01:59,370 --> 00:02:03,060
Facebook research made some great
progress with this just two years ago when

35
00:02:03,240 --> 00:02:08,240
they released a paper introducing this
really cool idea called a memory network.

36
00:02:08,460 --> 00:02:13,350
LSTM networks proved to be a useful
tool in tasks like tech summarization,

37
00:02:13,380 --> 00:02:18,380
but their memory is encoded by hidden
states and weights is too small for very,

38
00:02:19,170 --> 00:02:23,160
very long sequences of data
be that a book or a movie,

39
00:02:23,520 --> 00:02:26,640
a way around this for language
translation for example,

40
00:02:26,641 --> 00:02:31,641
was to store multiple LSTM states and
use an attention mechanism to choose

41
00:02:31,771 --> 00:02:32,604
between them,

42
00:02:32,700 --> 00:02:37,680
but they developed another strategy that
outperform an LSTM for Q and a systems.

43
00:02:37,950 --> 00:02:42,950
The idea was to allow a neural network
to use an external data structure as

44
00:02:43,051 --> 00:02:44,100
memory storage.

45
00:02:44,250 --> 00:02:48,180
It learns where to retrieve their
required memory from the memory bank in a

46
00:02:48,181 --> 00:02:49,140
supervised way.

47
00:02:49,590 --> 00:02:52,710
When it came to answering questions from
[inaudible] data that was generated,

48
00:02:52,950 --> 00:02:57,810
that Info was pretty easy to come by. But
in real world data, it is not that easy.

49
00:02:58,080 --> 00:03:02,500
Most recently there was a four month
long Kaggle contest that a startup called

50
00:03:02,560 --> 00:03:06,280
metamind place in the top 5% four.
To do this,

51
00:03:06,281 --> 00:03:11,050
they built a new state of the art model
called a dynamic memory network that

52
00:03:11,051 --> 00:03:14,890
built on Facebook's initial idea.
That's the one we'll focus on.

53
00:03:14,891 --> 00:03:18,280
So let's build it programmatically
using care Os. This dataset,

54
00:03:18,350 --> 00:03:19,780
it's pretty well organized.

55
00:03:19,810 --> 00:03:23,980
It was created by Facebook AI research
for the specific goal of improving

56
00:03:23,981 --> 00:03:27,160
textual reasoning.
It's grouped into 20 different tasks.

57
00:03:27,340 --> 00:03:30,160
Each task test a different
aspect of reasoning,

58
00:03:30,161 --> 00:03:35,080
so overall it provides a good overview
of all the different capabilities of your

59
00:03:35,081 --> 00:03:35,914
learning model.

60
00:03:35,980 --> 00:03:40,060
There are a thousand questions
for training and a thousand
for testing per task.

61
00:03:40,360 --> 00:03:44,560
Each question is paired with a statement
or a series of statements as well as an

62
00:03:44,561 --> 00:03:49,360
answer. The goal is to have one model
that can succeed in all tasks easily.

63
00:03:49,690 --> 00:03:53,530
We'll use pretrained glove vectors to
help create a sequence of word vectors

64
00:03:53,740 --> 00:03:57,820
from our input sentences and these
vectors will act as inputs to the model.

65
00:03:58,090 --> 00:04:02,770
The DMN and architecture defines two
types of memory, semantic and episodic.

66
00:04:03,010 --> 00:04:05,770
These input vectors are
considered the semantic memory,

67
00:04:05,800 --> 00:04:09,160
whereas episodic memory may
contain other knowledge as well,

68
00:04:09,340 --> 00:04:10,870
and we'll talk about that in a second.

69
00:04:11,020 --> 00:04:14,920
We can fetch our babbled Dataset from
the web and split them into training and

70
00:04:14,921 --> 00:04:15,311
testing.

71
00:04:15,311 --> 00:04:19,390
Data glove will help convert our words
to vectors so they're ready to be fed

72
00:04:19,391 --> 00:04:21,280
into our model.
The first module,

73
00:04:21,310 --> 00:04:26,170
the input module is a gru or gated
recurrent unit that runs on a sequence of

74
00:04:26,171 --> 00:04:30,070
word vectors.
A Gru cell is kind of like an LSTM cell,

75
00:04:30,190 --> 00:04:34,480
but it's more computationally efficient
since it only has two gates and it

76
00:04:34,481 --> 00:04:35,950
doesn't use a memory unit.

77
00:04:36,190 --> 00:04:40,030
The two gates control when it's content
is updated and when it's erased.

78
00:04:40,240 --> 00:04:42,480
A recent update,

79
00:04:42,710 --> 00:04:45,300
recent update or recent,

80
00:04:47,600 --> 00:04:52,520
and the hidden state of the input module
represents the input process so far in

81
00:04:52,550 --> 00:04:53,383
a vector.

82
00:04:53,540 --> 00:04:58,040
It outputs hidden states after every
sentence and he's outputs are called facts

83
00:04:58,190 --> 00:05:01,130
in the paper because they represent
the essence of what is fed.

84
00:05:01,310 --> 00:05:04,970
Given a word vector and the previous time
step director will compute the current

85
00:05:04,971 --> 00:05:08,570
time step vector. The update gate
is a single layer neural network.

86
00:05:08,571 --> 00:05:13,571
We sum up the Matrix multiplications
and add a bias term and then the sigmoid

87
00:05:13,641 --> 00:05:17,690
squashes it to a list of values
between zero and one the output vector.

88
00:05:17,960 --> 00:05:20,900
We do this twice with
different sets of weights.

89
00:05:21,110 --> 00:05:24,860
Then we use a reset gate that we'll
learn to ignore the past time steps when

90
00:05:24,861 --> 00:05:26,420
necessary.
For example,

91
00:05:26,421 --> 00:05:30,050
if the next sentence has nothing to
do with those that came before it,

92
00:05:30,470 --> 00:05:34,220
the uptake gate is similar in that it
can learn to ignore the current time step

93
00:05:34,250 --> 00:05:38,000
entirely. Maybe the current sentence
has nothing to do with the answer,

94
00:05:38,001 --> 00:05:42,350
whereas previous ones did.
Then there's the question module.

95
00:05:42,440 --> 00:05:47,440
It processes the question word by word
and I'll put a vector by using the same

96
00:05:47,451 --> 00:05:50,570
gru as the input module
and the same weights.

97
00:05:50,690 --> 00:05:54,140
We can encode both of them by
creating embedding layers for both.

98
00:05:54,500 --> 00:05:57,350
Then we'll create an episodic
memory representation.

99
00:05:57,560 --> 00:06:01,370
Both the motivation for this in the paper
came from the hippocampus function in

100
00:06:01,371 --> 00:06:02,204
our brain.

101
00:06:02,420 --> 00:06:06,350
It's able to retrieve can portal states
that are triggered by some response,

102
00:06:06,351 --> 00:06:08,570
like a sight or a sound,

103
00:06:08,990 --> 00:06:12,950
both the fact and questioned vectors that
are extracted from the input enter the

104
00:06:12,951 --> 00:06:15,860
episodic memory module.
It's composed of two nested.

105
00:06:15,861 --> 00:06:19,430
You are use the energy are you
generate what are called episodes.

106
00:06:19,640 --> 00:06:22,940
It does this by passing over
the facts from the input module,

107
00:06:23,120 --> 00:06:24,980
but when updating it's interstate,

108
00:06:25,070 --> 00:06:29,120
it takes into account the output of an
attention function. On the current fact,

109
00:06:29,150 --> 00:06:33,770
the attention function gives a score
between zero and one to each fact and so

110
00:06:33,890 --> 00:06:38,890
the gru ignores facts with low scores
after each full pass on all the facts,

111
00:06:39,140 --> 00:06:42,980
the energy are you outputs an episode
which is then fed to the outer gru.

112
00:06:43,040 --> 00:06:47,300
The reason we need multiple episodes is
so our model can learn what part of a

113
00:06:47,301 --> 00:06:51,470
sentence it should pay attention to.
After realizing after one pass,

114
00:06:51,650 --> 00:06:54,530
that's something else is important.
With multiple passes,

115
00:06:54,531 --> 00:06:57,470
we can gather increasingly
relevant information.

116
00:06:57,680 --> 00:07:02,210
We can initialize our model and set
its loss function as categorical cross

117
00:07:02,211 --> 00:07:06,770
entropy with the stochastic gradient
descent implementation rms prop.

118
00:07:07,110 --> 00:07:09,680
Then train it on the given
data using the fit function.

119
00:07:09,740 --> 00:07:13,340
We can test this code in the browser
without waiting for it to train because

120
00:07:13,341 --> 00:07:14,270
luckily for us,

121
00:07:14,271 --> 00:07:18,290
this researcher uploaded a web app with
a fully trained model of this code.

122
00:07:18,440 --> 00:07:22,490
We can generate a story which is a
collection of sentences each describing an

123
00:07:22,491 --> 00:07:25,820
event in sequential order.
Then we'll ask it a question.

124
00:07:26,060 --> 00:07:27,530
Pretty high accuracy response.

125
00:07:27,770 --> 00:07:31,880
Let's generate another story and ask
it another question. Hero status.

126
00:07:32,120 --> 00:07:34,250
Let's go over the three
key facts we've learned.

127
00:07:34,550 --> 00:07:39,260
Gru is control the flow of data like
LSTM cells but are more computationally

128
00:07:39,261 --> 00:07:42,380
efficient using just two gates.
Update and reset.

129
00:07:42,740 --> 00:07:46,820
Dynamic memory networks offer state of
the art performance in question answering

130
00:07:46,821 --> 00:07:51,821
systems and they do this by using both
semantic and episodic memory inspired by

131
00:07:51,921 --> 00:07:55,500
the hippocampus. Drum
roll please. Nevermind.

132
00:07:55,850 --> 00:07:58,970
Nemanja told Meek is decoding
challenge winner from last week.

133
00:07:59,270 --> 00:08:02,840
He implemented his own neural machine
translator by training it on movie

134
00:08:02,841 --> 00:08:04,940
subtitles in both English and German.

135
00:08:05,210 --> 00:08:07,880
You can see all the results
in his eye python notebook.

136
00:08:08,030 --> 00:08:11,770
Amazing work wizard of the week
and the runner up is Michelle Batu.

137
00:08:11,780 --> 00:08:15,020
Despite the massive amount of
training time and empty requires,

138
00:08:15,200 --> 00:08:19,010
Michelle was able to achieve some
great results. I vow to both of you.

139
00:08:19,190 --> 00:08:22,370
This week's challenge is to
make your own Q and a chat bot.

140
00:08:22,610 --> 00:08:25,550
All the details are in the read me get
humbling links going to comments and all

141
00:08:25,551 --> 00:08:26,930
announce a winner a week from today.

142
00:08:27,020 --> 00:08:30,050
Please subscribe for more programming
videos, check out this related video,

143
00:08:30,260 --> 00:08:33,920
and for now I've got to ask the right
questions, so thanks for watching.

