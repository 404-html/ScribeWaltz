1
00:00:00,210 --> 00:00:04,560
Okay Google, what would
we learn? Hello world,

2
00:00:04,561 --> 00:00:07,290
it's Siraj and 2018 will be the year.

3
00:00:07,410 --> 00:00:10,470
We finally start to see
chat bots go mainstream.

4
00:00:10,710 --> 00:00:15,030
In this video we'll build a chat bot
using Google's tensorflow machine learning

5
00:00:15,060 --> 00:00:18,630
framework. If you've created
an awesome product or service,

6
00:00:18,840 --> 00:00:22,920
you've got to find a way to get it
out there for people to see it. Right?

7
00:00:23,280 --> 00:00:28,280
Content marketing is the
process of creating an organic
growth channel for your

8
00:00:28,351 --> 00:00:32,700
business using content. This
could be blog posts, videos,

9
00:00:32,850 --> 00:00:35,190
infographics who had prank videos,

10
00:00:35,400 --> 00:00:39,810
content that can help attract
thousands of potential customers,

11
00:00:40,110 --> 00:00:45,110
and since content creation is itself
a very time consuming process,

12
00:00:45,450 --> 00:00:49,890
we need to make sure we're delivering
it in the most efficient way possible.

13
00:00:50,250 --> 00:00:54,840
The ability to gain insights from
user behavior via machine learning

14
00:00:54,870 --> 00:00:59,310
technologies has completely
changed the marketing game.

15
00:00:59,670 --> 00:01:04,670
Now companies can interpret exactly what
their customers are like and what they

16
00:01:05,431 --> 00:01:09,660
do on the Internet. Email inboxes
have become more and more cluttered,

17
00:01:09,661 --> 00:01:14,661
so buyers have moved to social media
to follow the brands they really care

18
00:01:14,731 --> 00:01:15,450
about.

19
00:01:15,450 --> 00:01:20,450
Smart brands have moved from email
marketing to social media marketing to

20
00:01:21,001 --> 00:01:25,410
interact with and engage their
customers to maximize sales.

21
00:01:25,650 --> 00:01:30,650
Most people stick to using just five
apps and today messaging apps have over 5

22
00:01:31,650 --> 00:01:33,060
billion monthly active users,

23
00:01:33,240 --> 00:01:37,710
which means that for the first time
people are using them more than social

24
00:01:37,711 --> 00:01:38,544
networks.

25
00:01:38,610 --> 00:01:43,380
It's fair to say users are spending
more time on whatsapp and messenger than

26
00:01:43,381 --> 00:01:45,540
they do on Twitter and Facebook.

27
00:01:45,810 --> 00:01:50,550
That's because chatting is more
of an engaging activity. Lol, Lma,

28
00:01:50,551 --> 00:01:50,911
Mayo,

29
00:01:50,911 --> 00:01:55,860
rof l brands need to start engaging
their customers through these messaging.

30
00:01:55,861 --> 00:02:00,861
Mediums and chatbots are the solution
chatbots or a type of program that

31
00:02:01,141 --> 00:02:02,700
converse with a user.

32
00:02:02,940 --> 00:02:07,940
They mimic humans both in conversational
ability and in performing any task.

33
00:02:08,730 --> 00:02:09,481
Alan terrain,

34
00:02:09,481 --> 00:02:14,010
one of the fathers of computer science
burst spoke of the concept in the fifties

35
00:02:14,011 --> 00:02:18,960
and since then the concept of a chat
Bot has evolved through the decades.

36
00:02:19,230 --> 00:02:24,230
They're gaining popularity in the
implementation of real world applications

37
00:02:24,900 --> 00:02:29,900
since they can cater the customer
experience by providing engaging support,

38
00:02:30,600 --> 00:02:35,220
product recommendations and
conversational marketing campaigns.

39
00:02:35,520 --> 00:02:40,500
They never sleep, they're cost efficient
and they work well enough to increase.

40
00:02:40,530 --> 00:02:44,520
All sorts of metrics from clickthrough
rates to customer sentiment.

41
00:02:44,850 --> 00:02:49,560
They can function as a support
agent, a lawyer, teacher, doctor toy,

42
00:02:49,561 --> 00:02:54,540
or even as a companion that enriches
your life. Domino's pizza for example,

43
00:02:54,541 --> 00:02:59,541
created a chat Bot for Facebook messenger
where users can order delivery through

44
00:03:00,310 --> 00:03:04,990
their chat Bot and get status
updates on the delivery. Nick Dutch,

45
00:03:05,020 --> 00:03:10,020
the marketing lead at the company admitted
that it didn't necessarily translate

46
00:03:10,810 --> 00:03:12,820
to a huge surge in sales,

47
00:03:13,060 --> 00:03:17,140
but it did result in an improvement
in experience and as a result,

48
00:03:17,170 --> 00:03:22,170
sentiment towards the brand consulting
groups like bought works.ai or using AI

49
00:03:22,661 --> 00:03:27,661
to create chatbots for brands like
change.org Tony Robbins and media posts to

50
00:03:28,391 --> 00:03:32,920
help them engage consumers across
a wide variety of platforms,

51
00:03:33,160 --> 00:03:36,610
tailoring the technology to
each brands specific needs.

52
00:03:36,880 --> 00:03:41,880
There are also tons of services that led
companies build chatbots for their own

53
00:03:42,101 --> 00:03:45,100
needs like Amazon, Lex, Ibm Watson,

54
00:03:45,180 --> 00:03:47,770
a zero bought services and motion.ai.

55
00:03:48,100 --> 00:03:52,200
Clearly the tools and interest are there,
but how are we supposed to,

56
00:03:52,201 --> 00:03:56,230
to choose what tools we need
to build our own chat bot,

57
00:03:56,650 --> 00:03:59,230
brush and relay anyone we can easily use.

58
00:03:59,231 --> 00:04:04,231
A service that lets us set a few rules
and deploy our own chat Bot in a few

59
00:04:04,540 --> 00:04:06,520
minutes,
but rather than just doing that,

60
00:04:06,700 --> 00:04:09,340
let's talk about how this
all works under the hood.

61
00:04:09,370 --> 00:04:13,390
Technically there are two major
types of dialogue systems,

62
00:04:13,690 --> 00:04:16,960
goal oriented ones and
general conversation ones.

63
00:04:17,290 --> 00:04:20,830
General conversation models
can be divided into two types,

64
00:04:21,130 --> 00:04:25,360
generative and selective models.
The idea is the same for both.

65
00:04:25,420 --> 00:04:30,070
We'll input some dialogue and it will
predict the answer for that context.

66
00:04:30,310 --> 00:04:35,200
I mean all of machine learning follows
the same basic steps for a model,

67
00:04:35,260 --> 00:04:39,280
build it, train it and test
it, but before we even do that,

68
00:04:39,310 --> 00:04:42,280
we need to find some
quality dialogue data.

69
00:04:42,550 --> 00:04:47,550
What we need is a dialogue dataset and
since labeled datasets are easiest to

70
00:04:48,011 --> 00:04:49,660
learn from,
we'd want that.

71
00:04:49,900 --> 00:04:53,140
We'd call each row in it
a context replied pair.

72
00:04:53,500 --> 00:04:56,080
The context could be
one or several inputs,

73
00:04:56,081 --> 00:04:58,720
sentences and the reply will be to label.

74
00:04:58,930 --> 00:05:03,930
Sometimes there is an eos or end of
sequence token at the end of each sentence

75
00:05:03,941 --> 00:05:04,774
in the batch.

76
00:05:04,900 --> 00:05:09,900
This helps machine learning algorithms
understand sentence bounds and update its

77
00:05:09,941 --> 00:05:11,680
internal states wisely.

78
00:05:12,070 --> 00:05:15,610
We can find this dialogue
dataset on various websites,

79
00:05:15,730 --> 00:05:19,870
support calls that were recorded,
movie dialogues, rap battles.

80
00:05:19,930 --> 00:05:24,460
All of these are real human
conversations that our AI can learn from.

81
00:05:24,730 --> 00:05:29,230
Let's start with generative models. One
of my favorite machine learning papers,

82
00:05:29,350 --> 00:05:34,350
a neuro conversational model tackle this
problem head on a couple of years ago.

83
00:05:35,380 --> 00:05:40,380
They use a model called
sequence to sequence to model
the dialogue given to them

84
00:05:41,290 --> 00:05:42,430
in a Dataset.

85
00:05:42,790 --> 00:05:47,790
This model is represented by two recurrent
neural networks with a different set

86
00:05:47,921 --> 00:05:51,850
of parameters each while
regular feed forward.

87
00:05:51,850 --> 00:05:56,850
Neural networks are given a new data point
at every time step during training to

88
00:05:57,231 --> 00:06:02,231
learn from recurrent neural networks
are given both a new data points and the

89
00:06:03,501 --> 00:06:08,450
learned hidden states from the previous
time step at each new time step during

90
00:06:08,451 --> 00:06:12,500
training. In this way, there is a
recurrence in what it's learning.

91
00:06:12,650 --> 00:06:14,570
It's learning not just from the data.

92
00:06:14,750 --> 00:06:19,310
It's the learning from how it's learned
to before a sort of recurrence or a

93
00:06:19,311 --> 00:06:22,100
feedback loop. Loopity loop, Scoop d dupe.

94
00:06:22,460 --> 00:06:27,410
This is really useful for learning from
sequences of data where predicting the

95
00:06:27,411 --> 00:06:31,940
next word in a sequence depends
not just on the previous words,

96
00:06:32,120 --> 00:06:34,190
but all the words that came before it.

97
00:06:34,550 --> 00:06:37,340
The first recurrent net
is called the encoder.

98
00:06:37,550 --> 00:06:40,220
It's given a sequence of context tokens,

99
00:06:40,370 --> 00:06:44,090
one at a time and updates it's
hidden state. Accordingly.

100
00:06:44,360 --> 00:06:47,450
After processing the
whole context sequence,

101
00:06:47,480 --> 00:06:52,480
it produces a final hidden state which
incorporates the context and uses it for

102
00:06:52,641 --> 00:06:54,320
generating the answer.

103
00:06:55,160 --> 00:06:58,040
The other recurring net
is called the decoder.

104
00:06:58,100 --> 00:07:03,100
It's job is to take the
context representation from
the encoder as input and

105
00:07:03,681 --> 00:07:07,790
output. An answer. The reply
generation process works like this.

106
00:07:08,120 --> 00:07:12,620
The decoders hidden state is initialized
using the final hidden state from the

107
00:07:12,621 --> 00:07:16,610
encoder. The Eos token is the
first input to the decoder,

108
00:07:16,760 --> 00:07:21,350
which updates it's hidden state.
A word is sampled from the last layer.

109
00:07:21,410 --> 00:07:26,360
It's fed in as input. The hidden state
is updated and a new word is output.

110
00:07:26,540 --> 00:07:31,540
This process is repeated in a loop
until an eos token is output or some

111
00:07:31,941 --> 00:07:35,900
predefined maximum answer
length threshold is hit.

112
00:07:36,260 --> 00:07:38,420
This process is considered inference.

113
00:07:38,540 --> 00:07:43,130
It's the process that a chat bot model
goes through in real time once it's

114
00:07:43,131 --> 00:07:44,330
already been trained.

115
00:07:44,670 --> 00:07:48,920
The training part works slightly
differently in each decoding step.

116
00:07:49,070 --> 00:07:53,120
We use the correct word instead
of the generated one as the input.

117
00:07:53,360 --> 00:07:57,170
Basically the decoder consumes
a correct reply sequence,

118
00:07:57,320 --> 00:08:01,600
but with the last token removed
and the Eos token pretended.

119
00:08:01,910 --> 00:08:06,800
The goal during training is to maximize
the probability of the correct next word

120
00:08:06,830 --> 00:08:07,940
on each time step.

121
00:08:08,210 --> 00:08:11,930
We're asking the network to predict
the next word in the sequence.

122
00:08:12,080 --> 00:08:14,750
By providing it with a correct prefix.

123
00:08:15,110 --> 00:08:19,880
We minimize the error through the most
popular optimization strategy in machine

124
00:08:19,881 --> 00:08:21,650
learning backpropagation,

125
00:08:21,800 --> 00:08:24,920
which I of course have a
five minute video on swag.

126
00:08:25,130 --> 00:08:29,360
See the link in the video description.
This type of model worked pretty well,

127
00:08:29,361 --> 00:08:34,250
but it still had some generic
responses like okay, or I don't know.

128
00:08:34,730 --> 00:08:39,730
Another major problem was
that it generated inconsistent
responses like asking

129
00:08:40,071 --> 00:08:43,970
the same question twice and
getting two different answers.

130
00:08:44,420 --> 00:08:47,210
A newer paper that
dealt with these issues,

131
00:08:47,360 --> 00:08:51,080
what's called a persona based
neuro conversational model.

132
00:08:51,380 --> 00:08:56,380
The authors used speaker ids for each
utterance in order to generate an answer

133
00:08:57,450 --> 00:09:02,310
which learned not based only on the
encoder state but also on speaker

134
00:09:02,311 --> 00:09:05,520
embeddings.
When it comes to selective models,

135
00:09:05,550 --> 00:09:10,350
instead of estimating the probability
of a certain reply given a certain

136
00:09:10,351 --> 00:09:11,184
context,

137
00:09:11,370 --> 00:09:16,370
they learn a similarity function where
a reply is one of the elements in a

138
00:09:16,411 --> 00:09:20,190
predefined pool of possible
answers the network.

139
00:09:20,191 --> 00:09:25,191
We'll take context and a possible
reply as inputs and we'll return the

140
00:09:25,231 --> 00:09:30,231
confidence of how appropriate they are
to each other to neural networks can be

141
00:09:30,241 --> 00:09:33,330
used here and they can
be of any given type.

142
00:09:33,660 --> 00:09:38,610
The first network is used for the
context and the second one for the reply.

143
00:09:38,880 --> 00:09:43,440
These networks will take it's input and
embed them into a vector representation.

144
00:09:43,890 --> 00:09:48,890
Then the similarity between the context
and replied vector is computed using

145
00:09:49,381 --> 00:09:51,900
cosign similarity.
During inference,

146
00:09:51,901 --> 00:09:56,901
we can simply calculate the similarity
between a given context and all possible

147
00:09:57,511 --> 00:10:01,080
answers to choose the one
with the maximum similarity.

148
00:10:01,260 --> 00:10:04,980
In order to train the model,
we'll use triplet loss,

149
00:10:05,010 --> 00:10:09,510
which includes the context,
the correct reply and the wrong reply.

150
00:10:09,780 --> 00:10:11,400
By minimizing this loss.

151
00:10:11,520 --> 00:10:16,520
We learned the similarity function in a
ranking way where absolute values aren't

152
00:10:16,650 --> 00:10:19,770
informative,
so which type of model should we use?

153
00:10:20,130 --> 00:10:24,120
Generative models can generate
almost any type of answer,

154
00:10:24,360 --> 00:10:29,360
but the con is that it's difficult to
impose certain properties on model replies

155
00:10:29,701 --> 00:10:33,120
like no curse words or speak
like a certain character.

156
00:10:33,420 --> 00:10:37,440
Selective models have a more
restricted pool of answers,

157
00:10:37,590 --> 00:10:39,420
but they are more customizable.

158
00:10:39,750 --> 00:10:44,750
The best way to evaluate a chat bot
is through human evaluation currently,

159
00:10:45,330 --> 00:10:50,070
but as research progresses, we'll come
to a more systematic approach here.

160
00:10:50,670 --> 00:10:52,980
Despite a lot of work in this area,

161
00:10:53,040 --> 00:10:58,040
neural dialogue systems are not ready to
talk with humans in an open domain and

162
00:10:58,891 --> 00:11:00,360
give them helpful answers,

163
00:11:00,570 --> 00:11:05,570
but close domain applications like
technical support are a perfect fit.

164
00:11:06,450 --> 00:11:11,430
Three things to remember from this video.
More people are using messaging apps,

165
00:11:11,610 --> 00:11:14,790
then social media apps
in this day and age,

166
00:11:15,000 --> 00:11:20,000
which offers a huge opportunity for
brands to engage their customers through

167
00:11:20,210 --> 00:11:21,043
chatbots.

168
00:11:21,200 --> 00:11:25,290
Chatbots can follow either a
generative or selective process.

169
00:11:25,530 --> 00:11:30,390
Both involve neural networks and there
are a ton of services that will help you

170
00:11:30,391 --> 00:11:33,900
build chatbots,
each with varying degrees of control.

171
00:11:34,020 --> 00:11:38,070
Links to some in the video description.
Are you pumped to build a chat by yet?

172
00:11:38,310 --> 00:11:41,760
You should be subscribe for more
gems like this. And for now,

173
00:11:41,820 --> 00:11:44,700
I'm going to engage my audience,
so thanks for watching.

