1
00:00:00,060 --> 00:00:00,893
Hello world,

2
00:00:00,960 --> 00:00:05,730
it's arrived and let me show you how
to learn deep learning in six weeks.

3
00:00:05,820 --> 00:00:06,511
What's next?

4
00:00:06,511 --> 00:00:11,511
Rocket Science in one week and this video
will explain the six week curriculum

5
00:00:12,480 --> 00:00:15,960
I've created to help you learn
the art of deep learning.

6
00:00:16,380 --> 00:00:21,380
The only prerequisite is knowing basic
python and by the end of this curriculum

7
00:00:22,620 --> 00:00:27,390
you'll have a broad understanding of
some of the key technologies that make up

8
00:00:27,450 --> 00:00:28,380
deep learning.

9
00:00:28,740 --> 00:00:33,740
You might be thinking why deep learning
without machine learning machine

10
00:00:34,081 --> 00:00:39,081
learning is a broad set of algorithms
use to derive insights from data sets.

11
00:00:40,170 --> 00:00:43,440
Deep learning is a subset
of all of those algorithms,

12
00:00:43,470 --> 00:00:48,470
specifically all the various types of
neural networks and when applied to

13
00:00:49,050 --> 00:00:53,220
massive datasets and given
massive computing power,

14
00:00:53,580 --> 00:00:58,050
these neural networks will outperform
all other models. Most of the time.

15
00:00:58,680 --> 00:01:03,680
Deep learning is the hottest field in
AI right now and it's responsible for

16
00:01:03,900 --> 00:01:08,900
everything from Google's latest duplex
assistant to Tesla's self driving cars to

17
00:01:09,931 --> 00:01:11,790
robot companions.

18
00:01:12,360 --> 00:01:16,080
You can get a job as a deep learning
engineer by browsing through listings like

19
00:01:16,110 --> 00:01:16,943
angel list,

20
00:01:16,980 --> 00:01:21,960
Hacker News and indeed.com and when
it comes to getting hired by these

21
00:01:21,961 --> 00:01:22,800
employers,

22
00:01:23,160 --> 00:01:27,210
usually what they're looking
for is experience building,

23
00:01:27,211 --> 00:01:32,211
training and deploying deep learning
models for real life use cases.

24
00:01:32,740 --> 00:01:36,600
I uploading one project every
week to your get hub profile.

25
00:01:36,750 --> 00:01:41,750
You'll have built a substantial portfolio
to show perspective employers and some

26
00:01:42,571 --> 00:01:46,800
of them can even be deployed to
Heroku to be usable as a web app.

27
00:01:47,040 --> 00:01:51,690
So where do we begin? Deep learning
is built on mathematical principles.

28
00:01:51,990 --> 00:01:55,830
It's all math really specifically.
It's linear Algebra,

29
00:01:55,980 --> 00:01:59,280
probability theory,
Calculus and statistics.

30
00:01:59,610 --> 00:02:04,610
These are sub fields of math that cover
an enormous breadth of topics each and

31
00:02:04,981 --> 00:02:07,830
it's a little daunting to
try to tackle them all.

32
00:02:07,980 --> 00:02:12,270
Ideally we could learn from a
source that teaches us the subjects,

33
00:02:12,480 --> 00:02:17,160
but not all of them. Just the relevant
parts that apply to deep learning.

34
00:02:17,700 --> 00:02:20,310
The deep learning book by
Google brain researcher,

35
00:02:20,340 --> 00:02:23,220
Ian Goodfellow does a perfect job of that.

36
00:02:23,580 --> 00:02:26,910
It's free and openly
available on deep learning.

37
00:02:26,911 --> 00:02:31,911
book.org for the first week of this
curriculum read part one of this book.

38
00:02:33,210 --> 00:02:38,210
It does an incredible job of diving into
specific subtopics in each subject that

39
00:02:39,241 --> 00:02:44,241
are used consistently in deep learning
terms like a derivative and doc product

40
00:02:44,611 --> 00:02:49,530
are explained in an easy to understand
way with ample explanation behind the

41
00:02:49,531 --> 00:02:51,540
math notation that's used.

42
00:02:51,870 --> 00:02:56,870
I've also included a cheat sheet for
math notation to help you understand what

43
00:02:56,881 --> 00:02:59,260
the symbols mean. If
you don't, don't worry.

44
00:02:59,261 --> 00:03:03,520
If you don't get how all of these concepts
are used in a neural network right

45
00:03:03,521 --> 00:03:04,354
away.

46
00:03:04,480 --> 00:03:09,450
Just get yourself familiar with the
ideas presented in the book first. No,

47
00:03:09,490 --> 00:03:14,200
that a matrix is a group of numbers and
matrices can be modified in different

48
00:03:14,201 --> 00:03:15,180
ways.
No,

49
00:03:15,280 --> 00:03:19,900
that a distribution models
the probabilities of
different possible outcomes

50
00:03:20,050 --> 00:03:24,040
occurring in an experiment.
Once you've read part one,

51
00:03:24,070 --> 00:03:26,770
it's time to build your
first neural network.

52
00:03:27,010 --> 00:03:31,870
Watch my build a neural network in four
minutes video on youtube to get started.

53
00:03:31,960 --> 00:03:36,960
Then read Andrew Track's amazing blog
post called a neural network in 11 lines

54
00:03:38,231 --> 00:03:39,064
of python.

55
00:03:39,190 --> 00:03:43,420
Fire up your text editor and start
coding your neural network in Python.

56
00:03:43,480 --> 00:03:45,070
You don't have to memorize it to,

57
00:03:45,071 --> 00:03:49,690
you could even type it out line by line
as you watch my video or stare at the

58
00:03:49,691 --> 00:03:50,524
finish code,

59
00:03:50,650 --> 00:03:55,330
but the simple act of typing it out
will be beneficial to your memory.

60
00:03:56,170 --> 00:04:00,010
Your network is simply a series of
operations that are applied to some input

61
00:04:00,011 --> 00:04:02,710
data until it results in some output data.

62
00:04:03,070 --> 00:04:05,080
Nothing really special about that,

63
00:04:05,200 --> 00:04:10,060
but the real magic of these things as
a result of an optimization technique

64
00:04:10,240 --> 00:04:12,070
called back propagation,

65
00:04:12,430 --> 00:04:16,540
it's how a neural network learns
to improve its output over time.

66
00:04:16,630 --> 00:04:18,850
It's a technique from calculus.

67
00:04:19,090 --> 00:04:23,470
I have a great video on this called
backpropagation in five minutes.

68
00:04:23,650 --> 00:04:26,650
Check that one out.
By the end of this week,

69
00:04:26,680 --> 00:04:31,680
you should have a basic idea of how a
simple feed forward neural network works

70
00:04:32,580 --> 00:04:36,790
backpropagation included.
Once you've mastered that,

71
00:04:36,880 --> 00:04:39,280
everything else becomes easier.

72
00:04:39,850 --> 00:04:44,850
Every other neural network is just some
variation of this and each variation

73
00:04:45,400 --> 00:04:49,360
excels at a specific use case.
Speaking of variations,

74
00:04:49,361 --> 00:04:52,630
let's move on to week two
convolutional networks.

75
00:04:52,990 --> 00:04:57,400
There's a lot of different types
of data out there. Numbers, text,

76
00:04:57,550 --> 00:04:58,360
video,

77
00:04:58,360 --> 00:05:02,470
audio in different types of networks
can be used to learn from each of them.

78
00:05:03,130 --> 00:05:08,130
While feedforward networks are great for
learning the mapping between numerical

79
00:05:08,320 --> 00:05:10,450
input and output data,

80
00:05:10,840 --> 00:05:15,340
convolutional networks are wellmade
for learning from image data sets.

81
00:05:15,490 --> 00:05:19,750
If we think of an image as a group
of numbers each describing it's pixel

82
00:05:19,751 --> 00:05:22,480
intensity value on an RGB scale,

83
00:05:22,780 --> 00:05:27,340
then we can consider it a matrix and
this matrix can be input into a neural

84
00:05:27,341 --> 00:05:32,140
network operated on and result in an
output which is a class probability.

85
00:05:32,680 --> 00:05:37,390
Confidence were invented by
young macaroon's team over
two decades ago and are

86
00:05:37,391 --> 00:05:41,650
still responsible for some of the state
of the art advances in computer vision

87
00:05:41,651 --> 00:05:44,380
technology including driverless vehicles.

88
00:05:44,650 --> 00:05:49,650
A great resource to learn about them is
the convolutional neural networks course

89
00:05:50,020 --> 00:05:54,310
on Coursera taught by professor
Andrew Ng of Stanford University.

90
00:05:54,730 --> 00:05:59,730
It dives into both the architecture and
application specific details in an easy

91
00:06:00,261 --> 00:06:03,710
to understand way like
all neural networks,

92
00:06:03,711 --> 00:06:08,090
confidence have lots of variations,
deep convolutional networks,

93
00:06:08,210 --> 00:06:13,040
skip connection networks that can also
be used as building blocks for more

94
00:06:13,041 --> 00:06:16,280
complicated models like
variational auto encoders,

95
00:06:16,310 --> 00:06:20,250
so it's important to really understand
the details of how these work.

96
00:06:20,320 --> 00:06:23,390
A good supplement to the
course is Andre Karpov.

97
00:06:23,400 --> 00:06:25,460
These lecture notes on CNN,

98
00:06:25,730 --> 00:06:30,730
a part of his cs two 31 and convolutional
neural networks for visual recognition

99
00:06:31,400 --> 00:06:32,233
course.

100
00:06:32,300 --> 00:06:36,890
He has some amazingly detailed technical
documentation covering how they worked.

101
00:06:37,160 --> 00:06:40,820
I also have some detailed videos on CNN,
a linked to them as well.

102
00:06:40,910 --> 00:06:43,430
Moving on to week three
we're current networks.

103
00:06:43,670 --> 00:06:48,670
While feedforward nets are
great for numerical data
and confidence are great for

104
00:06:48,950 --> 00:06:52,520
images were current networks
are great for sequential data.

105
00:06:52,820 --> 00:06:56,840
Any kind of data where
time matters, audio, video,

106
00:06:57,050 --> 00:07:00,500
since videos are a sequence of
image frames, stock price data,

107
00:07:00,740 --> 00:07:04,190
recurrent networks are the
perfect network to use here.

108
00:07:04,790 --> 00:07:09,790
Why you might ask normally neural networks
while training only use the next data

109
00:07:10,941 --> 00:07:12,440
point as an input,

110
00:07:12,620 --> 00:07:17,620
but recurrent networks take both the next
data point and the learned state value

111
00:07:18,410 --> 00:07:23,410
from the previous time step as input to
this recurrence allows it to remember

112
00:07:23,601 --> 00:07:26,210
data sequentially course on.

113
00:07:26,211 --> 00:07:29,990
This is again on Coursera by
professor Ang called sequence models.

114
00:07:30,290 --> 00:07:34,520
This will cover the variations
of recurrent networks
including long short term

115
00:07:34,521 --> 00:07:39,110
memory and gated recurrent unit
neural networks. Additionally,

116
00:07:39,140 --> 00:07:42,890
I have some great videos on this link to
those will be in the provided syllabus

117
00:07:42,920 --> 00:07:44,060
at the end of this week.

118
00:07:44,061 --> 00:07:48,590
Make sure to write out a simple or
current network using Andrew tracks,

119
00:07:48,620 --> 00:07:52,430
Lstm RNN python blog post as a guide.

120
00:07:52,850 --> 00:07:56,150
Now that we have the basic types
of neural networks out of the way,

121
00:07:56,300 --> 00:08:00,620
we can dive into some of the tooling
before it will be all about getting

122
00:08:00,621 --> 00:08:04,010
yourself familiar with some
of the tools in this space.

123
00:08:04,310 --> 00:08:05,750
And because up to this point,

124
00:08:05,930 --> 00:08:09,200
you've only built your
neural networks using num Pi.

125
00:08:09,440 --> 00:08:13,810
You'll appreciate the benefits of using
libraries like tensorflow and care

126
00:08:13,820 --> 00:08:17,030
office rather than typing
out grading updates by hand.

127
00:08:17,210 --> 00:08:20,990
You can benefit from TensorFlow's
automatic differentiation. For example,

128
00:08:21,410 --> 00:08:23,030
of all the deep learning libraries,

129
00:08:23,090 --> 00:08:28,090
I'd found tensorflow to be the best
tool to use since it offers a complete

130
00:08:28,250 --> 00:08:32,360
pipeline for AI development
including building testing,

131
00:08:32,390 --> 00:08:35,270
training and serving models in production.

132
00:08:35,480 --> 00:08:39,740
A great course on this is
cs 20 by Stanford called
tensorflow for deep learning

133
00:08:39,741 --> 00:08:43,130
research. You can find all
of those videos on Youtube.

134
00:08:43,400 --> 00:08:47,600
I also have an awesome
playlist on tensorflow on my
channel that I'll link too.

135
00:08:47,690 --> 00:08:52,490
If you can understand tensorflow, it's
computation graph, the basic operators,

136
00:08:52,550 --> 00:08:57,540
how models are saved and how to serve
them in you will be unstoppable.

137
00:08:58,310 --> 00:09:03,150
US is just a wrapper on top of tensorflow
that makes it even easier to write

138
00:09:03,151 --> 00:09:03,984
models.

139
00:09:03,990 --> 00:09:08,310
Read the documentation page examples to
quickly get an understanding of how it

140
00:09:08,311 --> 00:09:08,850
works.

141
00:09:08,850 --> 00:09:12,810
You also want to read this blog post
that compares some of the Best Gpu cloud

142
00:09:12,811 --> 00:09:17,811
providers so you get a sense of their
pros and cons and can decide on which one

143
00:09:17,911 --> 00:09:21,270
is the best one for you to use.
At the end of this week,

144
00:09:21,300 --> 00:09:25,440
you should write out a simple image
classification demo using tensorflow as

145
00:09:25,441 --> 00:09:26,160
practice.

146
00:09:26,160 --> 00:09:31,160
Once we've got that down in week five we
can peer into one of the newest models

147
00:09:31,290 --> 00:09:35,040
in deep learning,
the generative adversarial network.

148
00:09:35,310 --> 00:09:40,020
This allows us to generate all sorts of
data and it's currently very popular.

149
00:09:40,380 --> 00:09:42,870
We can learn about these
models from youtube.

150
00:09:43,080 --> 00:09:45,480
I've got some great videos on gans.

151
00:09:45,690 --> 00:09:49,470
You can also find some
interesting lectures on gains
that I've compiled in the

152
00:09:49,471 --> 00:09:54,471
syllabus by researchers across the field
and explore their possibilities across

153
00:09:54,541 --> 00:09:58,950
a wide range of applications
and two projects for this week.

154
00:09:59,130 --> 00:10:03,870
Build again from scratch and build again.
Using tensorflow for the last week,

155
00:10:03,871 --> 00:10:07,680
we can focus on the most
bleeding edge of all techniques,

156
00:10:07,830 --> 00:10:09,990
deep reinforcement learning.

157
00:10:10,530 --> 00:10:13,770
This is what's responsible for some of
the latest breakthroughs in the field,

158
00:10:13,771 --> 00:10:17,190
including Alphago, the
Atari DPQ learner, and more.

159
00:10:17,610 --> 00:10:22,610
Berkeley recently released a free course
called cs two 94 deep reinforcement

160
00:10:23,311 --> 00:10:24,030
learning.

161
00:10:24,030 --> 00:10:28,110
You can find all the videos on youtube
or reddit community of students and a

162
00:10:28,111 --> 00:10:30,960
bunch of help or materials
on their website.

163
00:10:31,470 --> 00:10:35,700
This one is nontrivial and it could even
take you two weeks to finish this bit.

164
00:10:35,790 --> 00:10:40,110
Reinforcement learning is different from
supervised and unsupervised learning.

165
00:10:40,320 --> 00:10:45,320
While these either try to learn a
mapping or cluster similar data points

166
00:10:45,961 --> 00:10:46,680
together,

167
00:10:46,680 --> 00:10:51,680
our El tries to learn by trial and error
and some kind of environment where time

168
00:10:51,781 --> 00:10:54,630
is a dementia,
not just a static data set.

169
00:10:55,080 --> 00:11:00,030
It's very likely that the brain learns
using some combination of our l and other

170
00:11:00,031 --> 00:11:03,360
types of learning,
which could be why deep RL,

171
00:11:03,361 --> 00:11:07,110
a combination of learning styles
has gotten such incredible results.

172
00:11:07,470 --> 00:11:11,820
For a final project, create a deep cute
learning algorithm using tensorflow.

173
00:11:12,060 --> 00:11:15,840
You can have it playing Atari Games
using the open AI gym environment.

174
00:11:15,930 --> 00:11:18,990
Deep learning is the dark art of our time,

175
00:11:18,991 --> 00:11:23,610
extremely powerful and mysteriously
good at everything we throw at it,

176
00:11:23,970 --> 00:11:25,680
followed the curriculum I've created.

177
00:11:25,800 --> 00:11:30,000
Find a study buddy to help you stick to
it and post your experiences in the slot

178
00:11:30,001 --> 00:11:32,520
channel. Good luck. Welcome
to the end of the video.

179
00:11:32,550 --> 00:11:36,840
If your model is not converging, hit the
subscribe button and it will for now.

180
00:11:36,870 --> 00:11:40,200
I'll get to learn deep learning again,
so thanks for watching.

