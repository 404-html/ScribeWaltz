1
00:00:00,070 --> 00:00:00,821
Her world.

2
00:00:00,821 --> 00:00:04,900
It's Saroj and we're going to make an a
half that reads an article of texts and

3
00:00:04,901 --> 00:00:07,210
creates a one sentence summary out of it.

4
00:00:07,390 --> 00:00:12,390
Using the power of natural
language processing language
is in many ways the seat

5
00:00:12,550 --> 00:00:13,390
of intelligence.

6
00:00:13,420 --> 00:00:17,950
It's the original communication protocol
that we invented to describe all the

7
00:00:17,951 --> 00:00:21,850
incredibly complex processes
happening in our neocortex.

8
00:00:22,090 --> 00:00:25,090
Do you ever feel like you're getting
flooded with an increasing amount of

9
00:00:25,360 --> 00:00:29,110
articles and links and videos to
choose from as this data grows,

10
00:00:29,140 --> 00:00:32,170
the importance of semantic
entity does as well.

11
00:00:32,410 --> 00:00:35,980
How can you say the most important
things in the shortest amount of time?

12
00:00:36,130 --> 00:00:40,240
Having a generated summary lets you decide
whether you want to deep dive further

13
00:00:40,270 --> 00:00:42,220
or not and the better it gets,

14
00:00:42,221 --> 00:00:46,330
the more we'll be able to apply it to
more complex language like that in a

15
00:00:46,331 --> 00:00:48,850
scientific paper for even an entire book.

16
00:00:49,330 --> 00:00:52,700
The future of NLP is a very bright
one. Interestingly enough, no.

17
00:00:52,790 --> 00:00:57,140
One of the earliest use cases for
machine summarization was by the Canadian

18
00:00:57,141 --> 00:01:01,370
government in the early nineties for a
weather system they invented called fog.

19
00:01:01,910 --> 00:01:04,610
Instead of sifting through
all the meteorological data,

20
00:01:04,611 --> 00:01:07,160
they had access to manually,
they led fog,

21
00:01:07,161 --> 00:01:11,420
read it and generate a weather
forecast from it on a recurring basis.

22
00:01:11,570 --> 00:01:16,040
It had a set textual template and it
would fill in the values for the current

23
00:01:16,041 --> 00:01:20,630
weather. Given the data, something
like this, it was just an experiment,

24
00:01:20,660 --> 00:01:25,160
but they found that sometimes people
actually prefer the computer generated

25
00:01:25,161 --> 00:01:26,720
forecasts to the human ones,

26
00:01:26,990 --> 00:01:30,530
partly because the generated one's
used more consistent terminology.

27
00:01:30,710 --> 00:01:34,760
A similar approach has been applied in
fields with lots of data that needs human

28
00:01:34,761 --> 00:01:37,760
readable summaries like
finance and in medicine.

29
00:01:37,910 --> 00:01:42,230
Summarizing a patient's medical data has
proven to be a great decision support

30
00:01:42,231 --> 00:01:46,790
tool for doctors. Most summarization
tools in the past or extractive,

31
00:01:46,970 --> 00:01:51,110
they selected unexisting subset of words
or numbers from some data to create a

32
00:01:51,111 --> 00:01:55,280
summary. What you and I do, something
a little more complex than that.

33
00:01:55,520 --> 00:01:56,540
When we summarize,

34
00:01:56,570 --> 00:02:01,220
our brain builds an internal semantic
representation of what we've just read.

35
00:02:01,430 --> 00:02:03,530
And from that we can generate a summary.

36
00:02:03,860 --> 00:02:08,030
This is instead an abstract of method
and we can do this with deep learning.

37
00:02:08,060 --> 00:02:09,320
What can't we do with it?

38
00:02:09,560 --> 00:02:14,420
So let's build a tech summarizer that
can generate a headline from a short

39
00:02:14,421 --> 00:02:15,860
article using care Os.

40
00:02:16,130 --> 00:02:19,280
We're going to use this collection of
news articles as our training data.

41
00:02:19,490 --> 00:02:21,080
We'll convert it to pickle format,

42
00:02:21,081 --> 00:02:24,080
which essentially means converting
it into a raw bytes stream.

43
00:02:24,380 --> 00:02:29,060
Pickling is a way of converting a python
object into a character stream so we

44
00:02:29,061 --> 00:02:32,510
can easily reconstruct that
object in another python script.

45
00:02:32,750 --> 00:02:33,920
Modularity for the wind,

46
00:02:34,080 --> 00:02:37,820
we're saving the data as a tool with
the heading description and keywords.

47
00:02:38,120 --> 00:02:41,210
The heading and description or the
list of headings and their respective

48
00:02:41,211 --> 00:02:45,470
articles in order and the keywords are
akin to tags, but we won't be using those.

49
00:02:45,471 --> 00:02:46,250
In this example,

50
00:02:46,250 --> 00:02:51,170
we're going to first tokenize or split
up the text into individual words because

51
00:02:51,410 --> 00:02:53,060
that's the level we're going to deal with.

52
00:02:53,061 --> 00:02:57,020
This data in our headline will
be generated one word at a time.

53
00:02:57,290 --> 00:03:00,490
We want some way of
representing words numerically.

54
00:03:00,860 --> 00:03:05,620
Angio coined a term for this called word
embeddings back in 2003 but they were

55
00:03:05,621 --> 00:03:09,820
first made popular by a team
of researchers at Google
when they were least word

56
00:03:09,821 --> 00:03:13,180
to Vec inspired by boys.
To men. Just kidding.

57
00:03:13,500 --> 00:03:18,130
Word to Vec is a two layer neural net
trained on a big label text corpus.

58
00:03:18,131 --> 00:03:20,560
It's a pre trained model you can download.

59
00:03:20,860 --> 00:03:25,180
It takes a word as its input and
produces a vector as it's output.

60
00:03:25,450 --> 00:03:30,160
One vector per word. Creating word vectors
lets us analyze words mathematically.

61
00:03:30,161 --> 00:03:35,161
So these high dimensional
vectors represent words and
each dimension in codes a

62
00:03:35,351 --> 00:03:37,750
different property like gender or title.

63
00:03:37,840 --> 00:03:42,370
The magnitude along each access represents
the relevance of that property to a

64
00:03:42,371 --> 00:03:46,690
word. So we could say king plus
men minus woman equals queen.

65
00:03:46,960 --> 00:03:50,380
We can also find the similarity between
words, which equates to distance.

66
00:03:50,560 --> 00:03:53,710
Word to VEC offers a predictive
approach to creating word vectors,

67
00:03:53,711 --> 00:03:58,030
but another approach is count based
and the popular algorithm for that.

68
00:03:58,060 --> 00:04:00,280
His glove short for global vectors.

69
00:04:00,610 --> 00:04:05,230
If first constructs a large co-occurrence
matrix of words by context for each

70
00:04:05,231 --> 00:04:09,730
word I he row, it'll count how
frequently it sees it in some contexts,

71
00:04:09,731 --> 00:04:13,120
which is the column.
Since the number of contexts can be large,

72
00:04:13,240 --> 00:04:16,330
it factorized is the matrix to
get a lower dimensional matrix,

73
00:04:16,540 --> 00:04:18,460
which represents words by features.

74
00:04:18,610 --> 00:04:22,900
So each row has a feature representation
for each word and they also traded on a

75
00:04:22,901 --> 00:04:24,190
large text corpus.

76
00:04:24,490 --> 00:04:28,030
Both performed similarly well
by glove trains a little faster.

77
00:04:28,031 --> 00:04:28,900
So we'll go with that.

78
00:04:29,140 --> 00:04:33,010
We'll download the pretrained glove word
vectors from this link and save them to

79
00:04:33,011 --> 00:04:37,340
disk. Then we'll use them to initialize
and embedding matrix with our tokenize

80
00:04:37,341 --> 00:04:39,370
vocabulary from our training data.

81
00:04:39,490 --> 00:04:43,480
We'll initialize it with random numbers
and copy all the glove weights of words

82
00:04:43,590 --> 00:04:47,710
show up in our training vocabulary and
for every word outside this embedding

83
00:04:47,711 --> 00:04:52,310
matrix will find the closest word inside
the Matrix by measuring the coastline

84
00:04:52,390 --> 00:04:54,010
distance of glove vectors.

85
00:04:54,100 --> 00:04:57,370
Now we've got this matrix of word
embeddings that we could do so many things

86
00:04:57,371 --> 00:04:57,670
with.

87
00:04:57,670 --> 00:05:01,150
So how are we going to use these word
embeddings to create a summary headline

88
00:05:01,360 --> 00:05:05,160
for a novel article? We feed it.
Let's back up for a second, Ben.

89
00:05:05,230 --> 00:05:09,580
A squad first introduce a
neural architecture called
sequence to sequence in

90
00:05:09,581 --> 00:05:14,581
2014 at later inspired the Google brain
team to use it for text summarization

91
00:05:14,711 --> 00:05:15,544
successfully.

92
00:05:15,640 --> 00:05:19,960
It's called sequence to sequence because
we are taking an input sequence and al

93
00:05:19,961 --> 00:05:22,990
putting not a single value,
but a sequence as well.

94
00:05:23,500 --> 00:05:27,470
We're going to end code. Then
we be code. We join it in code.

95
00:05:27,710 --> 00:05:30,290
Then we z code went up.
He did a book.

96
00:05:30,291 --> 00:05:34,520
It gets back to rise and
when I decode that as Mariah,

97
00:05:34,870 --> 00:05:37,990
so we use two recurrent networks,
one for each sequence.

98
00:05:38,320 --> 00:05:40,150
The first is the encoder network.

99
00:05:40,180 --> 00:05:44,170
It takes an input sequence and creates
an encoded representation of it.

100
00:05:44,560 --> 00:05:47,920
The second is the decoder network.
We feed it as its input,

101
00:05:48,130 --> 00:05:52,120
that same encoded representation and
it will generate an output sequence by

102
00:05:52,121 --> 00:05:55,690
decoding it. There are different ways
we could approach this architecture.

103
00:05:55,840 --> 00:05:59,810
One approach would be to
let encoder network learn
needs in bedding from scratch

104
00:06:00,050 --> 00:06:01,400
by feeding in our training data,

105
00:06:01,730 --> 00:06:06,500
but we're taking a less computationally
expensive approach because we already

106
00:06:06,501 --> 00:06:11,270
have learned embeddings from glove.
When we build our encoder LSTM network,

107
00:06:11,271 --> 00:06:14,600
we'll set those pretrained embeddings
as our first layers weights.

108
00:06:14,930 --> 00:06:18,980
The embedding layer is meant to turn
input integers into fixed size vectors.

109
00:06:18,981 --> 00:06:21,430
Anyway,
we've just given it a huge headstart.

110
00:06:21,440 --> 00:06:23,720
By doing this and when
we train this model,

111
00:06:23,750 --> 00:06:28,370
it will just find tune or improve the
accuracy of our embeddings as a supervised

112
00:06:28,371 --> 00:06:32,930
classification problem where the input
data is our set of vocab words and the

113
00:06:32,931 --> 00:06:35,570
labels are there
associated headline words.

114
00:06:35,840 --> 00:06:40,760
We'll minimize the cross entropy loss
using rms prop. Now for our decoder,

115
00:06:40,820 --> 00:06:42,770
our decoder will generate headlines.

116
00:06:42,950 --> 00:06:47,240
It will have the same LSTM architecture
as our encoder and we'll initialize its

117
00:06:47,241 --> 00:06:50,360
weights using our same
pretrained glove embeddings.

118
00:06:50,600 --> 00:06:54,770
It will take as input
the vector representation
generated after feeding in the

119
00:06:54,771 --> 00:06:56,480
last word of the input text,

120
00:06:56,630 --> 00:07:01,630
so it will first generate
its own representation using
it's embedding layer and

121
00:07:01,671 --> 00:07:05,240
the next step is to convert
this representation into a word,

122
00:07:05,241 --> 00:07:07,460
but there is actually one more step.

123
00:07:07,790 --> 00:07:12,110
We need a way to decide what part of
the input we need to remember like names

124
00:07:12,111 --> 00:07:14,870
and numbers.
We talked about the importance of memory.

125
00:07:14,871 --> 00:07:16,970
That's why we use LSTM cells,

126
00:07:17,120 --> 00:07:20,780
but another important aspect of
learning theory is attention.

127
00:07:20,960 --> 00:07:24,110
Basically what is the most
relevant data to memorize?

128
00:07:24,440 --> 00:07:27,020
Our decoder will generate
a word as it's output.

129
00:07:27,200 --> 00:07:31,280
And that same word will be fed in as
input when generating the next word,

130
00:07:31,310 --> 00:07:34,820
until we have a headline,
we use an attention mechanism.

131
00:07:34,820 --> 00:07:38,480
When out putting each word in
the decoder for each output word,

132
00:07:38,510 --> 00:07:42,620
it computes a weight over each of the
input words that determines how much

133
00:07:42,621 --> 00:07:45,350
attention should be
paid to that input word.

134
00:07:45,770 --> 00:07:50,150
All the weights some up to one and are
used to compute a weighted average of the

135
00:07:50,151 --> 00:07:54,530
last hidden layers generated. After
processing each of the input at words,

136
00:07:54,800 --> 00:07:59,150
we'll take that weighted average and
input it into the softmax layer along with

137
00:07:59,270 --> 00:08:02,450
the last hidden layer from the
current step of the decoder.

138
00:08:02,540 --> 00:08:06,410
So let's see what our model generates for
this article after training. All right,

139
00:08:06,411 --> 00:08:08,330
we've got this headline
generated beautifully,

140
00:08:08,450 --> 00:08:11,090
and let's do it once more
for a different article.

141
00:08:11,480 --> 00:08:14,330
Couldn't have said it better myself.
So to break it down,

142
00:08:14,450 --> 00:08:19,220
we can use retrained word vectors using
a model like love easily to avoid having

143
00:08:19,221 --> 00:08:23,570
to create them ourselves to generate an
output sequence of words given an input

144
00:08:23,571 --> 00:08:27,320
sequence of words. We use a neural
encoder decoder architecture,

145
00:08:27,321 --> 00:08:30,920
and by adding an attention
mechanism to our decoder,

146
00:08:31,040 --> 00:08:34,760
it can help it decide what is the
most relevant token to focus on when

147
00:08:34,761 --> 00:08:36,110
generating new texts.

148
00:08:36,170 --> 00:08:39,200
The winner of the coding challenge
from the last video is just soon.

149
00:08:39,201 --> 00:08:42,710
See he wrote an AI composer
in 100 lines of code.

150
00:08:42,980 --> 00:08:47,000
Last week's challenge was non trivial
and he managed to get a working demo up.

151
00:08:47,001 --> 00:08:50,150
So definitely check out his
repo wizard of the week.

152
00:08:50,240 --> 00:08:53,870
The coding challenge for this video is
to use a sequence to sequence model with

153
00:08:53,871 --> 00:08:57,830
care os to summarize a piece of text
posts or get hub link in the comments and

154
00:08:57,831 --> 00:08:59,480
I'll announce the winner.
Next video,

155
00:08:59,630 --> 00:09:01,490
please subscribe for
more programming videos.

156
00:09:01,491 --> 00:09:05,300
And for now I've got to remember to
pay attention. So thanks for watching.

