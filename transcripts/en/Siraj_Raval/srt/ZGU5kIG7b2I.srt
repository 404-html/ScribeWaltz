1
00:00:01,110 --> 00:00:04,530
Hello world. It's Raj.
Welcome to this live stream.

2
00:00:04,560 --> 00:00:09,090
I'm very excited to be here today. We're
going to generate some fake news. No,

3
00:00:09,091 --> 00:00:10,980
I'm just kidding.
It's not fake news.

4
00:00:11,250 --> 00:00:15,420
It is generated Wikipedia articles.
Okay?

5
00:00:15,780 --> 00:00:18,210
That's what we're going to do today.
And uh,

6
00:00:18,450 --> 00:00:23,450
I'm very excited for this because we
are finally going to code and Lstm from

7
00:00:24,271 --> 00:00:28,230
scratch.
We are going to code an Lstm from scratch.

8
00:00:28,260 --> 00:00:31,500
That means that we are doing it
from scratch, not num py, scratch,

9
00:00:31,501 --> 00:00:33,520
but tensorflow scratch.
No care.

10
00:00:33,521 --> 00:00:37,170
Os we're going to code out
the internals of an LSTM cell.

11
00:00:37,320 --> 00:00:41,730
And here's what it's going to look like.
It's going to look like this. Okay?

12
00:00:43,770 --> 00:00:48,750
It's going to look like this, just
like this. And we can see that, right?

13
00:00:48,751 --> 00:00:50,430
Let me make the text a little bigger.

14
00:00:53,210 --> 00:00:57,410
We can see that. Okay? So
we'll give it some input tax.

15
00:00:57,411 --> 00:01:02,290
I plan to make the world a better
place by, and then it's going to, uh,

16
00:01:02,360 --> 00:01:03,560
generate the rest of the text.

17
00:01:03,561 --> 00:01:07,580
Now this is actually really bad because
it only took one iteration of training,

18
00:01:07,581 --> 00:01:11,240
but it's gonna get better over time.
Okay? So that's what we're going to do.

19
00:01:12,290 --> 00:01:17,270
And we are, we are, we are psyched
to do this. Thank you guys.

20
00:01:17,330 --> 00:01:20,720
Hi everybody. Let's start with
our five minute Q and. A. Okay.

21
00:01:20,721 --> 00:01:23,510
So ask me deep learning machine learning,

22
00:01:23,720 --> 00:01:26,750
any kind of questions you want Ama,
go for it.

23
00:01:28,120 --> 00:01:28,720
Yeah.

24
00:01:28,720 --> 00:01:31,210
Oh, and we are, I am live at
the end. Please subscribe.

25
00:01:31,211 --> 00:01:34,690
And if you want to see more live streams
there. Every Wednesday at 10:00 AM pst,

26
00:01:34,810 --> 00:01:38,460
also at the upload Vr studios with
the wizard of Oz, Oz. Balaban. Ian,

27
00:01:38,470 --> 00:01:40,210
who was going to be up there.
There he is.

28
00:01:40,211 --> 00:01:41,860
He's going to be the
voice of God later on.

29
00:01:41,861 --> 00:01:45,280
He's going to help me answer questions.
Okay. So ask me some questions.

30
00:01:45,281 --> 00:01:46,480
We're going to get started.

31
00:01:46,990 --> 00:01:50,150
We're going to do some virtual reality
today to virtual reality, to, to demos,

32
00:01:50,151 --> 00:01:51,460
to math as well.
Hi,

33
00:01:51,461 --> 00:01:56,170
Palosh Mohan Leo Party Tuber.
Okay.

34
00:01:56,710 --> 00:01:59,170
Questions?
Can you explain more on the text?

35
00:01:59,230 --> 00:02:04,210
Summarizer the tech summarizer was,
um,

36
00:02:04,960 --> 00:02:07,090
an LSTM network that you
use memory and attention.

37
00:02:07,091 --> 00:02:09,580
And this next weekly video that's
coming out in three days will definitely

38
00:02:09,581 --> 00:02:13,630
explain that more. Okay.
Uh, come to Brazil someday.

39
00:02:13,960 --> 00:02:16,660
Someday I want to go to Brazil.
What is best machine learning?

40
00:02:17,020 --> 00:02:22,000
I love that question. This channel,
could you train an Lstm with a chat bot?

41
00:02:22,001 --> 00:02:25,240
Yes. Could you train a chat Bot with
an Lstm is what you meant to say?

42
00:02:25,510 --> 00:02:27,490
The answer is yes,
you can.

43
00:02:29,020 --> 00:02:33,850
What's your opinion about semantic
web? Semantic web? The cement.

44
00:02:33,910 --> 00:02:38,910
The semantic web is a word that describes
the web of words and the web of text

45
00:02:39,520 --> 00:02:42,640
and it's all really connected in there.
It's hyperlinked together.

46
00:02:42,730 --> 00:02:44,290
We still have to improve right now.

47
00:02:44,291 --> 00:02:46,990
Links go one way but we
want links to go both ways,

48
00:02:46,991 --> 00:02:48,640
which was the original vision for the web.

49
00:02:48,970 --> 00:02:51,700
Zana do was a project
that did that and IPFS.

50
00:02:51,701 --> 00:02:54,850
The interplanetary file system
is aiming to go back to that,

51
00:02:54,851 --> 00:02:58,780
so check out that protocol. Will
you use syntax net? Not In this.

52
00:02:58,870 --> 00:03:00,610
We not using transfer learning.

53
00:03:00,611 --> 00:03:03,610
We are building it from
scratch as a learning exercise.

54
00:03:04,210 --> 00:03:07,690
Why do you do not use chaos?
Because we want to learn all of it.

55
00:03:07,960 --> 00:03:11,440
Thank you for joining the class and
you'd Udacity. Thanks King Burrito.

56
00:03:11,680 --> 00:03:16,680
Best framework for sound processing
in python for sound processing fun.

57
00:03:18,010 --> 00:03:21,160
There are quite a lot,
um,

58
00:03:22,270 --> 00:03:23,500
for sound processing

59
00:03:28,730 --> 00:03:33,520
Dirac the IRA c is a good one, uh,
to, it's by a German, a programmer.

60
00:03:33,700 --> 00:03:38,290
What harp hardware do you recommend? Have
you tried FPG A's for deep learning? Uh,

61
00:03:38,410 --> 00:03:42,640
Nvidia is a good g as a good brand
for Gpu, specifically the Titan X.

62
00:03:42,880 --> 00:03:45,940
Although if I were to say, you
know, between cloud and local,

63
00:03:45,941 --> 00:03:48,520
I would choose the cloud specifically AWS.

64
00:03:49,810 --> 00:03:53,710
What is your background in mathematics
or Raj? I took some math in high school.

65
00:03:53,950 --> 00:03:58,540
I took some math in college.
Uh, I takes, took some math
online. You're really, it's,

66
00:03:58,541 --> 00:04:02,020
it's, we have to think about this
as a lifelong learning process.

67
00:04:02,021 --> 00:04:05,350
I'm still learning math. It's not like I
just stopped learning math and now I'm,

68
00:04:05,490 --> 00:04:09,100
I'm good to go.
We need to switch the view from a PR,

69
00:04:09,340 --> 00:04:13,120
from learning being a period in our
lives to a lifelong learning process.

70
00:04:13,121 --> 00:04:16,960
So I've studied linear Algebra and
Calculus and Statistics and I'm still

71
00:04:16,961 --> 00:04:17,860
studying these things.

72
00:04:18,370 --> 00:04:21,280
Two more questions and then the five
minutes is up and we're going to code this

73
00:04:21,281 --> 00:04:24,280
thing. Okay. The second
question, the first question is,

74
00:04:27,790 --> 00:04:31,720
well, you make an AI robot in
future, like personal assistant,

75
00:04:31,960 --> 00:04:35,140
personal assistant, uh, w I, yes,

76
00:04:35,141 --> 00:04:37,630
I have several chat bot
videos in terms of hardware.

77
00:04:37,750 --> 00:04:40,390
So hardware is a little more
effort than just software.

78
00:04:40,510 --> 00:04:43,310
I definitely want to do more
hardware and I will, uh,

79
00:04:43,390 --> 00:04:45,520
let me just finish this course
first and then you know,

80
00:04:45,521 --> 00:04:47,980
a lot of crazy cool things will,
will start coming out.

81
00:04:48,100 --> 00:04:52,200
One more question and then we're going
to get started with the code. This,

82
00:04:52,450 --> 00:04:53,710
the next question is

83
00:04:57,530 --> 00:05:01,890
how can I get out of Venezuela with AI?
Okay.

84
00:05:04,520 --> 00:05:08,420
Wow. I don't know. Okay. No,

85
00:05:08,810 --> 00:05:09,643
here's how you do it.

86
00:05:09,890 --> 00:05:13,400
You would just study this stuff well
enough that one of these big companies

87
00:05:13,401 --> 00:05:16,040
would want to hire you and host you.
So just study it well enough.

88
00:05:16,130 --> 00:05:19,250
You can do it in Venezuela, you can do
it in Africa, you can do it anywhere.

89
00:05:19,400 --> 00:05:23,450
You can become a world class machine
learning expert anywhere in the world

90
00:05:23,600 --> 00:05:25,820
because of the Internet.
So get good at this stuff.

91
00:05:25,850 --> 00:05:29,720
People will want you increase
your personal value and
people will want you and

92
00:05:29,721 --> 00:05:32,930
you can go anywhere you want. The
barriers to entry to travel will decrease.

93
00:05:33,110 --> 00:05:37,340
That's it for the questions. Now we're
going to get started with this code. Okay,

94
00:05:37,370 --> 00:05:38,510
let's get started with the code.

95
00:05:39,350 --> 00:05:43,340
So what we're gonna do is we're going
to generate text and we're going to

96
00:05:43,341 --> 00:05:46,970
generate Wikipedia articles. Okay?
And by the way, with the questions,

97
00:05:46,971 --> 00:05:48,830
I'm not going to let
it interrupt my coding,

98
00:05:48,950 --> 00:05:52,250
so I'm going to code for something
like 15 minutes and then go back to the

99
00:05:52,251 --> 00:05:54,860
questions and then we'll just
repeat that process. Okay.

100
00:05:55,070 --> 00:05:58,340
I was reading feedback from last
time and I want to better, so let's,

101
00:05:58,370 --> 00:05:59,420
we're going to code some,

102
00:05:59,480 --> 00:06:03,110
we're going to generate some text
from in the style of Wikipedia. Okay?

103
00:06:03,111 --> 00:06:03,944
That's what we're going to do.

104
00:06:04,010 --> 00:06:07,430
So let's look at the data
set that we're going to use.

105
00:06:07,431 --> 00:06:08,900
This is the data set and by the way,

106
00:06:08,901 --> 00:06:13,550
check the check the video description
for a link to the get hub repository.

107
00:06:13,850 --> 00:06:16,210
And I'm going to use VR by the
way, to show some math. So it's,

108
00:06:16,211 --> 00:06:18,800
so stay tuned for that. Okay.
So that's what we're gonna do.

109
00:06:20,480 --> 00:06:21,313
And uh,

110
00:06:25,320 --> 00:06:29,280
okay, so let's get started with this. This
is the Dataset that we're going to use.

111
00:06:29,370 --> 00:06:33,210
It is from salesforce, Matt and mine. So
metamind got bought out by salesforce.

112
00:06:33,420 --> 00:06:38,220
Great purchase. Metamind had some
really great technology. And by the way,

113
00:06:38,221 --> 00:06:40,350
if you start an AI startup,
you will

114
00:06:41,310 --> 00:06:44,190
very likely to get bought out
if you bring any sort of value.

115
00:06:44,310 --> 00:06:47,760
These big companies are snatching
up AI startups like they are candy,

116
00:06:48,090 --> 00:06:49,500
like they are candies.

117
00:06:49,560 --> 00:06:53,670
So it is a good time to be an AI
right now is a very lucrative time.

118
00:06:53,880 --> 00:06:58,880
This dataset is a collection of 100
million tokens preprocessed for us from

119
00:06:59,281 --> 00:07:03,630
Wikipedia articles. So they took a bunch
of Wikipedia articles, tokenize them,

120
00:07:03,750 --> 00:07:06,610
and then they, uh, this is the Dataset.

121
00:07:06,660 --> 00:07:10,050
So this is the data set we're going
to use to train our model on, okay,

122
00:07:10,260 --> 00:07:12,150
that's our dataset.
Let's get started.

123
00:07:12,330 --> 00:07:16,260
The first thing we're
going to do is import our
dependencies, dependencies, time.

124
00:07:16,560 --> 00:07:18,320
What are the dependencies that we need?
Well,

125
00:07:18,360 --> 00:07:21,570
of course we need num
Pi for vectorization.

126
00:07:21,571 --> 00:07:25,740
We want to vectorize our data
before we feed it into our model.

127
00:07:26,040 --> 00:07:30,360
The next step is to import,
uh, the art shows to map.

128
00:07:30,510 --> 00:07:34,110
The next step is to import random. Okay,
and why are we going to import random?

129
00:07:34,260 --> 00:07:38,580
We want to generate a po probability
distribution, right? For generating texts.

130
00:07:38,581 --> 00:07:40,770
This is going to help us generate text.
Uh,

131
00:07:40,800 --> 00:07:42,570
the next is tensorflow
from machine learning.

132
00:07:42,780 --> 00:07:47,310
And then finally we have one more
and that is a date time, date,

133
00:07:47,970 --> 00:07:51,270
time. Okay. So this is going to show,
this is for clocking the training time.

134
00:07:51,420 --> 00:07:54,930
We want to see, you know, where we
are in the training process. Okay?

135
00:07:54,931 --> 00:07:58,980
So that's it for our dependencies.
And now we're going to open our text.

136
00:07:59,100 --> 00:08:01,350
So let's check out this text.
It's going to be,

137
00:08:01,351 --> 00:08:04,380
so I've already downloaded it and
here's, here it is, by the way, uh,

138
00:08:04,700 --> 00:08:08,860
this is the one that you should
download. Where is it? It is right here.

139
00:08:09,430 --> 00:08:10,263
This one?

140
00:08:10,620 --> 00:08:10,860
Yeah,

141
00:08:10,860 --> 00:08:15,150
right here. The 4.3 megs. Okay.
Oh, actually no. The one 81 mags.

142
00:08:15,151 --> 00:08:19,930
Do you want more data? So download the
one 81 megs. It's an a. Dot. Raw increase.

143
00:08:19,950 --> 00:08:21,990
The font size, I can
do that. It's in a dot.

144
00:08:25,400 --> 00:08:26,233
Raw.

145
00:08:26,290 --> 00:08:31,090
How's that? That's amazing. I am very
excited for this. Okay. It's an a. Dot.

146
00:08:31,091 --> 00:08:33,280
Raw format.
So that's gonna be Wiki Dot Texstar raw.

147
00:08:33,290 --> 00:08:37,540
So I've already downloaded it and
it's right in my folder right here.

148
00:08:37,690 --> 00:08:42,170
So I'll say Wiki Tex.
Dot. Raw Dot Reed. Okay,

149
00:08:42,200 --> 00:08:43,490
so that's our dataset.

150
00:08:43,520 --> 00:08:47,840
And we're going to print out the length
of the text that we have so far in the

151
00:08:47,841 --> 00:08:50,780
number of characters.
And it's going to be,

152
00:08:52,520 --> 00:08:54,710
it's going to show show
how long this text is.

153
00:08:54,711 --> 00:08:56,550
And then we want to print
the head of the tax.

154
00:08:56,551 --> 00:08:59,400
We're just going to show
the first collection of
characters, just so you know,

155
00:08:59,401 --> 00:09:00,234
for ourselves.

156
00:09:00,320 --> 00:09:03,960
And is the great thing about having a
Jupiter notebook is that we can just say

157
00:09:04,320 --> 00:09:07,560
how much do we want? And I'm going to
say I want the first thousand tokens.

158
00:09:07,561 --> 00:09:12,120
So let's see if this prince
out and it will for sure,

159
00:09:12,660 --> 00:09:16,200
definitely pronounced. Okay. So we
have how many characters we have.

160
00:09:16,201 --> 00:09:21,030
We have 128 no, we have 1
million, 1.2 million characters.

161
00:09:21,031 --> 00:09:22,380
We have a lot of characters.

162
00:09:23,040 --> 00:09:27,120
And we're going to print this out.
Okay.

163
00:09:27,121 --> 00:09:28,170
Character by character.

164
00:09:28,171 --> 00:09:31,440
And in fact we are going to
feed characters into our model.

165
00:09:31,830 --> 00:09:33,620
We're going to feed
characters into a model.

166
00:09:33,621 --> 00:09:37,380
So it's going to be character by
character training and then character by

167
00:09:37,381 --> 00:09:41,430
character output. And we're going
to talk about ways to improve that.

168
00:09:44,620 --> 00:09:47,620
Okay? So that's, that's
that's it for our tax,

169
00:09:47,750 --> 00:09:49,270
that's what the text we're going to use.

170
00:09:49,271 --> 00:09:52,580
And these are Wikipedia articles
that are already tokenized.

171
00:09:52,600 --> 00:09:56,320
That means that each word is it's
own token. It's its own string.

172
00:09:56,950 --> 00:10:00,370
And this is good for this is
this our preprocessing steps.

173
00:10:00,371 --> 00:10:03,160
So now we're going to
print out the characters.

174
00:10:03,400 --> 00:10:08,170
So now we want to print out our characters
and sort them and sort them as well.

175
00:10:08,440 --> 00:10:12,370
That should be reversed. So our
characters are going to be sorted. Okay.

176
00:10:12,371 --> 00:10:13,210
We want to sort them.

177
00:10:13,211 --> 00:10:17,920
Python has some great native functions
for sorting characters. And, uh,

178
00:10:17,921 --> 00:10:19,720
so this is actually a nested,

179
00:10:22,710 --> 00:10:26,590
it was, it wasn't my AI, it was, it
was the wizard of Oz. He added that.

180
00:10:26,591 --> 00:10:28,480
So that was good. I was kind
of surprised. I was like, whoa.

181
00:10:29,230 --> 00:10:32,020
But eventually it will be an AI.
So now we have charts size,

182
00:10:32,170 --> 00:10:35,140
which is going to be the
length of the characters.

183
00:10:35,320 --> 00:10:36,820
And then we want to print
out the number of care.

184
00:10:36,850 --> 00:10:39,910
So this is an exercise to show the
characters that we're going to be dealing

185
00:10:39,911 --> 00:10:40,150
with.

186
00:10:40,150 --> 00:10:42,790
This is very important because we want
to show the characters that we're going

187
00:10:42,791 --> 00:10:45,010
to be dealing with here.
Okay?

188
00:10:45,130 --> 00:10:49,600
So we have our charge size and we
want to print out all of this chars.

189
00:10:49,660 --> 00:10:52,360
And then lastly, we want to say we
don't even need to print. Hello. Okay.

190
00:10:52,420 --> 00:10:54,070
So here's what's going to happen.

191
00:10:55,190 --> 00:10:56,023
Hold on.

192
00:10:56,080 --> 00:10:58,110
So we have sorted let would it,

193
00:11:02,270 --> 00:11:03,230
oh,
there we go.

194
00:11:03,740 --> 00:11:06,560
Okay. We have 259 characters
total. So what did we do?

195
00:11:06,770 --> 00:11:10,700
You just do a set is an unordered
collection with no duplicate elements.

196
00:11:10,701 --> 00:11:13,870
So we first remove duplicates,
then we uh,

197
00:11:14,000 --> 00:11:18,020
and then we turn it back into a list.
Okay. And then we sorted the list.

198
00:11:18,230 --> 00:11:20,120
And so it's sorted Alpha numerically.

199
00:11:22,060 --> 00:11:22,893
Uh,

200
00:11:23,460 --> 00:11:26,430
please post the link. Yes, the wizard
of article posts a link on my behalf.

201
00:11:27,050 --> 00:11:30,360
And so these are our characters sorted
alphanumerically all of these are the

202
00:11:30,540 --> 00:11:32,340
character of the are the characters are,

203
00:11:32,350 --> 00:11:35,010
we're going to use 259 unique characters,

204
00:11:35,750 --> 00:11:36,583
sorry.

205
00:11:38,160 --> 00:11:42,840
And the link to that,
I'm going to post that in the chat room.

206
00:11:43,110 --> 00:11:47,780
Here's the link to the Dataset.
So that's it for our characters.

207
00:11:47,781 --> 00:11:48,614
And now

208
00:11:53,680 --> 00:11:55,480
there we go.
So that's it for our characters.

209
00:11:55,481 --> 00:11:59,110
So we saw a little bit of
data preprocessing and then
we're going to get to the

210
00:11:59,111 --> 00:11:59,501
machine learning.

211
00:11:59,501 --> 00:12:04,501
So now we want to have a method
to convert characters to ids.

212
00:12:05,981 --> 00:12:07,900
Why?
Because it's,

213
00:12:09,460 --> 00:12:12,820
it's good to have this mapping because
we're going to need it to create labels.

214
00:12:12,821 --> 00:12:16,070
And I'll talk about specifically
what we're considering labels, right?

215
00:12:16,071 --> 00:12:19,450
Th the labels here aren't so clear cut.
They're not like ones or Zeros.

216
00:12:19,451 --> 00:12:22,210
We just have a collection of
characters. What are the labels? Well,

217
00:12:22,420 --> 00:12:26,320
we'll hold on tight. We're going to figure
out exactly what that is in a second.

218
00:12:26,321 --> 00:12:31,321
So we'll start off with
creating a dictionary to
convert our characters to ids.

219
00:12:32,200 --> 00:12:35,920
And we're going to enumerate through all
of those characters to create ids for

220
00:12:35,921 --> 00:12:38,440
each of them. So these are
dictionary objects. Okay?

221
00:12:38,590 --> 00:12:40,930
These are dictionary objects and
I can just paste this in again,

222
00:12:40,931 --> 00:12:42,760
but this time it's going
to be id to character.

223
00:12:43,090 --> 00:12:47,920
So these two functions are going to
convert our characters to ids and vice

224
00:12:47,950 --> 00:12:52,850
versa. Okay? And that's
what that's going to do.

225
00:12:53,900 --> 00:12:58,460
Make sure I have all of that.
Their character or characters.
Yep, Yep, Yep, Yep, Yep.

226
00:12:58,940 --> 00:13:00,920
There's something missing
here. There we go. Okay.

227
00:13:01,250 --> 00:13:04,280
So that's our character to ids
isn't gonna convert character.

228
00:13:04,281 --> 00:13:09,230
So [inaudible] and bias and
numer in new arrayed, right?

229
00:13:09,231 --> 00:13:12,470
Spelling is such a bee spelling as a B.

230
00:13:15,040 --> 00:13:15,873
Okay?

231
00:13:16,510 --> 00:13:17,200
So that's what we're going to do.

232
00:13:17,200 --> 00:13:19,840
So now we're going to
create this helper method.

233
00:13:19,841 --> 00:13:23,810
So this helper method is going to,
uh,

234
00:13:23,870 --> 00:13:28,870
generates the probability of each
character of each next character.

235
00:13:29,860 --> 00:13:33,400
Okay? So this is going to help us generate
the probability of each next care.

236
00:13:33,401 --> 00:13:36,310
So character by character,
we're going to predict a character,

237
00:13:36,400 --> 00:13:38,110
and then we're gonna
generate the next character.

238
00:13:38,111 --> 00:13:40,000
So we're going to our
model output a character,

239
00:13:40,150 --> 00:13:41,560
and we want to generate
the next character.

240
00:13:41,561 --> 00:13:44,470
So this is going to be a helper method
and what's claimed more as we go.

241
00:13:44,471 --> 00:13:48,790
So we're going to say sample. So given
a prediction. So the prediction is,

242
00:13:49,210 --> 00:13:49,750
uh,

243
00:13:49,750 --> 00:13:54,750
it is a list of the prediction is going
to be a list of possible characters.

244
00:13:56,260 --> 00:14:00,510
And this method is going to return the,
the,

245
00:14:01,470 --> 00:14:03,070
the most likely character.

246
00:14:03,100 --> 00:14:06,220
So it's actually a list of characters
that were in that we're going to input.

247
00:14:06,340 --> 00:14:09,430
And then we're going to use this method
to predict what is the most likely of

248
00:14:09,490 --> 00:14:11,260
those characters.

249
00:14:11,261 --> 00:14:16,261
So it's a multi-class classification
problem where the classes themselves are

250
00:14:16,541 --> 00:14:20,770
characters. This is gonna make more sense
once we start building the model. Okay?

251
00:14:20,950 --> 00:14:24,730
So we have samples. Okay. So the first
thing we're going to do is want to say

252
00:14:26,530 --> 00:14:30,340
random dot uniform
zero through one.

253
00:14:30,341 --> 00:14:33,590
And so this is going to generate a,
a uniform distribution okay.

254
00:14:33,700 --> 00:14:37,930
Through the interview interval of
zero to one. Why to, okay. So yeah,

255
00:14:38,200 --> 00:14:40,480
I was going to say we
have Socratic Mishra,

256
00:14:40,481 --> 00:14:45,481
a Jew all Daya and get the nerves
run and asking why two dictionaries,

257
00:14:45,971 --> 00:14:47,580
why do you need to use two of them?
Okay,

258
00:14:47,590 --> 00:14:50,380
so we're using two dictionaries
because we want to convert characters,

259
00:14:50,381 --> 00:14:52,040
two ids and to characters.

260
00:14:52,400 --> 00:14:56,210
And the reason for this is
we want a way to encode, uh,

261
00:14:56,360 --> 00:14:58,250
these characters because we want to,
first,

262
00:14:58,251 --> 00:15:01,990
we want to convert the
characters to ids so that, uh,

263
00:15:02,120 --> 00:15:05,530
we can convert them back
later. And this is for, uh,

264
00:15:06,350 --> 00:15:09,110
it's for indexing. We need a way
to index these characters, right?

265
00:15:09,111 --> 00:15:11,150
More than just a, B, c, d, e.

266
00:15:11,270 --> 00:15:15,170
We need an a numeric way of indexing
these characters so we can refer to them.

267
00:15:15,320 --> 00:15:18,950
We want to store and retrieve
them optimally. So this is a way,

268
00:15:19,130 --> 00:15:23,060
this is a way for us to index them similar
to a, you know, it's a key value pair,

269
00:15:23,061 --> 00:15:26,050
right?
So the,

270
00:15:26,560 --> 00:15:31,460
the character id is going to be
the length of the prediction.

271
00:15:31,700 --> 00:15:33,140
Minus one,
because,

272
00:15:33,530 --> 00:15:36,650
uh, because, uh,

273
00:15:37,130 --> 00:15:39,560
since the length is greater than
the indices starting at zero.

274
00:15:39,561 --> 00:15:43,970
So that's what we add a minus one
at the end. So we want to generate,

275
00:15:44,030 --> 00:15:47,990
so we have a list of a
prediction predicted characters.

276
00:15:48,560 --> 00:15:50,360
We have a list of predicted characters,

277
00:15:52,310 --> 00:15:53,410
uh,
and uh,

278
00:15:54,330 --> 00:15:59,160
we want to say, so for each
character prediction probability.

279
00:16:00,210 --> 00:16:04,110
So for each of them. So we're going
to iterate through each of these

280
00:16:05,700 --> 00:16:06,870
predictions.
Okay.

281
00:16:06,871 --> 00:16:09,570
So we're going to iterate through
the length of the prediction list.

282
00:16:09,571 --> 00:16:12,390
This is a list of possible characters,
probabilities.

283
00:16:12,540 --> 00:16:15,300
It's a list of possible
character probabilities.

284
00:16:15,540 --> 00:16:20,100
And we want to say for each of
them, let's, let's, uh, first say,

285
00:16:20,101 --> 00:16:21,880
so as is going to store
the prediction character.

286
00:16:21,881 --> 00:16:23,790
So there's going to store
prediction character.

287
00:16:25,370 --> 00:16:27,950
And
um,

288
00:16:29,500 --> 00:16:30,850
so we're going to say

289
00:16:33,490 --> 00:16:36,370
this is our first character. This is
our first prediction. And now this is,

290
00:16:36,580 --> 00:16:40,930
this is where the unit, this is where
the distribution, uh, comes into play.

291
00:16:40,960 --> 00:16:44,620
We're going to say if s is
greater than or equal to r,

292
00:16:44,740 --> 00:16:49,480
and so our is going to be a randomly
generated probability. Okay.

293
00:16:49,510 --> 00:16:53,050
So we are generating a probability and
there are different ways of doing this.

294
00:16:53,051 --> 00:16:56,890
But right now we are going to
randomly generate p probability. Okay?

295
00:16:57,610 --> 00:16:58,443
And uh,

296
00:17:00,540 --> 00:17:01,280
okay.

297
00:17:01,280 --> 00:17:04,000
So if the, if it, if it's
greater. So that means if the, if,

298
00:17:04,060 --> 00:17:07,970
if it's a likely probabilities.
So this is a generated threshold.

299
00:17:08,240 --> 00:17:10,160
If it's greater than that,
then we're going to say, okay,

300
00:17:10,161 --> 00:17:13,910
so this is the most likely character.
Set it to the character id.

301
00:17:16,430 --> 00:17:20,180
Okay? And then we're going to break.
And so once we have that character,

302
00:17:20,600 --> 00:17:25,490
once we have that likely character,
we want to one hot encoding, okay,

303
00:17:31,460 --> 00:17:34,190
we want to generate a character.
So we want to create,

304
00:17:34,550 --> 00:17:37,130
create a one hot encoding
of that character.

305
00:17:37,340 --> 00:17:39,920
So we're going to use a num Pi to do this.

306
00:17:39,980 --> 00:17:42,560
And so let me talk about what
one hot encoding is. Why are we,

307
00:17:42,680 --> 00:17:45,440
why are we one hot encoding
these characters? Okay.

308
00:17:45,560 --> 00:17:50,480
We are one hot encoding these
characters because, let me it.

309
00:17:50,490 --> 00:17:51,323
So it's going to be,

310
00:17:53,040 --> 00:17:56,160
we're going to one hot and code
these characters because we want to,

311
00:17:59,450 --> 00:18:04,160
one hot encoding is a way for us to
differentiate. So if we could say, uh,

312
00:18:04,790 --> 00:18:09,780
uh, we went to differentiate
not ranked value. So you know,

313
00:18:09,810 --> 00:18:10,920
if we,
if we were to say,

314
00:18:10,950 --> 00:18:15,950
if we were to just normally
encode like fish taco Bob Geyser,

315
00:18:16,170 --> 00:18:19,710
we could, you know, if we were to encode
this, normally it can be one, two, three,

316
00:18:19,711 --> 00:18:22,860
four. But the problem with that
is that gives them a ranking.

317
00:18:23,130 --> 00:18:25,710
We don't care about her
ranking Taco because it's too,

318
00:18:25,711 --> 00:18:29,070
is not somehow greater than fish.
These are just values. Okay.

319
00:18:29,250 --> 00:18:33,990
So one hot encoding is a way for us to
differentiate between different values

320
00:18:34,050 --> 00:18:35,160
without ranking them.

321
00:18:35,340 --> 00:18:40,260
And so we want to differentiate between
these values without ranking them.

322
00:18:40,440 --> 00:18:43,290
And One hot encoding looks more like this,
you know,

323
00:18:43,410 --> 00:18:45,750
they're just as collection of ones
and Zeros, it's just like that.

324
00:18:46,080 --> 00:18:48,060
And so to do that will generate,

325
00:18:48,130 --> 00:18:53,130
will create a zero matrix with non Pi's
zero function with the size of the jar

326
00:18:53,670 --> 00:18:57,810
size. And then we're going to say, uh,

327
00:18:57,870 --> 00:19:02,710
at the specify character index, set
it to one, so it's gonna be like zero.

328
00:19:02,711 --> 00:19:07,230
This is a drought one. There's there
where one is the index of that character.

329
00:19:07,260 --> 00:19:10,220
Okay.
That's it for our sample probability

330
00:19:11,840 --> 00:19:12,381
distribution.

331
00:19:12,381 --> 00:19:16,790
So hill at averse shopper asking what
if you're using two dictionaries will be

332
00:19:16,791 --> 00:19:21,680
very inefficient if you're
using a big dataset a and e

333
00:19:23,360 --> 00:19:25,820
or would it take forever?
May Basically, uh,

334
00:19:25,850 --> 00:19:30,850
well we actually see a lot of in memory
storage for big data sets and the no,

335
00:19:31,460 --> 00:19:34,710
so the answer is no, because
why? Because we want to a cue,

336
00:19:34,760 --> 00:19:37,370
it could be if we start it all in memory,
but we are,

337
00:19:37,590 --> 00:19:41,990
we are storing it in batches. So we are
flushing it, we're flushing the cache.

338
00:19:42,140 --> 00:19:43,420
This is our cash story and we're,

339
00:19:43,520 --> 00:19:46,910
we're going to flush it every so often
during training. So, but great question.

340
00:19:47,060 --> 00:19:48,710
And, and, and, and another
way, great way to do this,

341
00:19:48,711 --> 00:19:52,010
if you don't want to have to flush
it, uh, is to store it on disk.

342
00:19:52,160 --> 00:19:55,760
And for that we can just,
uh, we could, I mean, it's,

343
00:19:55,761 --> 00:19:57,590
it's a trade off because if
we didn't store it in memory,

344
00:19:57,591 --> 00:20:01,370
we would be a continuously retrieving
it. So then there's that, uh,

345
00:20:01,400 --> 00:20:02,870
computational complexity.

346
00:20:02,871 --> 00:20:06,260
So you're trading off memory
verses computation time for,

347
00:20:06,261 --> 00:20:10,250
for continuously reading it
from disk. Right. Every time.

348
00:20:11,080 --> 00:20:14,570
Uh,
so which one would be the best?

349
00:20:14,750 --> 00:20:18,980
I would say to do it in memory but then
to flush, but to flush it every so often.

350
00:20:20,280 --> 00:20:20,740
Okay.

351
00:20:20,740 --> 00:20:24,370
Okay. So that's, that's one hot
encoding. Okay. So that's for our sample.

352
00:20:25,300 --> 00:20:25,860
Okay.

353
00:20:25,860 --> 00:20:29,190
Okay. So now we're going to vectorize our
data before we feed it into our model.

354
00:20:29,191 --> 00:20:33,030
So vectorize our data. Okay. We
are just chugging along here.

355
00:20:33,090 --> 00:20:37,410
We're going to vectorize our data
to feed it into the model. Okay.

356
00:20:39,170 --> 00:20:42,460
So the length per section. So these
are, I'm going to first define art,

357
00:20:42,470 --> 00:20:43,251
some variables here.

358
00:20:43,251 --> 00:20:48,251
So the length per section is the size
of what we can consider sentence a

359
00:20:48,341 --> 00:20:50,530
sentence. This is how we're going
to feed data into the model.

360
00:20:50,560 --> 00:20:55,060
They're going to be 50 character long,
uh,

361
00:20:55,450 --> 00:20:58,270
batches.
And then each of those batches,

362
00:20:58,271 --> 00:21:01,510
those characters are what's going
to go directly into the model.

363
00:21:01,780 --> 00:21:05,590
But the size of these batches
are 50 characters long. Okay.

364
00:21:05,591 --> 00:21:08,170
And then I'm going to say
skip because we're going to,

365
00:21:09,710 --> 00:21:10,520
okay.

366
00:21:10,520 --> 00:21:13,370
I'll show you exactly what I'm
using. Skip in a second. Okay.

367
00:21:13,371 --> 00:21:15,560
And then we're going to create
an empty list, four sections.

368
00:21:15,561 --> 00:21:17,390
And then for next characters.

369
00:21:21,390 --> 00:21:22,223
Okay.

370
00:21:23,240 --> 00:21:25,010
Okay.
So,

371
00:21:27,470 --> 00:21:31,790
so that's that. Okay. So
here's why we use skip.

372
00:21:31,850 --> 00:21:35,270
So what we're gonna do is we're
going to fill this section's list.

373
00:21:35,280 --> 00:21:39,350
We want to fill the sections list with
chunks of tax. So every two characters,

374
00:21:39,590 --> 00:21:42,320
we want to create a new
50 character long section.

375
00:21:42,500 --> 00:21:47,150
So if it was something like this, so
okay, that's no. So if our tech was like,

376
00:21:47,210 --> 00:21:51,740
hello, I am [inaudible] Raj, this would
generate something like, you know,

377
00:21:51,920 --> 00:21:54,730
uh, so start off with, hello, I'm Sarah.

378
00:21:54,770 --> 00:21:57,860
There'll be the first one and then it
will skip to, and then we'll say low,

379
00:21:57,950 --> 00:22:02,660
I am Saroj whatever's next
because, and then skip to, so then,

380
00:22:02,720 --> 00:22:05,750
oh, I am Saroj because more, right?

381
00:22:05,751 --> 00:22:07,910
So it's going to keep generating
these texts and these,

382
00:22:08,000 --> 00:22:10,880
these chunks are overlapping
there overlapping chunks.

383
00:22:11,090 --> 00:22:12,830
And there are different
ways we could feed them in.

384
00:22:12,920 --> 00:22:15,740
We don't have to have them overlap.
We can have them all be unique.

385
00:22:15,830 --> 00:22:19,250
But in this case we're going to have them
overlap because we don't have a lot of

386
00:22:19,251 --> 00:22:22,580
data. So we want to reuse some
of the data if we can. Okay.

387
00:22:22,820 --> 00:22:25,970
Does that mean that our model is going
to be less accurate than we were if we

388
00:22:25,971 --> 00:22:30,550
were to give it just only unique taxed?
Yes, it will be less accurate then we,

389
00:22:30,560 --> 00:22:33,150
if we were to give it just unique taxed,
but uh,

390
00:22:33,350 --> 00:22:38,330
we don't have shitloads of computing
power, so we're going to do that.

391
00:22:38,331 --> 00:22:40,640
So now let's, let's do
this. So we're going to,

392
00:22:42,440 --> 00:22:46,040
we're going to do this.
So we're going to generate,

393
00:22:46,130 --> 00:22:49,160
so we're going to iterate
through, uh, are taxed.

394
00:22:50,750 --> 00:22:52,010
We're going to iterate through our text,

395
00:22:52,850 --> 00:22:57,850
skipping where we need to skip and we're
going to upend the sections list by

396
00:22:59,481 --> 00:23:01,280
saying, okay, so our text,

397
00:23:10,100 --> 00:23:10,933
okay,

398
00:23:12,250 --> 00:23:15,070
we're going to Upenn or sections
in our next characters list.

399
00:23:20,580 --> 00:23:21,930
You're good.
Okay.

400
00:23:22,110 --> 00:23:27,110
So then our next check characters
were good to compend text hi.

401
00:23:28,500 --> 00:23:30,810
Plus length per section.

402
00:23:33,420 --> 00:23:34,830
So we're filling the,

403
00:23:34,990 --> 00:23:39,480
the sections with those chunks of texts
and the next character is with the next

404
00:23:39,990 --> 00:23:40,823
character.

405
00:23:47,520 --> 00:23:52,380
Exactly. So these are,
this is us cure. No Siri,

406
00:23:52,381 --> 00:23:56,490
I hate Siri. Siri is such a terrible
assistant. So we're going to say,

407
00:23:57,090 --> 00:23:59,520
so now that we have these
chunks that I just described,

408
00:23:59,640 --> 00:24:01,770
we're going to vectorize them.
So now to vectorize them,

409
00:24:01,890 --> 00:24:03,360
we're going to use non pies.

410
00:24:09,360 --> 00:24:11,580
Opening bracket needed for a list.

411
00:24:16,470 --> 00:24:17,303
Hold on

412
00:24:18,720 --> 00:24:22,290
text, hold on. So zero length text.

413
00:24:23,580 --> 00:24:24,070
Okay.

414
00:24:24,070 --> 00:24:28,990
Minus length per section. Skip
text sections. Dot. Append.

415
00:24:30,130 --> 00:24:34,720
Hold on. Text. Let me
just, this is actually,

416
00:24:34,750 --> 00:24:36,100
this should be something else.
It should be,

417
00:24:37,870 --> 00:24:39,940
hold on,
hold on.

418
00:24:41,200 --> 00:24:43,050
So yeah,

419
00:24:45,600 --> 00:24:46,381
sections dot.

420
00:24:46,381 --> 00:24:51,381
Upenn should actually be the text I
apparently there's a missing bracket that

421
00:24:53,701 --> 00:24:57,780
our bond and Tom are saying.
Thank you. Thank you guys.

422
00:24:59,730 --> 00:25:04,590
Text and then the next
characters. Dot append text. Hi.

423
00:25:04,620 --> 00:25:06,600
Length per section.
Okay,

424
00:25:06,930 --> 00:25:11,850
so w so that's it. Oh no wait,

425
00:25:11,940 --> 00:25:13,500
wait, wait. You're mic.
Your mic just went off.

426
00:25:18,650 --> 00:25:20,520
Awesome.
One second.

427
00:25:22,470 --> 00:25:25,310
We're going to switch over one second.
One Sec.

428
00:25:31,860 --> 00:25:34,590
Are we gonna switch minds? I don't want,
I'm just going to switch the battery.

429
00:25:37,260 --> 00:25:41,340
I think that'll be quicker because it
really, I mean it's still worth your side.

430
00:25:41,341 --> 00:25:46,050
Just went out my side. I think we're good.

431
00:25:47,510 --> 00:25:52,340
And Five, four, three, two, one.

432
00:25:53,090 --> 00:25:56,930
And we're back. We're back.
We're back. We're back. Okay,
great. Okay, so where was I?

433
00:25:57,020 --> 00:26:01,520
So we're going to vectorize these inputs.
Okay.

434
00:26:01,550 --> 00:26:05,660
We're going to vectorize these inputs.
So to do that, we're going to say, okay,

435
00:26:07,490 --> 00:26:11,210
x equals Pi none.
Pi Dot Zeros and

436
00:26:14,370 --> 00:26:18,450
God, Damn, this is compact code. Okay.

437
00:26:20,770 --> 00:26:21,940
Length of the sections.

438
00:26:25,260 --> 00:26:26,093
Okay.

439
00:26:27,190 --> 00:26:28,360
We need to get to the machine learning.

440
00:26:28,361 --> 00:26:33,361
So what I'm gonna do is a paste it from
here and then explain it right here.

441
00:26:35,380 --> 00:26:37,600
We need to get to the machine learning.
Okay,

442
00:26:38,740 --> 00:26:40,690
we're going to vectorize
our inputs and outputs.

443
00:26:40,750 --> 00:26:42,970
They're going to both be zero
matrices that are going to be the,

444
00:26:43,440 --> 00:26:46,830
so it's going to be this length of the
sections by the length per section,

445
00:26:47,570 --> 00:26:52,470
by the length PR, uh, by the
length per section. Yes, exactly.

446
00:26:52,740 --> 00:26:55,710
So, and then, so what are our
labels? What are our labels?

447
00:26:55,740 --> 00:27:00,660
Are labels are going to be the next
character. So we're creating labels.

448
00:27:02,490 --> 00:27:06,390
Each of our predictions is going to have
a label and the label is going to be

449
00:27:06,391 --> 00:27:11,070
the next character. Okay, so,
so when we have an output,

450
00:27:11,130 --> 00:27:13,440
our output is going to be
our predicted next character.

451
00:27:13,560 --> 00:27:17,130
That's going to be our output label and
the actual label was going to be the

452
00:27:17,131 --> 00:27:18,510
real next character.

453
00:27:18,510 --> 00:27:23,220
So given some character in our sentence
like you know for the word thaw for h,

454
00:27:23,250 --> 00:27:24,600
the next word we know is he,

455
00:27:24,660 --> 00:27:28,620
so the label is going to be e but are
predicting next character is going to be z

456
00:27:28,830 --> 00:27:30,930
and w. And so this goes back
to that initial question,

457
00:27:31,050 --> 00:27:32,910
why convert a charge to ids?

458
00:27:33,060 --> 00:27:36,000
We need to wait an Alpha knew we need
a numeric way of representing these

459
00:27:36,001 --> 00:27:40,950
because we want to minimize our loss and
our loss function is going to minimize

460
00:27:41,130 --> 00:27:46,130
the distance between our predicted
character slash label and our actual next

461
00:27:46,201 --> 00:27:49,410
character. Slash. Label. Okay, so
that's what our labels are in this case.

462
00:27:49,470 --> 00:27:53,910
So we have someone pointing out
that range is misspelled actually.

463
00:27:54,140 --> 00:27:58,680
Um, he is saying that there's a Typo
in range above the function definition.

464
00:27:59,490 --> 00:28:04,110
Thank you. Above the function
definition ranges. Misspelled. Okay,

465
00:28:04,710 --> 00:28:07,860
so range is

466
00:28:09,730 --> 00:28:10,563
mm,

467
00:28:13,200 --> 00:28:17,220
range is, it says R,N
a.G instead of r a n. G.

468
00:28:17,221 --> 00:28:19,770
E maybe you can control
off it or something.

469
00:28:20,340 --> 00:28:25,230
R A. N. G. O. R. N. A. G. A. G. No. Hmm.

470
00:28:25,490 --> 00:28:30,180
Okay. No, I fixed that. Don't
worry about it. We're okay. Okay.

471
00:28:30,181 --> 00:28:34,410
So now we're getting to the
machine learning. Guys. Let's,
let's do this. Get Ready,

472
00:28:34,800 --> 00:28:39,090
get ready. We're about to do some machine
learning. Get ready for this. Okay.

473
00:28:39,091 --> 00:28:40,920
And we're going to add
some VR into the mix. Guys.

474
00:28:40,921 --> 00:28:44,880
We're going to add some VR into the mix.
This is the shit right now.

475
00:28:44,881 --> 00:28:48,810
This is the shit. Here we go. So
the first thing, so machine learning

476
00:28:51,620 --> 00:28:52,453
time.

477
00:28:54,210 --> 00:28:55,890
Okay.
So here we go.

478
00:28:55,950 --> 00:28:59,460
So the first thing we want to do is we
want to define our batch size. So how,

479
00:28:59,490 --> 00:29:02,640
so in the in general,
let's talk about these terminologies.

480
00:29:02,641 --> 00:29:05,400
They are continuously
used in machine learning.

481
00:29:05,730 --> 00:29:10,060
The batch side defines the
number of samples that are
going to be propagated, uh,

482
00:29:10,370 --> 00:29:14,280
uh, through the network. So, and
then one. And so we have batches.

483
00:29:14,430 --> 00:29:19,170
We have epochs and then we have um,
iterations.

484
00:29:19,320 --> 00:29:23,460
So for one epoch is going to be one
forward pass and one backward pass.

485
00:29:24,060 --> 00:29:25,950
And of all the training samples,

486
00:29:25,980 --> 00:29:30,330
the bad sides is a number of training
samples in one forward, backward pass.

487
00:29:30,331 --> 00:29:34,830
So the higher the batch size,
the more memory space we'll need.

488
00:29:35,010 --> 00:29:37,410
Okay.
So the more memory space we'll need.

489
00:29:39,810 --> 00:29:40,740
And so

490
00:29:47,530 --> 00:29:51,050
it's ready to go when we are. Okay, cool.
Thanks. We're about to do some PR. So,

491
00:29:51,350 --> 00:29:52,730
and then to show line numbers.

492
00:29:53,090 --> 00:29:55,670
Someone shout how to show
line numbers in a second.

493
00:29:56,670 --> 00:29:59,780
Showmen shout out if this guy wants
to know, well I code this out.

494
00:29:59,900 --> 00:30:02,870
So we have batch sizes, five
12, that's the bad size.

495
00:30:02,871 --> 00:30:06,800
And now we're going to define the number
of iterations, the number of iterations.

496
00:30:06,830 --> 00:30:07,850
How many do we want to have?

497
00:30:07,970 --> 00:30:12,300
Let's say 72,000 why 72,000 uh,

498
00:30:12,620 --> 00:30:17,510
we could have 70,000 we don't actually
have a lot of, because I don't have,

499
00:30:18,080 --> 00:30:20,240
I'm not running this in the cloud.
I'm running this locally.

500
00:30:22,480 --> 00:30:24,190
I'm running this locally.
Okay.

501
00:30:24,191 --> 00:30:27,490
So we've vectorize or input because
we want to feed them into our model.

502
00:30:27,491 --> 00:30:31,030
We always have to vectorize our data
before we feed it into our model.

503
00:30:31,060 --> 00:30:35,440
It's a way of converting it into
vectors slashed. Tensors okay,

504
00:30:35,800 --> 00:30:37,900
so now what we're going to
do is we're going to say,

505
00:30:38,200 --> 00:30:42,580
how often do we want to log while
let's log every 100 steps. Okay? Oh,

506
00:30:42,581 --> 00:30:47,581
so and so why did I set 72,000 or
70,000 because we don't have a lot of

507
00:30:47,771 --> 00:30:52,630
computing power.
So that's how often we want to log.

508
00:30:52,840 --> 00:30:55,450
Like where we are just for
printing. So you know, we'll,

509
00:30:55,480 --> 00:30:58,450
during our training iteration, we'll
use this as a checkpoint to say, okay,

510
00:30:58,451 --> 00:31:02,620
now time to print something out.
Okay, so now we have saved every,

511
00:31:02,780 --> 00:31:06,660
and so how often do we want to
save our model? This is also a, a,

512
00:31:06,720 --> 00:31:07,481
an important thing.

513
00:31:07,481 --> 00:31:12,481
We want to continuously save our model
so we don't lose where we are in time.

514
00:31:13,030 --> 00:31:16,990
Okay? And now we want to say, how
many hidden nodes do we want to have?

515
00:31:16,991 --> 00:31:21,550
So I'm going to say ten one hundred
and twenty four why 1024 well,

516
00:31:21,610 --> 00:31:24,730
when we're choosing our hidden knows,
if we have too few,

517
00:31:24,880 --> 00:31:26,710
then we result in under fitting.

518
00:31:26,711 --> 00:31:29,500
That means our model doesn't
have enough data to general.

519
00:31:29,501 --> 00:31:32,740
It doesn't have enough
general generalized storage.

520
00:31:32,770 --> 00:31:35,920
It's a hasn't stored enough generalized
vectors from what it's trained on.

521
00:31:36,040 --> 00:31:38,650
But if we have too many hit hidden nodes,

522
00:31:38,830 --> 00:31:43,830
then we're going to going
to overfit because it's to
fit to the data and it's be

523
00:31:44,091 --> 00:31:45,790
able to generalize well to other data.

524
00:31:46,030 --> 00:31:51,030
So we have to have that ideal number
of hidden notes and there's actually a

525
00:31:51,761 --> 00:31:55,600
formula for calculating the ideal
number of hidden nodes and you take into

526
00:31:55,601 --> 00:31:58,660
account the number of input nodes
and the number of output nodes.

527
00:31:58,960 --> 00:32:03,430
But right now we're going to choose
1,024 because it's in that range. Okay,

528
00:32:03,670 --> 00:32:05,440
so let's give it some starting texts.

529
00:32:05,441 --> 00:32:07,450
What does that text that
we want to train it on?

530
00:32:07,630 --> 00:32:11,860
Some initial texts from which it
generates an entire article from,

531
00:32:11,970 --> 00:32:14,290
so the starting text is going to be

532
00:32:16,190 --> 00:32:16,630
okay.

533
00:32:16,630 --> 00:32:20,200
I am thinking that that's
our starting text. Okay.

534
00:32:20,830 --> 00:32:22,510
That's what we're going
to generate data from.

535
00:32:26,470 --> 00:32:29,020
Okay. So that's that.
And so now we're going to

536
00:32:30,550 --> 00:32:35,500
say
that's our starting texts.

537
00:32:35,560 --> 00:32:36,393
Okay.

538
00:32:41,750 --> 00:32:46,700
Oh was so I'll call it test texts. Start
Test Star. That's what I'll call it.

539
00:32:47,030 --> 00:32:49,250
And so now we're going to save our model,
our model.

540
00:32:50,690 --> 00:32:54,260
And so we wanted to find a checkpoint
directory. So intenser flow,

541
00:32:54,530 --> 00:32:57,980
we define checkpoints
and checkpoints are just,

542
00:32:59,360 --> 00:33:04,160
they are flags. That's
that, that define how uh,

543
00:33:04,190 --> 00:33:05,930
where,
where in the training process we are.

544
00:33:05,931 --> 00:33:10,250
So ideally we want several checkpoints so
we can, you know, depending on, you know,

545
00:33:10,490 --> 00:33:13,970
just for testing purposes,
for debugging purposes,

546
00:33:14,210 --> 00:33:16,940
obviously the last checkpoint is
going to be the where we want.

547
00:33:17,120 --> 00:33:19,850
That's the model we want to
use is the fully trained model.

548
00:33:20,060 --> 00:33:21,890
And so this is actually some,

549
00:33:22,660 --> 00:33:23,493
uh,

550
00:33:25,940 --> 00:33:30,400
do you have a second for questions? I
have a second for questions. So, um,

551
00:33:31,090 --> 00:33:32,430
who do all is thing in this games,

552
00:33:32,510 --> 00:33:36,560
there'll be too much data and
held to be held in weights,

553
00:33:36,561 --> 00:33:38,480
will want it slow down.
Our processing.

554
00:33:38,960 --> 00:33:43,100
And Sahil is asking how did you find
the right number of hidden nodes?

555
00:33:43,880 --> 00:33:48,050
That is a great question. So heal is
killing it with this questions today.

556
00:33:49,040 --> 00:33:54,020
Okay, so the question was, hold on.
Okay, great. That they can pop up.

557
00:33:54,080 --> 00:33:57,010
Say it again. So, so he'll uh,

558
00:33:57,020 --> 00:34:00,560
was asking how did you find the
right number of hidden nodes?

559
00:34:01,220 --> 00:34:04,070
And you want to go with that?
Okay, that's a good one. Okay.

560
00:34:04,071 --> 00:34:07,550
So the number of hidden nodes, so
calculating the, the hidden nodes.

561
00:34:07,551 --> 00:34:11,390
So there's a formula towards the
formula. The formula is, okay,

562
00:34:11,391 --> 00:34:15,800
so this is a great one right here. Hold
on. Where's the formula? This right here.

563
00:34:15,801 --> 00:34:20,150
So check this out.
Okay.

564
00:34:20,170 --> 00:34:22,220
There's for supervised learning,

565
00:34:22,250 --> 00:34:23,990
which is what we're doing
because we have labels,

566
00:34:23,991 --> 00:34:25,970
we are creating those labels ourselves.

567
00:34:26,180 --> 00:34:31,180
Those labels are the next set of
characters and this formula where n is the

568
00:34:32,151 --> 00:34:36,940
number of input neurons and, and the
next one is the number of output neurons,

569
00:34:36,960 --> 00:34:40,580
number of the number of samples in the
training data set and some arbitrary

570
00:34:40,581 --> 00:34:43,010
scaling factor.
If we were to use this formula,

571
00:34:43,011 --> 00:34:46,550
this would give us a good estimate for
what our head notes should be. Okay.

572
00:34:46,760 --> 00:34:50,720
And this. So hopefully that
answers your question. So back to

573
00:34:52,220 --> 00:34:53,420
this.
So now

574
00:34:55,980 --> 00:35:00,980
let's build this model we're going to
build LSTM and we're about to use VR in

575
00:35:01,101 --> 00:35:02,970
is, I'll tell, I'll let you know.
I'll let you know. I'll let you know.

576
00:35:02,971 --> 00:35:07,350
I'll let you know. So let's build
our model. So let's create our graph,

577
00:35:07,530 --> 00:35:12,330
our computation graph. And in tensorflow
we always initialize a graph like this.

578
00:35:12,540 --> 00:35:13,373
And then

579
00:35:15,090 --> 00:35:19,230
we're going to say with graph.as default,
why?

580
00:35:19,260 --> 00:35:20,490
Why as default?

581
00:35:21,240 --> 00:35:24,270
This is our computation graph because
if we had multiple glip graphs,

582
00:35:24,360 --> 00:35:28,350
this is a way for us to define which
graph. If we could say graph has default,

583
00:35:28,590 --> 00:35:30,750
and then the identifier
for this graph is default,

584
00:35:30,751 --> 00:35:33,990
the identify for the next graph.
But in this case we just have one graph.

585
00:35:34,170 --> 00:35:37,290
Having multiple graphs
is not where we are. Now.

586
00:35:37,291 --> 00:35:41,490
We don't actually need to run multiple
Grad, we just need one graph. Okay,

587
00:35:41,520 --> 00:35:42,950
so is there no,

588
00:35:42,990 --> 00:35:46,530
is there another link that they should
be a link to apart from the Dataset?

589
00:35:48,120 --> 00:35:52,230
Uh, yes. So let me, let me
type that in. What did I just,

590
00:35:55,310 --> 00:36:00,260
and the, and I'll put in a description as
well for, I just pasted it. Cool. Oh, so,

591
00:36:00,710 --> 00:36:02,300
okay. So then, so we're
going to define us.

592
00:36:02,301 --> 00:36:06,170
The first thing that we build our model
is we want you to find our global stats.

593
00:36:06,230 --> 00:36:11,030
Okay. We wanted to find our global steps.
So our global step, our global step is,

594
00:36:11,031 --> 00:36:14,120
refers to the number of batches
that are seen by the graph.

595
00:36:14,330 --> 00:36:16,460
So every time a batch has provided,

596
00:36:16,640 --> 00:36:19,880
the weights are updated in the
direction that minimizes the loss.

597
00:36:20,090 --> 00:36:25,090
And global step just keeps track of
the number of batches seen so far.

598
00:36:25,760 --> 00:36:28,220
So we're going to start off as a
zero, okay. And it's going to be, hey,

599
00:36:28,250 --> 00:36:30,260
tensorflow variable,
uh,

600
00:36:30,261 --> 00:36:34,190
because variables are tensorflow
primitives that can be modified and we are

601
00:36:34,191 --> 00:36:36,410
going to modify it,
okay.

602
00:36:37,520 --> 00:36:40,370
Because we're going to add to
it as opposed to a t. F. Dot.

603
00:36:40,371 --> 00:36:43,520
Constant that never gets modified.
Those are immutable primitives.

604
00:36:43,970 --> 00:36:47,420
So let's build the shit out of this.

605
00:36:47,480 --> 00:36:49,970
So the first thing we're gonna
do is we're going to say,

606
00:36:50,300 --> 00:36:54,050
let's define our placeholder.
So are placeholders are
our gateways for our data.

607
00:36:54,410 --> 00:36:57,380
And these are going to be float 30
twos because they are 32 bit integers.

608
00:36:57,740 --> 00:37:01,100
And we're going to say we're
going to define this tensor size.

609
00:37:01,310 --> 00:37:04,160
So the tense or size, it's going
to be three dimensional. Okay.

610
00:37:04,460 --> 00:37:07,390
It's going to be three dimensional
because we have three parameters for it.

611
00:37:07,400 --> 00:37:12,080
This is the size of the tea, the data
that we're going to feed in to our model.

612
00:37:12,110 --> 00:37:16,710
Okay. And so to define this,
let me define both. Uh,

613
00:37:17,240 --> 00:37:19,160
gateways. We have two
place holders, right?

614
00:37:19,161 --> 00:37:21,950
One for our data and
then one for our labels.

615
00:37:22,130 --> 00:37:26,630
They're both going to be
32 bit integers and uh,

616
00:37:26,750 --> 00:37:29,120
the size of this one.
So here's how it goes down.

617
00:37:30,260 --> 00:37:33,650
Our data is going to be those character,

618
00:37:34,070 --> 00:37:36,170
a 50 length character,
uh,

619
00:37:36,200 --> 00:37:40,400
sentences that we are creating and our
labels are going to be the likely next

620
00:37:40,401 --> 00:37:44,690
character. A list of a likely next
characters, no, sorry, not sorry.

621
00:37:45,320 --> 00:37:46,153
Wrong.

622
00:37:47,180 --> 00:37:52,180
Our labels are going to be the actual
next characters depending on given our

623
00:37:52,551 --> 00:37:56,900
sentence. What does that next character,
the, the, the, the actual next character.

624
00:37:57,110 --> 00:37:58,700
We want to predict the next character.

625
00:37:58,850 --> 00:38:03,850
So the labels are there for us to
treat this as a classification problem.

626
00:38:04,221 --> 00:38:07,970
That's what they're there for.
So now that we have that,

627
00:38:07,971 --> 00:38:11,240
we're going to build our LSTM network.
This is what I've been waiting for.

628
00:38:11,360 --> 00:38:13,670
We're going to build this LSTM network.

629
00:38:13,700 --> 00:38:18,440
So let me show you this shit.

630
00:38:18,470 --> 00:38:19,250
Okay,

631
00:38:19,250 --> 00:38:23,690
we're going to build this LSTM network
and I have a great link to show you guys

632
00:38:23,720 --> 00:38:28,430
if I can find it. So here's the
link. I'm going to paste it in here.

633
00:38:29,000 --> 00:38:32,630
Here's the link and I'm going
to look at it on my screen.

634
00:38:34,130 --> 00:38:34,963
Where did I say it?

635
00:38:37,410 --> 00:38:42,110
Where's the link? No, hold
on. Where his, that plank.

636
00:38:42,120 --> 00:38:44,430
Hold on. I got this. Did I
say it here? No, I didn't.

637
00:38:45,020 --> 00:38:45,853
Oh

638
00:38:48,530 --> 00:38:52,310
No. So this is actually a better LSTM
cell. Look. Let's just say LSTM network.

639
00:38:52,370 --> 00:38:54,380
Check this out and then we're
going to go into VR in a second.

640
00:38:56,620 --> 00:38:57,453
Here it is

641
00:38:58,540 --> 00:39:01,870
an LSTM network and this is just,
this shows it through time. That's,

642
00:39:01,960 --> 00:39:05,860
that's why we have, that's why
we have two of them side by side,

643
00:39:05,861 --> 00:39:09,520
because it's showing it through time. So
if we were to just split this, in fact,

644
00:39:09,521 --> 00:39:11,260
I can just split it myself.

645
00:39:12,630 --> 00:39:13,463
Okay.

646
00:39:13,630 --> 00:39:18,310
That's an LSTM network right there
through time.

647
00:39:18,311 --> 00:39:21,280
So let me, let me actually just write
this down and do our get ready for VR.

648
00:39:21,550 --> 00:39:22,720
Get ready for Vr.
Here we go.

649
00:39:23,240 --> 00:39:27,320
Okay. Okay. So

650
00:39:29,660 --> 00:39:32,180
let's do this in Vr.

651
00:39:33,520 --> 00:39:37,630
So let's the womb,
we are now in virtual reality.

652
00:39:37,660 --> 00:39:42,610
We are now in virtual reality. Okay.
Here's what it is. So let me see this.

653
00:39:42,730 --> 00:39:46,690
Okay, great. So Undo. Yup.

654
00:39:47,860 --> 00:39:51,910
Nope. Here is us one to many. This
one, yes. To and do that. Okay.

655
00:39:51,911 --> 00:39:54,250
So here's what it is.
We are,

656
00:39:55,090 --> 00:39:58,290
so if you could get switched to
the bass. Okay, great. Okay. Yeah.

657
00:39:58,600 --> 00:40:01,990
So this is an LSTM cell, right?
And we can think of it as a box.

658
00:40:02,200 --> 00:40:07,030
So an Lstm cell that's really all in a,
an an LSTM recurrent network.

659
00:40:07,270 --> 00:40:11,600
The LSTM cell is, is uh, the neuron. It's,

660
00:40:11,620 --> 00:40:15,910
it is that neuron, right? So when we
feed data into an LSTM recurrent network,

661
00:40:16,090 --> 00:40:20,890
this is the neuron. So we have an
input gate, an output gate, uh,

662
00:40:20,891 --> 00:40:22,720
forget gate.
So we'll say Aye.

663
00:40:24,480 --> 00:40:25,680
Uh Oh

664
00:40:26,310 --> 00:40:30,870
f and then we have an internal hidden
state, which we'll define as h.

665
00:40:31,260 --> 00:40:34,140
And so the reason I'm doing
this in Vr is because,

666
00:40:34,440 --> 00:40:37,890
and this represents the copy of it.
So this is copy one, this is two,

667
00:40:38,190 --> 00:40:40,050
this is three.

668
00:40:41,190 --> 00:40:44,520
And so what happens is
through time we update this.

669
00:40:44,580 --> 00:40:47,010
So we back propagate through
time with this. And so what do,

670
00:40:47,011 --> 00:40:51,000
I mean each of these gates
has a series of weight values.

671
00:40:51,150 --> 00:40:54,960
So each of these has a
series of wait values. Okay.

672
00:40:55,050 --> 00:40:59,310
And these weight values are
going to be updated through time.

673
00:40:59,520 --> 00:41:01,920
And we have a standard,
we have a set,

674
00:41:02,880 --> 00:41:07,880
a set of equations that we perform every
time to calculate a what this hidden

675
00:41:08,251 --> 00:41:09,300
state is going to be.

676
00:41:09,690 --> 00:41:14,640
LSTS are a way for us to capture
longterm dependencies for us to capture

677
00:41:14,641 --> 00:41:18,590
longterm memory. And when
we back propagate through
time, we are updating the,

678
00:41:18,610 --> 00:41:20,550
the state of the cell.
Okay.

679
00:41:20,551 --> 00:41:23,760
And that's what I mean by through
time because as we add new batches,

680
00:41:23,761 --> 00:41:28,480
new sentences, ah, we are
feeding in, not just uh,

681
00:41:28,830 --> 00:41:32,880
the new data we are feeding in
the previous state of the STM. We,

682
00:41:32,940 --> 00:41:35,390
we're pre fitting in the
previous state of the LSTM.

683
00:41:35,510 --> 00:41:39,230
It's just going to back propagate through
time for as many iterations as we have.

684
00:41:39,440 --> 00:41:41,450
Okay. And we're going to, we're
going to do that right now.

685
00:41:41,451 --> 00:41:44,420
We're going to define all of these
gates programmatically. Okay.

686
00:41:44,421 --> 00:41:48,710
So that's it for Vr. I hope you guys
liked that little VR demo. That was cool.

687
00:41:48,770 --> 00:41:53,180
Let's see. Oh, we're really excited
about it. Oh, that's awesome. Okay,

688
00:41:53,210 --> 00:41:58,010
I'm glad you guys liked the VR. Okay.
So that's for the, that's it for the VR.

689
00:41:58,100 --> 00:42:00,230
Let's get back to the code.
So

690
00:42:02,340 --> 00:42:05,340
I'm glad you guys liked that.
So here we go with the code. So

691
00:42:14,070 --> 00:42:18,480
here we go. So let's
define our, uh, gates.

692
00:42:18,510 --> 00:42:22,020
So we have our input gate,
our output gates and hour

693
00:42:26,830 --> 00:42:31,270
we're going to or have our implicate
our Applegate, uh, and our, uh,

694
00:42:31,660 --> 00:42:36,010
forget the gates as well
as our internal state.

695
00:42:36,190 --> 00:42:40,420
So these are going to be calculated.
They will be calculated.

696
00:42:41,000 --> 00:42:41,833
Um,

697
00:42:45,330 --> 00:42:48,470
they will be calculated first.
Uh,

698
00:42:49,920 --> 00:42:54,720
uh, they're not related to each other.
They're going to be calculated in, uh,

699
00:42:54,900 --> 00:42:58,470
like, what's the word I'm trying
to look for it like siphons.

700
00:42:58,500 --> 00:43:00,740
They're going to be calculated in vacuums,
vacuums,

701
00:43:00,810 --> 00:43:05,560
there'll be calculated in vacuums.
And then we're going to,

702
00:43:05,640 --> 00:43:07,240
then we're going to add it out.
So here's what I mean by that.

703
00:43:07,480 --> 00:43:12,070
So let's write out the first thing,
the first side of weights,

704
00:43:12,250 --> 00:43:13,960
and then we'll write out the next one.

705
00:43:13,961 --> 00:43:17,920
So we're going to have a TF variable
to represent this, this value, okay.

706
00:43:17,921 --> 00:43:21,880
Because the why, because these
are mutable, mutable, primitive.

707
00:43:22,120 --> 00:43:26,420
And so the TF dot. Truncated
normal method, we'll define, uh,

708
00:43:26,950 --> 00:43:29,440
it's going to define
a random distribution.

709
00:43:29,441 --> 00:43:32,680
So we're going to initialize these
weights randomly as we do in network.

710
00:43:32,710 --> 00:43:36,880
In neural nets. We start off with a random
distribution, uh, and this, and the,

711
00:43:37,060 --> 00:43:38,110
this is going to be our

712
00:43:44,170 --> 00:43:46,000
char,
char size and our hidden nodes.

713
00:43:46,210 --> 00:43:49,570
And then this is our mean
and standard deviation.

714
00:43:49,571 --> 00:43:51,550
So these are values that define,
yeah,

715
00:43:53,710 --> 00:43:56,290
how that distribution is going
to look like. So remember it's a,

716
00:43:56,291 --> 00:43:58,930
it's a distribution, like a golf
Sian, right? It's a distribution.

717
00:43:58,931 --> 00:44:01,960
So we're going to generate those
weights randomly. So that's the weights.

718
00:44:02,050 --> 00:44:03,820
And so I'm going to define this.
Don't worry, don't worry about it.

719
00:44:04,300 --> 00:44:04,911
I'm going to define it.

720
00:44:04,911 --> 00:44:08,530
So we have our starting weights and
then we have our next set of weights.

721
00:44:08,830 --> 00:44:11,200
And so our next set of way I can
just actually copy and paste this.

722
00:44:12,160 --> 00:44:14,590
Our next set of weights,
let me just,

723
00:44:14,820 --> 00:44:18,220
it's going to be three lines of code and
then I'm going explain it. So TF dot.

724
00:44:18,221 --> 00:44:22,660
Truncated normal of hidden
nodes and then of hidden nodes,

725
00:44:22,900 --> 00:44:26,920
that distribution. And then lastly, our
bias. So this is going to happen for

726
00:44:29,330 --> 00:44:34,260
are uh, input gates. This
is our input gate. Okay.

727
00:44:34,261 --> 00:44:38,550
So this is our input gate.
This is,

728
00:44:38,790 --> 00:44:41,490
this is the low level guys.
This is the low level right now.

729
00:44:41,491 --> 00:44:44,790
So we have our bias.
So remember,

730
00:44:45,120 --> 00:44:46,860
think of each of these as a perceptron.

731
00:44:46,950 --> 00:44:48,840
These are neural nets
in and of themselves.

732
00:44:48,870 --> 00:44:53,580
We have neural nets inside of a neural
net neuroception, basically neuroception.

733
00:44:53,760 --> 00:44:54,593
So

734
00:44:56,970 --> 00:44:57,400
okay,

735
00:44:57,400 --> 00:45:00,190
here's what it is. Here's
what I have. Just coat it.

736
00:45:00,490 --> 00:45:03,220
What I've just coded is the input gate.

737
00:45:03,430 --> 00:45:05,500
So the input gate has,

738
00:45:06,230 --> 00:45:07,063
mmm.

739
00:45:11,440 --> 00:45:14,470
The implicate has the, uh,
what was I trying to say?

740
00:45:14,890 --> 00:45:18,640
The input gate has the
weights, um, and it has,

741
00:45:18,940 --> 00:45:22,720
it has an initial set of weights for the
input and then has waits for the output

742
00:45:22,870 --> 00:45:25,030
and then has a bias
vector. Okay. So it has,

743
00:45:25,300 --> 00:45:29,920
it has an initial set of weights and then
un UN un UN outputs that have weights

744
00:45:30,010 --> 00:45:34,910
and then a bias vector. And guess
what, these are calculated in vacuum.

745
00:45:34,911 --> 00:45:37,720
So none of these have anything
to do with each other yet.

746
00:45:37,870 --> 00:45:39,870
We're just initializing them.
So let me,

747
00:45:40,090 --> 00:45:42,670
let me just say this three
times or three more times.

748
00:45:42,671 --> 00:45:44,560
So this is going to be the input.
This is going to be our,

749
00:45:44,561 --> 00:45:48,400
forget where the weights for the input
and the wait for the previous output.

750
00:45:48,401 --> 00:45:49,234
So let me say this.

751
00:45:49,240 --> 00:45:54,240
So the weights for inputs then waits
for previous output as well as bias.

752
00:45:58,140 --> 00:46:02,490
All hail the input gate. We have our
implicate. So now that's our implicate.

753
00:46:02,550 --> 00:46:05,940
And so now we're going to
define our for decades.

754
00:46:06,870 --> 00:46:07,380
Okay,

755
00:46:07,380 --> 00:46:09,840
you guys are champs for sticking
through this because this is some,

756
00:46:09,870 --> 00:46:13,290
this is some good stuff. Forget gate.
And then we have our health book gate.

757
00:46:13,650 --> 00:46:16,600
We have our output gate,
we have zero Roi.

758
00:46:16,830 --> 00:46:21,390
And then we have uh oh and then I,
and then see I,

759
00:46:21,480 --> 00:46:25,920
this is us, our way of, of um,
this our memory cell. This is the,

760
00:46:25,940 --> 00:46:27,450
the internal hidden state.
Oh,

761
00:46:27,451 --> 00:46:29,430
and we're going to get into
those calculations in a second.

762
00:46:29,431 --> 00:46:33,150
We are just defining these gates. That's
all we're doing. We're defining gates.

763
00:46:33,420 --> 00:46:36,360
Gates have to be defined.
Bill Gates doesn't need to be defined.

764
00:46:36,361 --> 00:46:37,920
He's already defined by Microsoft.

765
00:46:38,070 --> 00:46:41,790
So we have our memory cell and
the rest of our gates. So we,

766
00:46:41,850 --> 00:46:44,130
we've named them all respectively.

767
00:46:44,131 --> 00:46:48,270
There are unique names and let me make
sure that the values inside of them are

768
00:46:48,330 --> 00:46:49,163
good to go.

769
00:46:49,320 --> 00:46:54,320
They are good to go because they are
there are initialize the same way.

770
00:46:54,870 --> 00:46:59,010
They're initialized the same way.
All of them. Okay. So yes. Great.

771
00:46:59,250 --> 00:47:04,050
Those are our cells. And oh wait, hold on.

772
00:47:05,110 --> 00:47:07,920
Input.
Forget how put,

773
00:47:08,310 --> 00:47:11,940
and lastly cell states,
those are our,

774
00:47:12,540 --> 00:47:16,890
those are our, I will definitely
do an outline at the end.

775
00:47:18,760 --> 00:47:19,580
Yeah,

776
00:47:19,580 --> 00:47:23,240
the eye is duplicating. Thank you. Okay,
so now let's, let's calculate the cell.

777
00:47:23,241 --> 00:47:27,860
We've defined them. So this is the actual
cell. This is how we define ourselves.

778
00:47:28,160 --> 00:47:28,993
And

779
00:47:34,210 --> 00:47:37,930
these are calculated separately.
No overlap until we get to this.

780
00:47:38,260 --> 00:47:43,260
And so let me paste this part because I
need to explain what is going on here.

781
00:47:46,760 --> 00:47:50,600
Okay? So here's what's going on.
Here's what, here's what the deal is.

782
00:47:51,290 --> 00:47:54,620
Let me make sure it's all visible
because we're about to explain this shit.

783
00:47:56,250 --> 00:47:56,620
Okay.

784
00:47:56,620 --> 00:48:01,300
Oh, don't hold your borrower says hold
your horses. No, that's a duplicate.

785
00:48:01,301 --> 00:48:03,970
That's a duplicate and no, no,
no, that's very useful actually.

786
00:48:04,090 --> 00:48:07,360
I wrote that because it's very
useful. Okay, so here's what it is.

787
00:48:07,630 --> 00:48:11,090
Here's what it is.
Here's what it is.

788
00:48:11,960 --> 00:48:15,430
We are calculus. So, so here,

789
00:48:15,500 --> 00:48:17,150
here are the equations by the way.

790
00:48:17,180 --> 00:48:21,560
So these are the set of equations
and this is their programmatic.

791
00:48:21,890 --> 00:48:25,700
These are the set of equations
for calculating our, what
do we want to calculate?

792
00:48:25,701 --> 00:48:30,701
We want to calculate an output and a
state and we're going to answer some

793
00:48:31,461 --> 00:48:35,630
questions. Go for on. Not a question
actually there seeing the W io. Uh,

794
00:48:35,631 --> 00:48:40,370
you have the, you have a twice instead
of it should be w underscores CEO.

795
00:48:40,730 --> 00:48:44,450
Thank you. Great. We are,

796
00:48:45,320 --> 00:48:49,250
these are the,
and I linked to this before.

797
00:48:49,370 --> 00:48:52,400
These are the set of equations and
I know it can be a little confusing.

798
00:48:52,490 --> 00:48:55,730
This is math terminology. You have to do
a little bit of, you know, refreshing.

799
00:48:55,820 --> 00:48:58,310
We're not sitting here doing
Algebra homework every day, right?

800
00:48:58,580 --> 00:49:01,580
So of course we're going to need a
little bit of refreshing. It's okay.

801
00:49:01,700 --> 00:49:05,960
These are just, uh, they're just, this
is a language. Math is a language.

802
00:49:05,961 --> 00:49:09,380
We want to remember this language and
how do we remember the language by

803
00:49:09,410 --> 00:49:12,440
practicing it? Okay, so this
is the language. Okay. The,

804
00:49:12,441 --> 00:49:16,790
these are the set of equations that it's
going to calculate our output state.

805
00:49:17,240 --> 00:49:20,210
And this is how this is what it's
represented. Like programmatically. No.

806
00:49:20,211 --> 00:49:25,010
So I prefer programmatic implementations
to the a and the link is going to

807
00:49:25,011 --> 00:49:29,360
happen in the, in the chat. I link to
it before, but it is in the description.

808
00:49:29,361 --> 00:49:31,730
All the links are in the chat
and the description. Yes. Thanks.

809
00:49:31,731 --> 00:49:34,430
[inaudible] wizard of Oz. So
that's what, that's what we did.

810
00:49:34,431 --> 00:49:37,400
And this is the LSTM cell.
Okay.

811
00:49:37,460 --> 00:49:41,240
So what we're gonna do is we're
going to matrix multiply the,

812
00:49:42,230 --> 00:49:43,910
which is essentially a dot product.

813
00:49:43,911 --> 00:49:48,911
We're going to multiply the input times
the input weights plus the output times

814
00:49:49,971 --> 00:49:52,760
the weight for the previous
output plus the bias.

815
00:49:52,910 --> 00:49:56,870
And the bias is our anchor points. It
makes sure it helps prevent over fitting.

816
00:49:57,380 --> 00:49:59,750
So that's how we calculate the input gate.

817
00:49:59,930 --> 00:50:03,980
And that calculation has nothing to
do with the rest of the other three

818
00:50:03,981 --> 00:50:04,551
calculations.

819
00:50:04,551 --> 00:50:09,230
Yet these are all the calculus
calculations that basically

820
00:50:11,060 --> 00:50:15,290
they're using the previous output and the
current input and there's summing them

821
00:50:15,291 --> 00:50:17,900
together to, to, to, uh,

822
00:50:19,330 --> 00:50:22,570
to represent how data
has changed over time.

823
00:50:22,960 --> 00:50:25,540
Now that we have these four,
these four,

824
00:50:27,040 --> 00:50:28,510
yeah,
essentially,

825
00:50:28,570 --> 00:50:33,190
uh, primitives that we've created the
input for gay output and memory cell.

826
00:50:33,250 --> 00:50:38,110
Now we can combine them all together.
And so this is that last equation.

827
00:50:38,290 --> 00:50:41,740
This is how we combine them together.
We take the four get date,

828
00:50:41,920 --> 00:50:44,050
we multiply it by the current state,

829
00:50:44,380 --> 00:50:47,260
and then we add it to the
input gate times the memory,

830
00:50:47,261 --> 00:50:50,590
still times the memory cell. So the, the,

831
00:50:50,800 --> 00:50:53,820
the state right here is the given state,
uh,

832
00:50:53,830 --> 00:50:58,060
where we are currently and the memory
cells is the state where we were before.

833
00:50:58,300 --> 00:51:01,270
So we take the input gate time,
what we remembered before,

834
00:51:01,420 --> 00:51:05,980
plus the forget gate times what we
now get because the F and why are we

835
00:51:05,981 --> 00:51:07,830
multiplying it because we want to see it.

836
00:51:07,831 --> 00:51:11,500
What do we want to forget from what
we're given and then what do we want to

837
00:51:11,560 --> 00:51:14,590
remember from what
we've stored previously.

838
00:51:14,800 --> 00:51:19,750
And that output is going to give us a
final state and we're going to take that

839
00:51:19,751 --> 00:51:24,700
state value and we're going to squash
it with a nonlinearity, with a,

840
00:51:25,720 --> 00:51:30,400
with a non linearity.
And the nonlinearity is going to be Tan h,

841
00:51:30,460 --> 00:51:33,730
which is a hyperbolic tangent of x.
Uh,

842
00:51:33,760 --> 00:51:36,850
which essentially we don't
have to get into the details.

843
00:51:36,880 --> 00:51:38,680
It is one of many nonlinearities.

844
00:51:38,860 --> 00:51:42,190
But what it does here is it squashes
it into a probability value.

845
00:51:42,191 --> 00:51:45,220
That means that converts the state,
whatever. It's, whatever it is,

846
00:51:45,221 --> 00:51:49,880
a floating point number into a
single probability value. And uh,

847
00:51:49,900 --> 00:51:53,740
we're gonna multiply it by the output
gate and that's going to be our output.

848
00:51:53,920 --> 00:51:55,390
And at the end we were turned our,

849
00:51:55,540 --> 00:52:00,250
our final output of the Lstm
cell and oh, sorry, the final,

850
00:52:00,251 --> 00:52:01,060
not sorry,

851
00:52:01,060 --> 00:52:06,060
the final state of the OCM and then
the output value that it calculates.

852
00:52:06,310 --> 00:52:08,500
So that's what that's going to do.
And then,

853
00:52:12,470 --> 00:52:13,303
yeah,

854
00:52:14,240 --> 00:52:17,030
let me know when you want
to take questions. We have
some, we have some questions.

855
00:52:17,031 --> 00:52:21,170
Yeah, go for the questions. So,
um, Kevin Nelson was asking,

856
00:52:21,171 --> 00:52:26,030
can you use tensorflow to visualize
the network at some point or Nah,

857
00:52:26,270 --> 00:52:30,160
I think that might've been a little bit
far back and Praveen just ask was it

858
00:52:30,170 --> 00:52:34,520
raspberry pi or on tense would flow.
Okay. So Kyi, Kevin's question.

859
00:52:34,550 --> 00:52:37,250
Let me paste this in and I'm going
to explain it to Kevin's question.

860
00:52:37,280 --> 00:52:40,370
Can we use tensorflow to visualize
it? Yes. With tensor board,

861
00:52:40,400 --> 00:52:44,690
which I'm sure you know, we, I need
to use tensor board more often.

862
00:52:44,691 --> 00:52:46,610
Thank you for the reminder.
We're not doing it this time,

863
00:52:46,611 --> 00:52:48,770
but we will in the future.
In fact,

864
00:52:48,771 --> 00:52:51,410
I'm probably going to have
a tensor board specific.

865
00:52:52,300 --> 00:52:52,690
Okay.

866
00:52:52,690 --> 00:52:57,130
Live stream coming up soon. So, and
then Praveen and Praveen was asking,

867
00:52:57,250 --> 00:53:00,520
um,
if the raspberry Pi on transfer flow,

868
00:53:01,120 --> 00:53:03,710
and there's people now having
conversations about raspberry,

869
00:53:03,720 --> 00:53:04,940
raspberry pie on tens of floods.

870
00:53:04,941 --> 00:53:09,941
So I'm pretty sure that somebody
made a tensorflow raspberry Pi a

871
00:53:11,920 --> 00:53:16,460
rapper. So there it is.

872
00:53:19,200 --> 00:53:20,033
Yeah,

873
00:53:20,590 --> 00:53:22,690
there it is. So here we go
with this. So back to this,

874
00:53:22,990 --> 00:53:26,640
we want to now,
uh,

875
00:53:26,730 --> 00:53:31,320
calculate the LSTM value over time.
So this is our unrolling over time.

876
00:53:31,321 --> 00:53:32,700
So before we start training,

877
00:53:33,090 --> 00:53:38,090
we want to feed it in all of those
values so that we can compute that those

878
00:53:38,251 --> 00:53:40,650
longterm dependencies.
And this is what's happening here.

879
00:53:40,920 --> 00:53:45,920
We're going to start off with an
empty set of matrices for our,

880
00:53:46,140 --> 00:53:50,460
our output in our state. And then we're
going to unroll the LSTM loop over time.

881
00:53:50,461 --> 00:53:51,151
So what does this mean?

882
00:53:51,151 --> 00:53:56,151
So for each section we're going to
calculate a state and an output from our

883
00:53:56,461 --> 00:54:00,390
LSTM. So to start off with, when
I equals zero, the first round,

884
00:54:00,630 --> 00:54:04,590
we're going to give it into our,
give it to our outputs.

885
00:54:04,620 --> 00:54:08,820
All I list, this is a list of outputs
that we are storing over time.

886
00:54:09,150 --> 00:54:13,110
And this is our labels list
that we are storing over time.

887
00:54:13,200 --> 00:54:17,730
So once we're past that initial iteration,
then we can start,

888
00:54:17,880 --> 00:54:22,320
uh, concatenating vectors
along a dimension access.
So we're not multiplying,

889
00:54:22,530 --> 00:54:25,170
we're just adding in all of those
outputs and all of those labels.

890
00:54:25,320 --> 00:54:30,210
Until finally at the last iteration,
we, we have our final value.

891
00:54:30,450 --> 00:54:31,740
Okay.
And so,

892
00:54:35,400 --> 00:54:38,820
okay, so then up to the
optimizer part, we're going to be

893
00:54:41,180 --> 00:54:46,130
okay. So I'm going to paste this in
as well. So then for their classifier,

894
00:54:48,550 --> 00:54:50,440
our classifier is, is, is,

895
00:54:50,530 --> 00:54:53,830
is only gonna run after our saved
output and save state were assigned.

896
00:54:53,831 --> 00:54:57,250
What does that mean? We have our list
of outputs. We have our list of states.

897
00:54:57,580 --> 00:54:59,050
I will show a demo at the end.

898
00:54:59,750 --> 00:55:01,960
Uh,
so

899
00:55:04,140 --> 00:55:08,190
now that we have our LSTM
unrolled over time, that the dope,

900
00:55:08,250 --> 00:55:13,140
the LSTM cell is the only thing that
we really, that we worry about in our,

901
00:55:13,141 --> 00:55:17,880
in an LSTM network, we just needed to
find the LSTM cell. And then we have, uh,

902
00:55:17,910 --> 00:55:21,420
the weights from cells,
plural.

903
00:55:21,630 --> 00:55:25,950
And then we have the weights from the
hidden layer of LSTM cells to the output

904
00:55:25,951 --> 00:55:27,690
layer.
And that's what we're defining here,

905
00:55:27,870 --> 00:55:31,800
that last set of weights for the bigger
network in and of itself, not the cells,

906
00:55:32,010 --> 00:55:36,720
but the bigger network and to those are
defined by WNB and they're generated

907
00:55:36,721 --> 00:55:38,700
randomly using the
truncated normal function.

908
00:55:39,000 --> 00:55:42,750
And then we're going to
calculate the logics.

909
00:55:42,900 --> 00:55:47,640
So logics is so largest is uh,
the,

910
00:55:47,790 --> 00:55:51,390
the function that operates on the
unscaled output of earlier layers.

911
00:55:51,570 --> 00:55:56,160
So it's a way for us to
think about, uh, what,

912
00:55:56,430 --> 00:55:58,230
uh,
what is going to be that,

913
00:55:59,220 --> 00:56:02,190
that value that we feed to
our loss function. Okay.

914
00:56:02,191 --> 00:56:04,410
So it is our prediction
outputs. It's a, it's a,

915
00:56:04,411 --> 00:56:07,920
it's a list of all of our predicted
outputs and we're going to compare it with

916
00:56:07,921 --> 00:56:09,720
the labels.
So we have predictions.

917
00:56:09,840 --> 00:56:12,300
And then we have our labels
and then we want to minimize.

918
00:56:12,301 --> 00:56:15,270
So he had predicted characters
and then actual characters.

919
00:56:15,271 --> 00:56:19,290
And we'll minimize the loss using
cross entropy with low budgets.

920
00:56:19,350 --> 00:56:23,190
So why are we using cross entropy?
Because Cross entropy,

921
00:56:23,350 --> 00:56:26,010
a good optimizer for,
uh,

922
00:56:26,890 --> 00:56:30,880
multi-class classification, which is
what this is. We have a question from go.

923
00:56:30,881 --> 00:56:33,970
Cool. Who's seeing many
people you normally use relo?

924
00:56:34,060 --> 00:56:38,400
Why use sigmoid or Tan and the
reasons? Uh, good question. Uh,

925
00:56:38,410 --> 00:56:42,790
so [inaudible] is 10.
It is becoming more popular these days.

926
00:56:43,220 --> 00:56:48,190
Uh, why use raw over tan? Uh,
that's a good question. In fact,

927
00:56:48,220 --> 00:56:52,060
I should have a video comparing different
activation functions and when and why

928
00:56:52,061 --> 00:56:56,260
you should use them. In this case,
we're not using rally Lou because,

929
00:56:56,680 --> 00:57:01,090
uh, the Tan function
because we're using, uh,

930
00:57:01,300 --> 00:57:06,300
a softmax specifically because we're using
softmax cross entropy for multi-class

931
00:57:06,431 --> 00:57:08,520
classification. Generally, uh,

932
00:57:08,560 --> 00:57:11,530
10 h is paired with
multi-class classification,

933
00:57:11,710 --> 00:57:14,980
whereas rail Relo is, uh, with,

934
00:57:16,840 --> 00:57:21,670
sorry, Tan h is with multi-class
classification for recurrent networks,

935
00:57:21,671 --> 00:57:24,940
whereas [inaudible] is more often
used for convolutional networks.

936
00:57:25,180 --> 00:57:27,020
We have seen them in recurrent networks,
uh,

937
00:57:27,070 --> 00:57:29,770
but they're mostly used for
convolutional networks. And, uh,

938
00:57:29,830 --> 00:57:31,870
I'm actually gonna have a
video coming out on that soon.

939
00:57:32,680 --> 00:57:35,950
Then we're going to use the reduced
mean function to calculate the average

940
00:57:36,040 --> 00:57:38,500
probability.
And that is our loss.

941
00:57:38,590 --> 00:57:42,490
So given all of those probabilities
across the list of possible characters,

942
00:57:42,730 --> 00:57:44,740
we're going to compute the average.
Okay.

943
00:57:44,860 --> 00:57:48,040
And then we're going to that as our
loss and that is a loss that we want to

944
00:57:48,041 --> 00:57:50,290
minimize using gradient descent,

945
00:57:50,500 --> 00:57:54,890
given our global step values that
keeps track of all of the, uh,

946
00:57:57,700 --> 00:57:59,770
values. Okay. So then we're going to,

947
00:58:00,660 --> 00:58:01,493
sorry.

948
00:58:01,660 --> 00:58:06,370
So then we're going to train our model
to here. Is it for our training step?

949
00:58:07,720 --> 00:58:11,290
Hold on. So let me make sure
this is all good. So online hate,

950
00:58:12,660 --> 00:58:14,610
batch size,
char size.

951
00:58:16,700 --> 00:58:17,533
Hold on.

952
00:58:21,350 --> 00:58:23,930
Okay. So here's what it is. It's sad.

953
00:58:24,950 --> 00:58:29,210
Tf Low 32 batch size charts
has invalid syntax because?

954
00:58:31,640 --> 00:58:32,473
Because

955
00:58:42,920 --> 00:58:45,950
it is an omnium. See how many
people are here right now?
Okay. Three and four. Nine.

956
00:58:45,951 --> 00:58:50,780
Good. So we have an invalid batch
size and an invalid char size. And,

957
00:58:50,781 --> 00:58:55,390
uh, oh right. There we go.

958
00:58:55,660 --> 00:58:56,493
Great.

959
00:58:57,230 --> 00:59:00,890
Great. Okay, so now for our training
step, so I'm going to paste this in. Uh,

960
00:59:00,891 --> 00:59:05,780
so here's how, here's how it's going to
work. Uh, so we have our graph. Okay.

961
00:59:05,781 --> 00:59:09,050
And we're going to first
initialize our variables.

962
00:59:09,140 --> 00:59:12,860
So this is our global variable and then
an offset value that I'm going to talk

963
00:59:12,861 --> 00:59:14,100
about. Their real, we got to,

964
00:59:14,150 --> 00:59:17,150
the real things to talk about here
is the offset value and then a saver.

965
00:59:17,151 --> 00:59:21,490
Cause we're going to just save
our model every so often. Okay.

966
00:59:21,790 --> 00:59:26,020
And uh, so for each of our training
steps, for each of our training steps,

967
00:59:26,290 --> 00:59:30,130
we're going to start off as zero.
So this is our actual training steps.

968
00:59:30,131 --> 00:59:31,990
We are going to feed it in as batches.

969
00:59:31,991 --> 00:59:36,991
We are data and labels that we feed
in as batches and the offset helps us

970
00:59:37,031 --> 00:59:41,710
calculate what those batches are going to
be, the size of those batches. Okay. And,

971
00:59:41,730 --> 00:59:44,650
and there's, there's a calm next to
bat size that you're missing. Yeah,

972
00:59:44,651 --> 00:59:45,484
I added that.
Thanks.

973
00:59:46,730 --> 00:59:47,040
Okay.

974
00:59:47,040 --> 00:59:48,600
Up there. Uh, and so,

975
00:59:50,140 --> 00:59:50,973
okay,

976
00:59:51,040 --> 00:59:53,380
we want to feed it to the
model model iteratively.

977
00:59:53,440 --> 00:59:56,560
So for the first part we'll calculate
those batches and the labels.

978
00:59:56,860 --> 00:59:58,570
We'll update the offset until,

979
00:59:58,571 --> 01:00:02,470
and we're using that offset to make sure
that we have the right data at the end,

980
01:00:03,100 --> 01:00:06,220
uh, at the very end right here. Okay.

981
01:00:06,221 --> 01:00:08,790
And then we're going to optimize it with
our loss function and then we're going

982
01:00:08,791 --> 01:00:12,400
to feed it in as dictionaries,
batch data, batch labels in batches.

983
01:00:12,700 --> 01:00:14,530
We get our training loss,
uh,

984
01:00:14,590 --> 01:00:18,880
and we and the run functions going to
run our optimizer given our loss. Right?

985
01:00:19,020 --> 01:00:22,990
And it's going to every step.
And then remember that those that's saved.

986
01:00:22,991 --> 01:00:26,320
Every variable we use to to uh,

987
01:00:28,040 --> 01:00:28,870
okay.

988
01:00:28,870 --> 01:00:29,890
Calculate that.
So

989
01:00:36,460 --> 01:00:40,020
think there might be a semi colon that's
needed. It's what people are saying.

990
01:00:40,930 --> 01:00:41,320
Okay.

991
01:00:41,320 --> 01:00:44,920
Where though, tell us
where it guys, hold on.

992
01:00:45,630 --> 01:00:49,690
Do you have to have variables?
Initializer yeah, coming up on our,

993
01:00:50,200 --> 01:00:53,320
on the album coming up on the
hour, coming up on the hour. Okay.

994
01:00:53,710 --> 01:00:58,370
initializer.run.
You're not actually,

995
01:00:58,610 --> 01:01:02,480
let's move this for a second. So, okay.

996
01:01:02,570 --> 01:01:05,750
So this is actually going to take a while
to train. We're running up on the hour.

997
01:01:06,140 --> 01:01:10,170
Let's, what did I have trained right
here? So, so here's what happened.

998
01:01:10,171 --> 01:01:13,160
So when I give it this,
so here's the demo and then we're going,

999
01:01:13,161 --> 01:01:17,300
I'm going to show more demo code.
Uh, so when I give it this value,

1000
01:01:17,960 --> 01:01:19,550
I plan to make the world a better place.

1001
01:01:19,670 --> 01:01:21,740
It's going to initialize the
graph and load the model.

1002
01:01:21,890 --> 01:01:24,230
So for every character
in the input sentence,

1003
01:01:24,440 --> 01:01:27,890
it's going to generate the next
character for every character is going to

1004
01:01:27,891 --> 01:01:28,724
generate.

1005
01:01:31,080 --> 01:01:35,560
Then the next character and okay,
so it's character out of character,

1006
01:01:35,561 --> 01:01:39,490
character out of character at a time.
So this is actually pretty bad right here.

1007
01:01:39,491 --> 01:01:43,420
Why? Because I trained it for one
iteration before I started this session.

1008
01:01:43,630 --> 01:01:47,530
I plan to make the world a better
place by off, I guess parts. It's act,

1009
01:01:47,930 --> 01:01:51,070
it looks like Latin because we are
generating it with one iteration.

1010
01:01:51,071 --> 01:01:54,750
But what happens when we generate
it with lots of iterations? Okay,

1011
01:01:55,250 --> 01:01:57,940
here's the demo of what happens
when we generate it with this exact,

1012
01:01:58,060 --> 01:02:02,230
this exact code,
this exact type of code and LSTM network.

1013
01:02:02,530 --> 01:02:06,400
When we generate lots of iterations and
you can see what happens over time here.

1014
01:02:07,360 --> 01:02:12,340
Uh, where is it? Hold on. Come
on baby. Here's what it is.

1015
01:02:13,570 --> 01:02:16,510
So the evolution of samples,
while training is a good example.

1016
01:02:16,570 --> 01:02:19,260
So it's just like this, right? Just
like what I've just showed right here.

1017
01:02:19,440 --> 01:02:22,500
It starts off really bad like this.

1018
01:02:22,800 --> 01:02:26,850
And then with each iteration at 308 or so,
that was at 100 iterations.

1019
01:02:27,060 --> 01:02:32,040
At 300 iterations, it's more like two
hours. At 500, it's more like five hours.

1020
01:02:32,160 --> 01:02:37,160
And I'm talking about on a CPU that's
like 2.4 gigahertz and then 700 it gets

1021
01:02:39,061 --> 01:02:43,590
better and better. And by 2000
iterations it is amazing. Okay.

1022
01:02:43,591 --> 01:02:47,160
It gets better at spelling, it gets better
at everything. That's it for our code.

1023
01:02:47,430 --> 01:02:52,230
Uh, and the, uh, linked to that
is in the description and so yeah,

1024
01:02:52,260 --> 01:02:54,630
last five minute Q and a and
then we're done for the day.

1025
01:02:55,810 --> 01:02:56,570
Okay.

1026
01:02:56,570 --> 01:02:59,600
Qa time. Yes. And a time. Cool.

1027
01:02:59,601 --> 01:03:03,020
So we have a couple of things people
were asking about different live streams

1028
01:03:03,021 --> 01:03:04,160
that you want you're planning to do.

1029
01:03:04,161 --> 01:03:07,400
So party was asking you if
you're going to do a drone,

1030
01:03:07,520 --> 01:03:11,670
a drone image, trucking
livestream, and uh,

1031
01:03:11,750 --> 01:03:15,330
Ozzie Campos asking if you're going
to do a stream of bought seek to seek,

1032
01:03:15,980 --> 01:03:19,670
seek to suite. The sequencing
was, yeah. So for the drone,

1033
01:03:20,220 --> 01:03:23,960
that's going to be the robot video.
I'll do a drone video soon. Yeah.

1034
01:03:24,440 --> 01:03:28,040
For sequence a sequence. That's what I'm
in right now. Like I'm in the sequence,

1035
01:03:28,041 --> 01:03:30,200
the sequence world. We, we
made a sequence, the sequence,

1036
01:03:30,230 --> 01:03:33,070
I made a sequence sequence video last
week. I'm making one this weekend.

1037
01:03:33,110 --> 01:03:36,650
I'm going to make one next
week for chatbots. So yeah,
that's different sequences.

1038
01:03:36,651 --> 01:03:41,560
The sequence. Are you a human or AI?
I am a human. I am a human for now.

1039
01:03:41,690 --> 01:03:43,880
Neil Neil Sandy said
something really cool.

1040
01:03:43,881 --> 01:03:47,300
He says it's a great way to
build confidence is through
your math pep talk that

1041
01:03:47,301 --> 01:03:51,050
you give. So that's shout out to
you. Oh yes. Thank you Neil. Yes,

1042
01:03:51,260 --> 01:03:54,110
math is just the language
voice shop. Ya. Uh,

1043
01:03:54,140 --> 01:03:58,070
asked earlier if you could please do an
outline of the program at the end cause

1044
01:03:58,071 --> 01:04:01,030
he was kind of losing track of how you're
yeah, let me do it. Ending outline.

1045
01:04:01,060 --> 01:04:05,280
Great. Any other questions before I do
the outline? Um, I think that's about,

1046
01:04:05,320 --> 01:04:09,220
that's the good stuff.
Okay. So here's the outline.

1047
01:04:09,670 --> 01:04:11,290
We started off with our dependencies.

1048
01:04:11,320 --> 01:04:14,800
This is a fast outline and
then we imported our tax,

1049
01:04:14,801 --> 01:04:17,230
that's Wikipedia text and we said,

1050
01:04:17,260 --> 01:04:22,060
let's look at let's store only the unique
characters because we want to generate

1051
01:04:22,061 --> 01:04:25,720
unique characters so that we can have
these two dictionaries to map the

1052
01:04:25,721 --> 01:04:30,010
characters to ids and an ids to characters
so that we can use that later on, uh,

1053
01:04:30,040 --> 01:04:32,280
for training. Then we grow, uh, uh,

1054
01:04:32,710 --> 01:04:36,460
a sample method to generate
the probability of each
next character given the

1055
01:04:36,461 --> 01:04:38,800
predicted value for that
comes out of our model.

1056
01:04:39,220 --> 01:04:43,510
We vectorize our data using non pie that
we fed in and these are 50 character

1057
01:04:43,511 --> 01:04:46,630
long sentences. We fed
it into our model. Okay.

1058
01:04:46,650 --> 01:04:50,170
And our model is an LSTM network and
LSTM network has an input gate. Uh,

1059
01:04:50,171 --> 01:04:54,280
forget gate on Applegate and in memory
cell. We unrolled it through time.

1060
01:04:54,790 --> 01:04:57,520
Okay. Before, before we
unrolled it through time,

1061
01:04:57,521 --> 01:04:59,140
we wrote up the equations for that.

1062
01:04:59,141 --> 01:05:03,730
How data on how data flow through
an LSTM cell and rapid. Okay.

1063
01:05:05,020 --> 01:05:05,853
Hold on.

1064
01:05:10,180 --> 01:05:14,050
No sleep. That's exactly my life.
No sleep. So that's a perfect beat.

1065
01:05:19,800 --> 01:05:23,650
How has the ship, oh, check your check
your sound settings on your laptop.

1066
01:05:23,770 --> 01:05:24,690
So go gutter system profits.

1067
01:05:25,350 --> 01:05:27,510
Oh my God. Okay. Stupid.

1068
01:05:28,590 --> 01:05:32,340
See I will gladly rapid because we were
in the last two minutes. So let me just,

1069
01:05:33,030 --> 01:05:37,560
if the Internet decides to work,
could you just play some rap?

1070
01:05:37,620 --> 01:05:41,730
Like some hip hop beat? Yeah.
Like if this just starts.

1071
01:05:42,910 --> 01:05:43,610
Okay.

1072
01:05:43,610 --> 01:05:45,170
Um,
just whatever.

1073
01:05:49,480 --> 01:05:50,313
Okay.

1074
01:05:52,110 --> 01:05:54,870
And the notebook has in the description.

1075
01:06:03,070 --> 01:06:05,500
Okay. So the notebook is
in the description and a

1076
01:06:08,690 --> 01:06:11,790
want to get a ration before I started
this session. Oh, that's violence.

1077
01:06:11,800 --> 01:06:12,890
To make the world a better place.

1078
01:06:17,650 --> 01:06:22,640
Does any good. Okay. No,
stop that. Stop that.

1079
01:06:23,800 --> 01:06:25,750
Okay. Here we go. We're, we're, we,

1080
01:06:27,470 --> 01:06:31,860
we took our LSTM state. We
David and input date, we get up,

1081
01:06:31,930 --> 01:06:35,870
forget gate, we David and how cook
cakes. I'm David a memory cell.

1082
01:06:35,900 --> 01:06:37,670
I calculated that output state.

1083
01:06:38,090 --> 01:06:40,850
I took the forget gate and
multiplied by the state,

1084
01:06:40,910 --> 01:06:44,960
I give it an input gate and
give it a memory sale. Yo.

1085
01:06:45,230 --> 01:06:50,210
And then I got that output value.
I got an output state output and a state.

1086
01:06:50,660 --> 01:06:53,480
That's what I did for my LSTM cell.

1087
01:06:53,810 --> 01:06:56,720
I calculated my hair's always either.

1088
01:06:57,770 --> 01:07:00,590
I unrolled it through time.
I was saying,

1089
01:07:00,591 --> 01:07:05,270
here are all my outputs and here are
my labels. Calculate it in the sale,

1090
01:07:05,271 --> 01:07:07,230
man.
I'm like Charles babble.

1091
01:07:07,640 --> 01:07:12,620
I got a classifier given all of
those cells unroll through time.

1092
01:07:12,770 --> 01:07:15,440
I had our weights and our biases.
Man,

1093
01:07:15,500 --> 01:07:20,390
I'm going online with 350 people live.
I got my loss function,

1094
01:07:20,570 --> 01:07:25,340
um, minimized. It would cross
entropy. Ah, this is my function.

1095
01:07:25,430 --> 01:07:29,870
I optimize gradient descent.
Man, I'm online all the time.

1096
01:07:30,140 --> 01:07:34,460
At the end I ran my training function
given the states that came back,

1097
01:07:34,580 --> 01:07:36,500
we brought back,
propagate it through time.

1098
01:07:36,710 --> 01:07:41,510
And then at the end we calculated our
training loss. We optimize, we optimize.

1099
01:07:41,511 --> 01:07:46,220
All right. Hey, hey, here we go. Yo.
That's how we did. We saved the model.

1100
01:07:46,280 --> 01:07:50,660
All right though. We saved it every day.
I'm going crazy. I love rapping, man.

1101
01:07:50,661 --> 01:07:55,130
It's my day today.
Okay. Yeah. So hype man.

1102
01:07:55,440 --> 01:07:56,960
Let's give him some love at buddy

1103
01:07:58,430 --> 01:08:00,940
does these kinds of streams
where they show machine learned,

1104
01:08:00,941 --> 01:08:03,890
they teach machine learning and they
stream at the same time and they wrap.

1105
01:08:03,920 --> 01:08:04,790
That's pretty amazing.

1106
01:08:05,140 --> 01:08:09,160
Thank you. Wizard of Oz coming at you. We
are. I am here at the upload VR studios,

1107
01:08:09,161 --> 01:08:13,150
so thanks everybody for watching
this livestream, uh, for being here.

1108
01:08:13,180 --> 01:08:14,170
I'm going to show the,

1109
01:08:14,440 --> 01:08:16,880
I'm going to show that code and
the code is in the description.

1110
01:08:16,881 --> 01:08:20,210
I'm going to add comments and read me.
Stop to it within the hour.

1111
01:08:20,211 --> 01:08:23,240
Thanks for coming guys. I love
you guys. A little, five less,

1112
01:08:23,300 --> 01:08:28,240
five more shout outs to names.
Sal Load. Javier Anthony,
a good CEO. Anthony Sparks,

1113
01:08:28,260 --> 01:08:32,270
sporanox and James. Hey James.
Good to see you. All right. Uh,

1114
01:08:32,300 --> 01:08:35,350
for now I've got to go practice my,
uh,

1115
01:08:35,390 --> 01:08:38,660
everything because I want to get better
at everything. Bigger, better, faster,

1116
01:08:38,661 --> 01:08:42,230
stronger. Uh, so for, that's what I've
got to do. So thanks for watching.

1117
01:08:43,080 --> 01:08:43,913
Ooh.

1118
01:11:12,020 --> 01:11:17,020
[inaudible].

