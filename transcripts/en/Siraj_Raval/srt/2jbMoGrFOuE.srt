1
00:00:00,360 --> 00:00:03,870
Watson, you beat me again.
Can we just be friends? Yes.

2
00:00:05,250 --> 00:00:06,060
Hello world.

3
00:00:06,060 --> 00:00:11,060
It's Suraj and today we're going to
use IBM's Watson discovery service to

4
00:00:11,640 --> 00:00:16,640
instantly analyze a modern
machine learning paper so
that we can find any issues

5
00:00:17,191 --> 00:00:19,200
encountered by the researchers.

6
00:00:19,620 --> 00:00:24,620
Finding useful patterns and insights
in unstructured data is not easy.

7
00:00:25,350 --> 00:00:30,350
We've continually improve the algorithms
we can use to do so from simple linear

8
00:00:30,571 --> 00:00:34,260
regression models to buy
directional recurrent networks,

9
00:00:34,470 --> 00:00:36,240
but there's still a big problem.

10
00:00:36,540 --> 00:00:40,590
Data preprocessing meaning acquiring,
normalizing,

11
00:00:40,591 --> 00:00:42,840
and organizing unstructured data.

12
00:00:43,020 --> 00:00:48,020
It's something that data scientists
spend as much as 80% of their time on.

13
00:00:48,480 --> 00:00:50,760
If we automate this process,

14
00:00:50,820 --> 00:00:55,820
that frees up much more time to focus on
types of models to use and analyze all

15
00:00:56,791 --> 00:01:00,750
the different results in a
rapid experimentation pipeline.

16
00:01:00,930 --> 00:01:05,930
IBM's Watson Discovery Service is
a great solution to this problem.

17
00:01:06,300 --> 00:01:11,300
It packages core Watson Api APIs like
natural language understanding and

18
00:01:11,640 --> 00:01:16,640
document conversion with simple tooling
that enables us to seamlessly upload and

19
00:01:17,731 --> 00:01:22,290
rich and index large collections
of private or public data.

20
00:01:22,590 --> 00:01:26,130
You don't even need engineering
or machine learning skills.

21
00:01:26,280 --> 00:01:27,870
With the web interface.

22
00:01:27,930 --> 00:01:32,930
Anyone can analyze enormous collections
of unstructured data in minutes.

23
00:01:33,450 --> 00:01:38,340
Discovery uses natural language processing
to allow for analysis for a wide

24
00:01:38,341 --> 00:01:42,510
range of document types,
including html, pdf, and word.

25
00:01:42,780 --> 00:01:47,520
We can also use it to find time based
correlations in data or even identify

26
00:01:47,521 --> 00:01:52,320
locations and geospatial coordinates
to uncover spatial correlations.

27
00:01:52,710 --> 00:01:57,540
It has quite a lot of capabilities that
when combined together offer powerful

28
00:01:57,541 --> 00:02:01,620
insights. For example, if
we're a payments API company,

29
00:02:01,800 --> 00:02:06,800
we can use Watson discovery to identify
all the companies acquired by one of our

30
00:02:07,621 --> 00:02:12,621
competitors over a specific period of
time so we can make more strategic growth

31
00:02:13,201 --> 00:02:16,710
decisions.
If we're a pharmaceutical company,

32
00:02:16,860 --> 00:02:21,860
we can identify the diseases that were
impacted by a target's chemical compound

33
00:02:22,560 --> 00:02:24,480
or if we're working at a shipping company,

34
00:02:24,570 --> 00:02:28,620
we can analyze which events specifically
impact our supply chain or shipping

35
00:02:28,621 --> 00:02:33,360
routes. This kind of data is embedded
in social media, news reports,

36
00:02:33,390 --> 00:02:34,350
legal documents,

37
00:02:34,351 --> 00:02:39,180
and other public or private data as
relationships and text discoveries.

38
00:02:39,181 --> 00:02:43,650
Ability to sift through hundreds of
thousands of relationships to provide

39
00:02:43,651 --> 00:02:45,570
insights is a powerful tool.

40
00:02:45,930 --> 00:02:50,630
The way the pipeline looks is that we
first give discovery our Dataset in

41
00:02:50,640 --> 00:02:52,500
whatever file format it's in.

42
00:02:52,800 --> 00:02:57,800
Then it will convert and enrich that data
using several Watson Api APIs that use

43
00:02:58,231 --> 00:03:02,080
natural language processing to
add metadata to the content,

44
00:03:02,290 --> 00:03:04,450
making it easily searchable as well.

45
00:03:04,690 --> 00:03:07,510
It will also clean and normalize the data.

46
00:03:07,720 --> 00:03:09,700
Once the data is normalized,

47
00:03:09,760 --> 00:03:13,990
it's indexed into a collection as
part of our environment in the cloud.

48
00:03:14,350 --> 00:03:19,150
Then we can query the data to see if
we get any actionable insights from the

49
00:03:19,180 --> 00:03:20,410
output we receive.

50
00:03:20,470 --> 00:03:25,210
So let's try out a demo of this service
for ourselves using a machine learning

51
00:03:25,211 --> 00:03:26,830
paper as our input.

52
00:03:27,280 --> 00:03:30,730
Our first step is to create
a free IBM cloud account.

53
00:03:30,940 --> 00:03:33,760
This is what allows us
to access the service.

54
00:03:34,060 --> 00:03:38,920
Then we'll create an instance of the
discovery service so that we can create an

55
00:03:38,921 --> 00:03:43,921
environment with one or
more data collections that
we will add our content to

56
00:03:45,160 --> 00:03:47,260
and associate with the configuration.

57
00:03:47,620 --> 00:03:50,620
We can click catalog
in the navigation bar.

58
00:03:50,650 --> 00:03:53,440
Then on Watson and the Watson Menu,

59
00:03:53,441 --> 00:03:58,441
we'll find our discovery service and once
there we can click create to wait for

60
00:03:58,991 --> 00:04:02,680
an instance to be created.
After a few seconds,

61
00:04:02,681 --> 00:04:05,680
our new service will be
listed on the dashboard.

62
00:04:05,950 --> 00:04:09,520
We can click on it to open
the discovery service page.

63
00:04:09,760 --> 00:04:12,640
We'll see a complete set
of online tools here,

64
00:04:12,641 --> 00:04:17,641
which we can use to set up an instance
of the service and populate it with data.

65
00:04:18,640 --> 00:04:22,720
We don't need to use API Apis to
configure and populate our service.

66
00:04:23,080 --> 00:04:25,690
We can do it all from the web dashboard.

67
00:04:26,020 --> 00:04:30,580
The content we upload is stored in a
collection that's part of an environment,

68
00:04:30,760 --> 00:04:32,950
but before we upload our content,

69
00:04:33,130 --> 00:04:36,280
we need to create the
environment and collection.

70
00:04:36,790 --> 00:04:41,530
A collection is a logical division of
our data in the environment when we

71
00:04:41,531 --> 00:04:45,670
deliver results,
each collection is query independently.

72
00:04:45,730 --> 00:04:49,780
We'll name our collection machine
learning issues. Once created,

73
00:04:49,781 --> 00:04:54,520
we can upload our documents by uploading
our research paper to our collection.

74
00:04:55,000 --> 00:04:56,890
Once the upload is finished,

75
00:04:56,920 --> 00:05:01,630
we'll see the document count has increased
to one insights from this data will

76
00:05:01,631 --> 00:05:06,280
be listed on the collection page
and we can already see some relevant

77
00:05:06,310 --> 00:05:09,760
information.
Now we're ready to query the data.

78
00:05:10,030 --> 00:05:14,050
We can select our collection from
the discovery tooling main page.

79
00:05:14,380 --> 00:05:18,670
At the top of the collection page,
we'll click on view data schema.

80
00:05:18,820 --> 00:05:23,650
Then we'll click the build queries
link at the top of the data schema page

81
00:05:23,890 --> 00:05:25,870
followed by search for documents.

82
00:05:25,900 --> 00:05:30,460
We want to find out what the issues were
from the paper so we can use natural

83
00:05:30,461 --> 00:05:32,890
language to query for the issues.

84
00:05:33,130 --> 00:05:36,370
We can be as vague or as
specific as we want here.

85
00:05:36,550 --> 00:05:40,300
The more specific we are,
the more targeted the results will be.

86
00:05:40,570 --> 00:05:41,950
When we run the query,

87
00:05:41,951 --> 00:05:46,750
we can click the Jason Table to see
that Jason responses that were returned.

88
00:05:47,110 --> 00:05:48,490
We can see a passage here.

89
00:05:48,550 --> 00:05:53,260
Passages are short excerpts that are
extracted from the full documents that our

90
00:05:53,261 --> 00:05:54,430
query returns.

91
00:05:54,760 --> 00:05:59,760
It seems like the main issue was a very
training time for this type of model,

92
00:06:00,080 --> 00:06:02,210
which confirms my hypothesis.

93
00:06:02,240 --> 00:06:05,090
This is going to make reviewing
papers so much easier.

94
00:06:05,240 --> 00:06:10,240
We can also see some generated semantic
analysis such as sentiment entities,

95
00:06:10,550 --> 00:06:15,230
categories and concepts are
document was converted to Jason,

96
00:06:15,350 --> 00:06:17,360
then enriched and indexed.

97
00:06:17,570 --> 00:06:22,570
By enrichment I mean adding Metadata as
such that Watson functions like entity

98
00:06:22,611 --> 00:06:23,444
extraction,

99
00:06:23,540 --> 00:06:28,130
concept tagging and sentiment analysis
can parse the data accordingly and if we

100
00:06:28,131 --> 00:06:32,120
want we can also access
Watson discovery via the API.

101
00:06:32,120 --> 00:06:36,740
Like in this example it's wrapped via
the python library and we can communicate

102
00:06:36,741 --> 00:06:41,270
with it directly. There are three
things to remember from this video.

103
00:06:41,450 --> 00:06:46,450
IBM's Watson Discovery Service allows
developers to unlock insights hidden in

104
00:06:47,150 --> 00:06:48,320
unstructured data.

105
00:06:48,590 --> 00:06:53,590
We can use it for both public and private
data sets and it uses natural language

106
00:06:54,171 --> 00:06:59,171
processing to help easily extract
semantic elements from data like sentiment

107
00:06:59,480 --> 00:07:03,920
entities and concepts. I've got a
coding challenge for you this week.

108
00:07:03,921 --> 00:07:08,921
Wizards use IBM's discovery service to
build an APP that lets a user analyze the

109
00:07:10,161 --> 00:07:13,640
sentiment around any given
topic that they searched for.

110
00:07:13,880 --> 00:07:17,840
Details are in the get hub link in the
video description and I'll give a shout

111
00:07:17,841 --> 00:07:20,600
out to the top two entries in a week.
Good luck.

112
00:07:21,080 --> 00:07:23,870
Please subscribe for more
programming videos. And for now,

113
00:07:24,140 --> 00:07:27,430
I've got to discover a new API,
so thanks for watching.

