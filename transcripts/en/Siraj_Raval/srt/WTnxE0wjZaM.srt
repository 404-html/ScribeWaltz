1
00:00:00,090 --> 00:00:03,840
Hello world, it's Saroj. And what
is the future of deep learning?

2
00:00:03,870 --> 00:00:05,640
That's the topic for today,

3
00:00:05,910 --> 00:00:08,820
inspired by something that
Geoffrey Hinton recently said.

4
00:00:09,180 --> 00:00:11,940
This talk is going to be
divided into three parts.

5
00:00:12,090 --> 00:00:14,700
I'm going to talk about
how backpropagation works,

6
00:00:14,940 --> 00:00:18,780
what the most popular deep
learning algorithms are right now.

7
00:00:18,990 --> 00:00:23,670
And finally seven research directions
that I have personally handpicked. Okay,

8
00:00:23,671 --> 00:00:26,160
that's the talk in three parts.
So let's get started here.

9
00:00:26,430 --> 00:00:31,260
So this whole video was inspired by
what Geoffrey Hinton recently said in an

10
00:00:31,261 --> 00:00:35,220
article. So Geoffrey Hinton is
the godfather of neural networks.

11
00:00:35,221 --> 00:00:39,990
He's the guy who invented
the backpropagation algorithm
back in [inaudible] 86

12
00:00:40,050 --> 00:00:44,430
which is the work horse of all,
almost almost all deep learning. Okay?

13
00:00:44,580 --> 00:00:46,470
So without back propagation,

14
00:00:46,620 --> 00:00:50,070
all the great things we're seeing in deep
learning would not be possible today.

15
00:00:50,190 --> 00:00:54,330
Self driving cars, image
classification, language translation,

16
00:00:54,331 --> 00:00:57,420
almost all of it is
because of backpropagation.

17
00:00:57,690 --> 00:01:02,690
So this what Hinton recently said is
causing shockwaves in the deep learning

18
00:01:02,911 --> 00:01:03,744
community.

19
00:01:03,780 --> 00:01:08,520
He said that he was deeply suspicious
of backpropagation his own algorithm and

20
00:01:08,521 --> 00:01:12,540
said, my view is to throw
it all away and start again.

21
00:01:12,930 --> 00:01:16,530
And I have to say that
I agree with Hinton.

22
00:01:16,770 --> 00:01:21,300
I know it's crazy, right? Because
backpropagation has just given us so much.

23
00:01:21,540 --> 00:01:25,050
But if we really want to get to
artificial general intelligence,

24
00:01:25,380 --> 00:01:29,400
we've got to do something that's more
complicated or just something else

25
00:01:29,401 --> 00:01:34,401
entirely because it's not just about
stacking layers and then back propagating

26
00:01:34,411 --> 00:01:38,100
some Eric gradient recursively that's
not going to get us to consciousness.

27
00:01:38,101 --> 00:01:42,900
That's not going to get us to systems
that are able to learn a huge variety of

28
00:01:42,901 --> 00:01:43,500
tasks.

29
00:01:43,500 --> 00:01:47,520
Everything from playing games to piloting
an aircraft to figuring out the most

30
00:01:47,521 --> 00:01:49,290
complex equations in the universe.

31
00:01:49,580 --> 00:01:54,120
It's gotta be about more than just
gradient based optimization. So,

32
00:01:54,380 --> 00:01:59,250
uh, let's first start. Let's first talk
about how backpropagation works. Okay,

33
00:01:59,251 --> 00:02:02,820
so the billion dollar question is
that it's probably multibillion dollar

34
00:02:02,821 --> 00:02:07,200
actually. How does the brain
learn so well from sparse,

35
00:02:07,340 --> 00:02:10,350
unlabeled data? That's how
we learn. We don't sit there,

36
00:02:10,360 --> 00:02:13,470
we don't have labels for everything
we're learning. If you look at a baby,

37
00:02:13,560 --> 00:02:18,000
it's incredible how much it learns
without some kind of supervision, right?

38
00:02:18,150 --> 00:02:20,550
It can learn how to do
all these different tasks,

39
00:02:20,640 --> 00:02:24,030
stack blocks in all these
little to learn how to speak.

40
00:02:24,270 --> 00:02:27,990
And there are no labels per se in the
sense that we use them in deep learning.

41
00:02:28,380 --> 00:02:32,580
It's all happening unsupervised. And
when up and sparse means few, right?

42
00:02:32,581 --> 00:02:37,140
So it's not about very dense
descriptive data. It's sparse, right?

43
00:02:37,141 --> 00:02:40,230
There's a lot of zeroes in it and
yet we can still learn from it.

44
00:02:40,260 --> 00:02:44,460
So how does it do this? Well, let's first
understand how backpropagation works.

45
00:02:44,700 --> 00:02:48,540
So in backpropagation,
so first of all,

46
00:02:48,541 --> 00:02:52,230
a neural network is a
huge composite function.

47
00:02:52,260 --> 00:02:54,990
It's a function consisting
of other functions.

48
00:02:55,200 --> 00:02:57,870
And these functions are all in
the form health layers, right?

49
00:02:57,871 --> 00:03:01,810
So you've got to input layer, a hidden
layer, maybe multiple hidden layers.

50
00:03:01,811 --> 00:03:03,280
And then finally an output layer.

51
00:03:03,490 --> 00:03:07,180
And you can look at it as this four
step process that I've got right here.

52
00:03:07,330 --> 00:03:11,590
So the first step is to receive a new
observation acts and a target label. Why?

53
00:03:11,740 --> 00:03:16,740
So this could be some image of a
cancer cell and then the label cancer,

54
00:03:17,770 --> 00:03:19,750
right? And it could be
either cancer or not cancer.

55
00:03:19,960 --> 00:03:23,410
And so you'll take that input and you
already have the label, right? That's,

56
00:03:23,560 --> 00:03:27,970
that's how backpropagation works. As long
as you know that label, you are golden,

57
00:03:28,000 --> 00:03:32,080
but you've got to know the label. So
you take that input, it's an image.

58
00:03:32,380 --> 00:03:35,950
Think of it as a series of pixels.
So it's just a huge group of numbers.

59
00:03:35,980 --> 00:03:37,030
So it's a vector,
right?

60
00:03:37,031 --> 00:03:40,750
So you take that group of numbers
and you then you go to step two,

61
00:03:40,751 --> 00:03:43,080
you feed it forward through the layers.
What do,

62
00:03:43,270 --> 00:03:47,920
what do I mean by feeding it forward?
You, you continually take that input,

63
00:03:48,100 --> 00:03:51,310
multiply it by some weight value,
add of bias value,

64
00:03:51,370 --> 00:03:54,430
and then activate it by
applying a nonlinearity to it.

65
00:03:54,640 --> 00:03:58,810
And you continually do that over and
over again until you have an output

66
00:03:58,811 --> 00:04:01,020
prediction. And I'm going to
go over this more in a second.

67
00:04:01,070 --> 00:04:05,350
We're going to look at the code,
but once you have that output prediction,

68
00:04:05,530 --> 00:04:07,770
you compare it to your actual prediction.

69
00:04:07,960 --> 00:04:12,740
Real label by doing a difference,
the subtraction, right? You, you,

70
00:04:12,750 --> 00:04:16,300
you're subtracting the actual from
the, from the predicted, right?

71
00:04:16,390 --> 00:04:19,870
And that that difference cause this,
these are all just numerical values.

72
00:04:20,050 --> 00:04:24,430
That difference is your error. And then
you go to the last part, step four,

73
00:04:24,670 --> 00:04:28,030
back propagating that error.
So once you have that error,

74
00:04:28,150 --> 00:04:31,630
you're going to compute the partial
derivative of that error with respect to

75
00:04:31,660 --> 00:04:34,570
each weight recursively for every layer.

76
00:04:34,750 --> 00:04:38,350
So you'll compete the partial derivative
with respect to the layer before.

77
00:04:38,500 --> 00:04:40,420
And then you take that
error gradient that you,

78
00:04:40,421 --> 00:04:43,360
that you just calculate it and use it
to compute the partial derivative with

79
00:04:43,361 --> 00:04:46,150
respect to the next layer.
And you'll keep doing that every time.

80
00:04:46,390 --> 00:04:50,920
And then what happens is you're going to
have a set of gradient values that you

81
00:04:50,921 --> 00:04:55,210
can then use to update all the weight's
in your network for as many as there are.

82
00:04:55,360 --> 00:04:59,980
So the process is input, feed
forward, get the error back,

83
00:04:59,981 --> 00:05:04,420
propagate uptick, the weights, repeat
feedforward, get the error back,

84
00:05:04,421 --> 00:05:05,020
propagate,

85
00:05:05,020 --> 00:05:09,310
repeat over and over and over and over
again a hundred thousand million times.

86
00:05:09,310 --> 00:05:11,920
Right?
And that is the backpropagation algorithm.

87
00:05:11,921 --> 00:05:14,500
We're going to go into it in more
detail, but that's at a high level. Okay.

88
00:05:14,770 --> 00:05:18,490
So the paper that I'm talking about
where hints and released this,

89
00:05:18,640 --> 00:05:21,940
I've got it linked to right here, but
it's a very old paper. It's, it was,

90
00:05:21,941 --> 00:05:26,050
it was done in 86 and the reason it was
created an 86 and the reason that Hinton

91
00:05:26,051 --> 00:05:30,640
is such a gene is because everybody was
telling him this is not going to work.

92
00:05:30,760 --> 00:05:34,450
You've got to think of something else.
But Hinton held strong to this belief,

93
00:05:34,480 --> 00:05:37,840
okay? And I think that is the
mark of a good researcher.

94
00:05:37,841 --> 00:05:42,520
If you really believe in something to
not let anything else influence what you

95
00:05:42,521 --> 00:05:45,880
believe in, right? Stick to your
belief. If you're wrong, you're wrong.

96
00:05:45,881 --> 00:05:48,700
But at least you stuck to
what you believed in and you,

97
00:05:48,701 --> 00:05:52,060
and you listen to other opinions as well.
But you really,

98
00:05:52,090 --> 00:05:55,420
you really believe in something.
So the reason that it works now,

99
00:05:55,421 --> 00:05:59,780
and it didn't work in the 80s is because
we have the computing power and the

100
00:05:59,781 --> 00:06:03,810
data necessary to have
these huge, amazing, uh,

101
00:06:03,860 --> 00:06:06,950
classifications and these
amazing generations, right?

102
00:06:06,951 --> 00:06:09,590
For classification and
generative models for both.

103
00:06:09,830 --> 00:06:14,000
So let's just look at this
canonical example of a very
basic neural network that

104
00:06:14,001 --> 00:06:17,720
uses backpropagation scratch out. The
input number four is just three inputs,

105
00:06:17,721 --> 00:06:20,750
right? So we have, it's a three layer
network and put hidden and output.

106
00:06:20,751 --> 00:06:24,200
I'm going to go through this kind of
fast because we were going to get to what

107
00:06:24,201 --> 00:06:26,720
really matters in a second here.
And if you've already,

108
00:06:26,810 --> 00:06:29,990
if you already know how backpropagation
works, just skip forward probably,

109
00:06:29,991 --> 00:06:31,460
I'm going to estimate
five minutes from now.

110
00:06:32,300 --> 00:06:37,300
So we have this very simple basic neural
network and the goal is to take some

111
00:06:37,551 --> 00:06:41,750
input data and then predict
the output label, right? So
we've got some input data,

112
00:06:41,900 --> 00:06:46,520
which is a series of triplets, zero zero
one zero one one you know, et cetera.

113
00:06:46,700 --> 00:06:51,020
And then we have a set of associated
labels, zero one one zero.

114
00:06:51,021 --> 00:06:53,810
So four zero zero one the
associated output label,

115
00:06:53,811 --> 00:06:56,780
why is zero and then et
Cetera, et Cetera, right?

116
00:06:56,781 --> 00:06:59,150
So those are our inputs and our outputs.

117
00:06:59,330 --> 00:07:01,910
We have this nonlinearity
which is a sigmoid function.

118
00:07:02,060 --> 00:07:05,900
It's an activation function. And,
uh, I'll talk about that in a second,

119
00:07:05,901 --> 00:07:07,310
but we'll take our input data.

120
00:07:07,311 --> 00:07:11,150
We'll take our output data and we want
to learn the mapping function rights.

121
00:07:11,151 --> 00:07:12,470
And then given some new output,

122
00:07:12,620 --> 00:07:16,040
zero one one or one zero
one some arbitrary triplet,

123
00:07:16,220 --> 00:07:20,900
we'll be able to correctly predict the
output label as it should be, right?

124
00:07:20,901 --> 00:07:23,360
So the first step is for us to
initialize our weight values.

125
00:07:23,510 --> 00:07:26,740
So our weight values are a set,
are both matrices, the are,

126
00:07:26,780 --> 00:07:28,970
they are randomly initialized matrices.

127
00:07:29,390 --> 00:07:32,450
And so what happens is when we
have our input, right? So again,

128
00:07:32,451 --> 00:07:34,880
remember just scratch out the fourth one.
It's cause there's only three.

129
00:07:34,881 --> 00:07:39,740
It's a triplet. We'll take that input
triplet, multiply it by the weights,

130
00:07:39,770 --> 00:07:43,430
and those are the matrices. And so
that's why you see these arrows, right?

131
00:07:43,460 --> 00:07:45,100
The reason we're using,
uh,

132
00:07:45,380 --> 00:07:50,240
they say we need linear Algebra in deep
learning is because the linear Algebra

133
00:07:50,360 --> 00:07:54,470
takes the standard Algebra operations
like multiplication and division and

134
00:07:54,471 --> 00:07:58,610
addition. And it applies it to groups
of numbers that are called matrices.

135
00:07:58,820 --> 00:08:03,230
So Linear Algebra defines operations
that we can apply to groups of numbers,

136
00:08:03,231 --> 00:08:07,460
matrices, for example, the dot product,
which is used heavily in deep learning.

137
00:08:07,490 --> 00:08:11,660
That's in fact that is the multiplication
we use in all types of deep neural

138
00:08:11,661 --> 00:08:15,380
networks. It's a way of multiplying
groups of numbers together,

139
00:08:15,410 --> 00:08:16,460
which is what we're doing,
right?

140
00:08:16,460 --> 00:08:19,880
We're taking our input and multiplying
by a weight matrix and then we take that

141
00:08:19,881 --> 00:08:24,230
result and we add some bias value
in a bias acts as our anchor.

142
00:08:24,231 --> 00:08:28,490
It's a way for us to have some kind of
baseline where we don't go below that in

143
00:08:28,491 --> 00:08:31,550
terms of a graph,
think of like y equals mx plus B.

144
00:08:31,700 --> 00:08:35,540
It's kind of like the y intercept for
this function that we are trying to learn,

145
00:08:35,541 --> 00:08:37,070
right?
And once we,

146
00:08:37,071 --> 00:08:41,870
we've multiplied our input times our
weight value added a bias and then applied

147
00:08:41,871 --> 00:08:43,310
some activation function to it,

148
00:08:43,311 --> 00:08:47,960
which is our nonlinearity the sigmoid
that's going to give us an output and we

149
00:08:47,961 --> 00:08:52,070
just take that output and do the same
process for the next layer and the next

150
00:08:52,071 --> 00:08:55,130
layer and however many layers we have.
Okay.

151
00:08:55,140 --> 00:08:57,150
So that's what we're going
to do using the dot product.

152
00:08:57,180 --> 00:09:00,840
And then we're going to back propagate
the error once we compute it.

153
00:09:00,900 --> 00:09:02,580
So back propagation,

154
00:09:02,790 --> 00:09:05,910
you don't need to know all of calculus
to understand backpropagation.

155
00:09:06,090 --> 00:09:10,170
You only need to know really three
concepts from calculus, the derivative,

156
00:09:10,320 --> 00:09:13,740
the partial derivative, and the chain
rule, which I'll go through in a second.

157
00:09:13,860 --> 00:09:15,690
So first I'll go through the derivative.

158
00:09:15,720 --> 00:09:20,070
So the derivative is the slope of the
tangent line of a function at some point.

159
00:09:20,370 --> 00:09:24,630
And the uneasy way to compute
the derivative for some
function like say y equals

160
00:09:24,631 --> 00:09:28,980
x squared or any function, is to compute
the power rule, which I have right here.

161
00:09:29,130 --> 00:09:32,790
So you'll take the exponent and you'll
subtract one from it and you'll take the

162
00:09:32,791 --> 00:09:37,110
original exponent and move it to the
coefficient. So for y equals x squared,

163
00:09:37,230 --> 00:09:40,620
you take the to move it to
the coefficient, subtract one.

164
00:09:40,621 --> 00:09:43,800
So then it becomes two x to the first,
which is two x.

165
00:09:43,920 --> 00:09:48,120
So the derivative of y equals x
squared is two x. So the reason,

166
00:09:48,150 --> 00:09:50,670
and so the derivative tells
us the rate of change.

167
00:09:50,790 --> 00:09:55,790
It tells us how fast some function is
changing and what what is happening for,

168
00:09:57,710 --> 00:10:01,320
for gradient based optimization in
neural networks and all of deep learning.

169
00:10:01,530 --> 00:10:04,150
Most of deep learning is we have some if,
if,

170
00:10:04,160 --> 00:10:07,650
if we were to map out all of the
possible error values on the x axis.

171
00:10:07,651 --> 00:10:11,340
So just imagine these are all errors and
then all of the possible weight values

172
00:10:11,341 --> 00:10:14,730
on the y axis, it would come out
to be a parabola just like this.

173
00:10:14,940 --> 00:10:17,610
And what we want to do is we want
to find that minimum error value.

174
00:10:17,611 --> 00:10:20,760
We want to find those wait values such
that it's going to give us the minimum

175
00:10:20,761 --> 00:10:25,170
air value. And what that means is we want
to find the minimum of that parabola.

176
00:10:25,410 --> 00:10:28,920
And the way we're going to
find that minimum of the
parabola is by computing the

177
00:10:28,921 --> 00:10:29,491
derivative,

178
00:10:29,491 --> 00:10:33,480
which tells us the rate of
change of wherever we are
and then we're going to use

179
00:10:33,481 --> 00:10:38,070
it to update our weights such that
we are iteratively, incrementally,

180
00:10:38,370 --> 00:10:42,660
continuously moving closer and closer
and closer and closer to that minimum

181
00:10:42,661 --> 00:10:44,640
point.
And once we have that minimum point,

182
00:10:44,641 --> 00:10:48,240
that is our optimal point where the
error is smallest and the weights are at

183
00:10:48,241 --> 00:10:51,840
their most optimal values such that
the errors in the me, the smallest,

184
00:10:51,870 --> 00:10:54,210
every time we make a prediction,
right?

185
00:10:54,270 --> 00:10:56,970
That's why it's called gradient
descent in general, right?

186
00:10:56,971 --> 00:11:01,530
So when we take this very, very popular
optimization formula, gradient descent,

187
00:11:01,560 --> 00:11:05,070
which I just described,
and we apply it to deep neural networks,

188
00:11:05,160 --> 00:11:07,740
we call it back propagation,
right?

189
00:11:07,741 --> 00:11:12,180
Because we are back propagating an error
gradient across every layer that we

190
00:11:12,181 --> 00:11:13,014
have.

191
00:11:13,530 --> 00:11:17,300
And so the reason we know we need to
know the derivative is because we're,

192
00:11:17,810 --> 00:11:21,030
because we're going to what we're actually
computing is the partial derivative

193
00:11:21,330 --> 00:11:24,780
derivative. Because a neural network
doesn't just have one variable.

194
00:11:24,781 --> 00:11:29,700
It has several variables, right? For
however, however complex your function is.

195
00:11:30,030 --> 00:11:30,960
So,
uh,

196
00:11:30,990 --> 00:11:34,920
we want to compute the partial derivative
of the error with respect to each

197
00:11:34,921 --> 00:11:38,010
weight, right? So when
I say with respect to,

198
00:11:38,011 --> 00:11:40,260
I'm talking about that way
and none of the others.

199
00:11:40,500 --> 00:11:43,410
So we'll you can think of a
partial derivative as saying, okay,

200
00:11:43,411 --> 00:11:47,250
well what is a partial derivative
with respect to x for this equation?

201
00:11:48,660 --> 00:11:51,930
What that means is we are only computing
the power rule for x and we are

202
00:11:51,931 --> 00:11:53,530
ignoring everything else.

203
00:11:53,680 --> 00:11:57,820
So y to the fourth is ignored and
when we compute the derivative of x,

204
00:11:57,970 --> 00:12:01,000
it's going to be one, right?
So then we are left with five.

205
00:12:01,001 --> 00:12:04,360
Why now if we're doing the partial
derivative with respect to you, why,

206
00:12:04,450 --> 00:12:05,560
then we don't care about x.

207
00:12:05,561 --> 00:12:09,490
We only care about why we do the power
rule for why to the forest and it's for y

208
00:12:09,500 --> 00:12:13,960
cubed plus five x because y
the derivative of y is one.

209
00:12:13,961 --> 00:12:16,480
So then x remains.
So that's what we're competing.

210
00:12:16,481 --> 00:12:19,690
We're competing the partial derivative
and that's what's going to give us our

211
00:12:19,691 --> 00:12:22,390
air gradient.
The gradient tells us how,

212
00:12:22,420 --> 00:12:26,440
which direction to move on that
parabola to get to that optimal,

213
00:12:26,470 --> 00:12:30,940
that minimum point gradient descent. And
the last part is the chain rule, right?

214
00:12:30,941 --> 00:12:33,910
Because a neural network is a
giant composite function, right?

215
00:12:34,110 --> 00:12:38,240
But what did I just describe? I describe
taking an input value multiplying it.

216
00:12:38,241 --> 00:12:42,040
So input times weight at a bias activate,
right? We've talked about this before.

217
00:12:42,220 --> 00:12:46,660
Input Times, wait at abayas activate.
That is the formula that is happening.

218
00:12:46,661 --> 00:12:49,900
That is the function, right?
That is happening at every layer.

219
00:12:51,340 --> 00:12:54,400
And these layers are nested.
So every time you add a layer,

220
00:12:54,490 --> 00:12:58,490
you're adding a nested function inside
of this giant composite function that is

221
00:12:58,491 --> 00:13:00,370
the,
that is the neural network.

222
00:13:00,640 --> 00:13:05,320
So the chain rule tells us how
to derive a composite function.

223
00:13:05,650 --> 00:13:09,250
What you would do for a composite
function is derive the outside,

224
00:13:09,400 --> 00:13:13,120
keep the inside and multiply it
by the derivative of the inside.

225
00:13:13,300 --> 00:13:17,860
So that is a rule and that rule applies
recursively for as many nested functions

226
00:13:17,861 --> 00:13:20,260
as you have.
So that's the chain rule.

227
00:13:20,500 --> 00:13:24,610
And so now that we understand that we
can do back propagation, that there's a,

228
00:13:24,611 --> 00:13:27,010
there's your calculus primer
on doing backpropagation.

229
00:13:27,340 --> 00:13:31,450
So the rest of this very canonical
example is saying for 60,000 iterations,

230
00:13:31,570 --> 00:13:35,470
let's feed forward that input
data through each layer.

231
00:13:35,650 --> 00:13:38,350
And so what we do for each layer
and say, okay, we've got k zero,

232
00:13:38,351 --> 00:13:42,390
that's our input multiplied by
the first synapse matrix. Uh,

233
00:13:42,910 --> 00:13:44,920
by multiply,
I'm talking about doc product.

234
00:13:44,950 --> 00:13:49,330
Thank you num Pi apply the activation
function or nonlinearity to it.

235
00:13:49,570 --> 00:13:53,200
So the activation function and the reason
we do that is because a neural network

236
00:13:53,230 --> 00:13:57,820
is a universal function. Approximator
I am, I am telling you a lot right now.

237
00:13:57,821 --> 00:14:01,060
So just uh, don't worry if you
don't understand everything,

238
00:14:01,150 --> 00:14:04,180
there's a lot more to come and then
rewatch this video and I've got a million

239
00:14:04,181 --> 00:14:07,510
other videos on this stuff as well. So
I'm very excited right now. Where was I?

240
00:14:07,870 --> 00:14:12,010
Okay. So we were taking the input
times the weight. We're at input time.

241
00:14:12,040 --> 00:14:15,400
And so in this case we don't have a
bias, right? Because this is very basic,

242
00:14:15,401 --> 00:14:19,510
but usually we have a bias.
So we're doing input times wait,

243
00:14:19,930 --> 00:14:24,670
activate, okay, bye. Times. I'm
talking about doc product. So we say,

244
00:14:24,700 --> 00:14:27,160
okay, and then you repeat
that again for the last layer.

245
00:14:27,161 --> 00:14:31,360
And then two is going to be our output
prediction and then we compute the error.

246
00:14:31,361 --> 00:14:34,840
But finding the difference between our
actual output and our predicted output.

247
00:14:35,290 --> 00:14:37,380
Then we perform backpropagation.

248
00:14:38,230 --> 00:14:42,670
We take that airway to gradient and we
see in what direction is this target

249
00:14:42,671 --> 00:14:47,440
value by computing the activation of that
output value and multiplying it by the

250
00:14:47,441 --> 00:14:51,010
error. And that's going to give us the
greatest value that delta, the change,

251
00:14:51,020 --> 00:14:51,560
right?

252
00:14:51,560 --> 00:14:55,190
And that Delta is what we're going to
use to update our weights in a second.

253
00:14:55,460 --> 00:14:59,480
But we've computed the delta the gradient
for this layer right though the the

254
00:14:59,570 --> 00:15:02,810
the hidden layer. Let's get it. Let's
compute the gradient for the next layer.

255
00:15:02,811 --> 00:15:06,360
So recursively.
So we'll use the K two delta ticket,

256
00:15:06,380 --> 00:15:09,890
see how much the k one value
contributed to the k two error.

257
00:15:10,280 --> 00:15:12,400
And once we've got that k one error,
we'll come,

258
00:15:12,440 --> 00:15:16,550
we'll do the same exact props process
again to compute the k one gradient.

259
00:15:16,551 --> 00:15:19,670
So the first layer is radiant.
And once we have both gradients,

260
00:15:19,790 --> 00:15:24,140
then we can up update both of those
wait values using those grades.

261
00:15:24,590 --> 00:15:26,330
And we just do that over and over again.

262
00:15:26,331 --> 00:15:30,650
60,000 iterations that is backpropagation.
So I want it to go into a tangent,

263
00:15:30,920 --> 00:15:34,100
no pun intended, to talk
about derivatives and, and,

264
00:15:34,160 --> 00:15:36,620
and great aunts and how
backpropagation works.

265
00:15:37,910 --> 00:15:40,820
But back propagation is the
workhorse of deep learning.

266
00:15:40,821 --> 00:15:41,900
And this is a great chart.

267
00:15:41,901 --> 00:15:45,650
The neural networks do that shows many
different types of neural networks.

268
00:15:46,010 --> 00:15:49,730
There are so many types of neural
networks out there. It's not just one.

269
00:15:49,731 --> 00:15:50,780
There's a lot,
right?

270
00:15:50,990 --> 00:15:55,990
And backpropagation is the optimization
strategy of choice for almost all of

271
00:15:56,751 --> 00:16:00,380
them, right? Almost all of them
use labeled data and then back.

272
00:16:00,440 --> 00:16:04,580
And then backpropagation
has an optimization strategy
to learn some mapping

273
00:16:04,581 --> 00:16:08,720
function, right? Everything is a function
in life. Everything is a function,

274
00:16:08,721 --> 00:16:09,770
love as a function.

275
00:16:09,920 --> 00:16:14,690
Emotions are a function that
the sound of the airplane above.

276
00:16:14,691 --> 00:16:18,620
And then relating that to how
fast velocity and you know,

277
00:16:18,621 --> 00:16:19,720
all these different variables,

278
00:16:19,750 --> 00:16:23,240
it's all you can represent everything
as a function. Math is everywhere.

279
00:16:23,241 --> 00:16:28,190
Math is all around us. Math is beautiful.
It's beautiful. Seriously. Oh my God,

280
00:16:28,191 --> 00:16:31,130
it's awesome. Anyway, um,
everything has a function, right?

281
00:16:31,131 --> 00:16:34,280
So we're trying to learn the
function and supervise. Learning.

282
00:16:34,281 --> 00:16:36,800
Using backpropagation is
a way for us to do that.

283
00:16:38,630 --> 00:16:43,190
So how do artificial in biological neural
networks compare? So this is a very,

284
00:16:43,220 --> 00:16:47,450
uh, basic view of how they
compare the idea. It, it,

285
00:16:47,480 --> 00:16:50,280
it's such a rough, it's such a rough, the,

286
00:16:50,281 --> 00:16:52,940
the initial perceptron
initial neural network,

287
00:16:53,180 --> 00:16:56,570
we're so roughly inspired by
biological neural networks.

288
00:16:56,720 --> 00:16:58,430
It wasn't like they were
saying, well, let's,

289
00:16:58,490 --> 00:17:02,090
let's implement a neurotransmitter
and let's, let's implement, you know,

290
00:17:02,180 --> 00:17:05,510
dopamine and dendrites in
all of their details. I mean,

291
00:17:05,511 --> 00:17:10,160
neurons are these very complex
cells. It's very basic. All the,

292
00:17:10,161 --> 00:17:13,280
the only inspiration is
saying you have some neuron.

293
00:17:13,460 --> 00:17:16,040
It's got a set of dendrites
that received some input.

294
00:17:16,280 --> 00:17:21,020
It performs some kind of activation, some
kind of activation on that neuron. What,

295
00:17:21,170 --> 00:17:24,340
what that means is it decides
whether or not to propagate that,

296
00:17:24,341 --> 00:17:28,520
that signal onward or not using
some function. And if it decides to,

297
00:17:28,521 --> 00:17:32,960
then it sends it out. That's it.
That's, that's the extent of,

298
00:17:33,110 --> 00:17:37,610
of of the inspiration between artificial
and biological neural networks. Right?

299
00:17:38,390 --> 00:17:39,650
Because we have some input,

300
00:17:39,950 --> 00:17:44,540
we compute some activation function like
Relu or sigmoid or you know there's,

301
00:17:44,541 --> 00:17:49,010
there's many of them out there and
then we output the value, right?

302
00:17:49,560 --> 00:17:51,900
So the brain has 100
billion of these neurons,

303
00:17:51,990 --> 00:17:56,160
numerous stand rights and
it uses parallel chaining.

304
00:17:56,161 --> 00:17:59,670
So each neuron is connected
to 10,000 plus others.

305
00:17:59,940 --> 00:18:01,590
Compare this to computers,
right?

306
00:18:01,740 --> 00:18:04,740
Computers don't have neurons
in terms of hardware,

307
00:18:04,950 --> 00:18:08,550
they are made of silicon and
they are serially changed,

308
00:18:08,551 --> 00:18:12,330
which means these transistors on or off
switches are each connected to two or

309
00:18:12,331 --> 00:18:14,490
three others and they form logic gates.

310
00:18:14,700 --> 00:18:17,340
So W and there are great
at storage and recall,

311
00:18:17,460 --> 00:18:21,150
even though they are not as
parallelize as our brain,

312
00:18:21,330 --> 00:18:24,600
they are still better than at some things.
We got to admit that we are like,

313
00:18:24,780 --> 00:18:27,630
it's better at calculating
numbers in, in memory, right?

314
00:18:27,690 --> 00:18:32,460
We can't compute a million times a
million, but a computer can. However,

315
00:18:32,461 --> 00:18:37,260
what our brain is really good at that
computers are not his creativity, right?

316
00:18:37,261 --> 00:18:42,120
We are able to take some idea that is
completely unrelated to another idea and

317
00:18:42,121 --> 00:18:46,440
apply it and then it results in
some amazing innovation or task.

318
00:18:46,890 --> 00:18:49,770
And we are great at connecting
different concepts together.

319
00:18:49,920 --> 00:18:54,210
We are great at being able to learn
many different things and apply our

320
00:18:54,300 --> 00:18:58,440
knowledge to many different tasks and
that's what we should be trying to do with

321
00:18:58,441 --> 00:18:59,274
Ai.

322
00:18:59,770 --> 00:19:04,190
And so there are some really key
differences between our brain and uh,

323
00:19:04,191 --> 00:19:06,570
artificial neural networks.
First of all,

324
00:19:06,571 --> 00:19:08,640
the everything in the brain is recurrent.

325
00:19:08,700 --> 00:19:12,720
That means there is always some kind of
feedback loop happening in any type of

326
00:19:12,750 --> 00:19:17,690
sensory or motor system, right? Not
all neural networks are recurrent. Um,

327
00:19:17,730 --> 00:19:19,530
there's a lot of lateral inhibition,

328
00:19:19,531 --> 00:19:23,760
which means that neurons are inhibiting
other neurons in the same layer.

329
00:19:23,910 --> 00:19:25,620
We haven't seen a lot of
that in deep learning.

330
00:19:25,950 --> 00:19:28,860
There is no such thing as a fully
connected layer in the brain.

331
00:19:28,861 --> 00:19:32,860
Connectivity is usually sparse,
although not random. Um,

332
00:19:33,540 --> 00:19:34,081
we usually,

333
00:19:34,081 --> 00:19:37,050
we have fully connected layers at
the end of our networks like say for

334
00:19:37,051 --> 00:19:40,080
convolutional networks. Uh, but in
the brain there are none, right?

335
00:19:40,500 --> 00:19:44,790
Everything is sparsely connected, but
it's, it's smartly sparsely connected.

336
00:19:46,380 --> 00:19:49,260
Brains are born prewired to
learn without supervision.

337
00:19:49,261 --> 00:19:50,710
So we talked about this a little bit,
right?

338
00:19:50,760 --> 00:19:54,750
How babies can know things even
though they don't, they aren't.

339
00:19:54,810 --> 00:19:57,780
There aren't given labels or any
kind of supervision. And lastly,

340
00:19:57,781 --> 00:20:01,950
the brain is super low power at least
compared to deep neural networks, right?

341
00:20:01,951 --> 00:20:04,590
The brain's power consumption
is about 20 watts.

342
00:20:04,770 --> 00:20:09,340
Compare that to arguably one of the
most advanced ais today. Alphago,

343
00:20:09,870 --> 00:20:14,870
it used about 1200 CPU and 176 GPU
is not to train but just to run.

344
00:20:16,500 --> 00:20:18,780
Just imagine how much,
how many watts that takes.

345
00:20:18,781 --> 00:20:23,700
That's like an order of up an order
of magnitude more power than our brain

346
00:20:23,701 --> 00:20:27,630
ticks, which is, which is
annoyingly inefficient, right?

347
00:20:27,631 --> 00:20:31,650
So we can definitely, definitely,
definitely improve on that.

348
00:20:31,980 --> 00:20:35,670
There's this great book by this Harvard
Psychologist Steven Pinker, which I've,

349
00:20:35,730 --> 00:20:38,970
I've read and I would highly recommend
it called how the mind works.

350
00:20:39,300 --> 00:20:42,000
And this book is from a
neuroscience perspective,

351
00:20:42,050 --> 00:20:46,200
not a machine learning perspective,
but we need more of that.

352
00:20:46,230 --> 00:20:49,930
We need more of that because there are
certainly a lot of secrets here that we

353
00:20:49,931 --> 00:20:53,110
haven't figured out, but we're trying. So
this is a great book to read and it's a,

354
00:20:53,350 --> 00:20:55,720
there's a great quote from that book
that I'm going to read out to you,

355
00:20:55,721 --> 00:20:57,370
which I particularly like.

356
00:20:57,520 --> 00:21:01,300
The quote is the brain is not a blank
slate of neuronal layers waiting to be

357
00:21:01,301 --> 00:21:02,770
pieced together and wire it up.

358
00:21:03,010 --> 00:21:07,300
We are born with brains already structured
for unsupervised learning in a dozen

359
00:21:07,301 --> 00:21:08,410
cognitive domains,

360
00:21:08,650 --> 00:21:13,270
some of which already work pretty
well without any learning at all.

361
00:21:13,390 --> 00:21:13,721
Right?

362
00:21:13,721 --> 00:21:17,680
Evolution has primed us to be able to
do certain things even though we don't

363
00:21:17,681 --> 00:21:22,270
have any real time learning happening,
it's just wired into us, right?

364
00:21:22,271 --> 00:21:26,560
So there is something to be said about
structure versus learning everything.

365
00:21:27,370 --> 00:21:30,610
Anyway. Okay, so we talked about
that. So where are we today? Right?

366
00:21:30,611 --> 00:21:31,540
So that was the first part.

367
00:21:31,541 --> 00:21:33,490
And here's the second part and
then we'll get to the third part.

368
00:21:33,520 --> 00:21:37,630
Research directions. So where are
we today in UN supervised learning?

369
00:21:37,631 --> 00:21:40,810
We know where we are with supervised
learning. That means when we have labels,

370
00:21:41,080 --> 00:21:42,670
but what if we don't have labels?

371
00:21:42,880 --> 00:21:46,750
What we can divide machine learning
into two types besides supervised and

372
00:21:46,751 --> 00:21:51,250
unsupervised classification
and generation, right?

373
00:21:51,280 --> 00:21:56,280
These are two tasks and one metal way
of looking at it as is as creativity and

374
00:21:56,351 --> 00:22:00,480
discovery when everything else is
automated. For us, when all of the, uh,

375
00:22:01,050 --> 00:22:04,330
s you know, all of the brainless
labor that we don't care about,

376
00:22:04,331 --> 00:22:05,680
when all of that is automated,

377
00:22:05,800 --> 00:22:09,220
what's going to be left for
us humans is our two tasks,

378
00:22:09,280 --> 00:22:13,720
creativity and discovery, right? What
can we create? What can we discover?

379
00:22:13,960 --> 00:22:17,770
And we're, and we, and we can frame
though those things as classification,

380
00:22:17,950 --> 00:22:22,120
discovery and creativity generation.
So for classification,

381
00:22:22,150 --> 00:22:24,430
what is something clustering,
right?

382
00:22:24,431 --> 00:22:29,230
Clustering is perhaps the most popular
technique when it comes to classification.

383
00:22:30,070 --> 00:22:33,310
And there are many ways to cluster data,
right? If you don't have the labels,

384
00:22:33,430 --> 00:22:34,570
but you do have the data,

385
00:22:34,720 --> 00:22:39,370
maybe you can learn clusters for all of
these labels such that there that you'll

386
00:22:39,371 --> 00:22:42,220
be able to know what
groups each cluster are in.

387
00:22:42,340 --> 00:22:44,350
So it's like learning without labels,
right?

388
00:22:44,560 --> 00:22:47,650
There are several strategies
to learn clusters from data.

389
00:22:47,830 --> 00:22:52,830
K-Means is perhaps the most popular
dimensionality reduction techniques like t

390
00:22:53,021 --> 00:22:57,880
distributed to the castic neighbor
embedding or ta Tss or principal component

391
00:22:57,881 --> 00:23:01,510
analysis.
There's anomaly anomaly detection,

392
00:23:02,440 --> 00:23:07,060
but most of them still used some
sort of supervised learning.

393
00:23:07,120 --> 00:23:11,830
And the ones that don't
use backpropagation are
not necessarily better.

394
00:23:11,950 --> 00:23:16,090
They're actually very simple algorithms.
Like k means is just, you know,

395
00:23:16,380 --> 00:23:19,200
these four steps right here.
It's very simple.

396
00:23:19,690 --> 00:23:23,470
It's just basic arithmetic and um,
that's where we are right now.

397
00:23:24,130 --> 00:23:26,020
There's also auto encoding,
right?

398
00:23:26,050 --> 00:23:28,630
Auto encoders are really popular
for unsupervised learning.

399
00:23:28,631 --> 00:23:31,720
The idea is that if you
are giving some input,

400
00:23:31,780 --> 00:23:36,160
tried to reconstruct that input as
your outputs, you have an input,

401
00:23:36,400 --> 00:23:40,090
you learned some dense representation
and you try to reconstruct it from there.

402
00:23:40,300 --> 00:23:44,380
And this is great for dimentionality
reduction, uh, learning, some feature,

403
00:23:44,410 --> 00:23:48,590
some features,
et Cetera for generation.

404
00:23:48,710 --> 00:23:52,470
Perhaps the most popular right now is
the generative adversarial network.

405
00:23:52,471 --> 00:23:56,480
So I met the Creator, Ian Goodfellow. We
had a good conversation in San Francisco.

406
00:23:56,481 --> 00:23:59,390
We had, you know, he's a
really smart guy. And really,

407
00:23:59,391 --> 00:24:03,350
I mean the idea was so basic,
right? It was such a basic,

408
00:24:03,410 --> 00:24:05,120
very intuitive idea.

409
00:24:05,360 --> 00:24:08,930
Yet it is the reason behind a
lot of hype and deep learning.

410
00:24:08,931 --> 00:24:13,850
Right now the idea is to have two
networks. One tries to fool the other,

411
00:24:13,851 --> 00:24:16,340
right? You have a discriminator
and you have a generator.

412
00:24:16,490 --> 00:24:18,710
And so what happens is
you have some data set,

413
00:24:18,711 --> 00:24:22,340
let's say some images and you
want to generate new images
that look very similar,

414
00:24:22,341 --> 00:24:26,130
but they're new. So what you do is
you take one network and it's a,

415
00:24:26,131 --> 00:24:30,650
it takes an input of one image. It
applies some distribution function to it,

416
00:24:30,651 --> 00:24:32,060
right?
Enlightened space.

417
00:24:32,210 --> 00:24:34,610
So what that means is like a
golf Sian or something like that.

418
00:24:34,730 --> 00:24:37,940
So it takes them Gosling distribution,
multiply it by that image.

419
00:24:38,060 --> 00:24:41,750
And so the image is basically a group
of numbers, right? The pixel values.

420
00:24:41,960 --> 00:24:44,240
And when you apply some
distribution value to it,

421
00:24:44,330 --> 00:24:46,460
you change those numbers ever so slightly.

422
00:24:46,670 --> 00:24:49,370
So then if you look at it as
a picture, it's the, it's,

423
00:24:49,371 --> 00:24:53,840
it's a slightly modified picture and that
picture is then fake and it only does

424
00:24:53,841 --> 00:24:55,580
this sometimes.
Sometimes it shows the real one,

425
00:24:55,581 --> 00:25:00,140
it shows a fake one and the
discriminator is a classifier it,

426
00:25:00,290 --> 00:25:03,730
right? So you know what the real
images and then you know what,

427
00:25:03,740 --> 00:25:06,470
but you don't know if the the
fake image is fake or not. Right.

428
00:25:06,471 --> 00:25:08,770
The classifier doesn't know.
So it's got it's so it,

429
00:25:08,990 --> 00:25:12,860
it tries to classify the fake image
and if it gets it right or wrong,

430
00:25:13,040 --> 00:25:15,020
you could take its prediction,

431
00:25:15,080 --> 00:25:17,240
compute an error value
between the real and the fake.

432
00:25:17,390 --> 00:25:20,600
And then again back propagate
an error gradient value, right?

433
00:25:20,601 --> 00:25:24,020
So you are still using
backpropagation across it.

434
00:25:24,021 --> 00:25:27,800
So the whole thing is what's called end
to end differentiable because we can

435
00:25:27,801 --> 00:25:32,060
differentiate every weight value
in the, in this, in the system.

436
00:25:32,510 --> 00:25:35,750
So even though there
are no explicit labels,

437
00:25:35,810 --> 00:25:40,610
we are still using backpropagation.
It's self supervised.

438
00:25:40,640 --> 00:25:42,680
So it's like we are creating the labels.

439
00:25:43,040 --> 00:25:47,270
Another great example are variational
auto encoders where we are embedding

440
00:25:47,330 --> 00:25:50,120
stochasticity inside of the model itself.

441
00:25:50,150 --> 00:25:53,510
That means inside of the layers
we have a random variable.

442
00:25:53,620 --> 00:25:58,620
What what that means is the neural network
is not deterministic or stochastic.

443
00:25:58,940 --> 00:26:01,130
You cannot predict what
the output is going to be.

444
00:26:01,310 --> 00:26:04,520
That means that if you have some input,
you feed it through these layers.

445
00:26:04,670 --> 00:26:06,080
One of them is a random variable.

446
00:26:06,081 --> 00:26:08,390
So it's a distribution
that's applied to that input.

447
00:26:08,540 --> 00:26:12,320
What happens is the output is going to
be some unpredictable new output that you

448
00:26:12,321 --> 00:26:15,380
didn't predict before, which is what
you're trying to generate. Right.

449
00:26:16,640 --> 00:26:18,020
And lastly,

450
00:26:18,021 --> 00:26:21,650
and these are the bleeding edge of
unsupervised learning models by the way.

451
00:26:21,860 --> 00:26:25,730
And lastly is the differential
neural computer. So I,

452
00:26:25,820 --> 00:26:30,820
I am going to go out on a limb and I'm
going to say that the DNC is the most

453
00:26:32,270 --> 00:26:36,680
advanced algorithm currently that uses,
uh,

454
00:26:36,710 --> 00:26:39,500
backpropagation out there.
Maybe,

455
00:26:39,501 --> 00:26:43,410
maybe Alphago is better, but we
haven't seen the source code for that.

456
00:26:43,411 --> 00:26:48,360
So I wouldn't know. But in terms of openly
available a source code, the DNC is, is,

457
00:26:48,630 --> 00:26:51,630
is, is amazing. It's also highly complex.

458
00:26:51,660 --> 00:26:55,320
There are so many moving parts in
a differentiable neural computer.

459
00:26:55,321 --> 00:26:56,250
And I have a video on this,

460
00:26:56,251 --> 00:26:59,910
just search DNC Suraj but there
are so many moving parts here.

461
00:26:59,911 --> 00:27:02,000
You've got read and write heads.
Um,

462
00:27:02,070 --> 00:27:06,780
but basically you are separating
memory from the network itself,

463
00:27:07,320 --> 00:27:07,621
right?

464
00:27:07,621 --> 00:27:12,510
So you have memory and the idea analogy
fit that they made was between DNA and

465
00:27:12,511 --> 00:27:17,100
the brain, right? So you have DNA,
this, this is encoded external memory.

466
00:27:17,101 --> 00:27:19,980
So you have an external memory store,
and then you have your,

467
00:27:19,990 --> 00:27:24,900
your internal controller, right?
And so the, the, the, the,

468
00:27:25,290 --> 00:27:29,760
the controller is pulling from the
memory and there are read and write heads

469
00:27:29,761 --> 00:27:33,140
between the controller and
the memory. Uh, between there,

470
00:27:33,141 --> 00:27:37,200
there are links between different rows in
the memory. Basically, you have, let me,

471
00:27:37,210 --> 00:27:38,460
let me show you this.
Let me show you this.

472
00:27:40,140 --> 00:27:44,850
You have so many different
differentiable parameters.

473
00:27:44,880 --> 00:27:48,870
All of the, you of, you have read
and write heads, you've LSTM cells.

474
00:27:49,110 --> 00:27:52,950
Every single one of these matrices are,

475
00:27:53,280 --> 00:27:56,520
and every single one of these
major streets are differentiable.

476
00:27:56,670 --> 00:28:01,530
So this is a gigantic, very complex
system and everything is differentiable,

477
00:28:02,040 --> 00:28:04,560
right? So there's that. And so now,

478
00:28:05,040 --> 00:28:10,040
and what they did was for the DNC was
they generated a random graph and have

479
00:28:10,680 --> 00:28:14,700
different subways and they use it to try
to predict where someone was going to

480
00:28:14,701 --> 00:28:18,120
go based on some questions,
which is just incredible.

481
00:28:18,120 --> 00:28:21,060
They also trained it on family
trees and a bunch of other things.

482
00:28:21,061 --> 00:28:26,040
But basically the best unsupervised
learning methods still require

483
00:28:26,050 --> 00:28:27,090
backpropagation.

484
00:28:27,180 --> 00:28:31,800
So my point here is that backpropagation
really is the workhorse of deep

485
00:28:31,801 --> 00:28:34,380
learning even in the unsupervised setting.

486
00:28:34,800 --> 00:28:39,530
But another thing I want to
say is that a lot of, uh,

487
00:28:39,750 --> 00:28:43,800
deep learning research is all about making
small incremental improvements off of

488
00:28:43,801 --> 00:28:48,330
existing ideas. And a lot
of times academia kind of
pushes us in that direction.

489
00:28:48,450 --> 00:28:50,670
It pushes you to make incremental changes.

490
00:28:50,671 --> 00:28:54,780
Maybe like tweaking one hyper parameter
or adding some new layer type or maybe

491
00:28:54,930 --> 00:28:58,890
new, some new cell type, like a
gru or whatever. But if you have,

492
00:28:59,250 --> 00:29:02,280
but if you, if you, if you
think of a radically new idea,

493
00:29:02,640 --> 00:29:06,180
you can really shake things up seriously.
And the idea,

494
00:29:06,490 --> 00:29:09,240
it doesn't even have to be that
difficult. It really does. It does.

495
00:29:09,300 --> 00:29:11,820
It doesn't even have to be that complex.
Like think of games.

496
00:29:12,020 --> 00:29:15,780
I think of generative adversarial
networks. It's such a simple idea.

497
00:29:16,050 --> 00:29:19,830
You have two networks. One tries
to fool the other, that's it.

498
00:29:19,860 --> 00:29:22,290
It's just two neural networks.
One tries to hold the other.

499
00:29:22,530 --> 00:29:26,760
And young lacount said this is the
hottest idea in the past 20 years in deep

500
00:29:26,761 --> 00:29:30,990
learning. And look at this. I mean this
idea was invented just two years ago.

501
00:29:31,170 --> 00:29:33,960
Look at the number of
gains that have been,

502
00:29:34,200 --> 00:29:38,280
have been inspired by that first paper.
There are so many,

503
00:29:38,281 --> 00:29:41,920
and this is in two years.
All of these different can go on.

504
00:29:42,160 --> 00:29:46,600
You could make an entire four month
course on all the different types of guns

505
00:29:46,601 --> 00:29:48,400
out there.
So my point is,

506
00:29:49,300 --> 00:29:53,580
anyone can think of a really good idea
when it comes to deep learning it, the,

507
00:29:53,581 --> 00:29:56,050
the playing field is level for everyone.

508
00:29:56,410 --> 00:30:00,100
So let's get to the future
research directions. Okay,

509
00:30:00,101 --> 00:30:02,650
so the first one,
so my thesis is this,

510
00:30:02,860 --> 00:30:07,480
is that unsupervised learning and
reinforcement learning must be the primary

511
00:30:07,510 --> 00:30:12,130
modes of learning because labels mean
little to a child growing up, right?

512
00:30:12,131 --> 00:30:16,600
So we need to use more reinforcement
learning, more unsupervised learning,

513
00:30:16,810 --> 00:30:20,050
and then we're going to get to somewhere
somewhere better than where we are

514
00:30:20,051 --> 00:30:23,010
right now. So the first, uh,

515
00:30:23,060 --> 00:30:25,690
we research direction is
Basie and deep learning,

516
00:30:25,691 --> 00:30:29,530
which is not discarding backpropagation
is just making it smarter.

517
00:30:29,710 --> 00:30:30,670
What do I mean by this?

518
00:30:30,910 --> 00:30:35,470
Baze Basie and logic is all about having
some prior assumption about how the

519
00:30:35,471 --> 00:30:37,870
world works versus frequent tests,

520
00:30:37,900 --> 00:30:41,830
which just assumes that we
have no assumptions, right?

521
00:30:42,040 --> 00:30:45,340
So when you take Basie and reasoning
and apply it to deep learning,

522
00:30:45,580 --> 00:30:47,530
you can have amazing results.

523
00:30:47,531 --> 00:30:50,980
And this has been proven in the
case of variational auto encoders,

524
00:30:52,120 --> 00:30:56,260
but deep learning struggles to model this
uncertainty. So when I talk one on one,

525
00:30:56,261 --> 00:31:00,820
I'm what I specifically mean when I say
Basie and deep learning is smarter wait,

526
00:31:00,821 --> 00:31:05,410
initialization and perhaps even smarter
hyper parameter initialization, right?

527
00:31:05,411 --> 00:31:10,411
And this kind of relates back to a child
and how evolution has primed us to know

528
00:31:10,571 --> 00:31:13,450
certain things before we've
learned them in real time. Right?

529
00:31:13,510 --> 00:31:15,580
There are certain
learnings we already have.

530
00:31:15,760 --> 00:31:19,510
We are weights in our head
are not initialized randomly.
When we start learning,

531
00:31:19,690 --> 00:31:24,610
we have some sort of smarter
wait initialization. So
Basie and logic is is a,

532
00:31:24,611 --> 00:31:27,070
is a great direction,
is a great research direction.

533
00:31:27,250 --> 00:31:30,580
Just combining those two fields,
Basie and logic and deep learning.

534
00:31:31,750 --> 00:31:34,930
The second one is called spike
timing dependent plasticity.

535
00:31:35,290 --> 00:31:38,260
And a great analogy for
this is saying you know,

536
00:31:38,261 --> 00:31:40,330
you're trying to predict if
it's going to be rainy or not.

537
00:31:40,540 --> 00:31:43,870
You can go out there and you can see if
it's going to rain literally with your

538
00:31:43,871 --> 00:31:48,280
own eyes or you can look at your roommate
who tends to take an umbrella every

539
00:31:48,281 --> 00:31:52,870
time he goes out and every single time
he walks out with an umbrella it happens

540
00:31:52,871 --> 00:31:55,780
to be rain. So rather than
try to go out there yourself,

541
00:31:55,781 --> 00:31:59,050
look at it's raining or not,
instead you just look at your roommate,

542
00:31:59,110 --> 00:32:02,680
see if he picks up an umbrella. And if he
does, you know that it's going to rain.

543
00:32:02,681 --> 00:32:03,820
So you take an umbrella.

544
00:32:04,030 --> 00:32:09,030
So the analogy applies to spike timing
dependent plasticity because you can't

545
00:32:09,281 --> 00:32:13,690
properly back propagate for weight updates
in a graph based network since since

546
00:32:13,691 --> 00:32:15,490
it's an asynchronous system,

547
00:32:15,580 --> 00:32:19,720
so we trust neurons that are
faster than us at the task.

548
00:32:19,780 --> 00:32:21,400
So it's all about timing,

549
00:32:21,550 --> 00:32:25,630
looking at neurons and how fast they're
firing and using those neurons as a

550
00:32:25,631 --> 00:32:28,030
signal,
as a signal for how we learn.

551
00:32:28,570 --> 00:32:33,570
So suppose we have two neurons a and B
and a synopsis onto be the Std p rural

552
00:32:34,421 --> 00:32:38,650
state that if a fire's and be fires
after a short delay the synapse will be

553
00:32:38,660 --> 00:32:40,160
potentiated.
Okay.

554
00:32:40,520 --> 00:32:44,240
So the magnitude of the weight increase
is inversely proportional to the delay

555
00:32:44,241 --> 00:32:45,920
between a and B firing.

556
00:32:46,100 --> 00:32:50,810
So we're taking timing into consideration
which deep pointing currently does not

557
00:32:50,811 --> 00:32:52,610
do the time of fire.

558
00:32:53,210 --> 00:32:56,090
The third idea is our
self organizing maps.

559
00:32:56,150 --> 00:32:59,270
So this is not a new idea at all,
but that's okay.

560
00:32:59,360 --> 00:33:01,220
That's another thing
that I want to mention.

561
00:33:01,640 --> 00:33:05,360
There is so much machine learning and
deep learning literature out there.

562
00:33:05,361 --> 00:33:10,070
There is a lot and a lot of times
the best ideas are forgotten.

563
00:33:10,160 --> 00:33:14,420
They are lost in the mix because there's
so much hype around certain ideas and

564
00:33:14,421 --> 00:33:18,410
sometimes it's unnecessary hype around
certain ideas and some of the best ideas

565
00:33:18,560 --> 00:33:22,280
could have been invented 2030 years ago.
I mean look at deep learning, right?

566
00:33:22,460 --> 00:33:26,690
So it's just all about finding those
ideas and self organizing maps are one of

567
00:33:26,691 --> 00:33:31,691
those ideas where you know this is an
older idea but it has a lot of potential

568
00:33:32,000 --> 00:33:34,970
and not many people know how these works,
how these work.

569
00:33:36,320 --> 00:33:41,320
But this is a type of neural network
that is used for unsupervised learning.

570
00:33:41,570 --> 00:33:43,910
So the idea is that we have,
we,

571
00:33:44,180 --> 00:33:47,090
we randomize the node wait
vectors in a map of them.

572
00:33:47,091 --> 00:33:50,690
So we have some weight vectors and then
we pick some input vector that's our,

573
00:33:50,720 --> 00:33:54,200
that's our input data and we traverse
each note in the map computing the

574
00:33:54,201 --> 00:33:58,280
distance between our input node and all
the other notes and then we find the

575
00:33:58,281 --> 00:33:59,770
node that is closest,

576
00:33:59,900 --> 00:34:04,190
the most similar to our input node that
is the best matching unit, the BMU.

577
00:34:04,610 --> 00:34:08,570
Then we update the wave vectors of the
notes in the neighborhood of the BMU by

578
00:34:08,571 --> 00:34:10,760
pulling them closer to the input vector.

579
00:34:11,150 --> 00:34:15,800
And what happens is this creates a self
organizing map and you can visualize it

580
00:34:15,801 --> 00:34:16,700
as different colors,

581
00:34:16,701 --> 00:34:20,990
but it's a basically clusters of
different data points cause basically

582
00:34:20,991 --> 00:34:24,180
clustering. And I think this is
a great idea. It doesn't use it.

583
00:34:24,380 --> 00:34:28,340
It doesn't use backpropagation. Uh,
and we should look more into that.

584
00:34:29,120 --> 00:34:33,800
The fourth idea, the fourth, the fourth
direction or synthetic gradients. So,

585
00:34:33,830 --> 00:34:36,350
uh,
Andrew Trask has a great,

586
00:34:36,380 --> 00:34:39,500
great blog post on this that I
highly recommend you check out.

587
00:34:39,710 --> 00:34:43,850
It's really in depth. But this
idea came out of deep mind. Uh,

588
00:34:43,851 --> 00:34:46,730
this idea came out of deep
mine and it's basically,

589
00:34:46,910 --> 00:34:51,910
it's a much faster version
of back backpropagation in
which you are not waiting

590
00:34:52,131 --> 00:34:53,810
as long to update your weights.

591
00:34:54,920 --> 00:34:59,750
So individual layers make a best guess
for what they think the data will say.

592
00:34:59,960 --> 00:35:03,770
Then they update their weights according
to that guest and they call this best

593
00:35:03,771 --> 00:35:07,640
guessed the synthetic gradient because
it's a prediction of what the grading

594
00:35:07,670 --> 00:35:09,950
will be,
not what it actually is.

595
00:35:10,220 --> 00:35:14,210
And that data is only used to help
update each layers guesser or synthetic

596
00:35:14,211 --> 00:35:17,490
gradient generator. And what
this does is it allows, uh,

597
00:35:17,590 --> 00:35:21,380
individual layers to learn in isolation,
which increases the speed of training.

598
00:35:21,590 --> 00:35:25,730
Individual layers can learn without
having to do a full forward and backward

599
00:35:25,731 --> 00:35:29,580
pass. Uh, so that's synthetic
gradients. And I think, and,

600
00:35:29,620 --> 00:35:32,420
and it's weird because even in
the machine learning sub Reddit,

601
00:35:32,421 --> 00:35:35,830
people were talking about
synthetic gradients. Uh,

602
00:35:35,960 --> 00:35:39,630
but some of the questions were, hey,
we need more of this. Why, why hasn't,

603
00:35:39,660 --> 00:35:42,780
why hasn't this been talked about
more? And people don't know, right?

604
00:35:42,781 --> 00:35:43,860
So this is a great idea.

605
00:35:43,861 --> 00:35:47,100
I came out of deep mind and definitely
learn more about synthetic gradients.

606
00:35:47,640 --> 00:35:51,240
The fifth research direction
is our evolutionary strategies.

607
00:35:51,300 --> 00:35:56,300
So open AI had a great blog posts on this
evolutionary strategies as a scalable

608
00:35:56,551 --> 00:35:58,650
alternative to reinforcement learning.

609
00:35:58,800 --> 00:36:03,800
But evolutionary strategies have not
given us a lot of success so far,

610
00:36:04,200 --> 00:36:07,440
but that's okay. Just intuitively
they make a lot of sense, right?

611
00:36:07,441 --> 00:36:10,500
Trying to resemble evolution.
You have fitness,

612
00:36:10,501 --> 00:36:15,000
you have a fitness function
that determines how fit
some individual is and these

613
00:36:15,001 --> 00:36:18,120
individuals mates, right? So
there's crossover and you know,

614
00:36:18,121 --> 00:36:20,310
it's basically survival of the fittest.
You have,

615
00:36:20,880 --> 00:36:24,660
you have mutation selection and
crossover via a fitness function.

616
00:36:25,200 --> 00:36:27,390
And you can do with a lot of games,
right?

617
00:36:27,391 --> 00:36:31,320
So you can have several neural networks
and you can use evolutionary strategies

618
00:36:31,440 --> 00:36:35,220
to have the best one win or,
or survive longer than the rest.

619
00:36:35,340 --> 00:36:37,440
So I think there's a,
there's a lot of potential for that.

620
00:36:37,680 --> 00:36:41,400
And it's very similar to
reinforcement learning. So if I,

621
00:36:41,530 --> 00:36:45,510
if I were to pick the
lowest hanging fruit, right?

622
00:36:45,511 --> 00:36:50,430
The lowest hanging fruit
in terms of revolutionary
ideas to come to the table of

623
00:36:50,431 --> 00:36:53,730
really radical changes,
it would be in reinforcement learning,

624
00:36:53,731 --> 00:36:54,960
deep reinforcement learning.

625
00:36:55,470 --> 00:36:58,740
Reinforcement learning is all about
learning from trial and error, right?

626
00:36:58,741 --> 00:37:02,370
You have some, you are some agent in
some, in some environments, right?

627
00:37:02,580 --> 00:37:04,230
It's called the agent environment loop.

628
00:37:04,320 --> 00:37:08,400
You perform an action in that
environment, you get a reward, yes or no,

629
00:37:08,430 --> 00:37:11,880
and then based on that reward, you
update your state, your learnings,

630
00:37:11,910 --> 00:37:13,500
and you continue that process.

631
00:37:13,980 --> 00:37:17,670
So AlphaGo used reinforcement learning,

632
00:37:17,671 --> 00:37:20,610
deep reinforcement learning to
get really good at its game.

633
00:37:21,660 --> 00:37:24,780
And there are so many low hanging
fruits and deep reinforcement learning.

634
00:37:24,930 --> 00:37:28,900
How do we learn the best
policy? Uh, uh, just there,

635
00:37:28,920 --> 00:37:30,540
there are so many unanswered questions.

636
00:37:30,690 --> 00:37:35,240
So reinforcement learning in
general is a great place to, uh,

637
00:37:35,310 --> 00:37:37,890
do to just focus on in terms of research.

638
00:37:38,640 --> 00:37:42,150
And the last one is the most capital
intensive and perhaps the hardest,

639
00:37:42,151 --> 00:37:43,500
but I just had to mention it right?

640
00:37:43,680 --> 00:37:47,310
We talked about how transistors
are on off switches and uh,

641
00:37:47,430 --> 00:37:51,900
they are chained together serially to
four to perform to form logic gates,

642
00:37:52,110 --> 00:37:53,820
whereas neural networks are,

643
00:37:54,240 --> 00:37:58,530
are parallel in their construction so
they're connected to 10,000 other ones.

644
00:37:58,830 --> 00:38:03,830
So perhaps instead of trying to replicate
the rules of intelligence in Silico or

645
00:38:04,081 --> 00:38:07,230
at least, uh, on the current
types of chips we have,

646
00:38:07,410 --> 00:38:10,620
let's just change the hardware
completely right at the hardware level.

647
00:38:10,980 --> 00:38:15,030
And IBM's neuromorphic chips are good
example of going in this direction.

648
00:38:15,380 --> 00:38:17,580
Google's TPU tensor processing unit.

649
00:38:17,820 --> 00:38:21,990
But basically the idea is to wire up
transistors in parallel. Like the brain.

650
00:38:22,380 --> 00:38:26,340
Really, I think, I think anyone can
can do this. You just, you know, you,

651
00:38:26,460 --> 00:38:27,870
if you have some idea,
I mean, think about it,

652
00:38:27,871 --> 00:38:32,250
the brain is only running on 20 watts,
so it can't be that expensive, right?

653
00:38:32,280 --> 00:38:34,810
In terms of hardware wetware right?
So,

654
00:38:35,020 --> 00:38:39,880
so if you have some idea you can crowd
funded for whatever hardware you want to

655
00:38:39,881 --> 00:38:43,570
build and then, you know, use we
funder or Kickstarter and yes,

656
00:38:43,571 --> 00:38:47,650
I think you can even have a startup for
hardware, for, for machine learning,

657
00:38:47,651 --> 00:38:49,810
for deep learning.
So what is my conclusion?

658
00:38:49,811 --> 00:38:53,620
Those are my seven research directions
that I wanted to talk about today as a

659
00:38:53,621 --> 00:38:57,850
way, as a, as a response to Hinton
talking about backpropagation.

660
00:38:58,240 --> 00:39:00,760
So what is my conclusion?
What do I think? I think.

661
00:39:00,761 --> 00:39:05,260
And so I agree with Andre Carpathia
who is one of the best deep learning

662
00:39:05,261 --> 00:39:08,350
researchers out there. He's a
director of Ai, a Tesla now.

663
00:39:09,130 --> 00:39:10,510
And the conclusion is this,

664
00:39:10,570 --> 00:39:15,340
let's create multiagent simulated
environments that heavily rely on

665
00:39:15,341 --> 00:39:19,120
reinforcement learning and
evolutionary strategies. A carpet.

666
00:39:19,121 --> 00:39:21,940
The Ed is great talk at y Combinator,
which I didn't attend,

667
00:39:21,941 --> 00:39:24,400
but the slides are online.
But check this out.

668
00:39:24,430 --> 00:39:26,380
He had this one slide
that said intelligence.

669
00:39:26,560 --> 00:39:31,150
The cognitive toolkit includes but is
not limited to all of these different

670
00:39:31,151 --> 00:39:36,040
aspects of intelligence,
attention, working memory,
long term memory, knowledge,

671
00:39:36,041 --> 00:39:38,410
representation, emotions, consciousness.

672
00:39:38,590 --> 00:39:42,540
There are so many different,
uh,

673
00:39:42,670 --> 00:39:46,570
topics that encompass learning.
It's this orchestra of different,

674
00:39:46,720 --> 00:39:50,800
of different concepts and they all
work together to define intelligence.

675
00:39:50,801 --> 00:39:51,790
Our intelligence

676
00:39:53,540 --> 00:39:57,970
to the conclusion is we need to create
environments that incentivize the

677
00:39:57,971 --> 00:40:00,970
emergence of this cognitive toolkit.

678
00:40:01,210 --> 00:40:04,540
So doing it the wrong way
is to use this environment.

679
00:40:04,600 --> 00:40:09,190
What does this incentivized incentivizes
a lookup table of correct moves, right?

680
00:40:09,191 --> 00:40:11,590
For pump.
But what is doing it right?

681
00:40:11,710 --> 00:40:16,570
This two agents in this world, there's
some food, there's some survival.

682
00:40:16,600 --> 00:40:20,830
They are learning to adapt to each other.
It's much more like real life itself,

683
00:40:20,831 --> 00:40:25,360
right? And that incentivizes a
cognitive toolkit, cooperation,

684
00:40:25,390 --> 00:40:28,840
attention, memory, emotions,
even right with more complexity.

685
00:40:29,170 --> 00:40:34,170
So it comes down to the exploration
versus exploitation dilemma from

686
00:40:34,631 --> 00:40:35,500
reinforcement learning.

687
00:40:35,680 --> 00:40:39,880
How much do we want to exploit existing
algorithms backpropagation by making

688
00:40:39,910 --> 00:40:44,020
incremental improvements versus how much
do we want to explore an entirely new

689
00:40:44,021 --> 00:40:46,600
ideas.
And we need people doing both.

690
00:40:46,600 --> 00:40:50,290
We need people improving the deep learning
algorithms because there's still a

691
00:40:50,291 --> 00:40:54,790
lot to be improved upon. But we also
need people working on exploration,

692
00:40:54,791 --> 00:40:57,430
like entirely new ideas.
In fact,

693
00:40:57,431 --> 00:41:00,970
I think we need more people focusing on
that then we have currently. So if I,

694
00:41:00,971 --> 00:41:02,590
if I were to, you know, take some away,

695
00:41:02,591 --> 00:41:06,820
I would say let's take 20% and put them
20% from the exploitation and put them

696
00:41:06,821 --> 00:41:10,870
in the exploration
category. But, um, yeah,

697
00:41:10,871 --> 00:41:11,860
it's just something to think about.

698
00:41:11,861 --> 00:41:15,070
I hope this video helped you think more
about all of these concepts and where

699
00:41:15,071 --> 00:41:16,780
we're headed and where we should go.

700
00:41:16,781 --> 00:41:20,110
I hope it gave you some ideas for
what you might be more interested in.

701
00:41:20,320 --> 00:41:23,470
And I'm going to keep making
videos like this, so yeah,

702
00:41:23,500 --> 00:41:26,920
please subscribe for more programming
videos. And for now, I've got to evolve,

703
00:41:27,040 --> 00:41:28,270
so thanks for watching.

