1
00:00:00,040 --> 00:00:03,400
Five days left to enroll in the
decentralized applications course.

2
00:00:03,490 --> 00:00:05,440
Lincoln's description,
hello world.

3
00:00:05,460 --> 00:00:10,460
It's a Raj or as my computer generated
version would say hello world.

4
00:00:10,691 --> 00:00:13,120
It's Siraj.
I don't know how well you could hear that,

5
00:00:13,121 --> 00:00:16,690
but that was the generated version
of my voice. He says a lot of things.

6
00:00:16,810 --> 00:00:19,270
What is this madness?
This is crazy.

7
00:00:20,540 --> 00:00:22,820
I like to travel.
He says a lot,

8
00:00:22,821 --> 00:00:26,690
but basically he has a deep learning
generated version of my boys.

9
00:00:26,870 --> 00:00:30,650
The Demo for today is going to
be us generating our own voices.

10
00:00:30,860 --> 00:00:34,820
What I just played was this web app
called liar bird behind the hood.

11
00:00:34,850 --> 00:00:39,850
They're using what's called a an encoder
decoder architecture to generate voices

12
00:00:40,221 --> 00:00:43,010
after having trained on your voices.
So definitely check that out.

13
00:00:43,011 --> 00:00:44,900
It's called liar bird.
Got Ai,

14
00:00:45,230 --> 00:00:49,070
but we're going to talk
about the technology that it
uses and it uses something

15
00:00:49,160 --> 00:00:53,990
that is very similar to
what Baidu released recently
by new recently released a

16
00:00:53,991 --> 00:00:58,991
paper called neuro voice cloning and
it's really rare for any of these big

17
00:00:59,091 --> 00:01:03,260
Chinese companies like 10 cent or Alibaba
or Baidu to release something open

18
00:01:03,261 --> 00:01:04,760
source.
So I'm very grateful.

19
00:01:04,761 --> 00:01:08,240
We should all be very grateful and we
should find ways to incentivize these

20
00:01:08,241 --> 00:01:12,410
companies to share more of their
findings because China is really, really,

21
00:01:12,411 --> 00:01:17,270
really going in on AI and we want
to see more open source work, uh,

22
00:01:17,450 --> 00:01:19,520
from that region of the world. So, uh,

23
00:01:19,550 --> 00:01:21,510
what we're gonna do is we're
going to talk about deep voice,

24
00:01:21,511 --> 00:01:25,760
one deep voice to the points three and
then the latest version of the boys,

25
00:01:25,761 --> 00:01:26,990
which is by dues,
um,

26
00:01:27,260 --> 00:01:31,190
text to speech system
called neural voice cloning.

27
00:01:31,370 --> 00:01:35,030
Basically you can say something and then
translate it into somebody else's voice.

28
00:01:35,180 --> 00:01:38,480
Like Kate Winslet for example. That's
why I had the picture of Kate Winslet,

29
00:01:38,630 --> 00:01:40,160
one of the authors of this,

30
00:01:40,161 --> 00:01:42,140
this get hub library that
we're going to look at later.

31
00:01:42,440 --> 00:01:45,830
He translated his voice
into Kate Winslet's voice.
So you could say something,

32
00:01:46,510 --> 00:01:49,560
enter it into the system, and then Kate
Winslet will say the exact same thing.

33
00:01:49,561 --> 00:01:54,170
That right. So, and we're going to look
at the code as well. It's tentraflow code.

34
00:01:54,260 --> 00:01:56,120
We're going to figure out
how this architecture works,

35
00:01:56,210 --> 00:02:00,200
how it's evolved over the past two years
at Baidu has been working on this and

36
00:02:00,201 --> 00:02:03,050
hopefully you'll get an idea of how
you yourself can do this as well.

37
00:02:03,200 --> 00:02:06,260
The link to the get hub repo is going
to be in a video description. Okay,

38
00:02:06,261 --> 00:02:08,960
so let's, let's go into
this. So deep voice one,

39
00:02:08,961 --> 00:02:13,250
so in February of last year by do
released the voice by do by the way,

40
00:02:13,251 --> 00:02:14,840
is like China's version of Google.

41
00:02:14,870 --> 00:02:18,230
It's like the biggest Chinese company
as focusing on deep learning right now.

42
00:02:18,530 --> 00:02:20,570
But they released a
system called deep voice,

43
00:02:20,600 --> 00:02:22,850
and what it was was a
production grade system.

44
00:02:22,851 --> 00:02:27,290
That means that means that it was a
system that was made for use in production

45
00:02:27,291 --> 00:02:31,430
environments for people to actually
use that apply deep learning to text,

46
00:02:31,490 --> 00:02:32,540
to speech synthesis.

47
00:02:32,690 --> 00:02:35,240
That means you type a sentence
and then a computer then reads,

48
00:02:35,241 --> 00:02:38,990
it reads that sentence and it
says what that sentence is, right?

49
00:02:39,020 --> 00:02:41,030
So the traditional tts,

50
00:02:41,060 --> 00:02:44,390
Texas speech pipeline
consists of hand engineering,

51
00:02:44,391 --> 00:02:46,970
all these features of like
what a boy sounds like,

52
00:02:46,971 --> 00:02:49,550
what a single word sounds like,
et cetera.

53
00:02:49,760 --> 00:02:53,240
But deep learning does away with feature
engineering, as you probably know,

54
00:02:53,270 --> 00:02:57,200
if not, it does away with it and said it
learns all the features that it needs.

55
00:02:57,560 --> 00:03:00,880
And so the system can be trained
and retrained and just a few hours.

56
00:03:01,150 --> 00:03:04,300
And here's the, here's the really
interesting bit by new clamp,

57
00:03:04,301 --> 00:03:07,570
a 400 x speedup over wavenet.

58
00:03:07,690 --> 00:03:12,690
So wave net was deep minds really
kick ass paper of the year last year,

59
00:03:13,480 --> 00:03:14,950
uh,
on tts systems.

60
00:03:15,250 --> 00:03:19,030
And that's a 400 x speedup on the
world's leading AI institution,

61
00:03:19,270 --> 00:03:23,740
which has really impressive. It's
true. It's very impressive. So yeah,

62
00:03:23,741 --> 00:03:27,170
China, we gotta, we gotta,
we gotta get in on China Sea,

63
00:03:27,200 --> 00:03:29,780
see what's going down in China.
You know what I'm saying? So, uh,

64
00:03:30,460 --> 00:03:31,870
so let's talk about this architecture.

65
00:03:32,800 --> 00:03:37,180
So what they use was an encoder decoder
architecture who was inspired by famous

66
00:03:37,181 --> 00:03:39,730
paper called sequence to sequence
learning that came out of Google.

67
00:03:39,940 --> 00:03:42,430
But the model looks like this.
Here's what the pipeline looks like.

68
00:03:42,670 --> 00:03:46,510
So you say something, right? You say
something, it could be like, hello world,

69
00:03:46,511 --> 00:03:49,660
it's Raj. And then it, it has
a text version of that, right?

70
00:03:49,661 --> 00:03:53,560
So there's a text version and what
it does is the model will burst.

71
00:03:53,590 --> 00:03:55,840
It'll take that text,
right? All of those words,

72
00:03:56,140 --> 00:03:58,810
and it will convert those words
into what are called phonemes.

73
00:03:59,320 --> 00:04:04,000
Phonemes are the way of describing
what a word sounds like. So here's,

74
00:04:04,001 --> 00:04:08,950
here's what I mean by that. So think
about the words though. And then rough.

75
00:04:09,100 --> 00:04:13,600
They both end in this, O u g H,
right? But they sound different.

76
00:04:13,720 --> 00:04:16,870
So how do we represent that
for a computer to read, right?

77
00:04:16,990 --> 00:04:20,380
So the way we represent that,
that is by using what are called phonemes,

78
00:04:20,381 --> 00:04:24,690
and there are dictionaries
of phonemes online. So a,

79
00:04:24,700 --> 00:04:25,840
so check this out the word,

80
00:04:25,870 --> 00:04:30,870
the phrase white room in the phony
version of that is w a y one.

81
00:04:32,291 --> 00:04:37,150
So [inaudible] means a certain way of
saying Hii. Cc the sound here. Ahi.

82
00:04:37,330 --> 00:04:41,830
When we say it in the context of white,
it makes a certain sound like hi,

83
00:04:41,890 --> 00:04:46,600
right white. But if we say Hii in
another context, like hi, it's different,

84
00:04:46,601 --> 00:04:49,990
it's hi, instead of I rights,
there's an h in front of it.

85
00:04:50,110 --> 00:04:52,420
So we could differentiate
that using phonemes.

86
00:04:52,510 --> 00:04:56,980
So one would be like a one and then
another would be [inaudible], right?

87
00:04:56,981 --> 00:04:58,720
So it's different than just saying like,

88
00:04:58,850 --> 00:05:01,630
here's what those letters are
that consists of the word,

89
00:05:01,660 --> 00:05:03,040
here's what it sounds like.

90
00:05:03,400 --> 00:05:07,480
So the first part of the model would
convert the text into the phonemes, right?

91
00:05:07,900 --> 00:05:11,650
Once it had the phonemes and would use
another model to convert those phonemes

92
00:05:11,651 --> 00:05:15,790
into a prediction of what the duration
was of those phonemes and in the

93
00:05:15,791 --> 00:05:17,710
frequency of those phonemes as well.

94
00:05:17,860 --> 00:05:22,250
So how long do we say those
phonemes in the context of, uh,

95
00:05:22,360 --> 00:05:26,320
of a sentence, right? How long does that
phony molasses, right. It could be small,

96
00:05:26,410 --> 00:05:28,300
it can be long and there's,
you know, it could,

97
00:05:28,301 --> 00:05:29,920
it could be a millisecond difference,

98
00:05:29,980 --> 00:05:33,850
but there's still a duration for how
long it is. Frequency means intonation.

99
00:05:33,851 --> 00:05:37,570
Like how high, how low? Think about
languages like Mandarin, right?

100
00:05:37,630 --> 00:05:41,200
Intonation matters. Like, you know,
there's like four tones in Mandarin, like,

101
00:05:43,570 --> 00:05:48,400
you know, like that. So, uh, tone matters,
duration matters. And then lastly,

102
00:05:48,401 --> 00:05:51,460
once it had all three of those things,
right, it had the phoneme count,

103
00:05:51,610 --> 00:05:53,680
it had the duration,
it had the frequency,

104
00:05:53,830 --> 00:05:56,290
it combined all of those
and fed it to a decoder.

105
00:05:56,500 --> 00:06:01,010
And what the decoder would do would be,
it would reconstruct the original audio,

106
00:06:01,100 --> 00:06:04,760
but in the context of the original voice,
right?

107
00:06:04,761 --> 00:06:08,960
So you have some text,
it predicts all of these three features,

108
00:06:09,020 --> 00:06:11,510
and then it's going to
construct the voice.

109
00:06:11,540 --> 00:06:16,340
So what would the spoken
version of the text,

110
00:06:16,341 --> 00:06:19,940
the, and that's what it did.
It wasn't converting anything
into a different voice,

111
00:06:20,120 --> 00:06:23,990
it was just reading the text.
But this was a really cool model.

112
00:06:24,050 --> 00:06:27,740
And the second step, like I said, was
predicted durations and frequencies.

113
00:06:27,980 --> 00:06:29,570
And by the way,
the way they did this,

114
00:06:29,571 --> 00:06:33,130
the way they predicted the durations
and frequencies frequencies by,

115
00:06:33,170 --> 00:06:35,930
is by using that model in the
middle that we talked about up here.

116
00:06:36,230 --> 00:06:39,920
But that model in the middle is
calledF segmentation model because it's

117
00:06:39,921 --> 00:06:44,780
segmented the audio clips
of each, uh, phony, right?

118
00:06:44,781 --> 00:06:46,130
So you've got the audio of the text,

119
00:06:46,370 --> 00:06:48,890
you at the phoneme that we're
feeding into the segmentation model,

120
00:06:49,070 --> 00:06:52,310
and it will map one to the other.
Once we've got those,

121
00:06:52,370 --> 00:06:55,430
we'll combine the phonemes of durations
and the frequencies to output a sound

122
00:06:55,431 --> 00:06:57,320
wave that represents the text.

123
00:06:58,400 --> 00:07:02,870
So they,
they'd model this after deep mines.

124
00:07:02,871 --> 00:07:06,230
Wavenet so they were inspired by
deep minds wavenet architecture,

125
00:07:06,500 --> 00:07:09,770
which looks like this.
So the deepmind wavenet architecture,

126
00:07:09,771 --> 00:07:12,980
we can make an entire video. In
fact, I, I have one on this. Uh,

127
00:07:13,010 --> 00:07:16,400
it's called generating music using
tensorflow. So check out that one.

128
00:07:16,550 --> 00:07:19,970
So wavelets architecture is really
interesting because it's able to say,

129
00:07:20,510 --> 00:07:23,990
how am I supposed to pronounce this in
the context of what I'm about to say?

130
00:07:24,200 --> 00:07:28,130
So it uses what's called an intention
mechanism to look at the input data and

131
00:07:28,131 --> 00:07:28,701
say,
okay,

132
00:07:28,701 --> 00:07:32,750
so let's say we're trying to have a
system that generates a speech of hello

133
00:07:32,751 --> 00:07:36,560
world. It's a garage. It's going to
look at the word Saroj in the future.

134
00:07:36,620 --> 00:07:38,900
To see how it should
pronounce hello world hits.

135
00:07:39,140 --> 00:07:42,610
Because you know how phrases like
they depend on like the, there's,

136
00:07:42,611 --> 00:07:43,520
there's a kind of flow,

137
00:07:43,521 --> 00:07:48,050
like an intonation flow when it comes to
phrases and saying something is always

138
00:07:48,140 --> 00:07:51,800
dependent on what else you're saying,
if that makes sense. So in a sentence,

139
00:07:51,920 --> 00:07:55,760
you're saying a couple of words and the
intonation of later words depends on the

140
00:07:55,761 --> 00:07:58,280
intonation of previous words.
It's kind of complicated,

141
00:07:58,281 --> 00:08:01,490
but it's basically looking at the
future to try to predict the past.

142
00:08:01,700 --> 00:08:03,830
And that's the attention
mechanism in this case.

143
00:08:04,250 --> 00:08:07,040
But basically it was an encoder
decoder architecture that's,

144
00:08:07,041 --> 00:08:11,510
it had two different models and then in
a few months later they released what's

145
00:08:11,511 --> 00:08:15,580
called deep voice to basically, it was a
same model, but they just scaled it up,

146
00:08:15,590 --> 00:08:18,260
meeting they,
they train it on even more speakers.

147
00:08:18,440 --> 00:08:22,640
So they were the scale ups from 20
hours of speech and a single voice to

148
00:08:22,641 --> 00:08:25,640
hundreds of hours of speech
with hundreds of voices.

149
00:08:26,900 --> 00:08:30,470
And each voice corresponds to a
single vector. So about 50 numbers,

150
00:08:30,471 --> 00:08:35,030
which summarize how to generate sounds
that imitate the target speaker. Right.

151
00:08:35,031 --> 00:08:39,890
So if we have some targets, speaker like
me, we train this model on my voice,

152
00:08:40,010 --> 00:08:44,210
it's going to generate a single vector
and that means a matrix with 50 different

153
00:08:44,211 --> 00:08:47,420
numbers in there. That's it.
And that represents my voice.

154
00:08:47,570 --> 00:08:51,980
And from that the model can generate
my voice like what we just did,

155
00:08:52,160 --> 00:08:56,880
which is really incredible. So then deep
voice three was a couple months later,

156
00:08:56,881 --> 00:09:00,670
so nearing like closer to closer to now,
which they said let's,

157
00:09:00,960 --> 00:09:03,630
let's do away with that model.
Let's create a new model.

158
00:09:03,660 --> 00:09:08,550
And it trained an order
of magnitude faster and it
allowed them to scale to over

159
00:09:08,551 --> 00:09:12,540
800 hours of training data.
Even more voices. 2,400 voices.

160
00:09:12,600 --> 00:09:13,433
Yes.

161
00:09:13,970 --> 00:09:17,330
So it looked like this. So it was still
a sequence to sequence model, right?

162
00:09:17,331 --> 00:09:19,220
It had an encoder and it had a decoder.

163
00:09:19,280 --> 00:09:22,340
But the difference was that
it uses convolutional blocks.

164
00:09:22,430 --> 00:09:26,630
So recall that convolutional networks
are used for image processing. However,

165
00:09:26,690 --> 00:09:28,160
if you look in papers recently,

166
00:09:28,280 --> 00:09:32,960
convolutional blocks are being used
more and more in a sequence learning in

167
00:09:32,961 --> 00:09:33,321
general,

168
00:09:33,321 --> 00:09:37,160
right in recurrent networks and in coder
decoder architectures because they'd

169
00:09:37,161 --> 00:09:39,200
done so well.
Our CNN for example,

170
00:09:39,201 --> 00:09:43,550
was used by Facebook research
recently to map someone's body in a,

171
00:09:43,551 --> 00:09:46,940
in a video and then turn it into
a three d model in real time.

172
00:09:47,150 --> 00:09:49,900
So convolutional networks are being used
more and more in sequence learning like

173
00:09:49,910 --> 00:09:52,400
sequences.
In that case would the brand of a video

174
00:09:54,370 --> 00:09:57,280
and what they use was a low
dimensional speaker embedding.

175
00:09:57,281 --> 00:10:01,810
So they represented every single speaker
with a different embedding and once

176
00:10:01,811 --> 00:10:03,520
they,
once they had that embedding,

177
00:10:03,550 --> 00:10:06,340
they could retrain the model
into somebody else's voice,

178
00:10:06,580 --> 00:10:09,460
which is kind of what led
up to neuro boys cloning,

179
00:10:09,461 --> 00:10:12,760
which is their most recent paper,
which came out just a few weeks ago.

180
00:10:13,180 --> 00:10:16,420
So neural flow voice cloning
is really impressive.

181
00:10:16,570 --> 00:10:21,310
It's really impressive because it was an
example of few shot generative modeling

182
00:10:21,340 --> 00:10:25,270
of speech. That means that using
only a few samples of my voice,

183
00:10:25,510 --> 00:10:26,650
like a few sentences,

184
00:10:26,980 --> 00:10:30,250
it will be able to then generate
speech in the style of meat.

185
00:10:30,370 --> 00:10:32,080
So it's like style transfer,
right?

186
00:10:32,110 --> 00:10:34,750
Taking van Gogh's paintings and
applying it to a novel paintings,

187
00:10:35,050 --> 00:10:36,160
but doing it for speech.

188
00:10:36,370 --> 00:10:39,700
And not only doing that but doing
it with just a few examples,

189
00:10:39,880 --> 00:10:43,450
which is incredible. It's incredible.
Right? So let's check this out. So,

190
00:10:43,720 --> 00:10:44,950
so what they did was they said,

191
00:10:44,951 --> 00:10:47,710
let's segment this problem
into two different categories.

192
00:10:48,010 --> 00:10:51,550
There's speech speaker adoption
and then their speaker encoding.

193
00:10:51,790 --> 00:10:53,620
So Speaker adaption is saying,

194
00:10:54,070 --> 00:10:58,180
let's say I want to say something
in the style of Kate Winslet.

195
00:10:58,480 --> 00:11:01,700
Speaker adoption is the
problem of saying I'm, I'm,

196
00:11:01,710 --> 00:11:03,250
I'm going to say something to this model.

197
00:11:03,370 --> 00:11:06,100
It's going to listen and then it's going
to output something in the style of

198
00:11:06,101 --> 00:11:08,170
Kate Winslet.
But in order to do that,

199
00:11:08,230 --> 00:11:11,860
it's got to have an embedding
a representation vector,

200
00:11:11,920 --> 00:11:14,110
essentially a vector of what I sound like.

201
00:11:14,111 --> 00:11:16,750
Remember that Matrix with 50 numbers
in it that represents the Rod.

202
00:11:17,050 --> 00:11:19,000
That is an example of speaker adoption.

203
00:11:19,090 --> 00:11:22,630
How do we train a model to
adapt to not only my voice,

204
00:11:22,631 --> 00:11:25,990
but to Kate Winslet scores? And then
there's a problem of speaker and coding.

205
00:11:26,140 --> 00:11:30,310
How do we encode my voice? How do we
represent my voice into that matrix?

206
00:11:30,520 --> 00:11:35,520
So segmenting the problem into those
two different problems helped them build

207
00:11:36,011 --> 00:11:39,690
this model. So in the case of
speaker adoption, what that,

208
00:11:39,700 --> 00:11:43,060
what it meant for them was fine tuning
a multi-speaker generative model with a

209
00:11:43,061 --> 00:11:46,660
few Connie examples and they train
the whole thing using backpropagation.

210
00:11:46,810 --> 00:11:49,990
So it was using backpropagation
for both the encoder and a decoder.

211
00:11:50,500 --> 00:11:54,400
And so what they found was when they
mapped out all the speaker embeddings that

212
00:11:54,401 --> 00:11:57,160
they learned from all the different
speakers that they trained on,

213
00:11:57,520 --> 00:11:58,900
they found that,
uh,

214
00:11:59,020 --> 00:12:02,770
there were clusters around
speakers who had the sim, how,

215
00:12:02,771 --> 00:12:04,720
who had similar regional dialects.

216
00:12:04,960 --> 00:12:07,900
There were also clusters around
speakers who had similar genders,

217
00:12:08,230 --> 00:12:10,330
males and females,
or clustered together.

218
00:12:10,480 --> 00:12:15,220
So males from a specific region all
cluster together in terms of where they

219
00:12:15,221 --> 00:12:19,270
weren't in this latent space. Right?
This embedding space, the learn space,

220
00:12:19,600 --> 00:12:21,160
which is really cool
if you think about it.

221
00:12:21,190 --> 00:12:26,080
There's a lot we can learn from that
in terms of how language works and how

222
00:12:26,081 --> 00:12:28,780
different people relate to each other
in terms of their language and where

223
00:12:28,781 --> 00:12:33,430
they're from. So there's that and
it was called neuro voice synthesis.

224
00:12:33,610 --> 00:12:36,610
So I thought, okay, cool. Let's
see, let's see what we can do here.

225
00:12:36,611 --> 00:12:39,790
Obviously they haven't really started
code, but they did release the paper.

226
00:12:39,791 --> 00:12:42,380
I took a look at the paper. It's a
really good paper, check it out. Uh,

227
00:12:42,460 --> 00:12:46,210
links are going to be in the description,
but liar bird was a way for us to,

228
00:12:46,610 --> 00:12:48,490
you know, really do
this with a web app. Uh,

229
00:12:48,500 --> 00:12:52,230
there's also this really great repository
that I found. Um, it's uh, it's all,

230
00:12:52,300 --> 00:12:54,490
it's only four months old,
so it's relatively new.

231
00:12:54,880 --> 00:12:58,180
And their architecture is very
similar to neural voice cloning,

232
00:12:58,181 --> 00:13:01,600
which is why I picked it for this video.
But basically they had two networks,

233
00:13:01,601 --> 00:13:03,040
right?
Just like the neuro voice clinic,

234
00:13:03,041 --> 00:13:04,900
like an encoder decoder type architecture.

235
00:13:05,170 --> 00:13:09,070
So the first network is an encoder
and the second one is a Dakota.

236
00:13:09,250 --> 00:13:13,150
We can also think of it as the
first network being a classifier.

237
00:13:13,450 --> 00:13:16,600
And the second network being
a synthesizer or a generator.

238
00:13:16,780 --> 00:13:17,740
Let's just look at the code here.

239
00:13:17,980 --> 00:13:20,680
So I've downloaded the code and
we're going to look at it right now.

240
00:13:20,980 --> 00:13:25,690
So under models our pie, we can
see some of the codes. So let's,

241
00:13:25,870 --> 00:13:29,200
let's make this bigger and, and see
what, see what's going on here. Okay,

242
00:13:29,201 --> 00:13:32,530
so they've got two networks here.
They've got network one and network too,

243
00:13:32,740 --> 00:13:34,630
and they're trained sequentially.
So we've,

244
00:13:34,720 --> 00:13:38,200
we're first going to train network one
and then we're going to try network to in

245
00:13:38,201 --> 00:13:41,380
this code. So network one is
going to act as a classifier.

246
00:13:41,620 --> 00:13:44,440
What they did was they trained it
on what's called a timid dataset.

247
00:13:44,620 --> 00:13:49,130
These are 630 speakers with
the label being the, uh,

248
00:13:49,240 --> 00:13:52,550
phonemes for each speaker, right? So if
a speaker says, you know, hello world,

249
00:13:52,551 --> 00:13:53,110
it's Raj.

250
00:13:53,110 --> 00:13:58,000
The label for that input data
is going to be h e y l two,

251
00:13:58,010 --> 00:14:00,760
right? The phonemes for the,
for that, for those words.

252
00:14:01,120 --> 00:14:04,630
And when it comes to
input data with labels,

253
00:14:04,720 --> 00:14:06,190
it's a form of supervised learning.

254
00:14:06,340 --> 00:14:10,750
So what the first network
did was essentially it was
just learning the mapping

255
00:14:10,751 --> 00:14:12,580
between the input data and the phoneme,
right?

256
00:14:12,760 --> 00:14:17,410
So then if you give it some novel speaker
like who, who it's never seen before,

257
00:14:17,560 --> 00:14:20,560
it'll be able to predict what
the phonemes are over time.

258
00:14:20,680 --> 00:14:23,980
So remember in neural networks are just
a series of matrix operations, right?

259
00:14:24,160 --> 00:14:27,550
And these are just numbers and operations
that are being applied to and using

260
00:14:27,551 --> 00:14:32,140
some optimization scheme
like backpropagation or
whatever genetic algorithms,

261
00:14:32,320 --> 00:14:34,990
it will slowly update the
wait values over time.

262
00:14:35,110 --> 00:14:37,270
So that that whenever
we have some input data,

263
00:14:37,420 --> 00:14:42,020
it's going to hit the right numbers
in those matrices to then output the,

264
00:14:42,021 --> 00:14:44,050
the most likely prediction,
right?

265
00:14:44,140 --> 00:14:46,750
Over time it's going to be
trained by minimizing the error,

266
00:14:46,780 --> 00:14:49,330
like how far off it is
from the actual label.

267
00:14:50,560 --> 00:14:53,240
So when you give it some novel speaker,
it's never seen before,

268
00:14:53,360 --> 00:14:56,890
it's able to predict what the
phonemes would be just because, uh,

269
00:14:57,080 --> 00:14:59,750
there's a likelihood that it's going to
be that based on what I've seen before.

270
00:15:00,260 --> 00:15:02,570
So if we look at this
for the first network,

271
00:15:02,600 --> 00:15:06,230
it first loads up that vocabulary right?
The phoneme to Idx,

272
00:15:06,231 --> 00:15:08,620
to a phoneme IDX to phoneme.

273
00:15:08,660 --> 00:15:11,060
And basically that those are
the labels and the phonemes,

274
00:15:11,061 --> 00:15:14,810
like the words and the phone names for
all those speakers. And then it does some,

275
00:15:15,110 --> 00:15:19,340
it does a form of data preprocessing by
using a very small neural network just

276
00:15:19,341 --> 00:15:22,280
to make sure that the data is normalized.
So they called that a pre net.

277
00:15:22,580 --> 00:15:26,150
Then once it's, once it's
got that a normalized data,
it feeds it to the network.

278
00:15:26,151 --> 00:15:29,300
So they call it here
in this code C, B H, g.

279
00:15:29,480 --> 00:15:33,230
But if we look in the, uh, modules

280
00:15:34,800 --> 00:15:38,630
CBHD, uh, it's actually just
a convolutional network,
right? So it's just, uh,

281
00:15:38,780 --> 00:15:41,120
it's just layers of convolutions
over and over and over again.

282
00:15:41,280 --> 00:15:45,170
Accomplish one normalized convolution to
normalize pollution, three normalized.

283
00:15:45,200 --> 00:15:50,180
And then if we go back to the models,
we'll see that, okay, it's, it's,

284
00:15:50,181 --> 00:15:54,740
it's got the input data, it's got the
labels and it has the, it has the output,

285
00:15:54,741 --> 00:15:57,500
right? So what is it predicted
label? And then lastly,

286
00:15:57,501 --> 00:16:00,980
it does a final linear projection
by using a fully connected layer.

287
00:16:01,160 --> 00:16:02,030
This is usually,

288
00:16:02,090 --> 00:16:05,390
this is like a very common thing in
neural networks to have your last layer of

289
00:16:05,391 --> 00:16:09,290
your network actually just be fully
connected or a dense layer in the case of

290
00:16:09,300 --> 00:16:10,133
tensorflow,

291
00:16:10,300 --> 00:16:14,090
a soft Max and then output a class
probability and then the predictions are

292
00:16:14,091 --> 00:16:17,900
going to be formatted using two and
32 and then we return all of those,

293
00:16:19,190 --> 00:16:19,430
right?

294
00:16:19,430 --> 00:16:23,000
So that's the first step to train this
first network on the input data and the

295
00:16:23,001 --> 00:16:25,490
phonemes. And once we've got
that fully trained network,

296
00:16:25,670 --> 00:16:28,310
then we can train the next network.
By the way,

297
00:16:28,311 --> 00:16:32,330
the loss function they're using for that
person network is cross entropy loss,

298
00:16:32,740 --> 00:16:36,260
which means we're just going to do them
essentially the mean squared error.

299
00:16:36,290 --> 00:16:39,800
That's just a different way of saying
mean squared error taking the mean of the,

300
00:16:40,130 --> 00:16:43,070
of the uh,
actual,

301
00:16:43,071 --> 00:16:45,950
the prediction minus the
actual label squaring it.

302
00:16:45,980 --> 00:16:49,140
And then returning that for the
second network. It's going to say, oh,

303
00:16:49,170 --> 00:16:53,270
we've got the first network
train knowledge generates
some new, some new audio,

304
00:16:53,271 --> 00:16:54,650
right?
So how is it going to do this?

305
00:16:54,920 --> 00:16:59,570
So when it comes to the new audio that
what they used was they used some speaker

306
00:16:59,571 --> 00:17:00,920
Dataset.
So for Kate Winslet,

307
00:17:01,040 --> 00:17:05,480
they used two hours of audio books
read by Kate Winslet. Okay, so,

308
00:17:05,840 --> 00:17:07,490
so what I mean is they said,

309
00:17:07,640 --> 00:17:11,750
let's first input what Caitlin's s says,

310
00:17:11,810 --> 00:17:16,010
that wave form into the first model,
right? So it's a fully trained model.

311
00:17:16,160 --> 00:17:20,390
They fed the, what Kate Winslet said, not
what the timid data sets speaker said,

312
00:17:20,480 --> 00:17:23,240
what Kate Winslet said to
that fully trained model.

313
00:17:23,480 --> 00:17:25,550
Now what's it going to output
after having been trained?

314
00:17:26,930 --> 00:17:29,240
It's going to output the
phonemes. Exactly, yes.

315
00:17:29,420 --> 00:17:32,330
Once it's got the phonemes
for what Kate wins, we'll say,

316
00:17:32,510 --> 00:17:36,830
then it's going to feed those phonemes
to this second network, right?

317
00:17:36,831 --> 00:17:39,380
Which is the speech synthesizer network.
Again,

318
00:17:39,410 --> 00:17:42,590
it's going to do some data processing
using this pre network again,

319
00:17:42,860 --> 00:17:46,160
and then it's going to feed it to the
same kind of convolutional network, right?

320
00:17:46,161 --> 00:17:49,770
It's, it's a similar convolutional
network, but it's in the reverse, right?

321
00:17:49,920 --> 00:17:54,390
So remember neural networks are just
a series of matrix operations, right?

322
00:17:54,540 --> 00:17:57,030
You have some input data,
you do some majors multiplication,

323
00:17:57,031 --> 00:18:00,600
you do an activation function and then
you repeat and everything's really a

324
00:18:00,601 --> 00:18:01,970
glorified version of that.

325
00:18:01,971 --> 00:18:05,430
And this just matrix multiplication
over and over and over and over again.

326
00:18:05,790 --> 00:18:07,890
And when it comes to deep learning,
research,

327
00:18:08,070 --> 00:18:11,550
deep learning research is this playing
around with what types of operations

328
00:18:11,551 --> 00:18:15,810
we're using, what sequence we're doing
those operations in. And in the end,

329
00:18:16,170 --> 00:18:20,040
some of the best models are
just researchers playing
around with what should

330
00:18:20,041 --> 00:18:23,640
come first or should we try
this multiplication here
or should we divide this

331
00:18:23,641 --> 00:18:27,570
and then use the mean? There's this
great xkcd where the guy's just like,

332
00:18:27,600 --> 00:18:30,330
how'd your deep learning model works
so well? And the other guys just like,

333
00:18:30,540 --> 00:18:30,901
I don't know,

334
00:18:30,901 --> 00:18:34,350
I just jumbled up all this linear Algebra
together and it made us great output.

335
00:18:34,380 --> 00:18:36,510
And it's basically the appointing
research in a nutshell.

336
00:18:37,440 --> 00:18:38,700
When it comes to the second network,

337
00:18:38,850 --> 00:18:41,040
it was able to generate
audio from the phonemes.

338
00:18:41,250 --> 00:18:44,490
So the way it did that was it use
a different type of loss function.

339
00:18:44,760 --> 00:18:47,910
And the different types of the different
types of loss function was called a

340
00:18:47,911 --> 00:18:50,280
reconstruction loss.
So we could see it right here.

341
00:18:50,310 --> 00:18:52,620
So it use a reconstruction loss.

342
00:18:54,390 --> 00:18:58,570
It had the phonemes and using those
phonemes it applied the series of majors

343
00:18:58,590 --> 00:19:01,980
operations in this convolutional network
that at first it knew nothing, right?

344
00:19:02,040 --> 00:19:05,280
So it's just that Dah, Dah, Dah, Dah, Dah.
Here's the output, a jumble of numbers.

345
00:19:05,640 --> 00:19:07,080
And so that jumble of numbers,

346
00:19:07,150 --> 00:19:11,460
they compared it to the original
what Kate Winslet set, right?

347
00:19:11,790 --> 00:19:15,690
What did it, what it Kaywin's that
originally say that's the actual outputs.

348
00:19:15,900 --> 00:19:18,420
So then it computer or
reconstruction laws,

349
00:19:18,600 --> 00:19:22,410
which is how well could it reconstruct
what Kate Winslet said from the phonemes

350
00:19:22,411 --> 00:19:24,750
that were generated from the first model.
You see what I'm saying?

351
00:19:25,110 --> 00:19:26,610
It fed what Kate ones.

352
00:19:26,611 --> 00:19:30,990
I said to the first model output phonemes
phonemes to the second model output,

353
00:19:30,991 --> 00:19:34,260
what the, the reconstructed
version of what Caitlin's and say,

354
00:19:34,380 --> 00:19:37,950
which is actually just a bunch of numbers
at first and compared that output to

355
00:19:37,951 --> 00:19:41,640
what Kate ones, it's voice originally
was that the reconstruction error,

356
00:19:42,120 --> 00:19:44,760
they'll the loss between those
and it used that difference.

357
00:19:44,761 --> 00:19:48,780
And it minimized that difference over
time using backpropagation. Okay.

358
00:19:48,810 --> 00:19:53,810
So over time then the second model was
able to better reconstruct the Kaywin's.

359
00:19:55,340 --> 00:19:57,750
It's voice over time.
Now that's just Kate Winslet.

360
00:19:57,900 --> 00:20:01,530
If we kept beating at different speakers,
it will learn multiple speakers, right?

361
00:20:01,531 --> 00:20:04,470
So it could reconstruct all of
those different speakers over time.

362
00:20:04,680 --> 00:20:06,210
But they just fed Kate Winslet.

363
00:20:06,360 --> 00:20:09,240
So eventually right after
training both models in that way.

364
00:20:09,330 --> 00:20:13,740
So the first was a classifier
trained using cross entropy loss.

365
00:20:14,340 --> 00:20:18,630
The second was a generator and the loss
was instead of reconstruction loss,

366
00:20:18,930 --> 00:20:20,970
the first was trained
on a million speakers.

367
00:20:21,060 --> 00:20:24,480
The second was trade on just Kate Winslet.
So then if I say something,

368
00:20:24,660 --> 00:20:28,650
it will go through the first, create a
phoneme, list that peanut to the second,

369
00:20:28,800 --> 00:20:32,370
and then you'll reach, you'll reconstruct
it as Kate Winslet after training.

370
00:20:32,820 --> 00:20:35,610
So that's, that's how they did that.
And they did it using tensorflow.

371
00:20:35,730 --> 00:20:40,530
The model works, um, the guy even has
a, a demo of it as well, work with,

372
00:20:40,540 --> 00:20:43,320
and where can we see them? It's
on soundcloud, you called it.

373
00:20:43,790 --> 00:20:47,530
What's the culture of the revolution
thus far? Had exhausted the young cub

374
00:20:51,950 --> 00:20:53,140
it's seems,

375
00:20:53,710 --> 00:20:57,040
yeah, you can check out the demo there
as well, but yeah, pretty cool stuff.

376
00:20:57,100 --> 00:20:57,933
All the lengths that up,

377
00:20:58,060 --> 00:21:00,670
the stuff I've talked about are going to
be in the video description and I hope

378
00:21:00,671 --> 00:21:01,750
you found this video useful.

