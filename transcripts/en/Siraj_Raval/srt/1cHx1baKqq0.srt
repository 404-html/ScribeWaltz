1
00:00:00,060 --> 00:00:00,811
Bowling world,

2
00:00:00,811 --> 00:00:05,811
it's Saroj and today we're going to do
some GPU programming to add the elements

3
00:00:05,821 --> 00:00:09,630
of two arrays together.
Sound super simple, I know,

4
00:00:09,631 --> 00:00:14,070
but it will be a way for us to introduce
ourselves to the key concepts that make

5
00:00:14,071 --> 00:00:18,530
up the incredible world of parallel
computing used heavily in deep learning.

6
00:00:18,900 --> 00:00:22,830
The field of computer graphics traces
its roots back to the earliest days of

7
00:00:22,831 --> 00:00:23,790
computer science,

8
00:00:24,000 --> 00:00:29,000
HP and IBM or some of the first companies
to introduce silicon hardware capable

9
00:00:29,491 --> 00:00:34,200
of generating imagery which helped fuel
the decades long transformation of the

10
00:00:34,201 --> 00:00:38,460
southern SF bay area into the
technology hub known as Silicon Valley.

11
00:00:38,580 --> 00:00:42,360
There were endless applications of
computer graphics in everything from

12
00:00:42,450 --> 00:00:45,750
scientific simulations
to movies to video games.

13
00:00:46,050 --> 00:00:51,050
Programmers created graphical software
and that software was broken down into a

14
00:00:51,571 --> 00:00:56,190
series of instructions that the central
processing unit of a computer was

15
00:00:56,191 --> 00:00:57,780
capable of carrying out.

16
00:00:58,140 --> 00:01:02,580
The CPU is the main chip in a computer
responsible for telling all the other

17
00:01:02,581 --> 00:01:07,581
components in a computer what to do using
it's given instructions just like your

18
00:01:07,741 --> 00:01:09,300
friendly neighborhood dictator.

19
00:01:10,320 --> 00:01:14,220
If we look at the difference between
your early games and modern day games,

20
00:01:14,340 --> 00:01:18,000
it is truly remarkable how much of
a difference in realism there is.

21
00:01:18,270 --> 00:01:22,500
The process of rendering a seen consists
of a number of steps a computer must

22
00:01:22,501 --> 00:01:27,501
perform in a specific sequence and the
sequence is called the graphics pipeline.

23
00:01:28,410 --> 00:01:31,140
The first step is to set up
the objects in the scene.

24
00:01:31,260 --> 00:01:35,550
Object models are a mesh of
triangles that describe its shape.

25
00:01:35,760 --> 00:01:40,440
This is done by listing the
exact coordinates of every
corner of every triangle.

26
00:01:40,620 --> 00:01:44,070
Coordinate systems need an
origin and in a model's file,

27
00:01:44,130 --> 00:01:47,850
it's just an arbitrary place, but
the place, the model in the scene,

28
00:01:47,940 --> 00:01:52,410
all the vertex coordinates need to be
transformed so that the origin is now a

29
00:01:52,411 --> 00:01:55,920
place in the scene.
Once the objects are placed in the scene,

30
00:01:56,040 --> 00:01:59,250
the coordinates are again
transformed into camera space.

31
00:01:59,280 --> 00:02:03,420
These transformations require the use
of matrix operations for older games.

32
00:02:03,421 --> 00:02:07,080
With simple models.
This was entirely doable by a CPU,

33
00:02:07,320 --> 00:02:09,270
but as models got more complex,

34
00:02:09,420 --> 00:02:13,740
the amount of transformations outgrew
the capabilities of a CPU alone.

35
00:02:13,860 --> 00:02:16,630
So a company named and video
came along and released.

36
00:02:16,650 --> 00:02:21,650
The first graphics processing unit in
99 capable of hardware transform and

37
00:02:21,921 --> 00:02:26,921
lighting a CPU consists of a few
cores optimize for sequential serial

38
00:02:27,271 --> 00:02:28,104
processing.

39
00:02:28,260 --> 00:02:33,260
While a GPU has a massively parallel
architecture consisting of thousands of

40
00:02:33,541 --> 00:02:34,260
smaller,

41
00:02:34,260 --> 00:02:38,670
more efficient course designed for
handling multiple tasks simultaneously.

42
00:02:38,790 --> 00:02:42,930
So while traditionally software has been
written for serial computation where

43
00:02:42,931 --> 00:02:45,510
instructions are executed
one after the other,

44
00:02:45,660 --> 00:02:50,280
there are some algorithms that can be
framed to utilize parallel computation.

45
00:02:50,550 --> 00:02:55,020
Searching for a specific
word in a document, for
example, can be done serially.

46
00:02:55,140 --> 00:02:58,350
That means iterating through
every word from start to finish,

47
00:02:58,410 --> 00:03:02,800
but it can also be done in parallel by
having 10 different workers search a

48
00:03:02,801 --> 00:03:06,850
different section of the document
for our target word at the same time.

49
00:03:07,120 --> 00:03:12,070
But some algorithms like calculating
the Nth FIBONACCI number can't be made

50
00:03:12,071 --> 00:03:16,210
parallel since we can only find it after
the proceeding to Fibonacci numbers

51
00:03:16,350 --> 00:03:17,890
have been calculated in order.

52
00:03:17,980 --> 00:03:21,820
Matrix operations for deep learning
can easily be parallelized.

53
00:03:22,000 --> 00:03:25,270
Deep neural networks can have
millions of parameters to train,

54
00:03:25,450 --> 00:03:29,310
all of which can be done much
faster by utilizing GPU. Yes,

55
00:03:29,560 --> 00:03:33,550
there are several programming frameworks
out there that led programmers harness

56
00:03:33,640 --> 00:03:37,090
GPU course like open cl opiod ACC,

57
00:03:37,210 --> 00:03:39,730
but the most popular is Nvidia's Kuda,

58
00:03:39,850 --> 00:03:43,360
which stands for compute
unified device architecture.

59
00:03:43,570 --> 00:03:48,570
Kuda is a toolkit that acts
as an extension of c with
its own programming model

60
00:03:48,970 --> 00:03:52,990
that lets programmers run
their code on and videos gps.

61
00:03:53,290 --> 00:03:58,290
The Cu Dnn library built on top of Kuta
specifically for deep learning is used

62
00:03:58,391 --> 00:04:03,280
by every single major framework from
tensorflow two Pi Torch under the hood.

63
00:04:03,580 --> 00:04:08,020
So let's talk about Gpu architecture
in the way that could have used it.

64
00:04:08,320 --> 00:04:11,260
The key part of Kuta code
is the kernel program.

65
00:04:11,500 --> 00:04:16,030
The kernel is a function that can
be executed in parallel on the GPU.

66
00:04:16,270 --> 00:04:18,730
It's executed by an array of Kuta threats.

67
00:04:19,000 --> 00:04:23,770
All threads run the same code and each
thread has an id that it uses to compute

68
00:04:23,771 --> 00:04:26,230
memory addresses and
make control decisions.

69
00:04:26,410 --> 00:04:30,880
You can run thousands of these threads
on the GPU and Kuda organizes threads

70
00:04:30,881 --> 00:04:33,850
into a grid hierarchy of thread blocks.

71
00:04:34,030 --> 00:04:37,960
A grid is a set of thread blocks
that can be processed on the device.

72
00:04:37,990 --> 00:04:38,823
In parallel.

73
00:04:38,980 --> 00:04:43,210
Each thread block is a set of concurrent
threads that can cooperate amongst

74
00:04:43,211 --> 00:04:47,080
themselves and access a shared
memory space private to the block.

75
00:04:47,260 --> 00:04:51,820
It's the programmer's job to specify the
grid glock organization on each kernel

76
00:04:51,821 --> 00:04:56,080
call since it can be different each time
within the limits set by their specific

77
00:04:56,081 --> 00:05:00,730
GPU. Let's write some Kuta code to get
a better understanding of how all these

78
00:05:00,731 --> 00:05:05,050
components play together.
We'll start with some basic c plus plus.

79
00:05:05,200 --> 00:05:09,550
Our program will simply add the elements
of two arrays with a million elements,

80
00:05:09,551 --> 00:05:12,460
each only two dependencies io stream,

81
00:05:12,550 --> 00:05:17,050
which will help us take input commands
and show output results and math,

82
00:05:17,110 --> 00:05:21,130
which defines various math operations
we can use in the main function.

83
00:05:21,220 --> 00:05:25,120
We'll initialize an integer array with
space to store a million elements.

84
00:05:25,480 --> 00:05:29,770
Then we'll initialize pointers
to both of our arrays, x and Y,

85
00:05:29,890 --> 00:05:33,970
which space allocated for both of them
using our previously initialized variable

86
00:05:33,971 --> 00:05:38,950
and we can fill both arrays by creating
a for loop and filling each index and

87
00:05:38,951 --> 00:05:40,900
each array with float values.

88
00:05:41,080 --> 00:05:46,080
We'll use both filled arrays
and the number of elements
in each as parameters to

89
00:05:46,091 --> 00:05:48,220
an add function.
In our ad function.

90
00:05:48,221 --> 00:05:52,750
We'll create a for loop to add all end
elements in each array and set the result

91
00:05:52,751 --> 00:05:56,350
to each index in the y. Ira.
After this loop is finished,

92
00:05:56,410 --> 00:06:00,890
why contain the sum of each elements
in both arrays? When we're done,

93
00:06:00,891 --> 00:06:05,720
we can free the memory from both a res
and exit the program by returning zero.

94
00:06:05,930 --> 00:06:10,930
We can save the code in this file as
ad dot CPP and compile it with a c plus

95
00:06:10,971 --> 00:06:15,170
plus compiler claim for me since I'm on
Mac g plus plus if you're on windows or

96
00:06:15,171 --> 00:06:18,200
Linux,
that was a simple c plus plus program.

97
00:06:18,230 --> 00:06:22,910
No Kuta needed but adding the elements
in both arrays can be parallelized on

98
00:06:22,911 --> 00:06:25,400
many cores of the GPU,
so let's get turnt.

99
00:06:25,880 --> 00:06:29,780
The first step for us is a Turner had
function into a function that the GPU can

100
00:06:29,781 --> 00:06:31,610
run called a colonel in Kuda.

101
00:06:31,850 --> 00:06:36,320
All we have to do is at the global
specifier to the function and that'll tell

102
00:06:36,321 --> 00:06:40,880
the Kuda c plus plus compiler that this
is a function that runs on the Gpu and

103
00:06:40,881 --> 00:06:42,920
can be called from CPU code.

104
00:06:43,160 --> 00:06:47,900
We also need to allocate memory that is
accessible by not just the CPU but the

105
00:06:47,901 --> 00:06:48,890
GPU as well.

106
00:06:49,070 --> 00:06:53,000
Karuna uses the concept of
unified memory to make this easy.

107
00:06:53,150 --> 00:06:58,150
It provides a single memory space
accessible by all gps ncps on your system.

108
00:06:58,550 --> 00:07:02,780
We'll need to replace the calls to new
in our code with calls to Kuta Malaak

109
00:07:02,781 --> 00:07:03,614
managed.

110
00:07:03,710 --> 00:07:08,710
This will allocate data in unified
memory and returns a pointer that we can

111
00:07:08,721 --> 00:07:12,530
access from CPU or Gpu code.
After this,

112
00:07:12,590 --> 00:07:17,540
we can launch the ad colonel on the Gpu
by using the triple angle bracket syntax.

113
00:07:17,750 --> 00:07:22,580
We'll just add it to the call
to add before the parameter
list and the CPU needs

114
00:07:22,581 --> 00:07:25,910
to wait until the kernel is done
before it accesses the results.

115
00:07:25,910 --> 00:07:29,630
Because Kuda colonel launches
don't block the calling CPU thread.

116
00:07:29,810 --> 00:07:33,110
We can make this happen by
calling Kuda device synchronize.

117
00:07:33,230 --> 00:07:38,230
Before freeing the memory are one liner
launched one GPU thread to run ad.

118
00:07:38,960 --> 00:07:43,960
We can save this code in a file called
ad.cu for Cuda and compile it with NBCC,

119
00:07:44,990 --> 00:07:47,360
the Kuda c plus plus compiler as written.

120
00:07:47,361 --> 00:07:51,350
The kernel is only correct for a single
thread. Since every thread that it runs,

121
00:07:51,440 --> 00:07:53,330
we'll perform add on the whole array.

122
00:07:53,480 --> 00:07:58,280
We can find out how long it
takes to colonel to run by
using the GPU profiler as

123
00:07:58,281 --> 00:08:00,410
well,
but can we make it faster?

124
00:08:00,740 --> 00:08:05,150
Our execution configuration tells the
cooter runtime how many parallel threads

125
00:08:05,151 --> 00:08:09,050
to use for the launch on the GPU.
There are two parameters here.

126
00:08:09,230 --> 00:08:10,760
Let's change the second one,

127
00:08:10,850 --> 00:08:14,740
which is the number of threads
in a thread block Kuda gps,

128
00:08:14,780 --> 00:08:19,780
Ron kernels using blocks of threads
that are a multiple of 32 in size,

129
00:08:19,880 --> 00:08:24,880
so let's try to 56 but doing this alone
will cause it to do the computation once

130
00:08:25,491 --> 00:08:29,810
per thread rather than spreading the
computation across parallel threads.

131
00:08:29,960 --> 00:08:32,720
To do it properly,
we need to modify the kernel.

132
00:08:32,990 --> 00:08:37,430
We can get the indices of the running
threads using the thread id keyword for

133
00:08:37,431 --> 00:08:40,660
the current thread and it's
block and the block dim keyword.

134
00:08:40,670 --> 00:08:42,380
For the number of threads in the block,

135
00:08:42,560 --> 00:08:46,340
we can modify the loop to stride
through the array with parallel threats.

136
00:08:46,490 --> 00:08:49,730
If we save it and run it again,
we'll see a huge speedup.

137
00:08:49,940 --> 00:08:54,680
It went from using one thread to
two 56 threads, so this makes sense.

138
00:08:54,830 --> 00:08:56,310
To summarize what we've learned.

139
00:08:56,311 --> 00:09:01,311
Kuda is a toolkit that lets programmers
use Nvidia's Gpu to parallelize their

140
00:09:01,771 --> 00:09:02,400
code.

141
00:09:02,400 --> 00:09:07,400
It's an extension of see with its own
programming model consisting of threads,

142
00:09:07,470 --> 00:09:12,470
blocks and grids and the newer version
uses a unified memory architecture which

143
00:09:13,081 --> 00:09:17,340
lets us access allocated
data from CPU and Gpu Code.

144
00:09:17,590 --> 00:09:20,530
Winter of last week's coding
challenges out there too.

145
00:09:21,250 --> 00:09:25,030
He used to neural network based models
that he implemented in tensorflow.

146
00:09:25,260 --> 00:09:29,860
We popular indicators from the field of
market analysis to predict trends give

147
00:09:29,861 --> 00:09:33,280
his costar star on get hub
and the runner up is who top,

148
00:09:33,760 --> 00:09:38,510
who converted to stock prices into audio
data and ran deep minds wavenet on it.

149
00:09:39,070 --> 00:09:41,650
What a while and brilliant idea.
I love it.

150
00:09:42,050 --> 00:09:46,460
This week's challenge is to add Kuda to
a simple algorithm and benchmark just

151
00:09:46,461 --> 00:09:50,330
how fast the speedup was.
Details are in the read me poster,

152
00:09:50,331 --> 00:09:53,420
get hope link in the comments and
winners will be announced next week.

153
00:09:53,510 --> 00:09:56,000
I hope you liked this video.
Please subscribe if you did,

154
00:09:56,001 --> 00:09:59,690
and for now I've got to free up
some memory, so thanks for watching.

