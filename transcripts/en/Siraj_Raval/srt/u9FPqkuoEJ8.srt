1
00:00:00,060 --> 00:00:03,030
Hello world,
it's Saroj and let's make our own speech.

2
00:00:03,031 --> 00:00:04,620
Recognizer using tensorflow,

3
00:00:04,860 --> 00:00:08,310
speech recognition has gotten so
much better in the past few decades.

4
00:00:08,700 --> 00:00:12,690
In the 50s the general consensus amongst
computer scientists was that speech

5
00:00:12,691 --> 00:00:15,870
signals needed to first be split
into little phonetic units.

6
00:00:16,170 --> 00:00:18,210
Then those units could
be grouped into words,

7
00:00:18,420 --> 00:00:20,370
but even though this seems
like it would work well,

8
00:00:20,580 --> 00:00:22,680
this approach did not
give us good results.

9
00:00:22,890 --> 00:00:27,890
The first ever speech
recognizer was called Audrey
by bell labs in 1952 it could

10
00:00:27,990 --> 00:00:32,280
only recognize spoken numbers between
one and nine and was built with analog

11
00:00:32,310 --> 00:00:36,480
electronic circuits. The
renowned scientist and BP
at Bell Labs, John Pearce,

12
00:00:36,720 --> 00:00:40,200
Dan speech recognition research because
the results weren't promising enough,

13
00:00:40,410 --> 00:00:43,980
but a small group of visionaries at
a newly formed team called Darpa,

14
00:00:44,160 --> 00:00:47,730
went against popular opinion and
created a system called heartbeat.

15
00:00:47,790 --> 00:00:52,410
It used 15,000 interconnected nodes and
each represented all of the possible

16
00:00:52,440 --> 00:00:53,700
utterances within the domain.

17
00:00:54,060 --> 00:00:58,500
That youth a brut force search algorithm
to map the speech to the right notes to

18
00:00:58,501 --> 00:01:02,130
get the text. This approach was
slightly better, but then it blew,

19
00:01:02,131 --> 00:01:04,770
invented something called
the hidden Markov Model.

20
00:01:04,840 --> 00:01:09,150
Hmms represented utterances as states
and probabilistically predicted what a

21
00:01:09,151 --> 00:01:13,740
word was given the phenoms it was made
up of when words like you are pronounced,

22
00:01:13,800 --> 00:01:18,800
they can have different durations like
you or you and h and m's captured the

23
00:01:19,051 --> 00:01:22,290
plasticity of words by using
a probabilistic approach.

24
00:01:22,500 --> 00:01:26,070
The hmm pretty much maintained its
position as king of speech recognition

25
00:01:26,340 --> 00:01:29,850
throughout the eighties and nineties as
researchers improve them more and more,

26
00:01:29,910 --> 00:01:33,960
and some Weirdos kept trying this dumb
technique called artificial neural

27
00:01:33,961 --> 00:01:37,980
networks, but of course it didn't
get good results. One of them,

28
00:01:37,981 --> 00:01:42,300
this Guy Geoffrey Hinton kept on trying
out neural networks until all of a

29
00:01:42,301 --> 00:01:45,900
sudden, a couple of years ago, it
started outperforming everything.

30
00:01:45,960 --> 00:01:47,150
Did people tell you,
Jeffrey,

31
00:01:47,160 --> 00:01:49,650
you're wasting your time many
times and would you say back,

32
00:01:49,740 --> 00:01:51,750
give me another six months and
I'll prove to you that it works.

33
00:01:53,820 --> 00:01:58,110
The key was to give it more
data and computing power.
This is deep learning. Now.

34
00:01:58,111 --> 00:02:02,460
These deep neural nets are how services
like Siri and echo and Google now hear

35
00:02:02,461 --> 00:02:06,000
you speak, and with Google's machine
learning framework tensor flow,

36
00:02:06,360 --> 00:02:10,410
we're going to build our own deep neural
network that learns to recognize spoken

37
00:02:10,411 --> 00:02:14,580
numbers. We're going to download a
labeled Dataset of people saying numbers,

38
00:02:14,880 --> 00:02:18,690
build a neural network, train it
on that data, then test it out.

39
00:02:18,710 --> 00:02:20,940
See if we can recognize
other spoken numbers.

40
00:02:21,180 --> 00:02:25,290
It'll be 20 lines of python code and
I'll explain things as we go. Ready?

41
00:02:25,350 --> 00:02:26,970
Let's first import TF Lauren.

42
00:02:27,070 --> 00:02:31,020
Tia Florent is a high level library built
on top of tensorflow that is easier to

43
00:02:31,021 --> 00:02:32,910
read and great for the past prototyping.

44
00:02:33,150 --> 00:02:36,210
Our other import is a helper class
we've created called speech data.

45
00:02:36,450 --> 00:02:39,270
This will help fetch data from
the web and format it for us.

46
00:02:39,540 --> 00:02:40,800
Now that we have our libraries,

47
00:02:40,890 --> 00:02:44,400
let's define our hyper parameters or
tuning knobs. We have three of them.

48
00:02:44,610 --> 00:02:46,290
The first one is the learning rate.

49
00:02:46,410 --> 00:02:50,250
The learning rate is what we apply
to this weight updating process.

50
00:02:50,340 --> 00:02:53,040
The greater the learning rate,
the faster our network trains,

51
00:02:53,160 --> 00:02:56,610
the lower the learning rate,
the more accurate our network predicts.

52
00:02:56,760 --> 00:03:00,550
So it represents a tradeoff
between time and accuracy. Next,

53
00:03:00,551 --> 00:03:04,720
we'll define how many steps we want
to train for 300,000 we have our hyper

54
00:03:04,721 --> 00:03:06,430
parameters.
Now we can batch our data.

55
00:03:06,610 --> 00:03:09,550
This is where we'll use our help
a class speech data specifically,

56
00:03:09,551 --> 00:03:13,690
it's batch generator function. This
function will download a set of Wav files.

57
00:03:13,870 --> 00:03:16,970
Each weight file is a recording
of a different spoken digit like

58
00:03:18,910 --> 00:03:20,920
and each is labeled with a written digits.

59
00:03:20,980 --> 00:03:23,860
It will return the list of
labeled speech files as a batch.

60
00:03:24,130 --> 00:03:28,150
Then we can split our batch into training
and testing data with pythons built in

61
00:03:28,151 --> 00:03:32,140
next function. We'll use the same
data for testing for simplicity,

62
00:03:32,230 --> 00:03:36,340
so it'll be able to recognize a speaker
we trained it on, but not other speakers.

63
00:03:36,550 --> 00:03:38,560
Now that we have our
training and testing data,

64
00:03:38,740 --> 00:03:40,510
it's time to make our neural network,

65
00:03:40,720 --> 00:03:43,270
so what kind of neural network
should we use for this?

66
00:03:43,390 --> 00:03:44,920
I'm going to wrap the answer to this.

67
00:03:45,150 --> 00:03:50,150
Did you drop out? Turn it up once a one
problem is classifying images, input,

68
00:03:50,190 --> 00:03:54,780
image, output, label. Ain't no sequences.
I want to many problems. Gives them image,

69
00:03:54,781 --> 00:03:58,890
captions, input image, output
sequence, and all of them abstractions.

70
00:03:58,980 --> 00:04:03,980
A many to one problem is sentiment
analysis and puts sequence output positive

71
00:04:04,200 --> 00:04:09,190
ain't no accident a problem that's many
to many is a sequence of waves output

72
00:04:09,270 --> 00:04:11,670
sequence of words.
Then our model saves

73
00:04:11,990 --> 00:04:14,420
since spoken words are a
sequence of sound waves.

74
00:04:14,560 --> 00:04:18,500
We want to use a recurrent neural network
since they're capable of processing

75
00:04:18,501 --> 00:04:19,334
sequences,

76
00:04:19,430 --> 00:04:22,700
so we'll initialize our net by calling
TF Lauren's input data function.

77
00:04:22,910 --> 00:04:24,770
This initial input layer
will be the gateway.

78
00:04:24,771 --> 00:04:28,640
That data is fed into the network and
the parameter will help define the shape

79
00:04:28,641 --> 00:04:33,140
of our input data or as tensorflow calls
it our input tenser and tenser is a

80
00:04:33,141 --> 00:04:37,340
fancy word for a multidimensional array
of data are two parameters will be the

81
00:04:37,341 --> 00:04:38,390
width and the height.

82
00:04:38,630 --> 00:04:42,170
The width is the number of features that
are extracted from our utterances in

83
00:04:42,171 --> 00:04:46,490
our speech. Data help reclass
and the height is the Max
length of each utterance.

84
00:04:46,700 --> 00:04:51,700
For our next layer we use tee up learns
Lstm or long short term memory function

85
00:04:51,920 --> 00:04:53,640
in a recurrent net.
The output data,

86
00:04:53,641 --> 00:04:57,170
its contents is influenced not only
by the input we've just put in,

87
00:04:57,290 --> 00:05:00,620
but by the entire history of
inputs through our recurring loop.

88
00:05:00,800 --> 00:05:04,850
Lsts are the type of recurrent net that
can remember everything it's fed and

89
00:05:04,851 --> 00:05:08,720
because of that they outperform
regular recurrent nets consistently.

90
00:05:08,810 --> 00:05:12,020
We'll use our previous layer as our
first parameter it since we are feeding

91
00:05:12,021 --> 00:05:15,020
tensors from one layer to the
next and the number of neurons.

92
00:05:15,110 --> 00:05:18,470
There's not really a rule for knowing
how many neurons use in a layer.

93
00:05:18,710 --> 00:05:22,520
Two few will lead to bad predictions and
too many will overfit to our training

94
00:05:22,521 --> 00:05:26,690
data, meaning it won't generalize well
let's pick one 28 and then aren't dropout

95
00:05:26,691 --> 00:05:29,060
value,
which says how much dropout do we want?

96
00:05:29,310 --> 00:05:33,110
Dropout helps prevent overfitting by
randomly turning off some neurons during

97
00:05:33,111 --> 00:05:33,710
training,

98
00:05:33,710 --> 00:05:38,300
so data is forced to find
new paths between layers
allowing for more generalized

99
00:05:38,301 --> 00:05:40,640
model.
Our next layer will be fully connected,

100
00:05:40,641 --> 00:05:44,780
meaning every neuron in the previous
layer will be connected to its neurons and

101
00:05:44,781 --> 00:05:48,380
our number of classes are 10 since
we are only recognizing 10 digits,

102
00:05:48,740 --> 00:05:51,050
we'll set the activation
function to softmax,

103
00:05:51,170 --> 00:05:54,740
which will convert numerical
data into probabilities. Lastly,

104
00:05:54,741 --> 00:05:56,780
we'll create our output
layer as a regression,

105
00:05:56,990 --> 00:06:00,200
which will output a single
predicted number four our utterance.

106
00:06:00,440 --> 00:06:04,820
We're using the popular Adam optimizer
to minimize our categorical cross entropy

107
00:06:04,821 --> 00:06:08,300
loss function over time so we
get a more accurate prediction.

108
00:06:08,480 --> 00:06:11,570
Now we can initialize our
network using TF learns,

109
00:06:11,600 --> 00:06:15,680
deep neural net function and set tensor
board verbose to three which means we

110
00:06:15,681 --> 00:06:19,100
want a detailed visualization,
will initialize our training loop,

111
00:06:19,101 --> 00:06:22,790
then fit our model to the training and
testing data for 10 epochs with our

112
00:06:22,791 --> 00:06:24,020
specified batch size.

113
00:06:24,390 --> 00:06:27,230
Then we'll predict a spoken digits
value from our training data.

114
00:06:27,470 --> 00:06:30,830
We also make sure to save our bottle
for later use and print our results.

115
00:06:31,100 --> 00:06:32,000
Let's run this thing.

116
00:06:32,230 --> 00:06:36,140
Tf Learn has a nice log of important
training variables built in just from

117
00:06:36,141 --> 00:06:40,160
running the fit function, so we don't have
to specify what things to print after.

118
00:06:40,160 --> 00:06:42,020
It's done training,
it'll predict the digits.

119
00:06:42,140 --> 00:06:45,710
And if we wanted to we could just record
ourselves saying a number and place it

120
00:06:45,711 --> 00:06:48,800
in the data directory, then
predict that. So to break it down,

121
00:06:48,890 --> 00:06:52,880
LSTM neural networks are using
state of the art speech recognition.

122
00:06:53,120 --> 00:06:54,140
We can use TF,

123
00:06:54,141 --> 00:06:58,310
learn to quickly build and train a deep
neural network to recognize speech and

124
00:06:58,311 --> 00:07:02,210
good hyper parameters like the learning
rate are those that are balanced between

125
00:07:02,211 --> 00:07:04,220
trade offs like time and accuracy.

126
00:07:04,320 --> 00:07:08,000
The winner of the coding challenge
from the last video is mic. Then Holst.

127
00:07:08,240 --> 00:07:12,100
The challenge was to generate text in
the style of Lord of the rings is NTF

128
00:07:12,110 --> 00:07:16,690
learn mic trained an LSTM network on
again devilfish language, then headed out,

129
00:07:16,691 --> 00:07:20,170
put new Gandel fish like yellow to low,
tease her,

130
00:07:20,200 --> 00:07:22,250
refer that ass of the week.

131
00:07:22,280 --> 00:07:25,460
No new coding challenge for this video
is the challenge from the make a video

132
00:07:25,461 --> 00:07:28,460
game. But video is still running.
You haven't watched it yet.

133
00:07:28,580 --> 00:07:32,090
Check out the link to it in the
description and the due date is Thursday,

134
00:07:32,091 --> 00:07:36,410
December 15th at noon pst, which is
one week from today. Please subscribe.

135
00:07:36,411 --> 00:07:40,820
And for now, I've got to get tickets to
ICLR next year, so thanks for watching.

