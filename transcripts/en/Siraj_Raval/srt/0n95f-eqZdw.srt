1
00:00:00,060 --> 00:00:03,660
Are the real surreal. No,
I'm the real survivor. No,

2
00:00:03,960 --> 00:00:07,200
I'm the real Siraj.
Hello world.

3
00:00:07,230 --> 00:00:12,230
It's Saroj and top research lab open AI
has recently caused quite a controversy

4
00:00:12,691 --> 00:00:13,920
in the Ai Community.

5
00:00:14,250 --> 00:00:18,240
Their new AI model dubbed
Gpt to is truly remarkable.

6
00:00:18,420 --> 00:00:21,180
It's able to generate texts
that legit looks like a human,

7
00:00:21,181 --> 00:00:23,340
wrote it after being given some prompt.

8
00:00:23,610 --> 00:00:28,020
The controversial part though is that in
their blog they stated that they didn't

9
00:00:28,021 --> 00:00:32,790
want to release the fully trained
version of it because they were concerned

10
00:00:32,820 --> 00:00:36,840
about potential malicious
applications of this technology.

11
00:00:37,140 --> 00:00:39,630
Within a few days of that blog post,

12
00:00:39,660 --> 00:00:44,190
almost every single well known AI
researcher had something to say about it.

13
00:00:44,191 --> 00:00:44,970
On Twitter,

14
00:00:44,970 --> 00:00:49,200
the director of research at Nvidia
got into a flame war with their policy

15
00:00:49,201 --> 00:00:50,034
director.

16
00:00:50,130 --> 00:00:55,130
The creator of Karastan announced it
as unnecessarily hyping up research and

17
00:00:55,771 --> 00:00:59,790
many articles have now been written
about it that make it seem like the

18
00:00:59,791 --> 00:01:04,110
terminator is about to bust out of
our computer screens and kill us all.

19
00:01:04,410 --> 00:01:07,320
Just use a picture of Jigglypuff instead.

20
00:01:07,321 --> 00:01:10,830
She is the most dangerous
fighter in super smash brothers.

21
00:01:11,220 --> 00:01:12,840
The big question here is,

22
00:01:12,930 --> 00:01:17,670
should open AI have kept
their model closed or should
they have open sourced it?

23
00:01:18,000 --> 00:01:20,370
I'll answer that at the end of this video,

24
00:01:20,371 --> 00:01:25,290
but first I'll start by explaining its
potential applications, both good and bad,

25
00:01:25,560 --> 00:01:29,940
how it works in technical detail
and how we can prevent misuse.

26
00:01:30,030 --> 00:01:34,320
Open Ai has done some phenomenal
work for the Ai Community.

27
00:01:34,560 --> 00:01:39,560
Their algorithm dubbed open AI five beat
some of the world's best dota players.

28
00:01:39,991 --> 00:01:44,991
Recently they developed an algorithm
that learned how to achieve a really high

29
00:01:45,121 --> 00:01:50,121
score in the notoriously hard game
Montezuma's revenge from a single human

30
00:01:51,211 --> 00:01:52,140
demonstration.

31
00:01:52,560 --> 00:01:57,560
They trained a robotic hand to manipulate
physical objects with unprecedented

32
00:01:57,811 --> 00:02:02,340
dexterity. I wonder what else
that hand could do. Interestingly,

33
00:02:02,400 --> 00:02:06,840
a lot of the techniques they
used had existed for many years,

34
00:02:06,990 --> 00:02:09,750
but what they showed was
that with enough scale,

35
00:02:09,930 --> 00:02:14,190
these techniques will start to
surpass all previous metrics.

36
00:02:14,340 --> 00:02:19,230
Reaching a new state of the art.
Their newest model gpt to is no different.

37
00:02:19,380 --> 00:02:24,120
It's a collection of existing ideas
repackaged at scale and by scale I mean

38
00:02:24,121 --> 00:02:29,100
trained on a lot of data using a lot of
computing power as is generally the case

39
00:02:29,310 --> 00:02:31,590
when it comes to deep learning research.

40
00:02:31,950 --> 00:02:36,950
The authors used a neuro architecture
called the transformer configured it.

41
00:02:37,441 --> 00:02:40,320
What a progressively larger
number of parameters.

42
00:02:40,560 --> 00:02:45,560
Then trained it to predict the next
word in a sequence of text using a large

43
00:02:46,141 --> 00:02:50,760
dataset consisting of 40 gigabytes
of text scraped from the web.

44
00:02:51,000 --> 00:02:54,060
When given the task of writing
a response to the prompt,

45
00:02:54,090 --> 00:02:58,500
recycling is good for the world.
No, you could not be more wrong.

46
00:02:58,770 --> 00:03:03,770
The algorithm generated a
detailed coherent argument
as to why recycling is bad

47
00:03:04,031 --> 00:03:05,080
for the environment.

48
00:03:05,410 --> 00:03:09,880
One of the open AI employees printed
that response out and taped it above the

49
00:03:09,881 --> 00:03:14,881
companies recycling bin to remind people
of the power of AI when given a prompt

50
00:03:15,521 --> 00:03:20,080
for what seems like a believable news
story, it's able to generate the rest.

51
00:03:20,200 --> 00:03:23,860
I've never seen such coherent
texts generated by an AI before.

52
00:03:23,980 --> 00:03:28,780
It's more convincing than 21 savage, his
American accent, like all technology.

53
00:03:28,781 --> 00:03:33,760
Humans can use it to empower other humans
or exploit other humans in terms of

54
00:03:33,761 --> 00:03:34,594
empowerment.

55
00:03:34,660 --> 00:03:39,660
This could help end writer's block giving
aspiring authors the ability to ideate

56
00:03:39,731 --> 00:03:44,350
much faster, give it a possible scenario
and it'll generate what could happen,

57
00:03:44,380 --> 00:03:46,750
accelerating the ability to write.

58
00:03:46,960 --> 00:03:51,100
It could also allow for endless
entertainment options in gaming,

59
00:03:51,310 --> 00:03:55,990
allowing for narratives, storylines and
dialogues that are completely unique.

60
00:03:56,020 --> 00:03:58,630
Every time you play through a game,
grand theft,

61
00:03:58,631 --> 00:04:03,520
auto and gpt to is able
to not only write prompts,

62
00:04:03,640 --> 00:04:07,330
it's also able to translate texts
from one language to another.

63
00:04:07,420 --> 00:04:10,990
Summarize long articles and
answer trivia questions.

64
00:04:11,260 --> 00:04:15,970
That means we can use this as a coherent
chat bot to help companies scale their

65
00:04:15,971 --> 00:04:17,050
customer support.

66
00:04:17,290 --> 00:04:21,720
We could give it any book or article or
scientific paper we'd like and have it

67
00:04:21,730 --> 00:04:25,750
summarize the key points for us,
gpt to will save us time,

68
00:04:25,900 --> 00:04:30,820
improve our ability to write and help
create more fulfilling experiences in

69
00:04:30,821 --> 00:04:34,780
entertainment and gaming when it
comes to exploitation. However,

70
00:04:35,020 --> 00:04:38,110
this could be used to generate
misleading news articles.

71
00:04:38,320 --> 00:04:42,070
It can also be used to impersonate
others online and in general.

72
00:04:42,190 --> 00:04:47,190
This brings us even closer to completely
automating the production of abusive or

73
00:04:47,681 --> 00:04:51,250
faked content to post on
social media at scale.

74
00:04:51,460 --> 00:04:54,550
We can be sure it will be used
in all the ways I've listed.

75
00:04:54,551 --> 00:04:59,551
Both good and bad propaganda is as old
as language itself and propagandists have

76
00:04:59,741 --> 00:05:04,741
used every available technology to
scale the distribution of their message.

77
00:05:05,230 --> 00:05:10,230
What's different now is that no longer
is this kind of power limited to entities

78
00:05:10,271 --> 00:05:15,271
like governments with enough money to
buy the necessary people and machines

79
00:05:15,580 --> 00:05:17,530
capable of spreading their message.

80
00:05:17,740 --> 00:05:22,740
Now anyone anywhere can do it at scale
with a model like gpt to before we

81
00:05:24,311 --> 00:05:29,140
discuss ways of preventing misuse.
Let's discuss how it actually works.

82
00:05:29,290 --> 00:05:31,060
Starting with, of course, as always,

83
00:05:31,061 --> 00:05:36,061
the data gpt to was trained on a huge
corpus of links that were scraped from the

84
00:05:36,101 --> 00:05:36,934
Internet.

85
00:05:37,000 --> 00:05:41,230
The researchers collected their training
data by using reddit as a filter.

86
00:05:41,530 --> 00:05:44,650
They collected the most up
voted links from reddit.

87
00:05:44,770 --> 00:05:47,680
About 8 million of them
then scraped their text,

88
00:05:47,800 --> 00:05:52,270
creating a compact training Dataset
of about 40 gigabytes in size.

89
00:05:52,450 --> 00:05:57,100
They called it web text and extracted
the source text directly from html.

90
00:05:58,250 --> 00:06:02,390
Then they needed to decide how to encode
the text so that I was in a proper

91
00:06:02,450 --> 00:06:04,640
format to perform machine learning on.

92
00:06:04,910 --> 00:06:09,140
They chose a technique called
bite pair and coding or Bpe,

93
00:06:09,380 --> 00:06:11,840
which is used to encode
the input sequence.

94
00:06:12,200 --> 00:06:17,200
BPE was originally proposed as a data
compression algorithm back in the 90s when

95
00:06:17,241 --> 00:06:22,241
Green Day made good music then adopted
to solve the open vocabulary issue in

96
00:06:22,731 --> 00:06:23,900
machine translation.

97
00:06:24,320 --> 00:06:28,520
Since it's easy to run into rare and
unknown words when translating to a new

98
00:06:28,521 --> 00:06:33,260
language, since rare words can often
be decomposed into multiple sub words,

99
00:06:33,530 --> 00:06:38,530
BPE finds the best word segmentation
by iteratively and greedily merging

100
00:06:38,901 --> 00:06:40,580
frequent pairs of characters.

101
00:06:40,820 --> 00:06:45,470
It starts by treating
individual characters as tokens.
Then iteratively mergers.

102
00:06:45,471 --> 00:06:49,520
The most common token pairs
end times after tokenization,

103
00:06:49,521 --> 00:06:54,230
these tokens need to be arranged into
matrices to be ready to fed into a model

104
00:06:54,320 --> 00:06:57,170
as inputs. Num Py, the Python Matrix.

105
00:06:57,171 --> 00:07:00,140
Math library helps do
this for the model itself.

106
00:07:00,141 --> 00:07:03,860
They used what's called a transformer.
Deep minds Alpha Star,

107
00:07:03,861 --> 00:07:08,600
which be a top starcraft two player
also used a version of a transformer.

108
00:07:08,930 --> 00:07:13,610
The transformer architecture was created
by the Google brain team and it's

109
00:07:13,611 --> 00:07:18,611
turning out to be one of the largest
contributions to AI replacing recurrent

110
00:07:19,101 --> 00:07:21,890
networks as the goto model
for sequence learning,

111
00:07:22,310 --> 00:07:25,820
gpt to stands for generative
pre-training transformer to,

112
00:07:26,030 --> 00:07:30,830
it's an unsupervised language model
built on the ideas of open Ai's first

113
00:07:30,831 --> 00:07:35,330
version compared to the
original transformer
architecture proposed by Google,

114
00:07:35,570 --> 00:07:40,570
the transformer decoder model used
by open AI discards the encoder part,

115
00:07:40,640 --> 00:07:43,010
so there's only a single input sentence.

116
00:07:43,010 --> 00:07:46,430
Instead of two separate
source and target sentences.

117
00:07:46,850 --> 00:07:51,770
The model applies multiple transformer
blocks over the embeddings of input

118
00:07:51,771 --> 00:07:52,604
sequences.

119
00:07:53,000 --> 00:07:58,000
Each of these blocks contains what's
called a multi-headed self attention layer

120
00:07:58,550 --> 00:07:59,690
and a feed forward layer.

121
00:08:00,050 --> 00:08:04,580
Each of these layers perform a different
series of matrix operations on the

122
00:08:04,581 --> 00:08:09,470
input data. The final output produces
a distribution over target tokens.

123
00:08:09,620 --> 00:08:13,760
After a softmax normalization,
which is known to output probabilities.

124
00:08:14,060 --> 00:08:15,920
There aren't explicit labels here.

125
00:08:15,950 --> 00:08:20,240
It's merely asked to predict the next
word in the sequence given the previous

126
00:08:20,241 --> 00:08:21,650
words.
If anything,

127
00:08:21,651 --> 00:08:26,390
the label is actually just whatever word
it's asked to predict label section.

128
00:08:26,960 --> 00:08:30,620
For the loss function. It uses what's
called the negative log likelihood,

129
00:08:30,650 --> 00:08:35,650
but without backward computation in the
context window of size k located before

130
00:08:36,441 --> 00:08:40,940
the target word, the loss looks like
this. It's not task specific either.

131
00:08:40,970 --> 00:08:45,560
It uses the pretrained language model
directly, so say in a labeled Dataset,

132
00:08:45,590 --> 00:08:48,020
each input has end tokens and one label.

133
00:08:48,020 --> 00:08:52,580
Why gpt first processes
the inputs sequence through
the pretrained transformer

134
00:08:52,581 --> 00:08:57,180
decoder and the last layers output for
the last token is the hidden state.

135
00:08:57,570 --> 00:09:00,600
Then with only one new
trainable weight matrix,

136
00:09:00,750 --> 00:09:04,230
it can predict a distribution
over potential. Next words,

137
00:09:04,620 --> 00:09:07,020
every time it tries to
predict the next word,

138
00:09:07,050 --> 00:09:10,710
it compute an error using the difference
between the actual word and the

139
00:09:10,711 --> 00:09:13,800
predicted word and uses
that to update its weights.

140
00:09:13,890 --> 00:09:18,060
Using Goodall gradient descent linked to
how that works in the video description

141
00:09:18,210 --> 00:09:22,920
and that's it. It had zero prior
understanding of language or how it works.

142
00:09:23,070 --> 00:09:25,170
Kind of like Kanye during interviews.

143
00:09:25,530 --> 00:09:28,290
The training data isn't
even task specific,

144
00:09:28,291 --> 00:09:31,170
but after training on it
over the course of a week,

145
00:09:31,380 --> 00:09:36,240
using several cloud TPU is that likely
costs several tens of thousands of

146
00:09:36,240 --> 00:09:40,380
dollars. It was able to generate
realistic texts, summarize articles,

147
00:09:40,470 --> 00:09:43,560
translate languages and answer questions,

148
00:09:43,620 --> 00:09:46,920
so how do we prevent misuse
of this kind of technology?

149
00:09:47,070 --> 00:09:51,930
A group of Cambridge researchers created
a role playing game that anyone can

150
00:09:51,931 --> 00:09:56,931
access in the browser right now called
get bad news.com you play the role of an

151
00:09:57,601 --> 00:10:02,580
anonymous person who slowly rises to
power using an army of automated bots

152
00:10:02,610 --> 00:10:03,810
spreading fake news.

153
00:10:03,960 --> 00:10:08,610
It takes 15 minutes to play and lets you
see what it's like to create and spread

154
00:10:08,611 --> 00:10:11,340
fake news at scale.
After I played it,

155
00:10:11,341 --> 00:10:14,970
I definitely felt like I had a much more
critical eye when it came to reading

156
00:10:14,971 --> 00:10:16,740
texts that I saw online.

157
00:10:17,160 --> 00:10:22,160
Educating ourselves on the potential
for this technology is one part of the

158
00:10:22,501 --> 00:10:27,120
solution. Similar to how when people
learned about the potential for Photoshop,

159
00:10:27,121 --> 00:10:31,380
they became less likely to be fooled
when staying Photoshop pictures.

160
00:10:31,410 --> 00:10:35,040
The other part of the solution
is ironically AI. Yes.

161
00:10:35,070 --> 00:10:36,780
Using AI to fight AI,

162
00:10:36,960 --> 00:10:41,280
I found a tool called big box that uses
AI to classify whether a piece of text

163
00:10:41,430 --> 00:10:45,840
is real or fake after having been
trained on a labeled Dataset. It's basic,

164
00:10:45,841 --> 00:10:50,430
but a really cool demo you can download
and run it pretty fast. Using docker,

165
00:10:50,670 --> 00:10:55,020
I gave it the generated gpt to
text and it classified it as fake,

166
00:10:55,200 --> 00:10:59,040
which was pretty cool. Sometimes
you have to fight fire with fire.

167
00:10:59,190 --> 00:11:01,830
Technology will solve
many of our problems,

168
00:11:01,980 --> 00:11:06,390
but it will also give us new problems to
deal with. Now back to the big question,

169
00:11:06,391 --> 00:11:10,350
was it ethical for open AI to have
kept it's fully trained model private?

170
00:11:10,950 --> 00:11:15,450
Plenty of people right now have the
ability to easily recreate gpt to from

171
00:11:15,451 --> 00:11:17,310
scratch.
The instructions are all there.

172
00:11:17,311 --> 00:11:22,020
They released a paper blog post and
the code for a smaller version of it.

173
00:11:22,350 --> 00:11:26,760
I'm certain that the team at open AI was
aware of this before they released it

174
00:11:26,761 --> 00:11:31,440
all. What they did was brave.
They've taken the first
step of asking the question,

175
00:11:31,560 --> 00:11:36,560
how difficult should we
make this technology to use
and help start an important

176
00:11:36,720 --> 00:11:41,220
ongoing dialogue in the community?
If a model is trained on private data,

177
00:11:41,250 --> 00:11:44,690
like patient records,
that model could easily be reversed,

178
00:11:44,691 --> 00:11:47,280
engineered to reveal patient identities.

179
00:11:47,580 --> 00:11:50,700
And even if a model is
trained on fully public data,

180
00:11:50,820 --> 00:11:55,820
sometimes aggregated data in the of ml
models can become more sensitive than the

181
00:11:56,381 --> 00:11:58,330
data as a whole.
For example,

182
00:11:58,480 --> 00:12:02,800
an authoritarian regime could reverse
engineer a model train on Twitter profiles

183
00:12:02,920 --> 00:12:06,100
to find the location of
potential dissidents. Thus,

184
00:12:06,101 --> 00:12:10,960
it's important to use privacy preserving
tools in general while developing AI

185
00:12:10,961 --> 00:12:15,550
models linked to how that works. We'll be
in the video description in the future.

186
00:12:15,580 --> 00:12:20,170
Open Ai and other research labs,
if they feel their model could be misused,

187
00:12:20,440 --> 00:12:23,920
should commit to open sourcing
a model by a certain date.

188
00:12:24,220 --> 00:12:28,370
That way it gives the rest of the
community time to come up with counter

189
00:12:28,371 --> 00:12:31,210
measures for the potential negative uses.

190
00:12:31,960 --> 00:12:34,780
And as for the sensationalist journalists,

191
00:12:34,960 --> 00:12:38,740
please stop using terminator
images for AI new stories.

192
00:12:39,100 --> 00:12:44,100
These are mathematical tools that can
be used to empower people or exploit

193
00:12:44,261 --> 00:12:47,620
people,
not conscious super intelligent beings.

194
00:12:47,800 --> 00:12:52,180
They are matrix operations being
computed on silicon chips powered by

195
00:12:52,210 --> 00:12:56,690
electricity does sum it all up in a
single sentence. Keep calm and learn AI.

196
00:12:57,040 --> 00:12:59,080
And what do you think
about an AI as a decision?

197
00:12:59,110 --> 00:13:02,380
Let me know in the comments section and
please subscribe for more programming

198
00:13:02,381 --> 00:13:05,710
videos. For now. I've
got to recreate gpt too,

199
00:13:06,040 --> 00:13:07,660
so thanks for watching.

