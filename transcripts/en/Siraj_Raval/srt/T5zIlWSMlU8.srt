1
00:00:00,090 --> 00:00:04,920
Hello world. It's Saroj and what
does the Anti Facebook look like?

2
00:00:05,550 --> 00:00:07,380
Snapchat,
no.

3
00:00:08,040 --> 00:00:12,810
Facebook's newsfeed Ai Algorithm is
causing some serious problems and in this

4
00:00:12,811 --> 00:00:15,330
video we'll learn how
it works technically.

5
00:00:15,570 --> 00:00:18,930
Then discuss some solutions to
the problems that it presents.

6
00:00:19,350 --> 00:00:22,350
Facebook uses AI for pretty
much all of it's products,

7
00:00:22,351 --> 00:00:25,800
whether it's showing you the
most relevant comments, fighting,

8
00:00:25,801 --> 00:00:30,801
spammers or summarizing trending news
this year they've reached over 2 billion

9
00:00:32,281 --> 00:00:37,080
monthly active users making it the
biggest social network in the world.

10
00:00:37,470 --> 00:00:41,130
It makes sense.
They provide users with free services.

11
00:00:41,131 --> 00:00:45,810
They can use messaging store as much of
their data on Facebook servers as they'd

12
00:00:45,811 --> 00:00:48,510
like.
Host live streams all for free.

13
00:00:48,780 --> 00:00:52,950
The way users actually pay for Facebook
is not what their money but with their

14
00:00:52,951 --> 00:00:56,580
data. Anytime we use
Facebook, it learns about us.

15
00:00:56,850 --> 00:01:01,110
Facebook knows which of its users have
depression, which ones are full of hate,

16
00:01:01,350 --> 00:01:04,500
which ones are recently married,
where they live, et cetera.

17
00:01:04,770 --> 00:01:09,570
This is very valuable information for
companies who have products or services

18
00:01:09,780 --> 00:01:10,920
they would like to sell.

19
00:01:11,310 --> 00:01:16,310
Facebook's business model centers around
giving advertisers access to a very

20
00:01:16,501 --> 00:01:17,760
targeted audience.

21
00:01:18,150 --> 00:01:22,290
Advertisers can choose the type of person
they'd like to promote their brand too

22
00:01:22,291 --> 00:01:25,620
in detail,
and Facebook facilitates that process.

23
00:01:25,950 --> 00:01:30,300
They pay Facebook for your data,
which means that when you use Facebook,

24
00:01:30,600 --> 00:01:34,290
you, and specifically your
attention is the product.

25
00:01:34,680 --> 00:01:38,460
This materializes in the way
Facebook's flagship product,

26
00:01:38,461 --> 00:01:40,020
the newsfeed works.

27
00:01:40,380 --> 00:01:45,380
Let's imagine we're Facebook and we want
to show a user the most optimal content

28
00:01:45,750 --> 00:01:50,130
in their newsfeed such that they keep
their eyes on it as long as possible.

29
00:01:50,430 --> 00:01:54,810
We've got a list of activities that
users have performed in the past called

30
00:01:54,811 --> 00:01:58,230
events.
An event can be a post made by a friend,

31
00:01:58,470 --> 00:02:03,450
a conversation between two friends on new
group being created, things like that,

32
00:02:03,750 --> 00:02:07,770
and different users will interact
with these events in different ways.

33
00:02:08,130 --> 00:02:12,510
One might like an event, the other could
ignore it, the other could reshare it,

34
00:02:12,540 --> 00:02:17,540
and we can represent all of these
actions as numerical values and we can

35
00:02:17,731 --> 00:02:19,830
represent all of this as a matrix.

36
00:02:20,220 --> 00:02:24,030
The roads would be all the unique
users and the columns would be all the

37
00:02:24,031 --> 00:02:26,190
possible events for Facebook.

38
00:02:26,340 --> 00:02:30,840
This matrix would be bigger than
Apple's evaluation and we're making two

39
00:02:30,841 --> 00:02:31,680
assumptions here.

40
00:02:31,920 --> 00:02:36,720
The first is that users who interact
with events in a similar way share one or

41
00:02:36,721 --> 00:02:39,300
more hidden preferences.
For example,

42
00:02:39,360 --> 00:02:42,120
liking the same posts or
viewing the same article.

43
00:02:42,540 --> 00:02:47,250
The second is that users with shared
preferences are likely to respond in the

44
00:02:47,251 --> 00:02:49,110
same way to the same events.

45
00:02:49,470 --> 00:02:54,470
Thus we are relying only on observed user
behavior here to make recommendations,

46
00:02:54,900 --> 00:02:59,860
which is considered collaborative
filtering and a popular way to this is by

47
00:02:59,861 --> 00:03:02,530
using a technique called
Matrix factorization.

48
00:03:03,100 --> 00:03:07,210
Every entry in our matrix captures
a user's reaction to a given event.

49
00:03:07,600 --> 00:03:12,600
That rating could be explicit directly
generated by user feedback or implicit

50
00:03:12,820 --> 00:03:16,300
based on features like time spent
interacting with set events.

51
00:03:16,450 --> 00:03:20,770
If a user has never rated an event,
the Matrix entry is zero.

52
00:03:20,950 --> 00:03:25,390
Often in these type of matrices,
the majority of the entries are at zero.

53
00:03:25,750 --> 00:03:30,040
We're assuming that there's a set of
features common to all of these events and

54
00:03:30,041 --> 00:03:33,100
events differ in how they
express these features.

55
00:03:33,520 --> 00:03:36,640
Each user has their own
reaction to each feature.

56
00:03:36,700 --> 00:03:39,160
Independent of the events are users.

57
00:03:39,161 --> 00:03:44,161
Events rating can be approximated then
by summing the users strengths for each

58
00:03:44,291 --> 00:03:48,760
feature weighted by the degree to
which the event expresses this feature.

59
00:03:49,300 --> 00:03:53,380
These features are hidden factors
we'll need to transform the matrix to

60
00:03:53,381 --> 00:03:56,680
represent these hidden
factors using linear Algebra.

61
00:03:57,040 --> 00:03:59,380
This is called low rank approximation.

62
00:03:59,440 --> 00:04:04,440
It's the process of compressing the sparse
information in our matrix into a much

63
00:04:04,481 --> 00:04:06,160
lower dimensional space.

64
00:04:06,520 --> 00:04:11,520
Then we can calculate the rating of a
given user for a given event by taking the

65
00:04:11,711 --> 00:04:13,600
dot product of two vectors,

66
00:04:13,750 --> 00:04:18,280
which we're going to do in the context
of a neural network so that we can

67
00:04:18,281 --> 00:04:22,960
leverage the fact that we have so much
data and computing power as Facebook.

68
00:04:23,470 --> 00:04:27,640
The added terms like the weight matrix
will help us optimize our predictions

69
00:04:27,641 --> 00:04:32,080
over time. The output will be
the combination of both vectors.

70
00:04:32,380 --> 00:04:36,640
This output represents a prediction
of a user's rating for a given event.

71
00:04:37,000 --> 00:04:41,980
We can then define a loss
function to measure the
accuracy of this approximation.

72
00:04:42,400 --> 00:04:47,200
It sums the square difference between
the approximate rating and the actual

73
00:04:47,201 --> 00:04:49,840
rating from the training set.
Lastly,

74
00:04:49,870 --> 00:04:54,870
we can perform gradient descent to use
that computed error to update our weights

75
00:04:55,060 --> 00:04:58,450
and every iteration that
error will get smaller.

76
00:04:58,540 --> 00:05:01,360
While our predicted user ratings,
we'll get better.

77
00:05:01,930 --> 00:05:06,930
Ai optimized ads are different
than regular ads because
it's no longer a one way

78
00:05:07,061 --> 00:05:10,210
relationship.
It's two way you view the ad,

79
00:05:10,240 --> 00:05:15,240
but the ad also views you and how you
react so they can manipulate you subtly.

80
00:05:15,910 --> 00:05:20,410
Facebook decided to not
only perform surveillance on
literally everything you do

81
00:05:20,411 --> 00:05:21,910
to advertise to you.

82
00:05:22,210 --> 00:05:26,140
They also decided to perform
direct behavior modification.

83
00:05:26,680 --> 00:05:31,450
Human survival has necessitated that
our brains pay more urgent attention to

84
00:05:31,451 --> 00:05:33,670
possible bad outcomes than to good ones.

85
00:05:34,030 --> 00:05:38,890
Facebook's attention optimizing AI has
learned to exploit this and to keep us

86
00:05:38,891 --> 00:05:42,010
engaged and manipulate our behavior.
For example,

87
00:05:42,040 --> 00:05:47,040
making people argue leads to more
engagement which leads to more attention,

88
00:05:47,200 --> 00:05:51,790
which leads to more ad revenue,
human decency, kindness, empathy.

89
00:05:51,940 --> 00:05:56,470
These things are not
profitable for companies like
Facebook all operating under

90
00:05:56,471 --> 00:05:59,420
the business of a tension maximisation.

91
00:05:59,780 --> 00:06:04,160
This has resulted in a much more
polarized society globally where existing

92
00:06:04,161 --> 00:06:09,161
opinions are reinforced endlessly and
arguments are encouraged by AI optimized

93
00:06:10,191 --> 00:06:13,970
content. It's tearing apart.
The fabric of human society.

94
00:06:14,180 --> 00:06:19,180
Never before has one entity been so
powerful to manipulate the psyche of 2

95
00:06:19,220 --> 00:06:21,800
billion people.
The initial founders of Facebook,

96
00:06:21,830 --> 00:06:26,830
including Sean Parker and
Chamath Palihapitiya have
both stated that they regret

97
00:06:27,380 --> 00:06:31,430
the effects on society that
their business model has caused,

98
00:06:31,490 --> 00:06:33,140
especially on children.

99
00:06:33,590 --> 00:06:37,160
The short term solution to
this problem is awareness.

100
00:06:37,430 --> 00:06:42,170
The more people that know just how good
Facebook's AI is at manipulating them in

101
00:06:42,171 --> 00:06:44,630
ways they don't even fully understand yet,

102
00:06:44,900 --> 00:06:47,570
the more likely they'll
be to protect themselves.

103
00:06:48,020 --> 00:06:52,700
The center for humane technology has a
list of ways to take back control of your

104
00:06:52,701 --> 00:06:55,100
attention from these social networks.

105
00:06:55,220 --> 00:07:00,220
Some suggestions include turning off
all notifications except from people and

106
00:07:00,740 --> 00:07:05,450
going gray scale to avoid the positive
reinforcement that colorful icons are

107
00:07:05,540 --> 00:07:07,700
designed to unlock in our heads.

108
00:07:08,060 --> 00:07:12,140
A more ambitious solution is
to build the anti Facebook,

109
00:07:12,440 --> 00:07:15,650
a social network that uses
a different business model.

110
00:07:15,980 --> 00:07:18,110
[inaudible] is one example of this.

111
00:07:18,380 --> 00:07:22,850
Their business model is facilitating
users sending funds to other users,

112
00:07:23,000 --> 00:07:24,020
not advertising.

113
00:07:24,440 --> 00:07:29,240
That's an optimization objective that
can be much more easily aligned with the

114
00:07:29,241 --> 00:07:31,880
users wellbeing than eyeball seconds.

115
00:07:32,090 --> 00:07:36,590
Another thing to note about [inaudible]
is that you can peep and tweet at the

116
00:07:36,591 --> 00:07:37,424
same time,

117
00:07:37,520 --> 00:07:42,140
which is an effective way to erode the
network effects that protect more popular

118
00:07:42,141 --> 00:07:43,190
social networks.

119
00:07:43,490 --> 00:07:47,370
You can even easily sync your
followers from Twitter to p path.

120
00:07:47,720 --> 00:07:52,310
It's features like that that show us that
it doesn't have to be inconvenient for

121
00:07:52,311 --> 00:07:57,130
users to find an alternative to Facebook.
Another example is mastodon.

122
00:07:57,230 --> 00:08:02,120
This is a Twitter clown that anyone can
host instead of one provider in charge

123
00:08:02,121 --> 00:08:05,750
of the newsfeed. There are
several. It's a federated network,

124
00:08:05,930 --> 00:08:10,340
more decentralized than Facebook.
It provides the same resources,

125
00:08:10,341 --> 00:08:14,930
but with the added ability to exit it.
Commoditizes feed providers.

126
00:08:15,200 --> 00:08:18,200
If one provider is starting to
put too many ads on their feed,

127
00:08:18,410 --> 00:08:20,120
a user can just switch to another.

128
00:08:20,510 --> 00:08:24,850
There are some technical challenges to
building decentralized applications and I

129
00:08:24,940 --> 00:08:28,490
tell them all in my past course
and in my pass book on the topic,

130
00:08:29,060 --> 00:08:33,590
there's also the possibility of
Facebook be centralizing itself.

131
00:08:34,010 --> 00:08:37,700
Facebook's original mission was
establishing an Internet identity.

132
00:08:37,850 --> 00:08:40,160
That's what made them
different from my space.

133
00:08:40,220 --> 00:08:44,750
It seems like a natural continuation for
them to try and position themselves to

134
00:08:44,751 --> 00:08:49,751
be the root of human identity
on the Internet and having
a unique crypto address

135
00:08:50,150 --> 00:08:54,020
not owned by anyone would do just that.
Just a few months ago,

136
00:08:54,021 --> 00:08:57,300
they announced their biggest
ever management reshuffling,

137
00:08:57,510 --> 00:09:01,860
which included the launch of an
exploratory blockchain group that reports

138
00:09:01,861 --> 00:09:04,200
directly to the company's CTO.

139
00:09:04,350 --> 00:09:07,590
If Facebook released
their own crypto currency,

140
00:09:07,591 --> 00:09:11,310
they could pay their users for
engaging with their platform,

141
00:09:11,520 --> 00:09:14,130
say one face coin per uploaded photo.

142
00:09:14,580 --> 00:09:17,400
This would also give
equity to all of its users,

143
00:09:17,401 --> 00:09:21,030
which would retain them and get
them to help grow the platform.

144
00:09:21,031 --> 00:09:25,740
Because right now that growth is
stagnating as users wake up to Facebook's

145
00:09:25,741 --> 00:09:30,540
problems. It could even give them
voting rights as to how Facebook is run,

146
00:09:30,720 --> 00:09:33,300
which would increase
their brand reputation.

147
00:09:33,720 --> 00:09:38,700
This would be profitable for Facebook
and it would help solve their alignment

148
00:09:38,701 --> 00:09:41,760
problem because through
this new business model,

149
00:09:41,910 --> 00:09:46,910
they'd be more inclined to show users
content that would benefit them and paying

150
00:09:47,040 --> 00:09:51,840
over 2 billion people globally for their
data starts to look like a universal

151
00:09:51,841 --> 00:09:56,700
basic income which would offset some of
the effects that automation technology

152
00:09:56,701 --> 00:09:59,340
will have on labor based jobs.

153
00:09:59,520 --> 00:10:01,740
Speaking of a new class of jobs,

154
00:10:01,830 --> 00:10:06,830
linear envisions that who start seeing
mediators of individual data or mids

155
00:10:07,080 --> 00:10:08,400
start forming as well.

156
00:10:08,670 --> 00:10:13,230
This is a nonprofit organization that
negotiates data royalties for groups of

157
00:10:13,231 --> 00:10:15,660
people with similar interests.
For instance,

158
00:10:15,690 --> 00:10:20,220
a botanist who likes to hike could join
a mid for people who compiled useful

159
00:10:20,221 --> 00:10:24,360
photographs and data about trees
in certain regions of the world.

160
00:10:24,660 --> 00:10:29,660
That data would be valuable to certain
companies and a protocol like open mind

161
00:10:30,090 --> 00:10:34,980
could facilitate this exchange of data
and capital in a decentralized way.

162
00:10:35,190 --> 00:10:39,960
So even though Facebook's business
model is bad for the human psyche in the

163
00:10:39,961 --> 00:10:40,651
short term,

164
00:10:40,651 --> 00:10:45,651
we can protect ourselves by adopting anti
addiction techniques so that we get as

165
00:10:46,081 --> 00:10:48,750
much of the good from
social media as we can.

166
00:10:48,751 --> 00:10:51,360
While avoiding the bad along the way,

167
00:10:51,361 --> 00:10:55,950
we can build other social apps that use
different business models to provide

168
00:10:55,951 --> 00:10:58,950
people with alternative options.
And who knows,

169
00:10:58,980 --> 00:11:03,980
maybe Facebook will become the anti
Facebook ushering in a new era of data

170
00:11:04,171 --> 00:11:08,670
transparency with the focus on human
wellbeing instead of attention.

171
00:11:08,910 --> 00:11:11,640
What do you think?
What is the Anti Facebook look like?

172
00:11:11,850 --> 00:11:14,430
Share your thoughts in the
comments section. Please subscribe.

173
00:11:14,431 --> 00:11:17,940
And for now I've got to use Twitter,
so thanks for watching.

