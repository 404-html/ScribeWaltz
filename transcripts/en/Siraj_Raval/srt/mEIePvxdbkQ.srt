1
00:00:00,240 --> 00:00:02,460
How did you know the
mighty ducks would win?

2
00:00:02,550 --> 00:00:07,550
Luck and reinforcement learning hello
world it Saroj and sports betting.

3
00:00:08,850 --> 00:00:13,850
It's a popular pastime for many and a
great use case for an important concept

4
00:00:14,101 --> 00:00:17,670
known as dynamic programming
that I'll discuss in this video.

5
00:00:18,120 --> 00:00:23,120
We know that reinforcement learning is
the subfield of machine learning that

6
00:00:23,730 --> 00:00:28,730
focuses on goal oriented algorithms
that involve time delayed label's called

7
00:00:29,221 --> 00:00:29,881
rewards.

8
00:00:29,881 --> 00:00:34,881
And we've also learned that the standard
way to represent this problem of an AI

9
00:00:35,761 --> 00:00:40,761
trying to maximize its rewards in a time
based environment setting is by using

10
00:00:41,491 --> 00:00:46,491
the mathematical framework known
as a Mark Cobb decision process.

11
00:00:47,070 --> 00:00:52,070
Now let's use these concepts to help
us frame our problem of betting on the

12
00:00:52,711 --> 00:00:54,060
winning hockey team.

13
00:00:54,480 --> 00:00:57,810
Assume that we are a
professional sports gambler,

14
00:00:57,840 --> 00:01:02,840
let's call ourselves JK Rowling Endo and
our career is to make money by betting

15
00:01:04,861 --> 00:01:08,850
on whether or not a certain
hockey team will win a game.

16
00:01:09,180 --> 00:01:13,740
And there's a series of these games that
will occur sequentially that are all

17
00:01:13,780 --> 00:01:15,180
apart of a tournament.

18
00:01:15,570 --> 00:01:20,570
Our goal is to make 100 US dollars by
successfully betting on the winning team.

19
00:01:21,480 --> 00:01:24,570
What can I say?
We dream big for each game.

20
00:01:24,571 --> 00:01:28,620
There are only two possible outcomes.
Either are chosen,

21
00:01:28,621 --> 00:01:30,990
team wins or they lose.

22
00:01:31,260 --> 00:01:35,340
We'll place a bet on that team at the
start of the game and if they do win,

23
00:01:35,520 --> 00:01:38,190
we will win the same amount to that.
We bet.

24
00:01:38,370 --> 00:01:43,290
Otherwise the opposite will happen
and we'll lose all the money. We bet.

25
00:01:43,830 --> 00:01:48,750
Let's also make one fundamental
assumption here that we'll touch on later.

26
00:01:49,140 --> 00:01:53,910
We'll assume that the home
team will win 40% of the time,

27
00:01:54,120 --> 00:01:56,400
no matter who the home team is.

28
00:01:56,700 --> 00:02:01,440
So for every game we have to decide how
much of our money we wanted to put at

29
00:02:01,441 --> 00:02:05,010
stake in the context of
reinforcement learning.

30
00:02:05,010 --> 00:02:08,670
Then we can consider this a
Mark Cobb decision process.

31
00:02:09,090 --> 00:02:13,320
The state would be how much
money we have at any given point.

32
00:02:13,710 --> 00:02:17,370
The actions are steaks or
bets that we would place.

33
00:02:17,790 --> 00:02:22,790
The reward is going to be zero on all
transitions except those in which we reach

34
00:02:22,981 --> 00:02:24,810
our goal.
Then it's plus one.

35
00:02:25,250 --> 00:02:28,290
What we'll want to then
learn is an optimal policy.

36
00:02:28,680 --> 00:02:33,680
Our policy will be a mapping between
levels of capital we have to how much we

37
00:02:34,291 --> 00:02:39,291
should bet the optimal policies should
then maximize the probability of reaching

38
00:02:39,691 --> 00:02:40,470
our goal,

39
00:02:40,470 --> 00:02:45,470
which is to make $100 and remember we
are making an assumption here that the

40
00:02:46,081 --> 00:02:49,830
home team will win 40% of the time.

41
00:02:50,040 --> 00:02:55,040
So we can set p the probability of the
home team winning to 0.4 since P is

42
00:02:56,101 --> 00:03:00,670
known, the entire problem space
is known and can that be solved?

43
00:03:00,910 --> 00:03:05,230
Meaning we have a complete
model of this environment.

44
00:03:05,590 --> 00:03:10,300
That's definitely not always the case.
But for this introductory example,

45
00:03:10,330 --> 00:03:15,330
it will be to make our problem easier and
it's good practice to denote any other

46
00:03:16,151 --> 00:03:21,151
characteristics of our mark comm decision
process that we can find like the fact

47
00:03:21,551 --> 00:03:23,620
that it can be considered episodic,

48
00:03:23,650 --> 00:03:28,650
meaning it lasts a finite amount of
time when the game ends rather than

49
00:03:28,961 --> 00:03:29,794
continuous.

50
00:03:30,130 --> 00:03:35,130
So how do we find this mythical Unicorn
function known as the optimal policy?

51
00:03:35,590 --> 00:03:39,020
Well. If we look back at
the Bible of RL called Rl,

52
00:03:39,100 --> 00:03:44,100
an introduction by sudden and Barto in
chapter four will come upon a pretty

53
00:03:44,171 --> 00:03:45,340
useful definition.

54
00:03:45,760 --> 00:03:50,620
The term dynamic programming refers to
a collection of algorithms that can be

55
00:03:50,621 --> 00:03:55,621
used to compute optimal policies given
a perfect model of the environment as a

56
00:03:56,730 --> 00:04:00,670
mark Haub decision process.
That's perfect for us.

57
00:04:00,671 --> 00:04:03,880
Since we already know the
transition probabilities,

58
00:04:04,120 --> 00:04:09,120
which indicates the probability of a
change from state s to the next state and

59
00:04:09,461 --> 00:04:10,960
we've got a reward function.

60
00:04:11,260 --> 00:04:16,260
Dynamic programming is a way of
breaking down complex problems into sub

61
00:04:17,471 --> 00:04:20,020
problems.
We solve the sub problems,

62
00:04:20,080 --> 00:04:24,820
then combine the solutions to those sub
problems to solve the larger problem.

63
00:04:25,090 --> 00:04:30,090
There are many different types of DP
techniques and it's actually used in many

64
00:04:30,910 --> 00:04:34,150
real world applications
like bio informatics,

65
00:04:34,420 --> 00:04:36,670
scheduling and routing algorithms.

66
00:04:37,240 --> 00:04:42,240
The specific dynamic programming algorithm
will use in this example to solve our

67
00:04:42,911 --> 00:04:46,060
problem is called value iteration.

68
00:04:46,420 --> 00:04:51,420
It turns out that any optimal policy
can be subdivided into two components,

69
00:04:52,030 --> 00:04:56,710
an optimal first action followed by
an optimal policy from the next state.

70
00:04:57,490 --> 00:05:02,200
If our first action is optimal and after
that we follow an optimal policy from

71
00:05:02,201 --> 00:05:03,310
where we end up,

72
00:05:03,520 --> 00:05:07,210
then we can conclude that the
overall behavior is optimal.

73
00:05:07,600 --> 00:05:12,190
These rules can be formalized by what's
called the principle of optimality.

74
00:05:12,460 --> 00:05:16,540
A theorem invented by the mathematician,
Richard Bellman, decades ago.

75
00:05:16,870 --> 00:05:21,870
It states that a policy achieves the
optimal value from state s if and only if

76
00:05:23,290 --> 00:05:28,290
for any next state reachable from s our
policy achieves the optimal value from

77
00:05:28,601 --> 00:05:33,601
state s so we can essentially break
down the description of what it means to

78
00:05:34,031 --> 00:05:39,031
have an optimal policy in terms of finding
the optimal policy from all states.

79
00:05:39,040 --> 00:05:41,740
We could end up in after that,

80
00:05:41,741 --> 00:05:46,570
we just need to do a one step look ahead
and discern what the best first action

81
00:05:46,571 --> 00:05:48,370
we could have taken his.

82
00:05:48,460 --> 00:05:51,880
That's the principle of
optimality applied to policies.

83
00:05:52,150 --> 00:05:56,050
So notice this somewhat
counterintuitive intermediate step here.

84
00:05:56,350 --> 00:05:59,120
Before we can compute the optimal policy,

85
00:05:59,420 --> 00:06:04,420
we need to compute the optimal action
value function called Kyu and there's a

86
00:06:05,001 --> 00:06:09,680
great formula that expresses this
very well. Can you guess what it is?

87
00:06:10,070 --> 00:06:14,930
We can express the value of any state as
a some of the immediate reward plus the

88
00:06:14,931 --> 00:06:16,460
value of the next date.

89
00:06:16,850 --> 00:06:20,720
This expression is popularly
called a bellman equation.

90
00:06:21,110 --> 00:06:26,110
It will help estimate the best action to
take using the value for a given state

91
00:06:26,360 --> 00:06:29,120
in order to find the optimal policy.

92
00:06:29,330 --> 00:06:32,510
So if we know the solution
to this sub problem,

93
00:06:32,540 --> 00:06:36,200
like the optimal value
function from the next state,

94
00:06:36,560 --> 00:06:41,300
then the value function given this states
can be found by a one step look ahead.

95
00:06:41,510 --> 00:06:45,770
Using the bellman equation in
the value iteration algorithm,

96
00:06:45,800 --> 00:06:50,800
we will iteratively apply the bellman
equation to compute the optimal value

97
00:06:51,441 --> 00:06:56,000
function. One Way to think about this
formula is that from a given state,

98
00:06:56,210 --> 00:07:01,210
there are many possible actions so we
can accumulate the values for how much

99
00:07:01,520 --> 00:07:06,520
each action would get in terms of expected
value and just assign the value of

100
00:07:07,251 --> 00:07:11,270
the state to that of the action
which return the highest value.

101
00:07:11,840 --> 00:07:16,700
We've basically taken the bellman equation
and turned it into an update rule.

102
00:07:17,150 --> 00:07:22,150
The Algorithm initializes via vests
with arbitrary random values than it

103
00:07:22,521 --> 00:07:25,550
repeatedly updates the state
action pair called queue.

104
00:07:25,760 --> 00:07:28,430
And the [inaudible] of s
values until they converge.

105
00:07:28,820 --> 00:07:32,750
Value Iteration is guaranteed to
converge on the optimal values.

106
00:07:33,140 --> 00:07:37,700
Guaranteed sounds pretty great, doesn't
it? Especially when money is involved,

107
00:07:37,730 --> 00:07:42,730
but let's take a closer look here in the
context of our sports betting problem,

108
00:07:43,371 --> 00:07:46,400
where the state is,
how much money we have,

109
00:07:46,790 --> 00:07:51,790
the actions are the states and the
reward is zero on all transitions except

110
00:07:52,371 --> 00:07:54,290
those in which we reach our goal.

111
00:07:54,440 --> 00:07:58,940
When it is plus one we can try
and compute the optimal policy.

112
00:07:59,330 --> 00:08:03,950
The state value function will give the
probability of winning from each state.

113
00:08:04,430 --> 00:08:09,430
A policy is a mapping from levels of
capital to steaks and the optimal policy

114
00:08:10,131 --> 00:08:13,100
maximizes the probability
of reaching the goal.

115
00:08:13,490 --> 00:08:18,490
The reason we can use value iteration is
because of the value of p four me four

116
00:08:18,890 --> 00:08:19,723
is known.

117
00:08:19,760 --> 00:08:24,440
If we were to graph out the
value estimates versus the
amount of capital we have,

118
00:08:24,470 --> 00:08:29,330
we'd see how the value function
changes over successive sweeps of value

119
00:08:29,331 --> 00:08:34,331
iteration and the final policy is found
for the case of p equals 0.4 we can then

120
00:08:35,660 --> 00:08:39,830
use that policy to make the
most likely bets in the future.

121
00:08:40,100 --> 00:08:40,940
Notice however,

122
00:08:40,941 --> 00:08:45,941
that we need that value of p to compute
the optimal policy using value iteration

123
00:08:46,790 --> 00:08:50,960
in general, even though dynamic
programming is very useful,

124
00:08:51,080 --> 00:08:52,880
it does have its limitations.

125
00:08:53,270 --> 00:08:58,050
We need a perfect environment model in
the form of a Mark Cobb decision process

126
00:08:58,051 --> 00:09:02,490
to do it and it's very
computationally expensive.

127
00:09:02,580 --> 00:09:07,580
Solving the bellman equation is a brute
force job we're doing by trying out

128
00:09:08,040 --> 00:09:12,930
every possible action. We
could do it every state. If
we tried to do this for say,

129
00:09:12,990 --> 00:09:14,130
a game of chess,

130
00:09:14,490 --> 00:09:19,490
we'd need computing power that would
make Google's global supercomputer looked

131
00:09:19,621 --> 00:09:22,560
like a super Nintendo system.
Still,

132
00:09:22,620 --> 00:09:27,620
it's a useful thought exercise and it is
used in many real world situations that

133
00:09:29,161 --> 00:09:32,910
don't require massive amounts of compute.
Also,

134
00:09:33,090 --> 00:09:34,440
did you notice something?

135
00:09:34,620 --> 00:09:39,620
We had that intermediate step where we
computed the optimal value function to

136
00:09:39,751 --> 00:09:42,000
compute the optimal policy,

137
00:09:42,300 --> 00:09:47,220
but what if we just skip that intermediate
step and just computed the optimal

138
00:09:47,221 --> 00:09:51,870
policy directly? What type of
algorithms should we use for that?

139
00:09:52,440 --> 00:09:54,780
That's the topic for the next video.

140
00:09:55,080 --> 00:09:57,780
Three points to remember
from this video though,

141
00:09:58,260 --> 00:10:03,260
dynamic programming refers
to the collection of
algorithms that can be used to

142
00:10:03,811 --> 00:10:08,460
compute optimal policies given a
perfect model of the environment.

143
00:10:08,820 --> 00:10:13,820
Value Iteration is a type of dynamic
programming algorithm that computes the

144
00:10:14,040 --> 00:10:17,070
optimal value function,
and consequently,

145
00:10:17,340 --> 00:10:22,340
the optimal policy and dynamic programming
is useful but limited because it

146
00:10:22,621 --> 00:10:27,510
requires a perfect environment model
and is computationally expensive.

147
00:10:27,840 --> 00:10:29,610
I am proud of you for making it here.

148
00:10:29,620 --> 00:10:33,450
Hit subscribe and your enemies
will become your friends. For now,

149
00:10:33,480 --> 00:10:37,080
I'm going to compute the optimal
life policy, so thanks for watching.

