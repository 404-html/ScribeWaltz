1
00:00:00,030 --> 00:00:03,510
Hello world. It's the Raj and
have you ever played doom before?

2
00:00:03,690 --> 00:00:07,770
It's a classic shooter and we're going
to use a technique called the actor

3
00:00:07,771 --> 00:00:11,970
critic to train an AI to beat
the game on its own deep minds,

4
00:00:11,971 --> 00:00:12,930
deep Q network,

5
00:00:12,970 --> 00:00:17,370
the same algorithm that beats so many
different games made history and brought

6
00:00:17,400 --> 00:00:19,650
our l back into the limelight.

7
00:00:19,980 --> 00:00:24,980
Let's just take a second to appreciate
how mind blowing the results it achieved.

8
00:00:25,081 --> 00:00:28,560
We're starting with no
knowledge on what winning means.

9
00:00:28,770 --> 00:00:32,670
It was able to explore the environment
and eventually master the game.

10
00:00:33,030 --> 00:00:36,360
Think about putting yourself
in that same situation.

11
00:00:36,630 --> 00:00:41,630
It'd be like asking you to play a game
without a rule book or an end goal and

12
00:00:41,701 --> 00:00:43,500
saying keep playing until you win.

13
00:00:43,680 --> 00:00:48,000
Oh and the possible results states you
could reach with a series of actions is

14
00:00:48,001 --> 00:00:51,150
infinite, I. E. A continuous
observation space.

15
00:00:51,480 --> 00:00:55,320
Only young Macoun could do
that despite this scenario.

16
00:00:55,321 --> 00:01:00,300
Deep Q was able to converge really quickly
on this seemingly impossible task by

17
00:01:00,301 --> 00:01:03,780
maintaining and slowly
updating action value pairs.

18
00:01:04,050 --> 00:01:08,730
But what happens when we have an infinite
input space and an infinite output

19
00:01:08,731 --> 00:01:12,900
space? Well, deep Q is no longer
an effective algorithm, but wait,

20
00:01:13,110 --> 00:01:16,860
isn't it completely independent of the
structure of the environment actions

21
00:01:17,070 --> 00:01:19,770
while it was independent
of what the actions were,

22
00:01:19,980 --> 00:01:24,720
but it was fundamentally premise on having
a finite output space. Think about it.

23
00:01:24,721 --> 00:01:29,721
The prediction looks to assign a score
to each possible action at each time step

24
00:01:29,880 --> 00:01:33,540
given the current environment state
and just uses the action that had the

25
00:01:33,541 --> 00:01:34,374
highest score.

26
00:01:34,650 --> 00:01:39,480
Basically it reduced the problem of
our l two assigning scores to actions,

27
00:01:39,690 --> 00:01:43,890
but how could this be possible if
we have an infinite input space,

28
00:01:44,220 --> 00:01:48,930
we basically need an infinitely large
table to keep track of all the Q values.

29
00:01:49,170 --> 00:01:50,610
Think of this task this way.

30
00:01:50,670 --> 00:01:54,480
Not only are we being given a game
without instructions about the game has a

31
00:01:54,481 --> 00:01:58,800
controller with infinite buttons on it.
That'd be a terrible controller, right?

32
00:01:58,950 --> 00:02:01,230
Almost as bad as the
playstation mouse was,

33
00:02:01,620 --> 00:02:06,210
deep queue is restricted to
a finite number of actions
and that's because of the

34
00:02:06,211 --> 00:02:08,040
way the model is structured.

35
00:02:08,130 --> 00:02:12,390
We've got to be able to iterate at each
time step to update how our position on

36
00:02:12,391 --> 00:02:14,400
a particular action has changed.

37
00:02:14,610 --> 00:02:18,270
That's why we're having the model
predict Q values rather than directly

38
00:02:18,271 --> 00:02:21,330
predicting what action to take.
If we had the ladder,

39
00:02:21,420 --> 00:02:25,170
we'd have no clue on how to update
the model to take into account that

40
00:02:25,171 --> 00:02:29,070
prediction and what reward we'd
receive for future predictions.

41
00:02:29,280 --> 00:02:33,630
So the issue stems from the fact that
it seems like our model has to output a

42
00:02:33,631 --> 00:02:38,631
tabulated table of the rewards
associated with all the possible actions.

43
00:02:38,820 --> 00:02:42,480
But here's an idea. What if instead
we split the model into two?

44
00:02:42,750 --> 00:02:44,580
What if we had two separate models,

45
00:02:44,610 --> 00:02:49,610
one that outputs the desired action and
the continuous space and another taking

46
00:02:49,800 --> 00:02:54,060
in an action as input to produce Q
values that seems like it could work.

47
00:02:54,090 --> 00:02:54,923
And guess what?

48
00:02:54,990 --> 00:02:59,650
It's exactly the idea behind the actor
critic model or actor critic methods are

49
00:02:59,651 --> 00:03:01,420
premised on having two models.

50
00:03:01,720 --> 00:03:05,740
And this theme of having multiple neural
nets interacting seems to be getting

51
00:03:05,770 --> 00:03:08,560
more popular,
not just in reinforcement learning,

52
00:03:08,740 --> 00:03:10,510
but in supervised learning as well.

53
00:03:10,540 --> 00:03:14,770
Like in the case of generative
adversarial networks and variational auto

54
00:03:14,771 --> 00:03:16,330
encoders,
seriously,

55
00:03:16,331 --> 00:03:20,320
almost everyone's slapped the word Gan
on their paper at nips and called it a

56
00:03:20,321 --> 00:03:24,490
day. But that's a side note. The
actor critic model has two components,

57
00:03:24,700 --> 00:03:26,230
an actor and a critic.

58
00:03:26,470 --> 00:03:30,910
The actor takes in the current environment
state and determines the best action

59
00:03:30,911 --> 00:03:31,300
to take.

60
00:03:31,300 --> 00:03:35,500
From there the critic plays the evaluation
role by taking in the environment,

61
00:03:35,501 --> 00:03:40,210
state and action and we're turning a
score that represents how good the action

62
00:03:40,240 --> 00:03:41,260
is for the state.

63
00:03:41,620 --> 00:03:46,240
One way to think about this
is as a playground with a
child being the actor and

64
00:03:46,241 --> 00:03:47,740
her parent being the critic.

65
00:03:48,040 --> 00:03:52,300
The kid is running around and exploring
all the possible options in this

66
00:03:52,301 --> 00:03:56,200
environment. They could slide down
a slide, take a ride on a swing,

67
00:03:56,320 --> 00:03:58,180
pull grass from the ground and eat it.

68
00:03:58,570 --> 00:04:02,830
The parent's job is to watch the kid
and either criticize or compliment them

69
00:04:03,040 --> 00:04:06,580
based on what they did. Taking the
environment into account. Actor,

70
00:04:06,581 --> 00:04:10,450
critic methods are advantageous to
policy gradient methods as well.

71
00:04:10,510 --> 00:04:14,110
Let's go into a bit more detail
of how actor critic works.

72
00:04:14,140 --> 00:04:18,730
But first I want to take a
second to explain the chain
rule because it's used in

73
00:04:18,731 --> 00:04:22,420
this method and it's just really
important in machine learning in general,

74
00:04:22,630 --> 00:04:26,110
like say in the case of
backpropagation for neural networks,

75
00:04:26,290 --> 00:04:30,490
the chain rule is a rule that comes
from calculus that helps us compute the

76
00:04:30,491 --> 00:04:35,140
derivative of a composite function
that is a function of functions.

77
00:04:35,410 --> 00:04:39,760
The idea is that you derive the outer
function and derive the inner function and

78
00:04:39,761 --> 00:04:43,570
multiplied them together and that'll
give you the derivative of the whole

79
00:04:43,571 --> 00:04:46,660
composite function.
And it's a recursive rule.

80
00:04:46,690 --> 00:04:50,440
So it applies for as many
nested functions there are,

81
00:04:50,680 --> 00:04:54,670
the derivative of the s is the slope of
the tangent line at a point on the curve

82
00:04:54,730 --> 00:04:58,240
of a function. It's a way to
represent the rate of change,

83
00:04:58,360 --> 00:05:02,740
meaning the amount by which a
function is changing at a given point.

84
00:05:03,010 --> 00:05:07,300
The chain rule will help us update the
parameters of our actor critic model.

85
00:05:07,450 --> 00:05:12,450
So in our system where the output of
one network feeds into the input of the

86
00:05:12,550 --> 00:05:17,170
other, updating the parameters of the
feeding network will shake up its output,

87
00:05:17,290 --> 00:05:21,640
which will then propagate
and be multiplied by any
further changes through the

88
00:05:21,670 --> 00:05:24,640
end of the pipeline.
When we implement our model,

89
00:05:24,670 --> 00:05:28,390
note that it has the same exact
task that deep Q would have,

90
00:05:28,600 --> 00:05:33,040
except we have two separate modules.
We start by defining the actor model.

91
00:05:33,130 --> 00:05:37,840
Given the current state of the environment
determined the best action to take

92
00:05:38,110 --> 00:05:40,660
and since the task is given numeric data,

93
00:05:40,840 --> 00:05:43,870
there's no need to involve any
complex layers in our network.

94
00:05:43,990 --> 00:05:45,280
Just fully connected once.

95
00:05:45,430 --> 00:05:49,840
So the actor model is just a series of
fully connected layers that match from

96
00:05:49,841 --> 00:05:53,260
the environment observation to a
point in the environment space.

97
00:05:53,290 --> 00:05:56,320
We're going to return a reference
to the input layer as well,

98
00:05:56,321 --> 00:05:58,280
and I'll explain why in a second,

99
00:05:58,700 --> 00:06:01,790
but how are we going to determine
the best action to take?

100
00:06:02,060 --> 00:06:05,660
The Q scores are now calculated
separately in the critic network.

101
00:06:05,960 --> 00:06:09,740
We're going to use an optimization
strategy called gradient ascent.

102
00:06:10,070 --> 00:06:14,840
This means attempting to reach a global
maximum by moving incrementally in the

103
00:06:14,841 --> 00:06:19,841
steepest direction of an incline and the
way we get the direction to move is by

104
00:06:20,151 --> 00:06:23,990
using the chain rule to
compute derivatives I. E.

105
00:06:23,990 --> 00:06:25,550
Gradients of the weights of our network.

106
00:06:25,970 --> 00:06:30,260
What change in parameters for our
actor model would result in the largest

107
00:06:30,290 --> 00:06:34,460
increase in the Q value which would
be predicted by the critic model.

108
00:06:34,820 --> 00:06:39,820
Since the output of the actor model is
the action and the critic evaluates based

109
00:06:40,310 --> 00:06:42,380
on an environment state action pair,

110
00:06:42,710 --> 00:06:44,930
we can see that the chain
rule helps us do this.

111
00:06:45,260 --> 00:06:49,790
Changing the parameters of the actor will
change the eventual Q value using the

112
00:06:49,820 --> 00:06:53,900
output of the actor network as our
middle link for our critic network.

113
00:06:53,960 --> 00:06:58,280
We're faced with the opposite issue.
The network is a bit more complicated,

114
00:06:58,460 --> 00:07:02,330
but training is straightforward.
The critic must take both the environment,

115
00:07:02,331 --> 00:07:06,680
state and action as inputs
and calculate a valuation.

116
00:07:06,980 --> 00:07:11,240
We use a series of fully connected
layers with the layer in the middle that

117
00:07:11,241 --> 00:07:15,230
merges the two before combining
into the final Q value prediction.

118
00:07:15,590 --> 00:07:19,010
We use one extra fully connected
layer on the environment.

119
00:07:19,010 --> 00:07:23,420
State input as compared to the action
will hold onto references of both the

120
00:07:23,480 --> 00:07:23,891
inputs,

121
00:07:23,891 --> 00:07:27,890
states and action since we don't need
to use them and doing updates for the

122
00:07:27,891 --> 00:07:31,840
actor network. The output queue with
respect to the action weights I. E.

123
00:07:31,841 --> 00:07:34,910
The gradient is directly
called in the training code.

124
00:07:35,300 --> 00:07:37,340
Since we have two training methods,

125
00:07:37,550 --> 00:07:40,040
we separate the code
into different functions.

126
00:07:40,100 --> 00:07:44,300
We're training on the state action pair
and using the critic model to predict

127
00:07:44,301 --> 00:07:46,760
the future reward rather than the actor.

128
00:07:47,090 --> 00:07:51,620
The actor just needs to be called with
the actions and states we encounter and

129
00:07:51,650 --> 00:07:54,980
our prediction could just iterate
through a trial called predict.

130
00:07:55,040 --> 00:07:56,810
Remember and train the agent.

131
00:07:57,140 --> 00:08:02,140
Notice how our doom bought after training
for about 20 hours on a single CPU is

132
00:08:02,481 --> 00:08:06,680
able to kill monsters and navigate
successfully through the environment.

133
00:08:06,750 --> 00:08:09,890
A lack of boss. All right,
three things to remember here.

134
00:08:10,010 --> 00:08:14,900
Actor critic methods work well when we
have both an infinite input space and

135
00:08:14,930 --> 00:08:17,900
infinite output space.
In an actor critic model,

136
00:08:18,020 --> 00:08:20,090
the actor takes in the
current environment,

137
00:08:20,091 --> 00:08:22,820
states and determines
the best action to take.

138
00:08:23,120 --> 00:08:27,200
The critic plays the evaluation role
by taking in the environment states and

139
00:08:27,201 --> 00:08:29,210
action and returning an action score.

140
00:08:29,510 --> 00:08:34,070
And actor critic methods tend to require
much less training time and policy

141
00:08:34,071 --> 00:08:38,100
gradient methods. Last week's
coding challenge winner. He's fine.

142
00:08:38,630 --> 00:08:43,460
He trained an AI to play tic tac toe
using the policy gradients strategy and

143
00:08:43,461 --> 00:08:46,880
he's documentation makes it
easy for you to reuse his code.

144
00:08:47,150 --> 00:08:48,560
Great Work Wizard of the week.

145
00:08:49,130 --> 00:08:53,630
And the runner up is [inaudible] Kumar
who use a convolutional network and cue

146
00:08:53,631 --> 00:08:55,920
learning to have an AI learn how to play

147
00:08:55,980 --> 00:08:57,990
flappy birds.
Really Clever Algorithm.

148
00:08:58,290 --> 00:09:02,130
This week's challenge is to generate
some music using deep learning.

149
00:09:02,430 --> 00:09:07,430
So the challenge is to either create a
model that generates music or generate

150
00:09:08,191 --> 00:09:10,740
music from one of the existing models
that we talked about here today.

151
00:09:10,920 --> 00:09:15,710
Send me your finished track and or tracks
to my Twitter handle at Taryn southern.

152
00:09:15,720 --> 00:09:19,650
You can upload it to any
site that you prefer. Um,

153
00:09:19,830 --> 00:09:22,680
just please don't include viruses and,
uh,

154
00:09:23,490 --> 00:09:27,630
and I will pick my favorite and I will
actually write lyrics and vocals to that.

155
00:09:27,960 --> 00:09:30,590
Karen is going to write lyrics for
this song. She's going to work with it,

156
00:09:30,591 --> 00:09:32,720
so definitely submit
it to her via Twitter.

157
00:09:32,930 --> 00:09:37,040
And I'll also announce the two top
entries next week in next week's video.

158
00:09:37,370 --> 00:09:40,070
Please subscribe for more programming
videos. And I think we got to go.

159
00:09:40,071 --> 00:09:43,760
We've got to go make some AI music.
It's true. So thanks for watching.

