1
00:00:00,090 --> 00:00:05,090
Hello world it Saroj and Alphago zero
is deep minds popular AI that beat the

2
00:00:06,241 --> 00:00:11,241
world champion at the game of go in a
match that was watched around the world.

3
00:00:12,240 --> 00:00:13,230
In this video,

4
00:00:13,231 --> 00:00:17,640
I'll explain how it works
using code and animations.

5
00:00:18,030 --> 00:00:18,930
If you're new here,

6
00:00:18,931 --> 00:00:23,790
don't forget to hit the subscribe button
to stay up to date on my latest AI

7
00:00:23,791 --> 00:00:24,630
content.

8
00:00:24,990 --> 00:00:29,990
Deep mind shocked the world in 2016 when
the initial version of Alphago beat 17

9
00:00:31,441 --> 00:00:35,880
time world champion Lisa Dole.
Just a year later,

10
00:00:35,910 --> 00:00:40,230
Alphago Zero unimproved virgin
that unlike its predecessor,

11
00:00:40,260 --> 00:00:44,580
was trained without any
data from real human gangs.

12
00:00:45,000 --> 00:00:49,920
It learned only by playing against
itself beat the world champion.

13
00:00:50,460 --> 00:00:53,400
In regard to Alphago,
Ian Goodfellow,

14
00:00:53,401 --> 00:00:58,401
our researcher at Google brain noted
that we're starting to see AI move beyond

15
00:00:59,131 --> 00:01:04,131
what humans can tell the computer to do
and the AI can actually figure out how

16
00:01:05,010 --> 00:01:07,920
to do something better
than the best human.

17
00:01:08,130 --> 00:01:13,130
The idea that AI can think in ways we
can't was exemplified when Alpha go played

18
00:01:14,041 --> 00:01:16,830
move 37 against Lisa Dole.

19
00:01:17,100 --> 00:01:20,490
It was a seemingly bad
move to the human master,

20
00:01:20,730 --> 00:01:23,970
but it turned out to be
crucial to its success.

21
00:01:24,270 --> 00:01:29,270
It had learned a way of playing that
surpassed the 2000 years worth of

22
00:01:29,551 --> 00:01:34,200
strategies humans had come up with,
but that should not seem scary.

23
00:01:34,650 --> 00:01:38,400
The dot product operations
became self aware.

24
00:01:39,060 --> 00:01:44,060
If we can combine this alien way of
thinking with our own to solve problems,

25
00:01:44,280 --> 00:01:48,990
there's nothing we can't do.
The possibilities really are endless.

26
00:01:48,991 --> 00:01:52,110
Whether it involves
discovery or creativity,

27
00:01:52,500 --> 00:01:57,300
we just need to make sure everybody
understands how this power works.

28
00:01:57,540 --> 00:02:02,540
A traditional game of go starts off
with an empty board and let's be real,

29
00:02:02,731 --> 00:02:03,840
probably some tea.

30
00:02:04,260 --> 00:02:09,240
Both players have an unlimited
supply of pieces called stones.

31
00:02:09,420 --> 00:02:13,500
One taking the Blackstones and
the other takes the white stones.

32
00:02:13,920 --> 00:02:18,920
The goal of the game is to use your
stones to form territories by surrounding

33
00:02:19,651 --> 00:02:21,360
vacant areas of the board.

34
00:02:21,660 --> 00:02:26,550
It's also possible to capture your
opponents stones by completely surrounding

35
00:02:26,551 --> 00:02:27,270
them.

36
00:02:27,270 --> 00:02:32,270
Both players take turns placing one of
their stones on an empty point at each

37
00:02:32,971 --> 00:02:35,400
turn with black starting off.

38
00:02:35,760 --> 00:02:40,560
These stones are placed on
the intersections of the
lines and one's played.

39
00:02:40,680 --> 00:02:45,570
Stones cannot be moved, but they
can be captured when the game ends.

40
00:02:45,630 --> 00:02:48,360
Both players count one point for each.

41
00:02:48,480 --> 00:02:53,480
They can point inside of their own
territory and one point for each stone

42
00:02:53,580 --> 00:02:58,580
they've captured the player
with the larger total
territory plus captured stones

43
00:02:59,260 --> 00:03:04,150
is declared the winner with so many
possible positions to play here.

44
00:03:04,180 --> 00:03:09,180
There are endless tactics to learn to
try and master this millennia old game.

45
00:03:10,300 --> 00:03:14,230
So when thinking about
developing an algorithm for Bo,

46
00:03:14,530 --> 00:03:18,520
we need to start listing the properties
of this game in a fundamental

47
00:03:18,550 --> 00:03:19,810
mathematical way.

48
00:03:20,140 --> 00:03:25,140
We know that go is a
discreet deterministic game
with perfect information.

49
00:03:26,650 --> 00:03:31,650
Meaning there are individually separate
and distinct moves and positions.

50
00:03:32,170 --> 00:03:34,210
Every move has a set outcome.

51
00:03:34,540 --> 00:03:39,540
There are two players competing against
one another and both players are able to

52
00:03:40,241 --> 00:03:42,820
see everything.
The whole position of the board.

53
00:03:43,000 --> 00:03:48,000
It turns out that tic TAC toe is another
game that has those same properties.

54
00:03:48,760 --> 00:03:50,620
Life isn't though,
unless you're Neil.

55
00:03:51,250 --> 00:03:54,490
If you were to develop an
AI to beat tic tac toe,

56
00:03:54,520 --> 00:03:59,140
then we could write a script that would
pick a move that results in the lowest

57
00:03:59,141 --> 00:04:00,880
score for the opponent.

58
00:04:01,210 --> 00:04:05,710
We know that there are three possible
values for a given game state.

59
00:04:05,770 --> 00:04:08,890
Either it's a loss,
a win or a draw,

60
00:04:09,070 --> 00:04:14,070
so our script would try out every possible
game states in real time forming a

61
00:04:15,221 --> 00:04:16,450
tree of states.

62
00:04:16,720 --> 00:04:21,100
Then perform a depth first search
to find the optimal move to play.

63
00:04:21,460 --> 00:04:22,960
When we run our script,

64
00:04:23,020 --> 00:04:28,020
we'll find that it will always either
win or draw with any human it plays

65
00:04:28,031 --> 00:04:32,680
against, but could we apply this
same tree search to go well?

66
00:04:32,681 --> 00:04:37,681
It turns out that there are more possible
go positions than there are atoms in

67
00:04:38,021 --> 00:04:38,920
the universe.

68
00:04:39,190 --> 00:04:44,190
So since go is such a complex game with
so many possible moves to play using a

69
00:04:46,061 --> 00:04:50,560
brut force search to find the
best one simply isn't possible.

70
00:04:50,620 --> 00:04:52,360
Using current computing hardware,

71
00:04:52,750 --> 00:04:57,750
we need a strategy to initially explore
several possible moves on the board.

72
00:04:58,930 --> 00:05:03,930
Then focus this exploration over time
as certain moves are found to be more

73
00:05:04,841 --> 00:05:07,420
likely to lead to wins than others.

74
00:05:07,750 --> 00:05:12,750
When our depth first search
method estimated the value
of a given state in the

75
00:05:12,791 --> 00:05:13,624
search,

76
00:05:13,720 --> 00:05:18,720
both players must play optimally choosing
the move that gives them the best

77
00:05:19,001 --> 00:05:23,890
value and this required
computationally expensive recursion,

78
00:05:24,070 --> 00:05:28,150
but instead of making the
players choose optimal moves,

79
00:05:28,330 --> 00:05:33,330
what if we computed the value of a state
by making the players choose random

80
00:05:33,671 --> 00:05:36,700
moves from there on and seeing who wins?

81
00:05:37,090 --> 00:05:41,260
This is the basic idea behind
the Monte Carlo tree search,

82
00:05:41,440 --> 00:05:46,440
the Goto Algorithm for writing bots to
play discreet deterministic games with

83
00:05:46,991 --> 00:05:48,100
perfect information.

84
00:05:48,460 --> 00:05:53,050
It uses random exploration to
estimate the value of a game state.

85
00:05:53,410 --> 00:05:58,410
A single random game can be
called a layout and if we
play thousand layouts from

86
00:05:58,881 --> 00:05:59,960
a given position,

87
00:06:00,050 --> 00:06:04,070
x and player a wins
more than half the time,

88
00:06:04,400 --> 00:06:09,400
it's likely that the position x is
better for player a than for player B.

89
00:06:09,770 --> 00:06:14,770
We can create a Monte Carlo value function
that estimates the value of a state

90
00:06:15,770 --> 00:06:18,260
using a given number of random play out.

91
00:06:18,590 --> 00:06:21,860
The code will be similar to
the depth first search code,

92
00:06:22,100 --> 00:06:26,870
but the difference is that instead of
iterating through all move possibilities

93
00:06:27,080 --> 00:06:30,200
and picking the best one,
we randomly choose moves.

94
00:06:30,800 --> 00:06:34,340
But random doesn't sound too intelligent,
does it?

95
00:06:34,880 --> 00:06:39,880
What if we could have our algorithm
choose a move using some knowledge of what

96
00:06:40,851 --> 00:06:45,110
moves are worth playing,
but not by trying them all out,

97
00:06:45,140 --> 00:06:48,380
which would be very
expensive computationally,

98
00:06:48,620 --> 00:06:53,620
but instead by computing a heuristic
approximation of the true value of each

99
00:06:54,681 --> 00:06:57,830
move and choosing moves within play outs.

100
00:06:57,950 --> 00:06:59,840
Based on this heuristic,

101
00:07:00,350 --> 00:07:04,760
the UCT or upper confidence
tree score can help with this.

102
00:07:05,030 --> 00:07:10,030
It's made for balancing both game
exploration and exploitation in a sensible

103
00:07:10,911 --> 00:07:14,360
way.
I computing a heuristic value of a move.

104
00:07:14,420 --> 00:07:19,420
The the is computed using the aggregated
score after playing a move in all

105
00:07:20,181 --> 00:07:24,620
simulations thus far.
The number of plays of that move,

106
00:07:24,890 --> 00:07:29,890
the number of games states
and an adjustable constant
representing the amount of

107
00:07:30,291 --> 00:07:35,291
exploration to allow this equation helps
balance exploitation by playing known

108
00:07:36,680 --> 00:07:40,010
moves with high values and exploration.

109
00:07:40,280 --> 00:07:45,110
Trying how moves that have a low visit
count and updating the algorithm with

110
00:07:45,260 --> 00:07:48,830
this new found knowledge
of how valuable they are.

111
00:07:49,340 --> 00:07:53,180
Notice that UCT involves
learning over time,

112
00:07:53,360 --> 00:07:58,360
whereas both DFS and plain MCTs or static
at first UCT is exploring lots of game

113
00:08:01,151 --> 00:08:03,740
states,
but as it collects more data,

114
00:08:03,950 --> 00:08:08,030
the random play outs become
less random and more heavy,

115
00:08:08,270 --> 00:08:13,270
meaning it explores move paths that
have already proven to be good choices,

116
00:08:13,790 --> 00:08:18,470
ignoring the rest.
This creates a self reinforcing cycle,

117
00:08:18,471 --> 00:08:22,400
becoming more skewed towards
good moves than bad moves.

118
00:08:22,730 --> 00:08:23,780
But check this out.

119
00:08:24,020 --> 00:08:29,020
Sometimes two game states can have
pieces in different positions,

120
00:08:29,420 --> 00:08:34,420
but the essential components of
the situation or the same meetings

121
00:08:35,041 --> 00:08:35,960
strategically,

122
00:08:36,230 --> 00:08:41,230
the difference in peace locations
might not matter to the UCT score.

123
00:08:41,600 --> 00:08:45,470
These are two distinct game states,
but to humans,

124
00:08:45,471 --> 00:08:50,471
we could intuitively know to that
similar strategies would apply to both.

125
00:08:50,840 --> 00:08:53,060
It's essentially the same game state.

126
00:08:53,780 --> 00:08:58,780
Rather than having AI memorize a surestep
value for every game state it finds

127
00:09:00,001 --> 00:09:02,430
and storing it in a hash table.

128
00:09:02,670 --> 00:09:07,380
What if it could learn heuristic
values from images of the game board?

129
00:09:07,770 --> 00:09:12,770
Alphago zero sprinkled in a convolutional
neural network to do just that.

130
00:09:13,290 --> 00:09:17,520
It used a massive 20 layer
residual network for this.

131
00:09:17,820 --> 00:09:22,740
This is a convolutional network that
avoids the vanishing gradient problem

132
00:09:23,010 --> 00:09:24,660
common in neural networks,

133
00:09:24,661 --> 00:09:29,661
which makes them unable to learn longterm
sequences by creating a shortcut at

134
00:09:30,691 --> 00:09:31,500
each layer,

135
00:09:31,500 --> 00:09:36,500
which allows the gradient to skip over
intermediate layers and reach the bottom

136
00:09:36,871 --> 00:09:40,110
during backpropagation
without being diminished.

137
00:09:40,350 --> 00:09:44,130
While neural nets usually
output a single fixed output,

138
00:09:44,370 --> 00:09:46,380
their network had two heads.

139
00:09:46,650 --> 00:09:51,650
One took the output of the first 20 layers
and produced probabilities of the go

140
00:09:51,781 --> 00:09:56,781
agent making certain moves and the other
took the output of the first 20 layers

141
00:09:57,870 --> 00:10:01,500
and output a probability of
the current player winning.

142
00:10:01,860 --> 00:10:06,860
So the training process of Alphago zero
for playing against itself in Games of

143
00:10:08,250 --> 00:10:13,170
digital go started off with
initializing the residual network.

144
00:10:13,380 --> 00:10:18,380
It used 1600 Monte Carlo tree search
simulations per move that it played and

145
00:10:18,901 --> 00:10:23,901
sampled about 2000 positions from
the most recent 500,000 games.

146
00:10:24,210 --> 00:10:28,920
Along with whether that game
was won or lost for every move.

147
00:10:28,950 --> 00:10:33,950
It recorded the results of the
MCTs evaluation of those positions,

148
00:10:34,800 --> 00:10:38,940
meaning how good the different
moves in these positions were.

149
00:10:39,240 --> 00:10:44,190
Ace on looking ahead and whether the
current player won or lost the game.

150
00:10:44,610 --> 00:10:49,610
The neural network was trained using
both the move evaluations produced by the

151
00:10:49,891 --> 00:10:54,891
MCTs search and whether the current player
won or lost at every 1000 iterations

152
00:10:57,360 --> 00:10:58,320
of this process.

153
00:10:58,320 --> 00:11:03,320
The algorithm evaluated the current
residual network against the previous best

154
00:11:04,111 --> 00:11:07,110
version.
If it won the majority of the Games,

155
00:11:07,170 --> 00:11:11,880
it would begin using it to generate
self play games instead of the previous

156
00:11:11,881 --> 00:11:13,650
version and that's really it.

157
00:11:13,651 --> 00:11:18,651
It just use simulations to generate
training data for its neural nets.

158
00:11:18,960 --> 00:11:22,680
Then learned from them in
a supervised way. In total,

159
00:11:22,681 --> 00:11:27,681
it took Google 40 days to train Alphago
zero to become the best go player in the

160
00:11:29,191 --> 00:11:33,450
world using 64 GPU and 19 CPS,

161
00:11:33,720 --> 00:11:38,490
which costs millions of dollars.
While the hardware is expensive right now,

162
00:11:38,520 --> 00:11:42,780
those costs are dropping.
And the idea of Alphago,

163
00:11:42,840 --> 00:11:47,840
an AI that's able to master such a
complex game better than any human can by

164
00:11:49,381 --> 00:11:53,970
playing against itself with no
human knowledge input is dope.

165
00:11:53,971 --> 00:11:56,400
Aif Hey, you made it to the
end. You beautiful ways,

166
00:11:56,401 --> 00:11:59,760
or you hit subscribe and connect
with me on Twitter, Instagram,

167
00:11:59,761 --> 00:12:04,350
and Facebook for more developer education.
For now, I've got to search a tree,

168
00:12:04,500 --> 00:12:05,820
so thanks for watching.

