1
00:00:00,030 --> 00:00:00,751
Hello world.

2
00:00:00,751 --> 00:00:05,751
It's a Raj and let's see if we can teach
an AI car how to drive to the top of a

3
00:00:06,301 --> 00:00:10,830
mountain using a super popular
strategy called Q learning.

4
00:00:11,130 --> 00:00:14,400
Since Video Games are
considered virtual worlds,

5
00:00:14,610 --> 00:00:19,610
we'll want to use reinforcement learning
to help our AI learn how to best

6
00:00:20,010 --> 00:00:24,240
interact with its environment.
That means using time delayed labels,

7
00:00:24,241 --> 00:00:29,241
Aka rewards to help it learn how best
to act in the environment through trial

8
00:00:29,611 --> 00:00:31,500
and error to complete its objective.

9
00:00:31,710 --> 00:00:35,730
If an AI has a model of all the
necessary elements of its environment,

10
00:00:35,910 --> 00:00:39,420
meaning the states rewards,
transition model, et cetera.

11
00:00:39,510 --> 00:00:43,350
Basically everything that makes
up a mark Haub decision process,

12
00:00:43,560 --> 00:00:48,420
it can easily then use a
planning algorithm to compute
a solution to whatever

13
00:00:48,421 --> 00:00:52,290
its objective is. So easy. A
liberal arts major could do it.

14
00:00:53,280 --> 00:00:56,070
This is considered model based learning.

15
00:00:56,190 --> 00:01:01,190
An Ai will interact in the environment
and from the history of its interactions.

16
00:01:01,350 --> 00:01:06,330
The AI will try to approximate the
environments models afterwards.

17
00:01:06,331 --> 00:01:07,830
Given the models it's learned.

18
00:01:07,950 --> 00:01:12,950
The AI can use the value iteration or
policy iteration algorithms to find an

19
00:01:13,021 --> 00:01:14,040
optimal policy,

20
00:01:14,250 --> 00:01:19,250
but our agent doesn't necessarily have
to try and learn an explicit model for

21
00:01:19,621 --> 00:01:20,454
its environment.

22
00:01:20,550 --> 00:01:25,380
It can also derive an optimal policy
directly from its interactions with the

23
00:01:25,381 --> 00:01:27,600
environment without building a model,

24
00:01:27,750 --> 00:01:30,870
and this is called model
free learning model.

25
00:01:30,871 --> 00:01:35,871
Free learning involves predicting the
value function of a certain policy without

26
00:01:36,810 --> 00:01:39,720
having a concrete model
of the environment.

27
00:01:39,960 --> 00:01:44,670
The simplest way to do this is
using the Manet Carlo technique,

28
00:01:45,090 --> 00:01:49,500
meaning using repeated random
sampling to obtain numerical results.

29
00:01:49,830 --> 00:01:54,510
But this only works on episodic tasks
where you have a certain set of actions.

30
00:01:54,690 --> 00:01:58,380
Then the episode ends with
some total reward money.

31
00:01:58,381 --> 00:02:02,940
Carlo learning states that the return of
estate is just the mean average of the

32
00:02:02,941 --> 00:02:06,660
total reward from when a
state appeared onwards.

33
00:02:06,810 --> 00:02:11,340
So in order to reduce the variance, we
can use a different method of prediction.

34
00:02:11,790 --> 00:02:16,560
Temporal difference learning updates
the values of each state based on a

35
00:02:16,561 --> 00:02:19,740
prediction of the final return.
For example,

36
00:02:19,860 --> 00:02:24,860
let's say Monte Carlo learning takes a
hundred actions and then updates them all

37
00:02:25,080 --> 00:02:26,820
based on the final return.

38
00:02:26,910 --> 00:02:31,910
Td learning would take an action and
then update the value of the previous

39
00:02:32,011 --> 00:02:35,190
action based on the value
of the current action.

40
00:02:35,580 --> 00:02:40,580
Td Learning has the advantage of updating
values on more recent trends in order

41
00:02:41,251 --> 00:02:44,250
to capture more of the
effect of a certain state.

42
00:02:44,610 --> 00:02:49,610
Td has a lower variance than Monte Carlo
as each update depends on less factors.

43
00:02:50,820 --> 00:02:51,511
However,

44
00:02:51,511 --> 00:02:56,511
Monte Carlo has no bias as values are
updated directly towards the final return.

45
00:02:57,300 --> 00:03:01,360
While TD has some bias, as values
are updated towards a prediction,

46
00:03:01,630 --> 00:03:06,630
both TD and Monte Carlo or actually
opposite ends of a spectrum between full

47
00:03:07,091 --> 00:03:09,550
look ahead and one step look ahead,

48
00:03:09,880 --> 00:03:14,880
any number of steps can be taken before
passing the return back to an action.

49
00:03:15,190 --> 00:03:17,200
So a question arises.

50
00:03:17,290 --> 00:03:20,860
How many steps is optimal to look ahead?
Well,

51
00:03:20,861 --> 00:03:25,240
it varies greatly depending
on the environment and it's
often a hardcoded hyper

52
00:03:25,241 --> 00:03:25,780
parameter.

53
00:03:25,780 --> 00:03:30,730
Td learning can be used to learn both
the value function and the Q function.

54
00:03:31,180 --> 00:03:35,260
What's that? You said, does Q stand
for quick time? No, thankfully.

55
00:03:35,680 --> 00:03:40,660
If we want our AI to always choose an
action that maximizes the discounted

56
00:03:40,661 --> 00:03:44,830
future reward,
we'd want to use some form of TD learning.

57
00:03:45,130 --> 00:03:50,130
We could define a function cue that
represents the maximum discounted future

58
00:03:50,291 --> 00:03:55,291
award when we perform an action a in
state s and continue optimally from that

59
00:03:56,051 --> 00:03:56,884
point on.

60
00:03:56,950 --> 00:04:01,720
The way to think about this function is
that it's the best possible score at the

61
00:04:01,721 --> 00:04:02,410
end of the game.

62
00:04:02,410 --> 00:04:06,850
After performing an action a in a state
s it's called the Q function because it

63
00:04:06,851 --> 00:04:11,230
represents the quality of a certain
action in a given state. Using it,

64
00:04:11,260 --> 00:04:15,690
we can estimate the score at the end of
the game knowing just the current state

65
00:04:15,940 --> 00:04:19,990
and action and not knowing actions
and rewards coming after that.

66
00:04:20,290 --> 00:04:25,290
So suppose we're in a state and deciding
whether we should take action a or B,

67
00:04:25,900 --> 00:04:29,680
we want to select the action that results
in the highest score at the end of the

68
00:04:29,681 --> 00:04:32,710
game.
Once we have the magical Q function,

69
00:04:32,860 --> 00:04:35,020
the answer becomes really simple.

70
00:04:35,200 --> 00:04:38,560
We can just pick the action
with the highest Q value.

71
00:04:38,770 --> 00:04:43,770
So we need to learn this Q function and
we can call this process cue learning.

72
00:04:44,470 --> 00:04:49,470
We can express the Q value of state s
and action a in terms of the Q value of

73
00:04:49,481 --> 00:04:54,370
the next state s. This is called the
bellman equation. If you think about it,

74
00:04:54,400 --> 00:04:55,630
it makes sense.

75
00:04:55,870 --> 00:05:00,870
Maximum future award for
the state inaction is the
immediate reward plus maximum

76
00:05:01,511 --> 00:05:03,790
future reward for the next state.

77
00:05:04,120 --> 00:05:09,070
The main idea in cue learning is that
we can iteratively approximate the Q

78
00:05:09,071 --> 00:05:13,090
function using the bellman equation.
In the simplest case,

79
00:05:13,150 --> 00:05:18,150
the Q function is implemented as a
table with states as rows and actions as

80
00:05:18,850 --> 00:05:23,590
columns. So to start the Q
table is initialized randomly.

81
00:05:23,830 --> 00:05:28,830
Then the agent starts to interact with
the environment and upon each interaction

82
00:05:29,170 --> 00:05:33,850
the agent will observe the reward of
its action and the state transition.

83
00:05:34,180 --> 00:05:39,180
The agent computes the observed Q value
and then updates its own estimate of

84
00:05:39,671 --> 00:05:40,360
cute.

85
00:05:40,360 --> 00:05:45,340
So we've got a basic iteration going
and it works for simple policies.

86
00:05:45,430 --> 00:05:50,200
However, there's a problem, it's
not exploring like Hernand Cortez.

87
00:05:50,201 --> 00:05:51,430
Shout out to AP history.

88
00:05:51,910 --> 00:05:56,290
Let's say we're at a state with two doors,
one door,

89
00:05:56,291 --> 00:06:00,650
it gives a reward of plus one every time
and the other door gives zero reward,

90
00:06:00,710 --> 00:06:05,360
but has a 10% chance of giving
a 100 reward on the first run.

91
00:06:05,420 --> 00:06:08,390
The second door doesn't get lucky
and returns a reward of zero.

92
00:06:08,720 --> 00:06:10,280
Since it's acting greedily,

93
00:06:10,370 --> 00:06:14,600
the Ai will only ever take the first
doors since the considers it the better

94
00:06:14,601 --> 00:06:18,680
option.
So how do we fix this random exploration?

95
00:06:18,770 --> 00:06:20,660
At some probability,
the program,

96
00:06:20,661 --> 00:06:24,530
we'll take a random action instead
of taking the optimal action.

97
00:06:24,950 --> 00:06:29,950
This allows it to eventually figure out
that there is some extremely high reward

98
00:06:29,961 --> 00:06:33,650
hidden behind the second door
if he tries it out. However,

99
00:06:33,651 --> 00:06:38,651
we want the AI to eventually converge at
some optimal policy so we can lower the

100
00:06:39,081 --> 00:06:43,610
probability of taking a random action
over time as the agent becomes more

101
00:06:43,611 --> 00:06:46,730
confident with its estimate of Q values.

102
00:06:46,760 --> 00:06:50,450
So using Q learning and
open Ai's gym environment,

103
00:06:50,480 --> 00:06:54,740
we can teach a mountain car how
to successfully climb a mountain.

104
00:06:55,130 --> 00:06:56,420
In this environment.

105
00:06:56,450 --> 00:07:00,830
There is a car on a one dimensional
track between two mountains.

106
00:07:01,040 --> 00:07:05,180
The goal of the car is to climb
the mountain on its right. However,

107
00:07:05,181 --> 00:07:09,560
the engine is not strong enough to climb
mountains without having to go back to

108
00:07:09,561 --> 00:07:14,270
gain some momentum by climbing
the mountain on the left here,

109
00:07:14,271 --> 00:07:18,020
the agent is the car and
possible actions are drive left,

110
00:07:18,200 --> 00:07:21,260
do nothing or drive
right at every time step,

111
00:07:21,410 --> 00:07:25,760
the agent receives a penalty of minus one
which means that the goal of the agent

112
00:07:25,880 --> 00:07:30,880
is the climb the right mountain as
fast as possible to minimize the sum of

113
00:07:31,101 --> 00:07:33,200
negative one penalties it receives.

114
00:07:33,410 --> 00:07:38,410
The observation is two
continuous variables
representing the velocity and the

115
00:07:38,511 --> 00:07:39,710
position of the car.

116
00:07:39,950 --> 00:07:44,180
Since the observation variables
are continuous for our algorithm,

117
00:07:44,360 --> 00:07:47,270
we desk or ties the observed
values in order to use Q learning.

118
00:07:47,450 --> 00:07:51,860
Initially the car will take forever
to climb if we select a random action,

119
00:07:52,070 --> 00:07:52,940
but after learning,

120
00:07:52,941 --> 00:07:56,900
it learns how to climb the mountain
within less than a hundred times steps.

121
00:07:57,770 --> 00:08:01,340
You're still with me, right? Of course
you are. You beautiful wizard. Look,

122
00:08:01,360 --> 00:08:04,430
there are just three key
points to remember here. Model.

123
00:08:04,431 --> 00:08:09,050
Free learning is when an AI can directly
derive an optimal policy from its

124
00:08:09,080 --> 00:08:14,080
interactions with the environment without
needing to create a model beforehand.

125
00:08:15,110 --> 00:08:19,400
Cue learning is a model free learning
technique that can be used to find the

126
00:08:19,430 --> 00:08:24,430
optimal action selection policy using
a Q function and the exploration versus

127
00:08:25,250 --> 00:08:30,250
exploitation dilemma is exemplified by
the question of whether an AI should

128
00:08:30,380 --> 00:08:35,380
trust the learnt values of Q enough to
select actions based on it or try other

129
00:08:35,601 --> 00:08:38,180
actions hoping that might
give it a better reward.

130
00:08:38,510 --> 00:08:41,900
The winner of last week's coding
challenge is odd. Nones a heat.

131
00:08:42,190 --> 00:08:47,120
I'd non created an AI in swift that
navigates around a grid to find the goal

132
00:08:47,210 --> 00:08:48,890
without knowing it beforehand.

133
00:08:49,130 --> 00:08:54,130
Then it constructs an optimal path to
the goal using model free reinforcement

134
00:08:54,411 --> 00:08:54,960
learning,

135
00:08:54,960 --> 00:08:59,960
superb job and shout out to Alberto
Garces for creating a really awesome value

136
00:09:00,511 --> 00:09:05,511
iteration algorithm for getting a taxi
from point a to point B efficiently.

137
00:09:05,970 --> 00:09:10,590
This week's challenge is to
create a simple cue learning
algorithm in python for

138
00:09:10,591 --> 00:09:14,820
an AI in any type of game world. Get
humbling scope in the comments section,

139
00:09:14,821 --> 00:09:19,440
and I will personally review and
announce the top two entries next Friday.

140
00:09:19,470 --> 00:09:21,600
Please subscribe for
more programming videos,

141
00:09:21,601 --> 00:09:25,800
and now I've got approximately my
value function, so thanks for watching.

