1
00:00:00,150 --> 00:00:03,900
I'm going to show you mathematically
what's happening in here. Are you ready?

2
00:00:04,680 --> 00:00:07,350
Hello world.
It's Saroj and the brain.

3
00:00:07,410 --> 00:00:12,410
It's deeply influenced so much machine
learning theory and in this video I'm

4
00:00:12,541 --> 00:00:17,310
going to take a little bit of a different
approach to my usual lectures and

5
00:00:17,311 --> 00:00:22,311
introduce four unique machine learning
algorithms in the context of both human

6
00:00:23,310 --> 00:00:24,600
and animal learning.

7
00:00:24,810 --> 00:00:29,810
Ultimately using one of them to train
a digital mouse to find the exit in a

8
00:00:30,430 --> 00:00:32,100
maze.
If you're new here,

9
00:00:32,101 --> 00:00:35,520
hit subscribe to stay updated
with my latest content.

10
00:00:35,730 --> 00:00:40,730
I'm going to constantly be pushing
educational videos to this channel every

11
00:00:41,250 --> 00:00:43,440
single week.
In the early 20th century,

12
00:00:43,470 --> 00:00:48,470
a physiologist named Ivan Pavlov was
studying the physiology of the gestion in

13
00:00:48,961 --> 00:00:50,490
dogs in the lab.

14
00:00:50,520 --> 00:00:55,520
He noticed that his dogs begin to salivate
in the presence of the technician who

15
00:00:55,861 --> 00:01:00,420
normally fed them rather than only
salivating in the presence of food.

16
00:01:00,750 --> 00:01:05,750
Pavlov was inspired by this informal
observation to conduct a more formal

17
00:01:06,721 --> 00:01:10,950
experiment.
He presented an audio stimuli to a dog,

18
00:01:10,980 --> 00:01:12,090
then gave it food.

19
00:01:12,300 --> 00:01:15,990
He did this a few times and then
when the dog heard the sound,

20
00:01:16,200 --> 00:01:20,830
even without the food, it started
salivating. That beat must've been fired.

21
00:01:21,210 --> 00:01:25,710
He concluded that if a particular stimulus
in the dog surroundings was present

22
00:01:25,860 --> 00:01:27,540
when the dog was given food,

23
00:01:27,720 --> 00:01:32,720
then that stimulus could become associated
with food and cause salvation on its

24
00:01:33,720 --> 00:01:36,960
own.
Due to this positive reinforcement loop.

25
00:01:37,290 --> 00:01:42,290
This experiments popularly called Pavlov's
dog experiment helped form the basis

26
00:01:43,740 --> 00:01:46,290
for associative learning theory.

27
00:01:46,620 --> 00:01:51,620
It describes the process by which a
person or animal learns an association

28
00:01:52,021 --> 00:01:53,820
between two Stimuli.

29
00:01:54,120 --> 00:01:57,600
The basic claims of
associative learning theory,

30
00:01:57,780 --> 00:02:02,780
our that reinforcement learning is the
acquisition of associations between

31
00:02:03,001 --> 00:02:05,580
states' actions and rewards.

32
00:02:05,910 --> 00:02:10,830
The animal is acquiring these associations
through learning. In this case,

33
00:02:10,831 --> 00:02:13,920
we can assume the state to be a stimulus.

34
00:02:14,130 --> 00:02:17,370
It's useful for predicting
the potential reward.

35
00:02:17,820 --> 00:02:22,820
It also claims that the modifications
of these associations are driven I

36
00:02:22,981 --> 00:02:24,300
prediction errors.

37
00:02:24,480 --> 00:02:28,680
That means the discrepancy between what
the animal expected to get and what it

38
00:02:28,681 --> 00:02:29,670
actually got.

39
00:02:30,000 --> 00:02:35,000
The most influential idea of associative
learning theory was invented by two

40
00:02:35,221 --> 00:02:39,630
researchers named rescore law and Wagner.
A couple decades later.

41
00:02:40,290 --> 00:02:44,790
They're model is a prediction error
based learning model in which stimuli

42
00:02:44,820 --> 00:02:46,140
acquire value.

43
00:02:46,260 --> 00:02:51,260
When there is a mismatch
between the prediction and
the outcome in the equation.

44
00:02:51,810 --> 00:02:56,810
The value of stimulus s at trial t is
set equal to the value of stimulus s at

45
00:02:57,991 --> 00:03:01,930
the previous time step plus
the reward and the expectation.

46
00:03:02,380 --> 00:03:06,550
The learning rate defines how much
this prediction error is weighted.

47
00:03:06,880 --> 00:03:11,350
This prediction error says that when
the animal gets more reward than it

48
00:03:11,351 --> 00:03:12,220
expected,

49
00:03:12,430 --> 00:03:17,430
it leads to strengthening
of the associative weights
and a negative prediction

50
00:03:17,921 --> 00:03:21,460
error leads to a weakening
of the associative weights.

51
00:03:21,790 --> 00:03:26,230
The risk score La Wagner model was
groundbreaking for two reasons.

52
00:03:26,560 --> 00:03:31,560
The first reason is that it was able to
explain a huge number of phenomena in

53
00:03:32,530 --> 00:03:35,260
fear conditioning.
Using that model,

54
00:03:35,261 --> 00:03:40,261
researchers were able to map out
the detailed structure of how these

55
00:03:40,451 --> 00:03:45,310
computations could be implemented
by a Mig Dulles circuitry.

56
00:03:45,730 --> 00:03:50,620
The second reason is that it helped
developed some early natural language

57
00:03:50,710 --> 00:03:54,250
processing applications
like part of speech tagging.

58
00:03:54,580 --> 00:03:57,640
I'm sure they could have named it
something more descriptive, but hey,

59
00:03:57,760 --> 00:03:59,530
it was the 70s but wild.

60
00:03:59,531 --> 00:04:04,090
The risk score low walked her model
provided a basis for associative learning

61
00:04:04,091 --> 00:04:09,070
theory. It only estimated a
single value. We know however,

62
00:04:09,100 --> 00:04:14,100
that biological brains are
able to represent uncertainty
about the world somehow

63
00:04:14,680 --> 00:04:16,990
since the world is full of uncertainty,

64
00:04:17,500 --> 00:04:22,450
probability theory suggests that to
properly represent a biological brains

65
00:04:22,510 --> 00:04:23,980
uncertainty of the world,

66
00:04:24,220 --> 00:04:29,220
it should utilize a probability
distribution over the
possible weights instead

67
00:04:29,860 --> 00:04:30,940
of a single value.

68
00:04:31,390 --> 00:04:36,390
We can use a famous statistical rule
invented by Thomas Bayes called never talk

69
00:04:36,941 --> 00:04:41,110
about fight club. Wait, that's
something else. It's called Bayes rule.

70
00:04:41,140 --> 00:04:43,570
It states that the posterior probability,

71
00:04:43,600 --> 00:04:48,600
which is the probability that the
hypothesis is true is equal to the prior

72
00:04:48,671 --> 00:04:49,361
probability,

73
00:04:49,361 --> 00:04:53,950
which is how well the hypothesis fits
existing knowledge multiplied by the

74
00:04:53,951 --> 00:04:55,450
likelihood of the evidence.

75
00:04:55,600 --> 00:04:59,440
If the hypothesis is true
divided by the prior probability,

76
00:04:59,441 --> 00:05:04,120
that the evidence itself is true where
the evidence is a description of how well

77
00:05:04,121 --> 00:05:06,940
the hypothesis explains the new evidence.

78
00:05:07,390 --> 00:05:12,010
Another way of saying this is that the
probability of event a occurring given

79
00:05:12,011 --> 00:05:15,790
that be as true is equal to
the probability of B occurring.

80
00:05:15,791 --> 00:05:20,791
Given that a is true multiplied by the
independent probability of a divided by

81
00:05:21,161 --> 00:05:25,960
the independent probability
of B, the Basie and
generalization of the risk score.

82
00:05:25,961 --> 00:05:30,490
Low Wagner model is embodied in
what's called the Coleman filter.

83
00:05:30,880 --> 00:05:35,880
It states that uncertainty grows over
time due to the random diffusion of the

84
00:05:36,401 --> 00:05:37,234
weights.

85
00:05:37,360 --> 00:05:42,360
It also states that this uncertainty
can be reduced by observing the data.

86
00:05:42,760 --> 00:05:47,760
It uses a series of measurements observed
over time and produce his estimates of

87
00:05:48,130 --> 00:05:52,720
unknown variables that tend to be more
accurate than those based on a single

88
00:05:52,721 --> 00:05:57,721
measurement alone by estimating a
joint probability distribution over the

89
00:05:57,741 --> 00:05:59,810
variables for each timeframe.

90
00:06:00,200 --> 00:06:05,200
The common filter has numerous
applications in technology
including navigation

91
00:06:05,451 --> 00:06:10,040
and control of vehicles like
Aircrafts sell flying planes. Yup.

92
00:06:10,280 --> 00:06:13,910
It's also applied in time
series analysis and robotics.

93
00:06:14,180 --> 00:06:18,470
It works by modeling the central nervous
system's control of movement because of

94
00:06:18,471 --> 00:06:23,270
the time delay between issuing motor
commands and receiving sensory feedback.

95
00:06:23,450 --> 00:06:28,430
The common filter is a realistic model
for making estimates of the current state

96
00:06:28,580 --> 00:06:31,190
of a motor system and using commands.

97
00:06:31,250 --> 00:06:34,070
It's a two step process
in the predictions step.

98
00:06:34,190 --> 00:06:38,180
It produces estimates of the current
state variables along with their

99
00:06:38,181 --> 00:06:42,680
uncertainties. Once the outcome of
the next measurement is observed,

100
00:06:42,920 --> 00:06:47,920
these estimates are updated
using a weighted average
with the most weight given

101
00:06:48,201 --> 00:06:50,480
to estimates with higher certainty.

102
00:06:50,750 --> 00:06:55,750
It's a recursive real time algorithm
using just the present input measurements

103
00:06:56,210 --> 00:07:00,470
and the previously calculated state
as well as its uncertainty matrix.

104
00:07:00,560 --> 00:07:03,890
So far we've broken
down tasks into trials,

105
00:07:04,130 --> 00:07:08,640
but real life operates in continuous
time and our algorithms are short

106
00:07:08,641 --> 00:07:12,500
shortsighted. They've only been able
to predict the immediate reward,

107
00:07:12,650 --> 00:07:17,650
the one that will be received in the very
next state to extend the capabilities

108
00:07:18,351 --> 00:07:20,900
of what we're able to
model mathematically.

109
00:07:21,260 --> 00:07:26,210
Let's switch from classical learning
theory to modern reinforcement learning

110
00:07:26,211 --> 00:07:31,211
theory with a focus on a specific
sequential decision problem.

111
00:07:31,550 --> 00:07:36,320
A mouse trying to find the exit
in a maze full of fire. Nevermind.

112
00:07:36,740 --> 00:07:41,740
There are two popular classes
of reinforcement learning
algorithms we can use to

113
00:07:42,351 --> 00:07:46,970
help solve this problem.
The first art model free techniques,

114
00:07:47,240 --> 00:07:50,540
these attempt to build
up a table of values.

115
00:07:50,780 --> 00:07:55,640
The value is the cumulative future
award at the animal expects to get when

116
00:07:55,641 --> 00:07:58,940
performing a particular action.
In a particular state.

117
00:07:59,420 --> 00:08:04,420
The idea here is that we are not building
an explicit internal model of the

118
00:08:04,431 --> 00:08:06,050
world.
Instead,

119
00:08:06,051 --> 00:08:10,910
it were directly trying to estimate
a lookup table from trial and error.

120
00:08:11,150 --> 00:08:12,890
Once we have the lookup table,

121
00:08:12,980 --> 00:08:17,900
we can use it to choose the optimal
action for any particular state.

122
00:08:18,440 --> 00:08:21,200
The other strategy is
model based learning.

123
00:08:21,440 --> 00:08:26,440
The idea is that the animal will
construct an internal model of the world.

124
00:08:26,930 --> 00:08:31,700
That means two things. First, it
will learn a transition function,

125
00:08:31,910 --> 00:08:35,840
meaning if it's in a particular
state and it takes an action,

126
00:08:35,960 --> 00:08:40,610
what's going to be the next
state and to a reward function,

127
00:08:40,640 --> 00:08:45,640
which estimates how much reward it
expects to get in any particular state.

128
00:08:46,490 --> 00:08:51,490
We can use dynamic programming here
or a variety of algorithms to simulate

129
00:08:51,980 --> 00:08:56,980
different paths in the environment and
pick actions that lead us to the optimal

130
00:08:57,421 --> 00:09:01,020
path.
Both of these algorithms have trade offs.

131
00:09:01,200 --> 00:09:05,640
The model free system is fast.
We just perform a table look up,

132
00:09:05,790 --> 00:09:09,150
but it's inflexible.
If the reward function changes.

133
00:09:09,240 --> 00:09:12,630
All the values in the
lookup table have to change.

134
00:09:12,840 --> 00:09:17,130
Whereas in the model based learning
system, it's more flexible.

135
00:09:17,280 --> 00:09:19,770
If the reward at some state changes,

136
00:09:19,860 --> 00:09:24,860
we just change the reward function at
that state and that reward will propagate

137
00:09:25,470 --> 00:09:29,790
to all of our values via the
reward function we've defined,

138
00:09:29,970 --> 00:09:31,680
but it's slower.

139
00:09:32,010 --> 00:09:35,730
A model free learning technique
called a temporal difference.

140
00:09:35,760 --> 00:09:40,760
Learning extended the risk score la
Wagner model by introducing a discount

141
00:09:41,671 --> 00:09:44,160
factor into the prediction error,

142
00:09:44,280 --> 00:09:47,700
which helps define how much
a reward matters to an agent.

143
00:09:47,850 --> 00:09:50,610
Depending on when in time it's received,

144
00:09:50,940 --> 00:09:55,710
meaning we can make rewards that
happen in the near term worth more.

145
00:09:56,040 --> 00:10:00,660
It was invented by two researchers
named son and Barto in the late eighties

146
00:10:00,720 --> 00:10:04,740
while they wrote their defining work,
an introduction to reinforcement learning.

147
00:10:05,040 --> 00:10:05,940
Unfortunately,

148
00:10:05,941 --> 00:10:10,941
no one really cared much for TD learning
until a paper came out in the late

149
00:10:13,260 --> 00:10:13,290
nineties a decade later,

150
00:10:13,290 --> 00:10:18,290
which showed that the temporal difference
model could accurately represent

151
00:10:18,720 --> 00:10:21,990
dopamine response in the brain.
That's dope.

152
00:10:22,290 --> 00:10:27,290
If we just represent a reward by
itself without any cues just to reward,

153
00:10:27,930 --> 00:10:30,930
we'll see a burst of dopamine neurons.
However,

154
00:10:31,050 --> 00:10:33,750
if that reward is predicted by some cue,

155
00:10:33,930 --> 00:10:36,540
so the Q reliably predicts a reward,

156
00:10:36,750 --> 00:10:41,250
we won't see a dopamine burst in
response to the reward. Instead,

157
00:10:41,400 --> 00:10:45,990
we'll see a dopamine burst as a response
to the queue that predicted that

158
00:10:45,991 --> 00:10:46,824
reward.

159
00:10:46,920 --> 00:10:51,690
This contradicts the popular view
of dopamine as a reward molecule.

160
00:10:52,050 --> 00:10:54,780
If it was just reporting reward,

161
00:10:54,940 --> 00:10:58,080
then we'd expect it to respond
when reward was delivered,

162
00:10:58,290 --> 00:11:02,670
regardless of whether that reward
was predicted or not. And in fact,

163
00:11:02,700 --> 00:11:07,700
it only responds to unpredicted
rewards when reward is surprising,

164
00:11:08,310 --> 00:11:12,240
the TD model helps explain
why that's happens.

165
00:11:12,450 --> 00:11:14,730
Once the reward is fully predicted,

166
00:11:14,880 --> 00:11:17,670
there will no longer be
any prediction error.

167
00:11:17,760 --> 00:11:22,110
And the dopamine response is reporting
these prediction errors from the model.

168
00:11:22,500 --> 00:11:27,240
It's been used to create game bots that
can be humans most popularly by deep

169
00:11:27,241 --> 00:11:31,830
mind and it's deep cue learning
algorithm able to beat many Atari Games.

170
00:11:32,310 --> 00:11:37,310
Td learning helps capture some important
properties of temporal dynamics as well

171
00:11:37,621 --> 00:11:39,090
as dopamine responses,

172
00:11:39,540 --> 00:11:43,890
but it lacks the uncertainty tracking
mechanism of the common filter.

173
00:11:44,100 --> 00:11:48,540
So we need a Basie and
version of TD learning and we
can call that the coal mine

174
00:11:48,600 --> 00:11:52,260
TD instead of just estimating
a single value of the weights.

175
00:11:52,440 --> 00:11:56,470
We're estimating amine and covariance
matrix for the weights of our model.

176
00:11:56,800 --> 00:12:01,800
We can think of all four of these models
along two dimensions based on what kind

177
00:12:02,531 --> 00:12:05,770
of estimate or they are either
Bayesean or point based,

178
00:12:06,010 --> 00:12:10,120
and what the target they're trying to
estimate is either immediate reward or

179
00:12:10,121 --> 00:12:14,200
value. Here are the three points
to remember from this video.

180
00:12:14,440 --> 00:12:19,440
Associative learning is a learning
process in which a new response becomes

181
00:12:19,601 --> 00:12:22,330
associated with a particular stimulus.

182
00:12:22,630 --> 00:12:25,660
When we build mathematical
models of learning,

183
00:12:25,661 --> 00:12:30,661
we can use distributions instead of single
values to help represent uncertainty

184
00:12:31,600 --> 00:12:32,590
about the world.

185
00:12:32,860 --> 00:12:37,480
And Tim poral difference learning is
a model free learning technique that

186
00:12:37,481 --> 00:12:42,310
predicts the expected value of a variable
occurring at the end of a sequence of

187
00:12:42,311 --> 00:12:42,910
states.

188
00:12:42,910 --> 00:12:46,720
You're good looking and smart a plus
for making it to the end of this video.

189
00:12:46,750 --> 00:12:50,080
Hit subscribe and I'll give you an a plus.
Plus for now,

190
00:12:50,110 --> 00:12:52,780
I've got to chase a reward,
so thanks for watching.

