1
00:00:00,060 --> 00:00:03,180
Hello world,
it's Saroj and automated trading.

2
00:00:03,181 --> 00:00:08,181
Bots are a popular way to earn a passive
income and with so many tools available

3
00:00:09,121 --> 00:00:13,590
on the internet, anyone can build
their own. In a few simple steps.

4
00:00:13,920 --> 00:00:18,920
In this video I'll explain how to use a
reinforcement learning strategy called

5
00:00:19,340 --> 00:00:24,340
cue learning to simultaneously predict
prices for three different stocks in a

6
00:00:25,411 --> 00:00:28,140
portfolio using several data points.

7
00:00:28,320 --> 00:00:30,990
If we look at finance industry reports,

8
00:00:30,991 --> 00:00:35,991
we'll find that stock on and future
trading are all heavily automated and the

9
00:00:37,291 --> 00:00:42,291
algorithms behind both trade execution
and optimization have yielded billions of

10
00:00:44,040 --> 00:00:49,040
dollars for a few companies that invested
early on in machine learning in the

11
00:00:49,290 --> 00:00:54,120
90s and early two thousands that means
that they've been working on this for up

12
00:00:54,121 --> 00:00:55,950
to two decades now,

13
00:00:56,100 --> 00:01:01,100
which is way before the deep learning
hype started in 2012 in fact,

14
00:01:01,440 --> 00:01:06,440
AI in finance is perhaps the most
mature application of AI in any industry

15
00:01:07,051 --> 00:01:11,730
setting and while lots of these companies
are very well funded so they can

16
00:01:11,731 --> 00:01:15,780
afford the hardware necessary to
run machine learning algorithms,

17
00:01:16,020 --> 00:01:21,020
the cost of hardware is following and
the accessibility of these algorithms is

18
00:01:21,241 --> 00:01:25,080
increasing as open source culture.
It goes mainstream.

19
00:01:25,620 --> 00:01:30,620
We can see this shift of activity move
into the realm of startups by browsing

20
00:01:31,051 --> 00:01:36,051
the landing pages of companies like new
yoedicke.ai that offered traders and AI

21
00:01:36,661 --> 00:01:41,460
system for back testing and daily
forecasting with automated trading.

22
00:01:41,910 --> 00:01:45,630
Another example is Alpaca dot.
Ai got to love that name,

23
00:01:45,870 --> 00:01:50,640
which creates a deep learning engine
that learns what your winning trades are

24
00:01:50,910 --> 00:01:54,420
and then generates a unique
trading algorithm for you.

25
00:01:54,600 --> 00:01:56,550
Based on that data alone,

26
00:01:57,090 --> 00:02:00,900
you can even tune your algorithm
with sliders without using code.

27
00:02:01,260 --> 00:02:05,430
There are so many data points we could
use that effect prices at different

28
00:02:05,431 --> 00:02:08,220
scales.
News and rumors affect prices,

29
00:02:08,221 --> 00:02:11,880
micro and macro economic cycles,
new laws,

30
00:02:12,150 --> 00:02:13,290
but let's start simple.

31
00:02:13,650 --> 00:02:17,250
Our starting data set
will be three CSV files.

32
00:02:17,340 --> 00:02:22,230
Each contains stock prices
for the past 17 years for IBM,

33
00:02:22,260 --> 00:02:24,600
Microsoft and Qualcomm prospectively.

34
00:02:24,990 --> 00:02:29,990
The data set was created using the Alpha
vantage API that lets us access both

35
00:02:30,210 --> 00:02:32,940
real time and historical stock data.

36
00:02:33,300 --> 00:02:36,950
We'll want to build an algorithm
that can learn from this dataset.

37
00:02:37,440 --> 00:02:40,650
Let's think about how
to frame this problem.

38
00:02:40,920 --> 00:02:45,920
We have both historical data and access
to real time data via the API that can

39
00:02:47,131 --> 00:02:49,980
constantly pipe in updated prices,

40
00:02:50,100 --> 00:02:52,320
which we can add to our CSV files.

41
00:02:52,710 --> 00:02:56,310
This is a real time environment
where time is a dimension.

42
00:02:56,640 --> 00:03:01,640
We know then to use reinforcement as
it takes time into consideration using

43
00:03:02,741 --> 00:03:03,730
reinforcement learning.

44
00:03:03,731 --> 00:03:07,780
We can frame our problem as
a mark Haub decision process,

45
00:03:08,050 --> 00:03:13,050
which consists of states' actions and
rewards are agent will be able to execute

46
00:03:13,421 --> 00:03:16,790
an action in this environment.
And luckily for us,

47
00:03:16,850 --> 00:03:21,340
the action space in this case is
pretty simple. For any given stock,

48
00:03:21,370 --> 00:03:24,490
it can only perform three actions.
Buy,

49
00:03:24,550 --> 00:03:26,770
sell or hold buy.

50
00:03:26,771 --> 00:03:31,420
We'll buy as much stock as possible based
on current stock prices and the amount

51
00:03:31,421 --> 00:03:33,640
of cash we have whilst sell.

52
00:03:33,640 --> 00:03:38,640
We'll sell all shares of a stock and add
the generated cash to our cash balance.

53
00:03:39,340 --> 00:03:41,470
If we're buying multiple stocks,

54
00:03:41,560 --> 00:03:44,980
we would then equally distribute
the cash we have for each.

55
00:03:45,310 --> 00:03:48,010
Hold on the other hand does nothing.

56
00:03:48,160 --> 00:03:53,160
So the number of actions at any given
time step is three to the n where n is the

57
00:03:54,011 --> 00:03:58,660
number of stocks in our portfolio.
Now at every time step,

58
00:03:58,661 --> 00:04:03,661
our agent will be in a state and from
that state will make an action for our

59
00:04:04,361 --> 00:04:05,980
state.
In this case,

60
00:04:06,160 --> 00:04:09,550
let's consider it a combination
of the current stock price,

61
00:04:09,730 --> 00:04:14,590
our account balance, and the number
of stocks we own. So, for example,

62
00:04:14,620 --> 00:04:17,140
if we own 50 shares of IBM stock,

63
00:04:17,290 --> 00:04:22,290
a hundred shares of Microsoft and 20
shares of Qualcomm and have $1,500 in our

64
00:04:22,601 --> 00:04:23,434
balance,

65
00:04:23,530 --> 00:04:28,530
we can represent our state as an array
which contains the amounts and prices of

66
00:04:29,710 --> 00:04:33,520
each stock as well as the
total account balances we have.

67
00:04:33,790 --> 00:04:38,790
So it's this state category of the MDP
that will contain any and all data that

68
00:04:39,071 --> 00:04:43,150
our algorithm could learn from.
And we can add more to it later.

69
00:04:43,210 --> 00:04:48,190
But let's start simply, if we simply plot
out the amount of capital we have four,

70
00:04:48,191 --> 00:04:49,510
three times steps,

71
00:04:49,780 --> 00:04:54,460
we'll see that the first two ideas for
our reward won't help the agent learn

72
00:04:54,670 --> 00:04:57,700
from the difference in value
between each time step.

73
00:04:57,820 --> 00:05:01,990
So we'll go with the third and this
exchange environment we've defined,

74
00:05:02,020 --> 00:05:04,210
there are actually multiple agents,

75
00:05:04,390 --> 00:05:09,220
meaning there are other traders with
their own respective account balances and

76
00:05:09,221 --> 00:05:13,360
open limit orders that are
affecting prices. Unfortunately,

77
00:05:13,361 --> 00:05:15,580
we don't have access to their data.

78
00:05:15,670 --> 00:05:20,140
So we're dealing with a partially
observable Markov decision process.

79
00:05:20,440 --> 00:05:24,850
And because it's partially observable
and we don't know what a full state looks

80
00:05:24,851 --> 00:05:25,390
like,

81
00:05:25,390 --> 00:05:30,390
we also don't know what either the
reward function or transition probability

82
00:05:30,850 --> 00:05:35,320
looks like. Dynamic programming
is a technique that we use.

83
00:05:35,380 --> 00:05:40,380
If we knew these two terms beforehand
to compute the optimal policy,

84
00:05:41,080 --> 00:05:43,600
but since we don't know them beforehand,

85
00:05:43,840 --> 00:05:48,840
we could instead use another model based
reinforcement learning technique to

86
00:05:48,881 --> 00:05:52,870
learn these two functions.
Once we do that,

87
00:05:52,900 --> 00:05:57,650
we'd be able to compute the optimal
policy because we'd eventually learn what

88
00:05:57,651 --> 00:06:02,651
the effect is going to be of taking a
particular action in a particular state.

89
00:06:03,860 --> 00:06:08,060
But what if we didn't need to
explicitly learn these two functions?

90
00:06:08,270 --> 00:06:12,740
What if we could just learn the mapping
from states to actions directly?

91
00:06:13,190 --> 00:06:18,190
We could compute a policy without
needing to construct a full model of our

92
00:06:18,291 --> 00:06:21,590
environment model free
reinforcement learning.

93
00:06:22,220 --> 00:06:26,270
That'd be much more computationally
efficient. So let's start with that.

94
00:06:26,540 --> 00:06:28,820
Of all the model free techniques,

95
00:06:29,000 --> 00:06:32,150
cue learning is the most
popular in cue learning.

96
00:06:32,180 --> 00:06:35,990
We define a function to noted as Q of Sna.

97
00:06:36,380 --> 00:06:41,380
This represents the maximum discounted
future reward when we perform an action

98
00:06:41,690 --> 00:06:45,560
in state s and continue
optimally from that point on.

99
00:06:45,890 --> 00:06:48,400
It's also called the
action value functions.

100
00:06:48,401 --> 00:06:51,020
Since it measures the value of an action,

101
00:06:51,320 --> 00:06:56,320
we can think of this function as the
highest possible account balance we can

102
00:06:56,541 --> 00:06:59,210
have at the end of a training episode.

103
00:06:59,450 --> 00:07:04,450
After performing action a in state s
it represents the quality of a certain

104
00:07:04,911 --> 00:07:07,850
action in a given state.
In our case,

105
00:07:07,851 --> 00:07:11,090
our possible actions are
either buy, sell, or hold.

106
00:07:11,390 --> 00:07:14,330
Once we'd have this queue function,
it will rate all three.

107
00:07:14,570 --> 00:07:18,170
Then we can just pick the action
that has the highest Q value.

108
00:07:18,500 --> 00:07:20,810
So how do we compute this Q value?

109
00:07:21,140 --> 00:07:26,140
We can express the Q value of state s
and action a in terms of the Q value of

110
00:07:26,541 --> 00:07:29,990
the next state.
This is called the bellman equation.

111
00:07:30,200 --> 00:07:35,200
It says that the maximum future reward
for this state and action is the

112
00:07:36,021 --> 00:07:41,000
immediate reward plus the maximum
future reward for the next state.

113
00:07:41,450 --> 00:07:46,250
The great thing about the bellman
equation is that it lets us represent the

114
00:07:46,251 --> 00:07:51,140
value of a given state in terms
of the value of the next date.

115
00:07:51,350 --> 00:07:55,070
It mathematically defines the
relationship between states,

116
00:07:55,250 --> 00:07:59,270
which makes it possible for us
to approximate the Q function.

117
00:07:59,660 --> 00:08:00,980
In the most simple case,

118
00:08:01,010 --> 00:08:06,010
the Q function is implemented as a
table with states as rows and actions as

119
00:08:06,501 --> 00:08:07,334
columns.

120
00:08:07,400 --> 00:08:12,350
We'll initialize Q randomly then observed
the initial state s which is going to

121
00:08:12,351 --> 00:08:16,550
be an array. Then we iteratively
compute the following four steps.

122
00:08:16,700 --> 00:08:18,950
We select an execute an action,

123
00:08:19,220 --> 00:08:23,900
then observed the reward and new state.
Using these three values.

124
00:08:23,930 --> 00:08:27,860
We use the bellman equation
to update our Q function,

125
00:08:28,020 --> 00:08:30,500
then set the current
state to the next state.

126
00:08:30,830 --> 00:08:35,360
We just keep repeating that process for
as long as we want to set an episode for

127
00:08:35,420 --> 00:08:38,000
based on just the data
we have given our model,

128
00:08:38,001 --> 00:08:40,490
we can view the performance in this plot.

129
00:08:40,910 --> 00:08:45,910
The red line is our initial investment
and the Green Line is the average after

130
00:08:46,131 --> 00:08:47,570
2000 runs.

131
00:08:47,700 --> 00:08:52,490
It looks like we made about 4,000
simulated dollars in a thousand days.

132
00:08:52,940 --> 00:08:56,970
Notice though that the portfolio
values are very volatile,

133
00:08:57,120 --> 00:08:59,940
which indicates the
instability of our agent.

134
00:09:00,180 --> 00:09:02,490
So while that was a decent profit,

135
00:09:02,670 --> 00:09:07,670
the variants is too high for us to
ignore that to longterm more data would

136
00:09:07,831 --> 00:09:12,750
definitely help. We could add in
binary company news for each stock,

137
00:09:12,870 --> 00:09:15,120
meaning positive or negative tweets.

138
00:09:15,480 --> 00:09:20,480
We could also add in some company
performance metadata like market share,

139
00:09:21,390 --> 00:09:25,740
growth rate, market
capitalization, profit margins,

140
00:09:25,930 --> 00:09:29,730
et cetera.
So our state array would be much bigger,

141
00:09:29,731 --> 00:09:32,100
encompassing many more values.

142
00:09:32,370 --> 00:09:36,870
That would mean that our state
action space would be much larger,

143
00:09:37,020 --> 00:09:42,000
so approximating our Q function
would take exponentially longer.

144
00:09:42,060 --> 00:09:46,500
However, there exists an algorithm
that can approximate any function.

145
00:09:46,740 --> 00:09:50,160
Can you guess what it
is? Yes. Neural networks.

146
00:09:50,340 --> 00:09:54,690
If we use a neural network to
approximate the Q function,

147
00:09:55,050 --> 00:09:58,680
we could then consider our
method deep cue learning,

148
00:09:59,070 --> 00:10:03,810
and of course there are more advanced
are El techniques out there by policy

149
00:10:03,811 --> 00:10:07,710
gradients, actor, critic, and
inverse reinforcement learning.

150
00:10:08,220 --> 00:10:10,770
All of which I'll talk
about in future videos.

151
00:10:11,100 --> 00:10:13,860
Three things to remember
from this video though,

152
00:10:14,190 --> 00:10:19,190
a partially observable Mark Cobb decision
process is one where we don't know

153
00:10:19,351 --> 00:10:23,040
what the true state looks like,
but we can observe a part of it.

154
00:10:23,530 --> 00:10:27,810
A cute table is one where the states
are rows and actions are columns.

155
00:10:27,960 --> 00:10:31,560
It helps us find the best
action to take for each state.

156
00:10:31,920 --> 00:10:36,900
And Cue learning is the process of
learning what this cute table is directly

157
00:10:37,170 --> 00:10:41,760
without needing to learn
either the transition,
probability or reward function.

158
00:10:42,000 --> 00:10:43,380
Thanks for watching my video.

159
00:10:43,410 --> 00:10:47,520
Please subscribe for more programming
videos and for now I've got to find the

160
00:10:47,521 --> 00:10:49,590
cute table,
so thanks for watching.

