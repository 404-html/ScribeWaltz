1
00:00:00,090 --> 00:00:03,870
Hello world, it's a Raj. And our
task today is going to be two.

2
00:00:03,871 --> 00:00:07,770
Tried to predict if a team
is gonna win a game or not.

3
00:00:08,040 --> 00:00:10,950
Now this is for football or
as Americans call it soccer,

4
00:00:11,130 --> 00:00:11,970
which is one of the most,

5
00:00:12,120 --> 00:00:15,750
which is the most popular game
globally when it comes to sports.

6
00:00:15,990 --> 00:00:18,480
And of all the domestic teams out there,

7
00:00:18,540 --> 00:00:22,110
the English premier league is
the most popular of all of them.

8
00:00:22,140 --> 00:00:26,580
So we're going to predict the outcome for
an English premier league team using a

9
00:00:26,581 --> 00:00:30,420
Dataset of past games. And this
Dataset I'll show it to you right now,

10
00:00:30,630 --> 00:00:32,850
has a bunch of different statistics.

11
00:00:33,060 --> 00:00:34,920
This is what the data set
looks like right here.

12
00:00:35,040 --> 00:00:38,160
You've got a home team and you've
got an away team right here.

13
00:00:38,161 --> 00:00:41,940
So it could be arsenal, it can be
Chelsea, Brighton, Manchester City.

14
00:00:42,060 --> 00:00:44,460
So you've got a home team
and you've got an away team.

15
00:00:44,700 --> 00:00:47,990
And then you've got a bunch of
statistics. So these are all acronyms. Uh,

16
00:00:48,090 --> 00:00:52,740
but I have definitions for all of these
acronyms that we can look at right over

17
00:00:52,800 --> 00:00:57,690
here, right? So we have acronyms for the,

18
00:00:57,850 --> 00:01:02,760
uh, full time home team
goals, the home team, the away
team, the shots, the target,

19
00:01:02,910 --> 00:01:05,880
the coroner's, the amount of yellow
cars, the amount of red cards.

20
00:01:06,000 --> 00:01:08,070
So there's a lot of different
statistics here, right?

21
00:01:08,220 --> 00:01:12,360
There's so many things that go into
what makes a team win or lose, right?

22
00:01:12,510 --> 00:01:16,530
And so we're going to take all of these
features and then we're going to use

23
00:01:16,531 --> 00:01:19,020
them to try to predict
the target or the label.

24
00:01:19,200 --> 00:01:23,670
And the label in our case is
going to be the FTR, which is the,

25
00:01:23,870 --> 00:01:28,710
uh, full time result. So
the FTR is right here,

26
00:01:28,740 --> 00:01:33,450
right? H a H. D, right? So it
could be either the home team, h,

27
00:01:33,540 --> 00:01:36,870
the away team a or a draw d.

28
00:01:36,870 --> 00:01:39,450
So it's a multi-class
classification problem.

29
00:01:39,630 --> 00:01:41,700
This is not a binary
classification problem.

30
00:01:41,701 --> 00:01:44,130
It's not just the home team wins or loses,

31
00:01:44,250 --> 00:01:48,060
it's multi-class cause there are
three possible labels, home team away,

32
00:01:48,061 --> 00:01:49,110
team or draw.

33
00:01:49,290 --> 00:01:52,080
So that's what we're going to try to
predict given all of those features in the

34
00:01:52,100 --> 00:01:56,550
Dataset. Before I show you the steps,
let me just demo this really quickly.

35
00:01:56,551 --> 00:02:00,720
So I can just say x test and then
just take the first uh, row from this.

36
00:02:00,721 --> 00:02:01,770
And the labels are gone.

37
00:02:01,771 --> 00:02:05,760
This is just for the all the features
given no label and we could see that it

38
00:02:05,761 --> 00:02:10,140
says home, right? So it's able to predict,
given all of those other features,

39
00:02:10,290 --> 00:02:14,490
whether or not a team is going to
win, lose or, or tie the game. Okay,

40
00:02:14,491 --> 00:02:15,540
so backup to this.

41
00:02:16,590 --> 00:02:19,470
We're going to try to predict a winning
football team and our steps are going to

42
00:02:19,471 --> 00:02:21,300
be a,
it's a four step process.

43
00:02:21,330 --> 00:02:24,570
So our steps are going to be to first
clean our data set and made sure that we

44
00:02:24,600 --> 00:02:27,150
only use features that we need.
What do I mean by that?

45
00:02:27,390 --> 00:02:29,550
When it comes to predicting
who's going to win a team?

46
00:02:29,700 --> 00:02:32,100
There's an entire industry
around this right there.

47
00:02:32,101 --> 00:02:36,870
Our pregame analyses by commentators
are postgame analyses by commentators

48
00:02:37,050 --> 00:02:41,340
entire channels like ESPN or dedicated
to trying to predict who's going to win a

49
00:02:41,341 --> 00:02:43,500
match. And in fact, even during the game,

50
00:02:43,501 --> 00:02:46,710
there are commentators trying to predict
who's going to win like during halftime,

51
00:02:46,711 --> 00:02:48,600
who's going to win the full game.
So this is a,

52
00:02:48,601 --> 00:02:51,310
this is something that's been
going on for forever, right?

53
00:02:51,311 --> 00:02:55,140
Since gladiator gladiator Roman days
or whatever it's been going on, right?

54
00:02:55,141 --> 00:02:57,000
People are trying to predict
who's going to win a match,

55
00:02:58,050 --> 00:03:01,330
but we're going to do something that
people don't do often and that is using

56
00:03:01,331 --> 00:03:04,900
statistical analysis or otherwise
known as machine learning,

57
00:03:04,901 --> 00:03:08,680
mathematical optimization to try
to predict who's going to win.

58
00:03:08,770 --> 00:03:09,491
If you think about it,

59
00:03:09,491 --> 00:03:12,700
this is like one of the most perfect
machine learning problems out there,

60
00:03:12,790 --> 00:03:14,380
trying to predict who's going to win.

61
00:03:14,381 --> 00:03:17,920
Think of all the features out there and
those features don't necessarily have to

62
00:03:17,921 --> 00:03:20,620
do with the game. They could be
the sentiment of the audience,

63
00:03:20,621 --> 00:03:24,880
the sentiment of the crowd
of news articles. How are
people talking about a team?

64
00:03:25,090 --> 00:03:29,230
What hashtags related to the team or
trending on Twitter? Are they home?

65
00:03:29,231 --> 00:03:31,330
Are they away?
What's the weather like that day?

66
00:03:31,390 --> 00:03:32,680
What are the forecast predictions?

67
00:03:32,710 --> 00:03:36,670
So there's so many different data points
that could go into potentially up from

68
00:03:36,671 --> 00:03:41,200
across the web telling us whether or
not a team is going to win or lose.

69
00:03:41,320 --> 00:03:43,630
But since I've never talked
about this topic before,

70
00:03:43,780 --> 00:03:47,650
I'm just going to start off from a very
basic level and based on your feedback

71
00:03:47,651 --> 00:03:48,940
and how you feel about this topic,

72
00:03:49,060 --> 00:03:52,150
I can talk about it more and do
more advanced things later. Okay,

73
00:03:52,151 --> 00:03:53,410
so we're going to clean our Dataset,

74
00:03:53,440 --> 00:03:56,050
then we're going to split it into
a training and a testing set.

75
00:03:57,040 --> 00:04:00,160
And what I mean by that is we're
going to use psychic learn to do that.

76
00:04:00,310 --> 00:04:01,121
I have still,

77
00:04:01,121 --> 00:04:05,170
I have yet to find a better library for
splitting training and testing data.

78
00:04:05,380 --> 00:04:07,600
Then psych it,
learn it is still like the best out there.

79
00:04:07,601 --> 00:04:11,660
Even if I'm using tensorflow or Pi Torch
to build my model, I'll still use psych.

80
00:04:11,661 --> 00:04:14,580
It learn to split my training and
testing data. It's just a one liner,

81
00:04:14,600 --> 00:04:16,570
super simple.
And then once we split it,

82
00:04:16,571 --> 00:04:19,210
we're going to train it on
three different classifiers.

83
00:04:19,211 --> 00:04:21,190
So remember this is a
classification problem,

84
00:04:21,220 --> 00:04:23,740
a multi-class classification problem.

85
00:04:23,950 --> 00:04:26,080
And so we're going to use
either at logistic regression,

86
00:04:26,260 --> 00:04:28,630
a support vector machine or,

87
00:04:28,631 --> 00:04:31,270
and I've talked about both of those
in my math of intelligence series,

88
00:04:31,390 --> 00:04:33,910
links to those in the description.
Uh,

89
00:04:33,970 --> 00:04:37,060
but I'll also talk about them a little
bit in this video just as a refresher.

90
00:04:37,210 --> 00:04:40,720
And the third one is a model that I
haven't talked about before and that's

91
00:04:40,721 --> 00:04:44,380
called xg boost. Well, you could think
of it as a technique model, same thing.

92
00:04:44,590 --> 00:04:46,630
So we're gonna use those
three as our classifiers.

93
00:04:46,631 --> 00:04:50,020
We're going to train all three of them
on the Dataset and then we're going to

94
00:04:50,021 --> 00:04:52,630
pick the classifier that
has the best result.

95
00:04:52,750 --> 00:04:56,110
And that is going to be the
classifier that we use to predict the,

96
00:04:56,230 --> 00:04:59,200
the winning team. And we're also
going to optimize its parameter.

97
00:04:59,360 --> 00:05:02,530
It's hyper parameters using grid search,
right?

98
00:05:02,531 --> 00:05:04,750
So we're using an ensemble
of machine learning methods,

99
00:05:04,751 --> 00:05:08,020
which cycle learn makes it very easy
to do. Once we pick the right one,

100
00:05:08,021 --> 00:05:12,370
then we'll optimize that model and then
that will take that optimized model and

101
00:05:12,371 --> 00:05:16,750
use that to predict the winning team. And
so the history of this is, like I said,

102
00:05:16,751 --> 00:05:20,350
it's been going on for a long time and
sports betting has just been increasing

103
00:05:20,351 --> 00:05:23,800
in popularity for many years, right?
If you look at the past five years,

104
00:05:23,801 --> 00:05:27,010
it's growing at double digit rates
and there's a lot of reasons for this.

105
00:05:27,011 --> 00:05:30,160
Number one is just the accessibility
of the Internet's rights.

106
00:05:30,370 --> 00:05:34,720
More people have Internet access and
and betting on the Internet it's easier

107
00:05:34,721 --> 00:05:35,820
than in person.
Uh,

108
00:05:35,821 --> 00:05:39,280
another reason is just that machine
learning is becoming democratized and so

109
00:05:39,281 --> 00:05:42,160
everybody's being able to build
these predictive models, uh,

110
00:05:42,400 --> 00:05:44,980
to try to predict these scores.
So this is,

111
00:05:44,981 --> 00:05:47,770
this is definitely a field that's
increasing in popularity and,

112
00:05:48,850 --> 00:05:51,550
and this is not something that's
happening in the fringe of society.

113
00:05:51,700 --> 00:05:54,310
This is a very mainstream task.
Kaggle,

114
00:05:54,311 --> 00:05:57,250
the data science community
hosts this yearly competition,

115
00:05:57,530 --> 00:06:00,920
March madness or machine learning mania,
whatever you want to call it,

116
00:06:01,070 --> 00:06:03,590
to try to predict the scores for the NCAA.

117
00:06:03,590 --> 00:06:07,190
That is basketball and you have an entire
community around this and people are

118
00:06:07,191 --> 00:06:09,320
trying out different
models and discussing them.

119
00:06:09,321 --> 00:06:12,950
So definitely check that link out as well.
So this is something that's happening.

120
00:06:12,951 --> 00:06:16,700
And I also found several
papers talking about this.

121
00:06:16,730 --> 00:06:20,000
So it's not just something that
people who want to make money do.

122
00:06:20,001 --> 00:06:24,920
This is something that
legitimate researchers at
academic institutions look into

123
00:06:24,921 --> 00:06:28,160
and try to try to predict.
Right? So from this paper I,

124
00:06:28,330 --> 00:06:29,960
I'm quoting verbatim,

125
00:06:29,961 --> 00:06:34,400
it is possible to predict a winner of
English county 2020 cricket games and

126
00:06:34,401 --> 00:06:38,300
almost two thirds of instances, right?
And then for this other paper right here,

127
00:06:38,570 --> 00:06:42,020
something that becomes clear from the
results is that Twitter contains enough

128
00:06:42,021 --> 00:06:46,460
information to be useful for
predicting outcomes for the Premier Li.

129
00:06:46,730 --> 00:06:49,100
That's for Twitter, right? Right here.

130
00:06:49,101 --> 00:06:52,460
So they use Twitter sentiment to try
to predict just Twitter alone to try to

131
00:06:52,461 --> 00:06:53,750
predict who's going to win.

132
00:06:53,870 --> 00:06:56,570
So there's a lot of different
angles we can look at here, right?

133
00:06:56,660 --> 00:07:00,230
We could use sentiment analysis,
we could use the past score history,

134
00:07:00,350 --> 00:07:02,480
we could use a whole
bunch of different things.

135
00:07:02,600 --> 00:07:03,860
We're going to use a score history,

136
00:07:03,861 --> 00:07:07,400
but you could try to simulate the game
and a simulation and then, you know,

137
00:07:07,401 --> 00:07:09,050
try to see from that.

138
00:07:09,190 --> 00:07:11,750
But you know that there's a lot
of different possibilities here.

139
00:07:11,930 --> 00:07:16,930
And check this out in 2014 being which
is owned by Microsoft correctly predicted

140
00:07:17,721 --> 00:07:22,721
the outcomes of for all the 15 games in
the knockout round for the 2014 World

141
00:07:23,420 --> 00:07:28,160
Cup, every single game, 15
of them will 100% accuracy.

142
00:07:28,400 --> 00:07:32,060
So you can be sure that beings
model is really good. However,

143
00:07:32,061 --> 00:07:35,960
they are not going to share it with us
because it's, it's of like, you know,

144
00:07:36,060 --> 00:07:38,540
a financial analyst at JP Morgan or chase.

145
00:07:38,720 --> 00:07:40,760
If they know how to
predict these stock prices,

146
00:07:40,970 --> 00:07:43,850
they're not going to tell us why would
they share their profits with us.

147
00:07:43,910 --> 00:07:47,210
So what we've got to do is we've got to
figure it out for ourselves to try to

148
00:07:47,211 --> 00:07:50,750
reverse engineer the techniques so
that we can benefit from it. Okay,

149
00:07:50,751 --> 00:07:54,080
so that was a little primer on the
background. So back to the Dataset.

150
00:07:54,840 --> 00:07:58,610
So this data set that I got is
from football data.co. Dot Uk.

151
00:07:58,611 --> 00:08:02,660
You can find it right here if
you go to slash data dot PHP.

152
00:08:02,900 --> 00:08:06,170
And then what I did was I selected
the England football results.

153
00:08:06,350 --> 00:08:11,300
And Luckily for us, they've got data sets
for every season back like two decades.

154
00:08:11,510 --> 00:08:13,100
So it's perfect.
And if you want one,

155
00:08:13,101 --> 00:08:16,160
you can just click on premiere league
and boom, it downloads just like that.

156
00:08:16,580 --> 00:08:17,720
And I showed you the data set.

157
00:08:17,721 --> 00:08:21,590
So one thing right off the bat that we
can notice is that if we were to just

158
00:08:21,620 --> 00:08:22,161
graph,

159
00:08:22,161 --> 00:08:25,340
and I've already done this beforehand
for us and it's in mark down right here,

160
00:08:25,580 --> 00:08:29,510
we'll see that the home team has
the majority stake of this graph.

161
00:08:29,660 --> 00:08:32,870
So that means right off the bat,
without doing any machine learning,

162
00:08:33,020 --> 00:08:37,190
we already know that if you are a home
team, you have an advantage to win.

163
00:08:37,340 --> 00:08:39,980
Probabilistically speaking,
if you're the home team,

164
00:08:40,070 --> 00:08:44,330
you're more likely to win than if
you're not just from v from that alone.

165
00:08:44,720 --> 00:08:47,300
And we can reason about this a
couple ways. We could say, well,

166
00:08:47,301 --> 00:08:48,200
if you're the home team,

167
00:08:48,201 --> 00:08:51,980
then you know football's a team sport
and the cheering crowd helps you and you

168
00:08:51,981 --> 00:08:54,860
don't need to travel for, you're
less fatigued. Uh, you know,

169
00:08:54,861 --> 00:08:58,110
you're familiar with the pitch and the
weather conditions, all these things.

170
00:08:58,111 --> 00:09:01,920
You had a hot dog from the standard and
it tastes really good. Just kidding. Uh,

171
00:09:01,950 --> 00:09:05,760
baseball, food or any kind of sports
or like stadium food is never good.

172
00:09:05,761 --> 00:09:08,190
You know what I'm saying?
I've got two great repositories for us.

173
00:09:08,191 --> 00:09:12,060
I'm about to start the code here, but
I've got one for another EPL prediction,

174
00:09:12,061 --> 00:09:15,810
great. I python notebook or Jupiter
Notebook and I've got one for that.

175
00:09:15,840 --> 00:09:19,830
That Kaggle competition that I just
talked about for NCAA prediction.

176
00:09:19,980 --> 00:09:21,870
Definitely check them both out.
And this guy,

177
00:09:21,871 --> 00:09:26,770
a dish pundit has really great
tutorials and software on his get hub.

178
00:09:26,771 --> 00:09:28,380
So just check out all of his,
uh,

179
00:09:28,530 --> 00:09:30,900
repositories cause he has some
really great example code.

180
00:09:31,200 --> 00:09:36,120
So what we're gonna do is I'm just going
to code out a good part of this just

181
00:09:36,121 --> 00:09:40,710
from the start and then we're
going to just go over the
rest. Okay. So don't save.

182
00:09:41,230 --> 00:09:43,830
All right, move, move,
move, move, move. Okay,

183
00:09:45,270 --> 00:09:48,420
so first things first.

184
00:09:48,450 --> 00:09:52,650
So our dependencies in this case are
going to be to import pandas for data

185
00:09:52,651 --> 00:09:53,700
preprocessing.

186
00:09:53,970 --> 00:09:58,170
We want to import pandas because that's
like the most popular data processing

187
00:09:58,171 --> 00:10:01,080
library. And we also talked
about xg boost, right?

188
00:10:01,081 --> 00:10:05,070
That is one of the other machine learning
models that we want to use a which is

189
00:10:05,071 --> 00:10:09,720
going to form a prediction model based
on an ensemble of decision trees,

190
00:10:09,900 --> 00:10:12,440
which I've talked about as
well as decision trees. Uh,

191
00:10:12,510 --> 00:10:15,660
and so another thing we're gonna do
is we're going to import a logistic

192
00:10:15,661 --> 00:10:18,060
regression, right? That's
model to have three.

193
00:10:18,061 --> 00:10:21,430
There are three different models
that we're going to train a,

194
00:10:21,450 --> 00:10:25,740
our data set on.
One of them is xg boost,

195
00:10:25,770 --> 00:10:27,450
the other is logistic regression,

196
00:10:27,480 --> 00:10:30,450
which is used whenever the response
variable is categorical, right?

197
00:10:30,451 --> 00:10:32,340
Either yes or no or you know,

198
00:10:32,341 --> 00:10:37,080
some kind of a non continuous
discrete value, you know, black,

199
00:10:37,081 --> 00:10:40,650
white, red, green, you know, things like
that. So which is perfect for us home,

200
00:10:40,651 --> 00:10:42,930
you know, Ho a win, lose or draw.

201
00:10:43,200 --> 00:10:46,020
So we have a logistic regression
and then we have one more,

202
00:10:46,021 --> 00:10:48,990
which is going to be the
support vector machine,

203
00:10:50,890 --> 00:10:53,250
right? It support vector machines.
I'll talk about that as well.

204
00:10:53,610 --> 00:10:56,820
And then finally we're going
to want to import this display,

205
00:10:57,740 --> 00:11:02,250
this display library, because we are
going to display our results. Okay.

206
00:11:02,251 --> 00:11:06,870
So that's it for our dependencies.
And now we can read our data set.

207
00:11:06,871 --> 00:11:10,710
So now we're going to go ahead and look
at pandas and paint is gonna is gonna.

208
00:11:10,711 --> 00:11:15,300
Let us read from our CSV file that we
downloaded that I've called final dataset

209
00:11:15,390 --> 00:11:19,740
dot CSV. And then once we have that,
we're going to preview that data.

210
00:11:19,741 --> 00:11:20,461
So I'm going to say,
okay,

211
00:11:20,461 --> 00:11:25,440
just go ahead and display the data that
I've just pulled into memory as a pandas

212
00:11:25,441 --> 00:11:26,850
data frame object.

213
00:11:27,150 --> 00:11:30,690
I'll look at its head that is just
the first few columns of that Dataset.

214
00:11:31,200 --> 00:11:35,220
And once I have that, oh,
can I can go ahead and print
it. And now we can see this,

215
00:11:35,221 --> 00:11:36,600
this data set,
what it looks like.

216
00:11:36,750 --> 00:11:39,210
And so notice there's a
whole bunch of acronyms here.

217
00:11:39,450 --> 00:11:42,450
Lots of datasets have acronyms like
this and they can be confusing.

218
00:11:42,450 --> 00:11:46,650
But like I said, I've got this, uh,
legend of what each acronym means. Uh,

219
00:11:46,900 --> 00:11:48,480
the home team gold difference,

220
00:11:48,690 --> 00:11:52,500
the difference in points of difference
in last year's prediction for the past

221
00:11:52,501 --> 00:11:55,450
three games, the winds for the past
three games, for the home team,

222
00:11:55,600 --> 00:11:58,930
the number of wins for the past three
games for the away team. So, you know,

223
00:11:58,931 --> 00:12:02,830
I've kind of aggregated this data and
I've just made it into something a little

224
00:12:02,831 --> 00:12:05,470
more consumable.

225
00:12:05,590 --> 00:12:09,520
And so still remember that we still have
one single target that we're trying to

226
00:12:09,521 --> 00:12:11,620
predict. And that is FTR, right?

227
00:12:12,280 --> 00:12:14,800
The fulltime result
for the full time game.

228
00:12:14,920 --> 00:12:18,910
Who is the team that won the home team,
the away team or was it a draw?

229
00:12:19,180 --> 00:12:21,370
And so that's our target
that we're trying to predict.

230
00:12:21,550 --> 00:12:26,230
So before we get into building this model,
let's first explore this data set.

231
00:12:27,730 --> 00:12:32,380
So if we were to explore this Dataset,
we could say, okay, so w first of all,

232
00:12:32,410 --> 00:12:35,440
let's just kind of think about what
is the win rates for the home team.

233
00:12:35,530 --> 00:12:38,740
So what is the win
rates for the home team?

234
00:12:39,160 --> 00:12:42,970
So how often does the home team win?
Aside from anything else?

235
00:12:43,030 --> 00:12:44,710
This is kind of what we
just talked about right?

236
00:12:44,830 --> 00:12:47,230
How do we do this programmatically?
What we say, okay,

237
00:12:47,360 --> 00:12:51,610
get the total number of matches and
that's going to be that first index in the

238
00:12:51,940 --> 00:12:56,290
data frame object and then calculate
the number of features from it.

239
00:12:56,380 --> 00:13:00,130
So we want the number of features and
we'll subtract one because one of them is

240
00:13:00,131 --> 00:13:02,980
going to be the label. That's not going
to be our feature, right? The FTR.

241
00:13:03,160 --> 00:13:04,690
So we'll subtract one from that.

242
00:13:04,870 --> 00:13:09,870
And then we're going to calculate
the the matches one by the home team,

243
00:13:11,050 --> 00:13:14,980
which is going to be the
length of the data. All right,

244
00:13:15,370 --> 00:13:19,570
Tom Dot ft FTR.
Okay.

245
00:13:19,630 --> 00:13:22,300
As for that for the home team.
So that's number of matches that are,

246
00:13:22,330 --> 00:13:26,620
that were won by the home team.
And finally we'll calculate the win rate,

247
00:13:26,950 --> 00:13:30,520
the win rate for the home team as well.

248
00:13:33,370 --> 00:13:34,510
And then once we have that,

249
00:13:34,511 --> 00:13:38,770
finally we can print out the results and
it's going to tell us exactly how many

250
00:13:38,771 --> 00:13:43,771
times the a home team has won
as a percentage of all the wins.

251
00:13:44,380 --> 00:13:46,270
So I can go ahead and print that.

252
00:13:46,420 --> 00:13:50,860
I've got this print statement right here
and then we can go ahead and see the

253
00:13:50,861 --> 00:13:53,990
result. Okay, so already this is the,

254
00:13:54,000 --> 00:13:55,540
this is the graph that I
showed at the beginning.

255
00:13:55,541 --> 00:14:00,541
46% about 46% of wins are from the team
that is home just right off the bat.

256
00:14:01,030 --> 00:14:03,820
Just something for us to know, right?
We're, we're exploring the data,

257
00:14:03,821 --> 00:14:07,450
we're trying to think about what are the
features that matter the most, right?

258
00:14:07,600 --> 00:14:08,530
Feature selection.

259
00:14:08,531 --> 00:14:11,230
That's the process that we're
going to now we're going through.

260
00:14:11,320 --> 00:14:13,450
So I remember when it
comes to deep learning,

261
00:14:13,660 --> 00:14:16,510
we don't have to really think
about what are the ideal features.

262
00:14:16,720 --> 00:14:20,740
Deep learning learns those features for
us. However, that's like a next step.

263
00:14:20,741 --> 00:14:24,520
We were just going to try to build some
more basic models first and then you

264
00:14:24,521 --> 00:14:27,790
know, whether or not, you know, based on
feedback of how you guys liked this topic,

265
00:14:27,940 --> 00:14:30,820
I might do a deep learning
video on sports analytics later,

266
00:14:31,000 --> 00:14:34,630
but right now we're just going to build
these three simple models and thinking

267
00:14:34,631 --> 00:14:36,170
about feature selection,
uh,

268
00:14:36,260 --> 00:14:40,720
is a really important skill to have
as a data scientist. So if right,

269
00:14:40,750 --> 00:14:43,240
which deep learning, you don't
have to do that. But again,

270
00:14:43,241 --> 00:14:47,080
you've got to have a lot of
GPS and crucially youth to
have a lot of data, right?

271
00:14:47,120 --> 00:14:49,750
You have to have a lot of data to be
able to do that. Now in this case,

272
00:14:49,751 --> 00:14:53,840
we don't have that much data. We in this
dataset set downloaded in like, you know,

273
00:14:53,870 --> 00:14:58,240
two seconds of course,
uh, it, it was only 500.

274
00:14:58,270 --> 00:15:02,000
It's only about 500 data points,
right? We want huge amounts of data,

275
00:15:02,001 --> 00:15:03,380
at least a hundred thousand.

276
00:15:03,381 --> 00:15:05,780
Now if we had at least a
hundred thousand data points,

277
00:15:05,960 --> 00:15:08,810
then this would be something to
use deep learning for it, right?

278
00:15:08,840 --> 00:15:11,270
If we're trying to aggregate
a bunch of different results,

279
00:15:11,300 --> 00:15:15,380
sentiment from Twitter, uh, past
team scores, a different, you know,

280
00:15:15,381 --> 00:15:18,770
talking points from other people, then we
would use something like deep learning.

281
00:15:19,670 --> 00:15:23,510
But in this case, we want to try to
visualize a distribution of this data.

282
00:15:23,600 --> 00:15:24,860
So what we'll do is we'll say,
okay,

283
00:15:24,861 --> 00:15:29,000
so from pandas does this great tool
that lets us compute what's called the

284
00:15:29,060 --> 00:15:31,790
scatter matrix.
And the scatter matrix

285
00:15:34,580 --> 00:15:37,580
basically shows how much one
variable affects the other.

286
00:15:37,700 --> 00:15:42,080
So we're going to scatter matrix for a
set of our features to try to predict,

287
00:15:42,110 --> 00:15:46,490
to try to see just visually what is
the correlation between these different

288
00:15:46,491 --> 00:15:49,370
features and see just for ourselves this,
this,

289
00:15:49,400 --> 00:15:52,970
this will help us pick the relevant
features that we want to use. Right?

290
00:15:52,971 --> 00:15:57,140
So we have the home team gold difference,
we have the away team gold difference,

291
00:15:57,141 --> 00:15:58,190
we have the home team points,

292
00:15:58,191 --> 00:16:01,760
the away team points that difference in
points and then the difference in last

293
00:16:01,760 --> 00:16:05,060
year's prediction. Okay. And
so once we visualize this,

294
00:16:05,090 --> 00:16:07,730
some of them have a positive correlation,
the line is going up,

295
00:16:07,820 --> 00:16:11,240
some of them have a negative correlation.
So that means like in terms of,

296
00:16:11,330 --> 00:16:13,970
so that means if the goals
increase for the home team,

297
00:16:14,060 --> 00:16:16,820
then maybe the points decrease
for the for the away team, right?

298
00:16:17,080 --> 00:16:20,000
And so that we can look at the
positive versus negative correlations.

299
00:16:20,150 --> 00:16:23,630
That's an indicator of how features
are related together. Right?

300
00:16:23,631 --> 00:16:26,360
This isn't have some direct
relation to what we're about to do,

301
00:16:26,510 --> 00:16:30,050
but it just good practice to think
about ways of visualizing our data,

302
00:16:30,110 --> 00:16:31,460
seeing the relationship between,

303
00:16:31,490 --> 00:16:35,210
between different features and then
tried to predict what those best features

304
00:16:35,230 --> 00:16:39,020
are for our model. Okay. So then
once we've explored our data,

305
00:16:39,170 --> 00:16:42,860
we're going to prepare it. So remember
we have one single target variable,

306
00:16:42,861 --> 00:16:47,420
one single objective or label as we
like to call it. And that is the FTR,

307
00:16:47,540 --> 00:16:50,060
the fulltime result.
So what we wanna do is say,

308
00:16:50,061 --> 00:16:54,530
given all of those other features,
try to predict the FTR. Okay.

309
00:16:54,590 --> 00:16:58,010
And make us some money. Yeah, no,
I'm just kidding. I mean, yes,

310
00:16:58,040 --> 00:16:59,570
actually you probably
want to make some money.

311
00:16:59,810 --> 00:17:01,760
We're trying to predict the
full time results, right?

312
00:17:01,880 --> 00:17:05,300
And so we're going to split it into
the FTR and then everything else,

313
00:17:05,510 --> 00:17:09,250
then we'll standardize it,
which means it's all going
to be on the same scale. We,

314
00:17:09,260 --> 00:17:13,580
that means we want all of our data to be
an integer format and we want it all to

315
00:17:13,581 --> 00:17:14,720
be on the same scale.

316
00:17:14,750 --> 00:17:17,900
So it's not like we have like one
feature is in the hundreds of thousands.

317
00:17:17,990 --> 00:17:21,170
And then the other feature is in the,
you know, between one and 10. If we're,

318
00:17:21,210 --> 00:17:24,890
if the, if they're going
to be small values, we want
them all to be small values.

319
00:17:25,040 --> 00:17:28,250
And what this does is it improves our
prediction capability of our model.

320
00:17:28,580 --> 00:17:30,320
So once we've standardized our data,

321
00:17:30,470 --> 00:17:33,350
then we're going to add these
three features, which is the,

322
00:17:33,440 --> 00:17:37,940
the last three wins for both sides. And
we looked at that before, right? H M one,

323
00:17:37,941 --> 00:17:40,550
two, three and then a, an
an a m one, two and three.

324
00:17:41,600 --> 00:17:45,320
So if we look back at the data,
some of the data was categorical.

325
00:17:45,321 --> 00:17:49,040
Like if we look at this Dataset, you
know, you have the referee, we have HTR,

326
00:17:49,170 --> 00:17:52,630
we don't want any of that, right?
We want all of our data to be, uh,

327
00:17:52,680 --> 00:17:56,460
either a number. We want it to be some
continuous variable, no discreet numbers.

328
00:17:56,670 --> 00:18:00,450
So we're going to preprocess
those features by saying,
create a new data frame.

329
00:18:00,720 --> 00:18:03,810
Find those feature columns that
are categorical by saying, if it's,

330
00:18:03,811 --> 00:18:05,730
if the data type is equal to equal,

331
00:18:05,731 --> 00:18:10,350
equal to object instead of an integer and
then convert it into an Integer, right?

332
00:18:10,440 --> 00:18:13,110
So that way we remove all
the categorical features.

333
00:18:13,260 --> 00:18:17,340
We only have one categorical variable
and that is are labeled the FTR.

334
00:18:17,490 --> 00:18:19,500
We don't want our features
to be categorical.

335
00:18:19,501 --> 00:18:22,860
Those are going to be continuous
variables. And so once we have that,

336
00:18:23,040 --> 00:18:25,170
we've preprocessed our data,
we've explored it,

337
00:18:25,260 --> 00:18:29,040
we've added the features that we thought
were most relevant and we could see

338
00:18:29,041 --> 00:18:32,970
them all here, right? No more categorical
features, they're all numbers.

339
00:18:33,150 --> 00:18:34,260
And so once we have that,

340
00:18:34,320 --> 00:18:39,320
now we can train and we can split our
model into a training and a testing

341
00:18:40,230 --> 00:18:43,740
dataset. If within very easy one
liner would psych it learn, right?

342
00:18:43,880 --> 00:18:47,850
And this is going to split our,
with the train train test split function,

343
00:18:48,000 --> 00:18:49,410
it's going to split that CSV.

344
00:18:49,440 --> 00:18:53,820
It's going to split that data frame object
into a training and a testing set and

345
00:18:53,821 --> 00:18:57,060
it already knows what the label is going
to be and it's going to put them all in

346
00:18:57,061 --> 00:18:59,040
a one dimensional array.
All of those labels,

347
00:18:59,041 --> 00:19:03,750
the FTR scores for each of the associated
inputs. And we have 12 features,

348
00:19:03,751 --> 00:19:08,340
right? We have 12 features for a
single input. And so for the next step,

349
00:19:08,430 --> 00:19:10,350
now we're going to
actually build this model.

350
00:19:10,410 --> 00:19:13,290
So I'm going to come back to these helper
functions that are going to help us

351
00:19:13,291 --> 00:19:16,860
train the model. But let's right
now just build this model, right?

352
00:19:16,861 --> 00:19:20,100
So I'll go down here. So let's just
write this out right now. Okay,

353
00:19:20,101 --> 00:19:21,420
so I'm going to say,
okay,

354
00:19:21,600 --> 00:19:24,690
so we know that the first
model that we want to try out,

355
00:19:24,750 --> 00:19:28,800
or at least one of the models that we
want to try out is logistic regression.

356
00:19:29,070 --> 00:19:32,100
I'll give it some random state
as a seed that, you know,

357
00:19:32,280 --> 00:19:34,810
this could be any number of things, right?
Well, I'm just going to say, you know,

358
00:19:34,830 --> 00:19:38,220
40 I could say 42 it doesn't matter,
but just some seed number.

359
00:19:38,221 --> 00:19:41,310
And we could try out different
seeds to see how the results vary.

360
00:19:41,311 --> 00:19:44,570
But I'm just going to, you know, put
some magic numbers down right now to, to,

361
00:19:44,640 --> 00:19:46,230
to get some results out.

362
00:19:46,530 --> 00:19:49,570
And so the next class if I were going
to build is a support vector machine.

363
00:19:49,571 --> 00:19:53,820
So the order of classifiers as I
initialize them, it doesn't really matter.

364
00:19:54,120 --> 00:19:56,040
So,
so that's irrelevant.

365
00:19:56,160 --> 00:20:00,240
But the fact that I am a initializing
them is important because it means that

366
00:20:00,241 --> 00:20:02,580
these are the three important
ones that we are using.

367
00:20:03,990 --> 00:20:07,290
And so my third classifier
is going to be xg boost.

368
00:20:07,320 --> 00:20:10,050
Now I'm going to talk about what
all of these are in a second,

369
00:20:10,051 --> 00:20:12,090
but let me just write them out here.

370
00:20:12,300 --> 00:20:16,290
We have an xg boost classifier
and then we have a seed.

371
00:20:16,291 --> 00:20:20,220
It's going to be maybe too,
let me print that. Boom,

372
00:20:21,000 --> 00:20:25,740
boom for a, B, and C. Okay.

373
00:20:25,980 --> 00:20:29,850
Right? And so if we train this,
we'll see that clearly. Uh,

374
00:20:29,851 --> 00:20:32,190
the xg boost library did the best.

375
00:20:32,280 --> 00:20:36,690
So we already know that xg boost
is the best model for this data.

376
00:20:36,780 --> 00:20:40,050
And notice that the xg boost
model had an accuracy score.

377
00:20:40,051 --> 00:20:45,051
And an f one score of about 74% that is
a 74% accuracy on the testing data sets,

378
00:20:46,560 --> 00:20:50,530
which is a really good, it's really good.
It's better than just guessing, right?

379
00:20:50,650 --> 00:20:53,590
It's way better than just trying
to guess what team is going to win.

380
00:20:53,740 --> 00:20:56,950
That's about a 75% accuracy
is pretty good. So let's,

381
00:20:56,951 --> 00:21:00,640
let's go back and see what these models
are, by the way. So these models,

382
00:21:00,880 --> 00:21:01,930
so logistic regression.

383
00:21:01,931 --> 00:21:06,931
Now remember I have a video on logistic
regression and I have a video on support

384
00:21:08,201 --> 00:21:11,530
vector machines. Just search both of
those on youtube. And then the word Saroj,

385
00:21:11,620 --> 00:21:15,010
it'll be the first link that shows up.
But for logistic regression,

386
00:21:15,130 --> 00:21:19,360
it's used to four to predict
the probability of an
occurrence of an event by

387
00:21:19,361 --> 00:21:20,950
fitting data to logistic curve.

388
00:21:21,100 --> 00:21:25,570
So a logistic curve consists
of this equation right here,
that probability, right?

389
00:21:25,690 --> 00:21:28,960
So if you have
two classes,

390
00:21:28,961 --> 00:21:33,310
if it's a binary classification problem,
whether or not someone is dead or alive,

391
00:21:33,490 --> 00:21:37,630
the x axis would be the concentration
of the toxin. Whether you're, you know,

392
00:21:37,631 --> 00:21:40,360
you're trying to predict if someone's
going to live or die based on this toxin

393
00:21:40,540 --> 00:21:44,170
and the y axis is going to be the
probability of each of those classes.

394
00:21:44,380 --> 00:21:48,250
And we use a logistic regression curve
to noted by this equation where you just

395
00:21:48,251 --> 00:21:52,390
plug in the x value and it will
output a probability to show that.

396
00:21:52,540 --> 00:21:56,380
Now in the multi-class case as
is our problem as is our problem,

397
00:21:56,620 --> 00:22:01,620
we're going to use a multinomial logistic
regression which the library does for

398
00:22:01,721 --> 00:22:03,850
us, but that's what the
logistic regression does.

399
00:22:03,851 --> 00:22:07,990
Its use extensively across a wide
range of fields and uh, it's a very,

400
00:22:07,991 --> 00:22:10,240
very popular model.
That's going to be our first,

401
00:22:10,241 --> 00:22:13,450
and these are all classification
model by the way. Remember,

402
00:22:13,451 --> 00:22:17,200
once we frame our problem, then we
can pick what model we want to use.

403
00:22:17,290 --> 00:22:19,990
We know that this is a
classification problem,

404
00:22:20,170 --> 00:22:23,950
therefore we wouldn't use a model that
is well suited for classification.

405
00:22:24,220 --> 00:22:26,920
And then the next question
is based on our data,

406
00:22:26,950 --> 00:22:28,840
which of the models is best to use?

407
00:22:29,110 --> 00:22:30,790
And we don't always know
that right off the bat,

408
00:22:30,791 --> 00:22:33,730
even very experienced data
scientists don't always know that.

409
00:22:33,760 --> 00:22:37,510
So they have to try out several
models to see which one works best.

410
00:22:38,680 --> 00:22:40,270
And then for support vector machines.

411
00:22:40,420 --> 00:22:42,940
So when a support vector
machine does is it will find,

412
00:22:42,941 --> 00:22:46,270
so let's say we have two classes and
we plot them in two dimensional space,

413
00:22:46,390 --> 00:22:47,620
just like in this image right here.

414
00:22:47,830 --> 00:22:51,640
What it will do is it will try to find
the points that are closest to each other

415
00:22:51,670 --> 00:22:54,580
to find the smallest margin
between both classes.

416
00:22:54,790 --> 00:22:58,720
And once it finds these points,
these support vectors,

417
00:22:58,810 --> 00:23:01,660
it will build a hyperplane right
in the middle. So that the,

418
00:23:01,970 --> 00:23:06,490
so the distance between that line and
both of those points is the smallest.

419
00:23:06,700 --> 00:23:09,730
And the reason it does that is so
then once we give it a new Dataset,

420
00:23:09,820 --> 00:23:13,060
a new data points,
whatever side of the line it falls on,

421
00:23:13,240 --> 00:23:15,640
that's the class that's going to be,
that's how we classify it.

422
00:23:15,790 --> 00:23:18,070
So in the simple case for two classes,

423
00:23:18,160 --> 00:23:21,520
it's just a line and then it just falls
and the data will fall on one of two

424
00:23:21,521 --> 00:23:23,650
sides.
But in the more complex case,

425
00:23:23,740 --> 00:23:27,220
you'll have all three classes and then
it will draw a line that's kind of curved

426
00:23:27,221 --> 00:23:27,970
between them.

427
00:23:27,970 --> 00:23:31,420
So it just like segment the graph
into the three different segments.

428
00:23:31,720 --> 00:23:33,070
But the idea is still the same.

429
00:23:33,071 --> 00:23:36,910
Finding those closest points and finding
the line that minimizes the margin.

430
00:23:37,660 --> 00:23:41,080
Okay, so that's for support vector
machines. And the last one is xg boost.

431
00:23:41,110 --> 00:23:44,210
So we talked about random
forests. It's kind of, it's,

432
00:23:44,211 --> 00:23:46,070
it's very similar to a random forest.

433
00:23:46,460 --> 00:23:50,900
The xg boost algorithm is one of the
most popular algorithms on Kaggle when it

434
00:23:50,901 --> 00:23:53,660
comes to winners.
A lot of xg boost is happening,

435
00:23:54,350 --> 00:23:57,140
but basically the classification
and regression tree,

436
00:23:57,141 --> 00:24:01,400
the decision tree that's used for both
classification and regression is a good

437
00:24:01,401 --> 00:24:04,730
model. It's not a great model, but it's
a good model. It's a very simple model,

438
00:24:04,731 --> 00:24:08,090
right? You give it a bunch of features
and it's going to slowly build.

439
00:24:08,240 --> 00:24:10,040
There's a variety of ways of doing this,

440
00:24:10,190 --> 00:24:14,810
but it's going to build a tree where each
a branch or level in the tree equates

441
00:24:14,811 --> 00:24:15,920
to one question,
right?

442
00:24:15,921 --> 00:24:18,470
So if you're trying to predict
whether or not it's going to rain,

443
00:24:18,550 --> 00:24:22,610
it will be like is the, is the, is
the sky cloudy? Yes. No. Yes. Okay.

444
00:24:22,850 --> 00:24:24,920
Did it rain yesterday? Yes. No. Yes. Okay.

445
00:24:24,921 --> 00:24:29,180
Then there's a 75% chance it's going to
rain. So that's a decision tree, right?

446
00:24:29,181 --> 00:24:32,300
So what xg boost does,
it's a gradient boosting technique.

447
00:24:32,450 --> 00:24:35,480
What it will do is it will
create a bunch of weak learners.

448
00:24:35,930 --> 00:24:39,290
Those are decision trees that are okay,
like their,

449
00:24:39,291 --> 00:24:41,330
their predictive
capability isn't that good.

450
00:24:41,480 --> 00:24:45,080
And it will combine the results of all
of them. So it's an ensemble method.

451
00:24:45,340 --> 00:24:49,460
It will take all these trees and then
find a result by using the prediction

452
00:24:49,461 --> 00:24:52,430
capability of all of those trees.
So that's for xg boost.

453
00:24:52,431 --> 00:24:56,120
And we have this tree right here where
we have input, age, gender, occupation,

454
00:24:56,240 --> 00:24:58,730
a bunch of different features.
And we're trying to answer the question,

455
00:24:58,910 --> 00:25:00,530
does this person like computer games,

456
00:25:00,710 --> 00:25:04,580
different trees will specialize in
answering different parts of the question.

457
00:25:04,730 --> 00:25:08,000
Like does this person uses the
computer daily? What's their age,

458
00:25:08,001 --> 00:25:11,090
what's their gender? And then we'll
combine the results from all of them.

459
00:25:11,180 --> 00:25:15,930
So the function of this kid is
the result of these two trees, uh,

460
00:25:16,010 --> 00:25:20,780
predictions combined.
So that's what each of those are.

461
00:25:20,781 --> 00:25:21,920
And then if we go up here,

462
00:25:21,921 --> 00:25:25,790
back to these a helper methods that I
was going to talk about. And by the way,

463
00:25:25,791 --> 00:25:28,640
the f one score is just the
measure of a model's accuracy.

464
00:25:28,670 --> 00:25:32,090
It's a very standard score that just
measures how accurate a model is.

465
00:25:32,360 --> 00:25:34,540
So back to these three helper methods,
uh,

466
00:25:34,610 --> 00:25:39,110
what we did here was we gave
the a train predict function,

467
00:25:39,111 --> 00:25:41,990
our classifier as well as the
training and testing datasets.

468
00:25:42,320 --> 00:25:46,130
So if we go back up here, we'll see
that in the train predict method,

469
00:25:46,250 --> 00:25:48,320
we indicated the classified
that we're going to use.

470
00:25:48,500 --> 00:25:51,770
Then we trained it given the,
uh, training and testing data.

471
00:25:52,040 --> 00:25:54,680
We predicted the results and
then we predicted the labels.

472
00:25:54,860 --> 00:25:58,180
And so this train predict method,
use the train classifier,

473
00:25:58,190 --> 00:26:02,930
my method to start a clock, fit the
model and then a print the results.

474
00:26:03,140 --> 00:26:04,940
And then for the labels
it started a clock,

475
00:26:04,941 --> 00:26:07,580
made a prediction and then stop the clock.
So that was it.

476
00:26:07,610 --> 00:26:11,000
They just predicted the labels and
then it fit the classifier. Just that.

477
00:26:11,480 --> 00:26:12,860
And so once we did that,

478
00:26:12,890 --> 00:26:17,600
we realized that xg boost gave
us the best result, right?

479
00:26:17,601 --> 00:26:21,170
So xg boost is the model that we
want to use a 74% accuracy. So,

480
00:26:21,240 --> 00:26:25,580
but that's not enough right? Now that
we know that xg boost is the best model,

481
00:26:25,820 --> 00:26:27,350
now we can say,
okay,

482
00:26:27,560 --> 00:26:31,460
let's optimize this model and what their
different ways that we can optimize

483
00:26:31,461 --> 00:26:33,140
this model. Right? But in our case,

484
00:26:33,141 --> 00:26:36,800
we're going to optimize it by
optimizing the hyper parameters.

485
00:26:36,801 --> 00:26:38,780
So this is hyper parameter optimization.

486
00:26:38,960 --> 00:26:42,560
There are a bunch of different hyper
parameters that go into xg boost.

487
00:26:42,650 --> 00:26:46,800
So we were kind of shielded from that
because we use the psychic learn library.

488
00:26:47,070 --> 00:26:51,930
But we can use psychic learn ironically
enough to optimize hyper parameters that

489
00:26:51,931 --> 00:26:56,100
we don't even normally see. So if we'd
come down here, we can import grid search,

490
00:26:56,340 --> 00:26:59,760
which is a Buddhist basically brute
forcing that. That's what grid searches.

491
00:26:59,761 --> 00:27:03,540
We're brute forcing all the
possible combinations of
all of the hyper parameters.

492
00:27:03,720 --> 00:27:06,120
We'll create an initial set
of hyper parameters here.

493
00:27:06,420 --> 00:27:08,580
We'll initialize the xg boost classifier,

494
00:27:08,830 --> 00:27:12,780
will make an effort and scoring function
and then perform grid search on that

495
00:27:12,781 --> 00:27:15,060
classifier with the scoring function.

496
00:27:15,150 --> 00:27:18,390
Given the initial parameters
that we just defined up here,

497
00:27:18,570 --> 00:27:23,190
and then it's going to find the ideal
parameters for that model and notice that

498
00:27:23,191 --> 00:27:26,850
the f one score and the
accuracy score increased, right?

499
00:27:26,850 --> 00:27:29,840
So after we optimize the hyper parameters,
the F1 scoring,

500
00:27:29,870 --> 00:27:31,440
the accuracy score increased,

501
00:27:31,560 --> 00:27:36,360
which means that our model is now way
more optimized anyway. Disclaimer,

502
00:27:36,450 --> 00:27:39,240
you know, you could make money using
this, you could lose money using this.

503
00:27:39,690 --> 00:27:42,840
Who knows, right? This is a, this
is a, this is an educated guests.

504
00:27:42,930 --> 00:27:46,240
This is a statistical guests
based on past data sets,

505
00:27:46,500 --> 00:27:50,780
but we can definitely improve this model,
right? We could bring in more data,

506
00:27:50,800 --> 00:27:53,640
more relevant features.
We could bring in sentiment analysis,

507
00:27:53,760 --> 00:27:57,780
we could add other features. But there's
also one more thing that I want to say.

508
00:27:57,840 --> 00:28:01,380
So it's able to predict whether or
not the home team will win, right?

509
00:28:01,590 --> 00:28:05,760
But based on a data point
from this CSV file, however,

510
00:28:05,850 --> 00:28:08,340
we don't always know what
all of these things are.

511
00:28:08,370 --> 00:28:11,760
How are we supposed to predict whether
or not there's going to be, you know,

512
00:28:11,790 --> 00:28:13,890
x number of fouls?
How are we going to,

513
00:28:13,920 --> 00:28:17,220
how are we supposed to know whether or
not there's gonna be x number of offsides?

514
00:28:17,340 --> 00:28:20,550
We don't necessarily know these things
beforehand, these features beforehand.

515
00:28:20,880 --> 00:28:25,880
So the trick is to pick features
that are completely predictable.

516
00:28:26,280 --> 00:28:27,113
What do I mean by that?

517
00:28:27,180 --> 00:28:31,110
That means that what are features that
are going to help predict who's gonna win?

518
00:28:31,350 --> 00:28:33,180
But those features
themselves are predictable.

519
00:28:33,181 --> 00:28:36,330
Like how many players are going
to be on the team? Well, you know,

520
00:28:36,331 --> 00:28:38,670
for a fact they're going to
be five players on the team.

521
00:28:38,940 --> 00:28:42,810
Who are the players going to be? What is
the lineup? When is the game happening?

522
00:28:42,960 --> 00:28:44,640
Where is the court?
Right?

523
00:28:44,641 --> 00:28:48,690
So things that you know for sure and
so it's all of your features are known,

524
00:28:48,691 --> 00:28:49,770
they're all predictable,

525
00:28:49,860 --> 00:28:53,550
then your result will not require
you to guess what those features are,

526
00:28:53,610 --> 00:28:57,390
which is the case in this
very basic example. Another,

527
00:28:57,420 --> 00:29:01,380
another way to improve the model is to
just use way, way, way more quality data.

528
00:29:01,470 --> 00:29:03,930
We could also just predict each
of these features themselves,

529
00:29:03,931 --> 00:29:07,530
like we could try to predict how many
goals are going to occur in a game.

530
00:29:07,840 --> 00:29:11,400
We could try to predict all these things
and then just have probabilistic values

531
00:29:11,401 --> 00:29:14,250
for all of these features and then
try to predict the home team, right?

532
00:29:14,251 --> 00:29:16,140
So there's a lot of machine
learning that could be happening,

533
00:29:16,320 --> 00:29:21,180
but ideally we know for sure what all
of these features are going to be and we

534
00:29:21,181 --> 00:29:23,340
can use them to then
predict the winning team.

535
00:29:23,370 --> 00:29:24,840
So if you're interested in this topic,

536
00:29:24,841 --> 00:29:28,110
definitely check out all the links in
the description and let me know what you

537
00:29:28,111 --> 00:29:29,910
guys thought of this
topic in the comments.

538
00:29:29,970 --> 00:29:32,520
Please subscribe for more
programming videos. And for now,

539
00:29:32,580 --> 00:29:36,360
I've got to play some soccer, I mean
football. So thanks for watching.

