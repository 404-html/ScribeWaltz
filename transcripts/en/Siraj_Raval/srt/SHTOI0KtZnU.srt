1
00:00:00,060 --> 00:00:00,840
Hello world.

2
00:00:00,840 --> 00:00:05,840
It's the Raj and I'm going to show you
how I read research papers and give you

3
00:00:06,151 --> 00:00:10,440
some additional tips on how you
can consume them more efficiently.

4
00:00:10,680 --> 00:00:12,990
Reading research papers is an art,

5
00:00:13,290 --> 00:00:16,860
whether the topic is machine
learning or cryptography,

6
00:00:17,010 --> 00:00:19,680
distributed consensus or networking.

7
00:00:19,830 --> 00:00:24,780
In order to truly have an educated
opinion on a particular topic in computer

8
00:00:24,781 --> 00:00:25,614
science,

9
00:00:25,680 --> 00:00:30,300
you've got to get yourself acquainted
with current research in that subfield.

10
00:00:30,540 --> 00:00:33,690
It's easy to agree with the claim
if it's got enough hype behind it,

11
00:00:33,691 --> 00:00:38,490
but being critical and balanced in
your assessment is a skill that can be

12
00:00:38,491 --> 00:00:42,840
learned. Phd Students are taught
how to do this in Grad school,

13
00:00:42,841 --> 00:00:45,210
but you too can learn how to do this.

14
00:00:45,480 --> 00:00:48,810
It just takes patience
and practice and coffee.

15
00:00:48,930 --> 00:00:51,180
Lots of coffee every single week.

16
00:00:51,210 --> 00:00:56,210
I read between 10 to 20 research papers
in order to keep up with the field and

17
00:00:56,791 --> 00:01:00,750
I've gotten better at it over time
and I don't have any graduate degrees.

18
00:01:00,840 --> 00:01:04,980
I'm just the guy who really loves this
stuff and I teach myself everything using

19
00:01:04,981 --> 00:01:07,770
our new collective university,
the Internet.

20
00:01:07,860 --> 00:01:12,860
One of my favorite resources to find
papers on machine learning is the machine

21
00:01:12,930 --> 00:01:14,020
learning subreddit.

22
00:01:14,490 --> 00:01:18,810
People post papers they find interesting
every day and they've also got this

23
00:01:18,811 --> 00:01:19,980
cool weekly.

24
00:01:19,981 --> 00:01:24,630
What are you reading thread where people
post the papers that interest them the

25
00:01:24,631 --> 00:01:25,770
most currently.

26
00:01:26,010 --> 00:01:31,010
Additionally there is this web app called
Archive sanity.com created by Andre

27
00:01:31,441 --> 00:01:35,580
carpathy which basically goes through
archive and finds the papers that are most

28
00:01:35,581 --> 00:01:38,970
relevant. You can filter them
by what interests you buy,

29
00:01:38,971 --> 00:01:42,840
which ones are most popular or by
the ones that are most cited lately.

30
00:01:43,110 --> 00:01:46,560
Google and deep mind respectively
published their work on their websites for

31
00:01:46,561 --> 00:01:51,030
easy access and there are
of course journals like
nature that you can find some

32
00:01:51,031 --> 00:01:52,770
top papers in easily.

33
00:01:53,370 --> 00:01:57,120
The pace of research is accelerating
and machine learning because of a few

34
00:01:57,121 --> 00:02:02,070
reasons not including Schmidt, Huber
in academia and in the public sphere.

35
00:02:02,160 --> 00:02:05,640
The democratization of data computing,
power,

36
00:02:05,790 --> 00:02:09,870
education and algorithms is all
steadily happening over the Internet.

37
00:02:10,140 --> 00:02:14,640
Because of this, more people are able to
make their own insights into this field.

38
00:02:14,970 --> 00:02:15,750
In the industry.

39
00:02:15,750 --> 00:02:20,750
The big tech companies profit more when
their own teams discover new machine

40
00:02:21,001 --> 00:02:24,870
learning methods.
So there's this race to create faster,

41
00:02:24,930 --> 00:02:26,490
more intelligent algorithms.

42
00:02:26,640 --> 00:02:31,020
All that is to say that there are a lot
of papers you could be reading right now,

43
00:02:31,230 --> 00:02:34,290
so how are you supposed to
know what to read? Well,

44
00:02:34,350 --> 00:02:38,430
what I've found is that every week there
are maybe two or three papers that are

45
00:02:38,431 --> 00:02:42,700
getting the most attention in machine
learning and the tools I've mentioned help

46
00:02:42,701 --> 00:02:44,190
me find them and read them,

47
00:02:44,430 --> 00:02:48,180
but most of my reading is a
result of me having a goal.

48
00:02:48,390 --> 00:02:52,260
That goal could be to learn more
about activation functions or perhaps

49
00:02:52,261 --> 00:02:55,320
probabilistic models that
use attention mechanisms.

50
00:02:55,650 --> 00:02:57,360
Once I've got that goal,

51
00:02:57,540 --> 00:03:02,540
it makes it much easier create a reading
strategy that points towards that goal.

52
00:03:03,190 --> 00:03:07,510
Just being a good math heavy machine
learning paper reader is not a goal to

53
00:03:07,511 --> 00:03:11,740
aspire to. Your stamina is more
of a function of human motivation,

54
00:03:11,920 --> 00:03:14,890
which is a function of the goals
you're trying to accomplish.

55
00:03:15,280 --> 00:03:20,280
I found that I can crush through and
understand the most difficult papers much

56
00:03:21,041 --> 00:03:23,590
more when I have a real reason to do so.

57
00:03:24,040 --> 00:03:27,100
So let's take the landmark
paper by a friend of mine,

58
00:03:27,160 --> 00:03:31,870
Ian Goodfellow on generative
adversarial networks as an example.

59
00:03:32,200 --> 00:03:34,000
There is a lot in this paper.

60
00:03:34,001 --> 00:03:39,001
He synthesizes some ideas here that made
young Macoun say that this concept was

61
00:03:39,130 --> 00:03:42,670
the coolest idea in deep
learning in the last 10 years.

62
00:03:43,180 --> 00:03:48,130
The way I read papers is by performing
a three pass approach on the first pass.

63
00:03:48,220 --> 00:03:50,530
I'll just skim through the
paper to get a gist of it,

64
00:03:50,740 --> 00:03:54,640
meaning I'll first read the title if the
title sounds interesting and relevant,

65
00:03:54,850 --> 00:03:59,470
generative adversarial networks. Yo,
let's go. I'll read the abstract,

66
00:03:59,620 --> 00:04:04,480
the abstract acts as a short standalone
summary of the work of the paper that

67
00:04:04,481 --> 00:04:08,440
people can use as an overview.
If the abstract is compelling,

68
00:04:08,500 --> 00:04:13,090
an adversarial process between two
neural networks that resembles a game.

69
00:04:13,260 --> 00:04:17,170
All right, this is lit. Then I'll
skim through the rest of the paper.

70
00:04:17,470 --> 00:04:20,590
By that I mean I'll carefully
read the introduction,

71
00:04:20,890 --> 00:04:25,420
then read the section and subsection
headings, but ignore everything else.

72
00:04:25,600 --> 00:04:29,860
Mainly ignore the math.
I never read the math on the first pass.

73
00:04:30,100 --> 00:04:35,100
Are we the conclusion at the end and
maybe glance over the references mentally

74
00:04:35,231 --> 00:04:38,560
ticking off the ones I've already read.
If there are any,

75
00:04:38,890 --> 00:04:41,350
I just assumed the math is
correct on the first pass.

76
00:04:41,590 --> 00:04:45,910
My goal for this first pass is to just
be able to understand the aims of the

77
00:04:45,911 --> 00:04:48,760
author. What are the paper's
main contributions here?

78
00:04:48,970 --> 00:04:51,040
What problems as the attempt to solve?

79
00:04:51,280 --> 00:04:54,910
Is this a paper I'm actually
interested in reading more of.

80
00:04:55,150 --> 00:04:56,860
Once I've done the first pass,

81
00:04:56,950 --> 00:05:01,360
I'll go back to see what other people are
saying about this paper and compare my

82
00:05:01,361 --> 00:05:03,190
initial observations to.

83
00:05:03,191 --> 00:05:07,930
There's basically the aim of this first
pass is to ensure that it's worth my

84
00:05:07,931 --> 00:05:10,060
time to continue analyzing this paper.

85
00:05:10,180 --> 00:05:15,100
Life is short and there are
too many things to read. If
it does peak my interest,

86
00:05:15,160 --> 00:05:18,190
then I'll reread it a second
time on the second pass.

87
00:05:18,310 --> 00:05:22,660
I'll read it again this time
more critically and I'll
also take notes as I go.

88
00:05:22,930 --> 00:05:26,410
I'll actually read all the English
text and I'll try to get a high level

89
00:05:26,411 --> 00:05:29,050
understanding of the math
that's happening in the paper,

90
00:05:29,290 --> 00:05:34,210
so it's a mini Max game that looks
to optimize a nash equilibrium. Okay.

91
00:05:34,211 --> 00:05:35,170
I kind of get that.

92
00:05:35,440 --> 00:05:40,390
Eventually the generator network creates
fake samples that are indistinguishable

93
00:05:40,450 --> 00:05:44,350
from the real thing, so the
discriminator is powerless. Cool.

94
00:05:44,590 --> 00:05:46,240
I'll read the finger descriptions.

95
00:05:46,241 --> 00:05:50,920
Any plots and graphs that are available
and try to understand the algorithm at a

96
00:05:50,921 --> 00:05:51,730
high level.

97
00:05:51,730 --> 00:05:55,900
A lot of times the author will break
down an equation by factoring it out.

98
00:05:56,230 --> 00:05:59,000
I avoid to analyze this
on the second pass,

99
00:05:59,390 --> 00:06:03,470
I see that it's using a loss function
called the colback Leibler divergence.

100
00:06:03,800 --> 00:06:08,180
Never heard of that one, but I do get the
concept of minimizing a loss function.

101
00:06:08,240 --> 00:06:11,510
When I read the experiments,
I'll try to evaluate the results.

102
00:06:11,600 --> 00:06:14,960
Are they repeatable? Are the
findings well supported by evidence.

103
00:06:15,170 --> 00:06:16,130
Once I'd done that,

104
00:06:16,160 --> 00:06:21,110
hopefully there is some associated code
with the repository available on get

105
00:06:21,111 --> 00:06:24,170
hub. I'll download the code
and start reading it myself.

106
00:06:24,350 --> 00:06:28,610
I'll try to compile and run the code
locally to replicate the results as well.

107
00:06:28,970 --> 00:06:32,270
Usually comments in the code
help further my understanding.

108
00:06:32,540 --> 00:06:36,600
I'll also look for any
additional resources on the
web that help further explain

109
00:06:36,601 --> 00:06:39,680
the text articles, summaries, tutorials.

110
00:06:39,830 --> 00:06:44,510
Usually a popular paper
will have a breakdown that
someone else has done online

111
00:06:44,780 --> 00:06:49,220
that will help drive the key points
home for me. After this second pass,

112
00:06:49,221 --> 00:06:53,360
I'll have a Jupiter notebook full of
notes and associated helper images.

113
00:06:53,480 --> 00:06:55,550
Since I teach this stuff on youtube,

114
00:06:55,820 --> 00:06:59,360
teaching is really the best way
to fully understand any topic.

115
00:06:59,630 --> 00:07:02,420
When it comes to the third pass,
it's all about the math.

116
00:07:02,660 --> 00:07:06,680
My focus on the third pass is to really
understand every detail of the mass.

117
00:07:06,950 --> 00:07:10,840
I might just use a pen and paper and
break down the equations in the paper

118
00:07:10,860 --> 00:07:11,693
myself.

119
00:07:11,840 --> 00:07:16,760
I'll use Wikipedia to help me understand
any of the more formal math concepts

120
00:07:16,761 --> 00:07:20,900
fully like the Kale divergence and
if I'm feeling really ambitious,

121
00:07:21,020 --> 00:07:25,280
I'll try to replicate the paper
programmatically using the hyper parameter

122
00:07:25,281 --> 00:07:28,130
settings and equations that it describes.

123
00:07:28,490 --> 00:07:32,660
After all of this I'll feel confident
enough to discuss it with other people.

124
00:07:32,900 --> 00:07:37,900
Reading papers is not easy and nobody can
read long manipulations of complicated

125
00:07:38,661 --> 00:07:41,900
equations fast.
The key is to never give up.

126
00:07:42,110 --> 00:07:47,030
Turn your frustrations into fuel to get
better. You will understand this paper.

127
00:07:47,180 --> 00:07:50,990
You will master this subject.
You will become awesome at this.

128
00:07:51,320 --> 00:07:54,980
It gets easier every time as you
build your Merkel Dag of knowledge.

129
00:07:55,280 --> 00:08:00,170
See what I did there. If you don't get a
math concept, guess what? Khan Academy.

130
00:08:00,171 --> 00:08:04,100
We'll teach you anything you
need to know for free and lastly,

131
00:08:04,250 --> 00:08:06,290
do not hesitate to ask for help.

132
00:08:06,500 --> 00:08:10,250
There are study groups and communities
online that are centered around the

133
00:08:10,251 --> 00:08:14,690
latest research in machine learning
that you can post your questions too.

134
00:08:15,080 --> 00:08:18,170
Don't be afraid to reach
out to researchers as well.

135
00:08:18,260 --> 00:08:22,460
You're actually doing them a favor by
having them explain to you in terms you

136
00:08:22,461 --> 00:08:23,294
understand.

137
00:08:23,510 --> 00:08:27,650
All scientists need more experience
translating complex topics.

138
00:08:27,950 --> 00:08:31,310
I've got lots of great links for you
in the description and I hope you found

139
00:08:31,311 --> 00:08:34,910
this video useful. If you want to
learn more about machine learning,

140
00:08:34,940 --> 00:08:38,330
AI and blockchain technology,
hit the subscribe button,

141
00:08:38,540 --> 00:08:43,010
and for now I've got to reread the capital
network paper, so thanks for watching.

