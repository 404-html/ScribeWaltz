1
00:00:04,210 --> 00:00:06,160
Raj and welcome to the mat

2
00:00:06,390 --> 00:00:07,260
of intelligence.

3
00:00:16,560 --> 00:00:18,030
For the next three months.

4
00:00:18,330 --> 00:00:22,140
We're going to take a journey through
the most important math concepts.

5
00:00:22,320 --> 00:00:24,390
That underlying machine learning.

6
00:00:24,930 --> 00:00:29,930
That means all the concepts you need
from the great disciplines of Calculus,

7
00:00:30,270 --> 00:00:34,170
linear Algebra,
probability theory and statistics.

8
00:00:34,500 --> 00:00:38,670
The prerequisites are knowing
basic python syntax and Algebra.

9
00:00:39,180 --> 00:00:44,180
Every single algorithm we code will be
done without using any popular machine

10
00:00:44,581 --> 00:00:49,581
learning library because the point of
this course is to help you build a solid

11
00:00:50,280 --> 00:00:55,170
mathematical intuition around building
algorithms that can learn from data.

12
00:00:55,650 --> 00:00:57,150
I mean,
let's face it,

13
00:00:57,420 --> 00:01:00,930
you could just use a black
box API for all this stuff,

14
00:01:01,200 --> 00:01:02,940
but if you have the intuition,

15
00:01:03,180 --> 00:01:08,180
you'll know exactly which algorithm to
use for the job or even custom make your

16
00:01:08,701 --> 00:01:10,920
own from scratch.
As humans,

17
00:01:10,980 --> 00:01:14,520
we are constantly receiving
data through our five senses.

18
00:01:14,550 --> 00:01:19,550
Then somehow we've got to make sense of
all of this chaotic input so that we can

19
00:01:20,281 --> 00:01:23,490
survive.
Thanks to the evolutionary process.

20
00:01:23,670 --> 00:01:26,100
We've developed brains
capable of doing this.

21
00:01:26,430 --> 00:01:31,080
We've got the most precious resource
in the universe. Intelligence,

22
00:01:31,470 --> 00:01:34,410
the ability to learn and apply knowledge.

23
00:01:34,560 --> 00:01:39,300
One way to measure our intelligence
against the rest of the animal kingdom is

24
00:01:39,301 --> 00:01:40,320
using a ladder.

25
00:01:40,740 --> 00:01:45,660
Arts is indeed the most generalized
type of intelligence capable of being

26
00:01:45,661 --> 00:01:48,480
applied to the widest variety of tasks,

27
00:01:48,750 --> 00:01:52,980
but that doesn't mean that we are
necessarily the best kind of intelligence.

28
00:01:53,040 --> 00:01:57,870
In the 1960s a primate researcher
named Dr Jane Goodall concluded that

29
00:01:58,050 --> 00:02:02,760
chimpanzees had been living in the
forest for hundreds of thousands of years

30
00:02:03,060 --> 00:02:06,810
without overpopulating or
destroying their environment at all.

31
00:02:06,920 --> 00:02:11,820
Orca is have the ability to sleep with
one hemisphere of their brain at a time,

32
00:02:12,030 --> 00:02:16,830
which allows them to recuperate while
being aware of their surroundings.

33
00:02:16,950 --> 00:02:20,640
In some ways,
animals are more intelligent than us.

34
00:02:20,820 --> 00:02:23,430
Intelligence consists of many dimensions.

35
00:02:23,640 --> 00:02:28,320
Think of it like a multidimensional
space of possibility when building AI,

36
00:02:28,321 --> 00:02:30,930
the human brain is a great roadmap.
After all,

37
00:02:30,931 --> 00:02:35,310
neural networks have achieved state of
the art performance in countless tasks,

38
00:02:35,550 --> 00:02:37,350
but it's not the only roadmap.

39
00:02:37,380 --> 00:02:42,030
There are many possible
types of intelligence out
there that we can and will

40
00:02:42,031 --> 00:02:42,864
create.

41
00:02:42,960 --> 00:02:47,940
Some will seem familiar to us and some
very alien thinking in a way we've never

42
00:02:47,941 --> 00:02:48,774
done before.

43
00:02:48,870 --> 00:02:53,670
Like when Alpha go played move 37 even
the best go players in the world were

44
00:02:53,671 --> 00:02:55,080
stunned at the move.

45
00:02:55,260 --> 00:02:59,350
It went against everything we've
learned about the game from millennia of

46
00:02:59,351 --> 00:03:00,040
practice,

47
00:03:00,040 --> 00:03:04,330
but it turned out to be an objectively
better strategy that led to its win.

48
00:03:04,790 --> 00:03:08,000
The many different types of
intelligence or like symphonies,

49
00:03:08,060 --> 00:03:12,080
each comprising of different instruments
and these instruments very not just

50
00:03:12,081 --> 00:03:12,914
their names dynamics,

51
00:03:13,090 --> 00:03:16,570
but in their pitch and
tempo and color and melody.

52
00:03:17,080 --> 00:03:21,370
The amount of data we're generating
is growing really fast. No,

53
00:03:21,371 --> 00:03:23,350
I mean really,
really fast.

54
00:03:23,410 --> 00:03:27,580
In the time since you started watching
this video and enough data was generated

55
00:03:27,790 --> 00:03:32,790
for you to spend an entire
lifetime analyzing and only
0.5% of all data ever is

56
00:03:34,840 --> 00:03:37,450
creating intelligence
isn't just a nice to have.

57
00:03:37,540 --> 00:03:39,610
It's a necessity put in the right hands.

58
00:03:39,640 --> 00:03:43,600
It will help us solve problems we never
dreamed could be possible to solve.

59
00:03:43,660 --> 00:03:46,270
So where do we start?
At its core,

60
00:03:46,540 --> 00:03:49,840
machine learning is all about
mathematical optimization.

61
00:03:50,230 --> 00:03:52,120
This is a way of thinking.

62
00:03:52,420 --> 00:03:56,470
Every single problem can be broken
down into an optimization problem.

63
00:03:56,500 --> 00:03:59,560
Once we have some dataset
that acts as our input,

64
00:03:59,740 --> 00:04:04,510
we'll build a model that uses that
data to optimize for an objective,

65
00:04:04,511 --> 00:04:09,400
a goal that we want to reach and the
way it does this is by minimizing some

66
00:04:09,430 --> 00:04:10,900
error value that we defined.

67
00:04:11,410 --> 00:04:14,830
One example problem could
be what should I wear today?

68
00:04:15,220 --> 00:04:20,140
I could frame this as optimizing for
stylishness. Instead of say comfort,

69
00:04:20,410 --> 00:04:25,300
then define an error that
I want to minimize as the
amount of ratings a group of

70
00:04:25,301 --> 00:04:30,301
people give me that are negative or even
what's the best design for my Ios apps

71
00:04:30,941 --> 00:04:33,430
home page.
Rather than hard coding.

72
00:04:33,431 --> 00:04:37,930
In some elements I could find a Dataset
of APP designs and their ratings from

73
00:04:37,931 --> 00:04:42,850
users. If I want to optimize for a
design that would be the highest rated.

74
00:04:42,970 --> 00:04:46,870
I would learn the mapping between
design styles and ratings.

75
00:04:47,020 --> 00:04:50,920
This is the way every single layer of
the stack will be built in the future.

76
00:04:51,070 --> 00:04:54,190
Sometimes our data is labeled,
sometimes it isn't.

77
00:04:54,400 --> 00:04:58,420
There are different techniques we can
use to find patterns in this data.

78
00:04:58,630 --> 00:05:01,750
And sometimes optimizing
for an objective can happen,

79
00:05:01,840 --> 00:05:04,300
not through the frame
of pattern recognition,

80
00:05:04,450 --> 00:05:09,220
but through the exploration
of many possibilities and
seeing what works and what

81
00:05:09,221 --> 00:05:13,360
doesn't. There are many ways that
we can frame the learning process.

82
00:05:13,361 --> 00:05:17,650
But the easiest way to learn,
it's when we use labeled data,

83
00:05:18,520 --> 00:05:21,160
mathematically speaking,
we have some input.

84
00:05:21,290 --> 00:05:25,120
There's a domain x where every point
of x has features that we observed.

85
00:05:25,330 --> 00:05:26,860
Then we have a label set y.

86
00:05:27,040 --> 00:05:31,390
So the data consists of a set of labeled
examples that we can denote this way.

87
00:05:31,720 --> 00:05:36,130
The output then would be a prediction
rule. So given a new x value,

88
00:05:36,280 --> 00:05:40,180
what it's associated, why value,
we've got to learn this mapping,

89
00:05:40,360 --> 00:05:44,590
which is an unknown distribution
over x to be able to answer this.

90
00:05:44,740 --> 00:05:49,330
So we have to measure some error function
that acts as a performance metric.

91
00:05:49,360 --> 00:05:54,240
So what we do is choose from a number
of possible models to represent this

92
00:05:54,241 --> 00:05:55,090
function well.

93
00:05:55,091 --> 00:05:59,960
Initially set some parameter values to
represent the mapping and we'd evaluate

94
00:05:59,961 --> 00:06:03,710
the initial results, measured
the error, update the parameters.

95
00:06:03,890 --> 00:06:05,300
And repeat this process,

96
00:06:05,330 --> 00:06:09,890
optimizing the model again and again
until it fully learns the mapping.

97
00:06:10,330 --> 00:06:13,890
Was it convex or concave functions
that were easier to optimize?

98
00:06:14,520 --> 00:06:18,780
I think convex. I really hope my
lab partners advocate optimization.

99
00:06:19,050 --> 00:06:20,090
I guess I should be thankful.

100
00:06:20,110 --> 00:06:23,370
Not many data scientists get a grant
from cern to detect the Higgs Boson.

101
00:06:24,000 --> 00:06:28,680
What was her name again?
Oh, Luis. I think. Yeah, she
did win an award from ICML.

102
00:06:29,040 --> 00:06:31,830
I wonder if she's cute.
That doesn't matter.

103
00:06:32,220 --> 00:06:34,890
I am not going to mix business
and pleasure. Not this time.

104
00:06:35,350 --> 00:06:37,330
Suppose I've got a bunch of data points.

105
00:06:37,810 --> 00:06:42,580
These are just data points like what
apple probably trained Siri on there.

106
00:06:42,581 --> 00:06:47,440
All x y value pairs were x represents
the distance a person bikes and why

107
00:06:47,441 --> 00:06:49,780
represents the amount
of calories they lost.

108
00:06:50,170 --> 00:06:52,540
We can just plot them on a graph like so.

109
00:06:52,720 --> 00:06:56,320
We want to be able to predict the
calories lost for a new person,

110
00:06:56,530 --> 00:06:59,920
even their biking distance.
How should we do this? Well,

111
00:06:59,921 --> 00:07:03,850
we could try to draw a line that
it's through all the data points,

112
00:07:04,240 --> 00:07:08,770
but it seems like our points are too
spaced out for a straight line to pass

113
00:07:08,771 --> 00:07:13,771
through all of them so we can settle
for drawing the line of best fit a line

114
00:07:13,811 --> 00:07:16,570
that goes through as many
data points as possible.

115
00:07:16,870 --> 00:07:21,430
Algebra tells us that the equation for
a straight line is of the form y equals

116
00:07:21,431 --> 00:07:25,900
mx plus B where m represents the
slope or steepness of the line and be

117
00:07:25,901 --> 00:07:28,270
represents.
It's why access intercept points.

118
00:07:28,630 --> 00:07:32,650
We want to find the optimal values
for B and m such that the line,

119
00:07:32,710 --> 00:07:36,370
it's as many points as possible.
Given any new x value,

120
00:07:36,460 --> 00:07:40,210
we can plug it into our equation and
it'll help put the most likely why value

121
00:07:40,270 --> 00:07:44,950
our error metric can be a measure of
closeness, which we can define like this.

122
00:07:45,100 --> 00:07:49,870
So let's start off with a random B and
m value and plot this line for every

123
00:07:49,871 --> 00:07:53,950
single data point we have. Let's
calculate it's associated. Why value?

124
00:07:54,220 --> 00:07:58,810
Then we'll subtract the actual why value
from it to measure the distance between

125
00:07:58,811 --> 00:08:03,190
the two. We'll want to square this
error to make our next steps easier.

126
00:08:03,520 --> 00:08:08,500
Once we sum all of these values, we get
a single value that represents our error.

127
00:08:08,650 --> 00:08:12,730
Given that line we just drew. Now,
if we did this process repeatedly,

128
00:08:12,760 --> 00:08:17,020
say 1,337 times for a bunch of
different randomly drawn lines,

129
00:08:17,350 --> 00:08:22,350
we could create a three d graph that
shows the air value for every associated B

130
00:08:22,421 --> 00:08:23,254
and m value.

131
00:08:23,650 --> 00:08:27,520
Notice how there is a valley in this
graph at the bottom of this valley,

132
00:08:27,700 --> 00:08:32,700
the error is at its smallest and so the
associated bnm values would be the line

133
00:08:32,920 --> 00:08:33,760
of best fit,

134
00:08:34,000 --> 00:08:37,690
where the distance between all our
data points and our line would be the

135
00:08:37,691 --> 00:08:40,600
smallest, but how do we find it? Well,

136
00:08:40,630 --> 00:08:44,680
we'll need to try out a bunch of different
lines to create this three d graph,

137
00:08:45,010 --> 00:08:50,010
but rather than just randomly drawing
lines over and over again with no signal,

138
00:08:50,590 --> 00:08:55,590
what if we could do it in a more efficient
way such that each successive we draw

139
00:08:56,160 --> 00:09:01,020
brings us closer and closer to the bottom
of this valley. We need a direction,

140
00:09:01,050 --> 00:09:04,830
a way to descend this valley.
What hip for a given function,

141
00:09:05,040 --> 00:09:07,320
we could find the slope
of it at a given point.

142
00:09:07,470 --> 00:09:11,220
Then that slope would point in a certain
direction towards the Minima of the

143
00:09:11,221 --> 00:09:14,670
graph and when we redraw our
lines over and over again,

144
00:09:14,880 --> 00:09:17,670
we could do so using the
slope as our compass,

145
00:09:17,820 --> 00:09:20,280
as our guide on how best to redraw

146
00:09:23,350 --> 00:09:27,400
towards the minima until our slope
approaches zero. In Calculus,

147
00:09:27,550 --> 00:09:30,250
we call this slope the
derivative of a function.

148
00:09:30,490 --> 00:09:33,850
Since we are updating to value B, and, m,

149
00:09:34,150 --> 00:09:37,300
we want to calculate the derivative
with respect to both of them,

150
00:09:37,750 --> 00:09:38,920
the partial derivative,

151
00:09:39,100 --> 00:09:43,120
the partial derivative with respect to
a variable means that we calculate the

152
00:09:43,121 --> 00:09:46,630
derivative of that variable
while ignoring the others,

153
00:09:46,810 --> 00:09:50,650
so it will compute the partial
derivative with respect to B.

154
00:09:50,950 --> 00:09:54,730
Then the partial derivative
with respect to em. To do this,

155
00:09:54,790 --> 00:09:56,170
we use the power rule.

156
00:09:56,410 --> 00:10:01,060
We multiply the exponent by
the Coefficient and subtract
one from the exponent.

157
00:10:01,390 --> 00:10:03,310
Once we have these two values,

158
00:10:03,490 --> 00:10:08,260
we can update both of these parameters
from our function by subtracting them

159
00:10:08,350 --> 00:10:12,790
from our existing DNM values and we
just keep doing that for a set number of

160
00:10:12,791 --> 00:10:14,500
iterations that we predefined.

161
00:10:14,710 --> 00:10:19,420
So this optimization technique that we
just performed is called gradient descent

162
00:10:19,660 --> 00:10:22,270
and it's the most popular
one in machine learning.

163
00:10:22,480 --> 00:10:26,200
So what do you need to remember
from this video? Three points.

164
00:10:26,470 --> 00:10:29,560
The derivative is the slope
of a function at a given line.

165
00:10:29,740 --> 00:10:32,920
The partial derivative is a slope
with respect to one variable.

166
00:10:32,950 --> 00:10:37,950
In that function we can use them to
compose a gradient which points in the

167
00:10:38,111 --> 00:10:42,700
direction of the local Minima of a
function and gradient descent is a very

168
00:10:42,701 --> 00:10:47,110
popular optimization strategy in machine
learning that uses the gradient to do

169
00:10:47,111 --> 00:10:50,050
this. The winter of last week's
coding challenge is typical hog.

170
00:10:50,240 --> 00:10:54,020
He created an encryption algorithm that's
pretty complex written in c plus plus

171
00:10:54,021 --> 00:10:55,960
that I've never heard of before.
Definitely check it out.

172
00:10:55,961 --> 00:10:58,200
Links in the description.
Great Work Wizard

173
00:10:58,260 --> 00:11:01,920
of the wick. Now it's your turn.
I've got a coding challenge for you.

174
00:11:02,370 --> 00:11:06,520
Implement grading dissent on your own
on a different datasets that all provide

175
00:11:06,710 --> 00:11:08,970
check checkup and get
hub link for details.

176
00:11:09,150 --> 00:11:10,860
And the winner will be
announced in a week.

177
00:11:10,950 --> 00:11:13,590
Please subscribe for more
programming videos. And for now,

178
00:11:13,680 --> 00:11:16,770
I've got to memorize the power rule,
so thanks for watching.

