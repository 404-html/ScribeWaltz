1
00:00:00,870 --> 00:00:03,120
Success.
The Ai beat me.

2
00:00:03,870 --> 00:00:08,870
Hello world it Saroj and just a few days
ago opened a eyes team of bots won a

3
00:00:09,871 --> 00:00:14,190
match against a team of some of
the best dota players in the world.

4
00:00:14,550 --> 00:00:18,360
I'll explain how they did it in
this video and if you're new here,

5
00:00:18,361 --> 00:00:23,040
hit the subscribe button to stay
up to date on my latest AI content.

6
00:00:23,490 --> 00:00:27,960
The event was streamed online to over
a hundred thousand people and it was

7
00:00:27,961 --> 00:00:28,794
massive.

8
00:00:28,980 --> 00:00:33,360
I was there in the thick of it amongst
the crowd and could feel the energy all

9
00:00:33,361 --> 00:00:38,310
around me. I even got to say hi to Andre
Carpathia, which was algorithmic. Yes,

10
00:00:38,311 --> 00:00:41,340
I'm starting that.
This is a big deal.

11
00:00:41,370 --> 00:00:45,780
The entire AI community is
very excited about the results.

12
00:00:46,080 --> 00:00:51,080
Open AI calls their algorithm five
because it's a set of five neural networks

13
00:00:51,720 --> 00:00:54,570
that work together to
defeat the opposing team.

14
00:00:54,960 --> 00:00:59,130
Open Ai Five has now played
against several opposing teams.

15
00:00:59,340 --> 00:01:03,870
Although it started off playing
against open AI employees.

16
00:01:04,110 --> 00:01:07,920
It moved onto audience members,
valve employees,

17
00:01:08,050 --> 00:01:11,460
an amateur team, a Semipro
team, and now this,

18
00:01:11,880 --> 00:01:16,880
the reason open AI is dedicating all of
their time and energy to creating bots

19
00:01:16,981 --> 00:01:21,981
that can beat humans at Games is because
the idea that if an AI can learn how to

20
00:01:22,441 --> 00:01:26,400
navigate and strategize
in an unstructured, messy,

21
00:01:26,460 --> 00:01:29,190
unpredictable environment
like a game world,

22
00:01:29,550 --> 00:01:34,550
we can transfer all of those skills that
it's learned onto complex real world

23
00:01:35,161 --> 00:01:40,110
problems like drug discovery,
materials design, climate change,

24
00:01:40,230 --> 00:01:40,861
and ASM.

25
00:01:40,861 --> 00:01:45,861
Our videos when IBM's deep blue beat the
world champion Garry Kasparov at chess

26
00:01:47,070 --> 00:01:52,070
in the 90s it surprised a lot of people
and the algorithms it used including

27
00:01:52,381 --> 00:01:53,340
tree search.

28
00:01:53,400 --> 00:01:58,400
Many macs and Alpha Beta pruning proved
to be useful in other applications like

29
00:01:59,730 --> 00:02:02,880
route planning for vehicles.
Just last year,

30
00:02:02,881 --> 00:02:07,881
a similar victory occurred when Google's
Alphago bought beat the best go player

31
00:02:07,951 --> 00:02:12,951
in the world go has a search space that's
orders of magnitude larger than chess

32
00:02:13,890 --> 00:02:18,890
and many wrongly predicted that it
wouldn't be possible for an AI to defeat a

33
00:02:19,351 --> 00:02:22,350
human so soon.
Now with open AI,

34
00:02:22,351 --> 00:02:27,351
five Dota or defense of the ancients
too is the newest game to be conquered.

35
00:02:28,440 --> 00:02:33,440
Dota two is an online multiplayer game
where two teams consisting of five

36
00:02:33,781 --> 00:02:38,781
players each compete to collectively
destroy a large structure defended by the

37
00:02:39,511 --> 00:02:44,250
opposing team called the ancient
while defending their own.

38
00:02:44,550 --> 00:02:49,110
It's a real time strategy game and
it's presented on a single map.

39
00:02:49,260 --> 00:02:51,540
From a three dimensional perspective.

40
00:02:51,750 --> 00:02:56,750
The game has 115 playable characters
that players can choose from.

41
00:02:57,090 --> 00:03:01,120
Each has its own design,
strength and weaknesses.

42
00:03:01,360 --> 00:03:02,740
Once the game begins,

43
00:03:02,741 --> 00:03:07,741
players start off with an experience
level of one with just one ability but are

44
00:03:08,441 --> 00:03:13,180
able to level up and become more
powerful during the course of the game.

45
00:03:13,240 --> 00:03:18,240
Up to a maximum level of 25
abilities are gradually unlocked.

46
00:03:19,360 --> 00:03:24,360
In 2011 the creators of Dota Valve
sponsored 16 teams to compete at the

47
00:03:25,601 --> 00:03:27,250
international,
uh,

48
00:03:27,251 --> 00:03:32,200
Dota specific sports tournament
for $1 million prize. Since then,

49
00:03:32,201 --> 00:03:37,201
it's become an annual championship
tournament with the prize money increasing

50
00:03:37,420 --> 00:03:39,070
each year.
In total,

51
00:03:39,071 --> 00:03:44,071
Dota two tournaments have earned teams
over $100 million in prize money up to

52
00:03:45,161 --> 00:03:50,110
now making it the highest earning
east sports game of all time.

53
00:03:50,380 --> 00:03:55,380
Brb switching careers players dedicate
real time and effort to become

54
00:03:55,691 --> 00:03:57,730
professionals at this game,

55
00:03:57,940 --> 00:04:02,020
which makes open Ai's win
that much more impressive.

56
00:04:02,440 --> 00:04:06,160
Open AI five consists
of five neural networks.

57
00:04:06,520 --> 00:04:10,660
Each neural network is a single layer,
10 24 units,

58
00:04:10,900 --> 00:04:12,580
long short term memory,

59
00:04:12,581 --> 00:04:17,500
recurrent network recurrent networks
specialize in sequence learning.

60
00:04:17,590 --> 00:04:21,970
Give it a sequence of words and it'll
predict the next word in that sequence.

61
00:04:22,420 --> 00:04:26,350
Unless the input data is too big,
if there are too many words,

62
00:04:26,440 --> 00:04:30,760
the recurrent network will start to forget
what it learned from the first words.

63
00:04:31,210 --> 00:04:33,460
A variance of recurrent networks.

64
00:04:33,490 --> 00:04:38,490
LSTM networks account for this problem
popularly known as the vanishing gradient

65
00:04:40,031 --> 00:04:45,031
problem by adding a clever combination
of operations and variables to the

66
00:04:45,881 --> 00:04:50,881
network such that during the gradient
descent process where the gradient value

67
00:04:51,281 --> 00:04:55,900
is what we use to update the weights
of the network accordingly during every

68
00:04:55,901 --> 00:04:58,240
training iteration doesn't vanish.

69
00:04:58,570 --> 00:05:01,180
It's maintained throughout the network.

70
00:05:01,210 --> 00:05:06,210
It's locked into the weights of the
LSTM network components called gates.

71
00:05:07,150 --> 00:05:11,020
Accordingly,
and since the gates are differentiable,

72
00:05:11,260 --> 00:05:16,260
the cell learns what's
important to remember and
what's important to forget over

73
00:05:17,171 --> 00:05:19,150
time.
During optimization,

74
00:05:19,600 --> 00:05:24,600
each open AI single layer LSTM network
receives the current Dota two games

75
00:05:25,201 --> 00:05:30,190
states as its input,
which is extracted from Valves Bot Api.

76
00:05:30,550 --> 00:05:35,550
This contains all the relevant data about
where players are and what's happening

77
00:05:36,100 --> 00:05:38,650
on the map inside of a vector.

78
00:05:38,950 --> 00:05:43,870
The output of each LSTM is
an action on what to do next.

79
00:05:44,140 --> 00:05:49,140
There are several possible action heads
and the LSTM chooses the specific action

80
00:05:50,081 --> 00:05:54,910
path to take, then executes it in
a supervised learning scenario.

81
00:05:54,911 --> 00:05:59,911
It's all about learning the input slash
label relationships so that given new

82
00:06:00,261 --> 00:06:02,420
data it can label it properly.

83
00:06:02,900 --> 00:06:07,100
But unfortunately there's no static
data set here. Sorry, Kaggle.

84
00:06:07,370 --> 00:06:12,370
It's a real time environment and the
signal is not an immediate label,

85
00:06:12,560 --> 00:06:15,890
but instead a time delayed
label called the reward.

86
00:06:16,310 --> 00:06:20,060
This is instead of paradigm
called reinforcement learning.

87
00:06:20,390 --> 00:06:25,390
The reward is provided by the environment
itself and specified by the creator of

88
00:06:26,991 --> 00:06:28,010
the environment.

89
00:06:28,370 --> 00:06:33,370
Open Ai developed a reward that
consisted of a combination of aggregated

90
00:06:33,591 --> 00:06:38,240
metrics, including net
worth, kills and assists.

91
00:06:38,570 --> 00:06:43,310
These rewards can either be positive
or negative depending on what the agent

92
00:06:43,311 --> 00:06:45,140
does in the environment.

93
00:06:45,560 --> 00:06:50,560
The agent receives the state of the
game as input takes an action in the

94
00:06:51,111 --> 00:06:56,111
environment and either receives a
positive or negative reward signal,

95
00:06:56,480 --> 00:07:01,480
which it uses to update its weights to
select an action more likely to receive

96
00:07:01,821 --> 00:07:04,280
her award.
Every training iteration.

97
00:07:04,700 --> 00:07:08,630
The agent in this case is
an LSTM neural network,

98
00:07:08,660 --> 00:07:13,660
and we can frame the way it has to make
decisions as what's called a Markov

99
00:07:14,840 --> 00:07:16,550
decision process.

100
00:07:17,090 --> 00:07:21,350
There's a set of possible game
states of possible actions,

101
00:07:21,500 --> 00:07:22,640
a reward function,

102
00:07:22,670 --> 00:07:27,290
and a description of each
actions affects in each state.

103
00:07:27,680 --> 00:07:32,180
The reason it's called Markov is because
it'll have the mark coff property,

104
00:07:32,480 --> 00:07:37,480
which states that the effects of an action
taken in a given state depend only on

105
00:07:38,631 --> 00:07:41,450
that state and not on the prior history.

106
00:07:41,900 --> 00:07:46,900
Although the dynamics of the environment
are outside of the agent's control,

107
00:07:47,180 --> 00:07:51,890
it will through the training process,
develop what's called a policy.

108
00:07:52,100 --> 00:07:57,100
A policy is defined as the probability
distribution of actions given a state.

109
00:07:58,970 --> 00:08:03,970
It can use this policy to decide what
the next best action to take would be.

110
00:08:04,730 --> 00:08:09,730
The objective of any reinforcement
learning agent is to maximize the expected

111
00:08:09,951 --> 00:08:13,760
reward when following a policy.
To learn this policy,

112
00:08:13,790 --> 00:08:15,770
lots of algorithms exist.

113
00:08:15,860 --> 00:08:19,940
One popular algorithm is
called policy gradients.

114
00:08:20,270 --> 00:08:24,080
The idea is that when an agent
takes an action in an environment,

115
00:08:24,320 --> 00:08:26,600
it might receive a positive reward,

116
00:08:26,870 --> 00:08:30,800
but what if that action is
actually bad in the long run?

117
00:08:31,070 --> 00:08:36,070
What if we could wait and see how this
action played out until someone wins the

118
00:08:36,771 --> 00:08:37,604
game?

119
00:08:37,760 --> 00:08:42,760
The PG algorithm helps learn a policy
that samples actions and then the actions

120
00:08:43,881 --> 00:08:48,881
that happened to eventually lead to good
outcomes get encouraged in the future.

121
00:08:49,250 --> 00:08:53,600
The actions taken that lead to
bad outcomes get discouraged,

122
00:08:54,050 --> 00:08:57,990
seems legit.
Each action has an advantage estimate.

123
00:08:58,230 --> 00:09:03,230
It numerically answers the question of
how much better is this action than the

124
00:09:04,080 --> 00:09:08,310
average or expected action
I take at this state.

125
00:09:08,580 --> 00:09:13,260
The gradients that are computed during
optimization are accumulated over a

126
00:09:13,261 --> 00:09:17,610
number of full played episodes,
then applied to the network,

127
00:09:17,670 --> 00:09:21,300
but it turns out there's a problem
with policy gradient methods.

128
00:09:21,510 --> 00:09:26,430
They're very sensitive to the choice of
step size during the training process.

129
00:09:26,640 --> 00:09:31,350
If a step is too small, then the
progress is small and if it's too large,

130
00:09:31,380 --> 00:09:32,970
the response will be noisy.

131
00:09:32,970 --> 00:09:37,380
Making it very challenging in
a policy gradients setting.

132
00:09:37,440 --> 00:09:40,740
If the agent ends up in a
poor region of action space,

133
00:09:41,010 --> 00:09:46,010
it can leave it in a position where it
has very few useful state action pairs to

134
00:09:46,201 --> 00:09:47,910
learn from.
For example,

135
00:09:47,911 --> 00:09:52,911
if an agent tries to navigate a maze to
find some cheese and had accidentally

136
00:09:53,310 --> 00:09:56,040
learned a policy of just
spinning in circles,

137
00:09:56,400 --> 00:10:01,230
it would have a hard time reaching any
actions that had a positive reward.

138
00:10:01,350 --> 00:10:05,310
Since it could only learn to
increase an action probability.

139
00:10:05,490 --> 00:10:10,020
By experiencing that action and
seeing that it leads forward,

140
00:10:10,110 --> 00:10:14,910
it literally gets stuck and its
own learned loop with no way out.

141
00:10:15,270 --> 00:10:20,270
How can we make an update
that's guaranteed to improve
our current policy or at

142
00:10:20,551 --> 00:10:25,050
least not make it worst God mode?
Of course. Just kidding. Well,

143
00:10:25,080 --> 00:10:28,440
if we separate our
policy into two policies,

144
00:10:28,650 --> 00:10:33,650
a post update policy called the new
policy and a pre update policy called the

145
00:10:34,441 --> 00:10:38,220
old policy,
we can calculate this at each state.

146
00:10:38,280 --> 00:10:43,280
We take a sum over the actions of the
new policy probability at that state

147
00:10:44,280 --> 00:10:49,280
multiplied by how much better that action
is than the average old policy action

148
00:10:50,460 --> 00:10:53,160
at that state termed the advantage.

149
00:10:53,550 --> 00:10:58,410
We then weigh each state's expected
new policy advantage by how likely that

150
00:10:58,411 --> 00:11:01,620
state is to be reached by the new policy.

151
00:11:01,860 --> 00:11:05,610
This gets us the expected
advantage of the new policy.

152
00:11:05,730 --> 00:11:10,730
Over the old one and lets us confirmed
that expected advantage is positive

153
00:11:11,160 --> 00:11:12,900
before we take a move.

154
00:11:13,050 --> 00:11:18,050
The approach of the trust region policy
optimization algorithm or t RPO is to

155
00:11:20,071 --> 00:11:24,840
calculate an estimate of the advantage
quantity we've just described,

156
00:11:25,170 --> 00:11:30,170
but doing so using the distribution of
states from the old policy rather than

157
00:11:30,931 --> 00:11:33,810
the state distribution
from the new policy.

158
00:11:34,230 --> 00:11:39,230
It controls the rate of policy change
by placing a constraint on the average

159
00:11:39,481 --> 00:11:44,481
difference or KL divergence between the
new and old policy after each update.

160
00:11:46,560 --> 00:11:50,820
Even still trp Oh,
has a very poor sample efficiency,

161
00:11:51,030 --> 00:11:56,030
usually taking a large number of training
steps to optimize open AI introduced

162
00:11:57,031 --> 00:12:02,031
their own spinoff of Trp
Rpo called proximal policy
optimization and that's the

163
00:12:03,371 --> 00:12:05,470
algorithm they used for their bots.

164
00:12:05,710 --> 00:12:10,710
It simplifies the problem by converting
trp Rpos KL divergence from a constraint

165
00:12:12,130 --> 00:12:16,900
into a penalty term with no need
to compute the KL divergence.

166
00:12:17,140 --> 00:12:19,330
The Algorithm is less complex.

167
00:12:19,480 --> 00:12:22,780
It strikes a balance between
ease of implementation,

168
00:12:22,840 --> 00:12:26,710
sample complexity and ease of tuning.
So Open Ai,

169
00:12:26,711 --> 00:12:31,711
trained by LSTM networks using PPO on
two 56 gps available on Google cloud,

170
00:12:33,700 --> 00:12:38,700
flying 180 years worth of games against
itself every day from walking to skill

171
00:12:40,151 --> 00:12:42,730
development,
to learning coordination.

172
00:12:42,820 --> 00:12:46,150
There are bots improved
throughout the process.

173
00:12:46,600 --> 00:12:51,100
The next step for the team is to move
on to the international and play against

174
00:12:51,101 --> 00:12:52,510
the best in the world.

175
00:12:52,720 --> 00:12:56,440
If you want to learn more about any
of the topics I've talked about,

176
00:12:56,500 --> 00:12:58,570
check out the links in
the video description.

177
00:12:58,990 --> 00:13:02,740
All your policy are belong to us.
Hit subscribe.

178
00:13:02,770 --> 00:13:06,970
If Ai excites you too, and make sure to
connect with me on Instagram, Facebook,

179
00:13:07,000 --> 00:13:11,260
and Twitter. For now, I've got to play
with some data, so thanks for watching.

