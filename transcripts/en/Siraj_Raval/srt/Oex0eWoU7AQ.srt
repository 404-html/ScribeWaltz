1
00:00:00,100 --> 00:00:01,500
How does a computer, yeah, make art.

2
00:00:01,830 --> 00:00:05,340
The great artists of our time have
all had their own distinct style.

3
00:00:05,560 --> 00:00:09,300
The Vinci could evoke wonder and
layer hidden messages. In his words,

4
00:00:09,570 --> 00:00:12,480
Goya was able to create an
unmatched sense of dread.

5
00:00:12,810 --> 00:00:15,540
They'll leave blurred the line
between reality and dreams.

6
00:00:15,810 --> 00:00:19,170
The loan artists given the right
medium is able to create beauty.

7
00:00:19,350 --> 00:00:21,090
When the film camera was first invented,

8
00:00:21,210 --> 00:00:24,900
it wasn't thought of as an artistic tool,
just one able to capture reality,

9
00:00:25,230 --> 00:00:29,130
but when artists got their hands on it,
it was the birth of a new era. In fact,

10
00:00:29,160 --> 00:00:31,200
every time we've created a technology,

11
00:00:31,380 --> 00:00:34,080
artists have found a way to
use it as a creative tool,

12
00:00:34,230 --> 00:00:38,070
whether it was meant to track our
hands or just to take hulks. Recently,

13
00:00:38,071 --> 00:00:42,360
advances in machine learning have allowed
us to generate amazing art pieces with

14
00:00:42,361 --> 00:00:43,230
a few lines of code.

15
00:00:43,530 --> 00:00:47,190
What if you could prototype in
a piece a hundred times faster?

16
00:00:47,340 --> 00:00:49,590
Having your medium actually
collaborate with you?

17
00:00:49,680 --> 00:00:53,340
If we can extend these biologically
learned patterns with machine learning

18
00:00:53,341 --> 00:00:57,510
patterns, it'll become much more clear
that it's not that machines are artistic

19
00:00:57,511 --> 00:01:01,780
competitors. It's that we've upgraded
our own creativity. Hello world,

20
00:01:01,781 --> 00:01:06,550
it's Saroj and let's write a python script
to transform any image into the style

21
00:01:06,551 --> 00:01:08,140
of an artist that we choose.

22
00:01:08,560 --> 00:01:12,460
One of the first attempts at computational
artistry with by a British artist

23
00:01:12,461 --> 00:01:16,270
named Harold Cohen in
1973 inspired by his wife,

24
00:01:16,271 --> 00:01:20,410
the prominent Japanese poet Hiromi Ito.
He created a program called Aaron,

25
00:01:20,590 --> 00:01:22,360
which created abstract drawings,

26
00:01:22,570 --> 00:01:27,040
Coen hand coded based structures into it
that we're used to arrive at a four it

27
00:01:27,041 --> 00:01:29,440
was a tree like structure
and using heuristics.

28
00:01:29,680 --> 00:01:33,790
The program was able to take encoded rules
and generate new combinations of what

29
00:01:33,791 --> 00:01:35,110
it knew incredibly.

30
00:01:35,111 --> 00:01:38,620
The paintings that generated ended up
being displayed at museums across the

31
00:01:38,621 --> 00:01:41,830
world from London's Tate
modern to San Francisco. Soma.

32
00:01:42,250 --> 00:01:46,990
Fast forward to 2015 when Google released
deep dream. The Internet went crazy.

33
00:01:50,930 --> 00:01:55,070
All Glory to hip, no toad. They trained
a convolutional net to classify images,

34
00:01:55,400 --> 00:01:59,810
then use an optimization technique to
enhance patterns in the input image rather

35
00:01:59,811 --> 00:02:03,470
than its own weights based on what
he had learned. Soon afterwards,

36
00:02:03,650 --> 00:02:08,000
three German researchers use a CNN to
transfer the style of a given painting to

37
00:02:08,030 --> 00:02:08,863
any image.

38
00:02:08,930 --> 00:02:12,410
They later created a website
called [inaudible] that
lets anyone do this easily.

39
00:02:12,590 --> 00:02:13,940
Since that initial paper,

40
00:02:13,941 --> 00:02:17,870
the Ai community has started to think
about the possibilities for artistry.

41
00:02:17,960 --> 00:02:18,860
You think machine learning,

42
00:02:19,520 --> 00:02:24,230
even Kristen Stewart from twilight publish
a paper on artistic style transfer as

43
00:02:24,231 --> 00:02:28,310
part of her new movie come
swim. Yeah. Also I'm aware wolf,

44
00:02:28,670 --> 00:02:32,120
so let's understand how this
style transfer process works.

45
00:02:32,270 --> 00:02:35,420
By writing our own script and
carrots with a tensorflow backend,

46
00:02:35,810 --> 00:02:37,190
we're going to use a base image,

47
00:02:37,191 --> 00:02:41,300
which is this extremely attractive
photo of me and a style reference image.

48
00:02:41,570 --> 00:02:45,290
Our script, we'll take the style of this
image and apply it to the base image,

49
00:02:45,500 --> 00:02:48,830
so we're going to feed these images into
a neural net by first converting them

50
00:02:48,831 --> 00:02:52,310
into the de facto data format
for all neural nets. Tensors.

51
00:02:52,610 --> 00:02:54,770
The variable function
from Ross is backend.

52
00:02:54,771 --> 00:02:57,020
Tensorflow is equivalent
to TF dot variable.

53
00:02:57,380 --> 00:02:59,590
The perimeter will be the
image converted an array.

54
00:02:59,800 --> 00:03:03,460
Then we'll do the same for the style
image and we'll create a combination image

55
00:03:03,610 --> 00:03:05,500
where we can later store our end result.

56
00:03:05,740 --> 00:03:08,980
We'll use a placeholder to initialize
it with a given with an height.

57
00:03:09,310 --> 00:03:12,850
Let's see if we successfully loaded
our images. Yup. This checks out.

58
00:03:13,150 --> 00:03:16,870
Now we'll want to combine these three
images into a single tensor that we can

59
00:03:16,871 --> 00:03:17,920
feed into our model.

60
00:03:18,190 --> 00:03:21,310
We'll use the concatenate function
to do this in just one line.

61
00:03:21,580 --> 00:03:26,200
The next step will be to download our
pretrained model called BGG 16 that chaos

62
00:03:26,201 --> 00:03:30,400
has wrapped for us beautifully setting
our input to our newly created tenser and

63
00:03:30,401 --> 00:03:34,180
the weights to the image net weights
we'll set include top to false since we

64
00:03:34,181 --> 00:03:37,120
don't want to include the fully connected
layer at the top of the network.

65
00:03:37,240 --> 00:03:42,240
Dgg 16 is a 16 layer convolutional net
created by the visual geometry group at

66
00:03:42,820 --> 00:03:47,710
Oxford that won the image net competition
in 2014 the idea here is that a

67
00:03:47,711 --> 00:03:52,150
convolutional net pretrained for image
classification on thousands of different

68
00:03:52,151 --> 00:03:56,650
images already knows how to encode
the information contained in an image.

69
00:03:56,920 --> 00:04:01,570
It's Lauren filters at each layer that
can detect certain generalized features.

70
00:04:01,900 --> 00:04:05,530
We're going to use these filters to
help us perform style transfer and the

71
00:04:05,531 --> 00:04:09,280
reason we don't need the convolutional
blog at the top is because it's fully

72
00:04:09,281 --> 00:04:09,671
connected.

73
00:04:09,671 --> 00:04:14,350
Layers and softmax function help classify
images by squashing the dimensionality

74
00:04:14,351 --> 00:04:17,870
feature map and how putting a
probability we're not going to classify.

75
00:04:17,920 --> 00:04:19,330
Just transform our input.

76
00:04:19,390 --> 00:04:23,500
We'll frame this style transfer task as
an optimization problem where we have

77
00:04:23,501 --> 00:04:27,340
some loss function that measures an
error value that we want to minimize.

78
00:04:27,580 --> 00:04:30,970
Our loss function in this case
can be decomposed into two parts.

79
00:04:31,210 --> 00:04:32,860
Content loss and style loss.

80
00:04:33,100 --> 00:04:36,610
We'll initialize a total loss is zero
and add each of them to it. First,

81
00:04:36,611 --> 00:04:37,480
the content loss,

82
00:04:37,660 --> 00:04:40,750
we can think of an image as having both
a style component and a content one.

83
00:04:40,960 --> 00:04:44,290
We know that the features that a
CNN learns are arranged in order of

84
00:04:44,291 --> 00:04:46,420
progressively more abstract compositions.

85
00:04:46,510 --> 00:04:50,740
Since the higher level features are more
extract detecting things like faces and

86
00:04:50,741 --> 00:04:52,930
the meaning of the universe
actually not that abstract,

87
00:04:53,110 --> 00:04:54,940
we can associate them with content.

88
00:04:55,270 --> 00:04:57,490
They detect the objects
that make up an image.

89
00:04:57,700 --> 00:05:00,580
When we run our output image and our
reference image through the network

90
00:05:00,581 --> 00:05:01,414
respectively,

91
00:05:01,450 --> 00:05:05,200
we'll get a set of feature representations
for both from a hidden layer that we

92
00:05:05,201 --> 00:05:08,650
choose and we'll measure the Euclidean
distance between them to calculate our

93
00:05:08,651 --> 00:05:12,370
law named after the ancient Greek
mathematician Euclid Alexandria.

94
00:05:12,490 --> 00:05:15,670
The idea of distance is very
useful in machine learning.

95
00:05:15,670 --> 00:05:19,480
We can use it to find rankings,
recommendations, similarities.

96
00:05:19,630 --> 00:05:22,360
It's a mathematical way of comparing data.

97
00:05:22,510 --> 00:05:25,060
The second loss will
calculate his style loss.

98
00:05:25,270 --> 00:05:28,030
This is still a function of our
networks hidden layer outputs,

99
00:05:28,150 --> 00:05:30,520
but it's slightly more complex.
Let's do this.

100
00:05:30,610 --> 00:05:34,120
We still pass both images through
the net to observe their activations,

101
00:05:34,121 --> 00:05:37,810
but instead of comparing the raw
activations directly, like for content,

102
00:05:38,110 --> 00:05:41,680
we'll add an extra step to measure
the correlation of the activations.

103
00:05:41,950 --> 00:05:42,880
For both of our images,

104
00:05:42,881 --> 00:05:46,330
we'll take what's called the grand matrix
of the activations hadn't given layer

105
00:05:46,331 --> 00:05:49,810
in the network. This will measure which
features tend to activate together.

106
00:05:50,050 --> 00:05:53,800
This is calculated by taking the inner
product of all the activations added

107
00:05:53,820 --> 00:05:56,620
given layer, which are a bunch
of vectors. One for each feature.

108
00:05:56,800 --> 00:06:01,040
So this resulting matrix contains the
correlations between every pair of feature

109
00:06:01,041 --> 00:06:02,060
maps at a given layer.

110
00:06:02,390 --> 00:06:06,410
It represents the tendency of features
to co occur in different parts of the

111
00:06:06,411 --> 00:06:08,030
image.
Once we have this,

112
00:06:08,031 --> 00:06:11,930
we can define their style loss as the
Euclidean distance between the gram

113
00:06:11,931 --> 00:06:15,980
matrices for the reference image and
output image and will compute the total

114
00:06:15,981 --> 00:06:20,060
style loss as a weighted sum of the
style loss at each layer we choose.

115
00:06:20,270 --> 00:06:21,590
It turns out that for style,

116
00:06:21,710 --> 00:06:25,880
using just a single layer like we did for
content loss doesn't get great results,

117
00:06:26,120 --> 00:06:27,460
but when using several layers

118
00:06:27,630 --> 00:06:32,450
results improve.
But it's good content and style from

119
00:06:34,470 --> 00:06:36,620
Susan.

120
00:06:43,740 --> 00:06:44,880
Now that we have our losses,

121
00:06:44,881 --> 00:06:49,500
we need to define gradients of the
output image with respect to the loss and

122
00:06:49,501 --> 00:06:53,610
then use those gradients to iteratively
improve our output image to minimize a

123
00:06:53,611 --> 00:06:54,150
loss.

124
00:06:54,150 --> 00:06:57,840
So we'll calculate the derivative of our
loss with respect to the activations in

125
00:06:57,841 --> 00:07:01,950
a given layer to get our gradients and
use them to update our output image,

126
00:07:02,190 --> 00:07:04,020
not our weight,
like we usually would.

127
00:07:04,230 --> 00:07:08,730
Gradients give us a direction on how to
update our output image such that the

128
00:07:08,731 --> 00:07:12,420
difference between the base image
and the style image becomes smaller.

129
00:07:12,510 --> 00:07:15,150
We can call our help classes
combination lost function,

130
00:07:15,450 --> 00:07:18,000
giving it the model and the
output image as perimeters,

131
00:07:18,150 --> 00:07:20,460
so we'll combine the
loss functions into one.

132
00:07:20,640 --> 00:07:24,360
Then get the gradient of the output
image with regard to the loss using the

133
00:07:24,361 --> 00:07:28,980
gradients function of chaos,
which translates to TF dot
gradients under the hood.

134
00:07:29,370 --> 00:07:33,510
This gives the symbolic radiant of one
tensor with respect to one or more other

135
00:07:33,511 --> 00:07:34,344
tensors.

136
00:07:34,440 --> 00:07:39,090
Next we'll run our optimization algorithm
called El bfgs over the pixels of our

137
00:07:39,091 --> 00:07:42,280
output image to minimize this loss,
which is very similar to this,

138
00:07:42,290 --> 00:07:44,790
the Cassick gradient descent,
but quicker to converge.

139
00:07:45,090 --> 00:07:46,320
We'll feed our minimizer function,

140
00:07:46,321 --> 00:07:49,500
the gradients we calculated and
it will output the result image.

141
00:07:49,860 --> 00:07:54,270
Let's see what this looks like.
Dope. I'm going to submit this to Gq.

142
00:07:54,450 --> 00:07:56,430
There are mobile apps
that do this as well.

143
00:07:56,431 --> 00:07:59,370
Prisma lets you pick filters on
your mobile device and our tea.

144
00:07:59,371 --> 00:08:01,860
Stowe even lets you
apply filters to video.

145
00:08:02,250 --> 00:08:05,140
We're still in the early stages of
using machine learning to create art.

146
00:08:05,141 --> 00:08:08,640
So there's a lot of opportunity in
this space. So to break it down,

147
00:08:08,641 --> 00:08:13,090
convolutional neural nets allow us to
transfer the style of any given image on

148
00:08:13,140 --> 00:08:17,580
to another. To do this will compute
loss functions for style and content.

149
00:08:17,581 --> 00:08:21,960
Using outputs from hidden layers we choose
and we can minimize our loss using an

150
00:08:22,020 --> 00:08:26,730
optimization scheme similar to the cast
gradient descent called El bfgs and of

151
00:08:26,731 --> 00:08:29,610
the coding challenge from last
week, is it tied? Of course key.

152
00:08:29,720 --> 00:08:33,960
Can you several features to model Google
stock data and artfully used rms prop

153
00:08:33,961 --> 00:08:37,930
as his optimization technique wizard of
the week and the runner up is Andreas

154
00:08:37,940 --> 00:08:39,330
wine.
Zach loved your plot.

155
00:08:39,480 --> 00:08:43,530
The coding challenge for this video is
to apply style transfer that combines a

156
00:08:43,531 --> 00:08:46,110
base image with two
different style images.

157
00:08:46,350 --> 00:08:49,440
Poster gave a blink in the comments and
I'll announce the winner. Next video,

158
00:08:49,710 --> 00:08:52,710
please subscribe for more videos like
this, check out this related video.

159
00:08:52,920 --> 00:08:56,460
And for now I've got to minimize
my losses. So thanks for watching.

