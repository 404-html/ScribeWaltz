1
00:00:04,610 --> 00:00:05,710
Oh,
a world,

2
00:00:05,711 --> 00:00:09,340
it's Suraj and today we're going to
code up a three layer recurrent neural

3
00:00:09,341 --> 00:00:13,240
network that can predict the sum of
two binary numbers in under 80 lines of

4
00:00:13,241 --> 00:00:16,840
python. But first I want to show
you guys what a genius I am.

5
00:00:16,900 --> 00:00:20,000
I'm going to recite the
alphabet backwards. Ready? Okay.

6
00:00:24,130 --> 00:00:28,440
Okay. Z Y for dammit.

7
00:00:28,600 --> 00:00:32,950
The reason it's so hard to repeat the
alphabet backwards or sing songs backwards

8
00:00:33,010 --> 00:00:36,910
is because we learned these concepts as
a sequence and when data is represented

9
00:00:36,911 --> 00:00:39,430
as a sequence positional memory matters.

10
00:00:39,450 --> 00:00:42,910
So let's say we wanted to train a neural
net on a video of a top spinning on a

11
00:00:42,911 --> 00:00:45,760
table. Each data point would
be a frame of the video.

12
00:00:45,850 --> 00:00:48,760
So if we want it to predict where
the top would be in the next frame,

13
00:00:48,940 --> 00:00:52,510
it would be super helpful to know
where the top was in the last frame.

14
00:00:52,570 --> 00:00:55,990
Sequential data like this is why
we built recurrent neural networks.

15
00:00:56,020 --> 00:01:00,130
Plain old neural nets expect fixed size
inputs and half fixed size outputs to

16
00:01:00,131 --> 00:01:02,950
the state of the hidden layer
is based only on input data.

17
00:01:03,070 --> 00:01:06,930
As it all flows in one direction or
feet forward in a recurrent neck.

18
00:01:06,940 --> 00:01:11,230
We incorporate the concept of memory in
order to do this at each time step we

19
00:01:11,231 --> 00:01:14,950
input a combination of the input data and
the hidden layer at the previous time.

20
00:01:14,951 --> 00:01:16,060
Step recursively.

21
00:01:16,120 --> 00:01:19,900
So if we try to predict the next word
in a song that contained phrases like I

22
00:01:19,901 --> 00:01:23,750
like eggs and I like toast,
brilliant songwriting,

23
00:01:23,830 --> 00:01:26,620
I know and our network tried
to predict the next word.

24
00:01:26,770 --> 00:01:28,180
How would it know what follows?

25
00:01:28,181 --> 00:01:32,260
I like it needs to know what part of the
song is in and hidden recurrence helps

26
00:01:32,261 --> 00:01:33,094
it do that.

27
00:01:34,810 --> 00:01:38,450
We're just fusing copy to help us
copy things and num Pi to do math.

28
00:01:38,510 --> 00:01:40,640
We're going to start out by
writing a sigmoid function.

29
00:01:40,670 --> 00:01:42,590
This function will help us map any value.

30
00:01:42,591 --> 00:01:44,750
We feed it into a value
between zero and one.

31
00:01:44,780 --> 00:01:47,990
It helps convert numbers to probabilities
will also write a function that

32
00:01:47,991 --> 00:01:50,060
generates the derivative of the sigmoid.

33
00:01:50,120 --> 00:01:53,060
The derivative is the slope
of a sigmoid at a given point.

34
00:01:53,090 --> 00:01:56,750
The derivative will help us to calculate
our error during training. Later on.

35
00:01:56,810 --> 00:02:00,470
Next we're going to create a lookup
table that maps integers to their binary

36
00:02:00,471 --> 00:02:01,550
representations.

37
00:02:01,640 --> 00:02:05,360
We'll initialize it as empty and set a
max length of the binary numbers we'll be

38
00:02:05,361 --> 00:02:05,780
adding.

39
00:02:05,780 --> 00:02:09,470
Then will compute the largest number of
possible to represent with the binary

40
00:02:09,471 --> 00:02:10,700
link we chose eight.

41
00:02:10,970 --> 00:02:14,810
Our lookup table is then
filled with binary numbers
so that we can easily return

42
00:02:14,811 --> 00:02:17,390
the binary value of any integer we input.

43
00:02:17,480 --> 00:02:19,670
Now it's time to initialize
our input variables.

44
00:02:19,730 --> 00:02:22,400
First we'll set our learning rate,
then our input dimensions.

45
00:02:22,430 --> 00:02:26,300
Since we will be adding two
numbers together, we'll be
feeding into bits strings,

46
00:02:26,360 --> 00:02:29,960
one character at a time, so we need
to have two inputs to the network.

47
00:02:29,990 --> 00:02:32,180
One for each of the numbers being added,
hidden,

48
00:02:32,181 --> 00:02:35,600
dim defines the size of our hidden layer
that we'll be storing our carry bit.

49
00:02:35,630 --> 00:02:39,080
And lastly, we define our output
size. Since we're only predicting one,

50
00:02:39,081 --> 00:02:42,440
some will say to one well initialize
three matrices of synapses.

51
00:02:42,560 --> 00:02:45,950
The first synapse matrix connects
our input layer to our hidden layer,

52
00:02:46,070 --> 00:02:48,380
so it has two rows and 16 columns.

53
00:02:48,440 --> 00:02:51,500
The next thing APP's Matrix connects
are hidden layer to our output layer,

54
00:02:51,590 --> 00:02:53,480
so it has 16 rows and one column.

55
00:02:53,570 --> 00:02:57,110
The last snaps connects are hidden
layer and the previous time step to the

56
00:02:57,111 --> 00:02:58,850
hidden layer in the current time step,

57
00:02:58,880 --> 00:03:01,840
it connects the hidden layer
in the current time. Step two,

58
00:03:01,841 --> 00:03:05,110
the hidden layer in the next time
step. Well, just to keep on using it,

59
00:03:05,140 --> 00:03:07,360
so 16 rows and 16 columns.
Next,

60
00:03:07,361 --> 00:03:10,750
we need three variables to store the
synapse updates for each of the matrices.

61
00:03:10,780 --> 00:03:13,900
Once we have enough sinaps updates will
update the matrix seats. All right,

62
00:03:13,930 --> 00:03:18,130
let's start training it to kill humans.
I mean, uh, predict binary songs. Yeah,

63
00:03:18,370 --> 00:03:20,830
we're going to iterate over
10,000 training samples.

64
00:03:20,890 --> 00:03:23,140
We'll start by generating
some random addition problem.

65
00:03:23,230 --> 00:03:27,220
We'll initialize our first variable to
use our lookup table to find the binary

66
00:03:27,221 --> 00:03:30,520
form of it and story in a, then
we'll do the same thing for B. Well,

67
00:03:30,521 --> 00:03:34,270
some both integers and get the binary
and coding for the some see what this

68
00:03:34,271 --> 00:03:37,270
store that neural networks
predictions and an empty binary array,

69
00:03:37,300 --> 00:03:40,930
which we'll call d will reset the
air measure to zero every time.

70
00:03:40,960 --> 00:03:44,500
This will help us see how each new bit
of data contributes to our predictions.

71
00:03:44,530 --> 00:03:46,210
We then initialize two lists.

72
00:03:46,240 --> 00:03:49,480
They will keep track of the layer two
derivatives and layer one values at each

73
00:03:49,481 --> 00:03:52,720
time step and since time step
zero has no previous hidden layer,

74
00:03:52,750 --> 00:03:54,340
we initialize one that's off.
Okay.

75
00:03:54,341 --> 00:03:57,520
Now we're ready to iterate through the
binary representation or generate an

76
00:03:57,521 --> 00:04:00,790
input first. This is a list of two
numbers, one for May and one from B.

77
00:04:00,850 --> 00:04:04,810
Y is the output and is the value of
the correct answer, either one or zero.

78
00:04:04,870 --> 00:04:07,480
Now it's time for the real
magic step of neural networks.

79
00:04:07,481 --> 00:04:10,690
We construct our hidden layer by
propagating our input to the hidden layer.

80
00:04:10,720 --> 00:04:14,500
Then we propagate from the previous
hidden layer to the current hidden layer.

81
00:04:14,530 --> 00:04:18,850
We some both of these two vectors and
passed them through the sigmoid function.

82
00:04:18,910 --> 00:04:21,970
After both the previous hidden layer
and input have been propagated through

83
00:04:21,971 --> 00:04:23,140
their various matrices.

84
00:04:23,170 --> 00:04:26,020
We some the information well
then construct our output layer.

85
00:04:26,050 --> 00:04:29,410
It propagates the hidden layer to the
output. In order to make a prediction,

86
00:04:29,440 --> 00:04:32,260
we can't forget to compute how
much the prediction missed.

87
00:04:32,290 --> 00:04:35,980
We'll store the derivative analyst
holding the derivative at each time step.

88
00:04:36,010 --> 00:04:40,150
Then we'll calculate the sum of the
absolute errors to track propagation.

89
00:04:40,180 --> 00:04:43,090
We'll end up with a some of the
error at each binary position.

90
00:04:43,120 --> 00:04:46,480
We'll then need to round the output
to a binary value in storage in the

91
00:04:46,481 --> 00:04:47,390
designated slot.

92
00:04:47,950 --> 00:04:51,580
We copy the layer one value into an array
so that at the next time that we can

93
00:04:51,581 --> 00:04:53,560
apply the hidden layer at the current one.
Okay,

94
00:04:53,561 --> 00:04:56,080
so we've done the forward
propagation for all time steps.

95
00:04:56,110 --> 00:04:59,560
We've computed derivatives at the
upper layers and stored them in a list.

96
00:04:59,590 --> 00:05:01,300
Now it's time to back propagate.

97
00:05:01,301 --> 00:05:04,360
Starting with the last time stepped
and propagating to the first.

98
00:05:04,390 --> 00:05:06,430
We indexed the input
data just like before,

99
00:05:06,490 --> 00:05:09,640
and select the current hidden layer from
the list and the previous hidden layer.

100
00:05:09,670 --> 00:05:10,930
Then the current output layer,

101
00:05:10,960 --> 00:05:13,360
it's time to compute the
current hidden layer error.

102
00:05:13,390 --> 00:05:17,020
Given the error at the hidden layer from
the future and the error at the current

103
00:05:17,050 --> 00:05:18,220
output layer.
Yay.

104
00:05:18,221 --> 00:05:21,580
All of our derivatives have been back
propagated at this current time step,

105
00:05:21,610 --> 00:05:25,480
so we can construct our synapse updates.
We're not actually updating them yet.

106
00:05:25,481 --> 00:05:26,980
Those ones back prop is done.

107
00:05:27,010 --> 00:05:29,200
Then we update our synapses
using the learning rate.

108
00:05:29,201 --> 00:05:32,680
We initialized then MTV update variables.
Let's see some results.

109
00:05:32,740 --> 00:05:35,260
The predicted value is
pretty off the mark at first,

110
00:05:35,261 --> 00:05:36,970
but with each iteration gets better.

111
00:05:37,000 --> 00:05:38,950
If you want to learn more
about recurrent neural nets,

112
00:05:38,980 --> 00:05:41,500
check out the description for links
and hit that subscribe button.

113
00:05:41,501 --> 00:05:44,770
For more programming videos. For now, I've
got to go synchronize them. Go routines,

114
00:05:44,800 --> 00:05:45,760
so thanks for watching.

