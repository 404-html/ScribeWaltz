1
00:00:00,150 --> 00:00:00,900
Hello world.

2
00:00:00,900 --> 00:00:05,900
It's a Raj and my demo today is landing
this space x rocket in a simulation

3
00:00:06,870 --> 00:00:07,950
using machine learning.

4
00:00:08,160 --> 00:00:12,120
This thing has learned how to properly
land using all of those gravitational

5
00:00:12,121 --> 00:00:15,870
Constance balancing. It's weights,
velocity, physics, all these things,

6
00:00:16,050 --> 00:00:19,230
but the key here is machine learning.
It's not rocket science.

7
00:00:19,470 --> 00:00:21,180
It's machine learning,
which is even better.

8
00:00:21,510 --> 00:00:23,970
So that's what we're going to do today.
I'm going to code this out.

9
00:00:24,000 --> 00:00:26,910
It's going to be about
150 lines of python.

10
00:00:27,090 --> 00:00:30,840
I'm going to be using the tensorflow
libraries, the tensorflow library,

11
00:00:30,841 --> 00:00:34,140
and the open Ai Jim Library.
But before I get there,

12
00:00:34,230 --> 00:00:38,280
we've got to learn a bit
about reinforcement learning
as a bit of background

13
00:00:38,281 --> 00:00:42,150
before we get to the technique that
it uses, which is called stay with me,

14
00:00:42,360 --> 00:00:44,640
proximal policy optimization.

15
00:00:44,641 --> 00:00:46,710
I know that's a mouthful and
we're going to get there,

16
00:00:46,711 --> 00:00:50,250
but first we've got to
learn some background on
reinforcement learning. Okay,

17
00:00:50,251 --> 00:00:53,520
so open Ai. It was started
by Elan Musk, Sam Altman,

18
00:00:53,550 --> 00:00:56,730
a bunch of really prominent people in the,
in the tech space.

19
00:00:56,850 --> 00:01:00,450
But the goal was to build safe
AI in a way that was distributed,

20
00:01:00,451 --> 00:01:05,451
that was open so everybody had access to
it in order to ensure a more beneficial

21
00:01:05,461 --> 00:01:06,540
future for humanity.

22
00:01:06,810 --> 00:01:10,800
And one of the major contributions
that open AI has made to the world is

23
00:01:10,801 --> 00:01:14,530
releasing the gym environment.
So Jim is this really cool, uh,

24
00:01:14,550 --> 00:01:19,350
library on get hub that allows
anybody to train their AI algorithms,

25
00:01:19,640 --> 00:01:22,920
uh, in a bunch of different
environments. It's very cool.

26
00:01:23,040 --> 00:01:26,100
A Universe was kind of
their next, um, environment,

27
00:01:26,130 --> 00:01:28,890
which they hired me to make
a video on back in the day.

28
00:01:29,070 --> 00:01:32,490
And then Ilan retweeted it.
It was awesome. But anyway,
that's a tangent. But Jim,

29
00:01:32,520 --> 00:01:37,200
Jim is kind of what has stayed in the
mainstream university and really take off

30
00:01:37,201 --> 00:01:40,530
for whatever reason. But, but Jim is still
there. It's, it's, it's still very active.

31
00:01:40,531 --> 00:01:42,930
People are still using it
and a bunch of examples,

32
00:01:43,080 --> 00:01:46,410
you can find it on get hub and they've
got a bunch of different environments

33
00:01:46,411 --> 00:01:51,411
from Atari Games to three d simulations
using moo Joko at toy tax class control

34
00:01:51,660 --> 00:01:56,400
a bunch of different examples, right?
So it's a collection of environments.

35
00:01:56,401 --> 00:01:59,370
It's all in python and it's
really easy to use. The,

36
00:01:59,400 --> 00:02:02,640
the whole point of it is so that you
don't have to focus on building the

37
00:02:02,641 --> 00:02:05,100
environment.
You can just focus on the algorithm.

38
00:02:05,220 --> 00:02:07,830
And they had this leaderboard
as well on the website,

39
00:02:07,831 --> 00:02:10,230
but that they took that
down for whatever reason.

40
00:02:10,440 --> 00:02:14,460
But you can still find a leaderboard
that compares the strength of different

41
00:02:14,461 --> 00:02:19,080
algorithms on, on get hub. And it's
in this, uh, Jim Wikipedia page.

42
00:02:19,290 --> 00:02:23,010
It was updated 25 days ago, but,
uh, I would still make a PR.

43
00:02:23,010 --> 00:02:25,170
If you have some kind of
update and they're gonna,

44
00:02:25,200 --> 00:02:27,780
they're likely going to accept
that, that request. Okay.

45
00:02:27,781 --> 00:02:32,580
So let's start with reinforcement
learning. So this is right.

46
00:02:32,581 --> 00:02:36,270
So there are different ways that we can
class the learning experience, right?

47
00:02:36,271 --> 00:02:40,800
Supervised unsupervised reinforcement
online, offline. You know,

48
00:02:41,460 --> 00:02:43,950
there's, there's a million different
ways that we can class learning,

49
00:02:43,951 --> 00:02:47,400
but anytime that we have some
kind of time element, right?

50
00:02:47,401 --> 00:02:49,320
So when we add the element of time,

51
00:02:49,590 --> 00:02:51,540
which is where an
environment comes into play,

52
00:02:51,750 --> 00:02:55,680
that's when we can use reinforcement
learning. It's a computational approach.

53
00:02:55,770 --> 00:02:56,460
We're an agent,

54
00:02:56,460 --> 00:03:01,240
interacts with an environment by taking
actions where it tries to maximize a

55
00:03:01,241 --> 00:03:05,170
reward. So this is a very
simple way of visualizing this.

56
00:03:05,171 --> 00:03:09,010
We have an agent that is a Bot, an
AI, a person, an animal, a human.

57
00:03:09,011 --> 00:03:11,620
Anything that takes an
action in the environment.

58
00:03:11,850 --> 00:03:13,390
If it's learning how to ride a bike,

59
00:03:13,420 --> 00:03:15,880
that action would be to
push down on the pedal.

60
00:03:16,210 --> 00:03:20,350
The environment will then update by
giving it a reward, right? So if, if I,

61
00:03:20,351 --> 00:03:23,500
if I push successfully,
then my muscle won't have any pain.

62
00:03:23,501 --> 00:03:25,870
So my brain is getting that reward back,
right?

63
00:03:25,871 --> 00:03:30,470
And I've updated the states
in my environment, my, my
foot is now lower and this,

64
00:03:30,471 --> 00:03:32,410
this just repeats over and over again.

65
00:03:32,620 --> 00:03:35,080
But if I push in the wrong
direction or my, you know,

66
00:03:35,081 --> 00:03:37,750
my foot goes just crazy
in a different direction,

67
00:03:37,900 --> 00:03:41,560
then I'm going to get a negative reward
and I want to avoid that negative reward.

68
00:03:42,700 --> 00:03:47,470
So a much more formal mathematical way
of describing this agent environment loop

69
00:03:47,680 --> 00:03:50,920
is to describe it as what's called
a mark Hoff decision process.

70
00:03:51,100 --> 00:03:53,830
This is based off of Andre
Markoff and early mathematicians.

71
00:03:53,950 --> 00:03:55,720
Initial principle to mark off property,

72
00:03:55,960 --> 00:04:00,130
but essentially it's a way of estimating
probabilities and how they change over

73
00:04:00,131 --> 00:04:03,160
time in an environment.
So we have a state,

74
00:04:03,420 --> 00:04:08,300
a state is how you, an agent is
currently in a game. Like what,

75
00:04:08,310 --> 00:04:11,940
what's its position, what's
its move, what's it's uh, the,

76
00:04:11,950 --> 00:04:16,240
the way it's positioned in relation
to other objects in the environment.

77
00:04:16,540 --> 00:04:18,520
And then we have an action.
This could be, you know,

78
00:04:18,730 --> 00:04:22,420
punching a wall or hitting a ping
pong ball or jumping up and down.

79
00:04:22,421 --> 00:04:24,010
It's something that the agent does.

80
00:04:24,340 --> 00:04:28,450
We have a transition probability that
that relates the states to the action.

81
00:04:28,720 --> 00:04:32,800
We have a reward, which is what the
agent receives if the action was correct.

82
00:04:32,980 --> 00:04:36,310
So by correct I'm saying how
does it affect the objective?

83
00:04:36,311 --> 00:04:38,440
What is the objective of the game to win,
to lose?

84
00:04:38,530 --> 00:04:41,650
How does it affect it if it's uh,
if it's a winning move,

85
00:04:41,680 --> 00:04:45,100
if it's a winning action, then the
reward will be something like a plus one.

86
00:04:45,130 --> 00:04:46,480
If it's not,
it's a minus one.

87
00:04:47,680 --> 00:04:50,890
And there are different quantities
that are involved here, like policies,

88
00:04:50,891 --> 00:04:55,120
utilities, Q values, and that's there.
There's a lot that we can go into.

89
00:04:55,121 --> 00:04:58,990
There's so many different ways and
pathways we can go into when we're talking

90
00:04:58,991 --> 00:05:00,190
about reinforcement learning.

91
00:05:00,580 --> 00:05:04,270
But what the one that I want to
focus on is the policy and the value,

92
00:05:04,271 --> 00:05:07,900
which I'm going to go into in a second.
But before we get into that,

93
00:05:08,050 --> 00:05:10,210
let's talk about offline
and online learning.

94
00:05:10,211 --> 00:05:14,320
Remember I said there's so many different
ways that we can class though the

95
00:05:14,321 --> 00:05:18,310
different learning processes and
one way is offline versus online.

96
00:05:18,520 --> 00:05:22,750
So online just means learning in
real time as we're playing the game,

97
00:05:22,900 --> 00:05:27,900
which is what we're going to do and
offline is before or after or in between

98
00:05:28,331 --> 00:05:28,810
games.

99
00:05:28,810 --> 00:05:32,830
So it's like we've already completed a
set of actions in an environment we're

100
00:05:32,831 --> 00:05:34,960
done and now we're learning from that.

101
00:05:35,230 --> 00:05:38,710
Whereas online is during the
actual process of interacting.

102
00:05:38,920 --> 00:05:43,750
That's what we're going to do.
So to summarize a bit about Arele,

103
00:05:43,840 --> 00:05:44,710
we have an agent.

104
00:05:44,711 --> 00:05:48,560
It interacts with an environment and
it learns what's called a policy.

105
00:05:48,561 --> 00:05:51,910
So this is a very important word.
We have a state and an action.

106
00:05:52,120 --> 00:05:56,950
And a policy describes how an agent
decides how to best interact with the

107
00:05:56,960 --> 00:06:01,310
environment using its current state and
the list of potential actions it could

108
00:06:01,311 --> 00:06:05,000
take, right? So it, it looks at what's
called the state transition matrix,

109
00:06:05,150 --> 00:06:09,470
which relates all the states to all the
actions. And the policy says, what's the,

110
00:06:09,950 --> 00:06:14,950
what's the optimal action I can take
right now that will maximize my goal,

111
00:06:15,470 --> 00:06:19,790
right? How do I get to my goal in the
most efficient, optimal way possible?

112
00:06:19,940 --> 00:06:23,060
And the policy will tell the
agent exactly how to do that.

113
00:06:23,180 --> 00:06:27,170
But in addition to the policy, there's
also another feature called the value,

114
00:06:27,530 --> 00:06:29,150
what's called the value function.

115
00:06:29,210 --> 00:06:33,590
So the value function will evaluate
every single possible move and it will

116
00:06:33,591 --> 00:06:37,220
create this list of all the
possible values for every move.

117
00:06:37,670 --> 00:06:40,220
And it will choose what
that best value is.

118
00:06:40,370 --> 00:06:44,420
The policy uses the value function
to then evaluate every move.

119
00:06:44,570 --> 00:06:47,510
So the value function is saying,
this move is, this is this good,

120
00:06:47,630 --> 00:06:50,720
this movie is this bad, this movie,
this is even better. And then the,

121
00:06:50,780 --> 00:06:54,620
and then the policy will use that value
function to then pick the best move.

122
00:06:54,860 --> 00:06:57,140
So the value function
is a part of the policy.

123
00:06:57,141 --> 00:07:02,141
The policy is the higher order function
on top of the value function and this

124
00:07:02,631 --> 00:07:06,830
value function can be learned
and approximated by any
learning and approximation

125
00:07:06,831 --> 00:07:10,880
approach like in our case, a neural
network, right? Function approximation.

126
00:07:11,480 --> 00:07:14,030
So we're going to learn about you
function and it's going to help us learn a

127
00:07:14,031 --> 00:07:19,031
policy and we can broadly
categorize reinforcement
learning into three different

128
00:07:19,881 --> 00:07:23,870
groups. For our sake, right now for
approximate policy optimization,

129
00:07:24,330 --> 00:07:28,730
there is the critic only method where we
first learn about you function and then

130
00:07:28,731 --> 00:07:30,350
use that to define a policy,
right?

131
00:07:30,351 --> 00:07:34,730
We we learn a value function using a
neural network and then use that to define

132
00:07:34,731 --> 00:07:35,564
the policy.

133
00:07:35,750 --> 00:07:39,770
Then there's an actor only method where
we directly searched the policy space.

134
00:07:39,980 --> 00:07:40,940
We don't have this.

135
00:07:41,260 --> 00:07:44,810
The policy is what's evaluating
all the moves without separating.

136
00:07:44,811 --> 00:07:49,070
These modules have a value and a policy
function, you know, independently.

137
00:07:49,130 --> 00:07:50,960
And then there's an actor critic method.

138
00:07:50,961 --> 00:07:53,030
This is where we have two
different neural networks.

139
00:07:53,180 --> 00:07:57,430
One learns the value and one learns
the policy, right? So the cue,

140
00:07:57,530 --> 00:08:00,890
the critic observes the actor
and evaluates its policy,

141
00:08:01,010 --> 00:08:04,790
determining when it needs to change.
So the critical be a neural network.

142
00:08:04,791 --> 00:08:06,350
The actor will be a neural network.

143
00:08:06,710 --> 00:08:11,120
The actor will then take action in the
environment and then the state will be

144
00:08:11,121 --> 00:08:15,050
returned to both the actor and the
critics, the critical observe this,

145
00:08:15,051 --> 00:08:18,380
the new states, and then
send, it's a Q value,

146
00:08:18,381 --> 00:08:22,520
which is the quality of that action
to the actor so that it can update its

147
00:08:22,521 --> 00:08:24,200
weights.
And the process repeats.

148
00:08:24,440 --> 00:08:28,940
And what's what's been found is to have
to do is when we have two different

149
00:08:28,941 --> 00:08:30,770
neural networks,
an actor and a critic,

150
00:08:30,920 --> 00:08:34,550
this usually outperforms both
acthar only and critic only methods.

151
00:08:35,630 --> 00:08:39,620
So we're getting closer to proximal policy
optimization. Let's keep going here.

152
00:08:39,830 --> 00:08:44,120
So there's this idea of the
policy gradient approach.

153
00:08:44,270 --> 00:08:46,730
Andre Carpathia has a
great blog post on this,

154
00:08:46,731 --> 00:08:49,610
and I'll link to it in the description,
but the idea is that,

155
00:08:49,730 --> 00:08:50,750
let's say we're in a game,
right?

156
00:08:50,751 --> 00:08:54,860
Let's say we're in Pong and we're trying
to evaluate how good each move is,

157
00:08:54,861 --> 00:08:55,530
right?

158
00:08:55,530 --> 00:08:59,400
Let's say one of the moves is moving
the paddle up and then the ball hits the

159
00:08:59,401 --> 00:09:02,970
paddle and then it goes to the opponent.
Well, one, one thing you could say is,

160
00:09:02,971 --> 00:09:07,530
well, this was good, right? So returning
to reward of plus one, but the problem is,

161
00:09:07,620 --> 00:09:12,330
what if that's move was good in the short
term, but it was bad in the long term.

162
00:09:12,510 --> 00:09:16,020
What if it was better to move the
pedal down? The ball would go down,

163
00:09:16,260 --> 00:09:19,830
the agent would hit, the opponent
would hit it. You hit it again,

164
00:09:19,980 --> 00:09:23,370
and then you were let more likely to
win the game. If you would've went down,

165
00:09:23,371 --> 00:09:27,600
then you went up. If you just
updated your weights immediately,

166
00:09:27,750 --> 00:09:30,540
you wouldn't have known that you,
instead,

167
00:09:30,570 --> 00:09:34,830
you should wait until the game is finished
before evaluating how good that move

168
00:09:34,831 --> 00:09:37,920
was. And that's where the policy
gradient approach comes in.

169
00:09:38,190 --> 00:09:42,960
It waits for the game to finish for
the episode to finish before using that

170
00:09:42,961 --> 00:09:46,050
result as a gradient to
then update the weights.

171
00:09:46,950 --> 00:09:50,820
So it's kind of like supervised learning
where the label is the result, right?

172
00:09:50,821 --> 00:09:54,150
The log probability of whether or
not that action was good or bad,

173
00:09:54,300 --> 00:09:58,980
and then updating the weights. So that's
the policy gradient method. So now,

174
00:09:59,010 --> 00:10:00,360
now on top of that,

175
00:10:00,480 --> 00:10:04,890
there's something called the trust
region policy optimization method.

176
00:10:04,920 --> 00:10:07,950
This is the same idea as
the policy gradient method,

177
00:10:08,130 --> 00:10:11,490
except it doesn't stray too
far from the old policy.

178
00:10:11,670 --> 00:10:15,960
So one of the downsides of the policy
gradient method is that sometimes updates

179
00:10:15,961 --> 00:10:20,961
are way too big and this causes a lot
of noise in the loss function and it,

180
00:10:21,450 --> 00:10:25,170
and it prevents the, the network
from, from converging on the,

181
00:10:25,230 --> 00:10:30,120
on the proper optimization scheme.
So what the trp Oh method does is it says,

182
00:10:30,330 --> 00:10:32,760
let's not stray too far
from the old policy.

183
00:10:32,850 --> 00:10:36,660
Stay near to the old policy making
too large a change is a bad thing.

184
00:10:36,661 --> 00:10:38,130
So how do we do this?
Well,

185
00:10:38,131 --> 00:10:41,490
a naive solution is to make
a very small step every time.

186
00:10:41,730 --> 00:10:43,560
But then how do you take small steps?

187
00:10:43,830 --> 00:10:48,810
What trp Oh does is it controls the
rate of policy change using a constraint

188
00:10:48,960 --> 00:10:53,010
then then comes PPO, right?
So Ppo proximate policy,

189
00:10:53,011 --> 00:10:56,760
implement optimization is an
implementation of trp. Oh,

190
00:10:56,970 --> 00:11:01,320
that adds what's called a KL divergence
term to training the loss function.

191
00:11:01,680 --> 00:11:02,760
And so there's a,
there's a,

192
00:11:02,790 --> 00:11:06,270
there's a KL divergence term
that trains the loss function,

193
00:11:06,480 --> 00:11:10,350
and there's also one that updates
the old policies. So check this out.

194
00:11:10,500 --> 00:11:13,560
Here's what the graph looks like for what
we're going to build in what's called

195
00:11:13,561 --> 00:11:17,730
tensor born. We have a po, we haven't
fault, we have a policy function,

196
00:11:18,000 --> 00:11:21,720
we have a old policy function,
we have an, we have a critic,

197
00:11:21,930 --> 00:11:26,100
and the actor or the actor is going
to be this policy function, okay?

198
00:11:27,180 --> 00:11:29,370
This is gonna make a lot more
sense when I start coding it,

199
00:11:29,580 --> 00:11:32,820
but just recognize that we have
what's called an actor, a critic,

200
00:11:33,120 --> 00:11:36,780
and this is an implementation of what's
called proximate policy optimization.

201
00:11:36,890 --> 00:11:38,940
All right,
so let's get to the code.

202
00:11:39,990 --> 00:11:43,650
So the first thing we're going to do is
we're going to import tensorflow, right?

203
00:11:43,650 --> 00:11:46,690
These are our,
this is our machine learning library,

204
00:11:46,691 --> 00:11:47,880
and we're going to import gym.

205
00:11:48,180 --> 00:11:52,400
And then we're going to go ahead and
create this PPO class, right? This,

206
00:11:52,410 --> 00:11:56,830
this is where our main object is going
to be that we're going to use later on.

207
00:11:58,240 --> 00:12:03,240
Now we're going to initialize it and
it via our python in it function.

208
00:12:03,581 --> 00:12:06,730
Go Python. Now, who's ready for this?

209
00:12:06,880 --> 00:12:10,200
The first thing we're going to do is
initialize our tensor flow session as a TF

210
00:12:10,210 --> 00:12:14,200
dot session, right? This allows us
to execute graphs or part of graphs.

211
00:12:14,410 --> 00:12:16,260
This is the tensor flow
graph, right? The, the,

212
00:12:16,270 --> 00:12:21,270
the main part of our pipeline that we're
going to place all of our objects into.

213
00:12:22,600 --> 00:12:24,580
Then we're going to create a place holder.

214
00:12:24,581 --> 00:12:28,510
This is just a variable that we will
later assign data too at some point.

215
00:12:28,511 --> 00:12:33,390
But right now it's going to be a float
32 object, um, the size of the tensor,

216
00:12:33,391 --> 00:12:36,370
or he's going to be none.
Give it some hyper parameter that I've,

217
00:12:36,520 --> 00:12:41,290
that I'm going to define later, and then
it's going to be called the state. Okay?

218
00:12:41,800 --> 00:12:44,440
Now we're ready to define our critic.
Okay,

219
00:12:44,441 --> 00:12:46,360
so this critic is going
to be a neural network.

220
00:12:46,361 --> 00:12:49,300
So we're gonna use
tensorflow is variable scope.

221
00:12:52,840 --> 00:12:57,720
So the variable scope keyword is it allows
us to create new variables and to to

222
00:12:57,790 --> 00:12:59,080
share already created ones.

223
00:12:59,710 --> 00:13:03,160
So this is just a way of organizing
our code of a a bit better.

224
00:13:03,460 --> 00:13:04,810
So the first layer of our network,

225
00:13:04,811 --> 00:13:08,920
we're going to use the layers sub module
and it's going to be a dense layer

226
00:13:08,921 --> 00:13:12,100
because every single note in the first
layer is going to be connected to the

227
00:13:12,101 --> 00:13:15,760
next layer. We're going to use that.
That's a placeholder as a data entry.

228
00:13:15,761 --> 00:13:20,370
It's a gateway for data into the network.
And then we're going to say the uh,

229
00:13:21,100 --> 00:13:23,050
nonlinearity is going to
be called [inaudible],

230
00:13:23,051 --> 00:13:27,190
which is a very popular nonlinear function
that makes sure our network can learn

231
00:13:27,191 --> 00:13:29,140
both linear and nonlinear functions.
Remember,

232
00:13:29,520 --> 00:13:33,710
neural networks are universal
function approximators. Now, well,

233
00:13:33,910 --> 00:13:38,540
let's create our value, right?
So the value is going to be, uh,

234
00:13:38,650 --> 00:13:43,650
this dense layer and it's going to be
the output of the first layer is going to

235
00:13:44,381 --> 00:13:46,450
be fed into this value variable.

236
00:13:47,290 --> 00:13:49,510
Now we're going to create
a discounted reward.

237
00:13:51,380 --> 00:13:54,800
This is a reward in the future that's
not worth quite as much as the reward

238
00:13:54,801 --> 00:13:58,310
right now. And we'll say, okay, this
is two is going to be a place holder.

239
00:13:59,980 --> 00:14:00,450
Okay.

240
00:14:00,450 --> 00:14:01,780
TF,
Daf float,

241
00:14:05,910 --> 00:14:06,743
32

242
00:14:08,730 --> 00:14:09,541
the size,

243
00:14:09,541 --> 00:14:13,530
we'll keep it small and we're going to
call it discounted reward to keep things,

244
00:14:14,910 --> 00:14:19,530
keep things, um, organized.

245
00:14:19,980 --> 00:14:21,540
And I'll call it discounted reward.

246
00:14:24,220 --> 00:14:26,100
Now,
uh,

247
00:14:26,300 --> 00:14:29,110
all right, so now the advantage, this
is the next variable we're going to use.

248
00:14:29,111 --> 00:14:31,900
It's called the advantage.
So we'll say the ta,

249
00:14:31,930 --> 00:14:36,070
the advantage is going to
be the difference between
the discounter to award and

250
00:14:36,071 --> 00:14:36,904
the value.

251
00:14:37,210 --> 00:14:40,630
And so the basically this estimates
rather than just discounted returns,

252
00:14:40,650 --> 00:14:44,620
it allows the agent to determine not
just how good a its actions were,

253
00:14:44,800 --> 00:14:47,620
but how much better they
turned out to be than expected.

254
00:14:47,890 --> 00:14:50,080
And this should be a minus,
not an equals. There we go.

255
00:14:52,380 --> 00:14:57,210
Now for our loss function.
So our loss function is going to,

256
00:14:57,500 --> 00:14:59,400
uh, minimize that
advantage over time, right?

257
00:14:59,401 --> 00:15:03,620
We want to make sure that advantage gets
smaller and smaller. So our expected, uh,

258
00:15:03,710 --> 00:15:06,810
action is going to be closer
and closer to what the real,

259
00:15:06,811 --> 00:15:10,740
the optimal action should be.
And it's the mean squared error.

260
00:15:10,741 --> 00:15:13,320
That's why we're using these
functions right here. And lastly,

261
00:15:13,321 --> 00:15:15,930
we're going to make sure to use
gradient descent because wine,

262
00:15:15,960 --> 00:15:20,960
because grading dissent is
the most popular optimization
strategy out there for

263
00:15:21,121 --> 00:15:24,540
neural networks. And one
implementation of that is called Adam.

264
00:15:25,130 --> 00:15:28,880
And I've got a great video on Adam.
If you just want to check out, um,

265
00:15:29,550 --> 00:15:32,940
what's it called?
Which activation function should you use?

266
00:15:32,941 --> 00:15:36,270
I talk about the difference between
Adam and nesteroff momentum and so many

267
00:15:36,271 --> 00:15:40,950
different amazing activation functions
using that loss function as his parameter.

268
00:15:41,790 --> 00:15:42,623
Okay.

269
00:15:43,300 --> 00:15:43,930
Yeah.

270
00:15:43,930 --> 00:15:48,580
So that's our critic.
We've got our critic, uh,
everybody's a critic these days,

271
00:15:48,581 --> 00:15:52,990
right? So now we're going to go
into our actor, the actor himself,

272
00:15:54,190 --> 00:15:56,200
Nicolas Cage,
because he's on the Internet.

273
00:15:56,590 --> 00:15:59,680
So the actor is going to be our
policy network by the way. Okay.

274
00:15:59,681 --> 00:16:03,700
So the actor's going to be our policy
network and our critic is actually our

275
00:16:03,701 --> 00:16:07,300
value network. I forgot to mention that
part. So the critic is the value network.

276
00:16:07,540 --> 00:16:09,360
And the actor's going to
be the policy networks.

277
00:16:09,361 --> 00:16:13,210
So we separate these two concepts
into a different, different modules.

278
00:16:14,290 --> 00:16:18,850
So for our actor, we're going to say,
okay, it's going to return the, uh,

279
00:16:18,940 --> 00:16:22,180
normal distribution of actions
and then the trainable parameters.

280
00:16:22,480 --> 00:16:24,980
But I'm going to go ahead and have a,
uh,

281
00:16:25,090 --> 00:16:29,710
simple helper function for this to
build a very similar neural network.

282
00:16:29,950 --> 00:16:33,550
I'll call it pie. Uh, and then
I'll say yes, it is trainable,

283
00:16:35,290 --> 00:16:38,920
was very similar to one layer neural
network, which I can define later.

284
00:16:39,310 --> 00:16:43,630
And so now what? Remember, we want
to have two different policies.

285
00:16:43,631 --> 00:16:48,460
We have an old policy, and then we
have a newer policy. So I'll say,

286
00:16:49,090 --> 00:16:54,090
let's have the same exact outputs
and we'll use the same exact,

287
00:16:54,790 --> 00:16:57,700
uh,
help her function,

288
00:16:57,970 --> 00:17:00,100
except we'll call this one the old policy.

289
00:17:00,280 --> 00:17:02,050
And then this one's not
going to be trainable

290
00:17:08,120 --> 00:17:12,680
now for our, uh, for us
to sample our actions.

291
00:17:12,681 --> 00:17:16,880
Now the next thing we want to do is we
want to sample our actions from both the

292
00:17:16,881 --> 00:17:20,240
old and the new policy networks

293
00:17:22,460 --> 00:17:26,540
so we can compare them. So again, we're
gonna use that variable scope, uh,

294
00:17:26,630 --> 00:17:29,030
function to then sample actions.

295
00:17:30,260 --> 00:17:31,093
Okay.

296
00:17:32,280 --> 00:17:36,780
And we say, we're going to say, okay,
so the sample operation is going to use,

297
00:17:37,250 --> 00:17:40,440
uh,
this TF dot squeeze function.

298
00:17:41,220 --> 00:17:43,050
And then we're going to sample from it.

299
00:17:45,040 --> 00:17:45,430
Okay.

300
00:17:45,430 --> 00:17:49,240
And the access is going to be zero
because we don't want multiple dimensions.

301
00:17:49,350 --> 00:17:53,550
And this is basically just going to choose
an action from, from our distribution,

302
00:17:53,580 --> 00:17:57,450
right? So it's going to output
a distribution of possible
actions we could take.

303
00:17:57,570 --> 00:18:01,350
That's what the neural net
learns for the Policy Network.

304
00:18:01,440 --> 00:18:03,450
And then we're going to just
choose an action from that.

305
00:18:06,380 --> 00:18:09,560
And then we're gonna say, okay, so
that's, that was a sample action.

306
00:18:09,770 --> 00:18:12,680
And now we want to have a,
another variable scope.

307
00:18:12,710 --> 00:18:16,460
We'll call this one the
update the old policy.

308
00:18:16,910 --> 00:18:18,530
This one's going to update the old policy.

309
00:18:18,650 --> 00:18:20,520
So then we'll just call it update old P,
I.

310
00:18:22,910 --> 00:18:23,743
Okay.

311
00:18:23,770 --> 00:18:26,140
And we'll say, okay, update old policy

312
00:18:28,270 --> 00:18:33,130
cold p. Dot. Assign p four,

313
00:18:33,131 --> 00:18:37,870
p, O p and zip. And then we
have those, the new policies,

314
00:18:37,871 --> 00:18:38,704
parameters.

315
00:18:43,540 --> 00:18:44,340
Okay.

316
00:18:44,340 --> 00:18:48,660
No. Okay. So that's it for our s for
us sampling from the actions from both.

317
00:18:48,900 --> 00:18:51,450
Now we can, uh, create two placeholders.

318
00:18:51,660 --> 00:18:54,840
So these are going to be placeholders
for the action and the advantage.

319
00:18:54,870 --> 00:18:58,740
And we're gonna use these to help us
compute our loss functions in a second.

320
00:18:58,950 --> 00:18:59,730
So we'll say,
okay,

321
00:18:59,730 --> 00:19:04,730
so the action stuff that Tfa
is going to be a place holder,

322
00:19:06,570 --> 00:19:10,580
not a hotline or a holder. This
is not bitcoin. A placeholder,

323
00:19:10,610 --> 00:19:12,180
TF dot float 32.

324
00:19:13,350 --> 00:19:17,640
It's going to be very similar size and
it's going to be called the action.

325
00:19:21,420 --> 00:19:24,450
And I'll just copy and paste it again
because it's a very similar line.

326
00:19:25,020 --> 00:19:27,090
And this time it's going
to be called the advantage.

327
00:19:30,630 --> 00:19:34,910
How's that TF advantage? And
this is going to be one. Okay.

328
00:19:35,420 --> 00:19:37,830
So now we have these placeholders

329
00:19:40,560 --> 00:19:44,750
and now we can finally say, let's
do cuts, computer lost functions.

330
00:19:45,060 --> 00:19:47,640
So again, variable Dusko
scope. Notice how I'm,

331
00:19:48,000 --> 00:19:51,930
I'm putting all of my modules
inside of this variable scope.

332
00:19:52,110 --> 00:19:57,110
And another reason this is useful is
because in the tensor flow graph we are

333
00:19:57,511 --> 00:20:01,510
able to name these things so
we can see how all of these uh,

334
00:20:01,560 --> 00:20:04,110
tenters are flowing through the nodes.

335
00:20:04,111 --> 00:20:08,370
In the computation graph that we
create inside of our loss function,

336
00:20:08,640 --> 00:20:11,460
we're going to have something called a
surrogate. Okay. So you might be thinking,

337
00:20:11,490 --> 00:20:14,670
what the hell is a surrogate?
So this is actually really interesting.

338
00:20:14,671 --> 00:20:17,580
So let me show you this.
So a surrogate is,

339
00:20:18,120 --> 00:20:22,770
so when it comes to sometimes in machine
learning, the air surface doesn't,

340
00:20:22,800 --> 00:20:26,970
isn't very smooth, right? So if you think
of gradient descent as dropping a ball,

341
00:20:27,060 --> 00:20:31,080
dropping a ball into a bowl, and then
having that ball find the minimum.

342
00:20:31,550 --> 00:20:34,470
So that's easy if the
air surface is curved,

343
00:20:34,590 --> 00:20:38,670
but sometimes it's noncontinuous.
So this is some discrete math for you,

344
00:20:38,850 --> 00:20:42,690
but sometimes it's surface doesn't just
easily go from one range to another.

345
00:20:42,691 --> 00:20:45,810
It's not smooth. It looks
more like this, right?

346
00:20:45,811 --> 00:20:48,490
So the ball won't just
flow down to the minimum.

347
00:20:48,550 --> 00:20:52,390
It's going to hit different steps.
It's going to go all over the place.

348
00:20:53,530 --> 00:20:57,730
So we want to make sure that air surface
is continuous, that it's very flat,

349
00:20:57,880 --> 00:21:00,280
that it's smooth,
that we can easily find the gradient.

350
00:21:00,460 --> 00:21:04,270
And to do that we'll create two loss
functions. We have a loss function,

351
00:21:04,480 --> 00:21:07,360
and then we have a loss function for
the loss function and we call this a

352
00:21:07,361 --> 00:21:10,330
surrogate loss function and that
the surrogate loss function,

353
00:21:10,331 --> 00:21:13,930
we'll first smooth out that earth's
surface and then we can find the minimum,

354
00:21:14,350 --> 00:21:16,960
which I think is very cool.
All right,

355
00:21:16,961 --> 00:21:20,400
we're going to use these two
terms to compute the k l penalty.

356
00:21:20,401 --> 00:21:23,230
So the Kale by the way
stands for Ku Black Liebler,

357
00:21:23,440 --> 00:21:26,410
two guys in the in like back in the day,
a couple of decades ago,

358
00:21:26,620 --> 00:21:28,360
who discovered a really
cool loss function.

359
00:21:28,540 --> 00:21:32,680
It's used a lot in generative adversarial
networks and in proximate policy

360
00:21:32,681 --> 00:21:36,100
optimization. But basically let's
go, let's go. Let's check this out.

361
00:21:36,101 --> 00:21:38,890
So the both of these two terms,

362
00:21:38,891 --> 00:21:43,840
the the ratio and the surrogates will
help us compute a kal Penn a penalty.

363
00:21:44,140 --> 00:21:47,590
And what's going to happen is we're
going to use the old policy and the new

364
00:21:47,591 --> 00:21:51,160
policy to come to compute the
Kale divergence between both.

365
00:21:51,430 --> 00:21:54,070
We'll take that Kale divergence term,
get the mean,

366
00:21:54,220 --> 00:21:56,770
and then compute a loss
using the surrogate.

367
00:21:56,980 --> 00:22:01,480
So then we're going to find the difference
between the surrogates and the lambda

368
00:22:01,481 --> 00:22:05,530
that we just computed times that Kale
divergence term and that's going to give

369
00:22:05,531 --> 00:22:06,490
us a loss function.

370
00:22:07,690 --> 00:22:11,140
We will minimize that loss
function using gradient descent.

371
00:22:11,310 --> 00:22:15,610
So these are the two separate distinct
respective loss functions that we're

372
00:22:15,611 --> 00:22:18,460
minimizing, one for the policy
and one for the value network,

373
00:22:18,520 --> 00:22:20,950
which will also calling the
actor and the critic network.

374
00:22:21,610 --> 00:22:24,190
We can then write all of that to
disk and then run the session.

375
00:22:24,910 --> 00:22:27,430
So I actually want to show you
some of the helper methods as well.

376
00:22:27,460 --> 00:22:31,800
That's the base code for
creating the policy and the uh,

377
00:22:31,870 --> 00:22:34,780
value networks. But let's look at
some of the helper methods as well.

378
00:22:34,960 --> 00:22:38,290
So for the build a net function
that I talked about, this is it,

379
00:22:38,291 --> 00:22:43,150
it's just a single layer neural network.
We compute a mean and a an a sigma.

380
00:22:43,151 --> 00:22:46,930
And then we use that to compute a normal
distribution and return that as well as

381
00:22:46,931 --> 00:22:51,730
the other parameters, the global
variables, but in the training loop,

382
00:22:51,760 --> 00:22:55,480
here's, here's the real, the real magic
where we implement what we just created.

383
00:22:55,750 --> 00:22:59,080
We'll say, let's start our training
loop, create an initial environment.

384
00:22:59,290 --> 00:23:02,770
We'll have a buffer for both the states,
the actions and the rewards,

385
00:23:02,980 --> 00:23:07,980
and in a single episode we'll use the
PPO class we just created to choose the

386
00:23:08,171 --> 00:23:12,400
best action dependent on the state and
that's going to return that action.

387
00:23:13,210 --> 00:23:16,960
Now we'll take that action and then
execute that in the environment using the

388
00:23:16,961 --> 00:23:21,670
step function really easy part of using
a open Ai's gym and that's going to

389
00:23:21,671 --> 00:23:25,570
return the state, the reward,
and then whether or not
we're done and some log, uh,

390
00:23:25,630 --> 00:23:27,340
criteria which are not actually using.

391
00:23:27,550 --> 00:23:31,510
We'll add all three of those to our
buffers and then will compute the total

392
00:23:31,511 --> 00:23:36,010
reward over time. Then we update
our PPO object. We'll say, well,

393
00:23:36,040 --> 00:23:40,120
let's get that value that get the using
the states and then get the discounted

394
00:23:40,121 --> 00:23:42,160
reward.
Then finally we're going to,

395
00:23:42,340 --> 00:23:46,700
we're going to pop all of those off the
stack and then update our policy using

396
00:23:46,701 --> 00:23:48,320
those that that new state,

397
00:23:48,321 --> 00:23:52,040
that new action and that
new reward and that's it.

398
00:23:52,040 --> 00:23:55,160
Then we can render the environment and
then display the frames as a gift for

399
00:23:55,161 --> 00:23:58,640
whatever we want to and return all
those rewards at the very, very end.

400
00:23:58,820 --> 00:24:02,270
This is the training loop where you run
it for a set number of epochs and then

401
00:24:02,271 --> 00:24:06,260
eventually what's going to happen is that
rocket is going to use both the policy

402
00:24:06,500 --> 00:24:08,060
and the value networks,

403
00:24:08,150 --> 00:24:13,100
the actor and the critic to learn what's
the best way to land on that landing

404
00:24:13,101 --> 00:24:17,000
pad, right? It's going to say if it
doesn't land, it's a minus one, right?

405
00:24:17,001 --> 00:24:21,170
If it does lend, it's a plus one and it
can use all of these different methods.

406
00:24:21,171 --> 00:24:25,400
Turn left, turn right,
turn up, go move faster,

407
00:24:25,401 --> 00:24:26,960
move slower on its booster,

408
00:24:27,170 --> 00:24:30,080
and then there's a bunch
of other gravitational
constants that are in the that

409
00:24:30,081 --> 00:24:31,630
environment set up for it that we,

410
00:24:31,700 --> 00:24:33,950
you can see in the documentation
I'm going to give you.

411
00:24:34,450 --> 00:24:36,440
Basically it's going to use
all of those to learn. Well,

412
00:24:36,441 --> 00:24:38,870
what's the best way to try all this out?

413
00:24:39,110 --> 00:24:42,140
The policy is computing
these stochastically that is,

414
00:24:42,141 --> 00:24:45,890
it's using a distribution to sample
from all the possible actions.

415
00:24:46,910 --> 00:24:50,930
And the value function is going
to say, well here are, here's the,

416
00:24:50,990 --> 00:24:53,600
here's how I'm going to rate
all of these possible actions.

417
00:24:53,760 --> 00:24:55,610
And the policy will say,
Hey, value function.

418
00:24:55,760 --> 00:24:59,210
I see how you're rating all of these
possible actions and I'm going to learn

419
00:24:59,211 --> 00:25:01,640
from you.
And the value functions learning as well.

420
00:25:01,760 --> 00:25:05,450
So there are two different sets of
gradients that are being composed that are

421
00:25:05,451 --> 00:25:09,950
being computed over time and eventually
through that plus one minus one feedback

422
00:25:09,951 --> 00:25:12,260
loop that we're getting from
open Ai's gym environment.

423
00:25:12,710 --> 00:25:16,370
These two networks going to work together
to help this thing land as efficiently

424
00:25:16,371 --> 00:25:19,310
as possible, right? So we can say, um,

425
00:25:20,670 --> 00:25:25,610
by Felon Ppo dot pie. And then

426
00:25:27,200 --> 00:25:30,110
we can see it run just like that.
So this thing is,

427
00:25:30,410 --> 00:25:34,160
this is a trained a version
of the algorithm. If you
want to train it yourself,

428
00:25:34,161 --> 00:25:37,520
I've got the source code in
the description, check it
out. It's the get hub link.

429
00:25:37,760 --> 00:25:40,790
And hope you liked this video, please
subscribe for more programming videos.

430
00:25:40,791 --> 00:25:44,510
And for now I've got to get a seat on
the falcon heavy, so thanks for watching.

