1
00:00:00,330 --> 00:00:04,590
How do we learn? Although Times may
change, some concepts stay the same.

2
00:00:04,890 --> 00:00:08,970
Unchanging information outlast the body.
It's stored in our brain,

3
00:00:08,971 --> 00:00:11,670
but can be passed down from
generation to generation.

4
00:00:12,030 --> 00:00:16,110
Our brain is capable of synthesizing the
diverse set of inputs we call our five

5
00:00:16,111 --> 00:00:20,250
senses and from them creating a
hierarchy of concepts. If we're lucky,

6
00:00:20,251 --> 00:00:23,310
we can learn a task while being
supervised by a teacher directly.

7
00:00:23,670 --> 00:00:26,880
While interacting with our environment,
we can feel our surroundings,

8
00:00:26,970 --> 00:00:30,300
see our obstacles and try
to predict the next steps.

9
00:00:30,540 --> 00:00:35,100
If we try at first and fail, that's okay.
Through the process of trial and error,

10
00:00:35,250 --> 00:00:36,600
we can learn anything,

11
00:00:36,870 --> 00:00:39,330
but what is it that gives our
brain this special capability?

12
00:00:39,480 --> 00:00:43,110
Unlike anything else in nature,
everything we've ever experienced or felt,

13
00:00:43,260 --> 00:00:47,370
all our thoughts and memories are very
sense of self is produced by the brain.

14
00:00:47,490 --> 00:00:48,570
At the molecular level.

15
00:00:48,660 --> 00:00:52,800
Our brain consists of an estimated 100
billion nerve cells called neurons.

16
00:00:52,980 --> 00:00:54,630
Each neuron has three jobs,

17
00:00:54,870 --> 00:00:56,950
receive a set of signals from
what are called [inaudible].

18
00:00:56,990 --> 00:01:00,630
Then drives integrate those signals
together to determine whether or not the

19
00:01:00,631 --> 00:01:03,720
information should be passed
on in the cell body or Soma.

20
00:01:03,990 --> 00:01:06,630
And then if the some of the
signals passes a certain threshold,

21
00:01:06,780 --> 00:01:11,400
send this resulting signal called the
action potential onwards via its Axon to

22
00:01:11,401 --> 00:01:13,270
the next set of neurons. Oh, we're all,

23
00:01:13,290 --> 00:01:16,680
it's the Raj and we're going to build
our own neural network in python.

24
00:01:16,920 --> 00:01:20,070
The rules that govern the brain
give rise to intelligence.

25
00:01:20,220 --> 00:01:24,900
It's the same algorithm that invented
modern language space flight Sheil above.

26
00:01:25,170 --> 00:01:26,700
It's what makes us us.

27
00:01:26,730 --> 00:01:30,600
It's what's allowed us to survive
and thrive on planet earth.

28
00:01:30,780 --> 00:01:32,670
But as far as we've come as a species,

29
00:01:32,820 --> 00:01:36,630
we still face a host of existential
threats to our existence.

30
00:01:36,930 --> 00:01:39,030
There's the impending
threat of climate change,

31
00:01:39,180 --> 00:01:43,050
the possibility of biochemical
warfare and an asteroid impact.

32
00:01:43,200 --> 00:01:46,920
These are non trivial problems that
could take our biological neural networks

33
00:01:47,160 --> 00:01:50,700
many generations to solve.
But what if we could harness this power?

34
00:01:50,940 --> 00:01:54,750
What if we could create an artificial
neural network and have it run on a non

35
00:01:54,751 --> 00:01:56,760
biological substrate like silicon?

36
00:01:57,090 --> 00:02:00,660
We could give it more computing power
and data than any one human would be

37
00:02:00,661 --> 00:02:05,370
capable of handling and haven't
felt problems a thousand
or even a million times

38
00:02:05,371 --> 00:02:06,960
faster than we could alone.

39
00:02:07,110 --> 00:02:11,790
In 1943 Q early computer scientist
named Warren Mcculloch and Walter pits

40
00:02:11,940 --> 00:02:14,700
invented the first
computational model of a neuron.

41
00:02:15,090 --> 00:02:19,470
They're model demonstrated a neuron that
received binary and puts some them and

42
00:02:19,471 --> 00:02:23,490
at the some exceeded a certain
threshold value. I'll put a one, if not,

43
00:02:23,550 --> 00:02:27,240
I'll put a zero. It was a simple
model, but in the early days of Ai,

44
00:02:27,270 --> 00:02:31,410
this was a big deal and got computer
scientist talking about the possibilities.

45
00:02:31,680 --> 00:02:32,520
A few years later,

46
00:02:32,521 --> 00:02:36,560
a psychologist named Brank Rosenblatt
was frustrated that the McCulloch pits

47
00:02:36,561 --> 00:02:39,000
model is still active
mechanism for learning.

48
00:02:39,120 --> 00:02:42,270
So he can see it's a neural
model that built on their idea,

49
00:02:42,420 --> 00:02:44,100
which he called the Perceptron,

50
00:02:44,430 --> 00:02:48,540
which is another word for a single
layer feed forward neural network.

51
00:02:48,750 --> 00:02:52,860
We call it feed forward because the data
just flows in one direction forward.

52
00:02:53,190 --> 00:02:56,670
The perceptron incorporated the
idea of weights on the inputs,

53
00:02:56,880 --> 00:02:59,160
so given some training
set of input output,

54
00:02:59,980 --> 00:03:04,180
it should learn a function from it by
increasing or decreasing the weights

55
00:03:04,181 --> 00:03:07,540
continuously. For each example,
depending on what its output was,

56
00:03:07,840 --> 00:03:11,830
these weight values are mathematically
applied to the input such that after each

57
00:03:11,831 --> 00:03:14,470
iteration,
the output prediction gets more accurate.

58
00:03:14,770 --> 00:03:17,200
The best understand this
process we call training.

59
00:03:17,410 --> 00:03:22,030
Let's build our own single layer neural
network in python using only num Pi as

60
00:03:22,031 --> 00:03:25,540
our dependency. In our main function,
we'll first initialize our neural network,

61
00:03:25,570 --> 00:03:27,820
which will later define as its own class.

62
00:03:28,090 --> 00:03:31,060
Then pronounce it starting weights
for our reference. When we demo it,

63
00:03:31,300 --> 00:03:34,360
we can now define our dataset.
We've got four examples.

64
00:03:34,600 --> 00:03:37,960
Each example has three input
values and one output value.

65
00:03:38,200 --> 00:03:39,490
They're all ones and Zeros.

66
00:03:39,670 --> 00:03:43,270
The t function transposes the
matrix from horizontal to vertical,

67
00:03:43,420 --> 00:03:45,580
so the computer is storing
the numbers like this.

68
00:03:45,760 --> 00:03:49,120
We'll train our neural network on these
values so that given a new list of ones

69
00:03:49,121 --> 00:03:49,601
and Zeros,

70
00:03:49,601 --> 00:03:52,450
it be able to predict whether or not
the output should be a one or zero.

71
00:03:52,630 --> 00:03:55,150
Since we are identifying
which category it belongs to,

72
00:03:55,360 --> 00:03:58,270
this is considered a classification
task and machine learning.

73
00:03:58,510 --> 00:04:02,410
We'll train our network on this data
by using them as arguments to our train

74
00:04:02,411 --> 00:04:06,730
function as well as a number 10,000
which is the amount of times we'd like to

75
00:04:06,760 --> 00:04:09,100
iterate during training.
After it's done training,

76
00:04:09,101 --> 00:04:12,670
we'll print out the updated weights so
we can compare them and finally it will

77
00:04:12,671 --> 00:04:16,450
predict the output. Given a new input,
we've got our main function ready,

78
00:04:16,451 --> 00:04:20,440
so let's now define our neural network
class. When we initialize the class,

79
00:04:20,710 --> 00:04:22,900
the first thing we will
want to do is seed.

80
00:04:22,901 --> 00:04:26,770
It will initialize our weight values
randomly in a second and seeding them.

81
00:04:26,771 --> 00:04:30,640
Make sure that it generates the same
numbers every time the program runs.

82
00:04:30,940 --> 00:04:32,230
This is useful for debugging.

83
00:04:32,231 --> 00:04:36,580
Later on we'll assign random weights to
a three by one matrix with values in the

84
00:04:36,581 --> 00:04:39,730
range of negative one to
one with a mean of zero.

85
00:04:39,731 --> 00:04:43,900
Since our single neuron has three input
connections and one output connection.

86
00:04:44,080 --> 00:04:46,300
Next we'll write out
our activation function,

87
00:04:46,301 --> 00:04:50,170
which in our case will be a sigmoid.
It describes an s shaped curve.

88
00:04:50,171 --> 00:04:54,010
We passed the weighted sum of the inputs
through it and it will convert them to

89
00:04:54,011 --> 00:04:56,320
a probability between zero and one.

90
00:04:56,620 --> 00:04:58,750
This probability will
help make our prediction.

91
00:04:58,960 --> 00:05:01,870
We'll use our sigmoid function
directly in our predicts function,

92
00:05:02,020 --> 00:05:06,340
which takes inputs as parameters and
passes them through our neuron to get the

93
00:05:06,341 --> 00:05:09,880
weight that some of our inputs will
compute the dot product of our inputs and

94
00:05:09,881 --> 00:05:10,714
our weights.

95
00:05:10,750 --> 00:05:14,260
This is how our weights govern the
attention of how data flows and our neural

96
00:05:14,261 --> 00:05:16,750
net and dysfunction will
return our prediction.

97
00:05:16,930 --> 00:05:20,050
Now we can write out our trained function,
which is the real meat of our code.

98
00:05:20,260 --> 00:05:23,920
Well, write a four loop to iterate
10,000 times as we specified.

99
00:05:24,130 --> 00:05:27,880
Then use our predict function to pass
the training set through the network and

100
00:05:27,881 --> 00:05:31,930
get the output value, which
is our prediction. Well,
next, calculate the error,

101
00:05:31,960 --> 00:05:35,290
which is a difference between the
desired output and our predicted output.

102
00:05:35,470 --> 00:05:39,700
We want to minimize this error as we
train and do this by iteratively updating

103
00:05:39,701 --> 00:05:40,360
our weights.

104
00:05:40,360 --> 00:05:43,900
We'll calculate the necessary adjustment
by computing the dot product of our

105
00:05:43,901 --> 00:05:48,280
inputs transpose and the error multiplied
by the gradient of the sigmoid curve,

106
00:05:48,520 --> 00:05:52,780
so less confident weights are adjusted
more and inputs that are zero don't cause

107
00:05:52,781 --> 00:05:56,560
changes to the weights.
This process is called gradient descent.

108
00:05:57,080 --> 00:05:58,400
I'm the sending that gradient.
Oh,

109
00:05:58,720 --> 00:06:02,330
we'll also write out that function that
calculates the derivative of our sigmoid,

110
00:06:02,510 --> 00:06:04,580
which gives us its gradient or slope.

111
00:06:04,820 --> 00:06:08,600
This measures how competent we are of
the existing weight value and helps us

112
00:06:08,630 --> 00:06:11,600
update our prediction in the
right direction. Finally,

113
00:06:11,601 --> 00:06:15,020
once we have our adjustment,
we'll update our weights with that value.

114
00:06:15,230 --> 00:06:19,490
This process of propagating our error
value back into our network to adjust our

115
00:06:19,491 --> 00:06:23,480
weights is called backpropagation.
Let's demo this baby in terminal.

116
00:06:23,660 --> 00:06:27,050
Because of the training set is so small,
it took milliseconds to train it.

117
00:06:27,290 --> 00:06:31,250
We can see that our weight values updated
themselves after all those iterations

118
00:06:31,251 --> 00:06:32,960
and when we fed it a novel input,

119
00:06:33,140 --> 00:06:35,660
it predicted that the output
was very likely a one.

120
00:06:36,140 --> 00:06:40,630
We just made our first neural network from
scratch anyways about backpropagation.

121
00:06:40,990 --> 00:06:41,823
I

122
00:06:44,560 --> 00:06:46,450
date update weights,

123
00:06:47,230 --> 00:06:52,230
backed up update wave that
propagates the update rate backdrop,

124
00:06:53,530 --> 00:06:57,970
update weight [inaudible] and
knows map to owes him ones inputs.

125
00:06:57,971 --> 00:07:02,680
Go in, add weights, get someone's past
that shit too. Must sigmoid function.

126
00:07:02,950 --> 00:07:07,660
Get that error, woods real and prediction.
And that's why I use gradient descent.

127
00:07:08,020 --> 00:07:13,020
It gives direction and it doesn't pretend
update weights and repeat 10,000 times

128
00:07:13,901 --> 00:07:16,570
outputs are lit.
I'll be doing just fine.

129
00:07:16,880 --> 00:07:20,030
So as dope as Rosenblatt's idea
was in the decades following it,

130
00:07:20,090 --> 00:07:23,180
neural networks didn't really give
us any kind of note the results.

131
00:07:23,450 --> 00:07:25,280
They could only accomplish simple things.

132
00:07:25,281 --> 00:07:28,880
But as the worldwide web grew from a
certain project to the massive nervous

133
00:07:28,881 --> 00:07:30,800
system for humanity that it is today,

134
00:07:31,070 --> 00:07:35,180
we've seen an explosion in data and
computing power and a small group of

135
00:07:35,181 --> 00:07:38,630
researchers funded by the
Canadian government held
fast to their belief in the

136
00:07:38,631 --> 00:07:42,290
power of neural networks to help
us find solutions from this data.

137
00:07:42,680 --> 00:07:47,210
When they took a neural net and made
it not one or two but many layers deep,

138
00:07:47,390 --> 00:07:50,120
gave it a huge dataset and
lots of computing power.

139
00:07:50,360 --> 00:07:54,500
They discovered that it could outperform
humans in tasks that we thought only we

140
00:07:54,501 --> 00:07:56,480
could do.
This is profound.

141
00:07:56,510 --> 00:08:00,590
Our biological no network is carbon
based sending electro chemicals like

142
00:08:00,770 --> 00:08:05,540
Acetylcholine and glutamate and
Serotonin as signals on artificial neural

143
00:08:05,541 --> 00:08:08,150
network doesn't even
exist in physical space.

144
00:08:08,360 --> 00:08:12,440
It's an abstract concept
we programmatically created
and it's represented on

145
00:08:12,441 --> 00:08:16,010
silicon transistors. Yet despite
the complete difference in mediums,

146
00:08:16,160 --> 00:08:20,330
they both develop a very similar mechanism
for processing information and the

147
00:08:20,331 --> 00:08:24,380
results show that perhaps there's a
law of intelligence encoded into our

148
00:08:24,381 --> 00:08:28,430
universe and we're coming ever closer
to finding it. So to break it down,

149
00:08:28,460 --> 00:08:32,900
a neural network is a biologically
inspired algorithm that learns to identify

150
00:08:32,901 --> 00:08:33,920
patterns in data.

151
00:08:34,280 --> 00:08:37,640
Backpropagation is a popular
technique to train a neural network,

152
00:08:37,730 --> 00:08:40,670
but continually updating weights.
The gradient descent,

153
00:08:40,970 --> 00:08:45,320
and when we train a many layered deep
neural network on lots of data using lots

154
00:08:45,321 --> 00:08:48,320
of computing power,
we call this process deep learning.

155
00:08:48,590 --> 00:08:51,260
The coding challenge winner
for last week is Ludo Blonde.

156
00:08:51,350 --> 00:08:55,350
Little made a really slick
I python notebook to demo,
not just today regression,

157
00:08:55,470 --> 00:09:00,060
but 3d regression as well on a climate
change dataset wizard of the week and the

158
00:09:00,061 --> 00:09:03,750
runner up is Amanullah Tariq.
He completed the bonus with great results.

159
00:09:04,020 --> 00:09:07,470
The challenge for this video is
to create a, not one, not two,

160
00:09:07,560 --> 00:09:11,940
but three layer feed forward neural
network using just num Pi. Post your gate,

161
00:09:11,950 --> 00:09:14,670
humbling in the comments,
and I'll announce the winner in one week.

162
00:09:14,940 --> 00:09:17,820
Please subscribe and for now
I've got to update my weights.

163
00:09:17,910 --> 00:09:19,170
So thanks for watching.

