1
00:00:00,120 --> 00:00:02,430
Hello world. It's a Raj. In this video,

2
00:00:02,431 --> 00:00:05,640
we're going to compare the most popular
deep learning frameworks out there right

3
00:00:05,641 --> 00:00:10,020
now to see what works best.
The deep learning space is
exploding with frameworks.

4
00:00:10,021 --> 00:00:14,700
Right now it's like every single week
some major tech company decides to open

5
00:00:14,701 --> 00:00:17,910
source their own deep learning library
and that's not including the dozens of

6
00:00:17,911 --> 00:00:21,630
deep learning frameworks being released
every single week on get hub by cowboy

7
00:00:21,631 --> 00:00:25,890
developers. How many layers you get.
Let's start off with psychic learn.

8
00:00:25,980 --> 00:00:30,090
Psychic was made to provide an easy to
use interface for developers to use off

9
00:00:30,091 --> 00:00:30,571
the shelf.

10
00:00:30,571 --> 00:00:34,830
General purpose machine
learning algorithms for both
supervised and unsupervised

11
00:00:34,831 --> 00:00:35,370
learning.

12
00:00:35,370 --> 00:00:39,000
Psyche provides functions that let you
apply classic machine learning algorithms

13
00:00:39,180 --> 00:00:40,740
like support vector machines,

14
00:00:40,860 --> 00:00:44,220
logistic regression's and k
nearest neighbor very easily,

15
00:00:44,340 --> 00:00:47,970
but the one type of machine
learning algorithm it doesn't
let you implement is a

16
00:00:47,971 --> 00:00:51,060
neural network.
It doesn't provide GPU support either,

17
00:00:51,180 --> 00:00:54,570
which is what helps neural network scale.
Since like two months ago,

18
00:00:54,660 --> 00:00:58,590
pretty much every single general purpose
algorithm that cycle or an implemented

19
00:00:58,740 --> 00:01:02,760
has since been implemented in tensorflow
psychic. You just got learned.

20
00:01:02,940 --> 00:01:04,110
There's also cafe,

21
00:01:04,111 --> 00:01:07,830
which was basically the first mainstream
production grade deep learning library

22
00:01:07,980 --> 00:01:11,100
started in 2013 but cafe
isn't very flexible.

23
00:01:11,130 --> 00:01:14,370
Think of a neural network as a
computational graph. In Cafe,

24
00:01:14,430 --> 00:01:15,930
each node is considered a layer,

25
00:01:15,990 --> 00:01:19,530
so if you want new layer types you have
to define the full forward backward

26
00:01:19,531 --> 00:01:23,460
ingredient updates. These
layers are building blocks
that are unnecessarily big.

27
00:01:23,550 --> 00:01:26,970
There's an endless list of them that
you can pick from in tensorflow.

28
00:01:27,090 --> 00:01:31,560
Each note is considered a tensor operation
like matrix at or Matrix multiply or

29
00:01:31,561 --> 00:01:36,000
convolution, and a layer can be defined
as a composition of those operations.

30
00:01:36,090 --> 00:01:40,080
So tensorflow is building blocks are
smaller, which allows for more modularity.

31
00:01:40,170 --> 00:01:43,050
Cafe also requires a lot
of unnecessary verbosity.

32
00:01:43,080 --> 00:01:45,660
If you want to support
both the CPU and the GPU,

33
00:01:45,720 --> 00:01:48,690
you need to implement extra functions
for each and you'd have to define your

34
00:01:48,691 --> 00:01:50,670
model using a plain text editor.

35
00:01:50,700 --> 00:01:54,900
That is just ghetto model should
be defined programmatically
because it's better

36
00:01:54,901 --> 00:01:58,080
for modularity between different
components. Also, cafes,

37
00:01:58,081 --> 00:02:00,750
main architect now works
on the tensorflow team.

38
00:02:00,790 --> 00:02:05,190
We're all out of cafe,
but speaking of modularity,

39
00:02:05,250 --> 00:02:06,260
let's talk about care.

40
00:02:06,270 --> 00:02:09,630
Ross terrace has been the goto source
to get started with deep learning for

41
00:02:09,631 --> 00:02:14,040
awhile because it provides a very high
level API to build deep learning models.

42
00:02:14,070 --> 00:02:18,210
Kara sits on top of the
other deep learning libraries
like piano in tensorflow.

43
00:02:18,270 --> 00:02:20,670
It uses an object oriented design,

44
00:02:20,700 --> 00:02:24,930
so everything is considered an object
be that layers, models, optimizers,

45
00:02:25,080 --> 00:02:28,590
and all the parameters of the model can
be accessed as object properties like

46
00:02:28,620 --> 00:02:30,540
model dot layers three dot output.

47
00:02:30,540 --> 00:02:33,690
We'll give you the output tensor for
the third layer in the model and model

48
00:02:33,691 --> 00:02:36,810
dealt layers three dealt weights is
a list of symbolic weight tensors.

49
00:02:36,840 --> 00:02:39,990
This is a cleaner interface as opposed
to the functional approach of making

50
00:02:39,991 --> 00:02:44,070
layers functions that create weights
when being called great documentation.

51
00:02:44,100 --> 00:02:48,480
It's all Gucci. Yes, I'm bringing that
back, but because it's so general purpose,

52
00:02:48,540 --> 00:02:50,430
it lacks on the side of performance.

53
00:02:50,580 --> 00:02:54,030
Chaos has been known to have performance
issues when used with a tensorflow

54
00:02:54,031 --> 00:02:56,520
backend since it's not
really optimized for it,

55
00:02:56,521 --> 00:02:59,060
but it does work pretty well
with the Theono backends.

56
00:02:59,170 --> 00:03:02,530
Two frameworks that are neck and neck
right now in the race to be the best

57
00:03:02,531 --> 00:03:06,580
library for both research and
industry are tensorflow and piano.

58
00:03:06,680 --> 00:03:09,850
Piano currently outperforms
tensorflow on a single GPU,

59
00:03:09,880 --> 00:03:14,710
but tensorflow outperforms the Yanno for
parallel execution across multiple gps.

60
00:03:14,800 --> 00:03:17,980
Deanna has got more documentation because
it's been around for awhile and it's

61
00:03:17,981 --> 00:03:22,870
got native windows support,
which tensorflow flow
doesn't yet dammit windows.

62
00:03:22,900 --> 00:03:26,650
In terms of syntax, let's just take a
look at some code to see some differences.

63
00:03:26,680 --> 00:03:29,680
We're going to compare two
scripts in tensorflow and theone.

64
00:03:29,710 --> 00:03:30,850
They both do the same thing,

65
00:03:30,890 --> 00:03:34,420
initialized symfony data and then learn
the line of best fit for that data so it

66
00:03:34,421 --> 00:03:35,740
can predict future data points.

67
00:03:35,770 --> 00:03:39,370
Let's look at the first step in both
tensorflow Angiano or generating the data

68
00:03:39,371 --> 00:03:41,590
pretty much the same way
using num py or raise,

69
00:03:41,620 --> 00:03:43,030
so there's not really a difference there.

70
00:03:43,090 --> 00:03:44,830
Let's look at the model
initialization parts.

71
00:03:44,860 --> 00:03:48,400
This is the basic y equals mx plus
B slope formula, intention flow.

72
00:03:48,401 --> 00:03:52,180
It doesn't require any special treatment
of the x and y variables. They're just,

73
00:03:52,181 --> 00:03:52,991
they're natively,

74
00:03:52,991 --> 00:03:57,010
but in piano we have to specifically say
that the variables are symbolic inputs

75
00:03:57,011 --> 00:03:57,790
to the function.

76
00:03:57,790 --> 00:04:01,120
The tension flow syntax of defining
the B w variables is cleaner.

77
00:04:01,180 --> 00:04:04,180
Then we implement our gradient descent
function, which is what helps us learn.

78
00:04:04,240 --> 00:04:06,910
We're trying to minimize the
mean squared error over time,

79
00:04:07,030 --> 00:04:08,770
which is what makes our
model more accurate.

80
00:04:08,771 --> 00:04:12,250
As we train the syntax for defining
what we're minimizing is pretty similar.

81
00:04:12,280 --> 00:04:14,920
Then when we look at the actual
optimizer which helps us do that,

82
00:04:14,950 --> 00:04:16,570
we'll notice a difference in syntax.
Again,

83
00:04:16,620 --> 00:04:20,080
tensorflow just gives you access to a
bunch of optimizers right out of the box.

84
00:04:20,140 --> 00:04:23,650
Things like gradient descent or Adam Piano
makes you do this from scratch and we

85
00:04:23,651 --> 00:04:26,890
have our training function which is
again more verbose. See the trend here.

86
00:04:27,110 --> 00:04:30,280
Deanna so far is making us
implement more code than tensorflow,

87
00:04:30,370 --> 00:04:33,940
so it seems to give us more fine grained
control but at the cost of readability.

88
00:04:33,941 --> 00:04:36,280
Finally we'll get to the
actual training part itself.

89
00:04:36,310 --> 00:04:39,520
They look pretty identical but tension
flows methodology of encapsulating the

90
00:04:39,521 --> 00:04:42,290
computational graph peels
conceptually cleaner than pianos.

91
00:04:42,320 --> 00:04:46,420
Tensorflow is just growing so fast
that it seems inevitable that whatever

92
00:04:46,421 --> 00:04:50,800
feature it lacks right now because of
how new it is, it will gain very rapidly.

93
00:04:50,950 --> 00:04:54,310
I mean just look at the amount of
activity happening in the tensorflow repo

94
00:04:54,430 --> 00:04:58,360
versus the theatro repo on get hub right
now and while Kara serves as an easy

95
00:04:58,361 --> 00:05:02,020
use wrapper around different libraries,
it's not optimized for tensorflow.

96
00:05:02,290 --> 00:05:05,830
A better alternative if you want to
learn and get started easily with deep

97
00:05:05,831 --> 00:05:10,540
learning is TF learn, which is basically
care os but optimized for tensorflow.

98
00:05:10,600 --> 00:05:14,680
So to sum things up, the best library
to use for research is tensorflow,

99
00:05:14,770 --> 00:05:18,760
the world class researchers that both
open AI and deep mind are now using it for

100
00:05:18,761 --> 00:05:19,511
production.

101
00:05:19,511 --> 00:05:23,530
The Best Library to use is still tends
to flow since it scales better across

102
00:05:23,531 --> 00:05:27,580
multiple Gpu then its closest
competitor piano. Lastly for learning,

103
00:05:27,581 --> 00:05:29,560
the best library use is TF learn,

104
00:05:29,590 --> 00:05:31,960
which is a high level
wrapper around tensorflow.

105
00:05:32,020 --> 00:05:34,180
That lets you get started really easily.
Also,

106
00:05:34,181 --> 00:05:37,340
shout out to Raho Deo for being
able to generate an upbeat mid.

107
00:05:37,341 --> 00:05:40,750
I file a bad ass of the week. Please
subscribe for more programming videos.

108
00:05:40,840 --> 00:05:44,410
For now. I've got to go worship tensorflow
some more. So thanks for watching.

