1
00:00:00,090 --> 00:00:03,750
An Ai has probably judged
your resume already.

2
00:00:05,400 --> 00:00:10,230
Hello World Saroj and what does
the perfect resume look like?

3
00:00:10,500 --> 00:00:15,500
We'll build an AI app to automatically
rates the value of a resume using

4
00:00:15,541 --> 00:00:19,230
advances in natural
language processing or NLP.

5
00:00:19,560 --> 00:00:24,560
The first generation of hiring systems
required a lot of human effort hiring

6
00:00:24,571 --> 00:00:29,571
teams would need to publish their open
positions to newspapers and the cable

7
00:00:29,761 --> 00:00:30,594
networks.

8
00:00:30,660 --> 00:00:35,340
Interested candidates would then apply
to the position by sending in their

9
00:00:35,341 --> 00:00:36,174
resumes.

10
00:00:36,270 --> 00:00:41,270
These resumes were then reviewed by the
hiring team and the best candidate was

11
00:00:42,031 --> 00:00:44,580
called in for another round of interviews.

12
00:00:44,880 --> 00:00:49,880
The second generation of hiring systems
introduced the concept of hiring

13
00:00:50,551 --> 00:00:51,420
agencies.

14
00:00:51,630 --> 00:00:56,630
They offer a solution in
which the candidates submits
their resume to an agency.

15
00:00:57,270 --> 00:00:59,730
The agency would act as a middleman.

16
00:00:59,970 --> 00:01:04,770
They would screen resumes and send to
candidates to suitable positions at

17
00:01:04,800 --> 00:01:09,630
different companies.
But now we're in the age of a Ai.

18
00:01:09,660 --> 00:01:13,440
We are reaching the third
generation in hiring systems.

19
00:01:13,740 --> 00:01:18,740
Candidates can upload their resumes to
an AI system that uses natural language

20
00:01:19,321 --> 00:01:23,640
processing to assess the
capability for a specific job.

21
00:01:23,940 --> 00:01:28,290
A company could then use this tool to
call in the best candidates for further

22
00:01:28,291 --> 00:01:31,680
interviews. I've got so many
rejections, but look at me now.

23
00:01:31,830 --> 00:01:36,830
This removes the cost for companies in
hiring an agency and results in a faster

24
00:01:37,591 --> 00:01:38,820
candidate pipeline.

25
00:01:39,120 --> 00:01:44,120
It also serves as a great tool for job
hunters rather than trying to use your

26
00:01:44,491 --> 00:01:49,491
own intuition and trying to gauge
whether certain parts of your resume our

27
00:01:49,561 --> 00:01:50,394
optimal,

28
00:01:50,400 --> 00:01:55,400
you can just use a tool to analyze your
resume for you and tell you what areas

29
00:01:56,251 --> 00:02:00,780
need to be improved on for this
specific job you're applying to.

30
00:02:01,140 --> 00:02:05,040
Red Score is one startup that does that.
For candidates.

31
00:02:05,220 --> 00:02:10,220
They can improve their chances of landing
a specific job by using their tool to

32
00:02:10,860 --> 00:02:14,130
optimize their resume and
on the recruiting side,

33
00:02:14,310 --> 00:02:18,750
sovereign does the same for
companies looking to hire candidates.

34
00:02:18,930 --> 00:02:20,310
So how do we do this?

35
00:02:20,620 --> 00:02:25,620
Rule based parsers are a traditional
way of screening resumes automatically.

36
00:02:26,190 --> 00:02:31,190
That means that a developer
will line-by-line write out
a set of if l statements

37
00:02:32,220 --> 00:02:37,050
that check for certain requirements
in a resume and if enough of those

38
00:02:37,051 --> 00:02:41,190
requirements are met for
a given preset threshold,

39
00:02:41,400 --> 00:02:46,400
then the resume moves on to the next
stage of the interview process else.

40
00:02:46,950 --> 00:02:48,300
It's tossed aside,

41
00:02:48,600 --> 00:02:53,340
but the problem is that there's a lot
of variability and ambiguity in the

42
00:02:53,341 --> 00:02:55,350
language used in resumes.

43
00:02:55,530 --> 00:03:00,520
There are different ways to write
dates and different job and skills.

44
00:03:00,550 --> 00:03:05,550
A pure every month someone's name could
be a company name or even an it skill.

45
00:03:07,570 --> 00:03:12,570
The only way for a resume parser to
deal with all of these edge cases is to

46
00:03:13,181 --> 00:03:18,181
truly understand the context in which
words occur and the relationships between

47
00:03:19,181 --> 00:03:19,840
them.

48
00:03:19,840 --> 00:03:24,370
No rule is 100% reliable except
the one about fight club.

49
00:03:24,640 --> 00:03:29,640
Machine learning solves this problem by
estimating the quality of these signals

50
00:03:30,820 --> 00:03:35,820
based on annotated data and by combining
the evidence from several signals,

51
00:03:36,640 --> 00:03:38,890
we can call these signals features.

52
00:03:38,950 --> 00:03:43,690
They are the encoded information that
is relevant for a prediction task.

53
00:03:44,050 --> 00:03:49,050
Lots of different machine learning
techniques require us to decide what these

54
00:03:49,091 --> 00:03:51,850
features are beforehand.
In fact,

55
00:03:51,851 --> 00:03:56,851
two of the most important aspects of
machine learning models are feature

56
00:03:56,980 --> 00:03:59,680
extraction and feature engineering.

57
00:04:00,040 --> 00:04:05,040
Those features are what supply
relevant information to the ML models.

58
00:04:06,880 --> 00:04:10,330
We can represent the word
overfitting for example,

59
00:04:10,420 --> 00:04:15,420
using various different
feature representations
and LP tasks like automatic

60
00:04:16,181 --> 00:04:21,181
summarization and topic extraction have
generally required developers to sort

61
00:04:21,851 --> 00:04:26,851
their corpus of text and two different
feature groups like bigrams and trigrams.

62
00:04:27,640 --> 00:04:29,440
If there are too few features,

63
00:04:29,500 --> 00:04:33,220
that model will have a hard
time making a useful prediction.

64
00:04:33,490 --> 00:04:34,990
If there are too many,

65
00:04:34,991 --> 00:04:39,610
that model can be slow and overfit
stay balanced with the force.

66
00:04:40,060 --> 00:04:45,060
We don't really know what
feature representation is
best for a given task and in

67
00:04:45,611 --> 00:04:50,530
the off chance that someone does, it
still relies on a human being in the loop.

68
00:04:50,920 --> 00:04:51,521
Ideally,

69
00:04:51,521 --> 00:04:56,521
we just want to be able to give a raw
data set to a model and have it produced

70
00:04:56,851 --> 00:04:59,590
an insights without any human help.

71
00:05:00,010 --> 00:05:05,010
Deep learning though allows these features
to be learned automatically for image

72
00:05:05,711 --> 00:05:07,390
classification.
For example,

73
00:05:07,750 --> 00:05:12,750
handcrafted features were required for
any model like detecting edges but with

74
00:05:13,361 --> 00:05:15,130
convolutional neural networks.

75
00:05:15,370 --> 00:05:20,370
Edge detection is instead learned
by the first convolutional layer.

76
00:05:20,860 --> 00:05:25,860
The filters learned to buy the first
layer of CNNS when given images are very

77
00:05:26,411 --> 00:05:31,411
reminiscent of Gabor filters which were
used for edge and texture detection

78
00:05:31,721 --> 00:05:33,790
traditionally and for the next layers,

79
00:05:33,910 --> 00:05:38,910
the filters begin to recognize specific
patterns and objects with increasing

80
00:05:38,951 --> 00:05:39,784
detail.

81
00:05:39,940 --> 00:05:44,770
All of these are learned feature
extractions as the features become further

82
00:05:44,771 --> 00:05:47,980
refined as we go up the model hierarchy.

83
00:05:48,310 --> 00:05:53,310
The change here is that architecture
engineering became the new feature

84
00:05:53,531 --> 00:05:54,340
engineering.

85
00:05:54,340 --> 00:05:58,640
Rather than having data scientists
figure out the for deep learning,

86
00:05:58,670 --> 00:06:02,690
they have to figure out the most
suitable hyper parameters for the model,

87
00:06:02,720 --> 00:06:05,870
like the learning rate
and the filter size.

88
00:06:06,110 --> 00:06:11,110
In a paper from last year called domain
adaption for resume classification,

89
00:06:11,750 --> 00:06:15,680
a group of students from Finland,
Sweden's jealous younger sibling,

90
00:06:15,950 --> 00:06:20,950
proposed a method for classifying
resume data of job applications into 27

91
00:06:22,401 --> 00:06:25,190
different job categories using CNNS.

92
00:06:25,730 --> 00:06:29,810
Even though CNNS are normally
used for image related tasks,

93
00:06:30,080 --> 00:06:35,080
we found that they can work well
for text classification as well.

94
00:06:35,690 --> 00:06:39,710
If we think about how we would
assess a resume before anything else,

95
00:06:39,890 --> 00:06:44,720
we need to make sure that the resume is
a right fit for the specific role we are

96
00:06:44,721 --> 00:06:45,800
hiring for.

97
00:06:46,070 --> 00:06:51,070
Once we have that we can assess the
amount of experience that candidate has,

98
00:06:51,260 --> 00:06:56,120
the quality of the brands they've worked
with in the past and the quality of

99
00:06:56,121 --> 00:06:57,620
their past relationships.

100
00:06:57,770 --> 00:07:02,770
So let's start with this basic example
of automatically weeding out resumes that

101
00:07:03,500 --> 00:07:08,090
aren't a fit for our specific role cover,
how exactly it works.

102
00:07:08,210 --> 00:07:10,850
Then talk about ways of improving it.

103
00:07:11,180 --> 00:07:16,180
Collecting training data for our resume
classification use case isn't easy.

104
00:07:16,490 --> 00:07:21,490
Resume data is difficult and costly to
obtain because it's considered sensitive

105
00:07:21,681 --> 00:07:22,514
information,

106
00:07:22,700 --> 00:07:27,290
but data about job descriptions
can be obtained much easier.

107
00:07:27,530 --> 00:07:32,530
The two domains constituted by
resume data and job description data,

108
00:07:33,080 --> 00:07:35,480
however are intrinsically related.

109
00:07:35,810 --> 00:07:40,810
Both domains are related to the
same job recommendation task,

110
00:07:41,090 --> 00:07:44,600
which is to match applicants
to suitable job offers.

111
00:07:44,840 --> 00:07:49,840
The resumes of applications have semantic
similarities with job descriptions,

112
00:07:50,390 --> 00:07:55,160
which belonged to the same job category.
They both contain skills,

113
00:07:55,161 --> 00:08:00,161
education duties as well as personal
characteristics of the desired candidate.

114
00:08:00,830 --> 00:08:05,830
So it seems reasonable to use transfer
learning in order to implement a domain

115
00:08:06,561 --> 00:08:11,561
adoption in order to leverage
the information contained
in the huge amount of

116
00:08:12,261 --> 00:08:16,730
labeled job description data.
In order to classify resume data.

117
00:08:17,060 --> 00:08:19,130
That's exactly what they did.

118
00:08:19,370 --> 00:08:22,150
They collected 90,000 job descriptions,

119
00:08:22,151 --> 00:08:25,430
snippets using the indeed job search Api.

120
00:08:25,730 --> 00:08:29,570
It enables access to short
jobs summaries given a keyword.

121
00:08:29,900 --> 00:08:34,670
The keywords they used were 27
different industrial job categories like

122
00:08:34,790 --> 00:08:39,790
accountants and insurance to very
boring jobs for testing the model.

123
00:08:40,040 --> 00:08:45,040
They use an anonymized data set
of about 500 resume data samples.

124
00:08:45,650 --> 00:08:50,650
Each sample labeled with one of the 27
categories based on the type of job that

125
00:08:51,021 --> 00:08:52,520
candidate is looking for.

126
00:08:52,730 --> 00:08:57,730
So the goal was to leverage easily
available job description data to train a

127
00:08:57,931 --> 00:09:02,820
model for classifying resume summary
snippets to better understand how the two

128
00:09:02,821 --> 00:09:03,840
domains deferred.

129
00:09:03,990 --> 00:09:08,990
They mapped out word frequencies of all
words appearing at least five times in

130
00:09:09,811 --> 00:09:11,130
both datasets.

131
00:09:11,460 --> 00:09:15,870
The results showed that in resumes
people were much more likely to use

132
00:09:15,900 --> 00:09:20,160
adjectives describing themselves
as adaptable and polite.

133
00:09:20,430 --> 00:09:24,720
Whereas job descriptions more often
mentioned roles like director and

134
00:09:24,721 --> 00:09:26,760
coordinator.
An interesting finding.

135
00:09:27,330 --> 00:09:31,860
The training data consistent of lots
of words like lots and lots and lots.

136
00:09:32,130 --> 00:09:34,410
If we observe these
words with the human eye,

137
00:09:34,620 --> 00:09:38,310
we're able to tell which words
relate to which other words.

138
00:09:38,490 --> 00:09:41,220
We're able to categorize certain words.

139
00:09:41,520 --> 00:09:46,520
That's because our brains have built an
internal representation of the intrinsic

140
00:09:46,980 --> 00:09:48,750
meaning behind words,

141
00:09:48,840 --> 00:09:52,710
so we can tell that even if two
words are spelled differently,

142
00:09:53,130 --> 00:09:54,390
they both have the same meaning.

143
00:09:54,870 --> 00:09:59,870
The way machines can do what we do is
he use vectors as that representation.

144
00:10:01,110 --> 00:10:06,110
This is a group of numbers and abstraction
that is learned from learning word

145
00:10:07,020 --> 00:10:08,430
meaning mappings.

146
00:10:08,760 --> 00:10:12,150
Word two VEC is a pretrained
model that did this.

147
00:10:12,300 --> 00:10:16,740
It learned word vectors from a
huge collection of generic words.

148
00:10:17,070 --> 00:10:20,640
Using this model, when you give it
a word, it'll output it's vector.

149
00:10:20,730 --> 00:10:25,730
Turning words into vectors allows us
to perform calculations on these words.

150
00:10:25,830 --> 00:10:28,170
I'm talking addition,
subtraction,

151
00:10:28,380 --> 00:10:32,670
numerically defining how
similar one word is to another.

152
00:10:32,910 --> 00:10:37,470
Math is my city and converting
our words into vectors.

153
00:10:37,500 --> 00:10:40,110
Then feeding those vectors into art.

154
00:10:40,111 --> 00:10:45,111
CNN is an important preprocessing step
as it's a numerical representation.

155
00:10:45,750 --> 00:10:47,100
Perfect for the Matrix.

156
00:10:47,101 --> 00:10:52,101
Operations will apply to it throughout
the neural network board propagation

157
00:10:52,650 --> 00:10:53,483
process.

158
00:10:53,490 --> 00:10:57,690
So once we use word to Vec to
convert our words to vectors,

159
00:10:57,870 --> 00:11:02,730
we'll build a convolutional network using
Carol Ross to then learn the mapping

160
00:11:02,731 --> 00:11:05,940
between those word vectors and the job.

161
00:11:05,941 --> 00:11:10,941
That category label the model did pretty
well and anybody can train their own on

162
00:11:11,491 --> 00:11:16,491
a cloud training service like Google
cloud or Floyd hub pretty easily given a

163
00:11:17,131 --> 00:11:17,970
new resume,

164
00:11:18,000 --> 00:11:23,000
it's converted into a vector fed into
the model and the percent likelihood that

165
00:11:23,011 --> 00:11:26,310
it belongs to a certain
job category is output.

166
00:11:26,700 --> 00:11:31,320
If that percent is over a
certain threshold we predefined
than it valid for our

167
00:11:31,321 --> 00:11:32,010
job.

168
00:11:32,010 --> 00:11:37,010
This alone greatly increases
the efficiency of the
hiring pipeline still here.

169
00:11:38,730 --> 00:11:40,890
Awesome.
Three points to remember.

170
00:11:41,100 --> 00:11:46,100
Natural language processing is a subset
of machine learning that lets us extract

171
00:11:46,830 --> 00:11:51,420
insights from text data rather
than engineering features.

172
00:11:51,600 --> 00:11:56,600
We can let deep learning learn relevant
features and focus on tuning model

173
00:11:57,011 --> 00:11:58,780
architectures instead.

174
00:11:59,050 --> 00:12:04,050
And the hiring pipeline currently requires
lots of unnecessary costly effort.

175
00:12:05,020 --> 00:12:06,130
There's a massive,

176
00:12:06,131 --> 00:12:11,080
massive opportunity to use
machine learning to make
the process more efficient

177
00:12:11,081 --> 00:12:13,570
for both candidates and recruiters.

178
00:12:14,340 --> 00:12:17,080
It is midterm time for this course.

179
00:12:17,290 --> 00:12:22,290
This week's coding challenge is to write
a script that classifies text using a

180
00:12:22,331 --> 00:12:26,560
CNN. Details are in the read me get
humbling scope in the comment section,

181
00:12:26,710 --> 00:12:30,730
and I'll announce the top two entries
next week. You made it to the end.

182
00:12:30,790 --> 00:12:34,810
Hit subscribe to be my friend and
you'll never be lonely again. For now,

183
00:12:34,840 --> 00:12:37,810
I've got to attend Google io,
so thanks for watching.

