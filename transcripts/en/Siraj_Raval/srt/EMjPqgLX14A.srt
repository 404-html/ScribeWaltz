1
00:00:00,030 --> 00:00:03,090
Hello world.
It's a Raj and just a few days ago,

2
00:00:03,150 --> 00:00:07,500
Facebook's AI research team
released a demo called dense posts.

3
00:00:07,800 --> 00:00:12,480
They were able to map out every single
pixel of a human body in a given video

4
00:00:12,481 --> 00:00:16,860
and not just for a single human, but
for many, many humans all at once,

5
00:00:17,130 --> 00:00:18,600
even more impressive.

6
00:00:18,780 --> 00:00:23,460
They were able to do this in videos with
lots of distractions and general chaos

7
00:00:23,490 --> 00:00:27,630
on a single GPU. Splost.
Nope, just deep learning.

8
00:00:28,020 --> 00:00:31,710
This is pretty cool and has tons of
applications and we're going to learn how

9
00:00:31,711 --> 00:00:33,480
they did it in this video.

10
00:00:33,750 --> 00:00:36,990
Creating d animated
characters is a lot of work.

11
00:00:37,140 --> 00:00:41,940
The tools to do so are expensive and
require teams of people working for months

12
00:00:41,941 --> 00:00:42,660
on end.

13
00:00:42,660 --> 00:00:47,660
Big Production Studios with the available
budgets are able to track humans and

14
00:00:47,941 --> 00:00:51,990
convert them to animations,
but for people without big budgets,

15
00:00:52,170 --> 00:00:55,290
this technology will
democratize that ability.

16
00:00:55,560 --> 00:00:58,470
Remember my video on the
deep fakes algorithm? Well,

17
00:00:58,500 --> 00:01:03,000
this can be used for a full body version
of that. If we can track a human,

18
00:01:03,210 --> 00:01:06,300
we can swap out their entire
body with a different one.

19
00:01:06,450 --> 00:01:11,160
Lending to some surreal scenarios like
Geoff Hinton charging into more door

20
00:01:11,190 --> 00:01:16,050
instead of Aragorn augmented
reality applications could
use this to label people

21
00:01:16,200 --> 00:01:20,540
that you see in real time with all sorts
of statistics and it would give away

22
00:01:20,610 --> 00:01:25,290
for machines to be able to better read
our body language for everything from

23
00:01:25,440 --> 00:01:30,180
sentiment analysis to full body
tracking to virtual reality gaming.

24
00:01:30,480 --> 00:01:33,960
This is the new state of the
art in human pose estimation.

25
00:01:33,961 --> 00:01:36,090
And to understand how it works,

26
00:01:36,270 --> 00:01:40,410
we're going to have to try
and work through the thought
process the authors had

27
00:01:40,620 --> 00:01:45,090
both in terms of building on previous
discoveries and trying different methods.

28
00:01:45,360 --> 00:01:48,000
So let's say we have a
video of a human dancing.

29
00:01:48,090 --> 00:01:53,040
This is a two dimensional grid of pixels,
but when you or I look at it,

30
00:01:53,160 --> 00:01:56,940
we're able to tell that there are
indeed three d objects that are being

31
00:01:56,941 --> 00:01:58,980
represented by this two d grid.

32
00:01:59,400 --> 00:02:03,960
We want our computer to have this
ability as well and in a way that we can

33
00:02:03,961 --> 00:02:04,711
visualize it.

34
00:02:04,711 --> 00:02:08,940
Meaning we'll want to transform this
tutti human into a three d model.

35
00:02:09,240 --> 00:02:12,870
Once we have that three d model,
we can do whatever we want with it,

36
00:02:12,900 --> 00:02:17,100
put it into any sort of scene,
swap it with another three d model,

37
00:02:17,160 --> 00:02:19,800
change its features,
lots of possibilities,

38
00:02:19,801 --> 00:02:24,801
but all of these possibilities depend
on us being able to construct a three

39
00:02:24,930 --> 00:02:29,670
model of this human in real time that
updates its movements as the human moves.

40
00:02:29,730 --> 00:02:32,370
When want to construct a correspondence,

41
00:02:32,550 --> 00:02:37,550
this is a computer vision term that is a
measure of how well pixels in one image

42
00:02:38,070 --> 00:02:40,500
correspond to pixels in another image.

43
00:02:40,770 --> 00:02:45,390
In our case it would be a two D to three
d image and we don't want any holes in

44
00:02:45,391 --> 00:02:50,190
our image. We want all the pixels
to be as close together as possible,

45
00:02:50,280 --> 00:02:55,110
so we'll call it a dense correspondence
and we'll just focus on doing this for

46
00:02:55,111 --> 00:02:55,651
humans.

47
00:02:55,651 --> 00:03:00,280
For now to create a dense correspondence
will need to perform some object

48
00:03:00,281 --> 00:03:04,390
detection,
object segmentation and pose estimation.

49
00:03:04,600 --> 00:03:06,420
That's a multilevel problems,

50
00:03:06,421 --> 00:03:10,240
so we want to start with a method
that's as simple as possible.

51
00:03:10,270 --> 00:03:13,030
We got so many different
avenues to go about this,

52
00:03:13,031 --> 00:03:17,590
but ideally we could use some sort of
labeled data set because having a label

53
00:03:17,591 --> 00:03:21,970
just makes training machine learning
algorithms much easier since it just

54
00:03:21,971 --> 00:03:26,530
involves learning a function
that represents the mapping
between the input data

55
00:03:26,710 --> 00:03:28,780
and the labels.
In this case,

56
00:03:28,810 --> 00:03:32,050
what would the ideal
labeled datasets look like?

57
00:03:32,320 --> 00:03:35,650
There are tons of labeled image
data sets out there these days.

58
00:03:35,830 --> 00:03:38,230
Pascal Cfr,
image net.

59
00:03:38,410 --> 00:03:43,270
These are basically big collections of
images of a variety of random objects

60
00:03:43,420 --> 00:03:45,250
each with their own particular label.

61
00:03:45,550 --> 00:03:50,050
These datasets have been a catalyst for
all of the recent progress in computer

62
00:03:50,051 --> 00:03:54,160
vision algorithms,
but we don't want to just label a human.

63
00:03:54,161 --> 00:03:59,080
If we see one in a video, we want to be
able to create a three d model of one.

64
00:03:59,440 --> 00:04:03,130
There's not really a Dataset of human
images with the label being the three d

65
00:04:03,131 --> 00:04:03,760
model,

66
00:04:03,760 --> 00:04:08,760
so we'd have to create one ourselves
and it would involve humans manually

67
00:04:09,251 --> 00:04:13,570
creating annotations that relates
three d images to surface spaced

68
00:04:13,870 --> 00:04:17,770
representations of the human body,
and that's exactly what they did.

69
00:04:18,160 --> 00:04:23,160
They basically asked the annotators to
annotate regions corresponding to defined

70
00:04:23,381 --> 00:04:25,870
body parts like the head and the legs.

71
00:04:26,350 --> 00:04:31,320
All of these annotations were labeled with
their corresponding three d body part,

72
00:04:31,450 --> 00:04:35,680
which acted as the label.
They did this for 50 k humans,

73
00:04:35,830 --> 00:04:40,810
which summed up to be a total of 5 million
manually annotated correspondences.

74
00:04:40,870 --> 00:04:45,250
Once we have our Dataset, we're going to
have to decide on what model to built.

75
00:04:45,520 --> 00:04:49,000
We know that deep learning is the
state of the art when it comes to

76
00:04:49,001 --> 00:04:53,740
classification and that convolutional
networks are the state of the art in image

77
00:04:53,741 --> 00:04:58,570
classification, but it's not just
classification we're trying to do,

78
00:04:58,840 --> 00:05:03,760
but regression as well. What's the
next move the human is going to make?

79
00:05:04,180 --> 00:05:05,560
The next question would be,

80
00:05:05,770 --> 00:05:09,790
has anyone done something similar
with a convolutional network?

81
00:05:10,180 --> 00:05:11,013
Of course.

82
00:05:11,620 --> 00:05:16,270
Luckily there was a recent architecture
called dense Raj that did this for all

83
00:05:16,271 --> 00:05:18,670
sorts of objects and got decent results,

84
00:05:18,790 --> 00:05:23,260
so we could start with that architecture.
In the first step, the network,

85
00:05:23,261 --> 00:05:27,910
we'll classify a pixel as belonging to
either the background or one of several

86
00:05:27,911 --> 00:05:31,990
region parts that gives a rough
estimate of surface coordinates.

87
00:05:32,260 --> 00:05:36,940
This is essentially a labeling task that
can be trained using gradient descent.

88
00:05:37,270 --> 00:05:38,440
In the second step,

89
00:05:38,470 --> 00:05:43,470
a regression model will indicate the
exact coordinates of the pixel within the

90
00:05:43,481 --> 00:05:44,314
region part.

91
00:05:44,500 --> 00:05:48,820
A more formal way of saying this is
that in the first stage they will assign

92
00:05:48,821 --> 00:05:53,740
position l two the body parts c that has
the highest likelihood as calculated by

93
00:05:53,741 --> 00:05:58,610
the classification branch and in the
second stage it will use the regressor to

94
00:05:58,611 --> 00:06:02,540
place the point l in the
continuous coordinate pair UV.

95
00:06:02,750 --> 00:06:06,950
That is a parameterization of
part c c can take 25 values.

96
00:06:06,951 --> 00:06:08,390
One would be the background,

97
00:06:08,540 --> 00:06:13,540
meaning that P is a 25 way classification
unit and we can train 24 regression

98
00:06:13,611 --> 00:06:18,020
functions are each of which provides
two d coordinates within its respective

99
00:06:18,021 --> 00:06:22,850
parts. Both the classification
and regression tasks are
trained by minimizing a

100
00:06:22,851 --> 00:06:24,350
respective loss function,

101
00:06:24,590 --> 00:06:29,030
but the regression loss is only taken
into account for a part if the pixel is

102
00:06:29,031 --> 00:06:32,210
within the specific part.
So our network can work,

103
00:06:32,211 --> 00:06:37,211
but it requires a lot of tasks for a
single network like parts segmentation and

104
00:06:37,401 --> 00:06:41,720
pixel localization. They use a
technique called region of interest.

105
00:06:41,780 --> 00:06:45,380
Pooling to create different
regions and fed the results,

106
00:06:45,381 --> 00:06:47,840
seeing features into
regions specific branches.

107
00:06:48,140 --> 00:06:52,760
This decompose the complexity of
the task into controllable modules,

108
00:06:52,820 --> 00:06:56,480
all of which which could be trained
jointly in an end to end approach.

109
00:06:56,660 --> 00:07:01,220
So it's a fully convolutional network on
top of region of interest pooling that

110
00:07:01,221 --> 00:07:03,440
is entirely devoted to two tasks,

111
00:07:03,680 --> 00:07:08,680
generating a classification and regression
head that provide part assignment and

112
00:07:08,871 --> 00:07:11,960
park coordinate predictions
to improve the model.

113
00:07:12,110 --> 00:07:17,110
They used a technique called cascading
meaning using a collection of models

114
00:07:17,510 --> 00:07:22,130
using all the information collected from
the output of one model as additional

115
00:07:22,131 --> 00:07:25,550
information for the next
classifier in the cascade.

116
00:07:25,880 --> 00:07:30,590
The output of the region of interest
align module feeds into the dense network

117
00:07:30,710 --> 00:07:34,580
as well as networks for the
masking and key points tasks.

118
00:07:35,300 --> 00:07:39,560
The first stage predictions from all
tasks are then combined and fed into a

119
00:07:39,561 --> 00:07:44,510
second stage refinement units of each
branch when experimenting on novel videos.

120
00:07:44,660 --> 00:07:49,520
There architecture worked well across
the board for not just one human but

121
00:07:49,580 --> 00:07:50,720
multiple humans.

122
00:07:50,990 --> 00:07:54,590
Unfortunately they haven't released
the initial source code just yet,

123
00:07:54,890 --> 00:07:58,520
but we can recreate what they did
using the details in the paper. Also,

124
00:07:58,521 --> 00:08:02,120
the Dataset they created is open source.
So definitely check that out.

125
00:08:02,150 --> 00:08:04,640
Three things to remember from this video.

126
00:08:04,910 --> 00:08:09,500
Dense pose is a new deep learning model
that can estimate three d models of

127
00:08:09,530 --> 00:08:12,890
multiple humans from just
a video on a single GPU.

128
00:08:13,310 --> 00:08:17,810
They use a collection of convolutional
networks to do this performing

129
00:08:17,900 --> 00:08:22,900
classification for assignment and
regression for coordinate to predictions.

130
00:08:23,660 --> 00:08:27,980
And they open sourced the data set
they created mapping humans to their

131
00:08:27,981 --> 00:08:31,940
corresponding three d models. Please
subscribe for more programming videos.

132
00:08:31,941 --> 00:08:35,420
And for now, I've got to make a good
bipolar, so thanks for watching.

