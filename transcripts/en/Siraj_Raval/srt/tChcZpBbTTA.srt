1
00:00:00,180 --> 00:00:00,900
Hello world,

2
00:00:00,900 --> 00:00:05,550
it's Saroj and if you only have a little
bit of data can you still learn from it?

3
00:00:05,790 --> 00:00:10,140
We're going to build a model that learns
to classify images using a very small

4
00:00:10,141 --> 00:00:14,370
dataset for training. The pointing
is still very much a dark heart.

5
00:00:14,580 --> 00:00:18,300
It's an emerging practice in the world
of machine learning that isn't well

6
00:00:18,301 --> 00:00:21,600
understood even by those
pushing the state of the art,

7
00:00:21,870 --> 00:00:26,310
which is exciting because it means that
there's so much potential for discovery

8
00:00:26,760 --> 00:00:31,380
and it's not just one algorithm. It's
a collection of them. Recurrent Nets,

9
00:00:31,530 --> 00:00:35,490
convolutional nets, restricted
ultimate machines, Barbra Streisand.

10
00:00:36,780 --> 00:00:41,160
These are all networks that can
recognize patterns in the world and they

11
00:00:41,161 --> 00:00:43,350
themselves have shared patterns.

12
00:00:43,590 --> 00:00:47,130
One shared pattern is that they
all use a hierarchy of layers.

13
00:00:47,190 --> 00:00:51,360
The other is using differentiable
layers so that we can use gradient based

14
00:00:51,361 --> 00:00:53,910
optimization to improve their prediction.

15
00:00:54,090 --> 00:00:57,030
Design patterns are nothing
new in computer science.

16
00:00:57,030 --> 00:01:01,320
There are many books on design patterns
for topics like object oriented

17
00:01:01,380 --> 00:01:05,970
programming and user interfaces.
When it comes to deep learning,

18
00:01:06,180 --> 00:01:09,180
there's not really a definitive
design pattern guide.

19
00:01:09,210 --> 00:01:11,970
We're all kind of figuring it
out collectively right now.

20
00:01:12,150 --> 00:01:15,930
You might glaze over a deep learning
paper and assume that there is some solid

21
00:01:15,931 --> 00:01:20,370
mathematical foundation behind what the
researchers attempted because you see

22
00:01:20,430 --> 00:01:25,230
all sorts of equations for things like
Hilbert spaces and measure theory.

23
00:01:25,350 --> 00:01:29,340
But the reality is that our collective
understanding is still pretty minimal.

24
00:01:29,460 --> 00:01:33,600
Theories are often formulated because
they are mathematically convenient.

25
00:01:33,660 --> 00:01:37,020
For example, the dossier and
distribution is ubiquitous,

26
00:01:37,260 --> 00:01:41,610
not because it's some divine construct
that the universe has bestowed on us,

27
00:01:41,790 --> 00:01:44,370
but because it's
mathematically convenient.

28
00:01:44,580 --> 00:01:48,900
So defining standard design patterns for
pattern recognition networks is a field

29
00:01:48,901 --> 00:01:51,600
right for discovery.
In the context of machine learning,

30
00:01:51,720 --> 00:01:54,930
we can call it metal learning
or learning to learn.

31
00:01:55,170 --> 00:01:58,470
Can we design a system that
learns how best to learn? That is.

32
00:01:58,471 --> 00:02:02,250
It learns how to perform
well at an immediate task
in the short term and in the

33
00:02:02,280 --> 00:02:06,030
longterm it learns a common
structure across many tasks.

34
00:02:06,240 --> 00:02:10,500
We see metal level constructs in nature
all the time. DNA is a great example.

35
00:02:10,501 --> 00:02:11,820
It carries the instructions,

36
00:02:12,090 --> 00:02:16,110
the blueprint to create learning
systems that can expire our brains,

37
00:02:16,410 --> 00:02:20,790
but it acts as longterm memory by
transcending death. Just like Oracle.

38
00:02:21,120 --> 00:02:25,200
As long as there is a mechanism for
memory and one to alter behavior based on

39
00:02:25,201 --> 00:02:29,010
that memory and that mechanism can
serve as a metal level construct.

40
00:02:29,250 --> 00:02:30,150
In the past few months,

41
00:02:30,151 --> 00:02:33,390
there have been several papers on
metal learning that had been published,

42
00:02:33,391 --> 00:02:38,220
but I want to talk about one that uses
metal learning as a tool to solve another

43
00:02:38,221 --> 00:02:38,880
task.

44
00:02:38,880 --> 00:02:43,830
One shot learning the goal of learning
from one or only a few data points.

45
00:02:44,010 --> 00:02:47,640
This is what we should be aiming for.
Since GPU costs are too damn ha,

46
00:02:48,150 --> 00:02:52,200
they used a modified version of a
model called a neural Turing machine.

47
00:02:52,440 --> 00:02:56,310
The learned declassified character
images with just a few examples.

48
00:02:56,790 --> 00:03:01,540
The mind first raise up the idea of
Mtms in 2014 it contains two components.

49
00:03:01,630 --> 00:03:05,020
The first is a neural network that
we call the controller and the other,

50
00:03:05,021 --> 00:03:09,250
it's a memory bank. The controller
takes vectors as inputs and outputs,

51
00:03:09,251 --> 00:03:11,290
vectors as well,
just like all neural nets.

52
00:03:11,320 --> 00:03:16,320
But what makes it special is that it
also interacts with a memory matrix using

53
00:03:16,811 --> 00:03:20,860
read and write operations. This is where
the Turing machine analogy comes from,

54
00:03:21,040 --> 00:03:22,210
not just cause it sounds dope,

55
00:03:22,540 --> 00:03:27,400
but because a turning machine manipulates
symbols on a strip of tape according

56
00:03:27,401 --> 00:03:28,930
to a table of rules.

57
00:03:28,990 --> 00:03:33,400
It's like having a working memory for a
brain and network learns how best to use

58
00:03:33,401 --> 00:03:36,850
its memory when learning a solution to
a given problem. For the controller,

59
00:03:36,851 --> 00:03:38,770
they use an LSTM recurrent network.

60
00:03:38,830 --> 00:03:42,820
Since it's internal state is a function
of the current state and the input to

61
00:03:42,821 --> 00:03:46,090
the system. It can perform
context dependent computation.

62
00:03:46,091 --> 00:03:50,980
So a signal at a current time step can
influence the network's behavior later on.

63
00:03:51,280 --> 00:03:55,990
We need all the components including
the memory store to be differentiable so

64
00:03:55,991 --> 00:04:00,430
that we can incrementally update their
values during training. To achieve this,

65
00:04:00,431 --> 00:04:04,990
they added an attention mechanism so
that each read and write operation

66
00:04:05,110 --> 00:04:09,130
interacts to a tuneable degree
with all the elements in memory.

67
00:04:09,131 --> 00:04:13,120
Rather than addressing a single element
like a normal touring machine would.

68
00:04:13,180 --> 00:04:16,840
Each row in the memory matrix
represents a memory location.

69
00:04:16,990 --> 00:04:21,700
Read and write heads use a weighting
vector with a component for each location.

70
00:04:21,880 --> 00:04:26,590
So if there are 10 memory locations and
the weighting vector with just one value

71
00:04:26,591 --> 00:04:27,131
at index,

72
00:04:27,131 --> 00:04:32,131
three would focus the attention of the
memory operation on location three but a

73
00:04:32,291 --> 00:04:37,210
weighting vector like this spreads its
attention to the memory across multiple

74
00:04:37,211 --> 00:04:38,044
locations.

75
00:04:38,320 --> 00:04:43,090
A re operation is just a combination of
the memory matrix and weighting vector.

76
00:04:43,330 --> 00:04:48,330
A write operation though has two parts
on a race operation than an ad operation.

77
00:04:48,460 --> 00:04:52,960
Way these read and write heads are
produced is by combining two memory

78
00:04:52,961 --> 00:04:56,440
addressing mechanisms.
The first is content based.

79
00:04:56,590 --> 00:05:01,590
We focus on locations based
on the similarity between
their current values and

80
00:05:01,661 --> 00:05:06,040
the controllers emitted values.
The second is location based.

81
00:05:06,220 --> 00:05:11,220
It puts silicates iterations across
locations of the memory and random access

82
00:05:11,651 --> 00:05:12,484
jump,

83
00:05:15,740 --> 00:05:16,573
right?

84
00:05:22,700 --> 00:05:22,850
So

85
00:05:22,850 --> 00:05:26,080
the authors of our one shot
learning paper knew that Mtms,

86
00:05:26,081 --> 00:05:30,890
we're a subset of memory augmented neural
networks and they saw the potential to

87
00:05:30,920 --> 00:05:33,860
improve on it so that they could
learn from just a little data.

88
00:05:34,160 --> 00:05:38,540
They discovered that using a pure content
based memory writer instead of content

89
00:05:38,570 --> 00:05:41,150
plus location,
let them do just this.

90
00:05:41,210 --> 00:05:45,440
That's because there's a trade off
when training man s more complex.

91
00:05:45,441 --> 00:05:49,040
The memory mechanism,
the more training the controller requires,

92
00:05:49,250 --> 00:05:54,250
the data set has 1600 separate classes
and only a few examples per class.

93
00:05:54,440 --> 00:05:55,880
Perfect for one shot learning,

94
00:05:56,210 --> 00:06:01,210
they randomly five classes and randomly
assigned each class a label between one

95
00:06:01,730 --> 00:06:03,800
and five so the model gets shown.

96
00:06:03,801 --> 00:06:08,270
An instance of a class tries to classify
it and it gets informed of what the

97
00:06:08,271 --> 00:06:09,590
correct label is.

98
00:06:09,740 --> 00:06:14,210
We'll only need tensorflow and num Pi for
our model will first define our memory

99
00:06:14,211 --> 00:06:17,180
bank and internalizing each of
the variables that make it up.

100
00:06:17,390 --> 00:06:19,070
Then we can define our controller.

101
00:06:19,071 --> 00:06:24,071
A feedforward neural network will define
each set of weights and biases layer by

102
00:06:24,801 --> 00:06:27,230
layer until we've
reached the output layer.

103
00:06:27,500 --> 00:06:31,280
We can define the interaction that
happens between both components under the

104
00:06:31,281 --> 00:06:35,060
step function, which is called
every time step. During training,

105
00:06:35,420 --> 00:06:37,250
just like with a regular NTM,

106
00:06:37,400 --> 00:06:42,140
we read a vector from memory that is a
linear combination of it's rose scaled,

107
00:06:42,280 --> 00:06:45,680
a normalized wait vector
for the given input x.

108
00:06:45,800 --> 00:06:47,870
The read vector will produce a key.

109
00:06:48,110 --> 00:06:53,110
We compare each key against each row in
memory using the cosine similarity as a

110
00:06:53,451 --> 00:06:56,090
measure.
This produces the read wait vector,

111
00:06:56,091 --> 00:07:00,650
which tells us how much each row should
contribute to the linear combination.

112
00:07:00,800 --> 00:07:04,910
The difference here is that there is no
extra perimeter to control the read wait

113
00:07:04,911 --> 00:07:07,640
vectors concentration to write to memory.

114
00:07:07,641 --> 00:07:12,050
The controller interpolate between writing
to the most recently read memory rose

115
00:07:12,260 --> 00:07:15,020
and writing to the least
used memory recipes.

116
00:07:15,290 --> 00:07:19,550
Using the read wait vector at a previous
time step and the weight vector that

117
00:07:19,551 --> 00:07:21,680
captures the least used memory location.

118
00:07:21,920 --> 00:07:26,570
The controller combines the two using
a scalar parameter and the sigmoid

119
00:07:26,571 --> 00:07:29,300
function to create a right wait vector.

120
00:07:29,600 --> 00:07:34,600
Each row in memory is then updated using
the right way vector and the key issued

121
00:07:34,701 --> 00:07:35,600
by the controller.

122
00:07:35,930 --> 00:07:39,770
The model eventually returns the
probabilities for each class as a Vectra.

123
00:07:40,040 --> 00:07:42,380
After we've initialize
our tensorflow session,

124
00:07:42,590 --> 00:07:46,340
we use gradient descent via
Adam to optimize our network.

125
00:07:46,370 --> 00:07:49,940
For every image labeled pair
we feed in via a dictionary.

126
00:07:50,270 --> 00:07:52,790
We'll print out our results
iteratively after training.

127
00:07:52,791 --> 00:07:57,320
We can test it out on some different
recognizable characters and notice how the

128
00:07:57,321 --> 00:07:59,180
accuracy is surprisingly good.

129
00:07:59,450 --> 00:08:02,900
Normally training time would take
a lot longer for similar results.

130
00:08:03,140 --> 00:08:07,940
These results are very promising for
lunch and learning. That's all it takes.

131
00:08:07,941 --> 00:08:10,820
Some training folks.
Let's get down to brass tacks.

132
00:08:10,940 --> 00:08:15,020
A metal learning system learns how to
perform well at an immediate task and also

133
00:08:15,021 --> 00:08:18,710
learns a common structure
across many tasks. Memory,

134
00:08:18,740 --> 00:08:23,180
augmented neural networks like the neural
Turing machine use a controller and an

135
00:08:23,181 --> 00:08:27,230
external memory store to perform metal
learning and metal learning can be a way

136
00:08:27,231 --> 00:08:31,910
to achieve one shot learning, which means
learning from one or a few examples.

137
00:08:32,180 --> 00:08:35,780
This week's coding challenge is to use
a memory augmented network to learn to

138
00:08:35,781 --> 00:08:39,470
classify two classes of animals.
He tells her and the read me get humbling.

139
00:08:39,471 --> 00:08:42,770
It's going to comments and winners
will be announced in one week,

140
00:08:42,890 --> 00:08:45,590
and although this is the
last video for this course,

141
00:08:46,100 --> 00:08:47,270
I'm still just getting started.

142
00:08:47,330 --> 00:08:50,480
Please subscribe for more programming
videos and for now I've got to go

143
00:08:50,500 --> 00:08:52,580
celebrate.
So thanks for watching.

