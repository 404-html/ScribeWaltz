1
00:02:14,850 --> 00:02:18,060
He won't work in the go live. Are
we ready? All right. Hello world.

2
00:02:18,090 --> 00:02:22,410
It's Saroj and welcome to
this live stream. I'm so
excited to see everybody here.

3
00:02:22,710 --> 00:02:26,100
What I'm going to be doing in this
livestream is I'm going to be solving this

4
00:02:26,101 --> 00:02:30,360
Kaggle challenge. It's called the
invasive species monitoring challenge.

5
00:02:30,361 --> 00:02:35,361
There's a lot of plants that are possibly
going to destroy the ecology of this

6
00:02:35,671 --> 00:02:39,750
area and what we can do is we can
identify those images with those invasive

7
00:02:39,751 --> 00:02:43,980
plants. And the way I'm going to
do this is using Pi torch. Okay,

8
00:02:43,981 --> 00:02:48,480
so Pi Torch is a shout out to all the
Pi Torch lovers in the house right now.

9
00:02:48,750 --> 00:02:53,400
Pi Torch is a neural network library
and it was invented by Facebook's AI

10
00:02:53,401 --> 00:02:56,370
research lab and it's been
gaining a lot of traction,

11
00:02:56,400 --> 00:03:01,180
especially in the research community.
Now, tensor flow is the leading, you know,

12
00:03:01,210 --> 00:03:05,680
neural networks, deep learning library
out there, and most people use tensorflow.

13
00:03:05,860 --> 00:03:09,730
However, when it comes to research,
I'm seeing a lot of Pi torch happening,

14
00:03:09,731 --> 00:03:13,150
and in this video I'm going to describe
some of the unique features of Pi Torch.

15
00:03:13,151 --> 00:03:15,970
We're going to code some pie torch.
We're going to solve this challenge.

16
00:03:16,150 --> 00:03:19,390
We're going to learn about resonance,
a res residual networks,

17
00:03:19,391 --> 00:03:22,990
which is a type of neural network
that has this really cool idea of skip

18
00:03:22,991 --> 00:03:25,990
connections. So I'm going to talk about
that. There's an identity mapping.

19
00:03:25,991 --> 00:03:28,690
There's some math, there's a little bit
of everything. You know, you guys know,

20
00:03:28,720 --> 00:03:32,200
you guys know what the deal is.
Okay, so let's get started with this.

21
00:03:32,201 --> 00:03:36,220
Thank you everybody for coming here
today. I've got a little, yeah, yeah.

22
00:03:36,250 --> 00:03:39,280
Pi Torch is amazing, right?
All right, so here we go.

23
00:03:39,281 --> 00:03:42,010
So Pi Torch versus tensorflow,
right?

24
00:03:42,070 --> 00:03:45,580
I've got this little graph of like
research use cases versus production,

25
00:03:45,610 --> 00:03:49,480
Google versus Facebook, but let's talk
about some of the unique features, right?

26
00:03:49,480 --> 00:03:52,780
So we're going to just compare it with
tensorflow just because tensor flow is

27
00:03:52,781 --> 00:03:54,060
the, you know, the,

28
00:03:54,061 --> 00:03:58,420
the most used in terms of developer
activity on get hub library out there.

29
00:03:58,810 --> 00:04:03,250
If we look at the documentation,
I think tensorflow clearly wins here.

30
00:04:03,460 --> 00:04:07,060
I mean the tensorflow
documentation is massive because a,

31
00:04:07,061 --> 00:04:10,750
it's been around so long. B, they really
putting an effort into it. And when I,

32
00:04:11,020 --> 00:04:15,160
when I say documentation, I'm not just
talking about the API docs on the website,

33
00:04:15,161 --> 00:04:19,060
I'm talking about good hub. I'm
talking about number of, of examples.

34
00:04:19,061 --> 00:04:20,770
I'm talking about video content.

35
00:04:21,040 --> 00:04:24,220
I'm talking about a lot of different
documentation, whether it's on youtube,

36
00:04:24,221 --> 00:04:26,320
you know,
whatever tension flow clearly wins.

37
00:04:26,321 --> 00:04:31,300
Whereas if you go to the Pi torch
documentation as I, as I am right now, um,

38
00:04:31,360 --> 00:04:34,270
let's make me a little smaller on the
corner so we can see this documentation.

39
00:04:35,770 --> 00:04:39,030
If we look at the, the Pi
Torch Documentation, uh,

40
00:04:39,250 --> 00:04:42,940
if you look at any of the functions,
well these are, these are like guides,

41
00:04:42,941 --> 00:04:46,120
but let's look at some of the
functions like Auto Grad for example.

42
00:04:46,600 --> 00:04:48,850
They usually just a single
page for a function,

43
00:04:48,851 --> 00:04:52,720
whereas I feel like there should
be more here. Uh, but yeah,

44
00:04:52,840 --> 00:04:56,890
I think tentraflow wins in, in, in
terms of documentation. But we're a Pi.

45
00:04:56,891 --> 00:05:01,190
Torch wins is when it comes to the
graph itself, the computation graph.

46
00:05:01,191 --> 00:05:05,350
So this is an image of a neural network
that was built in tensorflow that we're

47
00:05:05,351 --> 00:05:08,470
visualizing intense or board,
which is the visualization tool.

48
00:05:09,100 --> 00:05:12,160
And if you think about it,
neural networks are computation graphs.

49
00:05:12,161 --> 00:05:15,970
There are just chains of operations
applied to variables. And it's just the,

50
00:05:16,030 --> 00:05:18,490
just the chain of that over
and over and over again, right?

51
00:05:18,670 --> 00:05:22,090
Whether it's a dot product operation,
whether it's an identity mapping,

52
00:05:22,091 --> 00:05:22,930
whatever it is,

53
00:05:23,140 --> 00:05:26,710
you have a variable apply operation and
just keep doing that until you get an

54
00:05:26,711 --> 00:05:31,210
output. So there's a difference
here, right? So what Pi Torch does,

55
00:05:31,211 --> 00:05:34,660
and this is what I believe is the
really the key feature of Pi Torch.

56
00:05:34,661 --> 00:05:36,790
What really sets it out
from the, from the, um,

57
00:05:37,090 --> 00:05:39,070
competitors here or the other libraries,

58
00:05:39,250 --> 00:05:41,890
is the fact that you can
create a dynamic graph.

59
00:05:41,891 --> 00:05:46,780
So it's a graph is created at runtime
dynamically. So just like this,

60
00:05:46,781 --> 00:05:51,190
as you're saying in this gift here
at runtime, the graph is built,

61
00:05:51,191 --> 00:05:55,860
whereas with tensorflow, it's a static
computation graph. Okay. So at runtime,

62
00:05:55,870 --> 00:06:00,260
the graph already was at compile time
and now it's just static right there.

63
00:06:00,530 --> 00:06:03,410
So why, why did tentraflow
do this? Well, first of all,

64
00:06:03,411 --> 00:06:05,150
for for production use cases,

65
00:06:05,151 --> 00:06:09,500
it's very valuable to have a static
computation graph and their speed reasons

66
00:06:09,501 --> 00:06:11,690
for that. There's distributed
training reasons for that.

67
00:06:12,080 --> 00:06:15,590
But a lot of the newer neural
network architectures in research,

68
00:06:15,620 --> 00:06:17,780
especially any of the recurrent models,
right?

69
00:06:17,780 --> 00:06:19,820
Where there's this recursion happening,

70
00:06:20,660 --> 00:06:25,630
it's better to have that graph be
allocated dynamically at runtime.

71
00:06:25,670 --> 00:06:28,040
So the graph is built as it's being run.

72
00:06:28,370 --> 00:06:32,780
And the reason for this is because a
lot of times new operations are applied

73
00:06:32,900 --> 00:06:37,460
that wouldn't be applied before. So that's
the whole idea behind a dynamic graph,

74
00:06:37,461 --> 00:06:39,170
right?
You can make changes to it.

75
00:06:39,410 --> 00:06:43,840
There's no static nus status to city,
what's the word for what,

76
00:06:43,841 --> 00:06:46,910
what's like the Noun for
static? Well adjective. Anyway,

77
00:06:47,630 --> 00:06:48,910
it's more static and a,

78
00:06:48,940 --> 00:06:52,700
and so when it comes to these newer
recurrent architectures with attention

79
00:06:52,701 --> 00:06:57,440
mechanisms, with um, you know,
LSTM networks, gru networks, uh,

80
00:06:57,441 --> 00:06:58,850
and now with the new one,

81
00:06:58,851 --> 00:07:03,680
which I have an amazing video coming out
on by the way now Lou, um, which, um,

82
00:07:03,710 --> 00:07:06,860
you know, Trask released a paper on.
I know you guys are waiting for that.

83
00:07:06,861 --> 00:07:09,200
I've been waiting for that but
don't worry, it's coming on Sunday.

84
00:07:09,200 --> 00:07:12,590
It's a sick video. I have so many
videos coming out this weekend.

85
00:07:12,591 --> 00:07:14,910
You have no idea. Open AI five, it's,

86
00:07:14,930 --> 00:07:18,650
it's going to be literally social media
will light up when these videos come out.

87
00:07:18,651 --> 00:07:23,420
By the way. Now back to this, the
computation graph is different.

88
00:07:23,421 --> 00:07:27,470
You have a static versus dynamic graph.
So Pi Torch wins in this case. However,

89
00:07:27,471 --> 00:07:32,240
I want to note that,
uh, that Nalo exactly.

90
00:07:32,720 --> 00:07:37,310
I want to note that tensorflow did like
kind of tack this on later with the

91
00:07:37,311 --> 00:07:41,630
imperative graph and that works too.
So there is a way,

92
00:07:41,631 --> 00:07:45,890
however it's not native and the way that
it was with Pi Torch for visualizations

93
00:07:45,891 --> 00:07:49,010
tensor board clearly wins.
Pi Torch has got nothing,

94
00:07:49,040 --> 00:07:50,780
at least from the Pi Torch team.

95
00:07:51,020 --> 00:07:55,100
I don't know about any like random rogue
hacker on get hub who created some this

96
00:07:55,101 --> 00:08:00,080
library. But tensor board is king here.
Tensorflow wins for debugging. Okay.

97
00:08:00,290 --> 00:08:02,000
Pi Torch wins.
Okay.

98
00:08:02,001 --> 00:08:06,980
Pi Torch wins for debugging because
there's a TFD bugger called TF DBG.

99
00:08:07,220 --> 00:08:11,240
Um, but a lot of people, there's
a lot of github issues about it,

100
00:08:11,241 --> 00:08:14,990
not working about it, not properly
documenting things that are going wrong,

101
00:08:15,260 --> 00:08:19,730
whereas people just tend to generally
loved Pi. Torch is built in debugger.

102
00:08:20,630 --> 00:08:24,560
So Pi Torch wins there, right? So rather
than like, you know, just saying, print,

103
00:08:24,561 --> 00:08:27,770
print, print, print prints, you can
actually use these, uh, you know,

104
00:08:27,771 --> 00:08:31,170
set trace functions that say PDB has,
which is beautiful.

105
00:08:31,171 --> 00:08:36,020
And it gives you a very detailed stack
trace about what you need. And lastly,

106
00:08:36,021 --> 00:08:37,670
interoperability,
right?

107
00:08:37,671 --> 00:08:40,970
So sometimes we can't just do
everything natively in those languages.

108
00:08:40,971 --> 00:08:44,330
We have to switch to c plus plus or Kuta.

109
00:08:44,540 --> 00:08:47,930
And we need a language that can talk
to those in a very easy to use way.

110
00:08:48,170 --> 00:08:50,000
And in terms of interoperability,

111
00:08:52,580 --> 00:08:56,430
I think Pi two are twins here because
it's all about writing code that among the

112
00:08:56,431 --> 00:09:00,450
different versions of CPS and
gps and as you can see here, um,

113
00:09:00,630 --> 00:09:05,220
it's very easy to have an
interoperable graph. Okay. So that's,

114
00:09:05,250 --> 00:09:08,950
those are just some of the features that
I wanted to talk about. And now, uh,

115
00:09:09,000 --> 00:09:10,620
before I actually,

116
00:09:12,930 --> 00:09:15,120
before I actually,
um,

117
00:09:16,140 --> 00:09:20,280
I so guys I want to say I'm not the
Messiah by the way because I had been

118
00:09:20,281 --> 00:09:24,090
watching wild, wild country on Netflix
and I don't want to be that guy.

119
00:09:24,300 --> 00:09:26,610
I am not a god.
I'm one of you.

120
00:09:26,670 --> 00:09:29,760
I'm a developer and don't
worship me as a god. Okay.

121
00:09:29,761 --> 00:09:31,350
Cause that's not what we're doing here.

122
00:09:31,530 --> 00:09:35,550
I am just another developer and we
don't want that to be the case. Okay.

123
00:09:35,551 --> 00:09:39,570
So anyway, let's code outs in pie
charts. If you want to call me a god,

124
00:09:39,571 --> 00:09:42,060
that's fine though. No, I'm just
kidding. Anyway, so if, lets,

125
00:09:42,061 --> 00:09:43,740
let's just coat out some
simple pie torch here.

126
00:09:43,741 --> 00:09:47,970
So all we have to do is say import
torch and we have some pilots going on.

127
00:09:47,971 --> 00:09:49,350
See I just compiled that.
Okay,

128
00:09:49,351 --> 00:09:53,940
so now what I want to do is I want to
build a simple neural networks so we could

129
00:09:53,941 --> 00:09:56,430
just see like how this works. Okay. So
I'm going to do this really quickly.

130
00:09:56,431 --> 00:09:59,340
I'm going to, I'm going to hack
out a script here using Pi torch.

131
00:10:00,780 --> 00:10:03,400
And once we do that,
uh,

132
00:10:03,600 --> 00:10:07,590
then we're going to have our neural net.
Okay?

133
00:10:07,591 --> 00:10:11,250
So simple to layer network. Okay? So,

134
00:10:11,251 --> 00:10:15,060
so far the torch class has a module and
I'm going to show you exactly what that

135
00:10:15,061 --> 00:10:19,020
module is going to let us do.
We can build a linear layer very easily.

136
00:10:21,340 --> 00:10:26,290
Uh, right, exactly. Yeah,
you guys are hilarious.

137
00:10:26,980 --> 00:10:28,540
Okay. So, right,

138
00:10:28,541 --> 00:10:31,840
we've got to have the super function
as in any like initialization.

139
00:10:31,841 --> 00:10:35,650
We've got to have our super function
and once we have our super function to

140
00:10:35,651 --> 00:10:39,490
initialize our constructor,
we can say,

141
00:10:40,980 --> 00:10:44,040
now, after I build this, we're
going to start looking at some,

142
00:10:44,070 --> 00:10:46,290
we're going to do some
exploratory data analysis,

143
00:10:46,650 --> 00:10:49,350
but I first want to just write
out the script. So we have, um,

144
00:10:51,010 --> 00:10:54,730
we have something there for us.
Okay? So in my initialization script,

145
00:10:54,940 --> 00:10:58,240
I'm going to be using the input,
I'm going to have it hidden states,

146
00:10:58,241 --> 00:11:02,230
and I'm going to have an output, right?
So d, n h, a n, d out. That's my input,

147
00:11:02,231 --> 00:11:05,260
the hidden state and my output.
And once I have that,

148
00:11:05,320 --> 00:11:09,310
now I can create this super function.
And so inside of this,

149
00:11:09,550 --> 00:11:11,230
I'll have my first linear layer.

150
00:11:11,440 --> 00:11:16,330
So torches neural net module allows us
to create linear layers just like this.

151
00:11:16,630 --> 00:11:19,900
Now the inputs to the linear layer
is going to be the input data,

152
00:11:19,901 --> 00:11:24,340
and then its output is going
to be the hidden state. Okay?

153
00:11:24,460 --> 00:11:29,000
And I see that some people say
that I have a battery issue. Well,

154
00:11:29,030 --> 00:11:31,750
that the battery issue is gone.
All right, so back to this.

155
00:11:32,110 --> 00:11:35,890
So for our linear layer, and we want
to make sure the code is very readable.

156
00:11:35,890 --> 00:11:39,670
So what I'm gonna do is I'm going to
put it right here and I hope it's very

157
00:11:39,671 --> 00:11:43,240
readable to you. Okay. I hope
you can zoom in a little bit.

158
00:11:43,241 --> 00:11:46,780
Let me see how much I can zoom in. I
can't really, uh, please do into the code.

159
00:11:47,200 --> 00:11:49,390
All right. Let's see. You
know what? Let's just,

160
00:11:49,810 --> 00:11:53,410
let's just strip this out and like sublime
or something so you guys can really

161
00:11:53,710 --> 00:11:56,470
see what the, that's some Nalo
Code. Don't look at now Lou guys,

162
00:11:56,471 --> 00:12:01,030
I was just making this totally look at
this sick code by the way. I was, uh,

163
00:12:01,060 --> 00:12:04,330
anyway, uh, where were we? Right?

164
00:12:04,331 --> 00:12:09,331
So let me make this bigger and we
make it in python and then we're good.

165
00:12:10,420 --> 00:12:13,930
We're good to go. So that way, cause
sometimes with, you know these like,

166
00:12:15,400 --> 00:12:19,930
okay, no, I'll just call it torch
dot pie, but around, on the desktop,

167
00:12:20,200 --> 00:12:22,450
boom. Torched. Okay,

168
00:12:22,840 --> 00:12:26,560
so torched by Board Yon Lacuna on Twitter.
Okay,

169
00:12:26,800 --> 00:12:29,260
so now we have the hidden state and now
we're going to have the output, right?

170
00:12:29,261 --> 00:12:33,040
So the input to the new hidden state is
he the input to the next layer is going

171
00:12:33,041 --> 00:12:36,340
to be the output from the previous layer.
We know that. Okay, so very simple stuff.

172
00:12:37,060 --> 00:12:41,180
Now what else do we need? We need a
forward methods. So for some reason,

173
00:12:41,200 --> 00:12:44,080
Pi Torch code,
usually names there,

174
00:12:44,260 --> 00:12:49,210
forward propagation method as forward
instead of just like build model.

175
00:12:49,211 --> 00:12:54,070
Usually this is called build model,
but a lot of Pi Torch code uses, um,

176
00:12:55,150 --> 00:12:55,661
forward.

177
00:12:55,661 --> 00:13:00,370
And I think the reason for that is
because forward propagation is dynamic is

178
00:13:00,371 --> 00:13:01,920
right.
It's happening at runtime.

179
00:13:01,930 --> 00:13:06,930
So people who are writing Pi Torch Code
want to be very specific about what they

180
00:13:09,431 --> 00:13:14,260
are doing and to be
specific and to be real, uh,

181
00:13:14,710 --> 00:13:15,860
it is better

182
00:13:16,070 --> 00:13:19,610
call, you know, whatever.

183
00:13:22,210 --> 00:13:24,640
It's better to call it forward
because that's what it is. Okay.

184
00:13:24,641 --> 00:13:27,130
So we've got the forward code.
Now I need to have my,

185
00:13:29,290 --> 00:13:33,430
um, what else do I need? A batch
size. So that's an n is batch size.

186
00:13:33,460 --> 00:13:36,430
I'm going to have our input,
my hidden states and my output.

187
00:13:36,820 --> 00:13:41,710
And then it's going to be called
64 1,110 so these are going to be,

188
00:13:41,711 --> 00:13:46,030
my name's from my number, my batch size.
So now I can create some random tensors.

189
00:13:46,031 --> 00:13:48,950
So the great thing
about torch is it's got,

190
00:13:48,970 --> 00:13:52,090
it's kind of like num
Pi is built into torch,

191
00:13:52,270 --> 00:13:54,580
so we don't actually have to use num py,

192
00:13:55,120 --> 00:13:57,760
num pies basically built into torch.

193
00:13:58,000 --> 00:14:01,550
So any kind of like matrix multiplication
that we want to do, uh, it,

194
00:14:01,551 --> 00:14:05,080
it's all happening inside of Pi Torch.
Okay.

195
00:14:06,910 --> 00:14:10,480
And let me just see what else people
are going to hear. Jupiter is fun. Cool.

196
00:14:10,481 --> 00:14:14,590
Zooming. Yeah, great. Much love
to everybody as well. Of course,

197
00:14:14,950 --> 00:14:18,550
of course. By the way, Dean
applications, I'm, we're, we're, um,

198
00:14:18,640 --> 00:14:22,240
we're almost done reviewing everything
and uh, that's coming out this weekend.

199
00:14:22,570 --> 00:14:24,730
So there's a lot.
That's a lot that's happening.

200
00:14:25,090 --> 00:14:27,340
I also have my masterpiece
that's coming out,

201
00:14:27,341 --> 00:14:31,300
but I can't talk about that right now. My
masterpiece is, is, is coming out soon.

202
00:14:31,490 --> 00:14:34,180
Don't worry about it. Uh, right. So,

203
00:14:34,181 --> 00:14:37,450
so we created random tensors
to hold inputs and outputs.

204
00:14:37,451 --> 00:14:41,560
So what are tensors tensors or
end dimensional vectors, right?

205
00:14:41,561 --> 00:14:43,990
So you have one dimension.
So that's just a group of numbers.

206
00:14:44,120 --> 00:14:47,260
You have two dimensions. It's uh,
you know, two dimensional. It's a,

207
00:14:47,261 --> 00:14:50,830
it's an XL spreadsheet, three dimensions,
four dimensions, you know, et cetera.

208
00:14:50,831 --> 00:14:54,680
So the generalization of the
vector format for end dimensions,

209
00:14:54,681 --> 00:14:59,270
which is what we need because we don't
know how many dimensions are input data

210
00:14:59,271 --> 00:15:01,340
is going to be.
And so now that we have our model,

211
00:15:01,341 --> 00:15:03,680
we can just build it just like this,
using our input,

212
00:15:03,690 --> 00:15:08,120
our hidden state in our output as the, uh,

213
00:15:09,290 --> 00:15:11,750
the values. By the way,
if you're new here,

214
00:15:11,751 --> 00:15:16,070
hit the subscribe button to stay up to
date with my latest AI content. Okay?

215
00:15:16,760 --> 00:15:19,520
All right. So, um, and tell your
friends because I'm not going anywhere.

216
00:15:19,521 --> 00:15:24,290
I'm going to be doing this until I
will never stop. So get ready for this.

217
00:15:25,070 --> 00:15:27,110
We have our loss function,
our loss function.

218
00:15:27,111 --> 00:15:29,360
We can choose between
multiple loss functions.

219
00:15:29,361 --> 00:15:33,440
Can anybody guess what I'm going to
use, right? We have cross entropy.

220
00:15:33,441 --> 00:15:36,740
We have what's, what's the most simple
loss function that we could use here?

221
00:15:37,040 --> 00:15:40,940
You should know this. If you've
watched at least three of my videos,

222
00:15:41,000 --> 00:15:44,750
you should notice the answer is
the mean squared error loss. Why?

223
00:15:44,751 --> 00:15:48,940
Because it's the most simple
of them all for uh, for

224
00:15:50,780 --> 00:15:52,910
demo purposes. So that's the one, right?

225
00:15:52,940 --> 00:15:54,950
We're going to find
the mean squared error.

226
00:15:54,951 --> 00:15:57,730
We're going to find a different
between our output in our, um,

227
00:15:59,680 --> 00:16:00,010
yeah.

228
00:16:00,010 --> 00:16:03,730
What is Pi Torch useful for?
For Research, for research,

229
00:16:03,731 --> 00:16:07,930
not for necessarily for production,
but for resort, for research.

230
00:16:08,320 --> 00:16:11,320
Now what's our optimizer going to be?
We all should know this.

231
00:16:11,350 --> 00:16:14,890
Every single person, even if
you've never seen one of my videos,

232
00:16:14,891 --> 00:16:17,950
you'll notice the answer is
to castic gradient descent,

233
00:16:18,280 --> 00:16:21,190
the most important algorithm
in machine learning.

234
00:16:21,460 --> 00:16:25,540
If you don't know how gradient
descent works, my friend,

235
00:16:25,570 --> 00:16:30,040
you need to know because it is the most
important to machine learning model that

236
00:16:30,041 --> 00:16:34,660
you need ever in, in machine
learning. Okay? Right? Our Algorithm

237
00:16:36,370 --> 00:16:38,440
now. Okay, so we've got that.

238
00:16:39,310 --> 00:16:43,810
I think this is supposed
to be, yeah, right. No.

239
00:16:45,700 --> 00:16:46,050
Okay.

240
00:16:46,050 --> 00:16:48,570
Now lastly, we're going to have our
training loop and then we're good. So

241
00:16:51,160 --> 00:16:51,670
yeah.

242
00:16:51,670 --> 00:16:55,630
Uh, so for 500 iterations we're
going to perform our forward pass.

243
00:16:55,900 --> 00:16:57,340
So why predict

244
00:17:00,130 --> 00:17:05,030
RMSC Adam? I see we have some opinionated
wizards in here, which I love to see.

245
00:17:05,450 --> 00:17:08,820
I love to see that.
Uh,

246
00:17:09,770 --> 00:17:11,210
so that's gonna be our forward pass.

247
00:17:11,211 --> 00:17:13,880
We're going to compute the predicted
y and once we have our prediction,

248
00:17:13,881 --> 00:17:17,510
what are we going to do? Guys? We know
how supervised learning works, right?

249
00:17:17,840 --> 00:17:21,680
We've done this so many times. If we
don't know how supervised learning works,

250
00:17:21,950 --> 00:17:24,770
this is the first step.
If you're new to machine learning,

251
00:17:25,010 --> 00:17:27,920
focus on supervised learning,
which is what I'm doing right now.

252
00:17:29,180 --> 00:17:32,300
Once you finish supervised learning,

253
00:17:32,510 --> 00:17:37,400
then you move on to unsupervised learning.
And what do you do when you finish? Uh,

254
00:17:37,660 --> 00:17:42,020
what, what do you do when
you finish unsupervised and
supervised learning? Who's,

255
00:17:42,021 --> 00:17:44,540
who's got the answer for me?
I'm going to wait for in the comments.

256
00:17:44,750 --> 00:17:49,750
Someone is going to say the
answer to what you do after you,

257
00:17:50,070 --> 00:17:50,903
um,

258
00:17:51,480 --> 00:17:55,730
learn both supervised and unsupervised
learning because I know somebody is going

259
00:17:55,731 --> 00:18:00,450
to know exactly what it is.
Okay. So now I'm going to and I,

260
00:18:00,810 --> 00:18:05,460
okay, cool. That's it. Dah, Dah,
Dah, Dah. Now what did I call this?

261
00:18:05,461 --> 00:18:06,750
So in my desktop

262
00:18:08,250 --> 00:18:10,620
I called this torch dot pie

263
00:18:12,160 --> 00:18:16,810
module attribute has no
attribute. Exactly. Reinforcement
learning. Yes. Good job.

264
00:18:16,811 --> 00:18:20,680
Everybody. Reinforcement
learning module has no attribute.

265
00:18:20,740 --> 00:18:24,090
Nn Yes. Okay. Okay. Okay. Okay.

266
00:18:27,010 --> 00:18:27,880
Let me see what's up.

267
00:18:32,760 --> 00:18:34,680
Uh,
let's see here.

268
00:18:37,950 --> 00:18:39,060
Oh, Gotcha. Gotcha.

269
00:18:41,320 --> 00:18:46,070
Torched dot nn module.
Let's see here. Let me,

270
00:18:49,990 --> 00:18:53,050
hmm. I wonder if I do this in Colab, what
the deal is. Well, actually I need to,

271
00:18:55,350 --> 00:18:58,500
we just paste it in here.
This usually

272
00:19:01,580 --> 00:19:02,413
goes for it.

273
00:19:02,720 --> 00:19:06,680
Oh, right. Spaces and indentations
and stuff. Right? Right.

274
00:19:07,280 --> 00:19:12,030
The forward,
oh my God.

275
00:19:16,570 --> 00:19:18,580
D learning.
Exactly.

276
00:19:20,880 --> 00:19:21,713
Let's try that out.

277
00:19:23,760 --> 00:19:26,730
Oh my God.
See?

278
00:19:26,731 --> 00:19:28,950
And now you're going to watch
me do debug in real time guys,

279
00:19:30,210 --> 00:19:33,120
because I've got some Kaggle to do.
I've got some capital to do.

280
00:19:34,350 --> 00:19:36,450
Oh my God.

281
00:19:39,300 --> 00:19:42,240
[inaudible] this is not okay guys.

282
00:19:42,241 --> 00:19:44,670
I'm talking about high torch Pi Torch.

283
00:19:52,630 --> 00:19:53,463
[inaudible] [inaudible]

284
00:19:54,590 --> 00:19:58,300
Pi torch examples, not colab. See,

285
00:19:58,320 --> 00:20:03,320
I was just searching that you can see
a bit of things that I was searching.

286
00:20:04,610 --> 00:20:05,590
Dah, Dah, Dah.

287
00:20:05,600 --> 00:20:10,600
I'm looking now for do you want
people to do Q and a in the meantime,

288
00:20:11,020 --> 00:20:13,210
just ask some questions that
I'm going to answer while I,

289
00:20:13,240 --> 00:20:16,730
while I get this up and running. So
just, uh, we've got some great examples,

290
00:20:16,731 --> 00:20:18,890
by the way. I mean, look
at the amount of examples.

291
00:20:18,891 --> 00:20:21,890
Pi Torch has forced some of the
most advanced models out there.

292
00:20:22,160 --> 00:20:25,380
I'm talking about variational
auto encoders. My favorite, uh,

293
00:20:25,400 --> 00:20:27,920
supervised model variational
audit encoders. It's a,

294
00:20:27,940 --> 00:20:30,830
it can actually be unsupervised as well.
Um,

295
00:20:35,190 --> 00:20:39,250
see what Pi Torch has here.
A 60 minute blitz.

296
00:20:40,540 --> 00:20:45,050
What is Pi Torch? Damn. Look at that. Uh,

297
00:20:45,670 --> 00:20:46,930
neural networks.
Yes.

298
00:20:49,070 --> 00:20:49,903
Right.

299
00:20:55,640 --> 00:21:00,620
Oh, right. Duh. That's what it was.

300
00:21:00,650 --> 00:21:03,770
I just needed that,
right.

301
00:21:04,790 --> 00:21:08,270
No module name nn
no module.

302
00:21:08,570 --> 00:21:10,170
Important torch done in it.

303
00:21:14,330 --> 00:21:15,163
Oh my God.

304
00:21:18,740 --> 00:21:22,430
Exactly. In Soviet Russia.
That's what I like to hear.

305
00:21:23,210 --> 00:21:24,950
We've got a lot to do,
guys.

306
00:21:25,490 --> 00:21:29,450
I wanted to give a very basic pie
torch example before stepping into the

307
00:21:29,451 --> 00:21:34,280
exploratory data analysis.
We've got a lot packed in for the session.

308
00:21:34,460 --> 00:21:39,380
Seriously. What am I missing
here? Let's see. Oh my God.

309
00:21:39,410 --> 00:21:42,320
Okay. Let's see. Let's see. Let's see.

310
00:21:44,920 --> 00:21:45,753
Hmm.

311
00:21:51,170 --> 00:21:52,003
Seriously.

312
00:21:52,930 --> 00:21:53,111
See,

313
00:21:53,111 --> 00:21:56,560
this is that demo thing where it's like
the code is working and then right when

314
00:21:56,561 --> 00:22:00,310
you get to the demo,
everything just kind of,

315
00:22:00,670 --> 00:22:03,700
I'm going to reinstall Pi Torch. I
mean, if that's what it's got to be,

316
00:22:03,820 --> 00:22:04,680
I'll reinstall pipe

317
00:22:04,740 --> 00:22:08,970
torch.
Okay.

318
00:22:10,020 --> 00:22:14,350
All right. Check, check
the chat. How did you

319
00:22:19,210 --> 00:22:22,060
lost the last one that
the reference to is that

320
00:22:25,110 --> 00:22:25,351
Oh,

321
00:22:25,351 --> 00:22:28,900
is the torch for object you created a
struck with the reference or a num py or

322
00:22:28,901 --> 00:22:33,680
ray or does it use it's own vectors
to create an array? It's using, uh,

323
00:22:33,750 --> 00:22:37,080
it's using num Pi under the hood.
I'm pretty sure it is actually.

324
00:22:37,500 --> 00:22:39,750
Actually that's what I assumed.

325
00:22:40,600 --> 00:22:40,950
Okay.

326
00:22:40,950 --> 00:22:43,530
I assumed it was using
its own version of num Pi,

327
00:22:44,270 --> 00:22:45,200
but

328
00:22:46,860 --> 00:22:49,980
no, no, it's its own thing.
Nd Arrays are similar,

329
00:22:50,510 --> 00:22:54,920
but it's not an empire.
That's very cool. Okay. Um,

330
00:22:55,400 --> 00:22:58,190
what I wanna do you know what

331
00:23:04,010 --> 00:23:04,620
okay.

332
00:23:04,620 --> 00:23:06,120
Is check out

333
00:23:08,590 --> 00:23:13,590
colab and inside of Colab Your A.

334
00:23:16,080 --> 00:23:16,913
Dot.
Py File.

335
00:23:18,450 --> 00:23:21,000
Really? Oh, right.

336
00:23:22,860 --> 00:23:25,380
That could help,
right?

337
00:23:27,120 --> 00:23:31,050
Fund out Pie.
Yeah.

338
00:23:35,820 --> 00:23:37,830
Nope. All right. Let's see.

339
00:23:41,240 --> 00:23:43,650
All right,
so we're going to use a

340
00:23:46,700 --> 00:23:50,780
new colab. So Google Colab, check
it out. If you haven't, by the way,

341
00:23:51,260 --> 00:23:55,910
we can install Pi torch like this
into colab. Now anybody can do this.

342
00:23:56,390 --> 00:23:59,480
So we just installed Pi Torch
and once we install pie towards,

343
00:23:59,481 --> 00:24:04,160
we're going to install, um,
we're going to see if we can,

344
00:24:06,770 --> 00:24:07,603
okay.

345
00:24:07,620 --> 00:24:11,160
You are seeing some real time
debugging by the way. Uh,

346
00:24:11,190 --> 00:24:15,270
let's see on a Gpu.
This is definitely gonna work.

347
00:24:15,840 --> 00:24:19,180
There we go.
We are in sandbox.

348
00:24:19,200 --> 00:24:22,950
Let me answer some questions while we're
waiting here. Uh, rename towards a pie.

349
00:24:22,951 --> 00:24:27,630
Colab is the best, isn't it? Do you
want the questions? Yeah. Questions.

350
00:24:27,631 --> 00:24:31,320
While we're, while we're waiting
for this to load. Tech guy asked,

351
00:24:31,321 --> 00:24:34,500
what is one shot learning against someone
else else you to talk about the school

352
00:24:34,501 --> 00:24:38,760
of Ai. Okay. Uh, so what
is one shot learning?

353
00:24:38,761 --> 00:24:42,990
So one shot learning is the dream is
the dream of all machine learning to be

354
00:24:42,991 --> 00:24:47,340
able to learn from little small datasets.
And um,

355
00:24:47,370 --> 00:24:51,690
what has been done in that on that
front memory? Augmented neural networks,

356
00:24:51,730 --> 00:24:52,650
man ends,

357
00:24:52,680 --> 00:24:57,030
I think Facebook was
probably not the first,

358
00:24:57,031 --> 00:25:01,590
but the most successful in marketing as a,

359
00:25:01,650 --> 00:25:04,590
as a research lab.
I have one shot learning algorithm.

360
00:25:04,591 --> 00:25:06,660
It's called the memory
augmented neural network.

361
00:25:06,661 --> 00:25:10,380
It came out about two years ago
and they did it with what I think,

362
00:25:10,381 --> 00:25:15,150
not theM and ist Dataset. They did
it with, um, the Omni gloss data set,

363
00:25:15,151 --> 00:25:18,630
which they proved that they could
classify images with only like,

364
00:25:18,631 --> 00:25:22,110
I think under a 50 samples. So if you want
to learn more about one shot learning,

365
00:25:22,111 --> 00:25:24,720
check that out.
What does this school of AI next week?

366
00:25:25,020 --> 00:25:28,170
So that's what I have to say about
that. Seriously, check this out.

367
00:25:28,171 --> 00:25:31,140
See guys sometimes, sometimes, you know,

368
00:25:31,170 --> 00:25:35,610
in real time you got some
issues and you gotta just

369
00:25:39,140 --> 00:25:43,880
like, exactly. I think that worked.
Did that work? Maybe that worked.

370
00:25:44,870 --> 00:25:45,703
Let's see now.

371
00:25:50,570 --> 00:25:54,830
So now we're going to have some
pie torch happening. Yes. Good,

372
00:25:55,520 --> 00:26:00,080
good. Oh my God. Yes. All right. So let's

373
00:26:01,530 --> 00:26:04,080
see. Anyway. Okay, we got,
we got to keep going guys.

374
00:26:04,740 --> 00:26:08,070
We have a lot more thanks to code,
which are not going to fail by the way.

375
00:26:08,220 --> 00:26:11,880
So I wanted to give you that. This is
going to work. Check that out. No Gpu,

376
00:26:12,000 --> 00:26:14,730
seriously. No GPS available. Oh,

377
00:26:14,731 --> 00:26:18,870
we've got to activate the
GPU torch dot double two CPU.

378
00:26:21,070 --> 00:26:24,390
Oh, there's also this thing
called Kaggle kernels,

379
00:26:24,391 --> 00:26:27,900
which I'm going to talk
about as well. Uh, okay.

380
00:26:31,330 --> 00:26:35,430
Let me just, uh, placed this in here.

381
00:26:37,290 --> 00:26:38,123
Let's see now.

382
00:26:47,190 --> 00:26:48,023
Okay,

383
00:26:51,300 --> 00:26:55,440
so you choose GPU notebook setting. Oh,
right, right, right. No book selling.

384
00:26:55,540 --> 00:27:00,330
Where's that
change?

385
00:27:00,331 --> 00:27:01,350
Runtime to GPU.

386
00:27:05,890 --> 00:27:08,740
Oh, cool. There we go. Thank you. Okay,

387
00:27:08,741 --> 00:27:13,741
so what I'm gonna do is I'm like
determined to make this work now.

388
00:27:13,811 --> 00:27:15,430
Like I don't even care.
I'm like,

389
00:27:15,431 --> 00:27:20,380
I actually have a vendetta against this
because like it was totally working and

390
00:27:20,381 --> 00:27:21,360
now it's not. Okay. So

391
00:27:22,980 --> 00:27:23,813
where were we?

392
00:27:26,830 --> 00:27:27,940
I literally,

393
00:27:30,060 --> 00:27:35,040
where'd I get this from?
Where did I get this from?

394
00:27:35,480 --> 00:27:38,700
There we go.
This is the deal.

395
00:27:39,930 --> 00:27:40,763
No,

396
00:27:45,230 --> 00:27:49,520
no. Module name. Torch
installed torch. Yes,

397
00:27:50,540 --> 00:27:54,380
we installed torch.
We did that.

398
00:27:55,650 --> 00:27:55,950
Okay.

399
00:27:55,950 --> 00:28:00,950
And now once we've installed
torch in a new code snippet,

400
00:28:02,340 --> 00:28:04,790
we're going to have our code
run while this is installing,

401
00:28:05,470 --> 00:28:10,330
answering some questions here,
changed the python for perfect,

402
00:28:10,360 --> 00:28:12,100
perfect advice.
There's no python for

403
00:28:14,010 --> 00:28:15,900
uh,
okay.

404
00:28:19,620 --> 00:28:22,820
Important torch dot.
Well,

405
00:28:22,840 --> 00:28:27,730
I did that important torch
and then as an and I did that,

406
00:28:28,660 --> 00:28:30,190
that was literally like,

407
00:28:31,280 --> 00:28:34,670
while this is compiling,
we've got to keep going here.

408
00:28:35,210 --> 00:28:39,500
Let us do some exploratory data analysis.
Guys. We have our data set. Let's,

409
00:28:39,530 --> 00:28:41,780
let's look at our problem and
we're going to get back to torch.

410
00:28:41,810 --> 00:28:44,570
We're going to get back to torch while
this is running. So what do we have here?

411
00:28:44,571 --> 00:28:47,390
Let's look at our data and then we're
going to look at kernels as well.

412
00:28:47,630 --> 00:28:51,890
We have our data, which is going to be
a set of images of different plants.

413
00:28:51,891 --> 00:28:55,760
We have labels for each,
and I went ahead and downloaded these.

414
00:28:56,060 --> 00:29:00,500
And so what we can do is look at our data.
So let's see if we can do this.

415
00:29:00,710 --> 00:29:03,350
Let's start off doing this
locally and then we're gonna,

416
00:29:04,820 --> 00:29:08,510
we're going to, uh, go further.

417
00:29:09,930 --> 00:29:11,190
So,
um,

418
00:29:11,970 --> 00:29:16,080
let's write out a read image function.
Okay.

419
00:29:16,081 --> 00:29:18,390
So we want to read images
and show them right here.

420
00:29:18,660 --> 00:29:23,630
So that's the first thing I'm
gonna do. So I'm going to say, uh,

421
00:29:23,640 --> 00:29:28,080
using open CV and I got to, I have
to open CV, I have to import open CV,

422
00:29:28,081 --> 00:29:32,370
which is called CVT. Open CV can
be kind of a pain to install. Um,

423
00:29:32,430 --> 00:29:34,860
so if you don't have this
installed locally, uh,

424
00:29:34,861 --> 00:29:38,550
I would recommend using either colab
or Kaggle colonels to do this in the

425
00:29:38,551 --> 00:29:43,480
browser. But if you do have it
locally, then things can be easy.

426
00:29:43,660 --> 00:29:47,920
Her, uh, for this. Okay,

427
00:29:47,921 --> 00:29:51,580
so that's a read image file. And I'm going
to have a show image. I mean function.

428
00:29:51,581 --> 00:29:55,510
Now I have a show image
function and once I do that,

429
00:29:57,920 --> 00:29:58,580
okay,

430
00:29:58,580 --> 00:30:03,200
then I can show the image and I'm going
to have to import Matt Oh Matt plot live

431
00:30:03,201 --> 00:30:08,150
for that import. Um, map plot,

432
00:30:08,151 --> 00:30:10,580
live.py plot as PLT.

433
00:30:11,270 --> 00:30:16,270
And I have both of those and this is going
to be a little percentage marker that

434
00:30:18,411 --> 00:30:23,170
makes sure that it's in line.
Okay,

435
00:30:23,171 --> 00:30:27,970
so now let's show some images. So let
me see if that works in valid syntax.

436
00:30:27,971 --> 00:30:32,800
CVT color. Okay, good.
Now let's see, sample pick

437
00:30:35,080 --> 00:30:39,130
equals. Let's take one of our
samples from our local directory,

438
00:30:39,131 --> 00:30:43,930
which I have and call, let's just pick
one. Let's say maybe like number one,

439
00:30:44,470 --> 00:30:47,140
the first jpeg.
Let's see if we can show this in the

440
00:30:49,000 --> 00:30:49,833
browser.

441
00:30:51,780 --> 00:30:54,870
Image is not defined right?

442
00:30:54,930 --> 00:30:57,240
So the image is going to be,

443
00:30:57,420 --> 00:30:58,253
MMM,

444
00:31:00,680 --> 00:31:04,340
no, not that sample picked show image. Uh,

445
00:31:04,370 --> 00:31:08,660
sample pick. Okay. And now
we can see it. Yes. Okay.

446
00:31:08,661 --> 00:31:11,960
So here's our first sample and now we can,

447
00:31:11,990 --> 00:31:16,910
we can kind of browse and see what
we have in our dataset. Uh, right,

448
00:31:16,911 --> 00:31:21,320
there's a bunch of images here. Okay,
let me, let me look at some more images,

449
00:31:21,650 --> 00:31:26,480
right? Um, and these images have an
associated label that we're going to now,

450
00:31:26,510 --> 00:31:29,480
um, we're going to now look at, okay,

451
00:31:29,481 --> 00:31:32,660
so let's see.
Um,

452
00:31:32,930 --> 00:31:37,040
how many invasive versus noninvasive
images do we have? Right? So let's, let's,

453
00:31:37,070 --> 00:31:40,260
let's draw a graph of that.
So, uh, we're our label.

454
00:31:40,261 --> 00:31:43,940
So our labels using a pandas data frame,
let me import Pantos as well.

455
00:31:44,510 --> 00:31:48,470
Import pandas as PD.
Okay.

456
00:31:48,710 --> 00:31:52,510
So our data frame also, let me answer
some questions. What do we have here? Um,

457
00:31:59,470 --> 00:32:04,300
document clustering.
A LDA latent dear Schley allocation.

458
00:32:04,330 --> 00:32:05,620
Perfect method for that.

459
00:32:05,621 --> 00:32:10,020
It's a part of Andrew [inaudible]
machine learning course. What is auto ml?

460
00:32:10,030 --> 00:32:15,030
It is Google's a hyper parameter tuning
framework in the browser on Google

461
00:32:16,001 --> 00:32:19,660
cloud. Pillow is all pill
is better. Your rights.

462
00:32:20,110 --> 00:32:24,250
Do you think nowadays python is sufficient
to use ml for robotics or do we also

463
00:32:24,251 --> 00:32:27,400
need c plus plus and c?
That's a great question.

464
00:32:28,750 --> 00:32:29,583
Okay.

465
00:32:29,630 --> 00:32:32,090
I mean when it comes to robotics,

466
00:32:32,091 --> 00:32:34,680
you're dealing with a
lot of hardware and uh,

467
00:32:34,790 --> 00:32:39,500
unfortunately at the lowest level when
it comes to reading like these, these,

468
00:32:39,800 --> 00:32:44,120
these drivers and these like hardware
inputs manually, a lot of that,

469
00:32:44,121 --> 00:32:46,970
a lot of those libraries are
still in c or c plus plus.

470
00:32:46,971 --> 00:32:50,930
So if you're doing robotics
specifically like physical robots,

471
00:32:50,931 --> 00:32:52,040
not even in simulation.

472
00:32:52,070 --> 00:32:55,360
And I think you need some c
plus plus knowledge to be real.

473
00:32:56,360 --> 00:32:58,460
Just don't think you would get
through the whole thing with python.

474
00:32:58,610 --> 00:33:00,800
That would be a dream. It's
happening. We're not there yet. Okay.

475
00:33:00,801 --> 00:33:04,730
So we have our training labels and
now we want to see how much of each,

476
00:33:04,731 --> 00:33:06,950
do we have a right?
So

477
00:33:08,690 --> 00:33:13,550
we need to import a seaborne.
Seaborne is gonna help us visualize this.

478
00:33:14,450 --> 00:33:16,310
It's another data visualization library.

479
00:33:16,950 --> 00:33:17,783
Okay.

480
00:33:19,400 --> 00:33:21,350
So what am I going to have this style be?

481
00:33:21,351 --> 00:33:26,351
Let's have it be a dark grid and we're
going to say the x axis is going to be

482
00:33:29,301 --> 00:33:34,301
the invasive plants that we have and the
data is going to be our labels that we

483
00:33:34,641 --> 00:33:39,560
just imported into a pandas data frame.
And now using a map plot live,

484
00:33:39,561 --> 00:33:41,540
we can give a title to this graph.

485
00:33:41,930 --> 00:33:44,900
Just like if we had to like do
a report or something on this,

486
00:33:45,200 --> 00:33:49,550
we'll say invasive versus
noninvasive images.

487
00:33:50,420 --> 00:33:52,220
Um,
yeah.

488
00:33:52,970 --> 00:33:57,050
Then we're have our x
label B are invasive.

489
00:33:57,740 --> 00:33:59,600
I guess we can specify a font size,

490
00:33:59,630 --> 00:34:04,490
let's say like 20 and then our why label
is going to be of course the number,

491
00:34:04,520 --> 00:34:08,840
right? So the images versus the number
and we'll make it the same font size.

492
00:34:08,841 --> 00:34:12,550
I know I misspelled
font size there and uh,

493
00:34:16,080 --> 00:34:19,200
okay. So let's see what that
looks like. Of course there's a,

494
00:34:22,860 --> 00:34:26,690
Oh, okay. Okay. Okay. Okay. Right.

495
00:34:28,640 --> 00:34:29,473
There we go.

496
00:34:31,160 --> 00:34:32,990
Yes. Good. Okay,

497
00:34:34,400 --> 00:34:35,233
good.

498
00:34:35,400 --> 00:34:37,260
So what do we have here?

499
00:34:38,090 --> 00:34:38,640
Okay.

500
00:34:38,640 --> 00:34:42,060
Count invasive versus noninvasive.

501
00:34:42,900 --> 00:34:45,970
[inaudible]
good.

502
00:34:46,670 --> 00:34:47,870
So clearly,

503
00:34:49,660 --> 00:34:53,170
clearly we have much more invasive
images where one is invasive.

504
00:34:53,650 --> 00:34:58,120
So that's, that's a good thing to note.
Okay, good. That this is what we want.

505
00:34:58,180 --> 00:35:02,070
Um, you know, ideally it's like half
and half. So we can see like what,

506
00:35:02,071 --> 00:35:06,820
what has an ideal, ideally,
ideally we have a lot of
data, but you know, whatever,

507
00:35:07,180 --> 00:35:10,900
uh, I think this is, no, this is enough
data because this is an older competition.

508
00:35:10,901 --> 00:35:14,140
And if we look at some of the
kernels, we can see that, um,

509
00:35:15,320 --> 00:35:18,140
on Kaggle we can see that it's been,

510
00:35:18,680 --> 00:35:23,450
it's been done a comma, right? Right.

511
00:35:23,451 --> 00:35:27,170
So what kind of models should we
build here? Right? So using Pi torch,

512
00:35:27,171 --> 00:35:30,950
what kind of learning models should we
build, build here to build a classifier.

513
00:35:30,951 --> 00:35:33,920
And now there's a lot of different
convolutional nets out there. There's,

514
00:35:34,270 --> 00:35:39,120
you know, Alex net inception
resonant, uh, VGG 16,

515
00:35:39,121 --> 00:35:42,780
BGG 19. There's like a million of
them and there's not a million.

516
00:35:42,781 --> 00:35:45,570
There's like 10 ish, like, but dense net.

517
00:35:45,690 --> 00:35:47,940
What we can do is we can try them all out,

518
00:35:48,120 --> 00:35:51,090
but one that I have not
talked about before,

519
00:35:51,091 --> 00:35:54,930
and I'm going to take this opportunity
to talk about it, is resonant.

520
00:35:54,990 --> 00:35:57,660
So residual networks.
Okay.

521
00:35:57,661 --> 00:36:00,420
So let me talk about that really
quickly and then we'll build that. Okay.

522
00:36:00,421 --> 00:36:05,280
We'll build a, a resonant
architecture. So, um, if we,

523
00:36:05,490 --> 00:36:08,790
if when we're using recurrent
networks, uh, there's this,

524
00:36:08,820 --> 00:36:11,670
there's a problem that occurs called
the vanishing gradient problem.

525
00:36:11,730 --> 00:36:13,290
Now if you want to learn more about that,

526
00:36:13,350 --> 00:36:16,650
just Google vantage and gradients to Raj
at least through videos will show up on

527
00:36:16,651 --> 00:36:17,190
youtube.

528
00:36:17,190 --> 00:36:22,190
But a Tldr when we are back propagating
a neural network using some kind of um,

529
00:36:24,270 --> 00:36:28,810
gradient descent based optimization
strategy like Adam, you know,

530
00:36:28,980 --> 00:36:32,570
stochastic gradient descent, all
sorts of great in a sense, um,

531
00:36:33,450 --> 00:36:34,380
variations,

532
00:36:34,980 --> 00:36:39,480
the gradient gets smaller and smaller
and smaller as that value is back

533
00:36:39,481 --> 00:36:42,300
propagate its right. So back
propagated means, you know,

534
00:36:42,330 --> 00:36:45,450
this is the value that's
going to tell our weights,

535
00:36:45,451 --> 00:36:50,120
how to update to be better at predicting
the output, the, the, the, the,

536
00:36:50,121 --> 00:36:54,660
the real output.
And so we are using calculus to optimize.

537
00:36:54,661 --> 00:36:59,661
Remember neural networks are our models
created with linear Algebra and they are

538
00:36:59,911 --> 00:37:02,940
optimized with calculus.
So that's where you need calculus,

539
00:37:02,941 --> 00:37:07,050
you need calculus to optimize and you
need linear Algebra to build the models.

540
00:37:07,290 --> 00:37:10,350
And to perform those operations on them.
So during,

541
00:37:10,351 --> 00:37:13,460
so the reason we your calculus
and because uh, we're,

542
00:37:13,630 --> 00:37:18,240
we're computing the partial derivative
of the output with respect to the

543
00:37:18,241 --> 00:37:22,410
weights, the partial derivative of
the error with respect to the weights.

544
00:37:22,470 --> 00:37:24,090
And we keep doing that for every layer.

545
00:37:24,510 --> 00:37:26,340
So the problem is that
this gradient vanishes.

546
00:37:26,341 --> 00:37:30,300
And this image right here shows how you
notice how it's getting more and more

547
00:37:30,301 --> 00:37:34,290
translucent over time. That gradient gets
smaller and smaller, smaller. So these,

548
00:37:34,470 --> 00:37:36,180
so to,
to be real that,

549
00:37:36,210 --> 00:37:40,770
so the gradient is actually a vector
of the partial derivatives that are

550
00:37:40,771 --> 00:37:42,540
computed, right? That's the gradient.

551
00:37:43,140 --> 00:37:47,280
And those values get smaller and smaller
and smaller. So they diminish over time.

552
00:37:47,430 --> 00:37:51,150
So this is a problem because the network
is not, it needs that gradient value.

553
00:37:51,151 --> 00:37:54,090
You can even think of the gradient
as a kind of memory, right?

554
00:37:54,091 --> 00:37:57,630
So LSTM networks solve this
for a recurrent networks,

555
00:37:58,020 --> 00:38:01,920
but how do we do this for convolutional
networks, for, for image based networks.

556
00:38:02,900 --> 00:38:04,230
So some researchers had this,

557
00:38:04,260 --> 00:38:08,640
I think it was a genius idea to
literally just skip over some of the

558
00:38:08,641 --> 00:38:12,090
intermediary layers.
And have the by skip over,

559
00:38:12,091 --> 00:38:15,690
I'm talking about have the gradient
skip over intermediary layers.

560
00:38:15,960 --> 00:38:18,240
And by doing that you're,
it's not going to diminish.

561
00:38:18,270 --> 00:38:20,250
You literally just skip
them over. What do, I mean?

562
00:38:20,280 --> 00:38:24,840
Let's talk about this mathematically.
So in the Resnick architecture,

563
00:38:25,020 --> 00:38:27,570
we have a bunch of what are
called convolutional blocks.

564
00:38:27,571 --> 00:38:29,550
These are standard convolutional blocks,
right?

565
00:38:29,551 --> 00:38:34,170
So convolution pooling activation that's
inside of a block and you repeat that

566
00:38:34,320 --> 00:38:38,740
convolution pooling operation. And then
activation function and repeat, repeat,

567
00:38:38,741 --> 00:38:43,510
repeat, repeat. But what if we can
add what's called an identity mapping.

568
00:38:43,511 --> 00:38:45,670
So what you're seeing
on the left here is the,

569
00:38:45,730 --> 00:38:50,730
the novelty of resonate where we took
Afa backs and we modified it by adding an

570
00:38:52,420 --> 00:38:57,400
extra, um, dot product operation to it.
And this is called the identity mapping.

571
00:38:57,730 --> 00:39:02,680
So what better way to explain
the identity mapping then saying,

572
00:39:02,950 --> 00:39:03,671
well first of all,

573
00:39:03,671 --> 00:39:08,320
we have our function f of x and we
are changing it to f of x plus x,

574
00:39:08,321 --> 00:39:13,321
where x is the identity mapping and this
modifies the original operation such

575
00:39:13,571 --> 00:39:16,930
that the gradient will not diminish
over time. So in care, Ross,

576
00:39:16,931 --> 00:39:21,160
here's a very simple example
programmatically of what I
mean by a residual block.

577
00:39:21,370 --> 00:39:26,230
So in the residual network, we are
renaming a convolutional block two eight.

578
00:39:26,290 --> 00:39:28,330
By the way, don't leave, I'm going
to wrap at the end. By the way,

579
00:39:28,930 --> 00:39:33,190
we are renaming the convolutional block
tweet residual block by adding in this

580
00:39:33,191 --> 00:39:37,510
identity mapping. So normally we
would just say this right here, right?

581
00:39:37,511 --> 00:39:40,880
So in our block we would say convolution,
you know,

582
00:39:40,930 --> 00:39:45,220
normalization activation function, which
is really Lou, you know, in this case,

583
00:39:45,430 --> 00:39:49,120
which helps, um, with the vanishing
gradient problem for other reasons.

584
00:39:49,360 --> 00:39:52,450
There's so many like little explanation
tangents that could be going on here.

585
00:39:52,451 --> 00:39:56,890
By the way, there's so
much here, but we are,

586
00:39:56,980 --> 00:39:59,230
what we're doing here is we're
adding this part right here.

587
00:39:59,231 --> 00:40:02,350
So what I have highlighted
is the identity mapping.

588
00:40:02,351 --> 00:40:06,880
It's what is added to the,
um, initial set of operations.

589
00:40:07,090 --> 00:40:10,750
So notice here that the shortcut
is actually, if, first of all,

590
00:40:10,751 --> 00:40:13,360
we have to specify that we want
this to be a shortcut, right?

591
00:40:13,361 --> 00:40:17,170
So we're going to say true if we want
this to be a true residual block.

592
00:40:17,470 --> 00:40:19,060
And if we do this,
this,

593
00:40:19,120 --> 00:40:23,710
this adds this shortcut operation
to it right here. And then we just,

594
00:40:24,130 --> 00:40:28,720
and then once we've completed that, then
we add it to the what was initially the,

595
00:40:29,020 --> 00:40:32,770
the chain of operations and then
we returned that block as a whole.

596
00:40:32,980 --> 00:40:36,550
So notice that it's not necessarily
that something is being replaced here,

597
00:40:36,610 --> 00:40:40,570
it's an extra operation that's added
to the existing chain of operations.

598
00:40:40,900 --> 00:40:45,900
But what this does is we can think about
it as skipping over because it's it,

599
00:40:46,390 --> 00:40:48,250
because it's not a zero mapping,

600
00:40:48,251 --> 00:40:51,820
it's actually adding
value to it because it's,

601
00:40:52,720 --> 00:40:56,770
the reason it's like skipping over is
because whereas the value would diminish

602
00:40:56,771 --> 00:41:01,540
over time because we're adding value
to that existing chain of operations.

603
00:41:01,810 --> 00:41:04,330
It's a non zero value,
which is what we want.

604
00:41:04,331 --> 00:41:06,490
We don't want a zero
value for our gradient.

605
00:41:07,690 --> 00:41:11,840
So that's the idea behind
a residual network. Uh,

606
00:41:12,520 --> 00:41:16,330
the original mapping F of x
is recaps into f of x plus X.

607
00:41:17,560 --> 00:41:18,130
And it's,

608
00:41:18,130 --> 00:41:21,910
the theory was that it's
easier to optimize a residual
mapping then to optimize

609
00:41:21,911 --> 00:41:26,610
the original unreferenced snapping.
And that is the,

610
00:41:26,660 --> 00:41:30,290
the paraphrasing of what
I've just said. Okay. So, uh,

611
00:41:32,000 --> 00:41:35,990
it is a good place. Start
Learning Ai. Yes. Now, um,

612
00:41:36,020 --> 00:41:38,990
in terms of hints about
what's up to what's coming up,

613
00:41:39,020 --> 00:41:42,890
I would say of reinforcement learning, but
that's all I can say right now. So Eda,

614
00:41:42,891 --> 00:41:47,720
we did that resonate. Okay.
So let's, let's do this now,
right? So the, first of all,

615
00:41:47,750 --> 00:41:51,440
let's see what Kaggle has for us.
So Kaggle recently released, well,

616
00:41:51,441 --> 00:41:52,910
they kind of renamed what already existed,

617
00:41:52,911 --> 00:41:57,320
but they recently released
this idea of a kernel. Okay.

618
00:41:57,321 --> 00:42:02,120
So a colonel is basically like Google
Colab, but it's built into casual,

619
00:42:02,121 --> 00:42:03,680
by the way,
Google bought Kaggle.

620
00:42:03,740 --> 00:42:06,110
So it makes sense that they're
using the same infrastructure.

621
00:42:06,560 --> 00:42:08,270
So if we look at a kernel
here, let's, let's,

622
00:42:08,271 --> 00:42:11,370
let's see what they have here for
kernels. This is the most, uh,

623
00:42:11,400 --> 00:42:16,170
upvoted ker kernel for this, uh, for
this Dataset is saying, use carrots.

624
00:42:16,190 --> 00:42:19,850
Pretrained VGG 16.
So if we open up this kernel,

625
00:42:21,000 --> 00:42:22,430
I'll go ahead and forth the notebook.

626
00:42:22,760 --> 00:42:27,650
It's got literally everything I need
to run this in the browser. Okay.

627
00:42:27,651 --> 00:42:31,010
So what this is doing is it's in
the browser, it's got a notebook,

628
00:42:31,040 --> 00:42:35,210
it's got a python compile a, it's got
a python interpreter in the browser.

629
00:42:35,510 --> 00:42:40,280
It's got all the dependencies I need for
this specific datasets. It's got, um,

630
00:42:40,370 --> 00:42:43,550
the data set itself is important right
here that you could see as input right

631
00:42:43,551 --> 00:42:46,220
here with all of those,
which is Super Handy, right?

632
00:42:46,221 --> 00:42:50,330
We don't have to download test.seven Z,
we don't have to unzips tested seven Z,

633
00:42:50,450 --> 00:42:53,810
then we don't have to combine it with
the labels. It's all already there,

634
00:42:53,900 --> 00:42:57,380
which is super useful.
Now this is very cool,

635
00:42:57,590 --> 00:43:00,980
but we are not doing care os right?
We are doing Pi torch.

636
00:43:00,981 --> 00:43:04,970
So let's go back and let's look for
a [inaudible] 20. If we can find one.

637
00:43:05,270 --> 00:43:10,190
Can anybody see a chart?
Here we go. Pi Torch starter
0.98. That looks pretty good.

638
00:43:10,191 --> 00:43:12,710
So let's click on that one
and see what we got here.

639
00:43:14,870 --> 00:43:16,190
Fork notebook.

640
00:43:16,790 --> 00:43:20,030
This guy went ahead and
did this in Pi Torch,

641
00:43:20,031 --> 00:43:24,440
which is super awesome. And
uh, how are we doing on time?

642
00:43:27,590 --> 00:43:31,680
Cool. Wow. Time flies when you're, when
you're debugging. All right. So, uh,

643
00:43:31,700 --> 00:43:35,750
48 minutes. Okay, we got to end this
before it's too late. All right,

644
00:43:35,751 --> 00:43:39,560
so now what we can do is we
can just run it in the browser.

645
00:43:39,561 --> 00:43:43,850
Notice this just compiled in the browser
num Pi Pan does. We imported that. Okay.

646
00:43:43,851 --> 00:43:47,420
What about torch?
Notice how I was having a problem before.

647
00:43:47,630 --> 00:43:52,580
A No problem anymore. It is all there.
I'm compiling this code. Compiling,

648
00:43:53,000 --> 00:43:55,490
compiling. Now what is he
using here, by the way? Okay,

649
00:43:55,491 --> 00:44:00,330
so he's transforming the data. Great.
He's loading up the data. Cool. Uh,

650
00:44:00,710 --> 00:44:04,130
LR scheduler.
That means I'm learning rates scheduler.

651
00:44:04,131 --> 00:44:08,510
He's deciding what type of learning
rate to use here based on the number of,

652
00:44:08,960 --> 00:44:12,170
looks like a predefined,
predefined threshold of number of epochs.

653
00:44:13,010 --> 00:44:16,550
He's got a pretrained weight file that
he is. So he's doing transfer learning.

654
00:44:16,551 --> 00:44:20,480
Cool. And he's using dense net.
Okay. Dense. Now that's cool.

655
00:44:22,370 --> 00:44:24,260
And he's our sample submissions.

656
00:44:24,350 --> 00:44:28,940
Submit and train the network.
Okay,

657
00:44:29,960 --> 00:44:30,830
cool.
Well,

658
00:44:30,831 --> 00:44:34,230
this is going to take a while because
training neural networks is not something

659
00:44:34,231 --> 00:44:37,620
that just happens, right? It's
so anyway, check that out.

660
00:44:37,621 --> 00:44:39,300
So that's Pi Torch in the browser.

661
00:44:39,301 --> 00:44:43,230
Everything you need preloaded
with the Dataset. A super useful.

662
00:44:43,290 --> 00:44:45,090
Now what are we going to
do here? Well, we, well,

663
00:44:45,091 --> 00:44:48,210
we need to replace dense
net with a resonates,

664
00:44:48,920 --> 00:44:52,860
but an IX also explain what dense
that is and how that works. Uh,

665
00:44:52,980 --> 00:44:56,340
but it looks like we've run out of time.
Go ahead and answer, ask some questions.

666
00:44:56,520 --> 00:44:59,820
I'm going to end this with a rap,
but ask some questions.

667
00:45:01,780 --> 00:45:02,490
Okay.

668
00:45:02,490 --> 00:45:06,310
DGG is yes,
but we,

669
00:45:06,370 --> 00:45:10,360
but we need to understand how
resonant works. Okay. Lua is awesome.

670
00:45:12,370 --> 00:45:13,203
Love you too.

671
00:45:13,470 --> 00:45:17,100
What is unit net unit? Great idea.
Joe Unit is interesting because it's,

672
00:45:17,340 --> 00:45:20,670
it's kind of a reflection of itself and
the later layers we'll talk about that.

673
00:45:20,671 --> 00:45:21,930
Pooling is a great example.

674
00:45:22,950 --> 00:45:27,090
Pooling is not all of the data and
the input is relevant to the output.

675
00:45:27,091 --> 00:45:30,390
So what pooling does is it
chooses what's the most relevant.

676
00:45:30,391 --> 00:45:31,680
So what does most mean?
Well,

677
00:45:31,681 --> 00:45:35,430
most in the case of Max pooling
can be what has the most, um,

678
00:45:35,580 --> 00:45:39,870
what's just the biggest number
in some, um, region of the input.

679
00:45:39,900 --> 00:45:41,640
And then we do that.
So that's Max pooling.

680
00:45:41,641 --> 00:45:45,300
And that's generally the most used
pulling operation in neural networks.

681
00:45:45,301 --> 00:45:48,840
There are other kinds as well.
Um,

682
00:45:49,260 --> 00:45:53,730
can you talk about learning rate
schedule or Api? Um, four Pi torch.

683
00:45:53,731 --> 00:45:58,000
I'll talk more about Pi torch. Where
can we learn more about Pi Torch guys?

684
00:45:58,020 --> 00:46:02,400
So what I do when I'm learning about
any kind of code is I don't look at

685
00:46:03,030 --> 00:46:04,230
anything but get hub,
right?

686
00:46:04,231 --> 00:46:08,520
I just go straight to get hub and I look
at some example code and hopefully the

687
00:46:08,521 --> 00:46:12,450
example code is well documented.
This is the repository to look at.

688
00:46:13,020 --> 00:46:16,320
A reinforcement learning is probably
what I look at cause that's what I'm most

689
00:46:16,321 --> 00:46:19,830
interested in right now.
Uh, this is cool. Okay.

690
00:46:19,831 --> 00:46:22,410
I mean it could have better
documentation but that works.

691
00:46:22,740 --> 00:46:26,010
Now I'm going to end this with a rap and
I'm going to just pick some random word

692
00:46:26,011 --> 00:46:27,900
from there,
from the comments.

693
00:46:29,190 --> 00:46:31,020
Oh, let me see here. For alphabet,

694
00:46:31,560 --> 00:46:35,520
alphabet acrobatics rep by Blackalicious.
Okay.

695
00:46:35,730 --> 00:46:36,690
Two people said that.

696
00:46:36,810 --> 00:46:38,580
How forbid acrobatics

697
00:46:39,200 --> 00:46:43,460
alphabet aerobics.
Okay.

698
00:46:44,930 --> 00:46:49,370
Alpha alphabet aerobics.
Okay. Um, cool. I have,

699
00:46:49,371 --> 00:46:50,840
I found it.
Never heard this before

700
00:46:55,880 --> 00:46:56,713
given,

701
00:46:58,400 --> 00:47:03,140
are you serious guys? You guys in trolling
me, aren't you? All right, let's do this.

702
00:47:03,860 --> 00:47:07,430
I'm waiting for the beats.
Seriously. Is this even a rap like,

703
00:47:11,810 --> 00:47:12,643
okay.

704
00:47:13,090 --> 00:47:17,380
Okay, fine. Alright, wrap
on cue on monster. Golang.

705
00:47:18,530 --> 00:47:19,363
Okay.

706
00:47:21,560 --> 00:47:25,880
I like Gold Lang. I do it.
No, no, no. Let me restart.

707
00:47:26,510 --> 00:47:30,170
I like gold. Lame. No,
no, no. See, hold on. This

708
00:47:30,220 --> 00:47:32,570
is just the most Wac beat I've ever heard.
Like how,

709
00:47:32,620 --> 00:47:34,660
how is anybody supposed to wrap over this?

710
00:47:34,740 --> 00:47:35,573
Okay.

711
00:47:36,630 --> 00:47:39,210
It's getting faster as well.
There's not even like a set tempo here.

712
00:47:39,240 --> 00:47:41,910
It's just I'm out. I'm going to
just do it anyway. Here we go.

713
00:47:45,240 --> 00:47:49,470
I like Golang. I'm slow. Maine. I
like do it like I'm Ilan Musk made.

714
00:47:49,620 --> 00:47:53,550
It's going faster and faster. I like
concurrently. See, I keep my go.

715
00:47:53,551 --> 00:47:56,940
Routines running. You
see? Yeah, go routines.

716
00:47:57,480 --> 00:48:00,450
This is so wack dude. This seriously
like, I'm not even going to,

717
00:48:00,720 --> 00:48:05,220
I'm just going to talk about, I'm just
going to acapella dis. I like Gold Lang.

718
00:48:05,310 --> 00:48:10,050
It's got concurrent model.
It's got concurrent sub
routines. That's how it goes.

719
00:48:10,140 --> 00:48:14,280
You see what I mean? I do.
One, two, three is distributed.

720
00:48:14,850 --> 00:48:19,000
Can't you see? I do it across
different TPU then Gpu. Yes.

721
00:48:19,320 --> 00:48:24,120
I do it on my CPU. I
do it on my, all right.

722
00:48:24,121 --> 00:48:28,980
That's it for this, for this live stream,
guys. That's is how we do it. We don't,

723
00:48:29,030 --> 00:48:33,270
we literally don't care if we succeed,
if we fail,

724
00:48:33,271 --> 00:48:36,810
if we do both, because we just
keep on going. No matter what,

725
00:48:37,050 --> 00:48:41,160
nothing will stop us. We are going
to do so much good for the world,

726
00:48:41,340 --> 00:48:46,080
and thank you for tuning in. Uh, for
now I've got to make some videos,

727
00:48:46,260 --> 00:48:47,490
so thanks for watching.

728
00:48:47,960 --> 00:48:48,793
Okay.

729
00:49:33,960 --> 00:49:34,330
Ooh.

