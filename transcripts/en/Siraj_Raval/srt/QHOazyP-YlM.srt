1
00:00:00,120 --> 00:00:03,780
Hello world, it's Siraj. And
how risky is your credit?

2
00:00:03,810 --> 00:00:08,490
That is the question that we're answering
today by looking at this credit risk

3
00:00:08,491 --> 00:00:10,800
data set.
It is a German credit risk data set.

4
00:00:10,950 --> 00:00:15,720
We want to know based on your employment
history, based on your family history,

5
00:00:15,721 --> 00:00:17,850
your in, you know, your income,

6
00:00:18,150 --> 00:00:21,540
are you at risk for not
paying back your loan?

7
00:00:21,810 --> 00:00:25,320
Usually we have people who do this and
it takes a long time and you have to,

8
00:00:25,440 --> 00:00:29,220
there's a bunch of biases built into the
system. You know, you meet the person,

9
00:00:29,221 --> 00:00:33,120
whether you're an insurance agent or you
work for some bank and you're trying to

10
00:00:33,121 --> 00:00:35,310
assess whether or not this
person deserves alone.

11
00:00:35,670 --> 00:00:40,670
As humans we have a lot of biases and
these biases don't necessarily add real

12
00:00:41,011 --> 00:00:43,950
value to whether or not this person
deserves a loan or not. Right?

13
00:00:44,130 --> 00:00:48,570
So the way to fix that is to let machines
do the job because machines can find

14
00:00:48,571 --> 00:00:50,790
relations in data that we can't.

15
00:00:51,060 --> 00:00:55,770
So the data set we're going to look at
is a bunch of financial attributes of

16
00:00:55,771 --> 00:00:58,680
somebody, the status of their
existing checking account.

17
00:00:58,681 --> 00:01:02,550
This is a German data set by the way.
And I found it on the UCI,

18
00:01:02,760 --> 00:01:06,600
a website also a that is just a
great website to find data sets on.

19
00:01:06,601 --> 00:01:09,810
So definitely check that website
out if you haven't already.

20
00:01:10,530 --> 00:01:12,420
We're going to look at
their credit history,

21
00:01:12,421 --> 00:01:16,770
the duration of payments they've made,
the purchases they made, the cars,

22
00:01:16,771 --> 00:01:19,650
furniture, all of these
things are features, right?

23
00:01:19,950 --> 00:01:24,300
They're all features and we can use that
to assess whether or not this person

24
00:01:24,840 --> 00:01:29,100
has, uh, is at risk for is at risk
for not paying back their loans.

25
00:01:29,640 --> 00:01:34,410
This is used in a bunch of different
fields, insurance of finance, um,

26
00:01:34,800 --> 00:01:37,200
whether or not to rent a
house to somebody, right?

27
00:01:37,230 --> 00:01:39,220
The landlord assesses
whether or not you can pay,

28
00:01:39,260 --> 00:01:43,410
pay it back savings account that
whether or not they're employed,

29
00:01:43,650 --> 00:01:48,180
all these features and the label is,
this is a history of credit risk.

30
00:01:48,180 --> 00:01:50,580
So there's also a label
based on all these features.

31
00:01:50,581 --> 00:01:55,581
This person has already been
assessed as a at risk or not at risk,

32
00:01:55,950 --> 00:02:00,870
right? So that's, that's the
mapping that we're going to learn,

33
00:02:00,871 --> 00:02:02,280
right? It's a, it's a binary mapping.

34
00:02:02,281 --> 00:02:05,730
And the way we're going to do that
is by building a random forest.

35
00:02:05,880 --> 00:02:10,410
That's really the goal here is to learn
about random forests and uh, yeah,

36
00:02:10,411 --> 00:02:12,750
so basically what it would look
like is something like this,

37
00:02:12,751 --> 00:02:14,580
like this picture that
we're looking at right now.

38
00:02:15,000 --> 00:02:19,320
Eventually once we built this a random
forest, which consists of several,

39
00:02:19,321 --> 00:02:22,170
what's what are called decision trees,
it'll look like this.

40
00:02:22,260 --> 00:02:26,250
We'll feed it a new data point and then
it will iteratively ask a series of

41
00:02:26,251 --> 00:02:27,180
questions like,

42
00:02:27,510 --> 00:02:31,740
is there checking account
balance above 200 or below 200?

43
00:02:32,190 --> 00:02:36,240
And then based on that, it'll ask another
set of questions. Like if you say no,

44
00:02:36,390 --> 00:02:39,150
if the answer is no, based on
that data point, you'll say, well,

45
00:02:39,151 --> 00:02:40,800
what's the length of
their current employment?

46
00:02:40,830 --> 00:02:43,680
And then are they credible
or not credible? Right?

47
00:02:43,681 --> 00:02:47,550
So it'll just keep going down this,
this, um, this chain of decisions.

48
00:02:47,551 --> 00:02:50,340
A decision tree. Okay. So that's
what we're going to build.

49
00:02:50,341 --> 00:02:54,210
We're going to build a random forest.
So what is a random forest? Well,

50
00:02:54,630 --> 00:02:57,960
a random forest is a
collection of decision trees.

51
00:02:57,990 --> 00:02:59,800
So let's talk about
what a decision tree is.

52
00:03:00,030 --> 00:03:03,730
So a decision tree is actually
a relatively simple concept,

53
00:03:03,910 --> 00:03:06,580
and so I just didn't even talk
about that in this series so far.

54
00:03:06,730 --> 00:03:10,480
I'm just going straight into random
forests because decision trees are easy

55
00:03:10,481 --> 00:03:13,270
stuff. We want to get right into
the random forest spot, right?

56
00:03:13,420 --> 00:03:15,220
But let's go over decision
trees really quickly.

57
00:03:16,290 --> 00:03:21,290
So a decision tree is basically a set of
decisions on whether or not to classify

58
00:03:22,241 --> 00:03:23,074
something.

59
00:03:23,110 --> 00:03:28,110
The technical name for this
is the classification and
regression tree or cart.

60
00:03:28,600 --> 00:03:33,490
And it was invented by a dude named
Leo Breman, uh, couple of decades ago,

61
00:03:33,491 --> 00:03:36,940
like two decades ago. And it can be
used for both, as the name suggests,

62
00:03:37,030 --> 00:03:40,840
classification and regression,
both of those things.

63
00:03:40,990 --> 00:03:43,930
But we're going to use it for
classification because that is,

64
00:03:44,290 --> 00:03:45,580
that is what we're trying to do,

65
00:03:45,581 --> 00:03:49,540
or we're trying to classify whether or
not someone is credible or not credible.

66
00:03:50,200 --> 00:03:54,310
Okay. So how does this thing work?
Well, you have a set of features, right?

67
00:03:54,311 --> 00:03:58,180
Let's say the features are the
temperature, the wind speed,

68
00:03:58,181 --> 00:04:01,790
and the air pressure. And based
on these three features, uh,

69
00:04:01,810 --> 00:04:03,820
we want to classify whether
or not it's rainy. Well,

70
00:04:03,821 --> 00:04:07,690
what the decision tree that we build
is going to do is it's going to create

71
00:04:07,930 --> 00:04:09,670
iteratively,
be iteratively,

72
00:04:09,730 --> 00:04:11,760
I should say recursively
actually it's going to recurs.

73
00:04:11,920 --> 00:04:14,620
The tree itself is built recursively
you'll see what I mean when I,

74
00:04:14,920 --> 00:04:15,760
when we look at the code,

75
00:04:16,090 --> 00:04:19,480
but the tree itself has built recursively
and for all of these features they

76
00:04:19,481 --> 00:04:23,830
will ask a series of questions until it
classifies it as raining or not raining.

77
00:04:24,160 --> 00:04:27,760
So the real question is how do
we build this thing? What is,

78
00:04:27,880 --> 00:04:32,880
how do we build the optimal tree where
it is asking the rights a threshold

79
00:04:32,891 --> 00:04:33,341
values.

80
00:04:33,341 --> 00:04:37,120
Like how does it know that the temperature
should be greater than or less than

81
00:04:37,121 --> 00:04:39,730
70 degrees Fahrenheit?
And then based on that answer,

82
00:04:39,820 --> 00:04:44,200
how does it know whether the wind speed
should be greater than 4.5 specifically?

83
00:04:44,290 --> 00:04:46,990
Where are these magic numbers coming from?
Well,

84
00:04:46,991 --> 00:04:50,470
they're coming from [inaudible],

85
00:04:50,500 --> 00:04:52,720
the Gini index,
that's what it's called.

86
00:04:52,721 --> 00:04:56,560
It's called the Gini index genie
in a bottle genie in a bottle,

87
00:04:56,561 --> 00:05:00,100
not genie in a bottle. Gd, it's
the Italian Jeannie. So, uh,

88
00:05:00,160 --> 00:05:02,630
it was some dude named Jeanie.
I actually don't know. Uh,

89
00:05:02,710 --> 00:05:06,010
we're not going to get into that.
Anyway, let's talk about the Gini Index.

90
00:05:06,011 --> 00:05:09,040
So the Gini index is
the loss function here.

91
00:05:09,610 --> 00:05:13,810
But the difference here between what we've
done before with grading dissent with

92
00:05:13,811 --> 00:05:18,010
Newton's method is that there is no
convex optimization happening here.

93
00:05:18,220 --> 00:05:22,030
We're not trying to find the
minimum of some convex function.

94
00:05:22,210 --> 00:05:25,060
There is no error that
we're trying to minimize.

95
00:05:25,180 --> 00:05:28,210
The Gini index is a cost
function that works differently.

96
00:05:28,600 --> 00:05:32,470
And here's how it works. Basically for
every single point in our data set, right,

97
00:05:32,560 --> 00:05:34,120
we've got a bunch of different features.

98
00:05:34,330 --> 00:05:37,720
We want to find that
ideal threshold value.

99
00:05:37,721 --> 00:05:41,080
And so I'm going to explain this once and
then explain it again when we get into

100
00:05:41,081 --> 00:05:43,870
the code. But we want to find
that ideal feature, right?

101
00:05:43,870 --> 00:05:47,440
What that ideal value for a feature.
Okay.

102
00:05:47,441 --> 00:05:50,650
So what do I mean by that?
So here's how it works.

103
00:05:51,760 --> 00:05:55,480
Check this out. We've got a Dataset with
a bunch of features, right? 10 features,

104
00:05:55,481 --> 00:05:58,910
right? So one, let's say, let's
say one of them is the income.

105
00:05:58,970 --> 00:06:02,060
So the income could be anywhere,
and I'm gonna use USD for this example.

106
00:06:02,300 --> 00:06:06,740
It could be anywhere from $10,000
a year to $1 million a year.

107
00:06:06,920 --> 00:06:10,340
So what the Gini index is going
to do is it's going to go through,

108
00:06:10,430 --> 00:06:14,990
so what we're gonna do is we're going to
iterate through every single data point

109
00:06:15,260 --> 00:06:16,520
for that feature.

110
00:06:16,610 --> 00:06:19,430
So we're going to say we're
going to iterate through
every single data point and

111
00:06:19,431 --> 00:06:21,860
we're going to compute this Gini index,

112
00:06:21,950 --> 00:06:26,900
which is one minus the sum
of it is the no. So, so the,

113
00:06:27,020 --> 00:06:29,990
the Gini Index is whereas
the formula for it,

114
00:06:33,890 --> 00:06:35,240
it is one,

115
00:06:37,000 --> 00:06:41,530
it is wine minus the average times the
average where the average is proportion,

116
00:06:41,531 --> 00:06:46,180
right? One minus the average of all of
the class values times that average.

117
00:06:46,181 --> 00:06:47,800
And that gives us the Gini Index.

118
00:06:47,980 --> 00:06:51,090
And so what we want to do is it
comes out to some single value.

119
00:06:51,091 --> 00:06:55,120
So I'm a scalar value. And so
basically we use, we start from,

120
00:06:55,240 --> 00:06:59,560
we start from data 0.0 and we go up to
data point and where n is the number of

121
00:06:59,561 --> 00:07:04,090
data points and we compute the index
for each of these data points for a

122
00:07:04,091 --> 00:07:05,230
specific feature,
right?

123
00:07:05,320 --> 00:07:10,090
So let's say the first data point is
10,000 will compute a Gini index for that

124
00:07:10,091 --> 00:07:14,500
data point for that value,
for that amount of income.

125
00:07:14,740 --> 00:07:18,790
And so what happens is
basically it goes on a scale. So

126
00:07:20,540 --> 00:07:22,940
a Gini score of zero is the worst case.

127
00:07:23,000 --> 00:07:27,710
Gini score of zero means that based on
that index for all the other data points

128
00:07:27,800 --> 00:07:28,633
they're gonna be,

129
00:07:28,640 --> 00:07:32,450
they're gonna be evenly split between
less than that value and greater than that

130
00:07:32,451 --> 00:07:34,040
value.
But that's not what we want.

131
00:07:34,340 --> 00:07:38,120
Ideally we want a Gini score of
one that is the ideal Gini core.

132
00:07:38,121 --> 00:07:43,121
And what that means is that for that
given a value that given value for that

133
00:07:43,371 --> 00:07:46,790
specific feature,
all the classes from one,

134
00:07:47,090 --> 00:07:51,500
all the classes from all the data points
from one class will be on one side of

135
00:07:51,501 --> 00:07:52,760
that threshold value.

136
00:07:52,970 --> 00:07:55,760
And all the data points from the other
class will be on the other side of that

137
00:07:55,761 --> 00:07:56,594
threshold value.

138
00:07:56,660 --> 00:08:00,980
That will give us a one value that is
the a one and Jeannie value, right?

139
00:08:01,220 --> 00:08:05,930
And so what we do is we just compute the
Gini index for every single data point

140
00:08:05,931 --> 00:08:09,230
for every single feature. And we just
do that for every feature, right? So we,

141
00:08:09,440 --> 00:08:13,940
so for let's say for income or
Compute, the Gini index all say, okay,

142
00:08:13,941 --> 00:08:18,050
so let's start with 10,000 we'll compare
every other day at point of 10,000 and

143
00:08:18,051 --> 00:08:21,290
see whether it lies on the left or right,
whether it's greater or less than.

144
00:08:21,530 --> 00:08:25,010
And that we get we then we compute
the Gini index from that. Okay.

145
00:08:25,250 --> 00:08:28,880
Which is this formula right here. And
we do that for every single data point.

146
00:08:29,000 --> 00:08:32,750
And so what's gonna happen is we're going
to have a collection of genie indices,

147
00:08:32,900 --> 00:08:36,470
will have a set of Jeannie indices and
then we'll pick the one that is the

148
00:08:36,471 --> 00:08:41,471
highest and the highest one is the one
such that the data points are most our

149
00:08:42,200 --> 00:08:44,750
most uh,
not evenly split.

150
00:08:44,900 --> 00:08:48,950
That means the most data points from
class a are going to be on the left or

151
00:08:48,951 --> 00:08:52,430
right. And the most data point from the
other class will be on the opposite side.

152
00:08:52,431 --> 00:08:53,210
Do you see what I'm saying?

153
00:08:53,210 --> 00:08:57,610
And by side I mean greater
than or less so the,

154
00:08:57,620 --> 00:08:59,130
so the worst genie,

155
00:08:59,400 --> 00:09:03,960
the worst case is when the data points
are evenly split. We don't want that.

156
00:09:04,020 --> 00:09:06,990
We want them to be all on one
side and all on the other side.

157
00:09:07,140 --> 00:09:11,330
That means that when we get a new data
point, it'll plop down right into it's,

158
00:09:11,480 --> 00:09:15,540
it's, it's a little bucket
with all the rest of, it's a
related data points, right?

159
00:09:15,660 --> 00:09:20,660
So that's the Gini index and there are
different measures of a loss when it

160
00:09:20,911 --> 00:09:25,170
comes to
arithmetic based machine learning models.

161
00:09:25,410 --> 00:09:28,200
Instead of Matrix,
a matrix,

162
00:09:28,201 --> 00:09:32,100
operation based machine learning models
like we see with neural networks. Okay,

163
00:09:32,101 --> 00:09:35,250
so that's a Gini score, Gini index,
whatever you want to call it.

164
00:09:35,340 --> 00:09:40,170
So how do we build this decision tree?
Well, it is. There are two parts. We,

165
00:09:40,171 --> 00:09:42,270
first we've got to construct the tree.
So we,

166
00:09:42,330 --> 00:09:44,340
that's a recursive
process that you'll see.

167
00:09:44,610 --> 00:09:47,460
We have to construct the tree
and once we construct the tree,

168
00:09:47,670 --> 00:09:49,110
then we prune the tree.

169
00:09:49,111 --> 00:09:53,580
So that means we identify and remove the
irrelevant branches that might lead to

170
00:09:53,581 --> 00:09:57,840
outliers to increase classification
accuracy. So wait a second,

171
00:09:57,841 --> 00:10:01,620
you might be asking, why are we building
a random forest in the first place?

172
00:10:01,710 --> 00:10:04,410
Why can't we just build a
decision tree alone? Well,

173
00:10:04,411 --> 00:10:09,180
what happens is if you just build a
decision tree, that's not fun. No,

174
00:10:09,181 --> 00:10:12,750
there's a better answer. If you
just build a decision tree, then

175
00:10:14,430 --> 00:10:16,050
your decision tree could be over fit.

176
00:10:16,170 --> 00:10:18,900
That is a big problem when it
comes to decision trees, right?

177
00:10:19,100 --> 00:10:22,810
It's the decision tree gets over fit to
the data, right? It's like, it's like, uh,

178
00:10:23,430 --> 00:10:25,800
the boy,
someone might memorize an eye chart.

179
00:10:25,830 --> 00:10:28,890
It's not like they can see it
properly, right? With one eye closed,

180
00:10:28,950 --> 00:10:32,910
they just memorize the position of where
everything is. Right? In that same way,

181
00:10:32,911 --> 00:10:35,100
we don't want to over fit
for a data to our data.

182
00:10:35,101 --> 00:10:40,101
So the way to prevent that is to create
a bunch of decision trees on random sub

183
00:10:40,771 --> 00:10:44,940
samples of the data. So we'll define
subset, some set of sub samples,

184
00:10:45,090 --> 00:10:48,480
and we'll say for each of these sub
samples we'll create a decision tree.

185
00:10:48,750 --> 00:10:53,750
Then once we have a bunch of decision
trees that we've trained right then by

186
00:10:54,331 --> 00:10:54,721
train,

187
00:10:54,721 --> 00:10:58,110
I mean we've computed the Gini index
for all of the features and then we've

188
00:10:58,111 --> 00:11:01,620
recursively built the tree.
It's a binary tree. By the way,

189
00:11:01,800 --> 00:11:05,190
I didn't mention that the t decision
trees are binary trees right there.

190
00:11:05,190 --> 00:11:10,110
Either I left node or write note or no
node, right? It's the, it's the last node.

191
00:11:10,590 --> 00:11:15,000
And so, um, and if you haven't
reviewed binary trees, I mean,

192
00:11:15,001 --> 00:11:17,010
we're essentially building
a binary tree right now.

193
00:11:17,011 --> 00:11:20,070
But if you want to learn more about data
structures and algorithms or if you're

194
00:11:20,100 --> 00:11:23,940
a curious if you should know
data structures and algorithms
for machine learning,

195
00:11:24,180 --> 00:11:27,600
the answer is yes for two reasons.
One, just for the logic sake,

196
00:11:27,780 --> 00:11:31,620
you need to know how data is stored
because machine learning isn't just about

197
00:11:31,770 --> 00:11:35,280
matrix operations. It's all
also about storing data, right?

198
00:11:35,430 --> 00:11:40,410
Serializing and storing data in the most
efficient way possible and retrieving

199
00:11:40,411 --> 00:11:42,360
it.
And if you want to build algorithms,

200
00:11:42,361 --> 00:11:46,620
you got to have your basic data structure
and algorithm knowledge intact. Okay.

201
00:11:46,621 --> 00:11:48,330
I just wanted to say that back to this,

202
00:11:49,020 --> 00:11:53,250
the way random forest work are each of
the decision trees that are generated.

203
00:11:53,500 --> 00:11:55,690
Then we'll just,
once we have a new data point,

204
00:11:55,691 --> 00:11:57,940
we'll run it through all
of those decision trees.

205
00:11:58,060 --> 00:12:01,360
They'll all make a prediction and then
we'll make a majority vote. So we'll,

206
00:12:01,370 --> 00:12:06,250
we'll calculate a majority vote. So
each of the votes for each of the trees,

207
00:12:06,340 --> 00:12:07,720
whatever the majority vote,

208
00:12:07,990 --> 00:12:10,960
which is the class that is the
class that we're predicting.

209
00:12:11,140 --> 00:12:15,370
And what this does is it gives us higher
accuracy than just using a decision

210
00:12:15,371 --> 00:12:16,204
tree alone.

211
00:12:16,240 --> 00:12:21,240
Let me also say that random forests are
one of the most used machine learning

212
00:12:22,061 --> 00:12:23,050
techniques out there.

213
00:12:23,200 --> 00:12:27,430
They can be because they can be used
for both classification and regression.

214
00:12:27,431 --> 00:12:31,660
And there in lies almost 90 what?
90 plus percent of problems. Right?

215
00:12:32,080 --> 00:12:36,190
And it also works well for very small
data sets, which we tend to have a lot of.

216
00:12:38,830 --> 00:12:41,920
And so that's,
it's so random forest or just use the law.

217
00:12:41,921 --> 00:12:46,270
They're used so much that, how can
I, how can I, how can I say this?

218
00:12:46,570 --> 00:12:50,590
There you so much that the guy,
what's his name? Josh Gordon.

219
00:12:50,650 --> 00:12:54,040
The Google dude,
his name on Twitter is random for us.

220
00:12:54,070 --> 00:12:58,540
So they are very useful. Hi Josh. If
you're watching this. Okay, back to this.

221
00:12:59,260 --> 00:13:03,610
The, okay, so we're training, we're
training it on subsets of data, right?

222
00:13:03,611 --> 00:13:06,370
One subset per tree.
And that is our random forest.

223
00:13:06,371 --> 00:13:09,700
It's a forest because it consists
of trees as you've probably guessed.

224
00:13:09,760 --> 00:13:13,510
And so if you create a giant random
forest, you get lord of the rings.

225
00:13:13,630 --> 00:13:16,630
Rivendale style. No, you don't,
you, the bigger, the better.

226
00:13:16,631 --> 00:13:19,450
Generally you'll see at the end,
the more trees we add,

227
00:13:19,600 --> 00:13:23,490
the better our accuracy
score gets, right? So yeah,

228
00:13:23,491 --> 00:13:26,920
in each of our nodes are going to
represent a set of feature splits. Right?

229
00:13:26,921 --> 00:13:31,690
So what's the color green?
Red. Green. Okay. What's the
size? Small. Big. Big. Okay.

230
00:13:31,691 --> 00:13:34,810
That fruit is a watermelon, right?
So we just recursively do that.

231
00:13:35,260 --> 00:13:38,920
Are there other good examples of this you
might be asking? And the answer is yes,

232
00:13:38,921 --> 00:13:41,080
of course there are other good examples.
The stock price,

233
00:13:41,081 --> 00:13:44,110
prediction and classification.
I've got two great examples here.

234
00:13:44,410 --> 00:13:47,110
Definitely check them out.
The documentation is pretty sparse,

235
00:13:47,111 --> 00:13:50,410
but the code itself, it's not using any
libraries. So definitely check it out.

236
00:13:50,670 --> 00:13:53,220
All right,
so now the,

237
00:13:53,560 --> 00:13:55,710
let's go into the order of functions
that we're going to follow.

238
00:13:55,730 --> 00:13:58,180
So we're not gonna have time
to write every single function.

239
00:13:58,181 --> 00:13:59,530
We're not using any libraries,

240
00:13:59,740 --> 00:14:04,390
but we will write the two most
important functions split and get split.

241
00:14:04,450 --> 00:14:08,770
And that's going to really take, take
on the majority of our, uh, logic.

242
00:14:09,100 --> 00:14:11,680
But that's what we're going to do.
And that's going to be 40 lines of code,

243
00:14:11,681 --> 00:14:12,514
but for the rest of it,

244
00:14:12,670 --> 00:14:16,300
this is the order of functions that I'm
going to follow the chain of functions.

245
00:14:16,540 --> 00:14:20,440
So let's just get right down into what
this chain of functions will look like.

246
00:14:20,980 --> 00:14:25,390
So I'm going to first of all
look at our dependencies here.

247
00:14:25,391 --> 00:14:30,070
So I'm going to import seed from random
and feed is going to generate pseudo

248
00:14:30,071 --> 00:14:32,230
random numbers.
This is useful for debugging.

249
00:14:32,231 --> 00:14:35,800
You want to do it anytime you have some
random numbers and you want to debug

250
00:14:35,801 --> 00:14:37,930
your code and production or otherwise.

251
00:14:38,050 --> 00:14:41,440
It's always great to have some seed
so that the random numbers that are

252
00:14:41,441 --> 00:14:44,320
generated start from the
same point every time.

253
00:14:45,010 --> 00:14:47,770
And so that's just great for
reproducibility of results.

254
00:14:48,280 --> 00:14:50,380
Or I'm also going to import rand range is,

255
00:14:50,381 --> 00:14:55,310
so it's going to return a
randomly selected element
from a range of numbers, CSV,

256
00:14:55,311 --> 00:14:58,240
because our Dataset is, by the way,
let me, let's see. Our dataset,

257
00:14:58,280 --> 00:15:02,480
our Dataset is a CSV file.
It our Dataset is a CSV file.

258
00:15:02,481 --> 00:15:06,410
So let's open our dataset
and see what it looks like.

259
00:15:08,330 --> 00:15:11,900
It is the numeric data right here,
right?

260
00:15:11,901 --> 00:15:15,890
So all of it is numeric at the end.
The result is either two or one, right?

261
00:15:15,891 --> 00:15:19,850
It's a binary level, either two or
one, and the rest are like 15 features.

262
00:15:19,851 --> 00:15:23,150
Here we're going to use every single
one of them, no features election.

263
00:15:23,151 --> 00:15:25,760
We're going to use every single
one of these features. Okay?

264
00:15:25,761 --> 00:15:28,880
And it's using arithmetic so
that we're only importing math.

265
00:15:29,060 --> 00:15:33,560
We're not even importing num Pi. We're
only importing the math library. Okay,

266
00:15:33,561 --> 00:15:34,700
so let's,
let's look at this thing.

267
00:15:34,850 --> 00:15:38,690
So we've got some really basic data
loader functions here. Load CSV,

268
00:15:38,930 --> 00:15:40,490
initialize a Dataset,
has a list,

269
00:15:40,491 --> 00:15:43,700
open it as a readable file
initialize a CSV reader,

270
00:15:43,910 --> 00:15:47,450
and then for every row in the dataset
appended to this Dataset Matrix,

271
00:15:47,451 --> 00:15:51,680
it's a two d matrix. Return it. And so
we have an in memory version of our data.

272
00:15:52,010 --> 00:15:56,120
We know that part that's, that's, that's
general to all machine learning really.

273
00:15:56,121 --> 00:15:58,820
Whenever you're reading a CSP fall,
what else do we have here?

274
00:15:59,060 --> 00:16:01,430
We have two more helper methods functions.

275
00:16:01,700 --> 00:16:04,850
One to convert a column to
an inch and one to convert,

276
00:16:04,940 --> 00:16:08,510
one to convert a string to an aunt
and one to convert an into a stream.

277
00:16:08,840 --> 00:16:12,470
And that's if we have string values.
But in this case we don't,

278
00:16:12,471 --> 00:16:16,520
we have numerical value so we don't
need this. Okay, so let's go into this.

279
00:16:16,521 --> 00:16:20,940
This order that I was talking about, the
order of algorithms. So the first, um,

280
00:16:21,650 --> 00:16:24,320
so the first one went to look at
is this main code here, right?

281
00:16:24,321 --> 00:16:27,260
So we started off with the seeds
so that we always start with the,

282
00:16:27,380 --> 00:16:30,860
with the same random numbers.
We loaded up our Dataset, right?

283
00:16:30,860 --> 00:16:34,130
That CSV file and then we
converted our strings to integers.

284
00:16:34,131 --> 00:16:37,550
We don't actually need to do
that. But then we said, okay,

285
00:16:37,551 --> 00:16:41,330
so how many folds do we want to have?
And folds means sub samples of data.

286
00:16:41,331 --> 00:16:44,780
So we want five sub samples.
What is the nax depth?

287
00:16:44,781 --> 00:16:48,080
And the depth means how many nodes,
what is the depth of the tree, right?

288
00:16:48,260 --> 00:16:51,660
How many levels of that tree
do we want to create through,

289
00:16:51,670 --> 00:16:55,010
we're going to say Max 10 levels.
And these are our hyper parameters.

290
00:16:55,011 --> 00:16:59,150
We can tune them, we can make them more
or less, and we'll have different results.

291
00:16:59,410 --> 00:17:02,090
They're kind of like note the number
of neurons in a neural network, right?

292
00:17:03,150 --> 00:17:04,520
And so we say,
what's the minimum size?

293
00:17:04,521 --> 00:17:08,540
What's the minimum size for each of those
nodes? How many features do we have?

294
00:17:08,541 --> 00:17:09,860
We'll count all of those as well.

295
00:17:09,860 --> 00:17:14,060
And then what we're gonna do is we're
going to create three different random

296
00:17:14,070 --> 00:17:17,630
forests, or we're going to create
one random forest with just one tree.

297
00:17:17,631 --> 00:17:18,980
So it's actually a decision tree.

298
00:17:19,190 --> 00:17:22,370
And then one with five trees
and then one with 10 trees.

299
00:17:22,520 --> 00:17:27,520
And then we'll assess and then we'll
assess how good each of these random

300
00:17:27,801 --> 00:17:29,870
forests are by measuring
the number of trees,

301
00:17:30,110 --> 00:17:33,560
the accuracy score for each of them.
Okay? That's what we're going to do.

302
00:17:33,770 --> 00:17:36,410
So notice that here is
the big boy right here.

303
00:17:36,411 --> 00:17:41,030
This evaluate algorithm is that main
function that we're going to use to train

304
00:17:41,031 --> 00:17:44,830
our model. So we're going to give
evaluate algorithm, our dataset.

305
00:17:44,990 --> 00:17:48,530
We're going to give it the,
the random forest model that we've built,

306
00:17:48,680 --> 00:17:53,040
the number of folds or the sub samples of
the data, how big we want to treat a, B,

307
00:17:53,110 --> 00:17:56,610
the mid size, the sample
size, the number of trees,

308
00:17:56,611 --> 00:17:58,710
and the number of features
that we've counted.

309
00:17:58,830 --> 00:18:02,160
So let's look at what this evaluates
how rhythm function looks like,

310
00:18:02,161 --> 00:18:03,960
because that's really the big boy,
right?

311
00:18:03,961 --> 00:18:07,920
We want to see what is going on in this
main, this big function right here.

312
00:18:08,130 --> 00:18:10,950
So what I'll do is I will go right here.
Okay.

313
00:18:10,951 --> 00:18:15,390
So what it's doing is it's going to say
this, let's look at this. Okay, ready?

314
00:18:15,600 --> 00:18:20,010
Let's, let's look at this. What it's
gonna do is it's going to say, okay,

315
00:18:20,011 --> 00:18:23,160
so for a given dataset
and for a given algorithm,

316
00:18:24,210 --> 00:18:27,120
which is the random forest algorithm
that we're going to feed it,

317
00:18:27,540 --> 00:18:31,770
let's say the folds are the sub samples
that are used to train and validate our

318
00:18:31,771 --> 00:18:32,520
models.

319
00:18:32,520 --> 00:18:36,810
So we'll split our data into a
training and a validation set,

320
00:18:37,820 --> 00:18:42,090
um, by the number of folds. So
what do I mean by that? Well. Okay,

321
00:18:42,091 --> 00:18:46,590
let's look at that cross validation split
method. Where is that? It's right here.

322
00:18:46,800 --> 00:18:50,220
So, so we basically want to
split the data into cables.

323
00:18:50,250 --> 00:18:54,750
The original sample is randomly
partitioned into k equal size sub samples.

324
00:18:54,751 --> 00:18:56,640
And then of those case sub samples,

325
00:18:56,880 --> 00:19:01,350
a single sub sample is retained as the
validation data and the rest are going to

326
00:19:01,351 --> 00:19:02,640
be used for training data.

327
00:19:02,820 --> 00:19:06,420
It's splitting the data so that k minus
one sub samples are used for training.

328
00:19:06,540 --> 00:19:10,440
And then there's one sub sample that's
left for validation. That's it. Okay.

329
00:19:10,530 --> 00:19:13,290
So back to this, we talked
about, uh, that function.

330
00:19:13,291 --> 00:19:17,430
So once we've split our data into those
folds, then we're going to say, okay,

331
00:19:17,431 --> 00:19:18,780
let's score each of them,
right?

332
00:19:18,781 --> 00:19:22,470
Because we're evaluating
our random forest algorithm.

333
00:19:22,471 --> 00:19:25,770
So we'll say for each of the sub
samples that we have of our data,

334
00:19:25,980 --> 00:19:29,460
let's create a copy of the data and
then remove the given subsample.

335
00:19:29,620 --> 00:19:34,230
We'll initialize a test set because
this algorithm really does two things.

336
00:19:34,231 --> 00:19:34,891
It try it,

337
00:19:34,891 --> 00:19:38,910
it trains our model on the training data
and then it tests it out on the testing

338
00:19:38,911 --> 00:19:43,200
data. That is, it makes predictions on
the testing data, ignoring the labels.

339
00:19:43,530 --> 00:19:45,480
So we'll say okay for each.

340
00:19:45,570 --> 00:19:50,570
So we'll add each row in a
given sub sample to the test
set so that we have test

341
00:19:51,301 --> 00:19:55,200
samples as well. And then we'll get
the predicted label, right, so for,

342
00:19:55,230 --> 00:19:57,680
so we'll use a random forest algorithm.
This,

343
00:19:57,690 --> 00:20:00,510
this is our random force algorithm that
that's the next thing we're going to

344
00:20:00,511 --> 00:20:04,700
look at. But it's going to get all the
predicted labels from them, right? For,

345
00:20:04,710 --> 00:20:08,850
from our training and our testing set.
And then it'll get the actual labels,

346
00:20:08,851 --> 00:20:13,020
right? So, and once it has the predicted
labels and it has the actual labels,

347
00:20:13,200 --> 00:20:16,650
then it can compare the two
via this accuracy metric.

348
00:20:16,890 --> 00:20:21,890
And the accuracy metric is a scalar value
and it is how we assess the validity

349
00:20:22,410 --> 00:20:24,720
of every random forest that we've built.

350
00:20:24,870 --> 00:20:29,870
And so the accuracy metric to go into
this is really simple for each label.

351
00:20:30,720 --> 00:20:33,330
If the actual label matches
the predicted label,

352
00:20:33,540 --> 00:20:36,990
add one to the correct iterator and
then we just calculate the percentage of

353
00:20:36,991 --> 00:20:41,040
predictions that were correct, which
is the number of them divided by the,

354
00:20:41,600 --> 00:20:46,380
the number of correct divided
by the actual ones times
a hundred. Really simple.

355
00:20:46,500 --> 00:20:50,830
Like I said, it's all arithmetic.
It's all plus minus, plus minus,

356
00:20:50,920 --> 00:20:55,420
multiply and divide. That's all this is.
There's, there's no linear Algebra here.

357
00:20:55,421 --> 00:21:00,400
It's all Algebra. Um, so, but
despite how simple this model it is,

358
00:21:00,401 --> 00:21:04,210
it is quite powerful, which
is why it's awesome. Okay,

359
00:21:04,211 --> 00:21:08,410
so that's our evaluate algorithm
function. So let's keep going down, right?

360
00:21:08,411 --> 00:21:13,210
We're going down the chain, go into
the Moon, Moon Ride, Moon Ride,

361
00:21:13,300 --> 00:21:17,320
Moon ride mood. I'm just adding my
crazy aunt as well. So back to this,

362
00:21:17,980 --> 00:21:19,450
if you got that referenced,
you're cool.

363
00:21:19,600 --> 00:21:22,210
If you didn't get their references
from spongebob back to this,

364
00:21:22,211 --> 00:21:26,070
you're still cool. So back to this, so
we're evaluating our algorithm here.

365
00:21:26,080 --> 00:21:29,110
Let's evaluate this thing.
So if we're evaluating our algorithm,

366
00:21:29,530 --> 00:21:33,880
so what does this algorithm function even
do? What, what is this? Right? Let's,

367
00:21:33,881 --> 00:21:38,170
let's see what this algorithm function is.
What this algorithm function is,

368
00:21:38,500 --> 00:21:43,240
is it is our random forest.
That is what it is.

369
00:21:43,540 --> 00:21:45,610
We say for the range
of trees that we have,

370
00:21:45,760 --> 00:21:48,490
let's compute a subsample of those trees.

371
00:21:48,700 --> 00:21:52,870
And then for that sub sample we'll
build a specific decision tree for that

372
00:21:52,871 --> 00:21:55,840
specific sub sample.
And then once we built that tree,

373
00:21:55,841 --> 00:22:00,070
we'll add it to our list of trees and
then we're going to make predictions based

374
00:22:00,071 --> 00:22:04,190
on all of those trees and we'll return
the list. The predictions right?

375
00:22:04,210 --> 00:22:07,780
Seems simple enough, right? So
what does this bagging predict?

376
00:22:07,810 --> 00:22:10,450
Now notice how we're, we just keep
going down the chain, right? We,

377
00:22:10,630 --> 00:22:12,100
we just keep going down the chain.

378
00:22:12,460 --> 00:22:15,670
So this is the list of trees
responsible for making a prediction for,

379
00:22:15,671 --> 00:22:16,960
with each decision tree.

380
00:22:17,080 --> 00:22:21,100
So it combines a predictions from each
decision tree and we select the most

381
00:22:21,101 --> 00:22:24,220
common prediction, the one that
comes up the most, that that label,

382
00:22:24,310 --> 00:22:28,120
that the sum of that label is greater
than the rest. The, some of the others,

383
00:22:28,121 --> 00:22:31,900
right? And there are only two labels
because this is binary classification.

384
00:22:32,140 --> 00:22:34,270
You can also do
multi-class classification,

385
00:22:34,330 --> 00:22:36,670
but we're not going to talk
about that right now. Okay.

386
00:22:36,671 --> 00:22:40,540
So then we've talked about
bagging predict. So what's
next? Sub Sample, right?

387
00:22:40,570 --> 00:22:43,990
Where does this the sub sample function?
So how are we sub sampling?

388
00:22:43,991 --> 00:22:48,100
How are we choosing how to split this
data? Right? That's the question.

389
00:22:48,250 --> 00:22:52,510
How are we choosing how to do
that? Well, the answer is we are,

390
00:22:52,570 --> 00:22:56,230
we are creating a random subsample and
this is where our randomness comes in,

391
00:22:56,231 --> 00:23:00,490
right? We are creating a random subsample
in this random range for the number of

392
00:23:00,491 --> 00:23:05,410
samples and the data and we'll add that
list of sub samples to this sample array

393
00:23:05,500 --> 00:23:06,311
and then return that.

394
00:23:06,311 --> 00:23:10,720
So the sample array or list contains
all of those samples from our,

395
00:23:10,750 --> 00:23:14,650
from our Dataset, right? We split
them. And so that's for sub sampling.

396
00:23:14,651 --> 00:23:18,160
And so now where were we? Oh,
so we talked about sub sampling,

397
00:23:18,400 --> 00:23:21,280
the building tree part. Let's, let's
see how the tree itself is built.

398
00:23:21,281 --> 00:23:24,850
So if we give it some sub sample,
the depth and size of the tree,

399
00:23:24,851 --> 00:23:27,790
that's too hyper parameters as
well as the number of features.

400
00:23:27,910 --> 00:23:31,930
We expect this function to build
this tree, so let's look at how this,

401
00:23:31,960 --> 00:23:35,740
how this function works. How is
it building the tree itself? Well,

402
00:23:35,741 --> 00:23:38,920
inside of this function we
said we noticed that it's,

403
00:23:39,030 --> 00:23:40,960
it's first using this method gets split,

404
00:23:40,961 --> 00:23:43,960
which we're going to code and which
is where the meat of the code goes,

405
00:23:44,080 --> 00:23:44,913
but we're going to build a,

406
00:23:45,530 --> 00:23:49,180
and that involves creating the root node
and that's going to get that split and

407
00:23:49,181 --> 00:23:52,010
that that's going to this, this is
going to output the root node, right?

408
00:23:52,011 --> 00:23:56,630
That that first node and then we'll call
the split function that's going to call

409
00:23:56,631 --> 00:24:00,020
itself recursively to build out,
build out the whole tree.

410
00:24:00,170 --> 00:24:01,850
So once we've got that root node,

411
00:24:02,030 --> 00:24:04,820
then we're going to call split
recursively and so it's just going to

412
00:24:04,821 --> 00:24:08,810
continuously build that tree
recursively by calling itself.

413
00:24:09,050 --> 00:24:12,830
In case you haven't heard of recursion,
it's when a function calls itself,

414
00:24:12,980 --> 00:24:17,060
it's like inception, except
it's recursion. Yeah. Wow.

415
00:24:17,061 --> 00:24:19,520
I never actually made that reference
until I just said that, right?

416
00:24:19,700 --> 00:24:23,750
Inception is recursion
a dream inside a dream.

417
00:24:24,830 --> 00:24:29,150
Whoa. Okay. Back to this.
Um, right. Uh, right.

418
00:24:29,151 --> 00:24:33,620
So where were we? Uh, so we were
at split and get split. So let's,

419
00:24:33,650 --> 00:24:35,150
let's write out get split,
right?

420
00:24:35,180 --> 00:24:37,370
So get split is the first one
that we want to write out.

421
00:24:37,880 --> 00:24:40,940
This is going to select the best
split point in a Dataset, right? That,

422
00:24:40,941 --> 00:24:41,870
that key question.

423
00:24:41,990 --> 00:24:45,980
How do we know when to split our data
in this decision tree of the many

424
00:24:45,981 --> 00:24:48,320
decisions trees that we
make in our random forest?

425
00:24:48,560 --> 00:24:51,230
The answer is we'll have
to compute it this way.

426
00:24:51,231 --> 00:24:54,110
This is an exhaustive
and greedy algorithm.

427
00:24:54,111 --> 00:24:57,710
That means it is just going to go through
every single iteration that it can,

428
00:24:57,740 --> 00:25:01,580
right? There are no heuristics
here. There's no educated guesses,

429
00:25:01,610 --> 00:25:03,050
there are no educated guesses.

430
00:25:03,320 --> 00:25:07,400
It's going to go through every single
data point to compute that split.

431
00:25:07,520 --> 00:25:10,220
So let's look at what this looks like.
Well, we've got to give it, first of all,

432
00:25:10,221 --> 00:25:14,420
our Dataset of course. And we want to
give it the number of features, right?

433
00:25:14,510 --> 00:25:17,390
Because for each of those features,
we're going to compute that split.

434
00:25:17,690 --> 00:25:18,710
So given a data set,

435
00:25:18,711 --> 00:25:23,090
we got to check every value on each
attribute as a candidate split.

436
00:25:24,680 --> 00:25:25,350
So we're gonna,

437
00:25:25,350 --> 00:25:28,940
what we're gonna do is we're going to
say let's get all of those class values

438
00:25:28,941 --> 00:25:33,260
right and that, that, that set of
class values is going to be a list.

439
00:25:33,530 --> 00:25:37,460
That set of class values is going to be
a list and it's going to be a list for

440
00:25:37,520 --> 00:25:39,110
every single data point.

441
00:25:40,010 --> 00:25:44,120
And so we'll save all of the rows from
our dataset are our data points, right?

442
00:25:44,630 --> 00:25:46,670
We have that,
we know that

443
00:25:48,980 --> 00:25:52,820
we want to calculate the
index, the value, the score,

444
00:25:52,850 --> 00:25:56,000
and the groups can.
So

445
00:25:59,060 --> 00:26:04,060
we'll initialize all of these as really
big numbers and they will be updated

446
00:26:04,460 --> 00:26:08,390
with time, but we'll initialize them
as very big numbers as well. Okay,

447
00:26:08,510 --> 00:26:12,140
so check this out.
So the Gini Index,

448
00:26:12,170 --> 00:26:15,620
essentially it gives us two things. It
gives us an index and it gives us a value,

449
00:26:15,621 --> 00:26:20,450
the index of the feature of the
feature of the, so it gives us the,

450
00:26:20,480 --> 00:26:25,480
it gives us the index of some feature
whose value is the optimal value to split

451
00:26:26,211 --> 00:26:28,670
the data on for that feature.
Do you see what I'm saying?

452
00:26:28,940 --> 00:26:31,820
It gives us the index of say for income,

453
00:26:32,030 --> 00:26:36,260
let's say that 30,000 is the best feature
is going to be that decision node.

454
00:26:36,470 --> 00:26:40,130
From then we can split everything
based on 30,000 that is where the,

455
00:26:40,160 --> 00:26:45,030
the classes are most split. It's going
to give us the index of 30,000 in the,

456
00:26:45,031 --> 00:26:48,570
in the Dataset as well as the
value, whatever it is, 30,000 right?

457
00:26:48,571 --> 00:26:51,540
So that's the pair that
the Gini index gives us,

458
00:26:51,541 --> 00:26:56,280
the index and the value and it will also
give us a score and the groups and the

459
00:26:56,281 --> 00:27:01,080
groups are the sub samples. And the score
is the, is is how good it is, right?

460
00:27:01,081 --> 00:27:02,400
It's a measure of how good it is.

461
00:27:02,700 --> 00:27:07,700
So we all want to initialize our features
here as a list and then we're going to

462
00:27:09,961 --> 00:27:12,600
say, okay, so while the

463
00:27:13,930 --> 00:27:14,390
yeah,

464
00:27:14,390 --> 00:27:17,740
number of features is less than,
uh,

465
00:27:19,250 --> 00:27:20,031
the number of features,

466
00:27:20,031 --> 00:27:23,360
whereas this is going to be zero and we're
going to increase it every time as we

467
00:27:23,361 --> 00:27:25,820
iterate through each of the features.
Well,

468
00:27:25,821 --> 00:27:28,940
let's see this decide some
random range, right? Some,

469
00:27:29,030 --> 00:27:34,030
some random index in our
dataset to then to a pen,

470
00:27:35,001 --> 00:27:36,020
to our features list.

471
00:27:36,200 --> 00:27:40,250
So we'll say if the index
is not in the features,

472
00:27:41,120 --> 00:27:45,320
which it won't be then a pen at first,
but eventually it will,

473
00:27:45,590 --> 00:27:46,281
will append,

474
00:27:46,281 --> 00:27:50,630
it will append the index wherever we
are to the list of features that we

475
00:27:50,631 --> 00:27:53,390
initialize is empty.
And then

476
00:27:55,330 --> 00:27:58,750
once we've done that, we'll say, um,

477
00:28:00,620 --> 00:28:00,970
yeah,

478
00:28:00,970 --> 00:28:04,990
every index in the Dataset for
each of those indices, let's,

479
00:28:05,590 --> 00:28:07,960
let's go through every
single row in the datasets.

480
00:28:08,290 --> 00:28:09,730
So we're competing groups here,
right?

481
00:28:09,731 --> 00:28:13,590
So we're competing groups to split
our data into, so we're saying, well,

482
00:28:13,730 --> 00:28:15,700
we want to decide the,

483
00:28:15,850 --> 00:28:20,190
the test values that we want
to split as well as the, um,

484
00:28:21,280 --> 00:28:22,270
the Gini Index.

485
00:28:22,510 --> 00:28:26,500
And so the Gini index is what
we're computing right here.

486
00:28:26,501 --> 00:28:30,940
This is the point that we're computing
are Gini index for the current group of

487
00:28:30,941 --> 00:28:32,760
data that we're in, right? So for,

488
00:28:32,780 --> 00:28:36,130
for we've picked a feature and we're going
to iterate through every row for that

489
00:28:36,131 --> 00:28:36,640
feature.

490
00:28:36,640 --> 00:28:40,300
We're going to compute the Gini index for
all of those values and we're going to

491
00:28:40,301 --> 00:28:44,590
pick the GDN decks. That is the,
that is the, uh, largest, right?

492
00:28:44,591 --> 00:28:45,880
And that is what we're going for.

493
00:28:46,030 --> 00:28:48,730
And that once we picked the
genie and Nexatus largest,

494
00:28:48,940 --> 00:28:53,330
that will give us the index and value to
then build out that, that note of this,

495
00:28:53,410 --> 00:28:56,260
the decision tree. Okay. So then, um,

496
00:28:57,340 --> 00:28:59,620
we've computed that and then
we're going to say, okay,

497
00:28:59,650 --> 00:29:04,550
if the Gini index is less
than the optimal score, uh,

498
00:29:04,570 --> 00:29:09,570
then we want to say we're going to
update these values to the new values,

499
00:29:09,910 --> 00:29:13,120
to the score,
the value of the index and the groups.

500
00:29:22,010 --> 00:29:22,843
Okay.

501
00:29:25,600 --> 00:29:30,520
So we're going to use a dictionary to
store the node in the decision tree by its

502
00:29:30,521 --> 00:29:33,580
name. So we'll say return, where were we?

503
00:29:33,820 --> 00:29:36,580
So we'll say return the index.

504
00:29:37,000 --> 00:29:37,833
Okay.

505
00:29:40,900 --> 00:29:42,670
As well as the value,

506
00:29:43,360 --> 00:29:45,760
the value and

507
00:29:48,680 --> 00:29:53,210
the groups, the groups, right?

508
00:29:53,211 --> 00:29:55,610
So we went to index the
value in the groups, right?

509
00:29:55,700 --> 00:29:59,660
Cause we've computed all those, right?
So this, that function gives us,

510
00:29:59,720 --> 00:30:02,960
gives us the root node, right?
And so once we have the root node,

511
00:30:03,170 --> 00:30:05,210
then we can actually
perform the splitting,

512
00:30:05,211 --> 00:30:06,980
write it down the best flooding point.

513
00:30:07,130 --> 00:30:10,220
And so now we want to recursively
compute this splitting itself.

514
00:30:10,310 --> 00:30:13,370
So that's where our split function
comes in. Given that root node,

515
00:30:13,580 --> 00:30:18,230
how do we build the tree such that it
is split along the ideal aligns. Okay,

516
00:30:18,231 --> 00:30:21,850
so let's, let's see. Let's write out this
split function. Okay, so it's going to,

517
00:30:21,920 --> 00:30:24,710
so basically, so this is the
binary tree part. If you've,

518
00:30:24,890 --> 00:30:29,570
if you've created a binary tree before
it is exactly the same. So given some,

519
00:30:30,360 --> 00:30:31,970
uh, root node, right,

520
00:30:32,210 --> 00:30:37,210
we're going to say some root node will
compute a left and right leaf for that

521
00:30:37,491 --> 00:30:40,820
node. And then we'll delete the
original node. So we'll say, okay,

522
00:30:40,821 --> 00:30:43,580
so now we can delete that.
And then

523
00:30:46,440 --> 00:30:47,580
once we've done that,

524
00:30:48,030 --> 00:30:51,690
we can check if either the left
or right group of nodes is empty.

525
00:30:51,810 --> 00:30:55,530
So we're checking if either the left note
or the right note is empty. And if so,

526
00:30:55,740 --> 00:31:00,120
then we create a terminal node using the,
the, the records that we do have here,

527
00:31:00,121 --> 00:31:05,040
right? So an a terminal node. By the way,
let's look at what a terminal note is.

528
00:31:05,310 --> 00:31:08,970
We select a class value for a group of
rows and then return the most common

529
00:31:08,971 --> 00:31:10,860
output value in that list of rose.

530
00:31:10,861 --> 00:31:13,980
What is the most common health
put value in that list of rose.

531
00:31:14,220 --> 00:31:17,370
And that is the most common class.
So that's what we're doing.

532
00:31:17,371 --> 00:31:21,150
We're selecting the most common
class. Okay? So that's the first part.

533
00:31:21,390 --> 00:31:24,150
And so we want to check if we've
reached our maximum depth, right?

534
00:31:24,151 --> 00:31:28,020
So that that depth is our hyper parameter
is a threshold for how large we want

535
00:31:28,021 --> 00:31:32,390
our tree to be. So we'll check if we've,
if we've reached that point. So we'll say,

536
00:31:32,600 --> 00:31:33,433
um,

537
00:31:36,180 --> 00:31:40,290
if the depth is greater than
or equal to the Max depth,

538
00:31:40,980 --> 00:31:41,813
then

539
00:31:49,350 --> 00:31:52,500
so if we reached our Max depth and then
we create a terminal node, that's what,

540
00:31:52,510 --> 00:31:56,940
that's what that's saying. Okay. So then,
um, we've, we've got two more parts here.

541
00:31:57,500 --> 00:32:02,370
All right, so the next
part is to say, okay,

542
00:32:02,371 --> 00:32:05,040
so first the two groups of data
that are split by the node,

543
00:32:05,041 --> 00:32:08,400
we retrieved them when we store them
in the left and rights variables here.

544
00:32:08,550 --> 00:32:09,690
And then we delete that node.

545
00:32:09,970 --> 00:32:14,670
Then we check if either the left or the
right group of is empty. And if they are,

546
00:32:14,700 --> 00:32:18,660
we create a terminal node using the
records that we already have right here.

547
00:32:19,020 --> 00:32:21,180
And so the terminal node by the way,

548
00:32:21,360 --> 00:32:24,840
is where we just select the class
value that's the most used, right?

549
00:32:26,000 --> 00:32:30,630
And that is the, that is the output class.
The output class, the terminal node.

550
00:32:30,631 --> 00:32:34,440
Is that, what is that? What is the,
what is the prediction itself? Right?

551
00:32:34,470 --> 00:32:37,980
So that's the end point. And
so then, um, we check if,

552
00:32:37,981 --> 00:32:40,580
so then we check if either the
left or right group of roses empty.

553
00:32:40,581 --> 00:32:42,230
And if so we create
that terminal and road.

554
00:32:42,530 --> 00:32:44,900
So then we check if we've
reached our maximum depth.

555
00:32:45,110 --> 00:32:49,040
And if so we create a terminal node. And
so that's what this part is. And then,

556
00:32:49,130 --> 00:32:51,770
and so lastly,
if the group of rows is too small,

557
00:32:52,100 --> 00:32:54,230
we'll create a terminal node else.

558
00:32:54,231 --> 00:32:57,650
We'll add the left node in a depth first
fashion until the bottom of the tree

559
00:32:57,651 --> 00:33:00,620
has reached on.
This branch will do this.

560
00:33:00,750 --> 00:33:03,140
So we'll do the same for
the right child as well.

561
00:33:03,350 --> 00:33:05,570
And so the right side is
in process in the same way.

562
00:33:05,571 --> 00:33:09,890
And then as we rise back up to construct
a tree all the way back up to the route.

563
00:33:10,250 --> 00:33:11,570
Okay,
so get split.

564
00:33:11,600 --> 00:33:15,020
Notice how gets split has been
called here over and over again. Uh,

565
00:33:15,050 --> 00:33:17,810
two more functions and then, and
then we're, we're good with this.

566
00:33:17,990 --> 00:33:22,250
The two more functions I had where the
Gini index and so the Gini index is like

567
00:33:22,251 --> 00:33:25,580
I said, it is, it was that
formula right up here. Okay.

568
00:33:25,610 --> 00:33:28,550
This is the Gini index or Gini Score,
whatever you want to call it.

569
00:33:28,790 --> 00:33:33,790
So the Gini index splits a
dataset involving one input
feature and one value for

570
00:33:33,891 --> 00:33:37,850
that feature, right? What the Gini index
gives us is remember that pair that,

571
00:33:37,980 --> 00:33:42,560
that the value and the
index of some feature, some
features for some data points,

572
00:33:42,920 --> 00:33:46,760
right? And that's, that's the line
that we then split or that's the,

573
00:33:47,070 --> 00:33:50,780
the boundary from which we can split
database on that feature in the future.

574
00:33:51,200 --> 00:33:54,860
And so the way we compute
that is it starts off at zero.

575
00:33:54,860 --> 00:33:57,920
It's some scalar value and we're
computing it for all of the data points.

576
00:33:58,190 --> 00:34:01,490
So for each class value that we have for
all of our classes and we only have two

577
00:34:01,880 --> 00:34:04,230
fraudulent or not fraudulent,
um,

578
00:34:05,030 --> 00:34:08,780
and we only have two credit worthy or
not credit worthy for each of those

579
00:34:08,781 --> 00:34:11,810
classes, we'll select a
random subset of that class,

580
00:34:12,140 --> 00:34:15,020
will compute the average
value for that feature. Um,

581
00:34:15,080 --> 00:34:20,080
and then will compute p times one minus
p where p is the average and that is our

582
00:34:20,601 --> 00:34:22,360
genie scaler. Okay. And we'll,

583
00:34:22,400 --> 00:34:25,850
we'll add them all up together because
we have all of those cause it's the sum

584
00:34:25,940 --> 00:34:29,390
of all of those values and that's where
the sigma notation comes in and will

585
00:34:29,391 --> 00:34:32,750
return that as a Gini score. Okay.
And we compute that for all of,

586
00:34:32,751 --> 00:34:34,310
that's the subsets of our data.

587
00:34:35,270 --> 00:34:40,190
And so the last function to show you
is the predict function and the predict

588
00:34:40,191 --> 00:34:44,930
function is right here,
right? So when, whenever we're
actually making predictions,

589
00:34:44,931 --> 00:34:48,860
this is how it works, it navigates
down the tree. This is asking,

590
00:34:49,040 --> 00:34:51,770
is this person employed or nuts
with this person? Go to school.

591
00:34:51,920 --> 00:34:54,710
What is this person's social security
number? What does this person, you know,

592
00:34:54,711 --> 00:34:56,990
just a bunch of random
questions based on the features.

593
00:34:56,991 --> 00:34:59,990
Each of the features that we
have so predict is recursive.

594
00:34:59,991 --> 00:35:02,930
So whereas the node is always
changing for a given row,

595
00:35:03,230 --> 00:35:05,360
the node could be the left
note or the right node.

596
00:35:05,480 --> 00:35:09,470
So whether or not the value for some data
point is the less than or greater than

597
00:35:09,590 --> 00:35:13,310
some nodes, threshold value that
we've computed using the Gini index,

598
00:35:13,520 --> 00:35:17,570
it will then update the node and then use
that as the new parameter to then run,

599
00:35:17,571 --> 00:35:21,470
predict again. And eventually once it's
reach the terminal node, the last node,

600
00:35:21,471 --> 00:35:25,790
the label, it will return the label
and that and we, and then because,

601
00:35:25,820 --> 00:35:28,960
and that's for one decision tree.
And because we have a random forest,

602
00:35:29,110 --> 00:35:32,930
it's computing that for
every single decision tree
we sum up the values and then

603
00:35:32,931 --> 00:35:36,320
we use the one that is the majority vote.
And that is our prediction.

604
00:35:36,350 --> 00:35:40,380
So then if we test our code,
we'll notice that the, uh,

605
00:35:40,410 --> 00:35:45,390
we've got our, uh, accuracy scores
here. And so the accuracy is getting,

606
00:35:45,450 --> 00:35:48,630
is improving every time. So we've tried
it for three different random forests.

607
00:35:49,420 --> 00:35:51,720
We tried it, we tried it for
one with one decision tree.

608
00:35:51,721 --> 00:35:54,930
We tried it for one with five decision
trees and we tried to it for one with 10

609
00:35:54,931 --> 00:35:59,640
decision trees. And every time the
accuracy accuracy score improved.

610
00:35:59,910 --> 00:36:02,650
And so what this means is
if we give it a 100, uh,

611
00:36:02,700 --> 00:36:07,140
100 tree random forest or a thousand
tree forest, it's going to do really,

612
00:36:07,141 --> 00:36:10,010
really well. Okay? So, uh,

613
00:36:10,140 --> 00:36:13,980
and then we'll be able to predict whether
or not someone's someone is worthy of

614
00:36:13,981 --> 00:36:17,930
getting their credit assessed or not. And
if you made it to the end of this, um,

615
00:36:18,060 --> 00:36:20,700
I'm very happy. So thank
you. And that's all.

616
00:36:21,060 --> 00:36:23,760
Please subscribe for more
programming videos. And for now,

617
00:36:23,820 --> 00:36:26,670
I've got to do something random.
So thanks for watching.

