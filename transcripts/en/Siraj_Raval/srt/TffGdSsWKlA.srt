1
00:00:00,090 --> 00:00:00,840
Hello world,

2
00:00:00,840 --> 00:00:05,220
it's Saroj and we're going to try to
predict earthquakes in this video using

3
00:00:05,221 --> 00:00:06,210
data science.

4
00:00:06,420 --> 00:00:11,040
Now this is an ongoing challenge on the
popular data science competition website,

5
00:00:11,190 --> 00:00:14,340
Kaggle.
It's called La n l earthquake prediction.

6
00:00:14,550 --> 00:00:18,990
And the idea is to use an existing
datasets of seismic activity to try to

7
00:00:18,991 --> 00:00:22,260
predict when an earthquake
will happen and their people.

8
00:00:22,261 --> 00:00:25,980
This is an ongoing challenge. There
are people who are, as we speak,

9
00:00:26,250 --> 00:00:30,360
submitting their best work to try to
predict an earthquake using this provided

10
00:00:30,370 --> 00:00:33,930
dataset. And in this video we
have three learning objectives.

11
00:00:33,931 --> 00:00:38,280
The first learning objective is to get
some idea as to how to think like a data

12
00:00:38,281 --> 00:00:39,750
scientist.
That's the first one.

13
00:00:40,050 --> 00:00:43,890
The second one is to understand
the cat boost algorithm,

14
00:00:44,100 --> 00:00:48,000
which stands for categorical
gradient boosting. It's very popular.

15
00:00:48,150 --> 00:00:50,220
Don't worry if you don't understand that.
We're going to go into that.

16
00:00:50,490 --> 00:00:55,260
And the third objective is to understand
the support vector machine algorithm,

17
00:00:55,261 --> 00:00:58,950
specifically support vector regression,
not classification,

18
00:00:59,100 --> 00:01:03,720
which is usually used for. So those
are our three main learning objectives.

19
00:01:03,900 --> 00:01:06,180
There's going to be a lot of math,
there's going to be code.

20
00:01:06,270 --> 00:01:09,660
And we're going to do that all in
this video in a seven step process.

21
00:01:09,750 --> 00:01:12,410
As you can see here,
we're going to go from zero installing,

22
00:01:12,411 --> 00:01:16,710
solving all of those
dependencies to exploring the
Dataset and then implementing

23
00:01:16,711 --> 00:01:20,160
the math and the code, et
Cetera. So it's super exciting.

24
00:01:20,250 --> 00:01:23,750
And if you haven't subscribed, okay,
so let's get into this. So they're,

25
00:01:23,760 --> 00:01:27,810
our first step is to try to understand
the background of this problem.

26
00:01:27,990 --> 00:01:31,590
So predicting earthquakes has long
been thought to be near impossible,

27
00:01:31,710 --> 00:01:35,760
but if we were able to predict earthquakes
than we could save countless lives

28
00:01:35,761 --> 00:01:38,970
and properties, these are very
destructive forces of nature.

29
00:01:39,090 --> 00:01:43,440
So there are many ways of predicting
the magnitude which equates to how

30
00:01:43,441 --> 00:01:45,300
destructive and earthquake is.

31
00:01:45,600 --> 00:01:49,890
So we can see here our first equation
for magnitude that I'm circling with my

32
00:01:49,950 --> 00:01:53,760
mouse pointer log based,
10 of a over a zero,

33
00:01:54,120 --> 00:01:56,190
a zero is the base quake.

34
00:01:56,220 --> 00:02:01,220
The standard calibration earthquake for
testing purposes and a is the intensity

35
00:02:03,271 --> 00:02:04,950
of the actual earthquake.

36
00:02:04,951 --> 00:02:08,580
So when we divide those two values
and take log base 10 of them,

37
00:02:08,730 --> 00:02:12,630
we get the magnitude and there are
different ways of writing this equation.

38
00:02:12,631 --> 00:02:16,440
As we can see here, we can denote
it as I, we can set, I equal to,

39
00:02:16,441 --> 00:02:19,860
I base zero times 10 to the m,
et Cetera.

40
00:02:20,790 --> 00:02:23,670
And so that doesn't necessarily
have to do with this problem.

41
00:02:23,671 --> 00:02:26,700
It's just really interesting to
note as background knowledge.

42
00:02:26,850 --> 00:02:31,830
So when we take that magnitude and we
try to see what the places with the worst

43
00:02:31,860 --> 00:02:34,620
earthquakes are, we'll find that
San Francisco is number two.

44
00:02:34,621 --> 00:02:38,570
Chilay is number one. And this is
a bad thing, right? So we wanted,

45
00:02:38,590 --> 00:02:41,730
this is a picture of an earthquake and
we want to try to predict when this is

46
00:02:41,731 --> 00:02:42,564
going to happen.

47
00:02:44,010 --> 00:02:46,740
So the idea is that given seismic signals,

48
00:02:46,800 --> 00:02:50,550
we're asked to predict the time
until the onset of earthquakes.

49
00:02:50,551 --> 00:02:55,320
Now these earthquakes or actually
laboratory earthquakes. So in the lab,

50
00:02:55,321 --> 00:02:59,650
two plates were put under pressure
and this resulted in sheer stress.

51
00:03:00,010 --> 00:03:02,980
And we can see what this looks like by
playing this little video right here,

52
00:03:03,250 --> 00:03:05,770
right at three 24 he's
demonstrating this process

53
00:03:08,300 --> 00:03:12,420
wish on the end of the sample
and cause it to share like that.

54
00:03:12,960 --> 00:03:15,980
And so that sharing would be
motion on the undefault surfing.

55
00:03:16,380 --> 00:03:18,360
So it's just like that,
just like in the lab.

56
00:03:18,720 --> 00:03:22,620
And so we can see the training data is
just a single sequence of signal and it

57
00:03:22,621 --> 00:03:25,080
seems to come from one experiment.
However,

58
00:03:25,081 --> 00:03:29,370
the testing data consists of several
different sequences called segments that

59
00:03:29,371 --> 00:03:31,470
may correspond to different experiments.

60
00:03:31,680 --> 00:03:35,520
And we can see a preview of that
right on the website if we go to data.

61
00:03:35,610 --> 00:03:39,870
And so here on this Kaggle page we can
see a sample of what it looks like and

62
00:03:39,871 --> 00:03:44,520
all of these segmentations have a segment
id and then there's a time to failure.

63
00:03:44,880 --> 00:03:49,050
Okay, so there, there are two features
there, right? So two columns there.

64
00:03:49,790 --> 00:03:50,390
Okay.

65
00:03:50,390 --> 00:03:53,750
And so we want to try to figure out the
relationship between those two columns.

66
00:03:53,960 --> 00:03:57,590
So before we begin, definitely check
out these two helpful resources.

67
00:03:57,710 --> 00:03:59,840
So the first one is called Kaggle,
colonels.

68
00:03:59,841 --> 00:04:04,160
And what these are are basically
Jupiter notebooks and it's a way for the

69
00:04:04,161 --> 00:04:08,570
community on Kaggle to share what they've
done with this challenge and they're

70
00:04:08,571 --> 00:04:10,220
super cool.
Definitely check them out.

71
00:04:10,430 --> 00:04:14,810
They definitely gave me a lot of ideas
as to how to approach this problem and

72
00:04:14,811 --> 00:04:17,570
they will for you as well.
How to think about this problem.

73
00:04:17,690 --> 00:04:18,620
It's a great initiative.

74
00:04:18,621 --> 00:04:23,270
And the second one is the website papers
with code that the community is really

75
00:04:23,390 --> 00:04:25,220
digging,
including me right now.

76
00:04:25,221 --> 00:04:29,060
And if you have an idea for a different
type of model or whatever model you want

77
00:04:29,061 --> 00:04:33,470
to use, this is the website to search for,
right? So let's say that I want to use,

78
00:04:33,471 --> 00:04:36,020
I don't know,
genetic algorithms.

79
00:04:36,480 --> 00:04:36,950
Okay.

80
00:04:36,950 --> 00:04:38,150
For supervised learning.

81
00:04:38,151 --> 00:04:42,590
Then I can just type in genetic supervised

82
00:04:43,210 --> 00:04:44,680
and see what shows up.
Okay.

83
00:04:44,681 --> 00:04:48,940
So two papers showed up and we can see
the paper and the code, which is awesome.

84
00:04:48,941 --> 00:04:53,230
This website is awesome. I endorse this
website, I approved this message. Okay.

85
00:04:53,530 --> 00:04:57,380
So, um, so those are the two that
definitely check those out. All right, so,

86
00:04:57,510 --> 00:05:00,730
so there we go with that.
And now we are on to step one.

87
00:05:00,760 --> 00:05:04,330
Here we go for about to install an import,
our dependencies.

88
00:05:04,630 --> 00:05:08,560
So of course we need to install Kaggle
because that's how we're going to

89
00:05:08,561 --> 00:05:12,550
actually retrieve this data set. And
by the way, if you're watching this,

90
00:05:12,551 --> 00:05:17,440
I want you to open up
colab.research.google.com and
just do this with me because

91
00:05:17,441 --> 00:05:19,270
you can,
this is all in the cloud.

92
00:05:19,420 --> 00:05:21,760
You don't have to worry about
configurations or dependencies.

93
00:05:21,910 --> 00:05:23,500
It's all happening in the cloud.
Okay?

94
00:05:24,430 --> 00:05:26,500
So I mean locally you
don't have to worry about,

95
00:05:26,950 --> 00:05:30,070
so let's install that and now
we're going to install the num Pi,

96
00:05:30,071 --> 00:05:34,390
but the latest version to perform
some math operations specifically for

97
00:05:34,391 --> 00:05:37,390
statistical features, which
we'll talk about. And lastly,

98
00:05:37,780 --> 00:05:40,300
we're going to install the cat,
boost the library,

99
00:05:40,301 --> 00:05:44,980
which will allow us to use cap
boost. Great. We installed that.

100
00:05:44,981 --> 00:05:47,500
So now that we've installed
those dependencies,

101
00:05:47,501 --> 00:05:50,260
we can start importing all of
the ones that we're going to use.

102
00:05:50,530 --> 00:05:54,730
So of course we're going to import pandas
because that's for data preprocessing.

103
00:05:55,150 --> 00:05:59,450
We're also going to import num Pi,
which is going to be four matrix math.

104
00:06:01,410 --> 00:06:03,090
Now for machine learning cat boost,

105
00:06:03,150 --> 00:06:07,020
we're going to use just like we installed
specifically it's regressor which

106
00:06:07,021 --> 00:06:09,960
cause, because this is going to be a
regression model, which I'll talk about

107
00:06:12,240 --> 00:06:16,020
and pool which I'll
also talk about and uh,

108
00:06:16,530 --> 00:06:19,350
I guess for scaling we
will want to use psych.

109
00:06:19,560 --> 00:06:23,970
It learns preprocessing,
uh, what was it called?

110
00:06:23,971 --> 00:06:26,100
Standard scaler.
Standard scaler.

111
00:06:28,750 --> 00:06:29,100
Okay.

112
00:06:29,100 --> 00:06:32,160
Uh, what else? For hyper
parameter optimization,

113
00:06:32,161 --> 00:06:35,520
we'll want to use grid search.

114
00:06:36,660 --> 00:06:39,660
That means finding those ideal
hyper parameters for arc,

115
00:06:39,840 --> 00:06:43,160
a cap boost algorithm also for
arch support vector machines.

116
00:06:43,161 --> 00:06:46,710
Speaking of a psychic learn,
how does that second learn?

117
00:06:46,711 --> 00:06:50,400
Has Everything seriously? It's
got it all. It's got it all.

118
00:06:51,060 --> 00:06:54,750
And for Colonel Ridge. Okay,
so I got to talk about that.

119
00:06:54,780 --> 00:06:57,510
We're not going to talk about that right
now, but we will at the, at the end.

120
00:06:58,020 --> 00:07:01,290
But Colonel Ridge is
kernel methods are a way of

121
00:07:03,000 --> 00:07:08,000
improving support vector
machine predictions and
making sure that we can create

122
00:07:09,661 --> 00:07:14,430
a classifier line or regression line in
a feature space that we can visualize or

123
00:07:14,580 --> 00:07:18,630
in a lower dimensional featured space.
So coronal ridge.

124
00:07:22,520 --> 00:07:22,960
Okay.

125
00:07:22,960 --> 00:07:24,640
And then of course data visualization.

126
00:07:24,641 --> 00:07:28,300
We're going to import for this
specific use case map plot line.

127
00:07:32,610 --> 00:07:35,610
Great. Okay. So those
are our dependencies.

128
00:07:38,570 --> 00:07:41,210
That worked. Awesome. So
now we're on to step two,

129
00:07:41,211 --> 00:07:45,070
importing our dataset from Kaggle, right?
So we want to collect that data set.

130
00:07:45,170 --> 00:07:48,680
So to do that we're going to use
Google colabs own file access feature,

131
00:07:48,681 --> 00:07:53,570
which is awesome, which lets us
import files directly into coke colab.

132
00:07:53,960 --> 00:07:55,340
Now once we've done that,

133
00:07:55,370 --> 00:08:00,290
we can create an object called uploaded
that will receive the result of whatever

134
00:08:00,291 --> 00:08:05,291
we upload and then we can perform this
command line operation that will move

135
00:08:05,991 --> 00:08:08,840
that Kaggle dot. Jason Haas, I'll
talk about what I'm, what I'm,

136
00:08:08,930 --> 00:08:13,640
what I'm doing in a
second. Uh, doc, Kaggle

137
00:08:16,400 --> 00:08:18,980
and hand.
C H mod 600.

138
00:08:28,140 --> 00:08:31,710
Okay.
So what we're going to do now is run this

139
00:08:36,580 --> 00:08:40,270
and we're going to upload our Kaggle
Dot Jason File. So what this is,

140
00:08:40,271 --> 00:08:45,271
is a way for Colab to know what
your authentication details are.

141
00:08:45,791 --> 00:08:50,470
It's the first thing you do is you go to
my account and then you click on create

142
00:08:50,471 --> 00:08:55,180
new API token and it will download
that API token has a Jason file.

143
00:08:55,290 --> 00:08:59,100
So then we'll go back and we'll upload
that Kaggle dot Jason File. All right,

144
00:08:59,101 --> 00:09:01,830
so once we've uploaded
our Kaggle dot Jason File,

145
00:09:01,980 --> 00:09:06,980
we can access the Kaggle Api directly
using Kaggle competitions list,

146
00:09:07,620 --> 00:09:12,120
and it's going to show all
of the competitions that
exist there. Now that, that,

147
00:09:12,140 --> 00:09:15,420
that was just for, you
know, demo purposes.

148
00:09:15,421 --> 00:09:18,150
Now we can actually download
the earthquake data.

149
00:09:34,450 --> 00:09:34,811
It's a great,

150
00:09:34,811 --> 00:09:38,650
now it's downloading that data directly
from Kaggle into my colab run time

151
00:09:38,651 --> 00:09:43,000
environment. And once I have that, so I'm
going to see where it is on my system.

152
00:09:43,420 --> 00:09:47,980
Looks like. Okay, there we go. We got it.
We got to unzip this, uh, training file.

153
00:09:47,981 --> 00:09:52,300
So we'll use the unzipped
command, uh, to, to, to unzip it,

154
00:09:52,301 --> 00:09:55,210
trained.csv.zip.
And then we'll see what it looks like.

155
00:09:55,870 --> 00:09:59,650
And once it's done on Zipping, we can
see it right here, totally unzipped.

156
00:09:59,920 --> 00:10:02,050
And we can now go on to step three,

157
00:10:02,200 --> 00:10:05,170
which is performing
exploratory data analysis.

158
00:10:05,500 --> 00:10:08,730
So our first step is going to be to
extract that training data into a pan,

159
00:10:08,731 --> 00:10:12,700
does data frame so we can look at it,
visualize it,

160
00:10:12,701 --> 00:10:16,210
and then eventually perform
some statistical analysis on it.

161
00:10:17,390 --> 00:10:17,970
Okay.

162
00:10:17,970 --> 00:10:21,990
So for Eda, we've got our handy
dandy, a pandas function here,

163
00:10:22,200 --> 00:10:26,040
specifically read CSV, one of the
most used functions and data science.

164
00:10:26,400 --> 00:10:29,430
And we know the name of our file
as we can see it right above us.

165
00:10:29,640 --> 00:10:30,990
It's called train dot CSV.

166
00:10:31,560 --> 00:10:36,430
And how many data points do we want to
import a, this is a pretty big file.

167
00:10:36,431 --> 00:10:40,860
So let's just say 6
million because why not?

168
00:10:41,220 --> 00:10:43,620
And then we're going to say,
well, let's name our data types.

169
00:10:43,621 --> 00:10:46,560
Our first data type is going
to be called acoustic data.

170
00:10:47,870 --> 00:10:48,290
Okay.

171
00:10:48,290 --> 00:10:53,290
And that data type is going to be a an
integer 16 bit integer and our next data

172
00:10:55,040 --> 00:10:57,350
type is going to be
called time to failure.

173
00:10:59,000 --> 00:10:59,833
Yeah,

174
00:10:59,880 --> 00:11:01,560
I mean we can name these anything we want,

175
00:11:01,590 --> 00:11:06,420
but we're just going to call it that
and since this is a very floaty number,

176
00:11:06,421 --> 00:11:09,930
we're going to call it floats 64 there's
a lot of decimal places here and once

177
00:11:09,931 --> 00:11:14,190
we have that we can try and
print out the first 10 entries.

178
00:11:15,240 --> 00:11:20,220
Let's say 10 and there we go.
That's our data set right there.

179
00:11:20,221 --> 00:11:25,221
The first 10 entries and these values
are are all going to change and now our

180
00:11:25,411 --> 00:11:27,380
next step is to visualize this data.

181
00:11:27,410 --> 00:11:31,950
Now specifically we want to visualize
the first 1% of samples in this data.

182
00:11:31,951 --> 00:11:35,910
Now I've already written out this part
in particular right here because data

183
00:11:35,911 --> 00:11:39,780
visualization is not a super hard,
right? We can do this in Tablo.

184
00:11:39,781 --> 00:11:43,740
Drag and drop interfaces exist on
a zero and all of these platforms.

185
00:11:43,920 --> 00:11:48,390
So I just wrote this out here and when
we plot this out, when we compile this,

186
00:11:49,590 --> 00:11:49,860
okay,

187
00:11:49,860 --> 00:11:51,660
when we compile this,

188
00:11:52,110 --> 00:11:56,210
we'll see that right before the time to
failure, right before this blue line,

189
00:11:56,211 --> 00:11:59,650
this time to failure,
that is when the biggest uh,

190
00:11:59,651 --> 00:12:03,130
acoustic a quake quake happens.

191
00:12:03,340 --> 00:12:05,170
So we know that there is a,

192
00:12:05,320 --> 00:12:10,320
a point before the actual earthquake where
there's a spike in acoustic activity,

193
00:12:11,771 --> 00:12:16,200
seismic seismographic activity.
Now,

194
00:12:16,201 --> 00:12:19,500
if we were to visualize not just that,
but all of the data,

195
00:12:20,580 --> 00:12:23,340
not just the first 1%,
we'll see that actually

196
00:12:24,080 --> 00:12:24,420
yeah,

197
00:12:24,420 --> 00:12:27,250
at right before the earthquake,

198
00:12:27,310 --> 00:12:30,820
which is that this blue line is vertical
Blue Line right before this vertical

199
00:12:30,821 --> 00:12:33,880
Blue Line, the earthquake,
every time there is a,

200
00:12:34,090 --> 00:12:37,570
there is a spike in acoustic activity.
So there that,

201
00:12:37,571 --> 00:12:39,370
there is our pattern right there.
And,

202
00:12:39,490 --> 00:12:43,960
and eda exploratory data analysis
helped us visualize this pattern.

203
00:12:43,961 --> 00:12:47,800
So we know that the way it
goes is spike in activity,

204
00:12:47,830 --> 00:12:51,460
earthquake and this just keeps happening.
Other questions we could ask is,

205
00:12:51,550 --> 00:12:55,810
is this interval constant to any other
factors affect this? Right? We're,

206
00:12:55,811 --> 00:13:00,811
we're framing our question by
using exploratory exploratory
data analysis bit by

207
00:13:01,631 --> 00:13:02,464
bit.

208
00:13:03,040 --> 00:13:03,610
Okay,

209
00:13:03,610 --> 00:13:07,180
so we'd done that part and now let's
move on to feature engineering.

210
00:13:07,181 --> 00:13:09,580
But before we do that,
I also want to give out,

211
00:13:09,670 --> 00:13:14,020
give out a shout out to Anton law loss.
So his colonel was really interesting.

212
00:13:14,021 --> 00:13:15,810
What he did was he,

213
00:13:16,090 --> 00:13:20,530
he had the intuition to say that
the data looks like sound waves.

214
00:13:20,531 --> 00:13:23,350
It's also waiting as we can see
in this visual analysis here.

215
00:13:23,560 --> 00:13:26,670
So he converted it into an audio file.
He didn't always did.

216
00:13:26,910 --> 00:13:29,890
Then he animated the wave and
compared it to a dubstep song,

217
00:13:29,891 --> 00:13:32,830
which is interesting. So the,
his idea was that, you know,

218
00:13:32,831 --> 00:13:34,810
perhaps just like a dubstep song,

219
00:13:34,960 --> 00:13:38,350
we can hear the buildup of an
earthquake before the drop.

220
00:13:38,680 --> 00:13:41,500
So that's a really cool colonel
that I would definitely check out.

221
00:13:41,890 --> 00:13:46,060
It's right here,
it's called audio analysis with animation.

222
00:13:46,590 --> 00:13:47,200
Okay.

223
00:13:47,200 --> 00:13:51,910
He's got a bunch of samples here. We can
even play it to see what it sounds like

224
00:13:55,120 --> 00:13:58,200
and then we can play to see what
it sounds like with the dubstep.

225
00:14:09,370 --> 00:14:10,203
Okay,

226
00:14:10,540 --> 00:14:15,070
so interesting. Anyway, data is
amazing. So now onto step four,

227
00:14:15,100 --> 00:14:18,370
feature engineering.
So remember we only had those two columns,

228
00:14:18,371 --> 00:14:21,580
but there are a lot of features we
could have that could help increase the

229
00:14:21,581 --> 00:14:25,270
accuracy of our regression model,
which we're going to build in a second.

230
00:14:25,570 --> 00:14:27,820
So which features do we want to add?

231
00:14:28,060 --> 00:14:31,150
Now there are a lot of
tried and true features,

232
00:14:31,210 --> 00:14:35,050
statistical features that have
improved models in the past.

233
00:14:35,230 --> 00:14:38,170
So what we're gonna do is
we're going to add about nine,

234
00:14:38,171 --> 00:14:41,740
maybe 10 statistical
features to our data set.

235
00:14:41,800 --> 00:14:43,120
So it's going to be a lot more features.

236
00:14:43,121 --> 00:14:47,350
And hopefully the idea is that this is
going to improve the capabilities of our

237
00:14:47,351 --> 00:14:49,300
model. It's going to have
a better prediction value.

238
00:14:49,301 --> 00:14:54,050
So let's do all of that in
a single function called
generate features given our

239
00:14:54,060 --> 00:14:54,893
dataset.

240
00:14:57,260 --> 00:14:57,520
Okay,

241
00:14:57,520 --> 00:15:00,430
so that's what we call
a generate features.

242
00:15:00,431 --> 00:15:04,750
And we're going to say ass train is going
to start off as a list and we're going

243
00:15:04,751 --> 00:15:08,110
to continually append that
list with a series of features.

244
00:15:08,140 --> 00:15:09,880
And I'll explain what each
of these features are.

245
00:15:09,881 --> 00:15:13,330
So of course the mean is the
average value of our data set,

246
00:15:13,570 --> 00:15:18,570
the standard deviation tells us how
spread our Dataset is from the mean.

247
00:15:19,800 --> 00:15:20,630
Okay.

248
00:15:20,630 --> 00:15:24,530
The minimum value is just the
minimum value in our dataset.

249
00:15:33,520 --> 00:15:36,180
Kurtosis I'm going to explain
in a second that's an,

250
00:15:36,230 --> 00:15:39,970
that might be a newer one for you if you
haven't been in this space for a long,

251
00:15:39,971 --> 00:15:44,050
long time. Uh, what else? Skew.
That's a, that's a fun one.

252
00:15:45,660 --> 00:15:48,990
What else?
Um,

253
00:15:54,320 --> 00:15:58,640
maybe tile values a value
or two and we'll split it.

254
00:15:58,710 --> 00:16:03,710
It's just like that
Quan tile and that's it.

255
00:16:04,850 --> 00:16:05,683
This is add those.

256
00:16:14,400 --> 00:16:16,170
And we will now return

257
00:16:19,900 --> 00:16:24,340
what this looks like
and there we go.

258
00:16:24,341 --> 00:16:27,250
So let me explain what each of these
features are and then we're going to talk

259
00:16:27,251 --> 00:16:30,250
about how we can add them to our data set.

260
00:16:30,251 --> 00:16:33,430
So of course the mean
is sigma notation here.

261
00:16:33,640 --> 00:16:36,480
We take all those values and
divide by the number of them. Uh,

262
00:16:36,490 --> 00:16:40,030
the median is the middle number, right?
If when we ordered them sequentially,

263
00:16:40,031 --> 00:16:40,960
what is the middle number?

264
00:16:41,290 --> 00:16:46,240
The Standard Deviation is tells us how
spread out our data is. So here's a,

265
00:16:46,270 --> 00:16:49,600
here's a visual representation
of the standard deviation.

266
00:16:49,601 --> 00:16:54,601
So here it says 99.7% of the data
for this example case is within our,

267
00:16:55,931 --> 00:16:58,270
within three standard
deviations of the mean.

268
00:16:58,660 --> 00:17:03,640
So if this data set
looked much more spread,

269
00:17:03,641 --> 00:17:05,530
this,
this distribution of data,

270
00:17:05,770 --> 00:17:10,060
then it could be up to 15
standard deviations or 20 or
you know, a bigger number.

271
00:17:10,330 --> 00:17:13,900
So it's how spread our distribution is.
That's the standard deviation.

272
00:17:13,901 --> 00:17:18,901
And the way we find that is twoF is just
take the mean and subtract each data

273
00:17:19,781 --> 00:17:22,150
points, square it, add
them all up together,

274
00:17:22,660 --> 00:17:25,210
and then divide by the number
of data points minus one,

275
00:17:25,330 --> 00:17:28,210
take the square root of the whole thing.
That's our standard deviation.

276
00:17:28,240 --> 00:17:32,110
Kurtosis then uses the standard
deviation to compute itself.

277
00:17:32,111 --> 00:17:37,111
We use that the STD to compute a kurtosis
where and so the way this works is we

278
00:17:37,841 --> 00:17:42,841
take the mean and we say take each data
point minus the mean over the number of

279
00:17:43,570 --> 00:17:47,710
data points and do that for as many data
points as there are sigma notation and

280
00:17:47,711 --> 00:17:50,160
divide by the standard deviation.
The fourth power.

281
00:17:50,400 --> 00:17:53,700
Now the Kurtosis tells us
it's a measure of whether the,

282
00:17:53,760 --> 00:17:57,750
whether the data is heavy tailed or
light tailed relative to a normal

283
00:17:57,751 --> 00:17:59,610
distribution.
As you can see in this picture,

284
00:17:59,850 --> 00:18:03,900
positive Kurtosis is higher negative
is lower normal distribution.

285
00:18:04,860 --> 00:18:09,240
And the reason this is
important is because it tells
us how many outliers we can

286
00:18:09,241 --> 00:18:10,350
expect in our data.

287
00:18:10,500 --> 00:18:15,060
Data sets with higher Kurtosis
tend to have heavy tails
or at outliers, right? So,

288
00:18:15,061 --> 00:18:18,240
and the opposite is true as well.
And then there come skew.

289
00:18:18,241 --> 00:18:21,720
So XQ tells us the asymmetry
of our distribution of data.

290
00:18:23,370 --> 00:18:26,850
So this can be positive, negative
or undefined. And lastly,

291
00:18:26,851 --> 00:18:31,580
kwan tiles are basically
ways of of dividing our, the,

292
00:18:31,740 --> 00:18:34,830
the, the range of probability
distributions in our data.

293
00:18:35,040 --> 00:18:36,690
And we can have several Quan tiles.

294
00:18:36,691 --> 00:18:41,691
It's just a way of segmenting that data
out and there's a lot of statistical

295
00:18:41,941 --> 00:18:45,810
features out there. We could have
added more and it, it would, you know,

296
00:18:46,440 --> 00:18:48,330
it would affect our
prediction in different ways.

297
00:18:48,331 --> 00:18:51,960
There's no way to fully know that. I
mean there's model evaluation techniques.

298
00:18:51,961 --> 00:18:56,490
We'd have to try it out.
That's, that's part of the fun,
right? So, so there's that.

299
00:18:57,780 --> 00:19:00,720
Now in this line, we're going to add
all those features to this dataset.

300
00:19:00,840 --> 00:19:03,900
Once we have added those
features to our Dataset,

301
00:19:04,140 --> 00:19:08,550
we can visualize the completed datasets
with the new features that we added

302
00:19:08,820 --> 00:19:11,100
using the describe function.
So there we go.

303
00:19:11,101 --> 00:19:15,450
We've got our new features that have been
added to our dataset. Boom. All right,

304
00:19:16,620 --> 00:19:20,940
so now onto step five,
cat boost. So capp boost,

305
00:19:21,150 --> 00:19:25,020
let's start with gradient boosting
because cap boost is a type of gradient

306
00:19:25,021 --> 00:19:27,120
boosting. So grading, boosting,

307
00:19:27,121 --> 00:19:31,290
it's a technique that can be used for
both regression and for classification

308
00:19:31,291 --> 00:19:32,880
problems,
both types.

309
00:19:33,300 --> 00:19:37,170
And it produces a prediction
model in the form of an ensemble.

310
00:19:37,171 --> 00:19:40,530
That means many a collection
of weak prediction models.

311
00:19:40,531 --> 00:19:42,390
Typically they use decision trees.

312
00:19:42,660 --> 00:19:47,660
Now I want to note that gradient boosting
is not specific to decision trees.

313
00:19:47,790 --> 00:19:51,660
We just tend to use decision trees
because they're easy to implement and they

314
00:19:51,661 --> 00:19:52,890
tend to give good results.

315
00:19:53,040 --> 00:19:56,880
But this say generalizable technique
that can be applied to any type of model.

316
00:19:57,150 --> 00:19:59,400
Here we go for regression.
The idea,

317
00:19:59,430 --> 00:20:03,750
and let's talk about it in the context
of decision trees for explanations sick.

318
00:20:03,960 --> 00:20:05,310
So with gradient boosting,

319
00:20:05,311 --> 00:20:09,210
we start off with a single tree and we
are training it on some distribution of

320
00:20:09,211 --> 00:20:12,330
the data and it's going to build
a decision tree out of that data,

321
00:20:12,390 --> 00:20:15,330
right to to be able to make
a prediction about that data.

322
00:20:15,360 --> 00:20:18,480
Once we have that decision tree,
it's not going to be perfect.

323
00:20:18,510 --> 00:20:21,660
They're going to be data points,
parts of the distribution that are off.

324
00:20:21,690 --> 00:20:24,120
And that's the hidden state.
It could be better.

325
00:20:24,420 --> 00:20:28,680
So we take that hidden state and we use
it to create yet another decision tree.

326
00:20:29,130 --> 00:20:33,500
And so the idea is that we're focusing
on those data points that are not as, um,

327
00:20:33,600 --> 00:20:38,310
encompassed in the regression line or the
regression model of the decision tree.

328
00:20:38,610 --> 00:20:42,450
And we're going to focus on those data
points and more in the next decision tree.

329
00:20:42,750 --> 00:20:45,660
And that's going to create a hidden state,
a representation of the data.

330
00:20:45,840 --> 00:20:48,580
And we're going to do that again.
So we iteratively do that.

331
00:20:48,581 --> 00:20:50,890
And so the equation then becomes h of x,

332
00:20:50,891 --> 00:20:55,390
where x is all of the data is equal to
the first hidden state of x plus the

333
00:20:55,391 --> 00:20:59,110
second hidden state of x, which are the
hidden states of each of these trees.

334
00:21:00,660 --> 00:21:01,560
So in general,

335
00:21:01,740 --> 00:21:06,300
boosting works by iteratively learning
week classifiers and adding them to a

336
00:21:06,301 --> 00:21:09,990
final strong classifier.
And after a week learner is added,

337
00:21:10,110 --> 00:21:14,280
the data's reweighted so that wrongly
classified examples gain weight and

338
00:21:14,281 --> 00:21:17,730
correctly classified samples lose weight.
So in this way,

339
00:21:17,940 --> 00:21:22,590
the weak learners are trained
to concentrate more on
the misclassified examples

340
00:21:22,591 --> 00:21:24,750
as we can see in this image right here,

341
00:21:27,770 --> 00:21:31,820
right? So, uh, great and boosting. It's
a type of grading dissent to algorithm.

342
00:21:31,821 --> 00:21:34,820
And we can see this pseudo code
explains it very well, right?

343
00:21:34,821 --> 00:21:39,821
So we have some data sample distribution
d and so for m equals one two M or m is

344
00:21:40,611 --> 00:21:44,300
a number of base models. We want
to use, that's up for us to decide.

345
00:21:44,960 --> 00:21:48,020
We can train our base model from
the training sample distribution,

346
00:21:48,050 --> 00:21:48,950
compute the error,

347
00:21:48,980 --> 00:21:52,250
and then adjust the distribution of the
next model to make the mistake of the

348
00:21:52,251 --> 00:21:57,251
model more evidence than we output the
constructive based model and use that as

349
00:21:57,351 --> 00:22:01,130
the input to the next iteration
for em, uh, models, right?

350
00:22:01,131 --> 00:22:04,370
So however many models we
decide that we want to use,

351
00:22:04,610 --> 00:22:08,150
and that's the basic idea
behind a gradient boosting,

352
00:22:08,420 --> 00:22:12,980
iteratively improving a bunch of weak
learned weekly learned models until we get

353
00:22:12,981 --> 00:22:17,180
a final strong model that is
better than the sum of its parts.

354
00:22:18,170 --> 00:22:21,650
So now onto cat boost,
which is a version of gradient boosting.

355
00:22:21,860 --> 00:22:25,490
There's actually several types of gradient
boosting techniques. Xg boost is one,

356
00:22:25,630 --> 00:22:27,080
you know,
add a boost there a lot.

357
00:22:27,810 --> 00:22:28,130
Okay,

358
00:22:28,130 --> 00:22:33,020
so Yan index is the Russian Google and
they use gradient boosting a lot and they

359
00:22:33,021 --> 00:22:36,830
use it to power a lot of their services.
So the idea is that for,

360
00:22:36,831 --> 00:22:38,420
for cat boost,

361
00:22:38,900 --> 00:22:42,020
cat booze stands for
categorical gradient boosting.

362
00:22:42,560 --> 00:22:46,540
So what they do is
they say,

363
00:22:47,200 --> 00:22:50,080
let's handle categorical
features automatically.

364
00:22:50,320 --> 00:22:54,490
So without any explicit preprocessing
to convert categories into numbers,

365
00:22:54,850 --> 00:22:59,050
cat boots will convert categorical values
into numbers using various statistics

366
00:22:59,051 --> 00:23:04,000
and combinations of the existing
categorical features and combine both the

367
00:23:04,001 --> 00:23:07,930
categorical and numerical features. And
so there are some advantages. There's uh,

368
00:23:07,960 --> 00:23:08,500
you know,

369
00:23:08,500 --> 00:23:12,430
usually tends to perform better than
the other types of gray and boosting.

370
00:23:12,431 --> 00:23:13,360
As you can see here,

371
00:23:13,510 --> 00:23:17,440
it has the lowest log loss values of
the gradient boosting algorithms that we

372
00:23:17,441 --> 00:23:21,040
see here. So it's, it's really
popular, it's getting more popular.

373
00:23:21,041 --> 00:23:22,600
And the Eli five of this,

374
00:23:22,780 --> 00:23:27,400
as you can see here is we model the data
with simple models and then we find the

375
00:23:27,401 --> 00:23:28,090
errors.

376
00:23:28,090 --> 00:23:31,660
These errors signified data points that
are difficult to fit by the simple model.

377
00:23:31,810 --> 00:23:33,490
And for later models iteratively,

378
00:23:33,520 --> 00:23:36,550
we focus on those hard to fit
data points to get them right.

379
00:23:36,551 --> 00:23:40,810
And in the end we combine
all the predictors together
by giving some weights to

380
00:23:40,811 --> 00:23:41,644
each predictor.

381
00:23:42,330 --> 00:23:42,940
Okay,

382
00:23:42,940 --> 00:23:46,900
so let's go ahead and implement
this now here our handy, uh,

383
00:23:46,940 --> 00:23:48,470
psych it learn library.
Thank you.

384
00:23:48,480 --> 00:23:53,300
Psychic learn where we'll
use this pooling keyword

385
00:23:54,830 --> 00:23:57,680
to collect the training data,
both the

386
00:23:59,720 --> 00:24:01,460
input and the output data.

387
00:24:02,750 --> 00:24:07,750
And then we'll initialize our regression
model using the regress regressor

388
00:24:08,810 --> 00:24:11,750
keyword.
How many iterations?

389
00:24:11,751 --> 00:24:16,751
Let's say 10,000 our loss function is
going to be as a Kaggle competition

390
00:24:17,451 --> 00:24:22,130
suggest mean absolute error and the
type of boosting we're going to do,

391
00:24:24,680 --> 00:24:28,940
which there are many types
is going to be ordered.

392
00:24:31,490 --> 00:24:36,490
And now we go back and we will fit
the model to the training data and the

393
00:24:39,081 --> 00:24:39,980
testing data.

394
00:24:43,640 --> 00:24:44,473
Okay.

395
00:24:45,300 --> 00:24:50,220
And printout the best score.
And now it's training.

396
00:24:51,800 --> 00:24:53,450
And then when we see our score,

397
00:24:53,451 --> 00:24:57,340
we'll see that our mean squared error
is 1.8599999999999999 which keeps us in

398
00:24:57,650 --> 00:25:00,830
maybe top 300 to top
500 it could be better.

399
00:25:00,831 --> 00:25:04,460
But my point is that this is our
first model, our first attempt.

400
00:25:04,730 --> 00:25:08,320
So obviously it's not going to be the
best and we'll just iteratively improve.

401
00:25:08,330 --> 00:25:11,210
Right? So that's, that's
the idea, right? So

402
00:25:11,700 --> 00:25:12,210
yeah,

403
00:25:12,210 --> 00:25:14,570
then we could, you know, submit
this for more. So let's get on.

404
00:25:14,630 --> 00:25:15,680
Let's go on to step six,

405
00:25:15,681 --> 00:25:20,240
which is implementing arch support vector
machine and the radial basis function

406
00:25:20,270 --> 00:25:24,860
colonel. So in simple regression, we're
trying to minimize an error, right?

407
00:25:24,861 --> 00:25:28,700
So regression models try to find the
relationship between variables, right?

408
00:25:28,701 --> 00:25:32,720
One or more variables. And in a
support vector machine for regression,

409
00:25:32,840 --> 00:25:37,580
we're trying to fit the air
within a certain threshold.
That's the difference. So

410
00:25:38,630 --> 00:25:41,910
the idea is that the blue line can be
considered what's called the hyper plane.

411
00:25:42,260 --> 00:25:46,130
The red lines are the boundary lines.
So the Blue Line is the line of best fit,

412
00:25:46,131 --> 00:25:50,480
where the red lines are boundary lines
that we're going to decide by some value,

413
00:25:50,481 --> 00:25:54,050
they're off from the hyper plane and
we're only going to consider data points

414
00:25:54,051 --> 00:25:58,160
within this margin.
So only those data points,

415
00:25:58,280 --> 00:26:00,140
there could be data
points all over the place.

416
00:26:00,290 --> 00:26:03,980
And so simple regression will take
into account all of those data points.

417
00:26:04,220 --> 00:26:08,540
But support vector regression will only
take into account the data points inside

418
00:26:08,541 --> 00:26:12,710
of that margin, right? And those margins
are defined by the support directors,

419
00:26:12,711 --> 00:26:17,150
which are the data points that are
closest to the line of best fit. So

420
00:26:18,060 --> 00:26:18,610
okay,

421
00:26:18,610 --> 00:26:22,210
that's our objective with the support
vector regression and the boundary lines.

422
00:26:22,211 --> 00:26:27,010
We draw our either plus or
minus e distance from the
hyper hyperplane where he

423
00:26:27,011 --> 00:26:28,450
has some value that we defined.

424
00:26:29,870 --> 00:26:34,370
So we can define the equation
for our boundary line as such,

425
00:26:34,580 --> 00:26:39,550
it's going to be uh, where you know y
equals mx plus B is the line. So, uh,

426
00:26:39,551 --> 00:26:39,880
we'll,

427
00:26:39,880 --> 00:26:44,130
we'll just add or subtract eight from
that equation and going to give us our

428
00:26:44,160 --> 00:26:45,120
line of best fit.

429
00:26:45,121 --> 00:26:48,840
So we're only so notice how there
are some data points outside of that,

430
00:26:48,990 --> 00:26:49,980
but we're not considering,

431
00:26:49,981 --> 00:26:53,370
those were only considering
and fitting our data to the,

432
00:26:53,430 --> 00:26:57,210
to those data points inside
of those boundary lines.

433
00:26:57,840 --> 00:26:59,880
And that's a support vector regression.

434
00:27:00,610 --> 00:27:01,020
Okay.

435
00:27:01,020 --> 00:27:05,010
So a lot of times we have what's
called a non linear decision boundary.

436
00:27:05,250 --> 00:27:07,890
That means that a straight
line cannot fit the data,

437
00:27:07,891 --> 00:27:11,490
which means our support vector machines
not going to work well and the weight.

438
00:27:11,491 --> 00:27:13,740
And so on the left, as you
can see here, you know,

439
00:27:13,860 --> 00:27:18,270
so we ideally we'd like to have the data
points separated such that if we were

440
00:27:18,271 --> 00:27:20,820
to classify or to regress the data,

441
00:27:21,060 --> 00:27:24,690
we would be able to linearly separate it
or linearly create the line of best fit.

442
00:27:25,170 --> 00:27:28,740
You know, how do you make a line of best
fit for that data over here, it's hard.

443
00:27:28,920 --> 00:27:32,250
Uh, it's impossible actually,
uh, in, in, in that space.

444
00:27:32,251 --> 00:27:36,960
So what the solution to that is to use
what's called the kernel trick and the

445
00:27:36,961 --> 00:27:37,720
kernel trick,
the,

446
00:27:37,720 --> 00:27:41,910
a very simple way of thinking about
kernels are they are a similarity function

447
00:27:41,940 --> 00:27:42,870
given two objects,

448
00:27:42,871 --> 00:27:47,400
the colonel will output some similarity
scores and we can use those scores to

449
00:27:47,401 --> 00:27:52,320
reframe the problem such that a linear
regression line or classification line

450
00:27:52,470 --> 00:27:55,590
can either separate or find the
line of best fit for the data.

451
00:27:55,920 --> 00:27:58,560
And there are a lot of different
types of kernels, right?

452
00:27:59,470 --> 00:27:59,740
Okay.

453
00:27:59,740 --> 00:28:01,690
So,
right inputs based a feature space,

454
00:28:01,900 --> 00:28:03,610
there are many different
types of kernels out.

455
00:28:03,611 --> 00:28:08,410
Therefore scms linear polynomial
Gaussian sigmoid. Uh, but in this,

456
00:28:08,500 --> 00:28:12,010
in this, he's Galcion were
radio basis as we can see here,

457
00:28:12,190 --> 00:28:14,380
but they're basically the
dot product of two vectors.

458
00:28:14,381 --> 00:28:15,490
We can think of it that way.

459
00:28:17,050 --> 00:28:20,710
And so this is the equation for
the support vector machine, right?

460
00:28:20,711 --> 00:28:24,610
Where we have a weight value
that is learned through
some optimization technique,

461
00:28:24,611 --> 00:28:27,920
light gradient descent. And then we
have the mx plus B, you know, the,

462
00:28:27,921 --> 00:28:31,840
the line of best fit format as well.
And these are supported vectors.

463
00:28:33,530 --> 00:28:36,260
So that's the basic idea behind
a support vector regression.

464
00:28:36,830 --> 00:28:39,110
And that is not as used.

465
00:28:39,200 --> 00:28:42,560
It's not used as much or understood
as much as a support vector machine

466
00:28:42,830 --> 00:28:45,410
specifically for classification.
So I thought I wanted,

467
00:28:45,411 --> 00:28:49,160
I wanted to shine some
light there on that, right?

468
00:28:49,190 --> 00:28:52,700
It's usually used for classification
where a linear decision surface called a

469
00:28:52,701 --> 00:28:55,010
hyperplane is used to
separate these classes.

470
00:28:55,840 --> 00:28:56,420
Okay.

471
00:28:56,420 --> 00:28:59,140
So what we're going to
do is we're going to uh,

472
00:28:59,630 --> 00:29:02,660
build the support vector machine using
psychic learn and then we're going to

473
00:29:02,661 --> 00:29:06,560
pick the ideal of hyper parameters
using a hyper parameter optimization

474
00:29:06,561 --> 00:29:09,710
technique called grid search, which you're
going to pick a bunch of values for.

475
00:29:09,711 --> 00:29:13,010
Each pair of values is going to
evaluate the validation error function,

476
00:29:13,011 --> 00:29:17,030
then pick the pair that gives the minimum
value of the validation error function.

477
00:29:17,031 --> 00:29:21,320
It's basically and try out the pears
until it finds the best pair for,

478
00:29:21,350 --> 00:29:25,550
for our model. And so what we can see
here is that I've scaled those values.

479
00:29:25,880 --> 00:29:30,880
I'm using my support vector, regression,
str, and these are my, you know, the,

480
00:29:30,881 --> 00:29:33,560
the possible values that I've
defined for my grid search,

481
00:29:33,561 --> 00:29:35,570
high parameter optimization technique.

482
00:29:35,780 --> 00:29:39,800
So it's going to try out the pairs of
gamma and see to try to find those,

483
00:29:39,920 --> 00:29:43,150
those ideal hyper parameters
for our model so that it's,

484
00:29:43,180 --> 00:29:46,210
that's going to fit the data
and grid search CV does that,

485
00:29:46,240 --> 00:29:50,140
the colonel is going to be right the
radial basis function as we saw before.

486
00:29:51,150 --> 00:29:55,270
And then we'll make a prediction and then
we'll output the best CV score, which,

487
00:29:55,390 --> 00:29:57,910
which is right here.
Now obviously we could do better.

488
00:29:57,940 --> 00:30:00,580
There are different types of models
we could use. We only use two.

489
00:30:00,820 --> 00:30:02,650
But remember our learning objectives here,

490
00:30:02,830 --> 00:30:06,910
we're learning how to think like a data
scientist at least a little bit. Uh,

491
00:30:06,970 --> 00:30:10,360
learning about the cat boost algorithm
and learning about the support vector

492
00:30:10,361 --> 00:30:14,770
machine algorithm, specifically support
vector regression. And we did that.

493
00:30:14,830 --> 00:30:18,100
So future ideas, uh, we
could try recurrent networks,

494
00:30:18,280 --> 00:30:22,870
we could try genetic algorithms, we could
try ordinary differential equations.

495
00:30:22,871 --> 00:30:26,740
And think about this as a time series
where there's just irregular data,

496
00:30:26,741 --> 00:30:31,180
which Oh, nets are great for. So there's
a lot we can do and it's super exciting.

497
00:30:31,181 --> 00:30:32,560
I hope you found this video useful.

498
00:30:32,680 --> 00:30:35,050
I'm going to have a bunch of helpful
links in the video description.

499
00:30:35,140 --> 00:30:37,390
What's a data science model
you want to learn more about?

500
00:30:37,391 --> 00:30:40,420
Let me know in the comments section and
please subscribe for more programming

501
00:30:40,421 --> 00:30:44,440
videos. For now, I'm going to find
a lasso. So thanks for watching.

