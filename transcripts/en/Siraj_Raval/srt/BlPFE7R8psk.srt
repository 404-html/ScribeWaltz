1
00:00:00,090 --> 00:00:02,610
Ai In 2019 hot dog,

2
00:00:02,611 --> 00:00:05,820
not hot dog at the same time hello world,

3
00:00:05,821 --> 00:00:10,500
it's Saroj and 2019 is going to be an
incredible year for the Ai Community.

4
00:00:10,710 --> 00:00:15,510
I'm going to briefly recap some of the
major highlights in the field from 2018

5
00:00:15,511 --> 00:00:19,380
then use some of those highlights to
make 10 predictions on what's going to

6
00:00:19,381 --> 00:00:24,330
happen in 2019 the subset of AI deep
learning has accounted for the majority of

7
00:00:24,331 --> 00:00:29,331
the public discourse on Ai in 2018 we've
seen some incredible new applications

8
00:00:29,761 --> 00:00:34,380
of it so far, but let me start by
stating something that might shock you.

9
00:00:34,890 --> 00:00:38,310
Even though I'm ethnically Indian,
my mom was born in Kenya,

10
00:00:38,311 --> 00:00:42,180
which technically makes me an
African American, but besides that,

11
00:00:42,510 --> 00:00:46,080
a substantial fraction of the biggest
developments in deep learning.

12
00:00:46,230 --> 00:00:51,230
We're more akin to tuning
existing techniques versus
creating qualitatively new

13
00:00:51,781 --> 00:00:52,614
ideas.

14
00:00:52,770 --> 00:00:56,490
It's been a year of refining existing
algorithms and the ones that received the

15
00:00:56,491 --> 00:01:01,230
most press combined deep learning
and reinforcement learning together.

16
00:01:01,530 --> 00:01:02,363
This technique,

17
00:01:02,370 --> 00:01:06,810
deep reinforcement learning enabled
open AI's Bot to defeat the world's best

18
00:01:06,811 --> 00:01:07,644
dota players.

19
00:01:07,830 --> 00:01:12,300
It enabled deep mind spot to beat the
world's best go player and enables Uber's

20
00:01:12,301 --> 00:01:14,340
bots to beat Montezuma's revenge.

21
00:01:14,670 --> 00:01:19,140
The idea is that made up these algorithms
were all at least a decade old.

22
00:01:19,141 --> 00:01:23,220
They just been combined in novel
ways to defeat these games.

23
00:01:23,490 --> 00:01:27,960
The idea is that if an AI can learn how
to optimize its score in a game world,

24
00:01:28,110 --> 00:01:32,700
it could possibly also learn how to
optimize any function in the real world,

25
00:01:32,701 --> 00:01:37,380
which deep minds Alpha fold actually
did later in the year by predicting the

26
00:01:37,381 --> 00:01:39,150
three d shapes of proteins,

27
00:01:39,151 --> 00:01:44,151
enabling scientists to design new cures
for diseases much more efficiently in

28
00:01:44,161 --> 00:01:45,840
terms of global developer activity.

29
00:01:45,841 --> 00:01:50,841
The two hottest areas of AI on good
hub centered around NLP and generative

30
00:01:51,360 --> 00:01:52,193
modeling.

31
00:01:52,290 --> 00:01:57,180
It was a successful application
of transfer learning
applied to NLP that brought

32
00:01:57,181 --> 00:01:58,590
such incredible results.

33
00:01:58,890 --> 00:02:03,390
Transfer learning is the art of being
able to apply pretrained models to data.

34
00:02:03,600 --> 00:02:07,800
The pretrained image net model help
accelerate the development of computer

35
00:02:07,801 --> 00:02:12,801
vision algorithms a few years ago and
in 2018 fast that AI's Ulm fit the Allen

36
00:02:15,390 --> 00:02:18,870
Institute Elmo open AI's transformer
and Google's Bert did the same thing for

37
00:02:18,871 --> 00:02:23,070
NLP. These pretrained models
help accelerate development.

38
00:02:23,310 --> 00:02:28,310
Google's Bert stands for by Directional
encoder representation from transformers

39
00:02:28,650 --> 00:02:30,000
come down, optimist, not you.

40
00:02:30,390 --> 00:02:34,980
It was a model that was built on the
ideas of the other pretrained models and

41
00:02:34,981 --> 00:02:38,490
redefine the state of the art
for 11 different NLP tasks,

42
00:02:38,640 --> 00:02:40,500
even surpassing human performance.

43
00:02:40,680 --> 00:02:45,600
And the challenging area of question
answering it made learning from text data

44
00:02:45,601 --> 00:02:50,601
far easier for any developer since it
meant instead of lengthy training times

45
00:02:50,790 --> 00:02:54,270
only minimal changes needed to
be made to the existing model,

46
00:02:54,330 --> 00:02:57,630
which made training processes
more like fine tuning.

47
00:02:57,870 --> 00:03:01,180
The other area with lots of
activity was generative models,

48
00:03:01,330 --> 00:03:04,330
models that create novel
data like video or text,

49
00:03:04,540 --> 00:03:07,450
specifically around generative
adversarial networks.

50
00:03:07,660 --> 00:03:10,810
So many variations of gans were released,
big gans,

51
00:03:11,010 --> 00:03:15,850
aggressively growing gans and video even
used again to generate an entire three

52
00:03:15,851 --> 00:03:20,500
d world, which they demoed at
the most popular Ai Conference
of the year. No rips.

53
00:03:20,800 --> 00:03:23,560
Their paper was titled
Video to Video Synthesis.

54
00:03:23,860 --> 00:03:28,720
We also saw a massive growth in the size
of the Ai community across all metrics.

55
00:03:28,721 --> 00:03:32,980
In 2018 number of students
enrolled in AI at universities,

56
00:03:33,160 --> 00:03:37,060
new online offerings like school of Ai,
developer activity on get hub,

57
00:03:37,180 --> 00:03:40,060
conference attendance,
number of AI businesses.

58
00:03:40,150 --> 00:03:45,150
Ai means global interest in
this technology increased
massively and although

59
00:03:45,551 --> 00:03:49,780
tensorflow flow is still the most
popular machine learning library overall,

60
00:03:49,870 --> 00:03:53,680
academia started to prefer
the Pie Torch Library. Lastly,

61
00:03:53,681 --> 00:03:58,030
in 2018 we saw much more of an
intersection between governments and AI.

62
00:03:58,240 --> 00:04:01,660
Palentier helped city governments
implement predictive policing.

63
00:04:01,661 --> 00:04:06,550
Technology defined the likelihood that
certain individuals would commit a crime

64
00:04:06,551 --> 00:04:07,480
in the future.

65
00:04:07,810 --> 00:04:12,790
Tesla's autopilot crash caused lawmakers
to consider the implications of self

66
00:04:12,791 --> 00:04:17,560
driving cars more deeply. Google
employees protested against project Maven,

67
00:04:17,561 --> 00:04:22,540
which was aimed at using AI to help the
u s military improve its efficiency.

68
00:04:22,780 --> 00:04:27,370
Cambridge Analytica came under fire
after it was reported that Facebook gave

69
00:04:27,371 --> 00:04:31,090
them access to the private data
of more than 50 million users,

70
00:04:31,091 --> 00:04:33,370
which they used for political purposes.

71
00:04:33,430 --> 00:04:37,480
Zuckerberg had to testify in front of
Congress which showed how embarrassingly

72
00:04:37,481 --> 00:04:42,481
inept lawmakers were at understanding
this technology that EU enacted Gdpr,

73
00:04:43,061 --> 00:04:47,890
which required businesses to protect
the personal data and privacy of EU

74
00:04:47,891 --> 00:04:51,730
citizens. Many other countries
followed and China supercharged.

75
00:04:51,760 --> 00:04:56,230
It's economic growth by applying AI to,
well everything.

76
00:04:56,560 --> 00:05:01,560
2018 was quite a wild ride and I'm
expecting 2019 to be an even wilder one.

77
00:05:01,900 --> 00:05:05,800
My first prediction is that client side
training will become a linchpin of AI

78
00:05:05,801 --> 00:05:10,690
model learning within edge mobile and
robotic process automation applications.

79
00:05:10,960 --> 00:05:15,280
Several Ios apps have already started
using the device side AI training,

80
00:05:15,400 --> 00:05:19,570
ensuring that face id recognizes users
consistently at people's pictures or

81
00:05:19,571 --> 00:05:23,140
accurately grouped in the photos app
and that the keyboard can predict what

82
00:05:23,141 --> 00:05:23,974
you'll type.

83
00:05:24,100 --> 00:05:29,100
We'll see the pendulum in AI shift from
a focus on the cloud to the edge cost.

84
00:05:29,381 --> 00:05:31,750
Drivers for this trend
are firstly bandwidth,

85
00:05:31,751 --> 00:05:36,340
meaning semi connected environments and
expensive cellular considerations as

86
00:05:36,341 --> 00:05:40,210
well as storage, which reduces the
amount of data sent to the cloud.

87
00:05:40,720 --> 00:05:45,640
This means that the dominant
AI development frameworks
will be re-engineered for

88
00:05:45,641 --> 00:05:50,641
superior cloud to edge
performance optimizing its
runtime for whatever platform

89
00:05:51,251 --> 00:05:56,251
it's running on Intel's
popular deep learning USB
stick demonstrated one step in

90
00:05:56,561 --> 00:06:01,561
this and whether at the cloud or at
the edge Coobernetti's orchestrated

91
00:06:01,821 --> 00:06:05,360
containers will become an
integral part of the AI pipeline.

92
00:06:05,660 --> 00:06:09,380
Coobernetti's is an open source
project for automating deployment,

93
00:06:09,440 --> 00:06:13,010
scaling and management of
containerized applications.

94
00:06:13,310 --> 00:06:17,660
Containers gained traction in AI because
there are a lot more space efficient

95
00:06:17,870 --> 00:06:19,070
than virtual machines.

96
00:06:19,310 --> 00:06:23,540
In 2018 lots of AI tool vendors
started supporting the deployment of

97
00:06:23,541 --> 00:06:28,541
containerized statistical models inside
of cloud native computing environments.

98
00:06:29,270 --> 00:06:30,800
In 2019 even more,

99
00:06:30,801 --> 00:06:35,801
we'll support deployment of containerized
AI models for orchestration across

100
00:06:35,841 --> 00:06:39,770
Kubernetes clusters and increasingly
heterogeneous pipelines.

101
00:06:39,980 --> 00:06:44,270
Cuba flow by Google is one project that
does this well and it will be a likely

102
00:06:44,271 --> 00:06:48,980
target that tool vendors will implement
and as AI gets deployed everywhere,

103
00:06:49,010 --> 00:06:54,010
upskilling non AI professionals to work
with AI will become an even more crucial

104
00:06:54,111 --> 00:06:58,460
part of any workforce strategy.
Literally keep calm and learn AI.

105
00:06:58,700 --> 00:07:01,220
Thus a new class of tools like auto ml,

106
00:07:01,221 --> 00:07:05,750
which streamlines and automates part of
the process for creating AI models will

107
00:07:05,751 --> 00:07:08,630
democratize AI even
further for these people.

108
00:07:08,690 --> 00:07:13,690
In 2019 auto ml will improve such
that any supervised learning task,

109
00:07:13,970 --> 00:07:16,280
meaning one where a label is involved,

110
00:07:16,340 --> 00:07:20,990
we'll be able to have algorithmic
selection and hyper parameter optimization

111
00:07:20,991 --> 00:07:23,030
confidently automated by a computer.

112
00:07:23,300 --> 00:07:28,250
It won't be a replacement
for the ML toolbox. Just
another tool to include in it.

113
00:07:28,550 --> 00:07:32,180
Supervised learning is currently
the dominant approach and AI,

114
00:07:32,210 --> 00:07:37,210
but it often requires the labor intensive
and time consuming process of getting

115
00:07:37,221 --> 00:07:39,890
humans to manually annotate data.

116
00:07:40,070 --> 00:07:42,320
There are also limits on what it can do.

117
00:07:42,440 --> 00:07:46,070
We need a better way to bridge
the gap between representation,

118
00:07:46,071 --> 00:07:48,110
learning and causal reasoning.

119
00:07:48,290 --> 00:07:53,290
In 2019 research attention will turn
away from supervised learning and towards

120
00:07:54,380 --> 00:07:58,460
unsupervised and reinforcement learning.
For example,

121
00:07:58,461 --> 00:08:03,461
a recent paper titled Unsupervised
Neural Machine Translation showed that

122
00:08:03,530 --> 00:08:07,580
unsupervised learning can help
solve language translation.

123
00:08:07,670 --> 00:08:11,840
The problem with supervised translation
is that you need parallel sentences to

124
00:08:11,841 --> 00:08:16,841
show an algorithm to directly not words
from a source text and say English into

125
00:08:16,940 --> 00:08:18,710
a target.
Texts like Hindi.

126
00:08:18,980 --> 00:08:23,390
The paper showed that by training a joint
embedding space between an Incode or

127
00:08:23,391 --> 00:08:25,400
model and a decoder model,

128
00:08:25,490 --> 00:08:28,910
a weak form of unsupervised
translation could be learned.

129
00:08:29,180 --> 00:08:33,500
This was an inspiring step in the right
direction and we'll see more of that

130
00:08:33,501 --> 00:08:34,334
this year.

131
00:08:34,610 --> 00:08:39,440
We'll also see more of a popularization
of reinforcement learning techniques

132
00:08:39,560 --> 00:08:41,900
until coach ray RL Google.

133
00:08:41,901 --> 00:08:46,901
Dopamine will start to see more of these
are l libraries being incorporated into

134
00:08:47,211 --> 00:08:48,800
enterprise environments.

135
00:08:48,950 --> 00:08:53,950
We'll see more on the use of imitation
learning where agents learn directly from

136
00:08:54,261 --> 00:08:59,250
the human supervisor and we'll also see
evolutionary strategies in use to solve

137
00:08:59,251 --> 00:09:04,251
challenges faced in our l expect much
more data efficient RL algorithms.

138
00:09:04,891 --> 00:09:09,300
Since all of us don't have hundreds of
GPS like deep mind does except for this

139
00:09:09,301 --> 00:09:11,280
guy training alpha go on,

140
00:09:11,281 --> 00:09:16,281
a single GPU will likely become a
reality in 2018 the deep fakes algorithm

141
00:09:16,620 --> 00:09:20,430
awakened the public to just how
powerful generative models could be.

142
00:09:20,580 --> 00:09:24,510
Even the Hollywood reporter reached out
to interview me about the potentials

143
00:09:24,540 --> 00:09:25,410
last year.

144
00:09:25,500 --> 00:09:30,000
This technology will mature in 2019
and we'll likely see the first major

145
00:09:30,001 --> 00:09:32,460
political scandal that uses it occur.

146
00:09:32,610 --> 00:09:37,610
We'll also see a company use a generative
model to automatically create digital

147
00:09:37,711 --> 00:09:42,711
walkthroughs for how Spiers museum
tours or even generate content for video

148
00:09:43,081 --> 00:09:46,290
games. Basically, the tools
for generative modeling,

149
00:09:46,291 --> 00:09:51,291
we'll continue to improve and all of these
incredible learning algorithms can be

150
00:09:52,081 --> 00:09:54,510
sped up by certain types of hardware.

151
00:09:54,810 --> 00:09:57,990
Quantum processors are one
great candidate for this.

152
00:09:58,020 --> 00:10:00,540
D-Wave released it's quantum API.

153
00:10:00,541 --> 00:10:05,541
Ibm and Righetti invited more developers
to use their quantum API and we'll

154
00:10:05,641 --> 00:10:10,641
start seeing at least a few examples of
enterprise applications using quantum

155
00:10:10,741 --> 00:10:14,220
computing to speed up their
machine learning algorithms.

156
00:10:14,250 --> 00:10:19,050
There's a lot of promise around using
quantum chips to speed up training time

157
00:10:19,200 --> 00:10:24,200
for certain ml algorithms like variational
auto encoders and quantum won't be

158
00:10:24,541 --> 00:10:27,330
the only hardware we'll see progress on.

159
00:10:27,600 --> 00:10:32,370
All sorts of AI systems on chips will
dominate the hardware accelerator wares.

160
00:10:32,610 --> 00:10:37,240
In 2019 we'll see a steady stream of
fresh hardware innovations in gps,

161
00:10:37,470 --> 00:10:41,970
tensor core processing units, field
programmable gate arrays, and so on.

162
00:10:42,150 --> 00:10:45,510
They'll come to market and
support faster and more efficient.

163
00:10:45,540 --> 00:10:50,540
Ai Processing and companies
will increasingly uses
technology for social good

164
00:10:51,240 --> 00:10:55,800
after the dark cloud that
Cambridge Analytica casts
on the community last year,

165
00:10:55,980 --> 00:10:59,760
society is demanding that companies
have a higher social purpose.

166
00:11:00,030 --> 00:11:05,010
In 2019 we will see an
increase in initiatives like
Google's Ai for social good

167
00:11:05,011 --> 00:11:09,600
program and Microsoft's Ai
for good initiative companies
will have to adapt not

168
00:11:09,601 --> 00:11:12,270
just morally but their
business models as well.

169
00:11:12,300 --> 00:11:16,500
They'll either become more data driven
in their decision making or not and be

170
00:11:16,501 --> 00:11:19,110
disrupted by those that do.
Lastly,

171
00:11:19,140 --> 00:11:22,950
Ai will create more jobs than
it takes according to gardener.

172
00:11:22,951 --> 00:11:27,951
At 1.8 million jobs will
be lost to automation with
manufacturing in particular

173
00:11:28,111 --> 00:11:32,760
singled out, but 2.3 million will
be created primarily in education,

174
00:11:32,940 --> 00:11:34,560
healthcare,
and the public sector.

175
00:11:34,830 --> 00:11:38,490
That totally makes sense as
the vast troves of health data,

176
00:11:38,640 --> 00:11:42,180
the need for personalized education
and various societal problems,

177
00:11:42,181 --> 00:11:44,190
all points in this direction.

178
00:11:44,310 --> 00:11:48,930
There is so much to be excited for
in 2019 three things to remember.

179
00:11:49,140 --> 00:11:54,140
Ai In 2018 was characterized by fine
tuning architectures in deep reinforcement

180
00:11:54,371 --> 00:11:58,120
learning, natural language
processing and generative modeling.

181
00:11:58,300 --> 00:12:03,300
Ai In 2019 will be characterized
by more client side training tools,

182
00:12:03,490 --> 00:12:07,000
advances in unsupervised learning,
reinforcement learning,

183
00:12:07,090 --> 00:12:09,760
and much better AI hardware.
Oh,

184
00:12:09,820 --> 00:12:14,020
and prepare for more data regulations
to be enacted across the globe.

185
00:12:14,230 --> 00:12:17,920
We're all waking up to the
power and potential of data.

186
00:12:18,220 --> 00:12:21,610
I and the rest of the school of
AI wish you a very happy new year.

187
00:12:21,611 --> 00:12:26,260
Wizards 2019 will be our time to shine.
What's one goal you have this year?

188
00:12:26,261 --> 00:12:29,560
Share it with me in the comment
section and please subscribe for more

189
00:12:29,561 --> 00:12:34,120
programming videos for now. I've got
a course plan, so thanks for watching.

