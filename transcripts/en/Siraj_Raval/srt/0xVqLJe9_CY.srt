1
00:00:00,090 --> 00:00:00,811
Hello world,

2
00:00:00,811 --> 00:00:05,100
it's Saroj and today we're going to
learn how to prepare a dataset before

3
00:00:05,101 --> 00:00:09,390
feeding it into a machine learning model
or example code will predict if someone

4
00:00:09,391 --> 00:00:13,500
is meditating or not by training
on a Dataset of brain scans.

5
00:00:13,710 --> 00:00:15,360
Data is raw information.

6
00:00:15,361 --> 00:00:19,380
It's a representation of both human
and machine observations of the world.

7
00:00:19,710 --> 00:00:23,400
Everything can be represented as
data, all science, art, literature,

8
00:00:23,580 --> 00:00:26,730
all of it can be represented as
ones and Zeros on a computer.

9
00:00:27,000 --> 00:00:30,540
When we enter a virtual world,
we are literally surrounded by data.

10
00:00:30,750 --> 00:00:34,500
Since it is the fundamental building
block of everything we see and when we

11
00:00:34,501 --> 00:00:38,370
observe something physical in real life,
it becomes data in our brain.

12
00:00:38,520 --> 00:00:42,960
Unless our universe is a simulation,
then everything you see is already data.

13
00:00:44,280 --> 00:00:48,210
If you don't work at a tech giant,
how are you supposed to get that data?

14
00:00:48,360 --> 00:00:51,000
That brings us to step
one in preparing data,

15
00:00:51,150 --> 00:00:56,150
deciding the right kind of data to use
that data set you use entirely depends on

16
00:00:56,671 --> 00:00:58,110
the problem you're trying to solve.

17
00:00:58,380 --> 00:01:02,340
If I want to build a chat bot that comes
up with new innovative product ideas,

18
00:01:02,610 --> 00:01:05,010
I'm not going to use a
Dataset of Tim Cook Dialogue.

19
00:01:05,310 --> 00:01:09,300
Data is a means to an end and the good
news is there is a public dataset for

20
00:01:09,301 --> 00:01:10,500
almost any topic.

21
00:01:10,501 --> 00:01:14,730
You can think about a couple of sites
I like to use to find cool datasets are

22
00:01:14,880 --> 00:01:18,930
Kaggle since I love the format of their
website and how they explain each of

23
00:01:18,931 --> 00:01:21,120
their datasets in detail.
Also,

24
00:01:21,121 --> 00:01:25,530
the datasets subreddit is great for
requesting datasets you want and there is

25
00:01:25,531 --> 00:01:29,580
this awesome list of public datasets in
the read me of this get hub repo that

26
00:01:29,581 --> 00:01:31,080
I'll link to in the description.

27
00:01:31,380 --> 00:01:34,230
Google's advanced search
feature is also super helpful.

28
00:01:34,410 --> 00:01:38,580
Usually combining a few keywords with
the word data or database is enough to

29
00:01:38,581 --> 00:01:40,830
find what we need and to make it easier.

30
00:01:40,831 --> 00:01:45,780
We can specify the type of file we want
like CSP and the type of domain like edu

31
00:01:45,781 --> 00:01:46,614
or Gov.

32
00:01:46,650 --> 00:01:51,180
Usually a website has an API that makes
it easier to get the data you need,

33
00:01:51,330 --> 00:01:52,051
but if it doesn't,

34
00:01:52,051 --> 00:01:56,520
you can use a library like beautiful
soup to take a raw html webpage and just

35
00:01:56,521 --> 00:02:00,380
scrape the data directly.
Diy data died. No.

36
00:02:00,720 --> 00:02:05,160
Once we decided the type of data we want,
our second step is to process it.

37
00:02:05,400 --> 00:02:09,600
We're going to write a function to
extract data from a brain scan data set.

38
00:02:09,750 --> 00:02:11,940
Then we can feed that
data into a single layer.

39
00:02:11,941 --> 00:02:14,760
Neural network created intenser flow,
the network.

40
00:02:14,761 --> 00:02:18,780
We'll create a separator line between
two classes so that given a new person's

41
00:02:18,781 --> 00:02:21,840
brain scan data, it can predict
if they are meditating or not.

42
00:02:22,020 --> 00:02:23,400
Let's take a look at this data.

43
00:02:23,430 --> 00:02:28,430
This is a list of neurological metrics
collected via an EEG device for a set of

44
00:02:28,501 --> 00:02:31,500
human volunteers.
There are two possible classes,

45
00:02:31,650 --> 00:02:36,650
either meditating represented by a one
or not meditating represented by a zero,

46
00:02:36,900 --> 00:02:40,500
and there are three features for
this data. A measure of mental focus,

47
00:02:40,770 --> 00:02:45,750
a measure of calmness and the volunteers
age want to format our data properly.

48
00:02:45,930 --> 00:02:49,980
Data could come in the form of a text
file or a relational database or like,

49
00:02:49,981 --> 00:02:54,450
well, we have a CSV and there's a library
to convert pretty much any file type to

50
00:02:54,451 --> 00:02:56,580
another.
So make sure you have your data,

51
00:02:56,581 --> 00:02:59,530
format it to a file type that
you most feel comfortable with.

52
00:02:59,860 --> 00:03:02,890
Once it's in the right format,
we'll want to clean the data.

53
00:03:02,950 --> 00:03:06,370
Sometimes we have instances in
our data that are incomplete.

54
00:03:06,640 --> 00:03:10,630
We can iterate through each row and
delete those instances by checking if the

55
00:03:10,631 --> 00:03:14,920
value is empty or not. We should
also decide what features to use.

56
00:03:15,160 --> 00:03:19,330
Deciding what features are important is
one of the key parts of data science.

57
00:03:19,540 --> 00:03:23,440
If we don't use the right features,
our model will make bad predictions.

58
00:03:23,680 --> 00:03:26,980
We only want to use features
that are relevant to our problem.

59
00:03:26,980 --> 00:03:30,400
Their gender has nothing to do
with their meditative state,

60
00:03:30,520 --> 00:03:34,840
so we can totally disregard that feature.
So let's first create two arrays.

61
00:03:35,020 --> 00:03:39,010
One Array, we'll hold our class labels.
The other array, we'll hold our features.

62
00:03:39,190 --> 00:03:42,580
We can iterate through every line in
our CSV file. Using this for loop.

63
00:03:42,790 --> 00:03:46,090
We'll define a row which is a single
instance as an array of values.

64
00:03:46,091 --> 00:03:49,000
By splitting the line by the
comma separator. Using this row,

65
00:03:49,001 --> 00:03:52,930
we can first get the associated class
label by retrieving the first value in our

66
00:03:52,931 --> 00:03:56,590
row array, converting it to an end
than adding it to our labels array.

67
00:03:56,800 --> 00:03:59,620
Now we can do the same thing
for our features are ray.

68
00:03:59,770 --> 00:04:01,930
We'll take each feature
and convert it to a float.

69
00:04:01,931 --> 00:04:03,700
Since we want precision in our values,

70
00:04:03,880 --> 00:04:06,280
our feature array is
now an array of arrays.

71
00:04:06,580 --> 00:04:09,640
Now that we've pulled our data
from our dataset file into memory,

72
00:04:09,820 --> 00:04:12,940
we've arrived at the last
step transforming the data.

73
00:04:14,410 --> 00:04:17,470
One possible transformation
is decomposition.

74
00:04:17,740 --> 00:04:21,190
Sometimes we have features that
are too complex like the date.

75
00:04:21,520 --> 00:04:25,540
If we're trying to predict which day in
October is most likely to get rainfall

76
00:04:25,541 --> 00:04:28,270
this year, we don't really
need the month and the year.

77
00:04:28,450 --> 00:04:31,540
If we decompose that feature
into just the day of the month,

78
00:04:31,840 --> 00:04:33,430
that'll make our model more accurate.

79
00:04:33,610 --> 00:04:36,040
Since we're satisfied with the
features we have in their class,

80
00:04:36,041 --> 00:04:37,330
labels will perform.

81
00:04:37,331 --> 00:04:41,860
The only transformation we need
will transform them into vectors.

82
00:04:41,920 --> 00:04:45,490
Vectors are numerical
representations of features.

83
00:04:45,880 --> 00:04:50,620
All features can be represented
as vectors, words, images, videos,

84
00:04:50,710 --> 00:04:55,210
all of it. We can take these vectors and
feed them into our neural net directly.

85
00:04:55,300 --> 00:04:59,920
We'll convert our array of arrays into
a two d matrix using numb Pi's matrix

86
00:04:59,921 --> 00:05:04,180
function and set the type to float.
This is a matrix of feature vectors.

87
00:05:04,390 --> 00:05:07,330
Each vector contains a list of features.
For an instance,

88
00:05:07,540 --> 00:05:11,800
we'll also want to transform our
class label array into a num py array.

89
00:05:11,920 --> 00:05:16,660
Because a number [inaudible] can easily
be converted into a one hot matrix.

90
00:05:17,050 --> 00:05:21,550
Then we'll return are fully processed
feature matrix and one hot label matrix.

91
00:05:21,640 --> 00:05:25,630
So what is one hot encoding yo DJ dropout.
Bump it.

92
00:05:25,720 --> 00:05:29,200
How was Portland? So I made it. No,
right. I'll put words in like cat, dog,

93
00:05:29,201 --> 00:05:33,850
duck in the lake. If I encode them as
numbers, it's a o k one, two, three, four,

94
00:05:33,851 --> 00:05:37,930
all on display. But if four is more
than one, is Palais more than cat? No,

95
00:05:38,200 --> 00:05:42,430
but I'm not gonna leave it at that because
I'm not trying to rank. Differentiate.

96
00:05:42,510 --> 00:05:44,950
Yeah,
differentiate.

97
00:05:44,980 --> 00:05:48,820
So I'll make each of vector of ones
and those different but not right.

98
00:05:48,850 --> 00:05:53,720
That's just how with go three, no two, no
one hot and code. Yeah. Once we have our

99
00:05:53,790 --> 00:05:57,910
data processed, we'll want to feed
it into our graph. In tensorflow,

100
00:05:57,980 --> 00:06:02,930
the placeholder object is considered the
gateway for data into our computation

101
00:06:02,931 --> 00:06:05,960
graph, so we'll want to
initialize two placeholders,

102
00:06:06,230 --> 00:06:10,790
one for our class labels and one for
our associate in feature vectors.

103
00:06:11,000 --> 00:06:14,450
And when we finally run our training
step using the run function,

104
00:06:14,690 --> 00:06:18,530
we can feed our data into the graph
using the feed dictionary parameter.

105
00:06:18,890 --> 00:06:22,670
The labeled placeholder gets the labels
and the feature place holder gets the

106
00:06:22,671 --> 00:06:24,380
features.
When we run our model,

107
00:06:24,381 --> 00:06:28,580
it'll show the classification line that
it created two separate the meditating

108
00:06:28,581 --> 00:06:32,810
from the non meditating people and
if we feed it a new instance, boom,

109
00:06:32,840 --> 00:06:36,110
it'll classify it just like that.
So to break it down,

110
00:06:36,200 --> 00:06:39,770
the steps to prepare a dataset
are selecting the right data,

111
00:06:40,100 --> 00:06:42,230
processing it and transforming it.

112
00:06:42,560 --> 00:06:45,140
You can easily find public
datasets on the web.

113
00:06:45,320 --> 00:06:49,550
The a number of sources that I'll link
to in the description or you say web

114
00:06:49,551 --> 00:06:52,610
scraping tool, like beautiful
soup to create them yourself.

115
00:06:52,910 --> 00:06:56,330
And once we have our data,
we'll convert them into vectors,

116
00:06:56,331 --> 00:06:59,840
which are numerical representations
that are machine learning model can

117
00:06:59,841 --> 00:07:00,674
understand.

118
00:07:00,740 --> 00:07:04,850
The winner of the make a game bought
challenge is Carl big highs and he created

119
00:07:04,851 --> 00:07:08,600
a bot that used deep cue learning.
His Bot fed the pixel data,

120
00:07:08,601 --> 00:07:12,950
it received to a convolutional neural
net that updated its policy over time

121
00:07:13,040 --> 00:07:14,510
through trial and error.
Also,

122
00:07:14,511 --> 00:07:17,630
this was his first get hub
repo that ass of the week,

123
00:07:17,690 --> 00:07:21,860
and the runner up is Roohan Vermont.
While my Demo Bot couldn't finish a lap,

124
00:07:22,040 --> 00:07:24,200
his could in just two and a half minutes.

125
00:07:24,470 --> 00:07:28,370
The coding challenge for this video
is to write a script that classifies a

126
00:07:28,371 --> 00:07:32,840
Pokemon by their element using a Dataset
that all provide post your get hub,

127
00:07:32,841 --> 00:07:35,960
humbling in the comments, and I'll
announce the winner in the next video.

128
00:07:36,170 --> 00:07:40,250
Please hit that subscribe button. And
for now I've got a decentralize the web,

129
00:07:40,490 --> 00:07:41,720
so thanks for watching.

