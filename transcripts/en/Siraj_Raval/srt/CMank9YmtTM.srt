1
00:00:00,380 --> 00:00:01,920
Too much.

2
00:00:07,430 --> 00:00:11,710
Hello world, it's a Raj and
welcome to this live stream. Uh,

3
00:00:11,810 --> 00:00:16,810
in this live stream I'm
going to be talking about a
node dot js and we're going

4
00:00:19,371 --> 00:00:21,950
to be using machine learning for that.
Uh,

5
00:00:21,980 --> 00:00:25,610
so the code is going to show
up behind me when I move it.

6
00:00:25,611 --> 00:00:28,380
So it's going to show up.
Just setting up some lives,

7
00:00:28,620 --> 00:00:32,030
a lot of things right now it looks like
the code is not behind. There we go. Okay,

8
00:00:32,031 --> 00:00:35,120
cool. Cool, cool, cool. Cool. All right,
so, okay, I'm so excited to be here.

9
00:00:35,300 --> 00:00:38,840
Thank you guys for coming. Uh, so
we have a lot to talk about. Today.

10
00:00:38,841 --> 00:00:43,841
We are going to build a node dot js app
from scratch and this app is going to be

11
00:00:45,771 --> 00:00:50,180
able to translate to in real
time. So check this out.
Here's the translation demo.

12
00:00:50,390 --> 00:00:53,210
Okay, let me make this a little
bigger. So this is in real time.

13
00:00:53,211 --> 00:00:57,590
This is what's happening.
Okay. I can say, how are you?

14
00:00:57,950 --> 00:01:01,250
And it's going to translate
it in real time into French.

15
00:01:01,251 --> 00:01:04,340
So English to French
translation in the browser.

16
00:01:04,490 --> 00:01:08,180
Immediately I can say hello world.

17
00:01:08,210 --> 00:01:12,470
It's Saroj. And then, uh,
it's going to say, you know,

18
00:01:12,471 --> 00:01:13,790
whatever it needs to say.
Okay?

19
00:01:13,791 --> 00:01:17,190
So that's the demo that we
are going to build today. Uh,

20
00:01:17,210 --> 00:01:20,960
this person is saying my left ear is
highly educated. Well, guess what?

21
00:01:20,961 --> 00:01:24,920
Your right ear is going to be as well
because there's no audio in the left ear.

22
00:01:24,921 --> 00:01:27,740
Okay? We're, we're gonna fix that. We're
going to fix that for you. Okay? So

23
00:01:28,290 --> 00:01:28,640
yeah,

24
00:01:28,640 --> 00:01:30,870
his day on, we're going to fix
that for you. In the meantime,

25
00:01:31,080 --> 00:01:33,150
let's get right into this. Okay? So, ah,

26
00:01:33,480 --> 00:01:36,570
what I want to do is just say five names
and then I'm going to start a coding

27
00:01:36,571 --> 00:01:39,930
this. Okay. So, uh, I'm unprepared, uh,

28
00:01:40,020 --> 00:01:44,730
silly Tian and Martin and Depok and uh,

29
00:01:45,520 --> 00:01:45,830
okay.

30
00:01:45,830 --> 00:01:48,500
Russia. Okay. And then the
question, I only see one question.

31
00:01:48,501 --> 00:01:51,800
Have you ever loved anybody? Yes. It's
called my community. It's called you guys.

32
00:01:51,801 --> 00:01:54,680
So that's why I do this thing.
Okay. So let's make everything big.

33
00:01:54,681 --> 00:01:55,970
And now we're going to look at this.

34
00:01:56,750 --> 00:02:00,920
So I'm going to talk a little bit
about what no job, no dot js is.

35
00:02:00,921 --> 00:02:04,880
And then we're going to talk about how
machine translation works using sequence

36
00:02:04,881 --> 00:02:09,140
to sequence models. And then we're going
to code this thing out in Java script.

37
00:02:09,170 --> 00:02:10,003
Okay?

38
00:02:11,540 --> 00:02:11,930
Okay.

39
00:02:11,930 --> 00:02:15,640
All right. So let's, let's get
right into this. So No. Dot js.

40
00:02:15,641 --> 00:02:19,010
So javascript was initially
invented just for,

41
00:02:19,450 --> 00:02:21,910
just for in the browser scripting.

42
00:02:21,911 --> 00:02:24,910
It wasn't made to create stand
alone applications, right?

43
00:02:24,911 --> 00:02:29,860
So you wouldn't build a model that
allows for requests to be coming in,

44
00:02:30,090 --> 00:02:31,150
uh,
in javascript.

45
00:02:31,151 --> 00:02:35,950
And javascript wasn't made initially to
handle a bunch of concurrent requests,

46
00:02:35,951 --> 00:02:39,190
you know, like for posts, requests
for, for, for data, et Cetera.

47
00:02:39,430 --> 00:02:42,730
But the developers of node
dot. Js said, you know what,

48
00:02:42,731 --> 00:02:45,820
let's create a javascript
runtime environment.

49
00:02:46,030 --> 00:02:50,440
That's an entire environment that you
can use javascript to create stand alone

50
00:02:50,441 --> 00:02:54,850
web apps. And since then it's kind of
exploded in popularity. Look, no. Dot.

51
00:02:54,851 --> 00:02:59,560
JS is not a new technology. It's a,
it's, it's, it's a relatively, um,

52
00:02:59,800 --> 00:03:02,920
you know, in, in, in programming
terms, it's an older technology.

53
00:03:02,921 --> 00:03:06,010
It's about five years or more
older, but it's a very, um,

54
00:03:06,070 --> 00:03:08,830
it's a very good technology
and we can use it.

55
00:03:08,890 --> 00:03:12,160
We can combine it with machine
learning for some amazing results.

56
00:03:12,280 --> 00:03:15,070
So if we look at the analogy of Java,

57
00:03:15,071 --> 00:03:18,740
so most of us are familiar with Java
because computer science one oh one class

58
00:03:18,741 --> 00:03:22,660
is a teach Java generally. And
if you're not, that's okay. Uh,

59
00:03:23,110 --> 00:03:27,340
but basically what I'm trying to show is
the analogy of what a node runs on and

60
00:03:27,341 --> 00:03:29,470
it runs on what's called
The v eight engine.

61
00:03:29,980 --> 00:03:34,540
And Va engine was created for c plus
plus specifically, but it, but yeah,

62
00:03:34,600 --> 00:03:36,400
you can use it with javascript now.

63
00:03:36,401 --> 00:03:41,401
And what it does is you type in javascript
in the browser and it compiles it

64
00:03:41,471 --> 00:03:46,090
down to very fast machine code that's
readable by your machine. Okay.

65
00:03:46,091 --> 00:03:50,830
So no longer is it? No
longer is javascript just
constrained to the browser.

66
00:03:50,980 --> 00:03:53,380
You can use it for all sorts of things.
Okay.

67
00:03:56,440 --> 00:04:00,820
Nice audio for you. Fix the audio. All
right, well it's fine now. Okay. So, uh,

68
00:04:01,570 --> 00:04:04,270
the other thing about
node dot js is so on.

69
00:04:04,300 --> 00:04:06,840
If we look at the official
node dot js website,

70
00:04:06,850 --> 00:04:11,690
let's look at the official node
dot. Js website or write node.

71
00:04:11,691 --> 00:04:11,891
Dot.

72
00:04:11,891 --> 00:04:16,891
JS is a javascript runtime engine built
on chrome's via javascript engine.

73
00:04:17,290 --> 00:04:20,710
And what it does is it creates an event.
And here's the key.

74
00:04:20,711 --> 00:04:21,544
Here's the key right here.

75
00:04:21,670 --> 00:04:25,750
It creates an event driven nonblocking
I io model that makes it lightweight,

76
00:04:25,790 --> 00:04:29,530
inefficient. Let's break this
down. I o means input output.

77
00:04:29,531 --> 00:04:33,070
So this can be anything from
reading files, riding local files,

78
00:04:33,071 --> 00:04:35,590
making an http request,
right?

79
00:04:35,650 --> 00:04:38,530
Anything that you were inputting into a
model and then you're getting an output

80
00:04:38,531 --> 00:04:40,690
that's an input output model.
But that,

81
00:04:40,750 --> 00:04:44,590
but the problem with input output
models is the idea of blocking, right?

82
00:04:44,590 --> 00:04:48,910
So blocking happens when you make a
request and there's another request.

83
00:04:48,970 --> 00:04:49,510
But wait,

84
00:04:49,510 --> 00:04:53,470
this request is not finished yet and now
it has to wait for the other request,

85
00:04:53,471 --> 00:04:57,310
right? So it's blocked. Request
number two is blocked from,

86
00:04:57,340 --> 00:05:02,140
from modifying, uh, the, the,
the APP because requests,

87
00:05:02,141 --> 00:05:03,160
one is in process.

88
00:05:03,370 --> 00:05:07,420
So what javascripts node dot js does is
it has this thing called the event loop.

89
00:05:07,450 --> 00:05:09,640
Okay. And this is what the
event, Luke looks like.

90
00:05:10,090 --> 00:05:14,560
The event loop consists of three different
modules. You have your call stack,

91
00:05:14,830 --> 00:05:17,740
you have the node Api Apis and
you have your callback queue

92
00:05:19,630 --> 00:05:23,410
and great two years for the audio.
Awesome. And also attention mechanisms.

93
00:05:23,411 --> 00:05:25,600
I know you guys want attention mechanisms.

94
00:05:25,900 --> 00:05:29,860
I need to make a dedicated video
about attention that's coming.

95
00:05:30,100 --> 00:05:33,430
You can't just be like, oh this is a
tension but and then that's it. No,

96
00:05:33,610 --> 00:05:37,240
attention is a is a
highly interesting topic.

97
00:05:37,510 --> 00:05:41,140
It's an advanced topic and we will get
into that. I know you guys want it.

98
00:05:41,160 --> 00:05:43,720
I want it to you know how much
I like attention mechanisms,

99
00:05:43,870 --> 00:05:48,070
but basically the idea of attention
mechanisms is normally in a neural network

100
00:05:48,071 --> 00:05:50,410
you have a bunch of things to choose from,
right?

101
00:05:50,830 --> 00:05:54,880
The network has a bunch of options
on what to make a prediction width,

102
00:05:55,180 --> 00:05:56,920
different submodules of data.

103
00:05:57,170 --> 00:06:01,310
And what attention does is it says here
is the most important part of that data

104
00:06:01,311 --> 00:06:05,030
to pay attention to and here's what
to make a prediction with. And it,

105
00:06:05,031 --> 00:06:07,370
what it comes down to
is probabilities rights.

106
00:06:07,520 --> 00:06:11,930
What's the percent likelihood that this
will result in an output that is most

107
00:06:11,931 --> 00:06:16,670
aligned with the label that I have.
And there's different methods for this.

108
00:06:16,671 --> 00:06:19,760
There's a whole field of study
dedicated to attention mechanisms.

109
00:06:19,880 --> 00:06:22,280
You could dedicate a two,
three years to this. Okay,

110
00:06:22,281 --> 00:06:27,200
so how does machine translation work?
And that was, that's a no dot js. Well,

111
00:06:27,230 --> 00:06:30,110
before I get some machine translation,
let me just finish explaining no.

112
00:06:30,111 --> 00:06:34,760
Dot js three modules here, the call
stack note APIs and the callback queue.

113
00:06:35,060 --> 00:06:39,890
Okay. This is, this is very,
you know, trivial stuff. And
so here's an example, right?

114
00:06:39,891 --> 00:06:42,980
So what happens is for these three
functions, well two functions,

115
00:06:43,190 --> 00:06:46,010
what happens is they are first,
when you execute this code,

116
00:06:46,470 --> 00:06:48,800
all of those functions
go onto the call stack.

117
00:06:48,950 --> 00:06:51,740
The call sec then calls the node Api Apis,
right?

118
00:06:51,741 --> 00:06:56,060
Set Timeout as a part of the node API.
And then both are both are place in the,

119
00:06:56,070 --> 00:06:57,320
into the callback queue.

120
00:06:57,530 --> 00:07:02,030
The callback queue is waiting for the call
stack to be empty and once it's empty,

121
00:07:02,031 --> 00:07:06,260
it's going to send one by one, uh,
different functions to that call stack.

122
00:07:06,410 --> 00:07:10,250
And then once and any new functions first,
enter the call stack,

123
00:07:10,370 --> 00:07:11,630
call the node Api Apis,

124
00:07:11,750 --> 00:07:15,380
then go to the callback queue and then
they're executed and the call stack again.

125
00:07:15,440 --> 00:07:16,820
So it's kind of this loop,
right?

126
00:07:16,821 --> 00:07:20,510
And so what happens is because
there's this process that's happening,

127
00:07:20,600 --> 00:07:24,320
there's no blocking. And so what that
means is you can make concurrent requests,

128
00:07:24,470 --> 00:07:27,380
not just one,
but a thousand requests at the same time.

129
00:07:27,590 --> 00:07:29,600
And because there's
this loop functionality,

130
00:07:29,720 --> 00:07:33,020
you don't have to wait for one request
to finish before starting the other one.

131
00:07:33,350 --> 00:07:37,790
Now onto machine translation here is our
input data and our output data. Okay?

132
00:07:37,791 --> 00:07:41,480
So what this is, let's look at
our data set. By the way. Uh,

133
00:07:41,540 --> 00:07:44,660
our data set is an input output dataset.
Uh,

134
00:07:45,950 --> 00:07:50,600
example LSTM sequence to
sequence a French data.

135
00:07:51,110 --> 00:07:53,330
This is a very popular dataset

136
00:07:55,880 --> 00:07:59,880
and there's a great tutorial for this
on the care Ross blog as well. Uh,

137
00:07:59,940 --> 00:08:04,790
minds is going to be more interactive,
but let's see where the input data is.

138
00:08:04,820 --> 00:08:07,880
Here it is. Okay. So that's
really what I want to.

139
00:08:07,880 --> 00:08:09,980
And plus ours is going to be
in Javascript, not in python.

140
00:08:10,340 --> 00:08:13,040
So this is our input data.
Okay. And they, this,

141
00:08:13,041 --> 00:08:17,930
this website has a bunch of great
data sets for every language.

142
00:08:17,931 --> 00:08:21,870
So a bunch of phrases like hello,
how are you with the associated, uh,

143
00:08:21,910 --> 00:08:25,700
other language that has translated to,
I'm also going to be taking questions.

144
00:08:25,730 --> 00:08:28,250
So in five minutes from now I'm
going to be taking questions,

145
00:08:28,400 --> 00:08:31,700
so be sure to ask those questions
before I start coding. Okay.

146
00:08:31,701 --> 00:08:35,600
So what we're going to do is we're going
to translate English to French. Okay.

147
00:08:35,601 --> 00:08:37,880
So we downloaded an English French pair.

148
00:08:37,881 --> 00:08:40,610
So the input looks like
this left column right here.

149
00:08:40,910 --> 00:08:43,190
The output looks like this
right column right here.

150
00:08:43,340 --> 00:08:47,600
And what we want to do is
learn that napping. Exactly.

151
00:08:47,601 --> 00:08:49,220
We want to learn the
mapping between the two.

152
00:08:49,610 --> 00:08:53,360
And so the first step for us
is to preprocess this data.

153
00:08:53,361 --> 00:08:55,470
So what's it's going to look like is this,

154
00:08:55,530 --> 00:08:59,460
it's going to be a set of word vectors
and here's what the first word vector

155
00:08:59,461 --> 00:09:04,200
will look like. Go becomes
g o period. That's it.

156
00:09:04,410 --> 00:09:08,430
And this is going to be an array where
every element in the array is a different

157
00:09:08,431 --> 00:09:12,150
character. That's a part of that
phrase. And the output will be the same.

158
00:09:17,960 --> 00:09:22,310
Great. Okay, so now, uh, for the
encoder decoder architecture,

159
00:09:22,311 --> 00:09:25,820
here's how it works. We have
to recurrent networks. Okay.

160
00:09:25,821 --> 00:09:28,730
We have a one recurrent network,
that's the, we call the end coder.

161
00:09:28,910 --> 00:09:32,720
We have another recurrent neural network
neural network called the decoder.

162
00:09:32,721 --> 00:09:36,650
So there's two neural networks here.
One acts as what's called the encoder.

163
00:09:36,651 --> 00:09:38,000
And the other,
as a decoder,

164
00:09:38,390 --> 00:09:42,680
the encoder is responsible for outputting
a fixed length in coding of the input

165
00:09:42,681 --> 00:09:43,760
English sentence.

166
00:09:44,990 --> 00:09:49,190
So recurrent networks are a type
of neural network that are fed,

167
00:09:49,191 --> 00:09:52,550
not just the new, a new data point
in the next time step, right?

168
00:09:52,551 --> 00:09:56,090
So feedforward networks are fed new
data points every time step, right?

169
00:09:56,091 --> 00:09:58,370
So here's one data point.
Here's the next day to point.

170
00:09:58,430 --> 00:09:59,360
Here's the next data point.

171
00:09:59,390 --> 00:10:03,950
And every time step that that hidden
weight matrix that that is the neural

172
00:10:03,951 --> 00:10:07,520
networks brain, essentially, it's
all of its learnings over time.

173
00:10:07,640 --> 00:10:09,140
It's a collection of numbers,
right?

174
00:10:09,141 --> 00:10:13,460
Ones and Zeros that gets better and better
through the optimization process for

175
00:10:13,461 --> 00:10:17,450
normal feed forward neural
networks. You're just feeding
in input data and that,

176
00:10:17,660 --> 00:10:21,770
uh, that, that, that weight matrix
for the network is kind of like Plato.

177
00:10:21,800 --> 00:10:26,150
It's been cool. It's been molded over
time to be more and more optimized, right?

178
00:10:26,151 --> 00:10:29,420
So it's be, it's becoming better and
better. So when you feed it a new output,

179
00:10:29,570 --> 00:10:33,020
it's going to give you the perfect,
when you painted a new input,

180
00:10:33,140 --> 00:10:34,700
it's going to give you the perfect output.

181
00:10:35,960 --> 00:10:38,930
But recurrent networks are not
just vetting new input data.

182
00:10:38,931 --> 00:10:43,700
So it's not just new data points. It's
also the, it's also the previous, um,

183
00:10:43,790 --> 00:10:47,750
hidden state. So it's the previous
version of that Plato Weight Matrix.

184
00:10:47,900 --> 00:10:49,310
So it's being fed both of those.

185
00:10:49,490 --> 00:10:52,460
And the reason for that is because we're
current networks are made for learning

186
00:10:52,461 --> 00:10:56,180
sequences. Now it does very well
with learning sequences, right?

187
00:10:56,181 --> 00:10:59,750
So predicting the next character,
the next number, the next image,

188
00:10:59,751 --> 00:11:02,510
the next frame in a sequence of images,
video,

189
00:11:02,900 --> 00:11:06,620
it does very well at predicting sequences.
The problem is that there is a,

190
00:11:06,710 --> 00:11:10,430
there is a problem with learning
long term memory, right?

191
00:11:10,431 --> 00:11:13,280
So if forgets what happens in the past,

192
00:11:13,460 --> 00:11:16,100
now I have some great
videos on LSTM networks.

193
00:11:16,220 --> 00:11:19,280
This problem is called the vanishing
gradient problem and we can talk about it

194
00:11:19,281 --> 00:11:24,080
forever, but basically Tldr, just
search LSTM Saroj for videos on that.

195
00:11:24,410 --> 00:11:25,260
But,
uh,

196
00:11:26,210 --> 00:11:30,260
the idea is that the gradient vanishes
slowly as we back propagate into previous

197
00:11:30,261 --> 00:11:34,450
layers. And so there's a version
of the recurrent network. Um,

198
00:11:37,590 --> 00:11:38,423
okay.

199
00:11:39,300 --> 00:11:43,980
The version of the recurrent network
is called a n l s t m network. Okay.

200
00:11:43,981 --> 00:11:46,890
So a long short term memory network.
And here's what it looks like.

201
00:11:46,891 --> 00:11:50,520
And let's talk about this for a second
because it's very interesting in,

202
00:11:50,670 --> 00:11:52,560
in addition to having a hidden state,

203
00:11:52,620 --> 00:11:57,520
this LSTM recurrent network has a cell
state. And here's what a cell looks like.

204
00:11:57,521 --> 00:12:01,090
Now, remember these words,
gates cells. These are just,

205
00:12:01,390 --> 00:12:04,570
these are just ways for us to
understand what this actually is.

206
00:12:04,720 --> 00:12:05,950
And what this actually is,

207
00:12:05,951 --> 00:12:09,520
is a series of operations that
we are applying to input data.

208
00:12:09,820 --> 00:12:11,900
And so we have what's
called an input gate. Uh,

209
00:12:11,910 --> 00:12:15,880
forget gate and I'm an
output gate and a cell state.

210
00:12:15,910 --> 00:12:18,940
We have four different things to be
thinking about here when we're thinking

211
00:12:18,941 --> 00:12:20,410
about an LSTM cell.

212
00:12:21,520 --> 00:12:25,900
And the whole idea behind this is to
trap that gradient over the long term.

213
00:12:26,080 --> 00:12:27,430
And this kind of,
this,

214
00:12:27,580 --> 00:12:32,580
this scheme has resulted in us being
able to trap that gradient in an earlier,

215
00:12:32,740 --> 00:12:36,430
uh, layers of the network such that
it can remember longterm sequences.

216
00:12:36,610 --> 00:12:40,270
So if you look at these gating
variables here, these three equations,

217
00:12:44,980 --> 00:12:46,750
if we look at these three equations,

218
00:12:47,580 --> 00:12:48,130
yeah.

219
00:12:48,130 --> 00:12:49,950
All right. And I'm going to
answer questions right after this.

220
00:12:50,190 --> 00:12:52,320
Let me just finish this.
If we look at these three equations,

221
00:12:52,321 --> 00:12:54,270
we have an equation for the forget gate,

222
00:12:54,271 --> 00:12:59,271
the input gate and the output Kate and
its input times weight add a bias and

223
00:12:59,671 --> 00:13:04,620
then the parentheses activate. Remember,
input times weight, add a bias,

224
00:13:04,920 --> 00:13:09,780
activate. These gates are just
single layer neural networks, right?

225
00:13:09,781 --> 00:13:12,150
They are perceptrons where
we're taking the input,

226
00:13:12,151 --> 00:13:15,450
multiplying it by its own
respective weight Matrix,

227
00:13:15,570 --> 00:13:17,790
adding a bias value
and taking that output.

228
00:13:18,030 --> 00:13:21,900
And what we do with all three of these
gates is we compute the cell state.

229
00:13:21,901 --> 00:13:26,760
So look at this equation right here,
okay? What this equation is, is a, it is,

230
00:13:26,850 --> 00:13:31,470
uh, when we are taking all of those gates
and we are, we're saying we're taking,

231
00:13:31,471 --> 00:13:32,520
going to take the,
um,

232
00:13:33,150 --> 00:13:33,640
okay,

233
00:13:33,640 --> 00:13:35,340
the first equation,

234
00:13:35,400 --> 00:13:39,430
multiply it by this cell state from the
previous time step plus the implicate

235
00:13:39,460 --> 00:13:41,680
times his cell state.
And that gives us our cell state.

236
00:13:41,980 --> 00:13:46,060
And then we take our output times tan
h the activation of the cell state.

237
00:13:46,150 --> 00:13:49,390
And that gives us our hidden state.
So these two variables right here,

238
00:13:49,391 --> 00:13:51,580
and this is our self safe
from the last time step.

239
00:13:51,820 --> 00:13:54,640
These two variables right
here are what we care about.

240
00:13:54,641 --> 00:13:58,450
And that's what the LSTM computes.
So when it comes to training,

241
00:13:58,480 --> 00:14:01,510
here's how it works, okay?
So here is how it works.

242
00:14:04,390 --> 00:14:07,010
Let me just say how it works and then
we're, we're, we're good with this. Okay?

243
00:14:07,011 --> 00:14:10,060
So what's happening is we take,
okay,

244
00:14:10,280 --> 00:14:13,970
so during the training phase,
we take our input sentence.

245
00:14:13,971 --> 00:14:15,980
This is an English sentence,
okay?

246
00:14:16,070 --> 00:14:19,790
We take that English sentence
and we feed it into the encoder.

247
00:14:19,791 --> 00:14:22,310
So this is a vectorized version
of the English sentence.

248
00:14:22,520 --> 00:14:27,140
We feed it to the first encoder,
the encoder, that's an LSTM network.

249
00:14:27,350 --> 00:14:29,930
Now,
what we care about is not the output.

250
00:14:29,960 --> 00:14:34,190
We care about the learn hidden state from
the encoder and what that hidden state

251
00:14:34,191 --> 00:14:37,940
is, we can call it a lot of things. We
could call it a a Plato weight Matrix.

252
00:14:38,120 --> 00:14:40,880
We can call it a, uh, we can call it a

253
00:14:41,800 --> 00:14:42,470
okay,

254
00:14:42,470 --> 00:14:46,550
a hidden state. We can call it, uh, uh, a
thought vector. We can call it a learning.

255
00:14:46,551 --> 00:14:51,190
We can call it, uh, you know, it's what
it's learned. It's a dense representation.

256
00:14:51,200 --> 00:14:54,680
It's a compressed version of
everything that it's learned over time,

257
00:14:54,830 --> 00:14:58,730
a generalized version. And we use
that learned hidden state from the,

258
00:14:58,790 --> 00:14:59,623
from the fruit,

259
00:14:59,680 --> 00:15:04,340
from each iteration of the training
loop to initialize the decoder.

260
00:15:04,460 --> 00:15:05,690
So that initial lot,

261
00:15:05,750 --> 00:15:09,860
so that hidden state that's learned from
the encoder is used to initialize the

262
00:15:09,861 --> 00:15:13,550
decoder. What the decoder is
fed is the French sentence.

263
00:15:13,551 --> 00:15:16,670
So the English sentence, check this
out. You got to listen to this.

264
00:15:16,910 --> 00:15:19,640
The English sentence is
given to the encoder,

265
00:15:19,940 --> 00:15:23,750
the encoder output on a hidden state,
it will have an output.

266
00:15:23,751 --> 00:15:26,420
But we don't care about that.
We only care about the hidden state.

267
00:15:26,421 --> 00:15:30,810
Then it's learned. We feed the
hidden state to the decoder now too,

268
00:15:30,870 --> 00:15:35,300
to initialize it and we feed the decoder
the French sentence. So what happened?

269
00:15:35,301 --> 00:15:40,301
What's going to happen is the decoder is
going to translate the French sentence

270
00:15:41,210 --> 00:15:44,750
using the learned hidden, say from
the encoder character by character.

271
00:15:44,900 --> 00:15:49,190
It's going to predict every next
character given that input. Now, uh,

272
00:15:49,910 --> 00:15:53,870
it's gonna output something that looks
very bad at first, but we have the label,

273
00:15:53,871 --> 00:15:58,670
which is the actual French phrase.
We'll compare it via some lost function.

274
00:15:58,850 --> 00:16:02,810
We'll use that to back, propagate
the gradient across both networks.

275
00:16:02,930 --> 00:16:06,230
And we repeat until eventually
the output is going to be very,

276
00:16:06,231 --> 00:16:09,890
very similar to the input, which is our
translated phrase. That's for training.

277
00:16:09,891 --> 00:16:13,280
And then lastly, for inference, we're
going to encode the input sequence,

278
00:16:13,430 --> 00:16:16,070
retrieved that state,
use it for the decoder,

279
00:16:16,071 --> 00:16:20,480
and then we don't give it a French
phrase. We just immediately, uh,

280
00:16:20,810 --> 00:16:24,470
output. We predict every next character
cause we don't have a label. Okay.

281
00:16:24,470 --> 00:16:27,680
So now for questions, five questions
and I'll get to the code we have.

282
00:16:28,280 --> 00:16:31,640
We have a lot of great code for use,
so don't even worry about that. Okay,

283
00:16:31,641 --> 00:16:34,540
so question one,
uh,

284
00:16:35,330 --> 00:16:37,070
how was it great for Linear Algebra?

285
00:16:37,160 --> 00:16:42,160
Linear Algebra is the study of
Algebra apply to groups of numbers,

286
00:16:42,261 --> 00:16:45,950
right? So normally you're multiplying
single numbers, four times five,

287
00:16:46,010 --> 00:16:48,530
five times five,
and you're off,

288
00:16:48,590 --> 00:16:51,230
you're applying operations
to single numbers.

289
00:16:51,231 --> 00:16:53,690
But when you have groups of numbers,
matrices,

290
00:16:53,840 --> 00:16:57,050
which are what neural networks
use that are run on Gpu,

291
00:16:57,110 --> 00:16:59,330
that can do massive parallel computation,

292
00:16:59,540 --> 00:17:04,190
you need a new type of math to be able
to apply operations to groups of numbers

293
00:17:04,310 --> 00:17:07,850
at the same time. And that's why
linear Algebra, that's what it does.

294
00:17:07,970 --> 00:17:10,550
And that's why it's important for
neural networks. Question two,

295
00:17:10,730 --> 00:17:14,480
when is the school of the ice starting?
It's starting right now. A question three.

296
00:17:14,540 --> 00:17:18,950
Oh and the next, the first course is
going to start in a few weeks. So, uh,

297
00:17:19,250 --> 00:17:20,083
question three.

298
00:17:21,710 --> 00:17:22,250
Hm.

299
00:17:22,250 --> 00:17:26,690
What are the benefits of RNN
with Lstm over our end with Gru,

300
00:17:29,240 --> 00:17:33,990
gru is actually, um, used more
often now, but I think lst,

301
00:17:33,991 --> 00:17:38,600
m's are easier to understands. Um, lastly.

302
00:17:39,900 --> 00:17:40,560
Okay.

303
00:17:40,560 --> 00:17:44,900
Is He app similar to Google
translator? Yes. Well, I mean,
Google translate uses, um,

304
00:17:44,940 --> 00:17:47,060
an encoder decoder architecture.
Uh,

305
00:17:47,190 --> 00:17:50,130
it's very different than this one
now because it's iterated over time,

306
00:17:50,131 --> 00:17:54,270
but it's essentially the same idea
of encoder and decoder architecture.

307
00:17:54,390 --> 00:17:55,410
One more question.

308
00:17:55,710 --> 00:18:00,480
Can you explain how data is formatted
and how it will be fed in? Yes.

309
00:18:01,220 --> 00:18:01,650
Okay.

310
00:18:01,650 --> 00:18:04,770
Data is going to be formatted
as an input vector, right?

311
00:18:04,771 --> 00:18:09,390
Where every character is its own
element in that vector or a re, uh,

312
00:18:09,391 --> 00:18:11,460
and then we're going to feed
that in. Okay. So now let's,

313
00:18:11,461 --> 00:18:16,150
let's get into the code here.
Uh,

314
00:18:16,400 --> 00:18:20,110
what do we got here? What time
are we at? Okay, cool. Cool,
cool, cool, cool, cool, cool.

315
00:18:20,670 --> 00:18:21,820
Uh, all right, so

316
00:18:26,740 --> 00:18:30,620
cool. Um, so okay, where
do we begin here? So, uh,

317
00:18:30,820 --> 00:18:34,720
if we go to node dot js, what we can do
is we can go to no job, no jazz stock,

318
00:18:34,721 --> 00:18:36,820
or we can go to downloads.

319
00:18:36,940 --> 00:18:39,700
And we could say what type of operating
system do we have? I have a Mac,

320
00:18:39,701 --> 00:18:41,640
so I'll click on that and
that's going to install node,

321
00:18:41,641 --> 00:18:45,550
the node dot js using a visual manager.
Okay.

322
00:18:45,551 --> 00:18:49,360
It's a visual package manager. Once
we have that you, if you just joined,

323
00:18:49,361 --> 00:18:52,240
you haven't missed the code part yet,
so that's the good part.

324
00:18:53,740 --> 00:18:56,440
The code is going to be in the
video description by the way,

325
00:18:56,530 --> 00:19:01,060
so definitely click on that. So inside
of node dot js, we can download this.

326
00:19:01,061 --> 00:19:03,340
We can say it. Let's, let's,
let's look in the docs here. Well,

327
00:19:03,341 --> 00:19:06,750
how do we get started with this,
right? No. Dot js looks hard. Well,

328
00:19:06,751 --> 00:19:11,710
guides are usually the best place to look
in any kind of technical documentation

329
00:19:11,711 --> 00:19:15,100
inside of the getting started guide.
Here's how we do it.

330
00:19:15,101 --> 00:19:19,630
We create a simple sublime
file or a simple text file.

331
00:19:19,690 --> 00:19:22,670
We paste this in and then we hit APP,
uh,

332
00:19:22,820 --> 00:19:27,070
and we name an APP dot js and then
we in the command line type in node,

333
00:19:27,071 --> 00:19:30,670
APP dot js and at local host 3000
we'll see the message. Hello world.

334
00:19:30,790 --> 00:19:32,830
That's how you install node dot.
Js.

335
00:19:33,070 --> 00:19:36,730
So what I'm gonna do is I'm going to get
right into the meat of this code that

336
00:19:36,731 --> 00:19:41,610
we're going to use a, it's going to be
beautiful and we got to start somewhere.

337
00:19:41,611 --> 00:19:46,140
So I'm going to write out this encoder
decoder architecture for ourselves, uh,

338
00:19:46,260 --> 00:19:50,760
for us, and then we're going to start
coding it. Okay. So beautiful. So, um,

339
00:19:53,150 --> 00:19:55,460
once we've installed tensorflow dot js,

340
00:19:55,461 --> 00:19:59,300
so npm installed tentraflow dot
js uh, we can import it to, right?

341
00:19:59,301 --> 00:20:02,780
So there's several ways to be
importing tensorflow. Dot. Js uh,

342
00:20:02,781 --> 00:20:07,781
but what I'm gonna do is I'm
going to import it using NPM,

343
00:20:08,720 --> 00:20:11,040
which is the node
package manager. Now, uh,

344
00:20:11,041 --> 00:20:14,240
there's also another package
manager you should know about yarn,

345
00:20:14,241 --> 00:20:16,850
which is just as good,
if not better than NPM,

346
00:20:17,030 --> 00:20:21,240
that Facebook created because they
were having problems with npm. Okay.

347
00:20:21,241 --> 00:20:23,570
And they tried everything really,
but they were still having problems.

348
00:20:23,600 --> 00:20:27,320
And at the scale of Facebook,
of course you're going to have problems.

349
00:20:29,850 --> 00:20:30,683
MMM.

350
00:20:31,110 --> 00:20:33,650
Right? So now we're going to
load the pretrained models.

351
00:20:33,660 --> 00:20:38,660
So this a class file will contain the
pretrained model for us that we can use

352
00:20:39,271 --> 00:20:41,610
later on. Uh, what else
are we going to import?

353
00:20:41,730 --> 00:20:45,630
We also have some basic dom elements
that I've written out in a separate file

354
00:20:45,631 --> 00:20:48,690
that we're going to look at, but
that's really, it tends to dot.

355
00:20:48,700 --> 00:20:50,740
JS is like our main file here.

356
00:20:51,270 --> 00:20:54,340
And then we're going to talk
about the other things. Okay.

357
00:20:54,341 --> 00:20:58,810
So now let's load our
pretrained models as well.

358
00:20:58,811 --> 00:21:03,250
So we're going to have some
pretrained models. Uh, we're
still gonna build models,

359
00:21:03,251 --> 00:21:08,080
but I just want to load them so we
have both options, right? So we could,

360
00:21:08,440 --> 00:21:12,220
we could load our pretrained
models or, uh, we can just,

361
00:21:15,410 --> 00:21:18,800
okay, cool. I just wanted to see
what people are saying. We can, uh,

362
00:21:18,830 --> 00:21:21,140
download pretrained models or we can

363
00:21:23,060 --> 00:21:24,860
have it from the web or
downloaded from the web.

364
00:21:24,861 --> 00:21:28,520
So I'm going to download it from
the web. I think it's, where's the,

365
00:21:29,510 --> 00:21:30,343
okay,

366
00:21:30,880 --> 00:21:31,713
what were these,

367
00:21:32,330 --> 00:21:33,500
uh,

368
00:21:35,280 --> 00:21:38,620
API APIs called? What was this
website called? Oh, here we go.

369
00:21:41,200 --> 00:21:41,760
Okay.

370
00:21:41,760 --> 00:21:44,580
That's what it was called. So
these are the, this is the,

371
00:21:44,670 --> 00:21:49,110
so Google has this pretrained model
on the web for us as a Jason File.

372
00:21:49,111 --> 00:21:53,460
So these models are saved as Jason files,
but it's not just the model.

373
00:21:53,461 --> 00:21:57,510
There's also metadata, right? So
metadata includes things like versioning,

374
00:21:57,511 --> 00:22:00,930
information, timestamps,
um, things like that.

375
00:22:00,960 --> 00:22:05,880
Everything that's around the model, but
it's not necessarily the model. Okay?

376
00:22:05,881 --> 00:22:09,270
So we're going to initialize
this as a constant file.

377
00:22:09,300 --> 00:22:11,040
These are hosted model URLs,

378
00:22:11,190 --> 00:22:14,290
so we can load pretrained models
will also build the model.

379
00:22:14,291 --> 00:22:18,180
So don't even worry about it.
Okay, so we've got that. Great.

380
00:22:18,570 --> 00:22:22,110
Now we have our train model.
We have our hosted URLs.

381
00:22:22,530 --> 00:22:27,410
Now we can, um, create
our translator class.

382
00:22:27,411 --> 00:22:30,810
So not now let's build a translator.
Okay, so here's how we do this.

383
00:22:31,110 --> 00:22:35,250
We define a class called translator.
Inside of the translator class.

384
00:22:35,700 --> 00:22:40,140
We're going to initialize it and we're
going to use the ASYNC function because

385
00:22:40,141 --> 00:22:42,930
we don't want to wait for
the dom elements to load.

386
00:22:43,230 --> 00:22:45,810
When we initialize our class file.

387
00:22:46,020 --> 00:22:49,620
Now we're going to initialize it
using the URLs that we are provided.

388
00:22:49,740 --> 00:22:51,510
We will build our own models for sure.

389
00:22:53,010 --> 00:22:56,970
Now I'm going to initialize that
URLs file as it as our own variable.

390
00:22:57,330 --> 00:22:57,961
I'm going to say,

391
00:22:57,961 --> 00:23:02,310
well here's our model and we're going
to wait for it to load using the await

392
00:23:02,340 --> 00:23:06,210
function. And I haven't going
to have a function for low load,

393
00:23:06,211 --> 00:23:11,211
the hosted pretrained model
using the URLs dot models.

394
00:23:12,270 --> 00:23:15,600
Okay. And we have that. Okay,

395
00:23:15,990 --> 00:23:19,990
so that is our model right there.
What it does,

396
00:23:20,020 --> 00:23:24,100
the await function by the way, it just
expresses it. That expression just,

397
00:23:24,550 --> 00:23:28,280
it causes the async function to uh,
the,

398
00:23:28,350 --> 00:23:32,710
to pause until this function that,
that it's a weighting has, has loaded.

399
00:23:32,711 --> 00:23:33,850
So that's why we did that.

400
00:23:34,390 --> 00:23:38,020
Now we're going to do the same thing
using the away to method for uh,

401
00:23:38,110 --> 00:23:41,650
the Metadata, right? We cause
we have metadata as well.

402
00:23:41,651 --> 00:23:43,390
It's not just the model itself.

403
00:23:43,391 --> 00:23:48,391
We have the metadata and this model
actually consists of two parts.

404
00:23:48,591 --> 00:23:50,410
We have our,
remember we have our encoder,

405
00:23:50,420 --> 00:23:55,420
so we're going to call it prepare
in coder model and it has rd coder.

406
00:23:55,491 --> 00:23:58,370
And in these functions that
I'm defining right here,

407
00:23:58,610 --> 00:24:03,020
we're going to segment out this
model and then prepare both of them.

408
00:24:03,770 --> 00:24:06,170
And I'll return this at the very end.

409
00:24:06,410 --> 00:24:11,410
That's our initialization
function for our translator class.

410
00:24:13,810 --> 00:24:16,790
Okay, so, um, let's start off with the,

411
00:24:16,870 --> 00:24:20,980
the metadata because right, it's not
just the model that we're loading.

412
00:24:20,981 --> 00:24:22,450
It's the metadata as well.

413
00:24:22,480 --> 00:24:27,370
We have two different things to
be loading here. Let me also see.

414
00:24:31,330 --> 00:24:36,270
Hi everybody. Okay. I just want to
see what everybody's saying. Cool,

415
00:24:36,271 --> 00:24:40,650
cool. We have people joining
it and we have people leaving.
Don't leave. Stay, stay.

416
00:24:40,680 --> 00:24:42,690
Okay.
This is important stuff.

417
00:24:42,750 --> 00:24:46,260
This is more important than web
development in general. Look,

418
00:24:46,261 --> 00:24:48,540
machine learning is the
future of all code. Okay?

419
00:24:48,570 --> 00:24:51,810
Everything's going to be machine learning
and if you want to stay on top of

420
00:24:51,811 --> 00:24:55,830
things, you need to understand how this
works. Starting with metadata. Okay,

421
00:24:55,831 --> 00:25:00,000
so back to metadata.
Back to metadata.

422
00:25:02,440 --> 00:25:03,273
Where were we?

423
00:25:04,950 --> 00:25:09,330
So we can retrieve this
metadata from our helper class.

424
00:25:10,110 --> 00:25:11,940
Cause we have a helper class here

425
00:25:13,950 --> 00:25:15,450
and we're going to use
the loader for member.

426
00:25:15,451 --> 00:25:18,120
I imported that loader
at the very beginning

427
00:25:20,280 --> 00:25:25,280
load hosted metta data using the URLs.

428
00:25:25,860 --> 00:25:26,693
Metadata.

429
00:25:31,020 --> 00:25:35,250
Okay, so that's the loader.
That's our translation. Meta data.

430
00:25:35,490 --> 00:25:38,280
Now we can say, well,
let's take our decoder.

431
00:25:42,070 --> 00:25:42,903
Okay.

432
00:25:43,940 --> 00:25:47,600
And so one of the, so now we're going
to retrieve something from the metadata.

433
00:25:47,601 --> 00:25:51,080
So we're, remember we were
treating what already exists,

434
00:25:51,320 --> 00:25:55,100
so we can say what we know that one
of the variables of the Metadata,

435
00:25:55,101 --> 00:25:59,330
and if we look into tensorflow, docs,
this has these, it has these features,

436
00:25:59,480 --> 00:26:04,480
but one of the variables of the input
data or of the metadata is the Max

437
00:26:05,151 --> 00:26:08,990
sequence length for both
the encoder and the decoder.

438
00:26:15,940 --> 00:26:16,181
Okay,

439
00:26:16,181 --> 00:26:20,620
let me zoom out a bit so people can see
what we're talking about here or there

440
00:26:20,621 --> 00:26:24,460
we go. Now we have the
same thing for our encoder.

441
00:26:24,461 --> 00:26:27,010
So we have a Max decoder length.

442
00:26:27,370 --> 00:26:31,180
We have a max in Kotor length.
And again,

443
00:26:31,181 --> 00:26:33,700
we can use that same
model to retrieve that.

444
00:26:34,030 --> 00:26:37,240
And it's going to look very similar except
it's going to be called the encoder.

445
00:26:40,790 --> 00:26:41,630
Yes.

446
00:26:42,710 --> 00:26:43,240
Yeah,

447
00:26:43,240 --> 00:26:44,260
you're right.
Cool.

448
00:26:44,780 --> 00:26:49,600
Uh,
Max encoder sequence length,

449
00:26:49,810 --> 00:26:54,100
right. Okay. Yes. So now we
have our Dakota or we ever
encoder sequenced length.

450
00:26:54,460 --> 00:26:57,930
And now, uh, there's,
there's actually, um,

451
00:27:01,000 --> 00:27:01,833
cool

452
00:27:02,490 --> 00:27:07,380
cons. Yes. So cons needs to be,
let's see. Translation, metadata.

453
00:27:07,410 --> 00:27:12,390
Oh, rivalry. Thanks. Um, right.

454
00:27:14,460 --> 00:27:17,460
There we go. Thank you.
Right? Yes, thank you guys.

455
00:27:17,970 --> 00:27:21,960
This is how we do it. Crowd sourced
debugging. So now back to this.

456
00:27:22,080 --> 00:27:24,690
So we have our,
we have a token index as well.

457
00:27:24,780 --> 00:27:28,200
Now the reason we're going to use this
token index is so we have a way of

458
00:27:28,230 --> 00:27:32,040
knowing where in the phrase we are,

459
00:27:32,280 --> 00:27:35,850
when we are predicting the
next character in the sequence,

460
00:27:36,000 --> 00:27:37,890
we need a way to predict where we are.

461
00:27:37,891 --> 00:27:41,610
And so the metadata has
that and it's called the,

462
00:27:41,940 --> 00:27:44,820
it's called the input to token index.

463
00:27:46,750 --> 00:27:50,080
Now we have that not just for the input,
but we have this for the target index.

464
00:27:50,110 --> 00:27:52,040
Now what the target index is,

465
00:27:52,270 --> 00:27:56,320
is it's the final label that
we are trying to predict,

466
00:27:57,280 --> 00:28:01,390
right?
So that label is the output French phrase.

467
00:28:02,290 --> 00:28:06,510
Okay? So that's our index. No,

468
00:28:06,511 --> 00:28:10,200
that's it for metadata.
Now I want to prepare the encoder model.

469
00:28:10,230 --> 00:28:13,650
Prepare encoder model.
Okay?

470
00:28:14,700 --> 00:28:18,350
And it's going to take
the model as its input.

471
00:28:18,351 --> 00:28:22,310
So it's going to segment out the
encoder model using the, the, the,

472
00:28:22,311 --> 00:28:25,070
the pretrained weights. And
we can, we can do is we can,

473
00:28:25,100 --> 00:28:29,960
we can train it on our own data, right?
So it's already been trained on uh,

474
00:28:29,980 --> 00:28:33,200
you know, language pairs, but
we can just train it ourselves.

475
00:28:33,201 --> 00:28:34,760
We can just add training to it.
Right?

476
00:28:34,761 --> 00:28:39,590
And also this is the future of machine
learning where you don't necessarily have

477
00:28:39,591 --> 00:28:43,430
to train your own model from scratch
every time because someone else has done

478
00:28:43,431 --> 00:28:46,610
that in the same way that you
don't have to write code, right?

479
00:28:46,611 --> 00:28:49,760
You don't have to write all the bare
bones code for everything every time

480
00:28:49,820 --> 00:28:52,880
someone has already done that.
So you import a library.

481
00:28:53,060 --> 00:28:56,510
So the paradigm that we're going to
start seeing with machine learning is one

482
00:28:56,511 --> 00:28:58,670
where we are not just importing libraries,

483
00:28:58,850 --> 00:29:02,510
we are importing pretrained
models and not just single models,

484
00:29:02,511 --> 00:29:05,570
but multiple models trained on
multiple different types of data.

485
00:29:05,870 --> 00:29:09,680
And we can combine all of these models
together and then train it on our own

486
00:29:09,681 --> 00:29:13,010
data and we won't need as
much data because of it.

487
00:29:13,160 --> 00:29:15,020
It's a much more efficient process.

488
00:29:17,670 --> 00:29:22,410
Now enough of that. And let's
get back to this. Okay. So, uh,

489
00:29:22,840 --> 00:29:25,510
back to the encoder model.
So what does this look like?

490
00:29:27,560 --> 00:29:31,310
So how many encoder Toki
encoder token count,

491
00:29:31,460 --> 00:29:34,970
how many encoder tokens do we have?
Right? So what these tokens are,

492
00:29:34,971 --> 00:29:39,530
are the number of characters that
we are inputting into the encoder.

493
00:29:39,830 --> 00:29:40,970
And what we can do is we get,

494
00:29:43,780 --> 00:29:44,180
yeah,

495
00:29:44,180 --> 00:29:45,170
for the length of this,

496
00:29:45,171 --> 00:29:48,770
and it's going to be of size of shape too
because there are three different, um,

497
00:29:49,160 --> 00:29:52,700
zero one, two. So those are the elements
in the array. There are three different,

498
00:29:52,910 --> 00:29:56,690
uh, words in a phrase and
we can say that's well,

499
00:29:56,691 --> 00:29:59,270
that's going to be the number of tokens.
Now we have to decide,

500
00:29:59,271 --> 00:30:01,580
well how many inputs
is going to be in this?

501
00:30:01,610 --> 00:30:05,840
And we can use the equal sign.
We could say,

502
00:30:05,870 --> 00:30:08,730
well modeled on input zero.
That's going to be the,

503
00:30:08,840 --> 00:30:11,180
that's going to be input to the model.
Now remember,

504
00:30:11,181 --> 00:30:15,020
we are using the hidden
states as the, as the, uh,

505
00:30:15,080 --> 00:30:20,080
initial app as the medium with
which to initialize a decoder.

506
00:30:20,450 --> 00:30:21,283
So,

507
00:30:23,400 --> 00:30:27,750
so we need to retrieve both of those.
So we have our hidden state, state H,

508
00:30:27,960 --> 00:30:30,270
which we're going to retrieve from the
models. And we're going to say, well,

509
00:30:30,271 --> 00:30:33,720
here's a specific layer
where it's out where it's at,

510
00:30:35,220 --> 00:30:38,880
but it's not enough to just, uh,
it's not enough to just say, we,

511
00:30:38,881 --> 00:30:43,260
we need the hidden state.
We also need to have that cell state.

512
00:30:43,261 --> 00:30:46,680
So remember,
recall I just talked about LSTM networks.

513
00:30:46,830 --> 00:30:51,240
And what LSTM networks do
is they don't just have a,

514
00:30:52,470 --> 00:30:56,670
they don't just offer a hidden states.

515
00:30:56,671 --> 00:30:58,680
They offer cell states.
That's what's learned.

516
00:30:58,681 --> 00:31:01,800
And what does that prevent that prevents
the vantage and gradient problem.

517
00:31:01,950 --> 00:31:05,610
And what does that do? That lets
us learn longterm sequences. Okay?

518
00:31:05,940 --> 00:31:08,760
So that's our hidden state.
That's our cell state.

519
00:31:08,790 --> 00:31:10,530
And what we can do is we can say,
well,

520
00:31:10,531 --> 00:31:15,531
let's create a new list and put those
values into this single list that we've

521
00:31:15,631 --> 00:31:17,010
just defined.

522
00:31:18,600 --> 00:31:23,010
And we'll take that list and we'll
use it to build the model. Okay,

523
00:31:24,610 --> 00:31:26,710
I see it. Translation, Metadata,

524
00:31:27,900 --> 00:31:28,733
uh,

525
00:31:30,330 --> 00:31:31,710
input token index.

526
00:31:37,060 --> 00:31:39,610
It is right.
Translation metadata.

527
00:31:42,750 --> 00:31:43,583
Okay.

528
00:31:44,650 --> 00:31:49,300
Yes, I will use a semi colon consistently.
We got some, we got some Java script. Um,

529
00:31:49,600 --> 00:31:53,500
people in here, which is good.
It's not just all python people.

530
00:31:53,950 --> 00:31:58,900
We want diversity. We want
variety guys. Okay. Javascript,

531
00:31:59,260 --> 00:32:03,700
python, look, python is what is obviously
where it's at for machine learning.

532
00:32:04,000 --> 00:32:05,620
And this is kind of controversial to say,

533
00:32:05,621 --> 00:32:09,850
but if I were to pick a number
two language for machine learning,

534
00:32:10,150 --> 00:32:14,500
I'm going to pick javascript,
not are not scholar, not go, not,

535
00:32:15,070 --> 00:32:18,430
not, uh, Matlab for sure. Um,

536
00:32:20,020 --> 00:32:22,930
but it's javascript. Um, and
there's a lot of reasons for that,

537
00:32:22,931 --> 00:32:26,620
but there's a lot of developer activity
around javascript libraries for machine

538
00:32:26,621 --> 00:32:31,360
learning and have a great video on that
coming out this weekend, by the way. So,

539
00:32:31,361 --> 00:32:35,410
um, now we can build the model. So now
we're actually using a tentraflow dot.

540
00:32:35,411 --> 00:32:38,500
Js function,
TF dot model to build this thing.

541
00:32:41,430 --> 00:32:43,770
And, um, we can say, well,

542
00:32:43,800 --> 00:32:47,610
here are our inputs and here our outputs
are outputs are going to be the states,

543
00:32:47,611 --> 00:32:51,540
right? Because we want
those states to be fed into

544
00:32:54,100 --> 00:32:54,933
the

545
00:32:55,680 --> 00:32:57,510
decoder.
So that's what we're out putting.

546
00:32:58,600 --> 00:32:59,170
Okay?

547
00:32:59,170 --> 00:33:04,030
Okay. So that's us preparing our encoder
and now let's prepare the decoder.

548
00:33:04,570 --> 00:33:06,230
Prepare.
Okay.

549
00:33:10,160 --> 00:33:14,780
Prepare decoder model using the same.

550
00:33:15,050 --> 00:33:17,120
Remember we are segmenting out this model.

551
00:33:19,970 --> 00:33:21,050
Oh, I see, I see.

552
00:33:21,760 --> 00:33:24,190
You're right. Yes, that's what you meant.

553
00:33:25,470 --> 00:33:28,710
Okay. I just want to fix some syntax
here and now we're back. Thank you.

554
00:33:29,190 --> 00:33:33,360
So prepared. Decoder model
looks like this. So again,

555
00:33:33,660 --> 00:33:38,550
we are going to do the same kind of
basic idea where we are deciding what we

556
00:33:38,551 --> 00:33:40,770
are.
We are trying to see what the number of

557
00:33:43,710 --> 00:33:45,330
a decoder tokens are going to be.

558
00:33:45,490 --> 00:33:50,490
So they nom decoder tokens and it's
gonna be modeled on input one that shape.

559
00:33:51,120 --> 00:33:54,420
And then we're going to say, well,
okay, so let's define our hidden state.

560
00:33:54,421 --> 00:33:58,770
So we have, our constants
are hidden state, uh,

561
00:33:58,771 --> 00:34:02,850
and that hidden state is going to be just
that. Again, we have our hidden state.

562
00:34:03,630 --> 00:34:07,620
Uh, now what we need to do
is define a latent dimension.

563
00:34:07,980 --> 00:34:11,280
Now the reason we are defining this
lightened dimension using the hidden state

564
00:34:11,600 --> 00:34:13,680
is it's going to help us define

565
00:34:15,970 --> 00:34:20,020
the input. Now let me talk about
what exactly I mean by that.

566
00:34:20,021 --> 00:34:25,021
So it's going to be one less than
the hidden states size or length.

567
00:34:25,990 --> 00:34:26,860
Because

568
00:34:34,000 --> 00:34:35,440
how many live people
do we have by the way?

569
00:34:37,900 --> 00:34:42,400
Constantly in dementia. Cool.
Cool. All right. So, um,

570
00:34:44,020 --> 00:34:48,820
so we're gonna use this latent dimension
to help us define the inputs directly

571
00:34:48,821 --> 00:34:52,390
into both hidden states.
So let me show you what I mean.

572
00:34:52,391 --> 00:34:54,580
So I have a decoder state input.
H

573
00:34:56,320 --> 00:34:59,020
I have a decoder,
state

574
00:35:01,130 --> 00:35:03,800
input C,
which is the cell states.

575
00:35:04,100 --> 00:35:07,970
And then I have a decoder
state inputs variable. Okay.

576
00:35:07,971 --> 00:35:09,440
So these are our three variables

577
00:35:12,450 --> 00:35:16,380
equal sign up 54. Thank
you. Okay, thank you.

578
00:35:19,240 --> 00:35:23,930
All right. Um, now, now,

579
00:35:24,350 --> 00:35:27,710
now we're going to actually use some
tensorflow. Dot. JS input functions.

580
00:35:28,010 --> 00:35:32,560
One is going to be t f. Dot. Input
shape using the latent dimension.

581
00:35:32,720 --> 00:35:34,790
Here is why we use the
lightened dimension.

582
00:35:34,900 --> 00:35:39,900
We are using the shape of the hidden
state minus one to define the decoder

583
00:35:40,231 --> 00:35:42,240
states hidden state.
Okay.

584
00:35:42,241 --> 00:35:47,241
So it's going to be minus one because we
are predicting the next character in a

585
00:35:47,431 --> 00:35:49,950
sequence, right? It's not the,
it's it's going to be the, if,

586
00:35:50,040 --> 00:35:54,930
if the input sequences t we're
predicting t plus one. So the next one.

587
00:35:55,230 --> 00:35:59,040
So the input shape has to start one
before [inaudible] 67 thank you.

588
00:36:00,900 --> 00:36:04,170
Latent dimension is the
shape we have our name,

589
00:36:04,200 --> 00:36:09,200
which is going to be decoder state in
cooked h k and now let's do the same

590
00:36:12,121 --> 00:36:14,580
thing. Let's make this a little
smaller so we can all see this.

591
00:36:15,180 --> 00:36:17,490
Let's do the same thing for

592
00:36:20,510 --> 00:36:25,010
the cell state and we'll do the same
thing. Now these are our inputs.

593
00:36:25,011 --> 00:36:28,790
Now that we've defined both of them,
now we can say, well, here's our

594
00:36:31,310 --> 00:36:34,970
input age and here's our inputs.
See,

595
00:36:35,180 --> 00:36:37,070
now these are our state inputs,
right?

596
00:36:37,071 --> 00:36:40,640
So these are our hidden states
and these are our, these are our,

597
00:36:40,820 --> 00:36:43,700
this is our hidden states
and ourselves state. Okay,

598
00:36:44,030 --> 00:36:48,170
now that we have both of them,
now let's retrieve the l s t m model.

599
00:36:48,620 --> 00:36:49,453
Just like that.

600
00:36:58,700 --> 00:37:01,280
Okay, so I just want to
take a little break to say,

601
00:37:01,820 --> 00:37:04,070
Quan Luis Clore said,

602
00:37:04,910 --> 00:37:08,540
how do you convince the public we really
live in the machine learning era? Well,

603
00:37:08,541 --> 00:37:13,340
it's too hard for a country like Bolivia
and suit America believe to believe in

604
00:37:13,341 --> 00:37:15,800
that.
I don't care where you live.

605
00:37:15,830 --> 00:37:20,810
You can live in Antarctica as long as
you have an internet connection and some

606
00:37:20,811 --> 00:37:21,950
kind of computing devices,

607
00:37:21,951 --> 00:37:26,951
it can be a hundred dollar convert to
your local currency Chromebook as long as

608
00:37:27,261 --> 00:37:29,270
you can access.
Let me show you this.

609
00:37:29,430 --> 00:37:33,830
Co lab.research.google.com.
Okay?

610
00:37:33,831 --> 00:37:37,910
You go to this in your browser, okay?
This could even be Internet explorer,

611
00:37:37,911 --> 00:37:39,560
like the worst browser ever,

612
00:37:40,910 --> 00:37:44,630
but as long as you have a browser,
you create a notebook.

613
00:37:44,780 --> 00:37:48,920
You say hello world in that notebook,

614
00:37:49,550 --> 00:37:53,790
you hit this.
It's compiling it on an Nvidia k,

615
00:37:53,791 --> 00:37:58,350
80 GPU. You have 12 hours of
training time on this thing for free.

616
00:37:58,740 --> 00:38:02,640
It compiled it on that. Any printed
it out? You have everything.

617
00:38:02,641 --> 00:38:06,090
You have your algorithms. Okay? That's
all available. Um, machine learning,

618
00:38:06,091 --> 00:38:10,380
sub reddit. It's available on the
tutorials online. You have the data.

619
00:38:10,410 --> 00:38:14,760
It's all available using awesome. Get
held public lists. Um, Reddit r slash.

620
00:38:14,840 --> 00:38:19,050
Datasets. You have the
education, me, other youtubers.

621
00:38:19,410 --> 00:38:24,330
You have the compute here. So I don't
want to hear, I live in South America,

622
00:38:24,331 --> 00:38:28,000
Bolivia, whatever. I can't do machine
learning. I don't care where you are here,

623
00:38:28,010 --> 00:38:32,010
you can do this. So don't ever, uh, give
me an excuse like that again. All right,

624
00:38:32,220 --> 00:38:37,120
so that's, that's enough of that. Um,
I, he will be suing me. No, no, no.

625
00:38:37,121 --> 00:38:40,390
Actually Microsoft asked me to
make an a zero video and I said no.

626
00:38:40,391 --> 00:38:44,050
So let's see them. Sue me again. All
right, so here we go. Back to this.

627
00:38:45,910 --> 00:38:49,440
Where was I? I got a little off track.
A little passionate about that one.

628
00:38:50,080 --> 00:38:51,240
Thank you.
Back to the,

629
00:38:52,290 --> 00:38:53,630
what about variable name?

630
00:38:54,530 --> 00:38:58,670
Right, right, right, right.
Thank you. Okay, so online 70.

631
00:39:00,130 --> 00:39:00,963
Okay.

632
00:39:01,430 --> 00:39:03,530
We have our TF dot input.

633
00:39:05,680 --> 00:39:09,460
Um, what, what went on here?

634
00:39:11,770 --> 00:39:14,470
Yeah.
Interesting.

635
00:39:17,020 --> 00:39:20,050
Looks fine to me. T F. Dot. Input Age.

636
00:39:22,750 --> 00:39:26,230
Okay, so back to the retrieving the
LSTM models. So we can say, well,

637
00:39:26,231 --> 00:39:29,420
here's our decoder and we already have,
um,

638
00:39:30,700 --> 00:39:32,370
our decoder,
uh,

639
00:39:33,160 --> 00:39:38,160
we're going to take that last
layer of the decoder decoder.

640
00:39:39,011 --> 00:39:39,844
Lstm

641
00:39:40,730 --> 00:39:41,780
good capital l.

642
00:39:43,110 --> 00:39:44,340
Oh, I see, I see, I see.

643
00:39:46,420 --> 00:39:50,380
Capital l there is no Capitol Hill.
Okay.

644
00:39:50,650 --> 00:39:51,730
So back to this,

645
00:39:53,470 --> 00:39:57,850
we're going to define our initial input
by saying all those layers going through

646
00:39:57,851 --> 00:40:00,550
their Dakota LSTM um

647
00:40:07,020 --> 00:40:10,640
oh, capital items instead of Oh, I
see, I see, I see, I see. So again,

648
00:40:10,820 --> 00:40:15,240
thank you capitol. I, here we go. Ah,

649
00:40:15,660 --> 00:40:16,830
Gotcha.
Got It.

650
00:40:18,200 --> 00:40:19,033
Cool.

651
00:40:21,230 --> 00:40:24,660
All right, cool. So now inputting
book Capitol Capitol. Great,

652
00:40:25,320 --> 00:40:28,770
please zoom guys. How much zooming
do you want here? Okay, here we go.

653
00:40:31,410 --> 00:40:36,390
The current state of LCM inputs.
Zero. So this is our initial input.

654
00:40:36,630 --> 00:40:36,841
Okay.

655
00:40:36,841 --> 00:40:41,310
This is our initial input and now we
can initialize the model using the first

656
00:40:41,311 --> 00:40:44,520
input and the hidden states input,
which we've already defined here.

657
00:40:44,790 --> 00:40:49,650
So we can say apply outputs
equals decoder.

658
00:40:49,651 --> 00:40:54,360
Lstm
how much time has passed so far?

659
00:40:55,230 --> 00:40:59,220
Do you coach? Thanks. No, I mean
like how long has this streaming

660
00:41:00,580 --> 00:41:01,890
decoder inputs

661
00:41:05,910 --> 00:41:10,710
state state. Cool, thanks.

662
00:41:12,750 --> 00:41:14,820
All right,
that's our outputs.

663
00:41:17,580 --> 00:41:20,830
All right. I have about 10 more lines
of code here. Ah, so stick around.

664
00:41:20,831 --> 00:41:24,100
I have 10 more lines of code and I'm
going to answer some questions at the very

665
00:41:24,101 --> 00:41:25,990
end. Okay. I'm going to
answer some questions.

666
00:41:25,991 --> 00:41:30,370
So don't go anywhere back to
this decoder states inputs.

667
00:41:30,430 --> 00:41:34,220
Right?
So now I have those outputs.

668
00:41:35,390 --> 00:41:36,223
MMM.

669
00:41:37,780 --> 00:41:41,890
So what we want to do is we
want to get that last output.

670
00:41:41,920 --> 00:41:43,270
It hasn't been activated yet.

671
00:41:43,271 --> 00:41:46,390
We haven't applied a dense
fully connected layer to it.

672
00:41:46,750 --> 00:41:51,580
So we have both of our hidden states, by
the way, we have both of them already,

673
00:41:51,581 --> 00:41:53,710
both the hidden states
and the cell states.

674
00:41:53,950 --> 00:41:56,290
And what we can do is we can just say,

675
00:41:56,590 --> 00:42:00,040
let's take the first element
of this apply outputs.

676
00:42:01,920 --> 00:42:02,560
Okay.

677
00:42:02,560 --> 00:42:07,560
Array or list and store that
in the decoders hidden states.

678
00:42:08,020 --> 00:42:10,840
And we'll do the same
thing for the cell state.

679
00:42:11,470 --> 00:42:16,150
And that's just going to be the next, the
next one. And then we have our hidden,

680
00:42:16,320 --> 00:42:18,400
so these hidden states,
we're going to get their own list.

681
00:42:18,520 --> 00:42:22,810
Now that we finally retrieved
these hidden states, we can,

682
00:42:23,020 --> 00:42:24,460
we can give them their own list.

683
00:42:26,200 --> 00:42:28,420
And so

684
00:42:28,510 --> 00:42:30,160
these alone aren't going to do anything.

685
00:42:30,161 --> 00:42:32,380
We have to apply a fully
connected layer to them.

686
00:42:32,650 --> 00:42:35,950
What this is going to do is it's
going to output a probability value.

687
00:42:36,100 --> 00:42:39,250
Why do we want a probability value?
That's what the dense layer is,

688
00:42:39,460 --> 00:42:42,070
because a probability value will allow us

689
00:42:44,100 --> 00:42:46,710
a probability value will allow us

690
00:42:51,730 --> 00:42:56,730
to make a prediction on what character
is the most probable next character.

691
00:42:57,461 --> 00:43:01,190
Right? So if it's going to say
you know, 70%, um, you know,

692
00:43:01,210 --> 00:43:04,390
h and it's very likely going
to be h as the next character.

693
00:43:04,630 --> 00:43:09,160
And that's what we apply a dense layer
to to it. So to get that final outputs,

694
00:43:10,480 --> 00:43:12,130
we're gonna use that dense layer

695
00:43:14,730 --> 00:43:15,563
two.

696
00:43:23,080 --> 00:43:23,913
MMM.

697
00:43:24,660 --> 00:43:27,540
To define that output.

698
00:43:28,080 --> 00:43:30,710
We have our decoder model or so
that's our, that's our final output.

699
00:43:30,720 --> 00:43:34,020
We applied the dense layer to the
outputs. Now we have a prediction. Okay.

700
00:43:34,021 --> 00:43:36,300
So the decoder model

701
00:43:40,860 --> 00:43:42,300
is going to use,

702
00:43:44,770 --> 00:43:46,270
it's its own TF model.

703
00:43:50,990 --> 00:43:51,690
Okay.

704
00:43:51,690 --> 00:43:56,690
And it's going to use as input to the
decoders inputs concatenated with the

705
00:43:57,751 --> 00:44:00,270
states,
the state inputs,

706
00:44:02,730 --> 00:44:06,180
outputs.
Thank you.

707
00:44:07,710 --> 00:44:09,840
And we have our outputs,

708
00:44:09,870 --> 00:44:13,500
which are the decoder outputs.
Okay.

709
00:44:13,501 --> 00:44:18,501
That's it for this little
portion of code gets,

710
00:44:18,930 --> 00:44:23,880
okay, let's review what I've written
here, by the way. Okay, let's review this.

711
00:44:25,900 --> 00:44:26,733
So

712
00:44:33,670 --> 00:44:37,620
my 97 right there we go.
Making sure I got all my,

713
00:44:38,640 --> 00:44:41,880
I still haven't registered sublime text
at this point. It's just become a meme.

714
00:44:42,150 --> 00:44:46,860
Like I will never register a
sublime text because I just,

715
00:44:47,550 --> 00:44:51,430
I dunno why sublime better like sponsor
me or something. I don't care now.

716
00:44:51,431 --> 00:44:54,660
I don't worry about it. I
love you sublime. I don't know
why. I just don't want to,

717
00:44:54,661 --> 00:44:56,010
I like being a rebel back to this.

718
00:44:58,080 --> 00:44:58,913
MMM.

719
00:44:59,990 --> 00:45:02,840
I, my friend Hawes Balaban Ian
who is helping me live stream.

720
00:45:02,841 --> 00:45:06,050
That's you sitting next to me. So He's
helping me with this live stream stuff.

721
00:45:06,080 --> 00:45:09,790
Okay. He's got to be our
channel back to this. Uh,

722
00:45:11,420 --> 00:45:13,510
okay. So let's just, let's just go.

723
00:45:13,550 --> 00:45:18,130
Anybody can go to TF js
examples and here's your,

724
00:45:18,150 --> 00:45:21,800
are all of these examples, by the way,
you have your card poll. All right.

725
00:45:21,801 --> 00:45:25,850
This balances a car pull in the
browser and it's got a live demo.

726
00:45:25,851 --> 00:45:29,000
Most of them have live demos in
the browser that you can see.

727
00:45:29,001 --> 00:45:33,350
If you just click see the example live,
I want you to for sure go to this, uh,

728
00:45:33,380 --> 00:45:37,730
get hub repository. If you haven't yet
looked at, think training in the browser.

729
00:45:38,530 --> 00:45:39,363
Okay?

730
00:45:39,590 --> 00:45:42,770
Okay. And so are, so what I'm gonna do,
I'm going to download this right now.

731
00:45:43,220 --> 00:45:47,120
I mean, download it. Let me
click open. Let me go to it.

732
00:45:48,450 --> 00:45:49,160
Okay.

733
00:45:49,160 --> 00:45:53,810
Terminal. Let me go into
it. Make this bigger.

734
00:45:54,080 --> 00:45:58,130
And now let's run this ourselves.
So CD translation,

735
00:45:58,790 --> 00:46:03,350
and then I'm going to answer
some questions right after
I run this demo. Okay?

736
00:46:03,351 --> 00:46:08,090
So if we go here, let's, let's look at
this. What does it say? Translation. Okay.

737
00:46:08,091 --> 00:46:12,920
It says to do two things. Yarn and
yarn. Watch. So yarn, I can do that.

738
00:46:13,250 --> 00:46:16,940
So what yarn is, is it's a version of
NPM that Facebook created. Remember,

739
00:46:18,420 --> 00:46:19,850
uh,
that that can,

740
00:46:20,710 --> 00:46:23,860
that bypasses a lot of the
issues with NPM at scale.

741
00:46:24,280 --> 00:46:27,970
And so once we've run yarn,
now we can run yarn watch.

742
00:46:28,420 --> 00:46:32,350
And what yarn watch is going to do
is in the browser at our local host,

743
00:46:32,590 --> 00:46:35,170
it's going to run that pretrained model.

744
00:46:39,760 --> 00:46:42,730
Okay? At local host,
one, two, three, four.

745
00:46:43,090 --> 00:46:46,990
Now it's going to show up here,
okay?

746
00:46:46,991 --> 00:46:49,570
That's the whole point is
that it needs to show up.

747
00:46:50,050 --> 00:46:53,890
Now we're going to load that pretrained
model and we're going to go ahead and

748
00:46:53,891 --> 00:46:57,550
say you are cool.

749
00:46:58,800 --> 00:47:03,630
Do [inaudible] that's my French,
right? They're homeless. Do flow,

750
00:47:03,631 --> 00:47:07,800
mush, homeless dude format.
Okay. Question Time.

751
00:47:07,830 --> 00:47:10,500
So just ask questions.
I'm reading all of this in real time.

752
00:47:10,530 --> 00:47:11,910
Let's answer some questions here.

753
00:47:13,260 --> 00:47:13,930
Yeah.

754
00:47:13,930 --> 00:47:16,210
Uh, linked to everything is going
to be in the video description.

755
00:47:16,240 --> 00:47:20,080
Definitely check out colab that
research.google.com check out the get hub

756
00:47:20,081 --> 00:47:22,630
repository for tenths
below dot. Js. I love you.

757
00:47:22,840 --> 00:47:25,300
The school of AI first course
will start in a few weeks.

758
00:47:25,390 --> 00:47:28,990
I have big things planned for everybody.
It's a very exciting time right now.

759
00:47:29,200 --> 00:47:33,780
And uh, yarn does not
come with Hadoop. However,

760
00:47:33,790 --> 00:47:35,590
you can use yarn with Hadoop.

761
00:47:36,070 --> 00:47:40,720
Why use a different activation function
h in one place? That's a great question.

762
00:47:40,870 --> 00:47:45,700
So, uh, check out my video. Which
activation function should I use?

763
00:47:45,730 --> 00:47:47,320
Saroj just Google that.

764
00:47:47,920 --> 00:47:52,390
Any advantage of no dot
js over Python r um, yes.

765
00:47:52,420 --> 00:47:57,420
One example is the ability to
easily use distributed computing,

766
00:47:58,481 --> 00:47:58,721
right?

767
00:47:58,721 --> 00:48:03,721
So client side machine learning involves
using the gps of whoever the host is.

768
00:48:04,540 --> 00:48:07,660
And if we use client side
machine learning with Javascript,

769
00:48:07,810 --> 00:48:11,020
you can train it on their
gps in real time. Now,

770
00:48:11,021 --> 00:48:13,930
you could do this with python
using Django and flask,

771
00:48:14,260 --> 00:48:16,810
but it's much easier with javascript.

772
00:48:17,600 --> 00:48:18,433
Okay,

773
00:48:19,330 --> 00:48:21,690
three more questions.
Uh,

774
00:48:25,900 --> 00:48:30,460
how can I make a related
word to a specific topic? Ai.

775
00:48:30,970 --> 00:48:34,990
Okay. Word two, Vec Ghoul that
word or no? Youtube that word.

776
00:48:34,991 --> 00:48:36,310
Two VEC Suraj.

777
00:48:36,940 --> 00:48:40,120
But you turn words into vectors and then
you can find a similarity between these

778
00:48:40,121 --> 00:48:44,830
vectors. It's a numerical operation
and we're, Yvonne says, I'm the cutest.

779
00:48:44,831 --> 00:48:49,630
I'm sure and passionate says deep
learning with Django. It's not a question.

780
00:48:50,200 --> 00:48:54,560
Last question. Okay. The
last question is the shawl.

781
00:48:54,590 --> 00:48:55,550
The shawl asks,

782
00:48:56,000 --> 00:48:59,900
can you suggest some good books for
linear Algebra with regard to machine

783
00:48:59,901 --> 00:49:04,070
learning? No, I'm not gonna. I'm
not gonna suggest good books. Why?

784
00:49:04,370 --> 00:49:09,350
Because you can very easily learn
linear Algebra from three blue,

785
00:49:09,351 --> 00:49:13,700
one brown. Okay? If you
search linear Algebra,

786
00:49:15,880 --> 00:49:20,350
my buddy grant here, promoting them
right now, essence of linear Algebra,

787
00:49:20,530 --> 00:49:24,520
definitely check out this series.
Amazing playlist.

788
00:49:24,730 --> 00:49:29,170
This is enough. Watch all of this.
Then watch math of intelligence by me.

789
00:49:30,310 --> 00:49:34,060
If I do say so myself on amazing playlist.
Okay.

790
00:49:34,061 --> 00:49:37,630
This is when I was in Europe.
I was just like going through that.

791
00:49:38,440 --> 00:49:42,850
I was just so hype. You know what I'm
saying? Like I was totally in the zone.

792
00:49:43,150 --> 00:49:47,410
I was combining art and music and like
everything together with machine learning,

793
00:49:47,411 --> 00:49:48,244
it was sick.

794
00:49:48,400 --> 00:49:53,080
Now I have to outdo that with my first
school of Ai Ai course and I will.

795
00:49:53,380 --> 00:49:57,490
But that, that's, that's that
question. Okay. So anyway.

796
00:49:58,610 --> 00:49:59,410
Okay,

797
00:49:59,410 --> 00:50:04,150
I love you too. Look, you guys don't
need to qualify. I'm not gay. Okay.

798
00:50:04,360 --> 00:50:08,920
Even if you are gay, that's fine.
Who Cares? Okay. People are gay.

799
00:50:09,070 --> 00:50:11,140
That's humans.
But I love you too.

800
00:50:11,350 --> 00:50:16,210
It's okay for us to be able to say I
love you as men, as women, whatever.

801
00:50:16,390 --> 00:50:20,770
Okay. We are beyond gender roles by
the way. We are you guys, listen,

802
00:50:21,400 --> 00:50:24,160
we are moving beyond being a human.

803
00:50:25,300 --> 00:50:29,360
We're moving beyond these bodies,
period. So forget about gender roles.

804
00:50:29,630 --> 00:50:34,630
We are merging into this super
intelligent civilization using Ai.

805
00:50:34,731 --> 00:50:37,970
So don't even trip about like, I'm
not gay. I love you by the way.

806
00:50:38,720 --> 00:50:42,980
And lastly, um, will it be free?

807
00:50:42,981 --> 00:50:45,260
Of course it's going to be free.
Yes, it's going to be free.

808
00:50:45,710 --> 00:50:48,230
And I have so much content
coming out for you this weekend.

809
00:50:48,231 --> 00:50:51,740
It's going to blow your mind. We are
living in such an exciting time right now.

810
00:50:51,740 --> 00:50:54,770
Next week we're going to have our
first school of Ai meetups starting.

811
00:50:54,890 --> 00:50:57,620
I'm going to contact all the people
who I've selected to be deans.

812
00:50:57,740 --> 00:51:00,260
It's going to be amazing.
You guys are amazing.

813
00:51:00,380 --> 00:51:03,080
This is the most important
community in the world.

814
00:51:03,140 --> 00:51:07,490
Nobody as the community globally is
doing as much in terms of impact.

815
00:51:07,670 --> 00:51:11,450
And you know why? Because nobody's
using AI as much as we are.

816
00:51:11,560 --> 00:51:16,370
We are the most AI aware, active,
hungry, ambitious community in the world.

817
00:51:16,520 --> 00:51:19,310
And it's all thanks to you guys.
All right, so that's it for this.

818
00:51:20,300 --> 00:51:23,470
I'm not going to rap because it's kind
of laggy right now, but I will wrap,

819
00:51:23,480 --> 00:51:27,200
I promise you in the next livestream.
Okay? So lots of reps are coming.

820
00:51:27,201 --> 00:51:29,570
I love you guys and thanks for watching.

