1
00:00:00,120 --> 00:00:04,920
Wow. A big, big thank you
to the Youtube Ai Algorithm.

2
00:00:04,921 --> 00:00:06,720
Thank you. Ai. Hello world.

3
00:00:06,750 --> 00:00:11,640
It's arrived in computers are pretty good
at learning from spreadsheets of data

4
00:00:11,641 --> 00:00:13,080
filled with numbers,

5
00:00:13,290 --> 00:00:18,120
but we humans communicate
with words not with numbers.

6
00:00:18,330 --> 00:00:23,330
The subfield of AI called natural
language processing or NLP is focused on

7
00:00:23,851 --> 00:00:28,680
enabling computers to understand
and communicate in human language.

8
00:00:28,980 --> 00:00:33,630
In this video I'll cover how NLP
has progressed over the years.

9
00:00:33,631 --> 00:00:38,631
Up until 2019 they don't explain how
to use a bleeding edge model called irt

10
00:00:39,360 --> 00:00:42,870
that makes it incredibly easy for anyone,
not you,

11
00:00:43,380 --> 00:00:46,320
for anyone to build NLP apps right now.

12
00:00:46,590 --> 00:00:50,960
We'll specifically use Burke to learn
from a Dataset of Amazon reviews that

13
00:00:50,961 --> 00:00:55,170
perform text classification
and automatic summarization.

14
00:00:55,650 --> 00:01:00,600
Language is a way to represent
information and we humans interpret this

15
00:01:00,601 --> 00:01:05,280
information from strings of text by
assessing three different criteria,

16
00:01:05,550 --> 00:01:06,510
syntax,

17
00:01:06,570 --> 00:01:11,570
semantics and pragmatics basically and
review syntax describes the form of the

18
00:01:12,721 --> 00:01:15,540
language usually specified by grammar.

19
00:01:15,840 --> 00:01:20,820
Natural language is much more complicated
than the formal language is used for

20
00:01:20,821 --> 00:01:25,020
programming. There are many
rules of syntax to abide by.

21
00:01:25,200 --> 00:01:30,150
I before e except after
c with 923 exceptions.

22
00:01:30,750 --> 00:01:35,490
Semantics describes the meaning of
words or sentences of the language and

23
00:01:35,491 --> 00:01:39,420
pragmatics describes how the words
relate to the world at large.

24
00:01:39,450 --> 00:01:44,450
It's about considering their context to
understand the difference between these

25
00:01:44,821 --> 00:01:48,780
three criteria. Take a look at
the following four sentences.

26
00:01:48,960 --> 00:01:53,670
The first sentence is appropriate at the
start of an article, it's syntactically,

27
00:01:53,790 --> 00:01:56,670
semantically and pragmatically correct.

28
00:01:57,030 --> 00:02:00,930
The second sentence is syntactically
and semantically correct,

29
00:02:01,080 --> 00:02:04,080
but pragmatically it
sounds kind of whack AAF.

30
00:02:04,200 --> 00:02:06,690
The third sentence is
syntactically correct,

31
00:02:06,691 --> 00:02:11,691
but semantically incorrect and the
last sentence is incorrect on all three

32
00:02:11,731 --> 00:02:15,210
fronts. Syntactically
semantically and pragmatically.

33
00:02:15,300 --> 00:02:19,560
Computer scientists have been creating
automatic systems to attempt to do this

34
00:02:19,561 --> 00:02:21,540
for just about 60 years now,

35
00:02:21,750 --> 00:02:24,960
which makes it an incredibly
young scientific discipline.

36
00:02:25,320 --> 00:02:30,320
We can trace the history of NLP back
to 1950 when the prominent computer

37
00:02:30,511 --> 00:02:32,690
scientists and it it Cumberbatch,
I,

38
00:02:32,691 --> 00:02:37,691
I mean Alan Turing published a landmark
paper titled Computing Machinery and

39
00:02:38,101 --> 00:02:38,934
intelligence,

40
00:02:39,030 --> 00:02:44,030
which proposed what's now called the
Turing test as a criteria on of true

41
00:02:44,191 --> 00:02:45,060
intelligence.

42
00:02:45,120 --> 00:02:49,770
The question the Turing test poses is
can a computer program fool a human into

43
00:02:49,771 --> 00:02:53,820
thinking it's a human via conversation?
A few years later, Noam Chomsky,

44
00:02:53,821 --> 00:02:58,821
a prominent linguist published a book
titled Syntactic structures which detailed

45
00:02:59,020 --> 00:03:04,020
a rule based system of how to structure
a grammatically correct phrases and this

46
00:03:04,301 --> 00:03:09,301
inspired many rule based approaches to
NLP generating a sentence was usually

47
00:03:09,941 --> 00:03:14,590
done by pulling syntactic
information from a database. In fact,

48
00:03:14,620 --> 00:03:19,620
up until the 1980s most NLP systems were
based on a complex set of hand written

49
00:03:20,021 --> 00:03:25,000
rules, but in the late 1980s hair
metal became an unfortunate reality.

50
00:03:25,210 --> 00:03:26,620
On a more related note,

51
00:03:26,650 --> 00:03:31,650
a revolution in NLP occurred when
researchers started using machine learning

52
00:03:31,690 --> 00:03:36,490
algorithms for language processing
instead of rule based algorithms.

53
00:03:36,520 --> 00:03:40,780
Mostly because the increase in available
computational power allowed for this

54
00:03:40,781 --> 00:03:43,630
strategy to outperform rule based systems.

55
00:03:43,900 --> 00:03:48,760
Some of the earliest use
learning algorithms like
decision trees produce systems

56
00:03:48,761 --> 00:03:53,230
of hard if then rules similar
to existing handwritten rules,

57
00:03:53,500 --> 00:03:54,970
but as time progressed,

58
00:03:55,150 --> 00:03:59,800
researchers increasingly
favorite statistical models
which make probabilistic

59
00:03:59,801 --> 00:04:04,801
decisions as to what a word or sentence
should sound like or mean instead of

60
00:04:05,591 --> 00:04:06,940
rule based decisions.

61
00:04:07,300 --> 00:04:12,220
Nowadays and LP systems like speech
recognition software rely on such

62
00:04:12,221 --> 00:04:16,870
statistical models to predict which
words were likely spoken by a user,

63
00:04:17,020 --> 00:04:21,850
which are more reliable. Alexa, show me
a photograph. Photograph by Nico Bath.

64
00:04:21,880 --> 00:04:25,750
Know a class of statistical models
called deep neural networks.

65
00:04:25,751 --> 00:04:30,751
Have been the key driver in most of
the recent NLP successes across a wide

66
00:04:31,241 --> 00:04:33,700
variety of tasks like machine translation,

67
00:04:33,940 --> 00:04:37,150
automatic summarization
and sentiment analysis.

68
00:04:37,480 --> 00:04:40,780
And in 2019 free open
source tools like Pi,

69
00:04:40,781 --> 00:04:45,781
torch colab and various text datasets
have enabled individuals and teams from

70
00:04:46,361 --> 00:04:51,361
across the globe to create powerful
applications that use NLP to solve real

71
00:04:52,031 --> 00:04:53,800
world problems.
For example,

72
00:04:53,920 --> 00:04:58,120
clever who is a finished startup providing
an instance site search solution for

73
00:04:58,150 --> 00:04:59,350
ecommerce stores.

74
00:04:59,560 --> 00:05:03,850
They're using text classification to
provide relevant search results for

75
00:05:03,851 --> 00:05:07,060
shoppers and actionable
insights for store owners.

76
00:05:07,180 --> 00:05:11,590
Another startup called English central
aims to make learning English much more

77
00:05:11,591 --> 00:05:16,300
fun by giving users instant feedback
on their pronunciation using speech

78
00:05:16,301 --> 00:05:17,590
recognition techniques.

79
00:05:17,830 --> 00:05:22,330
Yummly is building a platform for
recipe recommendations and search.

80
00:05:22,600 --> 00:05:24,640
They use NLP to understand,

81
00:05:24,820 --> 00:05:29,820
analyze and connect users with the
recipes they most enjoy old on cowboy or

82
00:05:30,371 --> 00:05:31,000
cowgirl.

83
00:05:31,000 --> 00:05:35,320
Before you go build an NLP startup
immediately you need to understand one

84
00:05:35,321 --> 00:05:37,390
concept really well,
Burt,

85
00:05:37,660 --> 00:05:42,660
which stands for bi-directional and
coder representations from transformers.

86
00:05:42,910 --> 00:05:45,640
I'll explain what each of
those words mean in a second.

87
00:05:45,910 --> 00:05:50,830
Bert is a fully trained language model
that Google released just a few months

88
00:05:50,830 --> 00:05:55,660
ago and it's been the most significant
breakthrough in NLP thus far.

89
00:05:56,200 --> 00:06:00,680
A language model is able to learn the
probability of word occurrence based on

90
00:06:00,681 --> 00:06:03,170
examples of text.
Traditionally,

91
00:06:03,320 --> 00:06:08,120
language models are trained by using the
previous end words to predict the next

92
00:06:08,121 --> 00:06:08,660
one,

93
00:06:08,660 --> 00:06:13,660
but ERT is a language model
that was trained by using
both the previous and next

94
00:06:14,211 --> 00:06:18,380
words when making predictions,
hence the word bi directional.

95
00:06:18,381 --> 00:06:20,240
Instead of unit directional.

96
00:06:20,720 --> 00:06:25,720
Burt was used to establish a new state
of the art in 11 and LP tasks including

97
00:06:25,911 --> 00:06:29,870
question, answering sentiment
analysis and automatic summarization.

98
00:06:30,290 --> 00:06:33,440
All of these tasks involve
a two step process,

99
00:06:33,620 --> 00:06:36,650
train a deep language
model on some text data,

100
00:06:36,980 --> 00:06:40,910
then give those representations
to a task specific model.

101
00:06:41,210 --> 00:06:43,130
For the first step of this process,

102
00:06:43,131 --> 00:06:47,450
the Goto technique for the past few years
is called word to Vec and it creates

103
00:06:47,451 --> 00:06:50,600
word representations
also called word vectors.

104
00:06:50,930 --> 00:06:55,340
It maps each word in the training
Dataset to a vector that represents some

105
00:06:55,341 --> 00:06:57,680
aspect of its meaning.
So for example,

106
00:06:57,710 --> 00:07:02,710
the word vector for teen would include
information about state and gender.

107
00:07:02,870 --> 00:07:07,070
These representations are generally
trained on large unlabeled data like a

108
00:07:07,071 --> 00:07:08,090
Wikipedia dump.

109
00:07:08,240 --> 00:07:12,980
Then use a train models on labeled
data for tasks like sentiment analysis.

110
00:07:13,310 --> 00:07:17,630
This allows models to leverage linguistic
data learned from larger datasets.

111
00:07:17,660 --> 00:07:22,160
The problem with word two VEC and similar
word vector techniques was that they

112
00:07:22,161 --> 00:07:25,940
didn't take context into account.
The Word Bank, for example,

113
00:07:25,970 --> 00:07:30,290
would have a different meaning
depending on the context it was used in.

114
00:07:30,620 --> 00:07:34,550
They have trouble capturing the
meaning of combinations of words.

115
00:07:34,880 --> 00:07:39,880
These limitations motivated the use of
recurrent networks as language models.

116
00:07:39,920 --> 00:07:43,850
Instead, instead of training a model
to map a single vector for each word,

117
00:07:43,970 --> 00:07:48,500
if these techniques train a neural network
to map a vector to each word based on

118
00:07:48,501 --> 00:07:52,370
the entire surrounding context.
Nowadays,

119
00:07:52,371 --> 00:07:55,190
the transformer,
a newer type of neural network,

120
00:07:55,310 --> 00:08:00,050
has eclipsed all variations of recurrent
networks for language modeling.

121
00:08:00,440 --> 00:08:04,370
A transformer consists of an encoder
network and a decoder network.

122
00:08:04,370 --> 00:08:09,370
So the phrase Bert means
using a transformer network
to create by Directional

123
00:08:10,570 --> 00:08:12,740
encoder representations.

124
00:08:13,040 --> 00:08:18,040
These representations can then be fed
into another model for some specific NLP

125
00:08:18,141 --> 00:08:20,570
task.
Unlike recurrent networks,

126
00:08:20,600 --> 00:08:24,920
transformer networks like Bert don't
use recurrent connections at all.

127
00:08:25,220 --> 00:08:29,120
They instead use attention over
the word sequence. Instead,

128
00:08:29,450 --> 00:08:34,450
attention is defined in neuroscience as
the ability to selectively concentrates

129
00:08:35,270 --> 00:08:39,260
on one aspect of the environment
while ignoring the rest.

130
00:08:39,560 --> 00:08:43,970
In deep learning, we mimic this concept
through the use of attention mechanisms.

131
00:08:44,330 --> 00:08:49,330
One way of doing this is to encode an
input sequence into not a single fixed

132
00:08:49,551 --> 00:08:51,320
vector,
but instead have a model.

133
00:08:51,380 --> 00:08:56,280
Learn how to generate a vector for each
output time step by adding an additional

134
00:08:56,281 --> 00:08:58,770
set of weights that
will later be optimized.

135
00:08:58,950 --> 00:09:01,590
So it doesn't just learn what to output.

136
00:09:01,770 --> 00:09:05,940
It learns how to selectively way parts
of the input data to maximize the

137
00:09:05,941 --> 00:09:07,980
likelihood of the proper output.

138
00:09:08,010 --> 00:09:12,930
Burt is composed of several attention
blocks to prevent it from having ADHD like

139
00:09:12,931 --> 00:09:17,010
I do. Each block transforms the
input using matrix operations.

140
00:09:17,190 --> 00:09:19,530
If we input as sequence of n words,

141
00:09:19,620 --> 00:09:22,860
the encoder will output
a sequence of tensors.

142
00:09:22,890 --> 00:09:26,610
These tensors are used by the decoder
to output a sequence of words.

143
00:09:26,820 --> 00:09:31,140
The architecture is optimized
using gradient descent
linked to how that works in

144
00:09:31,141 --> 00:09:32,100
the video description.

145
00:09:32,250 --> 00:09:36,420
The great thing about Bert is that it
comes fully trained out of the box.

146
00:09:36,540 --> 00:09:41,220
It took Google four days using several
cloud TPU use to train it on several

147
00:09:41,221 --> 00:09:43,350
languages.
So thank you Google.

148
00:09:43,470 --> 00:09:48,300
I guess all we need to do is fine tune
the final layer of Burt on our own

149
00:09:48,301 --> 00:09:52,500
training Dataset for whatever task we
choose and it will benefit from Burt's

150
00:09:52,501 --> 00:09:56,970
existing knowledge. So let's look
at our data set of Amazon reviews,

151
00:09:57,120 --> 00:10:01,740
which we'll use for two
tasks. Text classification
and automatic summarization.

152
00:10:01,800 --> 00:10:05,790
I text classification. We're talking
about classifying chunks of text.

153
00:10:05,970 --> 00:10:10,440
That could be anywhere from sentence
size to an entire paragraph in length as

154
00:10:10,441 --> 00:10:12,720
either a good review or a bad review.

155
00:10:12,930 --> 00:10:15,570
We'll first clone Bert
into our environment.

156
00:10:15,750 --> 00:10:18,090
Then we'll download
the Burton model files.

157
00:10:18,100 --> 00:10:22,110
These are wait values that represent
what it's learned from pre-training.

158
00:10:22,380 --> 00:10:26,610
Then we'll need to preprocess our
data into a format that Bert expects.

159
00:10:26,850 --> 00:10:28,770
Column One will be a row ID.

160
00:10:28,980 --> 00:10:31,890
Column Two is the label
for the route as an ENT.

161
00:10:32,130 --> 00:10:34,920
Column three is a column
of all the same letter.

162
00:10:34,950 --> 00:10:39,810
It's a throwaway column that we need
to include because Burt expects it WTF,

163
00:10:39,811 --> 00:10:44,340
right? Just roll with it and the text
examples will be in the last column,

164
00:10:44,610 --> 00:10:46,110
the ones we want to classify.

165
00:10:46,320 --> 00:10:49,500
We can do all this easily with
the pandas python library.

166
00:10:49,740 --> 00:10:53,700
Once we formatted our data, we can run
training. Once it's finished training,

167
00:10:53,701 --> 00:10:57,990
we'll use it to predict on new text data
by selecting the newly trained weights

168
00:10:57,991 --> 00:11:02,790
file as input, as well as some test review
and it will output a classification,

169
00:11:02,940 --> 00:11:04,500
either a good or bad review.

170
00:11:04,980 --> 00:11:08,490
Now if we want to perform
automatic summarization,

171
00:11:08,640 --> 00:11:13,590
we can use the same learned embeddings
well firstly to cluster them then extract

172
00:11:13,591 --> 00:11:17,730
one sentence from each cluster.
This is an unsupervised technique.

173
00:11:17,940 --> 00:11:21,690
These embeddings will be clustered in
high dimensional vector space where the

174
00:11:21,691 --> 00:11:26,040
number of clusters is equal to the
desired number of sentences we want.

175
00:11:26,041 --> 00:11:26,874
In the summary,

176
00:11:27,150 --> 00:11:31,650
each cluster of sentence embeddings can
be interpreted as a set of semantically

177
00:11:31,651 --> 00:11:36,651
similar sentences whose meaning can be
expressed by just one candidate sentence.

178
00:11:36,690 --> 00:11:37,500
In the summary,

179
00:11:37,500 --> 00:11:40,710
this candidate sentence is selected
to be the sentence who's vector

180
00:11:40,711 --> 00:11:43,860
representation is closest
to the cluster center.

181
00:11:44,100 --> 00:11:47,010
We then order the candidate
sentences to form a summary.

182
00:11:47,190 --> 00:11:51,330
This order is determined by the position
of the sentences in their related

183
00:11:51,360 --> 00:11:55,000
clusters. This technique is
considered extractive summarization.

184
00:11:55,960 --> 00:11:59,830
Amazing, right? There are three
things to remember from this video.

185
00:12:00,070 --> 00:12:05,070
Natural language processing is the study
of computational techniques to help

186
00:12:05,171 --> 00:12:09,430
computers understand and
communicate in human languages.

187
00:12:09,760 --> 00:12:11,110
Google's Burt's model.

188
00:12:11,111 --> 00:12:16,111
It makes it easy for anyone to create
an NLP application greatly reducing the

189
00:12:16,661 --> 00:12:20,110
amount of training, time,
data and compute necessary.

190
00:12:20,320 --> 00:12:25,320
And we can perform NLP tasks like text
summarization and text classification

191
00:12:25,600 --> 00:12:28,180
using birth. What do you
want to do with NLP? Next,

192
00:12:28,181 --> 00:12:31,540
let me know in the comments section and
please subscribe for more programming

193
00:12:31,541 --> 00:12:35,950
videos. For now, I've got to analyze
some texts, so thanks for watching.

