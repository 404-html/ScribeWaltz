1
00:00:00,150 --> 00:00:03,540
Hello world,
it's Saroj and drones are a lot of fun.

2
00:00:03,780 --> 00:00:08,730
We're going to train a drone
in a simulated environment
to be able to fly to any

3
00:00:08,731 --> 00:00:13,380
location. We give it autonomously
avoiding numerous obstacles.

4
00:00:13,381 --> 00:00:16,110
It's never seen before.
Along the way,

5
00:00:16,470 --> 00:00:20,280
we can think of drones as
aerial data capture devices.

6
00:00:20,460 --> 00:00:24,120
Each one can potentially hold
a huge amount of sensors,

7
00:00:24,330 --> 00:00:29,330
although right now most of them just use
a camera and if we connect a drone to a

8
00:00:30,270 --> 00:00:32,610
data storage and analytics pipeline,

9
00:00:32,820 --> 00:00:36,720
we can solve specific problems
using the data it's captured.

10
00:00:37,140 --> 00:00:37,973
For example,

11
00:00:38,010 --> 00:00:43,010
a startup called sky specs is using them
to identify anomalies for wind farms.

12
00:00:43,440 --> 00:00:46,800
When a wind farm has motors that
are running hotter than they should,

13
00:00:47,100 --> 00:00:52,100
they're drones are used to help monitor
the humidity and weather conditions to

14
00:00:52,231 --> 00:00:56,370
more quickly identify when
a problem will arise. Also,

15
00:00:56,400 --> 00:00:58,770
Cape is playing a vital role for mining.

16
00:00:58,770 --> 00:01:02,940
Companies are providing volume metric
mappings that help them measure their

17
00:01:02,941 --> 00:01:07,941
stockpiles with their high resolution
imagery fed into an image processing

18
00:01:08,461 --> 00:01:09,294
pipeline.

19
00:01:09,480 --> 00:01:13,800
The accuracy of asset measurement
can increase by up to 20 fold.

20
00:01:13,980 --> 00:01:18,090
It's these type of measurements that
help companies transition from a discrete

21
00:01:18,240 --> 00:01:20,400
to continuous forecasting cycle.

22
00:01:20,730 --> 00:01:24,270
Forecast don't have to be made in
intervals. When you have a drone,

23
00:01:24,420 --> 00:01:26,580
it can be an ongoing process.

24
00:01:26,820 --> 00:01:31,320
So before we start talking about the
theory behind how we're going to train our

25
00:01:31,321 --> 00:01:33,120
drone using machine learning,

26
00:01:33,420 --> 00:01:38,420
let's take a look at this simulated
environment called the Ross development

27
00:01:38,520 --> 00:01:39,353
studio.

28
00:01:39,570 --> 00:01:44,570
Ross or the robot operating system is a
framework for writing robots software.

29
00:01:45,360 --> 00:01:49,380
It's a collection of tools that make
the process of architecting our robot's

30
00:01:49,381 --> 00:01:51,150
behavior much easier.

31
00:01:51,420 --> 00:01:54,930
It's goal is to enable code
reusability in robotics,

32
00:01:54,931 --> 00:01:56,730
research and development.

33
00:01:57,120 --> 00:02:02,120
Robots can have all sorts of hardware
components from arms two legs,

34
00:02:02,610 --> 00:02:06,060
two can openers to seven
different types of sensors,

35
00:02:06,090 --> 00:02:08,520
all performing different
tasks at the same time.

36
00:02:08,850 --> 00:02:13,110
And while all of these hardware modules
might be performing different tasks,

37
00:02:13,350 --> 00:02:15,720
they all still have to talk to each other,

38
00:02:15,721 --> 00:02:18,630
which can get complicated to architect.

39
00:02:18,930 --> 00:02:23,790
What Ross does is it lets us abstract
each of these components as individual

40
00:02:23,791 --> 00:02:27,780
nodes and once we have our
drivers installed we can use,

41
00:02:27,781 --> 00:02:32,781
it's built in communications layer to
create a cohesive integrated system with

42
00:02:33,871 --> 00:02:35,880
concurrency support enabled.

43
00:02:36,360 --> 00:02:40,740
The Ross runtime graph is a peer to
peer network of processes that are

44
00:02:40,741 --> 00:02:45,450
potentially distributed across machines
coupled together using the Ross

45
00:02:45,451 --> 00:02:47,280
communication infrastructure.

46
00:02:47,460 --> 00:02:51,540
The Ross Development Studio
is an in browser environment,

47
00:02:51,600 --> 00:02:55,890
meaning we don't have to install
anything on our computers to use it.

48
00:02:56,310 --> 00:02:58,920
We just had to visit
the link in our browser.

49
00:02:59,170 --> 00:03:03,460
Once there we can create a new
project and name it something unique,

50
00:03:03,790 --> 00:03:08,020
then we can open the project by
clicking on the open project button.

51
00:03:08,350 --> 00:03:10,420
Once we have the environment open,

52
00:03:10,480 --> 00:03:13,660
we can go to the tools menu
and open a Linux shell.

53
00:03:13,930 --> 00:03:16,840
Inside the shell will go
to the source directory.

54
00:03:17,080 --> 00:03:22,080
This is the place where Ross code has
to be put into the rds in order to build

55
00:03:22,271 --> 00:03:26,350
tests the bug and execute it
against robots simulations.

56
00:03:26,650 --> 00:03:30,520
We can pull our code directly from get
hub right here if we wanted to to train

57
00:03:30,521 --> 00:03:35,320
our drone and then watch the parrot drone
simulation under the simulations menu.

58
00:03:35,740 --> 00:03:36,281
Finally,

59
00:03:36,281 --> 00:03:40,420
we can launch our package so it will
start training the parrot drone right from

60
00:03:40,421 --> 00:03:45,040
the shell using the Ross run command.
It's going to start moving and wild ways,

61
00:03:45,041 --> 00:03:46,150
but that makes sense.

62
00:03:46,360 --> 00:03:51,360
What our robot is doing is learning
using an algorithm called deep the

63
00:03:51,450 --> 00:03:56,080
terministic policy gradients.
Sounds complicated. I know.

64
00:03:56,081 --> 00:03:58,480
So let's take a step back.
Traditionally,

65
00:03:58,481 --> 00:04:03,481
the class of algorithms
considered reinforcement
learning only really worked in

66
00:04:04,121 --> 00:04:06,580
tiny disc retired grid worlds,

67
00:04:06,850 --> 00:04:11,710
and that stopped them from
gaining credibility as being
viable real world tools

68
00:04:12,070 --> 00:04:15,910
for context. Discrete data
can take only integer values,

69
00:04:15,911 --> 00:04:19,840
whereas as continuous data can
take on any value for games,

70
00:04:19,900 --> 00:04:24,820
that difference boils down to either
a few set of possible actions or many.

71
00:04:25,240 --> 00:04:29,710
When deep mind released
their deep reinforcement
learning algorithm called deep

72
00:04:29,711 --> 00:04:31,570
to the Ai community,

73
00:04:31,571 --> 00:04:36,571
saw that deep learning methods could be
used to solve high dimensional problems,

74
00:04:37,090 --> 00:04:38,380
powerful stuff.

75
00:04:38,410 --> 00:04:42,880
Perhaps it could also be used to
deal with continuous action spaces.

76
00:04:43,210 --> 00:04:48,210
Learning and continuous action spaces has
been a huge obstacle for AI developers

77
00:04:48,611 --> 00:04:53,611
and the most interesting real
world problems in robotics
control fall into this

78
00:04:54,011 --> 00:04:59,011
category if we disc retired our continuous
action space to finally we'll end up

79
00:04:59,981 --> 00:05:02,560
with what's called the
curse of dimensionality,

80
00:05:02,770 --> 00:05:06,160
which basically states that the
more dimensions you work with,

81
00:05:06,340 --> 00:05:09,310
the less affective
statistical techniques become.

82
00:05:09,730 --> 00:05:11,380
But on the flip side,

83
00:05:11,440 --> 00:05:16,440
a naive discretization of our action
space ignores valuable information about

84
00:05:17,351 --> 00:05:19,450
the geometry of our action domain.

85
00:05:19,810 --> 00:05:24,810
So deep mind created an
algorithm specifically to
solve this continuous action

86
00:05:25,871 --> 00:05:30,730
space problem called deep
deterministic policy gradients.

87
00:05:30,820 --> 00:05:35,680
It uses a mixture of two techniques,
policy gradients, and actor critic.

88
00:05:35,890 --> 00:05:37,330
Let's take a look at how both work.

89
00:05:37,331 --> 00:05:40,690
First policy is a mapping
of states to actions.

90
00:05:40,780 --> 00:05:45,760
It's how an agent decides how to interact
with an environment and an optimal

91
00:05:45,761 --> 00:05:47,590
policy can be learned.

92
00:05:48,070 --> 00:05:53,070
Policy Gradient Algorithms are a set of
techniques that optimize a policy end to

93
00:05:53,591 --> 00:05:58,591
end by computing noisy estimates of the
gradient of the expected reward of the

94
00:05:59,001 --> 00:06:02,630
policy. Then updating the policy
in the gradient direction.

95
00:06:03,110 --> 00:06:07,760
The gradient is a signal that we derive
using calculus that tells our learning

96
00:06:07,761 --> 00:06:11,450
agent how best to update its
knowledge of the environment.

97
00:06:11,900 --> 00:06:16,790
The algorithm will observe many training
examples of high rewards from good

98
00:06:16,820 --> 00:06:20,300
actions,
negative rewards for bad actions,

99
00:06:20,301 --> 00:06:25,160
and it'll slowly increased the
probability of the good actions over time.

100
00:06:25,670 --> 00:06:26,750
But in practice,

101
00:06:26,780 --> 00:06:30,890
getting one reward signal at the
end of a long episode of environment

102
00:06:30,891 --> 00:06:35,891
interaction makes it difficult to a
certain exactly which action was the good

103
00:06:35,931 --> 00:06:40,610
one. This is popularly known as
the credit assignment problem.

104
00:06:40,940 --> 00:06:45,800
One way to solve the credit assignment
problem is to use an actor critic

105
00:06:45,830 --> 00:06:46,663
algorithm.

106
00:06:47,060 --> 00:06:52,060
Actor critic algorithms
are used to represent the
policy function independently

107
00:06:53,030 --> 00:06:54,050
of the value function.

108
00:06:54,560 --> 00:06:58,760
The policy function structure is known
as the actor and the value function

109
00:06:58,761 --> 00:07:00,560
structure is called the critic.

110
00:07:00,980 --> 00:07:05,980
The actor produces an action given the
current environment state and the critic

111
00:07:06,410 --> 00:07:10,280
produces an error signal given the
state and results into a reward.

112
00:07:10,670 --> 00:07:15,670
If the critic is estimating the quality
or Q function that assigns values to

113
00:07:15,771 --> 00:07:19,550
possible actions,
it will also need the output of the actor.

114
00:07:19,880 --> 00:07:23,960
The output of the critic drives
learning in both actor and critic.

115
00:07:24,350 --> 00:07:27,110
We can use neural networks
to represent both of them.

116
00:07:27,440 --> 00:07:32,120
So the deep deterministic policy
gradients algorithm combines both of these

117
00:07:32,121 --> 00:07:32,954
techniques.

118
00:07:33,080 --> 00:07:38,080
It's a policy graded algorithm that
uses a stochastic behavior policy for

119
00:07:38,421 --> 00:07:39,350
exploration,

120
00:07:39,440 --> 00:07:44,440
but estimates a deterministic
target policy which is
easier to learn for context

121
00:07:44,570 --> 00:07:46,010
in deterministic models.

122
00:07:46,160 --> 00:07:50,780
The output of the model is fully
determined by the parameter values and the

123
00:07:50,781 --> 00:07:55,760
initial conditions. The castic models
possessed some inherent randomness.

124
00:07:56,150 --> 00:08:01,150
DDP PG uses a form of policy iteration
in which it evaluates the policy then

125
00:08:01,611 --> 00:08:04,640
follows the policy gradient
to improve performance.

126
00:08:05,030 --> 00:08:09,380
It's also an actor critic algorithm
in that it uses to neural networks.

127
00:08:09,590 --> 00:08:13,790
They compute action predictions for
the current state and generate an error

128
00:08:13,791 --> 00:08:15,410
signal at each time step.

129
00:08:15,740 --> 00:08:20,510
The input of the actor network is the
current state and the output is a scalar

130
00:08:20,511 --> 00:08:25,370
value representing an action chosen
from a continuous action space.

131
00:08:25,670 --> 00:08:30,500
So the critic output is the estimated
Q value of the current state and the

132
00:08:30,501 --> 00:08:32,270
action given by the actor.

133
00:08:32,600 --> 00:08:35,810
A specific theorem called
the deterministic policy.

134
00:08:35,811 --> 00:08:40,730
Gradient theorem is used as the update
rule for the weights of the actor network.

135
00:08:41,030 --> 00:08:45,740
The critic network is updated from the
gradients obtained from the error signal.

136
00:08:46,070 --> 00:08:51,070
There were two tricks on top of that that
the team at deep mind used to improve

137
00:08:51,651 --> 00:08:52,520
convergence.

138
00:08:52,880 --> 00:08:57,880
The first is using an experience buffer
using a replay buffer to store the

139
00:08:58,441 --> 00:09:00,930
experiences of an agent during training.

140
00:09:01,110 --> 00:09:06,110
Then randomly sampling from it while
learning has proven to be beneficial and

141
00:09:06,211 --> 00:09:10,940
the second is using a set of target
networks are using a set of target and

142
00:09:10,941 --> 00:09:14,280
networks to generate the targets
for our error computation.

143
00:09:14,580 --> 00:09:18,000
We regularize our learning
algorithm and increased stability.

144
00:09:18,150 --> 00:09:22,980
We can create a custom environment class
that uses Ross function calls to define

145
00:09:22,981 --> 00:09:24,660
our robot and its environment.

146
00:09:24,930 --> 00:09:28,920
We can start by registering our
environment into the pool of available

147
00:09:28,921 --> 00:09:31,800
environments of Open Ai's gym library.

148
00:09:32,100 --> 00:09:36,450
Then we can get the
configuration parameters
directly from the Ross parameter

149
00:09:36,451 --> 00:09:40,290
server and connect it to the
simulation called Gazebo.

150
00:09:40,740 --> 00:09:43,470
The main function is the step function.

151
00:09:43,530 --> 00:09:45,900
It's called during each training loop.

152
00:09:46,260 --> 00:09:50,550
This function receives as a parameter
the actions selected by the learning

153
00:09:50,610 --> 00:09:55,320
algorithm. This parameter is just
the number of the action selected,

154
00:09:55,350 --> 00:09:57,330
not the actual action.

155
00:09:57,570 --> 00:10:01,410
Our Algorithm won't know which
actions we have for this task.

156
00:10:01,411 --> 00:10:06,411
It only knows the number of
actions available and picks
one of them based on its

157
00:10:06,901 --> 00:10:08,490
current learning statics.

158
00:10:08,730 --> 00:10:13,440
We'll convert that number into the
actual action for the robot ourselves.

159
00:10:13,500 --> 00:10:14,333
After that,

160
00:10:14,700 --> 00:10:19,470
then we send that action to the robot
through a series of steps as follows,

161
00:10:19,890 --> 00:10:22,740
unpause this simulator,
send the command,

162
00:10:22,980 --> 00:10:25,560
wait for the execution of said command,

163
00:10:25,800 --> 00:10:29,550
observe the state of the environment
and again, pause the simulator.

164
00:10:29,610 --> 00:10:33,120
Then we processed the current states,
calculate the reward,

165
00:10:33,240 --> 00:10:34,590
return the current state,

166
00:10:34,710 --> 00:10:39,120
return the reward and return a boolean
value indicating whether we're done or

167
00:10:39,121 --> 00:10:42,390
not, meaning we completed
our objective. That's it.

168
00:10:42,540 --> 00:10:45,810
When we test our robot out
in the simulated environment,

169
00:10:45,990 --> 00:10:49,230
we'll see that at first it's
very sloppy in its movements,

170
00:10:49,231 --> 00:10:53,220
but eventually it gets really good
at navigating our environments.

171
00:10:53,250 --> 00:10:55,950
No matter how many obstacles it faces,

172
00:10:56,190 --> 00:11:00,930
we can then take this code and run it
on a real drone as some developers have

173
00:11:00,931 --> 00:11:04,230
and eventually apply it
to a real world problem.

174
00:11:04,350 --> 00:11:06,570
Three things to remember from this video.

175
00:11:06,690 --> 00:11:11,690
The robot operating system allows for
code reusability amongst roboticists and

176
00:11:12,721 --> 00:11:17,610
is currently the best free framework
out there to architect a robotic system.

177
00:11:17,970 --> 00:11:22,590
We can train a drone to navigate an
environment using the deep deterministic

178
00:11:22,650 --> 00:11:24,980
policy gradients,
algorithm or D.

179
00:11:24,990 --> 00:11:29,990
DPG and DDG is a mixture of policy
gradients and actor critic able to solve

180
00:11:31,201 --> 00:11:36,060
tasks in continuous action spaces. Please
subscribe for more programming videos.

181
00:11:36,061 --> 00:11:39,120
And for now, I've got a
fly, so thanks for watching.

