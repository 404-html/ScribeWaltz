1
00:00:00,030 --> 00:00:00,751
Hello world.

2
00:00:00,751 --> 00:00:05,580
It's the Raj and there are so many
different neural architectures that I feel

3
00:00:05,581 --> 00:00:10,320
like I could just talk about them all
day on Youtube. Dreams do come true.

4
00:00:10,830 --> 00:00:15,630
An auto encoder is a neural network
that's not just really useful for a lot of

5
00:00:15,631 --> 00:00:16,464
tasks.

6
00:00:16,560 --> 00:00:21,360
It's also an easy entry point to learn
more complex concepts in machine learning.

7
00:00:21,750 --> 00:00:26,550
Let's go over some theory and code in
order to grasp this model capable of

8
00:00:26,551 --> 00:00:30,750
everything from image colorization
to dialogue generation fully.

9
00:00:31,140 --> 00:00:35,790
If we've got data that's properly
labeled B that images or audio or text,

10
00:00:35,820 --> 00:00:39,870
we're in luck. Deep learning works
really well with labeled datasets.

11
00:00:40,050 --> 00:00:43,770
That's because there is always a
function that represents the relationship

12
00:00:43,890 --> 00:00:45,330
between both columns.

13
00:00:45,570 --> 00:00:49,140
It's easy to conceptualize
this if our dataset is numeric,

14
00:00:49,320 --> 00:00:54,120
like if our input data was a bunch of
numbers and the labels defined whether or

15
00:00:54,121 --> 00:00:57,840
not that input data was an
even number or an odd number,

16
00:00:58,140 --> 00:01:02,670
the function that represents the
relationship between these two columns is

17
00:01:02,700 --> 00:01:07,560
simple. If the input data is divisible
by two, the number is even Ellis.

18
00:01:07,561 --> 00:01:08,370
It's odd.

19
00:01:08,370 --> 00:01:13,370
All data types be that video or text can
be represented numerically and as such,

20
00:01:13,740 --> 00:01:16,410
there is always a function
that maps the relationship.

21
00:01:16,650 --> 00:01:20,340
It's just a more complex function
than the one we just discussed.

22
00:01:20,490 --> 00:01:24,420
So while it's kind of incredible that
we can speak to our computers now and

23
00:01:24,421 --> 00:01:27,180
they're able to transcribe
what we are saying be,

24
00:01:27,181 --> 00:01:31,440
that's Siri or Alexa or Google now.
Okay Google,

25
00:01:31,441 --> 00:01:34,200
do you love me? Ha ha ha ha. No.

26
00:01:34,590 --> 00:01:39,450
Speech recognition is just the result
of deep learning on labeled datasets.

27
00:01:39,660 --> 00:01:43,410
If a team of developers is trying to
create a speech recognition engine,

28
00:01:43,680 --> 00:01:48,330
they use a Dataset of audio clips
with their transcripts as the labels.

29
00:01:48,630 --> 00:01:53,310
Every single bite of the audio can be
broken down into a series of numbers and

30
00:01:53,311 --> 00:01:54,930
so can the text transcripts.

31
00:01:55,170 --> 00:01:59,640
Some combination of operations will
convert the input to the labels and that

32
00:01:59,641 --> 00:02:01,320
combination is the function.

33
00:02:01,740 --> 00:02:06,660
Neural networks can slowly approximate
or get closer and closer to this function

34
00:02:06,870 --> 00:02:11,130
through an iterative optimization
process. Also called training. In short,

35
00:02:11,131 --> 00:02:15,990
it's minimizing an error value at every
iteration so that given a novel audio

36
00:02:15,991 --> 00:02:19,980
clip, it can easily predict what
the transcript for it would be.

37
00:02:20,400 --> 00:02:24,540
Deep learning is essentially
performing a to B mappings. That's it.

38
00:02:24,990 --> 00:02:29,990
A more accurate way to say this is
that it's performing universal function

39
00:02:30,090 --> 00:02:31,140
approximation,

40
00:02:31,470 --> 00:02:35,750
meaning with sufficient data it can
approximate any function input alone

41
00:02:35,820 --> 00:02:39,240
application output, the likelihood
a customer, we'll repay it,

42
00:02:39,660 --> 00:02:42,780
input an email and I'll put the
possibility it's spam or not.

43
00:02:42,781 --> 00:02:45,660
Spam input usage patterns
for a fleet of cars,

44
00:02:45,661 --> 00:02:50,550
an output where to send a car next
like the dumpster if it was made by GM.

45
00:02:50,880 --> 00:02:54,510
Since there are an endless
array of applications for this,

46
00:02:54,750 --> 00:02:56,820
deep learning has gotten really popular,

47
00:02:57,120 --> 00:03:01,330
but while deep learning is good at
finding a function we don't already know,

48
00:03:01,360 --> 00:03:03,530
but have training data for it,

49
00:03:03,531 --> 00:03:08,080
surprisingly useful to find a function
we already know and then look at how we

50
00:03:08,081 --> 00:03:12,310
found it. All neural networks
are composite functions.

51
00:03:12,550 --> 00:03:17,020
That means they are functions of
functions. The more layers a network has,

52
00:03:17,140 --> 00:03:20,650
the more nested functions it
has for a three layer network.

53
00:03:20,740 --> 00:03:23,500
We'd multiply the input by
the first weight matrix,

54
00:03:23,620 --> 00:03:28,210
apply an activation function to it and
repeat the process. Once again, this time,

55
00:03:28,211 --> 00:03:31,300
using the output as our
new input input times wait,

56
00:03:31,390 --> 00:03:33,640
activates the result is our output.

57
00:03:33,880 --> 00:03:38,320
This can be represented as a composite
function since we're using the output of

58
00:03:38,321 --> 00:03:41,470
the first function as
input to the next function.

59
00:03:41,770 --> 00:03:44,800
But let's say our goal
wasn't to find a label. Why,

60
00:03:44,950 --> 00:03:47,830
but instead to reconstruct
the original input x,

61
00:03:48,100 --> 00:03:51,820
meaning if our input was an array
consisting of a few numbers,

62
00:03:52,060 --> 00:03:56,530
our network should output that same
input with those same exact numbers.

63
00:03:56,620 --> 00:03:59,230
After applying the series
of operations to it,

64
00:03:59,590 --> 00:04:03,340
we can call the first part of the network
that compresses the input into fewer

65
00:04:03,341 --> 00:04:05,020
bits.
The encoder,

66
00:04:05,260 --> 00:04:09,380
and we can call the second part that
reconstructs the image that decode is.

67
00:04:09,390 --> 00:04:14,080
So why should we care about doing this?
Well, we don't care about the output.

68
00:04:14,110 --> 00:04:18,160
It's just a replica of the input.
What we care about is the hidden layer.

69
00:04:18,580 --> 00:04:21,610
Once a network can reliably
reconstruct its input,

70
00:04:21,850 --> 00:04:26,200
the hidden layer must contain enough
information to represent the output.

71
00:04:26,500 --> 00:04:31,390
If as is typical, the hidden layer is
smaller than the input and output layers.

72
00:04:31,660 --> 00:04:36,010
What it represents is the same
information in a lower density.

73
00:04:36,310 --> 00:04:39,820
It's a much more dense
representation of the input data.

74
00:04:40,030 --> 00:04:41,740
One that is learned over time,

75
00:04:42,190 --> 00:04:46,510
although it turns out that there are
better techniques for data compression.

76
00:04:46,810 --> 00:04:51,520
Auto encoders are still really useful
for some tasks like dimensionality

77
00:04:51,521 --> 00:04:52,354
reduction.

78
00:04:52,510 --> 00:04:56,920
Once we have a more condensed
representation of some
multidimensional data,

79
00:04:57,160 --> 00:05:01,960
we can easily visualize it and just two
or three dimensions for further analysis.

80
00:05:02,260 --> 00:05:04,300
We can also use it for classification.

81
00:05:04,570 --> 00:05:09,100
The idea is that we trained an autoencoder
to reconstruct its instances of a

82
00:05:09,101 --> 00:05:13,660
particular class. We don't train it
on any instances of any other class.

83
00:05:14,020 --> 00:05:15,700
Then to classify new instances,

84
00:05:15,850 --> 00:05:18,700
we feed them to the auto
encoder at the input layer,

85
00:05:18,930 --> 00:05:23,500
get a reconstruction in the output layer
and compute the reconstruction error,

86
00:05:23,620 --> 00:05:28,210
which is usually a measure of distance
between the reconstruction and the input.

87
00:05:28,240 --> 00:05:31,570
If it generalizes to a new instance
and reconstructs it properly,

88
00:05:31,780 --> 00:05:35,410
then it's likely to be of the same class
as the incidence it used to train on.

89
00:05:35,860 --> 00:05:37,930
Anomaly detection is another use case.

90
00:05:38,020 --> 00:05:41,860
We first trained it on normal instances
so that if we feed it any anomalies,

91
00:05:41,980 --> 00:05:43,210
there'll be detected easily.

92
00:05:43,510 --> 00:05:47,140
If we train it to recognize anomaly
instances in our training set,

93
00:05:47,410 --> 00:05:49,840
it would only find the ones
that look like anomalies.

94
00:05:49,840 --> 00:05:52,360
It's already seen in many cases.

95
00:05:52,361 --> 00:05:55,410
We have very few anomalies
in our training that set,

96
00:05:55,810 --> 00:05:58,520
but when using an auto encoder,
this isn't a problem.

97
00:05:58,790 --> 00:06:03,350
So auto encoders are just neural networks
where the target output is the input.

98
00:06:03,620 --> 00:06:05,630
We don't actually need any new code.

99
00:06:05,900 --> 00:06:08,990
If we're just using a super simple
psychic learn like interface.

100
00:06:09,140 --> 00:06:12,170
We'd simply train our model by
changing a single parameter.

101
00:06:12,320 --> 00:06:16,670
Instead of model dot bit x and Y,
we just say model.fit x and x.

102
00:06:16,880 --> 00:06:20,150
All the usual training strategies
work with auto encoders including

103
00:06:20,270 --> 00:06:23,060
backpropagation and
regularization and drop out.

104
00:06:23,330 --> 00:06:27,500
It's fun to take an existing neural
network library and see what kind of low

105
00:06:27,501 --> 00:06:29,810
dimensional representations
we can come up with.

106
00:06:30,140 --> 00:06:34,100
If we were to build a really
simple autoencoder using
the care os deep learning

107
00:06:34,101 --> 00:06:37,040
library to reconstruct a
training set of images.

108
00:06:37,310 --> 00:06:42,170
Notice how a single fully connected
neuro layer acts as the encoder and as a

109
00:06:42,171 --> 00:06:45,470
decoder, and this model is
just a simple neural network.

110
00:06:45,590 --> 00:06:49,760
We're only calling it an auto encoder
because we're feeding it the input data as

111
00:06:49,761 --> 00:06:52,430
the labels.
Since we don't have any labels,

112
00:06:52,970 --> 00:06:57,020
the reconstructed results will look
very similar to the original input data.

113
00:06:57,140 --> 00:06:58,220
After we're done training,

114
00:06:58,400 --> 00:07:02,690
sometimes though the model could overfit
on the input data and in order for it

115
00:07:02,691 --> 00:07:06,950
to be able to learn a better, more
robust representation to the input data,

116
00:07:07,160 --> 00:07:11,500
we can manually add some noise and this
is called a de noising auto encoder.

117
00:07:11,720 --> 00:07:15,770
The amount of noise to apply to the input
usually takes the form of a percentage.

118
00:07:15,830 --> 00:07:18,680
There are so many different types
of auto encoders we could make,

119
00:07:18,681 --> 00:07:22,550
but one in particular that I really like
is called the variational autoencoder.

120
00:07:22,880 --> 00:07:25,820
This learns a latent variable
model of its input data.

121
00:07:26,030 --> 00:07:28,010
Instead of letting the
network learn some function,

122
00:07:28,190 --> 00:07:31,850
we're learning the parameters
of a probability distribution
that models our data.

123
00:07:32,000 --> 00:07:35,900
Then we can sample points from this
distribution and generate new input data

124
00:07:35,901 --> 00:07:39,860
samples meaning a Vav can be
considered a generative model.

125
00:07:40,370 --> 00:07:44,660
This lets us create all sorts of new
images and videos that have never existed

126
00:07:44,661 --> 00:07:47,360
before. Image colorization, chatbots,

127
00:07:47,600 --> 00:07:51,770
VA ease are right up there with generative
adversarial networks as one of my top

128
00:07:51,771 --> 00:07:53,480
five favorite deep learning models.

129
00:07:53,600 --> 00:07:57,260
Three points to encode in your biological
neural network. From this video,

130
00:07:57,560 --> 00:08:02,540
neural networks can slowly approximate
any function that maps inputs to outputs

131
00:08:02,720 --> 00:08:06,170
through an iterative optimization process.
Also called training.

132
00:08:06,530 --> 00:08:09,200
If we set the output to
be the same as the input,

133
00:08:09,440 --> 00:08:14,440
we can call this neural network and auto
encoder because it encodes a more dense

134
00:08:14,541 --> 00:08:18,740
representation of the input data and
there are many types of auto encoders we

135
00:08:18,741 --> 00:08:19,520
could make.

136
00:08:19,520 --> 00:08:23,690
A more recent generative model is
called the variational auto encoder,

137
00:08:23,840 --> 00:08:26,210
which learns a latent
variable of its input data.

138
00:08:26,270 --> 00:08:29,870
This week's coding challenge is to create
a simple auto encoder using the care

139
00:08:29,871 --> 00:08:31,070
os deep learning library.

140
00:08:31,280 --> 00:08:34,610
Post your get hub link in the comments
section and the winners will be announced

141
00:08:34,611 --> 00:08:38,060
in one week. Please subscribe for
more programming videos. And for now,

142
00:08:38,120 --> 00:08:41,500
I've got to solve AI or die trying.
So thanks for watching.

