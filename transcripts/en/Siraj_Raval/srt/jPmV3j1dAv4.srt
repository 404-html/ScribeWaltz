1
00:00:00,060 --> 00:00:00,751
Hello world.

2
00:00:00,751 --> 00:00:05,751
It's Saroj and let's visualize a Dataset
of eating habits to see what we can

3
00:00:05,941 --> 00:00:10,230
learn from it, shall we?
There's some really hard
scientific questions out there.

4
00:00:10,440 --> 00:00:15,360
Are we alone in the universe? What is
consciousness? What is dark matter?

5
00:00:15,390 --> 00:00:20,390
Really questions like these have
multimillion dollar payouts and have been

6
00:00:20,671 --> 00:00:24,510
troubling scientists for hundreds
of years, but guess what?

7
00:00:25,350 --> 00:00:28,350
We very likely already
have the answers to them.

8
00:00:28,590 --> 00:00:32,700
The problem is that they aren't in
plain sight. They're hidden in data.

9
00:00:32,940 --> 00:00:37,680
The former leader of the u s government
effort to sequence the human genome said

10
00:00:37,681 --> 00:00:39,540
that it took several years of effort,

11
00:00:39,600 --> 00:00:44,600
a huge team of researchers and $50
million to find the gene responsible for

12
00:00:44,731 --> 00:00:49,731
cystic fibrosis and 89 but
that same projects could
now be accomplished in a few

13
00:00:50,130 --> 00:00:54,120
days by a good Grad students.
You watching this right now? Yes,

14
00:00:54,121 --> 00:00:58,530
you can make a Nobel worthy
breakthrough with just your laptop.

15
00:00:58,830 --> 00:01:01,110
The data you need is freely available.

16
00:01:01,200 --> 00:01:04,260
You just have to discover the
hidden relationships in it.

17
00:01:04,500 --> 00:01:08,760
The field of dimensionality reduction
is all about discovering nonlinear

18
00:01:08,940 --> 00:01:13,260
nonlocal relationships in data that are
not obvious and the original feature

19
00:01:13,261 --> 00:01:16,620
space. If we reduce the number
of dimensions and some data,

20
00:01:16,830 --> 00:01:21,830
we can visualize it because a projection
in two or three d space can be plotted

21
00:01:21,871 --> 00:01:24,810
really easily unless you use PHP training.

22
00:01:24,811 --> 00:01:29,811
A model on a dataset with many dimensions
usually requires vast time and space

23
00:01:30,211 --> 00:01:33,300
complexity.
It also often leads to overfitting.

24
00:01:33,570 --> 00:01:37,380
Not all the features we have available
to us are relevant to our problem.

25
00:01:37,620 --> 00:01:40,680
If we reduced the dimensions,
we can reduce the noise,

26
00:01:40,710 --> 00:01:45,420
the unnecessary parts of the data and
find those that are surprisingly very

27
00:01:45,421 --> 00:01:50,421
closely related and once in a smaller
sub space we can more easily apply simple

28
00:01:50,640 --> 00:01:52,110
learning algorithms to it.

29
00:01:52,410 --> 00:01:56,580
We can divide dimensionality
reduction into two different topics.

30
00:01:56,880 --> 00:02:01,880
Feature selection and feature extraction
selection is all about finding the most

31
00:02:01,921 --> 00:02:03,630
relevant features to a problem.

32
00:02:03,900 --> 00:02:08,490
It can be based on our intuition as in
which features do we think would be the

33
00:02:08,491 --> 00:02:13,491
most relevant or we could let a model
one of the best features by itself.

34
00:02:15,930 --> 00:02:18,900
Extraction means finding new features.

35
00:02:18,930 --> 00:02:23,820
After transforming the data from a high
dimensional space to a lower dimensional

36
00:02:23,821 --> 00:02:24,654
space,

37
00:02:24,780 --> 00:02:29,520
we'll perform the ladder using a technique
called principle component analysis.

38
00:02:29,700 --> 00:02:34,200
Our dataset is going to be a record of
the 17 types of food that the average

39
00:02:34,201 --> 00:02:38,040
person consumes per week
for every country in the UK.

40
00:02:38,250 --> 00:02:41,310
So we've got 17 features slash.
Dimensions.

41
00:02:41,520 --> 00:02:44,190
Let's see what kind of insights
we can get from this data.

42
00:02:44,670 --> 00:02:48,900
PCA transforms variables into a new
set of variables which are a linear

43
00:02:48,901 --> 00:02:51,270
combination of the original variables.

44
00:02:51,480 --> 00:02:54,750
These new variables are known
as principal components.

45
00:02:54,990 --> 00:02:59,890
PCA is an orthogonal linear transformation
that transforms data to a new

46
00:02:59,891 --> 00:03:04,300
coordinate system such that the greatest
variance by some projection of the data

47
00:03:04,540 --> 00:03:06,460
locked on the first principal component.

48
00:03:06,730 --> 00:03:10,450
The second greatest variance on
the second component and so on,

49
00:03:10,930 --> 00:03:14,800
the variance is the measure
of how spread out the data is.

50
00:03:14,980 --> 00:03:18,970
If I were to measure the variance of the
height of a team of basketball players,

51
00:03:19,270 --> 00:03:20,440
it would be pretty low,

52
00:03:20,710 --> 00:03:24,010
but if I added a group of primary
school children to the mix,

53
00:03:24,310 --> 00:03:28,470
the variance would be pretty high.
Our first step is to standardize the data.

54
00:03:28,720 --> 00:03:31,570
PCA is a variance maximizing exercise.

55
00:03:31,780 --> 00:03:36,130
It projects the original data onto a
direction which maximizes variance.

56
00:03:36,490 --> 00:03:40,990
If we were to graph a small data set
that shows the amount of variance for the

57
00:03:40,991 --> 00:03:42,610
different principal components,

58
00:03:42,750 --> 00:03:47,750
it will seem like only one component
explains all the variants in the data like

59
00:03:47,771 --> 00:03:51,760
Putin at the g 20 summit,
but if we standardize the data first,

60
00:03:51,850 --> 00:03:56,850
then we'll see that the other components
do indeed contribute to the variance as

61
00:03:56,861 --> 00:03:57,610
well.

62
00:03:57,610 --> 00:04:02,410
Standardizing means putting the
data on the same unit scale for us,

63
00:04:02,411 --> 00:04:07,210
that would be grams for everything,
not a combination of kilograms and grams.

64
00:04:07,450 --> 00:04:10,720
That means the data should have a
mean of zero and a variance of one.

65
00:04:10,930 --> 00:04:14,200
Amit is just the average value
of all axes in the set x,

66
00:04:14,410 --> 00:04:18,040
which we can find by dividing the sum
of all the data points by the number of

67
00:04:18,041 --> 00:04:18,730
them.

68
00:04:18,730 --> 00:04:23,730
The way we calculate variance is by
computing the standard deviation squared.

69
00:04:24,430 --> 00:04:29,410
The standard deviation is the square root
of the average distance of data points

70
00:04:29,500 --> 00:04:30,333
to the mean.

71
00:04:30,430 --> 00:04:35,250
It's used to tell how measurements for
a group are spread out from the me.

72
00:04:35,440 --> 00:04:39,930
Once our data is standardized, we're
going to perform. I get it. Decomposition.

73
00:04:39,970 --> 00:04:41,650
So if your mama is so fat,

74
00:04:41,651 --> 00:04:45,670
she's not embeddable and three space
I can Paris will help fix that.

75
00:04:45,870 --> 00:04:50,870
I again is a German word that roughly
translates to characteristic and in linear

76
00:04:50,951 --> 00:04:55,930
Algebra and I can vector is a vector that
doesn't change its direction under the

77
00:04:55,931 --> 00:04:57,850
associated linear transformation.

78
00:04:58,510 --> 00:04:59,430
Decompose,

79
00:05:03,960 --> 00:05:06,150
decompose my sole

80
00:05:08,050 --> 00:05:11,570
value parity, compose my soul. Another

81
00:05:11,570 --> 00:05:15,410
way of putting it is that if we have
a non zero Vector v then it's an

82
00:05:15,440 --> 00:05:19,600
eigenvector of a square matrix.
A if a is a scalar multiple of the,

83
00:05:19,760 --> 00:05:24,760
so the land of scalar is an eigenvalue
or characteristic value associated with

84
00:05:24,861 --> 00:05:25,700
the eigenvector.

85
00:05:25,701 --> 00:05:30,701
V Eigen values are the
coefficients attached to eigen
vectors that give the axes

86
00:05:31,671 --> 00:05:32,504
magnitude.

87
00:05:32,570 --> 00:05:36,350
If we had a sheer mapping and displaced
every point in a fixed direction,

88
00:05:36,620 --> 00:05:40,700
notice how the Red Arrow changes
direction, but the Blue Arrow doesn't.

89
00:05:40,970 --> 00:05:45,020
The Blue Arrow is the eigen vector of
the mapping because it doesn't change

90
00:05:45,021 --> 00:05:49,280
direction and its length is
unchanged. It's eigenvalue. It's one.

91
00:05:49,430 --> 00:05:52,670
Both terms are important in many fields,
especially physics,

92
00:05:52,760 --> 00:05:57,140
since they can help measure the stability
of rotating bodies and oscillations,

93
00:05:57,260 --> 00:05:58,490
fiber rating systems.

94
00:05:58,850 --> 00:06:03,850
Many problems can be modeled with linear
transformations and I can vectors get

95
00:06:03,981 --> 00:06:05,420
very simple solutions.

96
00:06:06,690 --> 00:06:07,523
Okay,

97
00:06:08,280 --> 00:06:09,113
okay.

98
00:06:09,730 --> 00:06:10,563
Interventions.

99
00:06:14,500 --> 00:06:18,990
Why set her reasons?
PCA's deterministic. Yes,

100
00:06:19,170 --> 00:06:24,170
it so the correct fences and it
makes data portable on a two d graph,

101
00:06:25,410 --> 00:06:29,790
which so we could even painted ourselves.
I'm dawn to paying some pretty data.

102
00:06:30,370 --> 00:06:31,380
What's data by cameras?

103
00:06:32,250 --> 00:06:36,180
If we had a system of linear
differential equations, for example,

104
00:06:36,181 --> 00:06:40,860
to measure how the growth of population
of two species x and y affect one

105
00:06:40,861 --> 00:06:43,860
another,
like if one is a Predator of another,

106
00:06:43,950 --> 00:06:46,920
solving this system
directly is complicated,

107
00:06:47,100 --> 00:06:51,810
but if we could introduce
two new variables, Z and w,
which depends linearly on x,

108
00:06:52,020 --> 00:06:53,550
we can decouple the system.

109
00:06:53,551 --> 00:06:57,420
So instead we are dealing with
two independent functions.

110
00:06:57,750 --> 00:07:02,750
The eigenvectors and eigenvalues for
this matrix of coefficients do just this.

111
00:07:03,240 --> 00:07:07,800
They decoupled the ways in which a linear
transformation acts into a number of

112
00:07:07,860 --> 00:07:12,150
independent actions along separate
directions that can be dealt with

113
00:07:12,270 --> 00:07:16,770
independently. So we'll need to
construct a covariance matrix.

114
00:07:17,100 --> 00:07:20,700
Then we'll perform.
I can decompensation on that Matrix.

115
00:07:20,850 --> 00:07:25,310
A Matrix is just a table of values.
A covariance matrix is symmetric,

116
00:07:25,350 --> 00:07:29,940
so the table has the same heading across
the top as it does along the sides.

117
00:07:30,120 --> 00:07:34,770
It describes the variants of the data
and the covariants among variables.

118
00:07:35,040 --> 00:07:39,990
Covariants is a measure of
how two variables change
with respect to each other.

119
00:07:40,230 --> 00:07:44,700
It's positive when variables show
similar behavior and negative. Otherwise,

120
00:07:44,940 --> 00:07:49,080
PCA tries to draw straight lines
through data like linear regression.

121
00:07:49,380 --> 00:07:51,600
Each straight line is
a principal component,

122
00:07:51,810 --> 00:07:55,890
a relationship between an
independent and dependent variable.

123
00:07:56,190 --> 00:08:01,190
The number of principal components equals
the number of dimensions in the data

124
00:08:01,710 --> 00:08:06,450
and PCA's job is to prioritize them.
If two variables change together,

125
00:08:06,570 --> 00:08:10,920
it's very likely because one is acting
on the other or they're both subject to

126
00:08:10,921 --> 00:08:12,600
that same hidden force.

127
00:08:12,780 --> 00:08:17,460
Performing Eigen decomposition on our
covariance matrix helps us find the hidden

128
00:08:17,461 --> 00:08:19,200
forces at work in our data.

129
00:08:19,320 --> 00:08:23,850
Since we can't eyeball inter variable
relationships in high dimensional space.

130
00:08:23,880 --> 00:08:26,130
When calculating the covariance Matrix,

131
00:08:26,310 --> 00:08:31,080
the mean vector that's used to help do
so is one where each value represents a

132
00:08:31,081 --> 00:08:35,910
sample mean of a feature column in the
Dataset. Once we have our eigen pairs,

133
00:08:36,060 --> 00:08:38,730
we'll want to select the
principle components.

134
00:08:38,940 --> 00:08:43,890
We need to decide which ones can be
dropped and that's where our eigen values

135
00:08:43,891 --> 00:08:47,760
come in. We'll rank the
eigenvalues from highest to lowest,

136
00:08:47,880 --> 00:08:52,140
the lowest ones bear the least info about
the distribution of the data so we can

137
00:08:52,141 --> 00:08:55,980
drop a number of them like they're cold.
Next,

138
00:08:56,070 --> 00:08:57,900
we'll construct a projection matrix.

139
00:08:58,080 --> 00:09:01,830
This is just the Matrix of our
concatenated top K eigen vectors.

140
00:09:02,040 --> 00:09:06,750
We can choose how many dimensions we
want for our sub space by choosing that

141
00:09:06,751 --> 00:09:11,751
amount of Eigen vectors to construct our
d by k dimensional eigen vector matrix

142
00:09:12,031 --> 00:09:13,590
w.
Lastly,

143
00:09:13,620 --> 00:09:18,570
we'll use this projection
matrix to transform our
samples onto the subspace via

144
00:09:18,571 --> 00:09:20,550
a simple dot product operation.

145
00:09:20,790 --> 00:09:23,700
If we project our data
onto one dimensional space,

146
00:09:23,970 --> 00:09:26,250
then we can already see
something interesting.

147
00:09:26,490 --> 00:09:30,540
Notice how Northern Ireland is
a major outlier. It makes sense.

148
00:09:30,600 --> 00:09:31,830
According to the data.

149
00:09:31,980 --> 00:09:36,780
The Northern Irish consume
way more potatoes in alcohol
and way too few healthy

150
00:09:36,781 --> 00:09:40,290
options. The same thing happens
if we graft both components.

151
00:09:40,320 --> 00:09:44,010
We can see relations between data
points that we wouldn't otherwise.

152
00:09:44,130 --> 00:09:45,090
To summarize,

153
00:09:45,120 --> 00:09:50,120
principal component analysis is a
technique that transforms a dataset onto a

154
00:09:50,161 --> 00:09:55,161
lower dimensional sub space so we can
visualize and find hidden relationships in

155
00:09:55,381 --> 00:10:00,090
it. The principal components are eigen
vectors coupled with eigen values.

156
00:10:00,390 --> 00:10:04,440
They describe the direction in the
original feature space with the greatest

157
00:10:04,441 --> 00:10:09,210
variance in the data and the variance
is a measure of how spread out some data

158
00:10:09,211 --> 00:10:12,850
is. The winter of last week's
coding challenge his own job.

159
00:10:13,140 --> 00:10:15,450
He implemented a self organizing feature,

160
00:10:15,451 --> 00:10:18,180
math for colors and
for handwritten digits.

161
00:10:18,270 --> 00:10:22,200
Really efficient code and well documented.
Great job.

162
00:10:22,220 --> 00:10:26,790
Oh and wizard of the week and the runner
up is Hamas shake who developed a super

163
00:10:26,791 --> 00:10:31,500
detailed Jupiter notebook on self
organizing maps for class size effects on

164
00:10:31,501 --> 00:10:32,250
students.

165
00:10:32,250 --> 00:10:37,250
This week's coding challenge is to perform
PCA from scratch on a Dataset of your

166
00:10:37,561 --> 00:10:40,110
choice posts or get hub
link in the comments,

167
00:10:40,230 --> 00:10:42,690
and I'll give the winners
a shout out next week.

168
00:10:42,720 --> 00:10:45,240
Please subscribe for more
programming videos. And for now,

169
00:10:45,300 --> 00:10:48,810
I've got to release a music video,
so thanks for watching.

