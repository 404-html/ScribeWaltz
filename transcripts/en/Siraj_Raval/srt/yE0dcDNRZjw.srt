1
00:00:00,240 --> 00:00:04,980
You Suck rhymes like mine can't be
beat. That hurts my feelings. Oh, sorry.

2
00:00:05,010 --> 00:00:07,260
Do you want to play Pokemon
go instead? Sure. Yes.

3
00:00:12,360 --> 00:00:16,020
Hello world. It's Saroj and
I am a big fan of rap music.

4
00:00:16,080 --> 00:00:19,500
There's plenty of rap that degrades women,
promotes violence,

5
00:00:19,530 --> 00:00:23,220
and glorifies the gangster lifestyle.
But for all of its negatives,

6
00:00:23,280 --> 00:00:26,250
wrap is also a medium for
some incredible storytelling.

7
00:00:26,490 --> 00:00:31,020
It's a glimpse into real human suffering
in a hopeless place, a cry for justice,

8
00:00:31,021 --> 00:00:33,420
a distillation of the
human spirit of rebellion,

9
00:00:33,720 --> 00:00:36,870
and it's not just the content of rap
that's compelling, it's the style.

10
00:00:36,930 --> 00:00:40,140
Let's not even think about the subject
matter for a second rack gives you a

11
00:00:40,141 --> 00:00:44,050
certain lyrical freedom that
newly no other musical genre can.

12
00:00:44,160 --> 00:00:45,750
It's all about the rhymes.

13
00:00:45,780 --> 00:00:48,990
There's the perfect rhyme words
that end with the same sound.

14
00:00:49,080 --> 00:00:53,670
There's also the assonance rhyme where
only vowel sounds are shared like,

15
00:00:56,630 --> 00:00:59,970
and so because there's so much
creative potential in the rap game,

16
00:01:00,060 --> 00:01:03,600
we might need to introduce a
digital mc to the scene. Am I right?

17
00:01:03,660 --> 00:01:07,560
What if we could get a machine to
understand rack lyrics or even write them?

18
00:01:07,740 --> 00:01:09,690
It's not like this stuff
hasn't been tried before.

19
00:01:09,691 --> 00:01:13,590
It has several attempts have been made
to try and understand lyrics using

20
00:01:13,591 --> 00:01:17,730
machine learning. But one attempted
particular got some really great results.

21
00:01:17,820 --> 00:01:19,980
A pair of researchers
at Hong Kong University,

22
00:01:20,010 --> 00:01:22,950
so I had to download the
lyrics to 52,000 rap songs.

23
00:01:23,110 --> 00:01:26,490
Then train a model so that given a
novel song and model could identify it's

24
00:01:26,491 --> 00:01:30,720
rhyme scheme. So you ate a big Mac, your
breadth is whack, you need a tic tac.

25
00:01:30,810 --> 00:01:35,280
Take the whole pack is an example of the
AIA rhyme scheme since all the ending

26
00:01:35,281 --> 00:01:39,570
words Ryan, whereas life is a dream
future is now I eat ice cream.

27
00:01:39,571 --> 00:01:43,850
They're not saved. Gabel is an example of
a, B, a, B. Since every other line writes,

28
00:01:43,880 --> 00:01:47,160
the model they use to train on the
lyrics was called a hidden Markov model.

29
00:01:47,190 --> 00:01:51,290
Let's remove the word hidden for a second
and talk about how a plain old Markov

30
00:01:51,300 --> 00:01:52,133
model works.

31
00:01:52,140 --> 00:01:56,010
Let's say we want to predict the weather
and let's also say that the weather can

32
00:01:56,011 --> 00:02:00,870
only be one of three states, either sunny,
cloudy, or rainy for a hundred days.

33
00:02:00,871 --> 00:02:04,050
We record the weather and record
the transition between each day.

34
00:02:04,080 --> 00:02:07,380
Whenever we want to find the probability
of the weather being a certain state

35
00:02:07,381 --> 00:02:08,340
after a given day,

36
00:02:08,400 --> 00:02:12,330
we can just tally the number
of transitions of that
type and divide the number

37
00:02:12,330 --> 00:02:14,700
of days by 100 that's
how Markov models work.

38
00:02:14,730 --> 00:02:18,570
They help us predict the likelihood of
a future state. Pretty useful. Right.

39
00:02:18,600 --> 00:02:20,970
But what about a hidden Markov model?
Well,

40
00:02:21,000 --> 00:02:23,310
suppose we can't directly
observe the weather,

41
00:02:23,340 --> 00:02:25,730
so we can't really calculate
the transition probability.

42
00:02:25,731 --> 00:02:27,690
So the model is hidden from us,

43
00:02:27,750 --> 00:02:31,950
but we can observe related phenomenon
like the number of people with umbrellas

44
00:02:31,951 --> 00:02:35,580
on a given day because Rihanna said so.
So using one of many techniques,

45
00:02:35,610 --> 00:02:39,210
we can still find ways to calculate the
transition probabilities using these

46
00:02:39,211 --> 00:02:40,290
related variables.

47
00:02:40,320 --> 00:02:44,820
So Hidden Markov models are pretty cool
and their entire textbooks devoted to

48
00:02:44,821 --> 00:02:46,290
how they work.
In fact,

49
00:02:46,420 --> 00:02:49,680
it can be used not just to classify
lyrics but generate them as well.

50
00:02:49,710 --> 00:02:52,020
That same pair of researchers
published a later paper,

51
00:02:52,021 --> 00:02:55,690
but you know how during a Cypher one
MC challenges another with a verse.

52
00:02:55,710 --> 00:02:58,440
Then the opponent is supposed
to spit back some sick rhymes.

53
00:02:58,500 --> 00:03:01,300
They trained hmm to do this.
The results were interesting,

54
00:03:01,301 --> 00:03:05,350
but let's just say they're digital. Mc
wasn't exactly the second coming of Tupac.

55
00:03:05,380 --> 00:03:07,240
Let's take a look at a fresh approach.

56
00:03:07,300 --> 00:03:11,650
A group of researchers published a paper
just last month called dope learning,

57
00:03:11,770 --> 00:03:15,250
a computational approach to rap lyrics,
generation legendary.

58
00:03:15,280 --> 00:03:17,500
They use an algorithm called rank Svm,

59
00:03:17,560 --> 00:03:21,250
which was partially powered by a deep
neural network and they fed it a data set

60
00:03:21,251 --> 00:03:24,340
of all the songs from the top
100 English speaking rap artists.

61
00:03:24,370 --> 00:03:28,600
No idea how Lil Wayne got on that list.
So how does this algorithm work? Well,

62
00:03:28,601 --> 00:03:31,900
first they needed to extract features
from their rap corpus to feed to their

63
00:03:31,901 --> 00:03:35,560
model and they wanted features that
represented three metrics, rhyming,

64
00:03:35,650 --> 00:03:39,550
structural similarity and semantic
similarity. I'll those three metrics.

65
00:03:39,610 --> 00:03:43,390
The semantic similarity was the one that
required the use of a deep recurrent

66
00:03:43,450 --> 00:03:46,240
neural network and neural
net did what it does best.

67
00:03:46,300 --> 00:03:50,140
It found vector representations of words,
lines and groups of lines.

68
00:03:50,170 --> 00:03:53,080
Once the features were calculated,
they were input into the rink.

69
00:03:53,081 --> 00:03:56,230
Svm model rake SBM is a support
deck machine by the way,

70
00:03:56,320 --> 00:04:00,040
which is just a type of linear classifier
and the SPM eventually learn to

71
00:04:00,041 --> 00:04:03,130
predict the next line once who
was trained on the input features.

72
00:04:03,160 --> 00:04:07,210
They also want it to find a way to
quantify how good their algorithm was

73
00:04:07,240 --> 00:04:11,890
compared to human MCS and so they
calculated something called rhyme density,

74
00:04:11,920 --> 00:04:15,880
which is the average length of the longest
Ryan per word using rhymed density as

75
00:04:15,881 --> 00:04:16,510
a metric.

76
00:04:16,510 --> 00:04:21,340
They found that the algorithms generated
lyrics had a 21% higher density and the

77
00:04:21,341 --> 00:04:25,710
most rhyme dense human artists on their
list and inspect a deck play who let's

78
00:04:25,740 --> 00:04:29,650
write our own Rappler, his generator
using a hidden Markov model in Python.

79
00:04:29,680 --> 00:04:31,450
We only need two dependencies here,
random,

80
00:04:31,451 --> 00:04:34,780
which helps generate random numbers and
re which helps deal with text formatting.

81
00:04:34,840 --> 00:04:37,180
Our highest level of method
is called test Markoff in.

82
00:04:37,181 --> 00:04:38,850
It will initialize an
empty array called rap,

83
00:04:38,851 --> 00:04:41,590
lied and add all of our lyrics to
the rap library and returned the wrap

84
00:04:41,591 --> 00:04:45,160
generated from the rap library using the
stop word that was input by the user.

85
00:04:45,190 --> 00:04:47,530
The generated wrap.
We'll always start with the stop word.

86
00:04:47,590 --> 00:04:49,450
Let's take a look at
the ad to live function.

87
00:04:49,570 --> 00:04:52,030
It opens the lyrics file and construction.
Hmm.

88
00:04:52,180 --> 00:04:55,210
It iterates through every word and checks
this record to see if it's a new word

89
00:04:55,211 --> 00:04:58,540
or sequence. If a word or a sequence has
appeared before, he won't rerecord it,

90
00:04:58,780 --> 00:05:01,990
then it changes each count to a percentage
value or a transition probability.

91
00:05:02,140 --> 00:05:05,110
So once we constructed our model,
we can run the make rocks function.

92
00:05:05,200 --> 00:05:07,960
It'll take both the set of lyrics
and the start word as the parameters.

93
00:05:08,110 --> 00:05:09,730
It will continuously generate words.

94
00:05:09,880 --> 00:05:12,930
You have the markup next function
for up to 50 words. The markup.

95
00:05:12,931 --> 00:05:14,580
Next function either
returns a random word,

96
00:05:14,581 --> 00:05:17,680
it's the word it's novel or find a
word from the model. Probabilistically,

97
00:05:17,860 --> 00:05:19,600
let's test this out and
see what it generates.

98
00:05:19,750 --> 00:05:21,100
I'll start off my wrapper with homie.

99
00:05:21,190 --> 00:05:25,620
Homie grows Bunani likely
on Toten inspired enough,
basically, boy coming periods.

100
00:05:25,780 --> 00:05:26,850
Damn.
So dope.

101
00:05:26,920 --> 00:05:29,790
Check out the links down below for more
info and please subscribe for more ml

102
00:05:29,800 --> 00:05:33,880
videos. For now. I've got to
go fix a malformed requests,
so thanks for watching.

