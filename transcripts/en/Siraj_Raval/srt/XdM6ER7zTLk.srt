1
00:00:39,850 --> 00:00:44,050
Hello world. It's the Raj,
welcome to this live stream. Uh,

2
00:00:44,200 --> 00:00:46,720
if for those of you who are in my
previous life stream and doing one again,

3
00:00:46,721 --> 00:00:50,440
because why not? Uh, thank
you guys for coming today.

4
00:00:50,441 --> 00:00:55,180
What we're going to do is we're going
to find the relationship between student

5
00:00:55,181 --> 00:01:00,160
test scores and the amount
of hours studied. Now to us,

6
00:01:00,340 --> 00:01:03,820
clearly there must be some kind of
relationship, right? The more you study,

7
00:01:03,821 --> 00:01:05,380
the better your test score will be.

8
00:01:05,710 --> 00:01:10,630
But let's prove that mathematically
by using a strategy called linear

9
00:01:10,631 --> 00:01:14,860
regression. So linear regression is a
very simple machine learning technique.

10
00:01:15,220 --> 00:01:20,220
And the way we're going to optimize it
is by using probably not probably the

11
00:01:20,681 --> 00:01:25,090
most popular optimization method
there is gradient descent.

12
00:01:25,300 --> 00:01:29,830
So in this live stream you're going to
learn about gradient descent and linear

13
00:01:29,860 --> 00:01:33,370
regression. So this is going to
be awesome. So get ready for this.

14
00:01:33,730 --> 00:01:36,580
I'm glad to see that there are some
cool people in the room right now.

15
00:01:36,880 --> 00:01:41,170
What I'm going to do is I'm going to
first talk about conceptually how we're

16
00:01:41,171 --> 00:01:46,171
going to do this and then we're going
to implement it mathematically and

17
00:01:46,720 --> 00:01:49,170
programmatically.
It's really the same thing in the yet.

18
00:01:49,480 --> 00:01:53,380
So let's pull up what we need to pull up.

19
00:01:54,100 --> 00:01:58,900
A great guy, good number of people in
here. So let's go ahead and do this.

20
00:01:58,930 --> 00:02:03,040
So the first thing we want to do, so
what I have up here is uh, is a, uh,

21
00:02:03,041 --> 00:02:07,690
visualization. It's an animation
of what this looks like. Okay.

22
00:02:07,720 --> 00:02:11,170
And uh, it's going to start
off as a horizontal line.

23
00:02:11,171 --> 00:02:13,570
We want to find the line of best fit,

24
00:02:13,690 --> 00:02:17,380
we want to find the line of best fit so
that using this line we can then predict

25
00:02:17,590 --> 00:02:22,000
what a student's test score will be
given the amount of our studied or vice

26
00:02:22,001 --> 00:02:24,280
versa.
But how do we get that line of best fit?

27
00:02:24,281 --> 00:02:27,400
We'll we're going to use
gradient descent to do that.

28
00:02:27,820 --> 00:02:31,390
And so this is just a visualization of
what the gradient descent process will

29
00:02:31,391 --> 00:02:33,160
look like to get there.
Okay.

30
00:02:33,161 --> 00:02:37,510
And gradient descent is used everywhere
in machine learning and deep learning.

31
00:02:37,540 --> 00:02:42,130
Okay. We think about everything in terms
of optimization where we have some loss

32
00:02:42,131 --> 00:02:45,070
function that we want
to minimize overtime.

33
00:02:45,310 --> 00:02:49,090
And gradient descent is the
technique we use to do that. Okay.

34
00:02:49,091 --> 00:02:52,270
And so we're going to talk about that.
So let me start off by answering, uh, um,

35
00:02:52,390 --> 00:02:56,200
I'll do a two minutes worth of Q and a
for questions and then we'll get right

36
00:02:56,201 --> 00:03:00,160
into the code. So, hey, how's it going?

37
00:03:00,610 --> 00:03:05,110
Okay, we've got some, any questions?
Questions about deep learning, AI,

38
00:03:05,111 --> 00:03:07,270
machine learning,
uh,

39
00:03:08,800 --> 00:03:11,220
waiting for the wrap we'll see was well,

40
00:03:15,040 --> 00:03:18,930
yes. Questions. Questions really are the,

41
00:03:19,420 --> 00:03:21,370
the great thing to have in the beginning.

42
00:03:23,800 --> 00:03:26,980
Do Ama with other people.
Okay.

43
00:03:27,550 --> 00:03:30,970
So I'll give it,
I'll give it 10 more seconds.

44
00:03:31,150 --> 00:03:35,980
Ten nine, eight, seven.
We're about to get into this.

45
00:03:36,040 --> 00:03:38,950
We are about to get into it.
Once I started there was no stopping.

46
00:03:38,951 --> 00:03:43,180
I'm like an Oreo that really has no
relationship to what I just said.

47
00:03:43,181 --> 00:03:45,760
Can we do Q and a after the
end of the session? Yes we can.

48
00:03:45,970 --> 00:03:48,220
So we're going to do Q
and a every 15 minutes.

49
00:03:48,310 --> 00:03:51,010
So right now there are no
questions because no one
knows what the hell is going

50
00:03:51,011 --> 00:03:54,850
on. Is there any other method to
get leased? Square error? Yes.

51
00:03:55,180 --> 00:03:57,790
The method we're using here
is the sum of squared error.

52
00:03:57,820 --> 00:04:00,880
There are plenty of methods to do it,
but this is the most used one.

53
00:04:00,940 --> 00:04:02,260
And one more question is,

54
00:04:03,190 --> 00:04:07,870
is linear regression like this ml or DL?
Now that is a great question.

55
00:04:08,170 --> 00:04:12,160
Linear regression is just ml.
It's machine learning.

56
00:04:12,161 --> 00:04:13,570
There is no neural network,

57
00:04:13,690 --> 00:04:17,950
but the reason I'm doing this
is to demonstrate gradient
descent because you're

58
00:04:17,951 --> 00:04:22,810
going to use this in almost
every neural net that you built.

59
00:04:23,050 --> 00:04:26,230
You're going to use this
all over the place. Okay, so
that's it for the questions.

60
00:04:26,260 --> 00:04:29,560
Save your questions every 15 minutes
I'm going to be answering questions and

61
00:04:29,561 --> 00:04:32,320
this livestream will be an hour long,
more or less.

62
00:04:32,710 --> 00:04:34,320
So let's get started with the code,
shall we?

63
00:04:35,890 --> 00:04:39,250
And thanks to you Udacity for
hosting this. By the way, here we go.

64
00:04:41,070 --> 00:04:41,903
So

65
00:04:42,450 --> 00:04:43,050
yeah,

66
00:04:43,050 --> 00:04:47,940
to do this, let's look at our data first.
What is the data that we have? How do we,

67
00:04:48,780 --> 00:04:50,490
this is the Dataset
that we have by the way.

68
00:04:50,760 --> 00:04:54,460
So the data set is a collection of
test scores and amount of our study.

69
00:04:54,461 --> 00:04:59,461
So the x values are on the left hand
side and those are the amount of hours

70
00:04:59,550 --> 00:05:03,300
studying. So right here, 53 a 61.

71
00:05:04,170 --> 00:05:07,500
These are the amount of our study
and the y values are the test scores.

72
00:05:07,680 --> 00:05:12,000
Let's prove this relationship. That's
our dataset. It's in a data dot CSV file.

73
00:05:12,240 --> 00:05:15,480
You can find it on my get hub.
It is at the very top.

74
00:05:15,720 --> 00:05:18,630
It is the most recently
updated code repository.

75
00:05:18,810 --> 00:05:22,590
It might be the description as
well. Uh, I'm not sure if it's not.

76
00:05:22,591 --> 00:05:25,200
I would appreciate it if it is.
So let's get it.

77
00:05:25,230 --> 00:05:26,700
Go ahead and get started with the code.

78
00:05:26,970 --> 00:05:30,780
So the first thing I'm going to do is
I'm going to define the, let's see,

79
00:05:30,781 --> 00:05:35,640
the main function. I hope the code
is, uh, it is visible for you guys.

80
00:05:37,330 --> 00:05:38,920
You guys are going to code.
Yes.

81
00:05:39,050 --> 00:05:42,550
But what I want you to do is I want you
to pull up an editor, a text editor,

82
00:05:42,760 --> 00:05:45,770
and I want you to code with me.
So pull up sublime text,

83
00:05:45,771 --> 00:05:49,690
pull up Adam and start
coding with me. Okay. So

84
00:05:52,130 --> 00:05:54,140
let me get everything all
set up here are great.

85
00:05:54,350 --> 00:05:58,010
So the first thing we're gonna do is
we're going to say we're going to find our

86
00:05:58,011 --> 00:06:00,020
main function.
And

87
00:06:01,910 --> 00:06:02,690
okay,

88
00:06:02,690 --> 00:06:05,510
it's just a standard thing.
We always do this, don't we?

89
00:06:05,840 --> 00:06:08,120
Sometimes you just have
to do things that you,

90
00:06:10,120 --> 00:06:13,360
it's not really necessary,
but we'll go ahead and write that out.

91
00:06:13,660 --> 00:06:15,700
So that's it for our main function.

92
00:06:16,060 --> 00:06:20,560
And now we're going to define
the real main function,

93
00:06:20,561 --> 00:06:23,110
which is it our run function.
This is the highest level.

94
00:06:23,111 --> 00:06:26,260
So let's define our steps here.
The first thing we want to do,

95
00:06:26,380 --> 00:06:29,470
and this is what we do in machine
learning, is we pull our Dataset,

96
00:06:29,680 --> 00:06:34,680
we parse our dataset into memory so that
we can then run our algorithms on it.

97
00:06:36,190 --> 00:06:40,570
So the data set is going to be a
collection of points on an x, y plane.

98
00:06:40,900 --> 00:06:42,430
So we're going to say Jen from text.

99
00:06:42,610 --> 00:06:45,460
And the way we even have
this method available,

100
00:06:45,461 --> 00:06:48,940
Jen from taxed is by pulling from num Pi,
which is,

101
00:06:48,941 --> 00:06:53,320
are very, are very much used
machine learning library.

102
00:06:53,321 --> 00:06:55,890
Almost almost every,
uh,

103
00:06:56,020 --> 00:06:59,320
machine learning repository is
going to import num py in some way.

104
00:06:59,500 --> 00:07:01,900
And if it doesn't directly,
it's going to happen under the hood.

105
00:07:02,140 --> 00:07:06,520
Num Pi is the matrix multiplication
library for machine learning.

106
00:07:07,000 --> 00:07:08,320
Okay.
Uh,

107
00:07:13,840 --> 00:07:17,140
I have paid version of sublime. I'm
working on a different laptop than mine.

108
00:07:17,141 --> 00:07:19,390
We had some issues, so, uh, I just,

109
00:07:19,391 --> 00:07:21,790
I just downloaded this version was
flying it and don't worry about it.

110
00:07:21,880 --> 00:07:24,380
So what we're gonna do is we're
going to say Jen from tax,

111
00:07:24,381 --> 00:07:26,950
I'm gonna make this a little smaller
because this line is going to be kind of

112
00:07:26,951 --> 00:07:31,780
big. We're going to say data dot CSV. So
that is our CSE file that we just, uh,

113
00:07:31,781 --> 00:07:34,990
used.
And now I'm going to say delimiter.

114
00:07:36,040 --> 00:07:40,090
So this is just basic data parsing and
this method is from num py by the way.

115
00:07:40,120 --> 00:07:42,250
And that star little symbol,
it's a cow.

116
00:07:42,550 --> 00:07:45,880
Let's US pull it without having
to say NP or num py every time.

117
00:07:46,450 --> 00:07:49,750
So let's get our delimiter value
and that's going to be a comma.

118
00:07:50,140 --> 00:07:54,790
So the comma just means let's split it
by the comma and the comma is what's in

119
00:07:54,791 --> 00:07:58,930
between these values. If we were to
look at it in terms of a raw txt file,

120
00:07:59,200 --> 00:08:02,470
there'll be a common with common between
the hour study and the test scores.

121
00:08:02,590 --> 00:08:07,270
So it splits them both into, uh, uh,
a set of points, x, y value pairs.

122
00:08:07,720 --> 00:08:10,870
So that's our, those are our points. Okay.

123
00:08:10,871 --> 00:08:14,230
And so the next part is the learning rate.

124
00:08:14,590 --> 00:08:19,240
The learning rates is going to be 0.01.

125
00:08:19,480 --> 00:08:23,350
What the hell is this?
So learning rate is a hyper parameter.

126
00:08:23,530 --> 00:08:25,210
This is a hyper parameter.

127
00:08:25,360 --> 00:08:28,870
And hyper parameters in machine
learning are what we use

128
00:08:30,700 --> 00:08:34,330
as tuning knobs for our model.
So we have a bunch of these,

129
00:08:34,331 --> 00:08:36,310
and right now this is
the only one we have.

130
00:08:36,520 --> 00:08:40,900
The learning rate defines how fast our
model Lawrence. So you might be thinking,

131
00:08:40,901 --> 00:08:43,810
well, why don't we just set the
learning rate to a million? Well,

132
00:08:44,040 --> 00:08:47,560
the reason is like all things in
machine learning, it's a balance.

133
00:08:47,800 --> 00:08:50,950
It's kind of like a bell curve. So, uh,

134
00:08:51,430 --> 00:08:55,060
if the learning rate is too low,
our model will never converge.

135
00:08:55,320 --> 00:08:59,340
And if it's too high, then the
model will, uh, take, sorry.

136
00:08:59,490 --> 00:09:03,270
If the learning rate is too low, our
motto will be, uh, too slow to converge.

137
00:09:03,300 --> 00:09:07,710
Whereas if it's too high, our model will
never converge. So we want that balance,

138
00:09:07,711 --> 00:09:09,150
that optimal learning rate.

139
00:09:09,450 --> 00:09:12,960
And in this case it's going to be
0.01 in general in machine learning,

140
00:09:13,230 --> 00:09:17,370
we don't always know off the top of our
head what the best hyper parameters are

141
00:09:17,371 --> 00:09:18,204
going to be.

142
00:09:18,390 --> 00:09:22,110
So we have to guess and check now in the
bleeding edge of machine learning and

143
00:09:22,111 --> 00:09:26,070
deep learning right now is to learn
what those hyper parameters, uh,

144
00:09:26,071 --> 00:09:30,040
we'll be okay.
So

145
00:09:33,370 --> 00:09:34,600
that's it for that part.

146
00:09:34,601 --> 00:09:37,660
And let's just keep going here because
we were on a roll. So that's our, our,

147
00:09:37,780 --> 00:09:41,440
that's it for our learning, right?
And then we have our initial B value,

148
00:09:41,920 --> 00:09:44,680
which is going to be zero.
And then we have our initial m value,

149
00:09:44,800 --> 00:09:47,050
which is going to be
zero. What is this? Well,

150
00:09:47,051 --> 00:09:51,520
this is our y equals mx
plus B function from,

151
00:09:51,550 --> 00:09:54,690
from Algebra. We have the
slope formula, so that,

152
00:09:54,790 --> 00:09:59,740
so the B is our y intercept and
the m is our slope. It is the,

153
00:10:00,030 --> 00:10:02,620
the ideal slope that we want.

154
00:10:02,621 --> 00:10:05,650
So we're going to start off with zero
because we're going to learn these values

155
00:10:05,651 --> 00:10:08,950
over time.
Right now it's just be an m there's zero,

156
00:10:08,980 --> 00:10:10,920
but we're going to learn them over time.
So that's,

157
00:10:10,940 --> 00:10:14,320
that's our initial values for art.
So this is the slope formula.

158
00:10:15,490 --> 00:10:18,970
That's it to start off with.
And now we have our number of iterations.

159
00:10:19,030 --> 00:10:22,640
So how many, how many iterations
do we want to run our, uh,

160
00:10:23,400 --> 00:10:28,270
our training? Step Four. So we're going
to say a thousand. Why a thousand? Well,

161
00:10:28,300 --> 00:10:31,510
because we have such a small data set.
If the data set was bigger,

162
00:10:31,720 --> 00:10:36,720
then we would have to do 10,000 or
100,000 start incorporating gps and stuff

163
00:10:36,761 --> 00:10:41,110
like that. But we'll start off
with just a thousand. Okay. So

164
00:10:42,610 --> 00:10:43,510
there is that.

165
00:10:43,660 --> 00:10:48,660
And now I want to print
out the starting grow.

166
00:10:48,960 --> 00:10:52,450
Well, we'll forget the prince
step. We don't really need
that right now. Okay. So,

167
00:10:52,451 --> 00:10:55,510
so now we're going to say,
uh,

168
00:10:57,280 --> 00:10:59,920
let's get those ideal bnm values.

169
00:11:00,220 --> 00:11:05,220
Let's get the ideal bnm values by
using the gradient descent runner step.

170
00:11:05,590 --> 00:11:08,890
Now this is where the logic is going to
happen, which we're going to define now,

171
00:11:08,891 --> 00:11:12,220
right? So this is the highest value.
And then we're going to print out what,

172
00:11:12,520 --> 00:11:17,240
what be Nmr. So print, uh, print B,

173
00:11:17,280 --> 00:11:20,060
and then print him depending on what the,
the,

174
00:11:20,310 --> 00:11:21,690
those are going to be the optimal value.

175
00:11:21,691 --> 00:11:24,780
So this step is going to output the
optimal values for each of them.

176
00:11:25,020 --> 00:11:27,880
So what we're going to feed it is we're
going to feed everything we've just

177
00:11:27,881 --> 00:11:31,680
defined. We defined our points,
we define that initial p value.

178
00:11:31,860 --> 00:11:35,010
We defined the initial m value,
we defined the learning rate,

179
00:11:35,700 --> 00:11:38,880
we defined a number of iterations,
we defined a couple of things.

180
00:11:38,940 --> 00:11:42,480
And so we're going to speed
this all into the model. Okay?

181
00:11:42,630 --> 00:11:45,660
So that's it for the highest level.
And okay,

182
00:11:45,780 --> 00:11:48,780
so now what we're gonna do
is we're going to get into

183
00:11:52,830 --> 00:11:54,100
the gradient

184
00:11:54,100 --> 00:11:55,180
descent runner step.

185
00:11:55,300 --> 00:11:59,290
So the gradient descent runner step
is going to be defined now. Okay,

186
00:11:59,291 --> 00:12:01,750
so let's look at what this looks like.

187
00:12:01,900 --> 00:12:04,480
Given our values that we defined,

188
00:12:04,481 --> 00:12:09,481
which were the points and the starting
be value and the starting value and the

189
00:12:09,491 --> 00:12:13,480
learning rate and the
number of iterations. Okay,

190
00:12:14,050 --> 00:12:17,710
so let's do this. Okay, who's with me?

191
00:12:18,190 --> 00:12:20,200
We've got the B values.

192
00:12:20,590 --> 00:12:23,410
The B value to start off with
is going to be what we gave it,

193
00:12:23,440 --> 00:12:28,440
which is zero and the m value as you
can guess is also going to start off as

194
00:12:29,260 --> 00:12:33,130
zero. They'll both start off at zero.
We've got to learn these things.

195
00:12:33,190 --> 00:12:36,400
This is machine learning.
It's not machine define it statically,

196
00:12:36,580 --> 00:12:39,910
so for I in range number of iterations,

197
00:12:39,970 --> 00:12:43,540
which is which are a thousand
we're going to say, well,

198
00:12:43,660 --> 00:12:46,540
let's get those values.
Okay?

199
00:12:46,570 --> 00:12:48,910
Using something called the step gradient.

200
00:12:49,180 --> 00:12:53,560
Now that is where the real good stuff
happens. That's where it's at. Okay?

201
00:12:53,561 --> 00:12:57,070
We're going to define that,
but given what we start off with that B,

202
00:12:57,071 --> 00:13:01,030
which is zero in the m
and the array of points,

203
00:13:01,090 --> 00:13:02,500
so we're going to take those points.

204
00:13:02,560 --> 00:13:05,470
The x y value pairs and
feed it in as an array,

205
00:13:05,650 --> 00:13:08,500
we can convert that to an array
by using this function array.

206
00:13:08,830 --> 00:13:13,210
And then the learning rates, which is
a static value. It's a constant value.

207
00:13:13,450 --> 00:13:14,283
Okay?

208
00:13:17,500 --> 00:13:20,230
All right. And I will answer
questions in three minutes.

209
00:13:20,231 --> 00:13:24,880
So we have that for our learning rate
and then we're once we're done with that.

210
00:13:24,910 --> 00:13:29,780
So for all of those iterations, once we
have that, those optimal values for Bnm,

211
00:13:30,020 --> 00:13:32,650
we'll return them and then
we can print them in that,

212
00:13:33,160 --> 00:13:36,470
that highest level function.
Okay? So that's for bnm.

213
00:13:37,540 --> 00:13:40,240
That's our grading, the
samp runner step. Now,

214
00:13:40,270 --> 00:13:45,010
now it's time to define this
a

215
00:13:47,290 --> 00:13:50,710
step. Great in function. So now we're
going to define the step radiant function.

216
00:13:51,820 --> 00:13:56,820
Step gradient is going to be we given
the current B and given the current ham,

217
00:14:01,370 --> 00:14:02,203
yeah.

218
00:14:02,510 --> 00:14:06,380
Uh, we're going to give it the
points and then the learning rates.

219
00:14:07,220 --> 00:14:08,053
Okay,

220
00:14:11,140 --> 00:14:14,200
so now we're going to define
how gradient descent works.

221
00:14:14,230 --> 00:14:17,830
All of it is going to happen in this step,
but before we do that,

222
00:14:17,950 --> 00:14:21,100
before we do that,
let's write another function.

223
00:14:21,220 --> 00:14:24,460
So let's put this on hold for a second.
We're going to get to this,

224
00:14:24,730 --> 00:14:26,920
but let's write another
very important function.

225
00:14:26,921 --> 00:14:30,240
So we have two functions left in
this code really, but it's the,

226
00:14:30,241 --> 00:14:32,650
the real meat of it is going
to be explaining what they are.

227
00:14:32,980 --> 00:14:35,500
It's the step radiant and
we have one more function,

228
00:14:35,501 --> 00:14:40,501
which is the compute error for given
points function given B m and the points.

229
00:14:46,300 --> 00:14:50,290
So this, we're going to write
this one first. Let's talk
about what this is. Okay,

230
00:14:50,410 --> 00:14:54,320
so I have great visualization
here and we're going to

231
00:14:55,970 --> 00:14:58,040
talk about what this is.
So

232
00:15:00,540 --> 00:15:03,180
when we start off with this slope,
it's going to be zero, right?

233
00:15:03,181 --> 00:15:04,410
It's just a horizontal line.

234
00:15:04,560 --> 00:15:07,140
And then you see this is
what's happening in the code.

235
00:15:07,350 --> 00:15:10,110
You see that it's going up like that.
I love how the code is behind me.

236
00:15:10,111 --> 00:15:14,370
I can just do this. You see it going
up. So how does it get up? Well, for us,

237
00:15:14,430 --> 00:15:17,850
what we want to do is we want
to compute an error value.

238
00:15:18,240 --> 00:15:19,920
And what does that error value?
How do we,

239
00:15:20,040 --> 00:15:24,750
it's a way of for us to estimate how bad
our line is so we can update it every

240
00:15:24,751 --> 00:15:28,440
time, every step, every in machine
learning, we call it time step,

241
00:15:28,860 --> 00:15:32,370
every time step. We want you to
improve our model's prediction.

242
00:15:32,760 --> 00:15:36,270
And to do that we need an error die you
in this. Sometimes they call it error.

243
00:15:36,390 --> 00:15:38,820
Sometimes they call it loss error loss.

244
00:15:38,880 --> 00:15:42,900
It means the same thing we went to
minimize error. We want to minimize loss.

245
00:15:43,230 --> 00:15:46,740
And the way to do that is to
compute what that error is.

246
00:15:46,920 --> 00:15:50,880
And depending on the use case, it could
be different things. In this case,

247
00:15:50,940 --> 00:15:54,630
simple linear regression.
We have a set of points we want to,

248
00:15:56,530 --> 00:16:00,490
we want to uh,
define what that error is.

249
00:16:00,491 --> 00:16:02,380
So here's what it is we have,

250
00:16:02,430 --> 00:16:04,660
we start off with the line
and we have a set of points.

251
00:16:04,661 --> 00:16:08,800
And what we want to do is we want to
measure the distance between each of those

252
00:16:08,801 --> 00:16:12,250
points at a certain time step.
So just imagine the line is static.

253
00:16:12,460 --> 00:16:15,160
So for each time step, the line
is static. It doesn't move.

254
00:16:15,400 --> 00:16:20,110
We want to measure the distance
from each point to the, um,

255
00:16:20,920 --> 00:16:25,450
to the line. So we want, it's called
the sum of squared errors. Okay?

256
00:16:25,451 --> 00:16:28,870
So a sum of squared errors
is the name of this.

257
00:16:28,871 --> 00:16:33,010
And so what this looks like is
if we were to look at an image,

258
00:16:34,030 --> 00:16:38,720
it looks like this,
given some points,

259
00:16:38,840 --> 00:16:43,610
let's measure the distance from each of
those points to the line that we have

260
00:16:43,611 --> 00:16:48,611
drawn and then square them and then sum
them all together and then divide by the

261
00:16:49,401 --> 00:16:52,730
number. So it's divide by
the total number of points.

262
00:16:53,120 --> 00:16:55,460
And that is our error value.
And you can,

263
00:16:55,640 --> 00:16:57,830
and we want to minimize that error value.

264
00:16:57,831 --> 00:17:01,100
So just think about minimizing this error.
How are we going to minimize it?

265
00:17:01,130 --> 00:17:03,740
That's the next step, gradient
descent. But right now,

266
00:17:03,770 --> 00:17:08,720
this is how we calculate our error.
This is how we calculate our air.

267
00:17:08,900 --> 00:17:09,733
And,
uh,

268
00:17:11,560 --> 00:17:11,960
okay,

269
00:17:11,960 --> 00:17:13,520
so now let's,
uh,

270
00:17:14,960 --> 00:17:18,080
take some questions for five minutes and
then we're going to get back into the

271
00:17:18,110 --> 00:17:18,943
error value.

272
00:17:25,530 --> 00:17:29,700
Why am I so stupid? So let's
stop this self deprecation. Okay.

273
00:17:29,730 --> 00:17:34,710
No one is stupid. It's just
about who knows? These facts.
These are static facts.

274
00:17:34,770 --> 00:17:37,980
Who has spent the time and
energy to study this stuff. Okay.

275
00:17:38,760 --> 00:17:42,120
No self deprecation.
You guys are awesome for even being here.

276
00:17:42,121 --> 00:17:45,570
This stuff is the most important
stuff in the world. Okay.

277
00:17:48,360 --> 00:17:53,130
Can we use machine to not
for nonlinear equations? Yes.

278
00:17:53,430 --> 00:17:57,300
This is a linear equation but yes we
can use it for non linear equations

279
00:17:59,550 --> 00:18:01,560
for very, very high dimensional data. Yes.

280
00:18:01,561 --> 00:18:05,220
400 dimensional data but we will
get there when we get there.

281
00:18:05,910 --> 00:18:09,510
Could you make the font smaller?
Yes, I can do that. Okay.

282
00:18:09,511 --> 00:18:12,330
Two more questions and then we're going
to get right back into computing this

283
00:18:12,331 --> 00:18:17,160
error value programmatically
and mathematically could.

284
00:18:18,120 --> 00:18:21,990
Why is python better except for
the object oriented programming?

285
00:18:22,020 --> 00:18:26,940
Python is as my friend who works on
the tensorflow team at Google says the

286
00:18:26,941 --> 00:18:31,530
Lingua Franca of machine learning. It
is the language and why is it? Well,

287
00:18:31,560 --> 00:18:32,880
there's a couple of reasons.
One,

288
00:18:32,881 --> 00:18:37,290
it just had that headstart that people
who were the first to to to do machine

289
00:18:37,291 --> 00:18:40,710
learning algorithms decided that they
wanted to use python and we just kind of

290
00:18:40,711 --> 00:18:41,850
build from there.
That's one.

291
00:18:42,090 --> 00:18:46,110
Two is the amount of libraries that we
now have four and three is because it's

292
00:18:46,111 --> 00:18:49,680
just a dope language. Python is awesome
if you've coated in it for a while,

293
00:18:49,681 --> 00:18:50,910
you realize, wow, this,

294
00:18:51,210 --> 00:18:55,380
this does a lot of things that other
languages should do in a lot less code.

295
00:18:55,650 --> 00:18:58,980
It's more efficient. And
if you've come from a, uh,

296
00:18:58,981 --> 00:19:03,090
an ios background with objective C,
if you've come from a c black background,

297
00:19:03,091 --> 00:19:06,330
a c plus plus background,
we don't have to deal with deadlocks.

298
00:19:06,480 --> 00:19:10,770
We don't have to deal with memory leaks.
All of this is done by our interpreter.

299
00:19:10,771 --> 00:19:13,770
And we can just focus on what matters,
which is the algorithms.

300
00:19:13,950 --> 00:19:17,310
And so that's why python is used
because we can focus on the algorithms.

301
00:19:17,311 --> 00:19:18,150
One more question.

302
00:19:18,420 --> 00:19:22,140
Can you make a video about different
layers in a neural network? Uh,

303
00:19:24,600 --> 00:19:28,080
we'll see. We'll see. I don't know
exactly what you mean by that.

304
00:19:28,081 --> 00:19:30,720
That's why I said that. So, okay. So now
let's get back to computing this error.

305
00:19:30,810 --> 00:19:31,800
So how do we compute this?

306
00:19:31,801 --> 00:19:34,500
So get ready for some math and I'm
gonna explain what's happening here.

307
00:19:34,830 --> 00:19:37,050
So this is the error equation.
Okay.

308
00:19:37,051 --> 00:19:41,130
So this is how we compute
that sum of squared errors.

309
00:19:41,310 --> 00:19:46,200
So get ready for this. Okay, a
little refresher on on this. This is,

310
00:19:46,590 --> 00:19:51,170
this is Algebra, okay? We
have our y value, which is uh,

311
00:19:51,480 --> 00:19:55,980
the y value for a point, okay? And
then we have minus mx plus B. Well,

312
00:19:55,981 --> 00:20:00,270
what is mx plus B? Will it just
turns out that y equals mx plus B,

313
00:20:00,720 --> 00:20:05,340
right? So y equals mx plus B is the
same. Mx Plus B is the same thing as y.

314
00:20:05,460 --> 00:20:09,000
So we have two points. We
have the y point from our, uh,

315
00:20:09,510 --> 00:20:13,530
from the Dataset and then we have the
wide point on our line and we want to

316
00:20:13,531 --> 00:20:15,720
minimize the distance
between both of those points.

317
00:20:15,930 --> 00:20:17,850
And so that's why we are subtracting them.

318
00:20:18,030 --> 00:20:22,200
So we subtract the point from our data
set from the point on our line and then

319
00:20:22,201 --> 00:20:25,770
we square it. Well, why do we
square it? Well, for two reasons.

320
00:20:25,771 --> 00:20:30,240
One is because, uh, we want
the value to be positive.

321
00:20:30,241 --> 00:20:32,610
We don't want to have to deal
with negative values. Why?

322
00:20:32,790 --> 00:20:36,690
Because we are going to summit in a second
and two is because we don't actually

323
00:20:36,691 --> 00:20:40,800
care about the, the value itself, we
just care about the magnitude from.

324
00:20:40,830 --> 00:20:43,650
So we're looking at it from
a more abstract perspective.

325
00:20:43,651 --> 00:20:47,770
We just care about the magnitude of these
values. We went to minimize magnitude.

326
00:20:48,070 --> 00:20:49,850
So that's going to give us odd that the,

327
00:20:50,440 --> 00:20:55,440
the difference between r a y intercepts
and then little refresher on this e that

328
00:20:55,931 --> 00:20:59,800
means sigma, that's a, that's a notation
for that is called sigma notation.

329
00:21:00,160 --> 00:21:05,140
And what it does is it defines a set
of values that we want to iterate over.

330
00:21:05,200 --> 00:21:10,200
So what we're saying here is for when I
equals one up to n where n is the number

331
00:21:10,451 --> 00:21:13,840
of points.
So for all of those points we went to uh,

332
00:21:13,870 --> 00:21:18,400
measure the squared error values for
all of the points against our line.

333
00:21:18,760 --> 00:21:20,470
And then one over n means we want,

334
00:21:20,471 --> 00:21:24,550
you find the average of those and
that's going to give us one value.

335
00:21:24,551 --> 00:21:28,470
And that one value is the error value.
This,

336
00:21:28,471 --> 00:21:30,370
some of this squared errors.

337
00:21:30,670 --> 00:21:35,530
And every time step we want to minimize
its value using gradient descent.

338
00:21:35,680 --> 00:21:40,330
Okay?
Okay.

339
00:21:40,331 --> 00:21:44,800
So let's define this programmatically,
the sum of the squared errors.

340
00:21:48,090 --> 00:21:50,340
So we'll start off with error being zero.

341
00:21:50,341 --> 00:21:53,100
It's just there is no area
we have to calculate it.

342
00:21:53,101 --> 00:21:57,990
So what we're gonna do is we're going
to iteratively a compute this for every

343
00:21:57,991 --> 00:22:00,330
point that we have. Okay?
So we're going to say four.

344
00:22:00,331 --> 00:22:04,170
I in range zero.

345
00:22:04,230 --> 00:22:06,920
So we start off with zero until bill.
The,

346
00:22:07,290 --> 00:22:09,300
the ending point is the
number of points we have.

347
00:22:09,301 --> 00:22:13,890
So that's the length of the
points given those points. Okay?

348
00:22:13,920 --> 00:22:16,260
So for all the points, we're going to
do this, we're going to compute the air.

349
00:22:16,261 --> 00:22:17,550
So get ready for this.

350
00:22:17,551 --> 00:22:21,480
We're going to programmatically in code
or we're going to programmatically, uh,

351
00:22:21,510 --> 00:22:26,460
complete that math, that equation that we
just saw. So we'll start off by saying,

352
00:22:26,490 --> 00:22:31,110
well, what are the x values? And
we can find those x values by

353
00:22:32,760 --> 00:22:36,540
saying I where I is zero.

354
00:22:36,541 --> 00:22:41,240
So for the first column and in
the, in our Dataset, and then, oh,

355
00:22:41,380 --> 00:22:46,080
sorry, in our array, right? Cause they're
their points. And then we're going to say,

356
00:22:46,110 --> 00:22:50,760
well, what are our y
values? Okay, for one, so x,

357
00:22:50,761 --> 00:22:54,270
y value. So right now we have
one x value and one y value.

358
00:22:54,330 --> 00:22:57,690
And the four loop is because we're going
to continuously do this every time.

359
00:23:02,690 --> 00:23:05,780
So let's compute that total error.

360
00:23:05,900 --> 00:23:10,900
And so this line is going to represent
that equation that we just saw.

361
00:23:13,130 --> 00:23:16,760
Okay? This is, this line will represent
that equation programmatically.

362
00:23:17,120 --> 00:23:21,830
So we're going to say the total
air value is going to be a why.

363
00:23:21,831 --> 00:23:23,450
Remember it's y minus.

364
00:23:23,570 --> 00:23:27,770
We can literally just word
for or character by character.

365
00:23:27,980 --> 00:23:31,970
Say what this is going to
look like m times x Plus B,

366
00:23:34,290 --> 00:23:37,800
okay.
And then this means squared.

367
00:23:38,630 --> 00:23:41,190
So we're going to square that and
then we're going to add it every time.

368
00:23:41,370 --> 00:23:43,440
And so that's the sum
of the squared errors.

369
00:23:43,890 --> 00:23:48,830
And uh oh well this part is just going
to give us the sum of the squared errors.

370
00:23:48,831 --> 00:23:51,880
And then we have to,
we went to average it would that,

371
00:23:51,920 --> 00:23:55,100
that one over n that we had here,
that's that next step.

372
00:23:55,101 --> 00:23:59,480
So once we have all of these, we'll
divide it by the total number that we had.

373
00:24:01,720 --> 00:24:03,400
Return.
Total error

374
00:24:06,830 --> 00:24:11,390
value to the, Oh yes.

375
00:24:11,391 --> 00:24:14,840
Thank you for saying that. I
did miss an eye. You got it.

376
00:24:14,870 --> 00:24:19,870
Total error divided by float
over length times points.

377
00:24:22,430 --> 00:24:26,180
Okay. It makes sense, doesn't it?
Anthony, I appreciate you being here.

378
00:24:26,181 --> 00:24:29,420
Shout out to Anthony
Cooper. Uh, okay. So, um,

379
00:24:31,430 --> 00:24:34,460
we've got total error divided by float
length might have won. So that's it.

380
00:24:34,490 --> 00:24:38,810
That's what we had. That's our computing
error for our given points. Okay.

381
00:24:38,840 --> 00:24:41,100
Even God's make errors.
Everybody makes errors.

382
00:24:42,430 --> 00:24:43,263
Oh,

383
00:24:46,020 --> 00:24:51,000
let's see. Let's see, let's see. Okay,

384
00:24:51,001 --> 00:24:54,900
so now, and I'll answer questions
in six minutes. So every 15 minutes.

385
00:24:54,901 --> 00:24:58,440
So let's keep going here.
That's it for our compute hair function.

386
00:24:58,441 --> 00:24:59,190
So where were we?

387
00:24:59,190 --> 00:25:02,370
We were in that last function and the
most important function because it's

388
00:25:02,371 --> 00:25:05,850
talking about grading dissent.
Listen Up,

389
00:25:06,030 --> 00:25:10,830
because this is going to be used almost
every time when you're using deep neural

390
00:25:10,831 --> 00:25:15,660
networks all the time. No, this like
the back of your hand grading dissent.

391
00:25:16,170 --> 00:25:20,970
Let's go, let's go tales. I got some, see,

392
00:25:20,971 --> 00:25:24,060
I got some of my homies in here
at. Cool, so let's go with this.

393
00:25:24,210 --> 00:25:28,650
We've got gradient descent.
Let's do this. So for B, gradient

394
00:25:30,870 --> 00:25:34,580
starts off at zero and a four m gradient.

395
00:25:35,600 --> 00:25:38,450
It's also a source of a zero. All
right, before we even code this,

396
00:25:38,480 --> 00:25:42,140
let's explain what this is doing,
what the word gradient means.

397
00:25:42,680 --> 00:25:47,120
Let's look at this. Not
that. Let's look at this.

398
00:25:49,750 --> 00:25:53,560
What is this? This is a very
colorful rainbow period.
Let's get back to this. No,

399
00:25:53,561 --> 00:25:54,340
I'm just kidding.

400
00:25:54,340 --> 00:25:59,340
What this is is it's a graph
that shows three dimensions.

401
00:26:00,010 --> 00:26:03,070
The three dimensions are,
and it's the same graph.

402
00:26:03,100 --> 00:26:05,710
It's just looking at these
two images are the same graph.

403
00:26:05,740 --> 00:26:07,750
It's just looking at it
from a different angle.

404
00:26:07,930 --> 00:26:11,050
So let's just focus on this
one to not make it confusing.

405
00:26:11,230 --> 00:26:13,960
We're just looking at one graph right
here and it's going to make the whole

406
00:26:13,961 --> 00:26:17,410
thing smaller because that's how
math works. We have our y intercept,

407
00:26:17,411 --> 00:26:21,280
we have our slope, and we have our
error. Okay. So what this is, it's,

408
00:26:21,281 --> 00:26:25,510
it's a graph of all the
possible wine or steps,

409
00:26:25,750 --> 00:26:29,600
all the possible slopes and
all the possible error values.

410
00:26:29,601 --> 00:26:31,510
So remember that era
value that we calculated.

411
00:26:31,840 --> 00:26:34,990
And so if we were to map all of
those triplets so they could,

412
00:26:34,991 --> 00:26:39,991
we can think of them as triplets of x
are of slope y intercept and error value

413
00:26:40,991 --> 00:26:43,240
pairs,
it would make this graph.

414
00:26:43,590 --> 00:26:48,590
What we want to do is we want to find
that point where the error is smallest.

415
00:26:49,350 --> 00:26:51,900
We want to find the point
where the error is smallest.

416
00:26:52,170 --> 00:26:55,860
And if we look at this graph,
we can see what that point is visually.

417
00:26:56,040 --> 00:26:57,840
It's going to be at the
bottom of the curve.

418
00:26:57,870 --> 00:27:01,350
Now we call this in machine learning,
the local minima.

419
00:27:01,590 --> 00:27:05,230
Now the reason we say local
as opposed to nonlocal, uh,

420
00:27:05,310 --> 00:27:08,650
is because we have a
very simple graph here.

421
00:27:08,660 --> 00:27:13,310
So sometimes there are many minimum and
we want to find, um, which minimize.

422
00:27:13,311 --> 00:27:16,350
So that's that second order optimization.

423
00:27:16,351 --> 00:27:18,480
Right now we're focused on
first order optimization.

424
00:27:18,481 --> 00:27:23,481
So let's find that smallest point because
that smallest point at the very bottom

425
00:27:23,821 --> 00:27:28,110
of this graph is going to give us
the ideal y intercept and slope.

426
00:27:28,290 --> 00:27:33,210
So if we find the minimum
error, the minimal error,
the smallest error possible,

427
00:27:33,720 --> 00:27:38,460
we'll also get the ideal y
intercept and the ideal slope,

428
00:27:38,490 --> 00:27:39,630
the ideal bnm.

429
00:27:39,960 --> 00:27:43,980
And what do we do with those ideal bnm
values while we plugged them into our y

430
00:27:44,010 --> 00:27:45,690
equals mx plus B equation.

431
00:27:45,900 --> 00:27:49,740
And what happens when we plugged them
into our y equals mx plus B equation.

432
00:27:49,950 --> 00:27:54,330
We get the line of best fit.
Now I want to say, by the way,

433
00:27:54,810 --> 00:27:57,690
this is not the optimal way
of doing linear regression.

434
00:27:57,691 --> 00:28:02,590
We could compute these bnm values.
Uh, you think simple Algebra, uh,

435
00:28:02,670 --> 00:28:05,010
it doesn't really require us
to do creating the descent.

436
00:28:05,190 --> 00:28:08,760
We're only doing this strategy because
it's a way to learn gradient descent.

437
00:28:09,070 --> 00:28:13,950
There are easier ways to do this, but we
want to do this the dope way. Okay? So

438
00:28:15,840 --> 00:28:20,760
that's what that is. And so we're going
to compute this using grading dissent.

439
00:28:20,790 --> 00:28:24,060
Okay? So greatest sense. So that was
the first part of the explanation.

440
00:28:24,420 --> 00:28:29,420
And so if the reason I show this graph
is because the way we get that smallest

441
00:28:29,761 --> 00:28:32,820
point is by calculating
what's called the gradient.

442
00:28:32,820 --> 00:28:37,520
The gradient is also considered the slope.
Okay? Not to be confused with the, uh,

443
00:28:37,560 --> 00:28:38,610
with the m value,

444
00:28:39,300 --> 00:28:43,050
we're talking about a slope in the
direction of getting us to that gradient

445
00:28:43,051 --> 00:28:45,570
value.
So we have some why value,

446
00:28:45,571 --> 00:28:49,470
we have some B value and we want
to iteratively every iteration.

447
00:28:49,471 --> 00:28:54,270
We have a thousand iterations when it
iteratively move our point where we are in

448
00:28:54,271 --> 00:28:58,110
space in this three dimensional space,
down to that smallest point.

449
00:28:58,440 --> 00:29:01,260
And the way we do that is by
calculating the gradients.

450
00:29:01,261 --> 00:29:05,520
And the gradient gives us a direction.
It means slope, it means direction.

451
00:29:05,910 --> 00:29:08,780
And we need to talk about this for a
second. This is important. Listen Up.

452
00:29:08,880 --> 00:29:12,600
So gray and values are used all
over the place and machine learning.

453
00:29:12,780 --> 00:29:14,640
So I was talking to Ian Goodfellow,
right?

454
00:29:14,760 --> 00:29:17,370
He's the guy who invented
generative adversarial networks.

455
00:29:17,550 --> 00:29:20,660
And he was saying that some, some, uh,

456
00:29:20,670 --> 00:29:23,370
problems we can't do,

457
00:29:23,460 --> 00:29:25,980
we can't solve because we
don't have the gradient.

458
00:29:26,070 --> 00:29:29,430
So clearly gradients are
used across machine learning.

459
00:29:29,550 --> 00:29:32,940
Sometimes in machine learning we
call functions differentiable it,

460
00:29:33,000 --> 00:29:34,600
that's another word for,
uh,

461
00:29:34,640 --> 00:29:38,340
can we compute the gradient from it
using what's called a partial derivative.

462
00:29:38,850 --> 00:29:43,060
So to compute this
gradient value a direction,

463
00:29:43,061 --> 00:29:45,030
it's a tangent line.
Essentially a,

464
00:29:45,090 --> 00:29:47,170
a gradient descent is
essentially a tangent line.

465
00:29:47,171 --> 00:29:50,530
It's going to give us a
direction. Direction means
positive or negative. Do we,

466
00:29:50,680 --> 00:29:55,010
do we move up, do we, do we move down?
And so it gives us that line that,

467
00:29:55,011 --> 00:29:58,960
that direction that we want to move.
And this is another example of,

468
00:29:59,230 --> 00:30:02,410
of grading dissent.
It's essentially a bowl like curve,

469
00:30:07,420 --> 00:30:09,550
a bowl I curve.
And this is also

470
00:30:10,310 --> 00:30:10,800
okay,

471
00:30:10,800 --> 00:30:11,850
an example of

472
00:30:16,710 --> 00:30:19,290
bad internet connection or bad a server.

473
00:30:19,440 --> 00:30:24,150
So this would probably be
better. So, so yeah, it's a bowl.

474
00:30:24,360 --> 00:30:29,320
It's a bowl. We could think of this
entire process as a bowl and the ball are,

475
00:30:29,321 --> 00:30:32,430
our x are the points that we are,
that we have,

476
00:30:32,480 --> 00:30:37,410
whether that be the slope
or the y intercept and an
even more complex problems.

477
00:30:37,650 --> 00:30:41,790
The features and the numbers and the
words for natural language processing.

478
00:30:42,030 --> 00:30:47,030
It's all abstracted to a bowl like
problem where we are trying to find the

479
00:30:47,221 --> 00:30:48,900
optimal point for that bowl.

480
00:30:48,990 --> 00:30:53,760
It's going to give us to optimal values
that we need to make our model amazing.

481
00:30:53,970 --> 00:30:58,800
Okay, amazing. Slash. Optimal.
Should we go up? Should we go down?

482
00:30:58,890 --> 00:31:01,740
That's what the gradient gives us.
And to calculate that gradient.

483
00:31:01,741 --> 00:31:02,430
And let me say this,

484
00:31:02,430 --> 00:31:05,580
let's last thing and then we're going
to get right into the code to calculate

485
00:31:05,581 --> 00:31:06,414
that gradient.

486
00:31:06,420 --> 00:31:10,590
We're going to compute what's called
the partial derivative in calculus with

487
00:31:10,591 --> 00:31:13,680
respect to our values and
our values, our B, and m.

488
00:31:14,040 --> 00:31:16,560
So this is the equation
for the partial derivative.

489
00:31:20,020 --> 00:31:24,430
Okay? For that that we're going
to, we're going to take this,

490
00:31:24,431 --> 00:31:26,380
these equations,
these two equations,

491
00:31:26,590 --> 00:31:29,710
and we're going to programmatically
encode them. No, this is the last part,

492
00:31:29,711 --> 00:31:33,310
but it's also the most important part
because that is gradient descent.

493
00:31:33,550 --> 00:31:38,050
So we start off with why minus mx plus B,
which is why,

494
00:31:38,230 --> 00:31:41,650
okay.
And we compute the partial derivative.

495
00:31:41,740 --> 00:31:46,000
So what that means is we differentiate,
uh,

496
00:31:46,430 --> 00:31:49,120
this is from calculus. So we take
the V, the value, whatever it is.

497
00:31:49,150 --> 00:31:52,020
So little refresher. So if we have, um,

498
00:31:54,130 --> 00:31:57,970
let me just write this out actually. So
if we had an equation that was x squared,

499
00:31:58,600 --> 00:32:03,340
what is a derivative of this? We
take the exponent, this is a two,

500
00:32:03,550 --> 00:32:08,110
and we move it to the, uh,
in front of the variable.

501
00:32:08,111 --> 00:32:11,020
So it'd be two X. Okay.
So that's a driven it,

502
00:32:11,021 --> 00:32:14,770
but we are calculating the partial
derivative with respect to B and m.

503
00:32:14,771 --> 00:32:17,020
So what does that, what do you
mean by partial? By partial,

504
00:32:17,021 --> 00:32:21,500
I mean we're only using,
uh, the values that we, uh,

505
00:32:21,670 --> 00:32:24,690
we're calculating a
derivative using only B,

506
00:32:24,870 --> 00:32:29,870
and then we're calculating a respective
or a different partial derivative using

507
00:32:29,981 --> 00:32:32,050
just m. Okay. So let's,

508
00:32:32,290 --> 00:32:35,320
so this little symbol over here,

509
00:32:36,250 --> 00:32:40,130
this little squiggly just means partial
may play. Find the squiggly. Okay.

510
00:32:40,131 --> 00:32:42,860
And then let me also answer some questions
cause I think it's time to answer

511
00:32:42,861 --> 00:32:46,310
some question. This, this little
squiggly means partial derivative. Okay.

512
00:32:46,311 --> 00:32:50,290
So it's going to make a
lot more sense when we, uh,

513
00:32:50,530 --> 00:32:52,760
and code code this
programmatically. If, if,

514
00:32:52,761 --> 00:32:57,350
if you aren't as familiar with
the math terms. So questions.

515
00:32:59,810 --> 00:33:04,430
Okay.
Why are there two equations?

516
00:33:04,431 --> 00:33:08,660
There are two equations because we are
Tau collating respected derivatives for

517
00:33:08,661 --> 00:33:10,280
both B and.
M.

518
00:33:10,280 --> 00:33:15,280
Those are the two values that we are
trying to find and we're going to look at

519
00:33:15,651 --> 00:33:16,490
this programmatically.

520
00:33:16,491 --> 00:33:20,180
So two more questions and then
we're going to code this last step.

521
00:33:22,590 --> 00:33:26,790
Nice. So gray and sent helps
us to put rainbows at the,
okay. Hey that's not okay.

522
00:33:31,180 --> 00:33:34,300
Exactly.
Best use of time and money.

523
00:33:35,470 --> 00:33:38,500
Will there always be one low
point in the error function?

524
00:33:38,501 --> 00:33:43,501
We can work to using gradient descent
when we have a linear function?

525
00:33:43,750 --> 00:33:48,130
Yes, because it is our local
minima but sometimes we have very,

526
00:33:48,131 --> 00:33:50,950
very complex data.

527
00:33:51,220 --> 00:33:55,630
We have data that has hundreds of
features. We have data that is unlabeled.

528
00:33:55,870 --> 00:33:58,540
We have data that uh,
we don't,

529
00:33:58,570 --> 00:34:02,440
sometimes we run unsupervised
algorithms. That means, uh,

530
00:34:02,500 --> 00:34:04,180
algorithms that the delay,

531
00:34:04,300 --> 00:34:07,960
the data is not labeled and we
try to find something from it.

532
00:34:07,961 --> 00:34:11,620
So sometimes it clusters in certain
points and we say, oh, this cluster there,

533
00:34:11,800 --> 00:34:15,700
there must be some relationship here
in, in, in more complex cases. No.

534
00:34:15,920 --> 00:34:20,260
In more complex cases.
There are, um, there are,

535
00:34:20,290 --> 00:34:23,830
there could be low there, there
could be several local minima and uh,

536
00:34:24,340 --> 00:34:26,680
we want to find the ideal one, but
we'll get to that. Right? It's,

537
00:34:26,681 --> 00:34:27,910
it's just another set up.

538
00:34:28,150 --> 00:34:32,180
We want to learn where to do
gradient descent and that's,

539
00:34:32,260 --> 00:34:35,800
that's later steps. Um, one more question

540
00:34:37,780 --> 00:34:39,340
to start off with ml and AI,

541
00:34:39,341 --> 00:34:42,520
do you recommend starting with
the Mln d followed by note?

542
00:34:43,660 --> 00:34:44,493
Are there

543
00:34:47,350 --> 00:34:50,860
why the gradient gives the minimum that
that is a mathematical question? Yes.

544
00:34:51,100 --> 00:34:54,370
So why does the gradient give the minimum?
Because

545
00:34:56,320 --> 00:34:59,160
it doesn't give us the minimum
to start off with the gradient.

546
00:34:59,170 --> 00:35:01,430
Just skip tells us how
to update our value.

547
00:35:01,431 --> 00:35:05,080
Should we update them with a positive?
Should we update them with a negative?

548
00:35:05,410 --> 00:35:07,990
Should we multiply them by,
you know,

549
00:35:08,200 --> 00:35:13,200
how do we modify our values to make them
closer and closer to the optimal values

550
00:35:14,171 --> 00:35:17,560
where the error is the smallest
and once we're finally there,

551
00:35:17,770 --> 00:35:21,550
that is our minimum. Okay. It
doesn't directly give us a minimal,

552
00:35:21,580 --> 00:35:26,320
all it does is it gives us a direction
to minimize our error or loss. Okay.

553
00:35:27,010 --> 00:35:28,960
If we can compute a gradient.

554
00:35:29,260 --> 00:35:32,230
So that mean if a function
is differentiable,

555
00:35:32,530 --> 00:35:36,490
then we know that we can optimize it.
So let's write this out.

556
00:35:37,800 --> 00:35:40,530
Let's write out this step
grading function. Okay? So

557
00:35:43,980 --> 00:35:48,660
we are, so we, we, we call this
function in our grading dissent runner.

558
00:35:48,661 --> 00:35:52,020
Recall that we were weak,
we called it right here,

559
00:35:52,350 --> 00:35:55,080
given the B m points and learning rates.

560
00:35:55,110 --> 00:35:57,600
So now we're going to ride this out.

561
00:36:03,780 --> 00:36:04,780
You have are,
okay,

562
00:36:05,170 --> 00:36:09,280
so let me make this a little smaller so
we can really see what's going on here.

563
00:36:10,180 --> 00:36:11,230
And uh,

564
00:36:18,560 --> 00:36:23,560
zero m equals m gradient equals
zero n equals float length points.

565
00:36:29,560 --> 00:36:30,393
Okay?

566
00:36:30,900 --> 00:36:35,720
These are our initial gradient values
for B for further be great and the embryo

567
00:36:35,770 --> 00:36:39,000
and we're going to update them over
time and it's going to be the number of

568
00:36:39,001 --> 00:36:41,070
points. Okay? That's just
the number of points.

569
00:36:41,100 --> 00:36:45,420
And now we're going to calculate
the gradient steps. So,

570
00:36:46,110 --> 00:36:48,210
uh, okay. So for each of the points,

571
00:36:50,400 --> 00:36:53,190
we're getting great and values for each
of the points and we're going to put

572
00:36:53,191 --> 00:36:56,550
them all together and it's
gonna give us one, uh, uh,

573
00:36:57,060 --> 00:37:01,990
optimal bnm for each time step or a
more optimal bnm had each time separate

574
00:37:01,991 --> 00:37:04,860
member. This alone isn't going to
do it. We have to iteratively do it.

575
00:37:04,861 --> 00:37:08,760
And that's what we did beforehand.
Yes.

576
00:37:10,590 --> 00:37:14,160
Oh, you're right. Yes you are right. Yes.

577
00:37:14,790 --> 00:37:19,080
Oh guys, you, you got me. I already
defined it over here. That's what's up.

578
00:37:22,690 --> 00:37:27,340
I did define it here. Yes. Great.
Where was I moved down there.

579
00:37:27,360 --> 00:37:31,900
There we go. Okay, so a grain of sand
runner. And then we have the yes,

580
00:37:31,940 --> 00:37:36,940
the plot points and the n
value is the number of points.

581
00:37:39,220 --> 00:37:41,380
Okay. So now let's, uh,

582
00:37:42,490 --> 00:37:45,490
we're going to iterate through all of
those points that we just defined, right?

583
00:37:45,491 --> 00:37:48,700
The points, the length of
the points, and then, uh,

584
00:37:50,800 --> 00:37:53,530
so for each of those
x and y. Dot. U Paris.

585
00:37:53,590 --> 00:37:57,790
So for points where I is zero,

586
00:37:58,480 --> 00:38:03,190
where y is
not where I is,

587
00:38:03,230 --> 00:38:06,670
zero for each of those. X, Y, Paris,

588
00:38:09,220 --> 00:38:10,120
hi one.

589
00:38:13,930 --> 00:38:17,110
Let's calculate the gradient.
So how do we do that? Well,

590
00:38:17,170 --> 00:38:22,120
let me look back at this equation
and see what that was is,

591
00:38:22,450 --> 00:38:22,811
okay?

592
00:38:22,811 --> 00:38:27,340
So what we're doing here to calculate
this gradient or this partial derivative,

593
00:38:27,760 --> 00:38:31,180
okay?
A is what we are doing is we are going to

594
00:38:34,510 --> 00:38:39,220
take y mx plus B and then
multiply it by the x value.

595
00:38:39,910 --> 00:38:41,200
Okay?
Um,

596
00:38:42,190 --> 00:38:47,190
and then at a negative sign and then do
that for every point and then multiply

597
00:38:47,531 --> 00:38:52,120
it by two over the number of points.
Okay? We can do that. Let's do that.

598
00:38:53,470 --> 00:38:57,460
Remember, these are
laws. They are constant.

599
00:38:57,940 --> 00:39:02,680
The partial derivative is the same every
time. It is a law is a fundamental law.

600
00:39:02,710 --> 00:39:05,500
It is a beautiful law
because it's always the same.

601
00:39:05,501 --> 00:39:10,501
It is predictable and predictability is
great sometimes when you need it to be.

602
00:39:12,040 --> 00:39:14,830
So let's calculate that value

603
00:39:16,600 --> 00:39:17,440
native to end.

604
00:39:17,470 --> 00:39:22,470
That's what we had
negative two over n times.

605
00:39:22,750 --> 00:39:23,980
Why?
Remember,

606
00:39:23,981 --> 00:39:26,500
we are just writing it out just
like we just saw it in the equation.

607
00:39:26,501 --> 00:39:28,000
It's the same thing.

608
00:39:36,390 --> 00:39:39,810
Am current times x

609
00:39:42,860 --> 00:39:43,970
Plus B,

610
00:39:44,270 --> 00:39:49,270
current y minus mx plus
B times two over the end.

611
00:39:52,180 --> 00:39:55,400
Uh, negative. And then we're
continuously adding that. Right?

612
00:39:55,790 --> 00:39:59,750
So that's it for our grading for B,
that's going to give us a direction,

613
00:39:59,870 --> 00:40:03,710
aren't positive or negative
that we should update via. Okay.

614
00:40:03,980 --> 00:40:06,770
And then we want to do
the same thing for em.

615
00:40:07,130 --> 00:40:11,930
So in fact, I'll just paste
it because it's very similar,

616
00:40:11,931 --> 00:40:14,780
but it's a little different and I'll,
I'll show why it makes you,

617
00:40:14,781 --> 00:40:17,720
make sure you guys can see it all.
For our m gradient,

618
00:40:19,280 --> 00:40:24,280
it's going to be negative two n times x
Times y minus m current times x Plus B

619
00:40:27,261 --> 00:40:29,870
current. Yes. Okay. So

620
00:40:32,690 --> 00:40:35,120
we're doing this for every
point and we're going to see,

621
00:40:35,180 --> 00:40:39,050
we're going to concatenate all of those
values together using this plus equals

622
00:40:39,051 --> 00:40:43,520
function to get that, uh, be
gradient in the m gradient. Okay.

623
00:40:43,521 --> 00:40:48,290
And so once we have those values,
we're going to use them.

624
00:40:48,650 --> 00:40:50,630
Now it's time to use those values.

625
00:40:52,040 --> 00:40:52,360
Okay.

626
00:40:52,360 --> 00:40:53,650
To use those values,

627
00:40:54,430 --> 00:40:58,390
we're going to update the
real bnm values using them.

628
00:40:59,200 --> 00:41:03,100
And how do we, how do we use them? All
we say for let's say for B for example,

629
00:41:03,370 --> 00:41:07,030
we take what we have currently for B,
whatever it is at every step,

630
00:41:07,031 --> 00:41:09,460
because we're running this every time,
step,

631
00:41:09,461 --> 00:41:14,461
every of a thousand minus the
learning rate times the B gradient.

632
00:41:18,640 --> 00:41:22,960
This is why we define our learning rates
because we multiply it by the gradient

633
00:41:23,230 --> 00:41:25,120
and it's going to,
uh,

634
00:41:25,870 --> 00:41:30,580
it's going to convert it into
a value that we can then, uh,

635
00:41:30,610 --> 00:41:35,120
it can either make it convergence slower,
it could never converge,

636
00:41:35,420 --> 00:41:40,280
but this learning rate is going to
give us, uh, a good a convergence,

637
00:41:40,640 --> 00:41:44,570
uh, uh, rate of convergence is gonna
give us a good rate of convergence.

638
00:41:45,590 --> 00:41:50,270
And then we have the new M,
which is going to be m current,

639
00:41:50,330 --> 00:41:54,620
same deal, minus the learning rates. And
I'll answer questions in four minutes.

640
00:41:54,621 --> 00:41:55,460
So keep them ready,

641
00:42:00,510 --> 00:42:02,520
times the gradients

642
00:42:04,910 --> 00:42:07,370
return,
and then we'll return them

643
00:42:10,730 --> 00:42:12,080
just like that.
Okay.

644
00:42:12,670 --> 00:42:15,340
So what did we just do here?
Let me answer some questions.

645
00:42:22,790 --> 00:42:25,250
Oh, okay. I

646
00:42:36,340 --> 00:42:39,970
it is the learning rate.
It is the same thing.

647
00:42:40,450 --> 00:42:44,740
I just defined it differently
in the, uh, the step. Okay.

648
00:42:44,741 --> 00:42:49,710
So, yeah, so that's
for our gradient. Okay.

649
00:42:49,711 --> 00:42:53,190
Can you please explain again why
we are doing Newbie and knew him?

650
00:42:53,850 --> 00:42:58,530
So we had an initial bnm right. Those
words zero value, they were to zero,

651
00:42:58,920 --> 00:43:02,910
we had nothing. We want it to
update them using the gradient.

652
00:43:03,090 --> 00:43:06,390
So the gradient will tell us
how should we update them.

653
00:43:06,391 --> 00:43:08,490
Should it should be be zero,
should be one,

654
00:43:08,491 --> 00:43:11,940
should it be negative
one should be be 0.5.

655
00:43:12,180 --> 00:43:17,040
The gradient is a value that we uh,
use to update the BNN value,

656
00:43:17,041 --> 00:43:20,220
the initial bnm value to get
the new bnm value. To do that,

657
00:43:21,060 --> 00:43:25,710
we're going to calculate
the partial derivative with
respect to B and respect to

658
00:43:25,770 --> 00:43:26,011
m,

659
00:43:26,011 --> 00:43:30,780
which are these two respective lines
right here given all of our points.

660
00:43:30,990 --> 00:43:32,910
Okay. Given all of these
points were calculating,

661
00:43:32,911 --> 00:43:37,911
that will sum them all up and then we'll
multiply it by the learning rate and

662
00:43:38,461 --> 00:43:40,050
subtract that from what we have currently.

663
00:43:40,200 --> 00:43:42,840
And that's going to give
us our new bnm values.

664
00:43:49,360 --> 00:43:49,841
Okay.

665
00:43:49,841 --> 00:43:54,520
So can you explain why do you multiply
the learning rate times the gradient?

666
00:43:54,521 --> 00:43:59,440
Again, what's that value? So,
okay, so the learning rate,

667
00:43:59,920 --> 00:44:00,753
if we,

668
00:44:01,750 --> 00:44:05,080
so in a Dataset this small with
only a thousand data points,

669
00:44:05,260 --> 00:44:08,080
we don't really need a learning rate
more or less. I mean we don't really,

670
00:44:08,420 --> 00:44:10,090
our model is going to converge.

671
00:44:10,390 --> 00:44:14,410
This is just good practice to always
think about a learning rate because we use

672
00:44:14,411 --> 00:44:17,590
learning rates in deep learning, right?
We use them in deep neural networks.

673
00:44:18,550 --> 00:44:20,890
It's about a balance.

674
00:44:20,920 --> 00:44:25,600
You'll find that all of this stuff is all
about a balance time versus computing.

675
00:44:25,601 --> 00:44:29,470
If a power versus efficiency
versus amount of code,

676
00:44:29,471 --> 00:44:34,471
it's always a balance I'll learning rate
does is we are predefining how fast we

677
00:44:34,831 --> 00:44:36,090
want our model to train.

678
00:44:36,510 --> 00:44:40,590
And if we do it too fast then we'll
never converge. That means we'll never,

679
00:44:40,591 --> 00:44:43,730
going to convergence means
finding the optimal, uh,

680
00:44:44,380 --> 00:44:47,580
values for our function. Whatever
it is. If we do it too slow,

681
00:44:47,581 --> 00:44:50,400
then it will be too slow to converge.
So it's about time and balance.

682
00:44:50,940 --> 00:44:55,830
We've set 0.0001 for our learning right
here, statically ourselves manually.

683
00:44:56,010 --> 00:45:00,210
But you know,
ideally we can learn what that value is.

684
00:45:02,100 --> 00:45:04,860
The part where we sum it all up.
This is the last question someone asks,

685
00:45:05,160 --> 00:45:09,720
what is the part where we sum it all
up? Is it in the four loop? Uh, yes,

686
00:45:09,721 --> 00:45:13,170
it is in the for loop. We are summing
it all up right here. Plus equals.

687
00:45:13,380 --> 00:45:15,960
We're summing it all up. Okay, so,

688
00:45:18,370 --> 00:45:23,170
so yeah. So yeah, let's run this
code. That's it for the code.

689
00:45:23,171 --> 00:45:24,760
Actually let's run this thing.

690
00:45:25,090 --> 00:45:28,300
We've got python hundred
percent is going to be an error.

691
00:45:29,340 --> 00:45:31,140
Okay.
Of course.

692
00:45:32,690 --> 00:45:33,650
If name

693
00:45:37,490 --> 00:45:38,323
hold on.

694
00:45:45,230 --> 00:45:49,820
Oh yes. Two equal signs because
that's how we roll in Python.

695
00:45:58,590 --> 00:45:59,423
What are you talking about here?

696
00:46:01,720 --> 00:46:04,720
Deep learning is a subset of
machine learning. You are correct.

697
00:46:05,740 --> 00:46:07,180
It is a subset.

698
00:46:10,800 --> 00:46:12,600
And what we have here

699
00:46:17,690 --> 00:46:21,320
is a space that, there we go. Okay.

700
00:46:21,321 --> 00:46:23,150
So let's get to questions.

701
00:46:25,710 --> 00:46:27,410
Oh my God.
Wow.

702
00:46:27,520 --> 00:46:30,310
Oh, you know what it is is
because does this computer,

703
00:46:30,311 --> 00:46:31,450
so I'm using a new computer by the way,

704
00:46:31,451 --> 00:46:35,520
so it might not num Pi might
not be installed. We can
install with pit. Oh my God.

705
00:46:35,620 --> 00:46:39,700
Okay, so here's what it
is. Here's what it is. So

706
00:46:43,610 --> 00:46:45,620
let me look back at this.

707
00:46:56,020 --> 00:46:56,471
What does that,

708
00:46:56,471 --> 00:46:59,740
so I actually don't have an empire and
so on this computer I'm using a different

709
00:46:59,741 --> 00:47:03,430
computer. This was a last minute thing.
Don't worry about it. The code works.

710
00:47:04,480 --> 00:47:06,760
I just come,
I had a compiled earlier.

711
00:47:07,570 --> 00:47:11,560
Look at the values here.

712
00:47:12,630 --> 00:47:17,290
Uh, we've got the ideal after a thousand
iterations, B m and D, error value.

713
00:47:17,590 --> 00:47:22,360
This happens in milliseconds because we
have such a small data set. And also, um,

714
00:47:23,680 --> 00:47:27,160
no, no, no. Actually I do have num py.
Nevermind. I, cause I just compiled it.

715
00:47:27,161 --> 00:47:30,100
What am I thinking? Hold on. So

716
00:47:35,330 --> 00:47:38,060
let's see what we've got here.

717
00:47:42,040 --> 00:47:42,873
Okay.

718
00:47:43,220 --> 00:47:46,610
I'm going to do some debugging because
we've got some time. So why not? Let's,

719
00:47:46,640 --> 00:47:51,640
let's debug Jen from tax is not
defined because we've defined it.

720
00:48:04,030 --> 00:48:05,260
Right.

721
00:48:08,740 --> 00:48:09,573
Interesting.

722
00:48:14,690 --> 00:48:19,340
Also, this is the, this is the code
that I'm, that I'm reading it from.

723
00:48:33,550 --> 00:48:37,730
There it is. Okay. So there it
is. So I'm actually curious what,

724
00:48:37,731 --> 00:48:41,830
what the error was or was it,
it was

725
00:48:43,450 --> 00:48:46,240
Jen from text,
but it wasn't defined because,

726
00:48:49,980 --> 00:48:51,120
because,

727
00:48:57,580 --> 00:49:01,150
oh, I misspelled it. That's
why de, okay. Anyway,

728
00:49:01,720 --> 00:49:06,640
it's because I misspelled it. Uh, but
yes, I misspelled the gen from text,

729
00:49:08,770 --> 00:49:10,310
so I've heard a thousand iterations.

730
00:49:10,311 --> 00:49:14,330
We get the ideal be we get the ideal
and values both of them and we can plug

731
00:49:14,331 --> 00:49:16,490
them into our equation
and it's going to give us

732
00:49:18,110 --> 00:49:21,950
the optimal line of
best fit for our values,

733
00:49:21,951 --> 00:49:26,951
which we can then use to make a prediction
given a test score or given a right.

734
00:49:27,981 --> 00:49:28,814
That's what's up.

735
00:49:34,340 --> 00:49:38,510
Yes. That's what it was. Jen from
txt, not Jen from t e x. T. That's,

736
00:49:38,540 --> 00:49:43,070
that's exactly what it
was. Thank you. Okay. Can't
confirmed Illuminati confirmed.

737
00:49:43,160 --> 00:49:46,920
So questions, any questions guys? This
is going to give us the, this is that.

738
00:49:46,970 --> 00:49:49,130
This is what this code is
going to do right here.

739
00:49:55,470 --> 00:50:00,000
Exactly right. When, when
it's all about winning. Okay,

740
00:50:00,001 --> 00:50:03,960
so we've got any questions?
How many people do we have in here?

741
00:50:03,961 --> 00:50:07,260
We have a good number to 42
that's a good number. Okay.

742
00:50:07,261 --> 00:50:11,670
Single closed set of double how to use
the momentum technique in order to find

743
00:50:11,790 --> 00:50:16,760
not only the local minima
momentum is something that,

744
00:50:17,710 --> 00:50:18,543
okay,

745
00:50:19,400 --> 00:50:21,500
we don't really use in linear regression.

746
00:50:21,501 --> 00:50:26,501
We use that for a nonlinear
models with high dimensional data.

747
00:50:26,750 --> 00:50:30,770
We'll talk that later. Uh, can
we use a dynamic learning rate?

748
00:50:30,771 --> 00:50:35,120
That's a great question. Yes. Actually.
Um, if the learning rate adapts,

749
00:50:35,210 --> 00:50:38,690
ideally, that's honestly, that's
I get, that is what we should do.

750
00:50:38,840 --> 00:50:40,490
We should have a dynamic learning rate.

751
00:50:40,640 --> 00:50:43,670
We should not not just have a dynamic
learning rate. We should have dynamic.

752
00:50:44,090 --> 00:50:47,900
All of our hyper parameters be dynamic
and they're all learning in real time and

753
00:50:47,960 --> 00:50:52,090
responding to feedback and optimizing
themselves. This is actually, um,

754
00:50:52,280 --> 00:50:55,130
what's like on the bleeding edge
of machine learning right now.

755
00:50:55,131 --> 00:50:58,190
How do we learn to learn?
So it's kind of like a metal learning.

756
00:50:58,490 --> 00:51:00,140
Three more questions and
then we're out of here.

757
00:51:00,890 --> 00:51:03,110
Is it possible to overshoot the minimum?
Yes.

758
00:51:03,140 --> 00:51:06,500
And so that's why we have a learning
rate so we don't overshoot it. Um,

759
00:51:07,340 --> 00:51:10,250
for larger datasets, yes, we can
absolutely overshoot our minimum.

760
00:51:10,520 --> 00:51:15,110
Our model could just be training
forever and never converge to more.

761
00:51:15,140 --> 00:51:18,350
Are you going to implement
Lstm from scratch? Uh, yeah.

762
00:51:18,710 --> 00:51:20,630
I actually have a video where I do that.

763
00:51:20,660 --> 00:51:25,340
It is a generates Wikipedia articles.
Check that out.

764
00:51:26,210 --> 00:51:28,400
And then one more question.

765
00:51:32,710 --> 00:51:37,630
How large can our dataset
be with this technique?

766
00:51:38,080 --> 00:51:42,760
What if we had billions of data points,
uh,

767
00:51:43,150 --> 00:51:47,680
with this technique? This,
okay, so this technique will
scale this. We'll scale, uh,

768
00:51:47,681 --> 00:51:50,650
and it's a linear,
no pun intended,

769
00:51:50,651 --> 00:51:53,680
relationship to the amount of data
that you have. So the more data,

770
00:51:53,710 --> 00:51:57,790
the longer the training time, but this
will work for larger datasets that, that,

771
00:51:58,030 --> 00:52:01,630
that are linear. So that means that
there's, you know, an x, y value pair,

772
00:52:01,631 --> 00:52:05,680
two values, uh, that you want
to find the optimal form. Okay,

773
00:52:05,681 --> 00:52:06,760
I'll take one more question.

774
00:52:11,600 --> 00:52:15,080
Is the feedback related
to backpropagation? This
is not backpropagation.

775
00:52:15,081 --> 00:52:19,790
So backpropagation is a
form of gradient descent.

776
00:52:20,150 --> 00:52:21,920
This is not backpropagation.

777
00:52:22,310 --> 00:52:25,190
This is gradient descent
for linear aggression.

778
00:52:25,340 --> 00:52:28,970
If we take gradient descent and
apply it to deep neural networks,

779
00:52:29,180 --> 00:52:31,850
that's when it becomes backpropagation.

780
00:52:32,180 --> 00:52:34,700
So grading dissent is
the big is a big boy.

781
00:52:34,701 --> 00:52:39,170
And then backpropagation is
an implementation of grading
the scent where we are

782
00:52:39,171 --> 00:52:43,430
descending or gradient by propagating
an error backwards across all of our

783
00:52:43,431 --> 00:52:47,810
layers. So we go forwards and then we go
backwards. That's it for the questions.

784
00:52:48,500 --> 00:52:52,700
Thanks guys for showing up
and for learning this stuff.
This is the good stuff.

785
00:52:52,730 --> 00:52:54,320
Okay.
It's only going to get better.

786
00:52:54,321 --> 00:52:57,890
It's only going to be a
cooler from here on out. Okay.

787
00:52:57,891 --> 00:53:01,820
So for everything that you learn, moving
on from this is going to be awesome.

788
00:53:02,030 --> 00:53:05,270
So thanks guys for
showing up. Uh, for now,

789
00:53:05,630 --> 00:53:09,980
I've got to go descend my own gradients
in life, so thanks for watching.

