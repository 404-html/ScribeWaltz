1
00:00:00,120 --> 00:00:03,030
Hello world.
It's a Raj and mx net.

2
00:00:03,060 --> 00:00:05,640
It's Amazon's deep learning library.

3
00:00:05,730 --> 00:00:10,560
It's right up there with tensorflow
and cafe as one of the most used deep

4
00:00:10,561 --> 00:00:12,840
learning libraries out there.
And this video,

5
00:00:12,841 --> 00:00:17,700
I'm going to build a very simple neural
network using mx net and I'm going to go

6
00:00:17,701 --> 00:00:21,390
over all the different parts of its
Api so you could see how it works.

7
00:00:21,391 --> 00:00:23,490
It's got this imperative Api,

8
00:00:23,700 --> 00:00:27,270
it's got a distributed
training done really well.

9
00:00:27,420 --> 00:00:29,730
And what we're going to do
is talk about its features,

10
00:00:29,840 --> 00:00:31,830
compare it to other deep
learning frameworks,

11
00:00:31,950 --> 00:00:35,340
and then start writing some code in python
in this stupid or notebook that I've

12
00:00:35,341 --> 00:00:38,880
got up.
So an overview of mx net.

13
00:00:38,881 --> 00:00:41,840
It was founded by a couple
of universities. Uh, it's,

14
00:00:41,841 --> 00:00:46,490
it supports pretty much any time.
I mean it says here's CNNS and Lstm,

15
00:00:46,770 --> 00:00:50,070
but you can really make any type of neural
network that's been discovered so far

16
00:00:50,071 --> 00:00:53,670
with mx net. Uh, it
scalable so it to flow.

17
00:00:53,760 --> 00:00:56,790
But one thing that it has a
tensor flow does not have,

18
00:00:57,000 --> 00:01:00,780
is multilanguage support specifically
for a scholar and for our,

19
00:01:00,781 --> 00:01:04,000
and then for a few other languages
that we're going to look at. Uh,

20
00:01:04,050 --> 00:01:07,560
it's got a vibrant ecosystem both
in academia and in the industry.

21
00:01:07,561 --> 00:01:11,490
We have some examples of companies that
are using mx net right here in this

22
00:01:11,820 --> 00:01:16,740
picture right here. Baidu for
example, a University of Alberta.

23
00:01:16,890 --> 00:01:19,830
And why you Intel, uh,
and then for its features,

24
00:01:19,831 --> 00:01:22,950
I just kind of like screenshot
of this from the mx net website,

25
00:01:22,951 --> 00:01:26,370
but it's flexible. I mean that's just
kind of a generic term. Let's be real.

26
00:01:26,580 --> 00:01:31,500
It's portable. Okay. So it runs on CPU
or Gpu? Yes. Sounds good so far. Uh,

27
00:01:31,930 --> 00:01:35,040
right. We talked about multiple
languages, auto differentiation.

28
00:01:35,041 --> 00:01:38,310
What that means is that you can use
this to perform gradient descent

29
00:01:38,311 --> 00:01:41,340
automatically with a single function call.
So it is tends to flow,

30
00:01:41,341 --> 00:01:43,530
so it does a lot of the other libraries,
but it has this,

31
00:01:43,830 --> 00:01:45,360
it's distributed on the cloud.

32
00:01:45,361 --> 00:01:48,150
So it's actually very easy to
distribute your computation.

33
00:01:48,151 --> 00:01:51,330
And we're going to talk about that more
so than I would say even tensorflow.

34
00:01:51,600 --> 00:01:53,970
And then performance wise,
it's pretty damn fast.

35
00:01:53,971 --> 00:01:56,390
I mean I've looked on the
web and I've seen several uh,

36
00:01:56,580 --> 00:02:00,630
performance metrics like from several
articles of comparing mx nets performance

37
00:02:00,631 --> 00:02:04,050
versus tensorflow. And when it
comes to bigger, bigger datasets,

38
00:02:04,200 --> 00:02:08,880
mx net tends to outperform tensorflow.
So, which is interesting, but you know,

39
00:02:08,881 --> 00:02:10,860
because tensorflow has so
much activity on get hub,

40
00:02:10,861 --> 00:02:14,550
like more so than any other deep learning
framework yet mx net performance,

41
00:02:14,551 --> 00:02:17,520
better at scale in terms of datasets.
Uh,

42
00:02:17,940 --> 00:02:22,290
here's his other great comparison
graph comparing the top, you know,

43
00:02:22,291 --> 00:02:26,970
seven frameworks to each other based
on a, you know, a series of metrics,

44
00:02:26,971 --> 00:02:28,680
but then notice how yeah,

45
00:02:28,740 --> 00:02:33,060
mx net really excels when
it comes to multiple GPU
support. Uh, that's really the,

46
00:02:33,140 --> 00:02:36,540
the, the main thing and we're going to
go into that in code. If we look at this,

47
00:02:36,541 --> 00:02:40,050
a chart here of AI frameworks
and then AI infrastructure,

48
00:02:40,560 --> 00:02:45,240
mx net was really built with AWS in mind.
So if you plan on using AWS,

49
00:02:45,600 --> 00:02:50,100
mx net is probably the way to go just
because it works so well with AWS.

50
00:02:50,310 --> 00:02:54,480
You've got these prebuilt ams that are
machine images that you can just preload

51
00:02:54,481 --> 00:02:57,930
into an AWS instance,
like an [inaudible] or a GPU instance.

52
00:02:58,230 --> 00:03:02,530
And then whatever you're running with
mx net does tends to work more often,

53
00:03:02,650 --> 00:03:05,170
get less errors and if you're
using other frameworks,

54
00:03:06,370 --> 00:03:09,700
so then right down to tensorflow versus
mx net, forget the other frameworks.

55
00:03:10,060 --> 00:03:13,780
They're both multi Gpu Mxn is distributed,
tens of is not.

56
00:03:13,810 --> 00:03:16,750
This is actually changed.
So tensorflow is all distributed now.

57
00:03:17,080 --> 00:03:20,860
However mx net makes it easier to do.
So they're both mobile friendly. Okay.

58
00:03:20,861 --> 00:03:24,550
So now onto its really its core
features that that I really liked.

59
00:03:24,551 --> 00:03:28,180
The first one is the fact
that it has an imperative API,

60
00:03:28,181 --> 00:03:31,200
like compared to tensorflow,
it's got this imperative APIs.

61
00:03:31,210 --> 00:03:32,830
You might be wondering what
are you talking about here?

62
00:03:32,831 --> 00:03:36,580
So this is an example of imperative
programming, right? Okay.

63
00:03:36,581 --> 00:03:40,540
So what we're doing here is imperative
programming is a way to say,

64
00:03:40,660 --> 00:03:42,190
define by run.

65
00:03:42,191 --> 00:03:47,191
What that means is we're going to define
the parts of this computation graph

66
00:03:47,651 --> 00:03:50,560
that we're building. Like these
are, these are two matrices.

67
00:03:50,830 --> 00:03:55,450
We multiply them together and then we
can add, add other operations to that.

68
00:03:55,600 --> 00:03:58,720
So Pi Torch is another example
of an imperative framework,

69
00:03:59,020 --> 00:04:02,380
but then declarative
programming is defined and run.

70
00:04:02,381 --> 00:04:04,960
So we define the variables first,
then we run it.

71
00:04:04,990 --> 00:04:07,690
The problem is that this is
a static computation graph.

72
00:04:07,870 --> 00:04:11,560
It can't change over time, right? So you
define your variables, your operations,

73
00:04:11,710 --> 00:04:12,970
and then you run the program.

74
00:04:13,300 --> 00:04:15,270
This means that if you have
something like what say,

75
00:04:15,310 --> 00:04:16,900
let's say a recurrent network,

76
00:04:17,230 --> 00:04:21,970
a recurrent network works better
with a declarative, uh, with, sorry,

77
00:04:21,971 --> 00:04:26,140
an imperative, a programming approach
because it's defined during runtime.

78
00:04:26,141 --> 00:04:28,540
So any of the newer,
deep learning models,

79
00:04:28,600 --> 00:04:30,820
they use these features that
are real time. So you know,

80
00:04:30,821 --> 00:04:32,140
data is changing in real time.

81
00:04:32,141 --> 00:04:35,080
So that's why something like Pi Torch
works better than tensorflow because

82
00:04:35,081 --> 00:04:39,030
intention flow, you define the graph,
then you run it. Oh, with Pi torture,

83
00:04:39,040 --> 00:04:43,510
with Amex Net, you define the graph and
then it can modify itself. It's not a,

84
00:04:43,511 --> 00:04:47,320
it's not an immutable graph. It's
mutable. So it changes over time,

85
00:04:47,560 --> 00:04:49,330
which is good for a lot
of the newer models.

86
00:04:49,331 --> 00:04:52,390
The big con of this is that
it's hard to optimize. You know,

87
00:04:52,391 --> 00:04:56,740
there were some early on decisions that
the tensorflow team May to say this is

88
00:04:56,741 --> 00:04:59,470
going to be a declarative
language framework.

89
00:04:59,500 --> 00:05:02,620
And the reason was because it was
just easier to optimize. However,

90
00:05:02,621 --> 00:05:04,720
as the deep learning space
progressed over time,

91
00:05:04,900 --> 00:05:09,250
we found that imperative programming
was a better model for these real time

92
00:05:09,251 --> 00:05:11,410
changing models.
These to cast tech models,

93
00:05:11,620 --> 00:05:14,080
like one example would be
variational auto encoders.

94
00:05:15,430 --> 00:05:17,890
So then onto the mixed
programming paradigm, right?

95
00:05:17,891 --> 00:05:20,830
This is an example of how
Amex net actually does both.

96
00:05:20,831 --> 00:05:23,140
So you can go full imperative if you,
if you'd like,

97
00:05:23,290 --> 00:05:27,220
or you could use this symbolic execution,
that declarative execution.

98
00:05:27,221 --> 00:05:31,070
So you had that option. However, most
people, if you look at mx net code on,

99
00:05:31,270 --> 00:05:35,690
on get hub, most of it is imperative.
So that's it for the explanation.

100
00:05:35,691 --> 00:05:37,840
And let's just start coding
this thing, right? So, uh,

101
00:05:38,120 --> 00:05:41,950
I'm going to go over three
different features that I'm
going to go over the Ndra

102
00:05:41,950 --> 00:05:46,880
Api. I'm going to go over the module Api
and then I'm going to go over the, uh,

103
00:05:47,260 --> 00:05:48,520
symbol Api.
Okay.

104
00:05:48,521 --> 00:05:52,720
So these are the three really the
main parts of the mx net framework.

105
00:05:53,830 --> 00:05:57,300
So we can install it with this very simple
command. Pseudo, so that installs it.

106
00:05:57,301 --> 00:05:59,590
The simple line right here,
make sure you have Sudo installs.

107
00:05:59,591 --> 00:06:02,960
So the NDRA API is a core
data structure for mx net.

108
00:06:03,260 --> 00:06:07,280
It represents a multidimensional array,
an end dimensional array. And this,

109
00:06:07,281 --> 00:06:11,000
this enables imperative computation.
It executes code leisurely,

110
00:06:11,001 --> 00:06:15,380
allow me to automatically parallelize
multiple operations across the available

111
00:06:15,381 --> 00:06:16,214
hardware.

112
00:06:16,220 --> 00:06:19,580
So this is good for neural
networks because neural
networks are operating on end

113
00:06:19,581 --> 00:06:23,120
dimensional data rights. So if
you have a single feature, right,

114
00:06:23,121 --> 00:06:26,210
that's one dimension. If you had a
second feature that's two dimensions.

115
00:06:26,210 --> 00:06:29,570
If you had a third feature that's three
dimensions and you can keep going, right?

116
00:06:29,571 --> 00:06:29,781
You know,

117
00:06:29,781 --> 00:06:34,781
you can have a car's tire length and its
speed and its philosophy and its shape

118
00:06:35,481 --> 00:06:36,670
and all these things,
you know,

119
00:06:36,700 --> 00:06:41,270
so it's a multidimensional data structure
that we're inputting into a neural

120
00:06:41,271 --> 00:06:41,770
networks.

121
00:06:41,770 --> 00:06:46,310
You could think of all of these features
as dimensions and you can just think of

122
00:06:46,311 --> 00:06:49,700
a single data points, right?
A single car, for example,

123
00:06:50,000 --> 00:06:53,390
as a giant matrix or an
end dimensional array.

124
00:06:53,840 --> 00:06:56,720
And so the way that, uh,
mx had deals with this is,

125
00:06:56,721 --> 00:07:00,870
it calls it the end d array API.
And so there's an image of a,

126
00:07:01,020 --> 00:07:02,930
a handwritten character.
If you break it down,

127
00:07:02,931 --> 00:07:06,230
it's really just a two dimensional array
of numbers. If you add color to it,

128
00:07:06,231 --> 00:07:09,200
then it's a three dimensional array
and et cetera, et Cetera, et cetera.

129
00:07:09,201 --> 00:07:12,350
So let's start coding this so we
could see, you know how this works.

130
00:07:12,710 --> 00:07:17,710
So what I'll do is I'll create this nd
array and I'll say it's going to have

131
00:07:21,820 --> 00:07:24,950
an a,
it's going to have a two dimensional,

132
00:07:25,770 --> 00:07:30,730
a data structure.
Okay.

133
00:07:30,731 --> 00:07:34,840
And so now I can say a dot size six.
Okay.

134
00:07:34,841 --> 00:07:39,430
So it's got six different data
points inside of that nd array.

135
00:07:40,600 --> 00:07:42,610
I can also say,
Hey dot shape.

136
00:07:43,840 --> 00:07:46,930
So it's a two by three nd array object,
right?

137
00:07:46,931 --> 00:07:50,320
So he's got two data points and inside
there are three different data points

138
00:07:50,321 --> 00:07:55,300
inside of there. So it's two by three
dimensions. Okay. So then we can say

139
00:07:57,340 --> 00:07:59,610
we can even customize the,
um,

140
00:08:01,900 --> 00:08:06,660
we can customize what it holds.
So we can say using non pies,

141
00:08:06,670 --> 00:08:10,710
a dependency which is made from
h matrix math. We can say, uh,

142
00:08:11,500 --> 00:08:13,120
let's say we've got that same object,
right?

143
00:08:13,121 --> 00:08:16,210
So let's just copy and paste that in here.
It's that same object

144
00:08:20,430 --> 00:08:25,200
and we're going to define the data
type has numb pies into 32 data type.

145
00:08:26,040 --> 00:08:29,610
And then we'll say, well, what does
that data type that we just defined?

146
00:08:30,090 --> 00:08:34,470
And it's going to say in 32.
Now if we want to print it,

147
00:08:34,740 --> 00:08:39,060
we can say what is inside of this object?

148
00:08:39,270 --> 00:08:41,210
And then it prints it out.
Uh,

149
00:08:41,250 --> 00:08:44,760
we can also perform a series of
matrix operations on it very easily.

150
00:08:45,000 --> 00:08:49,800
If I just say, um, let me
just copy that back over here.

151
00:08:50,610 --> 00:08:54,150
The equals,
I'll call it a equals.

152
00:08:55,120 --> 00:08:59,450
Okay. I'll say B equals
a times a. So there's,

153
00:08:59,460 --> 00:09:01,860
there's are a element wise,

154
00:09:01,861 --> 00:09:04,730
matrix multiplication
and then be as num Pi.

155
00:09:05,040 --> 00:09:07,170
Tell us what's inside of be very easily.

156
00:09:10,720 --> 00:09:11,553
And there we go.

157
00:09:14,010 --> 00:09:16,020
So if we wanted to do
something like a dot product,

158
00:09:16,080 --> 00:09:19,380
if we want to do a dot product,
then we can say something like, uh,

159
00:09:20,940 --> 00:09:25,940
let's say we have the same object here
and then we say B is going to be the

160
00:09:28,131 --> 00:09:31,830
transpose of a right now
and let's do a dog product.

161
00:09:31,831 --> 00:09:36,831
So we're going to use the NDRA API APIs
dot function using both of our matrices.

162
00:09:38,040 --> 00:09:42,870
And then we can say, well what did we
just get using the as num Pi function?

163
00:09:44,360 --> 00:09:45,120
Okay.

164
00:09:45,120 --> 00:09:49,050
And it's saying uh,
mx.

165
00:09:51,350 --> 00:09:55,450
Oh yeah, right. Duh. There we go.

166
00:09:56,320 --> 00:09:59,950
So we can also define the,
what we want it to run on.

167
00:09:59,951 --> 00:10:02,920
Like we could say we want it to run on
the CPU, we want it to run on the GPU,

168
00:10:02,921 --> 00:10:05,410
et cetera. Right? So right
now we're just saying CPU,

169
00:10:05,710 --> 00:10:10,090
but we can really have it run on
any type of hardware, CPU, Gpu,

170
00:10:10,091 --> 00:10:14,830
and we can define that right here and
I'll just run that. Just like that. Okay.

171
00:10:14,831 --> 00:10:18,550
So that's it for the Ndra Api. Now
let's talk about the symbol Api. Okay.

172
00:10:18,551 --> 00:10:20,620
So this is a symbolic graph,
right?

173
00:10:20,621 --> 00:10:25,150
If you've ever visualize a computation
graph using tensor board or any of those

174
00:10:25,151 --> 00:10:28,750
tools, you'll see that it
looks like a graph like this,
right? Well, it's happening.

175
00:10:28,751 --> 00:10:33,550
Are these tensors are flowing through
these computation, um, these operators,

176
00:10:33,551 --> 00:10:36,820
right? So it's saying, take this
data point, take this data point,

177
00:10:36,821 --> 00:10:40,720
apply some matrix multiplication to it,
and take that output and then, you know,

178
00:10:40,721 --> 00:10:43,150
keep performing all
these operations to it.

179
00:10:43,151 --> 00:10:47,650
And this is essentially a neural
network. It's a computation graph. So,

180
00:10:48,530 --> 00:10:48,950
okay,

181
00:10:48,950 --> 00:10:50,930
here's an example of, of,
of what this looks like.

182
00:10:50,931 --> 00:10:54,710
So instead of doing it to the imperative
way, we can do it the declarative way,

183
00:10:54,711 --> 00:10:57,770
which is very similar, similar to
tensorflow, if we'd like to write.

184
00:10:58,330 --> 00:11:01,800
So it depends on your use case.
If you're doing a static, um,

185
00:11:02,000 --> 00:11:04,400
if you're doing a perceptron,
a feed forward neural network,

186
00:11:04,401 --> 00:11:07,700
you'd want to use a static,
um, computation graph.

187
00:11:07,730 --> 00:11:10,310
If you're doing a recurrent
network and LSTM network,

188
00:11:10,490 --> 00:11:12,830
then you'd probably want to
use the imperative a style.

189
00:11:12,831 --> 00:11:16,400
So this here's a stall for both of them.
So let's see what else we can do here.

190
00:11:16,401 --> 00:11:21,000
So if we say I'm a, let's
just, let's just see what,

191
00:11:21,001 --> 00:11:24,980
what's contained inside of
these variables. So we say run,

192
00:11:25,910 --> 00:11:28,230
we can see that these are all nd arrays.

193
00:11:28,231 --> 00:11:32,420
So we could see everything that's stored
inside of them and he was a result of

194
00:11:32,421 --> 00:11:35,720
all of those operations. And there we
go. We've got all of that there as well.

195
00:11:35,840 --> 00:11:39,470
But what type of a variable is,
well,

196
00:11:39,500 --> 00:11:42,170
it's an Ndra have an nd
array of an India Ray. Okay,

197
00:11:43,700 --> 00:11:48,530
three dimensions. So now, um, we'll say,

198
00:11:48,531 --> 00:11:52,700
well, what are the arguments
to eat? Like what makes up e?

199
00:11:52,730 --> 00:11:57,400
So we use the list arguments
function on that.

200
00:11:57,610 --> 00:12:02,320
So be consistent a, B, c, and d.
And we can say, okay, so now let's,

201
00:12:02,380 --> 00:12:05,350
um,
list the outputs.

202
00:12:12,780 --> 00:12:16,260
So what it says is that he depends
on variables a, B, c, and d,

203
00:12:16,650 --> 00:12:21,650
that the operation that computes he is
a sum and that he is indeed a B plus CD.

204
00:12:21,920 --> 00:12:23,850
And so we can do much more
than just plus or minus.

205
00:12:23,851 --> 00:12:26,520
We can do multiplication and division
and a lot of different operations,

206
00:12:26,521 --> 00:12:30,870
but those are the ones that we're
doing right here. So now if we,

207
00:12:30,930 --> 00:12:34,740
like if we,
if we want to bind data to operations,

208
00:12:34,860 --> 00:12:38,510
we can do that using a binding. Okay.
So let's say we've got this data, right?

209
00:12:38,630 --> 00:12:40,170
We've got four different data points.

210
00:12:40,200 --> 00:12:45,200
They're all of type in 32 and what we
want to do is we want to bind each MD or

211
00:12:45,781 --> 00:12:47,910
nd array to its corresponding symbol.

212
00:12:48,240 --> 00:12:52,920
So to do that we're going to have to
initialize an executer. So an executer,

213
00:12:55,440 --> 00:12:56,273
okay,

214
00:12:57,180 --> 00:13:01,830
is e. Dot. Bind? It was, we're
going to bind together on the CPU,

215
00:13:02,670 --> 00:13:04,470
several different variables with a,

216
00:13:04,471 --> 00:13:07,320
we're going to bind the
variable a to whatever's in aid,

217
00:13:07,380 --> 00:13:11,460
a data and then hold on.
And then

218
00:13:11,860 --> 00:13:12,693
B

219
00:13:13,950 --> 00:13:18,800
to whatever's in B data.
I also want to bind.

220
00:13:18,830 --> 00:13:22,730
See,
and I also want to bind

221
00:13:25,250 --> 00:13:26,083
d.

222
00:13:27,690 --> 00:13:31,590
There we go. So that's our
executer and we can say, well,

223
00:13:31,591 --> 00:13:35,460
what's in this executer?
And he'll tell us what,

224
00:13:35,461 --> 00:13:38,790
you've got an executer
object. Great. So now we can,

225
00:13:39,930 --> 00:13:42,000
we can let our input data
flow through the graph.

226
00:13:42,001 --> 00:13:43,470
And the way we're going to do that,
or it's going to say,

227
00:13:43,640 --> 00:13:45,990
well he data is going to,
we're going to use the x,

228
00:13:46,020 --> 00:13:48,480
we're going to use a forward
function of the executer.

229
00:13:48,690 --> 00:13:51,300
And that's going to afford all that
data through that computation graph,

230
00:13:51,301 --> 00:13:55,440
using the executer as the medium
to do that. And then we can say,

231
00:13:55,441 --> 00:14:00,441
well what do we got an e now and that
it's got this result 14 that's the result

232
00:14:01,060 --> 00:14:03,280
of all of those computations
that we performed.

233
00:14:06,300 --> 00:14:10,170
Okay? So the last bit is the module Api.
Now we can build our neural network. Okay,

234
00:14:10,410 --> 00:14:12,760
so to build our neural network,
we're going to do this. Okay?

235
00:14:13,350 --> 00:14:16,170
So what we're gonna do is we're going
to build a neural network that will

236
00:14:16,171 --> 00:14:18,750
predict a co a class or category,

237
00:14:18,810 --> 00:14:22,830
like 10 different ones from a sample of
a thousand different datas data points.

238
00:14:22,831 --> 00:14:26,520
Okay. So these are imaginary data
points and we can import these directly.

239
00:14:26,790 --> 00:14:30,630
So the first thing we're going to do
is obviously in port our dependencies,

240
00:14:30,631 --> 00:14:33,120
and then we're going to say, oh,
we want a thousand data points.

241
00:14:33,450 --> 00:14:36,180
We want 800 of them to be for training.
We have,

242
00:14:36,181 --> 00:14:37,890
we want the rest to be for testing.

243
00:14:38,400 --> 00:14:40,830
Here's how many features are
going to be 10 categories.

244
00:14:40,831 --> 00:14:44,760
And with a batch size of 10.
So every training and a rash iteration,

245
00:14:44,910 --> 00:14:48,810
we're training our model on 10
different data points every time. Okay.

246
00:14:48,811 --> 00:14:53,500
So now we can generate our dataset.
So to generate our Dataset,

247
00:14:53,750 --> 00:14:57,740
or we're going to use the
uniform function of, uh,

248
00:14:57,830 --> 00:15:02,450
the Ndra Api.
And to do that unit form

249
00:15:04,650 --> 00:15:09,650
below equal zero Pi equals
one shape equals sample count,

250
00:15:11,360 --> 00:15:12,220
feature account.

251
00:15:13,910 --> 00:15:18,050
And then we can print it out and
see what we've just created. And

252
00:15:20,440 --> 00:15:24,440
Oh, not Emek am decks. And
so that tells us, okay,

253
00:15:24,441 --> 00:15:26,150
you've created a data point.
It's, you know, it's got,

254
00:15:26,180 --> 00:15:27,890
it's using this uniform distribution.

255
00:15:27,891 --> 00:15:31,250
So it's just generating all these points
randomly according to some distribution.

256
00:15:31,251 --> 00:15:35,540
And then we have that and
to generate our categories,

257
00:15:36,260 --> 00:15:39,800
we'll just iterate through that, uh, this
random number generator and say, okay,

258
00:15:39,801 --> 00:15:42,110
these are going to be our categories
for each of these integers.

259
00:15:42,830 --> 00:15:46,610
Then to split our dataset into training
and testing, we'll split the data,

260
00:15:47,990 --> 00:15:51,410
right? We're going to use a crop
function to crop the data in half or not,

261
00:15:51,411 --> 00:15:54,920
not in half. 80, 20. It's
gonna be an 80, 20 split. Okay,

262
00:15:54,921 --> 00:15:57,620
so now we can build the network.
So here's what it looks like.

263
00:15:57,650 --> 00:16:00,590
It's very similar to
carrots or tensor flow.

264
00:16:00,770 --> 00:16:05,060
You're defining this neural network by
first initializing it with this data

265
00:16:05,061 --> 00:16:08,600
variable. You're saying, well, we want the
first layer to be fully connected, right?

266
00:16:08,601 --> 00:16:10,970
So a single layer
corresponds to a single line.

267
00:16:11,300 --> 00:16:13,280
It's got 64 neurons in that layer.

268
00:16:13,281 --> 00:16:16,400
So every neuron in that layer is going
to be connected to the next layer.

269
00:16:16,730 --> 00:16:20,330
We'll add this activation function
relu or rectified linear unit,

270
00:16:20,360 --> 00:16:24,350
a very popular type of activation
function, another fully connected layer,

271
00:16:24,470 --> 00:16:27,650
a soft max output, which is
essentially an activation function.

272
00:16:28,570 --> 00:16:33,310
And then we output the result
to this mod a variable. Now, uh,

273
00:16:33,380 --> 00:16:37,340
we can indeed inner iterate
on the Dataset, 10 samples
and 10 labeled at a time.

274
00:16:37,341 --> 00:16:41,330
And then we're going to call
reset to restore the iterator
to its original state.

275
00:16:41,540 --> 00:16:45,260
But we have to initialize this iterator.
Okay, so we need to build an iterator.

276
00:16:45,290 --> 00:16:47,750
That's, that's what we have to do
so that we can train in batches.

277
00:16:47,751 --> 00:16:52,280
That's what iterators or for. So here's
our iterator and it's going to say, well,

278
00:16:52,281 --> 00:16:56,000
train on the x data, the wide data
and with the batch size being batch,

279
00:16:56,120 --> 00:17:00,110
which we defined up here,
10 and then we'll say,

280
00:17:00,290 --> 00:17:05,060
train the model using this bind function,
here's the optimizer,

281
00:17:05,690 --> 00:17:10,130
go for it. And that's it. I
already initialize my optimism
miser before. So it's,

282
00:17:10,160 --> 00:17:10,971
it's taking a while,

283
00:17:10,971 --> 00:17:14,120
but basically this runs in like
10 seconds and then that's it.

284
00:17:14,480 --> 00:17:17,570
That's it for the mx and an API.
I hope you found this video useful.

285
00:17:17,600 --> 00:17:19,790
Please subscribe for more
programming videos. And for now,

286
00:17:19,791 --> 00:17:22,970
I've got to research some deep learning
frameworks, so thanks for watching.

