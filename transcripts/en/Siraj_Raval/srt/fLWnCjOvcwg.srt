1
00:00:01,620 --> 00:00:04,020
How do we prevent an AI apocalypse?

2
00:00:04,410 --> 00:00:08,970
The Dutch philosopher Spinosa once said
that peace is not an absence of war.

3
00:00:09,120 --> 00:00:13,530
It is a virtue, a state of mind,
a disposition for benevolence,

4
00:00:13,620 --> 00:00:15,210
confidence,
justice,

5
00:00:15,510 --> 00:00:19,820
all the great systems we have in our
lives exist only because people who had a

6
00:00:19,821 --> 00:00:24,540
vision of a more beautiful world,
we'll them into reality through hard work.

7
00:00:24,810 --> 00:00:28,590
We've created layers and layers
of these systems, systems of law,

8
00:00:28,591 --> 00:00:31,620
of transportation, of social
interaction, of health care,

9
00:00:31,980 --> 00:00:36,510
and they've been entirely reliant on
human intelligence for thousands of years.

10
00:00:36,720 --> 00:00:38,310
In the past few decades,
though,

11
00:00:38,400 --> 00:00:42,570
we've begun the process of creating
machine intelligence to improve them.

12
00:00:42,990 --> 00:00:45,750
The first phase involved
rule based intelligence,

13
00:00:45,960 --> 00:00:48,000
a series of if then statements,

14
00:00:48,180 --> 00:00:51,630
whether using a traffic light instead
of a human traffic controller or an

15
00:00:51,631 --> 00:00:55,410
automatic fair calculator. Instead of
trusting a driver for a fair price,

16
00:00:55,710 --> 00:00:59,100
we've collectively put our
trust into machine intelligence.

17
00:00:59,310 --> 00:01:03,150
Whereas before we had to trust humans.
It was scary at first.

18
00:01:03,151 --> 00:01:06,240
Initially we didn't understand
how the technology worked,

19
00:01:06,510 --> 00:01:10,530
but eventually it became the norm.
We found that it made the system better.

20
00:01:10,740 --> 00:01:15,660
If we trusted the machine to do the job.
We then moved on to using heuristics.

21
00:01:15,690 --> 00:01:19,260
In the next phase, our machines
started making educated guesses,

22
00:01:19,470 --> 00:01:23,220
whether that be finding the shortest
path between two points or predicting the

23
00:01:23,221 --> 00:01:25,110
most likely weather for tomorrow.

24
00:01:25,350 --> 00:01:28,530
We again learn to trust
them to do those jobs well.

25
00:01:28,920 --> 00:01:31,800
Now we're in the last phase,
the learning phase.

26
00:01:32,010 --> 00:01:35,730
We began teaching our machines had
to learn for themselves. Eventually,

27
00:01:35,731 --> 00:01:38,550
our cars will learn to
drive themselves chemists.

28
00:01:38,640 --> 00:01:41,940
We'll be able to teach their machines
to discover new drugs for themselves.

29
00:01:42,300 --> 00:01:46,710
Every single system we built up until
now will eventually be able to learn for

30
00:01:46,711 --> 00:01:51,420
itself just like we do.
But should we trust our learning systems?

31
00:01:51,600 --> 00:01:55,620
What happens when we solve intelligence
entirely and what does a benevolent

32
00:01:55,700 --> 00:01:58,410
superintelligence look like?
The famous Dutch painter,

33
00:01:58,411 --> 00:02:00,380
Rembrandt used to roam
the streets of Amsterdam,

34
00:02:00,390 --> 00:02:04,680
finding inspiration in these canal walk
waits for some of his greatest pieces in

35
00:02:04,681 --> 00:02:07,770
his studio. He would ask the
students to try and emulate work,

36
00:02:08,160 --> 00:02:10,350
work that took him decades
of training to create.

37
00:02:10,680 --> 00:02:14,730
We can now condense all that training
time using machine intelligence so that a

38
00:02:14,731 --> 00:02:18,900
novice can emulate his style
instantly and that goes for any skill.

39
00:02:19,020 --> 00:02:22,830
We'll all be able to master any
skill instantly by augmenting our own

40
00:02:22,831 --> 00:02:24,420
intelligence with machines.

41
00:02:24,720 --> 00:02:28,290
Our world and its current state
though is filled with suffering.

42
00:02:28,560 --> 00:02:33,060
Life is hard even for the most well
off the ultra rich purchase material

43
00:02:33,061 --> 00:02:36,300
possessions to account for the
loss of what really matters,

44
00:02:36,510 --> 00:02:39,180
the loss of connection,
the loss of intimacy,

45
00:02:39,181 --> 00:02:42,150
the loss of community AI can change that.

46
00:02:42,360 --> 00:02:46,950
It can end suffering as we know it
entirely. Enabling Utopia on earth,

47
00:02:47,130 --> 00:02:48,840
ending all war and disease,

48
00:02:49,020 --> 00:02:53,970
using insights that only a
super intelligent machine
could give us in a matter

49
00:02:53,970 --> 00:02:58,050
of seconds. Then girl once said, for my
part, I know nothing with any certainty,

50
00:02:58,290 --> 00:03:00,280
but the side of the makes me dream.

51
00:03:00,730 --> 00:03:05,730
He actualized his dreams using a canvas
and soon we'll be able to actualize our

52
00:03:05,861 --> 00:03:08,200
own using AI as our canvas.

53
00:03:08,590 --> 00:03:12,550
Whether that means generating entirely
new realities to live out our wildest

54
00:03:12,551 --> 00:03:17,200
fantasies in or creating a universal
problem solver where given any objective

55
00:03:17,201 --> 00:03:20,710
function, it can optimize for
it by minimizing an error value.

56
00:03:20,830 --> 00:03:25,390
But who gets to decide what that objective
function is for all the good that AI

57
00:03:25,391 --> 00:03:28,960
could bring us? There's also the
possibility of it being used for evil.

58
00:03:29,170 --> 00:03:33,040
Holland is world renowned for its
tolerant attitude and openminded spirit,

59
00:03:33,280 --> 00:03:34,690
but just a few decades ago,

60
00:03:34,691 --> 00:03:39,160
it's own police and civil service
forces were used to transport tens of

61
00:03:39,160 --> 00:03:41,530
thousands of Jews to
extermination camps in Germany,

62
00:03:41,800 --> 00:03:43,570
including Anne Frank and her family.

63
00:03:43,900 --> 00:03:47,500
Adolf Hitler was able to manipulate
hundreds of thousands of people into

64
00:03:47,501 --> 00:03:51,490
believing his own twisted ideology if
Hitler was able to get his hands on

65
00:03:51,500 --> 00:03:54,880
superintelligence.
We all be living in a nightmarish reality.

66
00:03:55,180 --> 00:03:56,920
As generative models improve,

67
00:03:57,070 --> 00:03:59,860
the ability to manipulate
people is becoming easier.

68
00:04:00,100 --> 00:04:04,600
Moneyed interests have already begun
conducting surveillance of our actions and

69
00:04:04,601 --> 00:04:08,500
analyzing our data to build a
weaponized AI propaganda machines.

70
00:04:08,740 --> 00:04:12,700
And these machines are used to manipulate
our opinions and behavior in order to

71
00:04:12,701 --> 00:04:17,620
advance political agendas in a way that's
never before been possible on AI could

72
00:04:17,621 --> 00:04:19,570
even come up with an objective of its own.

73
00:04:19,810 --> 00:04:22,690
And that objective could be to
annihilate our species entirely.

74
00:04:23,050 --> 00:04:27,430
This can happen directly because it
desires to end human life or indirectly in

75
00:04:27,431 --> 00:04:31,180
the way that we build canals to make
transport more efficient for us and in the

76
00:04:31,181 --> 00:04:34,330
process destroy countless
ecosystems of insects.

77
00:04:34,390 --> 00:04:38,230
So how do we ensure that the AI
we create is beneficial and safe?

78
00:04:38,650 --> 00:04:41,950
Superintelligence is by its
very definition smarter than us,

79
00:04:42,130 --> 00:04:46,570
able to reason at a level so different
from us that it's literally unimaginable.

80
00:04:46,900 --> 00:04:50,170
We could try and contain it to
block it from Internet access,

81
00:04:50,171 --> 00:04:52,420
but eventually it will learn to escape.

82
00:04:52,690 --> 00:04:55,960
And if one person doesn't let
their AI escape, another one will.

83
00:04:56,350 --> 00:04:59,290
We shouldn't try to stop progress in Ai.
Instead,

84
00:04:59,291 --> 00:05:02,410
we need to ensure that it's
aligned with our own values.

85
00:05:02,500 --> 00:05:06,370
We need to understand exactly how these
algorithms work so they produce the

86
00:05:06,371 --> 00:05:10,630
results that we want.
But who's to say what good values are?

87
00:05:10,840 --> 00:05:15,490
Should we be allowed to pay for
sex or consume any drug we want.

88
00:05:15,820 --> 00:05:19,360
We can't even collectively
decide on what good and evil is.

89
00:05:19,540 --> 00:05:24,220
Philosophers have been
debating what constitutes both
for thousands of years and

90
00:05:24,250 --> 00:05:26,680
AI lives on both sides of that debate.

91
00:05:26,890 --> 00:05:30,880
Viruses will use AI to learn how to
best break into security systems,

92
00:05:31,120 --> 00:05:32,230
but at the same time,

93
00:05:32,320 --> 00:05:36,370
security systems will use AI to learn
how best to defend against threats.

94
00:05:36,760 --> 00:05:40,690
Ai will increasingly be used to create
fake but realistic looking news,

95
00:05:40,870 --> 00:05:41,890
but at the same time,

96
00:05:41,950 --> 00:05:45,220
AI will also get better at detecting
what's real and what's fake.

97
00:05:45,400 --> 00:05:50,400
We're in the midst of a new arms race
and AI is the nuclear power of our time.

98
00:05:50,830 --> 00:05:55,600
As soon as any institution be
it government or corporation
learns of the power

99
00:05:55,601 --> 00:05:59,750
of Ai, they will to use it to
fulfill their own interests.

100
00:05:59,840 --> 00:06:04,430
We can learn a lot from our past.
The European enlightenment era writer,

101
00:06:04,550 --> 00:06:07,550
Lord Acton had words that
still ring true today.

102
00:06:07,610 --> 00:06:12,200
He said that freedom consists
of the distribution of
power and despotism and its

103
00:06:12,201 --> 00:06:16,190
concentration.
We must democratize this AI power.

104
00:06:16,340 --> 00:06:19,670
We must make it as accessible
to as many people as we can.

105
00:06:20,120 --> 00:06:23,780
That way the will of the collective
will override any one bad actor.

106
00:06:24,110 --> 00:06:28,340
And only by educating ourselves on
how exactly this intelligence works,

107
00:06:28,460 --> 00:06:32,050
will we be able to better align it
with our intended goals is there,

108
00:06:32,060 --> 00:06:36,410
it's up to us to ensure that our AI
values all the things that we do,

109
00:06:36,680 --> 00:06:41,510
that he cherishes laughter and connection,
that it preserves friendship and love.

110
00:06:41,600 --> 00:06:43,700
In our quest to understand intelligence,

111
00:06:44,000 --> 00:06:47,600
we'll learn more about who we are
and what it means to be human.

112
00:06:47,780 --> 00:06:52,190
And together we'll finally be able to
take that most sublime process guided by

113
00:06:52,191 --> 00:06:55,130
nature into our own hands.
Evolution.

