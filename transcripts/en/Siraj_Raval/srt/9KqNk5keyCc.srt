1
00:00:00,210 --> 00:00:00,930
Hello world.

2
00:00:00,930 --> 00:00:05,430
It's a Raj and today we're going to talk
about pose estimation using tensorflow.

3
00:00:05,431 --> 00:00:05,791
Dot.

4
00:00:05,791 --> 00:00:10,791
J S while you're seeing behind me is a
demo of realtime pose estimation using

5
00:00:11,491 --> 00:00:12,630
tensorflow dot js.

6
00:00:12,840 --> 00:00:16,740
What it's doing is it's estimating
what all my body part poses are.

7
00:00:16,741 --> 00:00:19,450
If I go back a little bit, it's going to
show like all these little things that,

8
00:00:19,451 --> 00:00:23,130
ah, okay, so you see a little bit of
that. I'm going to go back out of that.

9
00:00:23,370 --> 00:00:26,130
So that's the demo for this.
Today's video,

10
00:00:26,131 --> 00:00:29,430
and we're going to do this using
tensorflow. Dot. J Yes, this is really,

11
00:00:29,431 --> 00:00:32,760
really simple to do in the browser.
Anybody can do it.

12
00:00:32,910 --> 00:00:34,800
You don't need to
install any dependencies.

13
00:00:34,920 --> 00:00:36,930
You don't need to make any
kind of configurations.

14
00:00:36,931 --> 00:00:38,430
You don't need to download anything.

15
00:00:38,610 --> 00:00:43,050
That's the great thing about javascript
is that it's so simple to use and

16
00:00:43,290 --> 00:00:46,170
because tensorflow has now
been ported to the browser,

17
00:00:46,770 --> 00:00:50,100
pretty much anybody can make incredible
machine learning applications that

18
00:00:50,130 --> 00:00:55,110
anybody can access because pretty much
any computing device has javascript

19
00:00:55,111 --> 00:00:59,550
enabled for j for Browser access, right?
So that's what we're going to do today.

20
00:00:59,840 --> 00:01:01,740
I'm going to talk a little
bit about tensorflow. Dot.

21
00:01:01,741 --> 00:01:03,900
JS and then we're gonna go into the code,

22
00:01:04,050 --> 00:01:08,220
how pose estimation works architecturally,
programmatically, et Cetera, et cetera.

23
00:01:08,250 --> 00:01:13,200
Okay. So right. And they'll use to require
lots of time and money to get started.

24
00:01:13,410 --> 00:01:17,720
You had to build your own, you know,
deep learning rig by your own Gpu,

25
00:01:18,240 --> 00:01:22,560
you know, install this stuff,
configurations, Kuda drivers this, that,

26
00:01:22,561 --> 00:01:26,550
it was a huge headache.
And since then the whole field has,

27
00:01:26,580 --> 00:01:30,510
has kind of went into this
period of ease of use.

28
00:01:30,511 --> 00:01:33,660
Everything's been democratized,
everything's getting easier for anybody,

29
00:01:33,900 --> 00:01:35,940
which is awesome. And I
think that tensorflow. Dot.

30
00:01:35,941 --> 00:01:40,650
JS is a good example of that
being magnified in the, into the,

31
00:01:40,651 --> 00:01:41,820
into the eye of the public.

32
00:01:43,080 --> 00:01:47,400
But really there are three points that
I particularly find very interesting

33
00:01:47,401 --> 00:01:48,270
about tentraflow.
Dot.

34
00:01:48,271 --> 00:01:52,110
JS and in general machine learning
happening in the browser. W why?

35
00:01:52,111 --> 00:01:55,410
I think it's a good thing. So the
first reason is privacy, right?

36
00:01:55,411 --> 00:02:00,150
So privacy is a big issue. If you're a
health care company, if you are a, um,

37
00:02:00,210 --> 00:02:00,600
you know,

38
00:02:00,600 --> 00:02:03,840
if you have any kind of critical data
that you don't want people to know about,

39
00:02:04,080 --> 00:02:05,280
privacy is crucial.

40
00:02:05,281 --> 00:02:09,360
So ideally you could train your model
at the edge instead of on the cloud.

41
00:02:09,361 --> 00:02:12,210
That means on your device and
set up on someone else's device.

42
00:02:12,540 --> 00:02:15,090
And if you can do that,
you can keep the data local.

43
00:02:15,120 --> 00:02:17,220
You don't have to send it anywhere,
which is awesome.

44
00:02:17,580 --> 00:02:20,760
The second reason is wider distribution.
Like I said,

45
00:02:20,880 --> 00:02:24,030
pretty much any computing
device, phone, laptop,

46
00:02:24,031 --> 00:02:27,870
tablet has javascript enabled,
whether it's through a browser,

47
00:02:28,020 --> 00:02:31,230
whether it's through, you know,
whatever, they have javascript enabled,

48
00:02:31,231 --> 00:02:35,550
so you'll reach the widest audience.
The third point is distributed computing.

49
00:02:35,670 --> 00:02:37,700
So if we can leverage clients,

50
00:02:37,701 --> 00:02:41,490
side data for many users to
train a model in real time,

51
00:02:41,520 --> 00:02:44,400
I think this is particularly exciting.
If you think about it,

52
00:02:44,700 --> 00:02:47,550
you have a web app and you
deploy it to a bunch of people,

53
00:02:47,820 --> 00:02:52,820
you can train it on their data in real
time and make your model as a whole

54
00:02:53,280 --> 00:02:55,350
better, which is, which is awesome, right?

55
00:02:55,500 --> 00:02:57,660
So Uber and Waymo and all these companies,

56
00:02:57,661 --> 00:03:00,790
they're doing that for their self driving
cars. They're learning in real time.

57
00:03:00,791 --> 00:03:04,570
And then that, that those learnings are
being transferred to a global model.

58
00:03:04,900 --> 00:03:06,430
And now we can do this
for anything really.

59
00:03:06,431 --> 00:03:11,040
Not just for self driving cars but for
simple web apps, classification, you know,

60
00:03:11,140 --> 00:03:14,760
inference for all sorts of
things. Game playing, um,

61
00:03:16,210 --> 00:03:19,450
predictive analytics for, for, for
marketing companies and finance,

62
00:03:19,570 --> 00:03:22,630
the stock prices, things like that.
New Mariah is doing that in some way.

63
00:03:22,631 --> 00:03:24,700
But anyway, there's a, there's
a lot of potential there.

64
00:03:24,701 --> 00:03:28,930
And I have a video on tensorflow dot.
Js so just search it tensorflow. Dot.

65
00:03:28,931 --> 00:03:31,870
Js um,
Saroj on youtube and you'll find it.

66
00:03:31,871 --> 00:03:36,340
But I just want to go over it like one
simple part of it. So we have the, um,

67
00:03:36,400 --> 00:03:39,430
just this, this image
right here. So web GL is,

68
00:03:39,610 --> 00:03:42,400
is how graphics are computed in
the browser right now. It's a,

69
00:03:42,401 --> 00:03:43,870
it's a low level library for that.

70
00:03:44,200 --> 00:03:48,400
And so on top of what GL is the browser
and what you see on the right is soon

71
00:03:48,401 --> 00:03:50,080
because he's talking about key purpose,
right?

72
00:03:50,081 --> 00:03:52,360
So ideally you could train
this thing on Tpu is,

73
00:03:52,361 --> 00:03:56,720
but we don't really have that
yet in our machines. You know,

74
00:03:56,740 --> 00:04:01,180
we have gps and CPS, but you
can train intentional jazz
on CPS, Angie [inaudible],

75
00:04:01,210 --> 00:04:03,070
it doesn't matter. Uh, but right,

76
00:04:03,071 --> 00:04:07,030
so on top of the browser you have the
ops Api and the ops API is just like

77
00:04:07,031 --> 00:04:11,280
tensorflow it, right? You've
got variables, constants.
It's basically long code,

78
00:04:11,290 --> 00:04:14,860
you know, it's, it's detailed code of
how you define your computation graph,

79
00:04:15,010 --> 00:04:18,340
what it looks like, what the variables
are, what the parameters are,

80
00:04:18,460 --> 00:04:21,100
what the learning rate is with
all those hyper parameters are.

81
00:04:21,250 --> 00:04:24,430
You can do it in detail or
you can use the layers API,

82
00:04:24,431 --> 00:04:27,850
which is basically care os one line per
layer, Dah, Dah, Dah, Dah, Dah, Dah.

83
00:04:28,090 --> 00:04:31,120
You can do it in nine lines what you would
otherwise do in like a hundred lines,

84
00:04:31,600 --> 00:04:33,820
which is what the layers API is.
So that's,

85
00:04:33,821 --> 00:04:35,710
that's kind of high level
of what it looks like.

86
00:04:35,850 --> 00:04:37,900
If you want to do something simple,
easy layer is API.

87
00:04:37,901 --> 00:04:40,870
If you want to get detailed and make
sure your model is state of the art.

88
00:04:40,870 --> 00:04:45,460
Usually ops API and the pipeline
is actually very simple.

89
00:04:45,610 --> 00:04:49,570
So what I've seen on get hub in terms
of developer activity is a lot of

90
00:04:49,571 --> 00:04:53,860
developers are using pretrained models
because it's so easy to use, right?

91
00:04:53,861 --> 00:04:57,400
So transfer learning I think
is going to be a huge use case.

92
00:04:57,430 --> 00:05:00,580
It already is a huge use
case for tensorflow. Dot. Js.

93
00:05:00,970 --> 00:05:05,170
You can use a pretrained model, Alex
Net, mobile net, you know VGG nets,

94
00:05:05,190 --> 00:05:08,440
Google in that there's a million
pretrained models that were trained on

95
00:05:08,441 --> 00:05:11,500
different sets of images, on
Games, on human interactions,

96
00:05:11,540 --> 00:05:15,760
etc. Take one of those pretrained
models and you can use it in the browser

97
00:05:15,940 --> 00:05:20,380
easily with like a few lines of code.
So the pipeline is a three step process.

98
00:05:20,381 --> 00:05:23,950
The first part is to load a model,
right? So in order to do that,

99
00:05:24,190 --> 00:05:28,450
we need two components. We need the model
file and we need the weights manifest,

100
00:05:28,451 --> 00:05:32,110
right? So where to find the weights and
what the architecture of your model is,

101
00:05:32,111 --> 00:05:36,160
right? How many layers you have, what
are the hyper parameters and the weights,

102
00:05:36,250 --> 00:05:39,670
what does that, what does it learn? And
once you have those two things, boom,

103
00:05:39,760 --> 00:05:42,820
you have a, you have a pretrained
network that you can run in the browser.

104
00:05:44,050 --> 00:05:46,630
Programmatically speaking,
this is very simple, right?

105
00:05:46,631 --> 00:05:50,470
So we can load up a frozen model
from what wherever we have,

106
00:05:50,770 --> 00:05:54,880
we can create constants for where
our weights are, where our model is,

107
00:05:55,000 --> 00:05:57,130
what we want to name our input node,
what would you,

108
00:05:57,140 --> 00:06:01,310
what do we want to name our output now
and then we can load our frozen model

109
00:06:01,340 --> 00:06:05,540
just like this using the
model URL. Remember there's
two, those two components.

110
00:06:05,541 --> 00:06:07,340
That's all we need.
The rest of that,

111
00:06:07,400 --> 00:06:09,830
the code you see here is just
like for naming purposes,

112
00:06:09,831 --> 00:06:13,610
but this is this last line including
these two parameters are all you need.

113
00:06:13,670 --> 00:06:15,350
Is it then that single line,
that's it.

114
00:06:15,710 --> 00:06:18,860
To load up a model that's already been
trained and this could have been trained

115
00:06:18,890 --> 00:06:22,670
in tensorflow, python, it can be
trained in tensorflow, C, c plus plus,

116
00:06:22,820 --> 00:06:26,150
but you can import it into a
central node dot js very easily.

117
00:06:26,660 --> 00:06:28,570
The second step is preprocessing.

118
00:06:28,580 --> 00:06:31,790
So a neural network has a
specific input definition.

119
00:06:31,791 --> 00:06:35,720
You can't just input any kind of data,
it's got to be in the right format, right?

120
00:06:35,721 --> 00:06:37,460
You can't input a,
a,

121
00:06:37,550 --> 00:06:42,080
an 18 dimensional vector into a three
dimensional, a neural network, right?

122
00:06:42,081 --> 00:06:46,220
So if it's weight values are assuming a
certain dimension size, you can just say,

123
00:06:46,250 --> 00:06:48,470
oh, it's going to be this dimension
size and just force it in.

124
00:06:48,710 --> 00:06:49,543
It doesn't work like that.

125
00:06:49,790 --> 00:06:53,540
So you have to do some preprocessing
in order to do that. So to do that,

126
00:06:53,541 --> 00:06:57,140
first we'll convert pixels to
tensorflow. Dot. JS is input tensor.

127
00:06:58,220 --> 00:07:02,900
So we have some source and the source
is that, uh, the, the, the source of,

128
00:07:02,990 --> 00:07:06,740
of, of what, what we're training on. And
then what we can do is we can say, okay,

129
00:07:06,950 --> 00:07:08,930
let's take the input and then from pixels,

130
00:07:08,931 --> 00:07:11,000
the sources that the data
that we're training on,

131
00:07:11,150 --> 00:07:14,630
we'll convert it into pixels just like
that, that, so that's our first step.

132
00:07:14,631 --> 00:07:15,380
So now it's in these,

133
00:07:15,380 --> 00:07:19,010
this pixel aided format that's
appropriate potential flow that Jay, yes.

134
00:07:19,520 --> 00:07:23,540
Then we'll crop the inputs. So if we want
to use part of the image, so we'll say,

135
00:07:23,541 --> 00:07:26,210
where's the mobile net.
Dot. Crop image input.

136
00:07:26,420 --> 00:07:29,240
And that's that pixilated versions
and we cropped it. And lastly,

137
00:07:29,510 --> 00:07:32,370
we'll set the batch
input dimensions to zero.

138
00:07:32,371 --> 00:07:35,030
Since we only want to
infer one single image.

139
00:07:35,080 --> 00:07:37,760
Now it could be more if we
wanted it to infer more images,

140
00:07:37,761 --> 00:07:42,010
but for now we could set it to zero
just like that. And now we have this uh,

141
00:07:42,230 --> 00:07:45,680
image and we can return it just like
that and we'll return it and as a float

142
00:07:45,681 --> 00:07:48,860
value and we can add any other
kinds of preprocessing that we want.

143
00:07:49,370 --> 00:07:52,640
The last step,
step three is inference or prediction,

144
00:07:52,641 --> 00:07:55,160
whatever you want to call it.
And that's just two lines of code.

145
00:07:55,490 --> 00:08:00,230
Take that preprocessed source
and then make a prediction
based on that. Right? So,

146
00:08:00,290 --> 00:08:03,650
so, so just like that, we
take that process input,

147
00:08:03,920 --> 00:08:07,610
we feed it into the model and boom, it
outputs a prediction and just like that.

148
00:08:07,940 --> 00:08:11,300
And the result is going to be a dictionary
containing probabilities of every

149
00:08:11,301 --> 00:08:14,710
class that it predicts.
If it's a classification model and the,

150
00:08:14,711 --> 00:08:18,320
the class prediction is the one
with the highest probability, right?

151
00:08:18,321 --> 00:08:19,460
That's the highest likelihood.

152
00:08:20,090 --> 00:08:24,240
So there are some great resources to
learn more about tensorflow. Dot. Js uh,

153
00:08:24,260 --> 00:08:28,760
obviously the tensorflow. Dot. Js Docs
are incredible. I've got a video on it.

154
00:08:28,761 --> 00:08:32,000
This is a video. And also Daniel
Shiffman, who's a Youtuber,

155
00:08:32,030 --> 00:08:35,040
definitely check out his series
on tensorflow. Dot. Js um,

156
00:08:35,041 --> 00:08:36,770
I have a link to it right here.
Check it out.

157
00:08:37,130 --> 00:08:40,670
So that's it for a little bit of
a primer on tensorflow dot. Js.

158
00:08:40,910 --> 00:08:43,040
Now I want to talk about pose estimation.

159
00:08:43,041 --> 00:08:45,080
The problem we talked about
at the very beginning,

160
00:08:46,280 --> 00:08:49,580
so if you ever use a Microsoft connect,
right?

161
00:08:49,581 --> 00:08:51,200
Or if you've ever been
in a demo like that,

162
00:08:51,201 --> 00:08:54,770
you just show up in front of it and all
of a sudden he had maps. Your skeleton,

163
00:08:54,800 --> 00:08:56,490
right?
There's this like skeleton,

164
00:08:56,680 --> 00:09:00,570
a layer on top of your real body and
wherever you move it's going to move.

165
00:09:00,780 --> 00:09:03,450
So what it's done is
it's estimated your pose,

166
00:09:03,480 --> 00:09:06,480
the pose of your human
body or your arms are,

167
00:09:06,481 --> 00:09:09,510
and not just where your arms and
your hands are and your feet are,

168
00:09:09,630 --> 00:09:13,260
but where they are in relation to each
other. And that's what that line is.

169
00:09:13,261 --> 00:09:17,520
That's connecting those dots. So if we
think about the pose estimation problem,

170
00:09:17,550 --> 00:09:21,150
it's not,
it's not a generic classification problem.

171
00:09:21,330 --> 00:09:24,930
We're not trying to do to detect that,
hey, this is an arm or hey this is a foot,

172
00:09:25,320 --> 00:09:29,190
but we're trying to do is build
this post based on those things.

173
00:09:29,400 --> 00:09:31,170
So if we think about it intuitively,

174
00:09:31,680 --> 00:09:35,850
this is an image and we are
trying to process an image
using learning technology.

175
00:09:35,851 --> 00:09:39,240
We want to learn about
what oppose looks like.

176
00:09:39,241 --> 00:09:41,100
And once we learned what
a human pose books like,

177
00:09:41,250 --> 00:09:45,990
we can make an inference or prediction
on a novel post like myself and instantly

178
00:09:45,991 --> 00:09:49,470
it will know, right? So how do we do
that? Well, if we think about it, right?

179
00:09:49,471 --> 00:09:52,800
We were using our intuition about
neural networks. Images, okay?

180
00:09:52,830 --> 00:09:56,100
Immediately convolutional
networks springs to mind, right?

181
00:09:56,220 --> 00:10:00,750
So we'll use a convolutional network
to do this. The next question is,

182
00:10:01,110 --> 00:10:03,870
well, how do we do that? And
let me get to that in a second.

183
00:10:03,871 --> 00:10:08,871
But I also want to say that this idea
of post estimation has many use cases,

184
00:10:09,091 --> 00:10:12,570
right? So if we think about
augmented reality, right?
If you're in the game world,

185
00:10:12,660 --> 00:10:14,850
it should know where your
hands and feet and you know,

186
00:10:14,880 --> 00:10:19,880
even head are in order to make objects
in the environment move appropriately for

187
00:10:19,951 --> 00:10:22,470
animation as well. Right?
So you know, Gollum,

188
00:10:22,500 --> 00:10:25,740
you know all these like Lord of the rings
films where they have all the balls on

189
00:10:25,741 --> 00:10:28,710
the, on the person and they're addressing
this green or black suits and they're

190
00:10:28,711 --> 00:10:30,030
crawling and they're doing things.

191
00:10:30,031 --> 00:10:33,360
And then later on you see the
animated version of that, right?

192
00:10:33,361 --> 00:10:36,720
So normally this is a kind of thing
that's going to democratize animation.

193
00:10:37,050 --> 00:10:39,780
Big Hollywood studios that have
huge budgets have been the,

194
00:10:39,900 --> 00:10:43,050
the only people able to do this.
But with this,

195
00:10:43,051 --> 00:10:46,470
anybody can make these PPOs estimations
of people very easily. Okay.

196
00:10:46,471 --> 00:10:50,590
So there's that. And lastly, fitness,
right, fitness tracking to see, you know,

197
00:10:50,700 --> 00:10:53,270
if you're doing the right form, if
you're, if it's, you know, like whatever,

198
00:10:53,330 --> 00:10:57,300
whatever it is, it can do
that as well. So, um, right.

199
00:10:57,301 --> 00:10:59,700
So back to my explanation of how it works,

200
00:11:00,210 --> 00:11:03,060
we should use a convolutional
network that much we know. Now,

201
00:11:03,061 --> 00:11:04,420
the second part is the thing about,

202
00:11:04,830 --> 00:11:08,880
about the way it's actually going to
learn about oppose before inference.

203
00:11:09,060 --> 00:11:10,890
How is it going to learn?
What opposed it looks like?

204
00:11:11,100 --> 00:11:14,610
So if we think about it
that what's the easiest way?

205
00:11:14,670 --> 00:11:18,420
The easiest way is to think about
this as a supervised learning problem.

206
00:11:18,421 --> 00:11:21,390
So if we Google pose
estimation supervised,

207
00:11:21,600 --> 00:11:25,560
we've got a handful of papers that do
this and some of the more popular papers

208
00:11:25,561 --> 00:11:30,180
are using supervised learning, but there's
also, if we Google search unsupervised,

209
00:11:30,360 --> 00:11:32,910
there are unsupervised
methods as well to do this.

210
00:11:32,910 --> 00:11:37,560
So this would involve clustering of
some sort, but for the supervised part,

211
00:11:37,830 --> 00:11:40,260
if we think about it very intuitively,
what it's going to do,

212
00:11:40,261 --> 00:11:44,460
and now this looks very similar to if you
recall my Yolo object detection video,

213
00:11:44,461 --> 00:11:48,870
just search Yolo Suraj first link on
Youtube. If you look at this picture,

214
00:11:48,900 --> 00:11:53,900
it looks very similar to Yolo because
it's grouping all of these body parts into

215
00:11:54,011 --> 00:11:54,910
different classes.

216
00:11:54,940 --> 00:11:59,200
And then once and then once it's found
the one that is most encompassing of a

217
00:11:59,201 --> 00:12:00,070
given body part,

218
00:12:00,280 --> 00:12:03,130
we can then we can then say that
this is the right ankle or whatever.

219
00:12:03,430 --> 00:12:07,450
So at first it seems like a classification
problem because we have to identify

220
00:12:07,451 --> 00:12:10,780
that this is indeed a foot.
We don't have to tell the user that,

221
00:12:10,810 --> 00:12:13,900
but we have to identify that and
then identify other body parts.

222
00:12:14,320 --> 00:12:16,780
So if we think about it as a
supervised learning problem,

223
00:12:17,080 --> 00:12:20,030
we could give it labeled images
of body parts just like you know,

224
00:12:20,050 --> 00:12:24,580
a hand or an arm and different variations
like all those variations and what it

225
00:12:24,581 --> 00:12:28,720
would do separately or at the same model
would it would first learn the mapping

226
00:12:28,721 --> 00:12:32,710
between arm and arm image,
the mapping between hand and hand image,

227
00:12:32,970 --> 00:12:34,690
the hit mapping between,
you see what I'm saying?

228
00:12:34,870 --> 00:12:39,580
So it would learn the mapping between all
of those body parts and then once it's

229
00:12:39,581 --> 00:12:43,620
learn all those mappings, then once it
sees a person it's going to know Duh, Duh,

230
00:12:43,640 --> 00:12:45,910
Duh. Okay. That's everything
that I've learned.

231
00:12:46,210 --> 00:12:49,840
And it can label them with a point marker.
And once it's got that point marker,

232
00:12:49,930 --> 00:12:54,100
then we can connect the dots, literally
connect the dots. And that's just,

233
00:12:54,101 --> 00:12:56,260
you know, that's just, um,
that's not even machine learning,

234
00:12:56,261 --> 00:12:58,510
just connecting the dots.
That's just um, you know,

235
00:12:58,511 --> 00:13:01,750
d three or some kind of visualization
tool. But then we have the posts.

236
00:13:01,870 --> 00:13:04,360
So that's a supervised way
of doing it. Right? So,

237
00:13:04,450 --> 00:13:08,770
but then the unsupervised way would be
different. Now that's not as popular,

238
00:13:08,830 --> 00:13:10,570
but that is a way to do that.

239
00:13:10,770 --> 00:13:15,400
So I've got this great
video on that called,

240
00:13:15,610 --> 00:13:18,580
it's using the Will Ferrell image
will Ferrell image. It's actually,

241
00:13:18,581 --> 00:13:22,720
it's yellow again, yellow
object detection. Basically
it's looking for direction.

242
00:13:22,721 --> 00:13:26,680
So it considers all of these pixels as
gradients and it's looking for directions

243
00:13:26,681 --> 00:13:28,810
that these gradients of light are moving.

244
00:13:28,811 --> 00:13:32,660
So first converts the image of black
and whites and it sees where the, the,

245
00:13:32,670 --> 00:13:34,360
the direction,
all these great things are moving.

246
00:13:34,540 --> 00:13:37,600
And if the direction for say a nose,
he's like this, this, this, this, this,

247
00:13:37,660 --> 00:13:40,210
it's likely a note or it's likely some,

248
00:13:40,780 --> 00:13:45,110
some body part that sticks out because
it has a point like an elbow or a nose or

249
00:13:45,111 --> 00:13:47,050
a shoulder, um, or uh, you know,

250
00:13:47,051 --> 00:13:51,820
fingertips et cetera or ahead as like
it just stands out from in terms of

251
00:13:51,821 --> 00:13:52,540
lighting,

252
00:13:52,540 --> 00:13:56,080
it's using the clustering of lighting
to identify body parts and that's what

253
00:13:56,081 --> 00:13:58,330
unsupervised learning is all
about. Clustering, right?

254
00:13:58,990 --> 00:14:02,830
That's not as robust as as um,
mainstream,

255
00:14:02,831 --> 00:14:05,800
I guess you could say in the ml community
as a supervised methods which are

256
00:14:05,801 --> 00:14:08,980
easier, but it's still a way, it's a
very useful thing to think about as well.

257
00:14:10,750 --> 00:14:13,950
So pose net is the model that
we are training with, right?

258
00:14:13,951 --> 00:14:17,470
Suppose net is the model we're training
with. It was trained in pure tensorflow.

259
00:14:17,471 --> 00:14:18,790
We can port it to tensorflow.
Dot.

260
00:14:18,791 --> 00:14:23,260
Js and we can think about the problem
of pose estimation as to problems.

261
00:14:23,261 --> 00:14:26,980
Actually there's a problem
of single-person pose
estimation and then there's

262
00:14:26,981 --> 00:14:28,810
multiple pose estimation,
right?

263
00:14:28,811 --> 00:14:33,040
So if you have one person in an image
versus several people in an image,

264
00:14:33,910 --> 00:14:37,150
wildly different, you know, contexts.

265
00:14:37,330 --> 00:14:42,330
So single pose estimation is actually
faster because it's only one person,

266
00:14:42,670 --> 00:14:45,850
but multiple person is slower.
But then you can detect, you know,

267
00:14:45,851 --> 00:14:49,780
a hundred or whatever people.
And interestingly,

268
00:14:49,870 --> 00:14:53,450
multiple pose estimation, it doesn't
matter how many people there are,

269
00:14:53,690 --> 00:14:56,990
the complexity is in, it's not like
n squared or n cubed or something.

270
00:14:57,200 --> 00:15:00,680
The more people, it just gets linearly
more complex, which is very interesting.

271
00:15:01,340 --> 00:15:05,900
But anyway, the way this works is we
first let's talk about, you know, PostNet.

272
00:15:06,140 --> 00:15:10,760
So first an input RGB image is fed into
this convolutional network post net,

273
00:15:11,090 --> 00:15:15,710
and then either a single
pose or multipolar decoding
algorithm is used to detect

274
00:15:15,711 --> 00:15:18,500
poses and then the output is oppose.

275
00:15:19,550 --> 00:15:23,240
Now this pose is going to contain a
list of key points and an instance level

276
00:15:23,241 --> 00:15:25,550
confidence score for each detected person,
right?

277
00:15:25,551 --> 00:15:29,720
So here's the key point for this person's
elbow and this person's arm, et Cetera,

278
00:15:29,721 --> 00:15:33,350
et cetera. And then we can map those on
using some visualization tool, right?

279
00:15:33,351 --> 00:15:36,470
So these are just, um, coordinates
on an, on an image plane,

280
00:15:39,270 --> 00:15:42,070
right? So this guy, he's got
17 pose key points. That's it.

281
00:15:42,460 --> 00:15:45,810
And we can map those on and what is going
to do is it's gonna give us all those

282
00:15:45,811 --> 00:15:46,320
coordinates.

283
00:15:46,320 --> 00:15:51,320
It's our job to then map those onto the
image so we can view them same for this

284
00:15:51,721 --> 00:15:56,230
call as well. So, um,

285
00:15:56,560 --> 00:16:01,540
this single port pose estimation model,
it requires a couple of inputs,

286
00:16:01,541 --> 00:16:04,930
right? So a couple of parameters,
um, how much to scale the image,

287
00:16:04,931 --> 00:16:09,010
whether or not to flip it horizontally.
Uh, what's the, what is that image,

288
00:16:09,040 --> 00:16:12,730
what's the pose? And then the outputs
are going to be the Po's itself,

289
00:16:12,731 --> 00:16:16,060
the key points and then the model,
you know, whatever, you know,

290
00:16:16,061 --> 00:16:17,800
logging data that you add in as well.

291
00:16:20,910 --> 00:16:22,110
But we can,
we can,

292
00:16:22,140 --> 00:16:27,140
we can tune the level of the level of
quality we want by tuning the stride

293
00:16:28,411 --> 00:16:29,071
perimeter,
right?

294
00:16:29,071 --> 00:16:34,071
So we can tune how accurate it is for
one person by tuning the stride parameter

295
00:16:34,981 --> 00:16:39,180
from eight 16 up to 32 and then back
down again. And there's a trade off here.

296
00:16:39,181 --> 00:16:42,870
So one is faster in terms of
processing and want a slower one has.

297
00:16:43,110 --> 00:16:47,190
If it's faster, it has lower accuracy,
but if it's slower it has higher accuracy.

298
00:16:50,280 --> 00:16:50,560
Yeah.

299
00:16:50,560 --> 00:16:52,780
When PostNet processes that an image,

300
00:16:52,840 --> 00:16:56,920
it is in fact returning a heat map along
with offset vectors that can be decoded

301
00:16:56,921 --> 00:17:01,300
to find high competence areas in the
image that correspond to those key points.

302
00:17:01,450 --> 00:17:05,440
And this is what I was talking about when
I referenced the Yolo Object Detection

303
00:17:05,441 --> 00:17:09,730
Video, right? So it's looking for like
notice his nose, there's, there's,

304
00:17:09,760 --> 00:17:13,270
there's a gradient of light in a direction
that's moving towards the center of

305
00:17:13,271 --> 00:17:16,540
his notes and that can be
considered a heat map of key points.

306
00:17:21,810 --> 00:17:22,390
Okay.

307
00:17:22,390 --> 00:17:25,230
Out. So when it comes to, so that's
kind of the general way that mold,

308
00:17:25,260 --> 00:17:27,390
that single person pose estimation works.

309
00:17:27,540 --> 00:17:31,170
Multiple person pose estimation
is similar but it's slower.

310
00:17:31,290 --> 00:17:34,560
It has very similar inputs.
The image scaling factor,

311
00:17:34,620 --> 00:17:36,030
whether or not to flip it horizontally,

312
00:17:36,120 --> 00:17:41,120
the stride m and then there's the
additional one maximum pose detections.

313
00:17:41,311 --> 00:17:43,250
Right?
So how many people the,

314
00:17:43,251 --> 00:17:46,230
the high level threshold do you want
to detect in terms of their poses?

315
00:17:46,560 --> 00:17:48,570
And the output is going
to be all of their poses.

316
00:17:48,810 --> 00:17:53,100
Now when it comes to even multiple posts
detection, there's, there's, there's,

317
00:17:53,130 --> 00:17:57,000
there's more to that algorithm. But
notice how we can coat it just like this.

318
00:17:57,210 --> 00:18:02,210
This 10 line of code snippet shows us
how to do multiple pose estimation very

319
00:18:02,221 --> 00:18:02,880
easily.

320
00:18:02,880 --> 00:18:06,630
All we literally say is called
that function with those
inputs and it's going to

321
00:18:06,631 --> 00:18:10,050
give us all of those poses as an
output. All of those coordinates. Okay?

322
00:18:10,380 --> 00:18:14,310
So what I want to do now is code a little
bit to show you just how simple it is.

323
00:18:14,520 --> 00:18:18,270
Okay? So we can do this in html
very easily. I'm going to do that.

324
00:18:18,271 --> 00:18:21,420
In fact right now, um, this,

325
00:18:22,190 --> 00:18:23,370
tune this to html,

326
00:18:25,010 --> 00:18:25,390
okay,

327
00:18:25,390 --> 00:18:27,970
boom. Okay. So using
html, we can clean it.

328
00:18:27,971 --> 00:18:31,030
We can create any simple html file in our,
you know,

329
00:18:31,031 --> 00:18:34,570
text editor and then look at it in our
browser easily. But you might be thinking,

330
00:18:34,571 --> 00:18:37,500
wait a second, how are we
supposed to load a tentacle dot.

331
00:18:37,510 --> 00:18:41,110
JS If we don't have to
install any scripts? Well,

332
00:18:41,111 --> 00:18:42,640
we can call it directly.

333
00:18:45,590 --> 00:18:45,810
Okay.

334
00:18:45,810 --> 00:18:49,060
By saying the sorts of this script
is going to be online. So that,

335
00:18:49,061 --> 00:18:52,360
so it's unpacking that data from the,
from the interwebs.

336
00:18:52,870 --> 00:18:54,340
And it's saying that

337
00:18:58,010 --> 00:18:59,510
we want that data.

338
00:19:01,260 --> 00:19:01,570
Okay.

339
00:19:01,570 --> 00:19:05,230
Okay. All right. So that's US
loading tensorflow dot. Js.

340
00:19:05,290 --> 00:19:09,940
Now we'll load pose net.
So for loading pose net we'll say,

341
00:19:09,970 --> 00:19:14,920
well we have our library
that we want to use. Let's,

342
00:19:16,630 --> 00:19:17,050
okay,

343
00:19:17,050 --> 00:19:20,470
he used the model that we want to
use and that's called Posen it.

344
00:19:20,800 --> 00:19:25,800
So it's got a source equals
equals better data script,

345
00:19:29,290 --> 00:19:32,490
right?
Just like that done.

346
00:19:33,550 --> 00:19:34,240
Yeah.

347
00:19:34,240 --> 00:19:35,073
And

348
00:19:39,490 --> 00:19:41,260
Yeah.
Cool.

349
00:19:45,210 --> 00:19:48,170
Right? So there's, there's that,
there's that. I'll end the head head,

350
00:19:48,180 --> 00:19:51,420
her head tag just like that. And
that's it for our header. Now,

351
00:19:51,780 --> 00:19:56,160
now we can code this thing.
So let's say like,

352
00:19:56,161 --> 00:19:58,350
what do we want to run this
on? And I'll say, well,

353
00:19:58,351 --> 00:20:00,480
it's going to be this cat
image that I have locally.

354
00:20:00,690 --> 00:20:04,350
So I want it to detect the pose of the
cat, right? So this can be a human,

355
00:20:04,351 --> 00:20:06,150
it can be whatever it is,
but I'll just say cat.

356
00:20:09,090 --> 00:20:09,340
Okay.

357
00:20:09,340 --> 00:20:11,050
And that's going to be it for the body.

358
00:20:14,660 --> 00:20:16,730
And then now that I've got that,

359
00:20:19,530 --> 00:20:24,300
I can go ahead and place our, my code
inside of this script tag. So now,

360
00:20:24,450 --> 00:20:26,940
now comes at tentraflow dot.
Js I'm important to the library.

361
00:20:26,941 --> 00:20:31,020
I've imported the model and now
I can actually go this thing. So

362
00:20:33,290 --> 00:20:33,850
okay,

363
00:20:33,850 --> 00:20:35,620
I'm going to say,
well here are my inputs.

364
00:20:35,650 --> 00:20:38,380
The scale factor is people who'll be 0.5.

365
00:20:38,920 --> 00:20:42,280
The output stride will be 16.

366
00:20:42,970 --> 00:20:47,860
The whether or not to flip
it horizontally. Well,
let's not do this time.

367
00:20:48,430 --> 00:20:52,630
And the image itself is going to be an
elements that I pull from the dom using

368
00:20:52,631 --> 00:20:57,250
get element by ID.
It's going to be cold cat.

369
00:21:00,430 --> 00:21:04,720
And then I'll load in
posing it just like that.

370
00:21:06,100 --> 00:21:10,550
Postnet dot load. And then I'm going
to say let's return the net, um,

371
00:21:10,840 --> 00:21:14,670
estimate a single pose using that image
element, scaling factor, flip horizontal,

372
00:21:14,720 --> 00:21:15,760
and then output stride.

373
00:21:15,910 --> 00:21:18,760
And then it's going to print out
in the console that post. Okay, so

374
00:21:21,050 --> 00:21:25,480
let's see what this looks like. Um,
I'm going to say open TF dot. Html

375
00:21:27,250 --> 00:21:28,150
in the browser.

376
00:21:31,360 --> 00:21:36,220
Inspect in the console.
Here are my key points right here,

377
00:21:36,940 --> 00:21:40,040
right? [inaudible] all my
left I left year. This,

378
00:21:40,041 --> 00:21:43,160
this I did with not without installing
anything, this is happening locally.

379
00:21:43,161 --> 00:21:46,550
I just have a single image. It made those
predictions. Here's where everything is.

380
00:21:46,780 --> 00:21:48,830
Um, check out the links
in the video description.

381
00:21:48,860 --> 00:21:51,650
I've got links for you that's going to
show you how to do this in the browser

382
00:21:52,370 --> 00:21:54,740
code,
code wise and other learning resources.

383
00:21:54,940 --> 00:21:57,610
Hey, you made it to the end of the
video. If you want to be an AI,

384
00:21:57,611 --> 00:22:00,940
God hit the subscribe button for now.
I've got to make a pose,

385
00:22:01,120 --> 00:22:02,380
so thanks for watching.

