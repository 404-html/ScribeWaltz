1
00:00:00,060 --> 00:00:00,691
Hello world.

2
00:00:00,691 --> 00:00:05,100
It's the Raj and let's learn about a
popular new deep learning framework called

3
00:00:05,190 --> 00:00:09,540
Pi Torch. The name is inspired by the
popular torch deep learning framework,

4
00:00:09,690 --> 00:00:12,780
which was written in the Lua
programming language. Learning.

5
00:00:12,781 --> 00:00:17,070
Lula is a big barrier to entry if you're
just starting to learn deep learning

6
00:00:17,071 --> 00:00:21,600
and it doesn't offer the
modularity necessary to
interface with other libraries

7
00:00:21,601 --> 00:00:23,640
like a more accessible language would.

8
00:00:23,940 --> 00:00:28,530
So a couple of AI researchers who were
inspired by torches programming style

9
00:00:28,620 --> 00:00:32,190
decided to implement it in
python calling it high torch.

10
00:00:32,460 --> 00:00:36,390
They also added a few other really cool
features to the mix and we'll talk about

11
00:00:36,391 --> 00:00:41,070
the two main ones. The first key feature
of Pi Torch is imperative programming.

12
00:00:41,340 --> 00:00:44,880
An imperative program performs
computation as you typed it.

13
00:00:45,120 --> 00:00:49,110
Most Python code is imperative
and this num py example,

14
00:00:49,111 --> 00:00:53,040
we write four lines of code to ultimately
compute the value for d when the

15
00:00:53,070 --> 00:00:55,410
program executes c Equals d times a.

16
00:00:55,500 --> 00:00:58,860
It runs the actual computation than
in there just like you told it to.

17
00:00:59,040 --> 00:01:01,230
In contrast and it's symbolic program,

18
00:01:01,260 --> 00:01:05,880
there is a clear separation
between defining the
computation graph and compiling

19
00:01:05,881 --> 00:01:06,240
it.

20
00:01:06,240 --> 00:01:10,860
If we were to rewrite the
same code symbolically than
when c Equals d times a is

21
00:01:10,861 --> 00:01:14,580
executed, no computation
occurs at that line. Instead,

22
00:01:14,581 --> 00:01:18,630
these operations generate a computation
or symbolic graph and then we can

23
00:01:18,631 --> 00:01:22,560
convert the graft into a function that
can be called via the compile step.

24
00:01:22,890 --> 00:01:25,740
So computation happens as
the last step in the code.

25
00:01:26,340 --> 00:01:28,250
Both styles have there tradeoffs.

26
00:01:28,320 --> 00:01:32,550
Symbolic programs are more efficient
since you can safely reuse the memory of

27
00:01:32,551 --> 00:01:37,230
your values. For inplace
computation, tensorflow is
made to use symbolic program.

28
00:01:37,680 --> 00:01:41,910
Imperative programs are more flexible
since python is most suited for them.

29
00:01:41,911 --> 00:01:45,960
So you can use native python features
like printing out values in the middle of

30
00:01:45,961 --> 00:01:50,280
computation and injecting loops
into the computation flow itself.

31
00:01:50,490 --> 00:01:55,490
The second key feature of Pi Torch is
dynamic computation graphing as opposed to

32
00:01:55,531 --> 00:01:59,910
static computation graphing. In other
words, Pi Torch is defined by run.

33
00:02:00,060 --> 00:02:02,460
So at runtime the system
generates the graph structure.

34
00:02:02,820 --> 00:02:06,750
Tensorflow is defined and run where
we defined conditions and iterations.

35
00:02:06,900 --> 00:02:10,770
In the graph structure, it's like writing
the whole program before running it.

36
00:02:10,771 --> 00:02:12,900
So the degree of freedom is limited.

37
00:02:12,930 --> 00:02:15,750
So in tensorflow we defined
the computation graph once,

38
00:02:16,080 --> 00:02:18,930
then we can execute that
same graph many times.

39
00:02:19,200 --> 00:02:22,800
The great thing about this is that we
can optimize the graph at the start.

40
00:02:23,040 --> 00:02:27,180
Let's say in our model we want to use
some kind of strategy for distributing the

41
00:02:27,181 --> 00:02:29,220
graph across multiple machines.

42
00:02:29,400 --> 00:02:34,350
This kind of computationally expensive
optimization can be reduced by reusing

43
00:02:34,351 --> 00:02:35,041
the same graph.

44
00:02:35,041 --> 00:02:38,880
Static graphs worked well for neural
networks that are fixed size like

45
00:02:38,881 --> 00:02:41,490
feedforward networks or
convolutional networks,

46
00:02:41,670 --> 00:02:45,360
but for a lot of use cases it would
be useful if the graph structure could

47
00:02:45,540 --> 00:02:49,860
change depending on the input data,
like when using recurrent neural networks.

48
00:02:49,890 --> 00:02:50,671
In this snippet,

49
00:02:50,671 --> 00:02:55,050
we're using tensorflow to unroll a
recurrent network unit overboard vectors.

50
00:02:55,350 --> 00:02:58,950
To do this, we'll need to use a special
tensorflow function called wild.

51
00:02:59,560 --> 00:03:03,340
We have to use special nodes to represent
primitives like loops and conditionals

52
00:03:03,670 --> 00:03:08,500
because any control flow statements will
run only once when the graph is built.

53
00:03:08,680 --> 00:03:12,940
But a cleaner way to do this is to
use dynamic graphs instead where the

54
00:03:12,941 --> 00:03:16,630
computation graph is built and
rebuilt as necessary. At runtime,

55
00:03:16,990 --> 00:03:18,250
the code is more straightforward.

56
00:03:18,400 --> 00:03:21,460
Since we can use standard four
and if statements, anytime.

57
00:03:21,461 --> 00:03:23,830
The amount of work that
needs to be done is variable.

58
00:03:23,980 --> 00:03:28,630
Dynamic graphs are useful using dynamic
graphs makes debugging really easy since

59
00:03:28,631 --> 00:03:32,740
a specific line in our written code is
what fails as opposed to something deep

60
00:03:32,741 --> 00:03:33,790
under section.run.

61
00:03:33,970 --> 00:03:37,600
Let's build a simple two layer neural
network in Pi torch to get a feel for the

62
00:03:37,601 --> 00:03:41,770
syntax. We start by importing
our framework as well as
the autograph package,

63
00:03:41,800 --> 00:03:45,370
which we'll let our network
automatically implement backpropagation.

64
00:03:45,580 --> 00:03:48,220
Then we'll define our batch size,
input dimension,

65
00:03:48,280 --> 00:03:50,140
hidden dimension and output dimension.

66
00:03:50,440 --> 00:03:54,880
We'll then use those values to help
define tensors to hold inputs and outputs.

67
00:03:55,060 --> 00:03:59,890
Wrapping them in variables we'll set
requires gradients to false since we don't

68
00:03:59,891 --> 00:04:03,880
need to compute gradients with respect to
these variables during backpropagation.

69
00:04:04,300 --> 00:04:07,090
The next set of variables
we'll define our our weights.

70
00:04:07,240 --> 00:04:09,160
We'll initialize them
as variables as well,

71
00:04:09,161 --> 00:04:13,360
storing random tensors with the float
data type and since we do want to compute

72
00:04:13,361 --> 00:04:16,930
gradients with respect to these variables,
we'll set the flag to true.

73
00:04:17,170 --> 00:04:21,610
We'll define a learning rates and we
can begin our training loop for 500

74
00:04:21,611 --> 00:04:23,740
iterations.
During the forward pass,

75
00:04:23,741 --> 00:04:27,790
we can compute the predicted label
using operations on our variables.

76
00:04:28,030 --> 00:04:31,660
Mm stands for Matrix
multiply and clamp clamps.

77
00:04:31,660 --> 00:04:35,290
All the elements in the input range
into a range between men and Max.

78
00:04:35,380 --> 00:04:39,340
Once we've matrix multiplied for both
sets of weights to compute our prediction,

79
00:04:39,550 --> 00:04:42,610
we can calculate the difference
between them and square it,

80
00:04:42,670 --> 00:04:46,000
the sum of all the squared errors,
a popular loss function.

81
00:04:46,330 --> 00:04:48,040
Before we perform backpropagation,

82
00:04:48,280 --> 00:04:51,670
we need to manually zero the
gradients for both sets of weights.

83
00:04:51,730 --> 00:04:55,210
Since the gray and buffers have to be
manually reset before fresh grades are

84
00:04:55,211 --> 00:04:56,044
calculated,

85
00:04:56,200 --> 00:04:59,920
then we can run back propagation by simply
calling the backward function on our

86
00:04:59,921 --> 00:05:04,360
loss, it will compute the grain of our
loss with respect to all variables we set

87
00:05:04,361 --> 00:05:05,830
requires gradient the true for,

88
00:05:05,890 --> 00:05:09,610
and then we can update our weights
using gradient descent and our outputs.

89
00:05:09,611 --> 00:05:11,890
We'll look great. Pretty Dope. To sum up,

90
00:05:11,891 --> 00:05:15,880
high torch offers to really useful
features, dynamic computation graphs,

91
00:05:16,000 --> 00:05:18,550
and imperative programming.
Dynamic computation.

92
00:05:18,551 --> 00:05:23,200
Graphs are built and rebuilt as necessary
at runtime and imperative programs

93
00:05:23,350 --> 00:05:25,690
perform computation as you run them.

94
00:05:25,750 --> 00:05:30,190
There is no distinction between defining
the computation graph and compiling.

95
00:05:30,580 --> 00:05:31,000
Right now,

96
00:05:31,000 --> 00:05:35,230
tensorflow has the best documentation on
the web for a machine learning library,

97
00:05:35,231 --> 00:05:39,760
so it's still the best way for beginners
to start learning and it's best suited

98
00:05:39,761 --> 00:05:43,600
for production use since it was built
with distributed computing in mind.

99
00:05:43,840 --> 00:05:48,310
But for researchers, it seems like
Pi Torch has a clear advantage here.

100
00:05:48,610 --> 00:05:53,610
A lot of cool new ideas will benefit
and rely on the use of dynamic grass.

101
00:05:53,800 --> 00:05:57,050
Please subscribe for more programming
videos. And for now, I've got a torch,

102
00:05:57,090 --> 00:05:58,780
my hair,
so thanks for watching.

