1
00:00:00,240 --> 00:00:03,600
Red Pill or blue pill.
Let your curiosity guide you.

2
00:00:05,280 --> 00:00:10,230
Hello world it Saroj and curiosity is
a concept that we're all familiar with.

3
00:00:10,260 --> 00:00:13,350
It's the desire to know
or learn something.

4
00:00:13,351 --> 00:00:16,920
It's something that all of us
have experienced at some point.

5
00:00:17,490 --> 00:00:19,800
It's only human right,
wrong.

6
00:00:20,040 --> 00:00:25,040
Recently a team at Berkeley published a
paper on curiosity driven learning and

7
00:00:25,260 --> 00:00:30,260
they demonstrated how it helped enable
their AI agent to learn how to play the

8
00:00:30,511 --> 00:00:35,511
popular game of Super Mario brothers very
efficiently using the added benefit of

9
00:00:36,811 --> 00:00:40,470
curiosity to help Mario
explore his options.

10
00:00:40,740 --> 00:00:42,750
We'll discuss how it
all works in a second,

11
00:00:42,751 --> 00:00:46,620
but first we need to understand
why they decided to go there.

12
00:00:47,100 --> 00:00:52,100
In any kind of real time environment
scenario where time is a relevant data

13
00:00:52,771 --> 00:00:53,191
point,

14
00:00:53,191 --> 00:00:58,191
whether that's a simulation or a
smart power grid or robotic movement,

15
00:00:58,890 --> 00:01:03,000
we can use a class of algorithms called
reinforcement learning to learn an

16
00:01:03,030 --> 00:01:08,030
optimal policy for our agent so that
it can achieve a predefined objective.

17
00:01:08,700 --> 00:01:11,190
That objective could be
any number of things.

18
00:01:11,191 --> 00:01:14,880
Transferring power across
nodes at the lowest cost,

19
00:01:15,090 --> 00:01:20,040
detecting intruders in a real time
security system or just winning a game.

20
00:01:20,430 --> 00:01:24,810
An Ai agents policy then is a function
that given an environment state,

21
00:01:25,020 --> 00:01:29,310
we'll return the optimal action that
an agent should take to reach that

22
00:01:29,311 --> 00:01:30,144
objective.

23
00:01:30,450 --> 00:01:35,450
There are several possible functions that
our AI agent will need to approximate

24
00:01:35,700 --> 00:01:38,250
in order to learn the optimal policy,

25
00:01:38,430 --> 00:01:41,670
and several known techniques
can help it do that,

26
00:01:42,000 --> 00:01:45,240
but of all of the function
approximation techniques,

27
00:01:45,510 --> 00:01:48,720
neural networks have shown
the most promising results.

28
00:01:49,080 --> 00:01:54,080
Thus using neural networks to approximate
one or several functions involved in a

29
00:01:54,631 --> 00:01:59,631
reinforcement learning scenario is
considered deep reinforcement learning or

30
00:01:59,820 --> 00:02:00,780
deep RL,

31
00:02:01,020 --> 00:02:06,020
and it's no secret that deep RL is the
hottest subfield of AI right now with

32
00:02:06,210 --> 00:02:11,210
many researchers adding to the scope
of human knowledge on the topic every

33
00:02:11,310 --> 00:02:11,580
single week.

34
00:02:11,580 --> 00:02:15,030
One of them being the research group
that we're talking about in this video,

35
00:02:15,240 --> 00:02:19,380
they first identified two key problems
with all reinforcement learning

36
00:02:19,410 --> 00:02:23,250
algorithms. The first is the
problem of sparse rewards.

37
00:02:23,430 --> 00:02:27,630
That's the time difference between
an action and its associated reward.

38
00:02:28,110 --> 00:02:28,801
For example,

39
00:02:28,801 --> 00:02:33,801
if we use any reinforcement
learning algorithm to learn
how the game of breakout

40
00:02:34,321 --> 00:02:35,040
works,

41
00:02:35,040 --> 00:02:40,040
it would get a lot of reward feedback
immediately because it's such a fast paced

42
00:02:40,201 --> 00:02:40,950
game.

43
00:02:40,950 --> 00:02:45,840
But in a more complex scenario like say
splinter cell trying to sneak into a

44
00:02:45,960 --> 00:02:48,720
high security facility,
I miss this game.

45
00:02:49,110 --> 00:02:52,830
It's not immediately clear
what the rewards are for doing.

46
00:02:52,831 --> 00:02:57,831
Some set of actions will have to wait
until much later to see how an action

47
00:02:58,141 --> 00:03:01,660
plays out towards the
larger goal of sneaking in.

48
00:03:01,810 --> 00:03:05,590
The second problem is that
extrinsic rewards are not scalable.

49
00:03:05,770 --> 00:03:10,770
That means rewards that we humans define
extrinsic to the agent work well enough

50
00:03:11,531 --> 00:03:13,120
for simple environments.

51
00:03:13,120 --> 00:03:16,810
But when we scale to much
more complex environments,

52
00:03:17,050 --> 00:03:22,050
there are lots of rewards that we'd have
to think about and and code into the

53
00:03:22,331 --> 00:03:27,100
environment for our agent to learn
from hand coding these rewards is not a

54
00:03:27,101 --> 00:03:28,540
scalable approach.

55
00:03:28,660 --> 00:03:33,660
So their solution to these two problems
in reinforcement learning was to define

56
00:03:34,570 --> 00:03:37,870
a reward function that was
intrinsic to the agent,

57
00:03:37,871 --> 00:03:42,790
meaning that the agent generated by itself
and they called this reward function,

58
00:03:43,000 --> 00:03:43,900
curiosity.

59
00:03:44,380 --> 00:03:49,060
They were inspired by the fact that we
humans are so used to operating with

60
00:03:49,061 --> 00:03:52,630
rewards that are extremely sparse.
For example,

61
00:03:52,631 --> 00:03:57,631
some entrepreneurs will toil away at
building a startup even if there is no

62
00:03:58,001 --> 00:04:01,990
immediate reward insight,
their businesses could be failing.

63
00:04:02,110 --> 00:04:04,570
They could be running
out of investor support.

64
00:04:04,810 --> 00:04:07,930
Yet some entrepreneurs
will continue despite this.

65
00:04:08,740 --> 00:04:12,490
I'm not really sure who fits into
this category, but uh, anyway,

66
00:04:12,790 --> 00:04:17,590
psychologists call this intrinsic
motivation, otherwise known as curiosity.

67
00:04:17,950 --> 00:04:21,970
The entrepreneur pushes through the
uncertainty guided by the question.

68
00:04:22,300 --> 00:04:27,100
I wonder if I can do this and
tries to answer it by doing it.

69
00:04:27,520 --> 00:04:31,810
The researchers define curiosity as an
intrinsic reward that's equal to the

70
00:04:31,811 --> 00:04:36,811
error of an agent in predicting the
consequences of its own actions given its

71
00:04:37,601 --> 00:04:38,440
current state.

72
00:04:38,710 --> 00:04:43,150
It needs to be able to successfully
predict the next state given the current

73
00:04:43,151 --> 00:04:44,740
state and action taken.

74
00:04:45,220 --> 00:04:50,050
The basic idea of curiosity is to
encourage an agent to perform actions that

75
00:04:50,051 --> 00:04:55,051
reduce the uncertainty in the agent's
ability to predict the consequences of its

76
00:04:55,271 --> 00:04:56,104
own action.

77
00:04:56,410 --> 00:05:01,390
This uncertainty will be higher in areas
where the agent has spent less time or

78
00:05:01,391 --> 00:05:06,190
in areas with more complex dynamics.
To measure the agent's error,

79
00:05:06,220 --> 00:05:11,220
we need to build a model of environment
dynamics that predicts the next state

80
00:05:11,470 --> 00:05:13,600
given the current state and action.

81
00:05:14,110 --> 00:05:17,590
The question then becomes how
do we calculate this error?

82
00:05:18,130 --> 00:05:19,300
Let's take a step back.

83
00:05:19,480 --> 00:05:24,480
The question we're trying to answer is
how can our agent predict the next state?

84
00:05:24,850 --> 00:05:27,130
Given our current state and our action,

85
00:05:27,520 --> 00:05:32,260
we can define the curiosity as the error
between the predicted states given our

86
00:05:32,261 --> 00:05:36,460
current state and action as
well as the new real state.

87
00:05:36,820 --> 00:05:40,210
In the case of our video
game or agent is Mario,

88
00:05:40,330 --> 00:05:45,280
and each state will be a game frame
which consists of a bunch of pixels.

89
00:05:45,700 --> 00:05:50,700
The problem though is that there are so
many pixel values to predict in a single

90
00:05:51,041 --> 00:05:55,840
game frame, so the entire process will
be very computationally expensive.

91
00:05:56,440 --> 00:06:01,440
They decided that making a prediction
in the raw sensory space of game pixels

92
00:06:01,670 --> 00:06:04,460
wasn't the optimal way
to frame the problem.

93
00:06:04,640 --> 00:06:08,270
So they instead transformed
the raw sensory input,

94
00:06:08,390 --> 00:06:13,390
which is an array of pixels into a lower
dimensional space that only contains

95
00:06:13,581 --> 00:06:17,210
relevant information.
But what do we mean by relevant?

96
00:06:17,330 --> 00:06:21,980
What makes some data relevant and
other data irrelevant to our agent?

97
00:06:22,370 --> 00:06:27,370
The way they ensured that
it only contained relevant
information is by defining

98
00:06:27,591 --> 00:06:32,030
a rule for what data could be
considered relevant beforehand.

99
00:06:32,480 --> 00:06:33,380
Relevant data.

100
00:06:33,381 --> 00:06:38,381
It needs to model both environment objects
that can be controlled by the agent

101
00:06:39,050 --> 00:06:43,520
as well as environment objects
that can affect the agent.

102
00:06:44,060 --> 00:06:48,530
Everything else is not in the agent's
control and has no effect on them.

103
00:06:48,890 --> 00:06:53,390
And we can discard that. So in our
Super Mario game, our agent, Mario,

104
00:06:53,391 --> 00:06:55,370
would need to model himself.

105
00:06:55,371 --> 00:07:00,371
Since he's controlled by the agent
and as the enemy Coupas approach,

106
00:07:00,650 --> 00:07:04,370
we can't control them,
but they can affect our agent.

107
00:07:04,460 --> 00:07:07,400
So they're relevant. Keep
your enemies close, right?

108
00:07:07,880 --> 00:07:12,880
We don't need to model the cloud in the
sky because it doesn't affect our agent

109
00:07:12,950 --> 00:07:15,650
and we can't control it this way.

110
00:07:15,650 --> 00:07:18,170
By constraining what data we represent,

111
00:07:18,350 --> 00:07:23,350
we can develop a feature representation
with much less noise and our desired

112
00:07:23,810 --> 00:07:27,710
embedding space. We'll be compact
and informationally dense.

113
00:07:28,130 --> 00:07:33,130
So the researchers introduced
the intrinsic curiosity
module to help the agent

114
00:07:33,230 --> 00:07:38,180
generates curiosity. This module is
composed of two different neural networks.

115
00:07:38,510 --> 00:07:43,510
Instead of making predictions from a raw
sensory input space of pixels directly,

116
00:07:43,880 --> 00:07:48,880
we'll need to transform the sensory input
into a feature vector where only the

117
00:07:49,011 --> 00:07:53,990
information relevant to the actions
performed by the agent is represented.

118
00:07:54,260 --> 00:07:56,510
And in order to learn this feature space,

119
00:07:56,570 --> 00:08:01,570
we can train a neural network to predict
the agent's action given its current

120
00:08:01,640 --> 00:08:03,140
and next states.

121
00:08:03,530 --> 00:08:08,450
Because the neural network only predicts
the action it has no other incentive to

122
00:08:08,451 --> 00:08:13,451
represent within its feature embedding
space the factors of variation in the

123
00:08:13,521 --> 00:08:16,790
environment that doesn't
affect the agent itself.

124
00:08:16,970 --> 00:08:20,870
We can then utilize the learned feature
space to train a model that predicts the

125
00:08:20,871 --> 00:08:25,700
future representation of the next states
given the feature representation of the

126
00:08:25,701 --> 00:08:27,440
current state.
And the action.

127
00:08:27,770 --> 00:08:32,000
The prediction error of this model is
an intrinsic reward to encourage it's

128
00:08:32,001 --> 00:08:35,720
curiosity. We can formalize
these two models mathematically.

129
00:08:35,780 --> 00:08:40,780
The first is the inverse
model which encodes the state
s and s plus one the next

130
00:08:41,451 --> 00:08:45,800
state into two feature vectors that
are trained to predict an action.

131
00:08:46,070 --> 00:08:47,960
The second is the forward model.

132
00:08:48,050 --> 00:08:52,430
It takes us input a feature vector and
predicts the feature representation of

133
00:08:52,431 --> 00:08:53,300
the next state.

134
00:08:53,750 --> 00:08:58,350
The curiosity of our agent then will
be the difference between our predicted

135
00:08:58,351 --> 00:09:03,351
feature vector of the next state and the
real feature vector of the next state.

136
00:09:03,810 --> 00:09:04,351
Thus,

137
00:09:04,351 --> 00:09:09,351
we can construct the optimization problem
of this module as a concatenation of

138
00:09:09,421 --> 00:09:11,880
both inverse loss and forward loss.

139
00:09:12,180 --> 00:09:17,010
They trained their agent in the Super
Mario World using only curiosity based

140
00:09:17,011 --> 00:09:21,180
signal without any extrinsic
reward from the environment.

141
00:09:21,360 --> 00:09:26,360
They're Mario agents could learn to
cross over 30% of level one the agent who

142
00:09:26,481 --> 00:09:30,270
received no reward for killing
enemies or avoiding fatal events,

143
00:09:30,480 --> 00:09:33,510
yet it automatically
discovered these behaviors.

144
00:09:33,900 --> 00:09:38,280
A possible reason for this is that
getting killed by the enemy will result in

145
00:09:38,281 --> 00:09:42,930
seeing only a small part of the game
space satisfying it's curiosity.

146
00:09:43,230 --> 00:09:44,880
In order to remain curious,

147
00:09:44,910 --> 00:09:49,170
it's in the agent's interest to learn
how to kill and dodge enemies so that it

148
00:09:49,171 --> 00:09:51,390
can reach new parts of the game space.

149
00:09:51,600 --> 00:09:55,890
This suggests that curiosity provides
indirect supervision for learning

150
00:09:55,891 --> 00:10:00,690
behaviors. Pretty cool stuff,
right? As we learn more about Ai,

151
00:10:00,720 --> 00:10:03,690
we learn more about
ourselves and how we learn.

152
00:10:03,990 --> 00:10:06,570
There are three things to
remember from this video.

153
00:10:06,870 --> 00:10:11,400
Reinforcement learning suffers from
sparser awards and extrinsic rewards,

154
00:10:11,401 --> 00:10:13,050
which are not scalable.

155
00:10:13,260 --> 00:10:18,260
A solution is to create an agent that
creates intrinsic rewards itself using

156
00:10:18,301 --> 00:10:23,301
curiosity and the intrinsic curiosity
module consists of an inverse model used

157
00:10:23,430 --> 00:10:28,430
to learn the feature representation of a
given state as well as a next state and

158
00:10:28,531 --> 00:10:32,970
a forward dynamics model used to generate
the predicted representation of the

159
00:10:32,971 --> 00:10:36,570
next state. Please subscribe for
more programming videos, and for now,

160
00:10:36,600 --> 00:10:39,900
I've got to explore, not
exploited, so thanks for watching.

