1
00:00:00,030 --> 00:00:00,691
Hello world,

2
00:00:00,691 --> 00:00:04,530
it's Saroj and today we're going to
build a machine learning model called

3
00:00:04,531 --> 00:00:09,531
logistic regression to predict if someone
has diabetes or not given three other

4
00:00:10,681 --> 00:00:14,220
features, their weight, their
height and their blood pressure.

5
00:00:14,310 --> 00:00:18,150
Using those three features are going to
predict if they have diabetes or not.

6
00:00:18,240 --> 00:00:18,870
Okay,

7
00:00:18,870 --> 00:00:23,700
so in this video we're going to go
over to really two key concepts.

8
00:00:23,940 --> 00:00:26,190
We want to go over logistic regression,

9
00:00:26,220 --> 00:00:30,480
which is a machine learning model and
we want to go over the optimization

10
00:00:30,481 --> 00:00:34,350
technique, Newton's method. So we've
already talked about nudes method,

11
00:00:34,470 --> 00:00:38,790
but we want to really reinforce that
idea in our head of how it works.

12
00:00:39,000 --> 00:00:43,470
And the medium that we're going to use
to help reinforce that idea is this

13
00:00:43,471 --> 00:00:47,040
machine learning model called logistic
regression. So before we start,

14
00:00:47,041 --> 00:00:51,570
I just want to say one thing to to
really understand how logistic regression

15
00:00:51,571 --> 00:00:52,290
works,

16
00:00:52,290 --> 00:00:57,150
we're going to have to go over some terms
from all four or actually five pillars

17
00:00:57,151 --> 00:00:58,110
of machine learning,
old,

18
00:00:58,200 --> 00:01:01,350
all five disciplines that I talked
about at the beginning of this course.

19
00:01:01,560 --> 00:01:05,520
Calculus, linear Algebra,
probability theory and statistics.

20
00:01:05,521 --> 00:01:06,900
So actually four pillars,
not five.

21
00:01:07,200 --> 00:01:09,420
We have to talk about little bits
and pieces from each of them.

22
00:01:09,690 --> 00:01:14,690
So the reason I say that is don't worry
too much if you don't understand every

23
00:01:15,331 --> 00:01:18,090
single detail of why
something is the way it is.

24
00:01:18,390 --> 00:01:21,210
We're going to talk about
all of these concepts in the,

25
00:01:22,020 --> 00:01:24,270
in the direction or in the,

26
00:01:25,050 --> 00:01:29,490
in the context of learning about
logistic regression. Keep in mind,

27
00:01:29,491 --> 00:01:33,540
I have entire videos coming out on
all of these subjects. So for example,

28
00:01:33,541 --> 00:01:37,130
we'll talk about Matrix
transposes uh, but, and I'll tell,

29
00:01:37,131 --> 00:01:39,570
I'll say one reason why we
would use a matrix transpose,

30
00:01:39,580 --> 00:01:41,040
but I'm not going to say all the reasons,

31
00:01:41,310 --> 00:01:46,170
but I do have a video on matrices in
general and linear Algebra coming out in a

32
00:01:46,170 --> 00:01:46,591
few weeks,
right?

33
00:01:46,591 --> 00:01:49,770
So just don't worry too much if you
don't understand every single detail.

34
00:01:49,890 --> 00:01:52,650
There's so many mathematical
dependency chains that we can,

35
00:01:53,010 --> 00:01:56,280
that we can follow conceptually, right?
When we think about these things.

36
00:01:56,400 --> 00:02:00,990
But we want to have a very specific
chain of a very specific chain or bear to

37
00:02:01,000 --> 00:02:04,890
specific train of thoughts because there
are so many that we can go over, right?

38
00:02:04,891 --> 00:02:09,690
We only want to learn how logistic
regression works and to reinforce our

39
00:02:09,691 --> 00:02:11,190
knowledge of Newton's method.

40
00:02:11,340 --> 00:02:14,970
And if we do that and if we get how it
works and we're going to end and I'm

41
00:02:14,971 --> 00:02:17,610
going to define all the terms
that we're going to use, right?

42
00:02:17,611 --> 00:02:18,930
So you're going to get the definitions,

43
00:02:19,050 --> 00:02:22,760
you're going to understand how the
equations works and work. But if,

44
00:02:22,790 --> 00:02:25,680
if you don't understand every
single detail, don't worry.

45
00:02:25,681 --> 00:02:29,370
I have entire videos coming out on
it and I have it all planned out.

46
00:02:29,371 --> 00:02:31,310
So don't worry. Okay, so I just
wanted to say that there's,

47
00:02:31,560 --> 00:02:34,260
cause there's a lot that we're going to
coat that we're going to go over in this

48
00:02:34,261 --> 00:02:39,030
video. Okay. So, okay, now that I've
said that, let's go ahead and start.

49
00:02:39,180 --> 00:02:40,013
So that's,

50
00:02:40,020 --> 00:02:45,020
so our task is to compute is to predict
if someone has diabetes or not given

51
00:02:45,541 --> 00:02:49,620
their weight, height and their blood
pressure. And this is toy data.

52
00:02:49,621 --> 00:02:52,710
So we're actually going to generate
this feature, these features. Okay.

53
00:02:52,711 --> 00:02:54,900
These three features. Uh, okay.

54
00:02:54,901 --> 00:02:58,590
So let's start off with defining
what logistic regression is.

55
00:02:58,710 --> 00:03:02,710
So logistic regression compared
to linear regression is a,

56
00:03:02,740 --> 00:03:04,750
so let's compare to linear regression,
right?

57
00:03:04,751 --> 00:03:09,310
So the key difference between logistic
regression and linear regression is that

58
00:03:09,311 --> 00:03:13,480
linear regression, a predicts a
continuous outcome. What does that,

59
00:03:13,630 --> 00:03:17,710
so versus logistic Russian,
which computes a discrete outcome.

60
00:03:17,830 --> 00:03:22,600
So continuous is a means that we
can have values that range from,

61
00:03:22,930 --> 00:03:23,763
um,

62
00:03:24,280 --> 00:03:28,330
from negative infinity to infinity or
just an infinite possible set of values,

63
00:03:28,510 --> 00:03:33,160
right? So if you have some
series, some number series,
the next value could be any,

64
00:03:33,190 --> 00:03:37,180
any, you know, it could be any one
of a set of numbers out of infinity,

65
00:03:37,540 --> 00:03:40,900
but for a discrete outcome, it's either
one thing or it's another, right?

66
00:03:40,900 --> 00:03:44,980
We have you kind of, you box it
into some, some, some container.

67
00:03:45,070 --> 00:03:47,830
So a discrete outcome would
be is it blue or is it red?

68
00:03:48,040 --> 00:03:50,980
Whereas a continuous outcome would be,
um,

69
00:03:52,090 --> 00:03:56,860
what is the value? What is the next value
in the series of house prices? Right?

70
00:03:56,861 --> 00:03:59,950
You can have two values. It be one or
two, it can be some number between,

71
00:03:59,951 --> 00:04:02,530
they're like 1.5.
It could be a number between that.

72
00:04:02,531 --> 00:04:06,820
Like 1.25 and an infinitely small in that,
in that direction.

73
00:04:06,821 --> 00:04:09,580
Do you see what I'm saying? It could
be an infinite number of possibilities.

74
00:04:09,850 --> 00:04:11,770
And so linear aggression
is great for that.

75
00:04:12,040 --> 00:04:13,780
But what if we want to predict an outcome,

76
00:04:13,810 --> 00:04:16,810
like say if someone has diabetes or not?

77
00:04:17,050 --> 00:04:21,520
Well that is a discrete outcome and
he's a binary outcome. Either they do,

78
00:04:21,550 --> 00:04:23,350
yes or they don't know.

79
00:04:23,740 --> 00:04:28,740
And so logistic regression is basically
our way of modeling a discrete outcome,

80
00:04:29,261 --> 00:04:31,980
right? Uh, and so the outcome it was,

81
00:04:32,050 --> 00:04:35,770
which is the dependent variable a is
going to be continuous in the case of

82
00:04:35,771 --> 00:04:38,680
linear regression and discrete in
the case of logistic regression.

83
00:04:39,190 --> 00:04:42,070
And so why do we call it
logistic regression? Well,

84
00:04:42,071 --> 00:04:47,071
it all comes from that key function
called a logistic function that underlies,

85
00:04:47,520 --> 00:04:50,200
uh,
how that underlies the model.

86
00:04:50,201 --> 00:04:52,810
It is really the core
functionality of the model.

87
00:04:53,050 --> 00:04:56,590
And we also call the logistic function,
the sigmoid function.

88
00:04:56,770 --> 00:04:59,860
So if you've done deep learning before
or if you've done your own networks

89
00:04:59,861 --> 00:05:03,700
before and it's okay if you
haven't, it's used a lot. Okay,

90
00:05:03,701 --> 00:05:04,980
so this is what the function does.

91
00:05:04,981 --> 00:05:09,981
Sigmoid or logistic function looks like
it's one over one plus e to the negative

92
00:05:10,840 --> 00:05:15,490
x. Okay. And so why do we use e?

93
00:05:15,550 --> 00:05:18,100
So, uh, so there's a
lot of reasons. I mean,

94
00:05:18,101 --> 00:05:22,060
there's entire textbooks on
y e the natural number, uh,

95
00:05:22,090 --> 00:05:24,370
or EULAR is number is used up.

96
00:05:24,370 --> 00:05:28,990
But one great reason is because when
you take the derivative of e to the x,

97
00:05:29,230 --> 00:05:32,860
uh, you're going to get either
the x again and it's the only fun,

98
00:05:32,890 --> 00:05:36,340
only known function that does
this where if you derive it,

99
00:05:36,580 --> 00:05:39,970
it's going to give you the exact same
results. And of course, this goes for all,

100
00:05:39,971 --> 00:05:44,320
um, all versions of versions of either
the x where you can have any number of

101
00:05:44,321 --> 00:05:48,820
coefficients. So five e to the ax or
three e to the x, the same thing applies.

102
00:05:48,970 --> 00:05:53,620
And so the, the point of that is that it
makes it computationally convenient or

103
00:05:53,621 --> 00:05:55,120
mathematically convenience,
right?

104
00:05:55,240 --> 00:06:00,230
So you can see from a high level how
having a function that if you drive, uh,

105
00:06:00,320 --> 00:06:04,760
did you the same value is unique.
And in that uniqueness, uh,

106
00:06:04,790 --> 00:06:09,740
can help, uh, with other
computations. Okay. So, so,

107
00:06:09,741 --> 00:06:11,440
right, so, so that's,
that's why we use eat.

108
00:06:11,480 --> 00:06:13,490
That's one of the reasons
why we use he anyway.

109
00:06:13,520 --> 00:06:18,080
He is a part of the sigmoid or
logistic function is an s shaped curve.

110
00:06:18,290 --> 00:06:20,120
So given any number of values,

111
00:06:20,210 --> 00:06:24,860
it's going to output a value between
zero and one and that is a probability

112
00:06:24,861 --> 00:06:26,330
value. And that's what we want, right?

113
00:06:26,570 --> 00:06:30,020
We want the probability that
this person has diabetes,

114
00:06:30,110 --> 00:06:35,030
diabetes or they don't have diabetes.
And then based on some threshold,

115
00:06:35,031 --> 00:06:38,990
like let's say it's above 70%, we could
just say, oh well if it's above 70%,

116
00:06:39,020 --> 00:06:41,750
yes they have it or below.
No they don't. Right?

117
00:06:41,751 --> 00:06:45,650
So we can define our threshold or we
could just outputs that probability value,

118
00:06:45,651 --> 00:06:49,490
but we want a discrete outcome.
Write a Boolean, yes or no.

119
00:06:50,360 --> 00:06:53,420
So that's at the heart of the algorithm.
And

120
00:06:55,580 --> 00:07:00,470
so logistic regression, uh, uses an
equation as its representation. Okay?

121
00:07:00,471 --> 00:07:03,680
It is an equation value, and I'm going
to show you what the equation looks like,

122
00:07:04,040 --> 00:07:08,720
but the central premise of logistic
regression and when you want to use it is

123
00:07:08,870 --> 00:07:13,430
when you have an assumption that
your data is linearly separable.

124
00:07:13,670 --> 00:07:17,690
And that means that given end dimensions
where n is the number of features in

125
00:07:17,691 --> 00:07:19,970
your data,
you could separate it linearly.

126
00:07:19,971 --> 00:07:23,960
So you could draw a line that acts as
a decision boundary between all of your

127
00:07:23,961 --> 00:07:28,100
data points. So in a two dimensional
space, this would be an n minus one.

128
00:07:28,130 --> 00:07:30,800
So one dimensional line in
a three dimensional space,

129
00:07:30,801 --> 00:07:34,250
it would be a three minus
one or two dimensional plane.

130
00:07:34,460 --> 00:07:38,660
And this goes on for infinity, right?
For where n is the number of dimensions.

131
00:07:39,050 --> 00:07:40,370
And so that would be a hyperplane.

132
00:07:40,371 --> 00:07:44,630
So basically you're under the assumption
that your data is in fact linearly

133
00:07:44,631 --> 00:07:46,370
separable.
And if it is,

134
00:07:46,520 --> 00:07:51,520
then you can use logistic regression as
a way to model what the class is given

135
00:07:52,701 --> 00:07:57,500
the feature set. Okay, so, so
let's, let's talk about this.

136
00:07:57,530 --> 00:08:01,070
So what is the function for
logistic regression look like?

137
00:08:01,220 --> 00:08:05,330
So let's say we had the following
function right here to noted by these Beta

138
00:08:05,570 --> 00:08:07,550
Beta coefficients here,
right?

139
00:08:07,551 --> 00:08:11,390
So it starts with Beta zero and Beta
one Beta two and then you have these

140
00:08:11,420 --> 00:08:16,280
variables, x one x two. Now all
of these are our unique variables,

141
00:08:16,310 --> 00:08:20,780
right? So just think of it like y
equals, well skip the why but mx plus B,

142
00:08:21,080 --> 00:08:22,010
right?
And so you can,

143
00:08:22,040 --> 00:08:25,790
you can have an infinitely long equation
of this forum where you can have B plus

144
00:08:25,791 --> 00:08:30,620
B, three x three plus B, four x four
[inaudible] of this for. And so we're ex,

145
00:08:30,640 --> 00:08:34,220
we're where B is Beta,
which these are the coefficient values,

146
00:08:34,370 --> 00:08:36,500
which are the weights,
right? That we want to learn.

147
00:08:36,501 --> 00:08:39,620
We want to learn what the optimal weights
are that's going to output what we

148
00:08:39,621 --> 00:08:40,520
want it to output.

149
00:08:41,120 --> 00:08:45,320
And so if we had this function
and we plugged in some point,

150
00:08:45,410 --> 00:08:48,760
so let's just say it's this function
alone, if we were to plug in some point a,

151
00:08:48,761 --> 00:08:53,330
B or x, y, whatever you want to call
it, into that is a coordinate pair.

152
00:08:53,390 --> 00:08:57,000
If we were to plug it into this,
it could either give us a positive result,

153
00:08:57,210 --> 00:08:59,160
it could give us a negative result,

154
00:08:59,310 --> 00:09:03,000
or it could give us a a a zero,

155
00:09:03,001 --> 00:09:05,880
which is a point that lies
right on the decision boundary.

156
00:09:06,000 --> 00:09:08,520
So assuming that if it's
assumed that if it's positive,

157
00:09:08,521 --> 00:09:10,620
it's going to be of one class.
If it's negative,

158
00:09:10,621 --> 00:09:12,090
it's going to be of the other class.

159
00:09:12,270 --> 00:09:15,600
And if it's zero it's going to be right
in the middle. So it's of neither class.

160
00:09:16,200 --> 00:09:20,100
And so what we want is to
set this equal to some,

161
00:09:20,690 --> 00:09:25,440
to some probability value
or some probabilistic value,

162
00:09:25,710 --> 00:09:30,060
right? So, so we could
do one of several things.

163
00:09:30,061 --> 00:09:33,090
Well one is, let me just skip this for
a second. Let me, let me go down here.

164
00:09:33,270 --> 00:09:37,800
So one thing we could do is we could set
this equation not to just why like we

165
00:09:37,801 --> 00:09:39,060
did with linear aggression,

166
00:09:39,390 --> 00:09:44,370
but set it equal to the probability of x
given the function, right? So given this,

167
00:09:44,670 --> 00:09:45,780
given this function,

168
00:09:45,900 --> 00:09:49,800
we plug in the coefficients and it's
going to output a probability value.

169
00:09:50,040 --> 00:09:54,210
So we could use the probability or we
could use the odds and the difference

170
00:09:54,211 --> 00:09:55,230
between the probability.

171
00:09:55,231 --> 00:10:00,090
And the odds is that the odds is
this is this expression right here,

172
00:10:00,210 --> 00:10:04,410
which is the probability of x over
one minus the probability of x.

173
00:10:04,620 --> 00:10:09,360
So note that probability of acts and the
odds of x essentially denote the same

174
00:10:09,361 --> 00:10:11,010
thing,
which is the pro,

175
00:10:11,040 --> 00:10:15,300
the ratio of the probability of the
event happening versus it not happening.

176
00:10:15,510 --> 00:10:19,020
So why do we use the odds?
So here's why.

177
00:10:19,860 --> 00:10:21,720
So given,
you know,

178
00:10:21,760 --> 00:10:26,760
so given for possible values that we
could set our equation to probability the

179
00:10:27,811 --> 00:10:30,780
log of the probability,
the odds,

180
00:10:30,930 --> 00:10:35,220
and the log of the odds.
If we set the uh,

181
00:10:35,250 --> 00:10:37,530
equation equal to,
as you see right here,

182
00:10:37,650 --> 00:10:41,730
the log or logic or la la gets of the,
of the odds,

183
00:10:41,970 --> 00:10:46,620
then it's going to let us
have the greatest range of
values such that it's going

184
00:10:46,621 --> 00:10:49,470
to map to between zero and one.
Let me explain that.

185
00:10:49,650 --> 00:10:53,180
So that means that we can have
any value for our coefficients.

186
00:10:53,181 --> 00:10:57,110
So the the right hand side of this
equation right here, which is this,

187
00:10:57,300 --> 00:11:02,300
this equation right here can be between
negative infinity and an infinity and

188
00:11:03,270 --> 00:11:07,950
the left hand side will be equal to some
probability value between zero and one.

189
00:11:08,400 --> 00:11:12,480
And so what this means is we can have
any number of values that we can use for

190
00:11:12,481 --> 00:11:13,380
our coefficients.

191
00:11:13,470 --> 00:11:17,970
So it allows the widest
range of coefficients to be
used for our function such

192
00:11:17,971 --> 00:11:21,450
that the probability output is
going to be between zero and one.

193
00:11:22,440 --> 00:11:27,210
And when it comes to reasons of why do
we use the log probability versus the

194
00:11:27,211 --> 00:11:28,740
other three,
they're like the probability,

195
00:11:28,741 --> 00:11:33,630
the why do we use a log odds versus just
the logs versus just the odds alone,

196
00:11:33,990 --> 00:11:38,610
uh, and uh, versus the probability
versus the log probability. Well,

197
00:11:38,940 --> 00:11:41,860
to sum it up, it makes it easier. Uh,

198
00:11:41,970 --> 00:11:45,900
it can be extremely complicated if we
were to just use the probability alone

199
00:11:46,080 --> 00:11:50,890
because we would have to, uh, fine. I set
of constraints on the regression line, uh,

200
00:11:51,240 --> 00:11:56,030
for the regression coefficients
because basically, I mean the,

201
00:11:56,050 --> 00:11:58,570
the log put,
getting our computing,

202
00:11:58,571 --> 00:12:03,571
the log of the probability makes it easier
for us because the value is going to

203
00:12:04,511 --> 00:12:07,590
be between negative infinity and infinity
for what's whatever's on the right

204
00:12:07,591 --> 00:12:10,750
hand side. And there's a
lot more theory here, right?

205
00:12:10,751 --> 00:12:14,550
So there's a lot of probabilistic
theory here, but let's just say like,

206
00:12:14,551 --> 00:12:15,520
this is what I said at the beginning.

207
00:12:15,521 --> 00:12:19,870
Let's just for all intensive purposes
of all the things that we could set our

208
00:12:19,900 --> 00:12:22,720
equation too,
we're going to set it to the log odds.

209
00:12:22,721 --> 00:12:27,210
We're going to set it to the log
odds and that's it. Okay. So givens,

210
00:12:27,220 --> 00:12:30,340
whatever regression equation
we have on the right hand side,

211
00:12:30,341 --> 00:12:33,700
we're going to send it to a log odds
and that's going to give us the optimal

212
00:12:34,240 --> 00:12:38,890
relationship between any number of values
that we could use for our coefficients

213
00:12:39,070 --> 00:12:40,540
and a probability value,
right?

214
00:12:40,541 --> 00:12:45,541
So the general model for
logistic regression looks
like this where we have some,

215
00:12:46,180 --> 00:12:50,970
uh, log odds, probably log odds on
the left, uh, and then some cof.

216
00:12:51,070 --> 00:12:53,500
And then some function
on the rights, right? So,

217
00:12:54,580 --> 00:12:58,450
so in this case you could say the
probability of having a disease, uh,

218
00:12:58,810 --> 00:12:59,800
is p.

219
00:12:59,860 --> 00:13:03,850
So we say the log probability of
having that disease over one minus the,

220
00:13:04,480 --> 00:13:04,901
or sorry,

221
00:13:04,901 --> 00:13:08,680
the log of the probability of the disease
over one minus the probability of the

222
00:13:08,681 --> 00:13:11,650
disease is going to be
equal to our function.

223
00:13:11,700 --> 00:13:15,250
And these coefficients are the
effects of the genetic factors.

224
00:13:15,430 --> 00:13:19,630
And so each of these coefficients
are going to be basically tune,

225
00:13:19,660 --> 00:13:23,290
these are wait values that will be
learned as we learn as we train our model.

226
00:13:23,560 --> 00:13:27,490
And then these x values are going to be
the variables for those genetic factors.

227
00:13:27,491 --> 00:13:27,940
So they'll,

228
00:13:27,940 --> 00:13:31,990
each of those will represent one of
the features that that results in the

229
00:13:31,991 --> 00:13:36,850
probability that an individual has
a particular disease. Okay. So,

230
00:13:38,470 --> 00:13:39,303
right.
So,

231
00:13:39,520 --> 00:13:43,750
so this whole process that we're
doing here that this whole process of,

232
00:13:43,960 --> 00:13:48,960
of finding the ideal values
for the coefficients to,

233
00:13:49,570 --> 00:13:53,230
uh, to, to find the
ideal probability values.

234
00:13:53,290 --> 00:13:58,290
There are the ideal probability values
is called maximum likelihood estimation

235
00:13:58,661 --> 00:13:59,620
or MLE.

236
00:13:59,770 --> 00:14:04,360
So maximum likelihood estimation is a
general approach to estimating parameters

237
00:14:04,540 --> 00:14:08,320
and statistical models by
maximizing the likelihood function.

238
00:14:08,500 --> 00:14:11,380
So in deep learning we
call this back propagation.

239
00:14:11,650 --> 00:14:15,940
So there's a difference here
between NLE and optimization. Okay.

240
00:14:15,941 --> 00:14:20,320
So Newton's method is, is an
optimization algorithm. Okay?

241
00:14:20,321 --> 00:14:24,460
So you can use this algorithm to find
the maximum or minimum of many different

242
00:14:24,461 --> 00:14:28,150
functions including the likelihood
function. You can obtain ml,

243
00:14:28,750 --> 00:14:32,110
you can obtain maximum likelihood
estimates using different methods.

244
00:14:32,290 --> 00:14:36,160
And using an optimization algorithm
is one of them. Okay, so the,

245
00:14:36,190 --> 00:14:39,490
so let's talk about the word
maximum likelihood estimation.

246
00:14:39,820 --> 00:14:44,820
We are maximizing the likelihood that our
model classifies some novel data point

247
00:14:46,090 --> 00:14:50,200
as the correct class. That's why we
call it maximum likelihood estimation.

248
00:14:50,340 --> 00:14:54,530
We are trying to maximize the likelihood
that our model estimates or predicts

249
00:14:54,590 --> 00:14:57,530
the correct class for a novel data points.

250
00:14:57,890 --> 00:15:02,390
And so optimization is one technique
we could use to perform an ele.

251
00:15:02,910 --> 00:15:05,840
Uh, but there are other techniques, but
we don't have to worry about that. Okay.

252
00:15:05,841 --> 00:15:06,920
So like I said,

253
00:15:07,100 --> 00:15:11,660
we are focused right now we are focused
on logistic regression and Newton's

254
00:15:11,661 --> 00:15:14,060
method.
And we're trying to form,

255
00:15:14,150 --> 00:15:18,380
we're trying to perform NLE and we're
using new method as our optimization

256
00:15:18,381 --> 00:15:22,280
technique to perform an
Ellie. Okay. Um, right.

257
00:15:24,230 --> 00:15:28,700
So, so why use Newton's method? Right? So
we, we talked about logistic regression,

258
00:15:28,940 --> 00:15:32,120
uh, that what, what the general
equation for it looks like.

259
00:15:32,290 --> 00:15:33,590
And we talked about that.

260
00:15:33,620 --> 00:15:38,030
We talked about how the process of US
training our logistic regression model is

261
00:15:38,031 --> 00:15:40,550
called maximum likelihood estimation.

262
00:15:41,150 --> 00:15:45,020
And in machine learning or
in deep learning, it's called
backpropagation, right?

263
00:15:45,021 --> 00:15:48,380
So that's why you never hear the term MLE
and deep learning because we just call

264
00:15:48,381 --> 00:15:51,470
it backpropagation for supervised learning
where they are, whether it's a label,

265
00:15:52,310 --> 00:15:54,020
but in general, machine
learning, the word,

266
00:15:54,080 --> 00:15:57,680
the phrase maximum likelihood
estimation is used quite a bit.

267
00:15:58,220 --> 00:16:01,940
So anyway, why Newton's method? Well,

268
00:16:01,970 --> 00:16:06,800
it usually converges faster then
gradient descent when maximizing logistic

269
00:16:06,801 --> 00:16:10,940
regression log likelihood. So it's faster
when it comes to logistic regression.

270
00:16:11,480 --> 00:16:14,510
And, but the one thing to
note, and this is not a reason,

271
00:16:14,511 --> 00:16:19,430
but one thing to note is
that each iteration is more
expensive computationally

272
00:16:19,580 --> 00:16:24,350
than gradient descent because we are
calculating the inverse of the Hessian.

273
00:16:24,650 --> 00:16:26,930
And the Hessian, and we'll talk
more about this in a second,

274
00:16:27,260 --> 00:16:30,590
is a matrix of second
order partial derivatives.

275
00:16:30,710 --> 00:16:35,180
And those are derivatives with respect
to each of our coefficients or each of

276
00:16:35,181 --> 00:16:38,300
our weights in our function,
but it's not just the derivative of them.

277
00:16:38,630 --> 00:16:43,280
That would be the Jacobean. It's the
derivative of the derivative. Okay.

278
00:16:43,281 --> 00:16:47,210
And so let me say another thing. So, uh,

279
00:16:47,900 --> 00:16:52,900
the Hessian is going to be
used quite a bit in this video.

280
00:16:54,010 --> 00:16:57,140
Uh, and we're also going to use
some matrix terms that we're not,

281
00:16:57,950 --> 00:17:02,510
that we've never used before, but we'll
learn about them as we go and yeah.

282
00:17:02,540 --> 00:17:03,580
Anyway, so yeah, we're,

283
00:17:03,581 --> 00:17:06,350
we're going to learn some matrix
operations as well here anyway.

284
00:17:06,351 --> 00:17:10,940
So as long as the data points are not
very large, Nunes method is preferred.

285
00:17:11,180 --> 00:17:12,710
It's a preferred method. So, okay.

286
00:17:12,711 --> 00:17:16,220
So if we have a few data points and
we're using logistic regression,

287
00:17:16,670 --> 00:17:17,480
Newton's method,

288
00:17:17,480 --> 00:17:21,860
great for optimization to perform
maximum likelihood estimation.

289
00:17:22,220 --> 00:17:25,970
Okay. So one more thing before we
get to the code and that thing is,

290
00:17:26,090 --> 00:17:29,570
what are some other good examples of
logistic regression and Newton's method?

291
00:17:29,720 --> 00:17:33,800
Remember, although the utility that
we're trying to perform is the testing,

292
00:17:33,830 --> 00:17:36,170
if someone has diabetes or
not given some features,

293
00:17:36,440 --> 00:17:39,200
the data is in fact generated
is in fact toy data.

294
00:17:39,530 --> 00:17:44,450
And the way to improve your
understanding of this is to,

295
00:17:45,410 --> 00:17:48,320
uh,
let me remove this anyway,

296
00:17:48,770 --> 00:17:51,330
is to look at other examples.

297
00:17:51,390 --> 00:17:56,390
So I've got one example here that uses
a psychic learn a little bit for data

298
00:17:56,401 --> 00:17:58,470
preprocessing,
but more or less it's using,

299
00:17:58,500 --> 00:18:02,370
it's very mathematically a legible.

300
00:18:02,460 --> 00:18:05,940
So check out this one and
it does something similar
using Newton's method and

301
00:18:05,941 --> 00:18:08,880
it'll just horrible Russian
board spam classification.

302
00:18:08,881 --> 00:18:13,881
So that to the idea is that you can detect
if an email is spam or it's not spam.

303
00:18:14,670 --> 00:18:17,160
And then the other one is click
through rate classification.

304
00:18:17,161 --> 00:18:22,161
You can classify based on some consumers
clickthrough rates if they're going to

305
00:18:22,980 --> 00:18:26,070
or based on their other patterns
if they're going to click through.

306
00:18:26,190 --> 00:18:30,300
So you can compute the click through rate.
Anyway, these are two great examples.

307
00:18:30,301 --> 00:18:34,290
Definitely check them out to improve your
understanding of both Newton's method

308
00:18:34,440 --> 00:18:37,860
and of logistic regression.
Anyway, so yeah,

309
00:18:37,861 --> 00:18:41,220
so that's it for our examples.
We've defined logistic regression,

310
00:18:41,221 --> 00:18:44,340
we've defined Newton's
method both at a high level.

311
00:18:44,460 --> 00:18:47,520
And now let's get to the code.
All right, so for the code part,

312
00:18:48,510 --> 00:18:52,950
we've got our dependencies here and our
dependencies are importing num Pi for

313
00:18:52,951 --> 00:18:57,570
matrix math, pandas for data
manipulation. So a lot of data,

314
00:18:57,900 --> 00:19:01,920
a lot of data scientists use pandas a
lot. In fact, there's some when asked,

315
00:19:02,070 --> 00:19:04,440
what tool do you use?
They'll just say does,

316
00:19:04,441 --> 00:19:07,140
they won't even say python
because it's that ubiquitous.

317
00:19:07,290 --> 00:19:11,100
But basically pen does is a really
popular library for manipulating data.

318
00:19:11,101 --> 00:19:13,200
And what it does is it takes some Dataset,

319
00:19:13,260 --> 00:19:15,960
whether it's an excel
spreadsheet or something else,

320
00:19:16,170 --> 00:19:19,620
and it will put it into an object,
a native object in the, in the,

321
00:19:19,621 --> 00:19:22,590
in the dependency that
is called a data frame.

322
00:19:22,620 --> 00:19:24,840
And once your data is in a data frame,

323
00:19:24,960 --> 00:19:28,490
you can perform a whole bunch of really
easy getters and setters on that data.

324
00:19:28,491 --> 00:19:32,220
So you could specify the column and then
the row and you could delete that data,

325
00:19:32,460 --> 00:19:34,380
that data point or you know,
whatever.

326
00:19:34,620 --> 00:19:38,220
But it's basically very convenient
for data manipulation. Uh,

327
00:19:38,221 --> 00:19:40,380
and so then we have important
warnings for error logging.

328
00:19:40,380 --> 00:19:42,810
And we do have one single, very, very,

329
00:19:42,811 --> 00:19:47,280
very thin wrapper on top of num Pi.
But we're only gonna use it for one line.

330
00:19:47,281 --> 00:19:51,470
And I'll talk about the details
here, but basically it is a, it,

331
00:19:51,690 --> 00:19:56,160
it fits our data into a matrix anyway.

332
00:19:56,310 --> 00:19:59,910
Don't even worry about it
for all intensive purposes.
This is not using any, uh,

333
00:20:00,930 --> 00:20:03,870
you know, heavy libraries.
It's all very thin libraries.

334
00:20:04,440 --> 00:20:06,990
All the logic is going to be there anyway.
Okay?

335
00:20:06,991 --> 00:20:11,130
So the first thing we're
gonna do is we're going to,

336
00:20:11,720 --> 00:20:14,620
uh, we're going to define the
sigmoid function, right? So the,

337
00:20:14,630 --> 00:20:18,690
the function that talks about before
that s shaped curve and what the sigmoid

338
00:20:18,691 --> 00:20:23,280
function does is it outputs a
probability between zero and one. Okay?

339
00:20:23,310 --> 00:20:27,240
That's it. Which is one over
one plus e to the negative acts.

340
00:20:27,780 --> 00:20:31,170
So now I have this right here, this,
this, uh, but let's skip this for now.

341
00:20:31,170 --> 00:20:35,400
I'm going to come back to this.
So we defined our sigmoid function,

342
00:20:35,401 --> 00:20:40,050
which is that key function that is
the core of logistic regression.

343
00:20:40,080 --> 00:20:42,810
We defined that function on its own,
and we're going to use a later,

344
00:20:43,080 --> 00:20:46,650
but now let's define our hyper
parameters, right? And these are the,

345
00:20:46,830 --> 00:20:49,300
these are in machine learning,
we call them hyper parameters,

346
00:20:49,510 --> 00:20:52,090
and these are the parameters of
the model that we're building.

347
00:20:52,510 --> 00:20:55,480
So the first thing we're going to do is
we're going to set the seed and the seed

348
00:20:55,481 --> 00:20:59,830
is used for anytime you're using
any kind of randomly generated data.

349
00:21:00,010 --> 00:21:04,240
If you have a seed, then what?
Whenever you rerun that program,

350
00:21:04,270 --> 00:21:06,790
it's going to output
the same random numbers.

351
00:21:06,850 --> 00:21:10,960
So why is this useful for reproducibility?
Which is good for debugging, right?

352
00:21:10,961 --> 00:21:15,100
You always want the same values to be
generated so you can test your code.

353
00:21:15,140 --> 00:21:18,430
So it's basically for
reproducibility for debugging.

354
00:21:19,120 --> 00:21:22,630
And then we're going to define a
convergence tolerance as this very,

355
00:21:22,631 --> 00:21:24,130
very small number right here.

356
00:21:24,550 --> 00:21:28,210
Basically this is the minimum threshold
between the predicted output and the

357
00:21:28,211 --> 00:21:32,620
actual output is going to tell our
model when to stop learning, right?

358
00:21:32,621 --> 00:21:37,480
So when the difference
between our predicted output
in our actual output reaches

359
00:21:37,481 --> 00:21:40,570
this threshold, then we're going
to say, okay, that's good enough.

360
00:21:40,900 --> 00:21:45,190
As opposed to just saying, well, whenever
we finished the number of iterations,

361
00:21:45,191 --> 00:21:47,350
we're also adding this threshold,

362
00:21:47,351 --> 00:21:51,070
which we're going to call the convergence
tolerance and we'll use it and you'll

363
00:21:51,071 --> 00:21:55,270
see why later. But basically it's a
threshold for when we can stop learning.

364
00:21:56,530 --> 00:22:00,640
I had this other term here called
the l two regularization term,

365
00:22:00,820 --> 00:22:02,770
which is going to help us regularize.

366
00:22:02,830 --> 00:22:07,480
So now I'm going to go back up here
and talk about what regularization is.

367
00:22:07,690 --> 00:22:12,220
So regularization is a very important
technique and machine learning to prevent

368
00:22:12,300 --> 00:22:16,390
over fitting. We use it all over the
place and deep learning in general,

369
00:22:16,391 --> 00:22:17,860
machine learning,
we used to everywhere.

370
00:22:18,160 --> 00:22:20,710
Basically it's a technique
to prevent over fitting.

371
00:22:20,711 --> 00:22:22,090
And what do I mean by overfitting?

372
00:22:22,300 --> 00:22:26,860
That means when your model is to trained
on the training data and it's not going

373
00:22:26,861 --> 00:22:30,190
to generalize well for new data.
So if you give it some new data,

374
00:22:30,370 --> 00:22:34,210
it's not going to be able to make proper
predictions because the training data

375
00:22:34,211 --> 00:22:38,800
that you tested it that you train the
data that you trained it on is too

376
00:22:39,430 --> 00:22:43,420
similar. It's too homogenous. So if you
give it some very different Dataset,

377
00:22:43,450 --> 00:22:45,790
it's not going to be, it's
not gonna predict that. Well.

378
00:22:46,900 --> 00:22:51,370
So what regularization does is it is,
it's a technique to help prevent this.

379
00:22:51,550 --> 00:22:53,590
And we have several techniques
to prevent overfilling,

380
00:22:53,591 --> 00:22:56,800
but regularization is a very
popular and important one.

381
00:22:57,370 --> 00:23:01,810
And so mathematically speaking,
it adds a regularization term.

382
00:23:02,110 --> 00:23:06,220
So there are two types of regularization
right here that we're going to talk

383
00:23:06,221 --> 00:23:10,780
about. One is called the
l one regularization and
the other is called the Lt

384
00:23:10,870 --> 00:23:11,740
regularization.

385
00:23:12,310 --> 00:23:17,310
So the difference is l two is the
sum of the square of the weights,

386
00:23:18,070 --> 00:23:22,740
while l one is just the sum of the
weights, right? And so we have, uh, the,

387
00:23:22,990 --> 00:23:27,990
the actual output minus the predicted
output and then squared plus our

388
00:23:28,391 --> 00:23:29,470
regularization term,

389
00:23:29,620 --> 00:23:34,360
which is the sum of the weights versus
the sum of the square of the weights

390
00:23:34,480 --> 00:23:39,040
times this lambda term, which is
the regularization term. Okay.

391
00:23:39,041 --> 00:23:41,770
And that's going to give us,
and what that, so adding this,

392
00:23:41,800 --> 00:23:46,650
this edition is basically,
this helps our model, not, uh,

393
00:23:46,670 --> 00:23:51,530
get over fit to the data. And so when
should you use l one versus l to,

394
00:23:51,650 --> 00:23:55,370
well here's a list of when you should use
them and when you shouldn't. There are,

395
00:23:55,400 --> 00:23:59,300
there's actually a bigger list out there,
but at a high level we are,

396
00:23:59,350 --> 00:24:03,860
the reason we use it is use case
specific. So if you have, um,

397
00:24:05,810 --> 00:24:10,010
if you have a small datasets and you
don't care about computational efficiency

398
00:24:10,011 --> 00:24:12,920
because it's a very small Dataset,
then you would likely use l one.

399
00:24:13,250 --> 00:24:14,840
Whereas if you don't,
you could use l two.

400
00:24:15,080 --> 00:24:18,020
And then it also relates to
the sparsity of your data,

401
00:24:18,021 --> 00:24:22,700
which means how many Zeros this does
the data have, how sparse is it?

402
00:24:22,910 --> 00:24:27,140
If it's, if there's a lot of zeros
and it's very sparse. Okay. So, uh,

403
00:24:27,141 --> 00:24:29,390
and there's a lot more
reasons, but basically, uh,

404
00:24:29,420 --> 00:24:33,860
in this video we're going to be
using the l two regularization term.

405
00:24:34,310 --> 00:24:38,340
And so anyway, yeah, so that,
so there's that. Anyway, so, uh,

406
00:24:38,360 --> 00:24:42,230
we're going to use LTE regularization and
then we're going to have 20 iterations

407
00:24:42,231 --> 00:24:45,080
for training. Okay. So now,

408
00:24:45,110 --> 00:24:49,370
so these variables are used
to help create our data. Okay.

409
00:24:49,371 --> 00:24:52,430
And so these terms come
from linear Algebra.

410
00:24:52,760 --> 00:24:54,940
So the first term defines a,

411
00:24:54,980 --> 00:24:58,700
the covariance between x and Z.

412
00:24:58,700 --> 00:25:01,400
Covariants is a measure of how
two variables move together.

413
00:25:01,580 --> 00:25:04,220
So it measures whether the to
move in the same direction,

414
00:25:04,221 --> 00:25:07,730
which would be a positive covariance
or in the opposite direction,

415
00:25:07,731 --> 00:25:12,360
which would be a negative covariants.
So we have two of those. And so, uh,

416
00:25:12,410 --> 00:25:16,820
we want to set it 2.95.
So we have a positive a covariance.

417
00:25:16,821 --> 00:25:19,200
We want these very, we want, uh,

418
00:25:19,220 --> 00:25:23,090
to have our variables to be related very
closely and that would be our height

419
00:25:23,120 --> 00:25:26,060
and our weight and blood pressure
can be kind of off. We're,

420
00:25:26,070 --> 00:25:30,980
we're actually creating this data
right now so we can define what the,

421
00:25:31,370 --> 00:25:34,940
uh, what these terms are. Whereas a
normal data, we wouldn't define it.

422
00:25:35,150 --> 00:25:38,870
They would just exist, right? And then we
would have to discover these relations.

423
00:25:38,871 --> 00:25:41,510
But we're defining these
relations right now, right now.

424
00:25:41,750 --> 00:25:43,820
So then we have a number
of ops or observations,

425
00:25:43,821 --> 00:25:47,360
which is a thousand and a thousand is
the number of data points. We have,

426
00:25:47,361 --> 00:25:51,200
data points, observations, same
thing. And then we have a sigma turn,

427
00:25:51,201 --> 00:25:55,550
which is the variance of the noise,
which is how spread out is this data.

428
00:25:55,850 --> 00:25:59,750
Okay? So again, we're going
to get back to these terms.

429
00:25:59,751 --> 00:26:02,660
We have a lot of probability to go over
in the future, but at the high level,

430
00:26:02,690 --> 00:26:06,740
that's what those terms mean. The spread
of these, the spread of numbers, uh,

431
00:26:06,741 --> 00:26:09,830
the relationship between two variables
and the number of data points that we

432
00:26:09,831 --> 00:26:13,850
have. All right, so now we're
going to define our model settings.

433
00:26:14,060 --> 00:26:17,060
So we have a set of
true Beta coefficients.

434
00:26:17,090 --> 00:26:21,010
So let's just assume that we know what
the ideal coefficient should be for our

435
00:26:21,020 --> 00:26:22,070
model.
Okay,

436
00:26:22,071 --> 00:26:25,430
so we have those true Beta coefficients
and then we're going to have our

437
00:26:25,431 --> 00:26:29,840
predicted Beta coefficients. And we're
going to s we're going to, we're going to,

438
00:26:30,230 --> 00:26:34,400
uh, use the true Beta coefficients
to help us calculate, uh,

439
00:26:34,430 --> 00:26:36,110
our predicted Beta coefficients.

440
00:26:36,470 --> 00:26:39,410
And then we have a set of
variances for each of these inputs.

441
00:26:39,411 --> 00:26:42,140
And that is how spread out
each of the inputs are.

442
00:26:42,320 --> 00:26:46,530
And remember we are manually defining
these as well as our model will manually

443
00:26:46,531 --> 00:26:51,240
define what our model looks like, the
shape of our model, given our three uh,

444
00:26:51,690 --> 00:26:55,890
features, right x CNV, which are the
height, weight and blood pressure.

445
00:26:56,760 --> 00:26:57,031
Okay.

446
00:26:57,031 --> 00:27:01,950
So we've defined our model
hyper parameters and now
we can generate an organize

447
00:27:01,951 --> 00:27:04,260
our data, right? So to generate this data.

448
00:27:04,440 --> 00:27:07,920
Now this is assuming we don't have real
data, so we're going to generate it.

449
00:27:08,190 --> 00:27:12,630
And so I'm going to introduce a few uh,
statistical terms here,

450
00:27:12,750 --> 00:27:14,760
specifically the distribution,
right?

451
00:27:14,760 --> 00:27:19,740
So what is the distribution at a high
level of distribution is a function that

452
00:27:19,741 --> 00:27:24,741
provides us all the probabilities or
provides us the probabilities of all

453
00:27:25,561 --> 00:27:28,800
possible outcomes of a
sucky castic process.

454
00:27:28,801 --> 00:27:31,410
That is a process that
cannot be predicted.

455
00:27:31,620 --> 00:27:35,490
A stochastic process is one that cannot
be predicted and it deterministic

456
00:27:35,491 --> 00:27:38,400
process is one that can be predicted.
Okay?

457
00:27:38,401 --> 00:27:40,710
So it looks like a bell curve usually.

458
00:27:40,770 --> 00:27:43,080
And there's all these different
types of distributions out there.

459
00:27:43,081 --> 00:27:46,140
You've got Bernoulli by no meal normal.

460
00:27:46,260 --> 00:27:49,680
You have a whole bunch of different
distributions that represent all the

461
00:27:49,681 --> 00:27:52,740
probabilities of a possible outcome.
For a specific outcome.

462
00:27:52,770 --> 00:27:54,930
And so that's it at a high level.
And we have,

463
00:27:54,960 --> 00:27:57,600
I have an entire video on
distributions coming out later,

464
00:27:57,900 --> 00:28:00,750
but for now know that that's
what a distribution is.

465
00:28:01,020 --> 00:28:03,660
And we're going to use a
distribution to generate our data.

466
00:28:04,020 --> 00:28:08,040
We're going to use what's
called a multivariate normal
distribution to generate

467
00:28:08,160 --> 00:28:09,990
values for x and z.

468
00:28:10,080 --> 00:28:13,470
So we're going to use
the same distribution to
generate values for x and z

469
00:28:13,620 --> 00:28:18,300
because remember they are
closely correlated height
and weight and uh, there are,

470
00:28:18,360 --> 00:28:21,630
and we also define their covariance,
right, has very high 0.95.

471
00:28:22,050 --> 00:28:24,630
So X and Z are very,
are very closely related.

472
00:28:24,960 --> 00:28:28,440
And then the is going to be our variable
for our blood pressure and we'll,

473
00:28:28,441 --> 00:28:30,990
and we'll generate that
using a normal distribution.

474
00:28:31,200 --> 00:28:33,340
So a multivariate distribution,
normal district.

475
00:28:33,360 --> 00:28:37,530
So we used a multivariate normal
because we had multiple variables,

476
00:28:37,950 --> 00:28:41,750
x andZ and just a plain old normal
because we only had one variable it's

477
00:28:41,760 --> 00:28:43,350
generated from.
Okay.

478
00:28:44,670 --> 00:28:47,850
And we're going to compute what's
called the transpose of the results.

479
00:28:47,970 --> 00:28:50,130
And the results of these
are going to be in a matrix.

480
00:28:50,430 --> 00:28:54,270
And the transpose is when we take the
rows and the columns of the Matrix and we

481
00:28:54,271 --> 00:28:58,350
flipped them right? So that, that's just
what, that's a definition of a transpose.

482
00:28:59,700 --> 00:29:01,830
Okay. And that's gonna make it, um,

483
00:29:02,250 --> 00:29:06,240
more neatly formatted for us for future
operations that were going to perform on

484
00:29:06,241 --> 00:29:10,980
it. Okay, so we have our variables.
So let's create a pandas data frame.

485
00:29:10,981 --> 00:29:12,870
Remember I said how in pandas,

486
00:29:13,020 --> 00:29:17,160
a data frame is a very neat object
and neatly package object that lets us

487
00:29:17,190 --> 00:29:20,370
manipulate our data very easily.
So we'll put all those variable,

488
00:29:20,371 --> 00:29:24,030
all three of those variables in
our pandas data frame object.

489
00:29:24,420 --> 00:29:29,310
And then we're going to compute the log
odds for our three independent variables

490
00:29:29,490 --> 00:29:33,900
using the sigmoid function.
So we're going to take the sigmoid of,

491
00:29:34,740 --> 00:29:39,210
let's take these three functions times
or the dot product since it's technically

492
00:29:39,211 --> 00:29:41,590
a matrix.
And so just saying times,

493
00:29:41,591 --> 00:29:45,370
we're going to say the dot product cause
we're multiplying two matrices together,

494
00:29:45,820 --> 00:29:50,820
the Matrix of our features times or by
competing the dot product with that.

495
00:29:51,160 --> 00:29:55,510
And these, uh, these ideal
coefficients that we defined before,

496
00:29:55,750 --> 00:30:00,340
plus sigma, which is the,
which is the, uh, what was it?

497
00:30:00,341 --> 00:30:01,660
It was the variance of the noise.

498
00:30:01,661 --> 00:30:06,661
How spread out our data is times a normal
distribution between zero and one for

499
00:30:07,090 --> 00:30:08,320
each of our data points.
And,

500
00:30:09,670 --> 00:30:14,380
and so that's gonna give us the log odds,
which we can denote up here.

501
00:30:14,560 --> 00:30:18,250
Okay.
And once we have that,

502
00:30:19,390 --> 00:30:21,070
then we could say, okay, so those, that's,

503
00:30:21,071 --> 00:30:24,640
those are the log odds and we want to
compute the probability sample from a

504
00:30:24,641 --> 00:30:26,680
binomial distribution.
Okay.

505
00:30:26,681 --> 00:30:29,830
So a binomial random variable
is a number of successes.

506
00:30:29,831 --> 00:30:33,820
X has an end repeated trials
of a binomial experiment.

507
00:30:34,150 --> 00:30:39,010
A probability distribution of a binomial
random variable is called a binomial

508
00:30:39,011 --> 00:30:40,360
distribution,
right?

509
00:30:40,361 --> 00:30:44,470
So we have some y output value and
it's going to be between zero and one.

510
00:30:44,710 --> 00:30:47,440
And so we'll just compute all those
probabilities, like using this,

511
00:30:47,441 --> 00:30:51,940
so randomly generated probability values.
Okay.

512
00:30:51,941 --> 00:30:55,690
So then we can create a data frame
that's going to encompass our input data,

513
00:30:55,960 --> 00:31:00,370
our model formula and our
outputs, right? Just like this.

514
00:31:01,960 --> 00:31:05,230
So these are our expected output.
So we generated these outputs randomly,

515
00:31:05,231 --> 00:31:06,910
but they are expected outputs.

516
00:31:07,090 --> 00:31:11,500
So compare the expected outputs to our
predicted outputs and want to minimize,

517
00:31:11,650 --> 00:31:16,600
uh, the difference between them. Okay. And
that is our learning process. All right.

518
00:31:16,601 --> 00:31:17,860
And so if we were to print this out,

519
00:31:17,861 --> 00:31:21,640
we would get all these randomly generated
values and notice how we don't just,

520
00:31:21,670 --> 00:31:26,330
we don't just have, um, we don't just have
values for our three variables are our,

521
00:31:26,360 --> 00:31:31,210
sorry, our three features. We also have
values for each of the acts and then,

522
00:31:31,300 --> 00:31:35,950
uh, uh, the squared plus Z. Okay. And so,

523
00:31:35,980 --> 00:31:39,520
which is all a part of our model,
our formula,

524
00:31:39,521 --> 00:31:42,250
our function that we're
trying to learn here. Okay.

525
00:31:43,300 --> 00:31:48,280
And so we have that now we've defined
our data and we defined what a logistic

526
00:31:48,281 --> 00:31:51,970
regression is, how we optimize it,
Newton's method, we've defined our data.

527
00:31:52,330 --> 00:31:55,630
And now we can have this helper function,
which is going,

528
00:31:55,660 --> 00:31:58,630
which is just going to catch matrix
errors. That's it. It's not even,

529
00:31:58,830 --> 00:32:02,590
I wouldn't even really need it anyway.
So now it's time for us to,

530
00:32:03,670 --> 00:32:08,670
for us to learn our model and the way we
learn our model is by performing Nunes

531
00:32:10,001 --> 00:32:14,650
method for optimization. And so the
way for us to perform Nunes method is,

532
00:32:14,740 --> 00:32:19,510
is like, so okay for logistic regression,
here's how Newton's method works.

533
00:32:19,750 --> 00:32:24,750
So recall that Nunes method
from maximizing or minimizing
a given function f of

534
00:32:24,881 --> 00:32:25,631
its coefficient.

535
00:32:25,631 --> 00:32:30,040
So function given the Beta
value iteratively computes
the following estimate.

536
00:32:30,310 --> 00:32:30,581
Okay.

537
00:32:30,581 --> 00:32:35,320
So the ideal coefficient is going to
be whatever the current coefficient is

538
00:32:35,560 --> 00:32:38,530
minus the Hessian,
uh,

539
00:32:38,590 --> 00:32:43,590
the Hessian of our function or the inverse
of our Hashan Times the gradient of

540
00:32:44,001 --> 00:32:48,500
our function. And so that's, uh, how we,

541
00:32:48,560 --> 00:32:52,070
that's the high level of Newton's
method for logistic regression. Okay.

542
00:32:52,071 --> 00:32:52,910
So the Hessian,

543
00:32:52,911 --> 00:32:57,380
how do we compute the Hessian of the
log likelihood for logistic regression?

544
00:32:57,590 --> 00:32:58,760
The way we do that is like,

545
00:32:58,761 --> 00:33:03,761
so we say the Hessian of our function
is equal to the negative transpose of n

546
00:33:05,181 --> 00:33:07,250
times p plus one,

547
00:33:07,430 --> 00:33:12,430
which is what x represents n
times p plus one times end times.

548
00:33:13,371 --> 00:33:14,240
And a,

549
00:33:14,300 --> 00:33:18,980
an end times end diagonal matrix of
weights where each of these weights is p

550
00:33:18,981 --> 00:33:23,830
times one minus p times x
against, we already defined X. So,

551
00:33:23,860 --> 00:33:25,970
so multiply again.
Okay.

552
00:33:25,971 --> 00:33:30,230
So I know there was a lot to take in,
but that's the formula.

553
00:33:30,470 --> 00:33:33,620
That's the formula for
computing the Hessian.

554
00:33:33,710 --> 00:33:36,410
That's the formula for our
Hessian and then the function.

555
00:33:36,440 --> 00:33:39,230
Then the formula for our
gradient is much simpler.

556
00:33:39,231 --> 00:33:44,030
It's our gradient is the
transpose of acts, uh,

557
00:33:44,050 --> 00:33:48,560
times a column vector minus n
vector of probabilities. By the way,

558
00:33:48,590 --> 00:33:51,110
n is the number of data
samples samples we have.

559
00:33:51,710 --> 00:33:56,710
And the gradient is a vector of comp whose
components are the partial derivative

560
00:33:59,450 --> 00:34:02,360
with respect to each coefficient
we have in our function.

561
00:34:02,750 --> 00:34:06,980
Whereas the Hessian is a
vector where it's competing,

562
00:34:06,981 --> 00:34:09,320
not the partial derivative but the,

563
00:34:09,440 --> 00:34:12,200
but the partial derivative
of the partial derivative.

564
00:34:12,320 --> 00:34:15,290
So it's a derivative of the derivative.
It's a second order derivative.

565
00:34:15,530 --> 00:34:19,190
So if you were to say, um, uh,

566
00:34:19,220 --> 00:34:21,440
x cubed is your function,

567
00:34:21,560 --> 00:34:26,340
the derivative would be power
rule three x squared and but,

568
00:34:26,450 --> 00:34:30,740
and then the derivative of the
derivative would be six x. Okay.

569
00:34:30,741 --> 00:34:35,360
So that's how that works.
And so by the way,

570
00:34:35,420 --> 00:34:39,260
and so this is what, uh, that
w looks like it's a diagonal,

571
00:34:39,410 --> 00:34:41,870
which is if you were to
have a matrix of values,

572
00:34:41,871 --> 00:34:46,700
you would just take the diagonal of that
of p times one minus p where pure the

573
00:34:46,701 --> 00:34:50,840
predicted probabilities computed at the
current value of the coefficient that

574
00:34:50,841 --> 00:34:52,700
you have.
Okay.

575
00:34:52,701 --> 00:34:56,930
So and so you can connect this to
something called iteratively reweighted at

576
00:34:56,931 --> 00:34:59,480
least squares, but we're just not
going to do that. That's, that's,

577
00:34:59,481 --> 00:35:01,910
that's for a separate time has
just for it more understanding,

578
00:35:01,911 --> 00:35:04,880
but in fact I think it would be more
confusing because we haven't talked about

579
00:35:04,881 --> 00:35:08,540
that right now anyway.
Right?

580
00:35:08,541 --> 00:35:11,300
So to perform Newton's method,

581
00:35:11,330 --> 00:35:15,260
which is a second order optimization
technique on logistic regression,

582
00:35:15,680 --> 00:35:19,790
we're going to compute both the Hessian
and the gradient and we'll use both of

583
00:35:19,791 --> 00:35:24,590
those values to help find the optimal,
uh, coefficients for our function.

584
00:35:26,930 --> 00:35:31,520
And logistic regression is
basically on one side, it's a gem.

585
00:35:31,550 --> 00:35:33,470
It's a generic regression function.

586
00:35:33,820 --> 00:35:37,800
You know where you have some set of
coefficients for any number of parameter

587
00:35:37,801 --> 00:35:41,610
values for any number of features or
dimensions. And on the left hand side,

588
00:35:41,760 --> 00:35:46,760
you're not trying to equate it to a
single scalar that maps directly to that,

589
00:35:46,891 --> 00:35:51,480
that output. But instead a probability
value that is the log odds,

590
00:35:51,600 --> 00:35:55,440
the log of the probability
over one minus the probability,

591
00:35:56,820 --> 00:35:58,950
and this is going to come
out to an s shaped curve.

592
00:35:59,850 --> 00:36:04,850
So you can then predict the outcome of
some event or the class of what something

593
00:36:06,901 --> 00:36:11,640
is going to be, which is going to be
a discreet, a single value. Yes, no,

594
00:36:11,700 --> 00:36:16,410
or even multivariate, uh, or
even a multivariate output
like red, blue or green.

595
00:36:16,950 --> 00:36:21,870
Okay, so right now let's talk
about the implementation here.

596
00:36:21,871 --> 00:36:26,100
So we've talked about the equations. So
let's look at the code part now. Okay.

597
00:36:26,101 --> 00:36:29,160
So the first thing we're going to do is
we're going to compute the probability

598
00:36:29,161 --> 00:36:29,820
value.

599
00:36:29,820 --> 00:36:34,460
And to do that we're going to compute
the dot product of our coefficients are

600
00:36:34,500 --> 00:36:38,010
ideal coefficients.
And our w are coefficients that are,

601
00:36:38,340 --> 00:36:41,100
that are to be learned,
the coefficients that we're learning.

602
00:36:41,370 --> 00:36:44,940
And then we'll squash those
values with a sigmoid function.

603
00:36:44,941 --> 00:36:48,750
Squashing means we're converting
them into value between zero and one.

604
00:36:49,230 --> 00:36:51,360
And then we're only gonna
do that for two dimensions.

605
00:36:51,361 --> 00:36:56,361
So that's why we have the MDM
NDM men equals to a parameter.

606
00:36:58,560 --> 00:37:00,810
And then we're going to take the
transpose of that. And so for,

607
00:37:01,050 --> 00:37:02,400
for major CS competing,

608
00:37:02,401 --> 00:37:06,090
the transpose means taking the rows and
the columns and flipping them so the

609
00:37:06,091 --> 00:37:08,580
columns become the rows and vice versa.

610
00:37:09,180 --> 00:37:13,220
And that's just one of many
matrix operations that we do, uh,

611
00:37:14,010 --> 00:37:16,800
to keep it simple right now
it just makes things easier.

612
00:37:16,860 --> 00:37:20,370
It makes it more organize
and format it for,

613
00:37:20,490 --> 00:37:22,650
for the next operations
that we're going to do.

614
00:37:23,160 --> 00:37:26,310
We'll talk way more about
matrices later on anyway.

615
00:37:27,300 --> 00:37:29,400
So that's how we compute our probability,

616
00:37:29,430 --> 00:37:30,990
which is going to be an array of values.

617
00:37:31,320 --> 00:37:34,200
And then we're going to use that
probability to help compute our weights,

618
00:37:34,320 --> 00:37:36,990
which are, as we set
up here, right up here,

619
00:37:36,991 --> 00:37:40,860
the diagonal of the probability
times one minus p. Okay.

620
00:37:40,861 --> 00:37:43,710
And so we'll have that here.
And that's our weights.

621
00:37:43,711 --> 00:37:47,310
Those are our weights and we'll use
our weights to derive our Hessian.

622
00:37:47,340 --> 00:37:49,980
So remember this was a
formula for the Hessian.

623
00:37:50,160 --> 00:37:55,080
The negative negative x x is
transpose times w times x again,

624
00:37:55,590 --> 00:37:59,190
right? So in the case of
logistic regression, that's
how we compute the hash.

625
00:37:59,191 --> 00:38:01,740
And you can see variable by variable.

626
00:38:01,741 --> 00:38:05,520
That exact equation is then mimicked
here programmatically to compute that

627
00:38:05,521 --> 00:38:09,120
Hessian. And the same case would
be for our gradient, right?

628
00:38:09,121 --> 00:38:11,360
So our gradients is going to be x,

629
00:38:11,500 --> 00:38:14,790
x is transpose times,

630
00:38:15,120 --> 00:38:18,930
we're going to take the transpose of
backs and compute the dot product of it.

631
00:38:19,110 --> 00:38:21,210
And then y minus P,
right?

632
00:38:21,211 --> 00:38:25,650
We're why are predicted outputs
and then PR probability values.

633
00:38:25,860 --> 00:38:28,650
And that's gonna give us our
gradient value. And this, remember,

634
00:38:28,651 --> 00:38:30,150
this is all phrase single step.

635
00:38:30,330 --> 00:38:34,590
So this is for a single a collection
of data points, right? And we have a,

636
00:38:34,591 --> 00:38:37,360
we have to go through all
those rows and columns, right?

637
00:38:37,361 --> 00:38:41,590
But there's a very single step and we
have multiple steps during training, okay?

638
00:38:41,591 --> 00:38:44,860
So that's our gradient, our
Hessian, our weight values,

639
00:38:45,100 --> 00:38:49,270
and our probability values, okay?
And we're going to use all of those.

640
00:38:49,630 --> 00:38:53,830
We're going to use them, Paul, to
help us. Well, we, we use a Pete,

641
00:38:53,831 --> 00:38:58,300
we use P to help us compute w which
we used to compete our Hessian and our

642
00:38:58,301 --> 00:38:59,290
gradient as well.

643
00:38:59,530 --> 00:39:04,530
But basically we're going to use both
our Hessian and our gradient to compute

644
00:39:05,861 --> 00:39:10,630
what the optimal weights should be.
Right? So to compete, to do that,

645
00:39:10,631 --> 00:39:15,550
we're going to take the least
square solution. Um, but actually,

646
00:39:15,640 --> 00:39:20,470
so, okay, so I, okay, so we do this twice.

647
00:39:21,010 --> 00:39:23,830
We do,
it's once this Newton step,

648
00:39:23,890 --> 00:39:27,910
and this is basically a copy of what we
just did before, except for one thing.

649
00:39:28,240 --> 00:39:31,510
And that's this line right here. Notice
how right here I have Lynn alledged. Dot.

650
00:39:31,511 --> 00:39:35,770
Inthe versus Lin alledged.
Dot. Ella lst, LSTs s Q,

651
00:39:35,771 --> 00:39:37,180
which is least squares.

652
00:39:37,630 --> 00:39:42,630
So the reason that it's here twice
is because right now we're computing,

653
00:39:43,610 --> 00:39:46,120
uh,
the,

654
00:39:46,480 --> 00:39:51,480
the announced or the scalar that we're
going to use to update all of our

655
00:39:52,871 --> 00:39:56,650
coefficients, but we're doing it
using the full Hessian. Right?

656
00:39:56,770 --> 00:40:01,630
And so the reason that we did it two
ways is if we use the full Hessian,

657
00:40:01,780 --> 00:40:04,840
it can be computationally expensive
because there's a lot of values.

658
00:40:05,050 --> 00:40:08,110
But we can get around that by using a,

659
00:40:08,170 --> 00:40:12,430
by not computing the full Hessian but
using a solution called lease squares.

660
00:40:12,580 --> 00:40:16,300
And that's called the, so if we don't
have to compute the full Hessian,

661
00:40:16,570 --> 00:40:19,990
then that's computationally
less expensive, right?

662
00:40:20,290 --> 00:40:25,000
And so there are methods for doing this
and these are called quasi Newtonian

663
00:40:25,001 --> 00:40:27,400
methods,
or we don't compute the full Hessian,

664
00:40:27,730 --> 00:40:29,080
but we don't actually
have to talk about that.

665
00:40:29,081 --> 00:40:33,640
We'll just assume that we have to
compute the full Hessian right now. Okay.

666
00:40:33,641 --> 00:40:36,010
So that's what this line does.

667
00:40:36,310 --> 00:40:41,260
And it uses the regularization term we
defined before to make sure that this

668
00:40:41,261 --> 00:40:46,090
data is not overfit. Okay. So that's
what those into a Newton step.

669
00:40:46,270 --> 00:40:48,340
And it's going to return
this scalar value,

670
00:40:48,341 --> 00:40:52,960
this Beta value that we can use to update
the coefficient of our function that

671
00:40:52,961 --> 00:40:56,620
we're trying to learn.
And yeah,

672
00:40:57,100 --> 00:41:00,250
so then we have a convergence steps.

673
00:41:00,250 --> 00:41:02,530
So remember how we defined a threshold.

674
00:41:02,830 --> 00:41:05,110
That threshold is when we
want to stop learning, right?

675
00:41:05,200 --> 00:41:09,130
So given our old Beta values,
our new Beta values,

676
00:41:09,160 --> 00:41:11,920
and then our tolerance get
an a number of iterations,

677
00:41:12,340 --> 00:41:16,600
we'll check the change and
the coefficients by taking
the difference and making

678
00:41:16,601 --> 00:41:19,960
him performing absolute value because
we wanted it to be a positive value.

679
00:41:20,350 --> 00:41:24,940
And if the change is greater than our
threshold and we still have less iteration

680
00:41:25,000 --> 00:41:28,660
and we have, um, a certain
number of iterations still to go,

681
00:41:28,810 --> 00:41:31,750
then we'll return true
or false? False. Okay.

682
00:41:31,751 --> 00:41:34,340
So that's our way of stopping training.
Okay.

683
00:41:34,341 --> 00:41:38,030
So then to the real meat of our code,
right?

684
00:41:38,031 --> 00:41:41,030
So we'll go ahead and define
our initial set of coefficients.

685
00:41:41,240 --> 00:41:44,030
And these are our wake values,
right? That we want to learn.

686
00:41:44,200 --> 00:41:47,510
If they're all going to be Zeros rights
for the number of columns that we have,

687
00:41:47,690 --> 00:41:50,180
which are the number of
features, or they'll, they'll,

688
00:41:50,210 --> 00:41:53,900
they'll start off at zero and we'll
learn them over time given some number of

689
00:41:53,901 --> 00:41:56,030
iterations and,
uh,

690
00:41:56,300 --> 00:41:59,480
a boolean value that tells us if
we've reached convergence or not.

691
00:41:59,690 --> 00:42:03,620
So this is our way of saying should we
keep training or not and that that's what

692
00:42:03,621 --> 00:42:07,310
this is going to do. So we'll say
while not coefficients have converged.

693
00:42:07,460 --> 00:42:09,350
So if we haven't reached convergence,

694
00:42:09,650 --> 00:42:13,580
then go ahead and begin training using
Newton's method began optimizing.

695
00:42:13,850 --> 00:42:18,620
So we'll say we'll set the old
coefficients to our current values,

696
00:42:18,890 --> 00:42:22,070
perform a single step of Newton's
optimization on our data.

697
00:42:22,490 --> 00:42:24,410
And once we have that,

698
00:42:24,440 --> 00:42:29,270
we'll then take our coefficients and
it's going to output the updated a Beta

699
00:42:29,271 --> 00:42:31,250
values. Okay. And so then

700
00:42:33,470 --> 00:42:35,570
once we have our updated Beta values,

701
00:42:35,930 --> 00:42:39,020
then we can increment the
number of iterations that
were going through and then

702
00:42:39,021 --> 00:42:42,860
check if we've reached convergence
yet where our Beta values are the

703
00:42:42,861 --> 00:42:47,720
coefficients for the model that
we're learning. And we can say, hey,

704
00:42:47,721 --> 00:42:52,430
if these values are at this certain
threshold, we're good. We're done here.

705
00:42:52,910 --> 00:42:55,430
Okay.
So then when we print out the results,

706
00:42:55,640 --> 00:42:59,480
it's going to tell us the number of
iterations and then the Beta values over

707
00:42:59,481 --> 00:43:00,170
time.

708
00:43:00,170 --> 00:43:05,170
And ideally they become closer and closer
to our predefined Beta values for our

709
00:43:06,201 --> 00:43:09,020
dataset.
I know that's a lot to take in,

710
00:43:09,080 --> 00:43:13,430
but if you've got the basic idea of a
logistic regression being different from a

711
00:43:13,431 --> 00:43:17,150
linear regression and that it's trying
to predict the probability of a certain

712
00:43:17,180 --> 00:43:17,900
outcome,

713
00:43:17,900 --> 00:43:22,370
not just some strict linear mapping
between x and y and that we use Newton's

714
00:43:22,371 --> 00:43:27,371
method as a way to optimize logistic
regression and the w the way we compute

715
00:43:27,801 --> 00:43:32,450
Newton's method is to compete the second
derivatives with respect to each weight

716
00:43:32,480 --> 00:43:36,440
in our, in our function, the
coefficients, the weight values.

717
00:43:36,710 --> 00:43:40,430
If you got that part, that's all
that's really necessary for this video.

718
00:43:40,640 --> 00:43:43,880
There's a lot of terms that we still
have to go over a lot of probabilistic

719
00:43:43,881 --> 00:43:48,380
terms like variances,
very NCS and and noise values.

720
00:43:48,381 --> 00:43:51,080
There's so much more, so don't
worry about it. There's a lot,

721
00:43:51,260 --> 00:43:54,620
and we're going to go over that and then
the rest of this series, so that's it.

722
00:43:54,890 --> 00:43:56,720
Please subscribe for
more programming videos.

723
00:43:56,721 --> 00:44:00,800
And for now I've got to Ryan Beta with
Alpha and Beta, so thanks for watching.

