1
00:00:00,060 --> 00:00:00,781
Hello world,

2
00:00:00,781 --> 00:00:05,490
it's Saroj and what hyper parameters
should you use to train your model?

3
00:00:05,760 --> 00:00:08,070
You'll see these magic numbers a lot.

4
00:00:08,280 --> 00:00:12,450
They are the model values that are
set before you train on any dataset.

5
00:00:12,750 --> 00:00:13,680
An ML model.

6
00:00:13,700 --> 00:00:17,490
It's just a formula with a number of
parameters that need to be learned from

7
00:00:17,491 --> 00:00:18,180
data,

8
00:00:18,180 --> 00:00:22,650
but there are also parameters that can't
be directly learned from the regular

9
00:00:22,651 --> 00:00:26,130
training process,
and we call these higher level properties,

10
00:00:26,490 --> 00:00:31,350
hyper parameters. This could be the
number of trees in a random forest,

11
00:00:31,500 --> 00:00:34,380
the number of hidden
layers in a neural network,

12
00:00:34,620 --> 00:00:37,320
the learning rate for logistic regression.

13
00:00:37,350 --> 00:00:40,680
It's a process of trial and error
and it's not very intuitive.

14
00:00:40,681 --> 00:00:44,220
Since we're not great at
interpreting high dimensional data,

15
00:00:44,310 --> 00:00:48,180
legendary researchers like Oriole Vinyasa,
Ilya Sutskever,

16
00:00:48,480 --> 00:00:52,200
consider the possibility space
of hyper parameters there canvas.

17
00:00:52,260 --> 00:00:56,700
But what if we could have these
parameters learn the optimal values for

18
00:00:56,701 --> 00:00:59,370
themselves that would make life easier,
right?

19
00:00:59,700 --> 00:01:03,870
Let's see if we can figure out a really
basic strategy for ourselves and then

20
00:01:03,871 --> 00:01:04,830
try to improve it.

21
00:01:05,130 --> 00:01:09,210
I've got a Dataset here of tweets
that are labeled as either positive or

22
00:01:09,211 --> 00:01:13,290
negative perfect for a binary
classification problem,

23
00:01:13,560 --> 00:01:17,820
and let's say I built a support vector
machine to learn this mapping so it can

24
00:01:17,821 --> 00:01:20,670
then classify a new tweet immediately.

25
00:01:20,820 --> 00:01:25,320
This is called sentiment analysis and
it's a really popular task in NLP.

26
00:01:25,500 --> 00:01:28,560
If we mapped out these
vectors in two d space,

27
00:01:28,710 --> 00:01:32,790
we could imagine a curvy line that
separates the positive tweets from the

28
00:01:32,791 --> 00:01:37,110
negative ones, a decision boundary
separate but equal weight.

29
00:01:37,140 --> 00:01:38,070
That's something else.

30
00:01:38,340 --> 00:01:42,240
A support vector machine can help
us define this decision barrier.

31
00:01:42,450 --> 00:01:47,100
Since it's nonlinear, our SVM will
use what's known as the kernel trick.

32
00:01:47,310 --> 00:01:50,310
That means instead of trying
to fit a nonlinear model,

33
00:01:50,400 --> 00:01:55,380
we can map the data from the input
space to a new higher dimensional space

34
00:01:55,500 --> 00:02:00,500
called the feature space
by doing a nonlinear
transformation using a colonel or

35
00:02:01,200 --> 00:02:05,340
similarity function and then use a
linear model. In the feature space,

36
00:02:05,460 --> 00:02:10,460
we define our kernel or
similarity function between
tweak vectors as the radial

37
00:02:10,561 --> 00:02:11,520
basis function,

38
00:02:11,820 --> 00:02:16,410
which takes as input to vectors
and outputs a similarity
based on the following

39
00:02:16,411 --> 00:02:21,390
function. So the more similar two tweets
are the higher the output value from our

40
00:02:21,391 --> 00:02:21,960
function.

41
00:02:21,960 --> 00:02:26,460
There are two hyper parameters that
govern how our line is going to be drawn.

42
00:02:26,550 --> 00:02:30,150
Both of these hyper parameters
need to be selected very carefully.

43
00:02:30,300 --> 00:02:33,150
They depend on each other in unknown ways,

44
00:02:33,210 --> 00:02:37,350
so we can't just optimize one parameter
at a time. Then combine the result.

45
00:02:37,440 --> 00:02:42,180
What if we just tried every
single possible combination
of hyper parameters to

46
00:02:42,480 --> 00:02:45,780
brute force?
Assuming we've built our sem already,

47
00:02:45,870 --> 00:02:50,580
we can choose a set of possible values
for both of them and create a variable to

48
00:02:50,581 --> 00:02:53,040
store our model's accuracy for each set.

49
00:02:53,340 --> 00:02:57,360
Then we'll create a nested
for loop for every value of c,

50
00:02:57,420 --> 00:02:59,680
try every value of gamma inside our loop.

51
00:02:59,680 --> 00:03:03,280
We'll initialize our SVM with the
hyper parameters at that iteration.

52
00:03:03,400 --> 00:03:07,990
We'll train it and score it,
then compare its score to our best score.

53
00:03:08,110 --> 00:03:11,050
If it's better,
we'll update our values accordingly.

54
00:03:11,260 --> 00:03:16,260
This process will run for every hyper
parameter value we have until it finds the

55
00:03:16,900 --> 00:03:17,770
optimal ones.

56
00:03:18,040 --> 00:03:22,870
This technique is called grid search
because we've essentially made a grid of

57
00:03:22,871 --> 00:03:27,871
our search space and then evaluated each
hyper parameter setting at the points

58
00:03:28,030 --> 00:03:31,480
we've introduced for as many
dimensions as necessary.

59
00:03:31,840 --> 00:03:34,390
This was a pretty easy
strategy to implement,

60
00:03:34,540 --> 00:03:36,880
but this scales pretty poorly.

61
00:03:36,881 --> 00:03:39,910
The more hyper parameters
or dimensions we add,

62
00:03:39,970 --> 00:03:42,880
also known as the curse of dimensionality,

63
00:03:43,030 --> 00:03:46,420
I think we can do better
than an exhaustive search.

64
00:03:46,660 --> 00:03:51,430
We tried every combination of a preset
list of values of our hyper parameters,

65
00:03:51,730 --> 00:03:56,730
but what if instead we tried random
combinations of a range of values for a

66
00:03:56,831 --> 00:04:01,090
number of iterations we define this
won't guarantee that we'll get the best

67
00:04:01,091 --> 00:04:04,030
hyper parameter combination
like grid search did,

68
00:04:04,031 --> 00:04:07,900
but it will take a lot less time.
So manual search,

69
00:04:07,901 --> 00:04:11,320
grid search and random search
are all fine and dandy,

70
00:04:11,560 --> 00:04:15,850
but there's gotta be a more intelligent
way of doing this that incorporates

71
00:04:15,851 --> 00:04:16,540
learning.

72
00:04:16,540 --> 00:04:21,460
One technique that's very popular right
now is called Bayesian optimization.

73
00:04:21,820 --> 00:04:26,820
Last episode we talked about how base
theorem is a way to determine conditional

74
00:04:27,071 --> 00:04:27,904
probability.

75
00:04:28,060 --> 00:04:32,800
It shows us how to update an existing
prediction given new evidence.

76
00:04:33,040 --> 00:04:36,970
This forms the basis of the Bayesian
way of thinking as opposed to the

77
00:04:37,090 --> 00:04:38,890
frequentist way of thinking.

78
00:04:39,370 --> 00:04:42,490
These are the two different
approaches to probability.

79
00:04:42,520 --> 00:04:47,050
Basically it's like a mathematical
gang war between applied statisticians.

80
00:04:50,830 --> 00:04:55,550
How's it the cash? And Phil was my choices
that keep it objective. I'm selecting,

81
00:04:56,330 --> 00:04:58,910
it's all about the gear.
Bayesean

82
00:04:58,940 --> 00:05:00,080
means probabilistic.

83
00:05:00,260 --> 00:05:04,250
It focuses on the probability of
the hypothesis given the data.

84
00:05:04,460 --> 00:05:08,210
That means the data is fixed
and the hypothesis is random.

85
00:05:08,480 --> 00:05:12,860
The frequentist approach focuses on
the probability of the data given the

86
00:05:12,861 --> 00:05:16,730
hypothesis. So data is random
as in if we repeat the study,

87
00:05:16,731 --> 00:05:20,630
that data might come out differently,
but the hypothesis is fixed.

88
00:05:20,660 --> 00:05:25,400
We could apply frequentist or Bayesian
methods to pretty much any learning

89
00:05:25,430 --> 00:05:26,240
algorithm.

90
00:05:26,240 --> 00:05:31,220
They have different aims in the context
of hyper parameter optimization. Uh,

91
00:05:31,221 --> 00:05:35,930
Basie and approach takes advantage of
the information our model learns during

92
00:05:35,931 --> 00:05:37,610
the optimization process.

93
00:05:37,640 --> 00:05:42,200
The idea is that we pick some prior
belief about how our hyper parameters will

94
00:05:42,201 --> 00:05:47,201
behave and then search the perimeter
space by enforcing and updating our prior

95
00:05:47,511 --> 00:05:50,660
belief based on our ongoing measurement.

96
00:05:50,840 --> 00:05:53,690
So the trade off between exploration,

97
00:05:53,870 --> 00:05:58,730
making sure we visited the relevant
corners of our space and exploitation.

98
00:05:58,880 --> 00:06:01,250
Once we found the promising
region of our space.

99
00:06:01,400 --> 00:06:05,420
Finding an optimal value in it is
handled in a more intelligent way.

100
00:06:07,600 --> 00:06:10,950
No, we only have few weeks left
with mid serial last model.

101
00:06:11,550 --> 00:06:13,950
I know eventually we'll find
the right hyper parameters,

102
00:06:14,780 --> 00:06:18,000
but how we tried to frequent this
approach and it didn't work. I know,

103
00:06:18,030 --> 00:06:22,560
but we could try the Bayesean. Oh, so we
can bank on certain in our mobile itself,

104
00:06:22,980 --> 00:06:25,470
like random variable.
Exactly. A lot of people are,

105
00:06:25,490 --> 00:06:27,670
the models are using it these days.
Let's go ahead.

106
00:06:28,520 --> 00:06:33,520
Basie and optimization uses previously
evaluated points to compute a posterior

107
00:06:33,680 --> 00:06:36,590
expectation of what the loss f looks like.

108
00:06:36,800 --> 00:06:41,360
Then it samples a loss at a new point
that maximizes some utility of the

109
00:06:41,361 --> 00:06:42,860
expectation of f.

110
00:06:42,950 --> 00:06:47,510
That utility tells us which regions of
the domain of f our best to sample from

111
00:06:47,780 --> 00:06:51,440
this two step process is
repeated until convergence.

112
00:06:51,710 --> 00:06:53,120
For the prior distribution.

113
00:06:53,270 --> 00:06:56,360
We assume that F can be
described by a golf Sian process.

114
00:06:56,630 --> 00:07:01,340
A gaussian distribution often called a
normal distribution is described as a

115
00:07:01,341 --> 00:07:02,570
bell shaped curve.

116
00:07:02,960 --> 00:07:07,960
Distributions are equations that will
link outcomes of a statistical experiment

117
00:07:08,300 --> 00:07:12,650
with its probability of occurring.
The Gospel, Ian is quite popular.

118
00:07:12,710 --> 00:07:16,640
Half the data falls on the left of the
mean half falls on the right and this is

119
00:07:16,641 --> 00:07:18,500
useful in many situations.

120
00:07:18,800 --> 00:07:23,800
A Goss IOM process is a generation of
the golf Sian distribution over functions

121
00:07:23,840 --> 00:07:25,370
instead of random variables.

122
00:07:25,520 --> 00:07:30,520
So wild gosh in distributions
are specified by their
mean and variance Gaussian

123
00:07:30,891 --> 00:07:35,600
processes are specified by their main
function and covariance function.

124
00:07:35,630 --> 00:07:39,740
The way we find the best point to sample
f next from is to pick the point that

125
00:07:39,741 --> 00:07:42,080
maximizes an acquisition function.

126
00:07:42,170 --> 00:07:46,220
This is a function of the posterior
distribution over f that describes the

127
00:07:46,221 --> 00:07:49,040
utility for all values
of the hyper parameters.

128
00:07:49,190 --> 00:07:53,120
The values that have the highest utility
will be the values we compute the Los

129
00:07:53,121 --> 00:07:53,960
four next,

130
00:07:54,260 --> 00:07:58,940
we'll use the popular expected improvement
function where x is the current

131
00:07:58,970 --> 00:08:02,420
optimal set of hyper parameters.
By maximizing this,

132
00:08:02,510 --> 00:08:05,870
it will give us the point
that improves on f the most.

133
00:08:06,110 --> 00:08:08,600
So given the observed values f of x,

134
00:08:08,780 --> 00:08:13,010
we update the posterior expectation
of f using the GP model.

135
00:08:13,280 --> 00:08:17,390
Then we find that the new acts that
maximizes the acquisition function,

136
00:08:17,410 --> 00:08:19,220
the expected improvement,

137
00:08:19,430 --> 00:08:23,360
and finally compute the
value of f for the new X.

138
00:08:23,360 --> 00:08:25,790
Initially the algorithm will
explore the parameter space,

139
00:08:25,820 --> 00:08:30,820
but it quickly discovers the region with
best performance and samples points in

140
00:08:30,831 --> 00:08:34,220
that region. Much smarter
strategy, right? To summarize,

141
00:08:34,250 --> 00:08:38,240
we can optimize our hyper
parameters using several strategies,

142
00:08:38,420 --> 00:08:41,120
but Basie and optimization
looks most promising.

143
00:08:41,570 --> 00:08:45,890
Bayesean optimization picks a prior
belief about how the hyper parameters will

144
00:08:45,891 --> 00:08:50,891
behave and then searches the parameter
space by enforcing an updating that prior

145
00:08:51,291 --> 00:08:53,720
belief based on ongoing measurements.

146
00:08:53,930 --> 00:08:58,110
So Bayesians let their prior beliefs
influenced their predictions.

147
00:08:58,350 --> 00:09:01,860
Frequent tests don't.
And now to announce the real heroes,

148
00:09:01,890 --> 00:09:04,210
the wizards of the week
is Noah [inaudible].

149
00:09:04,410 --> 00:09:08,910
Noah built a naive Bayes classifier to
distinguish rap lyrics from biggie and

150
00:09:08,911 --> 00:09:10,830
Tupac.
He uses unigrams,

151
00:09:10,831 --> 00:09:15,270
bigrams and trigrams to count words and
achieve the best results with unigrams.

152
00:09:15,360 --> 00:09:15,901
Great work.

153
00:09:15,901 --> 00:09:20,700
Noah and the runner up is Hammad shake
his naive Bayes classifier detected spam

154
00:09:20,701 --> 00:09:21,660
and youtube comments.

155
00:09:21,870 --> 00:09:26,520
And he's a popular TF IDF technique to
determine how important and word was

156
00:09:26,521 --> 00:09:29,430
instead of bag of words.
Next level,

157
00:09:29,490 --> 00:09:34,110
this week's coding challenge is to write
out a Bayesean optimization strategy

158
00:09:34,290 --> 00:09:37,260
for the hyper parameters of
a linear regression model.

159
00:09:37,440 --> 00:09:40,930
See the get hub link in the description
for instructions and poster,

160
00:09:40,980 --> 00:09:44,430
get hub link in the comment section.
I'll announce a winner next week.

161
00:09:44,460 --> 00:09:46,350
Please subscribe for
more programming videos,

162
00:09:46,351 --> 00:09:50,100
and for now I've got to update my priors,
so thanks for watching.

