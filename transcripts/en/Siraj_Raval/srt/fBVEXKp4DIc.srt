1
00:00:00,010 --> 00:00:03,960
Are Live. Hello old. It's the Raj.

2
00:00:03,961 --> 00:00:07,590
We started a little late because I
was having some tensor board trouble,

3
00:00:07,591 --> 00:00:08,640
which I still am having,

4
00:00:08,970 --> 00:00:13,320
but it's all good because when I say I'm
going to go live, I'm going to go live.

5
00:00:13,321 --> 00:00:16,410
So today we are going to
talk about tenser board.

6
00:00:16,710 --> 00:00:20,340
So tensor board is the tool that comes
with tensorflow. So it's Builtin.

7
00:00:20,550 --> 00:00:22,440
So if you've,
if you've installed tensorflow,

8
00:00:22,560 --> 00:00:27,480
tensor board comes with it and the goal
of tensor board is to help you visualize

9
00:00:27,481 --> 00:00:30,750
your data. Uh, there's it, it
can help you optimize your data.

10
00:00:30,751 --> 00:00:32,930
It can help you debug
it, it can help you, uh,

11
00:00:33,480 --> 00:00:37,680
look at ways that you can improve it.
And there's also the plugins feature,

12
00:00:37,681 --> 00:00:41,190
which is coming soon. So I can't wait for
that. But yeah, it's a pretty cool tool.

13
00:00:41,250 --> 00:00:43,350
And I admit I haven't used it much,

14
00:00:43,380 --> 00:00:47,700
but I s B and that's because I think
the use case for it really shows itself

15
00:00:47,701 --> 00:00:51,030
when your data, it starts to get
really complex. That is your model.

16
00:00:51,031 --> 00:00:53,070
So when your model
start to get really big,

17
00:00:53,310 --> 00:00:55,680
that's when tensor board
becomes very useful.

18
00:00:55,860 --> 00:00:58,020
And what are the reasons
I'm demoing it now?

19
00:00:58,140 --> 00:01:00,120
So I already made a video
intensive board before,

20
00:01:00,300 --> 00:01:04,230
but one of the reasons I'm doing one
again is because the embedding visualizer

21
00:01:04,231 --> 00:01:08,210
is a pretty cool tool and I'd love to talk
about that. And it's a new tool and a,

22
00:01:08,220 --> 00:01:09,840
it's going to be really cool.
So in this demo,

23
00:01:09,841 --> 00:01:14,070
what we're going to do is we're
going to look at a m and I is tea.

24
00:01:14,071 --> 00:01:17,400
Now I know you guys have seen
Mni and I 1,000,001 times,

25
00:01:17,790 --> 00:01:19,200
but the point of this is not immunized.

26
00:01:19,330 --> 00:01:21,570
It's the embedding
visualizers specifically.

27
00:01:21,571 --> 00:01:26,010
So that's what we're going
to look at. And uh, so yeah,

28
00:01:26,011 --> 00:01:29,730
I'm going to start off by showing the
code, uh, that we're going to use a,

29
00:01:29,731 --> 00:01:32,140
and then we're going to
visualize it intense or board,

30
00:01:32,620 --> 00:01:37,410
and then we're going to look
at the details for each of
the sessions. All right,

31
00:01:37,411 --> 00:01:42,411
so let me start off by
looking at the code here.

32
00:01:43,140 --> 00:01:43,590
Uh,

33
00:01:43,590 --> 00:01:47,280
so what I'm gonna do is I'm
going to explain the code
and then we're going to go

34
00:01:47,281 --> 00:01:48,114
into the,

35
00:01:49,160 --> 00:01:49,993
uh,

36
00:01:50,460 --> 00:01:55,080
the session itself. Okay. Or
the code itself. All right,

37
00:01:55,081 --> 00:01:59,520
so to start off with, let me remove
some things. So to start off with,

38
00:01:59,521 --> 00:02:01,290
we're going to look at this code.
So I've got it up here.

39
00:02:01,380 --> 00:02:05,430
I want you guys to download the
code and follow along with me. Okay?

40
00:02:05,610 --> 00:02:08,970
So what this code does is it
is a convolutional network.

41
00:02:09,360 --> 00:02:12,270
It's a convolutional net and it

42
00:02:13,850 --> 00:02:17,330
is meant to classify a
handwritten characters. Okay?

43
00:02:17,540 --> 00:02:21,170
So let's look at what
it is and let me say,

44
00:02:21,470 --> 00:02:23,780
let me start off with a two
minute Q and a. Okay. So ask,

45
00:02:23,810 --> 00:02:28,160
go ahead and ask questions and
uh, yeah, it's gonna be awesome.

46
00:02:33,200 --> 00:02:37,880
We got code, we got questions, we've
got sublime, we got audio lag. Yeah.

47
00:02:37,910 --> 00:02:38,750
All the things we're here.

48
00:02:40,020 --> 00:02:41,170
I'll do luxury feel.

49
00:02:42,560 --> 00:02:45,440
We are all tensors.
Okay.

50
00:02:46,880 --> 00:02:50,900
Any questions guys before I get started?
Because I'm about to go in,

51
00:02:51,710 --> 00:02:53,690
go in on this,
no pun intended.

52
00:02:54,390 --> 00:02:55,223
Okay.

53
00:02:57,610 --> 00:03:02,290
How do we do? Okay. How do
we, the typology of a neural
network? Great question.

54
00:03:02,410 --> 00:03:06,280
Uh, so many decisions go into
that depending on your use case,

55
00:03:06,281 --> 00:03:07,390
what you're trying to do.

56
00:03:07,690 --> 00:03:12,130
But I would say there are a few
general rules to go by rules of thumb.

57
00:03:12,250 --> 00:03:15,580
One would be the more layers,
the more computation,

58
00:03:15,581 --> 00:03:20,260
but also the more accuracy your model.
We'll have, uh, what type of network.

59
00:03:20,290 --> 00:03:23,560
It depends on your use case.
Convolutional network for images,

60
00:03:23,740 --> 00:03:28,450
recurrent network for time series data
feed forward network for binary data.

61
00:03:28,720 --> 00:03:30,580
Uh,
and then more complex data.

62
00:03:30,581 --> 00:03:32,590
There are different types
of models to more questions.

63
00:03:32,591 --> 00:03:34,630
And then we're going to get started.
Uh,

64
00:03:34,631 --> 00:03:36,430
audio is a little low but
we'll take care of that.

65
00:03:36,640 --> 00:03:39,220
How do you use tensor
board with Floyd hub? Uh,

66
00:03:40,900 --> 00:03:43,930
I haven't heard of Floyd hub. I actually,
I've heard the term but I don't,

67
00:03:43,960 --> 00:03:48,550
I don't know what Floyd hub is. Uh,
how much product goes into that hair?

68
00:03:48,551 --> 00:03:52,780
So surprisingly none. It's just I
just roll out of bed and I, and I,

69
00:03:52,781 --> 00:03:57,580
and I do this one more
question, not about my hair. Uh,

70
00:03:57,730 --> 00:04:01,390
how do we use tensor board with AWS
servers? Now, that's a great question.

71
00:04:01,391 --> 00:04:05,080
So I actually have not
seen that done before. Uh,

72
00:04:05,140 --> 00:04:09,430
using tensor board in the
cloud, uh, would be a good idea.

73
00:04:09,460 --> 00:04:13,300
If you had, uh, uh, if you
were using transfer learning.

74
00:04:13,301 --> 00:04:17,470
So if you are using a model that you
yourself hadn't trained and it's a huge

75
00:04:17,471 --> 00:04:19,700
model, then using it, um,

76
00:04:19,780 --> 00:04:22,720
without having to download it directly
from the web would be actually,

77
00:04:22,721 --> 00:04:25,900
that'd be a great startup idea.
So not just AWS, but just visual.

78
00:04:25,990 --> 00:04:27,880
So here's a great startup
idea for you guys.

79
00:04:28,000 --> 00:04:32,440
Visualizing modeled architecture in
the web is a problem that has not been

80
00:04:32,441 --> 00:04:33,640
tackled at all right now.

81
00:04:33,641 --> 00:04:37,350
We have to do this locally and no one
wants to have to do that. So, uh, great.

82
00:04:37,360 --> 00:04:42,340
That would be a great problem.
Okay, so let's go into the
code. Okay guys. So let's,

83
00:04:42,370 --> 00:04:44,320
so what I'm going to do is I'm
not going to type out the code,

84
00:04:44,321 --> 00:04:48,690
I'm going to talk about it, and then
we're going to look at the tens board. Uh,

85
00:04:51,560 --> 00:04:54,050
then we're going to look at the tender
board version. Okay? So to start off with,

86
00:04:54,051 --> 00:04:57,950
let me, let me maximize this code
so we have a good look at it. Okay,

87
00:04:58,430 --> 00:05:02,910
let's maximize. So here are our
libraries. Okay. And then for URL live,

88
00:05:02,930 --> 00:05:06,230
I have this little
versioning because you know,

89
00:05:06,260 --> 00:05:10,790
for python three versus python
to URL live is annoying.

90
00:05:10,791 --> 00:05:15,200
So it's, we have to have this.
And so then these two constants,

91
00:05:15,290 --> 00:05:18,530
these constant values are what we
are doing here. And so this is the,

92
00:05:18,531 --> 00:05:21,590
this is the novel part
is we are importing the,

93
00:05:22,640 --> 00:05:23,090
okay,

94
00:05:23,090 --> 00:05:26,360
we are importing the
embeddings from the web.

95
00:05:26,361 --> 00:05:29,930
So let's look at what this link is.
So on get hub,

96
00:05:30,260 --> 00:05:33,800
if we look at this link,
we can see that

97
00:05:36,770 --> 00:05:40,370
that's an invalid request. That's
what we can see. But there's also,

98
00:05:44,090 --> 00:05:44,923
let's see.

99
00:05:49,310 --> 00:05:50,190
Mm.

100
00:05:50,760 --> 00:05:52,260
So okay,
hold on.

101
00:05:54,880 --> 00:05:55,713
Okay,

102
00:05:56,390 --> 00:05:58,100
so here's what we're,
we're

103
00:05:58,100 --> 00:05:59,720
pulling,
okay,

104
00:06:00,250 --> 00:06:03,400
these, so what are these? So let
me, I actually have these locally.

105
00:06:03,401 --> 00:06:04,630
So let me just open them locally.

106
00:06:04,900 --> 00:06:08,980
So there are two things
here that we are pulling.

107
00:06:09,010 --> 00:06:13,600
The first is a collection of sprites.
So these are sprites. So what are these?

108
00:06:13,660 --> 00:06:16,720
So what we are doing is we are
classifying handwritten characters. Now,

109
00:06:16,750 --> 00:06:20,410
most of us are familiar with this,
with this example, but this is novel.

110
00:06:20,411 --> 00:06:24,010
So for the embedding visualizer,
which is a new feature of tensor board,

111
00:06:24,550 --> 00:06:29,550
we can visualize the learned vectors that
are model creates when training on our

112
00:06:30,761 --> 00:06:35,740
input data and input labels. So we have
characters, right? One through nine,

113
00:06:36,040 --> 00:06:38,170
and these are what are considered sprites.

114
00:06:38,200 --> 00:06:41,950
So when we learn these mappings from
our model, and our model is training,

115
00:06:42,220 --> 00:06:45,190
it's learning what a seven is,
it's learning what the end,

116
00:06:45,191 --> 00:06:49,070
these vectors represent the correlation
between the actual number, the, the,

117
00:06:49,090 --> 00:06:51,370
the number seven,
and the image of seven.

118
00:06:51,490 --> 00:06:53,680
And that correlation is
represented as a vector.

119
00:06:53,860 --> 00:06:57,940
So the way that we can then represent
that vector is to have a sprite.

120
00:06:58,240 --> 00:07:02,740
And these are the sprites. And so
this is this, this, uh, image are,

121
00:07:02,970 --> 00:07:07,150
uh, is going to be cut into
a cut into squares. Okay?

122
00:07:07,151 --> 00:07:09,790
So starting from the top
left to the bottom right,

123
00:07:09,791 --> 00:07:13,390
we can think of it as a matrix.
And when we visualize it intense or board,

124
00:07:14,440 --> 00:07:17,170
when we visualize it, intention
board, these vectors are,

125
00:07:17,200 --> 00:07:20,140
these sprites are going to show up as the,
um,

126
00:07:21,040 --> 00:07:25,720
as the images for what we've just
visualized. Okay. So, so just like this,

127
00:07:26,290 --> 00:07:28,510
I'm doing this because I haven't
actually launched tensor board yet,

128
00:07:28,810 --> 00:07:32,330
but it would look like this, right?

129
00:07:32,331 --> 00:07:35,720
So see these fours and the Zeros,
those, those are the sprites. Those are,

130
00:07:35,721 --> 00:07:38,570
those are values that are coming
directly from this sprite file.

131
00:07:38,750 --> 00:07:42,080
And then associated with that,
we also have a,

132
00:07:43,040 --> 00:07:46,010
we have a, let me see anywhere
where, where'd I put this?

133
00:07:48,310 --> 00:07:52,390
Uh, labels file, which is a TSV
file. Okay. So the labels file is,

134
00:07:54,170 --> 00:07:57,400
it's a file extension.
The file extension for Tab d limited.

135
00:07:58,150 --> 00:07:59,380
It's like for spreadsheets.

136
00:07:59,410 --> 00:08:02,860
So it's important into an export
from spreadsheet software.

137
00:08:02,980 --> 00:08:06,490
And these are just the,
the associated numbers.

138
00:08:07,000 --> 00:08:11,050
These are the associated numbers.
It's such a weird format,

139
00:08:11,051 --> 00:08:15,700
but it every single number associates
with one of these images from this a

140
00:08:15,760 --> 00:08:18,490
matrix right here. Okay. So

141
00:08:20,300 --> 00:08:24,350
each number is associated. So that's
what, that's what we're doing. Okay. Um,

142
00:08:26,990 --> 00:08:31,950
okay, so that's, that's what that
is though. Let's keep going here.

143
00:08:32,100 --> 00:08:37,040
So the next part is for us to,
uh, download these embeddings.

144
00:08:37,041 --> 00:08:40,320
So what,
what this line does is it's,

145
00:08:40,710 --> 00:08:44,970
it is downloading the Mni data and
it's splitting it into three parts.

146
00:08:44,971 --> 00:08:48,230
You have 55,000 data
points of training data,

147
00:08:48,250 --> 00:08:53,040
10,000 points of testing data,
and then 5,000 points of validation data.

148
00:08:53,340 --> 00:08:57,420
And so we're going to call the images
and we're going to call the labels why?

149
00:08:57,450 --> 00:09:00,450
And each image is 28 by 28 pixels.
Okay?

150
00:09:00,451 --> 00:09:04,290
So these are each of these images and
that's what the read data sets function is

151
00:09:04,291 --> 00:09:08,580
doing. Having downloaded
that from the web, okay? And

152
00:09:10,360 --> 00:09:11,193
okay.

153
00:09:11,210 --> 00:09:14,270
And we're going to store that in m
and ist the, the, the variable. Okay?

154
00:09:14,660 --> 00:09:18,860
So let me increase the font
size a little bit. Okay, so now,

155
00:09:19,610 --> 00:09:22,790
okay, so now that we've done that,
we're going to define our layers.

156
00:09:22,820 --> 00:09:25,770
So we have convolutional
layers. Okay? So, uh,

157
00:09:26,930 --> 00:09:29,600
for our convolutional layers,
let's talk about what's happening here.

158
00:09:29,750 --> 00:09:33,410
So in a standard convolutional
network, uh, we have, what are,

159
00:09:33,500 --> 00:09:38,450
we have three layers and I'm going to
answer questions in 20 minute intervals.

160
00:09:38,451 --> 00:09:40,670
So in seven minutes I'll
start answering questions.

161
00:09:40,671 --> 00:09:44,150
But let's look at this
architecture right here.

162
00:09:44,151 --> 00:09:47,720
So I think an inception is a good example.

163
00:09:47,780 --> 00:09:52,010
Convolutional net, right? So

164
00:09:54,240 --> 00:09:58,950
in standard convolutional
networks, we have, uh, what
are considered blocks. Okay.

165
00:09:58,951 --> 00:10:01,740
So it doesn't matter what these words say,

166
00:10:01,741 --> 00:10:04,590
but we can just think of
these as blocks. Okay. So, uh,

167
00:10:04,740 --> 00:10:07,020
this is actually a huge convolutional net,

168
00:10:07,050 --> 00:10:09,810
probably one of the biggest in
the world, in the world. Uh,

169
00:10:09,840 --> 00:10:13,020
but the point is that we
have a convolutional layer.

170
00:10:13,200 --> 00:10:17,520
And in each of these blocks we
have three things. We have a,

171
00:10:18,120 --> 00:10:22,110
uh,
we have an input.

172
00:10:22,260 --> 00:10:26,040
So typically a CNN is composed of a stack
of convolutional modules that perform

173
00:10:26,041 --> 00:10:29,820
feature extraction. So each of those
modules consist of a pooling layer.

174
00:10:30,280 --> 00:10:34,710
It's gotten the, and then, uh, fully,
uh, fully connected layer. And then a,

175
00:10:34,770 --> 00:10:38,910
um, what's the next one? And then a soft
Max activation function. So we squash it.

176
00:10:39,090 --> 00:10:41,250
So we just keep doing that every time.
Okay.

177
00:10:41,251 --> 00:10:43,880
So we're going to define that
programmatically here. Uh,

178
00:10:44,190 --> 00:10:46,470
so let's go ahead and do that. So, um,

179
00:10:46,710 --> 00:10:51,450
so the first line here is the name scope.
So if so, or board we have named scopes,

180
00:10:51,451 --> 00:10:55,860
right? So named scopes basically
define, so this, this one's not working.

181
00:10:55,861 --> 00:10:58,980
So right now, boys, you what? We're going
to look at tensor board in a second,

182
00:10:59,430 --> 00:11:01,630
but right now let's look
at the names go feature.

183
00:11:01,631 --> 00:11:06,000
So there are so many variables.
There's so many tensors,

184
00:11:06,001 --> 00:11:10,730
there's so many components
to a computation graph.
So what names, scopes do,

185
00:11:10,740 --> 00:11:15,300
is there a way for us to encapsulate
all of that complexity under one name so

186
00:11:15,301 --> 00:11:19,730
that it's easier for us to
view in the graph? Okay. Uh,

187
00:11:19,770 --> 00:11:21,180
it's easier for us to view in the graph.

188
00:11:21,181 --> 00:11:23,220
So in this case we're using a name scope,

189
00:11:23,340 --> 00:11:26,480
and we're going to define it when we
call this, this function. But the,

190
00:11:26,481 --> 00:11:31,481
the name scope itself is
going to encapsulate the
weights and biases and the,

191
00:11:31,770 --> 00:11:34,800
uh, the convolutional
part, the activation part.

192
00:11:34,801 --> 00:11:37,860
And then the pooling
part of this, this, uh,

193
00:11:37,920 --> 00:11:41,160
this convolutional what
we can call a block. Okay?

194
00:11:41,161 --> 00:11:45,860
So convolutional block consisting of
these three layers of pooling, of, uh,

195
00:11:46,350 --> 00:11:49,350
of an activation function and
the weights and biases. So this,

196
00:11:49,351 --> 00:11:50,550
it's all going to be encapsulated.

197
00:11:50,551 --> 00:11:52,620
And then we're going to see
that in tenser board when we,

198
00:11:52,650 --> 00:11:56,500
when we visualize it in
a second. Okay? So, uh,

199
00:11:56,501 --> 00:11:59,620
so up TF variable maintains
the state of the graph. Okay?

200
00:11:59,621 --> 00:12:04,360
So that's what we were using variable
for and truncated normal is going to

201
00:12:04,390 --> 00:12:08,920
output the random values from a
truncated normal distribution.

202
00:12:09,520 --> 00:12:14,170
And so then the constant is going
to create a constant tensor.

203
00:12:14,590 --> 00:12:18,190
Okay? So that those, those are our
weights and our biases. This end,

204
00:12:18,220 --> 00:12:21,820
this is going to be for our first
a convolutional block. Okay.

205
00:12:22,970 --> 00:12:27,410
And then we're going to compute a twoD
convolution, given a four d input.

206
00:12:27,411 --> 00:12:31,430
That is the 40 tensor that is coming
directly from the place holder that we're

207
00:12:31,431 --> 00:12:34,670
going to define later. So right
when we input data into our model,

208
00:12:34,820 --> 00:12:37,820
it's going to go right into
this convolutional block. Okay?

209
00:12:37,910 --> 00:12:41,990
And then once we have that, we're going
to create what are called histograms.

210
00:12:42,080 --> 00:12:44,210
So let's talk about TF
summaries for a second.

211
00:12:44,211 --> 00:12:47,090
So what are these summaries
and why do we use them? So,

212
00:12:47,091 --> 00:12:51,920
so a summary is a flow operation
that outputs protocol buffers.

213
00:12:52,100 --> 00:12:56,330
So protocol buffers are a way of encoding,
are a way of encoding,

214
00:12:56,650 --> 00:13:00,310
uh, data. So it's, it's,

215
00:13:00,311 --> 00:13:04,180
it's serializing the data that
we have in memory to disc.

216
00:13:04,360 --> 00:13:07,900
So we're writing it to disk so that we
can then pull it intensive board and

217
00:13:07,901 --> 00:13:12,400
visualize it. So that's what the
TF summary, uh, function does.

218
00:13:12,550 --> 00:13:15,280
And there are different things
that we can use a summaries for.

219
00:13:15,281 --> 00:13:17,290
So intense or board.
We have,

220
00:13:17,710 --> 00:13:20,500
we have different types of
summaries and we can look,

221
00:13:20,501 --> 00:13:23,260
we can see what types they are
by looking at the tabs up here.

222
00:13:23,500 --> 00:13:27,790
So we have summaries for scalers, for
images, audios, graphs, distributions,

223
00:13:27,791 --> 00:13:29,470
histograms and embeddings.

224
00:13:29,590 --> 00:13:32,710
So we have different summaries for both
and we'll talk about when we use those

225
00:13:32,711 --> 00:13:36,060
summaries and why we
use them. But, um, for,

226
00:13:36,080 --> 00:13:39,700
for this case we're going to use histogram
summaries because we want to see a

227
00:13:39,701 --> 00:13:44,320
distribution of values across the weights,
biases and activations.

228
00:13:45,400 --> 00:13:48,850
Okay. Because we're randomly initializing
these values right now, right?

229
00:13:48,851 --> 00:13:53,440
With this truncated normal function. And
as, as we update those values over time,

230
00:13:53,590 --> 00:13:57,070
there's going to be a distribution of
possibilities and we want to show what

231
00:13:57,071 --> 00:14:01,510
they actually are and then also what
they could be. Uh, so then that,

232
00:14:01,570 --> 00:14:03,040
and that's useful for debugging.

233
00:14:03,070 --> 00:14:06,820
So we could then rerun our model with
a different set of hyper parameters and

234
00:14:06,821 --> 00:14:11,710
see how those distributions move. So
that's, um, what this function is.

235
00:14:11,711 --> 00:14:14,470
And then we have a Max pooling.
Uh,

236
00:14:15,610 --> 00:14:16,040
okay.

237
00:14:16,040 --> 00:14:17,750
We have a Max pooling,
uh,

238
00:14:18,270 --> 00:14:18,630
okay.

239
00:14:18,630 --> 00:14:22,270
Layer, which pooling in general is
really cool. So pooling, I mean pulling,

240
00:14:22,280 --> 00:14:25,290
there's so many different types of
pooling methods that we could use.

241
00:14:25,530 --> 00:14:29,050
And Max pooling seems to be the one that's
used most often. So if we could, it's,

242
00:14:29,060 --> 00:14:33,210
so for Max pooling, uh, let's say we
have a four by four matrix, right?

243
00:14:33,211 --> 00:14:36,300
We have a four by four matrix
and that is our input image.

244
00:14:36,510 --> 00:14:37,890
And we have a two by two filter.

245
00:14:37,891 --> 00:14:41,580
So the reason it's called convolutional
is because we are taking a filter,

246
00:14:41,581 --> 00:14:45,660
which is kind of like a flashlight and
we are convening around an image and

247
00:14:45,661 --> 00:14:49,360
we're only using the parts of it that
we find relevant. So it's like, uh,

248
00:14:49,430 --> 00:14:53,150
it's like taking a flashlight to an
image and at what those relevant features

249
00:14:53,151 --> 00:14:57,920
are. And there are also these, there's a,
there's another parameter called strides,

250
00:14:58,100 --> 00:15:02,480
which are essentially intervals. So at
what interval do we want to look at? Uh,

251
00:15:02,570 --> 00:15:06,200
the part of the image, we're looking
at every two pixels, every four pixels,

252
00:15:06,230 --> 00:15:10,400
every eight pixels. And depending
on what your stride length is, uh,

253
00:15:10,550 --> 00:15:14,310
it's going to it that we
need to tune that. So,

254
00:15:14,330 --> 00:15:16,520
so that our model is a better or worse.

255
00:15:16,700 --> 00:15:20,870
And we can do that through
trial and error. And what,
what pooling is, is it says,

256
00:15:21,020 --> 00:15:21,980
let's,
um,

257
00:15:22,980 --> 00:15:23,330
okay,

258
00:15:23,330 --> 00:15:27,980
let's take the Max of a region and just
use that as the input. So if we had a, uh,

259
00:15:30,010 --> 00:15:31,720
so if we had a, uh, let's see,

260
00:15:33,880 --> 00:15:36,520
let me show a little image for that
and then we'll keep going with this.

261
00:15:43,390 --> 00:15:45,760
So this is pooling right?
So this is if we could look at our image,

262
00:15:45,790 --> 00:15:50,230
because images are all matrices,
right? Why we use pooling.
So images are matrices.

263
00:15:50,390 --> 00:15:54,190
An image is a matrix of pixels.
So if you think of this image right over,

264
00:15:56,350 --> 00:15:59,960
I want to see if I can, there we go.
So this image over here, just that,

265
00:15:59,980 --> 00:16:04,180
the big image over there and then we,
that that's our image. So what pooling is,

266
00:16:04,320 --> 00:16:04,841
is it saying,

267
00:16:04,841 --> 00:16:08,920
what portion of the image do we want to
use when we take that data and pass it

268
00:16:08,921 --> 00:16:12,550
forward through the network and work with
Max pooling, we're going to say, well,

269
00:16:12,850 --> 00:16:16,360
if we were to split these up into
squares like four different squares,

270
00:16:16,540 --> 00:16:20,470
if we were to add up the value in
each of these squares, which um,

271
00:16:20,920 --> 00:16:25,300
which square, which a big square
would contain the most, uh, values.

272
00:16:25,301 --> 00:16:28,300
And that's the Max values. And
that's the, that's the square,

273
00:16:28,301 --> 00:16:30,160
the set that we're going to use.

274
00:16:30,170 --> 00:16:33,610
And that's a subset we're going to
pass forward in the network. Okay.

275
00:16:33,850 --> 00:16:37,000
And pooling is the most popular.
Taylor of the, of the, um,

276
00:16:37,210 --> 00:16:40,780
Max pooling is the most popular
of the pooling methods. Okay.

277
00:16:40,960 --> 00:16:44,950
So that's what we did for one
of our layers and right. So if,

278
00:16:44,970 --> 00:16:49,720
so following along we established our
weights and biases we are and then are

279
00:16:49,721 --> 00:16:53,160
accomplished no layer. We applied a, um,

280
00:16:56,500 --> 00:16:59,380
we applied an activation
function to it and we used Relu.

281
00:16:59,410 --> 00:17:00,820
And so why do we use Relu?

282
00:17:00,821 --> 00:17:05,020
Because a relu reduces the
likelihood of the vanishing gradient,

283
00:17:05,080 --> 00:17:09,840
which it recall for it,
for neural networks,

284
00:17:11,010 --> 00:17:12,670
a recall for a,

285
00:17:17,350 --> 00:17:19,480
for LFTs and time series data.

286
00:17:19,510 --> 00:17:22,360
The vanishing gradient is a huge
problem and it's also a problem in

287
00:17:22,361 --> 00:17:26,620
convolutional networks. Okay. So

288
00:17:27,080 --> 00:17:27,690
yeah,

289
00:17:27,690 --> 00:17:28,740
that's that layer.
Now,

290
00:17:28,800 --> 00:17:31,500
now we have another layer and then we're
going to get into actually building our

291
00:17:31,501 --> 00:17:33,840
model.
So this is our fully connected layer.

292
00:17:33,870 --> 00:17:36,420
And so what this does is
it performs classification.

293
00:17:36,690 --> 00:17:39,420
And we use this in all sorts of networks.

294
00:17:39,690 --> 00:17:44,690
It's not Esta [inaudible] connected layers
are using almost every type of neural

295
00:17:44,971 --> 00:17:46,110
network.
Um,

296
00:17:46,140 --> 00:17:51,090
and we use them pretty much before we use
them before we squash it into using an

297
00:17:51,091 --> 00:17:53,070
activation function to
output a prediction.

298
00:17:53,370 --> 00:17:57,670
So fully connected layers are usually
found at the end of neural networks, um,

299
00:17:58,590 --> 00:18:01,620
at the very end.
And what they do is they,

300
00:18:01,800 --> 00:18:05,850
and the reason that we use fully
connected layers is so that we can use all

301
00:18:05,851 --> 00:18:09,030
parts of the data because
we're about to because we

302
00:18:11,140 --> 00:18:11,973
mmm.

303
00:18:14,850 --> 00:18:18,150
Because we want to squash in.
Okay. So that's what that is.

304
00:18:18,151 --> 00:18:19,710
And we're going to do the
same thing we're going to use,

305
00:18:19,740 --> 00:18:22,590
we're going to use the histograms to
create summaries for the weights by and

306
00:18:22,680 --> 00:18:27,150
activations. Okay? So that is that.

307
00:18:27,180 --> 00:18:28,980
And so now let's build our model.
Okay.

308
00:18:28,981 --> 00:18:32,580
So we'll establish our graph
and initialize our session.

309
00:18:32,610 --> 00:18:34,240
And now we're going to
have placeholders that,

310
00:18:34,290 --> 00:18:36,150
and these are going to
be the gateways for data.

311
00:18:36,300 --> 00:18:40,920
So imagine we haven't done anything.
I've just, all I've done here is I have,

312
00:18:41,200 --> 00:18:42,033
uh,

313
00:18:42,270 --> 00:18:46,380
talked about what those layers are
going to be and then now we're going to

314
00:18:46,381 --> 00:18:48,780
actually initialize them.
So let's see if we have any questions.

315
00:18:48,781 --> 00:18:51,360
And then we're going to talk
about what this looks like.

316
00:18:52,440 --> 00:18:54,970
How does Max pooling work with an,

317
00:18:55,810 --> 00:19:00,170
an image of only ones and Zeros?
Oh,

318
00:19:00,171 --> 00:19:03,710
that's a good question. So the
ones would then, uh, the, the,

319
00:19:04,040 --> 00:19:07,790
the part of the matrix that has the most
ones would then be the pool that we use.

320
00:19:07,820 --> 00:19:09,020
That's the Max value.

321
00:19:09,770 --> 00:19:12,020
Two more questions and then we're
going to keep going with the model.

322
00:19:16,160 --> 00:19:17,780
How did he get those keyword?
Okay.

323
00:19:25,730 --> 00:19:28,610
If we had infinite resources,
we wouldn't do pooling. Right.

324
00:19:29,850 --> 00:19:30,683
Uh,

325
00:19:31,580 --> 00:19:35,000
that's a good question. So yeah.
Yes. The answer is yes. Yeah.

326
00:19:35,001 --> 00:19:36,230
I've never thought about it that way.
Yeah.

327
00:19:37,040 --> 00:19:40,850
Because all of a lot of machine learning
is a trade off between computational

328
00:19:40,851 --> 00:19:43,790
complexity and, uh, brevity, brevity,

329
00:19:43,791 --> 00:19:46,850
be that in code or brevity
be that in a training time,

330
00:19:46,851 --> 00:19:50,210
like we want to minimize the training
time, minimize the amount of code we have,

331
00:19:50,330 --> 00:19:54,920
minimized the computational complexity
so that we can maximize our, uh, and,

332
00:19:54,950 --> 00:19:57,740
but also maximize our prediction accuracy.

333
00:19:57,741 --> 00:20:00,770
So one more question and then we're
going to get started with this.

334
00:20:02,300 --> 00:20:06,920
Does Max pooling use a sig Monday,
a sigmoid function? Uh, no,

335
00:20:06,921 --> 00:20:09,740
it does not. It does not.
Max Pooling isn't a sig,

336
00:20:09,770 --> 00:20:14,240
isn't that an sigmoid is an activation
function. That's not Max pooling is, um,

337
00:20:15,320 --> 00:20:17,660
is an operation. Okay. So
not to build our model,

338
00:20:18,050 --> 00:20:21,530
we have are placeholders for our images.
So that's going to be our tensors.

339
00:20:21,680 --> 00:20:25,500
And then we're going to use this
summary. Remember the summary, um,

340
00:20:25,820 --> 00:20:30,020
function of tensorflow to create a
summary Protocol buffer for images.

341
00:20:30,021 --> 00:20:31,550
So we can visualize these under the

342
00:20:34,520 --> 00:20:37,100
images tab right here. We're going
to visualize that in the images tab,

343
00:20:37,940 --> 00:20:40,250
and then we're going to define our model.

344
00:20:40,251 --> 00:20:44,420
So depending on how many layers we want,
we have an if else statement.

345
00:20:44,510 --> 00:20:47,200
So if we have two layers,
then we can use that function.

346
00:20:47,201 --> 00:20:49,510
We just defined it to then,
um,

347
00:20:53,300 --> 00:20:57,580
to then call the layer. And then, uh,

348
00:20:58,510 --> 00:21:00,560
so if we have to layer,
then we're going to use that function.

349
00:21:00,561 --> 00:21:02,690
We call it to build a model else.
We'll just use one layer.

350
00:21:02,960 --> 00:21:05,390
And then once we have those
layers, we're going to, at the end,

351
00:21:05,391 --> 00:21:07,820
we're going to flatten it and then we're
going to feed it to a fully connected

352
00:21:07,821 --> 00:21:11,570
layer. And we flatten
it, uh, because it's, uh,

353
00:21:11,600 --> 00:21:15,350
it's more computationally efficient
for our model to read, um,

354
00:21:15,650 --> 00:21:19,760
a one dimensional tenser
than a two dimensional two
dimensional tensor. However,

355
00:21:19,940 --> 00:21:24,940
there are ways of not having to fly
in an image and then using that two

356
00:21:25,041 --> 00:21:27,800
dimensional,
a tensor directly.

357
00:21:28,010 --> 00:21:32,870
But however it's more computationally
efficient, uh, complex, not efficient.

358
00:21:33,020 --> 00:21:37,800
Okay. So those are our fully
connected layers and uh, oh,

359
00:21:37,801 --> 00:21:39,470
sorry.
Those are our convolutional layers.

360
00:21:39,650 --> 00:21:43,040
And then we'll take the flattened image
and feed it to our fully connected layer

361
00:21:45,280 --> 00:21:46,910
and then,
uh,

362
00:21:48,640 --> 00:21:52,780
we will create those embeddings so
that we can visualize them later.

363
00:21:52,900 --> 00:21:56,320
So at the end,
once we have those fully connected layers,

364
00:21:56,470 --> 00:21:58,870
that's where we're taking
our embeddings to visualize.

365
00:21:58,960 --> 00:22:01,780
So that's the part where the
embedding visualization is going to,

366
00:22:02,020 --> 00:22:06,250
is going to happen. So data's flowing
through our convolutional layers. Okay.

367
00:22:06,580 --> 00:22:09,010
Through the Max pooling,
through the activation functions,

368
00:22:09,220 --> 00:22:13,090
it's getting flattened and then fed into
the fully connected layer at the end.

369
00:22:13,360 --> 00:22:17,110
And that the result, the result that
the fully connected layer creates,

370
00:22:17,350 --> 00:22:19,270
that embedding is what
we're going to visualize.

371
00:22:19,271 --> 00:22:23,020
And it's gonna be so dope when we
visualize it. It's so cool. Okay. So, um,

372
00:22:24,210 --> 00:22:24,770
yeah,

373
00:22:24,770 --> 00:22:29,510
and I'll answer questions in five minutes.
So that's what we have for that.

374
00:22:29,750 --> 00:22:33,150
And so,
and then we're going to take the luggage,

375
00:22:33,200 --> 00:22:37,460
which is the probabilities that don't
equal one. It's, it's the output.

376
00:22:37,490 --> 00:22:39,050
It's we have the embedding,

377
00:22:39,051 --> 00:22:42,140
we stored that in the embedding variable
and now the lug nuts are going to be

378
00:22:42,141 --> 00:22:46,250
the output of the fully connected layer.
Hey Alexa.

379
00:22:47,840 --> 00:22:50,510
Hey computer.
Do you like tensorflow?

380
00:22:54,160 --> 00:22:54,993
Okay.

381
00:22:55,080 --> 00:22:58,380
No, that was a little, we had Alexa
here but she didn't work anyway.

382
00:22:58,620 --> 00:23:00,450
I thought it would be fun. So, uh, anyway,

383
00:23:00,451 --> 00:23:03,780
so the next step is for us
to define our loss function.

384
00:23:03,810 --> 00:23:08,400
So we're gonna use a standard cross
entropy loss function with luggage. So we,

385
00:23:08,580 --> 00:23:10,650
the log gets are all those
values that we output,

386
00:23:10,860 --> 00:23:13,710
we squash it and then that's it.
Once we have that,

387
00:23:13,711 --> 00:23:16,560
we can then generate
our prediction from it.

388
00:23:16,710 --> 00:23:20,280
So this output is going to be our set
of prediction values. Okay. From the,

389
00:23:20,310 --> 00:23:24,750
from this name scope, which we call x ENT,
which is short for cross entropy loss.

390
00:23:25,170 --> 00:23:26,160
And the output of this.

391
00:23:26,190 --> 00:23:30,600
So x ENT is going to be our loss and
we were going to visualize this loss

392
00:23:30,640 --> 00:23:31,471
intenser board as well.

393
00:23:31,471 --> 00:23:35,580
So remember member scopes are going to
group operations together so that we can

394
00:23:35,581 --> 00:23:40,230
then visualize them as a
whole intenser board. Okay.

395
00:23:40,260 --> 00:23:42,900
So then, okay,

396
00:23:42,901 --> 00:23:45,320
so now we've defined our
model and now we can it.

397
00:23:45,321 --> 00:23:48,080
So we don't just define
name scopes for our model.

398
00:23:48,081 --> 00:23:52,970
We also want to define it for
our, uh, for our tensors are.

399
00:23:52,971 --> 00:23:57,560
So for our, uh, training operation
and our testing operation.

400
00:23:57,740 --> 00:23:59,600
So for our training
operation we're using Adam.

401
00:23:59,810 --> 00:24:01,820
And so I've written down a
huge explanation here of what,

402
00:24:01,821 --> 00:24:06,050
like why do you use Adam and when
not to, but in a nutshell, Adam is,

403
00:24:06,890 --> 00:24:10,940
is, gives us better results than the
standard gradient descent optimizer,

404
00:24:11,180 --> 00:24:13,880
but it's more computationally complex.

405
00:24:14,000 --> 00:24:17,750
So if you're willing to make that trade
off, then I would go for Adam. Uh,

406
00:24:17,751 --> 00:24:21,170
and most of the papers these days
that are using a convolutional nets,

407
00:24:21,350 --> 00:24:24,920
I tend to see Adam used more often than
the standard grading dissent optimizer.

408
00:24:25,100 --> 00:24:28,820
But if you're a beginner, then I would
go for great dissent optimizer. Okay.

409
00:24:30,190 --> 00:24:33,970
Okay. So, and let me try Alexa one more
time. Do you think I should? Hey Alexa or,

410
00:24:34,120 --> 00:24:37,060
hey, hey computer. What Day is it today?

411
00:24:39,260 --> 00:24:40,600
What Day is it today?

412
00:24:42,220 --> 00:24:46,990
It's Wednesday, April 5th. It doesn't
like me. Okay, boys, the boys.

413
00:24:46,991 --> 00:24:51,850
Okay. So anyway, that works.
Anyway. Do we have our training?

414
00:24:51,851 --> 00:24:53,950
We have our accuracy. Ho. So
hold on guys. We're going to,

415
00:24:54,100 --> 00:24:56,650
we're about to visualize this
intense or board. I know, you know,

416
00:24:56,651 --> 00:24:59,440
I've been waiting for that to let's,
let's just get through this code. Okay.

417
00:24:59,740 --> 00:25:04,540
So we have 380 people here from all over
the world. Okay. We are growing so fast.

418
00:25:04,541 --> 00:25:08,440
It's amazing. It is amazing. So I am
so honored to be here with you guys.

419
00:25:08,650 --> 00:25:09,700
So where were we?

420
00:25:10,390 --> 00:25:14,920
We have defined our training function and
now we're going to define the, uh, the,

421
00:25:14,921 --> 00:25:18,280
the last name, scope. This is the last
name, scope that we're defining here,

422
00:25:18,790 --> 00:25:21,460
which is the accuracy. So we're
going to take the log, it's,

423
00:25:21,850 --> 00:25:24,100
and then we're going to, which is
a collection of the probabilities.

424
00:25:24,340 --> 00:25:28,750
And we're going to choose the, the, the,
the, the average, which using the org,

425
00:25:28,810 --> 00:25:32,830
or sorry, not the APP, not sorry.
We're going to use Arg Max,

426
00:25:32,860 --> 00:25:35,110
which is going to get the largest value.
Uh,

427
00:25:35,111 --> 00:25:39,160
and then we're going to get the average
using the reduced mean function and

428
00:25:39,161 --> 00:25:42,610
accuracy is going to be one scalar value.
That is our prediction right there.

429
00:25:42,611 --> 00:25:47,300
The accuracy. Okay. My desktop is pretty
crowded. It's a lot of editing. But uh,

430
00:25:48,070 --> 00:25:48,910
anyway,
where was I?

431
00:25:49,740 --> 00:25:50,160
Okay.

432
00:25:50,160 --> 00:25:51,480
Make this bigger.
Okay.

433
00:25:51,481 --> 00:25:56,190
So now is it's time to look at the
novel stuff about tensor board here.

434
00:25:56,191 --> 00:25:59,160
So in a lot of the initial
tensor board tutorials,

435
00:25:59,161 --> 00:26:02,400
it didn't have this because the
embedding visualizer didn't exist,

436
00:26:02,520 --> 00:26:04,110
but now it does.
So

437
00:26:06,070 --> 00:26:10,450
what we're going to do is we're
going to, we, the reason we, we, uh,

438
00:26:10,451 --> 00:26:12,270
summarize the all of the summer,

439
00:26:12,490 --> 00:26:15,700
the reason we merged the summaries before
we write to disk is because it's more

440
00:26:15,701 --> 00:26:19,270
computationally efficient. Uh, instead
of continuously writing them to disk,

441
00:26:19,330 --> 00:26:24,100
we just merged them all at switch one,
right? Versus like 10. Okay. Okay. So,

442
00:26:26,800 --> 00:26:28,690
uh, where was I? So we did that.

443
00:26:28,691 --> 00:26:31,120
And now we're going to initialize
our embedding matrix as a,

444
00:26:31,121 --> 00:26:32,290
as an array of Zeros.

445
00:26:32,560 --> 00:26:35,230
And then we're going to assign it the
embedding that we just calculate it.

446
00:26:35,231 --> 00:26:36,970
All right? So now it's
in our assignment. Okay.

447
00:26:36,971 --> 00:26:40,570
So now it's in our assignment and now
we're going to initialize a saver.

448
00:26:40,930 --> 00:26:45,270
And the saver is used to save
and restore all of our variables.

449
00:26:45,750 --> 00:26:46,583
Uh,

450
00:26:47,100 --> 00:26:51,470
and so then we're going to use
initialize the file writers.

451
00:26:51,471 --> 00:26:55,440
So we have our summaries and then we
have the file rider. So the summary, uh,

452
00:26:55,500 --> 00:26:58,950
operation creates the protocol buffers
so that we can write it to disk.

453
00:26:59,160 --> 00:27:03,210
And the file writer is
that, um, it is that

454
00:27:05,380 --> 00:27:08,200
method that we are able
to actually write to disk.

455
00:27:08,290 --> 00:27:11,470
So we create summaries and then we
send them to this using the file rider.

456
00:27:11,471 --> 00:27:14,230
So they both, they both, they
both go hand in hand. Okay.

457
00:27:14,560 --> 00:27:16,840
So that's why we use the fall rider.

458
00:27:18,010 --> 00:27:20,950
And so this part needs to be better.

459
00:27:21,130 --> 00:27:24,210
Let me just say that this part needs
to be better. So in the initial, uh,

460
00:27:24,250 --> 00:27:25,570
in tensorflow examples.
So there are,

461
00:27:25,630 --> 00:27:30,040
there actually are not a lot of
examples of tents or board that's use,

462
00:27:30,100 --> 00:27:33,970
um, that use this, um,

463
00:27:34,630 --> 00:27:38,410
there that you can look at the
embedding visualizer in there.

464
00:27:38,411 --> 00:27:42,400
Not a lot of examples of tens of board
that you can use the embedding visualizer

465
00:27:42,430 --> 00:27:44,380
in. So there should be,
so this is one of them.

466
00:27:44,381 --> 00:27:47,020
So definitely check out the get hub repo
because there are not a lot of them.

467
00:27:47,410 --> 00:27:51,070
The problem with this is that it's
using the config file directly from

468
00:27:51,080 --> 00:27:51,913
tensorflow.

469
00:27:51,940 --> 00:27:56,350
Now I would like it so that we can
define our own configuration file.

470
00:27:56,500 --> 00:27:58,960
But if we look at this profile,
let's look at what this looks like.

471
00:27:59,170 --> 00:28:03,010
It's pulling it from the web.
And so let's look at what this looks like.

472
00:28:05,310 --> 00:28:09,690
Four oh four. Here man. We've got to,
we've got to clean this, this up. No,

473
00:28:09,691 --> 00:28:13,920
it's not this, there it is. So here it is.

474
00:28:14,070 --> 00:28:16,740
So this is, these are the, um,

475
00:28:17,640 --> 00:28:22,470
these are the options that we're using
to build our embedding visualizer. Uh,

476
00:28:22,471 --> 00:28:24,810
so we have a tensor name,
we have metadata,

477
00:28:24,811 --> 00:28:27,270
and then we have our
project configuration. Okay.

478
00:28:27,450 --> 00:28:30,870
So we can also define these locally if
we want it to you by creating a pro,

479
00:28:31,170 --> 00:28:34,590
a pro buff file, um, uh, locally,

480
00:28:34,591 --> 00:28:36,570
and then calling it from disk.

481
00:28:36,750 --> 00:28:38,850
But right now it's calling it
from the web right here. Okay.

482
00:28:39,150 --> 00:28:43,530
So we could add multiple embeddings,
but we're just going to add one.

483
00:28:43,531 --> 00:28:47,730
And that's what we've learned from
our embedding matrix. Okay. So

484
00:28:49,550 --> 00:28:53,240
we have that and we're going to specify
the width and the height of it with a

485
00:28:53,241 --> 00:28:57,320
single thumbnail. Okay. And
so they're 28 by 28 pixels.

486
00:28:58,100 --> 00:28:59,480
Okay.
So that's it for that.

487
00:28:59,481 --> 00:29:03,170
And then we train it and then we save
the checkpoint every 500 iterations.

488
00:29:03,171 --> 00:29:07,020
So this thing is going to run for
2000 iterations with, with an,

489
00:29:07,030 --> 00:29:09,890
each batch is going to be a
hundred. Uh, and then, yeah.

490
00:29:10,040 --> 00:29:11,840
And so that's what the
training step looks like.

491
00:29:11,841 --> 00:29:15,440
And then we have our main function
where we call it. And then this,

492
00:29:16,040 --> 00:29:20,360
this make h parameter string function just
converts a hyper parameter string, uh,

493
00:29:20,390 --> 00:29:24,920
to, uh, one that is more detailed for us.

494
00:29:24,950 --> 00:29:26,960
It's just for us. Okay.
So that's the code.

495
00:29:26,961 --> 00:29:29,960
I want it to kind of blow through it a
little fast because I wanted to get the

496
00:29:29,961 --> 00:29:32,390
tents are board.
So before we get the tensor board,

497
00:29:32,391 --> 00:29:34,970
let me ask if there are any questions
before we get started with this.

498
00:29:37,540 --> 00:29:42,310
Okay. I'll take two questions
and this was before I started.

499
00:29:42,310 --> 00:29:44,580
Oh my God.
You guys know how it is with demos.

500
00:29:44,590 --> 00:29:48,890
Like literally it works perfectly.
It works.

501
00:29:48,920 --> 00:29:49,720
You could rewrite,

502
00:29:49,720 --> 00:29:53,920
I literally stopped it and rerun it like
three times before starting it. Boom.

503
00:29:53,921 --> 00:29:54,880
Tents board every,
you know,

504
00:29:55,000 --> 00:29:59,920
that little command line with tensor
born and it was running. And then five,

505
00:29:59,950 --> 00:30:04,600
literally almost like the gods of
deep learning. Where of of Demo.

506
00:30:05,140 --> 00:30:05,591
Hell,

507
00:30:05,591 --> 00:30:10,591
where there this I should say say they
didn't make it go or it didn't work.

508
00:30:10,870 --> 00:30:12,130
So we're going to figure it out.

509
00:30:12,131 --> 00:30:14,890
We're going to debug it in real time and
let's see if we can make tens of board

510
00:30:14,891 --> 00:30:18,130
work in real time. Okay. So
we'll see. So two questions.

511
00:30:18,250 --> 00:30:19,750
How do you interpret the histograms?

512
00:30:19,751 --> 00:30:22,210
I have trouble giving
meaning to the acce axes.

513
00:30:22,480 --> 00:30:24,640
That's a great question and I'm
going to talk about that when we,

514
00:30:24,700 --> 00:30:25,533
when we look at it,

515
00:30:25,720 --> 00:30:29,320
are there any third party wrappers for
tensor flow so as to convert symbolic

516
00:30:29,321 --> 00:30:34,321
programming to kind of simulation for
end user in object oriented programming?

517
00:30:37,010 --> 00:30:39,290
That is a big question
with a lot of parts to it.

518
00:30:39,710 --> 00:30:43,160
Third Party wrappers for tensorflow,
yes, there are tenser layer,

519
00:30:43,310 --> 00:30:46,280
look at tensor layer,
Google that tensor layer.

520
00:30:46,910 --> 00:30:51,500
Are you using GPU version
of tensorflow? Uh, yes I am.

521
00:30:51,740 --> 00:30:55,450
What is printable file do? So protocol
buffers, Google invented this, um,

522
00:30:57,390 --> 00:31:00,540
like a couple of years ago,
like six, seven years ago.

523
00:31:00,870 --> 00:31:03,840
But basically protocol buffers
are a serialization methods.

524
00:31:03,841 --> 00:31:07,110
So serializing as a way like
pickling is a form of serialization.

525
00:31:07,111 --> 00:31:11,250
It's a way of taking some data and
converting it to some standard format,

526
00:31:11,251 --> 00:31:13,690
like some generalized
standard format so that you,

527
00:31:13,760 --> 00:31:18,000
so that is so you can save it to disk
and then recreate that data in a,

528
00:31:18,001 --> 00:31:20,220
in a later form.
And another form later.

529
00:31:20,370 --> 00:31:23,700
One more question and then
we'll get started with this.

530
00:31:24,180 --> 00:31:27,720
How would you detect fake
news using ML and DL? So guys,

531
00:31:28,980 --> 00:31:33,960
we are at a point right now where the
tools available to us to generates data

532
00:31:34,050 --> 00:31:38,550
are getting better and better. So fake
news will get more and more realistic.

533
00:31:38,580 --> 00:31:40,740
But at the same time,
the classifiers,

534
00:31:40,741 --> 00:31:43,950
we have to detect what's fake and
what's real will get better as well.

535
00:31:44,370 --> 00:31:49,130
So it's crazy if you think about it. Uh,
viruses we'll get, we'll learn to fight.

536
00:31:49,150 --> 00:31:52,260
Well, we'll get better at learning
to find vulnerabilities in systems,

537
00:31:52,410 --> 00:31:53,520
but at the same time,

538
00:31:53,760 --> 00:31:58,680
virus detection algorithms will learn to
get better at detecting what is trying

539
00:31:58,681 --> 00:32:01,170
to attack it system.
So it's this,

540
00:32:01,171 --> 00:32:04,410
it's this battle is this constant battle
with both sides getting stronger and

541
00:32:04,411 --> 00:32:06,900
stronger. And machine learning
is at the forefront of it.

542
00:32:07,140 --> 00:32:11,640
And the way we make sure the good
side winds is by spreading AI.

543
00:32:11,641 --> 00:32:15,900
So making sure everybody has access to
it because if only a few have access to

544
00:32:15,901 --> 00:32:20,520
it, then bad things can happen. So
learn Ai, tell your friends about it.

545
00:32:20,521 --> 00:32:23,070
Spread AI awareness,
tell everybody about it.

546
00:32:24,210 --> 00:32:29,160
We got to get this power distributed
to everybody to prevent bad things from

547
00:32:29,161 --> 00:32:29,521
happening.

548
00:32:29,521 --> 00:32:33,870
So we're moving into a very beautiful
world and we have to make sure that

549
00:32:33,871 --> 00:32:37,140
everybody has access to this power.
Anyway, so that's it for my rant on that.

550
00:32:37,200 --> 00:32:39,800
Now what we're going to do is we're going
to visualize what we've just written

551
00:32:40,070 --> 00:32:41,780
and we're gonna make sure that it works.

552
00:32:42,020 --> 00:32:46,610
So what happened here is to run this.

553
00:32:46,670 --> 00:32:50,420
So let's see, okay, we want to run this
intense or board. What have we done here?

554
00:32:50,421 --> 00:32:54,360
We saved it to this log directory
right here. So it's in TM,

555
00:32:54,370 --> 00:32:59,120
TM m and I s t tutorial,
and that's where we saved it.

556
00:32:59,390 --> 00:33:02,990
Okay?
That's where our file writers saved it.

557
00:33:03,320 --> 00:33:05,180
So now what we want to do is we want to,

558
00:33:09,570 --> 00:33:13,650
we want to, um, train our model, right?

559
00:33:13,680 --> 00:33:17,040
So let's train this thing.
So we'll say like a bigger

560
00:33:18,860 --> 00:33:23,000
python, uh, where am I
am and I see seed a pie.

561
00:33:24,110 --> 00:33:25,670
And that's going to train the model.

562
00:33:27,210 --> 00:33:31,170
Hopefully it will train it.
So good.

563
00:33:31,650 --> 00:33:36,150
So good. So good. Okay, good. So great.
So now it's training the model. Okay.

564
00:33:36,151 --> 00:33:38,610
So while it's training,
and I encourage you guys to train it,

565
00:33:38,640 --> 00:33:43,640
it's going to take about five
to seven minutes on a CPU and

566
00:33:48,150 --> 00:33:48,331
Oh,

567
00:33:48,331 --> 00:33:51,180
it's going to take about five to seven
minutes on a standard CPU on your laptop.

568
00:33:51,181 --> 00:33:53,870
So don't even worry about
not training it locally.

569
00:33:53,871 --> 00:33:55,970
You could totally train locally.
So while it training,

570
00:33:55,971 --> 00:33:59,210
we're going to run tensor board.
So to run tensor board,

571
00:33:59,240 --> 00:34:02,090
now all of you who have tensorflow,
we'll have tensor board.

572
00:34:02,120 --> 00:34:05,780
There's nothing extra you have to um,
do.

573
00:34:05,781 --> 00:34:07,790
So let's initialize sensor board.

574
00:34:08,150 --> 00:34:10,970
I can do a 10 I can do
a Qa livestream someday,

575
00:34:11,180 --> 00:34:14,870
but right now we're going to just run
tensor boards. So to run tensor board,

576
00:34:14,880 --> 00:34:18,650
we run tensor board and then it's,
what was it?

577
00:34:18,651 --> 00:34:23,651
It's log directory equals and then
the path that we saved the logs too.

578
00:34:25,640 --> 00:34:29,810
So what happened is it stopped
working five minutes before the demo.

579
00:34:29,990 --> 00:34:33,440
So I'm going to paste it
and let's see what happened.

580
00:34:33,441 --> 00:34:34,430
We're going to debug this together.

581
00:34:34,430 --> 00:34:38,420
It's definitely going to throw an error
for some reason, which is so annoying,

582
00:34:38,421 --> 00:34:42,740
but it's, it is what it is. Come
on baby. Let's do this. Okay,

583
00:34:42,741 --> 00:34:46,070
so this is what I'm talking about.
So normally this would show the,

584
00:34:46,100 --> 00:34:50,120
the URL that we could then visit in
our browser to visualize our tensorflow

585
00:34:50,210 --> 00:34:53,030
computation graph,
but instead it's showing this error,

586
00:34:53,210 --> 00:34:56,660
tensorflow starting tender
board be 41 on port six oh six.

587
00:34:56,840 --> 00:34:58,610
So I don't know what that is.

588
00:34:58,611 --> 00:35:02,360
And so I was googling this one
minute before the stream started,

589
00:35:03,380 --> 00:35:07,490
but I found that someone else had this
issue, tends to board isn't showing,

590
00:35:07,790 --> 00:35:12,140
and this was a not a recent issue, this
was a year ago, but he had the same issue.

591
00:35:12,170 --> 00:35:16,790
And then Damn Manet, who I
interviewed, who is also the guy who's,

592
00:35:16,850 --> 00:35:18,210
you know,
in charge of this,

593
00:35:18,240 --> 00:35:21,680
this stuff on the tensorflow team said
something and I'm like, Blah Blah, blah,

594
00:35:21,681 --> 00:35:24,800
blah, blah. I mean this, this thing
goes on forever, but what's the result?

595
00:35:24,890 --> 00:35:28,300
Is this what I do? I can just
kind of like zoom through the, uh,

596
00:35:32,550 --> 00:35:34,210
this, the stuff. And so,

597
00:35:37,670 --> 00:35:41,170
okay, so he had debug. So he,

598
00:35:41,310 --> 00:35:44,850
so maybe adding a debug
flag at the end would help.

599
00:35:45,000 --> 00:35:48,150
So let's see what happens here. And again,
guys, if it doesn't work, we're going to,

600
00:35:48,180 --> 00:35:53,160
we're going to visualize it somehow.
What? We'll figure it out. Okay.

601
00:35:53,161 --> 00:35:54,690
So debug,
let's see.

602
00:35:55,930 --> 00:35:56,763
Oh No,
fuck

603
00:36:01,170 --> 00:36:02,003
right there.

604
00:36:06,380 --> 00:36:10,870
Okay.
You need this right now?

605
00:36:12,190 --> 00:36:15,760
Um, no, it's okay. Okay.

606
00:36:15,761 --> 00:36:20,761
So then we could try it again with the
debug flag as the get hub issue says.

607
00:36:21,820 --> 00:36:24,940
And um, okay. So yeah,
Ben has a good idea.

608
00:36:24,941 --> 00:36:26,860
Let's just open up local
host and see what happens.

609
00:36:30,400 --> 00:36:31,390
Local host

610
00:36:37,660 --> 00:36:42,120
66, what was it, six or six? No,
it was, um, six, six, six oh six.

611
00:36:46,080 --> 00:36:47,360
How many paws training here?

612
00:36:47,361 --> 00:36:51,590
Because it's really taking up compute
and I don't need that right now. Okay. So

613
00:36:57,350 --> 00:36:58,183
yeah.

614
00:36:59,470 --> 00:37:01,660
So something here is loading up.
Um,

615
00:37:07,860 --> 00:37:10,980
okay. Yes. Okay. Yes.

616
00:37:10,981 --> 00:37:13,860
I'm so happy that this is working.
I guys,

617
00:37:13,861 --> 00:37:17,790
I'm so happy that this is working
right now. This is the greatest thing.

618
00:37:17,820 --> 00:37:20,970
Who is this? God. Ben.
Benjamin Schulz. Larsen.

619
00:37:22,290 --> 00:37:25,710
Shout out to you. Wizard of the week.
Wizard of the live session. Okay.

620
00:37:26,100 --> 00:37:30,600
So here's what's up.
Let's visualize this.

621
00:37:30,960 --> 00:37:33,270
Okay. So, um,

622
00:37:33,370 --> 00:37:37,800
where were we for our scalers? So
let's look at the, you know what,

623
00:37:37,801 --> 00:37:39,180
let's just go straight into the embedding.

624
00:37:39,200 --> 00:37:42,180
Cause that's what I'm most
excited to look at. Look at this.

625
00:37:42,810 --> 00:37:47,340
You guys who have stuck around how
get to see this amazing, amazing,

626
00:37:47,700 --> 00:37:52,260
amazing visualization.
So let's talk about what this is.

627
00:37:52,800 --> 00:37:56,250
So what this is is it is our,

628
00:37:56,370 --> 00:37:58,380
these are our embeddings
that we generated.

629
00:37:58,381 --> 00:38:01,050
So remember in our fully connected layer,
we,

630
00:38:01,500 --> 00:38:05,460
we fed those into our
embedding visualizer. Okay. And

631
00:38:09,670 --> 00:38:13,000
the way that we are visualizing
them is with those sprites.

632
00:38:13,060 --> 00:38:16,690
So remember the sprites are, these are
these files and it's like a matrix.

633
00:38:16,691 --> 00:38:20,770
We're cutting it. It's like a matrix.
All, I dunno how many are there.

634
00:38:20,771 --> 00:38:23,830
There's like 28 by 28
or something like that.

635
00:38:24,040 --> 00:38:28,750
But these sprites represent each of these
embeddings that we've learned. Okay?

636
00:38:28,751 --> 00:38:31,440
So what, what we're looking at, right,

637
00:38:31,470 --> 00:38:36,470
right now is a visualization
of this craft and using PCA,

638
00:38:36,610 --> 00:38:41,020
principal component analysis as
a technique to map them out. So,

639
00:38:43,480 --> 00:38:46,450
so let's talk about what
this, what this is. Okay.

640
00:38:46,451 --> 00:38:50,860
So PCA versus Tsne is a, is a,
is a good question. So when,

641
00:38:50,861 --> 00:38:55,660
what do we use PCA, and when
would we use p a tsne. So,

642
00:38:55,750 --> 00:38:56,583
um,

643
00:38:58,990 --> 00:39:02,200
actually I had my notes on that.
Um,

644
00:39:04,360 --> 00:39:06,070
so I forgot about that.
Thanks.

645
00:39:07,530 --> 00:39:10,080
So Pca is a technique that we use to

646
00:39:11,700 --> 00:39:16,320
visualize data and it's actually most
of the time you'd want to use PCA over

647
00:39:16,321 --> 00:39:18,390
tsne most of the time.
Okay?

648
00:39:20,640 --> 00:39:21,473
Okay.

649
00:39:21,720 --> 00:39:25,950
Most of the time we'd want to
use a PCA over Tsne, but, um,

650
00:39:26,100 --> 00:39:29,400
in this case, but, and let's look
at what a tsne looks like. Boom.

651
00:39:29,401 --> 00:39:34,240
Let's find the nearest neighbors
using tsne and watch it move. Let me,

652
00:39:34,241 --> 00:39:37,720
let me make this big. Nope.
Wrong way the hell. Okay.

653
00:39:45,150 --> 00:39:49,500
Okay. So you see that they're clustering
here. These values are clustering in, in,

654
00:39:49,501 --> 00:39:52,890
uh, in, uh, in, uh, where they're
supposed to be. So all the six is,

655
00:39:52,891 --> 00:39:57,000
and all the threes and all the fours,
they're all clustering together. And,

656
00:39:58,850 --> 00:40:01,660
uh,
what we want to do is we want to,

657
00:40:02,970 --> 00:40:03,720
uh,

658
00:40:03,720 --> 00:40:05,340
compute the distances between them.

659
00:40:05,341 --> 00:40:06,960
So there's a lot of things
that we can do here.

660
00:40:06,961 --> 00:40:10,200
Once we have them visualize and we're
going to talk about what all those things

661
00:40:10,201 --> 00:40:13,530
are that we can do once they're
visualized. Okay. So before,

662
00:40:13,560 --> 00:40:16,740
so there's a lot we can do here. Uh, and

663
00:40:18,000 --> 00:40:18,180
okay.

664
00:40:18,180 --> 00:40:21,660
The idea of SME and Tsae is to
place neighbors close to each other.

665
00:40:21,750 --> 00:40:23,430
So that's what this is doing.
And,

666
00:40:23,431 --> 00:40:27,480
but the thing is that it almost
completely ignores the global structure,

667
00:40:27,780 --> 00:40:32,010
but PCA is the opposite. It tries to
preserve the global properties and the,

668
00:40:32,100 --> 00:40:36,450
those are the eigenvectors with high
variance. While it may lose, uh,

669
00:40:36,660 --> 00:40:37,493
um,

670
00:40:39,570 --> 00:40:44,340
it could lose the low variance
deviations between the neighbors.

671
00:40:44,490 --> 00:40:48,660
So it's a trade off and there's actually
a great a stack overflow link for that

672
00:40:48,840 --> 00:40:51,390
in the, uh, get hub like
right at the very top,

673
00:40:52,240 --> 00:40:56,370
the sack exchange showing like five
reasons you do want to use PCA over tsne.

674
00:40:56,780 --> 00:40:59,010
Um, does, that's what that is.

675
00:40:59,011 --> 00:41:02,880
So what we can do here is we don't
even just have to visualize the, uh,

676
00:41:07,400 --> 00:41:07,990
okay.

677
00:41:07,990 --> 00:41:11,110
We don't have to just visualize the,
um,

678
00:41:13,720 --> 00:41:16,300
okay.
So there's a lot of comments here about

679
00:41:17,170 --> 00:41:19,180
my hold on

680
00:41:19,840 --> 00:41:24,700
my name and stuff. How many people we
have here live? We're at 440 we have four.

681
00:41:24,701 --> 00:41:29,290
So I'm here to say this to 440 people
live. Okay. My name is Sarah [inaudible].

682
00:41:29,710 --> 00:41:33,130
I am Indian. My parents are from
India. Let me just say this. Okay.

683
00:41:33,650 --> 00:41:35,180
When I was 18 years old,

684
00:41:35,480 --> 00:41:40,480
I legally changed my name to Jason Scott
rebel when I was 18 years old because I

685
00:41:42,741 --> 00:41:46,580
wanted to do great things and I felt
like the only way to do that was to be

686
00:41:46,610 --> 00:41:48,440
white or at least anglicised.

687
00:41:48,710 --> 00:41:53,600
It was only three years later that I
legally changed my name back to Saroj

688
00:41:53,720 --> 00:41:57,140
revolve.
I learned to love myself over time,

689
00:41:57,260 --> 00:42:02,260
I learned to love my identity and now I
am unabashedly Saroj revolve an Indian

690
00:42:03,230 --> 00:42:07,250
and an American.
So I just wanted to say that publicly.

691
00:42:07,400 --> 00:42:10,460
It's embarrassing to admit, but
it, it, it gives vulnerability,

692
00:42:10,580 --> 00:42:13,190
but it's also a point of,
it just needs to be said.

693
00:42:13,250 --> 00:42:16,670
It needs to be said to the
world. I am Saroj. Yes,

694
00:42:16,671 --> 00:42:19,850
my parents are from India and
I love being who I am. Okay.

695
00:42:19,910 --> 00:42:24,090
So I am very Indian and a,
I love it. Love Indian food,

696
00:42:24,100 --> 00:42:25,800
love all the Indian culture.
Love India.

697
00:42:25,801 --> 00:42:30,410
I visited winter for six months and uh,
yeah, I just wanted to say that life.

698
00:42:31,040 --> 00:42:36,010
So where were we?
Uh,

699
00:42:36,040 --> 00:42:40,060
yeah, a lot of racism growing up in
Texas. But uh, anyway, back to this,

700
00:42:40,420 --> 00:42:41,890
back to PCA and TSN.
Yes.

701
00:42:42,330 --> 00:42:44,370
Where were we?
Um,

702
00:42:47,950 --> 00:42:51,850
where were we? So, uh, what
are we doing here? Dimension.

703
00:42:51,851 --> 00:42:54,760
So we have two d versus three
d. So we want to cheat. There's,

704
00:42:54,790 --> 00:42:59,290
there's several things that we can do
here with our graph and there's a lot of

705
00:42:59,291 --> 00:43:02,290
things happening at once. Let me
focus here. Okay. Where were we?

706
00:43:02,600 --> 00:43:03,433
Yes.

707
00:43:09,860 --> 00:43:10,693
MMM.

708
00:43:18,170 --> 00:43:20,130
Hi. Thanks guys. I appreciate
the support. Anyway,

709
00:43:20,430 --> 00:43:23,190
I guess I did want to see the comments
and see what people thought of, you know,

710
00:43:23,191 --> 00:43:26,730
it's, it's one of those things that I
haven't really admitted, um, before too.

711
00:43:26,731 --> 00:43:30,030
A lot of people. Um, but you guys
are my, you guys are my homies.

712
00:43:30,031 --> 00:43:31,530
You guys in my crew and uh,

713
00:43:31,680 --> 00:43:34,030
I'm just going to continue being real
with you guys and you know, I'm good.

714
00:43:34,070 --> 00:43:37,890
I'm going to continue giving you all
of me, all of Saroj, every part of me,

715
00:43:37,950 --> 00:43:39,810
every part that I've
always been afraid to show,

716
00:43:39,930 --> 00:43:42,510
whether it be rapping about
Tsne or whatever else it is.

717
00:43:42,930 --> 00:43:46,530
You have a very unique part about you
and you probably don't even realize this.

718
00:43:46,710 --> 00:43:51,710
You have a very special skill set and if
you find a way to encompass all of the

719
00:43:51,931 --> 00:43:56,010
things you can do into
one, um, go one activity,

720
00:43:56,100 --> 00:43:58,260
you will find success.
And that's what I'm here for.

721
00:43:58,261 --> 00:44:01,140
I'm here to help you be successful.
That is my goal.

722
00:44:01,350 --> 00:44:02,640
That is a reason that I do this.

723
00:44:02,760 --> 00:44:05,520
I want to help you become awesome
because if you're awesome,

724
00:44:05,670 --> 00:44:07,440
then our society will be awesome.

725
00:44:07,980 --> 00:44:10,860
This is power unlike anything
we've ever seen before.

726
00:44:10,861 --> 00:44:14,370
If you're able to understand this
stuff, you're going to be amazing. Okay,

727
00:44:14,460 --> 00:44:16,410
you're going to do great things.
So where were we?

728
00:44:20,190 --> 00:44:23,560
So Rod, you deserve a plate of Biryani.
That's my favorite comment of the day.

729
00:44:23,800 --> 00:44:27,370
Where were you? I would love
some. Okay. So, um, man,

730
00:44:27,371 --> 00:44:31,000
I'm just saying all sorts of things
right now. We were somewhere,

731
00:44:31,620 --> 00:44:33,420
we were somewhere else right now.
We were,

732
00:44:33,500 --> 00:44:34,333
uh,

733
00:44:36,050 --> 00:44:41,000
we were talking about DSME.
So we have our tensors and we have our,

734
00:44:43,430 --> 00:44:44,263
mmm.

735
00:44:45,070 --> 00:44:47,440
The color map and our labels
and our dimensions. Oh,

736
00:44:47,441 --> 00:44:50,620
you can change the colors too. So check
this out. You can change the colors too.

737
00:44:50,621 --> 00:44:55,390
So we can change the color by label. We
can us. So when we sphere eyes, the data,

738
00:44:55,720 --> 00:44:59,470
it, it, it puts it into it
more ball like structure.

739
00:44:59,740 --> 00:45:03,340
And we can also search for a
different things that we want.

740
00:45:03,430 --> 00:45:06,100
So if I can say seven,
it's going to show all the sevens.

741
00:45:06,160 --> 00:45:08,200
So this can also be used for word vectors.

742
00:45:08,201 --> 00:45:11,620
So remember in one of our videos we talked
about the difference between man and

743
00:45:11,621 --> 00:45:14,240
woman, like man plus
woman equals, well, no,

744
00:45:14,640 --> 00:45:17,170
a man plus woman equals
child or something like that.

745
00:45:17,171 --> 00:45:19,510
But Keon Queen that also works in here.

746
00:45:19,511 --> 00:45:23,440
And I would love to show you guys that
later, but a natural language processing,

747
00:45:23,800 --> 00:45:24,850
um,
later on.

748
00:45:25,730 --> 00:45:29,960
Um, uh, so

749
00:45:33,140 --> 00:45:37,190
yes. Um, so six,

750
00:45:37,550 --> 00:45:42,500
five, four, three, two, one that
was a count down to nothing.

751
00:45:42,501 --> 00:45:45,770
I was just typing this out. How cool
is this though? You could, you could,

752
00:45:46,130 --> 00:45:49,040
you could visualize it and you don't
have to just visualize your data.

753
00:45:49,041 --> 00:45:52,430
You can also visualize the weights.
You can visualize your biases,

754
00:45:52,610 --> 00:45:54,710
you can visualize a lot of
different things. And in fact,

755
00:45:54,830 --> 00:45:58,670
you can visualize stock prices.
What I would like to see, and it,

756
00:45:58,830 --> 00:46:03,420
this depends on the tensorflow
team releasing plugins
for tents or board is, um,

757
00:46:05,760 --> 00:46:09,030
is, uh, I would like to
see more plugins for this.

758
00:46:09,031 --> 00:46:12,270
So I'd like to see more people make
things for this so we can choose different

759
00:46:12,271 --> 00:46:17,040
components. Um, what else
can we do? We can, dude, oh,

760
00:46:17,041 --> 00:46:18,960
night view. That's, that's,
that's my stuff right there.

761
00:46:19,140 --> 00:46:21,510
And we can also select parts of it.
So look at this.

762
00:46:21,511 --> 00:46:25,290
So if we select this bounding
box selection and we select
some specific part of

763
00:46:25,291 --> 00:46:30,000
it, will we can then get those parts.
And then I wanna I want to show the, uh,

764
00:46:30,180 --> 00:46:32,910
the coastline distance.
So let me show how to do that.

765
00:46:32,911 --> 00:46:36,270
So if we were to isolate the points
that we've just bounded in a box,

766
00:46:36,510 --> 00:46:39,360
we could then find the coastline
similarity between those points,

767
00:46:39,660 --> 00:46:41,310
which is so by the way,

768
00:46:41,311 --> 00:46:44,730
bookmarks are to share this with other
people so you can download it and share

769
00:46:44,731 --> 00:46:46,560
it with other people.
Um,

770
00:46:51,190 --> 00:46:51,810
okay,

771
00:46:51,810 --> 00:46:53,100
now I want to,

772
00:46:57,000 --> 00:46:57,833
yeah,

773
00:46:59,480 --> 00:47:00,870
yeah, sure. So thanks.

774
00:47:02,220 --> 00:47:05,350
And we can do custom
visualizations as well. Um,

775
00:47:05,430 --> 00:47:09,080
we can say like from three to

776
00:47:11,250 --> 00:47:14,970
take these labels and then
match them using some vector

777
00:47:16,800 --> 00:47:20,130
use case. Um, a random vector. Anyway,

778
00:47:20,160 --> 00:47:22,530
let's talk about the other parts as
well there because it's not just this,

779
00:47:22,540 --> 00:47:26,460
there's, there's more to it than this.
We have, uh, nine minutes to go, right?

780
00:47:26,570 --> 00:47:29,620
Is that or, okay. We have minutes ago.

781
00:47:29,650 --> 00:47:31,660
Those last four minutes are
going to be for questions.

782
00:47:31,810 --> 00:47:35,350
Let's talk about these other parts. So
the scalers tab is for scalar summaries.

783
00:47:35,351 --> 00:47:38,620
These, those are single number
values that change over time. Okay.

784
00:47:38,621 --> 00:47:41,410
And what we have here are the
accuracy and the cost function,

785
00:47:41,411 --> 00:47:42,670
which is [inaudible].

786
00:47:43,060 --> 00:47:47,800
The x axis shows the time steps and
the y axis shows the accuracy or loss.

787
00:47:48,040 --> 00:47:50,410
And if we can increase the
graph for a closer look,

788
00:47:50,500 --> 00:47:53,500
but by doing this just like that and,

789
00:47:53,501 --> 00:47:56,540
or to view a wider range of data
points depending on the, uh,

790
00:47:56,670 --> 00:48:01,450
and which expands the y axis, just
like that. Okay. Um, we can also,

791
00:48:01,490 --> 00:48:04,360
um, DoubleClick to zoom out.
So double clicking zooms out.

792
00:48:04,361 --> 00:48:07,450
So we zoom in and then double
click to zoom out. Okay.

793
00:48:07,900 --> 00:48:11,740
And the step option shows the time steps.
Okay.

794
00:48:12,220 --> 00:48:15,070
And so a great thing about this is,

795
00:48:15,100 --> 00:48:18,670
so right now we only have two of these,
a two scalers,

796
00:48:18,671 --> 00:48:22,690
but if we had multiple scalers, so if
our computation graph was really complex,

797
00:48:22,960 --> 00:48:24,970
then we could group them together.

798
00:48:24,971 --> 00:48:29,950
So if I type in accuracy or sorry,
if I could type in like lol,

799
00:48:30,700 --> 00:48:34,450
it's like what's going to be here?
Nothing. But if I type in accuracy,

800
00:48:36,040 --> 00:48:40,420
it then encapsulates the accuracy that we
already have. So if we have names like,

801
00:48:40,510 --> 00:48:44,320
you know, accuracy slash accuracy,
one, accuracy slash accuracy, two,

802
00:48:44,440 --> 00:48:47,710
accuracies as accuracy three, and then
ours to create accuracy right here,

803
00:48:47,800 --> 00:48:51,460
it would then encapsulate all of those.
Okay. So that's great for um, doing, uh,

804
00:48:51,490 --> 00:48:54,340
in the browser encapsulation,
similar to names, scopes,

805
00:48:54,490 --> 00:48:56,680
but kind of like a Gui version of that.
Okay.

806
00:48:56,681 --> 00:49:01,030
Save your question to the end and then
smoothing makes the graph smoother or

807
00:49:01,120 --> 00:49:04,300
less smooth. Uh, less
smooth is if you want more.

808
00:49:04,390 --> 00:49:09,070
The trade off here is less pretty
but more accurate. What else we got?

809
00:49:09,071 --> 00:49:11,560
We got relative and then wall,
um,

810
00:49:11,950 --> 00:49:14,260
relative showed the time
relative to when it started,

811
00:49:14,270 --> 00:49:18,910
whereas wall shows the time of training
in general, like in, in real world time.

812
00:49:19,180 --> 00:49:22,210
What else do we have? We have
our images, so these are the um,

813
00:49:23,770 --> 00:49:28,120
the image summaries that we created.
We can visualize them right here. Okay.

814
00:49:28,380 --> 00:49:30,820
Um,
and then we have audio.

815
00:49:30,850 --> 00:49:35,830
So I haven't actually seen somebody use
audio for tensor board and believe me,

816
00:49:35,860 --> 00:49:40,270
I have looked, I have looked on the web
and I haven't seen anybody do it. Um,

817
00:49:41,560 --> 00:49:45,670
so I would love to see it. I would love
to see someone and I actually, you know,

818
00:49:45,720 --> 00:49:46,241
I have,
let's see,

819
00:49:46,241 --> 00:49:49,690
someone use wave wavenet or one of
these things that used to look at audio.

820
00:49:49,750 --> 00:49:51,250
I think Magenta might,

821
00:49:51,430 --> 00:49:55,240
but Magenta is so hard to get started.

822
00:49:55,241 --> 00:49:58,660
There's so many scripts and it's very
confusing. So let's look at our graph. Uh,

823
00:49:59,080 --> 00:50:00,880
I can't believe I saved
the graph for last.

824
00:50:01,240 --> 00:50:05,680
But the thing here is the graph is
these are our names, scopes, right?

825
00:50:05,681 --> 00:50:09,130
So we these continents
to calmed one fc one.

826
00:50:09,370 --> 00:50:12,070
These are the names scopes that we
created. And if we double click,

827
00:50:13,300 --> 00:50:16,900
we can then see the parts that are
encapsulated in the name scopes.

828
00:50:17,110 --> 00:50:21,370
So why are these colored,
uh, a different, uh, colors?

829
00:50:21,550 --> 00:50:24,820
So for every color, except for
gray, those are the default colors.

830
00:50:25,060 --> 00:50:26,890
So for every color,
um,

831
00:50:27,190 --> 00:50:31,460
flow automatically looks at the w
the data inside of these operations.

832
00:50:31,700 --> 00:50:35,780
And if they're the exact same, they know,
color it the same. So it's kind of a,

833
00:50:35,970 --> 00:50:38,090
it's,
it's a way of organizing data.

834
00:50:38,091 --> 00:50:41,900
So it's cleaner and it's a way for us
to know that it's going to be the same

835
00:50:42,170 --> 00:50:44,000
type of data.
Okay.

836
00:50:44,570 --> 00:50:49,520
And reason they did this is because in
its inception there was some bug and it

837
00:50:49,521 --> 00:50:51,860
was because in one of the layers
it was like one small difference.

838
00:50:51,861 --> 00:50:55,760
So now they have this by default. So
colors, it's going to color code itself.

839
00:50:56,000 --> 00:50:59,930
And if you look over here at the side,
you see this in it all by itself.

840
00:51:00,260 --> 00:51:04,760
That's because tensorflow detected,
sorry, tents are boring. I'm sorry.

841
00:51:04,930 --> 00:51:09,930
Tenser board detected that there was one
operation that was continuously used a

842
00:51:10,941 --> 00:51:15,350
lot. And so then it put it into its
own section because if we didn't,

843
00:51:15,351 --> 00:51:19,130
then it would be, it would look even, it
would look crazy. So let me show you this.

844
00:51:19,520 --> 00:51:24,050
We could put this in its function back
into the graph and then it looks like

845
00:51:24,051 --> 00:51:28,640
this because it's used all over the place.
But if we removed from the graph,

846
00:51:28,820 --> 00:51:31,670
we could see all of its connections
and have the graph be cleaner.

847
00:51:31,671 --> 00:51:34,150
So the reason it does this,
this auxiliary,

848
00:51:34,250 --> 00:51:39,170
auxiliary node function is to make
it cleaner to view. Okay. And then,

849
00:51:39,200 --> 00:51:43,580
um, we can also structured
by the, uh, by, by device,

850
00:51:43,581 --> 00:51:46,640
whether that's CPU or
GPU or the structure.

851
00:51:46,940 --> 00:51:50,900
And we can look at everything that's
inside of the graph like this and all the

852
00:51:50,901 --> 00:51:53,590
layers and increasingly,
uh,

853
00:51:54,410 --> 00:51:58,100
so much complexity. I
love it. I love it, man.

854
00:51:58,101 --> 00:52:03,101
I wish I had time to just make my own
project and just make it just really go

855
00:52:04,341 --> 00:52:07,070
dive, dive in on this. But I'm having
so much fun making content right now,

856
00:52:07,071 --> 00:52:09,410
so I'm just going to keep going
with this. Uh, two more things.

857
00:52:09,411 --> 00:52:13,490
I want to show really fast or the
distributions and the histograms. So

858
00:52:15,170 --> 00:52:18,260
the distributions are for their weights
and our biases there for layers.

859
00:52:18,500 --> 00:52:19,271
Remember we,

860
00:52:19,271 --> 00:52:23,690
we randomly initialize them but they
change over time and we want to show what

861
00:52:23,691 --> 00:52:26,600
they actually are compared to what
they possibly could be. So that,

862
00:52:26,750 --> 00:52:31,230
so the possibilities versus the
actuality or reality. And uh,

863
00:52:31,250 --> 00:52:35,700
one more is our, the histograms.
So histograms are, um,

864
00:52:37,190 --> 00:52:41,540
for a histogram plot allows you
to plot variables from your graph.

865
00:52:41,690 --> 00:52:43,100
So if your model has weights,

866
00:52:43,280 --> 00:52:46,370
the histogram showed you the values of
those weights and how they change with

867
00:52:46,371 --> 00:52:49,460
training. So this is, it's nice
little three d looking graph, which,

868
00:52:49,640 --> 00:52:54,090
which is pretty cool. And uh,
yeah, so that's it for this. Uh,

869
00:52:54,140 --> 00:52:56,330
we've got three more minutes
for questions. So let me,

870
00:52:56,360 --> 00:53:01,060
let me answer some questions and then
we're out of here. Uh, wow. That's, so my,

871
00:53:01,070 --> 00:53:05,180
there's even more people in here now than
there were before, which is insane. Now.

872
00:53:05,190 --> 00:53:08,990
Now there are 4,432 people here.
Okay. So let me see what people

873
00:53:21,020 --> 00:53:23,910
video did not pause in deep
neural networks. What is a,

874
00:53:24,140 --> 00:53:29,140
what is an alternative for gradient
descent for minimizing errors and deep on

875
00:53:29,290 --> 00:53:32,340
for progression, for minimizing
errors? So that's a great question. Um,

876
00:53:33,660 --> 00:53:35,190
there's the castic grading descent,

877
00:53:35,191 --> 00:53:38,850
which is different from standard
gradient descent. There are, um,

878
00:53:40,450 --> 00:53:44,020
a bunch of different
optimizers at a Grad. Um, Adam,

879
00:53:44,200 --> 00:53:46,930
if you look on the tensorflow
documentation for optimizers,

880
00:53:46,960 --> 00:53:50,650
you'll find a list of them. Two more or
two more questions. Well, deep learning,

881
00:53:50,651 --> 00:53:54,820
solve intelligence. So spoiler
alert. I don't think so.

882
00:53:54,850 --> 00:53:56,950
I don't think deep learning
will solve intelligence.

883
00:53:57,220 --> 00:53:58,870
I think it's a pathway to get there.

884
00:53:59,020 --> 00:54:02,340
We need to make more computationally
efficient models with, um,

885
00:54:02,590 --> 00:54:04,390
that need less data for the same result.

886
00:54:04,720 --> 00:54:07,900
And I feel like there's still something
that we're just missing something,

887
00:54:08,140 --> 00:54:13,000
something very fundamental,
something very basic. And, um, yeah.

888
00:54:13,390 --> 00:54:16,210
Uh, but we're, we'll, we'll
get there. We'll get there. Uh,

889
00:54:16,390 --> 00:54:18,670
let me know two more questions cause
we are, we have two more minutes.

890
00:54:19,030 --> 00:54:22,300
How do you explain the evolution of the
weights distribution given the evolution

891
00:54:22,301 --> 00:54:26,200
of the gradients distribution?
So recall,

892
00:54:26,201 --> 00:54:28,660
and I have a great video on this
backpropagation in five minutes,

893
00:54:29,050 --> 00:54:32,950
but the gradients give us a direction
to update our weights. So they are,

894
00:54:33,520 --> 00:54:36,510
and your, your question, well
it's gone now but how do we,

895
00:54:36,540 --> 00:54:39,540
something about relating the gradients
to the weights. So the weights.

896
00:54:41,600 --> 00:54:41,880
Okay.

897
00:54:41,880 --> 00:54:44,610
A great way to model that is to have,
um,

898
00:54:45,060 --> 00:54:47,670
histograms for both the
gradients and the weights.

899
00:54:47,820 --> 00:54:50,580
Right now we only have histograms for
the weights so you can see how they both

900
00:54:50,581 --> 00:54:53,460
change in real time. One
more question. Two more.

901
00:54:53,610 --> 00:54:55,650
Does our brand news back prop and no,
it does not.

902
00:54:55,860 --> 00:54:58,080
Our brain does not use back prop.
Um,

903
00:55:00,490 --> 00:55:01,323
uh,

904
00:55:01,470 --> 00:55:04,980
Adam optimizer versus
STD. Uh, I would say SGD.

905
00:55:06,950 --> 00:55:08,420
And one last question.

906
00:55:09,470 --> 00:55:13,490
Is Lb fts a true linear
regression technique?

907
00:55:14,300 --> 00:55:15,160
Uh,
uh,

908
00:55:15,860 --> 00:55:20,470
Lbf Gis is one of the most complicated
things that I've come by. I have to admit,

909
00:55:21,060 --> 00:55:24,760
um, I, I, I don't think it's
a linear regression technique.

910
00:55:25,340 --> 00:55:28,760
Um, yeah. Anyway, yeah.

911
00:55:30,450 --> 00:55:35,450
Blusher full was the LBF Jess was at
blusher fetcher gesture session or

912
00:55:35,721 --> 00:55:40,430
something like that. It was the name
of four scientists. Um, but, uh,

913
00:55:40,700 --> 00:55:42,470
anyway,
so that's it for this ice cream.

914
00:55:42,500 --> 00:55:44,630
We got 400 people at the end of
the livestream. That's amazing.

915
00:55:44,631 --> 00:55:48,540
Thanks guys for watching.
For now, I've got to go, um,

916
00:55:49,700 --> 00:55:53,380
make more technically accurate,
more mathematically accurate, uh,

917
00:55:53,450 --> 00:55:56,930
content for you guys because I really
want to be increased the learning

918
00:55:56,931 --> 00:56:01,340
capability of my content. So I love
you guys. So thanks for watching.

