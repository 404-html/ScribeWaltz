1
00:00:00,210 --> 00:00:02,220
Yo,
what's your research paper called?

2
00:00:06,060 --> 00:00:11,060
Hello world and a paper titled Everybody
Dance now out of UC Berkeley just

3
00:00:12,001 --> 00:00:16,380
dropped and it's causing a lot of
hype in the Ai Community right now,

4
00:00:16,560 --> 00:00:19,380
given a source video of a person dancing,

5
00:00:19,410 --> 00:00:22,200
ideally a professional
for the best results.

6
00:00:22,440 --> 00:00:27,150
They were able to transfer that dance
routine to another person called the

7
00:00:27,151 --> 00:00:27,984
target.

8
00:00:27,990 --> 00:00:32,610
After collecting just a few minutes of
training data from the target performing

9
00:00:32,611 --> 00:00:37,140
standard moves, the implications
of this technology are profound.

10
00:00:37,170 --> 00:00:39,090
It's like auto tune for dancing.

11
00:00:39,390 --> 00:00:44,340
No longer do we necessarily need to be
able to dance well in order to create a

12
00:00:44,341 --> 00:00:48,630
video of ourselves stancing
and notice that in their demo.

13
00:00:48,810 --> 00:00:53,550
Although the output video is clearly
a bit pixelated and blurry to being

14
00:00:53,551 --> 00:00:55,830
somewhat into the uncanny valley,

15
00:00:56,100 --> 00:01:00,600
it was still able to generate even the
reflection of the dancer in the glass

16
00:01:00,810 --> 00:01:04,530
behind him. Very impressive.
In the short term,

17
00:01:04,531 --> 00:01:09,531
we'll likely see this technology be used
for casual consumption in the form of a

18
00:01:09,811 --> 00:01:13,170
gift generating app or a snapchat filter.

19
00:01:13,470 --> 00:01:17,640
People will use this as a fun pastime,
but as it gets better,

20
00:01:17,641 --> 00:01:20,640
and it certainly will,
we'll start to see movie studios,

21
00:01:20,641 --> 00:01:25,560
celebrities and advertising agencies
using it to help enhance their productions

22
00:01:25,561 --> 00:01:26,970
in a low cost way.

23
00:01:27,240 --> 00:01:32,100
Why hire an entire background dance
crew when you can just generate them?

24
00:01:32,400 --> 00:01:37,400
A lead actor in a movie or a commercial
won't need intensive training to record

25
00:01:37,651 --> 00:01:42,651
any dance sequence and because it can
lower costs so drastically expect to see

26
00:01:43,651 --> 00:01:48,090
better quality content coming from the
immature video production community as

27
00:01:48,091 --> 00:01:52,770
well. In a way, it democratizes
dance. The dark side of this,

28
00:01:52,771 --> 00:01:56,160
just like the deep phase technology
that I explained a few months ago,

29
00:01:56,400 --> 00:02:00,870
is that it's moving us towards a
world where any video evidence can be

30
00:02:00,871 --> 00:02:05,070
manipulated, people can be framed
for crimes they didn't commit.

31
00:02:05,430 --> 00:02:09,960
Anyone can use AI to synthesize
voices, faces, photos,

32
00:02:09,961 --> 00:02:10,800
and now video.

33
00:02:11,370 --> 00:02:15,320
So we'll need to create new
methods of identity verification.

34
00:02:15,321 --> 00:02:16,380
Since video,

35
00:02:16,381 --> 00:02:21,381
whether from a smartphone or a police
body cam is still the key make or break

36
00:02:21,961 --> 00:02:24,240
element of many defense cases.

37
00:02:24,420 --> 00:02:28,760
One solution is to use AI to
fight AI. In fact, giphy cat,

38
00:02:28,770 --> 00:02:33,770
a popular video hosting
platform is already using AI
to classify videos is real

39
00:02:33,901 --> 00:02:38,400
or fake. Another solution is
drum roll please. A blockchain,

40
00:02:38,640 --> 00:02:43,640
a newer startup called factum has a
solution that's already been tested by the

41
00:02:45,000 --> 00:02:45,360
Department of Homeland Security.

42
00:02:45,360 --> 00:02:49,740
Since the Bitcoin blockchain is an
immutable data structure that no one can

43
00:02:49,741 --> 00:02:53,820
modify. Factum is using it
to timestamp video data from,

44
00:02:53,821 --> 00:02:58,821
say a security camera at specific
intervals because it digitally signs and

45
00:02:59,321 --> 00:03:03,700
hashes data instantly. You're right. As
the pixels are pulled off the camera,

46
00:03:04,000 --> 00:03:08,830
they can confidently claim that a video
was really taken by the camera that

47
00:03:08,831 --> 00:03:11,320
digitally signed the data.
In general,

48
00:03:11,321 --> 00:03:16,321
we can all use the blockchain to digitally
signed and confirm the authenticity

49
00:03:16,361 --> 00:03:18,430
of a video file that relates to us.

50
00:03:18,670 --> 00:03:22,090
The more people that add their
digital signature to a video,

51
00:03:22,240 --> 00:03:25,930
the more valid it can be considered.
This kind of scheme,

52
00:03:25,931 --> 00:03:30,520
we'll need to factor in the qualifications
of the people who sign a video file,

53
00:03:30,790 --> 00:03:33,670
but it's promising.
So back to our researchers,

54
00:03:33,940 --> 00:03:38,940
this is not the first attempt in this
field at creating new video content.

55
00:03:39,310 --> 00:03:41,800
I manipulating existing video footage.

56
00:03:42,100 --> 00:03:46,990
In 97 researchers created videos of
a subject saying a phrase they didn't

57
00:03:47,050 --> 00:03:51,820
originally order I finding frames where
the mouth position matched the desired

58
00:03:51,821 --> 00:03:52,654
speech.

59
00:03:52,810 --> 00:03:57,810
In 2003 a group used optical flow as a
descriptor to match different subjects

60
00:03:58,751 --> 00:04:00,640
performing similar actions.

61
00:04:01,000 --> 00:04:05,800
More recently it was demonstrated that
deep learning techniques can help with

62
00:04:05,801 --> 00:04:10,801
motion retargeting without supervised
data using generative adversarial nets.

63
00:04:11,530 --> 00:04:13,990
This work was very likely
inspired by that one.

64
00:04:14,320 --> 00:04:17,650
Our researchers split up their
training pipeline into three steps,

65
00:04:17,910 --> 00:04:21,100
pose estimation, global
pose, normalization.

66
00:04:21,280 --> 00:04:25,660
Then mapping from normalized posts,
stick figures to the target subject.

67
00:04:25,840 --> 00:04:28,750
Let's go over each first pose estimation.

68
00:04:29,050 --> 00:04:32,800
Once they found a suitable source
video of a professional dancer,

69
00:04:33,040 --> 00:04:37,990
they needed to encode the body positions
of the source subject and what better

70
00:04:37,991 --> 00:04:38,950
way to do that?

71
00:04:38,951 --> 00:04:43,210
Then by using the pretrained
pose detector called open pose,

72
00:04:43,330 --> 00:04:47,770
we can accurately estimate all of the
subjects multiple joint coordinates.

73
00:04:48,100 --> 00:04:50,110
This is a convolutional neural network,

74
00:04:50,140 --> 00:04:55,140
a specific series of matrix operations
that was optimized for pose estimation by

75
00:04:55,811 --> 00:04:58,240
using the common strategy
of gradient descent.

76
00:04:58,360 --> 00:05:01,870
They just downloaded the
pretrained weights file. In fact,

77
00:05:01,871 --> 00:05:06,760
anyone can try out pose estimation in
the browser today and I actually have a

78
00:05:06,761 --> 00:05:10,150
detailed video on how pose
estimation works already.

79
00:05:10,300 --> 00:05:12,190
Link is going to be in
the video description.

80
00:05:12,280 --> 00:05:16,800
They took those coordinates and drew
a representation of the resulting pose

81
00:05:16,840 --> 00:05:21,730
stick figure by plotting the key points
and drawing lines between the connected

82
00:05:21,731 --> 00:05:26,530
joints and because of video is just a
series of images in the form of video

83
00:05:26,531 --> 00:05:27,364
frames.

84
00:05:27,520 --> 00:05:31,990
They did this for every frame in the
video creating a rich dataset of dance

85
00:05:31,991 --> 00:05:32,824
poses.

86
00:05:33,070 --> 00:05:37,540
They then normalized each input image
to help account for the differences

87
00:05:37,660 --> 00:05:42,550
between the source and target body
shapes and locations within the frame.

88
00:05:42,760 --> 00:05:47,500
I analyzing joint positions for each
subject and using a linear mapping between

89
00:05:47,501 --> 00:05:50,980
the closest and farthest angle positions.
In both videos.

90
00:05:51,310 --> 00:05:56,310
They use this Dataset as input
to a generative adversarial
network for training.

91
00:05:56,680 --> 00:06:01,130
The is composed of two neural
networks called the generator and the

92
00:06:01,131 --> 00:06:02,000
discriminator.

93
00:06:02,300 --> 00:06:07,100
The generator applies a
series of transforms to the
input image to produce the

94
00:06:07,101 --> 00:06:07,970
output image.

95
00:06:08,270 --> 00:06:12,650
The discriminators job is to then
perform a binary classification,

96
00:06:12,830 --> 00:06:16,310
trying to discern if the
output image is real or fake.

97
00:06:16,460 --> 00:06:20,630
That is if it's actually the target
subject dancing or a fake version.

98
00:06:21,230 --> 00:06:24,980
The structure of the generator
is called an in coder decoder.

99
00:06:25,190 --> 00:06:29,330
It takes the input image and tries
to compress it into a much smaller

100
00:06:29,331 --> 00:06:32,270
representation using a series of incoders.

101
00:06:32,420 --> 00:06:36,920
These are operational blocks consisting
of a convolution and an activation

102
00:06:36,921 --> 00:06:40,520
function. The idea is that
by compressing it like this,

103
00:06:40,521 --> 00:06:44,480
we can hopefully have a higher
level representation of the data.

104
00:06:44,660 --> 00:06:46,460
After the final Incode layer.

105
00:06:46,880 --> 00:06:51,620
The decode layers do the exact opposite
using operational blocks consisting of a

106
00:06:51,690 --> 00:06:56,690
deconvolution and an activation function
and reverses the action of the encoder

107
00:06:57,141 --> 00:06:57,890
layers.

108
00:06:57,890 --> 00:07:02,630
A performance improvement here is to
directly connect the encoder layers to the

109
00:07:02,631 --> 00:07:05,300
decoder layers using skip connections.

110
00:07:05,660 --> 00:07:10,520
These skip connections give the network
the option of bypassing certain encoding

111
00:07:10,700 --> 00:07:14,000
and decoding parts if it
doesn't have a use for it.

112
00:07:14,390 --> 00:07:19,220
The specific type of Incode or decoder
architecture is called a you net.

113
00:07:19,340 --> 00:07:23,390
Meanwhile, the discriminator
has the job of using two images,

114
00:07:23,391 --> 00:07:28,391
one from the target video and one from
the generated output and deciding if the

115
00:07:28,641 --> 00:07:31,940
second image was produced
by the generator or not.

116
00:07:32,330 --> 00:07:37,190
It's a convolutional network
that structure looks similar
to the encoder section

117
00:07:37,191 --> 00:07:40,220
of the generator works
a little differently.

118
00:07:40,550 --> 00:07:45,350
The output is an image where each pixel
value represents how believable the

119
00:07:45,351 --> 00:07:49,880
corresponding section of the unknown
image is. To train this network,

120
00:07:49,881 --> 00:07:51,020
there are two steps.

121
00:07:51,200 --> 00:07:56,200
Training the discriminator D and training
the generator g to train d first g

122
00:07:56,931 --> 00:07:58,250
creates an output image,

123
00:07:58,550 --> 00:08:03,530
the looks at the input target pair and
the input output pair and produces a

124
00:08:03,531 --> 00:08:06,680
probability about how realistic they look.

125
00:08:07,010 --> 00:08:11,720
The weights of d are then adjusted
based on the classification error of the

126
00:08:11,721 --> 00:08:15,200
input output pair and the
input target pair. Geez.

127
00:08:15,201 --> 00:08:19,880
Weights are then adjusted based on the
output of d as well as the difference

128
00:08:19,881 --> 00:08:22,400
between the output and the target image.

129
00:08:22,520 --> 00:08:26,570
Both the Angie will improve
over time during this process.

130
00:08:26,900 --> 00:08:29,870
This training procedure
worked well enough,

131
00:08:29,930 --> 00:08:32,990
but they found that the
generated dance video frames,

132
00:08:32,991 --> 00:08:37,280
we're still kind of choppy and
didn't look very realistic,

133
00:08:37,430 --> 00:08:42,290
so they added in two techniques can
portal smoothing and a facial GAM.

134
00:08:42,710 --> 00:08:45,260
Instead of generating individual frames,

135
00:08:45,380 --> 00:08:50,380
they modified their generator to predict
two consecutive frames where the first

136
00:08:50,421 --> 00:08:54,140
output is conditioned on
its corresponding pose,

137
00:08:54,170 --> 00:08:58,290
stick figure and the generated
frame at the previous time step.

138
00:08:58,650 --> 00:09:03,650
The second output is conditioned on its
corresponding posts stick figure and the

139
00:09:04,021 --> 00:09:05,070
first output.

140
00:09:05,340 --> 00:09:09,690
So the discriminator was actually tasked
with determining both the differences

141
00:09:09,691 --> 00:09:14,691
in realism and the tempur coherence
between the fake sequence and the real

142
00:09:15,001 --> 00:09:18,660
sequence and to increase
the realism of the face.

143
00:09:18,750 --> 00:09:22,710
They use an additional Ghana
specifically for generating faces.

144
00:09:23,190 --> 00:09:27,720
Their model was able to create a
reasonably long video of a target person

145
00:09:27,721 --> 00:09:29,520
dancing given body movements,

146
00:09:29,760 --> 00:09:34,500
but the results still suffer from some
shakiness solutions to this could be to

147
00:09:34,501 --> 00:09:36,660
use more training data.
Of course,

148
00:09:37,020 --> 00:09:42,020
another would be to try out a different
normalization technique for both the

149
00:09:42,241 --> 00:09:44,220
source and target videos.

150
00:09:44,221 --> 00:09:49,080
Since the shakiness is likely a result
of the underlying differences between how

151
00:09:49,081 --> 00:09:51,960
the source and target subjects move,

152
00:09:51,990 --> 00:09:56,990
given their unique body structures or to
try out a different Gan loss function.

153
00:09:57,491 --> 00:10:00,210
Since Gan research is
very active right now,

154
00:10:00,450 --> 00:10:05,340
one only needs to glance@archivesanity.com
to see an example of what's being

155
00:10:05,341 --> 00:10:07,230
done there.
Overall,

156
00:10:07,231 --> 00:10:12,231
this research is very exciting and opens
up a lot of new possibilities for both

157
00:10:12,421 --> 00:10:16,920
researchers and entrepreneurs.
Is this video real? Who knows?

158
00:10:17,010 --> 00:10:20,820
Hit subscribe. Any way to stay
updated on my life's work. For now,

159
00:10:20,850 --> 00:10:24,300
I've got to synthesize a new GPU,
so thanks for watching.

