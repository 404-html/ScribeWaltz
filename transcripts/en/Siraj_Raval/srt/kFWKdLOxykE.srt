1
00:00:00,040 --> 00:00:01,530
Oh world,
it's Raj.

2
00:00:01,560 --> 00:00:06,240
And today we're going to learn how to
use a pretrained tensorflow model on an

3
00:00:06,300 --> 00:00:10,470
android device. So this applies to
any device that runs on android.

4
00:00:10,740 --> 00:00:15,720
We would train the model on our desktop
or on our laptop or on the server,

5
00:00:15,900 --> 00:00:20,340
and then we would use that pretrained
model on our mobile device.

6
00:00:20,490 --> 00:00:22,830
So there's no training that
would happen on the device.

7
00:00:22,831 --> 00:00:27,420
The training would happen on our bigger
machine, either a server or our laptop,

8
00:00:27,720 --> 00:00:28,800
but then the inference,

9
00:00:28,860 --> 00:00:33,420
the actual making of making predictions
that would happen on our android device.

10
00:00:33,630 --> 00:00:35,790
So that's what we're going to do today.
And uh,

11
00:00:35,791 --> 00:00:39,930
we're doing it for a handwritten character
image classifier. So the user would,

12
00:00:40,260 --> 00:00:41,430
here's,
here's what it looks like.

13
00:00:42,420 --> 00:00:47,420
So you draw an image and then you hit
detect and it's going to output the

14
00:00:47,881 --> 00:00:52,260
result. And the result is the probability
of what that class is. It's a,

15
00:00:52,410 --> 00:00:57,090
it's a multi-class classification
problem. So it's a very, very simple APP.

16
00:00:57,150 --> 00:01:00,210
And the point of this is to learn
how to use tensorflow with android.

17
00:01:00,420 --> 00:01:01,800
Very simple use case.
Right?

18
00:01:01,830 --> 00:01:03,930
And there are a couple other
examples that I'm going to give,

19
00:01:04,200 --> 00:01:07,290
but in this example, uh, it's
just as simple as it gets.

20
00:01:07,320 --> 00:01:08,610
So it's a great way to get started.

21
00:01:08,611 --> 00:01:11,970
You can use this repository that I'm
going to link to that I'm going to be

22
00:01:11,971 --> 00:01:16,740
looking at here as a base for any other
type of classification project that you

23
00:01:16,741 --> 00:01:20,910
want to make. Cool. So
that's what we're doing.

24
00:01:20,911 --> 00:01:23,310
We're going to write a handwritten
character image classifier.

25
00:01:23,430 --> 00:01:27,450
The user draws it with their hand and
then he hits the tech and it's going to

26
00:01:27,451 --> 00:01:28,284
classify it.

27
00:01:28,620 --> 00:01:33,620
And doing this on a server or on your
laptop is good because training is a very

28
00:01:34,051 --> 00:01:38,790
computationally expensive process and
it's a very energy intensive process,

29
00:01:38,791 --> 00:01:41,850
right?
We can't just plug our phone into our,

30
00:01:42,450 --> 00:01:46,350
into the wall all the time, but we
can do that for our laptop or desktop.

31
00:01:46,620 --> 00:01:48,420
So that's why we do it on those machines.

32
00:01:48,450 --> 00:01:51,450
But in France we can easily
do that on our phone.

33
00:01:51,900 --> 00:01:56,900
So what I have here a is this little
architecture image of what this looks like

34
00:01:57,361 --> 00:02:00,140
from a, from a high
level, we've got to, um,

35
00:02:00,330 --> 00:02:04,530
we've got two repositories here.
We have the android Sdk,

36
00:02:04,710 --> 00:02:07,920
and then we had the android n d k. Right.
So what's the difference here? Well,

37
00:02:07,921 --> 00:02:12,290
the STK is a standard developing
kit and the MDK is the, uh,

38
00:02:12,420 --> 00:02:14,580
native development kits.
What do I mean by that? Well,

39
00:02:14,581 --> 00:02:18,780
the SDK has written in Java and
the Ndk is written in c plus.

40
00:02:18,781 --> 00:02:23,781
Plus we use the Sdk as an interface to
code everything we want in Java at a high

41
00:02:24,541 --> 00:02:25,110
level.

42
00:02:25,110 --> 00:02:28,950
So that's all of the things that are
native to android activities and fragments

43
00:02:28,951 --> 00:02:30,090
and image listeners.

44
00:02:30,120 --> 00:02:34,110
So I'm going to actually go into quite
a bit of detail on how android works.

45
00:02:34,260 --> 00:02:37,200
I'm not sure how much, you know, I've
never really talked about this stuff.

46
00:02:37,350 --> 00:02:39,330
So rather than being a fly by,

47
00:02:39,331 --> 00:02:43,410
I'm just going to go as detailed as
possible and hopefully, you know,

48
00:02:43,470 --> 00:02:45,510
you don't already know all this stuff.
Okay.

49
00:02:45,511 --> 00:02:50,190
So I'm just going to assume you don't
know much about android. So, uh, right.

50
00:02:50,191 --> 00:02:53,500
So android has a very
specific way that things work.

51
00:02:53,501 --> 00:02:56,760
This is very specific way that objects
interact with each other, right?

52
00:02:56,761 --> 00:02:59,190
You have these activities and
then you have these fragments,

53
00:02:59,320 --> 00:03:00,940
image listeners and all of these.

54
00:03:01,150 --> 00:03:05,420
And then on click and create all of
these event listeners for how uh,

55
00:03:05,890 --> 00:03:07,810
you would interact with an android APP.
It's,

56
00:03:07,811 --> 00:03:10,240
it's specific to a
touchscreen like interface.

57
00:03:10,390 --> 00:03:13,720
And so that's what the Sdk is good for it.
You can do all of that stuff in Java.

58
00:03:14,050 --> 00:03:16,060
But what the NDK is good for,

59
00:03:16,240 --> 00:03:20,950
it's for looking at or is for working
with c plus plus files and what is

60
00:03:20,951 --> 00:03:24,910
tensorflow written in the core of
tensorflow. It's written in c plus plus.

61
00:03:25,030 --> 00:03:28,990
So that's why we use it so we can look so
we don't have to deal with c plus plus.

62
00:03:28,991 --> 00:03:33,910
That's why we had the SDK. But under
the hood, the SDK talks to the NDK,

63
00:03:34,030 --> 00:03:37,600
which is written in c plus plus. So what
happens is you see these two arrows.

64
00:03:37,690 --> 00:03:40,870
So this Arrow Arrow one
takes the image a bit map

65
00:03:40,900 --> 00:03:45,900
image@theuserdrawsandsendsittothistensorflowjni.cc
class,

66
00:03:46,031 --> 00:03:51,010
which is like the main tensorflow. See
a wrapper for android written in c plus.

67
00:03:51,011 --> 00:03:54,220
Plus. It takes that image and it, it,

68
00:03:55,480 --> 00:03:58,870
it turns it into a tensor.
So it resizes it into a tensor,

69
00:03:59,140 --> 00:04:02,830
gives it to the model of the pretrained
model, which is a pro buff file,

70
00:04:02,831 --> 00:04:06,130
which we'll talk about more,
but as a pretrained convolutional network.

71
00:04:06,790 --> 00:04:09,100
And it's going to output the prediction,

72
00:04:09,130 --> 00:04:12,940
which is a tensor to back to
that t plus c plus plus file.

73
00:04:13,240 --> 00:04:17,230
And then that CPI plus file is going to
return a list of probability values in

74
00:04:17,231 --> 00:04:21,670
the form of an rea straight to our SDK.

75
00:04:21,671 --> 00:04:22,960
That is our Java class.

76
00:04:23,200 --> 00:04:26,140
And we'll talk about all of these classes
in order from the highest level to the

77
00:04:26,141 --> 00:04:29,800
lowest level. Uh, but that's, that's
essentially what it's doing here.

78
00:04:30,820 --> 00:04:33,340
Okay. So that's the example
that I'm going to use.

79
00:04:33,341 --> 00:04:37,900
But there are several examples
of how tensorflow can work,
can work with android.

80
00:04:38,110 --> 00:04:42,100
So this one is probably the most
well documented one that I've seen,

81
00:04:42,400 --> 00:04:45,010
but it's called android tensorflow,
flow machine learning example.

82
00:04:45,520 --> 00:04:49,630
Basically it is a general purpose
classifier. So if you have a device,

83
00:04:49,780 --> 00:04:53,620
you can plug your device into
your computer with android studio.

84
00:04:53,770 --> 00:04:57,510
And then in real time is just going to
classify everything that you look at this

85
00:04:57,511 --> 00:05:00,040
wallet, you know, 29% wallet.

86
00:05:00,550 --> 00:05:04,740
That's a pretty bad classification
plan, present wallet. But Hey, uh,

87
00:05:04,780 --> 00:05:06,280
20% ballpoint pen.

88
00:05:06,490 --> 00:05:11,490
And the reason it's so well documented
is because it comes with an associated

89
00:05:11,771 --> 00:05:12,604
blog post,

90
00:05:12,850 --> 00:05:17,560
which has got all of these pointers
to work with Bazell and uh,

91
00:05:17,800 --> 00:05:20,710
you know how to set up your workspace so
that it's pointing to the right version

92
00:05:20,711 --> 00:05:24,280
of the STK and the NDK. Uh,
so yeah, all that stuff.

93
00:05:24,281 --> 00:05:27,550
But the reason I'm not doing that is
because it is more complicated than what

94
00:05:27,551 --> 00:05:32,020
we're going to do a, but
that's a good next step to go
to after you do this. Okay.

95
00:05:32,021 --> 00:05:33,730
So that's the image classifier example.

96
00:05:33,731 --> 00:05:36,490
There's another classifier
which is a little more detailed.

97
00:05:36,491 --> 00:05:40,720
So it does the same exact thing as the
first, but once it's classified the image,

98
00:05:40,930 --> 00:05:45,790
it then queries both Wikipedia and a
Wolfram Alpha for some more data about

99
00:05:45,791 --> 00:05:49,330
whatever object it is. So it will classify
an image like let's say it's a lemon,

100
00:05:49,510 --> 00:05:52,780
and then use the lemon as a parameter
when it makes an http request to the

101
00:05:52,781 --> 00:05:56,860
server and then it returns a short
paragraph description of what a lemon is.

102
00:05:57,050 --> 00:05:58,610
So you can see how this
can be pretty useful.

103
00:05:58,611 --> 00:06:02,210
For now we're going to need the apps that
are actually useful to day to day life.

104
00:06:03,170 --> 00:06:06,650
Uh, right. So there's that. And there's
one more that I wanted to talk about.

105
00:06:06,950 --> 00:06:10,580
They're all really classifiers cause you
know you have the camera on the device.

106
00:06:10,581 --> 00:06:12,200
So why not just classify everything?

107
00:06:12,380 --> 00:06:15,590
I mean there's so many use cases for
classification that haven't even been

108
00:06:15,591 --> 00:06:19,850
attempted before. It's such a, it's
such a broad application, right?

109
00:06:19,970 --> 00:06:23,270
So this is guest sketch and there's
actually a web app version of this too.

110
00:06:23,480 --> 00:06:26,150
But you would draw something and then
it would classify whatever it is.

111
00:06:26,330 --> 00:06:29,660
So obviously you would need a model that's
trained on a lot of different images.

112
00:06:29,661 --> 00:06:32,540
It's not just m and ist,
but like a lot of different images.

113
00:06:32,630 --> 00:06:34,070
So that's when you do something like this.

114
00:06:34,100 --> 00:06:37,850
Inception would be a great model to
use for this pretrained inception.

115
00:06:38,240 --> 00:06:40,640
I love how they named it inception even
though it has nothing to do with the

116
00:06:40,641 --> 00:06:45,410
movie, but uh, it works like a
dream, so, okay. That was anyway,

117
00:06:45,860 --> 00:06:48,170
where were we? Okay, so we're
talking about installation.

118
00:06:48,171 --> 00:06:50,930
So that's what we're first
going to talk about. So again,

119
00:06:50,931 --> 00:06:54,160
remember I'm assuming you don't,
you've never used android before. Uh,

120
00:06:54,260 --> 00:06:57,530
so if you have used android
before then just skip ahead.

121
00:06:57,531 --> 00:07:01,520
Like I'm going to estimate four
minutes, but okay. So the first episode,

122
00:07:01,521 --> 00:07:06,020
download android studio.
So android studio has everything you need.

123
00:07:06,021 --> 00:07:08,570
So you know, there are several things
that you could use with android.

124
00:07:08,840 --> 00:07:10,850
You can use eclipse,
the ide,

125
00:07:11,060 --> 00:07:14,780
we could use android studio or you could
just use this with terminal and a text

126
00:07:14,781 --> 00:07:15,440
editor.

127
00:07:15,440 --> 00:07:18,920
But I'm saying that you should use android
studio because whereas before it was

128
00:07:18,921 --> 00:07:22,130
super buggy, like when I was using
it a lot, a year and a half ago.

129
00:07:22,370 --> 00:07:25,250
Now I just downloaded a,
and I've been looking through a lot.

130
00:07:25,610 --> 00:07:27,410
It's actually really,
really good now.

131
00:07:27,411 --> 00:07:32,390
Like it takes care of a lot of dependency
issues for you inside of the ide,

132
00:07:32,391 --> 00:07:36,340
which is amazing. Uh, so you can
download it from here. It's got, it's,

133
00:07:36,360 --> 00:07:38,990
it's available for all
the operating systems.

134
00:07:40,040 --> 00:07:43,100
And so these three steps
are the next three steps.

135
00:07:43,310 --> 00:07:45,620
And actually if you're
using android studio,

136
00:07:45,680 --> 00:07:48,170
you can do it all inside
of android studio.

137
00:07:48,171 --> 00:07:50,390
You don't actually have to
do this from command line.

138
00:07:50,570 --> 00:07:55,570
This is if you're using
eclipse or terminal and a
raw text editor like sublime

139
00:07:56,541 --> 00:08:01,190
or something. But you would download
the STK and then you download the NDK.

140
00:08:01,250 --> 00:08:04,400
Okay. As well as the build
tools. And once you have those,

141
00:08:04,401 --> 00:08:09,200
you would point those to the right.
You would point your workspace to those,

142
00:08:09,320 --> 00:08:14,090
to the location of both. So
your app knows where to, uh,

143
00:08:14,330 --> 00:08:17,900
where to find these repositories. Okay. So

144
00:08:20,360 --> 00:08:24,920
that's what we've got there. And so in my
grade will file, I've got this, you know,

145
00:08:24,921 --> 00:08:29,210
the target Sdk version so that I
know where that that's going to be.

146
00:08:29,690 --> 00:08:33,380
Okay. And so let's see, what else
do we got here to properties,

147
00:08:33,770 --> 00:08:38,720
local properties. Cradle is the
build system for android by the way,

148
00:08:38,760 --> 00:08:42,040
UC use Maven, but now it uses
great old. So yeah, that's,

149
00:08:42,080 --> 00:08:44,150
those are the dependencies
that we would use.

150
00:08:44,270 --> 00:08:47,810
And then once you've
downloaded these dependencies,

151
00:08:48,080 --> 00:08:50,180
then you want to train the model,
right?

152
00:08:50,181 --> 00:08:54,560
So remember we train the model on a
desktop or on a server and we do that in

153
00:08:54,561 --> 00:08:58,170
python just like you would even
if you weren't using android,

154
00:08:58,171 --> 00:09:01,170
you would first train the model there.
So let's go over that.

155
00:09:01,171 --> 00:09:04,140
What the python file looks like at a
high level and then we'll keep going with

156
00:09:04,141 --> 00:09:07,590
the android app and the code,
the Associated Code.

157
00:09:07,591 --> 00:09:11,580
So for the model in Python,
let's see what we got here.

158
00:09:12,240 --> 00:09:14,880
This is what the model looks like.
So this is the care os version.

159
00:09:14,881 --> 00:09:15,720
There are two versions.

160
00:09:15,930 --> 00:09:18,300
We've got the care os version
and the tensorflow of virgin.

161
00:09:18,510 --> 00:09:21,150
But I'm going to go through the
kiosk version really quickly. Okay.

162
00:09:21,420 --> 00:09:25,200
So we're going to import tariffs and
Python. Okay. So once we have those,

163
00:09:25,650 --> 00:09:27,780
we're going to build a
convolutional network.

164
00:09:28,110 --> 00:09:31,680
A convolutional net is used
to detect images, right?

165
00:09:31,710 --> 00:09:36,270
Humble show nets are used for any
kind of image classification or even

166
00:09:36,271 --> 00:09:39,900
generation. We can use convolutional
nets for image generation.

167
00:09:40,460 --> 00:09:43,080
We just reverse the way things work.

168
00:09:43,081 --> 00:09:46,860
Like we would use gradient ascent or we
would slice off the last half and then

169
00:09:47,310 --> 00:09:51,140
add some kind of ad is to cass tech
note in there. So it generates, um,

170
00:09:51,690 --> 00:09:52,800
some novel data.

171
00:09:53,430 --> 00:09:57,690
We would add a random variable in a see
variational auto encoders for how that

172
00:09:57,691 --> 00:10:00,390
works.
But you can generate new data types.

173
00:10:00,391 --> 00:10:02,520
And also obviously generative
adversarial networks,

174
00:10:02,521 --> 00:10:05,400
which I've made videos on both of
those topics. See my you Udacity,

175
00:10:05,401 --> 00:10:07,950
deep learning and a degree
course all on youtube. Okay.

176
00:10:07,951 --> 00:10:11,310
So a couple of methods here. The first
one is to load the data. And this is just,

177
00:10:11,340 --> 00:10:12,540
you know,
standard reshaping.

178
00:10:12,541 --> 00:10:15,150
We have to remember we have
to vectorize our input data.

179
00:10:15,151 --> 00:10:17,190
What do I mean by vectorize?
I mean,

180
00:10:17,191 --> 00:10:21,420
we used to take our image raw image and
then convert it into a tensor format so

181
00:10:21,421 --> 00:10:23,850
that we can feed it into our model.
And so in this case,

182
00:10:23,851 --> 00:10:28,030
it is a four dimensional tensor,
right? So the length with, uh,

183
00:10:28,080 --> 00:10:32,430
the size and the, the,
the depth, which is,

184
00:10:32,820 --> 00:10:36,330
which is going to be one, right?
Because it's only a single layer image.

185
00:10:36,600 --> 00:10:41,400
So we reshape it using numb Pi's, native
reshaping functions. It's a float 32.

186
00:10:41,401 --> 00:10:45,120
So these are float 32. These
are 32 bits, a pixel values,

187
00:10:45,121 --> 00:10:48,720
who an image and out of 255 possible,
uh,

188
00:10:49,200 --> 00:10:53,790
and then we got out of 255
possible RGB values. And so once,

189
00:10:53,970 --> 00:10:56,340
then once we have that, uh, vectorize,

190
00:10:56,430 --> 00:10:59,760
we'll split it into a training and a
testing set. And that's it for load data.

191
00:11:00,000 --> 00:11:00,900
And then for a model,

192
00:11:01,050 --> 00:11:04,170
great thing about [inaudible] is it's
only a few lines and we can build a

193
00:11:04,171 --> 00:11:05,940
convolutional network just like that.

194
00:11:06,150 --> 00:11:09,330
It's going to be a sequential model
with three convolutional blocks.

195
00:11:09,540 --> 00:11:11,340
So we do perform convolution,

196
00:11:11,370 --> 00:11:15,210
which is essentially like taking a
flashlight over a filter of images and

197
00:11:15,211 --> 00:11:17,100
looking for what sticks out.

198
00:11:17,101 --> 00:11:21,150
So in this case it would be the Mni
is t the the actual digits, right?

199
00:11:21,150 --> 00:11:24,210
Cause they're black on white background.
So it would detect, hold that.

200
00:11:24,600 --> 00:11:25,980
And then we perform pooling.

201
00:11:26,100 --> 00:11:29,370
So pooling is an operation where
we look at an image like an array.

202
00:11:29,460 --> 00:11:32,370
So let's say you know,
we've got like four different,

203
00:11:33,600 --> 00:11:37,980
we've got a square, uh, where you know,

204
00:11:38,040 --> 00:11:42,300
each of the values in the array
represent is a different number.

205
00:11:42,570 --> 00:11:46,920
And then we, for Max pooling we would
take the maximum value from that square.

206
00:11:46,921 --> 00:11:49,410
So it's, let's say we
got like four, nine, 12,

207
00:11:49,411 --> 00:11:52,830
15 we would take that maximum value,
which is going to be a,

208
00:11:53,500 --> 00:11:55,540
so the Max value and that's you know,

209
00:11:55,541 --> 00:11:59,230
a post you opposed to average pooling
or one of those, this is just a,

210
00:11:59,350 --> 00:12:03,280
a more often used pulling
strategy. Okay. So I actually,

211
00:12:03,281 --> 00:12:05,980
I already explained it so I don't
need to go that, go to that. Okay.

212
00:12:05,981 --> 00:12:09,500
So we do polling and then we do it
again, again, perform convolution.

213
00:12:09,501 --> 00:12:14,501
And what this is doing is every time
it's creating more images and smaller

214
00:12:14,531 --> 00:12:16,510
versions of that image. Right? And

215
00:12:17,330 --> 00:12:17,900
Yeah,

216
00:12:17,900 --> 00:12:19,940
we keep doing that until
we're at the very end.

217
00:12:19,970 --> 00:12:24,970
And now we've got this very high
dimensional four by four by 256 dimension

218
00:12:25,041 --> 00:12:28,160
tensor. But we need, and how put
probability, right? We need an,

219
00:12:28,220 --> 00:12:29,300
we need a prediction die.

220
00:12:29,301 --> 00:12:33,530
You're already set up prediction values
for the probable classes multi-class

221
00:12:33,531 --> 00:12:36,440
classification. So what will we do? Well,

222
00:12:36,441 --> 00:12:40,700
we have to flatten that tensor into a,
a one dimensional array,

223
00:12:40,850 --> 00:12:43,070
which is the list of probabilities.
So we'll,

224
00:12:43,540 --> 00:12:48,540
we'll hit or flatten it and then we'll
add to fully connected layers to which

225
00:12:48,681 --> 00:12:51,770
are the dense layers.
And uh, the last, uh,

226
00:12:51,890 --> 00:12:56,370
activation will be a softmax function.
We're just going to output a single, uh,

227
00:12:56,630 --> 00:12:59,960
value for each of the class outputs.
Okay.

228
00:12:59,961 --> 00:13:01,760
So that's it for the model and then,

229
00:13:04,800 --> 00:13:05,633
yeah.

230
00:13:06,970 --> 00:13:07,180
Okay.

231
00:13:07,180 --> 00:13:10,420
Okay. Thanks Internet. Okay.

232
00:13:10,450 --> 00:13:14,270
So that's it for the building, the model
part. And then for training with care,

233
00:13:14,271 --> 00:13:15,640
Ross,
remember it's just two lines.

234
00:13:15,940 --> 00:13:19,420
We compile the model and by
compile we defined two main things.

235
00:13:19,421 --> 00:13:22,780
We define the loss function that
we're going to minimize because it's a

236
00:13:22,781 --> 00:13:24,640
multi-class classification problem.

237
00:13:24,910 --> 00:13:28,330
We're using categorical cross
entropy and because it's an end,

238
00:13:28,360 --> 00:13:31,380
what are we going to use to actually
minimize that loss? Or we're going to,

239
00:13:31,480 --> 00:13:34,660
we're gonna use an optimizer,
a form of gradient descent,

240
00:13:34,990 --> 00:13:37,360
which I have a video on called
evolution of gradient descent.

241
00:13:37,780 --> 00:13:39,100
And we're going to use add a Delta,

242
00:13:39,130 --> 00:13:44,020
which is an adaptive learning rates
gradient descent strategy. Okay.

243
00:13:44,021 --> 00:13:47,320
And so once we have that, then we're going
to actually train the model, which is,

244
00:13:47,500 --> 00:13:50,440
which we do with one line of fit function.
So in tenser flow,

245
00:13:50,680 --> 00:13:55,150
this would be the akin to defining a
session if finding the grass and then

246
00:13:55,390 --> 00:13:58,840
iterating through defining the for loop
where we iterate through all the batches.

247
00:13:58,960 --> 00:14:01,600
Here we just defined the number of
batches as a parameter to the number of

248
00:14:01,640 --> 00:14:05,920
epochs who do it all in one line and
then the training data as the input.

249
00:14:06,940 --> 00:14:10,090
Okay. And so that's the
first key part is training.

250
00:14:10,300 --> 00:14:13,870
And so the other key part is exporting
the model. So once we train it,

251
00:14:14,020 --> 00:14:18,400
we have to export it. So we have to
export it into a Prodo buff file,

252
00:14:18,430 --> 00:14:21,670
write a PD file,
PB file is a saved,

253
00:14:21,960 --> 00:14:26,230
uh wait file that are
a model lives on that.

254
00:14:26,410 --> 00:14:28,810
That is our train are trained model,
right?

255
00:14:28,811 --> 00:14:33,340
So once we've tried on model model and
we save it as a pro but file so we can

256
00:14:33,341 --> 00:14:35,620
then use it later.
And what is the protocol file?

257
00:14:35,710 --> 00:14:40,210
Well it is a serialized version of our
model, right? With all of the Lord waits.

258
00:14:40,600 --> 00:14:43,690
And so we do that in this method,
right?

259
00:14:43,690 --> 00:14:48,220
And this is the way to do it for
android. Okay. So well actually this,

260
00:14:48,221 --> 00:14:53,221
this can apply to any type of use case
but it is a general purpose saving

261
00:14:56,090 --> 00:14:56,751
function,
right?

262
00:14:56,751 --> 00:14:59,720
So definitely saved this function for
later use cause you're going to use this a

263
00:14:59,721 --> 00:15:02,960
lot whether you want to use your model
on a server with tentraflow serving,

264
00:15:03,080 --> 00:15:07,190
whether you want to use it
on Android, ios. I know Ios
has the, you know, the new,

265
00:15:07,191 --> 00:15:09,820
um, what was it called? Cornell,

266
00:15:10,010 --> 00:15:13,760
which is really cool and I'll talk about
that as well. But android is more used.

267
00:15:13,790 --> 00:15:18,500
So I'm going to talk about android
first. So, right, so we write the graph.

268
00:15:18,920 --> 00:15:20,510
Okay,
we write it and when we,

269
00:15:20,540 --> 00:15:23,690
the right graph function is going to help
us define what we're going to call it

270
00:15:23,990 --> 00:15:27,250
and then we do the actual saving
and then we freeze it. And what,

271
00:15:27,251 --> 00:15:28,430
what do I mean by freeze?
Well,

272
00:15:28,431 --> 00:15:31,310
freezing means we're going to save
our weights wherever they are.

273
00:15:31,670 --> 00:15:35,630
So our weights are at a certain,
you know, they're all a certain,

274
00:15:36,140 --> 00:15:38,270
they all have certain numerical values,
right?

275
00:15:38,271 --> 00:15:40,840
So we're going to freeze them at
those values. That means it is,

276
00:15:40,880 --> 00:15:44,270
it's essentially like saying a declared
these weights as constant, right?

277
00:15:44,271 --> 00:15:47,570
So you can't change it that
they are immutable variables.

278
00:15:48,110 --> 00:15:50,540
And then so once the weights are saved,

279
00:15:50,630 --> 00:15:54,120
we're going to open it again
and then resave it, uh,

280
00:15:54,200 --> 00:15:58,520
using this optimized for
inference function, which
saves it as float 32 values,

281
00:15:58,640 --> 00:16:01,670
which are, which are the type of
values we need for android devices.

282
00:16:01,760 --> 00:16:05,210
So that's why we save it,
open it, and then re save it.

283
00:16:05,270 --> 00:16:09,260
And then we'll convert it into flow 32
and then receive it. So we do it twice.

284
00:16:09,470 --> 00:16:11,890
Okay. And we on the second
time we save it. We,

285
00:16:12,000 --> 00:16:16,160
we have it serialized two string values.
Okay.

286
00:16:16,370 --> 00:16:19,220
So that's it for that.
And then for our main function,

287
00:16:19,221 --> 00:16:21,620
that's where we call all of these
functions that we've just defined.

288
00:16:21,860 --> 00:16:25,820
We make sure that there is no output
folder and if there is a, if there's not,

289
00:16:25,821 --> 00:16:29,240
then we create one, we load up our
data, save it in these variables,

290
00:16:29,420 --> 00:16:30,253
build the model,

291
00:16:30,320 --> 00:16:35,120
train it on the data that we just load
it up and then export that train model

292
00:16:35,660 --> 00:16:38,750
and then export that train
model using the tension flows,

293
00:16:39,560 --> 00:16:44,560
native saver function and the
model has parameters to the uh,

294
00:16:44,690 --> 00:16:49,660
ouch directory. And then we can use
that same model in Android, in an,

295
00:16:49,710 --> 00:16:52,220
in our android happened. Okay.
So that's it for that part.

296
00:16:53,060 --> 00:16:55,280
And so we've got that part and now

297
00:16:57,230 --> 00:17:00,410
we're going to look at this bar. Okay.
So let's assume we've trained it.

298
00:17:00,440 --> 00:17:03,920
We trained our model, it's, it's
saved. Now what do we do? Well,

299
00:17:04,010 --> 00:17:08,270
we've downloaded our SDK and
our Ndk we need one more thing.

300
00:17:08,271 --> 00:17:11,360
We need something tensorflow specific,
right?

301
00:17:11,600 --> 00:17:15,050
The SDK and the NDK or
just a part of android.

302
00:17:15,080 --> 00:17:19,370
They have nothing to do with tensorflow.
Remember up here, this, this,

303
00:17:19,460 --> 00:17:22,850
this part right here with a tensorflow
underscore Jay and I that cc,

304
00:17:23,240 --> 00:17:24,560
that is what Google made.

305
00:17:24,680 --> 00:17:29,680
Google made this themselves so that we
can use tensorflow functions on Android,

306
00:17:30,230 --> 00:17:32,570
on device.
So that's what we need to download.

307
00:17:32,571 --> 00:17:36,200
And the way that we download this is to,
is to download an AA.

308
00:17:36,201 --> 00:17:40,410
Our file that has an android Arcov
are archived file. So what is it?

309
00:17:40,760 --> 00:17:43,130
This is a way of archiving
that dependencies,

310
00:17:43,160 --> 00:17:46,460
all of those that we need into
a convenience, a little rapper,

311
00:17:46,461 --> 00:17:49,650
like one file that we have to look at,
but under the hood,

312
00:17:49,940 --> 00:17:53,850
inside of this file or all of the uh,
c plus plus files.

313
00:17:53,880 --> 00:17:56,820
But we just don't have to look at them.
If it wasn't wrapped like this,

314
00:17:56,880 --> 00:17:59,400
then we would have to look
at all the c plus plus files.

315
00:17:59,610 --> 00:18:03,360
And we would use a tool like Baisil
to build all of them from source.

316
00:18:03,570 --> 00:18:05,250
But the great thing about Aa harm is,

317
00:18:05,430 --> 00:18:09,570
is that it is compatible with the
great old build system that is built,

318
00:18:09,600 --> 00:18:11,010
that is made for android.

319
00:18:11,250 --> 00:18:16,250
So we can just download it and then import
it into android studio and it's going

320
00:18:17,041 --> 00:18:20,760
to know all of those files and it's good
to put them in the right directories

321
00:18:20,761 --> 00:18:21,660
for us.
So,

322
00:18:21,730 --> 00:18:26,310
so there's a lot of great magic
happening inside of Android Studio,

323
00:18:26,400 --> 00:18:29,130
which is actually very useful,
right? It saves time and energy.

324
00:18:29,250 --> 00:18:34,080
And I've linked to a tutorial
on how to import an AAR file,

325
00:18:34,230 --> 00:18:37,590
which is very similar to a jar file.
So it made it and we would use a lot more,

326
00:18:37,830 --> 00:18:41,520
we would use jars a lot more.
But in a grateful,

327
00:18:41,550 --> 00:18:45,030
we use AAR files a lot more.
So yeah,

328
00:18:45,060 --> 00:18:48,180
check out this tutorial on how to
do that. It's very simple. Okay.

329
00:18:48,181 --> 00:18:51,690
So once we've done that, we've got
our tensorflow specific dependencies,

330
00:18:51,840 --> 00:18:56,760
we've got our SDK or Ndk, we've downloaded
this repository. Once we'd done that,

331
00:18:56,790 --> 00:19:00,690
we could just literally compile it and
run it on the simulator or the device if

332
00:19:00,691 --> 00:19:01,524
you have one.

333
00:19:01,530 --> 00:19:06,530
One of the reasons I chose this specific
repository to demo is because it

334
00:19:07,081 --> 00:19:10,800
doesn't require a device, right?
Not everyone has a device.

335
00:19:10,860 --> 00:19:15,510
You can do this in the simulator because
all of these classifier examples that I

336
00:19:15,511 --> 00:19:18,360
talked about,
like this one and this one,

337
00:19:20,970 --> 00:19:24,810
and this one require you to have a
camera and you can't use a camera on a

338
00:19:24,811 --> 00:19:26,670
simulator, right? You
need the actual device.

339
00:19:27,030 --> 00:19:30,330
It will be cool if you could
use a Webcam and, but you can't.

340
00:19:30,540 --> 00:19:35,340
So this is the most accessible
a repository that I could find.

341
00:19:35,850 --> 00:19:38,100
Okay, so let's go over the
steps in this tutorial.

342
00:19:38,101 --> 00:19:38,940
So what are we going to do here?

343
00:19:38,941 --> 00:19:42,240
We've talked about the python
file and how that works.

344
00:19:42,420 --> 00:19:44,530
Now let's go into the
android specific codes.

345
00:19:44,531 --> 00:19:48,450
So we're going to go into android studio
and go from a high level to a low level.

346
00:19:48,630 --> 00:19:51,600
So we'll go, we'll, we'll,
we'll go down two paths.

347
00:19:51,840 --> 00:19:55,620
The first path we'll go down is
the actual visualization path.

348
00:19:55,650 --> 00:19:59,940
So we'll start at the main activity and
then we'll iteratively go down the chain

349
00:19:59,941 --> 00:20:03,420
of dependencies, draw, model,
draw a render, and then drop you,

350
00:20:03,421 --> 00:20:06,840
which is the lowest level of
what's happening. And that's
the draw specific code.

351
00:20:07,140 --> 00:20:09,900
And then we'll go over the
machine learning specific code.

352
00:20:10,080 --> 00:20:14,130
So that is attentive low classification
class and in the classifier class,

353
00:20:14,131 --> 00:20:17,460
which it calls as the lowest level,
the classification class. Okay.

354
00:20:17,550 --> 00:20:21,030
So once we've done that, then we'll
be able to compile and run our model.

355
00:20:21,630 --> 00:20:23,970
So let's, let's start,
let's start off here, right?

356
00:20:25,260 --> 00:20:28,260
So here's the model link to it.
It's in the description.

357
00:20:28,320 --> 00:20:33,150
Check it out if you haven't yet.
Uh, okay. So let's see here.

358
00:20:33,600 --> 00:20:37,900
We have
this main activity.

359
00:20:38,020 --> 00:20:42,550
So, and by the way, I have commented
every single line in this code.

360
00:20:42,551 --> 00:20:46,990
So there is no excuse to not,
uh, learn how this code works.

361
00:20:47,230 --> 00:20:49,570
Even if you don't listen to this video,
check out the link.

362
00:20:49,750 --> 00:20:54,400
Every single line is commented very
well. Okay? Even the dependencies. So

363
00:20:56,410 --> 00:21:00,130
what we're going to do is in the
main activity we're going to,

364
00:21:01,810 --> 00:21:05,980
uh, we're going to define the main
activity. So that sounds counterintuitive.

365
00:21:06,100 --> 00:21:10,570
We're going to define the only view
that the user, uh, deals with, right?

366
00:21:10,870 --> 00:21:14,410
Which is this one in the XML file,
right?

367
00:21:14,590 --> 00:21:15,910
This is it right here.

368
00:21:17,470 --> 00:21:22,090
So the user will,
in this box right here,

369
00:21:22,330 --> 00:21:26,410
we'll draw with their finger,
the number,

370
00:21:26,590 --> 00:21:28,870
and then he'll hit,
they'll hit the test button.

371
00:21:29,110 --> 00:21:32,890
And then in this text you see
the classes here we've got,

372
00:21:32,950 --> 00:21:34,750
we've got three classes
we've got to draw you.

373
00:21:34,970 --> 00:21:39,580
That's where the user draws the number.
And then we've got this button,

374
00:21:39,581 --> 00:21:43,000
this clear button, which will, which
will clear the canvas if we want.

375
00:21:43,270 --> 00:21:47,500
We've got this detect button,
which is going to perform inference.

376
00:21:47,530 --> 00:21:51,220
So it's going to take that image as input,
send it to our tensorflow

377
00:21:52,810 --> 00:21:53,590
model,

378
00:21:53,590 --> 00:21:57,910
through the tensor flow tension
flows through the android Sdk,

379
00:21:57,911 --> 00:22:02,830
which can access the tensor flow
c Plus c plus plus file on device.

380
00:22:03,050 --> 00:22:06,610
And it's going to return the outputs
and it's going to return that output and

381
00:22:06,611 --> 00:22:09,810
it's going to output it inside of
this text to you. Okay? So that's,

382
00:22:09,840 --> 00:22:13,750
that's like what the visualization looks
like. It's a one view app. Very simple.

383
00:22:14,490 --> 00:22:17,140
And so, and here are trained
models, by the way, write this.

384
00:22:17,410 --> 00:22:21,460
We've got to train models
and not just to train models.

385
00:22:21,550 --> 00:22:23,200
We have a set of labels,
right?

386
00:22:23,230 --> 00:22:27,010
Zero through nine so that we know what
are the possible class labels cause this

387
00:22:27,011 --> 00:22:30,610
is a multi-class classification problem.
And so we've got to train models,

388
00:22:30,611 --> 00:22:33,850
one for Ken Ross and one for tension
flow and we'll be using the care os one

389
00:22:33,851 --> 00:22:37,660
just because, uh, why not? But the
tensor flow one would probably be,

390
00:22:37,720 --> 00:22:41,680
will be more accurate, but it
takes longer to train. Okay.

391
00:22:41,681 --> 00:22:45,760
So let's go into this main activity and
look at what the, what is happening here,

392
00:22:45,850 --> 00:22:49,480
right? So we're going to
import a bunch of, um,

393
00:22:50,470 --> 00:22:52,690
drawing classes.
By the way,

394
00:22:52,691 --> 00:22:57,691
the activity is how the user interacts
with the android architecture.

395
00:22:58,211 --> 00:23:01,630
With the android APP. Almost all
activities interact with the user.

396
00:23:01,631 --> 00:23:04,720
So the activity class takes care
of creating a window for you,

397
00:23:04,810 --> 00:23:06,100
which you can place or Ui,

398
00:23:06,310 --> 00:23:09,640
which in which you can place your
Ui with the set content view.

399
00:23:09,760 --> 00:23:13,930
When we initialize it in the, uh, on
create function, right? Right here.

400
00:23:14,410 --> 00:23:18,040
Okay.
So you know,

401
00:23:18,060 --> 00:23:21,310
if the more complex your happies the
more activities that you would have,

402
00:23:21,430 --> 00:23:25,540
we've got one activity and that activity
is drawing the number and hitting the

403
00:23:25,541 --> 00:23:28,870
teks or hitting clear and repeating that.
That's the only activity.

404
00:23:29,020 --> 00:23:33,340
And in this activity, all of those actions
are encapsulated. So we've got that.

405
00:23:33,370 --> 00:23:37,120
And then, uh, we've got a
bunch of other dependencies.

406
00:23:37,150 --> 00:23:42,010
Each of them is going to be
used for classification as
well as drawing elements,

407
00:23:42,011 --> 00:23:42,161
right?

408
00:23:42,161 --> 00:23:47,161
So it's split basically the dependencies
either draw what we're looking for or

409
00:23:48,350 --> 00:23:52,670
let me make this bigger.
Can I make it better?

410
00:23:53,000 --> 00:23:57,080
Yes, yes I can. You can
do anything that you dream

411
00:23:58,970 --> 00:24:01,870
unless it's crazy. Well,
even then you could, uh,

412
00:24:02,000 --> 00:24:05,810
with the right amount of computation
and training. Okay. Okay.

413
00:24:05,840 --> 00:24:10,430
So let's see. Okay, so now
let's look at the main activity.

414
00:24:10,550 --> 00:24:14,330
So for the main activity, we've
got our UI elements, right?

415
00:24:14,331 --> 00:24:17,420
Those buttons that I just showed
you as well as the views, right?

416
00:24:17,750 --> 00:24:21,740
We've got it set up views and got the UI
elements and we've got our coordinates.

417
00:24:21,741 --> 00:24:23,690
So let's talk about each
of these in an order. Okay,

418
00:24:23,691 --> 00:24:28,280
so the pixel width is the width
of our image 28 by 28 pixels.

419
00:24:28,640 --> 00:24:30,350
The button. All right,
we've got two buttons.

420
00:24:30,351 --> 00:24:33,620
The class button is the detect button.
The clear button is a clear button.

421
00:24:33,910 --> 00:24:36,470
The text view is what app,
which shows the output.

422
00:24:36,471 --> 00:24:40,790
And the m classifiers is an array list,
which is an array with some, you know,

423
00:24:40,830 --> 00:24:45,080
it's, it's basically built on top
of the array class. Yeah, it's a,

424
00:24:45,081 --> 00:24:49,640
it's a list of so you can perform, you
know, getters and setters on an array.

425
00:24:50,140 --> 00:24:53,270
Uh, and so it's a list of
classifiers ever we're going to use.

426
00:24:53,420 --> 00:24:57,080
So the reason that they list is because
we're going to use to classifiers and

427
00:24:57,081 --> 00:24:59,900
this and this repository, not
just one. We're going to use two,

428
00:25:00,080 --> 00:25:03,980
one for tennis as low and one for care
Os. So we can compare the results, right?

429
00:25:03,981 --> 00:25:06,650
So that's why it's an array list.
And then we've got our views, right?

430
00:25:06,651 --> 00:25:11,540
So we've got our draw view, which is the
big view and we've got our draw model,

431
00:25:11,541 --> 00:25:13,880
which is the smaller version
where the user actually draws.

432
00:25:14,210 --> 00:25:18,710
And then we've got our set of points and
these are coordinance that the user is,

433
00:25:18,800 --> 00:25:21,860
is tapping that the user taps.
Okay.

434
00:25:22,130 --> 00:25:25,670
And then we've got the private
set of coordinates, which
is like a copy of those.

435
00:25:25,670 --> 00:25:28,460
But internally that we can then
modify inside of this class,

436
00:25:28,461 --> 00:25:32,630
but they're not exposed to publicly.
So other classes can't interact with them.

437
00:25:32,990 --> 00:25:35,180
And this is just good
programming practice too,

438
00:25:35,360 --> 00:25:40,360
to have your doc to have your variables
that are not going to interact with

439
00:25:40,491 --> 00:25:44,840
other classes, set them to privates
because you know, otherwise

440
00:25:46,550 --> 00:25:50,450
things can happen that you don't expect
where you're calling a class but it's

441
00:25:50,451 --> 00:25:52,610
not. And then it's going
to modify some variables.

442
00:25:52,700 --> 00:25:55,490
But because you haven't said it's
private, it's going to, yeah,

443
00:25:55,580 --> 00:25:57,380
because you haven't set that
some variables to private.

444
00:25:57,470 --> 00:25:59,540
It's going to modify those variables
and you don't want that to happen,

445
00:25:59,720 --> 00:26:01,550
so you have to set them to private.
Okay?

446
00:26:01,760 --> 00:26:05,360
So we've got that and now
and our on create method,

447
00:26:05,570 --> 00:26:08,480
which is like the basic building
block for an activity class,

448
00:26:08,570 --> 00:26:12,410
it's going to run only once for the
entire lifecycle of the activity, right?

449
00:26:12,680 --> 00:26:16,610
And we've got a bunch of these activities,
specific lifecycle methods,

450
00:26:16,880 --> 00:26:20,930
but the first one is on create and it
just runs once. Okay. So we've got that.

451
00:26:20,931 --> 00:26:24,620
And so now let's go ahead and
create an instance for this,

452
00:26:24,770 --> 00:26:27,380
for this main activity. And once we
have that, we're going to say, okay,

453
00:26:27,381 --> 00:26:30,920
so let's get that drawing view.
So we, we define this in Xmls,

454
00:26:30,960 --> 00:26:34,600
we're going to call it by its ID that
we define an XML store in drawing.

455
00:26:34,601 --> 00:26:37,700
You do the same for the model.
And then we're going to set the model.

456
00:26:37,730 --> 00:26:40,690
So we're going to take that view and
set its model to that draw models,

457
00:26:40,820 --> 00:26:43,830
super missile,
encapsulating it inside of that view.

458
00:26:44,040 --> 00:26:47,310
And then we're going to create an entouch
listener to activate whenever the user

459
00:26:47,311 --> 00:26:48,960
taps it. And then, uh,

460
00:26:49,110 --> 00:26:53,010
we're also gonna do the same for the
clear button as well as the detect button.

461
00:26:53,190 --> 00:26:55,260
Okay. And then we're going to
do it for the view. So this is,

462
00:26:55,290 --> 00:26:59,370
this is just us wiring
our XML to our class,

463
00:26:59,371 --> 00:27:03,360
our program out of class so we
can then manipulate these values.

464
00:27:05,070 --> 00:27:06,420
And then we've got a
couple of functions here.

465
00:27:06,600 --> 00:27:08,760
We've got our own resumed
function, which is, you know,

466
00:27:08,761 --> 00:27:12,480
assuming that the user has
gone, gone, gone out of the APP,

467
00:27:12,780 --> 00:27:15,210
it's going to call this
and call these methods,

468
00:27:15,450 --> 00:27:19,350
which basically it's a way of saving
our state and making sure that our APP

469
00:27:19,351 --> 00:27:23,400
doesn't crash. And these are
just, uh, basically like, um,

470
00:27:23,580 --> 00:27:27,090
they're like exceptions.
They're like android wide exception rules.

471
00:27:28,020 --> 00:27:31,080
So it's like saving where we are.
And so we've got that.

472
00:27:31,081 --> 00:27:32,280
And now let's load our model.

473
00:27:32,310 --> 00:27:36,120
So this is going to create a model object
in memory using the same tensorflow

474
00:27:36,150 --> 00:27:38,820
proto buff model file,
which contains all of the learn weights.

475
00:27:39,000 --> 00:27:42,780
So it's going to take these weights and
it's what you saved them into this m

476
00:27:42,781 --> 00:27:46,560
classifiers array list array.
So we can then use them.

477
00:27:46,561 --> 00:27:50,580
So it takes those saved weights in local
storage on android device and saves

478
00:27:50,581 --> 00:27:53,770
them into memories. So it's an in
memory array list that we can then, um,

479
00:27:54,510 --> 00:27:55,530
perform inference with.

480
00:27:56,780 --> 00:27:59,730
And so we have a try catch function
for that because if they're not there,

481
00:27:59,731 --> 00:28:03,540
then we have to say, hey, there's an error
initializing these classifiers. Okay.

482
00:28:03,541 --> 00:28:05,460
So then we've got this,
uh, on click methods.

483
00:28:05,461 --> 00:28:08,430
So this is whenever the user clicks
on anything. So we have a bunch of,

484
00:28:08,460 --> 00:28:12,090
if then statements to define where
if the user clicks what happens.

485
00:28:12,450 --> 00:28:16,410
So if the user clicks the clear button,
then we have to clear the model,

486
00:28:16,440 --> 00:28:19,530
reset it and invalidate any
new entries the user does.

487
00:28:19,890 --> 00:28:22,980
And then we set the text to empty
like nothing is there anymore.

488
00:28:23,610 --> 00:28:26,310
Now if the user clicks
the, uh, classified button,

489
00:28:26,370 --> 00:28:29,040
then we're going to take
the array of pixels, right?

490
00:28:29,041 --> 00:28:32,250
So we get the pixel data of everything
that a user has drawn up to that point.

491
00:28:32,700 --> 00:28:35,580
And we store it in a pixel
array and then we say, okay,

492
00:28:35,581 --> 00:28:38,610
we'll initialize a string is empty because
we're later going to fill it with the

493
00:28:38,611 --> 00:28:40,590
help of prediction. And then
we're going to say, okay,

494
00:28:40,591 --> 00:28:45,270
so for each of the classifiers, for
both tensorflow and chaos, recognize.

495
00:28:45,330 --> 00:28:48,150
And so I'll, I'll, I'll show you
what this recognized function does,

496
00:28:48,420 --> 00:28:53,340
but recognize or classify what those set
of pixels bar, you know, a zero, a one,

497
00:28:53,341 --> 00:28:55,350
a two or three. What, what is this? What,

498
00:28:55,390 --> 00:28:58,890
what did they use or draw and then
take that label. And if it's empty,

499
00:28:58,891 --> 00:29:03,390
then I'll an NEC, um, a question
mark. If it's not empty,

500
00:29:03,391 --> 00:29:08,370
then output the label and set
that into the text. Phew. Okay.

501
00:29:08,460 --> 00:29:10,830
So that's the highest
level of what is happening.

502
00:29:11,400 --> 00:29:15,780
And so now we've got this entouch method,
which when the user moves their finger,

503
00:29:15,810 --> 00:29:19,450
draw a line accordingly in
that direction. Okay? So,

504
00:29:21,520 --> 00:29:23,910
so we take the action and
we store it as an integer.

505
00:29:24,270 --> 00:29:28,800
And the actions all have predefined in,
uh, like in memory or not in memory, but,

506
00:29:29,120 --> 00:29:31,320
uh, that, that are a part of
android. So check this out.

507
00:29:31,321 --> 00:29:33,720
So if the action that the user has,

508
00:29:33,780 --> 00:29:35,850
so basically if the user
has touched the screen,

509
00:29:36,120 --> 00:29:40,500
which we defined with motion events,
action down, which is a zero.

510
00:29:40,501 --> 00:29:43,420
See these are all
predefined and Androids Sdk.

511
00:29:43,630 --> 00:29:47,380
So basically if the user has touched
the screen, again drawing the line,

512
00:29:47,830 --> 00:29:52,430
but if a user has moved, if the
user has started moving, then uh,

513
00:29:52,600 --> 00:29:56,680
start then start actually drawing
the line in a different direction.

514
00:29:56,860 --> 00:30:00,400
So touching it just initializes
the line drawing and moving.

515
00:30:00,401 --> 00:30:03,760
It actually begins drawing
the line more than just a dot.

516
00:30:04,240 --> 00:30:07,540
And then if a finger is lifted,
action up, then stop drawing.

517
00:30:09,430 --> 00:30:11,260
Okay, so then, uh, these are,

518
00:30:11,320 --> 00:30:13,930
these are the implementations of
these functions that we just find the,

519
00:30:14,100 --> 00:30:18,510
the price touchdown as well as the press.
Such move. So for the press touchdown,

520
00:30:18,600 --> 00:30:20,770
we get the coordinates of
wherever the user has touched,

521
00:30:21,010 --> 00:30:22,840
calculate those positions in memory,

522
00:30:23,110 --> 00:30:27,550
and then start drawing the line using
where we were before and where we are now.

523
00:30:28,060 --> 00:30:29,770
And then in this main drawing function,

524
00:30:29,771 --> 00:30:32,050
this is when the user is
actually moving their finger.

525
00:30:32,380 --> 00:30:34,090
Take both of those sets of coordinates,

526
00:30:34,210 --> 00:30:36,310
where we were before and where we are now.

527
00:30:36,520 --> 00:30:40,360
And then draw a line between those two
points and keep doing that every time.

528
00:30:41,200 --> 00:30:44,890
Okay. So yeah, and then for press such up,

529
00:30:44,920 --> 00:30:47,320
we just take draw models
and line function,

530
00:30:47,321 --> 00:30:50,380
which is going to help us
stop drawing the line. Okay.

531
00:30:50,381 --> 00:30:52,060
So that's the main activity.

532
00:30:52,330 --> 00:30:57,250
And so notice how we had the straw model
function that is continuously used all

533
00:30:57,251 --> 00:30:59,500
over the place. So let's look at
what's happening there, right?

534
00:31:00,190 --> 00:31:02,200
What is happening in the
straw model function? Well,

535
00:31:03,840 --> 00:31:05,250
in the draw model function

536
00:31:11,820 --> 00:31:12,750
we are,

537
00:31:20,720 --> 00:31:22,190
okay,
so we are drawing the model.

538
00:31:22,220 --> 00:31:23,720
That's what we're doing in
the draw model function.

539
00:31:23,960 --> 00:31:28,220
So it's a collection of getter and setter
functions that we can use later on to

540
00:31:28,221 --> 00:31:32,750
draw a character model. So we'll
initialize it with a line element,

541
00:31:32,930 --> 00:31:36,080
which is a, it's just two
values, the x and y coordinates.

542
00:31:36,320 --> 00:31:39,170
And then we'll have an internal
representation as well to manipulate those

543
00:31:39,171 --> 00:31:39,800
values.

544
00:31:39,800 --> 00:31:43,940
So we'll manipulate them and then we'll
return those manipulated values back to

545
00:31:43,941 --> 00:31:48,830
the public x y values. So then we can
then draw them on XML in the XML file.

546
00:31:49,220 --> 00:31:50,540
So for a single line,

547
00:31:50,541 --> 00:31:55,460
so we have a private line class
align consists of elements. Okay.

548
00:31:55,580 --> 00:32:00,410
So we have a model, a model consists of
lines and align consists of elements.

549
00:32:00,680 --> 00:32:02,660
Okay. So what is uh, so let
me talk about each of these.

550
00:32:02,920 --> 00:32:07,670
An element is the line from
point a to point B, right?

551
00:32:07,700 --> 00:32:10,190
So there are a bunch of
elements in a single line.

552
00:32:10,490 --> 00:32:14,630
Align is everything that you draw from
when you start to when you finish.

553
00:32:14,720 --> 00:32:17,780
So aligned to be an entire number.
Or it could be like, you know,

554
00:32:17,781 --> 00:32:22,390
you draw the first, Oh that's aligned and
then you draw the rest of the, you know,

555
00:32:22,400 --> 00:32:25,600
if you're trying to draw a six [inaudible]
oh and then you are the, you know,

556
00:32:25,640 --> 00:32:26,473
the curve.

557
00:32:26,600 --> 00:32:30,290
Both of those are two lines and inside
each of those lines or set of elements

558
00:32:30,291 --> 00:32:35,060
right? From point a to point B. So
that's, and then all of that is a model.

559
00:32:35,210 --> 00:32:38,510
Okay. The draw model. Okay.
So model line element.

560
00:32:38,600 --> 00:32:43,070
That's the of of what this looks like.
Object hierarchy. Okay. So we've got that.

561
00:32:43,100 --> 00:32:45,200
And so now let's

562
00:32:47,870 --> 00:32:49,940
talk about these other functions.

563
00:32:49,941 --> 00:32:54,550
So this is our current line
are the width and height of,

564
00:32:54,590 --> 00:32:58,310
of that line, height of that line. And
then we have the list of lines, right?

565
00:32:58,311 --> 00:33:02,020
So inside of a model there are lines
and inside of the lines there aren't

566
00:33:02,021 --> 00:33:04,970
elements. So we're defining a
raise for each of these objects.

567
00:33:05,720 --> 00:33:09,200
And so we've got to set a getter and
setter functions for each of them. Okay?

568
00:33:09,201 --> 00:33:13,370
So start line and line, add line elements,
get the sides of it, get the line,

569
00:33:13,371 --> 00:33:16,700
and then clear. Okay, so
that's it for our draw model.

570
00:33:16,880 --> 00:33:19,070
And then we've got draw a renderer.
So we're,

571
00:33:19,220 --> 00:33:21,290
so we're going down even lower level.

572
00:33:21,680 --> 00:33:25,970
So we're going down the
rabbit hole of lines.

573
00:33:26,390 --> 00:33:29,180
Okay?
So for our draw a renderer class,

574
00:33:30,410 --> 00:33:33,050
we're going to say let's draw
all these lines to a canvas.

575
00:33:33,320 --> 00:33:37,490
So we've got our render model. And so this
is just straight up drawing functions.

576
00:33:37,491 --> 00:33:40,400
So this is the straight up
drawing function. So all of those

577
00:33:42,500 --> 00:33:45,920
functions that we define in this main
activity as well as Deidre model,

578
00:33:46,340 --> 00:33:50,720
they are high level functions.
They're not actually drawing them to XML.

579
00:33:50,960 --> 00:33:54,260
This is how we're programmatically
deciding what drawing looks like.

580
00:33:54,770 --> 00:33:56,150
This is how we decided
with drawing looks like.

581
00:33:57,140 --> 00:33:59,120
But then in this draw a render function,

582
00:33:59,330 --> 00:34:03,350
we take all of that programmatic
logic and we apply it to XML.

583
00:34:03,440 --> 00:34:07,790
So we're going to directly manipulate
XML elements in this class or so we're

584
00:34:07,820 --> 00:34:10,010
essentially drawing lines on canvas.

585
00:34:10,460 --> 00:34:13,950
So we'll first set this set anti alias,
um,

586
00:34:14,060 --> 00:34:17,030
function to true because we want
to minimize distortion artifacts.

587
00:34:17,630 --> 00:34:19,850
It's just good practice to do.

588
00:34:20,930 --> 00:34:24,110
And then we get the sides of the line
to draw. And then given that side,

589
00:34:24,140 --> 00:34:27,410
we create a for loop. So get the
whole line from the model object,

590
00:34:27,620 --> 00:34:31,430
set its color to black and then get the
first of many lines that make up the

591
00:34:31,490 --> 00:34:35,570
overall line. So we get the element
size and we say that's a single element.

592
00:34:35,810 --> 00:34:37,580
And if the element size is less than one,

593
00:34:37,790 --> 00:34:41,800
that means that the user hasn't drawn
anything. So just skip it. But if it,

594
00:34:41,801 --> 00:34:45,590
if these, if the user has then
store that in an element object,

595
00:34:45,680 --> 00:34:49,430
get the coordinates of, of the, of that
element object, the x and y coordinates.

596
00:34:49,790 --> 00:34:53,300
And so for each coordinate in the lines
of for all of the elements in the line,

597
00:34:53,450 --> 00:34:56,870
we have a set of elements in an array,
right for a single line,

598
00:34:57,320 --> 00:35:01,250
each of those elements and the coordinates
and then draw them on the canvas

599
00:35:01,370 --> 00:35:03,500
using where we were before,
where we are now.

600
00:35:03,560 --> 00:35:07,190
And then this paints a object which
is going to create, make those,

601
00:35:07,250 --> 00:35:09,290
make that drawing black.
And then we,

602
00:35:09,560 --> 00:35:12,200
and then we make the new law
that we make the new coordinates,

603
00:35:12,201 --> 00:35:15,290
the old coordinates in this
last x and laughs wife function.

604
00:35:15,680 --> 00:35:18,920
And so that's draw renderer and now
we'll go to the lowest level for drawing,

605
00:35:18,921 --> 00:35:21,710
which is draw a view.
So for draw view,

606
00:35:21,711 --> 00:35:25,360
I mean this is almost like unnecessary
to go over because it's so low level I,

607
00:35:25,380 --> 00:35:28,220
this is just one of those classes that
you should just import and like not even

608
00:35:28,221 --> 00:35:31,220
worry about because there's not really
any machine learning specific knowledge

609
00:35:31,221 --> 00:35:34,060
here. But I'll go over anyway.
Just had a high level. Uh,

610
00:35:34,640 --> 00:35:39,410
we want to first a reset the use,
so it's empty to empty the drying.

611
00:35:39,411 --> 00:35:40,890
So we'll hit this reset function,

612
00:35:41,130 --> 00:35:44,550
which is going to take that bit map
image of whatever the user drew, drew,

613
00:35:44,730 --> 00:35:47,880
and then empty, empty out all the
pixel values so they're not black.

614
00:35:47,881 --> 00:35:51,270
So they're white again. And then,
uh, this is the set up function.

615
00:35:51,271 --> 00:35:54,830
It's a private function where we define
the views size, the size of the models,

616
00:35:54,840 --> 00:35:56,580
bitmap,
and then scale it.

617
00:35:56,580 --> 00:36:00,870
So it's properly scaled for whatever
android device that you're on. Again,

618
00:36:00,871 --> 00:36:04,260
this would be automatic, like ios
would do this automatically, uh, but,

619
00:36:05,510 --> 00:36:07,680
and when the user begins drawing,
initialize a model,

620
00:36:07,681 --> 00:36:12,360
render class and draw it on the canvas.
So this is where we,

621
00:36:12,390 --> 00:36:15,600
this is where we initialize that draw a
render a class that we just talked about.

622
00:36:15,750 --> 00:36:18,300
This is where we take it and we,
we apply it.

623
00:36:19,620 --> 00:36:24,180
And so then we draw, it's
whatever, whatever it drew,
whatever it says to draw,

624
00:36:24,181 --> 00:36:26,430
we then draw it.
And then we kept,

625
00:36:26,460 --> 00:36:30,750
so the rest of these are
basically calculating the
position of the finger on the

626
00:36:30,751 --> 00:36:32,700
screen. So very, very detailed. X,

627
00:36:32,701 --> 00:36:35,010
y coordinates of where your
finger is on the screen.

628
00:36:35,340 --> 00:36:37,470
And then we draw the
canvas using the bitmap,

629
00:36:37,530 --> 00:36:42,240
which is the image that we just drew.
We release it, which is just memory,

630
00:36:42,360 --> 00:36:43,380
a memory management.

631
00:36:43,381 --> 00:36:47,580
And so we released it from memory using
this recycling function because android

632
00:36:47,581 --> 00:36:49,260
uses garbage collection,

633
00:36:49,560 --> 00:36:53,420
which is a type of memory
management technique.

634
00:36:53,660 --> 00:36:58,660
So apple uses retain cycles and
android uses garbage collection.

635
00:36:59,760 --> 00:37:00,593
Okay.

636
00:37:00,900 --> 00:37:04,130
Two different methods.
Garbage collection is happening.

637
00:37:04,490 --> 00:37:06,560
I'm refreshing my mobile knowledge here.

638
00:37:06,830 --> 00:37:11,000
Garbage collection is
happening at runtime as um,

639
00:37:13,040 --> 00:37:17,690
automatic reference counting on high
west is happening at compiled time.

640
00:37:17,720 --> 00:37:20,420
That's difference. Uh,
so which one's better?

641
00:37:22,050 --> 00:37:24,580
There are pros and cons of both.
That's out of the scope of this.

642
00:37:24,581 --> 00:37:27,410
This is about machine learning,
but if you guys want more,

643
00:37:27,890 --> 00:37:31,400
let me know of this stuff because I've
done mobile development quite a bit

644
00:37:31,401 --> 00:37:35,990
before. Okay. So, uh, so then we
get the pixel data and then we,

645
00:37:36,020 --> 00:37:39,320
for the potential floats input. So this
is how we define the pixel. You know,

646
00:37:39,350 --> 00:37:41,350
every single pixel, we
have a for loop, we're t,

647
00:37:41,420 --> 00:37:44,630
we're going to every single pixel and
we're storing it in that pixel array.

648
00:37:44,750 --> 00:37:49,200
So for all of those other functions,
we can use that pixel array for both draw,

649
00:37:49,201 --> 00:37:54,050
model, draw renderer, uh, and even this,
this, this function drop drove you.

650
00:37:55,010 --> 00:37:57,920
Okay? So that's for all
of the uh, view logic.

651
00:37:57,921 --> 00:38:00,680
And now let's talk about the
machine learning specific logic.

652
00:38:01,760 --> 00:38:05,450
So it's are at the highest level, which
is our tensorflow classification class,

653
00:38:05,680 --> 00:38:09,140
and then our classifier, and then
our classification class. Okay?

654
00:38:09,500 --> 00:38:12,290
So for tens of classification we'll say,

655
00:38:16,130 --> 00:38:17,780
how does this work?
Okay,

656
00:38:17,781 --> 00:38:22,330
so why don't you have android native
native android from a bunch of standard

657
00:38:22,331 --> 00:38:26,300
hint android development kit
Sdk, Sdk, functionality is being,

658
00:38:26,540 --> 00:38:29,780
is being important here.
And then we've got this interface.

659
00:38:29,781 --> 00:38:33,860
So remember that c plus plus file
that I talked about right here,

660
00:38:34,960 --> 00:38:38,830
this is what we import it. Okay? So
this is coming directly from Google.

661
00:38:39,190 --> 00:38:43,840
This is what they wrote as the bridge
between tensorflow c plus plus an android.

662
00:38:44,080 --> 00:38:46,510
We import it right here.
Maybe by Google. Okay.

663
00:38:46,511 --> 00:38:51,370
And so what we're going to do is
we're going to implement this class.

664
00:38:51,400 --> 00:38:55,510
So this class that we're going to create
is an implementation of classifier.

665
00:38:55,630 --> 00:38:56,020
Wait,

666
00:38:56,020 --> 00:39:01,020
what is classifier classifier is the
public interface for how our APP interacts

667
00:39:02,471 --> 00:39:06,100
with the classifier. Okay? So that
so publicly we give it a string.

668
00:39:06,160 --> 00:39:08,980
So whatever we name it as well
as the recognize function,

669
00:39:08,981 --> 00:39:13,330
that is the one public function that we
are making visible to all the rest of

670
00:39:13,331 --> 00:39:16,960
the APP. The recognized function, which
just going to take the pixel array,

671
00:39:16,990 --> 00:39:21,490
has input of the image and then return
the classification. And like the output,

672
00:39:21,491 --> 00:39:24,490
this is a zero 26% chance, zero. You know,

673
00:39:25,090 --> 00:39:28,960
for what's a hundred minus 26 74% chance,

674
00:39:28,961 --> 00:39:32,560
not zero or at 74% percent
chance one or whatever it is.

675
00:39:33,280 --> 00:39:37,630
So
yeah,

676
00:39:37,870 --> 00:39:41,860
so it's an implementation of that. So
that's the interface for this class. Okay.

677
00:39:41,861 --> 00:39:45,400
So what we're gonna do is we're
going to say, okay, so this is old.

678
00:39:45,430 --> 00:39:46,900
This is the threshold that we defined.

679
00:39:47,080 --> 00:39:49,870
So it only returns if it's
at least this confidence,

680
00:39:49,871 --> 00:39:54,070
it's gotta be at least 10% confidence of
what it's looking at. And if it's not,

681
00:39:54,071 --> 00:39:55,540
don't return anything,
right?

682
00:39:55,541 --> 00:39:59,110
It's gotta be at least 10% confident of
whatever output probability that it's

683
00:39:59,111 --> 00:39:59,944
predicting.

684
00:40:00,520 --> 00:40:03,670
And so we have a set of variables here
that we're going to talk about when we

685
00:40:03,671 --> 00:40:05,980
implement them. So let's just go
through each of these functions.

686
00:40:06,270 --> 00:40:10,480
So the first function is the read
labels function. So given a saved,

687
00:40:10,510 --> 00:40:11,350
drawn model,

688
00:40:11,500 --> 00:40:15,280
let's read all the classification labels
that are stored and write them to our

689
00:40:15,340 --> 00:40:19,930
in memory labels list. So remember
we have this, um, classification.

690
00:40:20,100 --> 00:40:20,933
Where is it?

691
00:40:25,620 --> 00:40:26,453
Okay

692
00:40:26,870 --> 00:40:30,200
labeled file right here. You know, so
this is what we read it into memory.

693
00:40:31,100 --> 00:40:35,900
So into memory, we say.
So for all those labels,

694
00:40:36,020 --> 00:40:38,060
initialize them as an array list,
read them,

695
00:40:38,150 --> 00:40:41,660
add them to our Ra and return those
labels. That's what that does.

696
00:40:42,110 --> 00:40:46,810
So this is the, uh, this is the
second most important function. This,

697
00:40:46,820 --> 00:40:50,120
this recognizes the first most important,
but as the second most important.

698
00:40:50,300 --> 00:40:52,790
And so what this does is given a model,
okay,

699
00:40:52,791 --> 00:40:55,970
so given a model and it's
labeled file and as Metadata,

700
00:40:56,240 --> 00:41:00,350
let's fill out a class of our object with
all of the necessary made it Metadata,

701
00:41:00,560 --> 00:41:02,450
including the output prediction,
right?

702
00:41:02,451 --> 00:41:06,410
So these are all the attributes of the
model that we can basically get to inset

703
00:41:06,890 --> 00:41:09,200
later on.
So we've got a name that we defined,

704
00:41:09,320 --> 00:41:12,650
we've got the input by name for the input.
So okay,

705
00:41:12,680 --> 00:41:15,530
so we've got a name for the model.
We have a name for input data,

706
00:41:15,680 --> 00:41:17,180
we have her name for help,
a prediction.

707
00:41:17,600 --> 00:41:21,980
We have art labels that we just read
right from, from storage into memory.

708
00:41:22,340 --> 00:41:25,430
And then we have the models path that we,
and that's where the,

709
00:41:25,700 --> 00:41:29,960
that's where all the raw assets, asset
files, car, as well as where the model is.

710
00:41:30,350 --> 00:41:34,850
And so that's what he's inner
inference comes into play, right?

711
00:41:34,940 --> 00:41:37,730
That comes from Google.
Look, this, all this stuff.

712
00:41:38,900 --> 00:41:42,530
This is how we take data and feed it
to the model. This is the gateway.

713
00:41:42,780 --> 00:41:45,290
This class is 10 or tensorflow inference.

714
00:41:45,291 --> 00:41:48,710
Interface class is the
gateway between Androids,

715
00:41:48,770 --> 00:41:53,330
Java functionality and tension flows,
c plus plus functionality maybe by Google.

716
00:41:54,170 --> 00:41:57,800
And so this is what we define what that
is and we'll send it to the TF helper

717
00:41:57,860 --> 00:42:01,040
attribute. And so how big is the
input? Well, we'll define that.

718
00:42:01,280 --> 00:42:05,210
We'll pre allocate a buffer for the pro
bowl. Those outputs that come out, right,

719
00:42:05,211 --> 00:42:09,260
it's not going to just one output. This
is a multi-class classification problem.

720
00:42:09,261 --> 00:42:12,920
We have a set of outputs,
so stored in the hall and an array list.

721
00:42:14,420 --> 00:42:17,450
Okay, so then, so that's the
second most important class.

722
00:42:17,451 --> 00:42:20,300
And then remember this public class,
we're just exposing the name,

723
00:42:20,330 --> 00:42:23,900
the string that we define.
And this is the first most important,

724
00:42:24,410 --> 00:42:29,000
not class function. This is the first
most important function class function,

725
00:42:29,020 --> 00:42:32,950
do op, Dutch, English. I'm just switching
between languages and my mind is like,

726
00:42:34,700 --> 00:42:38,120
okay, so then for our recognized class,
which is the most important class,

727
00:42:38,360 --> 00:42:42,140
here's how it works. Using the
interface, we give them the input name,

728
00:42:42,200 --> 00:42:44,780
the raw pixels from the
drawing and the input size.

729
00:42:44,900 --> 00:42:48,500
So we feed that into our tensorflow
flow interface, right? That class.

730
00:42:48,890 --> 00:42:52,280
And then we check the probability
values. So we say, okay,

731
00:42:52,281 --> 00:42:53,990
if we want to keep those probability,

732
00:42:54,110 --> 00:42:56,900
if we want to keep those
probability values in memory,

733
00:42:56,930 --> 00:42:57,890
which is why it's a bullying,

734
00:42:58,130 --> 00:43:03,020
then feed it that those probabilities as
an array of probabilities, which we do,

735
00:43:03,080 --> 00:43:06,530
which, so we're going to send it to true
and then we get the possible outputs.

736
00:43:06,620 --> 00:43:10,340
So when we run it, we s we set
these output named as the perimeter,

737
00:43:10,490 --> 00:43:13,010
so it's going to fill
those applique values. Um,

738
00:43:13,610 --> 00:43:18,530
so then we get the possible outputs
using the names which are here,

739
00:43:18,800 --> 00:43:19,633
and

740
00:43:23,540 --> 00:43:25,220
this is where we pre allocate the buffer.

741
00:43:28,420 --> 00:43:32,170
So that, so then we get those possible
outputs, uh, via the fetch method.

742
00:43:32,171 --> 00:43:35,950
So we first run using those output
names and then we fetch it. Okay.

743
00:43:35,951 --> 00:43:38,650
So we take the actual
output and the output names,

744
00:43:38,651 --> 00:43:40,540
which is the list that
we fill with the output,

745
00:43:40,541 --> 00:43:45,160
whatever the output prediction is,
we fill it into that output names a list.

746
00:43:45,580 --> 00:43:49,990
And then we find the best classification
from those outputs. So we say,

747
00:43:50,020 --> 00:43:52,060
let's initialize our
classification object,

748
00:43:52,420 --> 00:43:56,080
and then we say find the
best classification for each
of the output predictions.

749
00:43:56,110 --> 00:43:59,020
If it's above the threshold
that we defined for accuracy,

750
00:43:59,350 --> 00:44:02,500
right it out to the view in
that text view. So we say, okay,

751
00:44:02,501 --> 00:44:05,470
for the length of each output,
print out the output,

752
00:44:05,590 --> 00:44:07,690
print out the labels just
for us for debugging.

753
00:44:07,990 --> 00:44:10,210
And then if the outputs is
greater than the threshold,

754
00:44:10,211 --> 00:44:14,110
hit the prediction value of the output
is greater than the, than the threshold.

755
00:44:15,250 --> 00:44:18,850
Okay. And it's greater than, uh, the, uh,

756
00:44:21,570 --> 00:44:22,403
yeah,
the output.

757
00:44:22,450 --> 00:44:24,820
And it's greater than the Alpha
prediction that we defined inside of that

758
00:44:24,821 --> 00:44:25,870
classification object.

759
00:44:26,050 --> 00:44:30,130
Then update the object with
the new output and it's label.

760
00:44:30,190 --> 00:44:33,000
And then we can write to
memory. Well, first of all,

761
00:44:33,001 --> 00:44:36,660
return it and then we'll write right
to memory when we call it. Okay. So

762
00:44:39,030 --> 00:44:44,010
yeah, so we talked about our classifier
and our classification class.

763
00:44:44,220 --> 00:44:46,680
Okay. So one more thing, right? So
the classification class itself,

764
00:44:46,980 --> 00:44:48,620
we've got two variables.

765
00:44:48,780 --> 00:44:52,740
COMP is the output prediction
and the label is the input level.

766
00:44:52,800 --> 00:44:57,240
And so we define the threshold
here. You know, w 1.0,

767
00:44:57,241 --> 00:45:01,230
which is 10% the minimum threshold.
And then the label, which is no,

768
00:45:01,380 --> 00:45:05,350
and the only public facing function
is this update function. Oh.

769
00:45:05,460 --> 00:45:07,150
And the get label and the get comp.

770
00:45:07,151 --> 00:45:09,450
So we to get her functions
and then an update function,

771
00:45:09,451 --> 00:45:14,220
which is how we update it, which
we called right here. Right.

772
00:45:15,880 --> 00:45:18,100
And that's it. So yeah, if
you have an android device,

773
00:45:18,101 --> 00:45:21,920
you can easily deploy it to that. Right.
Just compile it and then pick one. Oh,

774
00:45:21,980 --> 00:45:23,080
so I'm going to talk about one more thing.

775
00:45:23,380 --> 00:45:27,380
When would you use a model on your
device versus on the server? Uh,

776
00:45:27,730 --> 00:45:32,730
you would use it on the server if you
want to constantly deploy new versions of

777
00:45:33,551 --> 00:45:38,050
a model. So you'd use tensorflow
serving, right? The con to that, right.

778
00:45:38,051 --> 00:45:42,730
Because the con to that is then you would
need your mobile device to constantly

779
00:45:42,731 --> 00:45:46,210
make HTP requests. So it always has
to be connected to the Internet.

780
00:45:48,910 --> 00:45:51,040
What you could do is you
can have it both ways.

781
00:45:51,220 --> 00:45:55,270
So the best of both worlds would be to
have an inference model on the device,

782
00:45:55,420 --> 00:45:56,320
which is static,
right?

783
00:45:56,321 --> 00:45:59,440
You can't just update it once you
deploy a new version of the APP.

784
00:45:59,680 --> 00:46:01,000
So the user would have to update the APP,

785
00:46:01,210 --> 00:46:04,510
but you could have a version on the APP
and you can have a version on the server

786
00:46:04,630 --> 00:46:08,050
that is constantly updated that you can
set some frequency to check and then

787
00:46:08,051 --> 00:46:10,000
update the model locally using the server.

788
00:46:10,150 --> 00:46:13,510
So you should have the
best of both worlds. Okay.

789
00:46:13,511 --> 00:46:17,980
So let me answer two questions to end
this and then we're done. We're done.

790
00:46:18,370 --> 00:46:21,550
So the two questions are,
can you do a video that's like Siri,

791
00:46:21,551 --> 00:46:23,050
like how to code the Siri Product.

792
00:46:23,260 --> 00:46:26,770
So I could think about apple is they
don't really open source anything, right?

793
00:46:26,770 --> 00:46:30,190
So we don't actually know what their
machine learning, like how it works.

794
00:46:30,340 --> 00:46:31,840
I'm going to bet though that it's okay.

795
00:46:31,841 --> 00:46:35,590
Like it's not that good comparatively
because in this day and age,

796
00:46:35,620 --> 00:46:37,930
if you want respect,
if you want credibility,

797
00:46:38,020 --> 00:46:42,070
the name of the game is to publish and
publish often. And apple doesn't do that.

798
00:46:42,380 --> 00:46:43,660
Uh,
but how does Siri work?

799
00:46:43,810 --> 00:46:47,820
I'm going to guess that it's going
to use a set of predefined, um,

800
00:46:48,160 --> 00:46:51,490
responses on a server
making http requests.

801
00:46:51,700 --> 00:46:53,590
It's barely using any machine learning.

802
00:46:53,770 --> 00:46:56,300
Maybe it's using machine learning just to,
um,

803
00:46:56,410 --> 00:46:58,570
well it isn't using machine
learning to detect your speech,

804
00:46:58,840 --> 00:47:01,990
but I don't think it's using machine
learning to generate responses. If it is,

805
00:47:01,991 --> 00:47:04,990
I'd be very surprised. Uh, yeah,
so that's the first question.

806
00:47:04,991 --> 00:47:06,280
And then one more question is

807
00:47:07,990 --> 00:47:11,710
how to train feedback and get their
respective ratings predicted rating of new

808
00:47:11,711 --> 00:47:15,760
feedback. So that's a supervised
classification problem, right?

809
00:47:16,060 --> 00:47:19,180
You've got some texts and if you've got
a label and then you went to one of the

810
00:47:19,181 --> 00:47:23,080
mapping between the two. And so how
do you learn the mapping between two,

811
00:47:23,770 --> 00:47:28,140
like a label and an input? Uh,
supervised classification, right?

812
00:47:28,141 --> 00:47:30,130
This just general classification.

813
00:47:30,340 --> 00:47:34,390
The way you do this for text
is using an LSTM network,

814
00:47:34,391 --> 00:47:39,100
which is a type of recurring net
made for more or for text data.

815
00:47:39,370 --> 00:47:42,460
Cool. Please subscribe for more
programming videos. And for now,

816
00:47:42,520 --> 00:47:45,250
I've got to create a new series,
so thanks for watching.

