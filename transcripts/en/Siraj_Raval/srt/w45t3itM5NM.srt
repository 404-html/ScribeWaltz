1
00:00:00,090 --> 00:00:04,770
Ai. I know we just met, but
will you marry me? Yes sir.

2
00:00:04,771 --> 00:00:07,560
Roger.
Hello world.

3
00:00:07,590 --> 00:00:12,590
It's Sir Raj and Uber released a tool a
few weeks ago called Ludvig calling it a

4
00:00:12,661 --> 00:00:16,200
code free deep learning toolbox
and that brings up the question,

5
00:00:16,470 --> 00:00:19,650
can you really use AI if
you don't know how to code,

6
00:00:20,040 --> 00:00:23,370
the answer is definitively
Bush's is Omar wizzle.

7
00:00:23,430 --> 00:00:27,630
I'll explain how lewd vig works as well
as give a brief demo of using it to

8
00:00:27,631 --> 00:00:29,670
create a chat Bot without coding.

9
00:00:30,030 --> 00:00:34,110
Also use this opportunity to recommend
some other code free machine learning

10
00:00:34,111 --> 00:00:38,220
tools afterwards. So smash that
like button and let's get started.

11
00:00:38,580 --> 00:00:42,900
The amount of interest in AI technology
over the past decade keeps on

12
00:00:42,901 --> 00:00:43,734
increasing.

13
00:00:43,920 --> 00:00:47,850
Every company is becoming a tech company
and Marc Andreessen's famous quote,

14
00:00:47,910 --> 00:00:48,900
software is eating.

15
00:00:48,901 --> 00:00:53,490
The world has come to fruition no matter
what field you work in from agriculture

16
00:00:53,491 --> 00:00:55,650
to finance to Penguin Ology,

17
00:00:55,830 --> 00:00:59,970
you're going to be dealing with
data in some way, shape or form.

18
00:01:00,150 --> 00:01:02,340
If you have any kind of website or APP,

19
00:01:02,460 --> 00:01:05,550
you'll be generating data
from it and even if you don't,

20
00:01:05,580 --> 00:01:07,080
you can still learn from data.

21
00:01:07,470 --> 00:01:12,450
Basketball players can use data analytics
tools on replay videos to analyze what

22
00:01:12,451 --> 00:01:15,690
their ideal position should be
or how to improve their form.

23
00:01:15,960 --> 00:01:20,700
Teachers in public schools can use their
student's submission data to provide a

24
00:01:20,701 --> 00:01:24,240
more personalized learning
experience for each student.

25
00:01:24,360 --> 00:01:29,360
Boosting assessment scores musicians can
create sounds more optimized for their

26
00:01:29,431 --> 00:01:31,710
audiences.
Preferences like auto,

27
00:01:31,780 --> 00:01:36,450
auto autotune and people
of all professions can use
data to optimize how much

28
00:01:36,451 --> 00:01:40,020
time they spent on certain
tasks like sleeping, eating,

29
00:01:40,021 --> 00:01:43,650
running and meditating so that they
can improve their health and wellbeing.

30
00:01:43,980 --> 00:01:47,340
But reading data directly,
row by row, column by column,

31
00:01:47,490 --> 00:01:50,910
sorting through PDFs and
images and videos and comments.

32
00:01:51,210 --> 00:01:56,010
This is cumbersome and slow and as that
data grows in size, it's near impossible.

33
00:01:56,250 --> 00:02:00,750
That's where AI comes in. Ai Is a
collection of mathematical techniques.

34
00:02:00,900 --> 00:02:02,250
Many of them,
decades old.

35
00:02:02,550 --> 00:02:07,110
In the past you needed a phd to have
access to the learning materials necessary

36
00:02:07,260 --> 00:02:08,580
to understand AI,

37
00:02:08,760 --> 00:02:12,660
but with the internet came an absolute
explosion in the number of available

38
00:02:12,661 --> 00:02:16,920
courses and tutorials to help
explain how this technology works.

39
00:02:17,160 --> 00:02:21,810
The four pillars of Ai are education,
computation, data and algorithms.

40
00:02:22,050 --> 00:02:25,500
You've got access to data sets made
publicly available across the web.

41
00:02:25,710 --> 00:02:30,030
Access to computing resources on sites
like Google, Colab and Kaggle kernels,

42
00:02:30,330 --> 00:02:33,600
educational resources like my
channel, hit subscribe. By the way,

43
00:02:33,630 --> 00:02:38,100
I'm going to continue to create relevant
educational videos here every single

44
00:02:38,100 --> 00:02:42,270
week. I live to serve my youtube
audience and of course the algorithms,

45
00:02:42,390 --> 00:02:46,080
many of which have been open
sourced as code repositories.

46
00:02:46,650 --> 00:02:51,000
There are lots of different AI
algorithms and they require knowledge of

47
00:02:51,001 --> 00:02:55,680
different mathematical disciplines
like linear Algebra, calculus,

48
00:02:55,770 --> 00:02:57,780
probability,
theory and statistics.

49
00:02:57,990 --> 00:03:02,710
Generally developers use tools like the
c plus plus programming language and red

50
00:03:02,711 --> 00:03:07,711
bull to write out these
mathematical techniques like
support vector machines and

51
00:03:07,870 --> 00:03:12,160
decision trees so they could apply
them to data to retrieve insights.

52
00:03:12,340 --> 00:03:16,420
But C plus plus is a nontrivial
language to learn and while powerful,

53
00:03:16,421 --> 00:03:20,500
it's quite verbose, requiring many
lines of code to achieve a certain task.

54
00:03:20,800 --> 00:03:21,341
Nowadays,

55
00:03:21,341 --> 00:03:25,780
Python has become the most popular tool
to use machine learning models because

56
00:03:25,781 --> 00:03:30,070
it's readable and has a small learning
curve and a python library called care.

57
00:03:30,071 --> 00:03:34,690
Os has been the de facto easiest way
to use python to build machine learning

58
00:03:34,691 --> 00:03:36,610
models in just a few lines of code,

59
00:03:36,970 --> 00:03:40,390
but yet this still requires
code and not only that,

60
00:03:40,391 --> 00:03:44,830
it requires some knowledge of the ideal
hyper parameter values to use and what

61
00:03:44,831 --> 00:03:46,720
type of model to use in the first place.

62
00:03:47,080 --> 00:03:51,850
Can we make AI a tool that doesn't even
require programming knowledge to use?

63
00:03:52,240 --> 00:03:55,030
Uber's Ludvig is a step in that direction.

64
00:03:55,120 --> 00:03:59,770
Ludvig has been used internally by Uber
for two years and they just open sourced

65
00:03:59,771 --> 00:04:02,950
it to get more contributions
from the data science community.

66
00:04:03,400 --> 00:04:05,650
Ludvig is in fact a python library,

67
00:04:05,651 --> 00:04:09,610
but their code free claim comes from
the fact that you don't actually have to

68
00:04:09,611 --> 00:04:11,080
write any code to use it.

69
00:04:11,320 --> 00:04:14,860
All you need to do is input some
commands into a command line,

70
00:04:14,980 --> 00:04:19,720
which I'll show you how to do in your
web browser easily using Google's colab

71
00:04:19,721 --> 00:04:23,920
environment to create a chat Bot.
Lewd Vic has two main functions,

72
00:04:23,921 --> 00:04:26,440
training models and using
them to make predictions.

73
00:04:26,710 --> 00:04:31,710
We can go to colab.research.google.com
to boot up an AI training environment in

74
00:04:31,721 --> 00:04:34,690
the cloud.
Once we select Gpu as our runtime type,

75
00:04:34,840 --> 00:04:38,980
we can install Ludvig by copying and
pasting the insulation commands directly

76
00:04:38,981 --> 00:04:42,790
from the lewd vague get hub repository.
Don't hate copy paste.

77
00:04:43,030 --> 00:04:44,350
Once it's done installing,

78
00:04:44,351 --> 00:04:48,370
we can find a movie dialogue Dataset
as a CSV file and download it.

79
00:04:48,730 --> 00:04:53,640
This is a movie dialogue Dataset that
contains 220 k conversations between 10 k

80
00:04:53,650 --> 00:04:55,300
pairs of movie characters.

81
00:04:55,660 --> 00:04:59,590
This data set has two columns which
contained conversational elements.

82
00:04:59,830 --> 00:05:02,380
The first column is a
collection of statements,

83
00:05:02,410 --> 00:05:04,990
questions and responses from one person.

84
00:05:05,320 --> 00:05:09,370
The second column is a collection of
responses to the first columns, texts.

85
00:05:09,400 --> 00:05:13,630
We're going to use Ludvig to learn from
this data set so that our chat Bot knows

86
00:05:13,631 --> 00:05:15,010
how to respond to us.

87
00:05:15,340 --> 00:05:19,690
We can upload our CSV file directly to
Colab and once it's there we need to do

88
00:05:19,691 --> 00:05:23,380
one more thing before training the
model and that's defining the model

89
00:05:23,381 --> 00:05:24,550
definition file,

90
00:05:24,700 --> 00:05:27,970
which will copy and paste from Lewd
Vic's example in the user guide.

91
00:05:28,330 --> 00:05:29,800
I'll explain this in a second.

92
00:05:30,040 --> 00:05:34,750
Now we can train our model using the
command Ludvig experiments and two flags,

93
00:05:35,050 --> 00:05:39,310
one that asks for the name of the Dataset
and the second which asks for the name

94
00:05:39,311 --> 00:05:43,960
of the model definition file we just
created. That's it. It's training.

95
00:05:44,230 --> 00:05:47,620
We didn't have to code out a model
nor set any hyper parameters.

96
00:05:47,621 --> 00:05:48,730
It's already training.

97
00:05:49,060 --> 00:05:53,440
It automatically performs a random split
of the data into different training,

98
00:05:53,590 --> 00:05:55,360
validation and testing sets.

99
00:05:55,570 --> 00:05:59,480
Preprocessing processes them so they
ready for training and build models with

100
00:05:59,481 --> 00:06:01,010
preset hyper parameters.

101
00:06:01,310 --> 00:06:06,310
It trains the model until the accuracy
on devalidation sets stops improving or

102
00:06:06,321 --> 00:06:10,550
the maximum training time
is reached before you tattoo
the Lewd Vig logo to your

103
00:06:10,551 --> 00:06:11,780
forearm.
Check this out.

104
00:06:12,110 --> 00:06:17,000
We can also visualize how the model
improves over time by using the visualized

105
00:06:17,001 --> 00:06:21,500
command to test the model. We can run the
predict function and given some input,

106
00:06:21,710 --> 00:06:24,080
a conversational sentence.
In our case,

107
00:06:24,290 --> 00:06:27,710
it will give us an output using
the trained model. It's response.

108
00:06:27,980 --> 00:06:32,600
Now we can download and use the train
weights file for a mobile APP or web app

109
00:06:32,630 --> 00:06:36,560
to offer people a Chat Bot. That's all
it takes to train and test the model.

110
00:06:36,561 --> 00:06:41,360
Ludvig has examples in their documentation
for many different types of scenarios

111
00:06:41,361 --> 00:06:45,380
we'd want to use AI for in
areas like sentiment analysis,

112
00:06:45,560 --> 00:06:49,580
image classification, time series,
forecasting and text classification.

113
00:06:49,970 --> 00:06:52,760
Even image captioning,
which is non trivial to train.

114
00:06:52,880 --> 00:06:56,120
So let's take a step back behind
the curtain to see how it does this.

115
00:06:56,150 --> 00:07:00,530
Dilute vig python repository contains
a series of deep learning models,

116
00:07:00,560 --> 00:07:04,670
all sorts of neural networks,
convolutional recurrent, et cetera.

117
00:07:05,150 --> 00:07:09,170
The way they architect each pipeline for
every training process is by using the

118
00:07:09,171 --> 00:07:12,410
idea of data type specific
incoders and decoders.

119
00:07:12,800 --> 00:07:17,000
The idea is that incoders we'll map raw
data, whatever we input text videos,

120
00:07:17,001 --> 00:07:18,560
images into tensors,

121
00:07:18,740 --> 00:07:22,070
which are multidimensional
arrays of numbers than decoders.

122
00:07:22,071 --> 00:07:25,520
We'll map these 10 servers to outputs,
which are the predictions.

123
00:07:25,880 --> 00:07:28,430
It also uses the concept of a combiner,

124
00:07:28,460 --> 00:07:33,320
which combines the tensors from all
the input incoders processes them and

125
00:07:33,321 --> 00:07:36,260
returns a tensors to be used
for the output decoders.

126
00:07:36,500 --> 00:07:40,520
So each of the different data types
we have as features in our Dataset,

127
00:07:40,700 --> 00:07:44,990
whether they are numerical or binary has
its own encoder. And this is flexible.

128
00:07:45,020 --> 00:07:47,780
It can use different models
for the encoder and a decoder.

129
00:07:48,050 --> 00:07:52,130
The way it knows the characteristics of
the data in which encoder to use is by a

130
00:07:52,131 --> 00:07:56,870
small input from the data scientist.
That input is the model definition file.

131
00:07:57,110 --> 00:07:59,930
It specifies the input and
output features of the model.

132
00:08:00,110 --> 00:08:04,280
It helps Ludovic understand what are
we trying to predict and using which

133
00:08:04,281 --> 00:08:08,240
features of our data and since
Ludvig is a python library,

134
00:08:08,270 --> 00:08:12,290
it does have a programmatic API.
If you do want to use python,

135
00:08:12,500 --> 00:08:16,580
which looks pretty simple, load the
Yammel file, train the model and test it.

136
00:08:16,610 --> 00:08:21,410
It also allows developers to create their
own encoders and decoders to use for

137
00:08:21,411 --> 00:08:22,910
further experimentation.

138
00:08:23,210 --> 00:08:27,200
More of the focus of a data scientist
will be on fine tuning the architecture of

139
00:08:27,201 --> 00:08:30,470
the target model rather than
repetitive training work.

140
00:08:30,710 --> 00:08:34,370
The Yammel file can be as detailed
or as high level as we'd like.

141
00:08:34,550 --> 00:08:38,300
We can specify certain hyper
parameters or use the default ones.

142
00:08:38,660 --> 00:08:43,280
The whole point of this is a drastically
shorten the experimentation cycles in

143
00:08:43,281 --> 00:08:44,690
deep learning applications.

144
00:08:44,720 --> 00:08:48,950
So Luke Vig is one great step in the
direction of getting to a point where

145
00:08:48,951 --> 00:08:51,410
everyone gets to use AI technology,

146
00:08:51,590 --> 00:08:56,400
but as you can see it does still require
knowledge of Unix commands and flags.

147
00:08:56,640 --> 00:09:00,600
Another great tool is admittedly
Microsoft's Azure machine learning studio.

148
00:09:00,780 --> 00:09:04,860
It's a browser based visual drag and
drop environment where no coding is

149
00:09:04,861 --> 00:09:05,694
necessary.

150
00:09:05,760 --> 00:09:09,630
The idea is to let anyone go from idea
to deployment in a matter of clicks.

151
00:09:09,900 --> 00:09:13,620
I actually already have a video on that
link will be in the video description.

152
00:09:14,010 --> 00:09:17,280
A zero has been around for a
few years now. It works well,

153
00:09:17,340 --> 00:09:20,190
but it's still not extremely
simple to use. In fact,

154
00:09:20,230 --> 00:09:23,520
drag and drop as an interface for
machine learning isn't a new idea.

155
00:09:23,790 --> 00:09:27,900
In 92 accompany introduced a drag
and drop tool for advanced analytics.

156
00:09:28,080 --> 00:09:31,860
That's 26 years ago. Why
isn't everyone using AI? Then?

157
00:09:32,220 --> 00:09:36,270
The answer is that users still need to
know what to drag and where to drop it.

158
00:09:36,480 --> 00:09:40,830
Some of these tools have hundreds of
mathematical operators in the form of

159
00:09:40,831 --> 00:09:41,910
blocks to deal with,

160
00:09:42,090 --> 00:09:46,230
which requires training to understand
and it's still relatively complex for

161
00:09:46,231 --> 00:09:49,290
beginners.
Experts can easily use these tools,

162
00:09:49,291 --> 00:09:52,650
but experts don't want drag and drop.
If they understand the math,

163
00:09:52,651 --> 00:09:55,200
they just want to code.
It's easier to debug.

164
00:09:55,410 --> 00:10:00,090
So there is an excellent opportunity
here to create a web service that uses a

165
00:10:00,091 --> 00:10:02,790
freemium model and allows
absolute beginners,

166
00:10:02,791 --> 00:10:06,600
meaning people who don't know how
to code the ability to train models.

167
00:10:06,810 --> 00:10:09,870
All they would need to do is drag
and drop their data. That's it.

168
00:10:09,900 --> 00:10:14,270
Automate the process of building
complex data, preprocessing pipelines,

169
00:10:14,370 --> 00:10:17,340
hyper parameters, model
selection, everything.

170
00:10:17,700 --> 00:10:20,370
This is all already possible.
There are barriers,

171
00:10:20,371 --> 00:10:25,350
hyper parameter search strategies like
Basie and optimization and grid search

172
00:10:25,410 --> 00:10:29,880
that enables this capability and
there's even a proprietary solution for

173
00:10:29,881 --> 00:10:33,360
enterprises called data robot
that does this pretty well.

174
00:10:33,570 --> 00:10:37,350
The con is that it's an enterprise
price point that check it out though if

175
00:10:37,351 --> 00:10:38,460
you've got the budget for it.

176
00:10:38,910 --> 00:10:43,260
Another cool startup that has claimed
that they are working on this is Loeb.

177
00:10:43,560 --> 00:10:47,730
Their demo videos look pretty magical.
It's a glimpse into the future of Ai.

178
00:10:48,060 --> 00:10:51,510
They were acquired by Microsoft and
still haven't released our product to the

179
00:10:51,511 --> 00:10:54,360
general public,
so we'll have to wait on that.

180
00:10:54,570 --> 00:10:58,350
It's a drag and drop interface
that seems to marry ease of use and

181
00:10:58,351 --> 00:11:02,070
configurability pretty well. Also,
shout out to deep cognition.ai.

182
00:11:02,100 --> 00:11:04,740
It's both the visual editor and code ide.

183
00:11:04,950 --> 00:11:09,120
You can drag and drop blocks and it
creates code alongside it, which is sick.

184
00:11:09,210 --> 00:11:12,210
There are three things to remember
from this video. Uber's lewd.

185
00:11:12,211 --> 00:11:16,740
Vague is a python library that lets
anyone use deep learning by issuing a few

186
00:11:16,770 --> 00:11:21,200
unix commands, no code necessary.
It uses a flexible data types,

187
00:11:21,201 --> 00:11:22,350
specific encoder,

188
00:11:22,380 --> 00:11:26,910
decoder model that has default settings
but can also be fine tuned by the user

189
00:11:27,180 --> 00:11:31,480
and other code free options include
Microsoft as your data robot. Indeed,

190
00:11:31,500 --> 00:11:35,220
cognition. What's your favorite
tool to use for data analytics?

191
00:11:35,221 --> 00:11:38,700
Let me know in the comments section and
please subscribe for more programming

192
00:11:38,701 --> 00:11:43,140
videos. For now, I'm going to learn
some advanced math, so thanks a lot.

