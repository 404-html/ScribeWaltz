1
00:00:01,290 --> 00:00:05,910
Deep learning frameworks,
assemble hello world,

2
00:00:05,940 --> 00:00:10,350
it's Raj and there are so many
deep learning frameworks out there.

3
00:00:10,530 --> 00:00:13,800
How are you supposed to
know which one to use?

4
00:00:14,100 --> 00:00:18,510
I'm going to compare 10 of the most
popular deep learning frameworks in this

5
00:00:18,511 --> 00:00:23,511
video across a wide variety of metrics
from ease of installation to performance

6
00:00:24,451 --> 00:00:25,890
to popularity on get hub.

7
00:00:26,250 --> 00:00:31,110
After reviewing the merits and drawbacks
of each of them will be able to come to

8
00:00:31,111 --> 00:00:33,870
some kind of reasonable
conclusion at the end.

9
00:00:34,230 --> 00:00:37,350
So let's start with the obvious one.
Tensor flow.

10
00:00:37,650 --> 00:00:39,480
Out of all the deep learning frameworks,

11
00:00:39,660 --> 00:00:44,660
tensorflow is without a doubt the most
popular in terms of developer activity on

12
00:00:44,761 --> 00:00:47,250
get hub who will created it to help power.

13
00:00:47,251 --> 00:00:52,110
Almost all of it's massively scaled
services like Gmail and translate,

14
00:00:52,320 --> 00:00:55,290
then open sourced it for the rest of us.
Nowadays,

15
00:00:55,291 --> 00:00:57,390
recognizable brands like Uber,

16
00:00:57,540 --> 00:01:02,430
Airbnb and Dropbox have all decided to
leverage this framework for their own

17
00:01:02,431 --> 00:01:06,930
services. Currently it's best
supported client language is python,

18
00:01:07,140 --> 00:01:12,140
but there are also experimental interfaces
available in c plus Plus Java and go

19
00:01:12,750 --> 00:01:16,710
and because it's so popular
it has bindings for other
languages like c sharp and

20
00:01:16,711 --> 00:01:19,290
Giulia created by the
open source community.

21
00:01:19,620 --> 00:01:24,330
Having such a massive developer community
has resulted in tensorflow having rich

22
00:01:24,570 --> 00:01:29,570
detailed documentation not only from its
official website but from various third

23
00:01:29,611 --> 00:01:31,590
party sources from around the web.

24
00:01:31,890 --> 00:01:36,890
This documentation covers its various
features like tensor board cancer board

25
00:01:37,200 --> 00:01:40,500
lets developers monitor
the model training process,

26
00:01:40,770 --> 00:01:44,310
the various visualizations and
it's a crucial part of it's sweet.

27
00:01:44,550 --> 00:01:48,990
Another crucial part is tentraflow
serving which allows developers to easily

28
00:01:48,991 --> 00:01:53,991
serve their models at scale
in a production environment
and includes distributed

29
00:01:54,121 --> 00:01:54,954
training.

30
00:01:54,960 --> 00:01:59,460
Tensorflow lite even enables on device
inference with low latency for mobile

31
00:01:59,461 --> 00:02:00,294
phones,

32
00:02:03,230 --> 00:02:06,530
but despite all of this
tensorflow is pretty low level.

33
00:02:06,710 --> 00:02:10,640
You have to specify a lot of magic
numbers like the number of layers in your

34
00:02:10,641 --> 00:02:13,400
network,
the dimensions of your input data,

35
00:02:13,580 --> 00:02:18,110
and this requires a lot of boiler
plate coding on the developer's part,

36
00:02:18,320 --> 00:02:21,080
which can be tedious and difficult.
By default.

37
00:02:21,110 --> 00:02:26,110
Tensorflow lets developers create static
computation graphs at compile time.

38
00:02:27,140 --> 00:02:29,180
We must define it,
then run it,

39
00:02:29,210 --> 00:02:32,840
meaning all the conditions and iterations
in the graph structure have to be

40
00:02:32,841 --> 00:02:34,460
defined before it's run.

41
00:02:34,760 --> 00:02:37,970
If we want to make any changes
in the neural network structure,

42
00:02:38,150 --> 00:02:42,740
we have to rebuild it from scratch.
It was designed this way for efficiency,

43
00:02:42,741 --> 00:02:47,390
but a lot of the newer neural
architectures dynamically change,

44
00:02:47,480 --> 00:02:52,040
so this default define and run mode of
tensorflow is counterintuitive and can

45
00:02:52,041 --> 00:02:53,330
make the bugging difficult.

46
00:02:53,660 --> 00:02:57,980
They did add a define Byron option
called eager execution later on,

47
00:02:58,190 --> 00:02:59,180
but it's not native.

48
00:02:59,260 --> 00:03:04,090
Expect it to be even better in TF 2.0
which is about to release most of the time

49
00:03:04,091 --> 00:03:07,660
tensorflow is compared
to the Pi Torch Library,

50
00:03:07,870 --> 00:03:10,420
a native define Byron framework.

51
00:03:10,720 --> 00:03:15,250
Pi Torch was created by Facebook to help
power it services and it's now used by

52
00:03:15,251 --> 00:03:17,380
brands like Twitter and salesforce.

53
00:03:18,040 --> 00:03:23,040
Unlike tensorflow though your
income high torches default defined,

54
00:03:24,550 --> 00:03:29,550
Byron mode is more like
traditional programming while
training a Pi torch model

55
00:03:29,860 --> 00:03:31,990
for each iteration in an epoch.

56
00:03:32,140 --> 00:03:36,880
A computational graph is created after
each iteration. The graph is freed,

57
00:03:36,910 --> 00:03:41,500
meaning more available memory because
it defines the graph in a forward pass

58
00:03:41,501 --> 00:03:44,710
versus a defined then run
framework like tentraflow.

59
00:03:45,010 --> 00:03:50,010
Backpropagation is defined by how the
code is run and every single iteration can

60
00:03:50,201 --> 00:03:51,034
be different.

61
00:03:51,280 --> 00:03:55,840
Pi Torch records the values as they
happen in our code to build the dynamic

62
00:03:55,841 --> 00:03:59,740
graph as the code is run high torch.
Also nails debugging.

63
00:03:59,770 --> 00:04:04,770
We can use common debugging tools like
PDB or Pi charm and the modeling process

64
00:04:05,171 --> 00:04:08,650
is simple and transparent.
High Torch has declarative data,

65
00:04:08,651 --> 00:04:13,651
parallelism features a lot of pretrained
models and has modular parts that are

66
00:04:13,811 --> 00:04:17,140
relatively easy to combine
and just like tensor flow.

67
00:04:17,141 --> 00:04:20,500
It allows for distributed training.
On the flip side, however,

68
00:04:20,501 --> 00:04:25,120
a Pi torch lacks model serving in the
well thought out way that tensorflow does

69
00:04:25,300 --> 00:04:29,800
and lacks interfaces for monitoring
and visualization like cancer board,

70
00:04:30,100 --> 00:04:35,100
but you can connect Pi Torch to tensor
board via some third party libraries like

71
00:04:35,540 --> 00:04:38,980
cancer board x. If we look at
various papers from neuro hips,

72
00:04:39,010 --> 00:04:40,870
the biggest AI summit of the year,

73
00:04:41,110 --> 00:04:45,310
it's clear that researchers tend
to prefer Pi Torch to tensorflow.

74
00:04:45,790 --> 00:04:49,690
That's because it's best for
prototyping or small scale projects.

75
00:04:49,840 --> 00:04:52,900
When it comes to larger
across platform deployments.

76
00:04:53,230 --> 00:04:55,540
Tensorflow seems to be the better option,

77
00:04:55,750 --> 00:05:00,130
but I should also note that the popular
cafe two framework introduced by

78
00:05:00,131 --> 00:05:05,131
Facebook in 2017 is built for mobile and
large scale deployments in production

79
00:05:05,531 --> 00:05:09,490
environments and was recently
merged into Pi torch.

80
00:05:09,820 --> 00:05:14,110
This gives pie torch production grade
scalability. Curiously, deep mind.

81
00:05:14,111 --> 00:05:18,400
Perhaps the most prominent AI research
lab in the world doesn't use Pi torch.

82
00:05:18,640 --> 00:05:23,620
They use their own framework
called Sonnet, which is
built on top of tensorflow.

83
00:05:23,950 --> 00:05:24,431
Deep minds.

84
00:05:24,431 --> 00:05:28,930
Developers spent a lot of time having to
acquaint themselves with the underlying

85
00:05:28,931 --> 00:05:33,040
tension flow graphs in order to
correctly architect their applications.

86
00:05:33,310 --> 00:05:34,240
But with Sonnet,

87
00:05:34,360 --> 00:05:38,590
the creation of neural net components
was made easy because at first constructs

88
00:05:38,591 --> 00:05:42,580
python objects, which represent
some part of a neural network.

89
00:05:42,640 --> 00:05:46,390
Then separately connects these
objects into the computation graph.

90
00:05:46,840 --> 00:05:51,640
These modules simplify the training
process and can be combined to implement

91
00:05:51,641 --> 00:05:53,230
higher level networks.

92
00:05:53,560 --> 00:05:58,130
Developers can also easily extend Sonnet
by implementing their own modules.

93
00:05:58,340 --> 00:06:01,150
This makes switching
between models easier. Well,

94
00:06:01,160 --> 00:06:05,030
let's put the research versus production
pipeline debate aside for a second.

95
00:06:05,450 --> 00:06:08,930
What if you're just a beginner and just
want to learn how all this stuff works?

96
00:06:09,140 --> 00:06:13,310
The minimalist python based library
called Karrass can be run on top of

97
00:06:13,311 --> 00:06:16,480
tensorflow or Microsoft's CNTK care.

98
00:06:16,481 --> 00:06:20,960
Ross has support for a huge range of
neural network types and makes prototyping

99
00:06:20,990 --> 00:06:23,390
dead simple and the code is very readable.

100
00:06:23,690 --> 00:06:25,970
That's the reason I use
it as a teaching tool.

101
00:06:25,971 --> 00:06:29,990
So often in my videos it's
really easy on the eyes building.

102
00:06:29,991 --> 00:06:34,991
A massively complicated deep learning
model can be done in just a few lines of

103
00:06:35,061 --> 00:06:35,810
code.

104
00:06:35,810 --> 00:06:40,700
It has built in support for training on
multiple GPS and can be turned into flow

105
00:06:40,730 --> 00:06:44,630
estimators and train on
clusters of gps on Google cloud.

106
00:06:44,810 --> 00:06:48,620
But the downside of it being so high
level is that it's not as customizable.

107
00:06:48,890 --> 00:06:53,450
It's also constrained to the libraries.
It's built on like tensorflow, NCN, TK,

108
00:06:53,600 --> 00:06:57,200
so less functionality than a lower
level library like tensorflow,

109
00:06:57,410 --> 00:07:02,030
but easier to learn. Carrots is the
best learning tool for beginners.

110
00:07:02,140 --> 00:07:04,730
All right, let's move on
to mx net. Jeff basis.

111
00:07:04,760 --> 00:07:09,560
I mean Amazon's deep learning framework.
Mx Net has been adopted by AWS.

112
00:07:09,740 --> 00:07:13,610
Parts of apalled are rumored to be
using it and it offers APIs and a huge

113
00:07:13,611 --> 00:07:16,130
variety of languages natively.
Even Pearl,

114
00:07:16,550 --> 00:07:21,020
where mx net excels is in its ability to
scale linearly more so than tensorflow.

115
00:07:21,380 --> 00:07:25,940
The CTO of Amazon published benchmarks
for mx nets training throughputs using

116
00:07:25,941 --> 00:07:30,290
the inception v3 image analysis algorithms
and claim that the speedup obtained

117
00:07:30,291 --> 00:07:35,291
by running it across multiple gps
was very linear across 128 GPUs.

118
00:07:35,840 --> 00:07:38,990
Mx Net performed a hundred
times faster than a single GPU.

119
00:07:39,440 --> 00:07:43,010
Mx Net has a high performance
imperative API, which is pretty awesome.

120
00:07:43,310 --> 00:07:47,090
It's got to simplicity of carrots
and it's dynamic like Pi Torch,

121
00:07:47,240 --> 00:07:51,170
which makes debugging a lot
easier. Unlike Pi Torch, however,

122
00:07:51,310 --> 00:07:55,280
mx net supports hybridization as
part of its glue on interface.

123
00:07:55,670 --> 00:07:59,660
The hybrid block class seamlessly
combines declarative programming like

124
00:07:59,690 --> 00:08:04,690
tensorflow and imperative programming
like Pi Torch to offer the benefit of both

125
00:08:05,690 --> 00:08:08,120
users can quickly develop an debug models,

126
00:08:08,121 --> 00:08:12,830
would imperative programming and switch
to efficient declarative execution by

127
00:08:12,831 --> 00:08:15,650
simply calling hybrid
blocked dot hybridize.

128
00:08:15,980 --> 00:08:20,980
We'll notice mx net advantage and symbolic
Api APIs when training on many gps in

129
00:08:21,321 --> 00:08:25,400
some specific cases glue on his
three x faster than Pi Torch.

130
00:08:25,580 --> 00:08:29,780
But take this with a grain of salt as
benchmarks depending on so many factors

131
00:08:30,080 --> 00:08:34,310
and it's integration with AWS
is unbeatable because Duh,

132
00:08:34,311 --> 00:08:35,900
it's Amazon's own pipeline.

133
00:08:36,110 --> 00:08:41,110
Let's not forget Microsoft though CNTK
or Microsoft cognitive toolkit is a DL

134
00:08:41,301 --> 00:08:44,960
framework that supports python
c plus plus c sharp and Java.

135
00:08:45,200 --> 00:08:49,730
It's got support for CNNS and rns and
it's using Skype as Xbox and Cortana.

136
00:08:49,970 --> 00:08:53,510
It's targeted toward letting developers
easily build models for products in

137
00:08:53,511 --> 00:08:57,660
speech and image problems and it
offers support for Apache spark.

138
00:08:57,900 --> 00:09:01,950
It's the easiest of all the frameworks
who integrate into a zero Microsoft's

139
00:09:01,951 --> 00:09:02,850
cloud offering.

140
00:09:02,880 --> 00:09:07,830
And one thing in particular I like
about CNC K is that it handles passing

141
00:09:07,831 --> 00:09:12,450
sequences of varied length better
than the other frameworks. In TF,

142
00:09:12,540 --> 00:09:16,400
you have to do padding masking and
sometimes even write your own softmax

143
00:09:16,410 --> 00:09:20,220
function that ignores mask elements.
In Pi Torch,

144
00:09:20,250 --> 00:09:24,150
the scenario is less painful with
functions like pack petted sequences,

145
00:09:24,360 --> 00:09:25,890
but you still have to
pat at the beginning.

146
00:09:26,250 --> 00:09:29,610
Masking in general makes your
model vulnerable to errors.

147
00:09:29,730 --> 00:09:34,360
In CNC k you just have to
pass the sequence without
any padding or requiring a

148
00:09:34,361 --> 00:09:37,050
mask later on and
everything is taken care of.

149
00:09:37,410 --> 00:09:40,170
It handles sequences a
variable length internally.

150
00:09:40,380 --> 00:09:45,380
Some of the criticisms of CNTK include
it's strict license as they have not

151
00:09:45,421 --> 00:09:50,070
adopted conventional open source
licenses like GPL, ASF or MIT.

152
00:09:50,430 --> 00:09:54,270
The community seems to consist of mostly
windows developers who would like to

153
00:09:54,271 --> 00:09:58,740
include machine learning models in either
desktop or mobile applications. Also,

154
00:09:58,741 --> 00:10:02,670
shout out to chainer a framework
created by a Japanese startup.

155
00:10:02,970 --> 00:10:06,900
It's similar to Pi Torch and that
it has a native imperative Api,

156
00:10:07,200 --> 00:10:10,530
but it's difficult to debug.
The community is relatively small,

157
00:10:10,531 --> 00:10:15,390
but it's supported by giants like IBM,
Intel and in my fantasies make a Godzilla.

158
00:10:15,720 --> 00:10:20,070
It can be run on multiple gps with
little effort and the main use case we've

159
00:10:20,071 --> 00:10:22,620
seen thus far is in speech recognition,

160
00:10:22,650 --> 00:10:27,060
machine translation and
sentiment analysis. If you're
core programming languages,

161
00:10:27,061 --> 00:10:30,060
Java definitely take a look
at deep learning for j.

162
00:10:30,300 --> 00:10:34,860
It's written mainly for Java and Scala
and supports a huge variety of neural

163
00:10:34,861 --> 00:10:35,670
networks.

164
00:10:35,670 --> 00:10:40,320
It was made for enterprise scale and
works with Apache Hadoop and spark on

165
00:10:40,321 --> 00:10:43,470
distributed CPU and Gpu is also,

166
00:10:43,471 --> 00:10:45,450
their documentation is stellar.

167
00:10:45,720 --> 00:10:48,280
Java isn't very popular among
machine learning project,

168
00:10:48,281 --> 00:10:51,510
so it's hard to integrate
it with other ml libraries.

169
00:10:51,780 --> 00:10:56,280
Perhaps the main utility here is that
android apps are usually written in Java.

170
00:10:56,430 --> 00:11:00,750
Thus this would be a good choice if
you'd like to build a full stack Java

171
00:11:00,751 --> 00:11:01,231
pipeline,

172
00:11:01,231 --> 00:11:06,231
which includes android devices and
speaking of mobile chat out to core ml.

173
00:11:06,690 --> 00:11:10,200
It's not a framework that's made
to build models necessarily,

174
00:11:10,440 --> 00:11:15,270
but it does help you bring existing
models built in other frameworks to apple

175
00:11:15,271 --> 00:11:19,200
devices, and last but not
least, let's talk about onyx.

176
00:11:19,230 --> 00:11:21,060
A Pokemon with a pretty high HP,

177
00:11:21,061 --> 00:11:24,870
but also the open neural
network exchange format.

178
00:11:25,170 --> 00:11:28,710
It was developed in partnership
between Microsoft and Facebook.

179
00:11:28,860 --> 00:11:32,970
They both decided there was a need
for interoperability in the AI tools

180
00:11:32,971 --> 00:11:33,720
community.

181
00:11:33,720 --> 00:11:38,460
Since developers often find themselves
locked into one framework or ecosystem,

182
00:11:38,880 --> 00:11:43,350
onyx enables more of these tools to
work together by allowing them to share

183
00:11:43,380 --> 00:11:44,213
models.

184
00:11:44,250 --> 00:11:48,750
The idea is that you can train a model
with one tool stack and deploy it using

185
00:11:48,810 --> 00:11:53,370
another for inference and prediction.
To ensure this kind of interoperability,

186
00:11:53,410 --> 00:11:56,350
we must explore our model
into the onyx format,

187
00:11:56,351 --> 00:12:00,790
which is a serialized representation of
the model in a proto buff file. Overall,

188
00:12:00,791 --> 00:12:03,910
choosing the perfect framework
for a DL project can be hard.

189
00:12:03,910 --> 00:12:08,140
You have to take into account many factors
like the type of architecture you'll

190
00:12:08,141 --> 00:12:11,710
be developing with which programming
language you're going to use,

191
00:12:11,980 --> 00:12:16,480
the number of tools you need, et
cetera. Here are my conclusions.

192
00:12:16,720 --> 00:12:19,570
If you're a beginner to
programming in general,

193
00:12:19,720 --> 00:12:23,530
use care us as it's still the
easiest library to learn from.

194
00:12:23,830 --> 00:12:25,540
If you'd like to build a production,

195
00:12:25,541 --> 00:12:29,350
great application and deploy it
to Google cloud, use tensorflow.

196
00:12:29,740 --> 00:12:32,560
If you'd like to do research,
use high torch,

197
00:12:32,650 --> 00:12:36,640
but also checkout sonnet if
you prefer deploying to AWS,

198
00:12:36,670 --> 00:12:40,000
use mx net if you want
to deploy to Azure Ucn,

199
00:12:40,001 --> 00:12:43,900
t k if you're a Java developer.
The planning forJ is your best bet,

200
00:12:44,410 --> 00:12:47,740
I don't think has got anything unique
compared to the other frameworks.

201
00:12:47,770 --> 00:12:50,500
And once you've already
started building a model,

202
00:12:50,501 --> 00:12:55,480
use onyx to use tools from other
framework ecosystems with it. Oh,

203
00:12:55,481 --> 00:12:59,620
and anything ios related you can
leverage core ml for what's your favorite

204
00:12:59,621 --> 00:13:00,161
framework.

205
00:13:00,161 --> 00:13:03,610
Let me know in the comment section and
please subscribe for more programming

206
00:13:03,611 --> 00:13:07,900
videos. For now, I've got to define
and run, so thanks for watching.

