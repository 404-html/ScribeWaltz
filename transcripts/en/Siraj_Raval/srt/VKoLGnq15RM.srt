1
00:00:00,420 --> 00:00:01,920
Hello world.
It's it Raj.

2
00:00:01,950 --> 00:00:04,980
Geoffrey Hinton is one of the
godfathers of deep learning.

3
00:00:04,981 --> 00:00:09,030
In the 80s he popularized the
backpropagation algorithm,

4
00:00:09,120 --> 00:00:11,370
which is the reason deep
learning works so well,

5
00:00:11,580 --> 00:00:16,580
and he believed in the idea of neural
networks during the first major AI winter

6
00:00:16,681 --> 00:00:19,770
in the 80s while everyone else
didn't think that they would work,

7
00:00:19,980 --> 00:00:21,570
and that's why he's awesome.
But anyway,

8
00:00:21,571 --> 00:00:26,190
Hinton recently published a paper
on this idea of a capsule network,

9
00:00:26,370 --> 00:00:30,540
which he's been hinting at for a long
time. Hinting Hinton was hinting, yeah,

10
00:00:30,960 --> 00:00:34,350
he's been hinting at this for a long time
in the machine learning subreddit and

11
00:00:34,351 --> 00:00:35,550
talks that he's been doing,

12
00:00:35,700 --> 00:00:38,400
but a few days ago he finally
published the paper for it.

13
00:00:38,401 --> 00:00:41,850
So I was very excited to read it
and talk about it. But anyway,

14
00:00:41,940 --> 00:00:46,650
but it offers state of the art
performance on the m and ist a Dataset,

15
00:00:46,830 --> 00:00:50,070
which is the handwritten character
dataset. It's kind of the baseline.

16
00:00:50,071 --> 00:00:52,830
You probably know about it if
you've done any kind of AI before,

17
00:00:52,980 --> 00:00:56,340
but it's those handwritten characters
classifying them as the digits that they

18
00:00:56,341 --> 00:00:59,880
are,
and it become delusional networks at this.

19
00:00:59,881 --> 00:01:02,100
And convolutional networks
are the state of the art.

20
00:01:02,101 --> 00:01:05,150
So it's a really exciting time right now.
And what this video is,

21
00:01:05,151 --> 00:01:09,090
is me talking about what the state
of the art algorithm currently is,

22
00:01:09,150 --> 00:01:10,350
the convolutional network,

23
00:01:10,620 --> 00:01:14,070
all the developments that have
happened in convolutional networks.

24
00:01:14,310 --> 00:01:16,470
Then we'll talk about
capsules and how they work,

25
00:01:16,680 --> 00:01:21,030
and we'll end the video with me going
through the tensorflow code of a capsule

26
00:01:21,031 --> 00:01:24,600
network, right? So here's an image
of a convolutional neural network.

27
00:01:24,960 --> 00:01:29,370
And if we use a standard multilayer
perceptron with all the layers fully

28
00:01:29,371 --> 00:01:30,420
connected to each other,

29
00:01:30,570 --> 00:01:35,490
it would quickly become computationally
intractable because images are very high

30
00:01:35,491 --> 00:01:37,620
dimensional, right?
There's lots of pixels.

31
00:01:37,800 --> 00:01:42,330
And if we are continuously applying these
operations to every single pixel in an

32
00:01:42,331 --> 00:01:45,690
image and every layer,
it's going to take way too long.

33
00:01:45,930 --> 00:01:50,010
So the solution to this is was
to use a convolutional network.

34
00:01:50,190 --> 00:01:54,870
And this was really popularized by young
Macoun who is now director of AI at

35
00:01:54,870 --> 00:01:58,740
Facebook in the early nineties but the
convolutional network looks like this,

36
00:01:58,741 --> 00:02:01,320
right? So first we have
an input image, right?

37
00:02:01,350 --> 00:02:03,420
The input image has an associated label.

38
00:02:03,421 --> 00:02:05,760
So if it's a picture of
a car like we see here,

39
00:02:05,940 --> 00:02:08,220
then it's going to have an associated
label, which is car, right?

40
00:02:08,221 --> 00:02:12,210
So picture of a car and then car.
And you do that for an entire Dataset.

41
00:02:12,480 --> 00:02:16,290
And what a convolutional network will
do is it will learn the mapping between

42
00:02:16,291 --> 00:02:18,480
the input data and the output label.

43
00:02:18,600 --> 00:02:21,840
And so the idea is that eventually
you'll give it a picture of a car after

44
00:02:21,841 --> 00:02:26,250
training and it will know, hey, that's
a car because it's learned the mapping.

45
00:02:26,610 --> 00:02:28,740
When we first feed this network a car,

46
00:02:28,770 --> 00:02:31,290
it's going to first be applied
to the convolutional layer.

47
00:02:31,440 --> 00:02:32,850
And so in the convolutional layer,

48
00:02:32,851 --> 00:02:36,960
it's basically a series of Matrix
multiplications followed by a summation

49
00:02:36,961 --> 00:02:40,920
operation. And I have a video on
how convolutional networks worked,

50
00:02:40,921 --> 00:02:44,400
really in depth that I'm going to link
to in the video description under more

51
00:02:44,401 --> 00:02:47,880
learning resources. But right
now I'm going to go over
it at a high level, right?

52
00:02:47,881 --> 00:02:51,240
So in a convolutional layer,
it's kind of like a flashlight.

53
00:02:51,270 --> 00:02:54,690
It's which is being applied over
every single pixel in the image.

54
00:02:54,900 --> 00:02:57,510
And it's looking for the most
relevant parts of that image.

55
00:02:57,690 --> 00:03:00,400
And so it's a multiplication
and a summation operation.

56
00:03:00,610 --> 00:03:04,310
But basically the convolution layer
will output a feature map. The these,

57
00:03:04,510 --> 00:03:08,590
this feature map represents a bunch of
features that it's learned from that

58
00:03:08,591 --> 00:03:12,400
image, right? So a set of features
that are represented by matrices.

59
00:03:12,430 --> 00:03:16,750
And so once we have these features that
are learned by this filtering operation,

60
00:03:16,960 --> 00:03:21,160
then we're going to apply a non linearity
to it, like a rectified linear unit,

61
00:03:21,161 --> 00:03:24,870
for example. And when we apply
a nonlinearity to it, it's,

62
00:03:24,990 --> 00:03:26,500
it serves a lot of purposes.

63
00:03:26,501 --> 00:03:31,270
The first purpose is so that the network
can learn both linear and nonlinear

64
00:03:31,300 --> 00:03:34,870
functions, right? Because neural networks
are universal function approximators.

65
00:03:35,080 --> 00:03:39,150
But this for, for rectified
linear unit in particular, uh,

66
00:03:39,370 --> 00:03:42,730
one of the reasons for using that
as opposed to the other types of

67
00:03:42,731 --> 00:03:47,500
nonlinearities is because it allows to
be is because it solves the vanishing

68
00:03:47,501 --> 00:03:49,660
gradient problem during backpropagation.

69
00:03:49,690 --> 00:03:51,910
So when we forward
propagate through a network,

70
00:03:52,210 --> 00:03:54,970
we are performing a series of operations.

71
00:03:55,150 --> 00:03:59,800
We find the output a class probability,
we compare that to the actual label,

72
00:03:59,920 --> 00:04:03,550
we compute an error value and we
use the error to compute a gradient.

73
00:04:03,730 --> 00:04:07,420
And a gradient tells us how to update
our weights as we back propagate it

74
00:04:07,421 --> 00:04:08,920
through the network.
And what the,

75
00:04:08,980 --> 00:04:13,980
what really does is it says it helps
solve the vanishing gradient problem

76
00:04:14,440 --> 00:04:17,500
because sometimes as a
gradient is back propagating,

77
00:04:17,590 --> 00:04:21,230
it gets smaller and smaller and
smaller so that the weight update is,

78
00:04:21,280 --> 00:04:25,120
is smaller and smaller. And this is not
good. We want, we want a big way update,

79
00:04:25,121 --> 00:04:29,360
right? We don't want that weight update
to vanish. And so relu helps prevent that,

80
00:04:29,361 --> 00:04:30,580
that gradient from vanishing.

81
00:04:30,970 --> 00:04:34,950
And so at the end of this convolutional
block is the pooling operation.

82
00:04:35,180 --> 00:04:38,650
So let's say you have a matrix, right?
And inside of that Matrix of pixel values,

83
00:04:38,651 --> 00:04:40,060
you have a bunch of numbers,
right?

84
00:04:40,210 --> 00:04:45,210
For Pixel intensity is between zero
and two 55 and what what pooling does,

85
00:04:45,550 --> 00:04:47,050
like Max pooling for example,

86
00:04:47,051 --> 00:04:51,760
is it says let's create sections for
all of these different pixel values and

87
00:04:51,761 --> 00:04:55,510
let's only take the maximum pixel value
from each section and propagate that

88
00:04:55,511 --> 00:04:56,200
forward.

89
00:04:56,200 --> 00:05:00,490
So it's a smaller section
that's propagated forward
and this and this increases

90
00:05:00,640 --> 00:05:02,260
and it speeds up the training time.

91
00:05:04,000 --> 00:05:07,030
And so if we look at comp nets,
they're actually, you know,

92
00:05:07,060 --> 00:05:10,240
anyone can really implement a
convolutional network these days, right?

93
00:05:10,241 --> 00:05:14,590
With libraries like carrots, which are
very high level, anyone can implement,

94
00:05:14,650 --> 00:05:19,420
implement a very powerful convolutional
network and just a few lines of code

95
00:05:19,421 --> 00:05:23,770
where each line of code corresponds
to a layer in the network.

96
00:05:23,920 --> 00:05:26,980
So it's actually, it's, it's
very easy these days to do that.

97
00:05:27,250 --> 00:05:31,240
And a lot of people can do that nowadays,
right? So once you define your network,

98
00:05:31,270 --> 00:05:34,540
each layer with its own line of code,
then you can compile it,

99
00:05:34,541 --> 00:05:37,510
you can define the optimizer and the,
and the loss function.

100
00:05:37,720 --> 00:05:40,720
You'll train it with the fit function
and then you can evaluate it on your

101
00:05:40,721 --> 00:05:41,554
testing data.

102
00:05:42,070 --> 00:05:47,070
So I've got this huge graphic here of
the modern history of object recognition,

103
00:05:47,290 --> 00:05:49,240
which I'm not going to
go into in detail here,

104
00:05:49,241 --> 00:05:52,540
but it's definitely something to check
out after you watch this video because

105
00:05:52,541 --> 00:05:55,840
I've got the link to this on my
slides in the video description,

106
00:05:56,830 --> 00:06:01,070
but it's a really detailed image of the
history of convolutional networks across

107
00:06:01,071 --> 00:06:04,820
all the different images,
all the different image net competitions,

108
00:06:04,821 --> 00:06:06,830
and all the improvements
that have been made.

109
00:06:07,070 --> 00:06:10,580
But I will go through some that I
think are really, really significant.

110
00:06:10,790 --> 00:06:14,620
So one of the first
improvements to CNNs w, uh,

111
00:06:14,630 --> 00:06:19,160
was the Alex net a network,
which was in 2012. Right?

112
00:06:19,161 --> 00:06:20,750
So there were some key improvements here.

113
00:06:20,751 --> 00:06:23,960
So the first one was an introduced
three Lw, which I talked about,

114
00:06:23,990 --> 00:06:26,390
which helps prevent the
vanishing gradient problem.

115
00:06:26,720 --> 00:06:29,150
It also introduced the concept of dropout.

116
00:06:29,270 --> 00:06:34,070
So dropout is a technique where neurons
are randomly turned on and off in each

117
00:06:34,071 --> 00:06:37,790
layer to prevent over fitting.
So if your data is too homogenous,

118
00:06:38,810 --> 00:06:42,290
it's not going to be able to classify
images that are similar but different

119
00:06:42,291 --> 00:06:45,830
because it's, it's to fit, it's
overfit to your training data.

120
00:06:45,980 --> 00:06:50,900
So dropout is a regularization technique
that prevents overfitting by randomly

121
00:06:50,901 --> 00:06:53,420
turning neurons on and off.
And by doing this,

122
00:06:53,600 --> 00:06:57,380
the data is forced to find
new pathways to go through,

123
00:06:57,530 --> 00:06:59,690
and because it's forced
to find new pathways,

124
00:06:59,840 --> 00:07:02,000
the network is able to generalize better.

125
00:07:02,180 --> 00:07:06,200
It also introduced the idea of data
augmentation. So convolutional networks,

126
00:07:06,201 --> 00:07:07,520
as awesome as they are,

127
00:07:07,760 --> 00:07:12,560
they're really bad at detecting they're
really bad at classifying images if

128
00:07:12,561 --> 00:07:15,620
they're in different rotations
or if they're upside down.

129
00:07:15,740 --> 00:07:20,630
It's got to be in the kind of exact spot
that it was trained on or very close to

130
00:07:20,631 --> 00:07:23,310
it. So what Alex Net did, or the w,

131
00:07:23,370 --> 00:07:26,930
so what the author of Alex
and that did is he input,

132
00:07:26,960 --> 00:07:31,960
he fed in different rotations of images
into Alex net than just the single

133
00:07:32,151 --> 00:07:36,230
rotation, and that made it able to
generalize better to different rotations.

134
00:07:36,560 --> 00:07:40,700
Lastly, it was a deeper network.
So they just added on more layers.

135
00:07:40,701 --> 00:07:43,310
And this improved the
classification accuracy.

136
00:07:44,180 --> 00:07:47,630
After that there was VGG net, which was
a major improvement. And the really,

137
00:07:47,631 --> 00:07:51,110
the only big difference there
was them adding more layers.

138
00:07:51,170 --> 00:07:55,580
That was it really a after that
there was Google in that, right?

139
00:07:55,580 --> 00:08:00,490
So for Google and as it looks like this,
but convolutions with different filters,

140
00:08:00,491 --> 00:08:05,030
sizes are pro were processed on the
same input and then can concatenate it

141
00:08:05,031 --> 00:08:06,950
together.
So in a single layer,

142
00:08:07,100 --> 00:08:11,450
rather than going through
just one convolutional
operation or set of operations,

143
00:08:11,451 --> 00:08:14,750
remember it's multiplications
and then a summation operation.

144
00:08:14,990 --> 00:08:19,050
It's several of those together, right?
So it's multiplying some multiplying,

145
00:08:19,080 --> 00:08:20,330
some multiplying some,

146
00:08:20,540 --> 00:08:24,020
and then it takes the outputs of all
of those and then it can catenate those

147
00:08:24,021 --> 00:08:27,590
together. And that's what it
propagates forward. And by doing this,

148
00:08:28,060 --> 00:08:31,820
it allowed it to learn better
feature representations at each,

149
00:08:32,060 --> 00:08:33,860
at each layer. Right? So,

150
00:08:35,270 --> 00:08:38,060
and then there was resonant
and so resonant was,

151
00:08:38,420 --> 00:08:42,590
the idea behind resonate was, well,
if we just keep stacking layers,

152
00:08:42,591 --> 00:08:45,530
is the network get better every time?
And the answer was no,

153
00:08:45,531 --> 00:08:48,650
it's good up to a certain point.
And then if you add more,

154
00:08:48,651 --> 00:08:52,400
there's a drop in performance.
So what resonant said was, okay,

155
00:08:52,520 --> 00:08:56,550
we know that there is some optimal limit
to the number of layers we should add.

156
00:08:56,700 --> 00:09:00,840
So after every two layers, let's add
this element wise addition operation.

157
00:09:01,110 --> 00:09:05,040
So it just added this operation and
this improved gradient propagation.

158
00:09:05,220 --> 00:09:09,120
This made backpropagation easier and
it helped further improve the vanishing

159
00:09:09,121 --> 00:09:13,470
gradient problem. And after that there
was dense net and dense net propose,

160
00:09:13,810 --> 00:09:17,700
uh, connecting entire blocks
of layers to one another.

161
00:09:17,820 --> 00:09:21,960
So it was a more complex
connection scheme. So, um,

162
00:09:22,050 --> 00:09:23,340
there are some patterns here,
right?

163
00:09:23,341 --> 00:09:25,530
So there's some patterns here
for all of these networks.

164
00:09:25,800 --> 00:09:30,270
The networks are designed to be deeper
and deeper and there's also computational

165
00:09:30,271 --> 00:09:34,580
tricks that are being added onto these
convolutional networks like relu or drop

166
00:09:34,581 --> 00:09:38,610
out or batch normalization that
improved performance. And lastly,

167
00:09:38,611 --> 00:09:42,840
there is an increasing use of connections
between the layers of the network.

168
00:09:42,960 --> 00:09:47,730
But Hinton said, okay, there is a
problem with convolutional networks.

169
00:09:47,760 --> 00:09:52,650
So remember that convolutional networks
learn to classify images, right?

170
00:09:52,651 --> 00:09:54,330
So in the lower low it's Lael.

171
00:09:54,800 --> 00:09:58,950
So in the lowest layers of a convolutional
network, it's going to learn the,

172
00:09:58,980 --> 00:10:02,190
the lowest level features of what
it's seeing. So for dogs for example,

173
00:10:02,370 --> 00:10:03,210
in the lowest layer,

174
00:10:03,211 --> 00:10:07,800
it's going to learn the edges and the
curvature of your ear and that the

175
00:10:07,801 --> 00:10:12,480
curvature of the dog's ear, maybe
like a single tooth. And then as we,

176
00:10:12,540 --> 00:10:16,650
as we go up the hierarchy as in,
as we, as we go to the next layer,

177
00:10:16,651 --> 00:10:17,730
and then the next layer,

178
00:10:18,000 --> 00:10:21,240
each of the features that it's
learning are going to be more complex.

179
00:10:21,300 --> 00:10:24,240
So in the first layer of their edges
and the next layer they're going to be

180
00:10:24,241 --> 00:10:28,290
shapes and the next layer they're going
to be more complex shaped like an entire

181
00:10:28,290 --> 00:10:31,380
year. And then finally in the last
layer, they're going to be very,

182
00:10:31,381 --> 00:10:36,370
very complex shapes like the entire body
of a dog, for example. Right. And this,

183
00:10:36,380 --> 00:10:39,960
this is very similar to how
the human visual cortex works.

184
00:10:40,170 --> 00:10:43,800
We know for certain that there is
some kind of hierarchy happening.

185
00:10:43,801 --> 00:10:45,680
Whenever we look at something that's,

186
00:10:45,720 --> 00:10:48,570
there's this hierarchy of
neurons that are firing in order.

187
00:10:48,720 --> 00:10:52,980
When we tried to recognize something
that we see that that's the high level of

188
00:10:52,981 --> 00:10:53,670
what we know.

189
00:10:53,670 --> 00:10:57,870
We don't know the exact intricate
details of how the routing mechanism is

190
00:10:57,871 --> 00:11:01,770
between layers, but we do know that there
is some kind of hierarchy happening.

191
00:11:01,800 --> 00:11:03,120
Tween each layer rights.

192
00:11:04,080 --> 00:11:07,920
So there is a problem with convolutional
networks though. There are two reasons.

193
00:11:08,160 --> 00:11:08,881
First of all,

194
00:11:08,881 --> 00:11:13,800
sub sampling loses the precise spatial
relationships between higher level parts

195
00:11:13,950 --> 00:11:16,020
such as a nose and a mouth,
right?

196
00:11:16,020 --> 00:11:20,280
So it's not enough to just be able to
classify a nose and a mouth, right?

197
00:11:20,610 --> 00:11:24,630
It's like if you have a nose in the left
corner of an image and then you have a

198
00:11:24,631 --> 00:11:28,080
mouth in the right corner of an image
and then you have eyes at the bottom,

199
00:11:28,230 --> 00:11:32,760
you can't just say, oh, it has these
three features. It must be a face. No,

200
00:11:32,761 --> 00:11:37,020
there's also a spatial correlation.
The eyes have to be above the nose,

201
00:11:37,140 --> 00:11:39,270
which have to be above
the mouth, right? So,

202
00:11:39,300 --> 00:11:43,620
but sub sampling or pooling
loses this relationship. Uh,

203
00:11:43,800 --> 00:11:47,970
and also they can't extrapolate their
understanding of geometric relationships

204
00:11:48,090 --> 00:11:50,730
to radically new viewpoints.
Like I said before,

205
00:11:50,820 --> 00:11:55,240
convolutional networks are really bad at
detecting an image if it's in any kind

206
00:11:55,241 --> 00:11:58,930
of different position, right? If
it's rotated, if it's upside down,

207
00:11:59,050 --> 00:12:02,910
if it's to the left, to the right, it's
gotta be in the kind of general, uh,

208
00:12:02,950 --> 00:12:06,820
position that it was of the images that
are trained on. And this is a problem,

209
00:12:06,821 --> 00:12:10,480
right? The idea is that instead
of invariants, we should be,

210
00:12:10,510 --> 00:12:12,790
we should be striving for Echo variants,
right?

211
00:12:12,791 --> 00:12:16,570
So the original goal of sub sampling
or pooling, it's the same thing.

212
00:12:16,571 --> 00:12:21,250
Sub sampling or pooling is it tries to
make the neural activities in variant to

213
00:12:21,251 --> 00:12:24,710
small changes in, in
viewpoint, right? So, uh,

214
00:12:24,940 --> 00:12:29,380
what that means is no matter what,
rotate, what position or rotation,

215
00:12:29,470 --> 00:12:33,190
some images in the neural
network responds in the same way.

216
00:12:33,191 --> 00:12:37,030
The data flow is the same,
but it's better to aim for ECQUA variants.

217
00:12:37,031 --> 00:12:40,750
That means that if we rotate an image,
the neural network should also change.

218
00:12:40,751 --> 00:12:45,250
It should also adapt to how it's
learning from this image or how it's

219
00:12:45,340 --> 00:12:48,160
processing. This image. So we need,

220
00:12:48,161 --> 00:12:53,161
so what we need is a network that's more
robust to to changes in two changes in

221
00:12:53,410 --> 00:12:57,580
transformations of how images
are positioned and we need,

222
00:12:57,670 --> 00:13:01,660
we also need networks that are more
easily able to generalize, right? That's,

223
00:13:01,661 --> 00:13:03,400
that's just the general thing.
And all of Ai,

224
00:13:03,401 --> 00:13:07,720
we need algorithms that are more able to
generalize to data that it's never seen

225
00:13:07,721 --> 00:13:11,320
before based on what it's trained on.
And also there's one more thing.

226
00:13:11,321 --> 00:13:15,010
There's a 66 day old paper on
convolutional networks that just came out,

227
00:13:15,250 --> 00:13:19,330
which is a pixel attack for fooling
deep neural networks. Basically,

228
00:13:19,331 --> 00:13:23,950
the authors found that they could tweak
just a few pixels in an image that

229
00:13:23,951 --> 00:13:28,300
otherwise looks exactly like a, a
perfect classification, like a dog,

230
00:13:28,330 --> 00:13:30,640
that it would,
that it would predict perfectly,

231
00:13:30,790 --> 00:13:32,890
but by just changing these few pixels,

232
00:13:33,040 --> 00:13:37,060
they found that the entire network's
classification was, was really bad.

233
00:13:37,360 --> 00:13:40,740
And this is a problem, right? That's the,
that's just not how it should be. That's,

234
00:13:40,900 --> 00:13:45,400
it doesn't make sense that it's that
it's that separable twin attack. Right.

235
00:13:45,880 --> 00:13:48,880
If we're thinking about, if we're
thinking about creating self driving cars,

236
00:13:49,180 --> 00:13:54,060
right? These huge machines that are
flying across the road and they're,

237
00:13:54,070 --> 00:13:55,990
they're using computer
vision to detect things,

238
00:13:56,350 --> 00:13:59,580
they can't be susceptible to these
kinds of a pixel attacks, right?

239
00:13:59,590 --> 00:14:01,180
They've got to be very robust.

240
00:14:01,780 --> 00:14:05,200
So Hinton introduced this
idea of the capsule network,

241
00:14:05,410 --> 00:14:06,880
and here is an image of it,

242
00:14:08,140 --> 00:14:13,140
but the kind of the basic idea that
Hinton had was the human brain much must

243
00:14:13,721 --> 00:14:16,930
achieve translational in
variants in a much better way.

244
00:14:16,931 --> 00:14:19,090
It's got to do something
other than pooling.

245
00:14:19,510 --> 00:14:23,680
And he posits that the brain has
these modules that he calls capsules,

246
00:14:23,860 --> 00:14:26,230
which are really good at handling
different types of visual,

247
00:14:26,260 --> 00:14:29,380
visual stimulus and in coding
things in a certain way.

248
00:14:29,620 --> 00:14:31,870
So CNNs do routing by pooling,

249
00:14:31,871 --> 00:14:36,871
they route how data is transferred across
each layer by this pooling operation,

250
00:14:37,241 --> 00:14:41,590
right? So if, if we input an
image, we apply a convolution,

251
00:14:41,680 --> 00:14:44,740
a convolutional operation to it,
and then a nonlinearity,

252
00:14:44,890 --> 00:14:49,360
and then we pull it based on what
the output of that pooling layer is,

253
00:14:49,510 --> 00:14:53,030
the image is going to go in a certain
for the next layer, right? Uh,

254
00:14:53,180 --> 00:14:56,900
but it's going to hit certain units
in the next layer based on pooling.

255
00:14:57,140 --> 00:15:00,530
But pooling is a very crude
way to route data, right?

256
00:15:00,590 --> 00:15:02,600
There's gotta be a
better way to route data.

257
00:15:02,930 --> 00:15:07,820
So the basic basic idea behind a capsule
network is it's just a neural network

258
00:15:07,970 --> 00:15:10,730
where instead of just
adding another layer, right?

259
00:15:10,731 --> 00:15:13,160
So usually we're adding
different types of layers.

260
00:15:13,240 --> 00:15:16,940
It instead it nests a
new layer inside a layer.

261
00:15:17,280 --> 00:15:20,360
So that's all it is. It's
where instead of saying, okay,

262
00:15:20,361 --> 00:15:24,080
we've got this single layer neural
network, let's add a different layer. No,

263
00:15:24,110 --> 00:15:26,810
inside of that layer,
let's add another layer.

264
00:15:26,840 --> 00:15:29,720
So it's a nested layer
inside an inside of a layer.

265
00:15:29,900 --> 00:15:33,380
And that nested layer is called a capsule,
which is a group of neurons.

266
00:15:33,920 --> 00:15:38,920
And so a typical layer of neurons or
units becomes a layer of capsules.

267
00:15:39,650 --> 00:15:44,650
So instead of making the ne the network
deeper in the sense of height or I guess

268
00:15:44,721 --> 00:15:45,830
you could even say with,

269
00:15:46,130 --> 00:15:49,460
it's making a deeper in terms of
nesting or inner structure and that's,

270
00:15:49,490 --> 00:15:50,720
that's all it is basically.

271
00:15:51,020 --> 00:15:55,880
And this model is more
robust to transformations
in terms of rotation or how

272
00:15:55,881 --> 00:15:59,900
this image is, I mean it is chief
state of the art for m and ist,

273
00:16:00,050 --> 00:16:01,700
which is a really,
it's a big deal.

274
00:16:01,820 --> 00:16:06,170
We'll see how it scales later on to to
you know, really, really huge datasets.

275
00:16:06,171 --> 00:16:09,260
But for M and ist, that's,
that's, that's a pretty big deal.

276
00:16:09,920 --> 00:16:14,920
So the capsule network has two really
key features and the first one is layer

277
00:16:15,051 --> 00:16:18,230
based squashing.
And the second one is dynamic routing.

278
00:16:18,440 --> 00:16:20,060
So in a typical neural network,

279
00:16:20,090 --> 00:16:24,200
only the output of a single unit is
squashed by a nonlinearity. Right?

280
00:16:24,201 --> 00:16:28,790
So we have a set of output neurons
and the based on the output of each,

281
00:16:28,791 --> 00:16:30,890
we apply a nonlinearity to each of them.

282
00:16:31,100 --> 00:16:34,790
So instead of applying a nonlinearity
to each individual neuron,

283
00:16:35,060 --> 00:16:39,890
we grouped these neurons into a capsule
and apply nonlinearity to the entire set

284
00:16:39,891 --> 00:16:42,020
of neurons.
The vector of those neurons.

285
00:16:42,740 --> 00:16:47,180
And so when we apply nonlinearity,
it's, it's to the entire layer, right?

286
00:16:47,360 --> 00:16:52,250
Instead of individual neurons and
also implements a dynamic routing.

287
00:16:52,251 --> 00:16:53,600
So instead of,
uh,

288
00:16:53,620 --> 00:16:58,620
so it's replaces the scalar output feature
detectors of CNNS with vector output

289
00:16:58,821 --> 00:16:59,654
capsules,

290
00:16:59,750 --> 00:17:03,860
and it replaces Max pooling
by routing by agreement.

291
00:17:04,190 --> 00:17:07,910
So each of these capsules in each layer,
when they forward propagate data,

292
00:17:08,060 --> 00:17:10,250
it goes to the next
most relevant capsules.

293
00:17:10,251 --> 00:17:15,020
So it's kind of like a hierarchical
tree of nested layers inside of layers.

294
00:17:15,230 --> 00:17:18,350
And the cost of this new
architecture is this,

295
00:17:18,590 --> 00:17:23,330
this is the routing algorithm, right?
So basically the difference here,

296
00:17:23,390 --> 00:17:23,831
the difference,

297
00:17:23,831 --> 00:17:27,800
the key difference here between a regular
convolutional network is the forward

298
00:17:27,801 --> 00:17:30,470
pass has an extra outer loop,
right?

299
00:17:30,471 --> 00:17:35,240
So it takes far iterations over all
the units instead of one to compute the

300
00:17:35,241 --> 00:17:37,580
output and the data flow looks a little,

301
00:17:37,610 --> 00:17:42,290
is a little more complicated because it's
saying for every capsule that's nested

302
00:17:42,291 --> 00:17:45,020
inside of a layer,
apply these operations,

303
00:17:45,021 --> 00:17:48,020
whether it's a soft max
or a squashing function.

304
00:17:49,350 --> 00:17:53,310
And what it does is it does make
the gradient harder to calculate.

305
00:17:53,311 --> 00:17:57,360
And the model may suffer from vanishing
gradient on larger datasets which could

306
00:17:57,361 --> 00:18:00,300
prevent it from scaling and
becoming the next big thing. Right?

307
00:18:00,540 --> 00:18:05,010
I mean in the paper they only applied it
to em and ist not hundreds of thousands

308
00:18:05,011 --> 00:18:05,641
of images.

309
00:18:05,641 --> 00:18:09,930
But I suspect that this network is
going to scale really well because a,

310
00:18:09,931 --> 00:18:12,780
it's Hinton and you know,
if Hinton says something's going to work,

311
00:18:12,930 --> 00:18:14,280
it's probably going to work.
Okay,

312
00:18:14,281 --> 00:18:17,130
so let's go ahead and look at some
code to see what this looks like.

313
00:18:17,370 --> 00:18:22,370
So what I found is this tensorflow
and implementation of capsule net,

314
00:18:22,650 --> 00:18:25,680
which is still in progress.
I mean this paper is relatively new,

315
00:18:25,800 --> 00:18:29,490
but I found this code that we
can look at a right now. So this,

316
00:18:29,491 --> 00:18:32,730
this was built in tensorflow,
right? So in this layer,

317
00:18:33,180 --> 00:18:36,180
notice how it's only got two imports,
your num py in tensorflow.

318
00:18:36,180 --> 00:18:39,780
So it's a really clean,
it's a clean architecture here.

319
00:18:40,050 --> 00:18:43,170
So it's got this capsule,
convolutional layer, right?

320
00:18:43,171 --> 00:18:46,340
So it's going to have a number of
outputs, a kernel size, you know,

321
00:18:46,341 --> 00:18:48,080
these hyper parameters like striding.

322
00:18:48,440 --> 00:18:50,720
And so it's got this if l
statement where it says,

323
00:18:50,750 --> 00:18:54,440
if we're not going to do the routing
scheme, build it this way, but if we are,

324
00:18:54,470 --> 00:18:58,220
build it this way. So let's look at
both. If we have our routing scheme,

325
00:18:58,221 --> 00:19:00,920
which we should have,
it's going to say,

326
00:19:00,970 --> 00:19:04,220
and we'll start off with a list of all
of our capsules and then we'll iterate

327
00:19:04,221 --> 00:19:08,540
through the number of units we specified
and we'll say, okay, so for each, uh,

328
00:19:08,900 --> 00:19:12,380
let's go ahead and create a convolutional
layer, like a standard tensorflow,

329
00:19:12,381 --> 00:19:17,030
convolutional layer, and store that inside
of the capsules, this capsule variable.

330
00:19:17,300 --> 00:19:21,140
And then we'll take the capsule variable
and appended to the capsule list, right?

331
00:19:21,380 --> 00:19:25,220
And so eventually we're going to have
all of these convolutional layers inside

332
00:19:25,221 --> 00:19:29,030
of this capsule slayer. So note, so
this is how it's nested, right? So,

333
00:19:30,110 --> 00:19:32,990
and once we,
and once we have all of these layers,

334
00:19:33,050 --> 00:19:35,200
we'll concatenate them together to proof.

335
00:19:35,390 --> 00:19:39,140
We'll concatenate them together and
then we'll squash them with this novel

336
00:19:39,141 --> 00:19:42,650
nonlinearity function. And so, but
if we do have this routing mechanism,

337
00:19:42,651 --> 00:19:46,400
which we wish the paper had,
we'll say, okay, let's again,

338
00:19:46,401 --> 00:19:50,300
we'll create a list of capsules and
then for however many outputs we want,

339
00:19:50,510 --> 00:19:55,430
let's create this capsule class added to
the list and upended and then return a

340
00:19:55,431 --> 00:19:58,960
tensor with the shape, right? So
if we look at this capsule, uh,

341
00:19:58,970 --> 00:20:01,610
implementation right
here, this, this class,

342
00:20:03,410 --> 00:20:05,980
basically this is the routing
mechanism right here, right?

343
00:20:05,981 --> 00:20:10,370
Just the one that we saw right here,
this routing algorithm where it's saying,

344
00:20:10,371 --> 00:20:12,960
okay, let's go through,
let's go through, uh,

345
00:20:13,430 --> 00:20:17,420
let's go through the number of iterations
we want and apply these operations to

346
00:20:17,421 --> 00:20:21,980
them. So notice that there
is a nonlinearity here that
the paper talked about,

347
00:20:22,100 --> 00:20:26,540
which is not Relu, it's, it's a novel
nonlinearity, which looks like this.

348
00:20:27,890 --> 00:20:31,250
This is, this is kind of interesting.
This, this nonlinearity that they used,

349
00:20:31,340 --> 00:20:32,670
that they found,
uh,

350
00:20:32,810 --> 00:20:37,520
was good for applying to a group of
neurons rather than a single drawn, right?

351
00:20:37,521 --> 00:20:40,520
So for Relu we apply
it to a single neuron,

352
00:20:40,730 --> 00:20:44,030
but when we're applying a nonlinearity
to a group of neurons or a capsule,

353
00:20:44,180 --> 00:20:48,640
they found that this a nonlinearity works.
The works worked the best for them.

354
00:20:48,940 --> 00:20:51,670
And so that's what, that's
what, uh, is, is happening here.

355
00:20:52,300 --> 00:20:55,600
So that's the capsule layer.
So once we have the capsule layer,

356
00:20:55,720 --> 00:20:58,180
we can import it here
into this capital network,

357
00:20:58,450 --> 00:21:00,940
and then we can go ahead and
build our architecture, right?

358
00:21:00,941 --> 00:21:04,570
So we'll start off by saying, okay,
so we'll start off by saying, okay,

359
00:21:04,571 --> 00:21:08,650
for each layer we have a primary
capsular and we have a digit caps layer.

360
00:21:08,890 --> 00:21:13,840
Let's go ahead and add our a capsule,
a capital layer to them. So these are,

361
00:21:13,841 --> 00:21:16,780
these are both capsule layers,
right? So for each of these layers,

362
00:21:16,910 --> 00:21:21,010
there is a nested convolutional
network inside of each of those layers,

363
00:21:21,100 --> 00:21:24,550
which is a capsule.
And then inside of those capsules,

364
00:21:24,670 --> 00:21:26,650
the nonlinearity is applied to it.
The,

365
00:21:26,651 --> 00:21:30,160
the novel squashing function
that we saw in the paper.

366
00:21:31,570 --> 00:21:35,860
And then at the very end, by the way, at
the very end of this network, after it's,

367
00:21:35,861 --> 00:21:40,060
it's applied these operations
to each layer with each capsule,

368
00:21:40,270 --> 00:21:43,810
it applies this decoder
algorithm. So, um, and the paper,

369
00:21:43,811 --> 00:21:47,500
they found that at the end of the
network they could just use a decoder to

370
00:21:47,501 --> 00:21:50,110
reconstruct a digit from
the digit caps layer.

371
00:21:50,111 --> 00:21:54,160
So after it's applied this
first set of a capsule,

372
00:21:54,610 --> 00:21:56,830
this first set of capsule operations,

373
00:21:57,010 --> 00:22:02,010
then it then it can reconstruct the input
from that learned representation and

374
00:22:02,771 --> 00:22:07,660
then apply a reconstruction loss, uh,
to improve that learned representation.

375
00:22:08,170 --> 00:22:12,400
And that's what this is. This is the
reconstruction loss that is used to, uh,

376
00:22:12,430 --> 00:22:17,000
learn that representation and improve
that representation over time. So, uh,

377
00:22:17,030 --> 00:22:20,650
this and tensorflow implementation
is a work in progress. Uh,

378
00:22:20,670 --> 00:22:24,220
I've got a link to it in the video
description has lost my slides. Uh,

379
00:22:24,310 --> 00:22:27,910
but if you want you can train it. I've,
I've, I've been training it as well,

380
00:22:28,180 --> 00:22:29,500
and you just need to,

381
00:22:31,690 --> 00:22:33,640
all you have to do to train
it has just run python,

382
00:22:33,641 --> 00:22:37,480
train.py and it'll start training and
you'll see the loss decrease over time.

383
00:22:37,810 --> 00:22:40,240
Please subscribe for more
programming videos. And for now,

384
00:22:40,270 --> 00:22:43,210
I'm going to learn more about capsules.
So thanks for watching.

