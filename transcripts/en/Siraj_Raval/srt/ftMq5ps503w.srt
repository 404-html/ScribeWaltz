1
00:00:00,170 --> 00:00:02,600
Can we actually predict stock
prices with machine learning?

2
00:00:03,200 --> 00:00:06,620
Investors make educated
guesses by analyzing data.

3
00:00:06,860 --> 00:00:10,100
They'll read the news study of the
company history industry trends.

4
00:00:10,190 --> 00:00:13,400
There are lots of data points
that go into making a prediction.

5
00:00:13,670 --> 00:00:17,540
The prevailing theory is that stock prices
are totally random and unpredictable.

6
00:00:17,690 --> 00:00:20,330
A blindfolded monkey throwing
darts at a newspaper,

7
00:00:20,331 --> 00:00:24,470
financial pages could select a portfolio
that would do just as well as one

8
00:00:24,471 --> 00:00:28,160
carefully selected by experts.
But that raises the question,

9
00:00:28,400 --> 00:00:32,900
why do top firms like Morgan Stanley and
Citi group higher quantitative analysts

10
00:00:32,990 --> 00:00:34,280
to build predictive models?

11
00:00:34,370 --> 00:00:38,630
We have this idea of a trading floor being
filled with adrenaline and abused men

12
00:00:38,631 --> 00:00:42,020
with loose ties running around
yelling something into a phone.

13
00:00:42,021 --> 00:00:46,520
But these days are more likely to see
rows of machine learning experts quietly

14
00:00:46,521 --> 00:00:48,830
sitting in front of computer screens.
In fact,

15
00:00:48,831 --> 00:00:53,810
about 70% of all hoarders on Wall
Street are now placed by software.

16
00:00:53,990 --> 00:00:56,990
We're now living in the age
of the algorithm. Hello world,

17
00:00:56,991 --> 00:01:01,300
it's Saroj and today we're going to build
a deep learning model to predict stock

18
00:01:01,301 --> 00:01:06,040
prices, records a prices for traded
commodities go back thousands of years.

19
00:01:06,430 --> 00:01:11,080
Merchants along popular silk routes
would keep records of traded goods to try

20
00:01:11,081 --> 00:01:14,710
and predict price trends so that they
could benefit from them and finance.

21
00:01:14,711 --> 00:01:18,730
The field of quantitative analysts is
about 25 years old and even now it's still

22
00:01:18,731 --> 00:01:22,690
not fully accepted, understood or
widely used, just like Google plus.

23
00:01:22,990 --> 00:01:26,710
It's the study of how certain variables
correlate with stock price behavior.

24
00:01:26,920 --> 00:01:30,190
One of the first attempt at this
was made in the 70s by two British

25
00:01:30,191 --> 00:01:34,150
statisticians and box and Jenkins
using mainframe computers.

26
00:01:34,380 --> 00:01:38,830
The only historical data they had access
to where prices and volume they call

27
00:01:38,831 --> 00:01:42,490
their model or Rema, and at the time
it was slow and expensive to run,

28
00:01:42,670 --> 00:01:45,340
but by the eighties things
started to get interesting.

29
00:01:45,490 --> 00:01:48,190
Spreadsheets were invented so
that firms could model companies,

30
00:01:48,191 --> 00:01:52,000
financial performance and automated data
collection became a reality and with

31
00:01:52,001 --> 00:01:56,050
improvements in computing power
models could analyze data much faster.

32
00:01:56,200 --> 00:02:00,580
It was a renaissance on Wall
Street. People were excited
about the possibilities.

33
00:02:00,770 --> 00:02:03,400
They started showing up at seminars
and discussing their techniques.

34
00:02:03,580 --> 00:02:05,680
You should see what's going on
at the bigger firms. I mean,

35
00:02:05,681 --> 00:02:08,620
I know all the information,
but all of this quickly died down.

36
00:02:08,621 --> 00:02:12,940
Once people realize that what works is
actually a very valuable secret. Alright,

37
00:02:13,030 --> 00:02:17,020
get the fuck off. Since then, the most
successful quants have gone underground.

38
00:02:17,350 --> 00:02:18,280
In the past few years,

39
00:02:18,281 --> 00:02:22,990
we've seen lots of academic
papers published using
neural nets to predict stock

40
00:02:22,991 --> 00:02:26,470
prices with varying degrees of success.
But until recently,

41
00:02:26,471 --> 00:02:30,610
the ability to build these models has
been restricted to academics who spend

42
00:02:30,611 --> 00:02:34,720
their days writing very complex code.
Now with libraries like tensorflow,

43
00:02:35,020 --> 00:02:39,370
anyone can build powerful predictive
models trained on massive data sets.

44
00:02:39,490 --> 00:02:44,050
So let's build our own model using
kiosk with a tensorflow backend.

45
00:02:44,680 --> 00:02:45,790
For our training data,

46
00:02:45,791 --> 00:02:50,791
we'll be using the daily closing
price of the s and p 500 from January,

47
00:02:51,070 --> 00:02:52,270
2002 August,

48
00:02:52,270 --> 00:02:56,290
2016 this is a series of data points
indexed in time order or a time series.

49
00:02:56,470 --> 00:03:00,160
Our goal will be to predict the closing
price for any given after training,

50
00:03:00,250 --> 00:03:03,040
we can load our data using
a custom load data function.

51
00:03:03,070 --> 00:03:07,520
It essentially just reads our CSV file
into an array of values and normalizes

52
00:03:07,570 --> 00:03:10,330
them. Rather than feeding those
values directly into our model.

53
00:03:10,540 --> 00:03:12,850
Normalizing them improves convergence.

54
00:03:13,060 --> 00:03:17,260
We use this equation to normalize each
value to reflect percentage changes from

55
00:03:17,261 --> 00:03:18,094
the starting points,

56
00:03:18,130 --> 00:03:22,000
so we'll divide each price by the initial
price and subtract one when our model

57
00:03:22,001 --> 00:03:26,140
later makes a prediction will denormalize
the data using this formula to get a

58
00:03:26,141 --> 00:03:28,750
real world number out of it.
To build our model,

59
00:03:28,751 --> 00:03:32,650
we'll first initialize it as sequential
since it will be a linear stack of

60
00:03:32,651 --> 00:03:37,120
layers. Then we'll add our first layer,
which is an LSTM layer. So what is this?

61
00:03:37,600 --> 00:03:38,433
Let's back up for it.

62
00:03:39,380 --> 00:03:41,710
Recognizes feet single Eric's with me.

63
00:03:42,460 --> 00:03:47,460
You don't have to say what you did I
already know sound out from it's easy.

64
00:03:49,650 --> 00:03:52,260
Call the words forward,
but could we sing them

65
00:03:52,570 --> 00:03:53,403
words?

66
00:03:55,460 --> 00:03:59,630
No. The reason for this is because
we learned these words in a sequence.

67
00:03:59,660 --> 00:04:04,010
It's conditional memory. We can access
a word if we access the words before it.

68
00:04:04,370 --> 00:04:08,510
Memory matters when we have sequences
are thoughts have persistence,

69
00:04:08,750 --> 00:04:13,160
but feedforward neural nets don't they
except a fixed size vector as input like

70
00:04:13,161 --> 00:04:13,910
an image.

71
00:04:13,910 --> 00:04:17,420
So we couldn't use it to say predict
the next frame and a movie because that

72
00:04:17,421 --> 00:04:21,650
would require a sequence of image
vectors as inputs, not just one.

73
00:04:21,651 --> 00:04:25,640
Since the probability of a certain event
happening would depend on what happened

74
00:04:25,700 --> 00:04:26,960
every frame before it.

75
00:04:27,410 --> 00:04:31,610
We need a way to allow information
to persist and that's why we'll use a

76
00:04:31,611 --> 00:04:36,440
recurrent neural net. Recurrent Nets can
accept sequences of vectors as inputs,

77
00:04:36,590 --> 00:04:38,870
so recall that for
feedforward neural nets,

78
00:04:39,080 --> 00:04:42,590
the hidden layers weights are
based only on the input data,

79
00:04:42,710 --> 00:04:43,850
but in a recurrent net.

80
00:04:44,030 --> 00:04:48,470
The hidden layer is a combination of the
input data at the current time step and

81
00:04:48,471 --> 00:04:50,420
the hidden layer at a previous time step.

82
00:04:50,630 --> 00:04:53,960
The hidden layer is constantly
changing as it gets more inputs.

83
00:04:54,080 --> 00:04:58,130
And the only way to reach these hidden
states is with the correct sequence of

84
00:04:58,131 --> 00:05:02,390
inputs. This is how memory is incorporated
in and we can model this process

85
00:05:02,570 --> 00:05:03,403
mathematically.

86
00:05:03,560 --> 00:05:07,610
So this hidden state at a given time
step is a function of the input at that

87
00:05:07,611 --> 00:05:11,820
same time step modified by a weight
matrix like the ones used in feed forward

88
00:05:11,821 --> 00:05:16,550
nets added to the hidden state of the
previous time step multiplied by its own

89
00:05:16,670 --> 00:05:18,680
hidden state to hidden state matrix,

90
00:05:18,920 --> 00:05:22,520
otherwise known as a transition matrix
and because this feedback loop is

91
00:05:22,521 --> 00:05:24,710
occurring at every time
step in the series,

92
00:05:24,800 --> 00:05:29,120
each hidden state has traces of not only
the previous hidden state but also of

93
00:05:29,150 --> 00:05:30,560
all of those that proceeded it.

94
00:05:30,890 --> 00:05:34,640
That's why we call it recurrent in a
way we can think of it as copies of the

95
00:05:34,641 --> 00:05:37,580
same network,
each passing a message to the next.

96
00:05:37,640 --> 00:05:39,740
So that's the great thing
about recurrent nets.

97
00:05:39,741 --> 00:05:42,920
They're able to connect previous
data with the present task,

98
00:05:43,100 --> 00:05:46,220
but we still have a problem.
Take a look at this paragraph.

99
00:05:46,400 --> 00:05:50,420
It starts off with I hope Senpai will
notice me and end with she is my friend.

100
00:05:50,510 --> 00:05:51,500
He is my Senpai.

101
00:05:51,800 --> 00:05:55,400
Let's say we wanted to train a model
to predict this last word given all the

102
00:05:55,401 --> 00:05:56,234
other words.

103
00:05:56,330 --> 00:06:00,470
We need the from the very beginning of
the sequence to know that this word is

104
00:06:00,471 --> 00:06:05,360
probably Senpai, not something like buddy
or mate and irregular recurrent nets.

105
00:06:05,361 --> 00:06:08,330
Memories become more subtle
as they fade into the past.

106
00:06:08,510 --> 00:06:12,530
Since the error signal from later time
steps doesn't make it far enough back in

107
00:06:12,531 --> 00:06:13,870
time to influence it.

108
00:06:13,890 --> 00:06:18,380
Network at earlier time steps during
backpropagation Yoshua Bengio called this

109
00:06:18,381 --> 00:06:22,370
the vanishing gradient problem in one of
his most frequently cited papers titled

110
00:06:22,580 --> 00:06:25,790
Learning Longterm dependencies
with grading. Dissent is difficult.

111
00:06:26,000 --> 00:06:26,840
Love the bluntness.

112
00:06:27,170 --> 00:06:31,220
A popular solution to this is
a modification to recurrent
nets called long short

113
00:06:31,221 --> 00:06:32,054
term memory.

114
00:06:32,210 --> 00:06:36,830
Normally neurons are units that apply an
activation function like a sigmoid to a

115
00:06:36,831 --> 00:06:40,250
linear combination of their
inputs in an LSTM recurrent net.

116
00:06:40,430 --> 00:06:43,850
We instead replaced these neurons
with water called memory cells.

117
00:06:44,030 --> 00:06:48,620
Each cell has an input gate on output
gate and an internal state that feeds into

118
00:06:48,621 --> 00:06:51,500
itself across time steps with
a constant weight of one.

119
00:06:51,770 --> 00:06:55,430
This eliminates the vanishing gradient
problem since any gradient that flows

120
00:06:55,431 --> 00:06:59,690
into this self recurrent unit during
back prop is preserved indefinitely since

121
00:06:59,870 --> 00:07:02,480
errors multiplied by one
still have the same value.

122
00:07:02,630 --> 00:07:05,240
Each gate is an activation
function like sigmoid.

123
00:07:05,540 --> 00:07:09,380
During the forward pass the input gate
learns went to late activation pass into

124
00:07:09,381 --> 00:07:12,770
the cell and the Applegate learns
went to let activation pass out of it.

125
00:07:13,100 --> 00:07:16,940
During the backward pass the Applegate
Lauren's went to let air flow into the

126
00:07:16,941 --> 00:07:20,450
cell and the implicate ones went to let
it flow out of the cell through the rest

127
00:07:20,451 --> 00:07:24,500
of the network. So despite everything
else in a recurrent net staying the same,

128
00:07:24,650 --> 00:07:28,850
doing this more powerful update equation
for our hidden state results in our

129
00:07:28,851 --> 00:07:33,740
network, being able to remember longterm
dependencies. So for our LSTM layer,

130
00:07:33,741 --> 00:07:37,940
we'll set our input dimension to one
and say we want 50 units in this layer.

131
00:07:38,150 --> 00:07:42,650
Setting returned sequences to true means
this layer is output is always fed into

132
00:07:42,651 --> 00:07:46,880
the next layer. All Its activations can
be seen as the sequence of predictions.

133
00:07:47,060 --> 00:07:49,550
This first layer has made
from the inputs sequence.

134
00:07:49,760 --> 00:07:52,280
We'll add 20% dropout to this layer.

135
00:07:52,281 --> 00:07:57,110
Then initialize our second layer as
another LSTM with 100 units and set return

136
00:07:57,170 --> 00:07:58,520
sequence to false on it.

137
00:07:58,521 --> 00:08:01,880
Since it's output is only fed to the
next layer at the end of the sequence,

138
00:08:02,090 --> 00:08:04,190
it doesn't output a
prediction for the sequence.

139
00:08:04,220 --> 00:08:07,370
Instead a prediction director
for the whole input sequence,

140
00:08:07,610 --> 00:08:10,820
we'll use the linear dense layer to
aggregate the data from this prediction

141
00:08:10,821 --> 00:08:12,440
vector into one single value.

142
00:08:12,770 --> 00:08:16,700
Then we can compile our model using a
popular loss function called mean squared

143
00:08:16,701 --> 00:08:20,480
error and use grading dissent as
our optimizer labeled rms prop.

144
00:08:20,660 --> 00:08:22,430
We'll train our model
with the fic function.

145
00:08:22,520 --> 00:08:26,690
Then we can test it to see what it
predicts for the next 50 steps at several

146
00:08:26,691 --> 00:08:30,020
points in our graph and
visualize it using map plot line.

147
00:08:30,350 --> 00:08:33,530
It seems that for a lot of the price
movements, especially the big ones,

148
00:08:34,010 --> 00:08:37,790
there is quite the correlation between
our model's prediction and the actual

149
00:08:37,791 --> 00:08:41,020
data. So time to make some
money and play some too,

150
00:08:41,540 --> 00:08:45,650
but we'll our model be able to correctly
predict the closing price 100% of the

151
00:08:45,651 --> 00:08:47,660
time.
Hell to the no.

152
00:08:48,080 --> 00:08:52,280
It's an analytical tool to help us make
educated guesses about the direction of

153
00:08:52,281 --> 00:08:56,070
the market that is slightly better
than random. So to break it down,

154
00:08:56,160 --> 00:08:59,670
recurrent nets can model sequential data.
Since at each time step,

155
00:08:59,790 --> 00:09:03,030
the hidden state is effected by the
input and the previous hidden state.

156
00:09:03,360 --> 00:09:07,230
A solution to the vanishing gradient
problem for recurrent nets is to use long

157
00:09:07,231 --> 00:09:12,090
short term memory cells to remember
longterm dependencies and we can use LSTM

158
00:09:12,091 --> 00:09:16,170
networks to make predictions for time
series data easily using carrots.

159
00:09:16,320 --> 00:09:17,153
And tensorflow.

160
00:09:17,280 --> 00:09:20,340
The winter of the coding challenge from
the last video is the Shaw bought you.

161
00:09:20,540 --> 00:09:23,940
We shall use transfer learning to
create a classifier for cats and dogs.

162
00:09:24,090 --> 00:09:28,050
He chose a layer from a pretrained
tensorflow model and built his own custom

163
00:09:28,170 --> 00:09:32,460
convolutional net on top of it to make
training much faster. Wizard of the week,

164
00:09:32,640 --> 00:09:34,770
and the runner up is Jeshurun.
See,

165
00:09:34,950 --> 00:09:39,060
I loved how he added a command line
interface for users to input their images.

166
00:09:39,300 --> 00:09:43,140
The coding challenge for this video is
to use three different inputs instead of

167
00:09:43,141 --> 00:09:47,340
just one to train your LSTM network
to predict the price of Google stock

168
00:09:47,610 --> 00:09:49,800
detailed during in the read me
poser gambling in the comments,

169
00:09:49,801 --> 00:09:51,150
and I'll announce the winner in a week.

170
00:09:51,370 --> 00:09:53,470
Please subscribe for more
videos like this. And for now,

171
00:09:53,560 --> 00:09:57,760
I've got to count my stacks of layers,
so thanks for watching.

