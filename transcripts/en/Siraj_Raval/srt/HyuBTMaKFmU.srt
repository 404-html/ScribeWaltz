1
00:00:00,070 --> 00:00:00,761
Hello world.

2
00:00:00,761 --> 00:00:05,650
It's Saroj and let's train an AI to create
a set of topics for any news article

3
00:00:05,651 --> 00:00:06,520
that we give it.

4
00:00:06,760 --> 00:00:10,060
We can frame the machine learning
process a lot of different ways.

5
00:00:10,210 --> 00:00:14,050
Learning from data that has
labels, learning from data
that doesn't have labels,

6
00:00:14,170 --> 00:00:16,570
learning how to act on continuous data,

7
00:00:16,810 --> 00:00:19,960
but a more application
focused way of framing.

8
00:00:19,961 --> 00:00:24,961
The learning process is as
either a discriminative or
generative discriminative

9
00:00:25,781 --> 00:00:30,520
models. Tell us what some data is.
They discriminate, differentiates,

10
00:00:30,521 --> 00:00:33,580
classify, call it what you
will. They ask the question,

11
00:00:33,610 --> 00:00:37,510
is this a picture of bitcoin
or bitcoin cash? Seriously,

12
00:00:37,511 --> 00:00:42,310
what is going on there? What language
is this text? Who is singing this song?

13
00:00:42,430 --> 00:00:45,700
What show is this?
What type of object is this?

14
00:00:45,880 --> 00:00:50,680
Generative models are even more exciting.
They generate new data, new images,

15
00:00:50,681 --> 00:00:53,470
new videos, new music, new texts,

16
00:00:53,590 --> 00:00:58,030
which reminds me of a video by
another youtuber named Carey. For now,

17
00:00:58,150 --> 00:01:00,190
just know that there are two neural nets,

18
00:01:00,250 --> 00:01:04,180
a generator and a discriminator that are
each co-evolving evolving to outsmart

19
00:01:04,181 --> 00:01:07,840
the other. Definitely subscribe
to him, put more formally.

20
00:01:07,841 --> 00:01:12,520
Most of the advances in machine learning
have been in discriminative models

21
00:01:12,700 --> 00:01:17,350
where we try to estimate a function
called the posterior probability.

22
00:01:17,560 --> 00:01:22,390
That is the probability of y given x
where x is an input sample and why as an

23
00:01:22,391 --> 00:01:23,050
output.

24
00:01:23,050 --> 00:01:27,260
So we can imagine x to be an image and
why to be the kind of object that is in

25
00:01:27,261 --> 00:01:28,900
the image like a Tesla.

26
00:01:29,230 --> 00:01:33,790
It tells us how much the model believes
that there is a Tesla given an input

27
00:01:33,791 --> 00:01:38,230
image compared to all possibilities.
It knows about generative models.

28
00:01:38,260 --> 00:01:41,590
Instead estimate a function
called the joint probability,

29
00:01:41,830 --> 00:01:44,170
the probability of y and x.

30
00:01:44,260 --> 00:01:48,400
That is the probability that x is
an image and there is a Tesla in it.

31
00:01:48,430 --> 00:01:49,330
At the same time,

32
00:01:49,510 --> 00:01:54,070
the reason we estimate
the joint probability for
generative models is because

33
00:01:54,071 --> 00:01:58,570
using it, we can generate images
of Teslas by sampling car types.

34
00:01:58,571 --> 00:02:02,470
Why and new images acts from
the probability of y index.

35
00:02:02,560 --> 00:02:06,880
There are so many awesome generative
models out there that we could go over.

36
00:02:07,030 --> 00:02:09,820
Auto encoders,
try to reconstruct the input data,

37
00:02:09,970 --> 00:02:14,970
then we can use it's learned dense
representation to generate new types of

38
00:02:15,041 --> 00:02:19,900
similar data. Generative adversarial
networks consist of two neural networks,

39
00:02:20,080 --> 00:02:25,030
one trying to fool the other one by
presenting fake but realistic looking data

40
00:02:25,270 --> 00:02:27,100
as the discriminator improves.

41
00:02:27,190 --> 00:02:32,190
So does the generator until the generated
data is indistinguishable from the

42
00:02:32,291 --> 00:02:33,124
real thing.

43
00:02:45,850 --> 00:02:50,850
But let's start with a relatively basic
one called latent Dirichlet allocation

44
00:02:51,010 --> 00:02:55,360
or Lda Derris. Schley is a type
of distribution like the Gaussian,

45
00:02:55,480 --> 00:03:00,480
it's specified by a vector parameter
containing variables corresponding to each

46
00:03:01,001 --> 00:03:03,190
topic which we write as such.

47
00:03:03,430 --> 00:03:06,640
These are direct laid distributions
for three different topics.

48
00:03:06,790 --> 00:03:10,720
The bottom triangle has a side for each
topic and the closer a point on the

49
00:03:10,721 --> 00:03:14,350
triangle is to aside the higher
the probability of a topic.

50
00:03:14,500 --> 00:03:19,060
The curb is the probability density
function over the mixture of topics,

51
00:03:19,120 --> 00:03:22,330
so the edges of the triangle
all have a zero probability.

52
00:03:22,540 --> 00:03:27,220
Just like the probability Facebook has a
super intelligent AI sensationalist say

53
00:03:27,221 --> 00:03:27,940
yes,

54
00:03:27,940 --> 00:03:32,500
the word latent is there because a
variable we have to infer rather than

55
00:03:32,501 --> 00:03:34,840
directly observe is
called a latent variable.

56
00:03:34,900 --> 00:03:38,530
Since were directly observing
the words and not the topics,

57
00:03:38,560 --> 00:03:42,720
we refer to the topics as the latent
variables as well as a distributions

58
00:03:42,730 --> 00:03:47,530
themselves. We are allocating Leighton
Dera Schley distributions over text data.

59
00:03:47,800 --> 00:03:52,600
LDA is a way to automatically discover
topics from a set of documents and was

60
00:03:52,601 --> 00:03:56,350
created by Andrew [inaudible] who
popularize it in his Coursera course on

61
00:03:56,351 --> 00:03:59,630
machine learning. One of the things
I learned both when I was um,

62
00:03:59,890 --> 00:04:02,800
running the Google team and
now building a team at Baidu

63
00:04:04,960 --> 00:04:09,790
topic modeling isn't efficient way to
analyze large volumes of text data.

64
00:04:10,000 --> 00:04:13,000
There are many use cases for it.
Search engine rankings,

65
00:04:13,180 --> 00:04:16,240
analyzing how research topics
evolve in a conference,

66
00:04:16,450 --> 00:04:19,840
finding out the interests of
different users using their tweets.

67
00:04:20,050 --> 00:04:22,270
Suppose we have a collection of sentences.

68
00:04:22,480 --> 00:04:26,980
We could use Lda to automatically
discover topics that the sentence is

69
00:04:26,981 --> 00:04:31,630
contained. If we want to topics, then
the first topic would be a list of words,

70
00:04:31,780 --> 00:04:36,780
each with percent values that indicate
how relevant they are given a topic in

71
00:04:36,881 --> 00:04:39,700
that category.
The same goes for the second topic.

72
00:04:39,880 --> 00:04:44,800
Then it assigns percent values to each
sentence to define how relevant each

73
00:04:44,801 --> 00:04:46,360
topic is to that sentence.

74
00:04:51,250 --> 00:04:53,440
I think so.
I'm not sure what it was.

75
00:04:53,470 --> 00:04:55,720
I conditioned this result on a
couple of different data sets.

76
00:04:56,490 --> 00:05:01,420
So you applied for it.
Yup. I love it. Well,

77
00:05:01,840 --> 00:05:06,700
we shouldn't submit yet until we can
replicate and we got two weeks left.

78
00:05:06,940 --> 00:05:08,040
I'm going to write the paper.

79
00:05:08,410 --> 00:05:12,190
We can think of.
This whole thing has a three step process.

80
00:05:12,400 --> 00:05:15,280
We tell the algorithm how many
topics we think there are.

81
00:05:15,490 --> 00:05:19,120
The algorithm will assign every
word to a temporary topic.

82
00:05:19,360 --> 00:05:23,920
Then the algorithm checks and updates the
topic assignments looping through each

83
00:05:23,921 --> 00:05:27,400
word in every document.
Now it's look at it programmatically.

84
00:05:27,550 --> 00:05:32,410
We can import our documents using the
pandas library to load our data points

85
00:05:32,411 --> 00:05:35,080
into memory as a data frame object.

86
00:05:35,350 --> 00:05:38,830
We're going to perform some cleaning
steps here straight out of Compton.

87
00:05:38,860 --> 00:05:42,850
I mean the natural language processing
playbook. Since our words matter,

88
00:05:43,000 --> 00:05:46,660
three steps, tokenizing, stopping
and stemming tokenizing segments.

89
00:05:46,661 --> 00:05:50,320
A document into individual tokens
that we define the size of.

90
00:05:50,440 --> 00:05:52,780
Since we're applying
Lda at the word level,

91
00:05:52,810 --> 00:05:57,220
we'll tokenize or document into words.
Once we've converted our documents,

92
00:05:57,500 --> 00:06:00,890
a sack of tokens,
we'll want to remove stop words.

93
00:06:01,070 --> 00:06:06,070
These are the meaningless words that
won't contribute to our topics like for an

94
00:06:06,200 --> 00:06:08,270
or and art history majors.

95
00:06:08,660 --> 00:06:11,630
The very definition of
a stop word is flexible.

96
00:06:11,631 --> 00:06:15,200
We can define that list ourselves
or we can use a predefined list.

97
00:06:15,440 --> 00:06:17,660
It also depends on the topic or modeling.

98
00:06:17,870 --> 00:06:22,550
If we're modeling a collection of music
reviews and been terms like the who will

99
00:06:22,551 --> 00:06:25,940
be hard to model since the
will usually be removed,

100
00:06:25,941 --> 00:06:29,480
even though it's a part of the band name.
Next we'll perform stemming.

101
00:06:29,690 --> 00:06:34,690
This combines similar tokens in our case
words into a single token like a a an a

102
00:06:36,500 --> 00:06:37,880
could be reduced to a.

103
00:06:38,120 --> 00:06:43,120
Now we want to create a document term
matrix because we need to understand the

104
00:06:43,821 --> 00:06:47,960
frequency with which each term
occurs within each document.

105
00:06:48,140 --> 00:06:53,120
The rows are docs in the collection and
the columns are the terms will traverse

106
00:06:53,121 --> 00:06:54,230
through our documents,

107
00:06:54,260 --> 00:06:59,260
assigning unique integer ids to each
unique token while also counting words.

108
00:07:00,560 --> 00:07:02,480
All of this will be
stored in a dictionary.

109
00:07:02,750 --> 00:07:07,700
Now we're ready to generate an LDA
model using this document term matrix.

110
00:07:07,880 --> 00:07:12,440
We'll iterate through every document we
have in our dataset and randomly assign

111
00:07:12,500 --> 00:07:15,740
each word in the current
document to one of our topics.

112
00:07:15,860 --> 00:07:20,540
We'll go through each word in the
documents and for each topic compute to

113
00:07:20,541 --> 00:07:23,480
probabilities.
Then reassigned a new topic.

114
00:07:23,570 --> 00:07:25,550
By using these probabilities,

115
00:07:25,790 --> 00:07:29,990
the probability that we place a word in
the correct topic will increase every

116
00:07:29,991 --> 00:07:34,370
time since we will place more and
more words into the correct topic,

117
00:07:34,520 --> 00:07:39,050
we can examine the results by printing
out the number of topics we chose as well

118
00:07:39,051 --> 00:07:43,970
as a number of words per topic and these
topics seem very news related and the

119
00:07:43,971 --> 00:07:45,950
words seem to be very
relevant to the topic.

120
00:07:45,951 --> 00:07:48,980
So I think we did a pretty
good job to summarize,

121
00:07:48,981 --> 00:07:53,981
generative models learn a
joint probability distribution
between input data and

122
00:07:54,351 --> 00:07:55,100
the labels.

123
00:07:55,100 --> 00:07:59,750
While this discriminative models learn
the conditional probability distribution

124
00:07:59,870 --> 00:08:00,703
between them.

125
00:08:01,160 --> 00:08:05,690
Topic modeling is all about finding
the hidden thematic structure in a

126
00:08:05,691 --> 00:08:10,691
collection of documents and latent
Dirichlet allocation is a generative topic

127
00:08:11,871 --> 00:08:14,720
modeling technique that can
be implemented pretty easily.

128
00:08:15,110 --> 00:08:17,570
Hammad shake is the wizard of the week.

129
00:08:17,750 --> 00:08:22,370
He created a linear regression model
using a Bayesian approach to find the

130
00:08:22,371 --> 00:08:25,880
relationship between movie
sales and movie ratings.

131
00:08:26,390 --> 00:08:29,780
This is very likely the best made
Jupiter notebook on the topic.

132
00:08:29,781 --> 00:08:33,710
On all of get hubs. So take a look
at it for sure. Very well done.

133
00:08:33,990 --> 00:08:35,710
And the runner up is Noah Dell,

134
00:08:35,730 --> 00:08:39,150
who used Basie and optimization to find
the learning rate for a neural network

135
00:08:39,330 --> 00:08:42,720
that correlates bitcoin search
frequency and is price also.

136
00:08:42,721 --> 00:08:44,640
Definitely check that
one out. Great work. No.

137
00:08:45,100 --> 00:08:50,100
This week's challenge is to perform Lda
on a text data set of your choice bonus

138
00:08:50,321 --> 00:08:53,200
points for real world use
cases. Poster. Get help,

139
00:08:53,201 --> 00:08:56,170
link in the comments and winners
will be announced next week.

140
00:08:56,320 --> 00:08:58,240
Please subscribe for
more programming videos.

141
00:08:58,241 --> 00:09:02,140
And for now I've got to find a
better topic. So thanks for watching.

