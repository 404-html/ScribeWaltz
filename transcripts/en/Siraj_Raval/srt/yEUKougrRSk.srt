1
00:00:00,960 --> 00:00:04,920
Regression. Final answer. Yes.

2
00:00:05,460 --> 00:00:10,460
Hello world it Saroj and everyone in
the entire world needs to understand how

3
00:00:10,981 --> 00:00:12,240
regression works.

4
00:00:12,540 --> 00:00:17,540
It's a data science technique that can
quickly give us superpowers of insight

5
00:00:18,120 --> 00:00:21,930
from a massive,
sometimes unintelligible dataset.

6
00:00:22,200 --> 00:00:23,130
Using regression,

7
00:00:23,131 --> 00:00:28,131
we can predict the optimal way to allocate
funds in our organization to increase

8
00:00:28,471 --> 00:00:31,170
sales,
predict demand for a product,

9
00:00:31,350 --> 00:00:35,130
understand the effects of
certain treatments on a patient.

10
00:00:35,400 --> 00:00:40,350
Understand why certain factors contribute
to poverty or economic prosperity.

11
00:00:40,500 --> 00:00:43,980
I could literally go on for hours.
Ain't nobody got time for that.

12
00:00:44,070 --> 00:00:49,070
Whether it's in business research or
even lifestyle design regression can give

13
00:00:49,321 --> 00:00:54,321
us the answers we need from data to
improve our lives and the lives of others.

14
00:00:55,350 --> 00:00:59,880
Now, there are a lot of different
types of regression models in the wild,

15
00:00:59,881 --> 00:01:03,330
including logistic and
stepwise and elastic net.

16
00:01:03,720 --> 00:01:07,620
It can be overwhelming to
try and understand all of
them from a single video.

17
00:01:07,620 --> 00:01:12,510
So we're going to focus on just a few
of the most important ones for our

18
00:01:12,511 --> 00:01:13,344
problem.

19
00:01:13,410 --> 00:01:17,640
We'll play the role of a newly hired
data analysts at a corporation called

20
00:01:17,670 --> 00:01:22,110
engine using a Dataset of our
marketing expenses and ticket sales.

21
00:01:22,230 --> 00:01:24,450
For our newly opened Jurassic Park.

22
00:01:24,630 --> 00:01:29,010
We'll use a regression model to predict
how to optimally increase ticket sales

23
00:01:29,220 --> 00:01:30,053
next month.

24
00:01:30,270 --> 00:01:35,270
Data of finds a way identifying whether
or not the correlation between two or

25
00:01:35,281 --> 00:01:40,281
more variables represent a causal
relationship is rarely easy using human

26
00:01:40,861 --> 00:01:44,880
intuition alone, but if we could
figure that out in a mathematical way,

27
00:01:44,910 --> 00:01:49,020
it would be so much easier to make
business decisions quickly so we can spend

28
00:01:49,021 --> 00:01:53,790
the rest of our time playing Fortnite
to help answer these types of questions.

29
00:01:53,970 --> 00:01:56,580
Data scientists use regression models.

30
00:01:56,790 --> 00:02:01,350
Regressions are used to quantify the
strength of the relationship between one

31
00:02:01,351 --> 00:02:06,351
variable and other variables that
are thought to explain it these days.

32
00:02:06,540 --> 00:02:11,540
Running hundreds of thousands of
regressions has become extremely simple.

33
00:02:12,480 --> 00:02:16,110
In fact, we don't even need to
know how to code. That's right.

34
00:02:16,140 --> 00:02:19,230
Microsoft formerly run
by Youtube sensation.

35
00:02:19,231 --> 00:02:24,231
Bill Gates has built regression
functionality into its
XL product and almost all

36
00:02:25,141 --> 00:02:29,850
of the major tech companies have some
form of drag and drop interface that lets

37
00:02:29,851 --> 00:02:34,440
us immediately apply regression
analysis to a Dataset we upload.

38
00:02:34,710 --> 00:02:39,710
But using a programming language like
python can help us intuitively understand

39
00:02:40,320 --> 00:02:44,910
the mathematics that's happening behind
the scenes and if we understand the

40
00:02:44,911 --> 00:02:45,900
mathematics,

41
00:02:46,080 --> 00:02:51,080
it'll help us build an intuition around
when to use which model for whatever

42
00:02:51,901 --> 00:02:56,040
task we have. So let's start with
the first type of regression,

43
00:02:56,041 --> 00:02:57,990
the simple linear regression.

44
00:02:58,470 --> 00:03:02,740
This is a model that can show the
relationship between two variables.

45
00:03:02,980 --> 00:03:03,880
More specifically,

46
00:03:03,881 --> 00:03:08,881
it shows how the variation
in the dependent variables
can be captured I a change

47
00:03:10,150 --> 00:03:14,020
in the independent variable
in a business context.

48
00:03:14,080 --> 00:03:19,080
The dependent variables can also be called
the predictor for sales of a product

49
00:03:19,870 --> 00:03:22,270
performance,
pricing or risk.

50
00:03:22,690 --> 00:03:27,690
The independent variable also called
the explanatory variable explains the

51
00:03:27,850 --> 00:03:30,220
influence of the dependent variable.

52
00:03:30,520 --> 00:03:35,170
A simple linear regression model is
linear because all the terms in the model

53
00:03:35,171 --> 00:03:40,171
are either a constant value or a
parameter multiplied by an independent

54
00:03:40,811 --> 00:03:41,644
variable.

55
00:03:41,830 --> 00:03:46,830
The core idea is to figure out this model
which when plotted is the line of best

56
00:03:47,021 --> 00:03:48,250
fit for the data.

57
00:03:48,460 --> 00:03:53,290
The best fit line is the one where the
total prediction error of all the data

58
00:03:53,291 --> 00:03:55,660
points are as small as possible.

59
00:03:55,960 --> 00:03:59,980
We can consider the error as the
distance between the data point and the

60
00:03:59,981 --> 00:04:02,830
regression line.
So in our datasets,

61
00:04:02,860 --> 00:04:06,010
after we import pandas
for data manipulation,

62
00:04:06,280 --> 00:04:10,660
num Pi for Matrix math and
Plotly for data visualization,

63
00:04:10,930 --> 00:04:15,430
we can visualize our data first to
see that we're spending money on TV,

64
00:04:15,580 --> 00:04:20,580
radio and newspaper ads each week and the
amount of sales of our product changes

65
00:04:20,831 --> 00:04:23,950
every week as well. Yes, I said newspaper.

66
00:04:23,951 --> 00:04:28,951
Remember this is the 90s let's say we
want to find the relationship between just

67
00:04:29,350 --> 00:04:31,870
TV,
ad money and sales.

68
00:04:31,990 --> 00:04:35,530
Thus we'll want to use a
simple linear regression model.

69
00:04:35,710 --> 00:04:39,400
We can represent this as the
model y equals mx plus B.

70
00:04:39,640 --> 00:04:43,570
The values of both M and B should be
optimal such that the error is as small as

71
00:04:43,571 --> 00:04:44,770
possible when plotted.

72
00:04:44,920 --> 00:04:48,310
There are several different metrics
we can use to compute the error,

73
00:04:48,430 --> 00:04:51,700
so let's use a popular one
called the sum of squared error.

74
00:04:52,030 --> 00:04:56,050
This simply finds the difference between
the actual output and the predicted

75
00:04:56,051 --> 00:05:00,550
output squares. It adds them all up
and that's our final error value.

76
00:05:00,700 --> 00:05:05,620
We then use this error to
update the values for B and
m using a technique called

77
00:05:05,621 --> 00:05:09,250
gradient descent.
Rather than explain how that works,

78
00:05:09,251 --> 00:05:14,251
I'm going to link you to my video aptly
titled Linear regression using gradient

79
00:05:14,471 --> 00:05:16,150
descent in the video description.

80
00:05:16,390 --> 00:05:21,040
When we perform this simple
linear regression will find
that there does seem to

81
00:05:21,041 --> 00:05:24,430
be a relationship between
these two variables.

82
00:05:24,670 --> 00:05:29,530
The more we spend on TV ads, the
higher our sales are. Makes Sense,

83
00:05:29,531 --> 00:05:33,370
but we probably want a more
holistic view of our data,

84
00:05:33,371 --> 00:05:37,240
meaning we want to take into
account the other variables as well,

85
00:05:37,241 --> 00:05:42,241
like the newspaper and radio ads so
we can see how they all affect sales.

86
00:05:42,940 --> 00:05:45,490
If we try to plot just one other feature,

87
00:05:45,491 --> 00:05:49,420
the line we have now becomes a plane.
We're now in three d.

88
00:05:49,660 --> 00:05:53,530
The plane is the function that expresses
why as a function of the other two

89
00:05:53,531 --> 00:05:58,430
variables. This is the genesis of
the multiple linear regression model.

90
00:05:58,880 --> 00:06:02,510
There's now more than one input
variable use to predict the y value.

91
00:06:02,570 --> 00:06:07,220
A model with two input variables is a
longer equation than one expressed with

92
00:06:07,221 --> 00:06:07,970
one.

93
00:06:07,970 --> 00:06:12,710
We can take it a step further and take
into account all three input variables

94
00:06:12,920 --> 00:06:17,090
and create an equation for that
as well. Alas, if we do this,

95
00:06:17,120 --> 00:06:21,410
we are now limited in our capabilities
in terms of what we can visualize in a

96
00:06:21,411 --> 00:06:25,190
graph since we are now dealing
with more than three dimensions,

97
00:06:25,520 --> 00:06:27,890
but that's the wild world of data.

98
00:06:27,891 --> 00:06:31,850
Many data sets have tens if
not thousands of dimensions.

99
00:06:32,150 --> 00:06:36,560
If we again use gradient descent to find
the equation that best fits our data

100
00:06:36,740 --> 00:06:37,640
will receive a model,

101
00:06:37,641 --> 00:06:41,150
we can interpret specifically
the coefficients in the equation.

102
00:06:41,450 --> 00:06:45,470
We can correlate each learned
coefficient with a different feature.

103
00:06:45,770 --> 00:06:50,770
If all the other predictors
are held constant and we
change any of these values,

104
00:06:51,260 --> 00:06:56,260
we can tell what the corresponding
increase in the amount of sales would be,

105
00:06:56,840 --> 00:07:00,500
but which coefficients
are the most significant?

106
00:07:00,800 --> 00:07:02,720
Are they all equally significant?

107
00:07:02,930 --> 00:07:05,510
Does this equation explained
the meaning of life?

108
00:07:06,200 --> 00:07:09,230
Here we turned to the
art of model evaluation.

109
00:07:09,440 --> 00:07:12,770
Notice that all the coefficients
are greater than zero.

110
00:07:13,190 --> 00:07:17,670
That means all the variables
have an impact on the sales. Hmm.

111
00:07:17,750 --> 00:07:20,630
Before we keep evaluating
this model any further,

112
00:07:20,870 --> 00:07:23,420
I have a suspicion we
could use a better model.

113
00:07:23,630 --> 00:07:28,630
My intuition as a digital detective
is that TV ads far outweigh the other

114
00:07:28,941 --> 00:07:33,770
features in terms of significance and
we'll need to prove that are a linear

115
00:07:33,771 --> 00:07:36,590
regression model fits
the data relatively well?

116
00:07:36,740 --> 00:07:39,830
I mean it's not extremely off.
If it was,

117
00:07:39,831 --> 00:07:44,180
we'd want to try to instead
use a nonlinear regression.

118
00:07:44,630 --> 00:07:48,950
A nonlinear regression model is one
where not all the terms in the model are

119
00:07:48,951 --> 00:07:53,951
either a constant value or a parameter
multiplied by an independent variable.

120
00:07:54,590 --> 00:07:58,430
Both linear and nonlinear models
can fit curvature in data,

121
00:07:58,550 --> 00:08:00,950
so that's the main factor to consider,

122
00:08:01,340 --> 00:08:05,330
but since we're doing pretty well so far,
let's not go there right now.

123
00:08:05,750 --> 00:08:10,670
It's also important to note that both
single and multiple regression models can

124
00:08:10,671 --> 00:08:12,950
be either linear or nonlinear.

125
00:08:13,280 --> 00:08:16,910
These four terms are all different
ways of defining regression's.

126
00:08:17,240 --> 00:08:18,410
Before we kick back,

127
00:08:18,411 --> 00:08:23,300
relax and tweet that we've secured funding
for Tesla at four 20 we want to try

128
00:08:23,301 --> 00:08:28,100
to more types of linear regression models,
rich and lasso regressions.

129
00:08:28,490 --> 00:08:32,280
Any linear or multiple regression
model will be sub optimal.

130
00:08:32,330 --> 00:08:36,290
When there is a high colinearity
amongst the feature variables.

131
00:08:36,690 --> 00:08:41,690
Colinearity is a condition in which
some of the feature variables are highly

132
00:08:42,051 --> 00:08:42,884
correlated.

133
00:08:43,160 --> 00:08:47,900
It's sub optimal because colinearity
tends to interfere in determining the

134
00:08:47,901 --> 00:08:51,140
precise effect of each feature variable.

135
00:08:51,260 --> 00:08:55,070
There are a few different ways of
mathematically evaluating this,

136
00:08:55,080 --> 00:08:58,950
but we'll get to model
evaluation techniques later on.

137
00:08:59,310 --> 00:09:03,870
Right now let's just try out ridge
regression to see how it works.

138
00:09:04,110 --> 00:09:08,940
Ridge regression alleviates colinearity
amongst the regression predictor

139
00:09:08,941 --> 00:09:11,400
variables in a model.
Again,

140
00:09:11,580 --> 00:09:16,380
colinearity is we're one of the feature
variables in a multiple regression model

141
00:09:16,500 --> 00:09:21,500
can be linearly predicted from the others
with a substantial degree of accuracy.

142
00:09:22,320 --> 00:09:25,050
If the feature variables
are very much correlated,

143
00:09:25,051 --> 00:09:28,410
the final regression model is
going to cause overfitting,

144
00:09:28,470 --> 00:09:33,120
meaning our model won't be
generalized enough to new data points.

145
00:09:33,300 --> 00:09:36,720
It'd be perfect,
but only for the existing data points.

146
00:09:36,960 --> 00:09:40,080
We don't want that.
So to solve this issue,

147
00:09:40,320 --> 00:09:44,730
Ridge regression as a small squared
bias factor to the variables.

148
00:09:44,970 --> 00:09:49,970
This squared bias factor pulls the feature
variable coefficients away from this

149
00:09:50,281 --> 00:09:54,900
rigidness. I introducing a small
amount of bias into the model.

150
00:09:55,110 --> 00:09:56,880
It reduces the variance,

151
00:09:56,910 --> 00:10:00,690
which means the difference between
predicted data points and the mean of y

152
00:10:00,900 --> 00:10:01,733
immensely.

153
00:10:01,890 --> 00:10:06,810
This trade off of variants and bias will
produce much more useful coefficient

154
00:10:06,811 --> 00:10:07,644
estimates.

155
00:10:07,740 --> 00:10:12,740
When colinearity exists for Ridge
regression will introduce grid search CV.

156
00:10:13,320 --> 00:10:17,580
This is going to allow us to automatically
perform cross validation which will

157
00:10:17,581 --> 00:10:22,050
assess how the results of our model will
generalize to an independent data set

158
00:10:22,200 --> 00:10:24,120
with a range of different parameters.

159
00:10:24,270 --> 00:10:27,180
In order to find the
optimal value of Alpha,

160
00:10:27,360 --> 00:10:31,980
then we can find the best parameter and
the best mean squared error using the

161
00:10:31,981 --> 00:10:34,260
built in attributes.
Thank you. Psychic learn.

162
00:10:34,740 --> 00:10:39,720
Our optimal value of Alpha and MSC are
both slightly better than the ones we got

163
00:10:39,721 --> 00:10:42,780
from art multiple regression model.
This is good,

164
00:10:43,020 --> 00:10:47,520
but I think we can do even
better. Enter Lasso. Regression.

165
00:10:47,700 --> 00:10:51,990
Lasso regression is similar
to ridge regression and
that they both have the same

166
00:10:52,020 --> 00:10:54,390
premise.
In Lasso regression.

167
00:10:54,391 --> 00:10:59,391
We're again adding a bias term to the
regression function so that we can reduce

168
00:10:59,551 --> 00:11:04,551
the negative effects of colinearity
and consequently the model's variants,

169
00:11:04,710 --> 00:11:08,760
but instead of using a squared
bias like ridge regression does,

170
00:11:09,090 --> 00:11:12,990
Lasso regression uses a literal lasso.
Just kidding.

171
00:11:13,260 --> 00:11:14,610
I had to do that at least once.

172
00:11:14,640 --> 00:11:19,640
It instead uses an absolute value bias
and this difference has a huge impact on

173
00:11:20,041 --> 00:11:21,060
the bias variance.

174
00:11:21,060 --> 00:11:26,060
Trade off Lasso overcomes the disadvantage
of Ridge regression by not only

175
00:11:26,251 --> 00:11:31,251
punishing high values of the coefficients
but actually setting them to zero if

176
00:11:31,291 --> 00:11:33,660
they aren't relevant in this way,

177
00:11:33,661 --> 00:11:38,280
we might end up with less
features included in our
model then we started with,

178
00:11:38,880 --> 00:11:42,450
but this can be an advantage
when we implement lasso.

179
00:11:42,451 --> 00:11:47,451
It looks similar to Ridge
and our resulting alpha and
MSC are the best scores of

180
00:11:47,941 --> 00:11:52,080
any model we've used. Looks like Lasso
is the way to go. Using this model,

181
00:11:52,081 --> 00:11:56,320
we can give it values for how much we
will spend on different advertising

182
00:11:56,321 --> 00:12:00,130
mediums and it will predict
the amount of sales we'll make,

183
00:12:00,280 --> 00:12:04,900
which lets us much more
efficiently allocate funds
towards the medium we want to

184
00:12:04,901 --> 00:12:09,010
use saving us money and thus
increasing our revenue. Luckily,

185
00:12:09,011 --> 00:12:12,070
there are only three points
to remember from this video.

186
00:12:12,460 --> 00:12:16,990
A regression model is simple if it
has only one input and one output,

187
00:12:17,200 --> 00:12:20,770
but it's multiple if it has
multiple inputs and one output.

188
00:12:21,220 --> 00:12:25,660
A regression model is linear when all
the terms in the model are either a

189
00:12:25,661 --> 00:12:30,661
constant value or a parameter multiplied
by an independent variable else.

190
00:12:30,791 --> 00:12:35,500
It's nonlinear. Lastly, if a
data set has a high colinearity,

191
00:12:35,530 --> 00:12:37,900
meaning the inputs are highly correlated,

192
00:12:38,080 --> 00:12:42,130
we can use either ridge or lasso
regression to help ensure more accurate

193
00:12:42,131 --> 00:12:44,350
estimates of the regression coefficients.

194
00:12:44,500 --> 00:12:46,990
What do you want to use a
regression model to do next?

195
00:12:47,230 --> 00:12:50,710
Let me know in the comments section and
please subscribe for more technology

196
00:12:50,711 --> 00:12:54,940
videos. For now, I've got to find
a lasso, so thanks for watching.

