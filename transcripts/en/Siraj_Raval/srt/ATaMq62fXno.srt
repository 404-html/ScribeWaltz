1
00:00:00,090 --> 00:00:04,110
Hello world. It's a Raj and life
is pretty unpredictable, isn't it?

2
00:00:04,470 --> 00:00:09,470
Probabilistic programming is a technique
that helps us build a AI that account

3
00:00:10,321 --> 00:00:15,321
for real world uncertainty and we'll use
Uber's newly released tool pyro to help

4
00:00:16,561 --> 00:00:18,570
us understand how it works.

5
00:00:18,810 --> 00:00:21,870
Back when the glorious Soviet
Union was still around,

6
00:00:22,080 --> 00:00:26,460
programmers wrote everything in
the assembly programming language.

7
00:00:26,490 --> 00:00:31,490
They used three letter commands to
control which data got stored on which

8
00:00:31,861 --> 00:00:32,700
register,

9
00:00:32,970 --> 00:00:37,970
which mathematical operations the CPU
executed and which memory locations got

10
00:00:39,841 --> 00:00:42,720
copied to where.
As you can imagine,

11
00:00:42,750 --> 00:00:47,750
this was really tedious work but it did
allow programmers to create never before

12
00:00:47,911 --> 00:00:52,230
possible algorithms.
After a while though they wanted more,

13
00:00:52,470 --> 00:00:57,240
they didn't want to have to worry about
manually adding register values or

14
00:00:57,360 --> 00:01:00,390
copying numbers from one place to another.

15
00:01:00,510 --> 00:01:05,510
They wanted to focus on high level
ideas like functions and data flow.

16
00:01:05,670 --> 00:01:10,670
So along came compiled languages like
c and scripted languages like python.

17
00:01:11,190 --> 00:01:16,190
These higher level languages
let programmers do the
same task with much less

18
00:01:16,741 --> 00:01:17,310
code.

19
00:01:17,310 --> 00:01:22,310
Fast forward a couple of decades later
and deep learning coupled with big data

20
00:01:23,220 --> 00:01:27,640
is now the cool kid on the block
processor speeds and storage capacity,

21
00:01:27,641 --> 00:01:30,090
you have skyrocketed in capabilities.

22
00:01:30,091 --> 00:01:33,990
So computers can process
data like never before,

23
00:01:34,080 --> 00:01:37,770
which means programmers can
build tools like never before,

24
00:01:37,950 --> 00:01:40,950
like one of the many possible
deep learning models.

25
00:01:41,250 --> 00:01:46,250
The term model can be used to describe
a decision making tool or a way of

26
00:01:46,681 --> 00:01:49,050
representing an idea using math.

27
00:01:49,380 --> 00:01:53,190
We use models every day
to make our lives easier.

28
00:01:53,340 --> 00:01:57,930
A map is a model of locations.
Sheet music is a model of sounds.

29
00:01:58,110 --> 00:02:01,380
Even our brain is a model
of every decision we make.

30
00:02:01,560 --> 00:02:03,600
Models can be deterministic.

31
00:02:03,750 --> 00:02:08,190
That means that a given input will
always produce the same output.

32
00:02:08,490 --> 00:02:13,490
There's no randomness involved like blink
one 82 but most models are going to be

33
00:02:14,521 --> 00:02:18,840
probabilistic. That means they
take into account uncertainty,

34
00:02:18,990 --> 00:02:23,990
which the real world is full of applying
the same input to this kind of model

35
00:02:24,091 --> 00:02:28,590
twice could lead to two different outputs.
There are no guarantees.

36
00:02:28,860 --> 00:02:31,680
Modeling.
This kind of uncertainty isn't easy.

37
00:02:31,681 --> 00:02:36,330
So lots of machine learning models
that have been built thus far have been

38
00:02:36,360 --> 00:02:37,410
deterministic,

39
00:02:37,680 --> 00:02:42,680
but consider that pretty much all real
world data is incomplete or imperfect in

40
00:02:43,741 --> 00:02:44,574
some way.

41
00:02:44,670 --> 00:02:48,810
So having some kind of prior knowledge
of probabilities helps us make

42
00:02:48,811 --> 00:02:53,010
predictions. This is what
Bayesean inference helps us do.

43
00:02:53,220 --> 00:02:58,220
We make prior assumptions about how
the world is before making predictions.

44
00:02:59,350 --> 00:03:04,350
We represent these assumptions as numbers
and can update our model based on our

45
00:03:05,381 --> 00:03:06,520
observations.

46
00:03:06,880 --> 00:03:11,200
When programmers started to implement
this kind of model programmatically,

47
00:03:11,410 --> 00:03:16,390
they started to realize that, hey,
this is actually pretty complex to do.

48
00:03:16,600 --> 00:03:19,990
It's not about keeping just a
few probabilities in your head.

49
00:03:20,230 --> 00:03:23,920
You have to keep track of the
whole probability distribution.

50
00:03:24,040 --> 00:03:29,040
So ideally we need the probability
distributions to be at the heart of the

51
00:03:29,051 --> 00:03:32,530
program.
Since they are what is being manipulated.

52
00:03:32,710 --> 00:03:36,790
All other elements should revolve
around these probabilities.

53
00:03:36,910 --> 00:03:39,640
But with traditional
programming languages,

54
00:03:39,850 --> 00:03:44,850
they're only used in obscure sub
routines at their most basic level.

55
00:03:45,430 --> 00:03:49,810
Probabilistic programming languages
differ from deterministic ones.

56
00:03:49,960 --> 00:03:54,310
By allowing language
primitives to be the castic.

57
00:03:54,580 --> 00:03:58,930
The point of running the program is not
to find out what happens at the end.

58
00:03:59,170 --> 00:04:03,760
It's to figure out the correct values
for the variables that define our

59
00:04:03,761 --> 00:04:07,810
probability distributions.
What do we think the world is like?

60
00:04:07,960 --> 00:04:12,960
What kind of distributions with which
kinds of parameters have the expressive

61
00:04:13,001 --> 00:04:15,340
power to mimic the real world?

62
00:04:15,520 --> 00:04:19,780
Most probabilistic languages are
built on top of existing languages,

63
00:04:19,990 --> 00:04:23,680
but they're distinct enough in the way
they work that they can be viewed as a

64
00:04:23,681 --> 00:04:25,990
language instead of a library,

65
00:04:26,170 --> 00:04:29,230
one of the most popular ride
sharing apps in the world,

66
00:04:29,410 --> 00:04:34,410
Uber has to match riders and drivers
to get them to where they need to go as

67
00:04:34,481 --> 00:04:35,710
fast as possible.

68
00:04:35,980 --> 00:04:40,510
This simple task requires
optimization at every single step,

69
00:04:40,780 --> 00:04:43,870
optimal routing,
sensible pool combinations,

70
00:04:43,990 --> 00:04:47,800
deciding optimal timings,
avoiding harassment lawsuits.

71
00:04:48,190 --> 00:04:53,190
So in order to help solve these problems
and specifically model the uncertainty

72
00:04:53,560 --> 00:04:54,850
of the real world,

73
00:04:55,090 --> 00:04:59,980
they've developed a probabilistic
programming language called pyro that lets

74
00:04:59,981 --> 00:05:03,940
engineers and build models that
use Bayesean deep learning.

75
00:05:04,000 --> 00:05:08,530
It's written in Python and built on
top of the popular Pi Torch programming

76
00:05:08,531 --> 00:05:09,364
library.

77
00:05:09,400 --> 00:05:14,400
Pi Torch offers very fast tensor math
operations and automatic differentiation,

78
00:05:15,310 --> 00:05:19,060
Aka gradient descent,
a popular optimization strategy,

79
00:05:19,300 --> 00:05:22,660
both of which help make computation
faster and easier to create.

80
00:05:23,050 --> 00:05:28,050
Pie Torch also offers dynamic graph
definitions instead of static graph

81
00:05:28,091 --> 00:05:32,890
definitions. In tensorflow, for example,
you define a graph before you run it.

82
00:05:33,100 --> 00:05:34,090
In Pie torch,

83
00:05:34,210 --> 00:05:38,920
you define change and execute nodes
in the computation graph as you go at

84
00:05:38,921 --> 00:05:42,670
runtime, and this approach of
feels more native to python.

85
00:05:42,880 --> 00:05:47,260
The basic unit of pyro programs
is the stochastic function,

86
00:05:47,500 --> 00:05:52,210
which helps us explicitly compute the
probability of the outputs given the

87
00:05:52,211 --> 00:05:53,800
inputs.
For example,

88
00:05:53,830 --> 00:05:57,710
if we want to draw a sample x from
this unit normal distribution,

89
00:05:58,010 --> 00:06:02,030
then we could just do the following,
define a mean and unit variants.

90
00:06:02,031 --> 00:06:07,031
Using variables from Pi Torch love you
fast heads or math and we can compute its

91
00:06:07,311 --> 00:06:10,670
log probability according
to a normal distribution.

92
00:06:11,060 --> 00:06:16,060
We can also return a sample using a
named stochastic sample using the pyro

93
00:06:16,521 --> 00:06:17,001
sample.

94
00:06:17,001 --> 00:06:22,001
Primitive naming allows pyros backend to
uniquely identify sample statements and

95
00:06:22,911 --> 00:06:24,920
change their behavior at runtime.

96
00:06:25,160 --> 00:06:29,180
Depending on how the enclosing the
Catholic function is being used.

97
00:06:29,570 --> 00:06:34,570
If we had a bunch of data of daily mean
temperatures and cloud cover and what to

98
00:06:34,671 --> 00:06:39,140
reason about how temperature interacts
with whether it was sunny or cloudy,

99
00:06:39,350 --> 00:06:44,350
we can write out a simple stochastic
function that says use pyro dot sample to

100
00:06:44,661 --> 00:06:47,750
define a binary random variable,
cloudy,

101
00:06:47,870 --> 00:06:51,080
drawn from a distribution
with the given parameters.

102
00:06:51,260 --> 00:06:56,260
Then convert the value to a string so
that a returns values of whether that are

103
00:06:56,480 --> 00:06:59,690
easier to parse.
According to this model,

104
00:06:59,720 --> 00:07:04,720
it's cloudy 30% of the time and Sonny's
70% of the time we defined parameters we

105
00:07:05,061 --> 00:07:09,950
use to sample that temperature that
depend on the particular value of cloudy.

106
00:07:10,100 --> 00:07:15,100
We sample in line three then we return
the two values of cloudy and temp at the

107
00:07:16,041 --> 00:07:16,730
end,

108
00:07:16,730 --> 00:07:21,730
whether specifies a joint probability
distribution over to named random

109
00:07:21,771 --> 00:07:23,870
variables,
cloudy and temp.

110
00:07:24,080 --> 00:07:29,080
So this model helps us reason about how
likely it is to be cloudy or sunny given

111
00:07:29,780 --> 00:07:33,350
a temperature.
And because pyro is embedded in python,

112
00:07:33,560 --> 00:07:37,520
stochasticity can effect the control flow.
For example,

113
00:07:37,670 --> 00:07:42,670
we can construct recursive functions that
terminate the recursion randomly three

114
00:07:42,801 --> 00:07:43,820
summary points.

115
00:07:44,060 --> 00:07:48,530
Probabilistic programming is a technique
that models the uncertainty of the

116
00:07:48,531 --> 00:07:53,210
natural world by embedding randomness
into the models that we built.

117
00:07:53,250 --> 00:07:57,950
Basie and inference is a probabilistic
methodology that lets us use prior

118
00:07:57,951 --> 00:08:02,951
knowledge to make predictions and Pyro
is a probabilistic language built using

119
00:08:03,171 --> 00:08:08,171
python and Pi Torch that lets us build
Basie and deep learning models that are

120
00:08:08,181 --> 00:08:10,040
scalable and efficient.

121
00:08:10,100 --> 00:08:14,660
Last week's coding challenge winner is
Shannon Code who successfully built a

122
00:08:14,661 --> 00:08:18,290
demo that uses a
blockchain and AI together.

123
00:08:18,410 --> 00:08:23,150
He used open Ai's world of
bits environment to train
a reinforcement learning

124
00:08:23,151 --> 00:08:28,151
agent and the AI stores the training
results on an immutable public blockchain.

125
00:08:29,060 --> 00:08:33,670
Great first steps to getting
a fully autonomous training
agent for can start his

126
00:08:33,680 --> 00:08:35,270
repo wizard of the week.

127
00:08:35,780 --> 00:08:40,220
This week's coding challenge is to
use pyro to create a simple Basie and

128
00:08:40,221 --> 00:08:43,400
regression model on a
Dataset of your choice.

129
00:08:43,610 --> 00:08:45,410
Details are in the read me get humbling.

130
00:08:45,510 --> 00:08:48,440
Go in the comment section
and the top two submissions.

131
00:08:48,441 --> 00:08:52,460
Get a shout out for me next week, please
subscribe for more programming videos,

132
00:08:52,461 --> 00:08:56,240
and for now I've got to go update
my priors, so thanks for watching.

