1
00:00:00,090 --> 00:00:03,360
And my weight's keep changing.
Helps me making decisions.

2
00:00:03,390 --> 00:00:05,700
I got a predicted hello world,

3
00:00:05,730 --> 00:00:10,730
it Suraj and in this video I'll teach
you how backpropagation the most popular

4
00:00:11,581 --> 00:00:16,440
optimization algorithms in machine
learning works. Machine learning,

5
00:00:16,470 --> 00:00:20,700
deep learning, modern artificial
intelligence is in general,

6
00:00:20,701 --> 00:00:25,200
centered around a topic called
it mathematical optimization.

7
00:00:25,800 --> 00:00:30,800
Optimization is a distinct branch of
applied math that's useful in a ton of

8
00:00:31,141 --> 00:00:36,060
different industries like manufacturing,
transportation and economics.

9
00:00:36,480 --> 00:00:41,040
Optimization comes from the same
root as optimal, which means best.

10
00:00:41,430 --> 00:00:42,720
When you optimize something,

11
00:00:42,721 --> 00:00:47,721
you are making it the best and the
word best varies based on the context.

12
00:00:48,630 --> 00:00:50,040
If you're a basketball player,

13
00:00:50,041 --> 00:00:55,041
you might want to maximize your jump
and minimize your fouls to be like Mike,

14
00:00:56,670 --> 00:01:01,670
the concept of both maximizing
and minimizing or different
types of optimization

15
00:01:02,401 --> 00:01:07,320
problems. A basic optimization
problem consists of three core ideas.

16
00:01:07,740 --> 00:01:10,710
First, a function, let's call it f of x.

17
00:01:11,040 --> 00:01:14,730
We want to maximize or minimize
the output of this function.

18
00:01:14,910 --> 00:01:19,200
That's our goal or objective,
so let's call it an objective function.

19
00:01:19,650 --> 00:01:24,180
Secondly, we can't control the
output, but we can control the inputs.

20
00:01:24,330 --> 00:01:28,710
These are our variables. That could be
one or many depending on the use case.

21
00:01:29,250 --> 00:01:33,360
Lastly, there are usually some kind
of constraints on our objective.

22
00:01:33,690 --> 00:01:38,280
These places have limits on how big
or small some variables can get.

23
00:01:38,730 --> 00:01:40,740
In the case of our basketball player,

24
00:01:40,890 --> 00:01:44,460
if he's main goal is to maximize
his number of free throws,

25
00:01:44,640 --> 00:01:46,440
that becomes his objective function.

26
00:01:46,650 --> 00:01:51,430
He can choose to spend his time doing
any number of things like running or jump

27
00:01:51,431 --> 00:01:53,850
exercises to help achieve that goal.

28
00:01:53,880 --> 00:01:58,880
These are variables and there are certain
limits to the total amount of training

29
00:01:58,921 --> 00:02:03,060
time he has. For example, he
can't run for more than say,

30
00:02:03,061 --> 00:02:06,540
30 minutes each day due
to schedule conflicts.

31
00:02:06,570 --> 00:02:10,170
Aka Fortnite is too much fun.
These are constraints.

32
00:02:10,560 --> 00:02:14,400
There are a lot of variations
to this scenario. We could have,

33
00:02:14,670 --> 00:02:16,890
there could theoretically
be no constraints,

34
00:02:16,920 --> 00:02:20,280
which would mean this would
be an unlimited problem.

35
00:02:20,520 --> 00:02:23,680
It could have just one variable or many.

36
00:02:23,940 --> 00:02:27,780
These variables could be discreet,
having only integer values,

37
00:02:28,020 --> 00:02:31,050
or they could be continuous
taking on any value.

38
00:02:31,590 --> 00:02:36,330
The problem could be static or
unchanging over time or dynamic,

39
00:02:36,390 --> 00:02:40,290
meaning continual adjustments
could be made as changes occur.

40
00:02:40,350 --> 00:02:42,000
It could be deterministic,

41
00:02:42,030 --> 00:02:47,030
meaning specific inputs cause specific
outputs or stochastic meaning there is

42
00:02:47,971 --> 00:02:49,800
randomness to the outputs.

43
00:02:50,250 --> 00:02:54,180
So despite all of these
possible variations,

44
00:02:54,360 --> 00:02:58,980
the practice of framing this as an
optimization problem helps us solve it.

45
00:02:59,410 --> 00:03:00,400
Alternatively,

46
00:03:00,401 --> 00:03:05,200
we could just be guessing and checking
different inputs for our problem,

47
00:03:05,470 --> 00:03:10,470
but never fully knowing what the
ideal or optimal inputs should be.

48
00:03:10,990 --> 00:03:15,730
Optimization is a better solution
than this guess and check strategy.

49
00:03:16,090 --> 00:03:18,700
So how does all of this
relates to machine learning?

50
00:03:18,880 --> 00:03:23,880
Machine learning is the practice of
building mathematical models that we'll

51
00:03:24,281 --> 00:03:27,400
learn from datasets to make predictions.

52
00:03:27,880 --> 00:03:32,230
There are so many different ways
we can describe this process.

53
00:03:32,710 --> 00:03:36,790
One way is to say that it consists
of three parts. Representation,

54
00:03:36,940 --> 00:03:39,100
evaluation and optimization.

55
00:03:39,490 --> 00:03:44,490
Let's say we have a Dataset for a list
of NFL teams with the number of wins in a

56
00:03:45,220 --> 00:03:48,460
given season and the average points
per game for each of them does.

57
00:03:48,461 --> 00:03:53,461
The more points a team averages in a
game lead to more wins in a season.

58
00:03:54,400 --> 00:03:59,400
Can we predict how many games a team
will win based on a number of points,

59
00:03:59,770 --> 00:04:04,030
a team averages in a game?
We can use machine learning to find out.

60
00:04:04,090 --> 00:04:09,090
My dear Watson representation is the
space of models we could use for some

61
00:04:10,210 --> 00:04:11,043
dataset.

62
00:04:11,080 --> 00:04:16,080
There are so many different machine
learning models that we could use a three

63
00:04:16,811 --> 00:04:21,310
layer feed forward neural network,
a support vector machine,

64
00:04:21,520 --> 00:04:25,360
a decision tree,
but let's start with something simple.

65
00:04:25,510 --> 00:04:27,550
A linear regression model.

66
00:04:27,970 --> 00:04:31,240
It's aligned that intersects as
many data points as possible.

67
00:04:31,660 --> 00:04:35,620
Then using that line we can make
a prediction about any team.

68
00:04:36,190 --> 00:04:39,370
We can by hand draw out
this line ourselves,

69
00:04:39,670 --> 00:04:44,530
but we can't know for sure that it's
intersecting as many data points as it can

70
00:04:44,620 --> 00:04:45,910
with our human judgment.

71
00:04:46,360 --> 00:04:50,890
This process of trying to judge its
performance is called evaluation.

72
00:04:51,520 --> 00:04:55,300
In machine learning,
this is usually called the error function.

73
00:04:55,390 --> 00:05:00,160
There is a certain possibility space
of parameter values for our model and

74
00:05:00,400 --> 00:05:04,540
depending on what those parameters are,
the error will change.

75
00:05:04,900 --> 00:05:08,080
If we were to graph out
all of those possibilities,

76
00:05:08,140 --> 00:05:10,930
assuming that there are only
two possible parameters,

77
00:05:11,470 --> 00:05:16,470
we get a three d plane of possibility
of the many popular error functions.

78
00:05:16,690 --> 00:05:20,020
Let's try a simple one
called mean squared error.

79
00:05:20,290 --> 00:05:25,290
We essentially calculate the difference
between the models output and the actual

80
00:05:25,331 --> 00:05:28,920
data point. Our graph, we'll
have hills and valleys.

81
00:05:28,930 --> 00:05:33,760
We are evaluating for the height of this
landscape and the lowest area is the

82
00:05:33,761 --> 00:05:37,210
most optimal because that's the
point where the error is smallest.

83
00:05:37,570 --> 00:05:41,650
You'll then also give us the optimal
parameter values for our model to achieve

84
00:05:41,651 --> 00:05:42,484
that error.

85
00:05:42,520 --> 00:05:47,520
The process of searching this space of
our represented model to obtain a better

86
00:05:48,130 --> 00:05:50,860
valuation is called optimization.

87
00:05:51,280 --> 00:05:56,280
We have to traverse the landscape to
find the promised land of ideal parameter

88
00:05:56,681 --> 00:06:01,681
values for our model optimization is the
strategy of getting to where we want to

89
00:06:02,331 --> 00:06:06,230
go. We can use gradient
descent year to optimize.

90
00:06:06,710 --> 00:06:11,150
We'll initialize our parameters
or weights with random values.

91
00:06:11,360 --> 00:06:16,190
Then calculate the error using our
error function. This is a single number.

92
00:06:16,460 --> 00:06:21,110
Now we're going to use that error value
to compute the partial derivative.

93
00:06:21,530 --> 00:06:26,240
With respect to both of our parameter
values, we can call this the gradient.

94
00:06:26,770 --> 00:06:29,090
Alright, let's take a
step back. Unintended.

95
00:06:29,600 --> 00:06:33,650
The derivative is a term from calculus,
the study of change.

96
00:06:33,920 --> 00:06:38,920
It measures the steepness of a graph
of a function at some particular point.

97
00:06:39,260 --> 00:06:42,890
On that graph,
we can think of the derivative as a slope.

98
00:06:42,920 --> 00:06:47,690
That's the ratio of change in the value
of the function to the change in a

99
00:06:47,691 --> 00:06:48,524
variable.

100
00:06:48,560 --> 00:06:53,420
It's represented by this little squiggly
character and we want to change both of

101
00:06:53,421 --> 00:06:54,920
our parameter values,

102
00:06:54,950 --> 00:06:59,950
so will compute the partial derivative
of each meaning we derive one and treat

103
00:07:00,681 --> 00:07:02,810
the other as a constant respectively.

104
00:07:03,290 --> 00:07:06,980
We do that for both of them
and this gives us the gradient.

105
00:07:07,340 --> 00:07:10,880
The gradient is a multivariable
generation of the derivative.

106
00:07:11,150 --> 00:07:15,710
While a derivative can be defined on
functions of a single variable for

107
00:07:15,711 --> 00:07:19,370
functions of several variables,
the gradient takes its place.

108
00:07:19,460 --> 00:07:23,150
We can use it to update both of
our parameter values ever slow,

109
00:07:23,151 --> 00:07:27,770
slightly in a certain direction on
this plane that will move closer to the

110
00:07:27,771 --> 00:07:32,300
smallest possible error and we do
this over and over and over again.

111
00:07:32,330 --> 00:07:35,840
Over time, the parameters
converging to their optimal values.

112
00:07:36,140 --> 00:07:39,830
It's like letting a ball roll
down into the bottom of a bowl.

113
00:07:40,160 --> 00:07:44,810
The gradient is a compass that helps
us descend down the valley to where the

114
00:07:44,811 --> 00:07:45,411
error is.

115
00:07:45,411 --> 00:07:50,390
Smallest gradient descent can be used in
many different machine learning models,

116
00:07:50,391 --> 00:07:54,350
but it's most popularly
used in neural networks.

117
00:07:54,740 --> 00:07:56,780
In the context of neural networks.

118
00:07:57,080 --> 00:08:01,640
Grading dissent is just
renamed backpropagation
and your whole network is a

119
00:08:01,641 --> 00:08:05,300
function just like our linear
regression model is a function.

120
00:08:05,690 --> 00:08:10,690
It's a series of matrix operations that
are applied to some input data that

121
00:08:10,821 --> 00:08:12,230
results in an output.

122
00:08:12,830 --> 00:08:17,300
When we compute the error and use it
to compute the partial derivative with

123
00:08:17,301 --> 00:08:19,070
respect to its parameters,

124
00:08:19,460 --> 00:08:24,460
we get the gradient data is propagated
forward through the computation graph of

125
00:08:25,071 --> 00:08:25,910
operations.

126
00:08:26,300 --> 00:08:31,300
An error is computed and
propagated backwards by
computing gradients recursively

127
00:08:32,480 --> 00:08:35,180
for every set of layers in the network.

128
00:08:35,450 --> 00:08:40,450
So while the models all use different
mathematical operations to compute

129
00:08:40,611 --> 00:08:41,444
predictions,

130
00:08:41,570 --> 00:08:46,570
the optimization strategy of gradient
descent remains the same depending on how

131
00:08:46,731 --> 00:08:48,110
big our Dataset is,

132
00:08:48,111 --> 00:08:52,970
we can choose to run grading dissent
in batches and mini batches or in a

133
00:08:52,971 --> 00:08:53,930
stochastic way.

134
00:08:54,530 --> 00:08:59,530
Batch descent is when we compute the
error of all the training samples at once,

135
00:09:00,280 --> 00:09:01,840
every iteration of training.

136
00:09:02,380 --> 00:09:07,380
This can take a long time to compute for
big data sets of though so many batch

137
00:09:07,691 --> 00:09:08,590
gradient descent.

138
00:09:08,860 --> 00:09:13,180
Let's a split our training data into
many batches that can be processed

139
00:09:13,240 --> 00:09:14,110
individually.

140
00:09:14,740 --> 00:09:18,400
Stochastic gradient descent
is when we perform an update.

141
00:09:18,490 --> 00:09:20,680
Every single training observation,

142
00:09:21,130 --> 00:09:24,610
the size of our batch leads to
the fastest learning process.

143
00:09:24,910 --> 00:09:26,560
Depending on the size of our dataset.

144
00:09:26,920 --> 00:09:31,920
There've been a lot of variations to
gradient descent that have been used over

145
00:09:32,051 --> 00:09:32,884
the years.

146
00:09:33,160 --> 00:09:38,160
Sometimes the gradient gets stuck in
a local minimum and doesn't reach the

147
00:09:38,321 --> 00:09:39,430
global minimum.

148
00:09:39,850 --> 00:09:44,850
Momentum is a technique that uses the
moving average gradient instead of the

149
00:09:46,461 --> 00:09:48,860
immediate gradient at each time step.

150
00:09:48,980 --> 00:09:52,550
To preserve the momentum has
we approach smaller valleys.

151
00:09:53,120 --> 00:09:55,760
This can slow the convergence process,

152
00:09:55,790 --> 00:10:00,790
but a fixed to this is to use rms prop
which will adaptively scale the learning

153
00:10:00,981 --> 00:10:01,790
rate.

154
00:10:01,790 --> 00:10:06,790
A combination of both ideas is called
Adam or adaptive moment estimation.

155
00:10:07,670 --> 00:10:12,520
Adam is currently the most
popular iteration of gradient
descent and you'll see

156
00:10:12,521 --> 00:10:16,540
it a lot in code basis. I know
that was a lot, but stay with me.

157
00:10:16,570 --> 00:10:18,250
Three things to remember from this video.

158
00:10:18,700 --> 00:10:23,700
Mathematical optimization is a way of
framing a problem in order to maximize or

159
00:10:24,341 --> 00:10:26,710
minimize some goal or objective.

160
00:10:27,320 --> 00:10:31,790
Machine learning uses optimization to
find the optimal parameter values for a

161
00:10:31,791 --> 00:10:35,450
model to make the best
predictions and gradient descent.

162
00:10:35,480 --> 00:10:40,480
Also called backpropagation when applied
to neural networks is the most popular

163
00:10:41,251 --> 00:10:42,630
optimization strategy.

164
00:10:43,220 --> 00:10:46,250
What's my next topic? Using machine
learning, you could probably predict it.

165
00:10:46,490 --> 00:10:49,820
Subscribe for more educational
videos about Ai, and for now,

166
00:10:49,880 --> 00:10:53,180
I've got a guest lecturer at Ucla,
so thanks for watching.

