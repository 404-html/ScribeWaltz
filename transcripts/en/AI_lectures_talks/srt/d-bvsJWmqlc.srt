1
00:00:00,790 --> 00:00:04,100
Our distinguished speaker this evening,
Dennis Hassabis

2
00:00:05,620 --> 00:00:10,620
cofounded,
the artificial intelligence lab deep 

3
00:00:10,620 --> 00:00:13,391
mind,
and it's recognized worldwide as one of 

4
00:00:13,391 --> 00:00:17,101
the smartest thinkers in his field.
He was nicknamed the superhero of 

5
00:00:18,881 --> 00:00:23,881
artificial intelligence by the Guardian.
He's a former chess prodigy with degrees

6
00:00:25,450 --> 00:00:29,500
in computer science and cognitive.
This is making me sweat.

7
00:00:29,560 --> 00:00:31,930
He's so.
He's so clever.

8
00:00:32,350 --> 00:00:34,690
Anyway,
this evening's topic,

9
00:00:34,840 --> 00:00:39,840
creativity,
an ai draws on his eclectic experiences 

10
00:00:40,120 --> 00:00:42,310
as an artificial intelligence 
researcher,

11
00:00:42,640 --> 00:00:47,640
neuroscientists,
and video game designer to discuss the 

12
00:00:47,640 --> 00:00:51,091
implications of cutting edge research 
for creativity and scientific discovery.

13
00:00:52,750 --> 00:00:55,690
There'll be an opportunity for questions
at the end.

14
00:00:55,691 --> 00:00:56,370
I think,
um,

15
00:00:56,410 --> 00:00:58,570
I think tim was probably handling that 
as a writer.

16
00:00:59,380 --> 00:01:00,970
Anyway.
Without further ado,

17
00:01:00,971 --> 00:01:05,020
I'd love you to give a big welcome to 
Dennis [inaudible] office.

18
00:01:05,110 --> 00:01:10,110
Thank you.

19
00:01:17,740 --> 00:01:22,740
Thank you Chris for that introduction.
So it's a great honor for me to be here 

20
00:01:22,740 --> 00:01:27,511
at the academy to give this inaugural 
Rothschild's lecture in this wonderful 

21
00:01:27,511 --> 00:01:29,680
and inspiring amphitheater that we're 
sitting in today.

22
00:01:30,680 --> 00:01:35,680
I always love visiting there all academy
and I think it's great to have these 

23
00:01:35,680 --> 00:01:38,971
kinds of dialogues between the sciences 
and the arts and I think it's actually 

24
00:01:38,971 --> 00:01:42,991
going to become increasingly more vital 
as we rush headlong into the modern 

25
00:01:42,991 --> 00:01:47,851
technological world.
So today I'm going to explore a theme 

26
00:01:48,011 --> 00:01:50,380
that's the heart of everything at the 
wall academy,

27
00:01:50,650 --> 00:01:55,650
namely creativity,
and I'm going to examine it through the 

28
00:01:55,650 --> 00:01:59,251
lens of science and also more 
specifically through the lens of the 

29
00:01:59,251 --> 00:02:02,590
latest advances advances in artificial 
intelligence.

30
00:02:04,560 --> 00:02:09,560
So as all of you will know,
ai is the science of making machines 

31
00:02:09,560 --> 00:02:10,140
smart.
And as Chris mentioned,

32
00:02:10,290 --> 00:02:15,290
we found a deep mind in 2010 with the 
goal of trying to advance artificial 

33
00:02:16,291 --> 00:02:21,291
intelligence.
And we thought of AI and deep mind as a 

34
00:02:21,291 --> 00:02:26,151
kind of an Apollo program.
Efforts to advance ai as quickly as 

35
00:02:26,151 --> 00:02:28,140
possible.
And what we mean by that is try to bring

36
00:02:28,141 --> 00:02:31,680
together the world's greatest research.
Scientists and engineers.

37
00:02:31,900 --> 00:02:34,140
Give them all the resources they 
require,

38
00:02:34,141 --> 00:02:39,141
compute power and other things in order 
to see how much progress we could make 

39
00:02:39,141 --> 00:02:44,090
towards solving ai.
And we had an ambitious roadmap that 

40
00:02:44,090 --> 00:02:44,310
we're carrying out to this day.

41
00:02:46,380 --> 00:02:51,380
The other big thing,
idea behind deep mind and vision behind 

42
00:02:51,380 --> 00:02:52,980
it was to try and organize scientific 
endeavor in a new way.

43
00:02:53,760 --> 00:02:58,760
So the way I can kind of summarize that 
is we try to fuse together the best from

44
00:02:58,900 --> 00:03:01,870
academia,
blue sky thinking and ambitious thinking

45
00:03:01,871 --> 00:03:03,730
that you get in the best place with 
academia,

46
00:03:04,030 --> 00:03:08,290
with the best from the startup world.
So the kind of focus and energy and pace

47
00:03:08,470 --> 00:03:10,300
that you get at the world's best 
startups.

48
00:03:10,690 --> 00:03:15,690
And we didn't see why those two types of
environments had to be mutually 

49
00:03:15,690 --> 00:03:19,411
exclusive.
And we thought that we could advance 

50
00:03:19,411 --> 00:03:20,710
science more quickly if we could combine
the best from both of those worlds.

51
00:03:23,570 --> 00:03:28,570
Now mission at deep mind,
we articulated as a kind of to step 

52
00:03:28,570 --> 00:03:28,640
mission.
So step one,

53
00:03:28,880 --> 00:03:33,880
fundamentally solve intelligence.
So we'd like to understand what 

54
00:03:33,880 --> 00:03:34,820
intelligence is and we create it 
artificially.

55
00:03:35,810 --> 00:03:39,320
And then we believe if you do step one 
and a general enough way,

56
00:03:39,650 --> 00:03:43,220
then step two naturally follows.
We should be able to use this technology

57
00:03:43,221 --> 00:03:48,221
to solve almost everything else.
And that might sound a little bit 

58
00:03:48,221 --> 00:03:51,971
fanciful,
but I hope that by the end of the talk 

59
00:03:51,971 --> 00:03:55,900
of hope to have convinced you to at 
least think that maybe this is not so 

60
00:03:55,900 --> 00:03:55,900
far fetched after all,
uh,

61
00:03:55,900 --> 00:04:00,670
perhaps actually it's a logical next 
step after we have general artificial 

62
00:04:00,670 --> 00:04:05,141
intelligence.
So more prosaically we plan to do this 

63
00:04:06,461 --> 00:04:09,880
by building the world's first general 
purpose learning system.

64
00:04:10,940 --> 00:04:13,010
So what do those words mean?
General and learning?

65
00:04:13,310 --> 00:04:18,310
Well,
let me take you through two main 

66
00:04:18,310 --> 00:04:20,231
approaches.
Two main approaches to building ai kind 

67
00:04:20,231 --> 00:04:23,651
of falls between two schools of fault.
So in the early days of artificial 

68
00:04:23,651 --> 00:04:25,980
intelligence,
the main approach was,

69
00:04:25,981 --> 00:04:29,720
is what's called expert systems.
Sometimes it's called good old fashioned

70
00:04:29,721 --> 00:04:33,230
ai.
These days I'd go fly or traditional Ai,

71
00:04:33,231 --> 00:04:35,780
you could think of it.
And on the left hand side here,

72
00:04:36,350 --> 00:04:39,170
that's what what's involved is that we,
you know,

73
00:04:39,200 --> 00:04:44,200
teams are programmers and researchers 
hardcode knowledge in the form of rules 

74
00:04:44,200 --> 00:04:49,060
that they express in complex databases.
And you can imagine this sort of series 

75
00:04:49,251 --> 00:04:54,251
of thousands of if then rules trying to 
encapsulate the solution to whatever 

76
00:04:54,251 --> 00:04:55,790
problem the program is supposed to be 
dealing with.

77
00:04:57,260 --> 00:04:57,680
Now the,
the,

78
00:04:57,681 --> 00:05:02,120
the issue with those types of those 
types of systems is they can't generally

79
00:05:02,121 --> 00:05:05,270
deal with the unexpected and they're 
quite brittle because of that.

80
00:05:05,630 --> 00:05:08,720
So they're limited to the solutions that
have been pre programmed in them.

81
00:05:09,440 --> 00:05:14,440
They can't think for themselves and they
can't deal with anything that they 

82
00:05:14,440 --> 00:05:17,261
weren't already prepared for.
So what this means is they're limited to

83
00:05:18,170 --> 00:05:21,530
solutions that we can express as the 
programmers.

84
00:05:21,830 --> 00:05:26,830
So the program has themselves have to 
understand in new detail what the 

85
00:05:26,830 --> 00:05:31,421
solution is in order to handcraft 
encodes the Hanco these knowledge 

86
00:05:32,091 --> 00:05:32,720
systems.

87
00:05:34,050 --> 00:05:37,290
So these expert systems were inspired by
logic systems,

88
00:05:37,770 --> 00:05:42,770
logic theory.
If we now compare that to modern day 

89
00:05:43,471 --> 00:05:48,471
learning systems and which is sort of 
the advent of has been one of the 

90
00:05:48,471 --> 00:05:51,570
reasons behind the rejuvenation and the 
revolution in artificial intelligence in

91
00:05:51,571 --> 00:05:55,170
the last decade is because these 
learning systems are now really starting

92
00:05:55,171 --> 00:06:00,171
to work and learning systems.
They learned solutions from first 

93
00:06:00,171 --> 00:06:04,931
principles,
they learned directly from data and 

94
00:06:04,931 --> 00:06:07,271
directly from experience and they learn 
for themselves so they're not 

95
00:06:07,271 --> 00:06:10,931
preprogrammed with solutions.
They have to figure out solutions for 

96
00:06:10,931 --> 00:06:14,110
themselves and if we can build these 
systems in a gentle enough way,

97
00:06:14,350 --> 00:06:18,910
they can generalize to new tasks they've
never seen before and perhaps even solve

98
00:06:18,911 --> 00:06:21,250
things that we don't know how to 
ourselves,

99
00:06:21,760 --> 00:06:26,760
and that's the really amazing promise of
these systems is they could go beyond 

100
00:06:26,760 --> 00:06:30,301
the knowledge that we have ourselves and
also it's of interesting domains which 

101
00:06:30,301 --> 00:06:34,951
I'm going to talk about later in this 
lecture and in the main learning systems

102
00:06:35,631 --> 00:06:40,631
are inspired by neuroscience and 
informed by neuroscience and how the 

103
00:06:40,631 --> 00:06:45,611
brain works and that's where we get a 
lot of our inspiration from for the for 

104
00:06:45,611 --> 00:06:49,661
building these types of architectures.
So expert systems are inspired by logic 

105
00:06:49,661 --> 00:06:51,110
and learning.
Systems are inspired by the brain.

106
00:06:53,760 --> 00:06:54,150
Now,
still,

107
00:06:54,151 --> 00:06:59,151
the most famous example of an expert 
system was IBM's deep blue computer that

108
00:07:00,421 --> 00:07:03,570
beat Garry Kasparov in the late nights 
in the late nineties,

109
00:07:04,480 --> 00:07:06,870
who was at the time he was the world 
champion of chess,

110
00:07:06,900 --> 00:07:11,900
which I'm sure all of you remember now.
This was obviously a very impressive 

111
00:07:11,900 --> 00:07:13,620
technical feat.
And I remember this match very well.

112
00:07:13,621 --> 00:07:18,621
Um,
I was doing my undergraduate at 

113
00:07:18,621 --> 00:07:18,621
Cambridge and we were sort of watching 
this match.

114
00:07:18,621 --> 00:07:18,960
Um,
and as you can imagine,

115
00:07:18,961 --> 00:07:23,961
I was extremely interested in this from 
both from the chest side and the 

116
00:07:23,961 --> 00:07:25,980
computer science side.
And what I remember coming away from was

117
00:07:25,981 --> 00:07:30,981
actually,
although it's an impressive technical 

118
00:07:30,981 --> 00:07:33,081
feat,
I came away from this match more 

119
00:07:33,081 --> 00:07:35,391
impressed by Garry Kasparov,
mind than I was by the machine because 

120
00:07:35,391 --> 00:07:36,450
he was gary,
you know,

121
00:07:37,140 --> 00:07:40,080
this amazing sort of creative genius,
uh,

122
00:07:40,200 --> 00:07:45,200
probably one of the best,
if not the best chess player of all 

123
00:07:45,200 --> 00:07:45,200
time.

124
00:07:45,200 --> 00:07:47,520
And he was able to more or less hold his
own against this big brute of a machine.

125
00:07:47,521 --> 00:07:52,521
It was a huge super computer with 
obviously teams are program programmers 

126
00:07:52,521 --> 00:07:55,491
behind it with all these rules 
programmed into it and not only was he 

127
00:07:55,491 --> 00:07:59,930
able to compete on a more or less level 
footing with the machine here cause he 

128
00:07:59,930 --> 00:08:01,560
can do all the other things that we can 
do as humans.

129
00:08:01,561 --> 00:08:03,700
He could speak three languages,
drive a car,

130
00:08:03,701 --> 00:08:08,701
or ride a bike.
All of these other things that we can 

131
00:08:08,701 --> 00:08:08,701
marry out of things that we're able to 
do.

132
00:08:09,080 --> 00:08:13,020
And if you compare that to deep blue,
which is obviously amazing at chess,

133
00:08:13,280 --> 00:08:18,280
I'm the blue,
could not even play a strictly simpler 

134
00:08:18,280 --> 00:08:21,291
game,
say like noughts and crosses without 

135
00:08:21,291 --> 00:08:23,880
being totally reprogrammed.
So nothing in the knowledge base of deep

136
00:08:23,881 --> 00:08:28,881
blue would help it do anything else.
So it was this hard coded specialized 

137
00:08:29,251 --> 00:08:32,820
system that was only good for one thing 
playing chess.

138
00:08:33,390 --> 00:08:34,770
And it seemed to me that,
you know,

139
00:08:34,771 --> 00:08:38,010
in terms of thinking about intelligence,
something was missing here,

140
00:08:38,011 --> 00:08:39,930
some critical things was we're missing 
here.

141
00:08:40,170 --> 00:08:42,570
And what I believe was missing were 
these two notions.

142
00:08:42,571 --> 00:08:47,571
This notion of learning and this notion 
of generality and both of those things 

143
00:08:47,571 --> 00:08:50,700
were missing from deep blue and expert 
systems in general.

144
00:08:51,840 --> 00:08:54,700
And when I saw this match in after this 
match,

145
00:08:55,560 --> 00:09:00,560
one of the things I resolved to do is to
one day build a general games playing 

146
00:09:00,560 --> 00:09:03,270
machine that could play any game out of 
the box.

147
00:09:06,120 --> 00:09:11,120
So let's look at what's been happening 
with learning systems and actually the 

148
00:09:11,120 --> 00:09:15,771
system,
the kind of framework we think about 

149
00:09:15,771 --> 00:09:16,800
intelligence in at deep mind is a 
framework called reinforcement learning.

150
00:09:17,340 --> 00:09:22,340
And um,
the idea behind reinforcement learning 

151
00:09:22,340 --> 00:09:22,340
is that these systems,
these agents,

152
00:09:22,340 --> 00:09:27,081
we call them the AI systems,
that deep mind learn from first 

153
00:09:27,081 --> 00:09:30,471
principles through trial and error.
So that's how they build up knowledge 

154
00:09:30,471 --> 00:09:34,701
about the world.
And I'm just going to show you with the 

155
00:09:34,701 --> 00:09:35,310
aid of a simple diagram how these 
systems work at very high level.

156
00:09:36,450 --> 00:09:38,460
So first of all,
we start with the agent system.

157
00:09:38,461 --> 00:09:43,461
The AI system here on the left,
and the agent finds itself in some kind 

158
00:09:43,461 --> 00:09:48,381
of environment.
Now the environment could be the real 

159
00:09:48,381 --> 00:09:48,381
world,
in which case the agent,

160
00:09:48,381 --> 00:09:52,911
you can think of it as a robot,
a physical robot or the environment can 

161
00:09:52,911 --> 00:09:54,960
be in virtual environment like a game or
simulation,

162
00:09:55,250 --> 00:09:58,710
a computer simulation,
in which case the agent would be like an

163
00:09:58,711 --> 00:10:00,750
Avatar in that game environment.

164
00:10:01,500 --> 00:10:04,380
And the agent has been given a goal by 
the designers,

165
00:10:04,550 --> 00:10:05,100
uh,
to,

166
00:10:05,280 --> 00:10:09,150
to achieve within that environment.
Now,

167
00:10:09,151 --> 00:10:12,360
the agent only interacts with the 
environment in two ways.

168
00:10:12,840 --> 00:10:16,860
So firstly through it sensory operators 
and we normally use vision,

169
00:10:16,861 --> 00:10:20,400
but you could use other modalities like 
audition and touch.

170
00:10:20,730 --> 00:10:25,730
Um,
but we use vision and you get these 

171
00:10:25,730 --> 00:10:27,340
observations about the environment 
through your senses.

172
00:10:27,870 --> 00:10:32,870
And the observations also include 
rewards from the environment for doing 

173
00:10:32,870 --> 00:10:36,210
the right things.
And the first job of the agent system is

174
00:10:36,211 --> 00:10:38,940
to build up a model of the world out 
there,

175
00:10:39,000 --> 00:10:44,000
the environment now then how it works.
So it's got to figure out a statistical 

176
00:10:44,000 --> 00:10:45,420
model about the environment it finds 
itself in and,

177
00:10:45,570 --> 00:10:50,570
and the linkages in that environment.
And once it has a model of the world 

178
00:10:50,570 --> 00:10:53,280
which is continually updating based on 
your observations,

179
00:10:53,440 --> 00:10:57,840
then the second job of the agent system 
is to pick the right action to take.

180
00:10:58,230 --> 00:11:01,370
So at any moment in time,
I'm the agent system,

181
00:11:01,371 --> 00:11:06,371
I have a whole array of actions 
available to it and it's got to select 

182
00:11:06,371 --> 00:11:10,071
the best action that will get it closest
towards achieving its goal at that 

183
00:11:10,501 --> 00:11:14,220
moment in time.
And if the model of the world,

184
00:11:14,221 --> 00:11:19,221
the agent has is very good,
it can hypothesize in its mind what the 

185
00:11:19,221 --> 00:11:23,871
consequences of doing certain actions 
will be and what the likely change in 

186
00:11:23,871 --> 00:11:23,871
the environment will be.

187
00:11:25,280 --> 00:11:28,460
And you can think of this system in a 
cycle.

188
00:11:28,670 --> 00:11:31,320
So the agent,
once it runs out thinking time.

189
00:11:31,321 --> 00:11:33,550
So this is all a realtime system 
outputs.

190
00:11:33,560 --> 00:11:38,560
The best action is found so far.
The action gets executed and then that 

191
00:11:38,560 --> 00:11:39,860
may drive a new change in the 
environment,

192
00:11:39,861 --> 00:11:44,861
which then drives a new observation.
And then the agent updates his model of 

193
00:11:44,861 --> 00:11:48,551
the world and then selects a new action.
And this goes on in an incremental 

194
00:11:48,551 --> 00:11:52,181
fashion until eventually through 
sophisticated sort of trial error 

195
00:11:52,181 --> 00:11:54,070
processes,
the agent reaches its goal.

196
00:11:55,780 --> 00:11:57,850
Now though I started com,
looks quite simple.

197
00:11:58,000 --> 00:12:03,000
There's actually a huge amount of 
technical complexity behind this that 

198
00:12:03,000 --> 00:12:04,540
needs to be sold.
Very complex technical challenges.

199
00:12:04,870 --> 00:12:07,870
But we know that if we could solve all 
those challenges,

200
00:12:07,960 --> 00:12:12,960
this framework of reinforcement learning
is enough to give us general 

201
00:12:12,960 --> 00:12:14,500
intelligence and we know that because 
this is how the brain works.

202
00:12:14,740 --> 00:12:15,880
And um,
in fact,

203
00:12:15,881 --> 00:12:16,660
in,
in the,

204
00:12:16,661 --> 00:12:21,310
in the primate and human brain is the 
dopamine system and dopamine neurons,

205
00:12:21,430 --> 00:12:26,430
they implement a form of reinforcement 
learning and that actually allow us to 

206
00:12:26,430 --> 00:12:29,020
learn using this system of reinforcement
based on rewards.

207
00:12:32,660 --> 00:12:34,940
So how did we develop this further?
Well,

208
00:12:34,941 --> 00:12:39,941
the first work we did is partly because 
of my background in my previous career 

209
00:12:40,101 --> 00:12:44,000
of designing video games and building ai
systems for,

210
00:12:44,030 --> 00:12:45,640
for video games.
Um,

211
00:12:45,650 --> 00:12:50,650
I realized that games would be the 
perfect proving ground for developing 

212
00:12:50,650 --> 00:12:52,580
and testing ai algorithms.
Normally when you work on ai,

213
00:12:53,070 --> 00:12:56,600
you often work with robotics.
But the problem with robotics is,

214
00:12:56,610 --> 00:12:59,540
and we love robotics as an application 
area for ai,

215
00:13:00,260 --> 00:13:05,260
but as a development platform it's quite
tricky because you end up spending most 

216
00:13:05,260 --> 00:13:06,350
of your time on the hardware,
you know,

217
00:13:06,351 --> 00:13:09,860
dealing with the server motors on the 
robots and they always break and they're

218
00:13:09,861 --> 00:13:11,990
quite slow and they're very expensive to
use.

219
00:13:12,240 --> 00:13:14,750
Um,
and in fact it's much more convenient to

220
00:13:14,751 --> 00:13:19,360
use virtual simulations like games and 
actually test the sophistication of your

221
00:13:19,370 --> 00:13:22,940
Ai Algorithms in,
in those simulations.

222
00:13:24,280 --> 00:13:29,280
So we,
we started with games and in fact we 

223
00:13:29,280 --> 00:13:31,450
started with the most iconic,
sort of the first iconic game console,

224
00:13:31,451 --> 00:13:36,451
which was the Atari Twenty 600,
which some of you may remember from the 

225
00:13:36,451 --> 00:13:36,820
eighties and we,
it was the first,

226
00:13:36,821 --> 00:13:41,821
we'll sort of,
big console game that had a big 

227
00:13:41,821 --> 00:13:41,821
diversity of,
of games on it,

228
00:13:41,821 --> 00:13:42,910
very,
very different sorts of games.

229
00:13:43,270 --> 00:13:48,270
And we tested our system,
our first system back in 2013 on these 

230
00:13:48,521 --> 00:13:51,610
Atari Games.
Before we show you a video that of the,

231
00:13:51,611 --> 00:13:54,400
of the system working,
I just want to explain to you what it is

232
00:13:54,401 --> 00:13:57,250
that you're going to see.
So the agent system,

233
00:13:57,251 --> 00:14:01,810
which we call Dqn,
I'm only gets the raw pixels as inputs,

234
00:14:01,990 --> 00:14:04,540
so I'm only gets the pixels on the 
screen,

235
00:14:04,541 --> 00:14:09,541
the kind of values of the colors of the 
pixels on the screen as inputs it isn't 

236
00:14:09,541 --> 00:14:12,160
told anything else about the game.
Everything else is learned from scratch.

237
00:14:12,161 --> 00:14:15,490
It doesn't know what he's controlling.
It doesn't know how to get points.

238
00:14:15,640 --> 00:14:18,040
All it knows is here's a stream of 
numbers,

239
00:14:18,100 --> 00:14:23,100
30,000
numbers per frame and the goal is to 

240
00:14:23,100 --> 00:14:26,761
maximize the score,
doesn't know anything else about what 

241
00:14:26,761 --> 00:14:29,521
it's supposed to do,
so it has to learn everything else from 

242
00:14:29,521 --> 00:14:29,521
scratch.
And then the,

243
00:14:29,521 --> 00:14:33,460
the final sort of challenge if you like,
is this notion of generality.

244
00:14:33,461 --> 00:14:38,050
So we wanted one single system to be 
able to play all the different games out

245
00:14:38,051 --> 00:14:38,800
of the box.

246
00:14:42,200 --> 00:14:44,320
So I'm going to show you my favorite 
video,

247
00:14:44,340 --> 00:14:48,050
the Atari stuff working,
which is this game called breakout,

248
00:14:48,290 --> 00:14:50,930
which is one of those seminal games on 
the Atari.

249
00:14:51,650 --> 00:14:54,950
And in this game you control the bat to 
the bottom of the screen here.

250
00:14:54,951 --> 00:14:59,951
This pink bat that goes left and right 
and you've got to bounce this little 

251
00:14:59,951 --> 00:15:03,491
pink ball here,
this little pixel here on the left 

252
00:15:03,491 --> 00:15:06,640
against this rainbow colored wall and 
the idea of the game is you've got to 

253
00:15:06,640 --> 00:15:10,541
knock out all the bricks in the wall and
you've got five lives and you can't 

254
00:15:10,541 --> 00:15:12,500
allow the ball to go past the bat.
Otherwise you lose a life.

255
00:15:13,100 --> 00:15:18,100
So I'm just gonna run the video here and
you'll see the system improving over 

256
00:15:18,100 --> 00:15:22,481
time as it gets more experienced playing
more games and starts to figure out 

257
00:15:22,481 --> 00:15:23,210
what's happening in the game.

258
00:15:24,720 --> 00:15:29,720
So this is what it looks like after 100 
games and you can see the system is 

259
00:15:29,720 --> 00:15:32,100
starting to get the hang of what is 
supposed to be doing.

260
00:15:32,170 --> 00:15:33,720
So supposed to move the bat towards the 
ball,

261
00:15:33,721 --> 00:15:35,700
but it's missing the ball most of the 
time,

262
00:15:35,880 --> 00:15:40,880
but they're starting to get the idea 
that maybe it's a good idea to move the 

263
00:15:40,880 --> 00:15:41,730
bat towards the ball.
And then after 300 games.

264
00:15:42,530 --> 00:15:45,570
So now you can see it's about as good as
any human can play this.

265
00:15:45,690 --> 00:15:47,400
And it almost never misses the ball 
anymore,

266
00:15:47,401 --> 00:15:49,710
even when it's coming back very fast 
angles.

267
00:15:50,040 --> 00:15:50,730
So we thought,
wow,

268
00:15:50,731 --> 00:15:55,731
this is great.
But what happens if we left it playing 

269
00:15:55,731 --> 00:15:56,880
for another 200 games?
And to our surprise,

270
00:15:56,881 --> 00:15:58,770
what it did is it found this optimal 
strategy,

271
00:15:58,771 --> 00:16:03,771
which was to dig a tunnel around the 
lefthand side and then send the ball 

272
00:16:03,771 --> 00:16:07,800
behind the brick wall,
which was sort of an amazing solution to

273
00:16:07,801 --> 00:16:08,730
the problem in a way.

274
00:16:08,970 --> 00:16:10,170
And of course,
um,

275
00:16:10,560 --> 00:16:11,160
a,
a,

276
00:16:11,200 --> 00:16:16,200
you know,
it's sort of very low risk of the ball 

277
00:16:16,200 --> 00:16:18,411
camp go past your bat and it's very 
highly rewarding cause he hit many 

278
00:16:18,411 --> 00:16:20,040
bricks with just one shot.
So when we saw this,

279
00:16:20,041 --> 00:16:22,320
this was our first have since met since 
then,

280
00:16:22,321 --> 00:16:27,321
many Aha moments for us where we 
actually learned something from our own 

281
00:16:27,321 --> 00:16:30,570
system because the program is,
and the research is behind this,

282
00:16:30,760 --> 00:16:32,880
uh,
amazing researchers,

283
00:16:32,940 --> 00:16:34,800
but they're not so good at playing Atari
Games.

284
00:16:34,950 --> 00:16:38,310
So they didn't really know themselves 
about this tactic.

285
00:16:38,540 --> 00:16:43,540
Um,
and obviously it's being executed with 

286
00:16:43,540 --> 00:16:43,540
sort of incredible position from the,
from the system.

287
00:16:45,120 --> 00:16:50,120
So then we took these,
these systems and the next thing we 

288
00:16:50,120 --> 00:16:53,631
worked on and applied it to was probably
our most famous program called Alphago 

289
00:16:54,060 --> 00:16:58,650
and Alphago was our program using these 
reinforcement learning ideas,

290
00:16:59,100 --> 00:17:04,100
scaled up even further to play the 
ancient game of go and for those you 

291
00:17:04,100 --> 00:17:05,520
don't know the game and I encourage you 
all to learn.

292
00:17:05,560 --> 00:17:08,490
There's an amazing game that I think 
you'd all like,

293
00:17:08,810 --> 00:17:13,810
this is what the board looks like.
It's a very esoteric and artistic game 

294
00:17:13,810 --> 00:17:18,620
and it's played on a 19 by 19 grid and 
you take turns black and white take 

295
00:17:18,781 --> 00:17:23,010
turns to put stones on the vertices of 
the of the board,

296
00:17:23,190 --> 00:17:25,560
and the board initially starts empty and
it fills up.

297
00:17:27,380 --> 00:17:30,200
Now the history of go is long and 
storied one,

298
00:17:30,380 --> 00:17:33,320
it's over 3000 years old is invented in 
China,

299
00:17:33,640 --> 00:17:38,640
has played all over Asia and in fact 
it's considered in Asia to be more than 

300
00:17:38,640 --> 00:17:43,571
just a game.
It's something more kin to poetry or 

301
00:17:43,571 --> 00:17:43,571
art.
In fact,

302
00:17:43,571 --> 00:17:47,291
Confucius wrote about go as one of the 
four great arts that any true scholars 

303
00:17:47,291 --> 00:17:49,130
should master along with poetry,
calligraphy,

304
00:17:49,220 --> 00:17:54,220
music.
So it's really considered to be one of 

305
00:17:54,220 --> 00:17:57,021
these sort of profound arts,
like all of these other artistic 

306
00:17:57,021 --> 00:17:59,550
endeavors.
And today it's as popular as ever.

307
00:17:59,551 --> 00:18:02,070
40 million active players,
2000 professionals,

308
00:18:02,460 --> 00:18:06,630
and the game of go is incredibly simple.
I could teach you in five minutes.

309
00:18:06,631 --> 00:18:10,500
There's only two rules,
but the complexity that comes out of it,

310
00:18:10,730 --> 00:18:15,730
it was what makes it so elegant.
And one measure of that complexity is 

311
00:18:15,730 --> 00:18:18,240
the fact there are 10 to the power,
170 possible board positions.

312
00:18:18,330 --> 00:18:23,330
So that's a one with 117 zeros after it.
And that's more than there are atoms in 

313
00:18:23,330 --> 00:18:24,390
the universe,
right?

314
00:18:24,391 --> 00:18:26,910
So that's the level of complexity just 
comes out of these tools.

315
00:18:27,270 --> 00:18:28,100
And,
um,

316
00:18:28,320 --> 00:18:33,320
and that's,
that's what makes the game so deep and 

317
00:18:33,320 --> 00:18:33,320
so profound.
Uh,

318
00:18:33,320 --> 00:18:33,720
and,
you know,

319
00:18:33,721 --> 00:18:38,721
again,
sort of these ancient scholars thought 

320
00:18:38,721 --> 00:18:41,091
about go is containing some of the 
mysteries of the universe in it and 

321
00:18:41,091 --> 00:18:42,060
therefore was worthy of this incredible 
mouse study.

322
00:18:43,320 --> 00:18:45,600
And of course,
that complexity and uh,

323
00:18:45,601 --> 00:18:50,601
and the esoteric nature of the game is 
one of the reasons which makes it so 

324
00:18:50,601 --> 00:18:53,841
difficult for computers to play.
And the game of go proceeds one stone at

325
00:18:54,481 --> 00:18:59,481
a time,
when you place it down to the board 

326
00:18:59,481 --> 00:18:59,481
fills up like this.
So this is the end of the game.

327
00:18:59,481 --> 00:19:04,100
And the way you determine the winner is 
what you're trying to do with your 

328
00:19:04,100 --> 00:19:06,600
stones is surround off a wall off empty 
areas of territory.

329
00:19:06,900 --> 00:19:08,810
And then you count the number of,
um,

330
00:19:08,880 --> 00:19:13,880
uh,
the number of squares that you've 

331
00:19:13,880 --> 00:19:15,770
surrounded compared to your opponent.
And the person that surrounded the most 

332
00:19:15,770 --> 00:19:16,560
squares wins the game.
So in this case here,

333
00:19:16,561 --> 00:19:19,350
it's a very close game,
but white winds by one point.

334
00:19:20,790 --> 00:19:24,030
So why is go so hard for computers to 
play?

335
00:19:24,300 --> 00:19:28,920
So after deep blue beat Garry Kasparov,
the next big challenge,

336
00:19:28,940 --> 00:19:30,600
the sort of Mount Everest,
if you like,

337
00:19:30,810 --> 00:19:35,340
of um,
computer ai research was go and go,

338
00:19:35,341 --> 00:19:38,550
is much harder than chess for computers.
One,

339
00:19:38,580 --> 00:19:43,580
because of this enormous number of 
possibilities that I've just talked 

340
00:19:43,580 --> 00:19:44,850
about,
this 10 to 170 possible positions.

341
00:19:44,940 --> 00:19:46,860
So the search base is,
is much,

342
00:19:46,861 --> 00:19:51,861
much bigger than it is for chess,
but the second and kind of even harder 

343
00:19:51,861 --> 00:19:55,740
problem is I'm CESC per program chess 
engines including deep blue,

344
00:19:55,980 --> 00:19:58,170
rely on what's called an evaluation 
function.

345
00:19:58,350 --> 00:20:03,350
So this is one of these handcrafted 
rules based systems that tell the 

346
00:20:03,350 --> 00:20:07,011
machine which side is winning in the 
current position and that's what allows 

347
00:20:07,011 --> 00:20:11,570
the blue and it's a and his successes,
his descendants to figure out what the 

348
00:20:12,511 --> 00:20:17,460
right move is to play the palm go is 
it's such an Easter game,

349
00:20:17,670 --> 00:20:22,670
it's impossible to figure out what the 
right set of rules are and encapsulate 

350
00:20:22,670 --> 00:20:22,800
that in a rules based system.

351
00:20:23,220 --> 00:20:25,720
Even if you ask top go players,
you know,

352
00:20:25,770 --> 00:20:30,770
why did they make a particular move?
They'll often tell you it just felt 

353
00:20:30,770 --> 00:20:34,551
right,
but they won't actually be able to 

354
00:20:34,551 --> 00:20:34,551
explicitly tell you themselves why they 
pick the move.

355
00:20:34,590 --> 00:20:36,630
Whereas if you ask that to a top chess 
player,

356
00:20:36,780 --> 00:20:39,510
they'll almost certainly give you a 
specific plan.

357
00:20:39,511 --> 00:20:40,630
They were thinking about,
you know,

358
00:20:40,660 --> 00:20:43,590
I was planning a than I thought it would
happen and I was going to answer.

359
00:20:43,591 --> 00:20:47,800
See now that plan in the end may not be 
very good or may fail for some reason,

360
00:20:47,801 --> 00:20:50,230
but they normally have an explicit plan 
in go.

361
00:20:50,231 --> 00:20:54,310
It's much more about feel much more how 
an artist would think,

362
00:20:56,710 --> 00:21:01,710
and one way to think about that is the 
goal is primarily game about about 

363
00:21:01,710 --> 00:21:05,941
intuition rather than calculation,
which is more dominant in a game like 

364
00:21:06,101 --> 00:21:09,160
chess.
So that's how humans players,

365
00:21:09,190 --> 00:21:14,190
professional players deal with this 
enormous complexity and this evaluation 

366
00:21:14,190 --> 00:21:17,341
function.
They rely on their instincts and their 

367
00:21:17,341 --> 00:21:17,341
intuition.

368
00:21:17,341 --> 00:21:21,690
So we ended up taking a totally 
different approach to the way that chess

369
00:21:21,691 --> 00:21:26,691
computers were built.
And we built our Alphago with these 

370
00:21:26,691 --> 00:21:28,170
learning systems and we actually 
created.

371
00:21:28,540 --> 00:21:33,090
We were to neural networks,
which are loosely based on how the brain

372
00:21:33,091 --> 00:21:36,030
works to deal with these complex 
problems.

373
00:21:36,360 --> 00:21:40,380
We create a one your network called the 
policy network that takes in the current

374
00:21:40,381 --> 00:21:45,381
ball position and learns through looking
at millions of different games and 

375
00:21:45,381 --> 00:21:49,431
playing millions of different millions 
of games against itself and seeing and 

376
00:21:49,431 --> 00:21:50,350
experiencing millions of games of go.
Um,

377
00:21:50,570 --> 00:21:55,570
what sorts of moods are most likely to 
be played in a particular position so 

378
00:21:55,570 --> 00:21:58,140
you can think of it taking the current 
board position and returning to you,

379
00:21:58,230 --> 00:22:00,930
like the top five most likely moves to 
be made.

380
00:22:01,320 --> 00:22:06,320
So that really narrows down this 
enormous search base that you need to 

381
00:22:06,320 --> 00:22:09,321
explore.
We then have to look at everything 

382
00:22:09,321 --> 00:22:09,321
anymore,
like a brute force system would have to.

383
00:22:09,321 --> 00:22:10,260
You can just look at them,
most likely moves.

384
00:22:11,420 --> 00:22:15,710
And then the second thing that was kind 
of sort of an unknown whether this could

385
00:22:15,711 --> 00:22:20,711
be done was we built a system that could
take this to the second year network 

386
00:22:20,711 --> 00:22:25,181
called the value net here on the right,
the pink network that could take the 

387
00:22:25,181 --> 00:22:26,320
current board position and return a 
value.

388
00:22:26,321 --> 00:22:29,240
Your probability between zero and one of
who was winning.

389
00:22:29,600 --> 00:22:32,810
So zero would be white,
100 percent likely to win,

390
00:22:33,030 --> 00:22:36,320
a one would be black,
100 percent likely to win and point five

391
00:22:36,340 --> 00:22:41,340
would mean equal position and oversee 
over through a course of training of 

392
00:22:41,340 --> 00:22:43,190
millions paying millions of games 
against itself,

393
00:22:43,400 --> 00:22:48,400
it learns to predict from any position 
who is going to win the game and how 

394
00:22:48,591 --> 00:22:53,591
confident was in that prediction.
And so by combining these two neural 

395
00:22:53,631 --> 00:22:56,300
networks into one system which became 
the Alphago System,

396
00:22:56,510 --> 00:23:01,010
we're able to solve these two very 
difficult challenges that go presents.

397
00:23:02,080 --> 00:23:07,080
And once we had this system,
we decided to challenge one of the 

398
00:23:07,080 --> 00:23:10,861
greatest players ever in go history,
a genius South Korea and grandmaster 

399
00:23:12,310 --> 00:23:14,140
called Lisa Doll.
He's eight,

400
00:23:14,141 --> 00:23:14,770
one,
18,

401
00:23:14,771 --> 00:23:19,090
well titles and he was considered to be 
the greatest player of the past decade.

402
00:23:19,480 --> 00:23:22,930
And we had a million dollar challenge 
match in Seoul,

403
00:23:22,960 --> 00:23:25,300
South Korea,
in back in 2016.

404
00:23:25,630 --> 00:23:26,640
And um,
you know,

405
00:23:26,680 --> 00:23:28,240
before that match,
everybody,

406
00:23:28,241 --> 00:23:33,241
including Lisa doll,
thought it would be a whitewash 

407
00:23:33,241 --> 00:23:34,510
whitewash to Lisa doll because until 
this point,

408
00:23:34,690 --> 00:23:38,530
no go program had ever even beat in a 
professional player,

409
00:23:38,620 --> 00:23:41,140
let alone a world champion.
So,

410
00:23:41,240 --> 00:23:46,240
um,
so all these sort of traditional 

411
00:23:46,240 --> 00:23:46,240
techniques that were being used to make 
computers,

412
00:23:46,240 --> 00:23:51,190
even though they've been developed for a
further 20 years since the deepblue 

413
00:23:51,190 --> 00:23:54,310
match,
they still hadn't got anywhere near to 

414
00:23:54,310 --> 00:23:54,670
professional level in go.

415
00:23:56,310 --> 00:24:01,310
So we played this match and over 200 
million people across the world watched 

416
00:24:01,701 --> 00:24:06,701
the five matches and I'm Alphago 
incredibly won the match for one and it 

417
00:24:07,711 --> 00:24:12,711
was proclaimed by many experts both in 
ai and go to be a decade before its 

418
00:24:13,021 --> 00:24:14,110
time,
uh,

419
00:24:14,540 --> 00:24:19,540
and you know,
it was kind of a mentor match that I 

420
00:24:19,540 --> 00:24:21,531
think will go down in history in,
in sort of ai as an ai landmark and 

421
00:24:21,531 --> 00:24:24,300
there's often been called since then as 
a sort of sputnik event for ai,

422
00:24:24,640 --> 00:24:29,640
especially for China and Asia.
But the most important thing is 

423
00:24:29,640 --> 00:24:31,110
obviously we are,
Alphago won the match and that's what we

424
00:24:31,111 --> 00:24:33,420
built it for.
But the most interesting thing about the

425
00:24:33,421 --> 00:24:35,640
match was how Alphago won,
um,

426
00:24:35,720 --> 00:24:40,720
and how it played.
So I just want to explain to you a 

427
00:24:40,720 --> 00:24:44,310
little bit about Alphago's play.
Even though most of you may not know how

428
00:24:44,311 --> 00:24:44,670
to play.

429
00:24:44,670 --> 00:24:49,670
Go,
I think you can still appreciate what 

430
00:24:49,670 --> 00:24:52,161
Alpha go dead.
So this is a board position from game 

431
00:24:52,161 --> 00:24:54,120
too,
and this is move 37,

432
00:24:54,240 --> 00:24:56,370
which is probably the most famous move 
in the match.

433
00:24:56,670 --> 00:24:58,630
And um,
Alphago is black hair,

434
00:24:58,640 --> 00:25:03,640
it says very early in the game,
at least a dollar is white and Alphago 

435
00:25:03,640 --> 00:25:06,060
plays this move here on the right hand 
side outlined in red,

436
00:25:06,720 --> 00:25:11,720
this stone here.
And the key thing to notice about where 

437
00:25:11,720 --> 00:25:12,750
this stone has been placed is that it's 
on the fifth line.

438
00:25:13,170 --> 00:25:16,140
So you can see it's on the fifth line 
from the right hand side of the board,

439
00:25:16,390 --> 00:25:19,650
the board's 19 by 19.
Now in the openings,

440
00:25:19,770 --> 00:25:24,770
if you're professional,
you almost always play on the third or 

441
00:25:24,770 --> 00:25:27,771
fourth lines.
And that's the most important lines to 

442
00:25:27,771 --> 00:25:27,771
be,
um,

443
00:25:27,771 --> 00:25:29,340
to be disputing early on in the game of 
go.

444
00:25:30,180 --> 00:25:33,700
So a play on the fifth line this early 
on is kind of unthinkable.

445
00:25:33,760 --> 00:25:38,760
No professional player would even 
consider this move because it seems 

446
00:25:38,760 --> 00:25:42,220
suboptimal and wasteful.
And yet Alphago decided to play here.

447
00:25:42,490 --> 00:25:45,580
And then it turned out the reason the 
alpha go played,

448
00:25:45,581 --> 00:25:49,720
there was 100 moves later.
These two stones here in the bottom left

449
00:25:49,721 --> 00:25:54,721
hand corner that I ringed in red,
ended up kind of fighting on the bottom 

450
00:25:54,721 --> 00:25:58,591
left here with the board,
ended up spilling all the way into the 

451
00:25:58,591 --> 00:25:59,020
middle of the board and moving all 
across the board.

452
00:25:59,080 --> 00:26:04,080
And then 100 years later ended up 
connecting up perfectly with this stone 

453
00:26:04,080 --> 00:26:07,861
on the right hand side and move 37.
And that was ended up being decisive in 

454
00:26:07,861 --> 00:26:09,040
that battle.
And at one Alphago the whole game,

455
00:26:09,610 --> 00:26:14,610
right?
So somehow it's as if Alphago had 

456
00:26:14,610 --> 00:26:16,210
resigned,
the understood this was gonna happen and

457
00:26:16,211 --> 00:26:20,020
position that stone perfectly for 100 
moves into the future.

458
00:26:21,740 --> 00:26:26,740
So of course it will.
The interesting thing is we can all 

459
00:26:26,740 --> 00:26:30,820
think about what is creativity,
and I'm going to come and talk a little 

460
00:26:30,820 --> 00:26:32,210
bit more about that in the latter part 
of this talk,

461
00:26:32,480 --> 00:26:35,090
but we could all play an original movie 
in some sense.

462
00:26:35,091 --> 00:26:40,091
Even if we didn't add to play God,
we call just play a random move on the 

463
00:26:40,091 --> 00:26:40,220
board and that will be surprising in 
some sense,

464
00:26:40,490 --> 00:26:45,490
but the key thing about going is 
although it's considered to be an art 

465
00:26:45,490 --> 00:26:45,870
form,
it's like objective art,

466
00:26:46,100 --> 00:26:51,100
so a movie is only considered original 
and creative if it ends up being 

467
00:26:51,100 --> 00:26:52,320
effective and you can measure the 
effectiveness,

468
00:26:52,450 --> 00:26:57,450
obviously seeing the result of the game 
and then starting that afterwards and 

469
00:26:57,450 --> 00:27:01,071
seeing if that move really had a 
material difference to the outcome and 

470
00:27:01,591 --> 00:27:03,970
you don't have to take my word for it.
You can see this.

471
00:27:04,000 --> 00:27:08,510
I'm just going to play this very short,
that funny clip from the live commentary

472
00:27:08,790 --> 00:27:11,010
stream that was going out to the 
millions of players.

473
00:27:11,160 --> 00:27:14,790
There will be watching this on youtube 
and I'm on the right hand side.

474
00:27:14,791 --> 00:27:18,780
Here is the strongest player the West 
has ever produced.

475
00:27:18,870 --> 00:27:23,870
Michael Redmond,
who's nine damn professional and his 

476
00:27:23,870 --> 00:27:26,601
reaction,
he's watching the game live and 

477
00:27:26,601 --> 00:27:26,880
commentating on it to seeing this move.
Thirty seven,

478
00:27:27,410 --> 00:27:28,300
so hopefully you'll be your to here.

479
00:27:28,580 --> 00:27:33,580
The Google team was talking about is 
this kind of a value?

480
00:27:39,070 --> 00:27:44,070
That's very surprising move.
I thought I thought it was a mistake

481
00:27:45,390 --> 00:27:50,390
so you can hear that he thought it was a
mistake and he goes on to say later in 

482
00:27:50,390 --> 00:27:51,620
that clip that he thought it was a 
misclick,

483
00:27:51,680 --> 00:27:56,680
so he thought our computer operator had 
actually clicked the wrong place on the 

484
00:27:56,680 --> 00:27:56,830
board,
the computer board,

485
00:27:56,831 --> 00:28:01,490
because he couldn't believe that Alphago
or would play that move.

486
00:28:03,770 --> 00:28:08,770
Then of course I must mention that Lisa 
doll himself came up with his own 

487
00:28:08,871 --> 00:28:13,871
incredible,
brilliant moving game for which was the 

488
00:28:13,871 --> 00:28:13,871
game that he won,
almost move 78,

489
00:28:13,871 --> 00:28:18,370
which this move in the middle of this 
called a wedge move and that has been 

490
00:28:18,370 --> 00:28:19,370
analyzed by all the players around the 
world.

491
00:28:19,371 --> 00:28:22,550
Both this move and move 37 in the two 
years hence.

492
00:28:22,730 --> 00:28:24,950
And they're both being proclaimed to be 
amazing moves.

493
00:28:25,040 --> 00:28:27,230
And this move here,
I haven't got time to explain about it,

494
00:28:27,410 --> 00:28:32,410
but it triggered a misvaluation in Alpha
goes networks and that's what allowed 

495
00:28:32,410 --> 00:28:33,470
these adults to win,
to win that game.

496
00:28:34,640 --> 00:28:35,780
So for us,
this was a,

497
00:28:36,050 --> 00:28:41,050
an amazing sort of once in a lifetime 
experience and it was full of drama for 

498
00:28:41,050 --> 00:28:44,621
us and if you're interested to see a 
little bit more about the kind of the 

499
00:28:44,621 --> 00:28:46,520
human emotions and the spirit of human 
endeavor behind this match,

500
00:28:46,521 --> 00:28:51,521
I'd encourage you to watch this 
documentary award winning documentary 

501
00:28:51,521 --> 00:28:53,000
that done by this brilliant director,
Greg Coast,

502
00:28:53,001 --> 00:28:58,001
which is available on Netflix and you'll
see what went into the match and the 

503
00:28:58,001 --> 00:28:59,390
nuances behind it and what the goplayers
thought.

504
00:28:59,780 --> 00:29:01,670
But there's one thing I want to quote 
about from there,

505
00:29:01,671 --> 00:29:06,671
which is Lisa Dell's own thoughts and 
reflections on move 37 after the match.

506
00:29:07,190 --> 00:29:12,190
The director asked him what he thought 
about Alphago and route 37.

507
00:29:13,070 --> 00:29:18,070
And he said,
I thought Alphago is based on 

508
00:29:18,070 --> 00:29:18,070
probability calculation and it was 
merely a machine.

509
00:29:18,070 --> 00:29:19,850
But when I saw this move,
I changed my mind.

510
00:29:20,120 --> 00:29:25,120
Surely Alphago is creative.
This move was really be creative and 

511
00:29:25,120 --> 00:29:27,140
beautiful.
So as really amazing moment and I,

512
00:29:27,141 --> 00:29:29,450
I kind of nearly cried when I saw that 
on the film.

513
00:29:29,451 --> 00:29:32,000
After said I didn't see him say that in 
the live.

514
00:29:32,270 --> 00:29:34,490
And I thought it was an amazing thing 
for him to say,

515
00:29:34,491 --> 00:29:36,110
and,
and very,

516
00:29:36,140 --> 00:29:38,690
uh,
deep of him to realize that.

517
00:29:40,160 --> 00:29:45,160
So I want to now just talk a little bit 
about these words I've been using and 

518
00:29:45,160 --> 00:29:45,310
throwing around intuition and 
creativity.

519
00:29:45,490 --> 00:29:49,420
What do I mean by that?
At least in this context,

520
00:29:49,480 --> 00:29:54,480
and I should caveat this with,
and I'm sure we'll get into this in the 

521
00:29:54,480 --> 00:29:54,480
q and a,
that I'm not saying this is,

522
00:29:54,480 --> 00:29:56,920
encompasses all of what we think of is 
intuition and creativity.

523
00:29:57,100 --> 00:30:02,100
But I think at least want to think about
operationalizing some of these 

524
00:30:02,100 --> 00:30:04,210
definitions so we can discuss it in a 
scientific way.

525
00:30:05,640 --> 00:30:08,250
So intuition then the way I think about 
intuition,

526
00:30:08,430 --> 00:30:13,430
it's really,
it's implicit knowledge that we have 

527
00:30:13,430 --> 00:30:16,340
acquired through experience,
but it's knowledge that's not 

528
00:30:16,340 --> 00:30:19,320
consciously expressible or accessible.
So we can't consciously access it and we

529
00:30:19,321 --> 00:30:21,750
can't express it to others.
And that's what,

530
00:30:21,780 --> 00:30:23,760
why it seems a little bit mysterious to 
us.

531
00:30:24,150 --> 00:30:26,670
This kind of implicit knowledge.
Now,

532
00:30:26,671 --> 00:30:31,671
of course we know we have it and you can
test the existence in the quality of it 

533
00:30:31,671 --> 00:30:34,340
by testing it behaviorally.
You can verify behaviorally and in a,

534
00:30:34,350 --> 00:30:36,390
in a game like go,
it's very easy.

535
00:30:36,391 --> 00:30:41,220
You can give somebody a go position and 
ask them to come up with a move and then

536
00:30:41,221 --> 00:30:46,221
evaluate the quality of that move.
So I think that's what it encompasses 

537
00:30:46,551 --> 00:30:49,070
more intuition is.
So what about creativity?

538
00:30:49,690 --> 00:30:51,470
Well,
I think one way you could operationalize

539
00:30:51,471 --> 00:30:56,471
the definition of creativity is the 
ability to synthesize knowledge to in 

540
00:30:56,471 --> 00:30:57,530
the service of producing a novel or 
original idea.

541
00:30:58,430 --> 00:31:03,430
And I think under those definitions,
Africa in some sense clearly 

542
00:31:03,430 --> 00:31:05,660
demonstrated these abilities during this
match or beer.

543
00:31:05,661 --> 00:31:10,661
Obviously caveated by the fact that it's
still a very constrained domain of a 

544
00:31:10,661 --> 00:31:14,090
board game.
But let's think about creativity sort of

545
00:31:14,091 --> 00:31:19,091
more generally.
Here's another definition of creativity 

546
00:31:19,091 --> 00:31:22,451
is I think I got out of the Oxford 
dictionary the ability to use skill and 

547
00:31:22,451 --> 00:31:25,601
imagination to produce something new.
And I think there were at least three 

548
00:31:25,601 --> 00:31:28,820
types of creativity or three levels of 
creativity if you like.

549
00:31:30,500 --> 00:31:35,500
So the first type,
if you imagine that you're given three 

550
00:31:35,780 --> 00:31:40,780
or more examples in a particular topic 
and let's imagine for the moment that 

551
00:31:40,780 --> 00:31:45,491
the green dots are these examples and 
the white box is a particular topic or 

552
00:31:45,491 --> 00:31:49,480
field of endeavor and you also create 
something new.

553
00:31:50,570 --> 00:31:55,570
So one way you could do that is what I 
call interpolation and it's used often 

554
00:31:55,570 --> 00:31:59,600
in machine learning and ai as an ai term
and it's Appalachian you can think of is

555
00:31:59,840 --> 00:32:03,830
kind of like an averaging.
So here are three trainings on Paul's,

556
00:32:03,831 --> 00:32:08,831
here are three examples of things we 
would like from this world of 

557
00:32:08,831 --> 00:32:11,501
possibilities and you kind of find an 
average of those things and in some 

558
00:32:11,501 --> 00:32:13,850
sense that orange door is new,
right?

559
00:32:13,851 --> 00:32:15,200
It's not,
it's different from the,

560
00:32:15,320 --> 00:32:16,760
from the,
from the examples.

561
00:32:16,970 --> 00:32:21,970
And it's something new,
but it's still sort of contained within 

562
00:32:21,970 --> 00:32:22,510
the space.
This green dot,

563
00:32:22,511 --> 00:32:25,100
a green line,
the space that the examples cover,

564
00:32:27,160 --> 00:32:28,570
the next level of creativity,
which is,

565
00:32:28,571 --> 00:32:33,571
you know,
a higher level of creativity would be 

566
00:32:33,571 --> 00:32:33,571
extrapolation.
So now you know,

567
00:32:33,571 --> 00:32:36,670
you have those examples,
but instead of just finding an average,

568
00:32:36,820 --> 00:32:39,440
you're extending the boundaries of what 
you already know.

569
00:32:39,920 --> 00:32:44,720
So this will be the blue dots and you 
can see those three blue dots as outside

570
00:32:44,840 --> 00:32:49,840
of the boundaries that,
that sort of marked out by the training 

571
00:32:49,840 --> 00:32:49,940
examples,
the green dots.

572
00:32:51,640 --> 00:32:55,840
And then finally there's what I would 
call invention or innovation,

573
00:32:56,260 --> 00:32:57,070
which,
uh,

574
00:32:57,130 --> 00:33:02,130
which is here represented by the yellow 
door that's outside the white box 

575
00:33:02,141 --> 00:33:04,860
completely.
And this is something completely new,

576
00:33:04,990 --> 00:33:08,710
perhaps informed in some way by what's 
inside the box.

577
00:33:10,750 --> 00:33:12,120
Now,
you know,

578
00:33:12,220 --> 00:33:12,860
how,
uh,

579
00:33:12,910 --> 00:33:16,270
how we doing on the Ai Front where these
levels of creativity.

580
00:33:16,271 --> 00:33:19,870
So let's,
let's examine machine creativity and you

581
00:33:19,871 --> 00:33:21,070
know,
neural network systems,

582
00:33:21,071 --> 00:33:26,071
the kind of systems I showed you.
Sometimes the fashionable ones are 

583
00:33:26,071 --> 00:33:27,550
called deep learning these days.
They're pretty good interpolation,

584
00:33:27,780 --> 00:33:32,780
you know,
they're massive statistical machines if 

585
00:33:32,780 --> 00:33:32,780
you like.
And they're very good at,

586
00:33:32,800 --> 00:33:37,800
uh,
averaging things and spotting patterns 

587
00:33:37,800 --> 00:33:38,740
in data.
Then you have things,

588
00:33:38,741 --> 00:33:42,520
Alphago like systems which are getting 
pretty good,

589
00:33:42,521 --> 00:33:44,620
I would say extrapolation,
you know,

590
00:33:44,650 --> 00:33:49,650
finding new things beyond the boundaries
of even what the human designers knew 

591
00:33:50,231 --> 00:33:55,231
about,
but still within the same general 

592
00:33:55,231 --> 00:33:57,960
context.
And then you've got true invention,

593
00:33:58,200 --> 00:34:02,100
which I think no ai systems are anywhere
close to yet,

594
00:34:02,700 --> 00:34:03,840
right?
So this would be,

595
00:34:03,900 --> 00:34:06,300
instead of coming up with an original 
moving go,

596
00:34:06,390 --> 00:34:11,390
it will be inventing go right on venting
chess and there's no systems that are 

597
00:34:11,641 --> 00:34:14,280
able to do that.
But we can come up with,

598
00:34:14,300 --> 00:34:15,600
you know,
I think Alphago,

599
00:34:15,770 --> 00:34:17,760
uh,
definitely demonstrated extrapolation.

600
00:34:17,850 --> 00:34:22,850
It wasn't just averaging what humans 
have done before or mimicking what 

601
00:34:22,850 --> 00:34:24,150
humans have done before it was coming 
out with genuinely new ideas,

602
00:34:24,510 --> 00:34:28,110
but it can't invent something truly new.

603
00:34:30,420 --> 00:34:31,350
So you might ask,
well,

604
00:34:31,351 --> 00:34:32,700
what's missing?
Well,

605
00:34:32,701 --> 00:34:35,520
although AI systems have been pretty 
successful so far,

606
00:34:35,580 --> 00:34:38,280
there's actually a whole bunch of things
that we still need.

607
00:34:38,980 --> 00:34:43,890
I still need to crack.
And things like concepts,

608
00:34:44,160 --> 00:34:45,780
abstract,
abstract thinking,

609
00:34:45,810 --> 00:34:49,410
reasoning by analogy,
memory systems and imagination,

610
00:34:49,411 --> 00:34:51,340
which as we just saw earlier,
is in,

611
00:34:51,850 --> 00:34:53,850
in many of the definitions of 
creativity.

612
00:34:54,210 --> 00:34:59,210
And a lot of these terms of these ideas 
and capabilities are missing from our 

613
00:34:59,210 --> 00:35:04,191
current ai systems and this is where the
cutting edge of ai researchers at the 

614
00:35:04,191 --> 00:35:08,451
moment and we're working variously hard 
on all these different topics are just 

615
00:35:08,451 --> 00:35:13,100
mentioned and I think those things are 
key to this invention or this out of the

616
00:35:13,101 --> 00:35:18,101
box thinking because I think a lot of 
that comes from interdisciplinary 

617
00:35:18,101 --> 00:35:21,701
thinking,
spotting unusual connections between 

618
00:35:21,701 --> 00:35:24,320
different subjects and doing things like
imagining counterfactuals right?

619
00:35:24,321 --> 00:35:29,321
So imagine fantastical scenarios and a 
lot of our creativity I believe comes 

620
00:35:30,980 --> 00:35:35,300
from those capabilities that we 
currently don't have in our AI systems.

621
00:35:36,770 --> 00:35:41,770
So where can we look for inspiration?
And I've only got time to cover one of 

622
00:35:41,770 --> 00:35:45,501
those topics.
Each one of those could be a whole 

623
00:35:45,501 --> 00:35:45,720
lecture in itself,
but I'm just going to talk about a topic

624
00:35:45,721 --> 00:35:48,000
that I've studied for a long time 
imagination,

625
00:35:48,240 --> 00:35:51,240
which I think is one of the main keys to
creativity.

626
00:35:51,840 --> 00:35:56,840
And we can actually take our inspiration
from the brain and especially from what 

627
00:35:56,840 --> 00:35:58,320
I call a systems neuroscience point of 
view,

628
00:35:58,500 --> 00:36:01,170
which is a high level understanding of 
the brain.

629
00:36:01,350 --> 00:36:06,350
And interested in the algorithms and the
architecture of the brain uses and I 

630
00:36:06,350 --> 00:36:10,941
actually studied memory and imagination 
for my phd and I was very interesting.

631
00:36:10,981 --> 00:36:15,981
The question of how do we imagine what 
are the brain mechanisms behind 

632
00:36:15,981 --> 00:36:19,230
imagination?
And when I started my phd,

633
00:36:19,320 --> 00:36:24,320
one of the things I've started looking 
at was how memory works and I became 

634
00:36:24,320 --> 00:36:28,371
convinced that memory was a 
reconstructive process so you shouldn't 

635
00:36:28,371 --> 00:36:29,130
think of memory as a videotape.

636
00:36:29,190 --> 00:36:34,190
It's not a perfect recording.
And if we remember tomorrow we think 

637
00:36:34,190 --> 00:36:34,320
back to this lecture or what you had at 
lunch today,

638
00:36:34,560 --> 00:36:37,050
it wouldn't be.
It's not really a perfect video tape.

639
00:36:37,230 --> 00:36:39,840
You're actually going to reconstruct it 
from its components.

640
00:36:39,900 --> 00:36:43,050
So we reassemble our memories from our 
component,

641
00:36:43,051 --> 00:36:45,390
from components and we put them back 
together.

642
00:36:46,590 --> 00:36:48,870
So I was thinking if memory,
and there's a lot of evidence that's how

643
00:36:48,900 --> 00:36:51,780
memory works.
So I think if that home is as how memory

644
00:36:51,781 --> 00:36:54,480
works and you can think of memories is 
reconstructed process,

645
00:36:54,750 --> 00:36:57,930
then maybe imagination which is a 
constructive process.

646
00:36:58,080 --> 00:37:00,630
You're putting these components together
in a novel way.

647
00:37:00,930 --> 00:37:04,770
Maybe it relies on the same brain 
mechanisms and the same brain areas,

648
00:37:05,640 --> 00:37:08,430
so we know and we've known for $50 more 
than 50 years.

649
00:37:08,431 --> 00:37:11,910
That memory is reliant on an area of the
brain called the hippocampus,

650
00:37:12,120 --> 00:37:15,150
which is shown here in pink and is at 
the center of your brain.

651
00:37:15,540 --> 00:37:17,650
And without your hippocampus,
your,

652
00:37:17,660 --> 00:37:22,660
you will become our music.
And that's what happens in terrible 

653
00:37:22,660 --> 00:37:25,611
diseases like Alzheimer's.
And so what we thought is why don't we 

654
00:37:25,611 --> 00:37:27,930
test some patients who have damage to 
the hippocampus,

655
00:37:27,960 --> 00:37:32,960
but the rest of their brains intact on 
imagination tasks and see if they can 

656
00:37:32,960 --> 00:37:37,731
imagine.
And so what we did is quite a simple 

657
00:37:37,731 --> 00:37:38,370
test,
but no one had thought to do this for,

658
00:37:38,430 --> 00:37:43,430
you know,
even though we've been researching 

659
00:37:43,430 --> 00:37:45,171
memory for almost a hundred years now.
And we thought to test these patients on

660
00:37:46,430 --> 00:37:51,430
a simple imagination tasks where we got 
them to imagine scenarios like imagine 

661
00:37:51,430 --> 00:37:55,521
you're lying on a tropical,
a white sandy beach in a beautiful 

662
00:37:55,521 --> 00:37:55,521
tropical bay.
Um,

663
00:37:55,521 --> 00:37:57,240
if describe everything you can see 
around you.

664
00:37:57,990 --> 00:37:59,910
So this is no problem for healthy 
people.

665
00:38:00,240 --> 00:38:02,750
And um,
and we got the patients to,

666
00:38:02,770 --> 00:38:07,770
to describe this and we got age match 
than Iq match control subjects to also 

667
00:38:07,801 --> 00:38:08,910
describe scenarios.

668
00:38:09,090 --> 00:38:14,090
And what we found is that the patient 
descriptions were hugely impoverished 

669
00:38:14,090 --> 00:38:14,570
compared to their,
um,

670
00:38:14,610 --> 00:38:19,610
their control cohort.
And you can see here on the right hand 

671
00:38:19,610 --> 00:38:23,631
side,
this is a graph of the richness 

672
00:38:23,631 --> 00:38:23,631
measuring the richness of their 
descriptions.

673
00:38:23,631 --> 00:38:28,160
On the left hand bar is the patient's on
the right hand bar is the control 

674
00:38:28,411 --> 00:38:31,380
subjects who their imaginations are a 
lot richer.

675
00:38:32,220 --> 00:38:37,220
And what we found after further 
investigations is that the problem they 

676
00:38:37,220 --> 00:38:40,531
had was they couldn't bind together 
disparate elements of a scene into a 

677
00:38:40,531 --> 00:38:45,451
whole coherent hole.
So we call this spacial coherence 

678
00:38:45,451 --> 00:38:49,171
problem and that's what we think the 
hippocampus is actually doing for 

679
00:38:49,171 --> 00:38:53,131
imagination.
It's briny together all of these 

680
00:38:53,131 --> 00:38:54,620
elements into a hole.
And of course,

681
00:38:54,740 --> 00:38:57,680
you know the imagination.
What does it do for us?

682
00:38:57,800 --> 00:39:02,800
Well,
it's extremely valuable skill that 

683
00:39:02,800 --> 00:39:05,381
humans have and allows us to more 
accurately predict the future by 

684
00:39:05,381 --> 00:39:10,040
hypothesis hypothesizing about different
plans you could do and seeing how they 

685
00:39:10,040 --> 00:39:11,240
would turn out.
And also,

686
00:39:11,241 --> 00:39:16,241
I think it's the beginning of creativity
in the sense of allowing us to think of 

687
00:39:16,241 --> 00:39:20,290
counterfactual situations related,
did some brain scanning work on healthy 

688
00:39:20,290 --> 00:39:24,650
subjects imagining in brain scanners and
we found five different brain areas that

689
00:39:24,651 --> 00:39:27,890
were heavily involved in different 
aspects of imagining.

690
00:39:29,960 --> 00:39:34,700
So most recently then we've tried to 
recreate this aspect of imagination,

691
00:39:34,760 --> 00:39:39,760
so imagining scenes in our AI systems 
and we've recently had some big 

692
00:39:40,971 --> 00:39:45,680
breakthroughs on that front and we 
created a system called generative query

693
00:39:45,681 --> 00:39:49,250
network,
the Gq n and what the system was able to

694
00:39:49,251 --> 00:39:54,251
do is amazingly kind of reconstruct a 
three d model of a scene just from a 

695
00:39:54,891 --> 00:39:59,891
handful of Tuesday snapshots.
So imagine giving a the AI system a few 

696
00:40:00,530 --> 00:40:05,530
two d pictures of a seen a three d scene
and it recreates the whole three d scene

697
00:40:05,811 --> 00:40:08,780
just from those two stills to those few 
today stills.

698
00:40:10,370 --> 00:40:12,440
So then at that point to test the 
system,

699
00:40:12,650 --> 00:40:16,340
we ask the system to render the scene 
from a new angle.

700
00:40:16,490 --> 00:40:18,140
It's never,
it hasn't seen before.

701
00:40:18,500 --> 00:40:23,500
So we can,
we can ask it to render from any 

702
00:40:23,500 --> 00:40:25,511
arbitrary new angle.
So in computer graphics and in Ai 

703
00:40:27,350 --> 00:40:32,350
Circles,
this is called the inverse graphics 

704
00:40:32,350 --> 00:40:32,350
problem.
So if you imagine computer graphics,

705
00:40:32,350 --> 00:40:36,790
you know,
you have these algorithms and they 

706
00:40:36,790 --> 00:40:38,291
produce all these beautiful pictures 
that you see in games and in three d 

707
00:40:38,291 --> 00:40:42,161
artwork and CGI and there's a 
mathematical sort of equations that 

708
00:40:42,770 --> 00:40:46,000
basically cre create those three d 
scenes on.

709
00:40:46,001 --> 00:40:48,170
What this system is doing is doing the 
inverse of that,

710
00:40:48,171 --> 00:40:50,510
the reverse of that.
Here's a three d scene,

711
00:40:50,600 --> 00:40:55,600
here's some pictures of it,
now recover the genitive equations that 

712
00:40:55,600 --> 00:41:00,251
actually generate that seat.
So it's called the inverse graphics 

713
00:41:00,251 --> 00:41:04,181
problem.
Has been a longstanding problem in 

714
00:41:04,181 --> 00:41:04,181
computer graphics.

715
00:41:04,181 --> 00:41:05,110
So the scenes that we were able to do,
um,

716
00:41:05,270 --> 00:41:10,270
I should,
I should say are very simple scenes 

717
00:41:10,270 --> 00:41:10,270
currently,
but it's kind of amazing that this works

718
00:41:10,270 --> 00:41:10,940
at all.
So what I'm doing,

719
00:41:10,941 --> 00:41:12,490
I'm just going to show you a quick video
of,

720
00:41:12,510 --> 00:41:17,510
of this working while you're going to 
see is these kinds of quite toy like 

721
00:41:17,510 --> 00:41:19,090
three d scenes with three,
four,

722
00:41:19,100 --> 00:41:22,190
five objects in it,
geometric objects in it,

723
00:41:22,310 --> 00:41:22,860
like,
you know,

724
00:41:22,880 --> 00:41:26,120
spheres and hemispheres and circles and 
so on.

725
00:41:26,121 --> 00:41:31,121
And and,
and boxes of different colors and 

726
00:41:31,121 --> 00:41:33,761
different textures.
And what we do is we give the system a 

727
00:41:33,761 --> 00:41:38,201
couple of stills snapshots of the scene 
and then we tell it to render that from 

728
00:41:38,201 --> 00:41:43,061
any new angle.
So I'm going to show you that in this 

729
00:41:43,061 --> 00:41:44,570
video here.
So you'll see the scene on the left hand

730
00:41:44,571 --> 00:41:49,571
side here.
So this little box world with these 

731
00:41:49,571 --> 00:41:51,140
three objects in there and the system 
only gets two snapshots,

732
00:41:51,141 --> 00:41:56,141
view one and view two and then we ask it
to render the view from this new view,

733
00:41:57,801 --> 00:42:02,801
view three coming from another angle and
we would like to see what the image 

734
00:42:02,801 --> 00:42:04,040
looks like from that new angle.

735
00:42:06,490 --> 00:42:11,490
So we'll see here.
So it gets given view one that gets 

736
00:42:11,490 --> 00:42:14,491
input into the neural network and it 
gets processed and it gets represented 

737
00:42:14,491 --> 00:42:15,580
inside your network.

738
00:42:16,690 --> 00:42:18,820
Then we give it a new camera angle view 
too.

739
00:42:19,660 --> 00:42:21,190
So that's what it looks like from view 
to.

740
00:42:21,220 --> 00:42:24,250
We give that to the input and it adds 
that to it seemed representation.

741
00:42:24,940 --> 00:42:29,940
And then we ask it.
We queried is why it's called a janitor 

742
00:42:29,940 --> 00:42:32,230
query network.
What would it look like from this third 

743
00:42:32,230 --> 00:42:32,830
year and now a second year or network 
outputs,

744
00:42:32,890 --> 00:42:35,680
the new prediction of what that should 
look like,

745
00:42:35,860 --> 00:42:37,480
and then we compare it to the ground 
truth,

746
00:42:37,481 --> 00:42:42,481
what it really looks like and you can 
see that they almost matched perfectly 

747
00:42:42,481 --> 00:42:44,440
and then we're able to spin round.
Um,

748
00:42:44,550 --> 00:42:46,900
we're able to take care from any new 
angle,

749
00:42:47,120 --> 00:42:52,120
um,
and then we can give it a new pictures 

750
00:42:52,120 --> 00:42:52,390
of new rooms with different objects and 
you can see it can move around,

751
00:42:52,600 --> 00:42:53,620
zoom in,
zoom out.

752
00:42:53,860 --> 00:42:58,860
So it's just less if it was a computer 
game and we'd completely built a new 

753
00:42:58,860 --> 00:43:03,361
graphics engine to do that.
So it's just recovering that from these 

754
00:43:03,361 --> 00:43:07,471
two d stills.
Now obviously we're now building up to 

755
00:43:07,471 --> 00:43:11,370
scenes of higher complexity and 
eventually we would like to get to real 

756
00:43:11,370 --> 00:43:15,511
world scenes where you can recreate a 
real world seen just from some two d 

757
00:43:15,511 --> 00:43:15,511
pictures.

758
00:43:18,310 --> 00:43:21,400
So hopefully I've given you a good 
flavor of what's happening in Ai.

759
00:43:21,401 --> 00:43:23,440
The kind of cutting edge of ai at the 
moment.

760
00:43:23,860 --> 00:43:28,860
And even though I said earlier,
there are many unsolved problems too 

761
00:43:28,860 --> 00:43:32,760
that we have to still tackle even the 
kinds of technologies we have today 

762
00:43:32,760 --> 00:43:37,590
already proving very useful.
So I'm just going to briefly mention a 

763
00:43:37,590 --> 00:43:41,161
few applications.
So obviously there are a whole host of 

764
00:43:41,161 --> 00:43:42,430
commercial applications that we and 
others are looking at.

765
00:43:42,730 --> 00:43:45,730
So helping with healthcare,
medical diagnostics,

766
00:43:45,910 --> 00:43:49,900
we have a bunch of collaborations with 
hospitals around the world are all sorts

767
00:43:49,901 --> 00:43:54,901
of different areas,
especially with image recognition with 

768
00:43:54,901 --> 00:43:58,680
as work with optimization and energy.
We actually did some work for the Google

769
00:43:58,751 --> 00:44:02,980
data centers and we managed to say 40 
percent of the power the cooling systems

770
00:44:02,981 --> 00:44:07,600
used by more efficiently controlling all
the all the cooling equipment.

771
00:44:08,560 --> 00:44:13,560
I think there's lots of potential in 
education for personalized education 

772
00:44:13,560 --> 00:44:17,040
using these ai systems and also with 
virtual assistants on your phone and 

773
00:44:17,040 --> 00:44:20,671
making them a lot smarter.
So is it being used a lot in art and 

774
00:44:20,671 --> 00:44:21,800
design?
Many will.

775
00:44:21,801 --> 00:44:24,650
You know about this.
So especially in architecture,

776
00:44:25,220 --> 00:44:30,220
I believe that building the Opera House 
on the left hand side here was designed 

777
00:44:30,220 --> 00:44:34,210
using machine learning as was the engine
block on the bottom right here for car 

778
00:44:34,351 --> 00:44:39,351
engine.
And also there's be some interesting 

779
00:44:39,351 --> 00:44:39,351
things in art,
art transfer,

780
00:44:39,351 --> 00:44:44,221
transferring styles between different 
art styles on the same picture as well 

781
00:44:45,181 --> 00:44:46,970
as creating art itself on the top

782
00:44:47,010 --> 00:44:47,370
right.

783
00:44:49,040 --> 00:44:54,040
And then for me,
my particular passion is using it for 

784
00:44:54,040 --> 00:44:57,401
science to accelerate scientific 
endeavor and it's been used already 

785
00:44:57,401 --> 00:45:01,391
successfully.
These kinds of AI systems I've talked 

786
00:45:01,391 --> 00:45:03,401
about for discovering new exoplanets,
it's being used to try and control the 

787
00:45:03,401 --> 00:45:06,980
plasma and nuclear fusion reactors,
design new chemical compounds,

788
00:45:07,210 --> 00:45:12,210
um,
and detect disease and things like 

789
00:45:12,210 --> 00:45:15,041
retina scans,
so whole host of areas in both medicine 

790
00:45:15,041 --> 00:45:16,100
and science.
That I think was just the beginning of,

791
00:45:16,280 --> 00:45:19,670
I think we're going to see a huge 
revolution over the next decade.

792
00:45:21,380 --> 00:45:26,000
So just want to kind of close now by,
by just sort of going back to my initial

793
00:45:26,001 --> 00:45:29,030
statements about our mission statement 
and the way I think about that.

794
00:45:29,510 --> 00:45:34,510
So I think of ai as kind of like a Meta 
solution to a lot of the other problems 

795
00:45:34,510 --> 00:45:38,801
and challenges we have as a society.
So I think one of the big challenges we 

796
00:45:38,801 --> 00:45:41,540
face in all sorts of domains from 
science also to,

797
00:45:41,630 --> 00:45:46,630
into even things like entertainment is 
information overload and system 

798
00:45:47,001 --> 00:45:48,980
complexity.
There's just so much.

799
00:45:48,981 --> 00:45:53,981
We're kind of bombarded both in our 
personal lives and our professional 

800
00:45:53,981 --> 00:45:55,010
lives with just overwhelming amounts of 
information and data.

801
00:45:55,220 --> 00:45:57,980
So how can we make sense of all of these
data streams?

802
00:45:58,520 --> 00:45:59,950
And then the other thing is,
you know,

803
00:45:59,960 --> 00:46:04,960
as a society we want to kind of 
understand and master increasingly 

804
00:46:04,960 --> 00:46:06,950
complex systems,
some of which are boarding,

805
00:46:06,980 --> 00:46:11,930
bordering on chaotic systems.
Things like a macroeconomics climate.

806
00:46:12,110 --> 00:46:14,240
All of these areas where,
um,

807
00:46:14,330 --> 00:46:16,850
you know,
these systems are incredibly complicated

808
00:46:17,320 --> 00:46:19,100
that we would like to try and 
understand.

809
00:46:20,320 --> 00:46:25,320
And I think these are all huge 
challenges that we have without 

810
00:46:25,320 --> 00:46:29,151
something like ai helping us.
And for a long while sort of the,

811
00:46:29,341 --> 00:46:32,580
I guess the early two thousands,
the first decade of this century,

812
00:46:33,000 --> 00:46:38,000
big data was this huge buzzword.
And I think in a way big data is the 

813
00:46:38,000 --> 00:46:42,591
problem.
You can think of an ai as the answer 

814
00:46:42,591 --> 00:46:45,240
because everyone has got tons of data 
now or companies do and we all have tons

815
00:46:45,241 --> 00:46:47,820
of data,
but what you do with all of that data,

816
00:46:47,821 --> 00:46:52,821
how do you make sense of it?
And I think the only way to do that 

817
00:46:52,821 --> 00:46:55,330
actually at scale is to use ai and on 
that level,

818
00:46:55,420 --> 00:47:00,420
you know,
in a very general way you can think of 

819
00:47:00,420 --> 00:47:02,401
intelligence as a kind of process,
almost a magical process in some ways 

820
00:47:02,401 --> 00:47:06,960
that converts unstructured information 
or data into useful actionable 

821
00:47:06,960 --> 00:47:10,741
knowledge.
That's what intelligence I think is 

822
00:47:10,741 --> 00:47:13,320
fundamentally.
And Ai is a kind of way of automating 

823
00:47:13,320 --> 00:47:16,411
that process.
And as I've mentioned my personal dream 

824
00:47:16,411 --> 00:47:19,860
and why I spent my whole career working 
on ai is to use and build it as a 

825
00:47:19,860 --> 00:47:23,821
powerful tool to help the scientists and
experts and clinicians accelerate 

826
00:47:25,810 --> 00:47:28,180
desperately sort of needed scientific 
breakthroughs.

827
00:47:29,460 --> 00:47:29,950
Yeah.

828
00:47:30,220 --> 00:47:35,220
So I think it's an incredibly sort of 
exciting time and air holds incredible 

829
00:47:35,531 --> 00:47:39,760
promise for the future,
but you know,

830
00:47:39,761 --> 00:47:44,761
it must be used responsibly and safely 
just like any other powerful technology 

831
00:47:45,280 --> 00:47:48,640
and we have to ensure that it's used for
the benefit of everyone and the benefits

832
00:47:48,641 --> 00:47:49,840
accrue to everyone.

833
00:47:51,290 --> 00:47:53,350
You know,
I think of ai in it,

834
00:47:53,360 --> 00:47:58,360
in of itself.
It's an inherently neutral technology 

835
00:47:58,360 --> 00:48:01,490
and just like with every,
any powerful technology depends how we 

836
00:48:01,490 --> 00:48:02,370
decide as a society to deploy it and use
it.

837
00:48:03,170 --> 00:48:08,170
And on this topic,
I think a lot more research and 

838
00:48:08,170 --> 00:48:08,730
discussions needed with a wide set of 
stakeholders.

839
00:48:09,390 --> 00:48:10,310
And as I said,
as wise,

840
00:48:10,311 --> 00:48:15,311
I think it's very important to have 
dialogue like this between scientists 

841
00:48:15,311 --> 00:48:17,370
and technologists and artists and and 
the social sciences.

842
00:48:17,560 --> 00:48:22,560
And I think that's gonna be critical if 
we're going to get this right for 

843
00:48:22,560 --> 00:48:25,161
everyone.
And we've started ourselves several 

844
00:48:25,161 --> 00:48:27,450
efforts both internally at deep mind.
We have an ethics and society group with

845
00:48:27,451 --> 00:48:30,330
policy thinkers and philosophers and 
ethicists,

846
00:48:30,570 --> 00:48:32,970
and we've also been instrumental in co 
founding,

847
00:48:33,510 --> 00:48:36,900
a pan industry group called the 
partnership on Ai,

848
00:48:37,050 --> 00:48:42,050
which includes nonprofits and academics 
as well as the big companies trying to 

849
00:48:42,050 --> 00:48:44,190
think about these topics for the benefit
of everyone in society.

850
00:48:46,650 --> 00:48:48,970
So I just want to end this talk by 
thinking a little bit.

851
00:48:48,971 --> 00:48:52,780
We'll philosophy philosophically and for
me as a neuroscientist,

852
00:48:52,900 --> 00:48:57,900
one or the other really interesting 
things about this journey we're on is 

853
00:48:57,900 --> 00:49:00,961
that I believe that by trying to distill
intelligence into an algorithmic 

854
00:49:00,961 --> 00:49:05,011
constructs like we're doing with ai and 
then if we use that and compare that to 

855
00:49:05,011 --> 00:49:09,180
the human brain,
I think that might help us better 

856
00:49:09,180 --> 00:49:09,180
understand what's unique about our own 
minds,

857
00:49:09,180 --> 00:49:14,130
including profound mysteries like the 
nature of creativity that we've been 

858
00:49:14,130 --> 00:49:16,180
discussing,
what dreams are and perhaps even the big

859
00:49:16,181 --> 00:49:19,090
questions like consciousness.
And uh,

860
00:49:19,150 --> 00:49:23,440
as Richard Feynman said,
is one of my all time scientific heroes.

861
00:49:23,650 --> 00:49:26,440
What I cannot create,
I do not truly understand.

862
00:49:26,650 --> 00:49:29,440
And I think about that,
about intelligence.

863
00:49:29,860 --> 00:49:34,860
And I just want to finish and give the 
last word to find men actually and a 

864
00:49:34,860 --> 00:49:37,780
passage from one of his books that 
really inspired me when I was a child to

865
00:49:37,781 --> 00:49:42,781
think about science and arts and this is
the way I feel and it sort of echoes my 

866
00:49:42,781 --> 00:49:42,820
views on the topic.

867
00:49:43,390 --> 00:49:45,850
And he said,
a fireman said,

868
00:49:45,890 --> 00:49:50,890
well though I may not be quite as 
refined aesthetically as my artist 

869
00:49:50,890 --> 00:49:53,791
friend is,
he was walking through a meadow with is 

870
00:49:53,791 --> 00:49:57,420
a good friend of his who was an artist 
and they were looking at a flower and 

871
00:49:57,420 --> 00:49:59,320
they were discussing this and he said,
I can appreciate the beauty of a flower.

872
00:50:00,010 --> 00:50:02,890
At the same time,
I can see much more about the flower.

873
00:50:03,340 --> 00:50:06,250
I could imagine the cells in there,
the complicated action inside,

874
00:50:06,251 --> 00:50:11,251
which also have a kind of beauty.
The fact that the colors in the flower 

875
00:50:11,251 --> 00:50:15,190
evolved in order to attract insects to 
pollinate isn't is very interesting.

876
00:50:15,550 --> 00:50:17,860
It means that these insects can see the 
color.

877
00:50:18,820 --> 00:50:23,820
All kinds of interesting questions with 
the science knowledge only adds to the 

878
00:50:23,820 --> 00:50:24,970
excitement,
the mystery and the all of a flower.

879
00:50:25,570 --> 00:50:28,670
And I think is really right about that 
and that's why I love by science.

880
00:50:28,671 --> 00:50:29,750
I cannot thank you.

881
00:50:31,780 --> 00:50:36,780
Thank you.

882
00:50:43,860 --> 00:50:47,000
Dummies is an academy here waiting to,
um,

883
00:50:47,580 --> 00:50:49,710
not get at you,
but I'll ask you some questions,

884
00:50:49,711 --> 00:50:52,260
but I just want to pick up on a couple 
of points.

885
00:50:53,070 --> 00:50:57,660
Your background was in gaming.
It was competitive.

886
00:50:57,690 --> 00:51:00,240
You've come here,
I think in the spirit of collegiality,

887
00:51:00,241 --> 00:51:01,680
of openness,
of a dialogue.

888
00:51:02,670 --> 00:51:04,860
It was interesting in the,
in the go game,

889
00:51:05,260 --> 00:51:09,300
Lisa Dol said that he felt he was there 
defending human intelligence.

890
00:51:09,720 --> 00:51:14,400
And last,
I think we've gone past the stage,

891
00:51:14,401 --> 00:51:17,160
at least I hope we have the arts and 
science are pitted against each other.

892
00:51:17,610 --> 00:51:21,660
But do you think that element of 
competitiveness that humans,

893
00:51:21,661 --> 00:51:26,661
inevitable competitiveness is still 
essential in developing what it is 

894
00:51:27,901 --> 00:51:32,901
you're trying to develop an establishing
relationships and knowledge between ai 

895
00:51:32,901 --> 00:51:32,901
and human creativity?

896
00:51:33,150 --> 00:51:33,520
Yeah,
I mean,

897
00:51:33,600 --> 00:51:34,050
look,
it's,

898
00:51:34,140 --> 00:51:39,140
it's an interesting thing because 
competitiveness is obviously when you 

899
00:51:39,140 --> 00:51:39,580
positively and constructively is,
is,

900
00:51:39,780 --> 00:51:42,840
can be a very powerful driving force in 
a very good one for progress.

901
00:51:43,950 --> 00:51:48,950
In response to what Lisa Dole said,
I can understand why he felt like that 

902
00:51:48,950 --> 00:51:53,391
because he was representing the Ngo 
world and it was quite surprising for 

903
00:51:53,391 --> 00:51:53,490
him.
You know,

904
00:51:53,491 --> 00:51:56,550
he's definitely at least a decade before
he was expecting that to happen.

905
00:51:56,940 --> 00:52:01,230
But one thing you gotta remember is that
of course Alphago is a human endeavor to

906
00:52:01,440 --> 00:52:06,360
and there are all sorts of amazing 
programs and researchers on the team who

907
00:52:06,370 --> 00:52:11,370
were,
who spent their whole lives building up 

908
00:52:11,370 --> 00:52:13,641
their skills in the way Lisa Dole had in
his art to be able to program something 

909
00:52:13,651 --> 00:52:18,180
that the architecture behind Alphago,
which then went on to learn for itself,

910
00:52:18,181 --> 00:52:20,340
but it,
of course all the initial conditions was

911
00:52:20,341 --> 00:52:22,710
created by by human scientists.

912
00:52:22,950 --> 00:52:27,950
So I think the whole thing,
and if you see the film as a 

913
00:52:27,950 --> 00:52:31,311
celebration,
I think of the spirit of human endeavor 

914
00:52:31,311 --> 00:52:31,311
from all sides,
from the goplayers,

915
00:52:31,380 --> 00:52:34,560
the programmers,
everyone kind of collaborating together,

916
00:52:34,561 --> 00:52:39,561
including actually the journalists and 
writers who were writing about the 

917
00:52:40,530 --> 00:52:41,400
match.
In fact,

918
00:52:41,401 --> 00:52:46,401
that might want to.
Some of my favorite pieces of writing 

919
00:52:46,401 --> 00:52:46,401
were done by a wire journalist who was 
writing.

920
00:52:46,401 --> 00:52:49,110
I thought very poetically about the 
whole mash as he was watching it live,

921
00:52:49,410 --> 00:52:54,410
so I think it's actually a wonderful 
celebration of human ingenuity all 

922
00:52:54,410 --> 00:52:57,630
around and and I think you know,
after the match,

923
00:52:57,631 --> 00:52:58,830
since then,
if you taught him,

924
00:52:59,090 --> 00:53:04,090
he ends,
he's had time to reflect on that and I 

925
00:53:04,090 --> 00:53:04,090
think it's been amazing for the go.
Well they've.

926
00:53:04,090 --> 00:53:07,830
They've unleashed their own creativity 
because not only are they playing what's

927
00:53:07,831 --> 00:53:10,140
called like Alphago,
like moves also,

928
00:53:10,141 --> 00:53:15,141
many of the top go players I've spoken 
to has said was felt that there is free 

929
00:53:15,141 --> 00:53:16,140
their minds from the shackles of 
tradition,

930
00:53:16,320 --> 00:53:19,890
so they're all trying to think the 
unthinkable now and they've come up with

931
00:53:19,891 --> 00:53:24,891
their own brilliant new ideas that in 
the thousands of years and hundreds of 

932
00:53:24,891 --> 00:53:26,940
years past,
they have been told as as junior go,

933
00:53:26,941 --> 00:53:31,941
players not to not to do and sort of be 
told off for it and now they're able to 

934
00:53:31,941 --> 00:53:33,480
explore their own creativity

935
00:53:33,930 --> 00:53:37,080
without peddling a stereotype,
which is always a precluding to peddling

936
00:53:37,081 --> 00:53:41,580
a stereotype.
The world loves the notion of randomness

937
00:53:41,581 --> 00:53:46,581
and chance.
He wants to harness it and possibly for 

938
00:53:46,581 --> 00:53:46,581
many artists here,
certainly for me,

939
00:53:46,581 --> 00:53:51,141
and I'm not an artist.
That moment where the commentator 

940
00:53:51,141 --> 00:53:52,920
thought that a mistake had been made 
becomes really interesting.

941
00:53:53,280 --> 00:53:58,280
Less the resolution you saw the beauty,
but more the fact that Beckett's idea of

942
00:53:58,531 --> 00:54:02,250
failing or failing,
Metta lies at the heart of many people's

943
00:54:02,251 --> 00:54:07,251
creative vision,
see it as impossible pursuit of 

944
00:54:07,251 --> 00:54:09,710
perfection.
Whereas certain scientists theaters a 

945
00:54:09,710 --> 00:54:09,710
potential pursuit of perfection.
Again,

946
00:54:09,710 --> 00:54:14,591
that's a stereotyping.
Our predicates itself in certain areas 

947
00:54:14,591 --> 00:54:15,450
of having no rules of wanting to break 
the rules,

948
00:54:15,451 --> 00:54:20,451
that almost becomes a tedious job,
but actually at its best it offers 

949
00:54:20,451 --> 00:54:21,840
endless possibilities.
At its worst.

950
00:54:21,841 --> 00:54:26,010
It's an anoxic void of meaninglessness.
And how does that play into your pursuit

951
00:54:26,011 --> 00:54:27,350
of understanding human created?

952
00:54:27,450 --> 00:54:32,450
Well,
I think that's what I was talking about 

953
00:54:32,450 --> 00:54:32,450
or trying to talk about where the types 
of creativity.

954
00:54:32,450 --> 00:54:35,080
So I think the breaking of all the rules
and breaking outside of,

955
00:54:35,170 --> 00:54:37,060
you know,
going beyond what the rules allow you to

956
00:54:37,061 --> 00:54:39,400
do.
That for me would be true invention,

957
00:54:39,760 --> 00:54:44,760
which I was kind of having as the yellow
door outside the box and I think our 

958
00:54:44,760 --> 00:54:46,180
systems currently are not capable of 
that,

959
00:54:46,420 --> 00:54:48,640
right?
They're capable of of being creative,

960
00:54:48,641 --> 00:54:53,641
but within the rules so to speak,
which is what I was sort of meaning by 

961
00:54:53,641 --> 00:54:57,121
extrapolation,
so here's the rules of go come up with 

962
00:54:57,121 --> 00:55:00,301
some new motifs of new strategies and 
new tactics and new theories and it was 

963
00:55:00,301 --> 00:55:02,230
able to do that that we're genuinely 
new,

964
00:55:02,440 --> 00:55:07,440
so I think that is a genuine form of 
creativity but not the highest level of 

965
00:55:07,440 --> 00:55:11,461
creativity,
which would be something like coming up 

966
00:55:11,461 --> 00:55:11,461
with go in the first place.

967
00:55:11,590 --> 00:55:16,590
I think there are many people in this 
room who would love ai to take over the 

968
00:55:16,590 --> 00:55:19,870
role of the critic and I think the 
notion of criticism,

969
00:55:19,871 --> 00:55:22,180
how we judge things,
human taste,

970
00:55:22,181 --> 00:55:26,350
how we decide that something is more 
interesting or better than another is an

971
00:55:26,351 --> 00:55:28,060
inexact science.
It can be a poetry,

972
00:55:28,061 --> 00:55:31,690
but it's in an exact science.
How does that play into your thinking?

973
00:55:32,670 --> 00:55:37,670
What I think some aspects of aesthetic 
judgment could potentially be learned by

974
00:55:38,221 --> 00:55:40,440
these systems.
You've given enough training data,

975
00:55:40,441 --> 00:55:45,441
you know,
maybe there was some amazing art critic 

976
00:55:45,441 --> 00:55:47,931
or restaurant critic that you wanted to 
kind of mimic the judgment of and maybe 

977
00:55:49,231 --> 00:55:52,500
given enough data.
Some of those aspects could be judged,

978
00:55:52,530 --> 00:55:57,530
could be sort of mimicked in some way,
but I think it was still go beyond that 

979
00:55:57,530 --> 00:55:59,520
because when I went to an art critics 
judging arts,

980
00:55:59,700 --> 00:56:04,700
one of the things that I,
I regard about human created our why 

981
00:56:04,700 --> 00:56:08,041
she's.
Why I think it's higher than machine 

982
00:56:08,041 --> 00:56:09,831
created are,
is the part from the technicalities of 

983
00:56:09,831 --> 00:56:12,771
it is that there's always the imprint of
the artists through their artwork and I 

984
00:56:12,901 --> 00:56:17,901
think some of the soul of the artist 
come through that art and that's what 

985
00:56:17,901 --> 00:56:20,820
we're appreciating as human viewers of 
the art and where perhaps the art critic

986
00:56:20,821 --> 00:56:25,821
too.
So I always think of someone like Van 

987
00:56:25,821 --> 00:56:25,821
Gough,
his,

988
00:56:25,821 --> 00:56:29,240
this sort of the tortured nature of his 
soul comes for almost every brush stroke

989
00:56:29,260 --> 00:56:30,160
he,
he has.

990
00:56:30,161 --> 00:56:32,710
And that's one of the reasons why his 
art so incredible.

991
00:56:32,830 --> 00:56:37,830
And I think it wouldn't be the same even
if a machine could match it technically,

992
00:56:38,380 --> 00:56:43,380
which obviously is a big f anyway in of 
itself because I think part of what's 

993
00:56:43,380 --> 00:56:46,610
great about artists,
the is the is the sort of the,

994
00:56:46,620 --> 00:56:47,870
the,
the imprint of what they,

995
00:56:47,920 --> 00:56:50,550
of what the artist has experienced in 
creating the art.

996
00:56:50,850 --> 00:56:55,850
Well,
I mean there are many things that can 

997
00:56:55,850 --> 00:56:59,031
affirm,
but one of them is the affirmation that 

998
00:56:59,031 --> 00:56:59,031
I'm here,
I'm alive.

999
00:56:59,031 --> 00:56:59,031
What it is to be human wrestling with 
the human condition.

1000
00:56:59,280 --> 00:57:04,280
Presumably a machine can't do that.
But presumably you're arguing that at 

1001
00:57:04,280 --> 00:57:04,280
some stage in the not too distant 
future,

1002
00:57:04,280 --> 00:57:06,030
it might be able to have a semblance of 
that.

1003
00:57:06,090 --> 00:57:07,970
Or is that nonsense?
What's the timeframe?

1004
00:57:07,980 --> 00:57:09,630
I mean months,
years.

1005
00:57:09,930 --> 00:57:14,930
You'll obviously say not,
but is there a sense that we're looking 

1006
00:57:14,930 --> 00:57:16,680
at something approaching this in the 
next decade?

1007
00:57:16,760 --> 00:57:17,570
No,
I think,

1008
00:57:17,670 --> 00:57:19,820
I mean aspects of it in the next 
decades.

1009
00:57:19,880 --> 00:57:24,880
But I mean,
I think this is what I mentioned at the 

1010
00:57:24,880 --> 00:57:27,461
end really about what fascinates me is 
both a neuroscientist and a computer 

1011
00:57:27,461 --> 00:57:28,210
scientist is that,
you know,

1012
00:57:28,510 --> 00:57:31,300
what are these aspects of the brain 
that,

1013
00:57:31,710 --> 00:57:35,420
that mechanism cannot be done 
computationally.

1014
00:57:35,540 --> 00:57:40,540
Are there any and if they are,
what are they and what mechanisms do 

1015
00:57:40,540 --> 00:57:42,320
they use can be explained or is this 
something mysterious?

1016
00:57:42,470 --> 00:57:47,180
And I'm quite open minded about that and
I think what I see is part of what we're

1017
00:57:47,181 --> 00:57:49,550
doing,
which is this neuroscience inspired ai,

1018
00:57:49,820 --> 00:57:54,820
is let's see where that takes us.
And then we'll see which aspects remain 

1019
00:57:55,010 --> 00:57:57,520
that only the human brain can do.
And you know,

1020
00:57:57,521 --> 00:58:01,520
I think about that for creativity.
I think about that for a dreams.

1021
00:58:01,521 --> 00:58:04,940
I think about that for consciousness.
We don't know what these things are,

1022
00:58:05,480 --> 00:58:06,650
the nature of consciousness.

1023
00:58:06,800 --> 00:58:09,660
We don't know how they manifest 
themselves in,

1024
00:58:09,800 --> 00:58:11,570
in,
in the physics of our brain.

1025
00:58:11,600 --> 00:58:13,280
There are theories,
but we don't know.

1026
00:58:13,490 --> 00:58:16,590
And I think this may be one way of,
uh,

1027
00:58:16,620 --> 00:58:17,290
of,
of,

1028
00:58:17,320 --> 00:58:20,360
of,
of examining that is I'm trying to build

1029
00:58:20,361 --> 00:58:24,170
aspects of intelligence and then seeing 
what was missing.

1030
00:58:24,171 --> 00:58:25,690
And some of those things,
you know,

1031
00:58:25,700 --> 00:58:27,380
maybe impossible,
although for,

1032
00:58:27,440 --> 00:58:28,700
for the moment,
um,

1033
00:58:28,970 --> 00:58:33,970
you know,
at least from a biological point of 

1034
00:58:33,970 --> 00:58:35,711
view,
that doesn't seem to be anything long 

1035
00:58:35,711 --> 00:58:36,620
computable in the brain,
although there's speculation about that.

1036
00:58:36,621 --> 00:58:41,621
There's a famous mathematician called 
Roger Penrose who talks about quantum 

1037
00:58:41,621 --> 00:58:43,910
consciousness and he thinks there are 
quantum effects in the brain,

1038
00:58:44,120 --> 00:58:49,120
in which case if he's right,
then we will not be able to model those 

1039
00:58:50,901 --> 00:58:53,870
on a conventional computer.
Traditional classical computer.

1040
00:58:54,710 --> 00:58:59,710
But so far,
and biologists have looked for this 

1041
00:58:59,710 --> 00:58:59,710
quite hard there,
that they,

1042
00:58:59,710 --> 00:59:01,550
no one's found any quantum effects in 
the brain so far.

1043
00:59:01,970 --> 00:59:04,280
I love the idea that the computer would 
not,

1044
00:59:04,370 --> 00:59:05,900
it would,
we'd have to do,

1045
00:59:05,901 --> 00:59:07,550
is learn to invent,
reinvent,

1046
00:59:07,640 --> 00:59:10,010
go itself May.
Maybe in the end it will reinvent.

1047
00:59:10,430 --> 00:59:13,700
But are one of these art does,
is constantly reinvents itself.

1048
00:59:14,390 --> 00:59:17,330
You've already given us about six 
potential lectures.

1049
00:59:18,170 --> 00:59:23,170
I'm not so cheeky or opportunistic to 
ask you to come back and give series 

1050
00:59:23,170 --> 00:59:23,240
here,
but you should do really.

1051
00:59:23,780 --> 00:59:28,780
But I'm also conscious that there are 
people here in the brief time we have 

1052
00:59:28,780 --> 00:59:32,081
left now,
you will have questions to make 

1053
00:59:32,081 --> 00:59:33,210
questions to ask of you and I do think 
there should be another forum to,

1054
00:59:33,280 --> 00:59:35,330
to,
to look through the implications of much

1055
00:59:35,331 --> 00:59:38,540
of what you said could,
could ask people to ask questions rather

1056
00:59:38,541 --> 00:59:43,541
than make long statements.
I know that's difficult because there's 

1057
00:59:43,541 --> 00:59:43,541
so much that's been thrown out,
but I'd love to take some questions from

1058
00:59:43,541 --> 00:59:43,640
the floor.

1059
00:59:44,060 --> 00:59:46,460
Could you wait for the mic?
There's a hand up at the back there.

1060
00:59:46,520 --> 00:59:47,810
Thank you.
Hi.

1061
00:59:47,870 --> 00:59:49,910
Thank you.
A great lecture.

1062
00:59:50,240 --> 00:59:52,670
And following on from what you were just
discussing,

1063
00:59:52,970 --> 00:59:57,970
uh,
you said at the beginning that's a 

1064
00:59:57,970 --> 01:00:00,490
reinforcement learning.
We know that that model can lead to a 

1065
01:00:00,490 --> 01:00:02,150
general intelligence.
And so I was wondering if,

1066
01:00:02,240 --> 01:00:02,830
if,
uh,

1067
01:00:03,080 --> 01:00:04,390
in order to,
uh,

1068
01:00:04,580 --> 01:00:09,580
to get the general intelligence and the 
higher level of creativity like 

1069
01:00:09,580 --> 01:00:11,660
invention,
do you think we need to,

1070
01:00:12,130 --> 01:00:17,130
to work out consciousness and 
intentionality and do you agree with 

1071
01:00:17,130 --> 01:00:19,140
philosophers like John Self new say,
um,

1072
01:00:19,640 --> 01:00:20,690
that,
uh,

1073
01:00:20,720 --> 01:00:22,150
we need to understand the,
the,

1074
01:00:22,210 --> 01:00:25,580
the physical material of the brain 
rather than just the algorithm?

1075
01:00:27,080 --> 01:00:27,340
No,
I.

1076
01:00:27,341 --> 01:00:32,341
So I disagree with John Cell and um,
I do think that you can make progress on

1077
01:00:34,231 --> 01:00:37,570
this question without fully 
understanding the substrate and fat.

1078
01:00:37,640 --> 01:00:42,640
I,
I believe that intelligence will be 

1079
01:00:42,640 --> 01:00:45,921
substrate independent in the sense that 
we are forced to learning is the way 

1080
01:00:45,921 --> 01:00:49,491
we're going to try and build it.
But there are probably other ways of 

1081
01:00:49,491 --> 01:00:49,491
building intelligence.
They're more mathematical,

1082
01:00:49,491 --> 01:00:52,200
less neuroscience based,
and even the neuroscience based way like

1083
01:00:52,201 --> 01:00:53,220
we're doing,
you know,

1084
01:00:53,221 --> 01:00:54,870
we're really looking at the systems 
level,

1085
01:00:54,930 --> 01:00:58,380
the algorithmic level,
not at the actual wetware itself,

1086
01:00:58,381 --> 01:01:01,230
the exact way that neurons work and 
cortical columns.

1087
01:01:01,290 --> 01:01:06,290
Other people are doing that.
That's sometimes called whole brain 

1088
01:01:06,290 --> 01:01:09,111
emulation where you're effectively 
trying to reverse engineer the brain's 

1089
01:01:09,111 --> 01:01:10,230
precisely and implement it in the same 
way the brain does.

1090
01:01:10,500 --> 01:01:14,880
And I don't believe that will be 
necessary for intelligence as to whether

1091
01:01:14,881 --> 01:01:18,240
we'll need consciousness for true 
creativity and other things.

1092
01:01:18,540 --> 01:01:20,400
I'm not sure I,
I,

1093
01:01:20,460 --> 01:01:21,490
if I was,
you know,

1094
01:01:21,630 --> 01:01:26,370
I think there's a open scientific 
question and we need to get further with

1095
01:01:26,371 --> 01:01:31,371
the research to understand that.
But I would say that if I was to bet on 

1096
01:01:31,371 --> 01:01:31,371
it,
um,

1097
01:01:31,380 --> 01:01:36,380
I think it's likely that intelligence 
and consciousness are what's called 

1098
01:01:36,380 --> 01:01:40,731
double dissociable.
So I think you'll be able to have 

1099
01:01:40,731 --> 01:01:43,340
intelligent systems that will,
that we fantastically intelligent in 

1100
01:01:43,340 --> 01:01:46,821
terms of the capability but will not 
feel conscious in any way in the way 

1101
01:01:46,821 --> 01:01:46,821
that I do to you.
You do.

1102
01:01:46,821 --> 01:01:48,540
To me.
And I also think on the other end of the

1103
01:01:48,541 --> 01:01:50,280
spectrum,
if you look at animals,

1104
01:01:50,281 --> 01:01:52,110
for example,
like our pets,

1105
01:01:52,111 --> 01:01:57,111
like dogs and cats and so on,
I think it's pretty clear they have 

1106
01:01:57,111 --> 01:01:57,660
fought some form of consciousness.
You see them dreaming and,

1107
01:01:57,900 --> 01:02:02,900
and they seem to have those kinds of 
traits or self awareness and other 

1108
01:02:02,900 --> 01:02:04,920
things,
but obviously they're not close to human

1109
01:02:04,921 --> 01:02:09,921
level intelligence.
So it seems as though maybe the 

1110
01:02:09,921 --> 01:02:12,410
dissociable traits,
but um,

1111
01:02:12,510 --> 01:02:13,280
you know,
who,

1112
01:02:13,350 --> 01:02:15,180
who knows,
maybe we'll get in 20 years time,

1113
01:02:15,181 --> 01:02:20,181
we'll get to a point where we are okay,
we're sort of stuck against the brick 

1114
01:02:20,181 --> 01:02:23,481
wall and actually the reason we can't 
have more intelligent systems is we now 

1115
01:02:23,481 --> 01:02:23,940
understand what this consciousness thing
is.

1116
01:02:24,350 --> 01:02:26,870
Thank you for questions.
Beautifully balanced systematically.

1117
01:02:26,890 --> 01:02:27,080
Yeah.

1118
01:02:27,490 --> 01:02:28,280
Gentlemen that and then.

1119
01:02:32,390 --> 01:02:33,680
Hi,
thanks for the great lecture.

1120
01:02:33,770 --> 01:02:38,770
Um,
do you think that the current speed of 

1121
01:02:38,770 --> 01:02:40,991
ai research has had a negative impact on
its practices in the field and if so,

1122
01:02:43,940 --> 01:02:45,110
what do you think can be done about it

1123
01:02:47,430 --> 01:02:51,500
tag mean negative on,
on its own in terms of its applications?

1124
01:02:55,220 --> 01:02:55,640
Yeah.
No,

1125
01:02:55,641 --> 01:02:58,070
I don't think so.
I think it's mostly been positive.

1126
01:02:58,071 --> 01:02:59,570
I would say,
um,

1127
01:02:59,960 --> 01:03:02,900
I think like with any,
any hot topic and that's,

1128
01:03:03,050 --> 01:03:07,430
I think it's a bit too hyped and I think
that's caused a lot of um,

1129
01:03:07,640 --> 01:03:09,860
you know,
the sorts of bad cycles you get when,

1130
01:03:10,040 --> 01:03:10,340
when,
uh,

1131
01:03:10,370 --> 01:03:15,370
when uh,
an area gets to the top of the hype 

1132
01:03:15,370 --> 01:03:15,370
cycle.
So I think there's been a lot of amazing

1133
01:03:15,370 --> 01:03:19,211
work that's happened,
but some of the promises are over 

1134
01:03:19,211 --> 01:03:19,400
promising still compared to where we 
are.

1135
01:03:19,800 --> 01:03:24,800
Um,
and I think that sometimes can lead to 

1136
01:03:24,800 --> 01:03:25,770
some bad silence this rushed or in some 
way,

1137
01:03:25,890 --> 01:03:27,990
but I think mostly the community is 
actually very good.

1138
01:03:27,991 --> 01:03:30,960
The research community around Ai and 
it's very open.

1139
01:03:31,680 --> 01:03:36,680
Everyone publishes everything and I 
think it's pretty collegiate at the 

1140
01:03:36,680 --> 01:03:36,680
moment.

1141
01:03:36,680 --> 01:03:38,340
So I would say the research communities 
actually pretty solid.

1142
01:03:38,610 --> 01:03:43,610
And I actually think in order for us to 
get to better practice best practices 

1143
01:03:43,610 --> 01:03:46,440
and protocols that say around how these 
systems are deployed,

1144
01:03:46,710 --> 01:03:49,230
I actually think we need to get further 
with the systems.

1145
01:03:49,320 --> 01:03:54,320
So we have concrete systems to 
experiment on and actually figure out 

1146
01:03:54,320 --> 01:03:58,160
because compete science isn't,
it's not theoretical subjects in 

1147
01:03:58,160 --> 01:03:59,250
engineering discipline.
So in order for us to make progress with

1148
01:03:59,251 --> 01:04:04,251
that,
I think we have to have systems that we 

1149
01:04:04,251 --> 01:04:06,381
can actually test empirically test and I
think all the best science is done with 

1150
01:04:06,381 --> 01:04:10,641
empirical work in tandem with 
theoretical work and for us the 

1151
01:04:10,641 --> 01:04:12,120
empirical work is engineering.
Yes.

1152
01:04:20,750 --> 01:04:25,750
Thank you for the lecture.
I think I need an ai to help me process 

1153
01:04:25,750 --> 01:04:26,650
all the information you've just given us
in the last hour.

1154
01:04:27,500 --> 01:04:30,620
I'd like to take you back to your art 
and science comment at the beginning.

1155
01:04:30,621 --> 01:04:35,621
So have you looked at using Gq n two 
instead of trying to recreate three 

1156
01:04:36,561 --> 01:04:41,561
dimensional computer graphics,
potentially recreate architecture or 

1157
01:04:41,561 --> 01:04:46,360
environments that live longer exist,
whether it's because of war or just you 

1158
01:04:46,431 --> 01:04:47,840
know,
dilapidation there's,

1159
01:04:48,170 --> 01:04:53,170
there's a lot of amazing art in the 
world architecture that has been lost 

1160
01:04:53,180 --> 01:04:56,900
and there are a lot of paintings and 
photographic representations of that.

1161
01:04:56,901 --> 01:05:01,901
And is it something you guys had looked 
at in terms of trying to help us 

1162
01:05:01,901 --> 01:05:01,901
recapture some of that?
Yeah,

1163
01:05:01,901 --> 01:05:06,890
we have started to look at that.
So as I mentioned where we gq and we're 

1164
01:05:06,890 --> 01:05:11,031
now trying to build up to more complex 
scenes in and eventually we're world 

1165
01:05:11,031 --> 01:05:15,141
architecture would be a very interesting
to try and like a room or in a 

1166
01:05:15,220 --> 01:05:16,460
dilapidated room in a,
in a,

1167
01:05:16,461 --> 01:05:16,990
in a,
in,

1168
01:05:17,090 --> 01:05:18,460
in a,
in a structure.

1169
01:05:18,850 --> 01:05:22,480
Another area that's been worked on a lot
is what's called generative models,

1170
01:05:22,510 --> 01:05:27,510
which gun is an example where they're 
trying to fill in pictures or even drop 

1171
01:05:27,510 --> 01:05:29,610
photos and things where they try to,
um,

1172
01:05:30,100 --> 01:05:31,630
you know,
you can leave a missing part and it will

1173
01:05:31,631 --> 01:05:34,570
fill it in and they're not photo 
realistic yet.

1174
01:05:34,571 --> 01:05:37,270
They're not as good as the originals.
You know,

1175
01:05:37,271 --> 01:05:41,350
you would obviously spot it immediately 
as a generated by computer,

1176
01:05:41,500 --> 01:05:46,500
but they're getting better all the time.
And one of the issues is with 

1177
01:05:46,500 --> 01:05:50,470
architectures are anything more complex 
than our simple scenes is the system 

1178
01:05:50,470 --> 01:05:53,110
still don't really understand the 
semantics of a scene.

1179
01:05:53,350 --> 01:05:58,350
So they don't really understand that 
these objects are separate and what 

1180
01:05:58,350 --> 01:05:59,060
background is and for ground and so on.
And,

1181
01:05:59,120 --> 01:06:01,780
and how physics interacts with 
structures.

1182
01:06:02,020 --> 01:06:04,390
And that's the concept part I was 
talking about.

1183
01:06:04,630 --> 01:06:09,630
And I think systems like gun,
if they had abstractions and concepts 

1184
01:06:09,701 --> 01:06:14,701
would start being able to pass the late 
the world up into semantic meaning and 

1185
01:06:14,701 --> 01:06:15,970
structure,
which then allow them to model much more

1186
01:06:15,971 --> 01:06:19,120
complicated sentence.
So I think that's what's holding us back

1187
01:06:19,121 --> 01:06:20,890
right now.
But eventually I would expect to be able

1188
01:06:20,891 --> 01:06:21,880
to do those kinds of things

1189
01:06:22,000 --> 01:06:27,000
we should say the US was always harness 
technology and recently of painting was 

1190
01:06:27,000 --> 01:06:27,910
made by Algorithms.
And all I can say is,

1191
01:06:28,720 --> 01:06:30,460
well it might've made it into the summer
exhibition.

1192
01:06:30,580 --> 01:06:34,060
Yeah.
Thank you very much for that.

1193
01:06:34,120 --> 01:06:36,610
I just wanted to ask you a question 
about explainability.

1194
01:06:37,030 --> 01:06:39,880
So you mentioned about the move the 
Alphago made.

1195
01:06:39,881 --> 01:06:43,000
That was after the fact that go experts 
could say,

1196
01:06:43,040 --> 01:06:44,800
oh,
we know why it did that,

1197
01:06:45,520 --> 01:06:50,520
but you could also probably imagine 
situations where it would be harder to 

1198
01:06:50,520 --> 01:06:52,090
understand why a machine had made a 
decision that it did.

1199
01:06:52,530 --> 01:06:57,530
Um,
do you think it's important to build 

1200
01:06:57,530 --> 01:06:57,530
systems that are able to explain 
themselves?

1201
01:06:57,530 --> 01:07:01,440
Or do you think it's natural that we're 
going to kind of decouple away from 

1202
01:07:01,440 --> 01:07:01,660
machines and will kind of lose a bit of 
that agency?

1203
01:07:02,200 --> 01:07:03,340
No,
I think it's great question.

1204
01:07:03,341 --> 01:07:07,150
I think it's incredibly important that 
we have interpretability and our systems

1205
01:07:07,151 --> 01:07:12,151
for a couple of reasons.
One is it's useful to advance the 

1206
01:07:12,151 --> 01:07:15,481
science if the better you understand the
current systems obviously in what are 

1207
01:07:15,481 --> 01:07:15,481
their,
their limitations are,

1208
01:07:15,481 --> 01:07:16,450
but for any,
uh,

1209
01:07:16,480 --> 01:07:21,480
once you start deploying these ai 
systems for any safe safety critical 

1210
01:07:21,480 --> 01:07:25,170
application,
of course you would need to understand 

1211
01:07:25,170 --> 01:07:28,110
why the decision was made and I would 
actually advocate further and always 

1212
01:07:28,110 --> 01:07:31,561
have a human in the loop to make the 
final decision and think of the Ai as a 

1213
01:07:31,561 --> 01:07:33,730
tool that provides information to that 
ultimate human decision maker.

1214
01:07:34,180 --> 01:07:35,980
Um,
and in order to do that,

1215
01:07:35,981 --> 01:07:40,981
we need to explain these black box 
systems better and I don't worry about 

1216
01:07:40,981 --> 01:07:44,611
that as much as other people.
So I think we're just going through a 

1217
01:07:44,611 --> 01:07:47,491
phase at the moment where you can think 
of it in terms of the evolution of AI 

1218
01:07:47,491 --> 01:07:48,220
systems as in the last decade.

1219
01:07:48,220 --> 01:07:52,030
There's been a huge explosion of ai 
systems that are really good now and can

1220
01:07:52,031 --> 01:07:53,750
do interesting things.
Um,

1221
01:07:53,910 --> 01:07:56,160
but that's very new.
And uh,

1222
01:07:56,310 --> 01:07:58,240
the,
the challenge in that they know the last

1223
01:07:58,241 --> 01:08:01,180
10 years has been can we get these 
systems working at all?

1224
01:08:01,360 --> 01:08:04,900
Nevermind about interpretability now we 
have the working,

1225
01:08:05,020 --> 01:08:10,020
we have something to work on,
reverse engineer and analyze now asks 

1226
01:08:10,020 --> 01:08:13,750
and many other teams around the world 
are concentrating on building analysis 

1227
01:08:13,750 --> 01:08:14,410
tools.
So we're being analysis tools,

1228
01:08:14,750 --> 01:08:16,820
visualization tools,
all sorts of things.

1229
01:08:16,821 --> 01:08:21,821
Even doing behavioral testing like more 
like you'd have in a psychology lab,

1230
01:08:21,860 --> 01:08:22,450
you know,
to look,

1231
01:08:22,460 --> 01:08:25,160
think about both behaviorally testing 
it,

1232
01:08:25,430 --> 01:08:28,310
looking into the architecture,
measuring it,

1233
01:08:28,340 --> 01:08:30,890
almost like a doing brain analysis like 
neuroscience,

1234
01:08:30,891 --> 01:08:35,480
but on an artificial brain.
And so with all those tools are very,

1235
01:08:35,481 --> 01:08:38,930
very embryonic right now because I've 
only been in the last couple of years of

1236
01:08:38,950 --> 01:08:42,110
this being started to be worked on.
And I'm pretty sure I'm pretty confident

1237
01:08:42,111 --> 01:08:45,890
that with another sort of five years of 
work on those kinds of tools,

1238
01:08:46,250 --> 01:08:49,280
a lot of these systems right now that 
look quite black box,

1239
01:08:50,540 --> 01:08:54,440
black box systems will become 
sustainable and adaptable.

1240
01:08:54,710 --> 01:08:55,700
So,
uh,

1241
01:08:56,060 --> 01:08:57,650
you know,
I think it's vital,

1242
01:08:57,680 --> 01:09:02,190
but I think we're just a bit on the 
starting point of that and you know,

1243
01:09:02,240 --> 01:09:04,430
I wouldn't worry too much about that at 
the moment.

1244
01:09:04,580 --> 01:09:06,230
A lot of these systems are quite black 
box.

1245
01:09:06,710 --> 01:09:09,740
We've been rigorously program to stick 
to now we've crushed through it.

1246
01:09:09,830 --> 01:09:11,510
Let's be knocking.
Take one more question.

1247
01:09:11,520 --> 01:09:14,600
You going to say one more question?
Let's take the,

1248
01:09:15,040 --> 01:09:15,910
the woman that

1249
01:09:17,910 --> 01:09:21,200
then we can carry on over a drink.
He said offering up that misty will.

1250
01:09:21,520 --> 01:09:22,270
Okay.
Thank you.

1251
01:09:22,400 --> 01:09:25,840
Last word has always been pretty fearful
of ai thing.

1252
01:09:25,860 --> 01:09:28,440
There were presentations,
things like westward and terminator,

1253
01:09:28,450 --> 01:09:33,450
etc.
And I'm very glad that you mentioned 

1254
01:09:33,450 --> 01:09:35,701
ethics.
Just whAt do you think of the artworks 

1255
01:09:35,701 --> 01:09:38,851
for presentation of ai and how,
how your advIse on preventing that kind 

1256
01:09:39,251 --> 01:09:40,360
of future from happening?

1257
01:09:41,610 --> 01:09:42,630
Yeah,
I think the art world,

1258
01:09:42,631 --> 01:09:47,631
it would be nice if there were,
if it was a little bit more creative in 

1259
01:09:47,631 --> 01:09:47,631
some sense,
right,

1260
01:09:47,631 --> 01:09:51,501
because I think it's easy to.
I mean it's obviously more dramatic to 

1261
01:09:51,501 --> 01:09:52,400
have a,
you know,

1262
01:09:52,470 --> 01:09:56,670
dystopian futures and villains and so 
on.

1263
01:09:56,930 --> 01:09:59,040
It's obviously creates more excitement.
Um,

1264
01:09:59,190 --> 01:10:01,730
but obviously that's a,
you know,

1265
01:10:01,740 --> 01:10:04,320
I think it's,
most of those scenarios are pure science

1266
01:10:04,321 --> 01:10:06,330
fiction and we shouldn't worry about 
them too much.

1267
01:10:06,660 --> 01:10:11,660
I think that we actually need a lot of 
science fiction can be very helpful in 

1268
01:10:12,001 --> 01:10:15,510
terms of lots of scientists,
including myself who inspired by science

1269
01:10:15,511 --> 01:10:17,610
fiction to make some of the things they 
read.

1270
01:10:17,910 --> 01:10:19,920
Certainly for me,
I read probably too much science fiction

1271
01:10:19,921 --> 01:10:24,921
when I was young to try and make that 
come true and there are actually 

1272
01:10:24,921 --> 01:10:27,891
brilliant books about futures which with
ais and humans in them that have really 

1273
01:10:28,591 --> 01:10:33,591
interesting worlds like ian banks,
great writer his and also ask them off 

1274
01:10:34,320 --> 01:10:36,180
not his robot stories,
which I've never read.

1275
01:10:36,181 --> 01:10:39,210
Actually there is that.
Things like the foundation series,

1276
01:10:39,211 --> 01:10:44,211
which is more serious scifi I think is 
very interesting and it will be useful I

1277
01:10:44,311 --> 01:10:46,920
think,
to have to explore the whole spectrum of

1278
01:10:46,921 --> 01:10:50,820
possibilities with ai rather than this 
sort of quite crude,

1279
01:10:51,210 --> 01:10:55,440
a narrow way of exploring it.
And I didn't particularly like westworld

1280
01:10:55,441 --> 01:10:57,270
for example.
I think it's pretty boring and obvious.

1281
01:10:58,420 --> 01:11:03,420
I think it's a good note on which to end
the scientist comes into the royal 

1282
01:11:03,420 --> 01:11:06,721
academy and says that the outward needs 
to be more creative and that actually 

1283
01:11:06,721 --> 01:11:09,301
will accept science fiction and 
hollywood films and television as part 

1284
01:11:09,301 --> 01:11:09,640
of the broad visual culture.
Um,

1285
01:11:09,730 --> 01:11:14,040
we've Just finished a festival of ideas 
were in the main artistic practitioners.

1286
01:11:14,041 --> 01:11:16,260
Philosophers,
theorists have come to the academy.

1287
01:11:16,540 --> 01:11:20,070
We need to expand our networks.
We need to get out more.

1288
01:11:20,160 --> 01:11:25,160
We certainly need to generate more 
discussions with scientists at the 

1289
01:11:25,160 --> 01:11:26,580
cutting edge of artificial intelligence,
among other things.

1290
01:11:26,730 --> 01:11:31,730
We probably need to make this place a 
forum where human consciousness gets 

1291
01:11:31,730 --> 01:11:32,520
debated and you'd be a great person to 
do that.

1292
01:11:32,521 --> 01:11:34,750
But for this evening,
dennis hassabis,

1293
01:11:35,040 --> 01:11:36,050
thank you so much.

1294
01:11:36,390 --> 01:11:40,400
Thank you.

