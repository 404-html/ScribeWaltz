WEBVTT

1
00:00:01.050 --> 00:00:01.561
In this video,

2
00:00:01.561 --> 00:00:04.860
we'll discuss a variant of the contrasted average in south with them.

3
00:00:04.861 --> 00:00:07.740
No one has a persistent contrasted divergence.

4
00:00:10.020 --> 00:00:13.590
So we've seen that the contrasted divergence divergencies actually a fairly

5
00:00:13.591 --> 00:00:16.770
simple algorithm for training restricted Boltzmann machine.

6
00:00:17.010 --> 00:00:19.530
It requires that for each training example,

7
00:00:19.680 --> 00:00:23.760
we sample in a what we call a negative sample instilled,

8
00:00:23.970 --> 00:00:27.780
which is generated by performing a just a few steps of Gib sampling,

9
00:00:28.500 --> 00:00:32.190
which is initialized at the training observation x,

10
00:00:32.191 --> 00:00:37.191
t and then the update of the parameters as a very simple form which involves a

11
00:00:39.480 --> 00:00:44.480
computations both at the training example and at the negative sample X.

12
00:00:45.030 --> 00:00:49.800
Still,
I've also discussed at the end of the previous video that this,

13
00:00:49.890 --> 00:00:53.960
uh,
this algorithm CDK,
uh,
where,
uh,

14
00:00:54.000 --> 00:00:56.940
Kay is the number of gifts step,
uh,

15
00:00:57.000 --> 00:01:00.780
was actually a very good algorithm for training a restricted Boltzmann machine

16
00:01:00.781 --> 00:01:03.650
to extract some useful features,
uh,

17
00:01:03.690 --> 00:01:08.320
which is represented by the vector h of X.
So,

18
00:01:08.360 --> 00:01:11.670
um,
h of x is the vector of hidden units,

19
00:01:11.671 --> 00:01:13.940
probability and a,

20
00:01:13.980 --> 00:01:18.150
what we could do is train a restricted Boltzmann machine on some unlabeled data

21
00:01:18.600 --> 00:01:23.600
and then we could change our training set or our data set in general,

22
00:01:23.761 --> 00:01:28.320
all of our data so that instead of representing it by the x factor,

23
00:01:28.321 --> 00:01:31.290
we could represent it as the h of x factor.

24
00:01:31.620 --> 00:01:36.600
And then use that representation to train,
uh,
say a classifier safe.

25
00:01:36.601 --> 00:01:40.730
We're classifying images of digits.
We could feed that to a,
uh,

26
00:01:40.770 --> 00:01:45.180
that new representation to a linear classifier or another neural network.

27
00:01:45.690 --> 00:01:49.230
Uh,
we'll discuss also,
we can use that to initialize a neural network.

28
00:01:49.530 --> 00:01:52.590
But the CD a algorithm with gay couples,

29
00:01:52.591 --> 00:01:57.591
one actually works really well for extracting a good set of features.

30
00:01:58.380 --> 00:02:02.730
However,
when it comes to evaluating,
say,
a validation set,

31
00:02:02.731 --> 00:02:03.564
our test said,

32
00:02:03.660 --> 00:02:08.660
what's the value of the average negative log probability or in the log

33
00:02:08.671 --> 00:02:13.120
likelihood?
Uh,
it turns out that it's not doing that well.
Uh,

34
00:02:13.160 --> 00:02:17.110
it tends to perform a not so great.
Um,

35
00:02:17.280 --> 00:02:20.190
the reason for that is,
uh,

36
00:02:20.191 --> 00:02:23.610
so let's look back at our intuition for what city does.

37
00:02:23.840 --> 00:02:28.840
So imagine have an energy function which looks something like this where this

38
00:02:28.951 --> 00:02:31.610
would be a linear,
uh,

39
00:02:31.620 --> 00:02:34.410
organization of all the values for the x factor.

40
00:02:34.840 --> 00:02:36.750
And Imagine I have a training example here.

41
00:02:36.930 --> 00:02:39.600
So what that would be one training example.

42
00:02:39.690 --> 00:02:43.620
So what city does to get an extended is that it starts give sampling there.

43
00:02:43.621 --> 00:02:48.621
And what Gibbs sampling does is that essentially jumps like this and say I do

44
00:02:49.261 --> 00:02:51.180
case steps.
Then maybe,
uh,

45
00:02:51.300 --> 00:02:55.650
give somebody with a essentially fall say on that point here.
Uh,

46
00:02:55.840 --> 00:03:00.200
it doesn't go much beyond that point because the energy is much higher here.

47
00:03:00.201 --> 00:03:02.790
So the probability of sampling in that,
uh,

48
00:03:03.130 --> 00:03:05.240
in these points here is actually quite small.

49
00:03:06.280 --> 00:03:06.590
<v 1>Okay.</v>

50
00:03:06.590 --> 00:03:09.340
<v 0>Now because we do on the case steps of give sampling,</v>

51
00:03:09.360 --> 00:03:13.970
actually the probability of going well over here or here is actually quite small

52
00:03:13.971 --> 00:03:17.180
because I'm doing just a very few steps of give sampling.

53
00:03:17.210 --> 00:03:20.070
And each step really goes a,

54
00:03:20.090 --> 00:03:23.090
it doesn't go very far away from its previous point.

55
00:03:23.630 --> 00:03:27.740
I imagine that my energy function is such that there's then the cliff.

56
00:03:27.741 --> 00:03:30.680
And then there are values for x here,

57
00:03:30.681 --> 00:03:32.750
which actually do have very small energy.

58
00:03:33.290 --> 00:03:37.220
But because I'm using a few steps of give sampling,
uh,

59
00:03:37.430 --> 00:03:40.700
it's actually impossible for,
uh,
the,
uh,

60
00:03:40.730 --> 00:03:44.030
gift sampling change actually go like this.

61
00:03:44.031 --> 00:03:46.100
And I said eventually reached these points.

62
00:03:46.550 --> 00:03:50.530
So this might going from here to here might require a lot of steps of Gif

63
00:03:50.531 --> 00:03:55.370
sampling for,
uh,
such a,
uh,
a trajectory to be likely.

64
00:03:55.880 --> 00:03:59.930
And because of this,
it means that there will be regions with CD,

65
00:04:00.130 --> 00:04:03.510
a one or a CD five,
uh,
which,
uh,

66
00:04:03.530 --> 00:04:05.810
will not get their energies raised.

67
00:04:06.080 --> 00:04:09.650
And because these are regions of the input space,

68
00:04:09.680 --> 00:04:11.360
which doesn't have training data,

69
00:04:11.960 --> 00:04:15.860
then it means that when we were normalized the exponential in a negative

70
00:04:15.861 --> 00:04:20.470
energies,
the relative probability of this is going to be higher.
I see.

71
00:04:20.471 --> 00:04:24.660
Going to be lower,
sorry,
because we've assigned low probability,
uh,

72
00:04:24.680 --> 00:04:28.850
elsewhere and because everything needs to sum to one that it means that if I'm

73
00:04:28.851 --> 00:04:31.980
assigning probably here,
then I need to,
to,
uh,

74
00:04:32.000 --> 00:04:35.860
assign less probability here.
So,
uh,

75
00:04:35.870 --> 00:04:39.710
we'll see a very simple modification of the contrast of diverging salary with

76
00:04:39.711 --> 00:04:43.640
them,
which was essentially partly solve this problem and a yield,

77
00:04:43.641 --> 00:04:47.400
a much better estimator of the,
uh,

78
00:04:47.440 --> 00:04:49.550
in a much better model of the,

79
00:04:49.740 --> 00:04:53.820
a probability distribution that generated our training set.

80
00:04:56.400 --> 00:04:56.660
<v 1>Okay.</v>

81
00:04:56.660 --> 00:04:59.110
<v 0>So the idea is actually quite simple.
Um,</v>

82
00:04:59.240 --> 00:05:02.930
because we're performing few steps of GIB sampling in between updates,

83
00:05:02.960 --> 00:05:06.890
it means that we can never wander too far away from our training example.

84
00:05:07.580 --> 00:05:10.870
However,
in the previous iteration,
uh,

85
00:05:10.880 --> 00:05:15.560
so before our previous update,
we actually had performed some gifts,

86
00:05:15.640 --> 00:05:18.250
a sample,
and it's actually kind of the same,

87
00:05:18.280 --> 00:05:23.280
a shame that we can't reuse this sampling to generate our next sample of our

88
00:05:23.811 --> 00:05:28.070
next update.
And the idea of persistency is actually quite simple.

89
00:05:28.820 --> 00:05:32.270
Instead of using of initializing I Gibbs,

90
00:05:32.271 --> 00:05:34.700
Shane to the current training example.

91
00:05:35.740 --> 00:05:36.020
<v 1>Okay.</v>

92
00:05:36.020 --> 00:05:37.730
<v 0>We will actually use,</v>

93
00:05:38.310 --> 00:05:38.700
<v 1>okay,</v>

94
00:05:38.700 --> 00:05:42.420
<v 0>the,
uh,
exterior that we sample at the previous iteration.</v>

95
00:05:42.421 --> 00:05:45.910
So before our updates,
uh,
uh,
uh,

96
00:05:46.080 --> 00:05:49.860
based on the previous training example,
and,
and that's it,

97
00:05:49.861 --> 00:05:52.020
the rest of the algorithm stays exactly the same.

98
00:05:52.200 --> 00:05:54.690
So this means that if I've performed,
say,

99
00:05:54.691 --> 00:05:59.000
10 parameter updates and I'm performing just step of Gif sampling for every

100
00:05:59.001 --> 00:05:59.834
update,

101
00:05:59.870 --> 00:06:04.870
x stilled is going to be fairly close to the result of performing 10 steps of

102
00:06:06.681 --> 00:06:08.780
good sampling instead of just a single step.

103
00:06:09.470 --> 00:06:13.520
It's not exactly this because in between,
so during the gifts chain,

104
00:06:13.910 --> 00:06:16.950
uh,
the parameters of the RBM will have changed,
uh,

105
00:06:17.150 --> 00:06:20.420
because I'm updating the RBM as I'm doing this,
uh,

106
00:06:20.450 --> 00:06:25.300
but still it will allow for x still to wonder further away,
uh,
much more,
uh,

107
00:06:25.370 --> 00:06:27.710
far away from the training examples.

108
00:06:27.740 --> 00:06:30.920
Then if I keep re initializing the gifts chain,
uh,

109
00:06:31.100 --> 00:06:34.100
to the training example and the in practice,

110
00:06:34.101 --> 00:06:38.050
what we see is that indeed by performance is very small change.
Uh,

111
00:06:38.210 --> 00:06:43.210
we get a much better estimator of the distribution and we got much better

112
00:06:43.791 --> 00:06:48.700
performance in terms of a negative log likelihood of the validations or that

113
00:06:48.720 --> 00:06:53.600
that's set.
So of new examples.
Also,
I should mention that in practice,

114
00:06:53.660 --> 00:06:57.470
often instead of performing this in the stochastic,

115
00:06:57.471 --> 00:07:02.150
we will actually use a mini batch variants of this algorithm.
And also,

116
00:07:02.151 --> 00:07:06.890
instead of maintaining just a single gifts chain that keeps being,
uh,
updated.

117
00:07:07.100 --> 00:07:12.040
So instead of having just one sequence of x,
still,
we actually often maintain,
uh,

118
00:07:12.041 --> 00:07:15.890
a mini batch of x stills.
Uh,
and so in other words,

119
00:07:15.891 --> 00:07:20.300
there will be many gifts change that will be may Gibbs,
uh,
sorry,

120
00:07:20.301 --> 00:07:23.510
Gibbs change that will be maintained in parallel during training.

121
00:07:24.320 --> 00:07:25.580
So if you want to know more,

122
00:07:25.581 --> 00:07:30.581
you can look at the paper by Telemann ICML in 2008 and a,

123
00:07:31.191 --> 00:07:34.070
so that concludes our explanation for persistency.

